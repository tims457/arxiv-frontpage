{"created":"2024-02-27 18:59:18","title":"Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning","abstract":"A common failure mode for policies trained with imitation is compounding execution errors at test time. When the learned policy encounters states that were not present in the expert demonstrations, the policy fails, leading to degenerate behavior. The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states. However, in practice, this is often prohibitively expensive. In this work, we propose Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without the cost for eye-in-hand imitation learning problems. Instead of collecting new samples to cover out-of-distribution states, DMD uses recent advances in diffusion models to create these samples with diffusion models. This leads to robust performance from few demonstrations. In experiments conducted for non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a success rate of 80% with as few as 8 expert demonstrations, where naive behavior cloning reaches only 20%. DMD also outperform competing NeRF-based augmentation schemes by 50%.","sentences":["A common failure mode for policies trained with imitation is compounding execution errors at test time.","When the learned policy encounters states that were not present in the expert demonstrations, the policy fails, leading to degenerate behavior.","The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states.","However, in practice, this is often prohibitively expensive.","In this work, we propose Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without the cost for eye-in-hand imitation learning problems.","Instead of collecting new samples to cover out-of-distribution states, DMD uses recent advances in diffusion models to create these samples with diffusion models.","This leads to robust performance from few demonstrations.","In experiments conducted for non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a success rate of 80% with as few as 8 expert demonstrations, where naive behavior cloning reaches only 20%.","DMD also outperform competing NeRF-based augmentation schemes by 50%."],"url":"http://arxiv.org/abs/2402.17768v1","category":"cs.RO"}
{"created":"2024-02-27 18:58:54","title":"Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator","abstract":"Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment). In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments. We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments. Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot. An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system. We will open source code and models for others to replicate and build upon our system.","sentences":["Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment).","In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments.","We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments.","Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot.","An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system.","We will open source code and models for others to replicate and build upon our system."],"url":"http://arxiv.org/abs/2402.17767v1","category":"cs.RO"}
{"created":"2024-02-27 18:56:19","title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","abstract":"Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.","sentences":["Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs).","In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}.","It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption.","More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective.","Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs."],"url":"http://arxiv.org/abs/2402.17764v1","category":"cs.CL"}
{"created":"2024-02-27 18:55:50","title":"Reducing Unnecessary Alerts in Pedestrian Protection Systems Based on P2V Communications","abstract":"There are different proposals in the literature on how to protect pedestrians using warning systems to alert drivers of their presence. They can be based on onboard perception systems or wireless communications. The evaluation of these systems has been focused on testing their ability to detect pedestrians. A problem that has received much less attention is the possibility of generating too many alerts in the warning systems. In this paper, we propose and analyze four different algorithms to take the decision on generating alerts in a warning system that is based on direct wireless communications between vehicles and pedestrians. With the algorithms, we explore different strategies to reduce unnecessary alerts. The feasibility of the implementation of the algorithms was evaluated with a deployment using real equipment, and tests were carried out to verify their behavior in real scenarios. The ability of each algorithm to reduce unnecessary alerts was evaluated with realistic simulations in an urban scenario, using a traffic simulator with vehicular and pedestrian flows. The results show the importance of tackling the problem of driver overload in warning systems, and that it is not straightforward to predict the load of alerts generated by an algorithm in a large-scale deployment, in which there are multiple interactions between vehicles and pedestrians.","sentences":["There are different proposals in the literature on how to protect pedestrians using warning systems to alert drivers of their presence.","They can be based on onboard perception systems or wireless communications.","The evaluation of these systems has been focused on testing their ability to detect pedestrians.","A problem that has received much less attention is the possibility of generating too many alerts in the warning systems.","In this paper, we propose and analyze four different algorithms to take the decision on generating alerts in a warning system that is based on direct wireless communications between vehicles and pedestrians.","With the algorithms, we explore different strategies to reduce unnecessary alerts.","The feasibility of the implementation of the algorithms was evaluated with a deployment using real equipment, and tests were carried out to verify their behavior in real scenarios.","The ability of each algorithm to reduce unnecessary alerts was evaluated with realistic simulations in an urban scenario, using a traffic simulator with vehicular and pedestrian flows.","The results show the importance of tackling the problem of driver overload in warning systems, and that it is not straightforward to predict the load of alerts generated by an algorithm in a large-scale deployment, in which there are multiple interactions between vehicles and pedestrians."],"url":"http://arxiv.org/abs/2402.17763v1","category":"cs.NI"}
{"created":"2024-02-27 18:55:13","title":"Quantum Circuit Discovery for Fault-Tolerant Logical State Preparation with Reinforcement Learning","abstract":"One of the key aspects in the realization of large-scale fault-tolerant quantum computers is quantum error correction (QEC). The first essential step of QEC is to encode the logical state into physical qubits in a fault-tolerant manner. Recently, flag-based protocols have been introduced that use ancillary qubits to flag harmful errors. However, there is no clear recipe for finding a compact quantum circuit with flag-based protocols for fault-tolerant logical state preparation. It is even more difficult when we consider the hardware constraints, such as qubit connectivity and gate set. In this work, we propose and explore reinforcement learning (RL) to automatically discover compact and hardware-adapted quantum circuits that fault-tolerantly prepare the logical state of a QEC code. We show that RL discovers circuits with fewer gates and ancillary qubits than published results without and with hardware constraints of up to 15 physical qubits. Furthermore, RL allows for straightforward exploration of different qubit connectivities and the use of transfer learning to accelerate the discovery. More generally, our work opens the door towards the use of RL for the discovery of fault-tolerant quantum circuits for addressing tasks beyond state preparation, including magic state preparation, logical gate synthesis, or syndrome measurement.","sentences":["One of the key aspects in the realization of large-scale fault-tolerant quantum computers is quantum error correction (QEC).","The first essential step of QEC is to encode the logical state into physical qubits in a fault-tolerant manner.","Recently, flag-based protocols have been introduced that use ancillary qubits to flag harmful errors.","However, there is no clear recipe for finding a compact quantum circuit with flag-based protocols for fault-tolerant logical state preparation.","It is even more difficult when we consider the hardware constraints, such as qubit connectivity and gate set.","In this work, we propose and explore reinforcement learning (RL) to automatically discover compact and hardware-adapted quantum circuits that fault-tolerantly prepare the logical state of a QEC code.","We show that RL discovers circuits with fewer gates and ancillary qubits than published results without and with hardware constraints of up to 15 physical qubits.","Furthermore, RL allows for straightforward exploration of different qubit connectivities and the use of transfer learning to accelerate the discovery.","More generally, our work opens the door towards the use of RL for the discovery of fault-tolerant quantum circuits for addressing tasks beyond state preparation, including magic state preparation, logical gate synthesis, or syndrome measurement."],"url":"http://arxiv.org/abs/2402.17761v1","category":"quant-ph"}
{"created":"2024-02-27 18:53:18","title":"Learning to Program Variational Quantum Circuits with Fast Weights","abstract":"Quantum Machine Learning (QML) has surfaced as a pioneering framework addressing sequential control tasks and time-series modeling. It has demonstrated empirical quantum advantages notably within domains such as Reinforcement Learning (RL) and time-series prediction. A significant advancement lies in Quantum Recurrent Neural Networks (QRNNs), specifically tailored for memory-intensive tasks encompassing partially observable environments and non-linear time-series prediction. Nevertheless, QRNN-based models encounter challenges, notably prolonged training duration stemming from the necessity to compute quantum gradients using backpropagation-through-time (BPTT). This predicament exacerbates when executing the complete model on quantum devices, primarily due to the substantial demand for circuit evaluation arising from the parameter-shift rule. This paper introduces the Quantum Fast Weight Programmers (QFWP) as a solution to the temporal or sequential learning challenge. The QFWP leverages a classical neural network (referred to as the 'slow programmer') functioning as a quantum programmer to swiftly modify the parameters of a variational quantum circuit (termed the 'fast programmer'). Instead of completely overwriting the fast programmer at each time-step, the slow programmer generates parameter changes or updates for the quantum circuit parameters. This approach enables the fast programmer to incorporate past observations or information. Notably, the proposed QFWP model achieves learning of temporal dependencies without necessitating the use of quantum recurrent neural networks. Numerical simulations conducted in this study showcase the efficacy of the proposed QFWP model in both time-series prediction and RL tasks. The model exhibits performance levels either comparable to or surpassing those achieved by QLSTM-based models.","sentences":["Quantum Machine Learning (QML) has surfaced as a pioneering framework addressing sequential control tasks and time-series modeling.","It has demonstrated empirical quantum advantages notably within domains such as Reinforcement Learning (RL) and time-series prediction.","A significant advancement lies in Quantum Recurrent Neural Networks (QRNNs), specifically tailored for memory-intensive tasks encompassing partially observable environments and non-linear time-series prediction.","Nevertheless, QRNN-based models encounter challenges, notably prolonged training duration stemming from the necessity to compute quantum gradients using backpropagation-through-time (BPTT).","This predicament exacerbates when executing the complete model on quantum devices, primarily due to the substantial demand for circuit evaluation arising from the parameter-shift rule.","This paper introduces the Quantum Fast Weight Programmers (QFWP) as a solution to the temporal or sequential learning challenge.","The QFWP leverages a classical neural network (referred to as the 'slow programmer') functioning as a quantum programmer to swiftly modify the parameters of a variational quantum circuit (termed the 'fast programmer').","Instead of completely overwriting the fast programmer at each time-step, the slow programmer generates parameter changes or updates for the quantum circuit parameters.","This approach enables the fast programmer to incorporate past observations or information.","Notably, the proposed QFWP model achieves learning of temporal dependencies without necessitating the use of quantum recurrent neural networks.","Numerical simulations conducted in this study showcase the efficacy of the proposed QFWP model in both time-series prediction and RL tasks.","The model exhibits performance levels either comparable to or surpassing those achieved by QLSTM-based models."],"url":"http://arxiv.org/abs/2402.17760v1","category":"quant-ph"}
{"created":"2024-02-27 18:52:19","title":"Towards Optimal Learning of Language Models","abstract":"This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an \"LM-training-as-lossless-compression\" view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods. Our code can be found at https://aka.ms/LearningLaw.","sentences":["This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance.","Specifically, we present a theory for the optimal learning of LMs.","We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an \"LM-training-as-lossless-compression\" view.","Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective.","The theorem is then validated by experiments on a linear classification and a real-world language modeling task.","Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods.","Our code can be found at https://aka.ms/LearningLaw."],"url":"http://arxiv.org/abs/2402.17759v1","category":"cs.CL"}
{"created":"2024-02-27 18:51:52","title":"ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily Living","abstract":"Hand-Object Interactions (HOIs) are conditioned on spatial and temporal contexts like surrounding objects, pre- vious actions, and future intents (for example, grasping and handover actions vary greatly based on objects proximity and trajectory obstruction). However, existing datasets for 4D HOI (3D HOI over time) are limited to one subject inter- acting with one object only. This restricts the generalization of learning-based HOI methods trained on those datasets. We introduce ADL4D, a dataset of up to two subjects inter- acting with different sets of objects performing Activities of Daily Living (ADL) like breakfast or lunch preparation ac- tivities. The transition between multiple objects to complete a certain task over time introduces a unique context lacking in existing datasets. Our dataset consists of 75 sequences with a total of 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained action annotations. We develop an automatic system for multi-view multi-hand 3D pose an- notation capable of tracking hand poses over time. We inte- grate and test it against publicly available datasets. Finally, we evaluate our dataset on the tasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS).","sentences":["Hand-Object Interactions (HOIs) are conditioned on spatial and temporal contexts like surrounding objects, pre- vious actions, and future intents (for example, grasping and handover actions vary greatly based on objects proximity and trajectory obstruction).","However, existing datasets for 4D HOI (3D HOI over time) are limited to one subject inter- acting with one object only.","This restricts the generalization of learning-based HOI methods trained on those datasets.","We introduce ADL4D, a dataset of up to two subjects inter- acting with different sets of objects performing Activities of Daily Living (ADL) like breakfast or lunch preparation ac- tivities.","The transition between multiple objects to complete a certain task over time introduces a unique context lacking in existing datasets.","Our dataset consists of 75 sequences with a total of 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained action annotations.","We develop an automatic system for multi-view multi-hand 3D pose an- notation capable of tracking hand poses over time.","We inte- grate and test it against publicly available datasets.","Finally, we evaluate our dataset on the tasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS)."],"url":"http://arxiv.org/abs/2402.17758v1","category":"cs.CV"}
{"created":"2024-02-27 18:42:31","title":"Evaluating Very Long-Term Conversational Memory of LLM Agents","abstract":"Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.","sentences":["Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions.","Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored.","To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs.","Moreover, we equip each agent with the capability of sharing and reacting to images.","The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs.","Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions.","Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks.","Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues.","Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance."],"url":"http://arxiv.org/abs/2402.17753v1","category":"cs.CL"}
{"created":"2024-02-27 18:37:01","title":"$\u03b6$-QVAE: A Quantum Variational Autoencoder utilizing Regularized Mixed-state Latent Representations","abstract":"A major challenge in near-term quantum computing is its application to large real-world datasets due to scarce quantum hardware resources. One approach to enabling tractable quantum models for such datasets involves compressing the original data to manageable dimensions while still representing essential information for downstream analysis. In classical machine learning, variational autoencoders (VAEs) facilitate efficient data compression, representation learning for subsequent tasks, and novel data generation. However, no model has been proposed that exactly captures all of these features for direct application to quantum data on quantum computers. Some existing quantum models for data compression lack regularization of latent representations, thus preventing direct use for generation and control of generalization. Others are hybrid models with only some internal quantum components, impeding direct training on quantum data. To bridge this gap, we present a fully quantum framework, $\\zeta$-QVAE, which encompasses all the capabilities of classical VAEs and can be directly applied for both classical and quantum data compression. Our model utilizes regularized mixed states to attain optimal latent representations. It accommodates various divergences for reconstruction and regularization. Furthermore, by accommodating mixed states at every stage, it can utilize the full-data density matrix and allow for a \"global\" training objective. Doing so, in turn, makes efficient optimization possible and has potential implications for private and federated learning. In addition to exploring the theoretical properties of $\\zeta$-QVAE, we demonstrate its performance on representative genomics and synthetic data. Our results consistently indicate that $\\zeta$-QVAE exhibits similar or better performance compared to matched classical models.","sentences":["A major challenge in near-term quantum computing is its application to large real-world datasets due to scarce quantum hardware resources.","One approach to enabling tractable quantum models for such datasets involves compressing the original data to manageable dimensions while still representing essential information for downstream analysis.","In classical machine learning, variational autoencoders (VAEs) facilitate efficient data compression, representation learning for subsequent tasks, and novel data generation.","However, no model has been proposed that exactly captures all of these features for direct application to quantum data on quantum computers.","Some existing quantum models for data compression lack regularization of latent representations, thus preventing direct use for generation and control of generalization.","Others are hybrid models with only some internal quantum components, impeding direct training on quantum data.","To bridge this gap, we present a fully quantum framework, $\\zeta$-QVAE, which encompasses all the capabilities of classical VAEs and can be directly applied for both classical and quantum data compression.","Our model utilizes regularized mixed states to attain optimal latent representations.","It accommodates various divergences for reconstruction and regularization.","Furthermore, by accommodating mixed states at every stage, it can utilize the full-data density matrix and allow for a \"global\" training objective.","Doing so, in turn, makes efficient optimization possible and has potential implications for private and federated learning.","In addition to exploring the theoretical properties of $\\zeta$-QVAE, we demonstrate its performance on representative genomics and synthetic data.","Our results consistently indicate that $\\zeta$-QVAE exhibits similar or better performance compared to matched classical models."],"url":"http://arxiv.org/abs/2402.17749v1","category":"quant-ph"}
{"created":"2024-02-27 18:32:11","title":"When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning","abstract":"Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observable settings and propose research directions to help tackle these challenges.","sentences":["Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment.","What happens when human feedback is based only on partial observations?","We formally define two failure cases: deception and overjustification.","Modeling the human as Boltzmann-rational w.r.t.","a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both.","To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function.","In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity.","We caution against blindly applying RLHF in partially observable settings and propose research directions to help tackle these challenges."],"url":"http://arxiv.org/abs/2402.17747v1","category":"cs.LG"}
{"created":"2024-02-27 18:21:42","title":"Approaching Periodic Systems in Ensemble Density Functional Theory \\textit{via} Finite One-Dimensional Models","abstract":"Ensemble Density Functional Theory (EDFT) is a generalization of ground-state Density Functional Theory (GS DFT), which is based on an exact formal theory of finite collections of a system's ground and excited states. EDFT in various forms has been shown to improve the accuracy of calculated energy level differences in isolated model systems, atoms, and molecules, but it is not yet clear how EDFT could be used to calculate band gaps for periodic systems. We extend the application of EDFT toward periodic systems by estimating the thermodynamic limit with increasingly large finite one-dimensional \\enquote{particle in a box} systems, which approach the uniform electron gas. Using ensemble-generalized Hartree and Local Spin Density Approximation (LSDA) exchange-correlation functionals, we find that corrections go to zero in the infinite limit, as expected for a metallic system. However, there is a correction to the effective mass, as estimated from bi- and tri-ensembles, indicating promise for non-trivial results for EDFT on periodic systems. Singlet excitation energies are found to be positive, but triplet excitation energies are sometimes negative (a triplet instability), pointing to deficiencies of the approximations.","sentences":["Ensemble Density Functional Theory (EDFT) is a generalization of ground-state Density Functional Theory (GS DFT), which is based on an exact formal theory of finite collections of a system's ground and excited states.","EDFT in various forms has been shown to improve the accuracy of calculated energy level differences in isolated model systems, atoms, and molecules, but it is not yet clear how EDFT could be used to calculate band gaps for periodic systems.","We extend the application of EDFT toward periodic systems by estimating the thermodynamic limit with increasingly large finite one-dimensional \\enquote{particle in a box} systems, which approach the uniform electron gas.","Using ensemble-generalized Hartree and Local Spin Density Approximation (LSDA) exchange-correlation functionals, we find that corrections go to zero in the infinite limit, as expected for a metallic system.","However, there is a correction to the effective mass, as estimated from bi- and tri-ensembles, indicating promise for non-trivial results for EDFT on periodic systems.","Singlet excitation energies are found to be positive, but triplet excitation energies are sometimes negative (a triplet instability), pointing to deficiencies of the approximations."],"url":"http://arxiv.org/abs/2402.17742v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-27 18:19:32","title":"Testing the isotropy of cosmic acceleration with Pantheon+SH0ES: A cosmographic analysis","abstract":"We use a recent Pantheon+SH0ES compilation of Type Ia Supernova distance measurements at low-redshift, i.e., $0.01 \\leq z \\leq 0.10$, in order to investigate the directional dependency of the deceleration parameter ($q_0$) in different patches ($60^{\\circ}$ size) across the sky, as a probe of the statistical isotropy of the Universe. We adopt a cosmographic approach to compute the cosmological distances, fixing $H_0$ and $M_B$ to reference values provided by the collaboration. By looking at 500 different patches randomly taken across the sky, we find a maximum $\\sim 3\\sigma$ CL anisotropy level for $q_0$, whose direction points orthogonally to the CMB dipole axis, i.e., $(RA^{\\rm SN},DEC^{\\rm SN}) = (267^{\\circ},6^{\\circ})$ vs $(RA^{\\rm CMB},DEC^{\\rm CMB}) = (167^{\\circ},-7^{\\circ})$. We assessed the statistical significance of those results, finding that such a signal is expected due to the limitations of the observational sample. These results support that there is no significant evidence for a departure from the cosmic isotropy assumption, one of the pillars of the standard cosmological model.","sentences":["We use a recent Pantheon+SH0ES compilation of Type Ia Supernova distance measurements at low-redshift, i.e., $0.01 \\leq z \\leq 0.10$, in order to investigate the directional dependency of the deceleration parameter ($q_0$) in different patches ($60^{\\circ}$ size) across the sky, as a probe of the statistical isotropy of the Universe.","We adopt a cosmographic approach to compute the cosmological distances, fixing $H_0$ and $M_B$ to reference values provided by the collaboration.","By looking at 500 different patches randomly taken across the sky, we find a maximum $\\sim 3\\sigma$ CL anisotropy level for $q_0$, whose direction points orthogonally to the CMB dipole axis, i.e., $(RA^{\\rm SN},DEC^{\\rm SN})","= (267^{\\circ},6^{\\circ})$ vs $(RA^{\\rm CMB},DEC^{\\rm CMB})","= (167^{\\circ},-7^{\\circ})$. We assessed the statistical significance of those results, finding that such a signal is expected due to the limitations of the observational sample.","These results support that there is no significant evidence for a departure from the cosmic isotropy assumption, one of the pillars of the standard cosmological model."],"url":"http://arxiv.org/abs/2402.17741v1","category":"astro-ph.CO"}
{"created":"2024-02-27 18:18:23","title":"reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use","abstract":"The escalating prevalence of cannabis use, and associated cannabis-use disorder (CUD), poses a significant public health challenge globally. With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objective within the 2030 United Nations Agenda for Sustainable Development Goals (SDG). In this work, we develop an online reinforcement learning (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver personalized mobile health interventions aimed at reducing cannabis use among EAs. reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments. Moreover, reBandit employs Empirical Bayes and optimization techniques to autonomously update its hyper-parameters online. To evaluate the performance of our algorithm, we construct a simulation testbed using data from a prior study, and compare against commonly used algorithms in mobile health studies. We show that reBandit performs equally well or better than all the baseline algorithms, and the performance gap widens as population heterogeneity increases in the simulation environment, proving its adeptness to adapt to diverse population of study participants.","sentences":["The escalating prevalence of cannabis use, and associated cannabis-use disorder (CUD), poses a significant public health challenge globally.","With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objective within the 2030 United Nations Agenda for Sustainable Development Goals (SDG).","In this work, we develop an online reinforcement learning (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver personalized mobile health interventions aimed at reducing cannabis use among EAs.","reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments.","Moreover, reBandit employs Empirical Bayes and optimization techniques to autonomously update its hyper-parameters online.","To evaluate the performance of our algorithm, we construct a simulation testbed using data from a prior study, and compare against commonly used algorithms in mobile health studies.","We show that reBandit performs equally well or better than all the baseline algorithms, and the performance gap widens as population heterogeneity increases in the simulation environment, proving its adeptness to adapt to diverse population of study participants."],"url":"http://arxiv.org/abs/2402.17739v1","category":"cs.AI"}
{"created":"2024-02-27 18:12:58","title":"Learning-Based Algorithms for Graph Searching Problems","abstract":"We consider the problem of graph searching with prediction recently introduced by Banerjee et al. (2022). In this problem, an agent, starting at some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a hidden goal node $g$ while minimizing the total distance travelled. We study a setting in which at any node $v$, the agent receives a noisy estimate of the distance from $v$ to $g$. We design algorithms for this search task on unknown graphs. We establish the first formal guarantees on unknown weighted graphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error. Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic. Finally, we provide alternative simpler performance bounds on the algorithms of Banerjee et al. (2022) for the case of searching on a known graph, and establish new lower bounds for this setting.","sentences":["We consider the problem of graph searching with prediction recently introduced by Banerjee et al. (2022).","In this problem, an agent, starting at some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a hidden goal node $g$ while minimizing the total distance travelled.","We study a setting in which at any node $v$, the agent receives a noisy estimate of the distance from $v$ to $g$. We design algorithms for this search task on unknown graphs.","We establish the first formal guarantees on unknown weighted graphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error.","Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic.","Finally, we provide alternative simpler performance bounds on the algorithms of Banerjee et al. (2022) for the case of searching on a known graph, and establish new lower bounds for this setting."],"url":"http://arxiv.org/abs/2402.17736v1","category":"cs.DS"}
{"created":"2024-02-27 18:12:43","title":"High-Fidelity Neural Phonetic Posteriorgrams","abstract":"A phonetic posteriorgram (PPG) is a time-varying categorical distribution over acoustic units of speech (e.g., phonemes). PPGs are a popular representation in speech generation due to their ability to disentangle pronunciation features from speaker identity, allowing accurate reconstruction of pronunciation (e.g., voice conversion) and coarse-grained pronunciation editing (e.g., foreign accent conversion). In this paper, we demonstrably improve the quality of PPGs to produce a state-of-the-art interpretable PPG representation. We train an off-the-shelf speech synthesizer using our PPG representation and show that high-quality PPGs yield independent control over pitch and pronunciation. We further demonstrate novel uses of PPGs, such as an acoustic pronunciation distance and fine-grained pronunciation control.","sentences":["A phonetic posteriorgram (PPG) is a time-varying categorical distribution over acoustic units of speech (e.g., phonemes).","PPGs are a popular representation in speech generation due to their ability to disentangle pronunciation features from speaker identity, allowing accurate reconstruction of pronunciation (e.g., voice conversion) and coarse-grained pronunciation editing (e.g., foreign accent conversion).","In this paper, we demonstrably improve the quality of PPGs to produce a state-of-the-art interpretable PPG representation.","We train an off-the-shelf speech synthesizer using our PPG representation and show that high-quality PPGs yield independent control over pitch and pronunciation.","We further demonstrate novel uses of PPGs, such as an acoustic pronunciation distance and fine-grained pronunciation control."],"url":"http://arxiv.org/abs/2402.17735v1","category":"eess.AS"}
{"created":"2024-02-27 18:09:36","title":"Tower: An Open Multilingual Large Language Model for Translation-Related Tasks","abstract":"While general-purpose large language models (LLMs) demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open LLMs are competitive only when specializing on a single task. In this paper, we propose a recipe for tailoring LLMs to multiple tasks present in translation workflows. We perform continued pretraining on a multilingual mixture of monolingual and parallel data, creating TowerBase, followed by finetuning on instructions relevant for translation processes, creating TowerInstruct. Our final model surpasses open alternatives on several tasks relevant to translation workflows and is competitive with general-purpose closed LLMs. To facilitate future research, we release the Tower models, our specialization dataset, an evaluation framework for LLMs focusing on the translation ecosystem, and a collection of model generations, including ours, on our benchmark.","sentences":["While general-purpose large language models (LLMs) demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open LLMs are competitive only when specializing on a single task.","In this paper, we propose a recipe for tailoring LLMs to multiple tasks present in translation workflows.","We perform continued pretraining on a multilingual mixture of monolingual and parallel data, creating TowerBase, followed by finetuning on instructions relevant for translation processes, creating TowerInstruct.","Our final model surpasses open alternatives on several tasks relevant to translation workflows and is competitive with general-purpose closed LLMs.","To facilitate future research, we release the Tower models, our specialization dataset, an evaluation framework for LLMs focusing on the translation ecosystem, and a collection of model generations, including ours, on our benchmark."],"url":"http://arxiv.org/abs/2402.17733v1","category":"cs.CL"}
{"created":"2024-02-27 18:04:59","title":"Markovletics: Methods and A Novel Application for Learning Continuous-Time Markov Chain Mixtures","abstract":"Sequential data naturally arises from user engagement on digital platforms like social media, music streaming services, and web navigation, encapsulating evolving user preferences and behaviors through continuous information streams. A notable unresolved query in stochastic processes is learning mixtures of continuous-time Markov chains (CTMCs). While there is progress in learning mixtures of discrete-time Markov chains with recovery guarantees [GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexplored challenges. The intrigue in CTMC mixtures stems from their potential to model intricate continuous-time stochastic processes prevalent in various fields including social media, finance, and biology.   In this study, we introduce a novel framework for exploring CTMCs, emphasizing the influence of observed trails' length and mixture parameters on problem regimes, which demands specific algorithms. Through thorough experimentation, we examine the impact of discretizing continuous-time trails on the learnability of the continuous-time mixture, given that these processes are often observed via discrete, resource-demanding observations. Our comparative analysis with leading methods explores sample complexity and the trade-off between the number of trails and their lengths, offering crucial insights for method selection in different problem instances. We apply our algorithms on an extensive collection of Lastfm's user-generated trails spanning three years, demonstrating the capability of our algorithms to differentiate diverse user preferences. We pioneer the use of CTMC mixtures on a basketball passing dataset to unveil intricate offensive tactics of NBA teams. This underscores the pragmatic utility and versatility of our proposed framework. All results presented in this study are replicable, and we provide the implementations to facilitate reproducibility.","sentences":["Sequential data naturally arises from user engagement on digital platforms like social media, music streaming services, and web navigation, encapsulating evolving user preferences and behaviors through continuous information streams.","A notable unresolved query in stochastic processes is learning mixtures of continuous-time Markov chains (CTMCs).","While there is progress in learning mixtures of discrete-time Markov chains with recovery guarantees [GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexplored challenges.","The intrigue in CTMC mixtures stems from their potential to model intricate continuous-time stochastic processes prevalent in various fields including social media, finance, and biology.   ","In this study, we introduce a novel framework for exploring CTMCs, emphasizing the influence of observed trails' length and mixture parameters on problem regimes, which demands specific algorithms.","Through thorough experimentation, we examine the impact of discretizing continuous-time trails on the learnability of the continuous-time mixture, given that these processes are often observed via discrete, resource-demanding observations.","Our comparative analysis with leading methods explores sample complexity and the trade-off between the number of trails and their lengths, offering crucial insights for method selection in different problem instances.","We apply our algorithms on an extensive collection of Lastfm's user-generated trails spanning three years, demonstrating the capability of our algorithms to differentiate diverse user preferences.","We pioneer the use of CTMC mixtures on a basketball passing dataset to unveil intricate offensive tactics of NBA teams.","This underscores the pragmatic utility and versatility of our proposed framework.","All results presented in this study are replicable, and we provide the implementations to facilitate reproducibility."],"url":"http://arxiv.org/abs/2402.17730v1","category":"cs.LG"}
{"created":"2024-02-27 18:01:59","title":"Towards Fairness-Aware Adversarial Learning","abstract":"Although adversarial training (AT) has proven effective in enhancing the model's robustness, the recently revealed issue of fairness in robustness has not been well addressed, i.e. the robust accuracy varies significantly among different categories. In this paper, instead of uniformly evaluating the model's average class performance, we delve into the issue of robust fairness, by considering the worst-case distribution across various classes. We propose a novel learning paradigm, named Fairness-Aware Adversarial Learning (FAAL). As a generalization of conventional AT, we re-define the problem of adversarial training as a min-max-max framework, to ensure both robustness and fairness of the trained model. Specifically, by taking advantage of distributional robust optimization, our method aims to find the worst distribution among different categories, and the solution is guaranteed to obtain the upper bound performance with high probability. In particular, FAAL can fine-tune an unfair robust model to be fair within only two epochs, without compromising the overall clean and robust accuracies. Extensive experiments on various image datasets validate the superior performance and efficiency of the proposed FAAL compared to other state-of-the-art methods.","sentences":["Although adversarial training (AT) has proven effective in enhancing the model's robustness, the recently revealed issue of fairness in robustness has not been well addressed, i.e. the robust accuracy varies significantly among different categories.","In this paper, instead of uniformly evaluating the model's average class performance, we delve into the issue of robust fairness, by considering the worst-case distribution across various classes.","We propose a novel learning paradigm, named Fairness-Aware Adversarial Learning (FAAL).","As a generalization of conventional AT, we re-define the problem of adversarial training as a min-max-max framework, to ensure both robustness and fairness of the trained model.","Specifically, by taking advantage of distributional robust optimization, our method aims to find the worst distribution among different categories, and the solution is guaranteed to obtain the upper bound performance with high probability.","In particular, FAAL can fine-tune an unfair robust model to be fair within only two epochs, without compromising the overall clean and robust accuracies.","Extensive experiments on various image datasets validate the superior performance and efficiency of the proposed FAAL compared to other state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.17729v1","category":"cs.CV"}
{"created":"2024-02-27 18:01:21","title":"Linking Order to Strength in Metals","abstract":"The metallurgy and materials communities have long known and exploited fundamental links between chemical and structural ordering in metallic solids and their mechanical properties. The highest reported strength achievable through the combination of multiple metals (alloying) has rapidly climbed and given rise to new classifications of materials with extraordinary properties. Metallic glasses and high-entropy alloys are two limiting examples of how tailored order can be used to manipulate mechanical behavior. Here, we show that the complex electronic-structure mechanisms governing the peak strength of alloys and pure metals can be reduced to a few physically-meaningful parameters based on their atomic arrangements and used (with no fitting parameters) to predict the maximum strength of any metallic solid, regardless of degree of structural or chemical ordering. Predictions of maximum strength based on the activation energy for a stress-driven phase transition to an amorphous state is shown to accurately describe the breakdown in Hall-Petch behavior at the smallest crystallite sizes for pure metals, intermetallic compounds, metallic glasses, and high-entropy alloys. This activation energy is also shown to be directly proportional to interstitial (electronic) charge density, which is a good predictor of ductility, stiffness (moduli), and phase stability in high-entropy alloys, and in solid metals generally. The proposed framework suggests the possibility of coupling ordering and intrinsic strength to mechanisms like dislocation nucleation, hydrogen embrittlement, and transport properties. It additionally opens the prospect for greatly accelerated structural materials design and development to address materials challenges limiting more sustainable and efficient use of energy.","sentences":["The metallurgy and materials communities have long known and exploited fundamental links between chemical and structural ordering in metallic solids and their mechanical properties.","The highest reported strength achievable through the combination of multiple metals (alloying) has rapidly climbed and given rise to new classifications of materials with extraordinary properties.","Metallic glasses and high-entropy alloys are two limiting examples of how tailored order can be used to manipulate mechanical behavior.","Here, we show that the complex electronic-structure mechanisms governing the peak strength of alloys and pure metals can be reduced to a few physically-meaningful parameters based on their atomic arrangements and used (with no fitting parameters) to predict the maximum strength of any metallic solid, regardless of degree of structural or chemical ordering.","Predictions of maximum strength based on the activation energy for a stress-driven phase transition to an amorphous state is shown to accurately describe the breakdown in Hall-Petch behavior at the smallest crystallite sizes for pure metals, intermetallic compounds, metallic glasses, and high-entropy alloys.","This activation energy is also shown to be directly proportional to interstitial (electronic) charge density, which is a good predictor of ductility, stiffness (moduli), and phase stability in high-entropy alloys, and in solid metals generally.","The proposed framework suggests the possibility of coupling ordering and intrinsic strength to mechanisms like dislocation nucleation, hydrogen embrittlement, and transport properties.","It additionally opens the prospect for greatly accelerated structural materials design and development to address materials challenges limiting more sustainable and efficient use of energy."],"url":"http://arxiv.org/abs/2402.17728v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-27 17:58:09","title":"VRP-SAM: SAM with Visual Reference Prompt","abstract":"In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that empowers the Segment Anything Model (SAM) to utilize annotated reference images as prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM can utilize annotated reference images to comprehend specific objects and perform segmentation of specific objects in target image. It is note that the VRP encoder can support a variety of annotation formats for reference images, including \\textbf{point}, \\textbf{box}, \\textbf{scribble}, and \\textbf{mask}. VRP-SAM achieves a breakthrough within the SAM framework by extending its versatility and applicability while preserving SAM's inherent strengths, thus enhancing user-friendliness. To enhance the generalization ability of VRP-SAM, the VRP encoder adopts a meta-learning strategy. To validate the effectiveness of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO datasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual reference segmentation with minimal learnable parameters. Furthermore, VRP-SAM demonstrates strong generalization capabilities, allowing it to perform segmentation of unseen objects and enabling cross-domain segmentation.","sentences":["In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that empowers the Segment Anything Model (SAM) to utilize annotated reference images as prompts for segmentation, creating the VRP-SAM model.","In essence, VRP-SAM can utilize annotated reference images to comprehend specific objects and perform segmentation of specific objects in target image.","It is note that the VRP encoder can support a variety of annotation formats for reference images, including \\textbf{point}, \\textbf{box}, \\textbf{scribble}, and \\textbf{mask}.","VRP-SAM achieves a breakthrough within the SAM framework by extending its versatility and applicability while preserving SAM's inherent strengths, thus enhancing user-friendliness.","To enhance the generalization ability of VRP-SAM, the VRP encoder adopts a meta-learning strategy.","To validate the effectiveness of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO datasets.","Remarkably, VRP-SAM achieved state-of-the-art performance in visual reference segmentation with minimal learnable parameters.","Furthermore, VRP-SAM demonstrates strong generalization capabilities, allowing it to perform segmentation of unseen objects and enabling cross-domain segmentation."],"url":"http://arxiv.org/abs/2402.17726v1","category":"cs.CV"}
{"created":"2024-02-27 17:58:05","title":"MedContext: Learning Contextual Cues for Efficient Volumetric Medical Segmentation","abstract":"Volumetric medical segmentation is a critical component of 3D medical image analysis that delineates different semantic regions. Deep neural networks have significantly improved volumetric medical segmentation, but they generally require large-scale annotated data to achieve better performance, which can be expensive and prohibitive to obtain. To address this limitation, existing works typically perform transfer learning or design dedicated pretraining-finetuning stages to learn representative features. However, the mismatch between the source and target domain can make it challenging to learn optimal representation for volumetric data, while the multi-stage training demands higher compute as well as careful selection of stage-specific design choices. In contrast, we propose a universal training framework called MedContext that is architecture-agnostic and can be incorporated into any existing training framework for 3D medical segmentation. Our approach effectively learns self supervised contextual cues jointly with the supervised voxel segmentation task without requiring large-scale annotated volumetric medical data or dedicated pretraining-finetuning stages. The proposed approach induces contextual knowledge in the network by learning to reconstruct the missing organ or parts of an organ in the output segmentation space. The effectiveness of MedContext is validated across multiple 3D medical datasets and four state-of-the-art model architectures. Our approach demonstrates consistent gains in segmentation performance across datasets and different architectures even in few-shot data scenarios. Our code and pretrained models are available at https://github.com/hananshafi/MedContext","sentences":["Volumetric medical segmentation is a critical component of 3D medical image analysis that delineates different semantic regions.","Deep neural networks have significantly improved volumetric medical segmentation, but they generally require large-scale annotated data to achieve better performance, which can be expensive and prohibitive to obtain.","To address this limitation, existing works typically perform transfer learning or design dedicated pretraining-finetuning stages to learn representative features.","However, the mismatch between the source and target domain can make it challenging to learn optimal representation for volumetric data, while the multi-stage training demands higher compute as well as careful selection of stage-specific design choices.","In contrast, we propose a universal training framework called MedContext that is architecture-agnostic and can be incorporated into any existing training framework for 3D medical segmentation.","Our approach effectively learns self supervised contextual cues jointly with the supervised voxel segmentation task without requiring large-scale annotated volumetric medical data or dedicated pretraining-finetuning stages.","The proposed approach induces contextual knowledge in the network by learning to reconstruct the missing organ or parts of an organ in the output segmentation space.","The effectiveness of MedContext is validated across multiple 3D medical datasets and four state-of-the-art model architectures.","Our approach demonstrates consistent gains in segmentation performance across datasets and different architectures even in few-shot data scenarios.","Our code and pretrained models are available at https://github.com/hananshafi/MedContext"],"url":"http://arxiv.org/abs/2402.17725v1","category":"eess.IV"}
{"created":"2024-02-27 17:57:04","title":"Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners","abstract":"Video and audio content creation serves as the core technique for the movie industry and professional users. Recently, existing diffusion-based methods tackle video and audio generation separately, which hinders the technique transfer from academia to industry. In this work, we aim at filling the gap, with a carefully designed optimization-based framework for cross-visual-audio and joint-visual-audio generation. We observe the powerful generation ability of off-the-shelf video or audio generation models. Thus, instead of training the giant models from scratch, we propose to bridge the existing strong models with a shared latent representation space. Specifically, we propose a multimodality latent aligner with the pre-trained ImageBind model. Our latent aligner shares a similar core as the classifier guidance that guides the diffusion denoising process during inference time. Through carefully designed optimization strategy and loss functions, we show the superior performance of our method on joint video-audio generation, visual-steered audio generation, and audio-steered visual generation tasks. The project website can be found at https://yzxing87.github.io/Seeing-and-Hearing/","sentences":["Video and audio content creation serves as the core technique for the movie industry and professional users.","Recently, existing diffusion-based methods tackle video and audio generation separately, which hinders the technique transfer from academia to industry.","In this work, we aim at filling the gap, with a carefully designed optimization-based framework for cross-visual-audio and joint-visual-audio generation.","We observe the powerful generation ability of off-the-shelf video or audio generation models.","Thus, instead of training the giant models from scratch, we propose to bridge the existing strong models with a shared latent representation space.","Specifically, we propose a multimodality latent aligner with the pre-trained ImageBind model.","Our latent aligner shares a similar core as the classifier guidance that guides the diffusion denoising process during inference time.","Through carefully designed optimization strategy and loss functions, we show the superior performance of our method on joint video-audio generation, visual-steered audio generation, and audio-steered visual generation tasks.","The project website can be found at https://yzxing87.github.io/Seeing-and-Hearing/"],"url":"http://arxiv.org/abs/2402.17723v1","category":"cs.CV"}
{"created":"2024-02-27 17:56:49","title":"Taming Nonconvex Stochastic Mirror Descent with General Bregman Divergence","abstract":"This paper revisits the convergence of Stochastic Mirror Descent (SMD) in the contemporary nonconvex optimization setting. Existing results for batch-free nonconvex SMD restrict the choice of the distance generating function (DGF) to be differentiable with Lipschitz continuous gradients, thereby excluding important setups such as Shannon entropy. In this work, we present a new convergence analysis of nonconvex SMD supporting general DGF, that overcomes the above limitations and relies solely on the standard assumptions. Moreover, our convergence is established with respect to the Bregman Forward-Backward envelope, which is a stronger measure than the commonly used squared norm of gradient mapping. We further extend our results to guarantee high probability convergence under sub-Gaussian noise and global convergence under the generalized Bregman Proximal Polyak-{\\L}ojasiewicz condition. Additionally, we illustrate the advantages of our improved SMD theory in various nonconvex machine learning tasks by harnessing nonsmooth DGFs. Notably, in the context of nonconvex differentially private (DP) learning, our theory yields a simple algorithm with a (nearly) dimension-independent utility bound. For the problem of training linear neural networks, we develop provably convergent stochastic algorithms.","sentences":["This paper revisits the convergence of Stochastic Mirror Descent (SMD) in the contemporary nonconvex optimization setting.","Existing results for batch-free nonconvex SMD restrict the choice of the distance generating function (DGF) to be differentiable with Lipschitz continuous gradients, thereby excluding important setups such as Shannon entropy.","In this work, we present a new convergence analysis of nonconvex SMD supporting general DGF, that overcomes the above limitations and relies solely on the standard assumptions.","Moreover, our convergence is established with respect to the Bregman Forward-Backward envelope, which is a stronger measure than the commonly used squared norm of gradient mapping.","We further extend our results to guarantee high probability convergence under sub-Gaussian noise and global convergence under the generalized Bregman Proximal Polyak-{\\L}ojasiewicz condition.","Additionally, we illustrate the advantages of our improved SMD theory in various nonconvex machine learning tasks by harnessing nonsmooth DGFs.","Notably, in the context of nonconvex differentially private (DP) learning, our theory yields a simple algorithm with a (nearly) dimension-independent utility bound.","For the problem of training linear neural networks, we develop provably convergent stochastic algorithms."],"url":"http://arxiv.org/abs/2402.17722v1","category":"math.OC"}
{"created":"2024-02-27 17:56:10","title":"Content-Centric Prototyping of Generative AI Applications: Emerging Approaches and Challenges in Collaborative Software Teams","abstract":"Generative AI models are increasingly powering software applications, offering the capability to produce expressive content across varied contexts. However, unlike previous iterations of human-AI design, the emerging design process for generative capabilities primarily hinges on prompt engineering strategies. Given this fundamental shift in approach, our work aims to understand how collaborative software teams set up and apply design guidelines and values, iteratively prototype prompts, and evaluate prompts to achieve desired outcomes. We conducted design studies with 39 industry professionals, including designers, software engineers, and product managers. Our findings reveal a content-centric prototyping approach in which teams begin with the content they want to generate, then identify specific attributes, constraints, and values, and explore methods to give users the ability to influence and interact with those attributes. Based on associated challenges, such as the lack of model interpretability and overfitting the design to examples, we outline considerations for generative AI prototyping.","sentences":["Generative AI models are increasingly powering software applications, offering the capability to produce expressive content across varied contexts.","However, unlike previous iterations of human-AI design, the emerging design process for generative capabilities primarily hinges on prompt engineering strategies.","Given this fundamental shift in approach, our work aims to understand how collaborative software teams set up and apply design guidelines and values, iteratively prototype prompts, and evaluate prompts to achieve desired outcomes.","We conducted design studies with 39 industry professionals, including designers, software engineers, and product managers.","Our findings reveal a content-centric prototyping approach in which teams begin with the content they want to generate, then identify specific attributes, constraints, and values, and explore methods to give users the ability to influence and interact with those attributes.","Based on associated challenges, such as the lack of model interpretability and overfitting the design to examples, we outline considerations for generative AI prototyping."],"url":"http://arxiv.org/abs/2402.17721v1","category":"cs.HC"}
{"created":"2024-02-27 17:55:33","title":"The SMART approach to instance-optimal online learning","abstract":"We devise an online learning algorithm -- titled Switching via Monotone Adapted Regret Traces (SMART) -- that adapts to the data and achieves regret that is instance optimal, i.e., simultaneously competitive on every input sequence compared to the performance of the follow-the-leader (FTL) policy and the worst case guarantee of any other input policy. We show that the regret of the SMART policy on any input sequence is within a multiplicative factor $e/(e-1) \\approx 1.58$ of the smaller of: 1) the regret obtained by FTL on the sequence, and 2) the upper bound on regret guaranteed by the given worst-case policy. This implies a strictly stronger guarantee than typical `best-of-both-worlds' bounds as the guarantee holds for every input sequence regardless of how it is generated. SMART is simple to implement as it begins by playing FTL and switches at most once during the time horizon to the worst-case algorithm. Our approach and results follow from an operational reduction of instance optimal online learning to competitive analysis for the ski-rental problem. We complement our competitive ratio upper bounds with a fundamental lower bound showing that over all input sequences, no algorithm can get better than a $1.43$-fraction of the minimum regret achieved by FTL and the minimax-optimal policy. We also present a modification of SMART that combines FTL with a ``small-loss\" algorithm to achieve instance optimality between the regret of FTL and the small loss regret bound.","sentences":["We devise an online learning algorithm -- titled Switching via Monotone Adapted Regret Traces (SMART) -- that adapts to the data and achieves regret that is instance optimal, i.e., simultaneously competitive on every input sequence compared to the performance of the follow-the-leader (FTL) policy and the worst case guarantee of any other input policy.","We show that the regret of the SMART policy on any input sequence is within a multiplicative factor $e/(e-1) \\approx 1.58$ of the smaller of: 1) the regret obtained by FTL on the sequence, and 2) the upper bound on regret guaranteed by the given worst-case policy.","This implies a strictly stronger guarantee than typical `best-of-both-worlds' bounds as the guarantee holds for every input sequence regardless of how it is generated.","SMART is simple to implement as it begins by playing FTL and switches at most once during the time horizon to the worst-case algorithm.","Our approach and results follow from an operational reduction of instance optimal online learning to competitive analysis for the ski-rental problem.","We complement our competitive ratio upper bounds with a fundamental lower bound showing that over all input sequences, no algorithm can get better than a $1.43$-fraction of the minimum regret achieved by FTL and the minimax-optimal policy.","We also present a modification of SMART that combines FTL with a ``small-loss\" algorithm to achieve instance optimality between the regret of FTL and the small loss regret bound."],"url":"http://arxiv.org/abs/2402.17720v1","category":"cs.LG"}
{"created":"2024-02-27 17:55:26","title":"Computation of the expectation value of the spin operator $\\hat{S}^2$ for the Spin-Flip Bethe-Salpeter Equation","abstract":"Spin-flip methods applied to excited-state approaches like the Bethe-Salpeter Equation allow access to the excitation energies of open-shell systems, such as molecules and defects in solids. The eigenstates of these solutions, however, are generally not eigenstates of the spin operator $\\hat{S}^2$. Even for simple cases where the excitation vector is expected to be, for example, a triplet state, the value of $\\langle \\hat{S}^2 \\rangle$ may be found to differ from 2.00; this difference is called ``spin contamination.'' The expectation values $\\langle \\hat{S}^2 \\rangle$ must be computed for each excitation vector, to assist with the characterization of the particular excitation and to determine the amount of spin contamination of the state. Our aim is to provide for the first time in the spin-flip methods literature a comprehensive resource on the derivation of the formulas for $\\langle \\hat{S}^2 \\rangle$ as well as its computational implementation. After a brief discussion of the theory of the Spin-Flip Bethe-Salpeter Equation and some examples further illustrating the need for calculating $\\langle \\hat{S}^2 \\rangle$, we present the derivation for the general equation for computing $\\langle \\hat{S}^2 \\rangle$ with the eigenvectors from an SF-BSE calculation, how it is implemented in a Python script, and timing information on how this calculation scales with the size of the SF-BSE Hamiltonian.","sentences":["Spin-flip methods applied to excited-state approaches like the Bethe-Salpeter Equation allow access to the excitation energies of open-shell systems, such as molecules and defects in solids.","The eigenstates of these solutions, however, are generally not eigenstates of the spin operator $\\hat{S}^2$. Even for simple cases where the excitation vector is expected to be, for example, a triplet state, the value of $\\langle \\hat{S}^2 \\rangle$ may be found to differ from 2.00; this difference is called ``spin contamination.''","The expectation values $\\langle \\hat{S}^2 \\rangle$ must be computed for each excitation vector, to assist with the characterization of the particular excitation and to determine the amount of spin contamination of the state.","Our aim is to provide for the first time in the spin-flip methods literature a comprehensive resource on the derivation of the formulas for $\\langle \\hat{S}^2 \\rangle$ as well as its computational implementation.","After a brief discussion of the theory of the Spin-Flip Bethe-Salpeter Equation and some examples further illustrating the need for calculating $\\langle \\hat{S}^2 \\rangle$, we present the derivation for the general equation for computing $\\langle \\hat{S}^2 \\rangle$ with the eigenvectors from an SF-BSE calculation, how it is implemented in a Python script, and timing information on how this calculation scales with the size of the SF-BSE Hamiltonian."],"url":"http://arxiv.org/abs/2402.17719v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-27 17:53:13","title":"Towards a Digital Twin Framework in Additive Manufacturing: Machine Learning and Bayesian Optimization for Time Series Process Optimization","abstract":"Laser-directed-energy deposition (DED) offers advantages in additive manufacturing (AM) for creating intricate geometries and material grading. Yet, challenges like material inconsistency and part variability remain, mainly due to its layer-wise fabrication. A key issue is heat accumulation during DED, which affects the material microstructure and properties. While closed-loop control methods for heat management are common in DED research, few integrate real-time monitoring, physics-based modeling, and control in a unified framework. Our work presents a digital twin (DT) framework for real-time predictive control of DED process parameters to meet specific design objectives. We develop a surrogate model using Long Short-Term Memory (LSTM)-based machine learning with Bayesian Inference to predict temperatures in DED parts. This model predicts future temperature states in real time. We also introduce Bayesian Optimization (BO) for Time Series Process Optimization (BOTSPO), based on traditional BO but featuring a unique time series process profile generator with reduced dimensions. BOTSPO dynamically optimizes processes, identifying optimal laser power profiles to attain desired mechanical properties. The established process trajectory guides online optimizations, aiming to enhance performance. This paper outlines the digital twin framework's components, promoting its integration into a comprehensive system for AM.","sentences":["Laser-directed-energy deposition (DED) offers advantages in additive manufacturing (AM) for creating intricate geometries and material grading.","Yet, challenges like material inconsistency and part variability remain, mainly due to its layer-wise fabrication.","A key issue is heat accumulation during DED, which affects the material microstructure and properties.","While closed-loop control methods for heat management are common in DED research, few integrate real-time monitoring, physics-based modeling, and control in a unified framework.","Our work presents a digital twin (DT) framework for real-time predictive control of DED process parameters to meet specific design objectives.","We develop a surrogate model using Long Short-Term Memory (LSTM)-based machine learning with Bayesian Inference to predict temperatures in DED parts.","This model predicts future temperature states in real time.","We also introduce Bayesian Optimization (BO) for Time Series Process Optimization (BOTSPO), based on traditional BO but featuring a unique time series process profile generator with reduced dimensions.","BOTSPO dynamically optimizes processes, identifying optimal laser power profiles to attain desired mechanical properties.","The established process trajectory guides online optimizations, aiming to enhance performance.","This paper outlines the digital twin framework's components, promoting its integration into a comprehensive system for AM."],"url":"http://arxiv.org/abs/2402.17718v1","category":"cs.LG"}
{"created":"2024-02-27 17:52:33","title":"AmbigNLG: Addressing Task Ambiguity in Instruction for NLG","abstract":"In this study, we introduce AmbigNLG, a new task designed to tackle the challenge of task ambiguity in instructions for Natural Language Generation (NLG) tasks. Despite the impressive capabilities of Large Language Models (LLMs) in understanding and executing a wide range of tasks through natural language interaction, their performance is significantly hindered by the ambiguity present in real-world instructions. To address this, AmbigNLG seeks to identify and mitigate such ambiguities, aiming to refine instructions to match user expectations better. We introduce a dataset, AmbigSNI-NLG, consisting of 2,500 instances, and develop an ambiguity taxonomy for categorizing and annotating instruction ambiguities. Our approach demonstrates substantial improvements in text generation quality, highlighting the critical role of clear and specific instructions in enhancing LLM performance in NLG tasks.","sentences":["In this study, we introduce AmbigNLG, a new task designed to tackle the challenge of task ambiguity in instructions for Natural Language Generation (NLG) tasks.","Despite the impressive capabilities of Large Language Models (LLMs) in understanding and executing a wide range of tasks through natural language interaction, their performance is significantly hindered by the ambiguity present in real-world instructions.","To address this, AmbigNLG seeks to identify and mitigate such ambiguities, aiming to refine instructions to match user expectations better.","We introduce a dataset, AmbigSNI-NLG, consisting of 2,500 instances, and develop an ambiguity taxonomy for categorizing and annotating instruction ambiguities.","Our approach demonstrates substantial improvements in text generation quality, highlighting the critical role of clear and specific instructions in enhancing LLM performance in NLG tasks."],"url":"http://arxiv.org/abs/2402.17717v1","category":"cs.CL"}
{"created":"2024-02-27 17:49:52","title":"On Central Primitives for Quantum Cryptography with Classical Communication","abstract":"Recent work has introduced the \"Quantum-Computation Classical-Communication\" (QCCC) (Chung et. al.) setting for cryptography. There has been some evidence that One Way Puzzles (OWPuzz) are the natural central cryptographic primitive for this setting (Khurana and Tomer). For a primitive to be considered central it should have several characteristics. It should be well behaved (which for this paper we will think of as having amplification, combiners, and universal constructions); it should be implied by a wide variety of other primitives; and it should be equivalent to some class of useful primitives. We present combiners, correctness and security amplification, and a universal construction for OWPuzz. Our proof of security amplification uses a new and cleaner version construction of EFI from OWPuzz (in comparison to the result of Khurana and Tomer) that generalizes to weak OWPuzz and is the most technically involved section of the paper. It was previously known that OWPuzz are implied by other primitives of interest including commitments, symmetric key encryption, one way state generators (OWSG), and therefore pseudorandom states (PRS). However we are able to rule out OWPuzz's equivalence to many of these primitives by showing a black box separation between general OWPuzz and a restricted class of OWPuzz (those with efficient verification, which we call EV-OWPuzz). We then show that EV-OWPuzz are also implied by most of these primitives, which separates them from OWPuzz as well. This separation also separates extending PRS from highly compressing PRS answering an open question of Ananth et. al.","sentences":["Recent work has introduced the \"Quantum-Computation Classical-Communication\" (QCCC) (Chung et.","al.)","setting for cryptography.","There has been some evidence that One Way Puzzles (OWPuzz) are the natural central cryptographic primitive for this setting (Khurana and Tomer).","For a primitive to be considered central it should have several characteristics.","It should be well behaved (which for this paper we will think of as having amplification, combiners, and universal constructions); it should be implied by a wide variety of other primitives; and it should be equivalent to some class of useful primitives.","We present combiners, correctness and security amplification, and a universal construction for OWPuzz.","Our proof of security amplification uses a new and cleaner version construction of EFI from OWPuzz (in comparison to the result of Khurana and Tomer) that generalizes to weak OWPuzz and is the most technically involved section of the paper.","It was previously known that OWPuzz are implied by other primitives of interest including commitments, symmetric key encryption, one way state generators (OWSG), and therefore pseudorandom states (PRS).","However we are able to rule out OWPuzz's equivalence to many of these primitives by showing a black box separation between general OWPuzz and a restricted class of OWPuzz (those with efficient verification, which we call EV-OWPuzz).","We then show that EV-OWPuzz are also implied by most of these primitives, which separates them from OWPuzz as well.","This separation also separates extending PRS from highly compressing PRS answering an open question of Ananth et.","al."],"url":"http://arxiv.org/abs/2402.17715v1","category":"cs.CR"}
{"created":"2024-02-27 17:47:31","title":"A $p$-version of convolution quadrature in wave propagation","abstract":"We consider a novel way of discretizing wave scattering problems using the general formalism of convolution quadrature, but instead of reducing the timestep size ($h$-method), we achieve accuracy by increasing the order of the method ($p$-method). We base this method on discontinuous Galerkin timestepping and use the Z-transform. We show that for a certain class of incident waves, the resulting schemes observes(root)-exponential convergence rate with respect to the number of boundary integral operators that need to be applied. Numerical experiments confirm the findings.","sentences":["We consider a novel way of discretizing wave scattering problems using the general formalism of convolution quadrature, but instead of reducing the timestep size ($h$-method), we achieve accuracy by increasing the order of the method ($p$-method).","We base this method on discontinuous Galerkin timestepping and use the Z-transform.","We show that for a certain class of incident waves, the resulting schemes observes(root)-exponential convergence rate with respect to the number of boundary integral operators that need to be applied.","Numerical experiments confirm the findings."],"url":"http://arxiv.org/abs/2402.17712v1","category":"math.NA"}
{"created":"2024-02-27 17:43:51","title":"Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers","abstract":"In neural network binarization, BinaryConnect (BC) and its variants are considered the standard. These methods apply the sign function in their forward pass and their respective gradients are backpropagated to update the weights. However, the derivative of the sign function is zero whenever defined, which consequently freezes training. Therefore, implementations of BC (e.g., BNN) usually replace the derivative of sign in the backward computation with identity or other approximate gradient alternatives. Although such practice works well empirically, it is largely a heuristic or ''training trick.'' We aim at shedding some light on these training tricks from the optimization perspective. Building from existing theory on ProxConnect (PC, a generalization of BC), we (1) equip PC with different forward-backward quantizers and obtain ProxConnect++ (PC++) that includes existing binarization techniques as special cases; (2) derive a principled way to synthesize forward-backward quantizers with automatic theoretical guarantees; (3) illustrate our theory by proposing an enhanced binarization algorithm BNN++; (4) conduct image classification experiments on CNNs and vision transformers, and empirically verify that BNN++ generally achieves competitive results on binarizing these models.","sentences":["In neural network binarization, BinaryConnect (BC) and its variants are considered the standard.","These methods apply the sign function in their forward pass and their respective gradients are backpropagated to update the weights.","However, the derivative of the sign function is zero whenever defined, which consequently freezes training.","Therefore, implementations of BC (e.g., BNN) usually replace the derivative of sign in the backward computation with identity or other approximate gradient alternatives.","Although such practice works well empirically, it is largely a heuristic or ''training trick.''","We aim at shedding some light on these training tricks from the optimization perspective.","Building from existing theory on ProxConnect (PC, a generalization of BC), we (1) equip PC with different forward-backward quantizers and obtain ProxConnect++ (PC++) that includes existing binarization techniques as special cases; (2) derive a principled way to synthesize forward-backward quantizers with automatic theoretical guarantees; (3) illustrate our theory by proposing an enhanced binarization algorithm BNN++; (4) conduct image classification experiments on CNNs and vision transformers, and empirically verify that BNN++ generally achieves competitive results on binarizing these models."],"url":"http://arxiv.org/abs/2402.17710v1","category":"cs.LG"}
{"created":"2024-02-27 17:41:58","title":"Case-Based or Rule-Based: How Do Transformers Do the Math?","abstract":"Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar \"cases\" seen in the training corpus for help. We define these two different reasoning mechanisms as \"rule-based reasoning\" and \"case-based reasoning\". Since rule-based reasoning is essential for acquiring the systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason. To mitigate such problems, we propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform rule-based reasoning. Specifically, we provide explicit rules in the input and then instruct transformers to recite and follow the rules step by step. Through RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up to 12-digit addition with over 95% accuracy, which is over 40% higher than scratchpad. The significant improvement demonstrates that teaching LLMs to explicitly use rules helps them learn rule-based reasoning and generalize better in length.","sentences":["Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition.","While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same.","Instead, they may rely on similar \"cases\" seen in the training corpus for help.","We define these two different reasoning mechanisms as \"rule-based reasoning\" and \"case-based reasoning\".","Since rule-based reasoning is essential for acquiring the systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems.","Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason.","To mitigate such problems, we propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform rule-based reasoning.","Specifically, we provide explicit rules in the input and then instruct transformers to recite and follow the rules step by step.","Through RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up to 12-digit addition with over 95% accuracy, which is over 40% higher than scratchpad.","The significant improvement demonstrates that teaching LLMs to explicitly use rules helps them learn rule-based reasoning and generalize better in length."],"url":"http://arxiv.org/abs/2402.17709v1","category":"cs.AI"}
{"created":"2024-02-27 17:39:34","title":"Noise Aware Path Planning and Power Management of Hybrid Fuel UAVs","abstract":"Hybrid fuel Unmanned Aerial Vehicles (UAV), through their combination of multiple energy sources, offer several advantages over the standard single fuel source configuration, the primary one being increased range and efficiency. Multiple power or fuel sources also allow the distinct pitfalls of each source to be mitigated while exploiting the advantages within the mission or path planning. We consider here a UAV equipped with a combustion engine-generator and battery pack as energy sources. We consider the path planning and power-management of this platform in a noise-aware manner. To solve the path planning problem, we first present the Mixed Integer Linear Program (MILP) formulation of the problem. We then present and analyze a label-correcting algorithm, for which a pseudo-polynomial running time is proven. Results of extensive numerical testing are presented which analyze the performance and scalability of the labeling algorithm for various graph structures, problem parameters, and search heuristics. It is shown that the algorithm can solve instances on graphs as large as twenty thousand nodes in only a few seconds.","sentences":["Hybrid fuel Unmanned Aerial Vehicles (UAV), through their combination of multiple energy sources, offer several advantages over the standard single fuel source configuration, the primary one being increased range and efficiency.","Multiple power or fuel sources also allow the distinct pitfalls of each source to be mitigated while exploiting the advantages within the mission or path planning.","We consider here a UAV equipped with a combustion engine-generator and battery pack as energy sources.","We consider the path planning and power-management of this platform in a noise-aware manner.","To solve the path planning problem, we first present the Mixed Integer Linear Program (MILP) formulation of the problem.","We then present and analyze a label-correcting algorithm, for which a pseudo-polynomial running time is proven.","Results of extensive numerical testing are presented which analyze the performance and scalability of the labeling algorithm for various graph structures, problem parameters, and search heuristics.","It is shown that the algorithm can solve instances on graphs as large as twenty thousand nodes in only a few seconds."],"url":"http://arxiv.org/abs/2402.17708v1","category":"math.OC"}
{"created":"2024-02-27 17:38:55","title":"Rolling vesicles: From confined rotational flows to surface-enabled motion","abstract":"The interaction of surfaces in relative motion in wet environments is dominated by lubrication forces. Lubrication plays a pivotal role in biological processes and is key for the development of dynamic microscopic systems. Here, we develop motile vesicles that exploit lubrication forces to roll on substrates. The activity of the vesicle comes from the confined rotational flow generated by an externally-driven rotating particle, which is encapsulated within the vesicle by droplet-microfluidics. This confined fluid flows control vesicle rotation and enable vesicle rolling on a substrate. Lubrication forces driving vesicle rolling correlate with the frequency dependent shear behavior of the membrane. Our findings shed light on the principles of lubricated friction, offering insights for the design of motile vesicles that take advantage of frictional forces to target specific locations. Our system also opens an avenue to understand membrane confined fluid flows and their relevance for force transduction in biological systems.","sentences":["The interaction of surfaces in relative motion in wet environments is dominated by lubrication forces.","Lubrication plays a pivotal role in biological processes and is key for the development of dynamic microscopic systems.","Here, we develop motile vesicles that exploit lubrication forces to roll on substrates.","The activity of the vesicle comes from the confined rotational flow generated by an externally-driven rotating particle, which is encapsulated within the vesicle by droplet-microfluidics.","This confined fluid flows control vesicle rotation and enable vesicle rolling on a substrate.","Lubrication forces driving vesicle rolling correlate with the frequency dependent shear behavior of the membrane.","Our findings shed light on the principles of lubricated friction, offering insights for the design of motile vesicles that take advantage of frictional forces to target specific locations.","Our system also opens an avenue to understand membrane confined fluid flows and their relevance for force transduction in biological systems."],"url":"http://arxiv.org/abs/2402.17707v1","category":"cond-mat.soft"}
{"created":"2024-02-27 17:27:47","title":"The SCIP Optimization Suite 9.0","abstract":"The SCIP Optimization Suite provides a collection of software packages for mathematical optimization, centered around the constraint integer programming (CIP) framework SCIP. This report discusses the enhancements and extensions included in the SCIP Optimization Suite 9.0. The updates in SCIP 9.0 include improved symmetry handling, additions and improvements of nonlinear handlers and primal heuristics, a new cut generator and two new cut selection schemes, a new branching rule, a new LP interface, and several bug fixes. The SCIP Optimization Suite 9.0 also features new Rust and C++ interfaces for SCIP, new Python interface for SoPlex, along with enhancements to existing interfaces. The SCIP Optimization Suite 9.0 also includes new and improved features in the LP solver SoPlex, the presolving library PaPILO, the parallel framework UG, the decomposition framework GCG, and the SCIP extension SCIP-SDP. These additions and enhancements have resulted in an overall performance improvement of SCIP in terms of solving time, number of nodes in the branch-and-bound tree, as well as the reliability of the solver.","sentences":["The SCIP Optimization Suite provides a collection of software packages for mathematical optimization, centered around the constraint integer programming (CIP) framework SCIP.","This report discusses the enhancements and extensions included in the SCIP Optimization Suite 9.0.","The updates in SCIP 9.0 include improved symmetry handling, additions and improvements of nonlinear handlers and primal heuristics, a new cut generator and two new cut selection schemes, a new branching rule, a new LP interface, and several bug fixes.","The SCIP Optimization Suite 9.0 also features new Rust and C++ interfaces for SCIP, new Python interface for SoPlex, along with enhancements to existing interfaces.","The SCIP Optimization Suite 9.0 also includes new and improved features in the LP solver SoPlex, the presolving library PaPILO, the parallel framework UG, the decomposition framework GCG, and the SCIP extension SCIP-SDP.","These additions and enhancements have resulted in an overall performance improvement of SCIP in terms of solving time, number of nodes in the branch-and-bound tree, as well as the reliability of the solver."],"url":"http://arxiv.org/abs/2402.17702v1","category":"math.OC"}
{"created":"2024-02-27 17:23:40","title":"Gradient-based Discrete Sampling with Automatic Cyclical Scheduling","abstract":"Discrete distributions, particularly in high-dimensional deep models, are often highly multimodal due to inherent discontinuities. While gradient-based discrete sampling has proven effective, it is susceptible to becoming trapped in local modes due to the gradient information. To tackle this challenge, we propose an automatic cyclical scheduling, designed for efficient and accurate sampling in multimodal discrete distributions. Our method contains three key components: (1) a cyclical step size schedule where large steps discover new modes and small steps exploit each mode; (2) a cyclical balancing schedule, ensuring ``balanced\" proposals for given step sizes and high efficiency of the Markov chain; and (3) an automatic tuning scheme for adjusting the hyperparameters in the cyclical schedules, allowing adaptability across diverse datasets with minimal tuning. We prove the non-asymptotic convergence and inference guarantee for our method in general discrete distributions. Extensive experiments demonstrate the superiority of our method in sampling complex multimodal discrete distributions.","sentences":["Discrete distributions, particularly in high-dimensional deep models, are often highly multimodal due to inherent discontinuities.","While gradient-based discrete sampling has proven effective, it is susceptible to becoming trapped in local modes due to the gradient information.","To tackle this challenge, we propose an automatic cyclical scheduling, designed for efficient and accurate sampling in multimodal discrete distributions.","Our method contains three key components: (1) a cyclical step size schedule where large steps discover new modes and small steps exploit each mode; (2) a cyclical balancing schedule, ensuring ``balanced\" proposals for given step sizes and high efficiency of the Markov chain; and (3) an automatic tuning scheme for adjusting the hyperparameters in the cyclical schedules, allowing adaptability across diverse datasets with minimal tuning.","We prove the non-asymptotic convergence and inference guarantee for our method in general discrete distributions.","Extensive experiments demonstrate the superiority of our method in sampling complex multimodal discrete distributions."],"url":"http://arxiv.org/abs/2402.17699v1","category":"cs.LG"}
{"created":"2024-02-27 17:12:21","title":"Testing approximate infrared scattering radiative-transfer methods for hot Jupiter atmospheres","abstract":"The calculation of internal atmospheric (longwave) fluxes is a key component of any model of exoplanet atmospheres that requires radiative-transfer (RT) calculations. For atmospheres containing a strong scattering component such as cloud particles, most 1D multiple-scattering RT methods typically involve numerically expensive matrix inversions. This computational bottleneck is exacerbated when multitudes of RT calculations are required, such as in general circulation models (GCMs) and retrieval methods. In an effort to increase the speed of RT calculations without sacrificing too much accuracy, we investigate the applicability of approximate longwave scattering methods developed for the Earth science community to hot Jupiter atmospheres. We test the absorption approximation (AA) and variational iteration method (VIM) applied to typical cloudy hot Jupiter scenarios, using 64 stream DISORT calculations as reference solutions. We find the four-stream VIM variant is a highly promising method to explore using for hot Jupiter GCM and retrieval modelling, showing excellent speed characteristics, with typical errors $\\sim$1\\% for outgoing fluxes and within $\\sim$50\\%, but with larger errors in the deep cloud layer test case, for vertical heating rates. Other methods explored in this study were found to typically produce similar error characteristics in vertical heating rates.","sentences":["The calculation of internal atmospheric (longwave) fluxes is a key component of any model of exoplanet atmospheres that requires radiative-transfer (RT) calculations.","For atmospheres containing a strong scattering component such as cloud particles, most 1D multiple-scattering RT methods typically involve numerically expensive matrix inversions.","This computational bottleneck is exacerbated when multitudes of RT calculations are required, such as in general circulation models (GCMs) and retrieval methods.","In an effort to increase the speed of RT calculations without sacrificing too much accuracy, we investigate the applicability of approximate longwave scattering methods developed for the Earth science community to hot Jupiter atmospheres.","We test the absorption approximation (AA) and variational iteration method (VIM) applied to typical cloudy hot Jupiter scenarios, using 64 stream DISORT calculations as reference solutions.","We find the four-stream VIM variant is a highly promising method to explore using for hot Jupiter GCM and retrieval modelling, showing excellent speed characteristics, with typical errors $\\sim$1\\% for outgoing fluxes and within $\\sim$50\\%, but with larger errors in the deep cloud layer test case, for vertical heating rates.","Other methods explored in this study were found to typically produce similar error characteristics in vertical heating rates."],"url":"http://arxiv.org/abs/2402.17697v1","category":"astro-ph.EP"}
{"created":"2024-02-27 17:11:35","title":"Geometric Deep Learning for Computer-Aided Design: A Survey","abstract":"Geometric Deep Learning techniques have become a transformative force in the field of Computer-Aided Design (CAD), and have the potential to revolutionize how designers and engineers approach and enhance the design process. By harnessing the power of machine learning-based methods, CAD designers can optimize their workflows, save time and effort while making better informed decisions, and create designs that are both innovative and practical. The ability to process the CAD designs represented by geometric data and to analyze their encoded features enables the identification of similarities among diverse CAD models, the proposition of alternative designs and enhancements, and even the generation of novel design alternatives. This survey offers a comprehensive overview of learning-based methods in computer-aided design across various categories, including similarity analysis and retrieval, 2D and 3D CAD model synthesis, and CAD generation from point clouds. Additionally, it provides a complete list of benchmark datasets and their characteristics, along with open-source codes that have propelled research in this domain. The final discussion delves into the challenges prevalent in this field, followed by potential future research directions in this rapidly evolving field.","sentences":["Geometric Deep Learning techniques have become a transformative force in the field of Computer-Aided Design (CAD), and have the potential to revolutionize how designers and engineers approach and enhance the design process.","By harnessing the power of machine learning-based methods, CAD designers can optimize their workflows, save time and effort while making better informed decisions, and create designs that are both innovative and practical.","The ability to process the CAD designs represented by geometric data and to analyze their encoded features enables the identification of similarities among diverse CAD models, the proposition of alternative designs and enhancements, and even the generation of novel design alternatives.","This survey offers a comprehensive overview of learning-based methods in computer-aided design across various categories, including similarity analysis and retrieval, 2D and 3D CAD model synthesis, and CAD generation from point clouds.","Additionally, it provides a complete list of benchmark datasets and their characteristics, along with open-source codes that have propelled research in this domain.","The final discussion delves into the challenges prevalent in this field, followed by potential future research directions in this rapidly evolving field."],"url":"http://arxiv.org/abs/2402.17695v1","category":"cs.CG"}
{"created":"2024-02-27 17:09:45","title":"Optimal Control Barrier Functions: Maximizing the Action Space Subject to Control Bounds","abstract":"This letter addresses the constraint compatibility problem of control barrier functions (CBFs), which occurs when a safety-critical CBF requires a system to apply more control effort than it is capable of generating. This inevitably leads to a safety violation, which transitions the system to an unsafe (and possibly dangerous) trajectory. We resolve the constraint compatibility problem by constructing a control barrier function that maximizes the feasible action space for first and second-order constraints, and we prove that the optimal CBF encodes a dynamical motion primitive. Furthermore, we show that this dynamical motion primitive contains an implicit model for the future trajectory for time-varying components of the system. We validate our optimal CBF in simulation, and compare its behavior with a linear CBF.","sentences":["This letter addresses the constraint compatibility problem of control barrier functions (CBFs), which occurs when a safety-critical CBF requires a system to apply more control effort than it is capable of generating.","This inevitably leads to a safety violation, which transitions the system to an unsafe (and possibly dangerous) trajectory.","We resolve the constraint compatibility problem by constructing a control barrier function that maximizes the feasible action space for first and second-order constraints, and we prove that the optimal CBF encodes a dynamical motion primitive.","Furthermore, we show that this dynamical motion primitive contains an implicit model for the future trajectory for time-varying components of the system.","We validate our optimal CBF in simulation, and compare its behavior with a linear CBF."],"url":"http://arxiv.org/abs/2402.17694v1","category":"math.OC"}
{"created":"2024-02-27 17:07:18","title":"Autonomous Vehicles: Evolution of Artificial Intelligence and Learning Algorithms","abstract":"The advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies. Central to this evolu- tion is the integration of Artificial Intelligence (AI) and learning algorithms, propelling vehicles into realms of unprecedented autonomy. This paper provides a comprehensive exploration of the evolutionary trajectory of AI within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements. Commencing with a current landscape overview, the paper delves into the fundamental role of AI in shaping the autonomous decision-making capabilities of vehicles. It elucidates the steps involved in the AI-powered development life cycle in vehicles, addressing ethical considerations and bias in AI-driven software development for autonomous vehicles. The study presents statis- tical insights into the usage and types of AI/learning algorithms over the years, showcasing the evolving research landscape within the automotive industry. Furthermore, the paper highlights the pivotal role of parameters in refining algorithms for both trucks and cars, facilitating vehicles to adapt, learn, and improve performance over time. It concludes by outlining different levels of autonomy, elucidating the nuanced usage of AI and learning algorithms, and automating key tasks at each level. Additionally, the document discusses the variation in software package sizes across different autonomy levels","sentences":["The advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies.","Central to this evolu- tion is the integration of Artificial Intelligence (AI) and learning algorithms, propelling vehicles into realms of unprecedented autonomy.","This paper provides a comprehensive exploration of the evolutionary trajectory of AI within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements.","Commencing with a current landscape overview, the paper delves into the fundamental role of AI in shaping the autonomous decision-making capabilities of vehicles.","It elucidates the steps involved in the AI-powered development life cycle in vehicles, addressing ethical considerations and bias in AI-driven software development for autonomous vehicles.","The study presents statis- tical insights into the usage and types of AI/learning algorithms over the years, showcasing the evolving research landscape within the automotive industry.","Furthermore, the paper highlights the pivotal role of parameters in refining algorithms for both trucks and cars, facilitating vehicles to adapt, learn, and improve performance over time.","It concludes by outlining different levels of autonomy, elucidating the nuanced usage of AI and learning algorithms, and automating key tasks at each level.","Additionally, the document discusses the variation in software package sizes across different autonomy levels"],"url":"http://arxiv.org/abs/2402.17690v1","category":"cs.LG"}
{"created":"2024-02-27 17:05:41","title":"QoS prediction in radio vehicular environments via prior user information","abstract":"Reliable wireless communications play an important role in the automotive industry as it helps to enhance current use cases and enable new ones such as connected autonomous driving, platooning, cooperative maneuvering, teleoperated driving, and smart navigation. These and other use cases often rely on specific quality of service (QoS) levels for communication. Recently, the area of predictive quality of service (QoS) has received a great deal of attention as a key enabler to forecast communication quality well enough in advance. However, predicting QoS in a reliable manner is a notoriously difficult task. In this paper, we evaluate ML tree-ensemble methods to predict QoS in the range of minutes with data collected from a cellular test network. We discuss radio environment characteristics and we showcase how these can be used to improve ML performance and further support the uptake of ML in commercial networks. Specifically, we use the correlations of the measurements coming from the radio environment by including information of prior vehicles to enhance the prediction of the target vehicles. Moreover, we are extending prior art by showing how longer prediction horizons can be supported.","sentences":["Reliable wireless communications play an important role in the automotive industry as it helps to enhance current use cases and enable new ones such as connected autonomous driving, platooning, cooperative maneuvering, teleoperated driving, and smart navigation.","These and other use cases often rely on specific quality of service (QoS) levels for communication.","Recently, the area of predictive quality of service (QoS) has received a great deal of attention as a key enabler to forecast communication quality well enough in advance.","However, predicting QoS in a reliable manner is a notoriously difficult task.","In this paper, we evaluate ML tree-ensemble methods to predict QoS in the range of minutes with data collected from a cellular test network.","We discuss radio environment characteristics and we showcase how these can be used to improve ML performance and further support the uptake of ML in commercial networks.","Specifically, we use the correlations of the measurements coming from the radio environment by including information of prior vehicles to enhance the prediction of the target vehicles.","Moreover, we are extending prior art by showing how longer prediction horizons can be supported."],"url":"http://arxiv.org/abs/2402.17689v1","category":"cs.LG"}
{"created":"2024-02-27 17:02:00","title":"On the maximum intersecting sets of the general semilinear group of degree $2$","abstract":"Let $p$ be a prime and $q = p^k$. A subset $\\mathcal{F} \\subset \\operatorname{\\Gamma L}_{2}(q)$ is intersecting if any two semilinear transformations in $\\mathcal{F}$ agree on some non-zero vector in $\\mathbb{F}_q^2$. We show that any intersecting set of $\\operatorname{\\Gamma L}_{2}(q)$ is of size at most that of a stabilizer of a non-zero vector, and we characterize the intersecting sets of this size. Our proof relies on finding a subgraph which is a lexicographic product in the derangement graph of $\\operatorname{\\Gamma L}_{2}(q)$ in its action on non-zero vectors of $\\mathbb{F}_q^2$. This method is also applied to give a new proof that the only maximal intersecting sets of $\\operatorname{GL}_{2}(q)$ are the maximum intersecting sets.","sentences":["Let $p$ be a prime and $q =","p^k$. A subset $\\mathcal{F} \\subset \\operatorname{\\Gamma L}_{2}(q)$ is intersecting if any two semilinear transformations in $\\mathcal{F}$ agree on some non-zero vector in $\\mathbb{F}_q^2$.","We show that any intersecting set of $\\operatorname{\\Gamma L}_{2}(q)$ is of size at most that of a stabilizer of a non-zero vector, and we characterize the intersecting sets of this size.","Our proof relies on finding a subgraph which is a lexicographic product in the derangement graph of $\\operatorname{\\Gamma L}_{2}(q)$ in its action on non-zero vectors of $\\mathbb{F}_q^2$. This method is also applied to give a new proof that the only maximal intersecting sets of $\\operatorname{GL}_{2}(q)$ are the maximum intersecting sets."],"url":"http://arxiv.org/abs/2402.17687v1","category":"math.CO"}
{"created":"2024-02-27 16:54:56","title":"JKO schemes with general transport costs","abstract":"We modify the JKO scheme, which is a time discretization of Wasserstein gradient flows, by replacing the Wasserstein distance with more general transport costs on manifolds. We show when the cost function has a mixed Hessian which defines a Riemannian metric, our modified JKO scheme converges under suitable conditions to the corresponding Riemannian Fokker--Planck equation. Thus on a Riemannian manifold one may replace the (squared) Riemannian distance with any cost function which induces the metric. Of interest is when the Riemannian distance is computationally intractable, but a suitable cost has a simple analytic expression. We consider the Fokker--Planck equation on compact submanifolds with the Neumann boundary condition and on complete Riemannian manifolds with a finite drift condition. As an application we consider Hessian manifolds, taking as a cost the Bregman divergence.","sentences":["We modify the JKO scheme, which is a time discretization of Wasserstein gradient flows, by replacing the Wasserstein distance with more general transport costs on manifolds.","We show when the cost function has a mixed Hessian which defines a Riemannian metric, our modified JKO scheme converges under suitable conditions to the corresponding Riemannian Fokker--Planck equation.","Thus on a Riemannian manifold one may replace the (squared) Riemannian distance with any cost function which induces the metric.","Of interest is when the Riemannian distance is computationally intractable, but a suitable cost has a simple analytic expression.","We consider the Fokker--Planck equation on compact submanifolds with the Neumann boundary condition and on complete Riemannian manifolds with a finite drift condition.","As an application we consider Hessian manifolds, taking as a cost the Bregman divergence."],"url":"http://arxiv.org/abs/2402.17681v1","category":"math.AP"}
{"created":"2024-02-27 16:52:04","title":"Deformations of I-surfaces with elliptic singularities","abstract":"An I-surface $S$ is an algebraic surface of general type with $K_S^2 = 1$ and $p_g(S) = 2$. Recent research has centered on trying to give an explicit description of the KSBA compactification of the moduli space of these surfaces. The possible normal Gorenstein examples have been enumerated by work of Franciosi-Pardini-Rollenske. The goal of this paper is to give a more precise description of such surfaces in case their singularities are simple elliptic and/or cusp singularities, and to work out their deformation theory. In particular, under some mild general position assumptions, we show that deformations of the surfaces in question are versal for deformations of the singular points, with two exceptions where the discrepancy is analyzed in detail.","sentences":["An I-surface $S$ is an algebraic surface of general type with $K_S^2 = 1$ and $p_g(S) = 2$.","Recent research has centered on trying to give an explicit description of the KSBA compactification of the moduli space of these surfaces.","The possible normal Gorenstein examples have been enumerated by work of Franciosi-Pardini-Rollenske.","The goal of this paper is to give a more precise description of such surfaces in case their singularities are simple elliptic and/or cusp singularities, and to work out their deformation theory.","In particular, under some mild general position assumptions, we show that deformations of the surfaces in question are versal for deformations of the singular points, with two exceptions where the discrepancy is analyzed in detail."],"url":"http://arxiv.org/abs/2402.17677v1","category":"math.AG"}
{"created":"2024-02-27 16:49:50","title":"Symmetry breaking mechanisms of the 3BF action for the Standard Model coupled to gravity","abstract":"We study the details of the explicit and spontaneous symmetry breaking of the constrained 3BF action representing the Standard Model coupled to Einstein-Cartan gravity. First we discuss how each particular constraint breaks the original symmetry of the topological 3BF action. Then we investigate the spontaneous symmetry breaking and the Higgs mechanism for the electroweak theory in the constrained 3BF form, in order to demonstrate that they can indeed be performed in the framework of higher gauge theory. A formulation of the Proca action as a constrained 3BF theory is also studied in detail.","sentences":["We study the details of the explicit and spontaneous symmetry breaking of the constrained 3BF action representing the Standard Model coupled to Einstein-Cartan gravity.","First we discuss how each particular constraint breaks the original symmetry of the topological 3BF action.","Then we investigate the spontaneous symmetry breaking and the Higgs mechanism for the electroweak theory in the constrained 3BF form, in order to demonstrate that they can indeed be performed in the framework of higher gauge theory.","A formulation of the Proca action as a constrained 3BF theory is also studied in detail."],"url":"http://arxiv.org/abs/2402.17675v1","category":"hep-th"}
{"created":"2024-02-27 16:46:21","title":"SDF2Net: Shallow to Deep Feature Fusion Network for PolSAR Image Classification","abstract":"Polarimetric synthetic aperture radar (PolSAR) images encompass valuable information that can facilitate extensive land cover interpretation and generate diverse output products. Extracting meaningful features from PolSAR data poses challenges distinct from those encountered in optical imagery. Deep learning (DL) methods offer effective solutions for overcoming these challenges in PolSAR feature extraction. Convolutional neural networks (CNNs) play a crucial role in capturing PolSAR image characteristics by leveraging kernel capabilities to consider local information and the complex-valued nature of PolSAR data. In this study, a novel three-branch fusion of complex-valued CNN, named the Shallow to Deep Feature Fusion Network (SDF2Net), is proposed for PolSAR image classification. To validate the performance of the proposed method, classification results are compared against multiple state-of-the-art approaches using the airborne synthetic aperture radar (AIRSAR) datasets of Flevoland and San Francisco, as well as the ESAR Oberpfaffenhofen dataset. The results indicate that the proposed approach demonstrates improvements in overallaccuracy, with a 1.3% and 0.8% enhancement for the AIRSAR datasets and a 0.5% improvement for the ESAR dataset. Analyses conducted on the Flevoland data underscore the effectiveness of the SDF2Net model, revealing a promising overall accuracy of 96.01% even with only a 1% sampling ratio.","sentences":["Polarimetric synthetic aperture radar (PolSAR) images encompass valuable information that can facilitate extensive land cover interpretation and generate diverse output products.","Extracting meaningful features from PolSAR data poses challenges distinct from those encountered in optical imagery.","Deep learning (DL) methods offer effective solutions for overcoming these challenges in PolSAR feature extraction.","Convolutional neural networks (CNNs) play a crucial role in capturing PolSAR image characteristics by leveraging kernel capabilities to consider local information and the complex-valued nature of PolSAR data.","In this study, a novel three-branch fusion of complex-valued CNN, named the Shallow to Deep Feature Fusion Network (SDF2Net), is proposed for PolSAR image classification.","To validate the performance of the proposed method, classification results are compared against multiple state-of-the-art approaches using the airborne synthetic aperture radar (AIRSAR) datasets of Flevoland and San Francisco, as well as the ESAR Oberpfaffenhofen dataset.","The results indicate that the proposed approach demonstrates improvements in overallaccuracy, with a 1.3% and 0.8% enhancement for the AIRSAR datasets and a 0.5% improvement for the ESAR dataset.","Analyses conducted on the Flevoland data underscore the effectiveness of the SDF2Net model, revealing a promising overall accuracy of 96.01% even with only a 1% sampling ratio."],"url":"http://arxiv.org/abs/2402.17672v1","category":"cs.CV"}
{"created":"2024-02-27 16:41:54","title":"The Cost of Emulating a Small Quantum Annealing Problem in the Circuit-Model","abstract":"Demonstrations of quantum advantage for certain sampling problems has generated considerable excitement for quantum computing and has further spurred the development of circuit-model quantum computers, which represent quantum programs as a sequence of quantum gates acting on a finite number of qubits. Amongst this excitement, analog quantum computation has become less prominent, with the expectation that circuit-model quantum computers will eventually be sufficient for emulating analog quantum computation and thus rendering analog quantum computation obsolete. In this work we explore the basic requirements for emulating a specific analog quantum computation in the circuit model: the preparation of a biased superposition of degenerate ground states of an Ising Hamiltonian using an adiabatic evolution. We show that the overhead of emulation is substantial even for this simple problem. This supports using analog quantum computation for solving time-dependent Hamiltonian dynamics in the short and mid-term, assuming analog errors can be made low enough and coherence times long enough to solve problems of practical interest.","sentences":["Demonstrations of quantum advantage for certain sampling problems has generated considerable excitement for quantum computing and has further spurred the development of circuit-model quantum computers, which represent quantum programs as a sequence of quantum gates acting on a finite number of qubits.","Amongst this excitement, analog quantum computation has become less prominent, with the expectation that circuit-model quantum computers will eventually be sufficient for emulating analog quantum computation and thus rendering analog quantum computation obsolete.","In this work we explore the basic requirements for emulating a specific analog quantum computation in the circuit model: the preparation of a biased superposition of degenerate ground states of an Ising Hamiltonian using an adiabatic evolution.","We show that the overhead of emulation is substantial even for this simple problem.","This supports using analog quantum computation for solving time-dependent Hamiltonian dynamics in the short and mid-term, assuming analog errors can be made low enough and coherence times long enough to solve problems of practical interest."],"url":"http://arxiv.org/abs/2402.17667v1","category":"quant-ph"}
{"created":"2024-02-27 16:35:07","title":"Bayesian Differentiable Physics for Cloth Digitalization","abstract":"We propose a new method for cloth digitalization. Deviating from existing methods which learn from data captured under relatively casual settings, we propose to learn from data captured in strictly tested measuring protocols, and find plausible physical parameters of the cloths. However, such data is currently absent, so we first propose a new dataset with accurate cloth measurements. Further, the data size is considerably smaller than the ones in current deep learning, due to the nature of the data capture process. To learn from small data, we propose a new Bayesian differentiable cloth model to estimate the complex material heterogeneity of real cloths. It can provide highly accurate digitalization from very limited data samples. Through exhaustive evaluation and comparison, we show our method is accurate in cloth digitalization, efficient in learning from limited data samples, and general in capturing material variations. Code and data are available https://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization","sentences":["We propose a new method for cloth digitalization.","Deviating from existing methods which learn from data captured under relatively casual settings, we propose to learn from data captured in strictly tested measuring protocols, and find plausible physical parameters of the cloths.","However, such data is currently absent, so we first propose a new dataset with accurate cloth measurements.","Further, the data size is considerably smaller than the ones in current deep learning, due to the nature of the data capture process.","To learn from small data, we propose a new Bayesian differentiable cloth model to estimate the complex material heterogeneity of real cloths.","It can provide highly accurate digitalization from very limited data samples.","Through exhaustive evaluation and comparison, we show our method is accurate in cloth digitalization, efficient in learning from limited data samples, and general in capturing material variations.","Code and data are available https://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization"],"url":"http://arxiv.org/abs/2402.17664v1","category":"cs.CV"}
{"created":"2024-02-27 16:31:10","title":"Exact solutions and automorphic systems of the geopotential forecast equation","abstract":"The study of the recently constructed group foliation for the geopotential forecast equation is continued. The group foliation consists of two systems, namely the automorphic and resolving systems, the analysis of which facilitates the derivation of invariant solutions for the original equation. As obtaining a general solution to the resolving system (even to its reductions on subgroups) is problematic, its various particular solutions are considered. Consequently, the question arises concerning the specific forms of automorphic systems that correspond to exact solutions obtained through alternative methods. This is of interest for both comparing solutions derived through different approaches and for the integration of specific automorphic systems. The problem is discussed in a number of examples.","sentences":["The study of the recently constructed group foliation for the geopotential forecast equation is continued.","The group foliation consists of two systems, namely the automorphic and resolving systems, the analysis of which facilitates the derivation of invariant solutions for the original equation.","As obtaining a general solution to the resolving system (even to its reductions on subgroups) is problematic, its various particular solutions are considered.","Consequently, the question arises concerning the specific forms of automorphic systems that correspond to exact solutions obtained through alternative methods.","This is of interest for both comparing solutions derived through different approaches and for the integration of specific automorphic systems.","The problem is discussed in a number of examples."],"url":"http://arxiv.org/abs/2402.17663v1","category":"math-ph"}
{"created":"2024-02-27 16:25:46","title":"Heisenberg Soft Hair on Robinson-Trautman Spacetimes","abstract":"We study 4 dimensional $(4d$) gravitational waves (GWs) with compact wavefronts, generalizing Robinson-Trautman (RT) solutions in Einstein gravity with an arbitrary cosmological constant. We construct the most general solution of the GWs in the presence of a causal, timelike, or null boundary when the usual tensor modes are turned off. Our solution space besides the shape and topology of the wavefront which is a generic compact, smooth, and orientable $2d$ surface $\\Sigma$, is specified by a vector over $\\Sigma$ satisfying the conformal Killing equation and two scalars that are arbitrary functions over the causal boundary, the boundary modes (soft hair). We work out the symplectic form over the solution space using covariant phase space formalism and analyze the boundary symmetries and charges. The algebra of surface charges is a Heisenberg algebra. Only the overall size of the compact wavefront and not the details of its shape appears in the boundary symplectic form and is canonical conjugate to the overall mass of the GW. Hence, the information about the shape of the wavefront can't be probed by the boundary observer. We construct a boundary energy-momentum tensor and a boundary current, whose conservation yields the RT equation for both asymptotically AdS and flat spacetimes. The latter provides a hydrodynamic description for our RT solutions.","sentences":["We study 4 dimensional $(4d$) gravitational waves (GWs) with compact wavefronts, generalizing Robinson-Trautman (RT) solutions in Einstein gravity with an arbitrary cosmological constant.","We construct the most general solution of the GWs in the presence of a causal, timelike, or null boundary when the usual tensor modes are turned off.","Our solution space besides the shape and topology of the wavefront which is a generic compact, smooth, and orientable $2d$ surface $\\Sigma$, is specified by a vector over $\\Sigma$ satisfying the conformal Killing equation and two scalars that are arbitrary functions over the causal boundary, the boundary modes (soft hair).","We work out the symplectic form over the solution space using covariant phase space formalism and analyze the boundary symmetries and charges.","The algebra of surface charges is a Heisenberg algebra.","Only the overall size of the compact wavefront and not the details of its shape appears in the boundary symplectic form and is canonical conjugate to the overall mass of the GW.","Hence, the information about the shape of the wavefront can't be probed by the boundary observer.","We construct a boundary energy-momentum tensor and a boundary current, whose conservation yields the RT equation for both asymptotically AdS and flat spacetimes.","The latter provides a hydrodynamic description for our RT solutions."],"url":"http://arxiv.org/abs/2402.17658v1","category":"hep-th"}
{"created":"2024-02-27 16:25:42","title":"Classification of electronic nematicity in three-dimensional crystals and quasicrystals","abstract":"Electronic nematic order has been reported in a rich landscape of materials, encompassing not only a range of intertwined correlated and topological phenomena, but also different underlying lattice symmetries. Motivated by these findings, we investigate the behavior of electronic nematicity as the spherical symmetry of three-dimensional (3D) space is systematically lowered by the lattice environment. We consider all 32 crystallographic point groups as well as 4 major classes of quasicrystalline point groups, given the recent observations of electronic phases of interest in quasicrystalline materials and artificial twisted quasicrystals. Valuable insights are gained by establishing a mapping between the five-component charge-quadrupolar nematic order parameter of the electronic fluid and the 3D tensorial order parameter of nematic liquid crystals. We find that a uniaxial nematic state is only generically realized in polyhedral point groups (icosahedral and cubic), with the nematic director pointing along different sets of rotational symmetry axes. Interestingly, icosahedral point groups are the only ones in which the five nematic order parameter components transform as the same irreducible representation, making them the closest analog of 3D isotropic nematics. In axial point groups, one of the nematic components is always condensed, whereas the other four components decompose into an in-plane and an out-of-plane nematic doublet, resulting in biaxial nematic ground states. Because these two nematic doublets behave as $Z_q$-clock order parameters, this allows us to identify the types of crystals and quasicrystals that can host interesting electronic nematic phenomena enabled by the critical properties of the $q\\geq 4$ clock model, such as emergent continuous nematic fluctuations in 3D, critical phases with quasi-long-range nematic order in 2D, and Ashkin-Teller nematicity in 2D.","sentences":["Electronic nematic order has been reported in a rich landscape of materials, encompassing not only a range of intertwined correlated and topological phenomena, but also different underlying lattice symmetries.","Motivated by these findings, we investigate the behavior of electronic nematicity as the spherical symmetry of three-dimensional (3D) space is systematically lowered by the lattice environment.","We consider all 32 crystallographic point groups as well as 4 major classes of quasicrystalline point groups, given the recent observations of electronic phases of interest in quasicrystalline materials and artificial twisted quasicrystals.","Valuable insights are gained by establishing a mapping between the five-component charge-quadrupolar nematic order parameter of the electronic fluid and the 3D tensorial order parameter of nematic liquid crystals.","We find that a uniaxial nematic state is only generically realized in polyhedral point groups (icosahedral and cubic), with the nematic director pointing along different sets of rotational symmetry axes.","Interestingly, icosahedral point groups are the only ones in which the five nematic order parameter components transform as the same irreducible representation, making them the closest analog of 3D isotropic nematics.","In axial point groups, one of the nematic components is always condensed, whereas the other four components decompose into an in-plane and an out-of-plane nematic doublet, resulting in biaxial nematic ground states.","Because these two nematic doublets behave as $Z_q$-clock order parameters, this allows us to identify the types of crystals and quasicrystals that can host interesting electronic nematic phenomena enabled by the critical properties of the $q\\geq 4$ clock model, such as emergent continuous nematic fluctuations in 3D, critical phases with quasi-long-range nematic order in 2D, and Ashkin-Teller nematicity in 2D."],"url":"http://arxiv.org/abs/2402.17657v1","category":"cond-mat.str-el"}
{"created":"2024-02-27 16:24:59","title":"High dimensional Gross--Zagier formula: a survey","abstract":"We survey recent developments on generalizing the Gross--Zagier formula to high dimensional Shimura varieties, with an emphasis on the   Arithmetic Gan--Gross--Prasad conjecture and the relative trace formula approach.","sentences":["We survey recent developments on generalizing the Gross--Zagier formula to high dimensional Shimura varieties, with an emphasis on the   Arithmetic Gan--Gross--Prasad conjecture and the relative trace formula approach."],"url":"http://arxiv.org/abs/2402.17656v1","category":"math.NT"}
{"created":"2024-02-27 16:24:10","title":"Enumerating Permutations Avoiding Split Patterns 3|12 and 23|1","abstract":"In this paper, we give a formula for the number of permutations that avoid the split patterns $3|12$ and $23|1$ with respect to a position $r$. Such permutations count the number of Schubert varieties for which the projection map from the flag variety to a Grassmannian induces a fiber bundle structure. We also study the corresponding bivariate generating function and show how it is related to modified Bessel functions.","sentences":["In this paper, we give a formula for the number of permutations that avoid the split patterns $3|12$ and $23|1$ with respect to a position $r$. Such permutations count the number of Schubert varieties for which the projection map from the flag variety to a Grassmannian induces a fiber bundle structure.","We also study the corresponding bivariate generating function and show how it is related to modified Bessel functions."],"url":"http://arxiv.org/abs/2402.17654v1","category":"math.CO"}
{"created":"2024-02-27 16:21:28","title":"Navigator: A Decentralized Scheduler for Latency-Sensitive ML Workflows","abstract":"We consider ML query processing in distributed systems where GPU-enabled workers coordinate to execute complex queries: a computing style often seen in applications that interact with users in support of image processing and natural language processing. In such systems, coscheduling of GPU memory management and task placement represents a promising opportunity. We propose Navigator, a novel framework that unifies these functions to reduce job latency while using resources efficiently, placing tasks where data dependencies will be satisfied, collocating tasks from the same job (when this will not overload the host or its GPU), and efficiently managing GPU memory. Comparison with other state of the art schedulers shows a significant reduction in completion times while requiring the same amount or even fewer resources. In one case, just half the servers were needed for processing the same workload.","sentences":["We consider ML query processing in distributed systems where GPU-enabled workers coordinate to execute complex queries: a computing style often seen in applications that interact with users in support of image processing and natural language processing.","In such systems, coscheduling of GPU memory management and task placement represents a promising opportunity.","We propose Navigator, a novel framework that unifies these functions to reduce job latency while using resources efficiently, placing tasks where data dependencies will be satisfied, collocating tasks from the same job (when this will not overload the host or its GPU), and efficiently managing GPU memory.","Comparison with other state of the art schedulers shows a significant reduction in completion times while requiring the same amount or even fewer resources.","In one case, just half the servers were needed for processing the same workload."],"url":"http://arxiv.org/abs/2402.17652v1","category":"cs.DC"}
{"created":"2024-02-27 16:21:09","title":"CSI-Free Optimization of Reconfigurable Intelligent Surfaces with Interference by Using Multiport Network Theory","abstract":"Reconfigurable Intelligent Surfaces (RIS) will play a pivotal role in next-generation wireless systems. Despite efforts to minimize pilot overhead associated with channel estimation, the necessity of configuring the RIS multiple times before obtaining reliable Channel State Information (CSI) may significantly diminish their benefits. Therefore, we propose a CSI-free approach that explores the feasibility of optimizing the RIS for the uplink of a communication system in the presence of interfering users without relying on CSI estimation but leveraging solely some a priori statistical knowledge of the channel. In this context, we consider a multiport network model that accounts for several aspects overlooked by traditional RIS models used in Communication Theory, such as mutual coupling among scattering elements and the presence of structural scattering. The proposed approach targets the maximization of the average achievable rate and is shown to achieve performance that, in some cases, can be very close to the case where the RIS is optimized leveraging perfect CSI.","sentences":["Reconfigurable Intelligent Surfaces (RIS) will play a pivotal role in next-generation wireless systems.","Despite efforts to minimize pilot overhead associated with channel estimation, the necessity of configuring the RIS multiple times before obtaining reliable Channel State Information (CSI) may significantly diminish their benefits.","Therefore, we propose a CSI-free approach that explores the feasibility of optimizing the RIS for the uplink of a communication system in the presence of interfering users without relying on CSI estimation but leveraging solely some a priori statistical knowledge of the channel.","In this context, we consider a multiport network model that accounts for several aspects overlooked by traditional RIS models used in Communication Theory, such as mutual coupling among scattering elements and the presence of structural scattering.","The proposed approach targets the maximization of the average achievable rate and is shown to achieve performance that, in some cases, can be very close to the case where the RIS is optimized leveraging perfect CSI."],"url":"http://arxiv.org/abs/2402.17651v1","category":"cs.IT"}
{"created":"2024-02-27 16:20:46","title":"Comparison of the Effects of Interaction with Intentional Agent and Artificial Intelligence using fNIRS","abstract":"As societal interactions increasingly involve both intentional and unintentional agents, understanding their effects on human cognition becomes paramount. This study investigates the neural correlates of interacting with intentional versus artificial agents in a simulated tennis game scenario. Employing functional near-infrared imaging spectroscopy (fNIRS), we analyzed brain activity in 50 male participants during gameplay against both types of opponents. Our methodological approach ensures ecological validity by simulating real-world decision-making scenarios while participants undergo fNIRS scanning, avoiding the constraints of traditional neuroimaging methods. We focus on six prefrontal cortex channels, leveraging the 10-20 system, to capture nuanced differences in brain activity. Utilizing wavelet analysis, we dissected the data into frequency-specific differences, revealing subtle variations across different channels and frequency bands. Moreover, we quantified activity by comparing average data signals between rest and play modes across all points. Our findings unveil significant differences in neural activation patterns, particularly in one specific channel and frequency range, suggesting distinct cognitive processing when interacting with intentional agents. These results align with previous neuroimaging studies and contribute to understanding the neural underpinnings of human-agent interactions in naturalistic settings. While acknowledging study limitations, including sample homogeneity and spatial accuracy constraints, our findings underscore the potential of fNIRS in exploring complex cognitive phenomena beyond laboratory confines.","sentences":["As societal interactions increasingly involve both intentional and unintentional agents, understanding their effects on human cognition becomes paramount.","This study investigates the neural correlates of interacting with intentional versus artificial agents in a simulated tennis game scenario.","Employing functional near-infrared imaging spectroscopy (fNIRS), we analyzed brain activity in 50 male participants during gameplay against both types of opponents.","Our methodological approach ensures ecological validity by simulating real-world decision-making scenarios while participants undergo fNIRS scanning, avoiding the constraints of traditional neuroimaging methods.","We focus on six prefrontal cortex channels, leveraging the 10-20 system, to capture nuanced differences in brain activity.","Utilizing wavelet analysis, we dissected the data into frequency-specific differences, revealing subtle variations across different channels and frequency bands.","Moreover, we quantified activity by comparing average data signals between rest and play modes across all points.","Our findings unveil significant differences in neural activation patterns, particularly in one specific channel and frequency range, suggesting distinct cognitive processing when interacting with intentional agents.","These results align with previous neuroimaging studies and contribute to understanding the neural underpinnings of human-agent interactions in naturalistic settings.","While acknowledging study limitations, including sample homogeneity and spatial accuracy constraints, our findings underscore the potential of fNIRS in exploring complex cognitive phenomena beyond laboratory confines."],"url":"http://arxiv.org/abs/2402.17650v1","category":"q-bio.NC"}
{"created":"2024-02-27 16:15:28","title":"SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation","abstract":"We present SongComposer, an innovative LLM designed for song composition. It could understand and generate melodies and lyrics in symbolic song representations, by leveraging the capability of LLM. Existing music-related LLM treated the music as quantized audio signals, while such implicit encoding leads to inefficient encoding and poor flexibility. In contrast, we resort to symbolic song representation, the mature and efficient way humans designed for music, and enable LLM to explicitly compose songs like humans. In practice, we design a novel tuple design to format lyric and three note attributes (pitch, duration, and rest duration) in the melody, which guarantees the correct LLM understanding of musical symbols and realizes precise alignment between lyrics and melody. To impart basic music understanding to LLM, we carefully collected SongCompose-PT, a large-scale song pretraining dataset that includes lyrics, melodies, and paired lyrics-melodies in either Chinese or English. After adequate pre-training, 10K carefully crafted QA pairs are used to empower the LLM with the instruction-following capability and solve diverse tasks. With extensive experiments, SongComposer demonstrates superior performance in lyric-to-melody generation, melody-to-lyric generation, song continuation, and text-to-song creation, outperforming advanced LLMs like GPT-4.","sentences":["We present SongComposer, an innovative LLM designed for song composition.","It could understand and generate melodies and lyrics in symbolic song representations, by leveraging the capability of LLM.","Existing music-related LLM treated the music as quantized audio signals, while such implicit encoding leads to inefficient encoding and poor flexibility.","In contrast, we resort to symbolic song representation, the mature and efficient way humans designed for music, and enable LLM to explicitly compose songs like humans.","In practice, we design a novel tuple design to format lyric and three note attributes (pitch, duration, and rest duration) in the melody, which guarantees the correct LLM understanding of musical symbols and realizes precise alignment between lyrics and melody.","To impart basic music understanding to LLM, we carefully collected SongCompose-PT, a large-scale song pretraining dataset that includes lyrics, melodies, and paired lyrics-melodies in either Chinese or English.","After adequate pre-training, 10K carefully crafted QA pairs are used to empower the LLM with the instruction-following capability and solve diverse tasks.","With extensive experiments, SongComposer demonstrates superior performance in lyric-to-melody generation, melody-to-lyric generation, song continuation, and text-to-song creation, outperforming advanced LLMs like GPT-4."],"url":"http://arxiv.org/abs/2402.17645v1","category":"cs.SD"}
{"created":"2024-02-27 16:15:03","title":"Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data","abstract":"Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement. Among open-source models, Deepseek-coder-instruct, a code LLM pretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously. Code and data are in https://github.com/xxxiaol/QRData.","sentences":["Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited.","To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data.","The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers.","To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText.","We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models.","The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement.","Among open-source models, Deepseek-coder-instruct, a code LLM pretrained on 2T tokens, gets the highest accuracy of 37%.","Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously.","Code and data are in https://github.com/xxxiaol/QRData."],"url":"http://arxiv.org/abs/2402.17644v1","category":"cs.CL"}
{"created":"2024-02-27 16:11:05","title":"Variational Learning is Effective for Large Deep Networks","abstract":"We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence in support of effectiveness of variational learning.","sentences":["We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks.","We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch.","IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better.","We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data.","We find overwhelming evidence in support of effectiveness of variational learning."],"url":"http://arxiv.org/abs/2402.17641v1","category":"cs.LG"}
{"created":"2024-02-27 16:07:56","title":"Exergetic Port-Hamiltonian Systems Modeling Language","abstract":"Mathematical modeling of real-world physical systems requires the consistent combination of a multitude of physical laws and phenomenological models. This challenging task can be greatly simplified by hierarchically decomposing systems. Moreover, the use of diagrams for expressing such decompositions helps make the process more intuitive and facilitates communication, even with non-experts. As an important requirement, models have to respect fundamental physical laws such as the first and the second law of thermodynamics. While some existing modeling frameworks can make such guarantees based on structural properties of their models, they lack a formal graphical syntax. We present a compositional and thermodynamically consistent modeling language with a graphical syntax. As its semantics, port-Hamiltonian systems are endowed with further structural properties and a fixed physical interpretation such that thermodynamic consistency is ensured in a way that is closely related to the GENERIC framework. While port-Hamiltonian systems are inspired by graphical modeling with bond graphs, neither the link between the two, nor bond graphs themselves, can be easily formalized. In contrast, our syntax is based on a refinement of the well-studied operad of undirected wiring diagrams. The language effectively decouples the construction of complex models via the graphical syntax from physical concerns, which are dealt with only at the level of primitive subsystems that represent elementary physical behaviors. As a consequence, reuse of models and substitution of their parts becomes possible. Finally, by construction, systems interact by exchanging exergy, i.e. energy that is available for doing work, so the language is particularly well suited for thermodynamic analysis and optimization.","sentences":["Mathematical modeling of real-world physical systems requires the consistent combination of a multitude of physical laws and phenomenological models.","This challenging task can be greatly simplified by hierarchically decomposing systems.","Moreover, the use of diagrams for expressing such decompositions helps make the process more intuitive and facilitates communication, even with non-experts.","As an important requirement, models have to respect fundamental physical laws such as the first and the second law of thermodynamics.","While some existing modeling frameworks can make such guarantees based on structural properties of their models, they lack a formal graphical syntax.","We present a compositional and thermodynamically consistent modeling language with a graphical syntax.","As its semantics, port-Hamiltonian systems are endowed with further structural properties and a fixed physical interpretation such that thermodynamic consistency is ensured in a way that is closely related to the GENERIC framework.","While port-Hamiltonian systems are inspired by graphical modeling with bond graphs, neither the link between the two, nor bond graphs themselves, can be easily formalized.","In contrast, our syntax is based on a refinement of the well-studied operad of undirected wiring diagrams.","The language effectively decouples the construction of complex models via the graphical syntax from physical concerns, which are dealt with only at the level of primitive subsystems that represent elementary physical behaviors.","As a consequence, reuse of models and substitution of their parts becomes possible.","Finally, by construction, systems interact by exchanging exergy, i.e. energy that is available for doing work, so the language is particularly well suited for thermodynamic analysis and optimization."],"url":"http://arxiv.org/abs/2402.17640v1","category":"eess.SY"}
{"created":"2024-02-27 16:07:34","title":"Local heat current flow in the ballistic phonon transport of graphene nanoribbons","abstract":"Utilizing nonequilibrium Green's function method, we study the phonon local heat current flow in nanoscale graphene nanoribbons. Ballistic transport and boundary scattering lead to formation of atomic scale current vortices. Using the B\\\"uttiker probe approach, we further map out the temperature distribution in the junction. From the heat current and temperature distribution, we observe negative local resistance of the junctions, where heat current direction goes from colder to hotter regime. Moreover, we show that atomic scale defect can generate heat vortex at certain frequency, but it is averaged out when including contributions from all the phonon modes. These results extend the study of local heat vortex and negative temperature response in bulk hydrodynamic regime to atomic-scale ballistic regime, further confirming boundary scattering is crucial to generate backflow of heat current.","sentences":["Utilizing nonequilibrium Green's function method, we study the phonon local heat current flow in nanoscale graphene nanoribbons.","Ballistic transport and boundary scattering lead to formation of atomic scale current vortices.","Using the B\\\"uttiker probe approach, we further map out the temperature distribution in the junction.","From the heat current and temperature distribution, we observe negative local resistance of the junctions, where heat current direction goes from colder to hotter regime.","Moreover, we show that atomic scale defect can generate heat vortex at certain frequency, but it is averaged out when including contributions from all the phonon modes.","These results extend the study of local heat vortex and negative temperature response in bulk hydrodynamic regime to atomic-scale ballistic regime, further confirming boundary scattering is crucial to generate backflow of heat current."],"url":"http://arxiv.org/abs/2402.17639v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-27 15:59:37","title":"From Text Segmentation to Smart Chaptering: A Novel Benchmark for Structuring Video Transcriptions","abstract":"Text segmentation is a fundamental task in natural language processing, where documents are split into contiguous sections. However, prior research in this area has been constrained by limited datasets, which are either small in scale, synthesized, or only contain well-structured documents. In this paper, we address these limitations by introducing a novel benchmark YTSeg focusing on spoken content that is inherently more unstructured and both topically and structurally diverse. As part of this work, we introduce an efficient hierarchical segmentation model MiniSeg, that outperforms state-of-the-art baselines. Lastly, we expand the notion of text segmentation to a more practical \"smart chaptering\" task that involves the segmentation of unstructured content, the generation of meaningful segment titles, and a potential real-time application of the models.","sentences":["Text segmentation is a fundamental task in natural language processing, where documents are split into contiguous sections.","However, prior research in this area has been constrained by limited datasets, which are either small in scale, synthesized, or only contain well-structured documents.","In this paper, we address these limitations by introducing a novel benchmark YTSeg focusing on spoken content that is inherently more unstructured and both topically and structurally diverse.","As part of this work, we introduce an efficient hierarchical segmentation model MiniSeg, that outperforms state-of-the-art baselines.","Lastly, we expand the notion of text segmentation to a more practical \"smart chaptering\" task that involves the segmentation of unstructured content, the generation of meaningful segment titles, and a potential real-time application of the models."],"url":"http://arxiv.org/abs/2402.17633v1","category":"cs.CL"}
{"created":"2024-02-27 15:55:05","title":"Many-body perturbation theory for strongly correlated effective Hamiltonians using effective field theory methods","abstract":"Introducing low-energy effective Hamiltonians is usual to grasp most correlations in quantum many-body problems. For instance, such effective Hamiltonians can be treated at the mean-field level to reproduce some physical properties of interest. Employing effective Hamiltonians that contain many-body correlations renders the use of perturbative many-body techniques difficult because of the overcounting of correlations. In this work, we develop a strategy to apply an extension of the many-body perturbation theory, starting from an effective interaction that contains correlations beyond the mean field level. The goal is to re-organize the many-body calculation to avoid the overcounting of correlations originating from the introduction of correlated effective Hamiltonians in the description. For this purpose, we generalize the formulation of the Rayleigh-Schr\\\"odinger perturbation theory by including free parameters adjusted to reproduce the appropriate limits. In particular, the expansion in the bare weak-coupling regime and the strong-coupling limit serves as a valuable input to fix the value of the free parameters appearing in the resulting expression. This method avoids double counting of correlations using beyond-mean-field strategies for the description of many-body systems. The ground state energy of various systems relevant for ultracold atomic, nuclear, and condensed matter physics is reproduced qualitatively beyond the domain of validity of the standard many-body perturbation theory. Finally, our method suggests interpreting the formal results obtained as an effective field theory using the proposed reorganization of the many-body calculation. The results, like ground state energies, are improved systematically by considering higher orders in the extended many-body perturbation theory while maintaining a straightforward polynomial expansion.","sentences":["Introducing low-energy effective Hamiltonians is usual to grasp most correlations in quantum many-body problems.","For instance, such effective Hamiltonians can be treated at the mean-field level to reproduce some physical properties of interest.","Employing effective Hamiltonians that contain many-body correlations renders the use of perturbative many-body techniques difficult because of the overcounting of correlations.","In this work, we develop a strategy to apply an extension of the many-body perturbation theory, starting from an effective interaction that contains correlations beyond the mean field level.","The goal is to re-organize the many-body calculation to avoid the overcounting of correlations originating from the introduction of correlated effective Hamiltonians in the description.","For this purpose, we generalize the formulation of the Rayleigh-Schr\\\"odinger perturbation theory by including free parameters adjusted to reproduce the appropriate limits.","In particular, the expansion in the bare weak-coupling regime and the strong-coupling limit serves as a valuable input to fix the value of the free parameters appearing in the resulting expression.","This method avoids double counting of correlations using beyond-mean-field strategies for the description of many-body systems.","The ground state energy of various systems relevant for ultracold atomic, nuclear, and condensed matter physics is reproduced qualitatively beyond the domain of validity of the standard many-body perturbation theory.","Finally, our method suggests interpreting the formal results obtained as an effective field theory using the proposed reorganization of the many-body calculation.","The results, like ground state energies, are improved systematically by considering higher orders in the extended many-body perturbation theory while maintaining a straightforward polynomial expansion."],"url":"http://arxiv.org/abs/2402.17627v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-27 15:54:18","title":"An algebraic approach to gravitational quantum mechanics","abstract":"Most approaches towards a quantum theory of gravitation indicate the existence of a minimal length scale of the order of the Planck length. Quantum mechanical models incorporating such an intrinsic length scale call for a deformation of Heisenberg's algebra resulting in a generalized uncertainty principle and constitute what is called gravitational quantum mechanics. Here we adopt the pseudo or g-calculus to study various models of gravitational quantum mechanics. The free time evolution of a Gaussian wave packet is investigated as well as the spectral properties of a particle bound by an external attractive potential. Here the cases of a box with finite width and infinite walls, an attractive potential well of finite depth and a delta-like potential are considered.","sentences":["Most approaches towards a quantum theory of gravitation indicate the existence of a minimal length scale of the order of the Planck length.","Quantum mechanical models incorporating such an intrinsic length scale call for a deformation of Heisenberg's algebra resulting in a generalized uncertainty principle and constitute what is called gravitational quantum mechanics.","Here we adopt the pseudo or g-calculus to study various models of gravitational quantum mechanics.","The free time evolution of a Gaussian wave packet is investigated as well as the spectral properties of a particle bound by an external attractive potential.","Here the cases of a box with finite width and infinite walls, an attractive potential well of finite depth and a delta-like potential are considered."],"url":"http://arxiv.org/abs/2402.17626v1","category":"gr-qc"}
{"created":"2024-02-27 15:52:59","title":"CustomSketching: Sketch Concept Extraction for Sketch-based Image Synthesis and Editing","abstract":"Personalization techniques for large text-to-image (T2I) models allow users to incorporate new concepts from reference images. However, existing methods primarily rely on textual descriptions, leading to limited control over customized images and failing to support fine-grained and local editing (e.g., shape, pose, and details). In this paper, we identify sketches as an intuitive and versatile representation that can facilitate such control, e.g., contour lines capturing shape information and flow lines representing texture. This motivates us to explore a novel task of sketch concept extraction: given one or more sketch-image pairs, we aim to extract a special sketch concept that bridges the correspondence between the images and sketches, thus enabling sketch-based image synthesis and editing at a fine-grained level. To accomplish this, we introduce CustomSketching, a two-stage framework for extracting novel sketch concepts. Considering that an object can often be depicted by a contour for general shapes and additional strokes for internal details, we introduce a dual-sketch representation to reduce the inherent ambiguity in sketch depiction. We employ a shape loss and a regularization loss to balance fidelity and editability during optimization. Through extensive experiments, a user study, and several applications, we show our method is effective and superior to the adapted baselines.","sentences":["Personalization techniques for large text-to-image (T2I) models allow users to incorporate new concepts from reference images.","However, existing methods primarily rely on textual descriptions, leading to limited control over customized images and failing to support fine-grained and local editing (e.g., shape, pose, and details).","In this paper, we identify sketches as an intuitive and versatile representation that can facilitate such control, e.g., contour lines capturing shape information and flow lines representing texture.","This motivates us to explore a novel task of sketch concept extraction: given one or more sketch-image pairs, we aim to extract a special sketch concept that bridges the correspondence between the images and sketches, thus enabling sketch-based image synthesis and editing at a fine-grained level.","To accomplish this, we introduce CustomSketching, a two-stage framework for extracting novel sketch concepts.","Considering that an object can often be depicted by a contour for general shapes and additional strokes for internal details, we introduce a dual-sketch representation to reduce the inherent ambiguity in sketch depiction.","We employ a shape loss and a regularization loss to balance fidelity and editability during optimization.","Through extensive experiments, a user study, and several applications, we show our method is effective and superior to the adapted baselines."],"url":"http://arxiv.org/abs/2402.17624v1","category":"cs.CV"}
{"created":"2024-02-27 15:49:54","title":"Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image Modeling","abstract":"This work proposes a semantic segmentation network that produces high-quality uncertainty estimates in a single forward pass. We exploit general representations from foundation models and unlabelled datasets through a Masked Image Modeling (MIM) approach, which is robust to augmentation hyper-parameters and simpler than previous techniques. For neural networks used in safety-critical applications, bias in the training data can lead to errors; therefore it is crucial to understand a network's limitations at run time and act accordingly. To this end, we test our proposed method on a number of test domains including the SAX Segmentation benchmark, which includes labelled test data from dense urban, rural and off-road driving domains. The proposed method consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark.","sentences":["This work proposes a semantic segmentation network that produces high-quality uncertainty estimates in a single forward pass.","We exploit general representations from foundation models and unlabelled datasets through a Masked Image Modeling (MIM) approach, which is robust to augmentation hyper-parameters and simpler than previous techniques.","For neural networks used in safety-critical applications, bias in the training data can lead to errors; therefore it is crucial to understand a network's limitations at run time and act accordingly.","To this end, we test our proposed method on a number of test domains including the SAX Segmentation benchmark, which includes labelled test data from dense urban, rural and off-road driving domains.","The proposed method consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark."],"url":"http://arxiv.org/abs/2402.17622v1","category":"cs.CV"}
{"created":"2024-02-27 15:46:50","title":"The Weak Lefschetz property and unimodality of Hilbert functions of random monomial algebras","abstract":"In this work, we investigate the presence of the weak Lefschetz property (WLP) and Hilbert functions for various types of random standard graded Artinian algebras. If an algebra has the WLP then its Hilbert function is unimodal.   Using probabilistic models for random monomial algebras, our results and simulations suggest that in each considered regime the Hilbert functions of the produced algebras are unimodal with high probability. The WLP appears to be present with high probability most of the time. However, we propose that there is one scenario where the generated algebras fail to have the WLP with high probability.","sentences":["In this work, we investigate the presence of the weak Lefschetz property (WLP) and Hilbert functions for various types of random standard graded Artinian algebras.","If an algebra has the WLP then its Hilbert function is unimodal.   ","Using probabilistic models for random monomial algebras, our results and simulations suggest that in each considered regime the Hilbert functions of the produced algebras are unimodal with high probability.","The WLP appears to be present with high probability most of the time.","However, we propose that there is one scenario where the generated algebras fail to have the WLP with high probability."],"url":"http://arxiv.org/abs/2402.17618v1","category":"math.AC"}
{"created":"2024-02-27 15:45:15","title":"Quantifying the Resolution of a Template after Image Registration","abstract":"In many image processing applications (e.g. computational anatomy) a groupwise registration is performed on a sample of images and a template image is simultaneously generated. From the template alone it is in general unclear to which extent the registered images are still misaligned, which means that some regions of the template represent the structural features in the sample images less reliably than others. In a sense, the template exhibits a lower resolution there. Guided by characteristic examples of misaligned image features in one dimension, we develop a visual measure to quantify the resolution at each location of a template which is based on the observation that misalignments between the registered sample images are reduced by smoothing with the strength of the smoothing being related to the magnitude of the misalignment. Finally the resulting resolution measure is applied to example datasets in two and three dimensions. The corresponding code is publicly available on GitHub.","sentences":["In many image processing applications (e.g. computational anatomy) a groupwise registration is performed on a sample of images and a template image is simultaneously generated.","From the template alone it is in general unclear to which extent the registered images are still misaligned, which means that some regions of the template represent the structural features in the sample images less reliably than others.","In a sense, the template exhibits a lower resolution there.","Guided by characteristic examples of misaligned image features in one dimension, we develop a visual measure to quantify the resolution at each location of a template which is based on the observation that misalignments between the registered sample images are reduced by smoothing with the strength of the smoothing being related to the magnitude of the misalignment.","Finally the resulting resolution measure is applied to example datasets in two and three dimensions.","The corresponding code is publicly available on GitHub."],"url":"http://arxiv.org/abs/2402.17617v1","category":"eess.IV"}
{"created":"2024-02-27 15:44:12","title":"A Multi-Agent Model for Opinion Evolution under Cognitive Biases","abstract":"We generalize the DeGroot model for opinion dynamics to better capture realistic social scenarios. We introduce a model where each agent has their own individual cognitive biases. Society is represented as a directed graph whose edges indicate how much agents influence one another. Biases are represented as the functions in the square region $[-1,1]^2$ and categorized into four sub-regions based on the potential reactions they may elicit in an agent during instances of opinion disagreement. Under the assumption that each bias of every agent is a continuous function within the region of receptive but resistant reactions ($\\mathbf{R}$), we show that the society converges to a consensus if the graph is strongly connected. Under the same assumption, we also establish that the entire society converges to a unanimous opinion if and only if the source components of the graph-namely, strongly connected components with no external influence-converge to that opinion. We illustrate that convergence is not guaranteed for strongly connected graphs when biases are either discontinuous functions in $\\mathbf{R}$ or not included in $\\mathbf{R}$. We showcase our model through a series of examples and simulations, offering insights into how opinions form in social networks under cognitive biases.","sentences":["We generalize the DeGroot model for opinion dynamics to better capture realistic social scenarios.","We introduce a model where each agent has their own individual cognitive biases.","Society is represented as a directed graph whose edges indicate how much agents influence one another.","Biases are represented as the functions in the square region $","[-1,1]^2$ and categorized into four sub-regions based on the potential reactions they may elicit in an agent during instances of opinion disagreement.","Under the assumption that each bias of every agent is a continuous function within the region of receptive but resistant reactions ($\\mathbf{R}$), we show that the society converges to a consensus if the graph is strongly connected.","Under the same assumption, we also establish that the entire society converges to a unanimous opinion if and only if the source components of the graph-namely, strongly connected components with no external influence-converge to that opinion.","We illustrate that convergence is not guaranteed for strongly connected graphs when biases are either discontinuous functions in $\\mathbf{R}$ or not included in $\\mathbf{R}$. We showcase our model through a series of examples and simulations, offering insights into how opinions form in social networks under cognitive biases."],"url":"http://arxiv.org/abs/2402.17615v1","category":"cs.MA"}
{"created":"2024-02-27 15:43:53","title":"Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation","abstract":"Few-shot segmentation performance declines substantially when facing images from a domain different than the training domain, effectively limiting real-world use cases. To alleviate this, recently cross-domain few-shot segmentation (CD-FSS) has emerged. Works that address this task mainly attempted to learn segmentation on a source domain in a manner that generalizes across domains. Surprisingly, we can outperform these approaches while eliminating the training stage and removing their main segmentation network. We show test-time task-adaption is the key for successful CD-FSS instead. Task-adaption is achieved by appending small networks to the feature pyramid of a conventionally classification-pretrained backbone. To avoid overfitting to the few labeled samples in supervised fine-tuning, consistency across augmented views of input images serves as guidance while learning the parameters of the attached layers. Despite our self-restriction not to use any images other than the few labeled samples at test time, we achieve new state-of-the-art performance in CD-FSS, evidencing the need to rethink approaches for the task.","sentences":["Few-shot segmentation performance declines substantially when facing images from a domain different than the training domain, effectively limiting real-world use cases.","To alleviate this, recently cross-domain few-shot segmentation (CD-FSS) has emerged.","Works that address this task mainly attempted to learn segmentation on a source domain in a manner that generalizes across domains.","Surprisingly, we can outperform these approaches while eliminating the training stage and removing their main segmentation network.","We show test-time task-adaption is the key for successful CD-FSS instead.","Task-adaption is achieved by appending small networks to the feature pyramid of a conventionally classification-pretrained backbone.","To avoid overfitting to the few labeled samples in supervised fine-tuning, consistency across augmented views of input images serves as guidance while learning the parameters of the attached layers.","Despite our self-restriction not to use any images other than the few labeled samples at test time, we achieve new state-of-the-art performance in CD-FSS, evidencing the need to rethink approaches for the task."],"url":"http://arxiv.org/abs/2402.17614v1","category":"cs.CV"}
{"created":"2024-02-27 15:35:55","title":"Out-of-time-ordered correlators for Wigner matrices","abstract":"We consider the time evolution of the out-of-time-ordered correlator (OTOC) of two general observables $A$ and $B$ in a mean field chaotic quantum system described by a random Wigner matrix as its Hamiltonian. We rigorously identify three time regimes separated by the physically relevant scrambling and relaxation times. The main feature of our analysis is that we express the error terms in the optimal Schatten (tracial) norms of the observables, allowing us to track the exact dependence of the errors on their rank. In particular, for significantly overlapping observables with low rank the OTOC is shown to exhibit a significant local maximum at the scrambling time, a feature that may not have been noticed in the physics literature before. Our main tool is a novel multi-resolvent local law with Schatten norms that unifies and improves previous local laws involving either the much cruder operator norm (cf. [G. Cipolloni, L. Erd\\H{o}s, D. Schr\\\"oder. Elect. J. Prob. 27, 1-38, 2022]) or the Hilbert-Schmidt norm (cf. [G. Cipolloni, L. Erd\\H{o}s, D. Schr\\\"oder. Forum Math., Sigma 10, E96, 2022]).","sentences":["We consider the time evolution of the out-of-time-ordered correlator (OTOC) of two general observables $A$ and $B$ in a mean field chaotic quantum system described by a random Wigner matrix as its Hamiltonian.","We rigorously identify three time regimes separated by the physically relevant scrambling and relaxation times.","The main feature of our analysis is that we express the error terms in the optimal Schatten (tracial) norms of the observables, allowing us to track the exact dependence of the errors on their rank.","In particular, for significantly overlapping observables with low rank the OTOC is shown to exhibit a significant local maximum at the scrambling time, a feature that may not have been noticed in the physics literature before.","Our main tool is a novel multi-resolvent local law with Schatten norms that unifies and improves previous local laws involving either the much cruder operator norm (cf.","[G. Cipolloni, L. Erd\\H{o}s, D. Schr\\\"oder. Elect.","J. Prob.","27, 1-38, 2022]) or the Hilbert-Schmidt norm (cf.","[G. Cipolloni, L. Erd\\H{o}s, D. Schr\\\"oder.","Forum Math., Sigma 10, E96, 2022])."],"url":"http://arxiv.org/abs/2402.17609v1","category":"math-ph"}
{"created":"2024-02-27 15:34:15","title":"Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)","abstract":"In this paper, we explore the impact of augmenting pre-trained Encoder-Decoder models, specifically T5, with linguistic knowledge for the prediction of a target task. In particular, we investigate whether fine-tuning a T5 model on an intermediate task that predicts structural linguistic properties of sentences modifies its performance in the target task of predicting sentence-level complexity. Our study encompasses diverse experiments conducted on Italian and English datasets, employing both monolingual and multilingual T5 models at various sizes. Results obtained for both languages and in cross-lingual configurations show that linguistically motivated intermediate fine-tuning has generally a positive impact on target task performance, especially when applied to smaller models and in scenarios with limited data availability.","sentences":["In this paper, we explore the impact of augmenting pre-trained Encoder-Decoder models, specifically T5, with linguistic knowledge for the prediction of a target task.","In particular, we investigate whether fine-tuning a T5 model on an intermediate task that predicts structural linguistic properties of sentences modifies its performance in the target task of predicting sentence-level complexity.","Our study encompasses diverse experiments conducted on Italian and English datasets, employing both monolingual and multilingual T5 models at various sizes.","Results obtained for both languages and in cross-lingual configurations show that linguistically motivated intermediate fine-tuning has generally a positive impact on target task performance, especially when applied to smaller models and in scenarios with limited data availability."],"url":"http://arxiv.org/abs/2402.17608v1","category":"cs.CL"}
{"created":"2024-02-27 15:33:26","title":"Radar Resource Management for Active Tracking Using Split-Aperture Phased Arrays","abstract":"Flexible front-end technology will become available in future multifunction radar systems to improve adaptability to the operational theatre. A potential concept to utilize this flexibility is to subdivide radar tasks spatially over the array, the so-called split-aperture phased array (SAPA) concept. As radars are generally designed for their worst-case scenario, e.g., small targets at a large range, the power-aperture budget can be excessive for targets that do not fall within that class. To increase efficiency of the time budget of the radar front-end, the SAPA concept could be applied. In this paper, the SAPA concept is explored to assign radar resources for active tracking tasks of many targets. To do so, we formulate and solve the radar resource management problem for the SAPA concept by employing the quality of service based resource allocation model (Q-RAM) framework. It will be demonstrated by a simulation example that a radar can maintain a larger numbers of active tracking tasks when using the SAPA concept compared to the case that only the full array can be used per task.","sentences":["Flexible front-end technology will become available in future multifunction radar systems to improve adaptability to the operational theatre.","A potential concept to utilize this flexibility is to subdivide radar tasks spatially over the array, the so-called split-aperture phased array (SAPA) concept.","As radars are generally designed for their worst-case scenario, e.g., small targets at a large range, the power-aperture budget can be excessive for targets that do not fall within that class.","To increase efficiency of the time budget of the radar front-end, the SAPA concept could be applied.","In this paper, the SAPA concept is explored to assign radar resources for active tracking tasks of many targets.","To do so, we formulate and solve the radar resource management problem for the SAPA concept by employing the quality of service based resource allocation model (Q-RAM) framework.","It will be demonstrated by a simulation example that a radar can maintain a larger numbers of active tracking tasks when using the SAPA concept compared to the case that only the full array can be used per task."],"url":"http://arxiv.org/abs/2402.17607v1","category":"eess.SP"}
{"created":"2024-02-27 15:33:20","title":"Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem","abstract":"Existing learning-based methods for solving job shop scheduling problem (JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and neglect the rich and meaningful topological structures of disjunctive graphs (DGs). This paper proposes the topology-aware bidirectional graph attention network (TBGAT), a novel GNN architecture based on the attention mechanism, to embed the DG for solving JSSP in a local search framework. Specifically, TBGAT embeds the DG from a forward and a backward view, respectively, where the messages are propagated by following the different topologies of the views and aggregated via graph attention. Then, we propose a novel operator based on the message-passing mechanism to calculate the forward and backward topological sorts of the DG, which are the features for characterizing the topological structures and exploited by our model. In addition, we theoretically and experimentally show that TBGAT has linear computational complexity to the number of jobs and machines, respectively, which strengthens the practical value of our method. Besides, extensive experiments on five synthetic datasets and seven classic benchmarks show that TBGAT achieves new SOTA results by outperforming a wide range of neural methods by a large margin.","sentences":["Existing learning-based methods for solving job shop scheduling problem (JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and neglect the rich and meaningful topological structures of disjunctive graphs (DGs).","This paper proposes the topology-aware bidirectional graph attention network (TBGAT), a novel GNN architecture based on the attention mechanism, to embed the DG for solving JSSP in a local search framework.","Specifically, TBGAT embeds the DG from a forward and a backward view, respectively, where the messages are propagated by following the different topologies of the views and aggregated via graph attention.","Then, we propose a novel operator based on the message-passing mechanism to calculate the forward and backward topological sorts of the DG, which are the features for characterizing the topological structures and exploited by our model.","In addition, we theoretically and experimentally show that TBGAT has linear computational complexity to the number of jobs and machines, respectively, which strengthens the practical value of our method.","Besides, extensive experiments on five synthetic datasets and seven classic benchmarks show that TBGAT achieves new SOTA results by outperforming a wide range of neural methods by a large margin."],"url":"http://arxiv.org/abs/2402.17606v1","category":"cs.LG"}
{"created":"2024-02-27 15:31:00","title":"Equivariant ideals of polynomials","abstract":"We study existence and computability of finite bases for ideals of polynomials over infinitely many variables. In our setting, variables come from a countable logical structure A, and embeddings from A to A act on polynomials by renaming variables. First, we give a sufficient and necessary condition for A to guarantee the following generalisation of Hilbert's Basis Theorem: every polynomial ideal which is equivariant, i.e. invariant under renaming of variables, is finitely generated. Second, we develop an extension of classical Buchberger's algorithm to compute a Gr\\\"obner basis of a given equivariant ideal. This implies decidability of the membership problem for equivariant ideals. Finally, we sketch upon various applications of these results to register automata, Petri nets with data, orbit-finitely generated vector spaces, and orbit-finite systems of linear equations.","sentences":["We study existence and computability of finite bases for ideals of polynomials over infinitely many variables.","In our setting, variables come from a countable logical structure A, and embeddings from A to A act on polynomials by renaming variables.","First, we give a sufficient and necessary condition for A to guarantee the following generalisation of Hilbert's Basis Theorem: every polynomial ideal which is equivariant, i.e. invariant under renaming of variables, is finitely generated.","Second, we develop an extension of classical Buchberger's algorithm to compute a Gr\\\"obner basis of a given equivariant ideal.","This implies decidability of the membership problem for equivariant ideals.","Finally, we sketch upon various applications of these results to register automata, Petri nets with data, orbit-finitely generated vector spaces, and orbit-finite systems of linear equations."],"url":"http://arxiv.org/abs/2402.17604v1","category":"cs.LO"}
{"created":"2024-02-27 15:30:01","title":"Advancing sleep detection by modelling weak label sets: A novel weakly supervised learning approach","abstract":"Understanding sleep and activity patterns plays a crucial role in physical and mental health. This study introduces a novel approach for sleep detection using weakly supervised learning for scenarios where reliable ground truth labels are unavailable. The proposed method relies on a set of weak labels, derived from the predictions generated by conventional sleep detection algorithms. Introducing a novel approach, we suggest a novel generalised non-linear statistical model in which the number of weak sleep labels is modelled as outcome of a binomial distribution. The probability of sleep in the binomial distribution is linked to the outcomes of neural networks trained to detect sleep based on actigraphy. We show that maximizing the likelihood function of the model, is equivalent to minimizing the soft cross-entropy loss. Additionally, we explored the use of the Brier score as a loss function for weak labels. The efficacy of the suggested modelling framework was demonstrated using the Multi-Ethnic Study of Atherosclerosis dataset. A \\gls{lstm} trained on the soft cross-entropy outperformed conventional sleep detection algorithms, other neural network architectures and loss functions in accuracy and model calibration. This research not only advances sleep detection techniques in scenarios where ground truth data is scarce but also contributes to the broader field of weakly supervised learning by introducing innovative approach in modelling sets of weak labels.","sentences":["Understanding sleep and activity patterns plays a crucial role in physical and mental health.","This study introduces a novel approach for sleep detection using weakly supervised learning for scenarios where reliable ground truth labels are unavailable.","The proposed method relies on a set of weak labels, derived from the predictions generated by conventional sleep detection algorithms.","Introducing a novel approach, we suggest a novel generalised non-linear statistical model in which the number of weak sleep labels is modelled as outcome of a binomial distribution.","The probability of sleep in the binomial distribution is linked to the outcomes of neural networks trained to detect sleep based on actigraphy.","We show that maximizing the likelihood function of the model, is equivalent to minimizing the soft cross-entropy loss.","Additionally, we explored the use of the Brier score as a loss function for weak labels.","The efficacy of the suggested modelling framework was demonstrated using the Multi-Ethnic Study of Atherosclerosis dataset.","A \\gls{lstm} trained on the soft cross-entropy outperformed conventional sleep detection algorithms, other neural network architectures and loss functions in accuracy and model calibration.","This research not only advances sleep detection techniques in scenarios where ground truth data is scarce but also contributes to the broader field of weakly supervised learning by introducing innovative approach in modelling sets of weak labels."],"url":"http://arxiv.org/abs/2402.17601v1","category":"cs.LG"}
{"created":"2024-02-27 15:29:12","title":"Optimal shielding for Einstein gravity","abstract":"In order to construct asymptotically Euclidean, Einstein's initial data sets, we introduce the localized seed-to-solution method and establish the existence of classes of data sets that exhibit the gravity shielding phenomenon (or localization). We achieve optimal shielding in the sense that the gluing domain can be a collection of arbitrarily narrow nested cones while the metric and extrinsic curvature may be controlled at a super-harmonic rate, and may have arbitrarily slow decay (possibly beyond the standard ADM formalism). We also uncover several notions of physical and mathematical interest: normalized asymptotic kernel elements, localized energy functionals, localized ADM modulators, and relative energy-momentum vectors.","sentences":["In order to construct asymptotically Euclidean, Einstein's initial data sets, we introduce the localized seed-to-solution method and establish the existence of classes of data sets that exhibit the gravity shielding phenomenon (or localization).","We achieve optimal shielding in the sense that the gluing domain can be a collection of arbitrarily narrow nested cones while the metric and extrinsic curvature may be controlled at a super-harmonic rate, and may have arbitrarily slow decay (possibly beyond the standard ADM formalism).","We also uncover several notions of physical and mathematical interest: normalized asymptotic kernel elements, localized energy functionals, localized ADM modulators, and relative energy-momentum vectors."],"url":"http://arxiv.org/abs/2402.17598v1","category":"gr-qc"}
{"created":"2024-02-27 15:28:01","title":"Implicit Regularization via Spectral Neural Networks and Non-linear Matrix Sensing","abstract":"The phenomenon of implicit regularization has attracted interest in recent years as a fundamental aspect of the remarkable generalizing ability of neural networks. In a nutshell, it entails that gradient descent dynamics in many neural nets, even without any explicit regularizer in the loss function, converges to the solution of a regularized learning problem. However, known results attempting to theoretically explain this phenomenon focus overwhelmingly on the setting of linear neural nets, and the simplicity of the linear structure is particularly crucial to existing arguments. In this paper, we explore this problem in the context of more realistic neural networks with a general class of non-linear activation functions, and rigorously demonstrate the implicit regularization phenomenon for such networks in the setting of matrix sensing problems, together with rigorous rate guarantees that ensure exponentially fast convergence of gradient descent.In this vein, we contribute a network architecture called Spectral Neural Networks (abbrv. SNN) that is particularly suitable for matrix learning problems. Conceptually, this entails coordinatizing the space of matrices by their singular values and singular vectors, as opposed to by their entries, a potentially fruitful perspective for matrix learning. We demonstrate that the SNN architecture is inherently much more amenable to theoretical analysis than vanilla neural nets and confirm its effectiveness in the context of matrix sensing, via both mathematical guarantees and empirical investigations. We believe that the SNN architecture has the potential to be of wide applicability in a broad class of matrix learning scenarios.","sentences":["The phenomenon of implicit regularization has attracted interest in recent years as a fundamental aspect of the remarkable generalizing ability of neural networks.","In a nutshell, it entails that gradient descent dynamics in many neural nets, even without any explicit regularizer in the loss function, converges to the solution of a regularized learning problem.","However, known results attempting to theoretically explain this phenomenon focus overwhelmingly on the setting of linear neural nets, and the simplicity of the linear structure is particularly crucial to existing arguments.","In this paper, we explore this problem in the context of more realistic neural networks with a general class of non-linear activation functions, and rigorously demonstrate the implicit regularization phenomenon for such networks in the setting of matrix sensing problems, together with rigorous rate guarantees that ensure exponentially fast convergence of gradient descent.","In this vein, we contribute a network architecture called Spectral Neural Networks (abbrv.","SNN) that is particularly suitable for matrix learning problems.","Conceptually, this entails coordinatizing the space of matrices by their singular values and singular vectors, as opposed to by their entries, a potentially fruitful perspective for matrix learning.","We demonstrate that the SNN architecture is inherently much more amenable to theoretical analysis than vanilla neural nets and confirm its effectiveness in the context of matrix sensing, via both mathematical guarantees and empirical investigations.","We believe that the SNN architecture has the potential to be of wide applicability in a broad class of matrix learning scenarios."],"url":"http://arxiv.org/abs/2402.17595v1","category":"cs.LG"}
{"created":"2024-02-27 15:14:17","title":"Polymatroids are to finite groups as matroids are to finite fields","abstract":"Given a subgroup $\\mathcal{H}$ of a product of finite groups $\\mathcal{G} = \\displaystyle\\prod^n_{i=1} \\Gamma_i$ and $b>1,$ we define a polymatroid $P(\\mathcal{H},b).$ If all of the $\\Gamma_i$ are isomorphic to $\\mathbb{Z}/p\\mathbb{Z},$ $p$ a prime, and $b=p,$ then $P(\\mathcal{H},b)$ is the usual matroid associated to any $\\mathbb{Z}/p\\mathbb{Z}$-matrix whose row space equals $\\mathcal{H}.$ In general, there are many ways in which the relationship between $P(\\mathcal{H},b)$ and $\\mathcal{H}$ mirrors that of the relationship between a matroid and a subspace of a finite vector space. These include representability by excluded minors, the Crapo-Rota critical theorem, the existence of a concrete algebraic object representing the polymatroid dual of $P(\\mathcal{H},b),$ analogs of Greene's theorem and the MacWilliams identities when $\\mathcal{H}$ is a group code over a nonabelian group, and a connection to the combinatorial Laplacian of a quotient space determined by $\\mathcal{G}$ and $\\mathcal{H}.$ We use the group Crapo-Rota critical theorem to demonstrate an extension to hypergraphs of the classical duality between proper colorings and nowhere-zero flows on graphs.","sentences":["Given a subgroup $\\mathcal{H}$ of a product of finite groups $\\mathcal{G} = \\displaystyle\\prod^n_{i=1} \\Gamma_i$ and $b>1,$ we define a polymatroid $P(\\mathcal{H},b).$ If all of the $\\Gamma_i$ are isomorphic to $\\mathbb{Z}/p\\mathbb{Z},$ $p$ a prime, and $b=p,$ then $P(\\mathcal{H},b)$ is the usual matroid associated to any $\\mathbb{Z}/p\\mathbb{Z}$-matrix whose row space equals $\\mathcal{H}.$ In general, there are many ways in which the relationship between $P(\\mathcal{H},b)$ and $\\mathcal{H}$ mirrors that of the relationship between a matroid and a subspace of a finite vector space.","These include representability by excluded minors, the Crapo-Rota critical theorem, the existence of a concrete algebraic object representing the polymatroid dual of $P(\\mathcal{H},b),$ analogs of Greene's theorem and the MacWilliams identities when $\\mathcal{H}$ is a group code over a nonabelian group, and a connection to the combinatorial Laplacian of a quotient space determined by $\\mathcal{G}$ and $\\mathcal{H}.$ We use the group Crapo-Rota critical theorem to demonstrate an extension to hypergraphs of the classical duality between proper colorings and nowhere-zero flows on graphs."],"url":"http://arxiv.org/abs/2402.17582v1","category":"math.CO"}
{"created":"2024-02-27 15:13:34","title":"CosmoMIA: Cosmic Web-based redshift space halo distribution","abstract":"Modern galaxy surveys demand extensive survey volumes and resolutions surpassing current dark matter-only simulations' capabilities. To address this, many methods employ effective bias models on the dark matter field to approximate object counts on a grid. However, realistic catalogs necessitate specific coordinates and velocities for a comprehensive understanding of the Universe. In this research, we explore sub-grid modeling to create accurate catalogs, beginning with coarse grid number counts at resolutions of approximately $5.5,h^{-1}$ Mpc per side. These resolutions strike a balance between modeling nonlinear damping of baryon acoustic oscillations and facilitating large-volume simulations. Augmented Lagrangian Perturbation Theory (ALPT) is utilized to model the dark matter field and motions, replicating the clustering of a halo catalog derived from a massive simulation at $z=1.1$. Our approach involves four key stages:   Tracer Assignment: Allocating dark matter particles to tracers based on grid cell counts, generating additional particles to address discrepancies.   Attractor Identification: Defining attractors based on particle cosmic web environments, acting as gravitational focal points.   Tracer Collapse: Guiding tracers towards attractors, simulating structure collapse.   Redshift Space Distortions: Introducing redshift space distortions to simulated catalogs using ALPT and a random dispersion term.   Results demonstrate accurate reproduction of monopoles and quadrupoles up to wave numbers of approximately $k=0.6,h$ Mpc$^{-1}$. This method holds significant promise for galaxy surveys like DESI, EUCLID, and LSST, enhancing our understanding of the cosmos across scales.","sentences":["Modern galaxy surveys demand extensive survey volumes and resolutions surpassing current dark matter-only simulations' capabilities.","To address this, many methods employ effective bias models on the dark matter field to approximate object counts on a grid.","However, realistic catalogs necessitate specific coordinates and velocities for a comprehensive understanding of the Universe.","In this research, we explore sub-grid modeling to create accurate catalogs, beginning with coarse grid number counts at resolutions of approximately $5.5,h^{-1}$ Mpc per side.","These resolutions strike a balance between modeling nonlinear damping of baryon acoustic oscillations and facilitating large-volume simulations.","Augmented Lagrangian Perturbation Theory (ALPT) is utilized to model the dark matter field and motions, replicating the clustering of a halo catalog derived from a massive simulation at $z=1.1$. Our approach involves four key stages:   Tracer Assignment:","Allocating dark matter particles to tracers based on grid cell counts, generating additional particles to address discrepancies.   ","Attractor Identification: Defining attractors based on particle cosmic web environments, acting as gravitational focal points.   ","Tracer Collapse:","Guiding tracers towards attractors, simulating structure collapse.   ","Redshift Space Distortions:","Introducing redshift space distortions to simulated catalogs using ALPT and a random dispersion term.   ","Results demonstrate accurate reproduction of monopoles and quadrupoles up to wave numbers of approximately $k=0.6,h$ Mpc$^{-1}$.","This method holds significant promise for galaxy surveys like DESI, EUCLID, and LSST, enhancing our understanding of the cosmos across scales."],"url":"http://arxiv.org/abs/2402.17581v1","category":"astro-ph.CO"}
{"created":"2024-02-27 15:12:04","title":"Optimization of foreground moment deprojection for semi-blind CMB polarization reconstruction","abstract":"Upcoming Cosmic Microwave Background (CMB) experiments, aimed at measuring primordial CMB B-modes, require exquisite control of Galactic foreground contamination. Minimum-variance techniques, like the Needlet Internal Linear Combination (NILC), have proven effective in reconstructing the CMB polarization signal and mitigating foregrounds across diverse sky models without suffering from mismodelling errors. Still, residual contamination may bias the recovered CMB polarization at large angular scales when confronted with the most complex foreground scenarios. By adding constraints to NILC to deproject moments of the Galactic emission, the Constrained Moment ILC (cMILC) method has proven to enhance foreground subtraction, albeit with an associated increase in overall noise variance. Faced with this trade-off between foreground bias reduction and overall variance minimization, there is still no recipe on which moments to deproject and which are better suited for blind variance minimization. To address this, we introduce the optimized cMILC (ocMILC) pipeline, which performs full optimization of the required number and set of foreground moments to deproject, pivot parameter values, and deprojection coefficients across the sky and angular scales, depending on the actual sky complexity, available frequency coverage, and experiment sensitivity. The optimal number of deprojected moments, before paying significant noise penalty, is determined through a data diagnosis inspired by the Generalized NILC (GNILC) method. Validated on B-mode simulations of the PICO space mission concept with four challenging foreground models, ocMILC exhibits lower foreground contamination compared to NILC and cMILC at all angular scales, with limited noise penalty. This multi-layer optimization enables the ocMILC pipeline to achieve unbiased posteriors of the tensor-to-scalar ratio, regardless of foreground complexity.","sentences":["Upcoming Cosmic Microwave Background (CMB) experiments, aimed at measuring primordial CMB B-modes, require exquisite control of Galactic foreground contamination.","Minimum-variance techniques, like the Needlet Internal Linear Combination (NILC), have proven effective in reconstructing the CMB polarization signal and mitigating foregrounds across diverse sky models without suffering from mismodelling errors.","Still, residual contamination may bias the recovered CMB polarization at large angular scales when confronted with the most complex foreground scenarios.","By adding constraints to NILC to deproject moments of the Galactic emission, the Constrained Moment ILC (cMILC) method has proven to enhance foreground subtraction, albeit with an associated increase in overall noise variance.","Faced with this trade-off between foreground bias reduction and overall variance minimization, there is still no recipe on which moments to deproject and which are better suited for blind variance minimization.","To address this, we introduce the optimized cMILC (ocMILC) pipeline, which performs full optimization of the required number and set of foreground moments to deproject, pivot parameter values, and deprojection coefficients across the sky and angular scales, depending on the actual sky complexity, available frequency coverage, and experiment sensitivity.","The optimal number of deprojected moments, before paying significant noise penalty, is determined through a data diagnosis inspired by the Generalized NILC (GNILC) method.","Validated on B-mode simulations of the PICO space mission concept with four challenging foreground models, ocMILC exhibits lower foreground contamination compared to NILC and cMILC at all angular scales, with limited noise penalty.","This multi-layer optimization enables the ocMILC pipeline to achieve unbiased posteriors of the tensor-to-scalar ratio, regardless of foreground complexity."],"url":"http://arxiv.org/abs/2402.17579v1","category":"astro-ph.CO"}
{"created":"2024-02-27 15:11:36","title":"Mutual estimates of time-frequency representations and uncertainty principles","abstract":"In this paper we give different estimates between Lebesgue norms of quadratic time-frequency representations. We show that, in some cases, it is not possible to have such bounds in classical $L^p$ spaces, but the Lebesgue norm needs to be suitably weighted. This leads to consider weights of polynomial type, and, more generally, of ultradifferentiable type, and this, in turn, gives rise to use as functional setting the ultradifferentiable classes. As applications of such estimates we deduce uncertainty principles both of Donoho-Stark type and of local type for representations.","sentences":["In this paper we give different estimates between Lebesgue norms of quadratic time-frequency representations.","We show that, in some cases, it is not possible to have such bounds in classical $L^p$ spaces, but the Lebesgue norm needs to be suitably weighted.","This leads to consider weights of polynomial type, and, more generally, of ultradifferentiable type, and this, in turn, gives rise to use as functional setting the ultradifferentiable classes.","As applications of such estimates we deduce uncertainty principles both of Donoho-Stark type and of local type for representations."],"url":"http://arxiv.org/abs/2402.17578v1","category":"math.FA"}
{"created":"2024-02-27 15:09:20","title":"Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization","abstract":"Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.","sentences":["Large Language Models exhibit robust problem-solving capabilities for diverse tasks.","However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions.","These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games.","In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy.","Specifically, it involves a dynamic belief generation and reflection process for policy evolution.","Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy.","Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs.","Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models.","Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications."],"url":"http://arxiv.org/abs/2402.17574v1","category":"cs.AI"}
{"created":"2024-02-27 15:08:57","title":"Sparse Variational Contaminated Noise Gaussian Process Regression for Forecasting Geomagnetic Perturbations","abstract":"Gaussian Processes (GP) have become popular machine learning methods for kernel based learning on datasets with complicated covariance structures. In this paper, we present a novel extension to the GP framework using a contaminated normal likelihood function to better account for heteroscedastic variance and outlier noise. We propose a scalable inference algorithm based on the Sparse Variational Gaussian Process (SVGP) method for fitting sparse Gaussian process regression models with contaminated normal noise on large datasets. We examine an application to geomagnetic ground perturbations, where the state-of-art prediction model is based on neural networks. We show that our approach yields shorter predictions intervals for similar coverage and accuracy when compared to an artificial dense neural network baseline.","sentences":["Gaussian Processes (GP) have become popular machine learning methods for kernel based learning on datasets with complicated covariance structures.","In this paper, we present a novel extension to the GP framework using a contaminated normal likelihood function to better account for heteroscedastic variance and outlier noise.","We propose a scalable inference algorithm based on the Sparse Variational Gaussian Process (SVGP) method for fitting sparse Gaussian process regression models with contaminated normal noise on large datasets.","We examine an application to geomagnetic ground perturbations, where the state-of-art prediction model is based on neural networks.","We show that our approach yields shorter predictions intervals for similar coverage and accuracy when compared to an artificial dense neural network baseline."],"url":"http://arxiv.org/abs/2402.17570v1","category":"cs.LG"}
{"created":"2024-02-27 15:06:40","title":"Coherence generation with Hamiltonians","abstract":"We explore methods to generate quantum coherence through unitary evolutions, by introducing and studying the coherence generating capacity of Hamiltonians. This quantity is defined as the maximum derivative of coherence that can be achieved by a Hamiltonian. By adopting the relative entropy of coherence as our figure of merit, we evaluate the maximal coherence generating capacity with the constraint of a bounded Hilbert-Schmidt norm for the Hamiltonian. Our investigation yields closed-form expressions for both Hamiltonians and quantum states that induce the maximal derivative of coherence under these conditions. Specifically, for qubit systems, we solve this problem comprehensively for any given Hamiltonian, identifying the quantum states that lead to the largest coherence derivative induced by the Hamiltonian. Our investigation enables a precise identification of conditions under which quantum coherence is optimally enhanced, offering valuable insights for the manipulation and control of quantum coherence in quantum systems.","sentences":["We explore methods to generate quantum coherence through unitary evolutions, by introducing and studying the coherence generating capacity of Hamiltonians.","This quantity is defined as the maximum derivative of coherence that can be achieved by a Hamiltonian.","By adopting the relative entropy of coherence as our figure of merit, we evaluate the maximal coherence generating capacity with the constraint of a bounded Hilbert-Schmidt norm for the Hamiltonian.","Our investigation yields closed-form expressions for both Hamiltonians and quantum states that induce the maximal derivative of coherence under these conditions.","Specifically, for qubit systems, we solve this problem comprehensively for any given Hamiltonian, identifying the quantum states that lead to the largest coherence derivative induced by the Hamiltonian.","Our investigation enables a precise identification of conditions under which quantum coherence is optimally enhanced, offering valuable insights for the manipulation and control of quantum coherence in quantum systems."],"url":"http://arxiv.org/abs/2402.17567v1","category":"quant-ph"}
{"created":"2024-02-27 15:05:57","title":"Willmore-type variational problem for foliated hypersurfaces","abstract":"We study new Willmore-type variational problem for a hypersurface $M$ in $\\mathbb{R}^{n+1}$ equipped with an $s$-dimensional foliation ${\\cal F}$. Its general version is the Reilly-type functional $WF_{n,s}=\\int_M F(\\sigma^{\\cal F}_1,\\ldots,\\sigma^{\\cal F}_s)\\,{\\rm d}V$, where $\\sigma^{\\cal F}_i$ are elementary symmetric functions of the eigenvalues of the second fundamental form restricted on the leaves of $\\cal F$. The first and second variations of such functionals are calculated, conformal invariance of some of $WF_{n,s}$ is also shown. The Euler-Lagrange equation for a critical hypersurface with a transversally harmonic (e.g., Riemannian) foliation $\\cal F$ is found and examples with $s\\le2$ and $s=n$ are considered. Critical hypersurfaces of revolution are found, and it is shown that they are a local minimum for special variations.","sentences":["We study new Willmore-type variational problem for a hypersurface $M$ in $\\mathbb{R}^{n+1}$ equipped with an $s$-dimensional foliation ${\\cal F}$.","Its general version is the Reilly-type functional $WF_{n,s}=\\int_M F(\\sigma^{\\cal F}_1,\\ldots,\\sigma^{\\cal F}_s)\\,{\\rm d}V$, where $\\sigma^{\\cal F}_i$ are elementary symmetric functions of the eigenvalues of the second fundamental form restricted on the leaves of $\\cal F$. The first and second variations of such functionals are calculated, conformal invariance of some of $WF_{n,s}$ is also shown.","The Euler-Lagrange equation for a critical hypersurface with a transversally harmonic (e.g., Riemannian) foliation $\\cal F$ is found and examples with $s\\le2$ and $s=n$ are considered.","Critical hypersurfaces of revolution are found, and it is shown that they are a local minimum for special variations."],"url":"http://arxiv.org/abs/2402.17565v1","category":"math.DG"}
{"created":"2024-02-27 15:05:32","title":"Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers","abstract":"Automatic prompt optimization is an important approach to improving the performance of large language models (LLMs). Recent research demonstrates the potential of using LLMs as prompt optimizers, which can generate improved task prompts via iterative refinement. In this paper, we propose a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers. To connect these two approaches, we identify two pivotal factors in model parameter learning: update direction and update method. Focused on the two aspects, we borrow the theoretical framework and learning methods from gradient-based optimization to design improved strategies for LLM-based prompt optimizers. By systematically analyzing a rich set of improvement strategies, we further develop a capable Gradient-inspired LLM-based Prompt Optimizer called GPO. At each step, it first retrieves relevant prompts from the optimization trajectory as the update direction. Then, it utilizes the generation-based refinement strategy to perform the update, while controlling the edit distance through a cosine-based decay strategy. Extensive experiments demonstrate the effectiveness and efficiency of GPO. In particular, GPO brings an additional improvement of up to 56.8% on Big-Bench Hard and 55.3% on MMLU compared to baseline methods.","sentences":["Automatic prompt optimization is an important approach to improving the performance of large language models (LLMs).","Recent research demonstrates the potential of using LLMs as prompt optimizers, which can generate improved task prompts via iterative refinement.","In this paper, we propose a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers.","To connect these two approaches, we identify two pivotal factors in model parameter learning: update direction and update method.","Focused on the two aspects, we borrow the theoretical framework and learning methods from gradient-based optimization to design improved strategies for LLM-based prompt optimizers.","By systematically analyzing a rich set of improvement strategies, we further develop a capable Gradient-inspired LLM-based Prompt Optimizer called GPO.","At each step, it first retrieves relevant prompts from the optimization trajectory as the update direction.","Then, it utilizes the generation-based refinement strategy to perform the update, while controlling the edit distance through a cosine-based decay strategy.","Extensive experiments demonstrate the effectiveness and efficiency of GPO.","In particular, GPO brings an additional improvement of up to 56.8% on Big-Bench Hard and 55.3% on MMLU compared to baseline methods."],"url":"http://arxiv.org/abs/2402.17564v1","category":"cs.CL"}
{"created":"2024-02-27 15:05:13","title":"Structure-Guided Adversarial Training of Diffusion Models","abstract":"Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transformers (DiT) and outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512, respectively.","sentences":["Diffusion models have demonstrated exceptional efficacy in various generative applications.","While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples.","To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM).","In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch.","To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones.","SADM substantially improves existing diffusion transformers (DiT) and outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512, respectively."],"url":"http://arxiv.org/abs/2402.17563v1","category":"cs.CV"}
{"created":"2024-02-27 15:02:17","title":"An Empirical Study of the Generalization Ability of Lidar 3D Object Detectors to Unseen Domains","abstract":"3D Object Detectors (3D-OD) are crucial for understanding the environment in many robotic tasks, especially autonomous driving. Including 3D information via Lidar sensors improves accuracy greatly. However, such detectors perform poorly on domains they were not trained on, i.e. different locations, sensors, weather, etc., limiting their reliability in safety-critical applications. There exist methods to adapt 3D-ODs to these domains; however, these methods treat 3D-ODs as a black box, neglecting underlying architectural decisions and source-domain training strategies. Instead, we dive deep into the details of 3D-ODs, focusing our efforts on fundamental factors that influence robustness prior to domain adaptation.   We systematically investigate four design choices (and the interplay between them) often overlooked in 3D-OD robustness and domain adaptation: architecture, voxel encoding, data augmentations, and anchor strategies. We assess their impact on the robustness of nine state-of-the-art 3D-ODs across six benchmarks encompassing three types of domain gaps - sensor type, weather, and location.   Our main findings are: (1) transformer backbones with local point features are more robust than 3D CNNs, (2) test-time anchor size adjustment is crucial for adaptation across geographical locations, significantly boosting scores without retraining, (3) source-domain augmentations allow the model to generalize to low-resolution sensors, and (4) surprisingly, robustness to bad weather is improved when training directly on more clean weather data than on training with bad weather data. We outline our main conclusions and findings to provide practical guidance on developing more robust 3D-ODs.","sentences":["3D Object Detectors (3D-OD) are crucial for understanding the environment in many robotic tasks, especially autonomous driving.","Including 3D information via Lidar sensors improves accuracy greatly.","However, such detectors perform poorly on domains they were not trained on, i.e. different locations, sensors, weather, etc., limiting their reliability in safety-critical applications.","There exist methods to adapt 3D-ODs to these domains; however, these methods treat 3D-ODs as a black box, neglecting underlying architectural decisions and source-domain training strategies.","Instead, we dive deep into the details of 3D-ODs, focusing our efforts on fundamental factors that influence robustness prior to domain adaptation.   ","We systematically investigate four design choices (and the interplay between them) often overlooked in 3D-OD robustness and domain adaptation: architecture, voxel encoding, data augmentations, and anchor strategies.","We assess their impact on the robustness of nine state-of-the-art 3D-ODs across six benchmarks encompassing three types of domain gaps - sensor type, weather, and location.   ","Our main findings are: (1) transformer backbones with local point features are more robust than 3D CNNs, (2) test-time anchor size adjustment is crucial for adaptation across geographical locations, significantly boosting scores without retraining, (3) source-domain augmentations allow the model to generalize to low-resolution sensors, and (4) surprisingly, robustness to bad weather is improved when training directly on more clean weather data than on training with bad weather data.","We outline our main conclusions and findings to provide practical guidance on developing more robust 3D-ODs."],"url":"http://arxiv.org/abs/2402.17562v1","category":"cs.CV"}
{"created":"2024-02-27 14:59:48","title":"PHNet: Patch-based Normalization for Portrait Harmonization","abstract":"A common problem for composite images is the incompatibility of their foreground and background components. Image harmonization aims to solve this problem, making the whole image look more authentic and coherent. Most existing solutions predict lookup tables (LUTs) or reconstruct images, utilizing various attributes of composite images. Recent approaches have primarily focused on employing global transformations like normalization and color curve rendering to achieve visual consistency, and they often overlook the importance of local visual coherence. We present a patch-based harmonization network consisting of novel Patch-based normalization (PN) blocks and a feature extractor based on statistical color transfer. Extensive experiments demonstrate the network's high generalization capability for different domains. Our network achieves state-of-the-art results on the iHarmony4 dataset. Also, we created a new human portrait harmonization dataset based on FFHQ and checked the proposed method to show the generalization ability by achieving the best metrics on it. The benchmark experiments confirm that the suggested patch-based normalization block and feature extractor effectively improve the network's capability to harmonize portraits. Our code and model baselines are publicly available.","sentences":["A common problem for composite images is the incompatibility of their foreground and background components.","Image harmonization aims to solve this problem, making the whole image look more authentic and coherent.","Most existing solutions predict lookup tables (LUTs) or reconstruct images, utilizing various attributes of composite images.","Recent approaches have primarily focused on employing global transformations like normalization and color curve rendering to achieve visual consistency, and they often overlook the importance of local visual coherence.","We present a patch-based harmonization network consisting of novel Patch-based normalization (PN) blocks and a feature extractor based on statistical color transfer.","Extensive experiments demonstrate the network's high generalization capability for different domains.","Our network achieves state-of-the-art results on the iHarmony4 dataset.","Also, we created a new human portrait harmonization dataset based on FFHQ and checked the proposed method to show the generalization ability by achieving the best metrics on it.","The benchmark experiments confirm that the suggested patch-based normalization block and feature extractor effectively improve the network's capability to harmonize portraits.","Our code and model baselines are publicly available."],"url":"http://arxiv.org/abs/2402.17561v1","category":"cs.CV"}
{"created":"2024-02-27 14:52:56","title":"T-odd gluon distribution functions in a spectator model","abstract":"We present a model calculation of T-odd transverse-momentum-dependent distributions of gluons in the nucleon. The model is based on the assumption that a nucleon can emit a gluon, and what remains after the emission is treated as a single spectator particle. This spectator particle is considered to be on-shell, but its mass is allowed to take a continuous range of values, described by a spectral function. The final-state interaction that is necessary to generate T-odd functions is modeled as the exchange of a single gluon between the spectator and the outgoing parton.","sentences":["We present a model calculation of T-odd transverse-momentum-dependent distributions of gluons in the nucleon.","The model is based on the assumption that a nucleon can emit a gluon, and what remains after the emission is treated as a single spectator particle.","This spectator particle is considered to be on-shell, but its mass is allowed to take a continuous range of values, described by a spectral function.","The final-state interaction that is necessary to generate T-odd functions is modeled as the exchange of a single gluon between the spectator and the outgoing parton."],"url":"http://arxiv.org/abs/2402.17556v1","category":"hep-ph"}
{"created":"2024-02-27 14:51:56","title":"Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label","abstract":"Scribble-based weakly-supervised semantic segmentation using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives. Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision. However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation. In this study, we propose a class-driven scribble promotion network, which utilizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision. Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space. To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction, which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label's boundary. Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness of our method.The code is available at https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network.","sentences":["Scribble-based weakly-supervised semantic segmentation using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives.","Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision.","However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation.","In this study, we propose a class-driven scribble promotion network, which utilizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision.","Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space.","To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction, which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label's boundary.","Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness of our method.","The code is available at https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network."],"url":"http://arxiv.org/abs/2402.17555v1","category":"cs.CV"}
{"created":"2024-02-27 14:48:07","title":"Evaluation of Predictive Reliability to Foster Trust in Artificial Intelligence. A case study in Multiple Sclerosis","abstract":"Applying Artificial Intelligence (AI) and Machine Learning (ML) in critical contexts, such as medicine, requires the implementation of safety measures to reduce risks of harm in case of prediction errors. Spotting ML failures is of paramount importance when ML predictions are used to drive clinical decisions. ML predictive reliability measures the degree of trust of a ML prediction on a new instance, thus allowing decision-makers to accept or reject it based on its reliability. To assess reliability, we propose a method that implements two principles. First, our approach evaluates whether an instance to be classified is coming from the same distribution of the training set. To do this, we leverage Autoencoders (AEs) ability to reconstruct the training set with low error. An instance is considered Out-of-Distribution (OOD) if the AE reconstructs it with a high error. Second, it is evaluated whether the ML classifier has good performances on samples similar to the newly classified instance by using a proxy model. We show that this approach is able to assess reliability both in a simulated scenario and on a model trained to predict disease progression of Multiple Sclerosis patients. We also developed a Python package, named relAI, to embed reliability measures into ML pipelines. We propose a simple approach that can be used in the deployment phase of any ML model to suggest whether to trust predictions or not. Our method holds the promise to provide effective support to clinicians by spotting potential ML failures during deployment.","sentences":["Applying Artificial Intelligence (AI) and Machine Learning (ML) in critical contexts, such as medicine, requires the implementation of safety measures to reduce risks of harm in case of prediction errors.","Spotting ML failures is of paramount importance when ML predictions are used to drive clinical decisions.","ML predictive reliability measures the degree of trust of a ML prediction on a new instance, thus allowing decision-makers to accept or reject it based on its reliability.","To assess reliability, we propose a method that implements two principles.","First, our approach evaluates whether an instance to be classified is coming from the same distribution of the training set.","To do this, we leverage Autoencoders (AEs) ability to reconstruct the training set with low error.","An instance is considered Out-of-Distribution (OOD) if the AE reconstructs it with a high error.","Second, it is evaluated whether the ML classifier has good performances on samples similar to the newly classified instance by using a proxy model.","We show that this approach is able to assess reliability both in a simulated scenario and on a model trained to predict disease progression of Multiple Sclerosis patients.","We also developed a Python package, named relAI, to embed reliability measures into ML pipelines.","We propose a simple approach that can be used in the deployment phase of any ML model to suggest whether to trust predictions or not.","Our method holds the promise to provide effective support to clinicians by spotting potential ML failures during deployment."],"url":"http://arxiv.org/abs/2402.17554v1","category":"cs.LG"}
{"created":"2024-02-27 14:47:53","title":"OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web","abstract":"For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as \"Play the next song\", as well as longer horizon tasks such as \"Send an email to John Doe mentioning the time and place to meet\". Specifically, given a pair of screen image and a visually-grounded natural language task, the goal is to generate a script capable of fully executing the task. We run several strong baseline language model agents on our benchmark. The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents. Our benchmark provides a platform to measure and evaluate the progress of language model agents in automating computer tasks and motivates future work towards building multimodal models that bridge large language models and the visual grounding of computer screens.","sentences":["For decades, human-computer interaction has fundamentally been manual.","Even today, almost all productive work done on the computer necessitates human input at every step.","Autonomous virtual agents represent an exciting step in automating many of these menial tasks.","Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems.","They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention.","In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks.","Our scope extends beyond traditional web automation, covering a diverse range of desktop applications.","The dataset consists of fundamental tasks such as \"Play the next song\", as well as longer horizon tasks such as \"Send an email to John Doe mentioning the time and place to meet\".","Specifically, given a pair of screen image and a visually-grounded natural language task, the goal is to generate a script capable of fully executing the task.","We run several strong baseline language model agents on our benchmark.","The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents.","Our benchmark provides a platform to measure and evaluate the progress of language model agents in automating computer tasks and motivates future work towards building multimodal models that bridge large language models and the visual grounding of computer screens."],"url":"http://arxiv.org/abs/2402.17553v1","category":"cs.AI"}
{"created":"2024-02-27 14:44:11","title":"Emergency Caching: Coded Caching-based Reliable Map Transmission in Emergency Networks","abstract":"Many rescue missions demand effective perception and real-time decision making, which highly rely on effective data collection and processing. In this study, we propose a three-layer architecture of emergency caching networks focusing on data collection and reliable transmission, by leveraging efficient perception and edge caching technologies. Based on this architecture, we propose a disaster map collection framework that integrates coded caching technologies. Our framework strategically caches coded fragments of maps across unmanned aerial vehicles (UAVs), fostering collaborative uploading for augmented transmission reliability. Additionally, we establish a comprehensive probability model to assess the effective recovery area of disaster maps. Towards the goal of utility maximization, we propose a deep reinforcement learning (DRL) based algorithm that jointly makes decisions about cooperative UAVs selection, bandwidth allocation and coded caching parameter adjustment, accommodating the real-time map updates in a dynamic disaster situation. Our proposed scheme is more effective than the non-coding caching scheme, as validated by simulation.","sentences":["Many rescue missions demand effective perception and real-time decision making, which highly rely on effective data collection and processing.","In this study, we propose a three-layer architecture of emergency caching networks focusing on data collection and reliable transmission, by leveraging efficient perception and edge caching technologies.","Based on this architecture, we propose a disaster map collection framework that integrates coded caching technologies.","Our framework strategically caches coded fragments of maps across unmanned aerial vehicles (UAVs), fostering collaborative uploading for augmented transmission reliability.","Additionally, we establish a comprehensive probability model to assess the effective recovery area of disaster maps.","Towards the goal of utility maximization, we propose a deep reinforcement learning (DRL) based algorithm that jointly makes decisions about cooperative UAVs selection, bandwidth allocation and coded caching parameter adjustment, accommodating the real-time map updates in a dynamic disaster situation.","Our proposed scheme is more effective than the non-coding caching scheme, as validated by simulation."],"url":"http://arxiv.org/abs/2402.17550v1","category":"cs.NI"}
{"created":"2024-02-27 14:40:36","title":"The Maker-Breaker percolation game on a random board","abstract":"The $(m,b)$ Maker-Breaker percolation game on $(\\mathbb{Z}^2)_p$, introduced by Day and Falgas-Ravry, is played in the following way. Before the game starts, each edge of $\\mathbb{Z}^2$ is removed independently with probability $1-p$. After that, Maker chooses a vertex $v_0$ to protect. Then, in each round Maker and Breaker claim respectively $m$ and $b$ unclaimed edges of $G$. Breaker wins if after the removal of the edges claimed by him the component of $v_0$ becomes finite, and Maker wins if she can indefinitely prevent Breaker from winning.   We show that for any $p < 1$, Breaker almost surely has a wining strategy for the $(1,1)$ game on $(\\mathbb{Z}^2)_p$. This fully answers a question of Day and Falgas-Ravry, who showed that for $p = 1$ Maker has a winning strategy for the $(1,1)$ game. Further, we show that in the $(2,1)$ game on $(\\mathbb{Z}^2)_p$ Maker almost surely has a winning strategy whenever $p > 0.9402$, while Breaker almost surely has a winning strategy whenever $p < 0.5278$. This shows that the threshold value of $p$ above which Maker has a winning strategy for the $(2,1)$ game on $\\mathbb{Z}^2$ is non-trivial. In fact, we prove similar results in various settings, including other lattices and biases $(m,b)$.   These results extend also to the most general case, which we introduce, where each edge is given to Maker with probability $\\alpha$ and to Breaker with probability $\\beta$ before the game starts.","sentences":["The $(m,b)$ Maker-Breaker percolation game on $(\\mathbb{Z}^2)_p$, introduced by Day and Falgas-Ravry, is played in the following way.","Before the game starts, each edge of $\\mathbb{Z}^2$ is removed independently with probability $1-p$. After that, Maker chooses a vertex $v_0$ to protect.","Then, in each round Maker and Breaker claim respectively $m$ and $b$ unclaimed edges of $G$. Breaker wins if after the removal of the edges claimed by him the component of $v_0$ becomes finite, and Maker wins if she can indefinitely prevent Breaker from winning.   ","We show that for any $p < 1$, Breaker almost surely has a wining strategy for the $(1,1)$ game on $(\\mathbb{Z}^2)_p$. This fully answers a question of Day and Falgas-Ravry, who showed that for $p = 1$ Maker has a winning strategy for the $(1,1)$ game.","Further, we show that in the $(2,1)$ game on $(\\mathbb{Z}^2)_p$ Maker almost surely has a winning strategy whenever $p > 0.9402$, while Breaker almost surely has a winning strategy whenever $p < 0.5278$.","This shows that the threshold value of $p$ above which Maker has a winning strategy for the $(2,1)$ game on $\\mathbb{Z}^2$ is non-trivial.","In fact, we prove similar results in various settings, including other lattices and biases $(m,b)$.   These results extend also to the most general case, which we introduce, where each edge is given to Maker with probability $\\alpha$ and to Breaker with probability $\\beta$ before the game starts."],"url":"http://arxiv.org/abs/2402.17547v1","category":"math.PR"}
{"created":"2024-02-27 14:38:47","title":"COCOA: CBT-based Conversational Counseling Agent using Memory Specialized in Cognitive Distortions and Dynamic Prompt","abstract":"The demand for conversational agents that provide mental health care is consistently increasing. In this work, we develop a psychological counseling agent, referred to as CoCoA, that applies Cognitive Behavioral Therapy (CBT) techniques to identify and address cognitive distortions inherent in the client's statements. Specifically, we construct a memory system to efficiently manage information necessary for counseling while extracting high-level insights about the client from their utterances. Additionally, to ensure that the counseling agent generates appropriate responses, we introduce dynamic prompting to flexibly apply CBT techniques and facilitate the appropriate retrieval of information. We conducted dialogues between CoCoA and characters from Character.ai, creating a dataset for evaluation. Then, we asked GPT to evaluate the constructed counseling dataset, and our model demonstrated a statistically significant difference from other models.","sentences":["The demand for conversational agents that provide mental health care is consistently increasing.","In this work, we develop a psychological counseling agent, referred to as CoCoA, that applies Cognitive Behavioral Therapy (CBT) techniques to identify and address cognitive distortions inherent in the client's statements.","Specifically, we construct a memory system to efficiently manage information necessary for counseling while extracting high-level insights about the client from their utterances.","Additionally, to ensure that the counseling agent generates appropriate responses, we introduce dynamic prompting to flexibly apply CBT techniques and facilitate the appropriate retrieval of information.","We conducted dialogues between CoCoA and characters from Character.ai, creating a dataset for evaluation.","Then, we asked GPT to evaluate the constructed counseling dataset, and our model demonstrated a statistically significant difference from other models."],"url":"http://arxiv.org/abs/2402.17546v1","category":"cs.AI"}
{"created":"2024-02-27 14:36:49","title":"Accuracy of the Gross-Pitaevskii Equation in a Double-Well Potential","abstract":"The Gross-Pitaevskii equation (GPE) in a double well potential produces solutions that break the symmetry of the underlying non-interacting Hamiltonian, i.e., asymmetric solutions. The GPE is derived from the more general second-quantized Fock Schroedinger equation (FSE). We investigate whether such solutions appear in the more general case or are artifacts of the GPE. We use two-mode analyses for a variational treatment of the GPE and to treat the Fock equation. An exact diagonalization of the FSE in dual-condensates yields degenerate ground states that are very accurately fitted by phase-state representations of the degenerate asymmetric states found in the GPE. The superposition of degenerate asymmetrical states forms a cat state. An alternative form of cat state results from a change of the two-mode basis set.","sentences":["The Gross-Pitaevskii equation (GPE) in a double well potential produces solutions that break the symmetry of the underlying non-interacting Hamiltonian, i.e., asymmetric solutions.","The GPE is derived from the more general second-quantized Fock Schroedinger equation (FSE).","We investigate whether such solutions appear in the more general case or are artifacts of the GPE.","We use two-mode analyses for a variational treatment of the GPE and to treat the Fock equation.","An exact diagonalization of the FSE in dual-condensates yields degenerate ground states that are very accurately fitted by phase-state representations of the degenerate asymmetric states found in the GPE.","The superposition of degenerate asymmetrical states forms a cat state.","An alternative form of cat state results from a change of the two-mode basis set."],"url":"http://arxiv.org/abs/2402.17545v1","category":"quant-ph"}
{"created":"2024-02-27 14:25:20","title":"The optimizing mode classification stabilization of sampled stochastic jump systems via an improved hill-climbing algorithm based on Q-learning","abstract":"This paper addresses the stabilization problem of stochastic jump systems (SJSs) closed by a generally sampled controller. Because of the controller's switching and state both sampled, it is challenging to study its stabilization. A new stabilizing method deeply depending on the mode classifications is proposed to deal with the above sampling situation, whose quantity is equal to a Stirling number of the second kind. For the sake of finding the best stabilization effect among all the classifications, a convex optimization problem is developed, whose globally solution is proved to be existent and can be computed by an augmented Lagrangian function. More importantly, in order to further reduce the computation complexity but retaining a better performance as much as possible, a novelly improved hill-climbing algorithm is established by applying the Q-learning technique to provide an optimal attenuation coefficient. A numerical example is offered so as to verify the effectiveness and superiority of the methods proposed in this study.","sentences":["This paper addresses the stabilization problem of stochastic jump systems (SJSs) closed by a generally sampled controller.","Because of the controller's switching and state both sampled, it is challenging to study its stabilization.","A new stabilizing method deeply depending on the mode classifications is proposed to deal with the above sampling situation, whose quantity is equal to a Stirling number of the second kind.","For the sake of finding the best stabilization effect among all the classifications, a convex optimization problem is developed, whose globally solution is proved to be existent and can be computed by an augmented Lagrangian function.","More importantly, in order to further reduce the computation complexity but retaining a better performance as much as possible, a novelly improved hill-climbing algorithm is established by applying the Q-learning technique to provide an optimal attenuation coefficient.","A numerical example is offered so as to verify the effectiveness and superiority of the methods proposed in this study."],"url":"http://arxiv.org/abs/2402.17539v1","category":"math.OC"}
{"created":"2024-02-27 14:22:41","title":"Electrically driven cascaded photon-emission in a single molecule","abstract":"Controlling electrically-stimulated quantum light sources (QLS) is key for developing integrated and low-scale quantum devices. The mechanisms leading to quantum emission are complex, as a large number of electronic states of the system impacts the emission dynamics. Here, we use a scanning tunneling microscope (STM) to excite a model QLS, namely a single molecule. The luminescence spectra reveal two lines, associated to the emission of the neutral and positively charged molecule, both exhibiting single-photon source behavior. In addition, we find a correlation between the charged and neutral molecule's emission, the signature of a photon cascade. By adjusting the charging/discharging rate, we can control these emission statistics. This generic strategy is further established by a rate equation model revealing the complex internal dynamics of the molecular junction.","sentences":["Controlling electrically-stimulated quantum light sources (QLS) is key for developing integrated and low-scale quantum devices.","The mechanisms leading to quantum emission are complex, as a large number of electronic states of the system impacts the emission dynamics.","Here, we use a scanning tunneling microscope (STM) to excite a model QLS, namely a single molecule.","The luminescence spectra reveal two lines, associated to the emission of the neutral and positively charged molecule, both exhibiting single-photon source behavior.","In addition, we find a correlation between the charged and neutral molecule's emission, the signature of a photon cascade.","By adjusting the charging/discharging rate, we can control these emission statistics.","This generic strategy is further established by a rate equation model revealing the complex internal dynamics of the molecular junction."],"url":"http://arxiv.org/abs/2402.17536v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-27 14:21:32","title":"The ghost-gluon vertex in the presence of the Gribov horizon: general kinematics","abstract":"Correlation functions are important probes for the behavior of quantum field theories. Already at tree-level, the Refined Gribov Zwanziger (RGZ) effective action for Yang-Mills theories provides a good approximation for the gluon propagator, as compared to that calculated by nonperturbative methods such as Lattice Field Theory and Dyson-Schwinger Equations. However, the study of higher correlation functions of the RGZ theory is still at its beginning. In this work we evaluate the ghost-antighost-gluon vertex function in Landau gauge at one-loop level, in $d=4$ space-time dimensions for the gauge groups SU(2) and SU(3). More precisely, we extend the analysis conducted in [1] for the soft-gluon limit to an arbitrary kinematic configuration. We introduce renormalization group effects by means of a toy model for the running coupling and investigate the impact of such a model in the ultraviolet tails of our results. We find that RGZ results match fairly closely those from lattice simulations, Schwinger-Dyson equations and the Curci-Ferrari model for three different kinematic configurations. This is compatible with RGZ being a feasible theory for the strong interaction in the infrared regime.","sentences":["Correlation functions are important probes for the behavior of quantum field theories.","Already at tree-level, the Refined Gribov Zwanziger (RGZ) effective action for Yang-Mills theories provides a good approximation for the gluon propagator, as compared to that calculated by nonperturbative methods such as Lattice Field Theory and Dyson-Schwinger Equations.","However, the study of higher correlation functions of the RGZ theory is still at its beginning.","In this work we evaluate the ghost-antighost-gluon vertex function in Landau gauge at one-loop level, in $d=4$ space-time dimensions for the gauge groups SU(2) and SU(3).","More precisely, we extend the analysis conducted in [1] for the soft-gluon limit to an arbitrary kinematic configuration.","We introduce renormalization group effects by means of a toy model for the running coupling and investigate the impact of such a model in the ultraviolet tails of our results.","We find that RGZ results match fairly closely those from lattice simulations, Schwinger-Dyson equations and the Curci-Ferrari model for three different kinematic configurations.","This is compatible with RGZ being a feasible theory for the strong interaction in the infrared regime."],"url":"http://arxiv.org/abs/2402.17534v1","category":"hep-ph"}
{"created":"2024-02-27 14:16:39","title":"Black-box Adversarial Attacks Against Image Quality Assessment Models","abstract":"The goal of No-Reference Image Quality Assessment (NR-IQA) is to predict the perceptual quality of an image in line with its subjective evaluation. To put the NR-IQA models into practice, it is essential to study their potential loopholes for model refinement. This paper makes the first attempt to explore the black-box adversarial attacks on NR-IQA models. Specifically, we first formulate the attack problem as maximizing the deviation between the estimated quality scores of original and perturbed images, while restricting the perturbed image distortions for visual quality preservation. Under such formulation, we then design a Bi-directional loss function to mislead the estimated quality scores of adversarial examples towards an opposite direction with maximum deviation. On this basis, we finally develop an efficient and effective black-box attack method against NR-IQA models. Extensive experiments reveal that all the evaluated NR-IQA models are vulnerable to the proposed attack method. And the generated perturbations are not transferable, enabling them to serve the investigation of specialities of disparate IQA models.","sentences":["The goal of No-Reference Image Quality Assessment (NR-IQA) is to predict the perceptual quality of an image in line with its subjective evaluation.","To put the NR-IQA models into practice, it is essential to study their potential loopholes for model refinement.","This paper makes the first attempt to explore the black-box adversarial attacks on NR-IQA models.","Specifically, we first formulate the attack problem as maximizing the deviation between the estimated quality scores of original and perturbed images, while restricting the perturbed image distortions for visual quality preservation.","Under such formulation, we then design a Bi-directional loss function to mislead the estimated quality scores of adversarial examples towards an opposite direction with maximum deviation.","On this basis, we finally develop an efficient and effective black-box attack method against NR-IQA models.","Extensive experiments reveal that all the evaluated NR-IQA models are vulnerable to the proposed attack method.","And the generated perturbations are not transferable, enabling them to serve the investigation of specialities of disparate IQA models."],"url":"http://arxiv.org/abs/2402.17533v1","category":"cs.CV"}
{"created":"2024-02-27 14:16:19","title":"Retrieval is Accurate Generation","abstract":"Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from 42.61% to 81.58% in open-ended text generation. Remarkably, our model also achieves the best performance and the lowest latency among several retrieval-augmented baselines. In conclusion, we assert that retrieval is more accurate generation and hope that our work will encourage further research on this new paradigm shift.","sentences":["Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary.","We introduce a novel method that selects context-aware phrases from a collection of supporting documents.","One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents.","To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement.","Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation.","For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from 42.61% to 81.58% in open-ended text generation.","Remarkably, our model also achieves the best performance and the lowest latency among several retrieval-augmented baselines.","In conclusion, we assert that retrieval is more accurate generation and hope that our work will encourage further research on this new paradigm shift."],"url":"http://arxiv.org/abs/2402.17532v1","category":"cs.CL"}
{"created":"2024-02-27 14:14:23","title":"Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides","abstract":"Effective incident management is pivotal for the smooth operation of enterprises-level cloud services. In order to expedite incident mitigation, service teams compile troubleshooting knowledge into Troubleshooting Guides (TSGs) accessible to on-call engineers (OCEs). While automated pipelines are enabled to resolve the most frequent and easy incidents, there still exist complex incidents that require OCEs' intervention. However, TSGs are often unstructured and incomplete, which requires manual interpretation by OCEs, leading to on-call fatigue and decreased productivity, especially among new-hire OCEs. In this work, we propose Nissist which leverages TSGs and incident mitigation histories to provide proactive suggestions, reducing human intervention. Leveraging Large Language Models (LLM), Nissist extracts insights from unstructured TSGs and historical incident mitigation discussions, forming a comprehensive knowledge base. Its multi-agent system design enhances proficiency in precisely discerning user queries, retrieving relevant information, and delivering systematic plans consecutively. Through our user case and experiment, we demonstrate that Nissist significant reduce Time to Mitigate (TTM) in incident mitigation, alleviating operational burdens on OCEs and improving service reliability. Our demo is available at https://aka.ms/nissist_demo.","sentences":["Effective incident management is pivotal for the smooth operation of enterprises-level cloud services.","In order to expedite incident mitigation, service teams compile troubleshooting knowledge into Troubleshooting Guides (TSGs) accessible to on-call engineers (OCEs).","While automated pipelines are enabled to resolve the most frequent and easy incidents, there still exist complex incidents that require OCEs' intervention.","However, TSGs are often unstructured and incomplete, which requires manual interpretation by OCEs, leading to on-call fatigue and decreased productivity, especially among new-hire OCEs.","In this work, we propose Nissist which leverages TSGs and incident mitigation histories to provide proactive suggestions, reducing human intervention.","Leveraging Large Language Models (LLM), Nissist extracts insights from unstructured TSGs and historical incident mitigation discussions, forming a comprehensive knowledge base.","Its multi-agent system design enhances proficiency in precisely discerning user queries, retrieving relevant information, and delivering systematic plans consecutively.","Through our user case and experiment, we demonstrate that Nissist significant reduce Time to Mitigate (TTM) in incident mitigation, alleviating operational burdens on OCEs and improving service reliability.","Our demo is available at https://aka.ms/nissist_demo."],"url":"http://arxiv.org/abs/2402.17531v1","category":"cs.SE"}
{"created":"2024-02-27 14:14:06","title":"The reverse Burnett conjecture for null dusts","abstract":"Given a generic solution $\\mathbf{g}_0$ of the Einstein-null dusts system without restriction on the number of dusts, we construct families of solutions $(\\mathbf{g}_\\lambda)_{\\lambda\\in(0,1]}$ of the Einstein vacuum equations such that $\\mathbf{g}_\\lambda-\\mathbf{g}_0$ and $\\partial(\\mathbf{g}_\\lambda-\\mathbf{g}_0)$ converges respectively strongly and weakly to 0 when $\\lambda\\to0$. Our construction, based on a multiphase geometric optics ansatz, thus extends the validity of the reverse Burnett conjecture without symmetry to a large class of massless kinetic spacetimes. In order to deal with the finite but arbitrary number of direction of oscillations we work in a generalised wave gauge and control precisely the self-interaction of each wave but also the interaction of waves propagating in different null directions, relying crucially on the non-linear structure of the Einstein vacuum equations. We also provide the construction of oscillating initial data solving the vacuum constraint equations and which are consistent with the spacetime ansatz.","sentences":["Given a generic solution $\\mathbf{g}_0$ of the Einstein-null dusts system without restriction on the number of dusts, we construct families of solutions $(\\mathbf{g}_\\lambda)_{\\lambda\\in(0,1]}$ of the Einstein vacuum equations such that $\\mathbf{g}_\\lambda-\\mathbf{g}_0$ and $\\partial(\\mathbf{g}_\\lambda-\\mathbf{g}_0)$ converges respectively strongly and weakly to 0 when $\\lambda\\to0$. Our construction, based on a multiphase geometric optics ansatz, thus extends the validity of the reverse Burnett conjecture without symmetry to a large class of massless kinetic spacetimes.","In order to deal with the finite but arbitrary number of direction of oscillations we work in a generalised wave gauge and control precisely the self-interaction of each wave but also the interaction of waves propagating in different null directions, relying crucially on the non-linear structure of the Einstein vacuum equations.","We also provide the construction of oscillating initial data solving the vacuum constraint equations and which are consistent with the spacetime ansatz."],"url":"http://arxiv.org/abs/2402.17530v1","category":"math.AP"}
{"created":"2024-02-27 14:13:41","title":"Evaluation of block encoding for sparse matrix inversion using QSVT","abstract":"Three block encoding methods are evaluated for solving linear systems of equations using QSVT (Quantum Singular Value Transformation). These are ARCSIN, FABLE and PREPARE-SELECT. The performance of the encoders is evaluated using a suite of 30 test cases including 1D, 2D and 3D Laplacians and 2D CFD matrices. A subset of cases is used to characterise how the degree of the polynomial approximation to $1/x$ influences the performance of QSVT. The results are used to guide the evaluation of QSVT as the linear solver in hybrid non-linear pressure correction and coupled implicit CFD solvers. The performance of QSVT is shown to be resilient to polynomial approximation errors. For both CFD solvers, error tolerances of $10^{-2}$ are more than sufficient in most cases and in some cases $10^{-1}$ is sufficient. The pressure correction solver allows subnormalised condition numbers, $\\kappa_s$, as low as half the theoretical values to be used, reducing the number of phase factors needed. PREPARE-SELECT encoding relies on a unitary decomposition, e.g. Pauli strings, that has significant classical preprocessing costs. Both ARCSIN and FABLE have much lower costs, particularly for coupled solvers. However, their subnormalisation factors, which are based on the rank of the matrix, can be many times higher than PREPARE-SELECT leading to more phase factors being needed. For both the pressure correction and coupled CFD calculations, QSVT is more stable than previous HHL results due to the polynomial approximation errors only affecting long wavelength CFD errors. Given that lowering $\\kappa_s$ increases the success probability, optimising the performance of QSVT within a CFD code is a function of the number QSVT phase factors, the number of non-linear iterations and the number of shots. Although phase factor files can be reused, the time taken to generate them impedes scaling QSVT to larger test cases.","sentences":["Three block encoding methods are evaluated for solving linear systems of equations using QSVT (Quantum Singular Value Transformation).","These are ARCSIN, FABLE and PREPARE-SELECT.","The performance of the encoders is evaluated using a suite of 30 test cases including 1D, 2D and 3D Laplacians and 2D CFD matrices.","A subset of cases is used to characterise how the degree of the polynomial approximation to $1/x$ influences the performance of QSVT.","The results are used to guide the evaluation of QSVT as the linear solver in hybrid non-linear pressure correction and coupled implicit CFD solvers.","The performance of QSVT is shown to be resilient to polynomial approximation errors.","For both CFD solvers, error tolerances of $10^{-2}$ are more than sufficient in most cases and in some cases $10^{-1}$ is sufficient.","The pressure correction solver allows subnormalised condition numbers, $\\kappa_s$, as low as half the theoretical values to be used, reducing the number of phase factors needed.","PREPARE-SELECT encoding relies on a unitary decomposition, e.g. Pauli strings, that has significant classical preprocessing costs.","Both ARCSIN and FABLE have much lower costs, particularly for coupled solvers.","However, their subnormalisation factors, which are based on the rank of the matrix, can be many times higher than PREPARE-SELECT leading to more phase factors being needed.","For both the pressure correction and coupled CFD calculations, QSVT is more stable than previous HHL results due to the polynomial approximation errors only affecting long wavelength CFD errors.","Given that lowering $\\kappa_s$ increases the success probability, optimising the performance of QSVT within a CFD code is a function of the number QSVT phase factors, the number of non-linear iterations and the number of shots.","Although phase factor files can be reused, the time taken to generate them impedes scaling QSVT to larger test cases."],"url":"http://arxiv.org/abs/2402.17529v1","category":"quant-ph"}
{"created":"2024-02-27 14:11:32","title":"Predict the Next Word: <Humans exhibit uncertainty in this task and language models _____>","abstract":"Language models (LMs) are statistical models trained to assign probability to human-generated text. As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgements (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM's ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the 'next word prediction' task. This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertainty. We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty. We also verify the failure of expected calibration error (ECE) to reflect this, and as such, advise the community against relying on it in this setting.","sentences":["Language models (LMs) are statistical models trained to assign probability to human-generated text.","As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well.","This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgements (i.e., human evaluation) or a robust automated proxy (which is non-trivial).","At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context.","We exploit this fact and evaluate the LM's ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the 'next word prediction' task.","This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertainty.","We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty.","We also verify the failure of expected calibration error (ECE) to reflect this, and as such, advise the community against relying on it in this setting."],"url":"http://arxiv.org/abs/2402.17527v1","category":"cs.CL"}
{"created":"2024-02-27 14:08:40","title":"Political Pandering and Bureaucratic Influence","abstract":"This paper examines the impact of bureaucracy on policy implementation in environments where electoral incentives generate pandering. A two-period model is developed to analyze the interactions between politicians and bureaucrats, who are categorized as either aligned -- sharing the voters' preferences over policies -- or intent on enacting policies that favor elite groups. The findings reveal equilibria in which aligned politicians resort to pandering, whereas aligned bureaucrats either support or oppose such behavior. The analysis further indicates that, depending on parameters, any level of bureaucratic influence can maximize the voters' welfare, ranging from scenarios with an all-powerful to a toothless bureaucracy.","sentences":["This paper examines the impact of bureaucracy on policy implementation in environments where electoral incentives generate pandering.","A two-period model is developed to analyze the interactions between politicians and bureaucrats, who are categorized as either aligned -- sharing the voters' preferences over policies -- or intent on enacting policies that favor elite groups.","The findings reveal equilibria in which aligned politicians resort to pandering, whereas aligned bureaucrats either support or oppose such behavior.","The analysis further indicates that, depending on parameters, any level of bureaucratic influence can maximize the voters' welfare, ranging from scenarios with an all-powerful to a toothless bureaucracy."],"url":"http://arxiv.org/abs/2402.17526v1","category":"econ.GN"}
{"created":"2024-02-27 14:07:09","title":"Diffusion Model-Based Image Editing: A Survey","abstract":"Denoising diffusion models have emerged as a powerful tool for various image generation and editing tasks, facilitating the synthesis of visual content in an unconditional or input-conditional manner. The core idea behind them is learning to reverse the process of gradually adding noise to images, allowing them to generate high-quality samples from a complex distribution. In this survey, we provide an exhaustive overview of existing methods using diffusion models for image editing, covering both theoretical and practical aspects in the field. We delve into a thorough analysis and categorization of these works from multiple perspectives, including learning strategies, user-input conditions, and the array of specific editing tasks that can be accomplished. In addition, we pay special attention to image inpainting and outpainting, and explore both earlier traditional context-driven and current multimodal conditional methods, offering a comprehensive analysis of their methodologies. To further evaluate the performance of text-guided image editing algorithms, we propose a systematic benchmark, EditEval, featuring an innovative metric, LMM Score. Finally, we address current limitations and envision some potential directions for future research. The accompanying repository is released at https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods.","sentences":["Denoising diffusion models have emerged as a powerful tool for various image generation and editing tasks, facilitating the synthesis of visual content in an unconditional or input-conditional manner.","The core idea behind them is learning to reverse the process of gradually adding noise to images, allowing them to generate high-quality samples from a complex distribution.","In this survey, we provide an exhaustive overview of existing methods using diffusion models for image editing, covering both theoretical and practical aspects in the field.","We delve into a thorough analysis and categorization of these works from multiple perspectives, including learning strategies, user-input conditions, and the array of specific editing tasks that can be accomplished.","In addition, we pay special attention to image inpainting and outpainting, and explore both earlier traditional context-driven and current multimodal conditional methods, offering a comprehensive analysis of their methodologies.","To further evaluate the performance of text-guided image editing algorithms, we propose a systematic benchmark, EditEval, featuring an innovative metric, LMM Score.","Finally, we address current limitations and envision some potential directions for future research.","The accompanying repository is released at https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods."],"url":"http://arxiv.org/abs/2402.17525v1","category":"cs.CV"}
{"created":"2024-02-27 14:03:20","title":"Chromatic defect, Wood's theorem, and higher real $K$-theories","abstract":"Let $\\mathrm{X(n)}$ be Ravenel's Thom spectrum over $\\Omega \\mathrm{SU}(n)$. We say a spectrum $E$ has chromatic defect $n$ if $n$ is the smallest positive integer such that $E\\otimes \\mathrm{X(n)}$ is complex orientable. We compute the chromatic defect of various examples of interest: finite spectra, the Real Johnson--Wilson theories $\\mathrm{ER(n)}$, the fixed points $\\mathrm{EO}_n(G)$ of Morava $E$-theories with respect to a finite subgroup $G$ of the Morava stabilizer group, and the connective image of $J$ spectrum j. Having finite chromatic defect is closely related to the existence of analogues of the classical Wood splitting $\\mathrm{ko}\\otimes C(\\eta)\\simeq \\mathrm{ku}$. We show that such splittings exist in quite a wide generality for fp spectra $E$. When $E$ participates in such a splitting, $E$ admits a $\\mathbb Z$-indexed Adams--Novikov tower, which may be used to deduce differentials in the Adams--Novikov spectral sequence of $E$.","sentences":["Let $\\mathrm{X(n)}$ be Ravenel's Thom spectrum over $\\Omega","\\mathrm{SU}(n)$.","We say a spectrum $E$ has chromatic defect $n$ if $n$ is the smallest positive integer such that $E\\otimes \\mathrm{X(n)}$ is complex orientable.","We compute the chromatic defect of various examples of interest: finite spectra, the Real Johnson--Wilson theories $\\mathrm{ER(n)}$, the fixed points $\\mathrm{EO}_n(G)$ of Morava $E$-theories with respect to a finite subgroup $G$ of the Morava stabilizer group, and the connective image of $J$ spectrum j.","Having finite chromatic defect is closely related to the existence of analogues of the classical Wood splitting $\\mathrm{ko}\\otimes C(\\eta)\\simeq","\\mathrm{ku}$.","We show that such splittings exist in quite a wide generality for fp spectra $E$. When $E$ participates in such a splitting, $E$ admits a $\\mathbb Z$-indexed Adams--Novikov tower, which may be used to deduce differentials in the Adams--Novikov spectral sequence of $E$."],"url":"http://arxiv.org/abs/2402.17519v1","category":"math.AT"}
{"created":"2024-02-27 14:00:34","title":"Label-Noise Robust Diffusion Models","abstract":"Conditional diffusion models have shown remarkable performance in various generative tasks, but training them requires large-scale datasets that often contain noise in conditional inputs, a.k.a. noisy labels. This noise leads to condition mismatch and quality degradation of generated data. This paper proposes Transition-aware weighted Denoising Score Matching (TDSM) for training conditional diffusion models with noisy labels, which is the first study in the line of diffusion models. The TDSM objective contains a weighted sum of score networks, incorporating instance-wise and time-dependent label transition probabilities. We introduce a transition-aware weight estimator, which leverages a time-dependent noisy-label classifier distinctively customized to the diffusion process. Through experiments across various datasets and noisy label settings, TDSM improves the quality of generated samples aligned with given conditions. Furthermore, our method improves generation performance even on prevalent benchmark datasets, which implies the potential noisy labels and their risk of generative model learning. Finally, we show the improved performance of TDSM on top of conventional noisy label corrections, which empirically proving its contribution as a part of label-noise robust generative models. Our code is available at: https://github.com/byeonghu-na/tdsm.","sentences":["Conditional diffusion models have shown remarkable performance in various generative tasks, but training them requires large-scale datasets that often contain noise in conditional inputs, a.k.a. noisy labels.","This noise leads to condition mismatch and quality degradation of generated data.","This paper proposes Transition-aware weighted Denoising Score Matching (TDSM) for training conditional diffusion models with noisy labels, which is the first study in the line of diffusion models.","The TDSM objective contains a weighted sum of score networks, incorporating instance-wise and time-dependent label transition probabilities.","We introduce a transition-aware weight estimator, which leverages a time-dependent noisy-label classifier distinctively customized to the diffusion process.","Through experiments across various datasets and noisy label settings, TDSM improves the quality of generated samples aligned with given conditions.","Furthermore, our method improves generation performance even on prevalent benchmark datasets, which implies the potential noisy labels and their risk of generative model learning.","Finally, we show the improved performance of TDSM on top of conventional noisy label corrections, which empirically proving its contribution as a part of label-noise robust generative models.","Our code is available at: https://github.com/byeonghu-na/tdsm."],"url":"http://arxiv.org/abs/2402.17517v1","category":"cs.LG"}
{"created":"2024-02-27 14:00:08","title":"QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations","abstract":"Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal. In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting explanations but also generates more certain counterfactual examples. We showcase the performance of the QUCE method by comparing it with competing methods for both path-based explanations and generative counterfactual examples. The code repository for the QUCE method is available at: https://github.com/jamie-duell/QUCE.","sentences":["Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain.","The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data.","However, as the complexity of DNN models rises, interpretability diminishes.","In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions.","Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal.","In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty.","QUCE not only quantifies uncertainty when presenting explanations but also generates more certain counterfactual examples.","We showcase the performance of the QUCE method by comparing it with competing methods for both path-based explanations and generative counterfactual examples.","The code repository for the QUCE method is available at: https://github.com/jamie-duell/QUCE."],"url":"http://arxiv.org/abs/2402.17516v1","category":"cs.LG"}
{"created":"2024-02-27 13:57:16","title":"Integrated, bright, broadband parametric down-conversion source for quantum metrology and spectroscopy","abstract":"Broadband quantum light is a vital resource for quantum metrology and spectroscopy applications such as quantum optical coherence tomography or entangled two photon absorption. For entangled two photon absorption in particular, very high photon flux combined with high time-frequency entanglement is crucial for observing a signal. So far these conditions could be met by using high power lasers driving degenerate, type 0 bulk-crystal spontaneous parametric down conversion (SPDC) sources. This naturally limits the available wavelength ranges and precludes deterministic splitting of the generated output photons. In this work we demonstrate an integrated two-colour SPDC source utilising a group-velocity matched lithium niobate waveguide, reaching both exceptional brightness $1.52\\cdot10^6\\frac{\\mathrm{pairs}}{\\mathrm{s\\,mW\\,GHz}}$ and large bandwidth ($7.8\\,$THz FWHM) while pumped with a few mW of continuous wave (CW) laser light. By converting a narrow band pump to broadband pulses the created photon pairs show correlation times of $\\Delta \\tau \\approx 120\\,\\text{fs}$ while maintaining the narrow bandwidth $\\Delta \\omega_p \\ll 1\\,\\text{MHz}$ of the CW pump light, yielding strong time-frequency entanglement. Furthermore our process can be adapted to a wide range of central wavelengths.","sentences":["Broadband quantum light is a vital resource for quantum metrology and spectroscopy applications such as quantum optical coherence tomography or entangled two photon absorption.","For entangled two photon absorption in particular, very high photon flux combined with high time-frequency entanglement is crucial for observing a signal.","So far these conditions could be met by using high power lasers driving degenerate, type 0 bulk-crystal spontaneous parametric down conversion (SPDC) sources.","This naturally limits the available wavelength ranges and precludes deterministic splitting of the generated output photons.","In this work we demonstrate an integrated two-colour SPDC source utilising a group-velocity matched lithium niobate waveguide, reaching both exceptional brightness $1.52\\cdot10^6\\frac{\\mathrm{pairs}}{\\mathrm{s\\,mW\\,GHz}}$ and large bandwidth ($7.8\\,$THz FWHM) while pumped with a few mW of continuous wave (CW) laser light.","By converting a narrow band pump to broadband pulses the created photon pairs show correlation times of $\\Delta \\tau \\approx 120\\,\\text{fs}$ while maintaining the narrow bandwidth $\\Delta \\omega_p \\ll 1\\,\\text{MHz}$ of the CW pump light, yielding strong time-frequency entanglement.","Furthermore our process can be adapted to a wide range of central wavelengths."],"url":"http://arxiv.org/abs/2402.17515v1","category":"quant-ph"}
{"created":"2024-02-27 13:55:17","title":"Robust Unsupervised Crowd Counting and Localization with Adaptive Resolution SAM","abstract":"The existing crowd counting models require extensive training data, which is time-consuming to annotate. To tackle this issue, we propose a simple yet effective crowd counting method by utilizing the Segment-Everything-Everywhere Model (SEEM), an adaptation of the Segmentation Anything Model (SAM), to generate pseudo-labels for training crowd counting models. However, our initial investigation reveals that SEEM's performance in dense crowd scenes is limited, primarily due to the omission of many persons in high-density areas. To overcome this limitation, we propose an adaptive resolution SEEM to handle the scale variations, occlusions, and overlapping of people within crowd scenes. Alongside this, we introduce a robust localization method, based on Gaussian Mixture Models, for predicting the head positions in the predicted people masks. Given the mask and point pseudo-labels, we propose a robust loss function, which is designed to exclude uncertain regions based on SEEM's predictions, thereby enhancing the training process of the counting networks. Finally, we propose an iterative method for generating pseudo-labels. This method aims at improving the quality of the segmentation masks by identifying more tiny persons in high-density regions, which are often missed in the first pseudo-labeling stage. Overall, our proposed method achieves the best unsupervised performance in crowd counting, while also being comparable results to some supervised methods. This makes it a highly effective and versatile tool for crowd counting, especially in situations where labeled data is not available.","sentences":["The existing crowd counting models require extensive training data, which is time-consuming to annotate.","To tackle this issue, we propose a simple yet effective crowd counting method by utilizing the Segment-Everything-Everywhere Model (SEEM), an adaptation of the Segmentation Anything Model (SAM), to generate pseudo-labels for training crowd counting models.","However, our initial investigation reveals that SEEM's performance in dense crowd scenes is limited, primarily due to the omission of many persons in high-density areas.","To overcome this limitation, we propose an adaptive resolution SEEM to handle the scale variations, occlusions, and overlapping of people within crowd scenes.","Alongside this, we introduce a robust localization method, based on Gaussian Mixture Models, for predicting the head positions in the predicted people masks.","Given the mask and point pseudo-labels, we propose a robust loss function, which is designed to exclude uncertain regions based on SEEM's predictions, thereby enhancing the training process of the counting networks.","Finally, we propose an iterative method for generating pseudo-labels.","This method aims at improving the quality of the segmentation masks by identifying more tiny persons in high-density regions, which are often missed in the first pseudo-labeling stage.","Overall, our proposed method achieves the best unsupervised performance in crowd counting, while also being comparable results to some supervised methods.","This makes it a highly effective and versatile tool for crowd counting, especially in situations where labeled data is not available."],"url":"http://arxiv.org/abs/2402.17514v1","category":"cs.CV"}
{"created":"2024-02-27 13:54:48","title":"Latent Attention for Linear Time Transformers","abstract":"The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our \"Latte Transformer\" model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.","sentences":["The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence.","We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors.","The method is readily usable as a drop-in replacement for the standard attention mechanism.","Our \"Latte Transformer\" model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks.","Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token.","The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention."],"url":"http://arxiv.org/abs/2402.17512v1","category":"cs.CL"}
{"created":"2024-02-27 13:53:52","title":"Rethinking Mutual Information for Language Conditioned Skill Discovery on Imitation Learning","abstract":"Language-conditioned robot behavior plays a vital role in executing complex tasks by associating human commands or instructions with perception and actions. The ability to compose long-horizon tasks based on unconstrained language instructions necessitates the acquisition of a diverse set of general-purpose skills. However, acquiring inherent primitive skills in a coupled and long-horizon environment without external rewards or human supervision presents significant challenges. In this paper, we evaluate the relationship between skills and language instructions from a mathematical perspective, employing two forms of mutual information within the framework of language-conditioned policy learning. To maximize the mutual information between language and skills in an unsupervised manner, we propose an end-to-end imitation learning approach known as Language Conditioned Skill Discovery (LCSD). Specifically, we utilize vector quantization to learn discrete latent skills and leverage skill sequences of trajectories to reconstruct high-level semantic instructions. Through extensive experiments on language-conditioned robotic navigation and manipulation tasks, encompassing BabyAI, LORel, and CALVIN, we demonstrate the superiority of our method over prior works. Our approach exhibits enhanced generalization capabilities towards unseen tasks, improved skill interpretability, and notably higher rates of task completion success.","sentences":["Language-conditioned robot behavior plays a vital role in executing complex tasks by associating human commands or instructions with perception and actions.","The ability to compose long-horizon tasks based on unconstrained language instructions necessitates the acquisition of a diverse set of general-purpose skills.","However, acquiring inherent primitive skills in a coupled and long-horizon environment without external rewards or human supervision presents significant challenges.","In this paper, we evaluate the relationship between skills and language instructions from a mathematical perspective, employing two forms of mutual information within the framework of language-conditioned policy learning.","To maximize the mutual information between language and skills in an unsupervised manner, we propose an end-to-end imitation learning approach known as Language Conditioned Skill Discovery (LCSD).","Specifically, we utilize vector quantization to learn discrete latent skills and leverage skill sequences of trajectories to reconstruct high-level semantic instructions.","Through extensive experiments on language-conditioned robotic navigation and manipulation tasks, encompassing BabyAI, LORel, and CALVIN, we demonstrate the superiority of our method over prior works.","Our approach exhibits enhanced generalization capabilities towards unseen tasks, improved skill interpretability, and notably higher rates of task completion success."],"url":"http://arxiv.org/abs/2402.17511v1","category":"cs.RO"}
{"created":"2024-02-27 13:50:34","title":"Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning","abstract":"Vision-language models (VLMs) mainly rely on contrastive training to learn general-purpose representations of images and captions. We focus on the situation when one image is associated with several captions, each caption containing both information shared among all captions and unique information per caption about the scene depicted in the image. In such cases, it is unclear whether contrastive losses are sufficient for learning task-optimal representations that contain all the information provided by the captions or whether the contrastive learning setup encourages the learning of a simple shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for vision-language: a training and evaluation framework where we inject synthetic shortcuts into image-text data. We show that contrastive VLMs trained from scratch or fine-tuned with data containing these synthetic shortcuts mainly learn features that represent the shortcut. Hence, contrastive losses are not sufficient to learn task-optimal representations, i.e., representations that contain all task-relevant information shared between the image and associated captions. We examine two methods to reduce shortcut learning in our training and evaluation framework: (i) latent target decoding and (ii) implicit feature modification. We show empirically that both methods improve performance on the evaluation task, but only partly reduce shortcut learning when training and evaluating with our shortcut learning framework. Hence, we show the difficulty and challenge of our shortcut learning framework for contrastive vision-language representation learning.","sentences":["Vision-language models (VLMs) mainly rely on contrastive training to learn general-purpose representations of images and captions.","We focus on the situation when one image is associated with several captions, each caption containing both information shared among all captions and unique information per caption about the scene depicted in the image.","In such cases, it is unclear whether contrastive losses are sufficient for learning task-optimal representations that contain all the information provided by the captions or whether the contrastive learning setup encourages the learning of a simple shortcut that minimizes contrastive loss.","We introduce synthetic shortcuts for vision-language: a training and evaluation framework where we inject synthetic shortcuts into image-text data.","We show that contrastive VLMs trained from scratch or fine-tuned with data containing these synthetic shortcuts mainly learn features that represent the shortcut.","Hence, contrastive losses are not sufficient to learn task-optimal representations, i.e., representations that contain all task-relevant information shared between the image and associated captions.","We examine two methods to reduce shortcut learning in our training and evaluation framework: (i) latent target decoding and (ii) implicit feature modification.","We show empirically that both methods improve performance on the evaluation task, but only partly reduce shortcut learning when training and evaluating with our shortcut learning framework.","Hence, we show the difficulty and challenge of our shortcut learning framework for contrastive vision-language representation learning."],"url":"http://arxiv.org/abs/2402.17510v1","category":"cs.CV"}
{"created":"2024-02-27 13:47:52","title":"Comparative study of photo-induced electronic transport along ferroelectric domain walls in lithium niobate single crystals","abstract":"Ferroelectric domain wall conductivity (DWC) is an intriguing functional property, that can be controlled through external stimuli such as electric and mechanical fields. Optical-field control, as a non-invasive flexible handle, has rarely been applied so far, but significantly expands the possibility for both tuning and probing DWC. On the one hand, as known from Second-Harmonic, Raman, and CARS micro-spectroscopy, the optical in-and-out approach delivers parameters on the DW distribution, the DW inclination, and probes the DW vibrational modes; on the other hand, photons might be applied also to directly generate charge carriers within the DW, hence acting as a functional and spectrally tunable probe to deduce the integral or local absorption properties and bandgaps of conductive DWs. Here, we report on such an optoelectronic approach by investigating the photo-induced DWC (PI-DWC) in DWs of the model system lithium niobate, a material that is well known for hosting conductive DWs. We compare three different crystals containing different numbers of domain walls: (A) none, (B) one, and (C) many conductive DWs. All samples are inspected for their current-voltage (I-V) behavior (i) in darkness, and (ii) for different illumination wavelengths swept from 500 nm down to 310 nm. All samples show their maximum PI-DWC at 310 nm, i.e., at the optical bandgap of lithium niobate; moreover, sample (C) reaches PI-DWCs of several $\\mu$A. Interestingly, a noticeable PI-DWC is also observed for sub-bandgap illumination, i.e., wavelengths as high as 500 nm, hinting towards the existence and decisive role of electronic in-gap states that contribute to the electronic transport along DWs. Finally, conductive atomic force microscopy (c-AFM) investigations under illumination proved that the PI-DWC is confined to the DW area, and does not originate from photo-induced bulk conductivity.","sentences":["Ferroelectric domain wall conductivity (DWC) is an intriguing functional property, that can be controlled through external stimuli such as electric and mechanical fields.","Optical-field control, as a non-invasive flexible handle, has rarely been applied so far, but significantly expands the possibility for both tuning and probing DWC.","On the one hand, as known from Second-Harmonic, Raman, and CARS micro-spectroscopy, the optical in-and-out approach delivers parameters on the DW distribution, the DW inclination, and probes the DW vibrational modes; on the other hand, photons might be applied also to directly generate charge carriers within the DW, hence acting as a functional and spectrally tunable probe to deduce the integral or local absorption properties and bandgaps of conductive DWs.","Here, we report on such an optoelectronic approach by investigating the photo-induced DWC (PI-DWC) in DWs of the model system lithium niobate, a material that is well known for hosting conductive DWs.","We compare three different crystals containing different numbers of domain walls: (A) none, (B) one, and (C) many conductive DWs.","All samples are inspected for their current-voltage (I-V) behavior (i) in darkness, and (ii) for different illumination wavelengths swept from 500 nm down to 310 nm.","All samples show their maximum PI-DWC at 310 nm, i.e., at the optical bandgap of lithium niobate; moreover, sample (C) reaches PI-DWCs of several $\\mu$A. Interestingly, a noticeable PI-DWC is also observed for sub-bandgap illumination, i.e., wavelengths as high as 500 nm, hinting towards the existence and decisive role of electronic in-gap states that contribute to the electronic transport along DWs.","Finally, conductive atomic force microscopy (c-AFM) investigations under illumination proved that the PI-DWC is confined to the DW area, and does not originate from photo-induced bulk conductivity."],"url":"http://arxiv.org/abs/2402.17508v1","category":"physics.app-ph"}
{"created":"2024-02-27 13:46:45","title":"Thermodynamics-informed super-resolution of scarce temporal dynamics data","abstract":"We present a method to increase the resolution of measurements of a physical system and subsequently predict its time evolution using thermodynamics-aware neural networks. Our method uses adversarial autoencoders, which reduce the dimensionality of the full order model to a set of latent variables that are enforced to match a prior, for example a normal distribution. Adversarial autoencoders are seen as generative models, and they can be trained to generate high-resolution samples from low-resoution inputs, meaning they can address the so-called super-resolution problem. Then, a second neural network is trained to learn the physical structure of the latent variables and predict their temporal evolution. This neural network is known as an structure-preserving neural network. It learns the metriplectic-structure of the system and applies a physical bias to ensure that the first and second principles of thermodynamics are fulfilled. The integrated trajectories are decoded to their original dimensionality, as well as to the higher dimensionality space produced by the adversarial autoencoder and they are compared to the ground truth solution. The method is tested with two examples of flow over a cylinder, where the fluid properties are varied between both examples.","sentences":["We present a method to increase the resolution of measurements of a physical system and subsequently predict its time evolution using thermodynamics-aware neural networks.","Our method uses adversarial autoencoders, which reduce the dimensionality of the full order model to a set of latent variables that are enforced to match a prior, for example a normal distribution.","Adversarial autoencoders are seen as generative models, and they can be trained to generate high-resolution samples from low-resoution inputs, meaning they can address the so-called super-resolution problem.","Then, a second neural network is trained to learn the physical structure of the latent variables and predict their temporal evolution.","This neural network is known as an structure-preserving neural network.","It learns the metriplectic-structure of the system and applies a physical bias to ensure that the first and second principles of thermodynamics are fulfilled.","The integrated trajectories are decoded to their original dimensionality, as well as to the higher dimensionality space produced by the adversarial autoencoder and they are compared to the ground truth solution.","The method is tested with two examples of flow over a cylinder, where the fluid properties are varied between both examples."],"url":"http://arxiv.org/abs/2402.17506v1","category":"physics.comp-ph"}
{"created":"2024-02-27 13:44:09","title":"BASES: Large-scale Web Search User Simulation with Large Language Model based Agents","abstract":"Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation. Considering the scarcity and limit (e.g., privacy issues) of real user data, in this paper, we conduct large-scale user simulation for web search, to improve the analysis and modeling of user search behavior. Specially, we propose BASES, a novel user simulation framework with LLM-based agents, designed to facilitate comprehensive simulations of web search user behaviors. Our simulation framework can generate unique user profiles at scale, which subsequently leads to diverse search behaviors. To demonstrate the effectiveness of BASES, we conduct evaluation experiments based on two human benchmarks in both Chinese and English, demonstrating that BASES can effectively simulate large-scale human-like search behaviors. To further accommodate the research on web search, we develop WARRIORS, a new large-scale dataset encompassing web search user behaviors, including both Chinese and English versions, which can greatly bolster research in the field of information retrieval. Our code and data will be publicly released soon.","sentences":["Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation.","Considering the scarcity and limit (e.g., privacy issues) of real user data, in this paper, we conduct large-scale user simulation for web search, to improve the analysis and modeling of user search behavior.","Specially, we propose BASES, a novel user simulation framework with LLM-based agents, designed to facilitate comprehensive simulations of web search user behaviors.","Our simulation framework can generate unique user profiles at scale, which subsequently leads to diverse search behaviors.","To demonstrate the effectiveness of BASES, we conduct evaluation experiments based on two human benchmarks in both Chinese and English, demonstrating that BASES can effectively simulate large-scale human-like search behaviors.","To further accommodate the research on web search, we develop WARRIORS, a new large-scale dataset encompassing web search user behaviors, including both Chinese and English versions, which can greatly bolster research in the field of information retrieval.","Our code and data will be publicly released soon."],"url":"http://arxiv.org/abs/2402.17505v1","category":"cs.IR"}
{"created":"2024-02-27 13:43:50","title":"On the c-k constrained KP and BKP hierarchies: the Fermionic pictures, solutions and additional symmetries","abstract":"In this paper, we study two generalized constrained integrable hierarchies, which are called the $c$-$k$ constrained KP and BKP hierarchies. The Fermionic picture of the $c$-$k$ constrained KP hierarchy is given. We give some solutions for the $c$-$k$ constrained KP hierarchy by using the free Fermion operators and define its additional symmetries. Its additional flows form a subalgebra of the Virasoro algebra. Furthermore, the additional flows acting on eigenfunctions $q_{i}(t)$ and adjoint eigenfunctions $r_{i}(t)$ of the $c$-$k$ constrained KP hierarchy are presented. Next, we define the $c$-$k$ constrained BKP hierarchy and obtain its bilinear identity and solutions. The algebra formed by the additional symmetric flow of the $c$-$k$ constrained BKP hierarchy that we defined is still a subalgebra of the Virasoro algebra and it is a subalgebra of the algebra formed by the additional flows of the $c$-$k$ constrained KP hierarchy.","sentences":["In this paper, we study two generalized constrained integrable hierarchies, which are called the $c$-$k$ constrained KP and BKP hierarchies.","The Fermionic picture of the $c$-$k$ constrained KP hierarchy is given.","We give some solutions for the $c$-$k$ constrained KP hierarchy by using the free Fermion operators and define its additional symmetries.","Its additional flows form a subalgebra of the Virasoro algebra.","Furthermore, the additional flows acting on eigenfunctions $q_{i}(t)$ and adjoint eigenfunctions $r_{i}(t)$ of the $c$-$k$ constrained KP hierarchy are presented.","Next, we define the $c$-$k$ constrained BKP hierarchy and obtain its bilinear identity and solutions.","The algebra formed by the additional symmetric flow of the $c$-$k$ constrained BKP hierarchy that we defined is still a subalgebra of the Virasoro algebra and it is a subalgebra of the algebra formed by the additional flows of the $c$-$k$ constrained KP hierarchy."],"url":"http://arxiv.org/abs/2402.17503v1","category":"nlin.SI"}
{"created":"2024-02-27 13:41:32","title":"FedLPPA: Learning Personalized Prompt and Aggregation for Federated Weakly-supervised Medical Image Segmentation","abstract":"Federated learning (FL) effectively mitigates the data silo challenge brought about by policies and privacy concerns, implicitly harnessing more data for deep model training. However, traditional centralized FL models grapple with diverse multi-center data, especially in the face of significant data heterogeneity, notably in medical contexts. In the realm of medical image segmentation, the growing imperative to curtail annotation costs has amplified the importance of weakly-supervised techniques which utilize sparse annotations such as points, scribbles, etc. A pragmatic FL paradigm shall accommodate diverse annotation formats across different sites, which research topic remains under-investigated. In such context, we propose a novel personalized FL framework with learnable prompt and aggregation (FedLPPA) to uniformly leverage heterogeneous weak supervision for medical image segmentation. In FedLPPA, a learnable universal knowledge prompt is maintained, complemented by multiple learnable personalized data distribution prompts and prompts representing the supervision sparsity. Integrated with sample features through a dual-attention mechanism, those prompts empower each local task decoder to adeptly adjust to both the local distribution and the supervision form. Concurrently, a dual-decoder strategy, predicated on prompt similarity, is introduced for enhancing the generation of pseudo-labels in weakly-supervised learning, alleviating overfitting and noise accumulation inherent to local data, while an adaptable aggregation method is employed to customize the task decoder on a parameter-wise basis. Extensive experiments on three distinct medical image segmentation tasks involving different modalities underscore the superiority of FedLPPA, with its efficacy closely parallels that of fully supervised centralized training. Our code and data will be available.","sentences":["Federated learning (FL) effectively mitigates the data silo challenge brought about by policies and privacy concerns, implicitly harnessing more data for deep model training.","However, traditional centralized FL models grapple with diverse multi-center data, especially in the face of significant data heterogeneity, notably in medical contexts.","In the realm of medical image segmentation, the growing imperative to curtail annotation costs has amplified the importance of weakly-supervised techniques which utilize sparse annotations such as points, scribbles, etc.","A pragmatic FL paradigm shall accommodate diverse annotation formats across different sites, which research topic remains under-investigated.","In such context, we propose a novel personalized FL framework with learnable prompt and aggregation (FedLPPA) to uniformly leverage heterogeneous weak supervision for medical image segmentation.","In FedLPPA, a learnable universal knowledge prompt is maintained, complemented by multiple learnable personalized data distribution prompts and prompts representing the supervision sparsity.","Integrated with sample features through a dual-attention mechanism, those prompts empower each local task decoder to adeptly adjust to both the local distribution and the supervision form.","Concurrently, a dual-decoder strategy, predicated on prompt similarity, is introduced for enhancing the generation of pseudo-labels in weakly-supervised learning, alleviating overfitting and noise accumulation inherent to local data, while an adaptable aggregation method is employed to customize the task decoder on a parameter-wise basis.","Extensive experiments on three distinct medical image segmentation tasks involving different modalities underscore the superiority of FedLPPA, with its efficacy closely parallels that of fully supervised centralized training.","Our code and data will be available."],"url":"http://arxiv.org/abs/2402.17502v1","category":"cs.CV"}
{"created":"2024-02-27 13:36:55","title":"Intensive Care as One Big Sequence Modeling Problem","abstract":"Reinforcement Learning in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control. However, previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning. To enable training of foundation models for Healthcare as well as leverage the capabilities of state of the art Transformer architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream. To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous clinical records from MIMIC-IV dataset into a uniform event stream format, train a baseline model and explore its capabilities.","sentences":["Reinforcement Learning in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control.","However, previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning.","To enable training of foundation models for Healthcare as well as leverage the capabilities of state of the art Transformer architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream.","To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous clinical records from MIMIC-IV dataset into a uniform event stream format, train a baseline model and explore its capabilities."],"url":"http://arxiv.org/abs/2402.17501v1","category":"cs.LG"}
{"created":"2024-02-27 13:33:58","title":"Rigidity of nearly planar classes of graphs","abstract":"We explore the rigidity of generic frameworks in 3-dimensions whose underlying graph is close to being planar. Specifically we consider apex graphs, edge-apex graphs and their variants and prove independence results in the generic 3-dimensional rigidity matroid adding to the short list of graph classes for which 3-dimensional rigidity is understood. We then analyse global rigidity for these graph classes and use our results to deduce bounds on the maximum likelihood threshold of graphs in these nearly planar classes.","sentences":["We explore the rigidity of generic frameworks in 3-dimensions whose underlying graph is close to being planar.","Specifically we consider apex graphs, edge-apex graphs and their variants and prove independence results in the generic 3-dimensional rigidity matroid adding to the short list of graph classes for which 3-dimensional rigidity is understood.","We then analyse global rigidity for these graph classes and use our results to deduce bounds on the maximum likelihood threshold of graphs in these nearly planar classes."],"url":"http://arxiv.org/abs/2402.17499v1","category":"math.CO"}
{"created":"2024-02-27 13:27:15","title":"Validity of energy conditions of matter in traversable wormholes under the $f(Q)$ modified gravity theory","abstract":"In the framework of the theory of general relativity, in order to obtain stable traversable wormholes, matter needs to violate the null energy condition. It is well known that the violation of the energy condition (EC) of matter leads to various physical problems. To address this issue, researchers have turned their attention to exploring modified theories of gravity, aiming to avoid the violation of ECs by introducing geometric terms. In this paper, within the framework of the $f(Q)$ modified gravitational theory, we investigate the effectiveness of ECs for matter in traversable wormholes. We examine the compliance of four types of energy conditions (weak energy condition, null energy condition, dominant energy condition, and strong energy condition) in the model by selecting a power-law model for $f(Q)$ and considering different shape functions $b(r)$. Our study reveals that for traversable wormholes realized through the $f(Q)$ modified gravity theory using the power-law model $f(Q)=a(-Q)^n$, all four types of ECs for matter can be satisfied. There is no need to introduce exotic matter (violating the null energy condition) or special matter (violating other energy conditions) artificially in the physics of wormholes.","sentences":["In the framework of the theory of general relativity, in order to obtain stable traversable wormholes, matter needs to violate the null energy condition.","It is well known that the violation of the energy condition (EC) of matter leads to various physical problems.","To address this issue, researchers have turned their attention to exploring modified theories of gravity, aiming to avoid the violation of ECs by introducing geometric terms.","In this paper, within the framework of the $f(Q)$ modified gravitational theory, we investigate the effectiveness of ECs for matter in traversable wormholes.","We examine the compliance of four types of energy conditions (weak energy condition, null energy condition, dominant energy condition, and strong energy condition) in the model by selecting a power-law model for $f(Q)$ and considering different shape functions $b(r)$. Our study reveals that for traversable wormholes realized through the $f(Q)$ modified gravity theory using the power-law model $f(Q)=a(-Q)^n$, all four types of ECs for matter can be satisfied.","There is no need to introduce exotic matter (violating the null energy condition) or special matter (violating other energy conditions) artificially in the physics of wormholes."],"url":"http://arxiv.org/abs/2402.17498v1","category":"gr-qc"}
{"created":"2024-02-27 13:22:51","title":"REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering","abstract":"Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness of source relevance for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a new architecture for LLM based RAG system, by incorporating a specially designed rank head that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training. By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents. Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches. Our code and data can be accessed at https://github.com/RUCAIBox/REAR.","sentences":["Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs).","Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents).","To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA).","As the key motivation, we aim to enhance the self-awareness of source relevance for LLMs, so as to adaptively utilize external knowledge in RAG systems.","Specially, we develop a new architecture for LLM based RAG system, by incorporating a specially designed rank head that precisely assesses the relevance of retrieved documents.","Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training.","By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents.","Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches.","Our code and data can be accessed at https://github.com/RUCAIBox/REAR."],"url":"http://arxiv.org/abs/2402.17497v1","category":"cs.CL"}
{"created":"2024-02-27 13:22:47","title":"Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages","abstract":"Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced. Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment. Audios were labeled in valence and arousal dimensions by three non-experts and two experts, which were then combined to obtain a final label per dimension. The experts also provided an extra label corresponding to seven emotion categories. To set a baseline for future investigations using EMOVOME, we implemented emotion recognition models using both speech and audio transcriptions. For speech, we used the standard eGeMAPS feature set and support vector machines, obtaining 49.27% and 44.71% unweighted accuracy for valence and arousal respectively. For text, we fine-tuned a multilingual BERT model and achieved 61.15% and 47.43% unweighted accuracy for valence and arousal respectively. This database will significantly contribute to research on emotion recognition in the wild, while also providing a unique natural and freely accessible resource for Spanish.","sentences":["Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced.","Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment.","Audios were labeled in valence and arousal dimensions by three non-experts and two experts, which were then combined to obtain a final label per dimension.","The experts also provided an extra label corresponding to seven emotion categories.","To set a baseline for future investigations using EMOVOME, we implemented emotion recognition models using both speech and audio transcriptions.","For speech, we used the standard eGeMAPS feature set and support vector machines, obtaining 49.27% and 44.71% unweighted accuracy for valence and arousal respectively.","For text, we fine-tuned a multilingual BERT model and achieved 61.15% and 47.43% unweighted accuracy for valence and arousal respectively.","This database will significantly contribute to research on emotion recognition in the wild, while also providing a unique natural and freely accessible resource for Spanish."],"url":"http://arxiv.org/abs/2402.17496v1","category":"cs.SD"}
{"created":"2024-02-27 13:16:50","title":"The Mechanical Turkness: Tactical Media Art and the Critique of Corporate AI","abstract":"The extensive industrialization of artificial intelligence (AI) since the mid-2010s has increasingly motivated artists to address its economic and sociopolitical consequences. In this chapter, I discuss interrelated art practices that thematize creative agency, crowdsourced labor, and delegated artmaking to reveal the social rootage of AI technologies and underline the productive human roles in their development. I focus on works whose poetic features indicate broader issues of contemporary AI-influenced science, technology, economy, and society. By exploring the conceptual, methodological, and ethical aspects of their effectiveness in disrupting the political regime of corporate AI, I identify several problems that affect their tactical impact and outline potential avenues for tackling the challenges and advancing the field.","sentences":["The extensive industrialization of artificial intelligence (AI) since the mid-2010s has increasingly motivated artists to address its economic and sociopolitical consequences.","In this chapter, I discuss interrelated art practices that thematize creative agency, crowdsourced labor, and delegated artmaking to reveal the social rootage of AI technologies and underline the productive human roles in their development.","I focus on works whose poetic features indicate broader issues of contemporary AI-influenced science, technology, economy, and society.","By exploring the conceptual, methodological, and ethical aspects of their effectiveness in disrupting the political regime of corporate AI, I identify several problems that affect their tactical impact and outline potential avenues for tackling the challenges and advancing the field."],"url":"http://arxiv.org/abs/2402.17490v1","category":"cs.CY"}
{"created":"2024-02-27 13:12:58","title":"Complexity Assessment of Analog Security Primitives Using the Disentropy of Autocorrelation","abstract":"The study of regularity in signals can be of great importance, typically in medicine to analyse electrocardiogram (ECG) or electromyography (EMG) signals, but also in climate studies, finance or security. In this work we focus on security primitives such as Physical Unclonable Functions (PUFs) or Pseudo-Random Number Generators (PRNGs). Such primitives must have a high level of complexity or entropy in their responses to guarantee enough security for their applications. There are several ways of assessing the complexity of their responses, especially in the binary domain. With the development of analog PUFs such as optical (photonic) PUFs, it would be useful to be able to assess their complexity in the analog domain when designing them, for example, before converting analog signals into binary. In this numerical study, we decided to explore the potential of the disentropy of autocorrelation as a measure of complexity for security primitives as PUFs or PRNGs with analog output or responses. We compare this metric to others used to assess regularities in analog signals such as Approximate Entropy (ApEn) and Fuzzy Entropy (FuzEn). We show that the disentropy of autocorrelation is able to differentiate between well-known PRNGs and non-optimised or bad PRNGs in the analog and binary domain with a better contrast than ApEn and FuzEn. Next, we show that the disentropy of autocorrelation is able to detect small patterns injected in PUFs responses and then we applied it to photonic PUFs simulations.","sentences":["The study of regularity in signals can be of great importance, typically in medicine to analyse electrocardiogram (ECG) or electromyography (EMG) signals, but also in climate studies, finance or security.","In this work we focus on security primitives such as Physical Unclonable Functions (PUFs) or Pseudo-Random Number Generators (PRNGs).","Such primitives must have a high level of complexity or entropy in their responses to guarantee enough security for their applications.","There are several ways of assessing the complexity of their responses, especially in the binary domain.","With the development of analog PUFs such as optical (photonic) PUFs, it would be useful to be able to assess their complexity in the analog domain when designing them, for example, before converting analog signals into binary.","In this numerical study, we decided to explore the potential of the disentropy of autocorrelation as a measure of complexity for security primitives as PUFs or PRNGs with analog output or responses.","We compare this metric to others used to assess regularities in analog signals such as Approximate Entropy (ApEn) and Fuzzy Entropy (FuzEn).","We show that the disentropy of autocorrelation is able to differentiate between well-known PRNGs and non-optimised or bad PRNGs in the analog and binary domain with a better contrast than ApEn and FuzEn.","Next, we show that the disentropy of autocorrelation is able to detect small patterns injected in PUFs responses and then we applied it to photonic PUFs simulations."],"url":"http://arxiv.org/abs/2402.17488v1","category":"cs.CR"}
{"created":"2024-02-27 13:12:18","title":"Bit Rate Matching Algorithm Optimization in JPEG-AI Verification Model","abstract":"The research on neural network (NN) based image compression has shown superior performance compared to classical compression frameworks. Unlike the hand-engineered transforms in the classical frameworks, NN-based models learn the non-linear transforms providing more compact bit representations, and achieve faster coding speed on parallel devices over their classical counterparts. Those properties evoked the attention of both scientific and industrial communities, resulting in the standardization activity JPEG-AI. The verification model for the standardization process of JPEG-AI is already in development and has surpassed the advanced VVC intra codec. To generate reconstructed images with the desired bits per pixel and assess the BD-rate performance of both the JPEG-AI verification model and VVC intra, bit rate matching is employed. However, the current state of the JPEG-AI verification model experiences significant slowdowns during bit rate matching, resulting in suboptimal performance due to an unsuitable model. The proposed methodology offers a gradual algorithmic optimization for matching bit rates, resulting in a fourfold acceleration and over 1% improvement in BD-rate at the base operation point. At the high operation point, the acceleration increases up to sixfold.","sentences":["The research on neural network (NN) based image compression has shown superior performance compared to classical compression frameworks.","Unlike the hand-engineered transforms in the classical frameworks, NN-based models learn the non-linear transforms providing more compact bit representations, and achieve faster coding speed on parallel devices over their classical counterparts.","Those properties evoked the attention of both scientific and industrial communities, resulting in the standardization activity JPEG-AI.","The verification model for the standardization process of JPEG-AI is already in development and has surpassed the advanced VVC intra codec.","To generate reconstructed images with the desired bits per pixel and assess the BD-rate performance of both the JPEG-AI verification model and VVC intra, bit rate matching is employed.","However, the current state of the JPEG-AI verification model experiences significant slowdowns during bit rate matching, resulting in suboptimal performance due to an unsuitable model.","The proposed methodology offers a gradual algorithmic optimization for matching bit rates, resulting in a fourfold acceleration and over 1% improvement in BD-rate at the base operation point.","At the high operation point, the acceleration increases up to sixfold."],"url":"http://arxiv.org/abs/2402.17487v1","category":"cs.CV"}
{"created":"2024-02-27 13:12:00","title":"MGE: A Training-Free and Efficient Model Generation and Enhancement Scheme","abstract":"To provide a foundation for the research of deep learning models, the construction of model pool is an essential step. This paper proposes a Training-Free and Efficient Model Generation and Enhancement Scheme (MGE). This scheme primarily considers two aspects during the model generation process: the distribution of model parameters and model performance. Experiments result shows that generated models are comparable to models obtained through normal training, and even superior in some cases. Moreover, the time consumed in generating models accounts for only 1\\% of the time required for normal model training. More importantly, with the enhancement of Evolution-MGE, generated models exhibits competitive generalization ability in few-shot tasks. And the behavioral dissimilarity of generated models has the potential of adversarial defense.","sentences":["To provide a foundation for the research of deep learning models, the construction of model pool is an essential step.","This paper proposes a Training-Free and Efficient Model Generation and Enhancement Scheme (MGE).","This scheme primarily considers two aspects during the model generation process: the distribution of model parameters and model performance.","Experiments result shows that generated models are comparable to models obtained through normal training, and even superior in some cases.","Moreover, the time consumed in generating models accounts for only 1\\% of the time required for normal model training.","More importantly, with the enhancement of Evolution-MGE, generated models exhibits competitive generalization ability in few-shot tasks.","And the behavioral dissimilarity of generated models has the potential of adversarial defense."],"url":"http://arxiv.org/abs/2402.17486v1","category":"cs.CV"}
{"created":"2024-02-27 13:10:11","title":"EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions","abstract":"In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism.","sentences":["In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements.","We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles.","To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks.","Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations.","Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism."],"url":"http://arxiv.org/abs/2402.17485v1","category":"cs.CV"}
{"created":"2024-02-27 13:09:42","title":"Invariants of flat connections on 4-manifolds from Hopf group-algebras","abstract":"For a given group $G$, we construct an invariant of flat $G$-connections on 4-manifolds from a finite type involutory quasitriangular Hopf $G$-algebra. Hopf $G$-algebras are generalizations of Hopf algebras, equipped with gradings by $G$. In our construction, we color the dotted components of a Kirby diagram with elements of $G$ and employ the Hennings-type procedure. When $G$ is finite, we also define an invariant of 4-manifolds by summing the invariants over all flat $G$-connections.","sentences":["For a given group $G$, we construct an invariant of flat $G$-connections on 4-manifolds from a finite type involutory quasitriangular Hopf $G$-algebra.","Hopf $G$-algebras are generalizations of Hopf algebras, equipped with gradings by $G$. In our construction, we color the dotted components of a Kirby diagram with elements of $G$ and employ the Hennings-type procedure.","When $G$ is finite, we also define an invariant of 4-manifolds by summing the invariants over all flat $G$-connections."],"url":"http://arxiv.org/abs/2402.17484v1","category":"math.GT"}
{"created":"2024-02-27 13:08:34","title":"Automated Classification of Phonetic Segments in Child Speech Using Raw Ultrasound Imaging","abstract":"Speech sound disorder (SSD) is defined as a persistent impairment in speech sound production leading to reduced speech intelligibility and hindered verbal communication. Early recognition and intervention of children with SSD and timely referral to speech and language therapists (SLTs) for treatment are crucial. Automated detection of speech impairment is regarded as an efficient method for examining and screening large populations. This study focuses on advancing the automatic diagnosis of SSD in early childhood by proposing a technical solution that integrates ultrasound tongue imaging (UTI) with deep-learning models. The introduced FusionNet model combines UTI data with the extracted texture features to classify UTI. The overarching aim is to elevate the accuracy and efficiency of UTI analysis, particularly for classifying speech sounds associated with SSD. This study compared the FusionNet approach with standard deep-learning methodologies, highlighting the excellent improvement results of the FusionNet model in UTI classification and the potential of multi-learning in improving UTI classification in speech therapy clinics.","sentences":["Speech sound disorder (SSD) is defined as a persistent impairment in speech sound production leading to reduced speech intelligibility and hindered verbal communication.","Early recognition and intervention of children with SSD and timely referral to speech and language therapists (SLTs) for treatment are crucial.","Automated detection of speech impairment is regarded as an efficient method for examining and screening large populations.","This study focuses on advancing the automatic diagnosis of SSD in early childhood by proposing a technical solution that integrates ultrasound tongue imaging (UTI) with deep-learning models.","The introduced FusionNet model combines UTI data with the extracted texture features to classify UTI.","The overarching aim is to elevate the accuracy and efficiency of UTI analysis, particularly for classifying speech sounds associated with SSD.","This study compared the FusionNet approach with standard deep-learning methodologies, highlighting the excellent improvement results of the FusionNet model in UTI classification and the potential of multi-learning in improving UTI classification in speech therapy clinics."],"url":"http://arxiv.org/abs/2402.17482v1","category":"cs.SD"}
{"created":"2024-02-27 13:01:54","title":"Strongly $n$-AIR-tilting modules","abstract":"We introduce the notion of (strongly) $n$-AIR-tilting modules, which is a high dimension version of support $\\tau$-tilting modules. The relations between them and $n$-silting modules and $n$-quasi-tilting modules, as well as generalized two-term silting complexes, are investigated. Our results particularly suggest a way to negate the rank question for silting complexes.","sentences":["We introduce the notion of (strongly) $n$-AIR-tilting modules, which is a high dimension version of support $\\tau$-tilting modules.","The relations between them and $n$-silting modules and $n$-quasi-tilting modules, as well as generalized two-term silting complexes, are investigated.","Our results particularly suggest a way to negate the rank question for silting complexes."],"url":"http://arxiv.org/abs/2402.17477v1","category":"math.RT"}
{"created":"2024-02-27 12:53:15","title":"Fraud Detection with Binding Global and Local Relational Interaction","abstract":"Graph Neural Network has been proved to be effective for fraud detection for its capability to encode node interaction and aggregate features in a holistic view. Recently, Transformer network with great sequence encoding ability, has also outperformed other GNN-based methods in literatures. However, both GNN-based and Transformer-based networks only encode one perspective of the whole graph, while GNN encodes global features and Transformer network encodes local ones. Furthermore, previous works ignored encoding global interaction features of the heterogeneous graph with separate networks, thus leading to suboptimal performance. In this work, we present a novel framework called Relation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds local and global features into a target node. The simple yet effective network applies a modified GAGA module where each transformer layer is followed by a cross-relation aggregation layer, to encode local embeddings and node interactions across different relations. Apart from the Transformer-based network, we further introduce a Relation-Aware GNN module to learn global embeddings, which is later merged into the local embeddings by an attention fusion module and a skip connection. Extensive experiments on two popular public datasets and an industrial dataset demonstrate that RAGFormer achieves the state-of-the-art performance. Substantial analysis experiments validate the effectiveness of each submodule of RAGFormer and its high efficiency in utilizing small-scale data and low hyper-parameter sensitivity.","sentences":["Graph Neural Network has been proved to be effective for fraud detection for its capability to encode node interaction and aggregate features in a holistic view.","Recently, Transformer network with great sequence encoding ability, has also outperformed other GNN-based methods in literatures.","However, both GNN-based and Transformer-based networks only encode one perspective of the whole graph, while GNN encodes global features and Transformer network encodes local ones.","Furthermore, previous works ignored encoding global interaction features of the heterogeneous graph with separate networks, thus leading to suboptimal performance.","In this work, we present a novel framework called Relation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds local and global features into a target node.","The simple yet effective network applies a modified GAGA module where each transformer layer is followed by a cross-relation aggregation layer, to encode local embeddings and node interactions across different relations.","Apart from the Transformer-based network, we further introduce a Relation-Aware GNN module to learn global embeddings, which is later merged into the local embeddings by an attention fusion module and a skip connection.","Extensive experiments on two popular public datasets and an industrial dataset demonstrate that RAGFormer achieves the state-of-the-art performance.","Substantial analysis experiments validate the effectiveness of each submodule of RAGFormer and its high efficiency in utilizing small-scale data and low hyper-parameter sensitivity."],"url":"http://arxiv.org/abs/2402.17472v1","category":"cs.LG"}
{"created":"2024-02-27 12:52:44","title":"Bit Distribution Study and Implementation of Spatial Quality Map in the JPEG-AI Standardization","abstract":"Currently, there is a high demand for neural network-based image compression codecs. These codecs employ non-linear transforms to create compact bit representations and facilitate faster coding speeds on devices compared to the hand-crafted transforms used in classical frameworks. The scientific and industrial communities are highly interested in these properties, leading to the standardization effort of JPEG-AI. The JPEG-AI verification model has been released and is currently under development for standardization. Utilizing neural networks, it can outperform the classic codec VVC intra by over 10% BD-rate operating at base operation point. Researchers attribute this success to the flexible bit distribution in the spatial domain, in contrast to VVC intra's anchor that is generated with a constant quality point. However, our study reveals that VVC intra displays a more adaptable bit distribution structure through the implementation of various block sizes. As a result of our observations, we have proposed a spatial bit allocation method to optimize the JPEG-AI verification model's bit distribution and enhance the visual quality. Furthermore, by applying the VVC bit distribution strategy, the objective performance of JPEG-AI verification mode can be further improved, resulting in a maximum gain of 0.45 dB in PSNR-Y.","sentences":["Currently, there is a high demand for neural network-based image compression codecs.","These codecs employ non-linear transforms to create compact bit representations and facilitate faster coding speeds on devices compared to the hand-crafted transforms used in classical frameworks.","The scientific and industrial communities are highly interested in these properties, leading to the standardization effort of JPEG-AI.","The JPEG-AI verification model has been released and is currently under development for standardization.","Utilizing neural networks, it can outperform the classic codec VVC intra by over 10% BD-rate operating at base operation point.","Researchers attribute this success to the flexible bit distribution in the spatial domain, in contrast to VVC intra's anchor that is generated with a constant quality point.","However, our study reveals that VVC intra displays a more adaptable bit distribution structure through the implementation of various block sizes.","As a result of our observations, we have proposed a spatial bit allocation method to optimize the JPEG-AI verification model's bit distribution and enhance the visual quality.","Furthermore, by applying the VVC bit distribution strategy, the objective performance of JPEG-AI verification mode can be further improved, resulting in a maximum gain of 0.45 dB in PSNR-Y."],"url":"http://arxiv.org/abs/2402.17470v1","category":"cs.CV"}
{"created":"2024-02-27 12:48:31","title":"Noether inequality for irregular threefolds of general type","abstract":"Let $X$ be a smooth irregular $3$-fold of general type over $\\mathbb{C}$. We prove that the optimal Noether inequality $$ \\mathrm{vol}(X) \\ge \\frac{4}{3}p_g(X) $$ holds if $p_g(X) \\ge 16$ or if $X$ has a Gorenstein minimal model. Moreover, when $X$ attains the equality and $p_g(X) \\ge 16$, its canonical model can be explicitly described.","sentences":["Let $X$ be a smooth irregular $3$-fold of general type over $\\mathbb{C}$. We prove that the optimal Noether inequality $$ \\mathrm{vol}(X) \\ge \\frac{4}{3}p_g(X) $$ holds if $p_g(X) \\ge 16$ or if $X$ has a Gorenstein minimal model.","Moreover, when $X$ attains the equality and $p_g(X) \\ge 16$, its canonical model can be explicitly described."],"url":"http://arxiv.org/abs/2402.17468v1","category":"math.AG"}
{"created":"2024-02-27 12:48:01","title":"Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: a Survey","abstract":"Several adaptations of Transformers models have been developed in various domains since its breakthrough in Natural Language Processing (NLP). This trend has spread into the field of Music Information Retrieval (MIR), including studies processing music data. However, the practice of leveraging NLP tools for symbolic music data is not novel in MIR. Music has been frequently compared to language, as they share several similarities, including sequential representations of text and music. These analogies are also reflected through similar tasks in MIR and NLP. This survey reviews NLP methods applied to symbolic music generation and information retrieval studies following two axes. We first propose an overview of representations of symbolic music adapted from natural language sequential representations. Such representations are designed by considering the specificities of symbolic music. These representations are then processed by models. Such models, possibly originally developed for text and adapted for symbolic music, are trained on various tasks. We describe these models, in particular deep learning models, through different prisms, highlighting music-specialized mechanisms. We finally present a discussion surrounding the effective use of NLP tools for symbolic music data. This includes technical issues regarding NLP methods and fundamental differences between text and music, which may open several doors for further research into more effectively adapting NLP tools to symbolic MIR.","sentences":["Several adaptations of Transformers models have been developed in various domains since its breakthrough in Natural Language Processing (NLP).","This trend has spread into the field of Music Information Retrieval (MIR), including studies processing music data.","However, the practice of leveraging NLP tools for symbolic music data is not novel in MIR.","Music has been frequently compared to language, as they share several similarities, including sequential representations of text and music.","These analogies are also reflected through similar tasks in MIR and NLP.","This survey reviews NLP methods applied to symbolic music generation and information retrieval studies following two axes.","We first propose an overview of representations of symbolic music adapted from natural language sequential representations.","Such representations are designed by considering the specificities of symbolic music.","These representations are then processed by models.","Such models, possibly originally developed for text and adapted for symbolic music, are trained on various tasks.","We describe these models, in particular deep learning models, through different prisms, highlighting music-specialized mechanisms.","We finally present a discussion surrounding the effective use of NLP tools for symbolic music data.","This includes technical issues regarding NLP methods and fundamental differences between text and music, which may open several doors for further research into more effectively adapting NLP tools to symbolic MIR."],"url":"http://arxiv.org/abs/2402.17467v1","category":"cs.IR"}
{"created":"2024-02-27 12:46:38","title":"Distributed Estimation and Control for LTI systems under Finite-Time Agreement","abstract":"This paper considers a strongly connected network of agents, each capable of partially observing and controlling a discrete-time linear time-invariant (LTI) system that is jointly observable and controllable. Additionally, agents collaborate to achieve a shared estimated state, computed as the average of their local state estimates. Recent studies suggest that increasing the number of average consensus steps between state estimation updates allows agents to choose from a wider range of state feedback controllers, thereby potentially enhancing control performance. However, such approaches require that agents know the input matrices of all other nodes, and the selection of control gains is, in general, centralized. Motivated by the limitations of such approaches, we propose a new technique where: (i) estimation and control gain design is fully distributed and finite-time, and (ii) agent coordination involves a finite-time exact average consensus algorithm, allowing arbitrary selection of estimation convergence rate despite the estimator's distributed nature. We verify our methodology's effectiveness using illustrative numerical simulations.","sentences":["This paper considers a strongly connected network of agents, each capable of partially observing and controlling a discrete-time linear time-invariant (LTI) system that is jointly observable and controllable.","Additionally, agents collaborate to achieve a shared estimated state, computed as the average of their local state estimates.","Recent studies suggest that increasing the number of average consensus steps between state estimation updates allows agents to choose from a wider range of state feedback controllers, thereby potentially enhancing control performance.","However, such approaches require that agents know the input matrices of all other nodes, and the selection of control gains is, in general, centralized.","Motivated by the limitations of such approaches, we propose a new technique where: (i) estimation and control gain design is fully distributed and finite-time, and (ii) agent coordination involves a finite-time exact average consensus algorithm, allowing arbitrary selection of estimation convergence rate despite the estimator's distributed nature.","We verify our methodology's effectiveness using illustrative numerical simulations."],"url":"http://arxiv.org/abs/2402.17466v1","category":"eess.SY"}
{"created":"2024-02-27 12:42:06","title":"Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing","abstract":"Generative 3D part assembly involves understanding part relationships and predicting their 6-DoF poses for assembling a realistic 3D shape. Prior work often focus on the geometry of individual parts, neglecting part-whole hierarchies of objects. Leveraging two key observations: 1) super-part poses provide strong hints about part poses, and 2) predicting super-part poses is easier due to fewer superparts, we propose a part-whole-hierarchy message passing network for efficient 3D part assembly. We first introduce super-parts by grouping geometrically similar parts without any semantic labels. Then we employ a part-whole hierarchical encoder, wherein a super-part encoder predicts latent super-part poses based on input parts. Subsequently, we transform the point cloud using the latent poses, feeding it to the part encoder for aggregating super-part information and reasoning about part relationships to predict all part poses. In training, only ground-truth part poses are required. During inference, the predicted latent poses of super-parts enhance interpretability. Experimental results on the PartNet dataset show that our method achieves state-of-the-art performance in part and connectivity accuracy and enables an interpretable hierarchical part assembly.","sentences":["Generative 3D part assembly involves understanding part relationships and predicting their 6-DoF poses for assembling a realistic 3D shape.","Prior work often focus on the geometry of individual parts, neglecting part-whole hierarchies of objects.","Leveraging two key observations: 1) super-part poses provide strong hints about part poses, and 2) predicting super-part poses is easier due to fewer superparts, we propose a part-whole-hierarchy message passing network for efficient 3D part assembly.","We first introduce super-parts by grouping geometrically similar parts without any semantic labels.","Then we employ a part-whole hierarchical encoder, wherein a super-part encoder predicts latent super-part poses based on input parts.","Subsequently, we transform the point cloud using the latent poses, feeding it to the part encoder for aggregating super-part information and reasoning about part relationships to predict all part poses.","In training, only ground-truth part poses are required.","During inference, the predicted latent poses of super-parts enhance interpretability.","Experimental results on the PartNet dataset show that our method achieves state-of-the-art performance in part and connectivity accuracy and enables an interpretable hierarchical part assembly."],"url":"http://arxiv.org/abs/2402.17464v1","category":"cs.CV"}
{"created":"2024-02-27 12:39:23","title":"Training-Free Long-Context Scaling of Large Language Models","abstract":"The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models. When compared with proprietary models, our training-free 70B model attains 94% of the performance of gpt-3.5-16k, indicating it is a viable open-source alternative. All code and data used in this work are released at \\url{https://github.com/HKUNLP/ChunkLlama}.","sentences":["The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length.","Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training.","By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention.","In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models.","When compared with proprietary models, our training-free 70B model attains 94% of the performance of gpt-3.5-16k, indicating it is a viable open-source alternative.","All code and data used in this work are released at \\url{https://github.com/HKUNLP/ChunkLlama}."],"url":"http://arxiv.org/abs/2402.17463v1","category":"cs.CL"}
{"created":"2024-02-27 12:33:50","title":"Room Temperature Spin Filtering and Quantum Transport with Transition Metal-Doped Silicon Quantum Dot","abstract":"Spin filtering is a fundamental operation in spintronics, enabling the generation and detection of spin-polarized carriers. Here, we proposed and theoretically demonstrated that a 3d transition metal (TM) doped silicon quantum dot (SiQD) is a suitable candidate for spin filter device at room temperature. Using density functional theory (DFT), we investigate the structure, electronic properties, and magnetic behavior of TM-SiQD. Our calculations demonstrate that Mn-doped SiQD exhibits the highest stability. The designed spin-filter device using Mn-doped SiQD shows a spin-filtering efficiency of 99.9% at 300K electrode temperature along with very high conductance. This remarkable efficiency positions it as a promising candidate for room-temperature spintronic devices.","sentences":["Spin filtering is a fundamental operation in spintronics, enabling the generation and detection of spin-polarized carriers.","Here, we proposed and theoretically demonstrated that a 3d transition metal (TM) doped silicon quantum dot (SiQD) is a suitable candidate for spin filter device at room temperature.","Using density functional theory (DFT), we investigate the structure, electronic properties, and magnetic behavior of TM-SiQD.","Our calculations demonstrate that Mn-doped SiQD exhibits the highest stability.","The designed spin-filter device using Mn-doped SiQD shows a spin-filtering efficiency of 99.9% at 300K electrode temperature along with very high conductance.","This remarkable efficiency positions it as a promising candidate for room-temperature spintronic devices."],"url":"http://arxiv.org/abs/2402.17461v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-27 12:32:42","title":"Enhancement of mechanical squeezing via feedback control","abstract":"We explore the generation of nonclassical mechanical states by combining continuous position measurement and feedback control. We find that feedback-induced spring softening can greatly enhance position squeezing. Conversely, even with a pure position measurement, we find that spring hardening can enable momentum squeezing. Beyond enhanced squeezing, we show that feedback also mitigates degradation introduced by background mechanical modes. Together, this significantly lowers the barrier to measurement-based preparation of nonclassical mechanical states at room temperature.","sentences":["We explore the generation of nonclassical mechanical states by combining continuous position measurement and feedback control.","We find that feedback-induced spring softening can greatly enhance position squeezing.","Conversely, even with a pure position measurement, we find that spring hardening can enable momentum squeezing.","Beyond enhanced squeezing, we show that feedback also mitigates degradation introduced by background mechanical modes.","Together, this significantly lowers the barrier to measurement-based preparation of nonclassical mechanical states at room temperature."],"url":"http://arxiv.org/abs/2402.17460v1","category":"quant-ph"}
{"created":"2024-02-27 12:30:17","title":"PureLottery: Fair and Bias-Resistant Leader Election with a Novel Single-Elimination Tournament Algorithm","abstract":"Leader Election (LE) is crucial in distributed systems and blockchain technology, ensuring one participant acts as the leader. Traditional LE methods often depend on distributed random number generation (RNG), facing issues like vulnerability to manipulation, lack of fairness, and the need for complex procedures such as verifiable delay functions (VDFs) and publicly-verifiable secret sharing (PVSS). This Bachelor's thesis presents a novel approach to randomized LE, leveraging a game-theoretic assumption that participants, aiming to be chosen as leaders, will naturally avoid actions that diminish their chances. This perspective simplifies LE by eliminating the need for decentralized RNG. Introducing PureLottery, inspired by single-elimination sports tournaments, this method offers a fair, bias-resistant, and efficient LE solution for blockchain environments. It operates on the principle of two participants competing in each match, rendering collusion efforts useless. PureLottery stands out for its low computational and communication complexity, suitable for smart contract implementation. It provides strong game-theoretic incentives for honesty and is robust against adversaries, ensuring no increase in election chances through dishonesty. The protocol guarantees that each honest player has at least a 1/n chance of winning, irrespective of adversary manipulation among the other n-1 participants. PureLottery can also address related problems like participant ranking, electing multiple leaders, and leader aversion, showcasing its versatility across various applications, including lotteries and blockchain protocols. An open-source implementation is made available for public use.","sentences":["Leader Election (LE) is crucial in distributed systems and blockchain technology, ensuring one participant acts as the leader.","Traditional LE methods often depend on distributed random number generation (RNG), facing issues like vulnerability to manipulation, lack of fairness, and the need for complex procedures such as verifiable delay functions (VDFs) and publicly-verifiable secret sharing (PVSS).","This Bachelor's thesis presents a novel approach to randomized LE, leveraging a game-theoretic assumption that participants, aiming to be chosen as leaders, will naturally avoid actions that diminish their chances.","This perspective simplifies LE by eliminating the need for decentralized RNG.","Introducing PureLottery, inspired by single-elimination sports tournaments, this method offers a fair, bias-resistant, and efficient LE solution for blockchain environments.","It operates on the principle of two participants competing in each match, rendering collusion efforts useless.","PureLottery stands out for its low computational and communication complexity, suitable for smart contract implementation.","It provides strong game-theoretic incentives for honesty and is robust against adversaries, ensuring no increase in election chances through dishonesty.","The protocol guarantees that each honest player has at least a 1/n chance of winning, irrespective of adversary manipulation among the other n-1 participants.","PureLottery can also address related problems like participant ranking, electing multiple leaders, and leader aversion, showcasing its versatility across various applications, including lotteries and blockchain protocols.","An open-source implementation is made available for public use."],"url":"http://arxiv.org/abs/2402.17459v1","category":"cs.DS"}
{"created":"2024-02-27 12:27:51","title":"A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education","abstract":"Cyberbullying harms teenagers' mental health, and teaching them upstanding intervention is crucial. Wizard-of-Oz studies show chatbots can scale up personalized and interactive cyberbullying education, but implementing such chatbots is a challenging and delicate task. We created a no-code chatbot design tool for K-12 teachers. Using large language models and prompt chaining, our tool allows teachers to prototype bespoke dialogue flows and chatbot utterances. In offering this tool, we explore teachers' distinctive needs when designing chatbots to assist their teaching, and how chatbot design tools might better support them. Our findings reveal that teachers welcome the tool enthusiastically. Moreover, they see themselves as playwrights guiding both the students' and the chatbot's behaviors, while allowing for some improvisation. Their goal is to enable students to rehearse both desirable and undesirable reactions to cyberbullying in a safe environment. We discuss the design opportunities LLM-Chains offer for empowering teachers and the research opportunities this work opens up.","sentences":["Cyberbullying harms teenagers' mental health, and teaching them upstanding intervention is crucial.","Wizard-of-Oz studies show chatbots can scale up personalized and interactive cyberbullying education, but implementing such chatbots is a challenging and delicate task.","We created a no-code chatbot design tool for K-12 teachers.","Using large language models and prompt chaining, our tool allows teachers to prototype bespoke dialogue flows and chatbot utterances.","In offering this tool, we explore teachers' distinctive needs when designing chatbots to assist their teaching, and how chatbot design tools might better support them.","Our findings reveal that teachers welcome the tool enthusiastically.","Moreover, they see themselves as playwrights guiding both the students' and the chatbot's behaviors, while allowing for some improvisation.","Their goal is to enable students to rehearse both desirable and undesirable reactions to cyberbullying in a safe environment.","We discuss the design opportunities LLM-Chains offer for empowering teachers and the research opportunities this work opens up."],"url":"http://arxiv.org/abs/2402.17456v1","category":"cs.HC"}
{"created":"2024-02-27 12:26:45","title":"Segment anything model for head and neck tumor segmentation with CT, PET and MRI multi-modality images","abstract":"Deep learning presents novel opportunities for the auto-segmentation of gross tumor volume (GTV) in head and neck cancer (HNC), yet fully automatic methods usually necessitate significant manual refinement. This study investigates the Segment Anything Model (SAM), recognized for requiring minimal human prompting and its zero-shot generalization ability across natural images. We specifically examine MedSAM, a version of SAM fine-tuned with large-scale public medical images. Despite its progress, the integration of multi-modality images (CT, PET, MRI) for effective GTV delineation remains a challenge. Focusing on SAM's application in HNC GTV segmentation, we assess its performance in both zero-shot and fine-tuned scenarios using single (CT-only) and fused multi-modality images. Our study demonstrates that fine-tuning SAM significantly enhances its segmentation accuracy, building upon the already effective zero-shot results achieved with bounding box prompts. These findings open a promising avenue for semi-automatic HNC GTV segmentation.","sentences":["Deep learning presents novel opportunities for the auto-segmentation of gross tumor volume (GTV) in head and neck cancer (HNC), yet fully automatic methods usually necessitate significant manual refinement.","This study investigates the Segment Anything Model (SAM), recognized for requiring minimal human prompting and its zero-shot generalization ability across natural images.","We specifically examine MedSAM, a version of SAM fine-tuned with large-scale public medical images.","Despite its progress, the integration of multi-modality images (CT, PET, MRI) for effective GTV delineation remains a challenge.","Focusing on SAM's application in HNC GTV segmentation, we assess its performance in both zero-shot and fine-tuned scenarios using single (CT-only) and fused multi-modality images.","Our study demonstrates that fine-tuning SAM significantly enhances its segmentation accuracy, building upon the already effective zero-shot results achieved with bounding box prompts.","These findings open a promising avenue for semi-automatic HNC GTV segmentation."],"url":"http://arxiv.org/abs/2402.17454v1","category":"physics.med-ph"}
{"created":"2024-02-27 12:26:07","title":"DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning","abstract":"In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success rate in the development stage, while attaining 36% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per run with GPT-4, respectively.","sentences":["In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models.","Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario.","To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR).","In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism.","Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs.","Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success rate in the development stage, while attaining 36% improvement on average one pass rate across alternative LLMs in the deployment stage.","In both stages, DS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per run with GPT-4, respectively."],"url":"http://arxiv.org/abs/2402.17453v1","category":"cs.LG"}
{"created":"2024-02-27 12:11:56","title":"Conformal Shield: A Novel Adversarial Attack Detection Framework for Automatic Modulation Classification","abstract":"Deep learning algorithms have become an essential component in the field of cognitive radio, especially playing a pivotal role in automatic modulation classification. However, Deep learning also present risks and vulnerabilities. Despite their outstanding classification performance, they exhibit fragility when confronted with meticulously crafted adversarial examples, posing potential risks to the reliability of modulation recognition results. Addressing this issue, this letter pioneers the development of an intelligent modulation classification framework based on conformal theory, named the Conformal Shield, aimed at detecting the presence of adversarial examples in unknown signals and assessing the reliability of recognition results. Utilizing conformal mapping from statistical learning theory, introduces a custom-designed Inconsistency Soft-solution Set, enabling multiple validity assessments of the recognition outcomes. Experimental results demonstrate that the Conformal Shield maintains robust detection performance against a variety of typical adversarial sample attacks in the received signals under different perturbation-to-signal power ratio conditions.","sentences":["Deep learning algorithms have become an essential component in the field of cognitive radio, especially playing a pivotal role in automatic modulation classification.","However, Deep learning also present risks and vulnerabilities.","Despite their outstanding classification performance, they exhibit fragility when confronted with meticulously crafted adversarial examples, posing potential risks to the reliability of modulation recognition results.","Addressing this issue, this letter pioneers the development of an intelligent modulation classification framework based on conformal theory, named the Conformal Shield, aimed at detecting the presence of adversarial examples in unknown signals and assessing the reliability of recognition results.","Utilizing conformal mapping from statistical learning theory, introduces a custom-designed Inconsistency Soft-solution Set, enabling multiple validity assessments of the recognition outcomes.","Experimental results demonstrate that the Conformal Shield maintains robust detection performance against a variety of typical adversarial sample attacks in the received signals under different perturbation-to-signal power ratio conditions."],"url":"http://arxiv.org/abs/2402.17450v1","category":"eess.SP"}
{"created":"2024-02-27 12:07:57","title":"Modeling effects of starspots on stellar magnetic cycles","abstract":"Observations show that faster-rotating stars tend to have stronger magnetic activity and shorter magnetic cycles. The cyclical magnetic activity of the Sun and stars is believed to be driven by the dynamo process. The success of the Babcock-Leighton (BL) dynamo in understanding the solar cycle suggests an important role that starspots could play in stellar magnetic cycles. We aim at extending the BL mechanism to solar-mass stars with various rotation rates and explore the effects of emergence properties of starspots in latitudes and tilt angles on stellar magnetic cycles. We adopt a kinematic BL-type dynamo model operating in the bulk of the convection zone. The profiles of the large-scale flow fields are from the mean-field hydrodynamical model for various rotators. The BL source term in the model is constructed based on the rotation dependence of starspots emergence. That is, faster rotators have starspots at higher latitudes with larger tilt angles.Faster rotators have poloidal flux appearing closer to about $\\pm55^\\circ$ latitudes, where the toroidal field generation efficiency is the strongest because of the strongest latitudinal differential rotation there. It takes a shorter time for faster rotators to transport the surface poloidal field from their emergence latitude to the $\\pm 55^\\circ$ latitudes of efficient $\\Omega$-effect thus shortening their magnetic cycles. The faster rotators operate in a more supercritical regime due to a stronger BL $\\alpha$-effect relating to the tilt angles, which leads to stronger saturated magnetic fields and a coupling of the poloidal field between two hemispheres more difficult. Thus the magnetic field parity shifts from the hemispherically asymmetric mixed mode to quadrupole, and further to dipole when a star spins down. The emergence of starspots plays an essential role in the large-scale stellar dynamo.","sentences":["Observations show that faster-rotating stars tend to have stronger magnetic activity and shorter magnetic cycles.","The cyclical magnetic activity of the Sun and stars is believed to be driven by the dynamo process.","The success of the Babcock-Leighton (BL) dynamo in understanding the solar cycle suggests an important role that starspots could play in stellar magnetic cycles.","We aim at extending the BL mechanism to solar-mass stars with various rotation rates and explore the effects of emergence properties of starspots in latitudes and tilt angles on stellar magnetic cycles.","We adopt a kinematic BL-type dynamo model operating in the bulk of the convection zone.","The profiles of the large-scale flow fields are from the mean-field hydrodynamical model for various rotators.","The BL source term in the model is constructed based on the rotation dependence of starspots emergence.","That is, faster rotators have starspots at higher latitudes with larger tilt angles.","Faster rotators have poloidal flux appearing closer to about $\\pm55^\\circ$ latitudes, where the toroidal field generation efficiency is the strongest because of the strongest latitudinal differential rotation there.","It takes a shorter time for faster rotators to transport the surface poloidal field from their emergence latitude to the $\\pm 55^\\circ$ latitudes of efficient $\\Omega$-effect thus shortening their magnetic cycles.","The faster rotators operate in a more supercritical regime due to a stronger BL $\\alpha$-effect relating to the tilt angles, which leads to stronger saturated magnetic fields and a coupling of the poloidal field between two hemispheres more difficult.","Thus the magnetic field parity shifts from the hemispherically asymmetric mixed mode to quadrupole, and further to dipole when a star spins down.","The emergence of starspots plays an essential role in the large-scale stellar dynamo."],"url":"http://arxiv.org/abs/2402.17449v1","category":"astro-ph.SR"}
{"created":"2024-02-27 12:03:56","title":"Deep Learning Based Named Entity Recognition Models for Recipes","abstract":"Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. Recipes are cultural capsules transmitted across generations via unstructured text. Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation. Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels. Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER. Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity to create the machine-annotated dataset. A thorough investigation of NER approaches on these three datasets involving statistical, fine-tuning of deep learning-based language models and few-shot prompting on large language models (LLMs) provides deep insights. We conclude that few-shot prompting on LLMs has abysmal performance, whereas the fine-tuned spaCy-transformer emerges as the best model with macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated, augmented, and machine-annotated datasets, respectively.","sentences":["Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability.","Recipes are cultural capsules transmitted across generations via unstructured text.","Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation.","Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels.","Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively.","Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER.","Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity to create the machine-annotated dataset.","A thorough investigation of NER approaches on these three datasets involving statistical, fine-tuning of deep learning-based language models and few-shot prompting on large language models (LLMs) provides deep insights.","We conclude that few-shot prompting on LLMs has abysmal performance, whereas the fine-tuned spaCy-transformer emerges as the best model with macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated, augmented, and machine-annotated datasets, respectively."],"url":"http://arxiv.org/abs/2402.17447v1","category":"cs.CL"}
{"created":"2024-02-27 12:03:38","title":"Generalized Ces\u00e0ro operator acting on Hilbert spaces of analytic functions","abstract":"Let $\\mathbb{D}$ denote the unit disc in $\\mathbb{C}$. We define the generalized Ces\\`aro operator as follows   $$   C_{\\omega}(f)(z)=\\int_0^1 f(tz)\\left(\\frac{1}{z}\\int_0^z B^{\\omega}_t(u)\\,du\\right)\\,\\omega(t)dt,$$   where $\\{B^{\\omega}_\\zeta\\}_{\\zeta\\in\\mathbb{D}}$ are the reproducing kernels of the Bergman space $A^2_\\omega$ induced by a radial weight $\\omega$ in the unit disc $\\mathbb{D}$. We study the action of the operator $C_{\\omega}$ on weighted Hardy spaces of analytic functions $\\mathcal{H}_{\\gamma}$, $\\gamma >0$ and on general weighted Bergman spaces $A^2_{\\mu}$.","sentences":["Let $\\mathbb{D}$ denote the unit disc in $\\mathbb{C}$. We define the generalized Ces\\`aro operator as follows   $$   C_{\\omega}(f)(z)=\\int_0^1 f(tz)\\left(\\frac{1}{z}\\int_0^z B^{\\omega}_t(u)\\,du\\right)\\,\\omega(t)dt,$$   where $\\{B^{\\omega}_\\zeta\\}_{\\zeta\\in\\mathbb{D}}$ are the reproducing kernels of the Bergman space $A^2_\\omega$ induced by a radial weight $\\omega$ in the unit disc $\\mathbb{D}$. We study the action of the operator $C_{\\omega}$ on weighted Hardy spaces of analytic functions $\\mathcal{H}_{\\gamma}$, $\\gamma >0$ and on general weighted Bergman spaces $A^2_{\\mu}$."],"url":"http://arxiv.org/abs/2402.17446v1","category":"math.CV"}
{"created":"2024-02-27 11:57:52","title":"On the spherical harmonic-Bessel band-limited functions and corresponding Slepian concentration problem in $\\mathbb{R}^d$","abstract":"We study the Slepian spectral concentration problem for the spherical harmonic-Bessel band-limited functions in general $\\mathbb{R}^d$, $d\\geq 2$. In particular, we prove the bimodal distribution of eigenvalues of the concentration operators and give the asymptotic characterization of the Shannon number given a linear relation between the Bessel bandwidth and spherical harmonic bandwidth.","sentences":["We study the Slepian spectral concentration problem for the spherical harmonic-Bessel band-limited functions in general $\\mathbb{R}^d$, $d\\geq 2$. In particular, we prove the bimodal distribution of eigenvalues of the concentration operators and give the asymptotic characterization of the Shannon number given a linear relation between the Bessel bandwidth and spherical harmonic bandwidth."],"url":"http://arxiv.org/abs/2402.17444v1","category":"math.FA"}
{"created":"2024-02-27 11:57:28","title":"Ansible Lightspeed: A Code Generation Service for IT Automation","abstract":"The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been released, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for IT automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Red Hat Ansible Lightspeed with IBM Watson Code Assistant, further referred to as Ansible Lightspeed, is an LLM-based service designed explicitly for natural language to Ansible code generation.   In this paper, we describe the design and implementation of the Ansible Lightspeed service and analyze feedback from thousands of real users. We examine diverse performance indicators, classified according to both immediate and extended utilization patterns along with user sentiments. The analysis shows that the user acceptance rate of Ansible Lightspeed suggestions is higher than comparable tools that are more general and not specific to a programming language. This remains true even after we use much more stringent criteria for what is considered an accepted model suggestion, discarding suggestions which were heavily edited after being accepted. The relatively high acceptance rate results in higher-than-expected user retention and generally positive user feedback. This paper provides insights on how a comparatively small, dedicated model performs on a domain-specific language and more importantly, how it is received by users.","sentences":["The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity.","Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs.","Although many such tools have been released, almost all of them focus on general-purpose programming languages.","Domain-specific languages, such as those crucial for IT automation, have not received much attention.","Ansible is one such YAML-based IT automation-specific language.","Red Hat Ansible Lightspeed with IBM Watson Code Assistant, further referred to as Ansible Lightspeed, is an LLM-based service designed explicitly for natural language to Ansible code generation.   ","In this paper, we describe the design and implementation of the Ansible Lightspeed service and analyze feedback from thousands of real users.","We examine diverse performance indicators, classified according to both immediate and extended utilization patterns along with user sentiments.","The analysis shows that the user acceptance rate of Ansible Lightspeed suggestions is higher than comparable tools that are more general and not specific to a programming language.","This remains true even after we use much more stringent criteria for what is considered an accepted model suggestion, discarding suggestions which were heavily edited after being accepted.","The relatively high acceptance rate results in higher-than-expected user retention and generally positive user feedback.","This paper provides insights on how a comparatively small, dedicated model performs on a domain-specific language and more importantly, how it is received by users."],"url":"http://arxiv.org/abs/2402.17442v1","category":"cs.SE"}
{"created":"2024-02-27 11:54:00","title":"Duality between Seiberg-Witten Theory and Black Hole Superradiance","abstract":"The newly established Seiberg-Witten (SW)/Quasinormal Modes (QNM) correspondence offers an efficient analytical approach to calculate the QNM frequencies, which was only available numerically before. This is based on the fact that both sides are characterized by Heun-type equations. We find that a similar duality exists between Seiberg-Witten theory and black hole superradiance, since the latter can also be linked to confluent Heun equation after proper transformation. Then a dictionary is constructed, with the superradiance frequencies written in terms of gauge parameters. Further by instanton counting, and taking care of the boundary conditions through connection formula, the relating frequencies are obtained analytically, which show consistency with known numerical results.","sentences":["The newly established Seiberg-Witten (SW)/Quasinormal Modes (QNM) correspondence offers an efficient analytical approach to calculate the QNM frequencies, which was only available numerically before.","This is based on the fact that both sides are characterized by Heun-type equations.","We find that a similar duality exists between Seiberg-Witten theory and black hole superradiance, since the latter can also be linked to confluent Heun equation after proper transformation.","Then a dictionary is constructed, with the superradiance frequencies written in terms of gauge parameters.","Further by instanton counting, and taking care of the boundary conditions through connection formula, the relating frequencies are obtained analytically, which show consistency with known numerical results."],"url":"http://arxiv.org/abs/2402.17441v1","category":"hep-th"}
{"created":"2024-02-27 11:52:49","title":"Principled Architecture-aware Scaling of Hyperparameters","abstract":"Training a high-quality deep neural network requires choosing suitable hyperparameters, which is a non-trivial and expensive process. Current works try to automatically optimize or design principles of hyperparameters, such that they can generalize to diverse unseen scenarios. However, most designs or optimization methods are agnostic to the choice of network structures, and thus largely ignore the impact of neural architectures on hyperparameters. In this work, we precisely characterize the dependence of initializations and maximal learning rates on the network architecture, which includes the network depth, width, convolutional kernel size, and connectivity patterns. By pursuing every parameter to be maximally updated with the same mean squared change in pre-activations, we can generalize our initialization and learning rates across MLPs (multi-layer perception) and CNNs (convolutional neural network) with sophisticated graph topologies. We verify our principles with comprehensive experiments. More importantly, our strategy further sheds light on advancing current benchmarks for architecture design. A fair comparison of AutoML algorithms requires accurate network rankings. However, we demonstrate that network rankings can be easily changed by better training networks in benchmarks with our architecture-aware learning rates and initialization.","sentences":["Training a high-quality deep neural network requires choosing suitable hyperparameters, which is a non-trivial and expensive process.","Current works try to automatically optimize or design principles of hyperparameters, such that they can generalize to diverse unseen scenarios.","However, most designs or optimization methods are agnostic to the choice of network structures, and thus largely ignore the impact of neural architectures on hyperparameters.","In this work, we precisely characterize the dependence of initializations and maximal learning rates on the network architecture, which includes the network depth, width, convolutional kernel size, and connectivity patterns.","By pursuing every parameter to be maximally updated with the same mean squared change in pre-activations, we can generalize our initialization and learning rates across MLPs (multi-layer perception) and CNNs (convolutional neural network) with sophisticated graph topologies.","We verify our principles with comprehensive experiments.","More importantly, our strategy further sheds light on advancing current benchmarks for architecture design.","A fair comparison of AutoML algorithms requires accurate network rankings.","However, we demonstrate that network rankings can be easily changed by better training networks in benchmarks with our architecture-aware learning rates and initialization."],"url":"http://arxiv.org/abs/2402.17440v1","category":"cs.LG"}
{"created":"2024-02-27 11:50:05","title":"Exploiting Emotion-Semantic Correlations for Empathetic Response Generation","abstract":"Empathetic response generation aims to generate empathetic responses by understanding the speaker's emotional feelings from the language of dialogue. Recent methods capture emotional words in the language of communicators and construct them as static vectors to perceive nuanced emotions. However, linguistic research has shown that emotional words in language are dynamic and have correlations with other grammar semantic roles, i.e., words with semantic meanings, in grammar. Previous methods overlook these two characteristics, which easily lead to misunderstandings of emotions and neglect of key semantics. To address this issue, we propose a dynamical Emotion-Semantic Correlation Model (ESCM) for empathetic dialogue generation tasks. ESCM constructs dynamic emotion-semantic vectors through the interaction of context and emotions. We introduce dependency trees to reflect the correlations between emotions and semantics. Based on dynamic emotion-semantic vectors and dependency trees, we propose a dynamic correlation graph convolutional network to guide the model in learning context meanings in dialogue and generating empathetic responses. Experimental results on the EMPATHETIC-DIALOGUES dataset show that ESCM understands semantics and emotions more accurately and expresses fluent and informative empathetic responses. Our analysis results also indicate that the correlations between emotions and semantics are frequently used in dialogues, which is of great significance for empathetic perception and expression.","sentences":["Empathetic response generation aims to generate empathetic responses by understanding the speaker's emotional feelings from the language of dialogue.","Recent methods capture emotional words in the language of communicators and construct them as static vectors to perceive nuanced emotions.","However, linguistic research has shown that emotional words in language are dynamic and have correlations with other grammar semantic roles, i.e., words with semantic meanings, in grammar.","Previous methods overlook these two characteristics, which easily lead to misunderstandings of emotions and neglect of key semantics.","To address this issue, we propose a dynamical Emotion-Semantic Correlation Model (ESCM) for empathetic dialogue generation tasks.","ESCM constructs dynamic emotion-semantic vectors through the interaction of context and emotions.","We introduce dependency trees to reflect the correlations between emotions and semantics.","Based on dynamic emotion-semantic vectors and dependency trees, we propose a dynamic correlation graph convolutional network to guide the model in learning context meanings in dialogue and generating empathetic responses.","Experimental results on the EMPATHETIC-DIALOGUES dataset show that ESCM understands semantics and emotions more accurately and expresses fluent and informative empathetic responses.","Our analysis results also indicate that the correlations between emotions and semantics are frequently used in dialogues, which is of great significance for empathetic perception and expression."],"url":"http://arxiv.org/abs/2402.17437v1","category":"cs.CL"}
{"created":"2024-02-27 11:47:46","title":"Wykorzystanie rekonfigurowalnych matryc antenowych wraz z informacj\u0105 kontekstow\u0105","abstract":"Reconfigurable intelligent surfaces can be successfully used to control the radio environment. Simple control of the reflection angle of the signal from the surface allows maximization or minimization of the received power in specific places. The paper presents simulations where it is possible to receive a signal in a place where it was not possible, to detect the occupancy of the spectrum in a place where the sensor was unable to make correct detection or to minimize interference in a specific receiver.","sentences":["Reconfigurable intelligent surfaces can be successfully used to control the radio environment.","Simple control of the reflection angle of the signal from the surface allows maximization or minimization of the received power in specific places.","The paper presents simulations where it is possible to receive a signal in a place where it was not possible, to detect the occupancy of the spectrum in a place where the sensor was unable to make correct detection or to minimize interference in a specific receiver."],"url":"http://arxiv.org/abs/2402.17436v1","category":"cs.NI"}
{"created":"2024-02-27 11:43:41","title":"The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns","abstract":"Artificial intelligence is continuously seeking novel challenges and benchmarks to effectively measure performance and to advance the state-of-the-art. In this paper we introduce KANDY, a benchmarking framework that can be used to generate a variety of learning and reasoning tasks inspired by Kandinsky patterns. By creating curricula of binary classification tasks with increasing complexity and with sparse supervisions, KANDY can be used to implement benchmarks for continual and semi-supervised learning, with a specific focus on symbol compositionality. Classification rules are also provided in the ground truth to enable analysis of interpretable solutions. Together with the benchmark generation pipeline, we release two curricula, an easier and a harder one, that we propose as new challenges for the research community. With a thorough experimental evaluation, we show how both state-of-the-art neural models and purely symbolic approaches struggle with solving most of the tasks, thus calling for the application of advanced neuro-symbolic methods trained over time.","sentences":["Artificial intelligence is continuously seeking novel challenges and benchmarks to effectively measure performance and to advance the state-of-the-art.","In this paper we introduce KANDY, a benchmarking framework that can be used to generate a variety of learning and reasoning tasks inspired by Kandinsky patterns.","By creating curricula of binary classification tasks with increasing complexity and with sparse supervisions, KANDY can be used to implement benchmarks for continual and semi-supervised learning, with a specific focus on symbol compositionality.","Classification rules are also provided in the ground truth to enable analysis of interpretable solutions.","Together with the benchmark generation pipeline, we release two curricula, an easier and a harder one, that we propose as new challenges for the research community.","With a thorough experimental evaluation, we show how both state-of-the-art neural models and purely symbolic approaches struggle with solving most of the tasks, thus calling for the application of advanced neuro-symbolic methods trained over time."],"url":"http://arxiv.org/abs/2402.17431v1","category":"cs.AI"}
{"created":"2024-02-27 11:42:18","title":"Field equations and Noether potentials for higher-order theories of gravity with Lagrangians involving $\\Box^i R$, $\\Box^i R_{\u03bc\u03bd}$ and $\\Box^i R_{\u03bc\u03bd\u03c1\u03c3}$","abstract":"In this paper, we aim to perform a systematical investigation on the field equations and Noether potentials for the higher-order gravity theories endowed with Lagrangians depending on the metric and the Riemann curvature tensor, together with $i$th ($i=1,2,\\cdot\\cdot\\cdot$) powers of the Beltrami-d'Alembertian operator $\\Box$ acting on the latter. We start with a detailed derivation of the field equations and the Noether potential corresponding to the Lagrangian $\\sqrt{-g}L_R(R,\\Box R,\\cdot\\cdot\\cdot,\\Box^m R)$ through the direct variation of the Lagrangian and a method based upon the conserved current. Next the parallel analysis is extended to a more generic Lagrangian $\\sqrt{-g}L_{\\text{Ric}}(g^{\\mu\\nu}, R_{\\mu\\nu},\\Box R_{\\mu\\nu}, \\cdot\\cdot\\cdot,\\Box^m R_{\\mu\\nu})$, as well as to the generalization of the Lagrangian $\\sqrt{-g}L_{\\text{Ric}}$, which depends on the metric $g^{\\mu\\nu}$, the Riemann tensor $R_{\\mu\\nu\\rho\\sigma}$ and $\\Box^i R_{\\mu\\nu\\rho\\sigma}$s. Finally, all the results associated to the three types of Lagrangians are extended to the Lagrangian relying on an arbitrary tensor and the variables via $\\Box^i$ acting on such a tensor. In particular, we take into consideration of equations of motion and Noether potentials for nonlocal gravity models. For Lagrangians involving the variables $\\Box^i R$, $\\Box^i R_{\\mu\\nu}$ and $\\Box^i R_{\\mu\\nu\\rho\\sigma}$, our investigation provides their concrete Noether potentials and the field equations without the derivative of the Lagrangian density with respect to the metric. Besides, the Iyer-Wald potentials associated to these Lagrangians are also presented.","sentences":["In this paper, we aim to perform a systematical investigation on the field equations and Noether potentials for the higher-order gravity theories endowed with Lagrangians depending on the metric and the Riemann curvature tensor, together with $i$th ($i=1,2,\\cdot\\cdot\\cdot$) powers of the Beltrami-d'Alembertian operator $\\Box$ acting on the latter.","We start with a detailed derivation of the field equations and the Noether potential corresponding to the Lagrangian $\\sqrt{-g}L_R(R,\\Box R,\\cdot\\cdot\\cdot,\\Box^m R)$ through the direct variation of the Lagrangian and a method based upon the conserved current.","Next the parallel analysis is extended to a more generic Lagrangian $\\sqrt{-g}L_{\\text{Ric}}(g^{\\mu\\nu}, R_{\\mu\\nu},\\Box R_{\\mu\\nu}, \\cdot\\cdot\\cdot,\\Box^m R_{\\mu\\nu})$, as well as to the generalization of the Lagrangian $\\sqrt{-g}L_{\\text{Ric}}$, which depends on the metric $g^{\\mu\\nu}$, the Riemann tensor $R_{\\mu\\nu\\rho\\sigma}$ and $\\Box^i R_{\\mu\\nu\\rho\\sigma}$s.","Finally, all the results associated to the three types of Lagrangians are extended to the Lagrangian relying on an arbitrary tensor and the variables via $\\Box^i$ acting on such a tensor.","In particular, we take into consideration of equations of motion and Noether potentials for nonlocal gravity models.","For Lagrangians involving the variables $\\Box^i R$, $\\Box^i R_{\\mu\\nu}$ and $\\Box^i R_{\\mu\\nu\\rho\\sigma}$, our investigation provides their concrete Noether potentials and the field equations without the derivative of the Lagrangian density with respect to the metric.","Besides, the Iyer-Wald potentials associated to these Lagrangians are also presented."],"url":"http://arxiv.org/abs/2402.17429v1","category":"gr-qc"}
{"created":"2024-02-27 11:35:47","title":"Skyrmion Generation in a Plasmonic Nanoantenna through the Inverse Faraday Effect","abstract":"Skyrmions are topological structures characterized by a winding vectorial configuration that provides a quantized topological charge. In magnetic materials, skyrmions are localized spin textures that exhibit unique stability and mobility properties, making them highly relevant to the burgeoning field of spintronics. In optics, these structures open new frontiers in manipulating and controlling light at the nanoscale. The convergence of optics and magnetics holds therefore immense potential for manipulating magnetic processes at ultrafast timescales. Here, we explore the possibility of generating skyrmionic topological structures within the magnetic field induced by the inverse Faraday effect in a plasmonic nanostructure. Our investigation reveals that a gold nanoring, featuring a dark mode, can generate counter-propagating photocurrents between its inner and outer segments, thereby enabling the magnetization of gold and supporting a skyrmionic vectorial distribution. We elucidate that these photocurrents arise from the localized control of light polarization, facilitating their counter-propagative motion. The generation of skyrmions through the inverse Faraday effect at the nanoscale presents a pathway towards directly integrating this topology into magnetic layers. This advancement holds promise for ultrafast timescales, offering direct applications in ultrafast data writing and processing.","sentences":["Skyrmions are topological structures characterized by a winding vectorial configuration that provides a quantized topological charge.","In magnetic materials, skyrmions are localized spin textures that exhibit unique stability and mobility properties, making them highly relevant to the burgeoning field of spintronics.","In optics, these structures open new frontiers in manipulating and controlling light at the nanoscale.","The convergence of optics and magnetics holds therefore immense potential for manipulating magnetic processes at ultrafast timescales.","Here, we explore the possibility of generating skyrmionic topological structures within the magnetic field induced by the inverse Faraday effect in a plasmonic nanostructure.","Our investigation reveals that a gold nanoring, featuring a dark mode, can generate counter-propagating photocurrents between its inner and outer segments, thereby enabling the magnetization of gold and supporting a skyrmionic vectorial distribution.","We elucidate that these photocurrents arise from the localized control of light polarization, facilitating their counter-propagative motion.","The generation of skyrmions through the inverse Faraday effect at the nanoscale presents a pathway towards directly integrating this topology into magnetic layers.","This advancement holds promise for ultrafast timescales, offering direct applications in ultrafast data writing and processing."],"url":"http://arxiv.org/abs/2402.17426v1","category":"physics.optics"}
{"created":"2024-02-27 11:34:51","title":"Semi-parametric goodness-of-fit testing for INAR models","abstract":"Among the various models designed for dependent count data, integer-valued autoregressive (INAR) processes enjoy great popularity. Typically, statistical inference for INAR models uses asymptotic theory that relies on rather stringent (parametric) assumptions on the innovations such as Poisson or negative binomial distributions. In this paper, we present a novel semi-parametric goodness-of-fit test tailored for the INAR model class. Relying on the INAR-specific shape of the joint probability generating function, our approach allows for model validation of INAR models without specifying the (family of the) innovation distribution. We derive the limiting null distribution of our proposed test statistic, prove consistency under fixed alternatives and discuss its asymptotic behavior under local alternatives. By manifold Monte Carlo simulations, we illustrate the overall good performance of our testing procedure in terms of power and size properties. In particular, it turns out that the power can be considerably improved by using higher-order test statistics. We conclude the article with the application on three real-world economic data sets.","sentences":["Among the various models designed for dependent count data, integer-valued autoregressive (INAR) processes enjoy great popularity.","Typically, statistical inference for INAR models uses asymptotic theory that relies on rather stringent (parametric) assumptions on the innovations such as Poisson or negative binomial distributions.","In this paper, we present a novel semi-parametric goodness-of-fit test tailored for the INAR model class.","Relying on the INAR-specific shape of the joint probability generating function, our approach allows for model validation of INAR models without specifying the (family of the) innovation distribution.","We derive the limiting null distribution of our proposed test statistic, prove consistency under fixed alternatives and discuss its asymptotic behavior under local alternatives.","By manifold Monte Carlo simulations, we illustrate the overall good performance of our testing procedure in terms of power and size properties.","In particular, it turns out that the power can be considerably improved by using higher-order test statistics.","We conclude the article with the application on three real-world economic data sets."],"url":"http://arxiv.org/abs/2402.17425v1","category":"stat.ME"}
{"created":"2024-02-27 11:32:14","title":"Reinforced In-Context Black-Box Optimization","abstract":"Black-Box Optimization (BBO) has found successful applications in many fields of science and engineering. Recently, there has been a growing interest in meta-learning particular components of BBO algorithms to speed up optimization and get rid of tedious hand-crafted heuristics. As an extension, learning the entire algorithm from data requires the least labor from experts and can provide the most flexibility. In this paper, we propose RIBBO, a method to reinforce-learn a BBO algorithm from offline data in an end-to-end fashion. RIBBO employs expressive sequence models to learn the optimization histories produced by multiple behavior algorithms and tasks, leveraging the in-context learning ability of large models to extract task information and make decisions accordingly. Central to our method is to augment the optimization histories with regret-to-go tokens, which are designed to represent the performance of an algorithm based on cumulative regret of the histories. The integration of regret-to-go tokens enables RIBBO to automatically generate sequences of query points that satisfy the user-desired regret, which is verified by its universally good empirical performance on diverse problems, including BBOB functions, hyper-parameter optimization and robot control problems.","sentences":["Black-Box Optimization (BBO) has found successful applications in many fields of science and engineering.","Recently, there has been a growing interest in meta-learning particular components of BBO algorithms to speed up optimization and get rid of tedious hand-crafted heuristics.","As an extension, learning the entire algorithm from data requires the least labor from experts and can provide the most flexibility.","In this paper, we propose RIBBO, a method to reinforce-learn a BBO algorithm from offline data in an end-to-end fashion.","RIBBO employs expressive sequence models to learn the optimization histories produced by multiple behavior algorithms and tasks, leveraging the in-context learning ability of large models to extract task information and make decisions accordingly.","Central to our method is to augment the optimization histories with regret-to-go tokens, which are designed to represent the performance of an algorithm based on cumulative regret of the histories.","The integration of regret-to-go tokens enables RIBBO to automatically generate sequences of query points that satisfy the user-desired regret, which is verified by its universally good empirical performance on diverse problems, including BBOB functions, hyper-parameter optimization and robot control problems."],"url":"http://arxiv.org/abs/2402.17423v1","category":"cs.LG"}
{"created":"2024-02-27 11:31:40","title":"Symmetry restoration for TDiff scalar fields","abstract":"We explore the idea of restoring the full diffeomorphism (Diff) invariance in theories with only transverse diffeomorphisms (TDiff) by the introduction of additional fields. In particular, we consider in detail the case of a TDiff invariant scalar field and how Diff symmetry can be restored by introducing an additional vector field. We reobtain the corresponding dynamics and energy-momentum tensor from the covariantized action and analyze the potential and kinetic domination regimes. For the former, the theory describes a cosmological constant-type behaviour, while for the latter we show that the theory can describe an adiabatic perfect fluid whose equation of state and speed of sound is obtained in a straightforward way.","sentences":["We explore the idea of restoring the full diffeomorphism (Diff) invariance in theories with only transverse diffeomorphisms (TDiff) by the introduction of additional fields.","In particular, we consider in detail the case of a TDiff invariant scalar field and how Diff symmetry can be restored by introducing an additional vector field.","We reobtain the corresponding dynamics and energy-momentum tensor from the covariantized action and analyze the potential and kinetic domination regimes.","For the former, the theory describes a cosmological constant-type behaviour, while for the latter we show that the theory can describe an adiabatic perfect fluid whose equation of state and speed of sound is obtained in a straightforward way."],"url":"http://arxiv.org/abs/2402.17422v1","category":"gr-qc"}
{"created":"2024-02-27 11:23:39","title":"PANDAS: Prototype-based Novel Class Discovery and Detection","abstract":"Object detectors are typically trained once and for all on a fixed set of classes. However, this closed-world assumption is unrealistic in practice, as new classes will inevitably emerge after the detector is deployed in the wild. In this work, we look at ways to extend a detector trained for a set of base classes so it can i) spot the presence of novel classes, and ii) automatically enrich its repertoire to be able to detect those newly discovered classes together with the base ones. We propose PANDAS, a method for novel class discovery and detection. It discovers clusters representing novel classes from unlabeled data, and represents old and new classes with prototypes. During inference, a distance-based classifier uses these prototypes to assign a label to each detected object instance. The simplicity of our method makes it widely applicable. We experimentally demonstrate the effectiveness of PANDAS on the VOC 2012 and COCO-to-LVIS benchmarks. It performs favorably against the state of the art for this task while being computationally more affordable.","sentences":["Object detectors are typically trained once and for all on a fixed set of classes.","However, this closed-world assumption is unrealistic in practice, as new classes will inevitably emerge after the detector is deployed in the wild.","In this work, we look at ways to extend a detector trained for a set of base classes so it can i) spot the presence of novel classes, and ii) automatically enrich its repertoire to be able to detect those newly discovered classes together with the base ones.","We propose PANDAS, a method for novel class discovery and detection.","It discovers clusters representing novel classes from unlabeled data, and represents old and new classes with prototypes.","During inference, a distance-based classifier uses these prototypes to assign a label to each detected object instance.","The simplicity of our method makes it widely applicable.","We experimentally demonstrate the effectiveness of PANDAS on the VOC 2012 and COCO-to-LVIS benchmarks.","It performs favorably against the state of the art for this task while being computationally more affordable."],"url":"http://arxiv.org/abs/2402.17420v1","category":"cs.CV"}
{"created":"2024-02-27 11:05:34","title":"DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Model","abstract":"In the realm of subject-driven text-to-image (T2I) generative models, recent developments like DreamBooth and BLIP-Diffusion have led to impressive results yet encounter limitations due to their intensive fine-tuning demands and substantial parameter requirements. While the low-rank adaptation (LoRA) module within DreamBooth offers a reduction in trainable parameters, it introduces a pronounced sensitivity to hyperparameters, leading to a compromise between parameter efficiency and the quality of T2I personalized image synthesis. Addressing these constraints, we introduce \\textbf{\\textit{DiffuseKronA}}, a novel Kronecker product-based adaptation module that not only significantly reduces the parameter count by 35\\% and 99.947\\% compared to LoRA-DreamBooth and the original DreamBooth, respectively, but also enhances the quality of image synthesis. Crucially, \\textit{DiffuseKronA} mitigates the issue of hyperparameter sensitivity, delivering consistent high-quality generations across a wide range of hyperparameters, thereby diminishing the necessity for extensive fine-tuning. Furthermore, a more controllable decomposition makes \\textit{DiffuseKronA} more interpretable and even can achieve up to a 50\\% reduction with results comparable to LoRA-Dreambooth. Evaluated against diverse and complex input images and text prompts, \\textit{DiffuseKronA} consistently outperforms existing models, producing diverse images of higher quality with improved fidelity and a more accurate color distribution of objects, all the while upholding exceptional parameter efficiency, thus presenting a substantial advancement in the field of T2I generative modeling. Our project page, consisting of links to the code, and pre-trained checkpoints, is available at \\href{https://diffusekrona.github.io/}{https://diffusekrona.github.io/}.","sentences":["In the realm of subject-driven text-to-image (T2I) generative models, recent developments like DreamBooth and BLIP-Diffusion have led to impressive results yet encounter limitations due to their intensive fine-tuning demands and substantial parameter requirements.","While the low-rank adaptation (LoRA) module within DreamBooth offers a reduction in trainable parameters, it introduces a pronounced sensitivity to hyperparameters, leading to a compromise between parameter efficiency and the quality of T2I personalized image synthesis.","Addressing these constraints, we introduce \\textbf{\\textit{DiffuseKronA}}, a novel Kronecker product-based adaptation module that not only significantly reduces the parameter count by 35\\% and 99.947\\% compared to LoRA-DreamBooth and the original DreamBooth, respectively, but also enhances the quality of image synthesis.","Crucially, \\textit{DiffuseKronA} mitigates the issue of hyperparameter sensitivity, delivering consistent high-quality generations across a wide range of hyperparameters, thereby diminishing the necessity for extensive fine-tuning.","Furthermore, a more controllable decomposition makes \\textit{DiffuseKronA} more interpretable and even can achieve up to a 50\\% reduction with results comparable to LoRA-Dreambooth.","Evaluated against diverse and complex input images and text prompts, \\textit{DiffuseKronA} consistently outperforms existing models, producing diverse images of higher quality with improved fidelity and a more accurate color distribution of objects, all the while upholding exceptional parameter efficiency, thus presenting a substantial advancement in the field of T2I generative modeling.","Our project page, consisting of links to the code, and pre-trained checkpoints, is available at \\href{https://diffusekrona.github.io/}{https://diffusekrona.github.io/}."],"url":"http://arxiv.org/abs/2402.17412v1","category":"cs.CV"}
{"created":"2024-02-27 11:01:58","title":"A novel image space formalism of Fourier domain interpolation neural networks for noise propagation analysis","abstract":"Purpose: To develop an image space formalism of multi-layer convolutional neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions and analytically estimate noise propagation during CNN inference. Theory and Methods: Nonlinear activations in the Fourier domain (also known as k-space) using complex-valued Rectifier Linear Units are expressed as elementwise multiplication with activation masks. This operation is transformed into a convolution in the image space. After network training in k-space, this approach provides an algebraic expression for the derivative of the reconstructed image with respect to the aliased coil images, which serve as the input tensors to the network in the image space. This allows the variance in the network inference to be estimated analytically and to be used to describe noise characteristics. Monte-Carlo simulations and numerical approaches based on auto-differentiation were used for validation. The framework was tested on retrospectively undersampled invivo brain images. Results: Inferences conducted in the image domain are quasi-identical to inferences in the k-space, underlined by corresponding quantitative metrics. Noise variance maps obtained from the analytical expression correspond with those obtained via Monte-Carlo simulations, as well as via an auto-differentiation approach. The noise resilience is well characterized, as in the case of classical Parallel Imaging. Komolgorov-Smirnov tests demonstrate Gaussian distributions of voxel magnitudes in variance maps obtained via Monte-Carlo simulations. Conclusion: The quasi-equivalent image space formalism for neural networks for k-space interpolation enables fast and accurate description of the noise characteristics during CNN inference, analogous to geometry-factor maps in traditional parallel imaging methods.","sentences":["Purpose: To develop an image space formalism of multi-layer convolutional neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions and analytically estimate noise propagation during CNN inference.","Theory and Methods: Nonlinear activations in the Fourier domain (also known as k-space) using complex-valued Rectifier Linear Units are expressed as elementwise multiplication with activation masks.","This operation is transformed into a convolution in the image space.","After network training in k-space, this approach provides an algebraic expression for the derivative of the reconstructed image with respect to the aliased coil images, which serve as the input tensors to the network in the image space.","This allows the variance in the network inference to be estimated analytically and to be used to describe noise characteristics.","Monte-Carlo simulations and numerical approaches based on auto-differentiation were used for validation.","The framework was tested on retrospectively undersampled invivo brain images.","Results:","Inferences conducted in the image domain are quasi-identical to inferences in the k-space, underlined by corresponding quantitative metrics.","Noise variance maps obtained from the analytical expression correspond with those obtained via Monte-Carlo simulations, as well as via an auto-differentiation approach.","The noise resilience is well characterized, as in the case of classical Parallel Imaging.","Komolgorov-Smirnov tests demonstrate Gaussian distributions of voxel magnitudes in variance maps obtained via Monte-Carlo simulations.","Conclusion: The quasi-equivalent image space formalism for neural networks for k-space interpolation enables fast and accurate description of the noise characteristics during CNN inference, analogous to geometry-factor maps in traditional parallel imaging methods."],"url":"http://arxiv.org/abs/2402.17410v1","category":"cs.CV"}
{"created":"2024-02-27 11:01:18","title":"Using Programmable Drone in Educational Projects and Competitions","abstract":"The mainstream of educational robotics platforms orbits the various versions of versatile robotics sets and kits, while interesting outliers add new opportunities and extend the possible learning situations. Examples of such are reconfigurable robots, rolling sphere robots, humanoids, swimming, or underwater robots. Another kind within this category are flying drones. While remotely controlled drones were a very attractive target for hobby model makers for quite a long time already, they were seldom used in educational scenarios as robots that are programmed by children to perform various simple tasks. A milestone was reached with the introduction of the educational drone Tello, which can be programmed even in Scratch, or some general-purpose languages such as Node.js or Python. The programs can even have access to the robot sensors that are used by the underlying layers of the controller. In addition, they have the option to acquire images from the drone camera and perform actions based on processing the frames applying computer vision algorithms. We have been using this drone in an educational robotics competition for three years without camera, and after our students have developed several successful projects that utilized a camera, we prepared a new competition challenge that requires the use of the camera. In the article, we summarize related efforts and our experiences with educational drones, and their use in the student projects and competition.","sentences":["The mainstream of educational robotics platforms orbits the various versions of versatile robotics sets and kits, while interesting outliers add new opportunities and extend the possible learning situations.","Examples of such are reconfigurable robots, rolling sphere robots, humanoids, swimming, or underwater robots.","Another kind within this category are flying drones.","While remotely controlled drones were a very attractive target for hobby model makers for quite a long time already, they were seldom used in educational scenarios as robots that are programmed by children to perform various simple tasks.","A milestone was reached with the introduction of the educational drone Tello, which can be programmed even in Scratch, or some general-purpose languages such as Node.js or Python.","The programs can even have access to the robot sensors that are used by the underlying layers of the controller.","In addition, they have the option to acquire images from the drone camera and perform actions based on processing the frames applying computer vision algorithms.","We have been using this drone in an educational robotics competition for three years without camera, and after our students have developed several successful projects that utilized a camera, we prepared a new competition challenge that requires the use of the camera.","In the article, we summarize related efforts and our experiences with educational drones, and their use in the student projects and competition."],"url":"http://arxiv.org/abs/2402.17409v1","category":"cs.RO"}
{"created":"2024-02-27 10:57:23","title":"High-rate Generation and State Tomography of Non-Gaussian Quantum States for Ultra-fast Clock Frequency Quantum Processors","abstract":"Quantum information processors greatly benefit from high clock frequency to fully harnessing the quantum advantages before they get washed out by the decoherence. In this pursuit, all-optical systems offer unique advantages due to their inherent 100 THz carrier frequency, permitting one to develop THz clock frequency processors. In practice, the bandwidth of the quantum light sources and the measurement devices has been limited to the MHz range and the generation rate of nonclassical states to kHz order -- a tiny fraction of what can be achieved. In this work, we go beyond this limitation by utilizing optical parametric amplifier (OPA) as a squeezed-light source and optical phase-sensitive amplifiers (PSA) to realize high-rate generation of broadband non-Gaussian states and their quantum tomography. Our state generation and measurement system consists of a 6-THz squeezed-light source, a 6-THz PSA, and a 66-GHz homodyne detector. With this system, we have successfully demonstrated non-Gaussian state generation at a 0.9 MHz rate -- almost three orders of magnitude higher than the current state-of-the-art experiments -- with a sub-nanosecond wave packet using continuous-wave laser. The performance is constrained only by the superconducting detector's jitter which currently limits the usable bandwidth of the squeezed light to 1 GHz, rather than the optical and electronic systems. Therefore, if we can overcome the limitation of the timing jitter of superconducting detector, non-Gaussian state generation and detection at GHz rate, or even THz rate, for optical quantum processors might be possible with OPAs.","sentences":["Quantum information processors greatly benefit from high clock frequency to fully harnessing the quantum advantages before they get washed out by the decoherence.","In this pursuit, all-optical systems offer unique advantages due to their inherent 100 THz carrier frequency, permitting one to develop THz clock frequency processors.","In practice, the bandwidth of the quantum light sources and the measurement devices has been limited to the MHz range and the generation rate of nonclassical states to kHz order -- a tiny fraction of what can be achieved.","In this work, we go beyond this limitation by utilizing optical parametric amplifier (OPA) as a squeezed-light source and optical phase-sensitive amplifiers (PSA) to realize high-rate generation of broadband non-Gaussian states and their quantum tomography.","Our state generation and measurement system consists of a 6-THz squeezed-light source, a 6-THz PSA, and a 66-GHz homodyne detector.","With this system, we have successfully demonstrated non-Gaussian state generation at a 0.9 MHz rate -- almost three orders of magnitude higher than the current state-of-the-art experiments -- with a sub-nanosecond wave packet using continuous-wave laser.","The performance is constrained only by the superconducting detector's jitter which currently limits the usable bandwidth of the squeezed light to 1 GHz, rather than the optical and electronic systems.","Therefore, if we can overcome the limitation of the timing jitter of superconducting detector, non-Gaussian state generation and detection at GHz rate, or even THz rate, for optical quantum processors might be possible with OPAs."],"url":"http://arxiv.org/abs/2402.17408v1","category":"quant-ph"}
{"created":"2024-02-27 10:57:07","title":"A Neural Rewriting System to Solve Algorithmic Problems","abstract":"Modern neural network architectures still struggle to learn algorithmic procedures that require to systematically apply compositional rules to solve out-of-distribution problem instances. In this work, we propose an original approach to learn algorithmic tasks inspired by rewriting systems, a classic framework in symbolic artificial intelligence. We show that a rewriting system can be implemented as a neural architecture composed by specialized modules: the Selector identifies the target sub-expression to process, the Solver simplifies the sub-expression by computing the corresponding result, and the Combiner produces a new version of the original expression by replacing the sub-expression with the solution provided. We evaluate our model on three types of algorithmic tasks that require simplifying symbolic formulas involving lists, arithmetic, and algebraic expressions. We test the extrapolation capabilities of the proposed architecture using formulas involving a higher number of operands and nesting levels than those seen during training, and we benchmark its performance against the Neural Data Router, a recent model specialized for systematic generalization, and a state-of-the-art large language model (GPT-4) probed with advanced prompting strategies.","sentences":["Modern neural network architectures still struggle to learn algorithmic procedures that require to systematically apply compositional rules to solve out-of-distribution problem instances.","In this work, we propose an original approach to learn algorithmic tasks inspired by rewriting systems, a classic framework in symbolic artificial intelligence.","We show that a rewriting system can be implemented as a neural architecture composed by specialized modules: the Selector identifies the target sub-expression to process, the Solver simplifies the sub-expression by computing the corresponding result, and the Combiner produces a new version of the original expression by replacing the sub-expression with the solution provided.","We evaluate our model on three types of algorithmic tasks that require simplifying symbolic formulas involving lists, arithmetic, and algebraic expressions.","We test the extrapolation capabilities of the proposed architecture using formulas involving a higher number of operands and nesting levels than those seen during training, and we benchmark its performance against the Neural Data Router, a recent model specialized for systematic generalization, and a state-of-the-art large language model (GPT-4) probed with advanced prompting strategies."],"url":"http://arxiv.org/abs/2402.17407v1","category":"cs.NE"}
{"created":"2024-02-27 10:55:07","title":"LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning","abstract":"Visual Prompt Tuning (VPT) techniques have gained prominence for their capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual tasks using specialized learnable tokens termed as prompts. Contemporary VPT methodologies, especially when employed with self-supervised vision transformers, often default to the introduction of new learnable prompts or gated prompt tokens predominantly sourced from the model's previous block. A pivotal oversight in such approaches is their failure to harness the potential of long-range previous blocks as sources of prompts within each self-supervised ViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning (LSPT) - a revolutionary approach to visual representation learning. Drawing inspiration from the intricacies of the human brain, LSPT ingeniously incorporates long-term gated prompts. This feature serves as temporal coding, curbing the risk of forgetting parameters acquired from earlier blocks. Further enhancing its prowess, LSPT brings into play patch tokens, serving as spatial coding. This is strategically designed to perpetually amass class-conscious features, thereby fortifying the model's prowess in distinguishing and identifying visual categories. To validate the efficacy of our proposed method, we engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks. Our empirical findings underscore the superiority of LSPT, showcasing its ability to set new benchmarks in visual prompt tuning performance.","sentences":["Visual Prompt Tuning (VPT) techniques have gained prominence for their capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual tasks using specialized learnable tokens termed as prompts.","Contemporary VPT methodologies, especially when employed with self-supervised vision transformers, often default to the introduction of new learnable prompts or gated prompt tokens predominantly sourced from the model's previous block.","A pivotal oversight in such approaches is their failure to harness the potential of long-range previous blocks as sources of prompts within each self-supervised ViT.","To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning (LSPT) - a revolutionary approach to visual representation learning.","Drawing inspiration from the intricacies of the human brain, LSPT ingeniously incorporates long-term gated prompts.","This feature serves as temporal coding, curbing the risk of forgetting parameters acquired from earlier blocks.","Further enhancing its prowess, LSPT brings into play patch tokens, serving as spatial coding.","This is strategically designed to perpetually amass class-conscious features, thereby fortifying the model's prowess in distinguishing and identifying visual categories.","To validate the efficacy of our proposed method, we engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks.","Our empirical findings underscore the superiority of LSPT, showcasing its ability to set new benchmarks in visual prompt tuning performance."],"url":"http://arxiv.org/abs/2402.17406v1","category":"cs.CV"}
{"created":"2024-02-27 10:50:53","title":"Generative diffusion model for surface structure discovery","abstract":"We present a generative diffusion model specifically tailored to the discovery of surface structures. The generative model takes into account substrate registry and periodicity by including masked atoms and $z$-directional confinement. Using a rotational equivariant neural network architecture, we design a method that trains a denoiser-network for diffusion alongside a force-field for guided sampling of low-energy surface phases. An effective data-augmentation scheme for training the denoiser-network is proposed to scale generation far beyond training data sizes. We showcase the generative model by investigating silver-oxide phases on Ag$(111)$ where we are able to rediscover the ``$\\mathrm{Ag}_6$ model'' of $p(4\\times4)\\mathrm{O/Ag}(111)$ that took scientist years to uncover by means of human intuition.","sentences":["We present a generative diffusion model specifically tailored to the discovery of surface structures.","The generative model takes into account substrate registry and periodicity by including masked atoms and $z$-directional confinement.","Using a rotational equivariant neural network architecture, we design a method that trains a denoiser-network for diffusion alongside a force-field for guided sampling of low-energy surface phases.","An effective data-augmentation scheme for training the denoiser-network is proposed to scale generation far beyond training data sizes.","We showcase the generative model by investigating silver-oxide phases on Ag$(111)$ where we are able to rediscover the ``$\\mathrm{Ag}_6$ model'' of $p(4\\times4)\\mathrm{O/Ag}(111)$ that took scientist years to uncover by means of human intuition."],"url":"http://arxiv.org/abs/2402.17404v1","category":"physics.comp-ph"}
{"created":"2024-02-27 10:49:05","title":"Sora Generates Videos with Stunning Geometrical Consistency","abstract":"The recently developed Sora model [1] has exhibited remarkable capabilities in video generation, sparking intense discussions regarding its ability to simulate real-world phenomena. Despite its growing popularity, there is a lack of established metrics to evaluate its fidelity to real-world physics quantitatively. In this paper, we introduce a new benchmark that assesses the quality of the generated videos based on their adherence to real-world physics principles. We employ a method that transforms the generated videos into 3D models, leveraging the premise that the accuracy of 3D reconstruction is heavily contingent on the video quality. From the perspective of 3D reconstruction, we use the fidelity of the geometric constraints satisfied by the constructed 3D models as a proxy to gauge the extent to which the generated videos conform to real-world physics rules. Project page: https://sora-geometrical-consistency.github.io/","sentences":["The recently developed Sora model [1] has exhibited remarkable capabilities in video generation, sparking intense discussions regarding its ability to simulate real-world phenomena.","Despite its growing popularity, there is a lack of established metrics to evaluate its fidelity to real-world physics quantitatively.","In this paper, we introduce a new benchmark that assesses the quality of the generated videos based on their adherence to real-world physics principles.","We employ a method that transforms the generated videos into 3D models, leveraging the premise that the accuracy of 3D reconstruction is heavily contingent on the video quality.","From the perspective of 3D reconstruction, we use the fidelity of the geometric constraints satisfied by the constructed 3D models as a proxy to gauge the extent to which the generated videos conform to real-world physics rules.","Project page: https://sora-geometrical-consistency.github.io/"],"url":"http://arxiv.org/abs/2402.17403v1","category":"cs.CV"}
{"created":"2024-02-27 10:46:36","title":"A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)","abstract":"The paper proposes the Quantum-SMOTE method, a novel solution that uses quantum computing techniques to solve the prevalent problem of class imbalance in machine learning datasets. Quantum-SMOTE, inspired by the Synthetic Minority Oversampling Technique (SMOTE), generates synthetic data points using quantum processes such as swap tests and quantum rotation. The process varies from the conventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean distances, enabling synthetic instances to be generated from minority class data points without relying on neighbor proximity. The algorithm asserts greater control over the synthetic data generation process by introducing hyperparameters such as rotation angle, minority percentage, and splitting factor, which allow for customization to specific dataset requirements. The approach is tested on a public dataset of TelecomChurn and evaluated alongside two prominent classification algorithms, Random Forest and Logistic Regression, to determine its impact along with varying proportions of synthetic data.","sentences":["The paper proposes the Quantum-SMOTE method, a novel solution that uses quantum computing techniques to solve the prevalent problem of class imbalance in machine learning datasets.","Quantum-SMOTE, inspired by the Synthetic Minority Oversampling Technique (SMOTE), generates synthetic data points using quantum processes such as swap tests and quantum rotation.","The process varies from the conventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean distances, enabling synthetic instances to be generated from minority class data points without relying on neighbor proximity.","The algorithm asserts greater control over the synthetic data generation process by introducing hyperparameters such as rotation angle, minority percentage, and splitting factor, which allow for customization to specific dataset requirements.","The approach is tested on a public dataset of TelecomChurn and evaluated alongside two prominent classification algorithms, Random Forest and Logistic Regression, to determine its impact along with varying proportions of synthetic data."],"url":"http://arxiv.org/abs/2402.17398v1","category":"quant-ph"}
{"created":"2024-02-27 10:45:52","title":"Utilizing U-Net Architectures with Auxiliary Information for Scatter Correction in CBCT Across Different Field-of-View Settings","abstract":"Cone-beam computed tomography (CBCT) has become a vital imaging technique in various medical fields but scatter artifacts are a major limitation in CBCT scanning. This challenge is exacerbated by the use of large flat panel 2D detectors. The scatter-to-primary ratio increases significantly with the increase in the size of FOV being scanned. Several deep learning methods, particularly U-Net architectures, have shown promising capabilities in estimating the scatter directly from the CBCT projections. However, the influence of varying FOV sizes on these deep learning models remains unexplored. Having a single neural network for the scatter estimation of varying FOV projections can be of significant importance towards real clinical applications. This study aims to train and evaluate the performance of a U-Net network on a simulated dataset with varying FOV sizes. We further propose a new method (Aux-Net) by providing auxiliary information, such as FOV size, to the U-Net encoder. We validate our method on 30 different FOV sizes and compare it with the U-Net. Our study demonstrates that providing auxiliary information to the network enhances the generalization capability of the U-Net. Our findings suggest that this novel approach outperforms the baseline U-Net, offering a significant step towards practical application in real clinical settings where CBCT systems are employed to scan a wide range of FOVs.","sentences":["Cone-beam computed tomography (CBCT) has become a vital imaging technique in various medical fields but scatter artifacts are a major limitation in CBCT scanning.","This challenge is exacerbated by the use of large flat panel 2D detectors.","The scatter-to-primary ratio increases significantly with the increase in the size of FOV being scanned.","Several deep learning methods, particularly U-Net architectures, have shown promising capabilities in estimating the scatter directly from the CBCT projections.","However, the influence of varying FOV sizes on these deep learning models remains unexplored.","Having a single neural network for the scatter estimation of varying FOV projections can be of significant importance towards real clinical applications.","This study aims to train and evaluate the performance of a U-Net network on a simulated dataset with varying FOV sizes.","We further propose a new method (Aux-Net) by providing auxiliary information, such as FOV size, to the U-Net encoder.","We validate our method on 30 different FOV sizes and compare it with the U-Net.","Our study demonstrates that providing auxiliary information to the network enhances the generalization capability of the U-Net.","Our findings suggest that this novel approach outperforms the baseline U-Net, offering a significant step towards practical application in real clinical settings where CBCT systems are employed to scan a wide range of FOVs."],"url":"http://arxiv.org/abs/2402.17397v1","category":"eess.SP"}
{"created":"2024-02-27 10:44:52","title":"Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies","abstract":"Large Language Models (LLMs) have revolutionized the field of Natural Language Processing thanks to their ability to reuse knowledge acquired on massive text corpora on a wide variety of downstream tasks, with minimal (if any) tuning steps. At the same time, it has been repeatedly shown that LLMs lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution. In this work, we offer a systematic benchmarking of GPT-4, one of the most advanced LLMs available, on three algorithmic tasks characterized by the possibility to control the problem difficulty with two parameters. We compare the performance of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the Transformer-Encoder architecture recently introduced to solve similar tasks, the Neural Data Router. We find that the deployment of advanced prompting techniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating that state-of-the-art LLMs constitute a very strong baseline also in challenging tasks that require systematic generalization.","sentences":["Large Language Models (LLMs) have revolutionized the field of Natural Language Processing thanks to their ability to reuse knowledge acquired on massive text corpora on a wide variety of downstream tasks, with minimal (if any) tuning steps.","At the same time, it has been repeatedly shown that LLMs lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution.","In this work, we offer a systematic benchmarking of GPT-4, one of the most advanced LLMs available, on three algorithmic tasks characterized by the possibility to control the problem difficulty with two parameters.","We compare the performance of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the Transformer-Encoder architecture recently introduced to solve similar tasks, the Neural Data Router.","We find that the deployment of advanced prompting techniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating that state-of-the-art LLMs constitute a very strong baseline also in challenging tasks that require systematic generalization."],"url":"http://arxiv.org/abs/2402.17396v1","category":"cs.CL"}
{"created":"2024-02-27 10:41:39","title":"A Survey of Network Protocol Fuzzing: Model, Techniques and Directions","abstract":"As one of the most successful and effective software testing techniques in recent years, fuzz testing has uncovered numerous bugs and vulnerabilities in modern software, including network protocol software. In contrast to other fuzzing targets, network protocol software exhibits its distinct characteristics and challenges, introducing a plethora of research questions that need to be addressed in the design and implementation of network protocol fuzzers. While some research work has evaluated and systematized the knowledge of general fuzzing techniques at a high level, there is a lack of similar analysis and summarization for fuzzing research specific to network protocols. This paper offers a comprehensive exposition of network protocol software's fuzzing-related features and conducts a systematic review of some representative advancements in network protocol fuzzing since its inception. We summarize state-of-the-art strategies and solutions in various aspects, propose a unified protocol fuzzing process model, and introduce the techniques involved in each stage of the model. At the same time, this paper also summarizes the promising research directions in the landscape of protocol fuzzing to foster exploration within the community for more efficient and intelligent modern network protocol fuzzing techniques.","sentences":["As one of the most successful and effective software testing techniques in recent years, fuzz testing has uncovered numerous bugs and vulnerabilities in modern software, including network protocol software.","In contrast to other fuzzing targets, network protocol software exhibits its distinct characteristics and challenges, introducing a plethora of research questions that need to be addressed in the design and implementation of network protocol fuzzers.","While some research work has evaluated and systematized the knowledge of general fuzzing techniques at a high level, there is a lack of similar analysis and summarization for fuzzing research specific to network protocols.","This paper offers a comprehensive exposition of network protocol software's fuzzing-related features and conducts a systematic review of some representative advancements in network protocol fuzzing since its inception.","We summarize state-of-the-art strategies and solutions in various aspects, propose a unified protocol fuzzing process model, and introduce the techniques involved in each stage of the model.","At the same time, this paper also summarizes the promising research directions in the landscape of protocol fuzzing to foster exploration within the community for more efficient and intelligent modern network protocol fuzzing techniques."],"url":"http://arxiv.org/abs/2402.17394v1","category":"cs.NI"}
{"created":"2024-02-27 10:40:15","title":"Designing Chatbots to Support Victims and Survivors of Domestic Abuse","abstract":"Objective: Domestic abuse cases have risen significantly over the last four years, in part due to the COVID-19 pandemic and the challenges for victims and survivors in accessing support. In this study, we investigate the role that chatbots - Artificial Intelligence (AI) and rule-based - may play in supporting victims/survivors in situations such as these or where direct access to help is limited. Methods: Interviews were conducted with experts working in domestic abuse support services and organizations (e.g., charities, law enforcement) and the content of websites of related support-service providers was collected. Thematic content analysis was then applied to assess and extract insights from the interview data and the content on victim-support websites. We also reviewed pertinent chatbot literature to reflect on studies that may inform design principles and interaction patterns for agents used to support victims/survivors. Results: From our analysis, we outlined a set of design considerations/practices for chatbots that consider potential use cases and target groups, dialog structure, personality traits that might be useful for chatbots to possess, and finally, safety and privacy issues that should be addressed. Of particular note are situations where AI systems (e.g., ChatGPT, CoPilot, Gemini) are not recommended for use, the value of conveying emotional support, the importance of transparency, and the need for a safe and confidential space. Conclusion: It is our hope that these considerations/practices will stimulate debate among chatbots and AI developers and service providers and - for situations where chatbots are deemed appropriate for use - inspire efficient use of chatbots in the support of survivors of domestic abuse.","sentences":["Objective: Domestic abuse cases have risen significantly over the last four years, in part due to the COVID-19 pandemic and the challenges for victims and survivors in accessing support.","In this study, we investigate the role that chatbots - Artificial Intelligence (AI) and rule-based - may play in supporting victims/survivors in situations such as these or where direct access to help is limited.","Methods: Interviews were conducted with experts working in domestic abuse support services and organizations (e.g., charities, law enforcement) and the content of websites of related support-service providers was collected.","Thematic content analysis was then applied to assess and extract insights from the interview data and the content on victim-support websites.","We also reviewed pertinent chatbot literature to reflect on studies that may inform design principles and interaction patterns for agents used to support victims/survivors.","Results: From our analysis, we outlined a set of design considerations/practices for chatbots that consider potential use cases and target groups, dialog structure, personality traits that might be useful for chatbots to possess, and finally, safety and privacy issues that should be addressed.","Of particular note are situations where AI systems (e.g., ChatGPT, CoPilot, Gemini) are not recommended for use, the value of conveying emotional support, the importance of transparency, and the need for a safe and confidential space.","Conclusion: It is our hope that these considerations/practices will stimulate debate among chatbots and AI developers and service providers and - for situations where chatbots are deemed appropriate for use - inspire efficient use of chatbots in the support of survivors of domestic abuse."],"url":"http://arxiv.org/abs/2402.17393v1","category":"cs.CY"}
{"created":"2024-02-27 10:38:37","title":"Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and Humans","abstract":"Nowadays, technology is rapidly advancing: bots are writing comments, articles, and reviews. Due to this fact, it is crucial to know if the text was written by a human or by a bot. This paper focuses on comparing structures of the coarse-grained partitions of semantic paths for human-written and bot-generated texts. We compare the clusterizations of datasets of n-grams from literary texts and texts generated by several bots. The hypothesis is that the structures and clusterizations are different. Our research supports the hypothesis. As the semantic structure may be different for different languages, we investigate Russian, English, German, and Vietnamese languages.","sentences":["Nowadays, technology is rapidly advancing: bots are writing comments, articles, and reviews.","Due to this fact, it is crucial to know if the text was written by a human or by a bot.","This paper focuses on comparing structures of the coarse-grained partitions of semantic paths for human-written and bot-generated texts.","We compare the clusterizations of datasets of n-grams from literary texts and texts generated by several bots.","The hypothesis is that the structures and clusterizations are different.","Our research supports the hypothesis.","As the semantic structure may be different for different languages, we investigate Russian, English, German, and Vietnamese languages."],"url":"http://arxiv.org/abs/2402.17392v1","category":"cs.CL"}
{"created":"2024-02-27 10:37:13","title":"Robustness-Congruent Adversarial Training for Secure Machine Learning Model Updates","abstract":"Machine-learning models demand for periodic updates to improve their average accuracy, exploiting novel architectures and additional data. However, a newly-updated model may commit mistakes that the previous model did not make. Such misclassifications are referred to as negative flips, and experienced by users as a regression of performance. In this work, we show that this problem also affects robustness to adversarial examples, thereby hindering the development of secure model update practices. In particular, when updating a model to improve its adversarial robustness, some previously-ineffective adversarial examples may become misclassified, causing a regression in the perceived security of the system. We propose a novel technique, named robustness-congruent adversarial training, to address this issue. It amounts to fine-tuning a model with adversarial training, while constraining it to retain higher robustness on the adversarial examples that were correctly classified before the update. We show that our algorithm and, more generally, learning with non-regression constraints, provides a theoretically-grounded framework to train consistent estimators. Our experiments on robust models for computer vision confirm that (i) both accuracy and robustness, even if improved after model update, can be affected by negative flips, and (ii) our robustness-congruent adversarial training can mitigate the problem, outperforming competing baseline methods.","sentences":["Machine-learning models demand for periodic updates to improve their average accuracy, exploiting novel architectures and additional data.","However, a newly-updated model may commit mistakes that the previous model did not make.","Such misclassifications are referred to as negative flips, and experienced by users as a regression of performance.","In this work, we show that this problem also affects robustness to adversarial examples, thereby hindering the development of secure model update practices.","In particular, when updating a model to improve its adversarial robustness, some previously-ineffective adversarial examples may become misclassified, causing a regression in the perceived security of the system.","We propose a novel technique, named robustness-congruent adversarial training, to address this issue.","It amounts to fine-tuning a model with adversarial training, while constraining it to retain higher robustness on the adversarial examples that were correctly classified before the update.","We show that our algorithm and, more generally, learning with non-regression constraints, provides a theoretically-grounded framework to train consistent estimators.","Our experiments on robust models for computer vision confirm that (i) both accuracy and robustness, even if improved after model update, can be affected by negative flips, and (ii) our robustness-congruent adversarial training can mitigate the problem, outperforming competing baseline methods."],"url":"http://arxiv.org/abs/2402.17390v1","category":"cs.LG"}
{"created":"2024-02-27 10:31:00","title":"FairBelief - Assessing Harmful Beliefs in Language Models","abstract":"Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real-world applications without careful fairness auditing. This paper proposes FairBelief, an analytical approach to capture and assess beliefs, i.e., propositions that an LM may embed with different degrees of confidence and that covertly influence its predictions. With FairBelief, we leverage prompting to study the behavior of several state-of-the-art LMs across different previously neglected axes, such as model scale and likelihood, assessing predictions on a fairness dataset specifically designed to quantify LMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative assessment of the beliefs emitted by the models. We apply FairBelief to English LMs, revealing that, although these architectures enable high performances on diverse natural language processing tasks, they show hurtful beliefs about specific genders. Interestingly, training procedure and dataset, model scale, and architecture induce beliefs of different degrees of hurtfulness.","sentences":["Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real-world applications without careful fairness auditing.","This paper proposes FairBelief, an analytical approach to capture and assess beliefs, i.e., propositions that an LM may embed with different degrees of confidence and that covertly influence its predictions.","With FairBelief, we leverage prompting to study the behavior of several state-of-the-art LMs across different previously neglected axes, such as model scale and likelihood, assessing predictions on a fairness dataset specifically designed to quantify LMs' outputs' hurtfulness.","Finally, we conclude with an in-depth qualitative assessment of the beliefs emitted by the models.","We apply FairBelief to English LMs, revealing that, although these architectures enable high performances on diverse natural language processing tasks, they show hurtful beliefs about specific genders.","Interestingly, training procedure and dataset, model scale, and architecture induce beliefs of different degrees of hurtfulness."],"url":"http://arxiv.org/abs/2402.17389v1","category":"cs.CL"}
{"created":"2024-02-27 10:29:35","title":"GW Backgrounds associated with PBHs","abstract":"PBH formation requires high-density regions in the (random) density field filling the primordial universe. While only the largest (and so rarest) overdensities collapse to form PBHs, the rest cause large anisotropic stresses, which are the source of GWs. We provide an overview of the theoretical aspects of the GW backgrounds associated with PBHs from large primordial fluctuations. We consider GW backgrounds associated with PBH formation, PBH reheating and unresolved PBH binaries. We present several graphical summaries and illustrations for the busy reader.","sentences":["PBH formation requires high-density regions in the (random) density field filling the primordial universe.","While only the largest (and so rarest) overdensities collapse to form PBHs, the rest cause large anisotropic stresses, which are the source of GWs.","We provide an overview of the theoretical aspects of the GW backgrounds associated with PBHs from large primordial fluctuations.","We consider GW backgrounds associated with PBH formation, PBH reheating and unresolved PBH binaries.","We present several graphical summaries and illustrations for the busy reader."],"url":"http://arxiv.org/abs/2402.17388v1","category":"gr-qc"}
{"created":"2024-02-27 10:26:25","title":"A case study of sending graph neural networks back to the test bench for applications in high-energy particle physics","abstract":"In high-energy particle collisions, the primary collision products usually decay further resulting in tree-like, hierarchical structures with a priori unknown multiplicity. At the stable-particle level all decay products of a collision form permutation invariant sets of final state objects. The analogy to mathematical graphs gives rise to the idea that graph neural networks (GNNs), which naturally resemble these properties, should be best-suited to address many tasks related to high-energy particle physics. In this paper we describe a benchmark test of a typical GNN against neural networks of the well-established deep fully-connected feed-forward architecture. We aim at performing this comparison maximally unbiased in terms of nodes, hidden layers, or trainable parameters of the neural networks under study. As physics case we use the classification of the final state X produced in association with top quark-antiquark pairs in proton-proton collisions at the Large Hadron Collider at CERN, where X stands for a bottom quark-antiquark pair produced either non-resonantly or through the decay of an intermediately produced Z or Higgs boson.","sentences":["In high-energy particle collisions, the primary collision products usually decay further resulting in tree-like, hierarchical structures with a priori unknown multiplicity.","At the stable-particle level all decay products of a collision form permutation invariant sets of final state objects.","The analogy to mathematical graphs gives rise to the idea that graph neural networks (GNNs), which naturally resemble these properties, should be best-suited to address many tasks related to high-energy particle physics.","In this paper we describe a benchmark test of a typical GNN against neural networks of the well-established deep fully-connected feed-forward architecture.","We aim at performing this comparison maximally unbiased in terms of nodes, hidden layers, or trainable parameters of the neural networks under study.","As physics case we use the classification of the final state X produced in association with top quark-antiquark pairs in proton-proton collisions at the Large Hadron Collider at CERN, where X stands for a bottom quark-antiquark pair produced either non-resonantly or through the decay of an intermediately produced Z or Higgs boson."],"url":"http://arxiv.org/abs/2402.17386v1","category":"hep-ph"}
{"created":"2024-02-27 10:24:50","title":"Determinants of LLM-assisted Decision-Making","abstract":"Decision-making is a fundamental capability in everyday life. Large Language Models (LLMs) provide multifaceted support in enhancing human decision-making processes. However, understanding the influencing factors of LLM-assisted decision-making is crucial for enabling individuals to utilize LLM-provided advantages and minimize associated risks in order to make more informed and better decisions. This study presents the results of a comprehensive literature analysis, providing a structural overview and detailed analysis of determinants impacting decision-making with LLM support. In particular, we explore the effects of technological aspects of LLMs, including transparency and prompt engineering, psychological factors such as emotions and decision-making styles, as well as decision-specific determinants such as task difficulty and accountability. In addition, the impact of the determinants on the decision-making process is illustrated via multiple application scenarios. Drawing from our analysis, we develop a dependency framework that systematizes possible interactions in terms of reciprocal interdependencies between these determinants. Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user's mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes. Our findings can be seen as crucial for improving decision quality in human-AI collaboration, empowering both users and organizations, and designing more effective LLM interfaces. Additionally, our work provides a foundation for future empirical investigations on the determinants of decision-making assisted by LLMs.","sentences":["Decision-making is a fundamental capability in everyday life.","Large Language Models (LLMs) provide multifaceted support in enhancing human decision-making processes.","However, understanding the influencing factors of LLM-assisted decision-making is crucial for enabling individuals to utilize LLM-provided advantages and minimize associated risks in order to make more informed and better decisions.","This study presents the results of a comprehensive literature analysis, providing a structural overview and detailed analysis of determinants impacting decision-making with LLM support.","In particular, we explore the effects of technological aspects of LLMs, including transparency and prompt engineering, psychological factors such as emotions and decision-making styles, as well as decision-specific determinants such as task difficulty and accountability.","In addition, the impact of the determinants on the decision-making process is illustrated via multiple application scenarios.","Drawing from our analysis, we develop a dependency framework that systematizes possible interactions in terms of reciprocal interdependencies between these determinants.","Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user's mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes.","Our findings can be seen as crucial for improving decision quality in human-AI collaboration, empowering both users and organizations, and designing more effective LLM interfaces.","Additionally, our work provides a foundation for future empirical investigations on the determinants of decision-making assisted by LLMs."],"url":"http://arxiv.org/abs/2402.17385v1","category":"cs.AI"}
{"created":"2024-02-27 10:24:18","title":"BSW phenomenon for near-fine-tuned particles with external force: general classification of scenarios","abstract":"If two particles moving towards a black hole collide in the vicinity of the horizon, the energy $E_{c.m.}$ in the center of mass frame can grow indefinitely if one of particles is fine-tuned. This is the Ba\\~{n}ados, Silk and West (BSW) effect. One of objections against this effect consists in that for some types of a horizon fine-tuned particles cannot reach the horizon. However, this difficulty can be overcome if instead of exact fine-tuning, one of particle is nearly fine-tuned, with the value of small detuning being adjusted to the distance to the horizon. Such particles are called near-fine-tuned. We give classification of such particles and describe possible high energy scenarios of collision in which they participate. We analyze the ranges of possible motion for each type of particle and determine under which condition such particles can reach the horizon. We analyze collision energy $E_{c.m.}\\,$and determine under which conditions it may grow indefinitely. We also include into consideration the forces acting on particles and find when the BSW effect with nearly-fine-tuned particles is possible with finite forces. We demonstrate that the BSW effect with particles under discussion is consistent with the principle of kinematic censorship. According to this principle, $E_{c.m.}\\,$cannot be literally infinite in any event of collision (if no singularity is present), although it can be made as large as one likes.","sentences":["If two particles moving towards a black hole collide in the vicinity of the horizon, the energy $E_{c.m.}$ in the center of mass frame can grow indefinitely if one of particles is fine-tuned.","This is the Ba\\~{n}ados, Silk and West (BSW) effect.","One of objections against this effect consists in that for some types of a horizon fine-tuned particles cannot reach the horizon.","However, this difficulty can be overcome if instead of exact fine-tuning, one of particle is nearly fine-tuned, with the value of small detuning being adjusted to the distance to the horizon.","Such particles are called near-fine-tuned.","We give classification of such particles and describe possible high energy scenarios of collision in which they participate.","We analyze the ranges of possible motion for each type of particle and determine under which condition such particles can reach the horizon.","We analyze collision energy $E_{c.m.}\\,$and determine under which conditions it may grow indefinitely.","We also include into consideration the forces acting on particles and find when the BSW effect with nearly-fine-tuned particles is possible with finite forces.","We demonstrate that the BSW effect with particles under discussion is consistent with the principle of kinematic censorship.","According to this principle, $E_{c.m.}\\,$cannot be literally infinite in any event of collision (if no singularity is present), although it can be made as large as one likes."],"url":"http://arxiv.org/abs/2402.17383v1","category":"gr-qc"}
{"created":"2024-02-27 10:23:12","title":"Prospects of Identifying Hierarchical Triple Mergers for the Third-generation Ground-based Detectors","abstract":"A hierarchical triple merger (HTM) constitutes a type of event in which two successive black hole (BH) mergers occur sequentially within the observational window of gravitational wave (GW) detectors, which has important role in testing general relativity and studying BH population. In this work, we conduct an analysis to determine the feasibility of identifying HTMs from a large GW event catalog using the third-generation ground-based GW detectors. By comparing the Bhattacharyya coefficient that measures the overlap between the posterior distributions of the remnant and progenitor BH parameters, we find that the overlap between the event pair can serve as a preliminary filter, which balances between computational demand and the probability of false alarms. Following this initial, time-efficient, yet less accurate screening, a subset of potential HTM candidates will be retained. These candidates will subsequently be subjected to a more precise, albeit time-intensive, method of joint parameter estimation for verification. Ultimately, this process will enable us to robustly identify HTMs.","sentences":["A hierarchical triple merger (HTM) constitutes a type of event in which two successive black hole (BH) mergers occur sequentially within the observational window of gravitational wave (GW) detectors, which has important role in testing general relativity and studying BH population.","In this work, we conduct an analysis to determine the feasibility of identifying HTMs from a large GW event catalog using the third-generation ground-based GW detectors.","By comparing the Bhattacharyya coefficient that measures the overlap between the posterior distributions of the remnant and progenitor BH parameters, we find that the overlap between the event pair can serve as a preliminary filter, which balances between computational demand and the probability of false alarms.","Following this initial, time-efficient, yet less accurate screening, a subset of potential HTM candidates will be retained.","These candidates will subsequently be subjected to a more precise, albeit time-intensive, method of joint parameter estimation for verification.","Ultimately, this process will enable us to robustly identify HTMs."],"url":"http://arxiv.org/abs/2402.17381v1","category":"gr-qc"}
{"created":"2024-02-27 10:15:25","title":"Warm-Starting the VQE with Approximate Complex Amplitude Encoding","abstract":"The Variational Quantum Eigensolver (VQE) is a Variational Quantum Algorithm (VQA) to determine the ground state of quantum-mechanical systems. As a VQA, it makes use of a classical computer to optimize parameter values for its quantum circuit. However, each iteration of the VQE requires a multitude of measurements, and the optimization is subject to obstructions, such as barren plateaus, local minima, and subsequently slow convergence. We propose a warm-starting technique, that utilizes an approximation to generate beneficial initial parameter values for the VQE aiming to mitigate these effects. The warm-start is based on Approximate Complex Amplitude Encoding, a VQA using fidelity estimations from classical shadows to encode complex amplitude vectors into quantum states. Such warm-starts open the path to fruitful combinations of classical approximation algorithms and quantum algorithms. In particular, the evaluation of our approach shows that the warm-started VQE reaches higher quality solutions earlier than the original VQE.","sentences":["The Variational Quantum Eigensolver (VQE) is a Variational Quantum Algorithm (VQA) to determine the ground state of quantum-mechanical systems.","As a VQA, it makes use of a classical computer to optimize parameter values for its quantum circuit.","However, each iteration of the VQE requires a multitude of measurements, and the optimization is subject to obstructions, such as barren plateaus, local minima, and subsequently slow convergence.","We propose a warm-starting technique, that utilizes an approximation to generate beneficial initial parameter values for the VQE aiming to mitigate these effects.","The warm-start is based on Approximate Complex Amplitude Encoding, a VQA using fidelity estimations from classical shadows to encode complex amplitude vectors into quantum states.","Such warm-starts open the path to fruitful combinations of classical approximation algorithms and quantum algorithms.","In particular, the evaluation of our approach shows that the warm-started VQE reaches higher quality solutions earlier than the original VQE."],"url":"http://arxiv.org/abs/2402.17378v1","category":"quant-ph"}
{"created":"2024-02-27 10:13:30","title":"Accelerating Diffusion Sampling with Optimized Time Steps","abstract":"Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than $15$ seconds. Our extensive experiments on both unconditional and conditional sampling using pixel- and latent-space DPMs demonstrate that, when combined with the state-of-the-art sampling method UniPC, our optimized time steps significantly improve image generation performance in terms of FID scores for datasets such as CIFAR-10 and ImageNet, compared to using uniform time steps.","sentences":["Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps.","Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps.","While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps.","To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs.","This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver.","It can be efficiently solved using the constrained trust region method, taking less than $15$ seconds.","Our extensive experiments on both unconditional and conditional sampling using pixel- and latent-space DPMs demonstrate that, when combined with the state-of-the-art sampling method UniPC, our optimized time steps significantly improve image generation performance in terms of FID scores for datasets such as CIFAR-10 and ImageNet, compared to using uniform time steps."],"url":"http://arxiv.org/abs/2402.17376v1","category":"cs.CV"}
{"created":"2024-02-27 10:10:22","title":"An improved dense class in Sobolev spaces to manifolds","abstract":"We consider the strong density problem in the Sobolev space $ W^{s,p}(Q^{m};\\mathscr{N}) $ of maps with values into a compact Riemannian manifold $ \\mathscr{N} $. It is known, from the seminal work of Bethuel, that such maps may always be strongly approximated by $ \\mathscr{N} $-valued maps that are smooth outside of a finite union of $ (m -\\lfloor sp \\rfloor - 1) $-planes. Our main result establishes the strong density in $ W^{s,p}(Q^{m};\\mathscr{N}) $ of an improved version of the class introduced by Bethuel, where the maps have a singular set without crossings. This answers a question raised by Brezis and Mironescu.   In the special case where $ \\mathscr{N} $ has a sufficiently simple topology and for some values of $ s $ and $ p $, this result was known to follow from the method of projection, which takes its roots in the work of Federer and Fleming. As a first result, we implement this method in the full range of $ s $ and $ p $ in which it was expected to be applicable. In the case of a general target manifold, we devise a topological argument that allows to remove the self-intersections in the singular set of the maps obtained via Bethuel's technique.","sentences":["We consider the strong density problem in the Sobolev space $ W^{s,p}(Q^{m};\\mathscr{N}) $ of maps with values into a compact Riemannian manifold $ \\mathscr{N} $.","It is known, from the seminal work of Bethuel, that such maps may always be strongly approximated by $ \\mathscr{N} $-valued maps that are smooth outside of a finite union of $ (m -\\lfloor sp \\rfloor - 1) $-planes.","Our main result establishes the strong density in $ W^{s,p}(Q^{m};\\mathscr{N}) $ of an improved version of the class introduced by Bethuel, where the maps have a singular set without crossings.","This answers a question raised by Brezis and Mironescu.   ","In the special case where $ \\mathscr{N} $ has a sufficiently simple topology and for some values of $ s $ and $ p $, this result was known to follow from the method of projection, which takes its roots in the work of Federer and Fleming.","As a first result, we implement this method in the full range of $ s $ and $ p $ in which it was expected to be applicable.","In the case of a general target manifold, we devise a topological argument that allows to remove the self-intersections in the singular set of the maps obtained via Bethuel's technique."],"url":"http://arxiv.org/abs/2402.17373v1","category":"math.FA"}
{"created":"2024-02-27 10:10:12","title":"Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud Matching","abstract":"Point cloud matching, a crucial technique in computer vision, medical and robotics fields, is primarily concerned with finding correspondences between pairs of point clouds or voxels. In some practical scenarios, emphasizing local differences is crucial for accurately identifying a correct match, thereby enhancing the overall robustness and reliability of the matching process. Commonly used shape descriptors have several limitations and often fail to provide meaningful local insights on the paired geometries. In this work, we propose a new technique, based on graph Laplacian eigenmaps, to match point clouds by taking into account fine local structures. To deal with the order and sign ambiguity of Laplacian eigenmaps, we introduce a new operator, called Coupled Laplacian, that allows to easily generate aligned eigenspaces for multiple rigidly-registered geometries. We show that the similarity between those aligned high-dimensional spaces provides a locally meaningful score to match shapes. We initially evaluate the performance of the proposed technique in a point-wise manner, specifically focusing on the task of object anomaly localization using the MVTec 3D-AD dataset. Additionally, we define a new medical task, called automatic Bone Side Estimation (BSE), which we address through a global similarity score derived from coupled eigenspaces. In order to test it, we propose a benchmark collecting bone surface structures from various public datasets. Our matching technique, based on Coupled Laplacian, outperforms other methods by reaching an impressive accuracy on both tasks. The code to reproduce our experiments is publicly available at https://github.com/matteo-bastico/CoupledLaplacian and in the Supplementary Code.","sentences":["Point cloud matching, a crucial technique in computer vision, medical and robotics fields, is primarily concerned with finding correspondences between pairs of point clouds or voxels.","In some practical scenarios, emphasizing local differences is crucial for accurately identifying a correct match, thereby enhancing the overall robustness and reliability of the matching process.","Commonly used shape descriptors have several limitations and often fail to provide meaningful local insights on the paired geometries.","In this work, we propose a new technique, based on graph Laplacian eigenmaps, to match point clouds by taking into account fine local structures.","To deal with the order and sign ambiguity of Laplacian eigenmaps, we introduce a new operator, called Coupled Laplacian, that allows to easily generate aligned eigenspaces for multiple rigidly-registered geometries.","We show that the similarity between those aligned high-dimensional spaces provides a locally meaningful score to match shapes.","We initially evaluate the performance of the proposed technique in a point-wise manner, specifically focusing on the task of object anomaly localization using the MVTec 3D-AD dataset.","Additionally, we define a new medical task, called automatic Bone Side Estimation (BSE), which we address through a global similarity score derived from coupled eigenspaces.","In order to test it, we propose a benchmark collecting bone surface structures from various public datasets.","Our matching technique, based on Coupled Laplacian, outperforms other methods by reaching an impressive accuracy on both tasks.","The code to reproduce our experiments is publicly available at https://github.com/matteo-bastico/CoupledLaplacian and in the Supplementary Code."],"url":"http://arxiv.org/abs/2402.17372v1","category":"cs.CV"}
{"created":"2024-02-27 09:56:15","title":"Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis","abstract":"Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications.","sentences":["Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences.","These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations.","In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints.","DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid.","Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss.","To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning.","These advantages are readily achievable owing to the effective geometric representation employed in DynTet.","Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics.","Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications."],"url":"http://arxiv.org/abs/2402.17364v1","category":"cs.CV"}
{"created":"2024-02-27 09:55:34","title":"CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks","abstract":"Dynamic graphs are extensively employed for detecting anomalous behavior in nodes within the Internet of Things (IoT). Generative models are often used to address the issue of imbalanced node categories in dynamic graphs. Nevertheless, the constraints it faces include the monotonicity of adjacency relationships, the difficulty in constructing multi-dimensional features for nodes, and the lack of a method for end-to-end generation of multiple categories of nodes. This paper presents a novel graph generation model, called CGGM, designed specifically to generate a larger number of nodes belonging to the minority class. The mechanism for generating an adjacency matrix, through adaptive sparsity, enhances flexibility in its structure. The feature generation module, called multidimensional features generator (MFG) to generate node features along with topological information. Labels are transformed into embedding vectors, serving as conditional constraints to control the generation of synthetic data across multiple categories. Using a multi-stage loss, the distribution of synthetic data is adjusted to closely resemble that of real data. In extensive experiments, we show that CGGM's synthetic data outperforms state-of-the-art methods across various metrics. Our results demonstrate efficient generation of diverse data categories, robustly enhancing multi-category classification model performance.","sentences":["Dynamic graphs are extensively employed for detecting anomalous behavior in nodes within the Internet of Things (IoT).","Generative models are often used to address the issue of imbalanced node categories in dynamic graphs.","Nevertheless, the constraints it faces include the monotonicity of adjacency relationships, the difficulty in constructing multi-dimensional features for nodes, and the lack of a method for end-to-end generation of multiple categories of nodes.","This paper presents a novel graph generation model, called CGGM, designed specifically to generate a larger number of nodes belonging to the minority class.","The mechanism for generating an adjacency matrix, through adaptive sparsity, enhances flexibility in its structure.","The feature generation module, called multidimensional features generator (MFG) to generate node features along with topological information.","Labels are transformed into embedding vectors, serving as conditional constraints to control the generation of synthetic data across multiple categories.","Using a multi-stage loss, the distribution of synthetic data is adjusted to closely resemble that of real data.","In extensive experiments, we show that CGGM's synthetic data outperforms state-of-the-art methods across various metrics.","Our results demonstrate efficient generation of diverse data categories, robustly enhancing multi-category classification model performance."],"url":"http://arxiv.org/abs/2402.17363v1","category":"cs.RO"}
{"created":"2024-02-27 09:53:16","title":"CAPT: Category-level Articulation Estimation from a Single Point Cloud Using Transformer","abstract":"The ability to estimate joint parameters is essential for various applications in robotics and computer vision. In this paper, we propose CAPT: category-level articulation estimation from a point cloud using Transformer. CAPT uses an end-to-end transformer-based architecture for joint parameter and state estimation of articulated objects from a single point cloud. The proposed CAPT methods accurately estimate joint parameters and states for various articulated objects with high precision and robustness. The paper also introduces a motion loss approach, which improves articulation estimation performance by emphasizing the dynamic features of articulated objects. Additionally, the paper presents a double voting strategy to provide the framework with coarse-to-fine parameter estimation. Experimental results on several category datasets demonstrate that our methods outperform existing alternatives for articulation estimation. Our research provides a promising solution for applying Transformer-based architectures in articulated object analysis.","sentences":["The ability to estimate joint parameters is essential for various applications in robotics and computer vision.","In this paper, we propose CAPT: category-level articulation estimation from a point cloud using Transformer.","CAPT uses an end-to-end transformer-based architecture for joint parameter and state estimation of articulated objects from a single point cloud.","The proposed CAPT methods accurately estimate joint parameters and states for various articulated objects with high precision and robustness.","The paper also introduces a motion loss approach, which improves articulation estimation performance by emphasizing the dynamic features of articulated objects.","Additionally, the paper presents a double voting strategy to provide the framework with coarse-to-fine parameter estimation.","Experimental results on several category datasets demonstrate that our methods outperform existing alternatives for articulation estimation.","Our research provides a promising solution for applying Transformer-based architectures in articulated object analysis."],"url":"http://arxiv.org/abs/2402.17360v1","category":"cs.CV"}
{"created":"2024-02-27 09:52:27","title":"SoFA: Shielded On-the-fly Alignment via Priority Rule Following","abstract":"The alignment problem in Large Language Models (LLMs) involves adapting them to the broad spectrum of human values. This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards. This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions. Our preliminary analysis reveals that even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules. Therefore, we present PriorityDistill, a semi-automated approach for distilling priority following signals from LLM simulations to ensure robust rule integration and adherence. Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that the model responds appropriately.","sentences":["The alignment problem in Large Language Models (LLMs) involves adapting them to the broad spectrum of human values.","This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards.","This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions.","Our preliminary analysis reveals that even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules.","Therefore, we present PriorityDistill, a semi-automated approach for distilling priority following signals from LLM simulations to ensure robust rule integration and adherence.","Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that the model responds appropriately."],"url":"http://arxiv.org/abs/2402.17358v1","category":"cs.CL"}
{"created":"2024-02-27 09:34:16","title":"An Opacity-Free Method of Testing the Cosmic Distance Duality Relation Using Strongly Lensed Gravitational Wave Signals","abstract":"The cosmic distance duality relation (CDDR), expressed as DL(z) = (1 + z)2DA(z), plays an important role in modern cosmology. In this paper, we propose a new method of testing CDDR using strongly lensed gravitational wave (SLGW) signals. Under the geometric optics approximation, we calculate the gravitational lens effects of two lens models, the point mass and singular isothermal sphere. We use functions of {\\eta}1(z) = 1 + {\\eta}0z and {\\eta}2(z) = 1 + {\\eta}0z=(1 + z) to parameterize the deviation of CDDR. By reparameterizing the SLGW waveform with CDDR and the distance-redshift relation, we include the deviation parameters {\\eta}0 of CDDR as waveform parameters. We evaluate the ability of this method by calculating the parameter estimation of simulated SLGW signals from massive binary black holes. We apply the Fisher information matrix and Markov Chain Monte Carlo methods to calculate parameter estimation. We find that with only one SLGW signal, the measurement precision of {\\eta}0 can reach a considerable level of 0.5-1.3% for {\\eta}1(z) and 1.1-2.6% for {\\eta}2(z), depending on the lens model and parameters.","sentences":["The cosmic distance duality relation (CDDR), expressed as DL(z) = (1 + z)2DA(z), plays an important role in modern cosmology.","In this paper, we propose a new method of testing CDDR using strongly lensed gravitational wave (SLGW) signals.","Under the geometric optics approximation, we calculate the gravitational lens effects of two lens models, the point mass and singular isothermal sphere.","We use functions of {\\eta}1(z) = 1 + {\\eta}0z and {\\eta}2(z)","= 1 + {\\eta}0z=(1 + z) to parameterize the deviation of CDDR.","By reparameterizing the SLGW waveform with CDDR and the distance-redshift relation, we include the deviation parameters {\\eta}0 of CDDR as waveform parameters.","We evaluate the ability of this method by calculating the parameter estimation of simulated SLGW signals from massive binary black holes.","We apply the Fisher information matrix and Markov Chain Monte Carlo methods to calculate parameter estimation.","We find that with only one SLGW signal, the measurement precision of {\\eta}0 can reach a considerable level of 0.5-1.3% for {\\eta}1(z) and 1.1-2.6% for {\\eta}2(z), depending on the lens model and parameters."],"url":"http://arxiv.org/abs/2402.17349v1","category":"astro-ph.CO"}
{"created":"2024-02-27 09:23:54","title":"LocalGCL: Local-aware Contrastive Learning for Graphs","abstract":"Graph representation learning (GRL) makes considerable progress recently, which encodes graphs with topological structures into low-dimensional embeddings. Meanwhile, the time-consuming and costly process of annotating graph labels manually prompts the growth of self-supervised learning (SSL) techniques. As a dominant approach of SSL, Contrastive learning (CL) learns discriminative representations by differentiating between positive and negative samples. However, when applied to graph data, it overemphasizes global patterns while neglecting local structures. To tackle the above issue, we propose \\underline{Local}-aware \\underline{G}raph \\underline{C}ontrastive \\underline{L}earning (\\textbf{\\methnametrim}), a self-supervised learning framework that supplementarily captures local graph information with masking-based modeling compared with vanilla contrastive learning. Extensive experiments validate the superiority of \\methname against state-of-the-art methods, demonstrating its promise as a comprehensive graph representation learner.","sentences":["Graph representation learning (GRL) makes considerable progress recently, which encodes graphs with topological structures into low-dimensional embeddings.","Meanwhile, the time-consuming and costly process of annotating graph labels manually prompts the growth of self-supervised learning (SSL) techniques.","As a dominant approach of SSL, Contrastive learning (CL) learns discriminative representations by differentiating between positive and negative samples.","However, when applied to graph data, it overemphasizes global patterns while neglecting local structures.","To tackle the above issue, we propose \\underline{Local}-aware \\underline{G}raph \\underline{C}ontrastive \\underline{L}earning (\\textbf{\\methnametrim}), a self-supervised learning framework that supplementarily captures local graph information with masking-based modeling compared with vanilla contrastive learning.","Extensive experiments validate the superiority of \\methname against state-of-the-art methods, demonstrating its promise as a comprehensive graph representation learner."],"url":"http://arxiv.org/abs/2402.17345v1","category":"cs.LG"}
{"created":"2024-02-27 09:13:27","title":"SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned Latents","abstract":"Pedestrian trajectory prediction is the key technology in many applications for providing insights into human behavior and anticipating human future motions. Most existing empirical models are explicitly formulated by observed human behaviors using explicable mathematical terms with a deterministic nature, while recent work has focused on developing hybrid models combined with learning-based techniques for powerful expressiveness while maintaining explainability. However, the deterministic nature of the learned steering behaviors from the empirical models limits the models' practical performance. To address this issue, this work proposes the social conditional variational autoencoder (SocialCVAE) for predicting pedestrian trajectories, which employs a CVAE to explore behavioral uncertainty in human motion decisions. SocialCVAE learns socially reasonable motion randomness by utilizing a socially explainable interaction energy map as the CVAE's condition, which illustrates the future occupancy of each pedestrian's local neighborhood area. The energy map is generated using an energy-based interaction model, which anticipates the energy cost (i.e., repulsion intensity) of pedestrians' interactions with neighbors. Experimental results on two public benchmarks including 25 scenes demonstrate that SocialCVAE significantly improves prediction accuracy compared with the state-of-the-art methods, with up to 16.85% improvement in Average Displacement Error (ADE) and 69.18% improvement in Final Displacement Error (FDE).","sentences":["Pedestrian trajectory prediction is the key technology in many applications for providing insights into human behavior and anticipating human future motions.","Most existing empirical models are explicitly formulated by observed human behaviors using explicable mathematical terms with a deterministic nature, while recent work has focused on developing hybrid models combined with learning-based techniques for powerful expressiveness while maintaining explainability.","However, the deterministic nature of the learned steering behaviors from the empirical models limits the models' practical performance.","To address this issue, this work proposes the social conditional variational autoencoder (SocialCVAE) for predicting pedestrian trajectories, which employs a CVAE to explore behavioral uncertainty in human motion decisions.","SocialCVAE learns socially reasonable motion randomness by utilizing a socially explainable interaction energy map as the CVAE's condition, which illustrates the future occupancy of each pedestrian's local neighborhood area.","The energy map is generated using an energy-based interaction model, which anticipates the energy cost (i.e., repulsion intensity) of pedestrians' interactions with neighbors.","Experimental results on two public benchmarks including 25 scenes demonstrate that SocialCVAE significantly improves prediction accuracy compared with the state-of-the-art methods, with up to 16.85% improvement in Average Displacement Error (ADE) and 69.18% improvement in Final Displacement Error (FDE)."],"url":"http://arxiv.org/abs/2402.17339v1","category":"cs.CV"}
{"created":"2024-02-27 09:11:50","title":"Variety of general position problems in graphs","abstract":"Let $X$ be a vertex subset of a graph $G$. Then $u, v\\in V(G)$ are $X$-positionable if $V(P)\\cap X \\subseteq \\{u,v\\}$ holds for any shortest $u,v$-path $P$. If each two vertices from $X$ are $X$-positionable, then $X$ is a general position set. The general position number of $G$ is the cardinality of a largest general position set of $G$ and has been already well investigated. In this paper a variety of general position problems is introduced based on which natural pairs of vertices are required to be $X$-positionable. This yields the total (resp.\\ dual, outer) general position number. It is proved that the total general position sets coincide with sets of simplicial vertices, and that the outer general position sets coincide with sets of mutually maximally distant vertices. It is shown that a general position set is a dual general position set if and only if its complement is convex. Several sufficient conditions are presented that guarantee that a given graph has no dual general position set. The total general position number, the outer general position number, and the dual general position number of arbitrary Cartesian products are determined.","sentences":["Let $X$ be a vertex subset of a graph $G$. Then $u, v\\in V(G)$ are $X$-positionable if $V(P)\\cap X","\\subseteq \\{u,v\\}$ holds for any shortest $u,v$-path","$P$. If each two vertices from $X$ are $X$-positionable, then $X$ is a general position set.","The general position number of $G$ is the cardinality of a largest general position set of $G$ and has been already well investigated.","In this paper a variety of general position problems is introduced based on which natural pairs of vertices are required to be $X$-positionable.","This yields the total (resp.\\ dual, outer) general position number.","It is proved that the total general position sets coincide with sets of simplicial vertices, and that the outer general position sets coincide with sets of mutually maximally distant vertices.","It is shown that a general position set is a dual general position set if and only if its complement is convex.","Several sufficient conditions are presented that guarantee that a given graph has no dual general position set.","The total general position number, the outer general position number, and the dual general position number of arbitrary Cartesian products are determined."],"url":"http://arxiv.org/abs/2402.17338v1","category":"math.CO"}
{"created":"2024-02-27 09:10:41","title":"BiVRec: Bidirectional View-based Multimodal Sequential Recommendation","abstract":"The integration of multimodal information into sequential recommender systems has attracted significant attention in recent research. In the initial stages of multimodal sequential recommendation models, the mainstream paradigm was ID-dominant recommendations, wherein multimodal information was fused as side information. However, due to their limitations in terms of transferability and information intrusion, another paradigm emerged, wherein multimodal features were employed directly for recommendation, enabling recommendation across datasets. Nonetheless, it overlooked user ID information, resulting in low information utilization and high training costs. To this end, we propose an innovative framework, BivRec, that jointly trains the recommendation tasks in both ID and multimodal views, leveraging their synergistic relationship to enhance recommendation performance bidirectionally. To tackle the information heterogeneity issue, we first construct structured user interest representations and then learn the synergistic relationship between them. Specifically, BivRec comprises three modules: Multi-scale Interest Embedding, comprehensively modeling user interests by expanding user interaction sequences with multi-scale patching; Intra-View Interest Decomposition, constructing highly structured interest representations using carefully designed Gaussian attention and Cluster attention; and Cross-View Interest Learning, learning the synergistic relationship between the two recommendation views through coarse-grained overall semantic similarity and fine-grained interest allocation similarity BiVRec achieves state-of-the-art performance on five datasets and showcases various practical advantages.","sentences":["The integration of multimodal information into sequential recommender systems has attracted significant attention in recent research.","In the initial stages of multimodal sequential recommendation models, the mainstream paradigm was ID-dominant recommendations, wherein multimodal information was fused as side information.","However, due to their limitations in terms of transferability and information intrusion, another paradigm emerged, wherein multimodal features were employed directly for recommendation, enabling recommendation across datasets.","Nonetheless, it overlooked user ID information, resulting in low information utilization and high training costs.","To this end, we propose an innovative framework, BivRec, that jointly trains the recommendation tasks in both ID and multimodal views, leveraging their synergistic relationship to enhance recommendation performance bidirectionally.","To tackle the information heterogeneity issue, we first construct structured user interest representations and then learn the synergistic relationship between them.","Specifically, BivRec comprises three modules: Multi-scale Interest Embedding, comprehensively modeling user interests by expanding user interaction sequences with multi-scale patching; Intra-View Interest Decomposition, constructing highly structured interest representations using carefully designed Gaussian attention and Cluster attention; and Cross-View Interest Learning, learning the synergistic relationship between the two recommendation views through coarse-grained overall semantic similarity and fine-grained interest allocation similarity BiVRec achieves state-of-the-art performance on five datasets and showcases various practical advantages."],"url":"http://arxiv.org/abs/2402.17334v1","category":"cs.IR"}
{"created":"2024-02-27 18:15:22","title":"Observation of the $\u039e^-_\\mathrm{b}$ $\\to$ $\u03c8$(2S)$\u039e^-$ decay and studies of the $\u039e_\\mathrm{b}^{\\ast{}0}$ baryon in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"The first observation of the decay $\\Xi^-_\\mathrm{b}$ $\\to$ $\\psi$(2S)$\\Xi^-$ and measurement of the branching ratio of $\\Xi^-_\\mathrm{b}$ $\\to$ $\\psi$(2S)$\\Xi^-$ to $\\Xi^-_\\mathrm{b}$ $\\to$ J/$\\psi$$\\Xi^-$ are presented. The J/$\\psi$ and $\\psi$(2S) mesons are reconstructed using their dimuon decay modes. The results are based on proton-proton colliding beam data from the LHC collected by the CMS experiment at $\\sqrt{s}$ = 13 TeV in 2016-2018, corresponding to an integrated luminosity of 140 fb$^{-1}$. The branching fraction ratio is measured to be $\\mathcal{B}$($\\Xi^-_\\mathrm{b}$ $\\to$ $\\psi$(2S)$\\Xi^-$)/$\\mathcal{B}$($\\Xi^-_\\mathrm{b}$ $\\to$ J/$\\psi$$\\Xi^-$) = 0.84$^{+0.21}_{-0.19}$ (stat) $\\pm$ 0.10 (syst) $\\pm$ 0.02 ($\\mathcal{B}$), where the last uncertainty comes from the uncertainties in the branching fractions of the charmonium states. New measurements of the $\\Xi_\\mathrm{b}^{\\ast{}0}$ baryon mass and natural width are also presented, using the $\\Xi_\\mathrm{b}^-\\pi^+$ final state, where the $\\Xi^-_\\mathrm{b}$ baryon is reconstructed through the decays J/$\\psi \\Xi^-$, $\\psi$(2S)$\\Xi^-$, J/$\\psi \\Lambda$K$^-$, and J/$\\psi \\Sigma^0$K$^-$. Finally, the fraction of the $\\Xi^-_\\mathrm{b}$ baryons produced from $\\Xi_\\mathrm{b}^{\\ast{}0}$ decays is determined.","sentences":["The first observation of the decay $\\Xi^-_\\mathrm{b}$ $\\to$ $\\psi$(2S)$\\Xi^-$ and measurement of the branching ratio of $\\Xi^-_\\mathrm{b}$ $\\to$ $\\psi$(2S)$\\Xi^-$ to $\\Xi^-_\\mathrm{b}$ $\\to$ J/$\\psi$$\\Xi^-$ are presented.","The J/$\\psi$ and $\\psi$(2S) mesons are reconstructed using their dimuon decay modes.","The results are based on proton-proton colliding beam data from the LHC collected by the CMS experiment at $\\sqrt{s}$ = 13 TeV in 2016-2018, corresponding to an integrated luminosity of 140 fb$^{-1}$. The branching fraction ratio is measured to be $\\mathcal{B}$($\\Xi^-_\\mathrm{b}$ $\\to$ $\\psi$(2S)$\\Xi^-$)/$\\mathcal{B}$($\\Xi^-_\\mathrm{b}$ $\\to$ J/$\\psi$$\\Xi^-$) = 0.84$^{+0.21}_{-0.19}$ (stat) $\\pm$ 0.10 (syst) $\\pm$ 0.02 ($\\mathcal{B}$), where the last uncertainty comes from the uncertainties in the branching fractions of the charmonium states.","New measurements of the $\\Xi_\\mathrm{b}^{\\ast{}0}$ baryon mass and natural width are also presented, using the $\\Xi_\\mathrm{b}^-\\pi^+$ final state, where the $\\Xi^-_\\mathrm{b}$ baryon is reconstructed through the decays J/$\\psi \\Xi^-$, $\\psi$(2S)$\\Xi^-$, J/$\\psi \\Lambda$K$^-$, and J/$\\psi \\Sigma^0$K$^-$. Finally, the fraction of the $\\Xi^-_\\mathrm{b}$ baryons produced from $\\Xi_\\mathrm{b}^{\\ast{}0}$ decays is determined."],"url":"http://arxiv.org/abs/2402.17738v1","category":"hep-ex"}
{"created":"2024-02-27 17:33:23","title":"Federated Learning for Estimating Heterogeneous Treatment Effects","abstract":"Machine learning methods for estimating heterogeneous treatment effects (HTE) facilitate large-scale personalized decision-making across various domains such as healthcare, policy making, education, and more. Current machine learning approaches for HTE require access to substantial amounts of data per treatment, and the high costs associated with interventions makes centrally collecting so much data for each intervention a formidable challenge. To overcome this obstacle, in this work, we propose a novel framework for collaborative learning of HTE estimators across institutions via Federated Learning. We show that even under a diversity of interventions and subject populations across clients, one can jointly learn a common feature representation, while concurrently and privately learning the specific predictive functions for outcomes under distinct interventions across institutions. Our framework and the associated algorithm are based on this insight, and leverage tabular transformers to map multiple input data to feature representations which are then used for outcome prediction via multi-task learning. We also propose a novel way of federated training of personalised transformers that can work with heterogeneous input feature spaces. Experimental results on real-world clinical trial data demonstrate the effectiveness of our method.","sentences":["Machine learning methods for estimating heterogeneous treatment effects (HTE) facilitate large-scale personalized decision-making across various domains such as healthcare, policy making, education, and more.","Current machine learning approaches for HTE require access to substantial amounts of data per treatment, and the high costs associated with interventions makes centrally collecting so much data for each intervention a formidable challenge.","To overcome this obstacle, in this work, we propose a novel framework for collaborative learning of HTE estimators across institutions via Federated Learning.","We show that even under a diversity of interventions and subject populations across clients, one can jointly learn a common feature representation, while concurrently and privately learning the specific predictive functions for outcomes under distinct interventions across institutions.","Our framework and the associated algorithm are based on this insight, and leverage tabular transformers to map multiple input data to feature representations which are then used for outcome prediction via multi-task learning.","We also propose a novel way of federated training of personalised transformers that can work with heterogeneous input feature spaces.","Experimental results on real-world clinical trial data demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2402.17705v1","category":"cs.LG"}
{"created":"2024-02-27 16:19:37","title":"Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs","abstract":"Due to the widespread use of large language models (LLMs) in ubiquitous systems, we need to understand whether they embed a specific worldview and what these views reflect. Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings. However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning. We propose a series of tests which assess the reliability and consistency of LLMs' stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy domains. We study LLMs ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count. Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They evince a (left-wing) positive stance towards environment protection, social welfare but also (right-wing) law and order, with no consistent preferences in foreign policy, migration, and economy.","sentences":["Due to the widespread use of large language models (LLMs) in ubiquitous systems, we need to understand whether they embed a specific worldview and what these views reflect.","Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings.","However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning.","We propose a series of tests which assess the reliability and consistency of LLMs' stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy domains.","We study LLMs ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count.","Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They evince a (left-wing) positive stance towards environment protection, social welfare but also (right-wing) law and order, with no consistent preferences in foreign policy, migration, and economy."],"url":"http://arxiv.org/abs/2402.17649v1","category":"cs.CL"}
{"created":"2024-02-27 15:20:11","title":"Chronicles of CI/CD: A Deep Dive into its Usage Over Time","abstract":"DevOps is a combination of methodologies and tools that improves the software development, build, deployment, and monitoring processes by shortening its lifecycle and improving software quality. Part of this process is CI/CD, which embodies mostly the first parts, right up to the deployment. Despite the many benefits of DevOps and CI/CD, it still presents many challenges promoted by the tremendous proliferation of different tools, languages, and syntaxes, which makes the field quite challenging to learn and keep up to date. Software repositories contain data regarding various software practices, tools, and uses. This data can help gather multiple insights that inform technical and academic decision-making. GitHub is currently the most popular software hosting platform and provides a search API that lets users query its repositories. Our goal with this paper is to gain insights into the technologies developers use for CI/CD by analyzing GitHub repositories. Using a list of the state-of-the-art CI/CD technologies, we use the GitHub search API to find repositories using each of these technologies. We also use the API to extract various insights regarding those repositories. We then organize and analyze the data collected. From our analysis, we provide an overview of the use of CI/CD technologies in our days, but also what happened in the last 12 years. We also show developers use several technologies simultaneously in the same project and that the change between technologies is quite common. From these insights, we find several research paths, from how to support the use of multiple technologies, both in terms of techniques, but also in terms of human-computer interaction, to aiding developers in evolving their CI/CD pipelines, again considering the various dimensions of the problem.","sentences":["DevOps is a combination of methodologies and tools that improves the software development, build, deployment, and monitoring processes by shortening its lifecycle and improving software quality.","Part of this process is CI/CD, which embodies mostly the first parts, right up to the deployment.","Despite the many benefits of DevOps and CI/CD, it still presents many challenges promoted by the tremendous proliferation of different tools, languages, and syntaxes, which makes the field quite challenging to learn and keep up to date.","Software repositories contain data regarding various software practices, tools, and uses.","This data can help gather multiple insights that inform technical and academic decision-making.","GitHub is currently the most popular software hosting platform and provides a search API that lets users query its repositories.","Our goal with this paper is to gain insights into the technologies developers use for CI/CD by analyzing GitHub repositories.","Using a list of the state-of-the-art CI/CD technologies, we use the GitHub search API to find repositories using each of these technologies.","We also use the API to extract various insights regarding those repositories.","We then organize and analyze the data collected.","From our analysis, we provide an overview of the use of CI/CD technologies in our days, but also what happened in the last 12 years.","We also show developers use several technologies simultaneously in the same project and that the change between technologies is quite common.","From these insights, we find several research paths, from how to support the use of multiple technologies, both in terms of techniques, but also in terms of human-computer interaction, to aiding developers in evolving their CI/CD pipelines, again considering the various dimensions of the problem."],"url":"http://arxiv.org/abs/2402.17588v1","category":"cs.SE"}
{"created":"2024-02-27 13:34:08","title":"Predicting Instability in Complex Oscillator Networks: Limitations and Potentials of Network Measures and Machine Learning","abstract":"A central question of network science is how functional properties of systems arise from their structure. For networked dynamical systems, structure is typically quantified with network measures. A functional property that is of theoretical and practical interest for oscillatory systems is the stability of synchrony to localized perturbations. Recently, Graph Neural Networks (GNNs) have been shown to predict this stability successfully; at the same time, network measures have struggled to paint a clear picture. Here we collect 46 relevant network measures and find that no small subset can reliably predict stability. The performance of GNNs can only be matched by combining all network measures and nodewise machine learning. However, unlike GNNs, this approach fails to extrapolate from network ensembles to several real power grid topologies. This suggests that correlations of network measures and function may be misleading, and that GNNs capture the causal relationship between structure and stability substantially better.","sentences":["A central question of network science is how functional properties of systems arise from their structure.","For networked dynamical systems, structure is typically quantified with network measures.","A functional property that is of theoretical and practical interest for oscillatory systems is the stability of synchrony to localized perturbations.","Recently, Graph Neural Networks (GNNs) have been shown to predict this stability successfully; at the same time, network measures have struggled to paint a clear picture.","Here we collect 46 relevant network measures and find that no small subset can reliably predict stability.","The performance of GNNs can only be matched by combining all network measures and nodewise machine learning.","However, unlike GNNs, this approach fails to extrapolate from network ensembles to several real power grid topologies.","This suggests that correlations of network measures and function may be misleading, and that GNNs capture the causal relationship between structure and stability substantially better."],"url":"http://arxiv.org/abs/2402.17500v1","category":"nlin.AO"}
{"created":"2024-02-27 12:00:08","title":"Constraining the average magnetic field in galaxy clusters with current and upcoming CMB surveys","abstract":"Galaxy clusters that host radio halos indicate the presence of population(s) of non-thermal electrons. These electrons can scatter low-energy photons of the Cosmic Microwave Background, resulting in the non-thermal Sunyaev-Zeldovich (ntSZ) effect. We measure the average ntSZ signal from 62 radio-halo hosting clusters using the $Planck$ multi-frequency all-sky maps. We find no direct evidence of the ntSZ signal in the $Planck$ data. Combining the upper limits on the non-thermal electron density with the average measured synchrotron power collected from the literature, we place lower limits on the average magnetic field strength in our sample. The lower limit on the volume-averaged magnetic field is $0.1-0.01\\,\\mu$G, depending on the assumed power-law distribution of electron energies. We further explore the potential improvement of these constraints from the upcoming Simons Observatory and Fred Young Submillimeter Telescope (FYST) of the CCAT-prime collaboration. We find that combining these two experiments, the constraints will improve by a factor of $3-4$, which can be sufficient to rule out some power-law models.","sentences":["Galaxy clusters that host radio halos indicate the presence of population(s) of non-thermal electrons.","These electrons can scatter low-energy photons of the Cosmic Microwave Background, resulting in the non-thermal Sunyaev-Zeldovich (ntSZ) effect.","We measure the average ntSZ signal from 62 radio-halo hosting clusters using the $Planck$ multi-frequency all-sky maps.","We find no direct evidence of the ntSZ signal in the $Planck$ data.","Combining the upper limits on the non-thermal electron density with the average measured synchrotron power collected from the literature, we place lower limits on the average magnetic field strength in our sample.","The lower limit on the volume-averaged magnetic field is $0.1-0.01\\,\\mu$G, depending on the assumed power-law distribution of electron energies.","We further explore the potential improvement of these constraints from the upcoming Simons Observatory and Fred Young Submillimeter Telescope (FYST) of the CCAT-prime collaboration.","We find that combining these two experiments, the constraints will improve by a factor of $3-4$, which can be sufficient to rule out some power-law models."],"url":"http://arxiv.org/abs/2402.17445v1","category":"astro-ph.CO"}
{"created":"2024-02-27 10:14:57","title":"KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark","abstract":"As language models are often deployed as chatbot assistants, it becomes a virtue for models to engage in conversations in a user's first language. While these models are trained on a wide range of languages, a comprehensive evaluation of their proficiency in low-resource languages such as Korean has been lacking. In this work, we introduce KoDialogBench, a benchmark designed to assess language models' conversational capabilities in Korean. To this end, we collect native Korean dialogues on daily topics from public sources, or translate dialogues from other languages. We then structure these conversations into diverse test datasets, spanning from dialogue comprehension to response selection tasks. Leveraging the proposed benchmark, we conduct extensive evaluations and analyses of various language models to measure a foundational understanding of Korean dialogues. Experimental results indicate that there exists significant room for improvement in models' conversation skills. Furthermore, our in-depth comparisons across different language models highlight the effectiveness of recent training techniques in enhancing conversational proficiency. We anticipate that KoDialogBench will promote the progress towards conversation-aware Korean language models.","sentences":["As language models are often deployed as chatbot assistants, it becomes a virtue for models to engage in conversations in a user's first language.","While these models are trained on a wide range of languages, a comprehensive evaluation of their proficiency in low-resource languages such as Korean has been lacking.","In this work, we introduce KoDialogBench, a benchmark designed to assess language models' conversational capabilities in Korean.","To this end, we collect native Korean dialogues on daily topics from public sources, or translate dialogues from other languages.","We then structure these conversations into diverse test datasets, spanning from dialogue comprehension to response selection tasks.","Leveraging the proposed benchmark, we conduct extensive evaluations and analyses of various language models to measure a foundational understanding of Korean dialogues.","Experimental results indicate that there exists significant room for improvement in models' conversation skills.","Furthermore, our in-depth comparisons across different language models highlight the effectiveness of recent training techniques in enhancing conversational proficiency.","We anticipate that KoDialogBench will promote the progress towards conversation-aware Korean language models."],"url":"http://arxiv.org/abs/2402.17377v1","category":"cs.CL"}
{"created":"2024-02-27 08:47:19","title":"Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation","abstract":"The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices. Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides. However, in many real-world scenarios, the test environments may change dynamically (known as distribution shifts), which often results in degraded performance. Thus, one has to adapt the edge models promptly to attain promising performance. Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance. To address these, we encounter two primary challenges: 1) the edge model has limited computation power and may only support forward propagation; 2) the data transmission budget between cloud and edge devices is limited in latency-sensitive scenarios. In this paper, we establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online. In our CEMA, to reduce the communication burden, we devise two criteria to exclude unnecessary samples from uploading to the cloud, i.e., dynamic unreliable and low-informative sample exclusion. Based on the uploaded samples, we update and distribute the affine parameters of normalization layers by distilling from the stronger foundation model to the edge model with a sample replay strategy. Extensive experimental results on ImageNet-C and ImageNet-R verify the effectiveness of our CEMA.","sentences":["The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices.","Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides.","However, in many real-world scenarios, the test environments may change dynamically (known as distribution shifts), which often results in degraded performance.","Thus, one has to adapt the edge models promptly to attain promising performance.","Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance.","To address these, we encounter two primary challenges: 1) the edge model has limited computation power and may only support forward propagation; 2) the data transmission budget between cloud and edge devices is limited in latency-sensitive scenarios.","In this paper, we establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online.","In our CEMA, to reduce the communication burden, we devise two criteria to exclude unnecessary samples from uploading to the cloud, i.e., dynamic unreliable and low-informative sample exclusion.","Based on the uploaded samples, we update and distribute the affine parameters of normalization layers by distilling from the stronger foundation model to the edge model with a sample replay strategy.","Extensive experimental results on ImageNet-C and ImageNet-R verify the effectiveness of our CEMA."],"url":"http://arxiv.org/abs/2402.17316v1","category":"cs.CV"}
{"created":"2024-02-27 08:33:03","title":"Method of Tracking and Analysis of Fluorescent-Labeled Cells Using Automatic Thresholding and Labeling","abstract":"High-throughput screening using cell images is an efficient method for screening new candidates for pharmaceutical drugs. To complete the screening process, it is essential to have an efficient process for analyzing cell images. This paper presents a new method for efficiently tracking cells and quantitatively detecting the signal ratio between cytoplasm and nuclei. Existing methods include those that use image processing techniques and those that utilize artificial intelligence (AI). However, these methods do not consider the correspondence of cells between images, or require a significant amount of new learning data to train AI. Therefore, our method uses automatic thresholding and labeling algorithms to compare the position of each cell between images, and continuously measure and analyze the signal ratio of cells. This paper describes the algorithm of our method. Using the method, we experimented to investigate the effect of the number of opening and closing operations during the binarization process on the tracking of the cells. Through the experiment, we determined the appropriate number of opening and closing processes.","sentences":["High-throughput screening using cell images is an efficient method for screening new candidates for pharmaceutical drugs.","To complete the screening process, it is essential to have an efficient process for analyzing cell images.","This paper presents a new method for efficiently tracking cells and quantitatively detecting the signal ratio between cytoplasm and nuclei.","Existing methods include those that use image processing techniques and those that utilize artificial intelligence (AI).","However, these methods do not consider the correspondence of cells between images, or require a significant amount of new learning data to train AI.","Therefore, our method uses automatic thresholding and labeling algorithms to compare the position of each cell between images, and continuously measure and analyze the signal ratio of cells.","This paper describes the algorithm of our method.","Using the method, we experimented to investigate the effect of the number of opening and closing operations during the binarization process on the tracking of the cells.","Through the experiment, we determined the appropriate number of opening and closing processes."],"url":"http://arxiv.org/abs/2402.17310v1","category":"cs.CV"}
{"created":"2024-02-27 08:27:15","title":"Probing Multimodal Large Language Models for Global and Local Semantic Representation","abstract":"The success of large language models has inspired researchers to transfer their exceptional representing ability to other modalities. Several recent works leverage image-caption alignment datasets to train multimodal large language models (MLLMs), which achieve state-of-the-art performance on image-to-text tasks. However, there are very few studies exploring whether MLLMs truly understand the complete image information, i.e., global information, or if they can only capture some local object information. In this study, we find that the intermediate layers of models can encode more global semantic information, whose representation vectors perform better on visual-language entailment tasks, rather than the topmost layers. We further probe models for local semantic representation through object detection tasks. And we draw a conclusion that the topmost layers may excessively focus on local information, leading to a diminished ability to encode global information.","sentences":["The success of large language models has inspired researchers to transfer their exceptional representing ability to other modalities.","Several recent works leverage image-caption alignment datasets to train multimodal large language models (MLLMs), which achieve state-of-the-art performance on image-to-text tasks.","However, there are very few studies exploring whether MLLMs truly understand the complete image information, i.e., global information, or if they can only capture some local object information.","In this study, we find that the intermediate layers of models can encode more global semantic information, whose representation vectors perform better on visual-language entailment tasks, rather than the topmost layers.","We further probe models for local semantic representation through object detection tasks.","And we draw a conclusion that the topmost layers may excessively focus on local information, leading to a diminished ability to encode global information."],"url":"http://arxiv.org/abs/2402.17304v1","category":"cs.CL"}
{"created":"2024-02-27 08:02:48","title":"Active propulsion noise shaping for multi-rotor aircraft localization","abstract":"Multi-rotor aerial autonomous vehicles (MAVs) primarily rely on vision for navigation purposes. However, visual localization and odometry techniques suffer from poor performance in low or direct sunlight, a limited field of view, and vulnerability to occlusions. Acoustic sensing can serve as a complementary or even alternative modality for vision in many situations, and it also has the added benefits of lower system cost and energy footprint, which is especially important for micro aircraft. This paper proposes actively controlling and shaping the aircraft propulsion noise generated by the rotors to benefit localization tasks, rather than considering it a harmful nuisance. We present a neural network architecture for selfnoise-based localization in a known environment. We show that training it simultaneously with learning time-varying rotor phase modulation achieves accurate and robust localization. The proposed methods are evaluated using a computationally affordable simulation of MAV rotor noise in 2D acoustic environments that is fitted to real recordings of rotor pressure fields.","sentences":["Multi-rotor aerial autonomous vehicles (MAVs) primarily rely on vision for navigation purposes.","However, visual localization and odometry techniques suffer from poor performance in low or direct sunlight, a limited field of view, and vulnerability to occlusions.","Acoustic sensing can serve as a complementary or even alternative modality for vision in many situations, and it also has the added benefits of lower system cost and energy footprint, which is especially important for micro aircraft.","This paper proposes actively controlling and shaping the aircraft propulsion noise generated by the rotors to benefit localization tasks, rather than considering it a harmful nuisance.","We present a neural network architecture for selfnoise-based localization in a known environment.","We show that training it simultaneously with learning time-varying rotor phase modulation achieves accurate and robust localization.","The proposed methods are evaluated using a computationally affordable simulation of MAV rotor noise in 2D acoustic environments that is fitted to real recordings of rotor pressure fields."],"url":"http://arxiv.org/abs/2402.17289v1","category":"cs.RO"}
{"created":"2024-02-27 07:57:28","title":"Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder Super-resolution Network","abstract":"Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to effectively capture the complex spectral-spatial relationships and low-level details, while diffusion models represent a promising generative model known for their exceptional performance in modeling complex relations and learning high and low-level visual features. The direct application of diffusion models to HSI SR is hampered by challenges such as difficulties in model convergence and protracted inference time. In this work, we introduce a novel Group-Autoencoder (GAE) framework that synergistically combines with the diffusion model to construct a highly effective HSI SR model (DMGASR). Our proposed GAE framework encodes high-dimensional HSI data into low-dimensional latent space where the diffusion model works, thereby alleviating the difficulty of training the diffusion model while maintaining band correlation and considerably reducing inference time. Experimental results on both natural and remote sensing hyperspectral datasets demonstrate that the proposed method is superior to other state-of-the-art methods both visually and metrically.","sentences":["Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to effectively capture the complex spectral-spatial relationships and low-level details, while diffusion models represent a promising generative model known for their exceptional performance in modeling complex relations and learning high and low-level visual features.","The direct application of diffusion models to HSI SR is hampered by challenges such as difficulties in model convergence and protracted inference time.","In this work, we introduce a novel Group-Autoencoder (GAE) framework that synergistically combines with the diffusion model to construct a highly effective HSI SR model (DMGASR).","Our proposed GAE framework encodes high-dimensional HSI data into low-dimensional latent space where the diffusion model works, thereby alleviating the difficulty of training the diffusion model while maintaining band correlation and considerably reducing inference time.","Experimental results on both natural and remote sensing hyperspectral datasets demonstrate that the proposed method is superior to other state-of-the-art methods both visually and metrically."],"url":"http://arxiv.org/abs/2402.17285v1","category":"cs.CV"}
{"created":"2024-02-27 07:46:54","title":"RISAR: RIS-assisted Human Activity Recognition with Commercial Wi-Fi Devices","abstract":"Human activity recognition (HAR) is integral to smart homes, security, and healthcare. Existing systems face challenges such as the absence of line-of-sight links, insufficient details regarding the sensing subject, and inefficiencies in noise reduction and feature extraction from channel state information. To address these issues, this study builds a reconfigurable intelligent surface (RIS)-assisted passive human activity recognition (RISAR) system, compatible with commercial Wi-Fi devices. RISAR employs a RIS to augment the orientation and spatial diversity of Wi-Fi signals, thereby facilitating improved detection capabilities. A new denoising and feature extraction technique, the high-dimensional factor model, based on random matrix theory, is proposed for noise elimination and salient temporal feature extraction. On this basis, a dual-stream spatial-temporal attention network model is developed for assigning variable weights to different characteristics and sequences, mimicking human cognitive processes in prioritizing essential information. Experimental analysis shows that RISAR significantly outperforms existing HAR systems in accuracy and efficiency, achieving an average accuracy of 97.26%. These findings not only highlight the adaptability of the RISAR system but also underscore its potential as a robust solution for activity recognition across complex environments.","sentences":["Human activity recognition (HAR) is integral to smart homes, security, and healthcare.","Existing systems face challenges such as the absence of line-of-sight links, insufficient details regarding the sensing subject, and inefficiencies in noise reduction and feature extraction from channel state information.","To address these issues, this study builds a reconfigurable intelligent surface (RIS)-assisted passive human activity recognition (RISAR) system, compatible with commercial Wi-Fi devices.","RISAR employs a RIS to augment the orientation and spatial diversity of Wi-Fi signals, thereby facilitating improved detection capabilities.","A new denoising and feature extraction technique, the high-dimensional factor model, based on random matrix theory, is proposed for noise elimination and salient temporal feature extraction.","On this basis, a dual-stream spatial-temporal attention network model is developed for assigning variable weights to different characteristics and sequences, mimicking human cognitive processes in prioritizing essential information.","Experimental analysis shows that RISAR significantly outperforms existing HAR systems in accuracy and efficiency, achieving an average accuracy of 97.26%.","These findings not only highlight the adaptability of the RISAR system but also underscore its potential as a robust solution for activity recognition across complex environments."],"url":"http://arxiv.org/abs/2402.17277v1","category":"eess.SY"}
{"created":"2024-02-27 07:31:30","title":"Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas","abstract":"The study of cooperation within social dilemmas has long been a fundamental topic across various disciplines, including computer science and social science. Recent advancements in Artificial Intelligence (AI) have significantly reshaped this field, offering fresh insights into understanding and enhancing cooperation. This survey examines three key areas at the intersection of AI and cooperation in social dilemmas. First, focusing on multi-agent cooperation, we review the intrinsic and external motivations that support cooperation among rational agents, and the methods employed to develop effective strategies against diverse opponents. Second, looking into human-agent cooperation, we discuss the current AI algorithms for cooperating with humans and the human biases towards AI agents. Third, we review the emergent field of leveraging AI agents to enhance cooperation among humans. We conclude by discussing future research avenues, such as using large language models, establishing unified theoretical frameworks, revisiting existing theories of human cooperation, and exploring multiple real-world applications.","sentences":["The study of cooperation within social dilemmas has long been a fundamental topic across various disciplines, including computer science and social science.","Recent advancements in Artificial Intelligence (AI) have significantly reshaped this field, offering fresh insights into understanding and enhancing cooperation.","This survey examines three key areas at the intersection of AI and cooperation in social dilemmas.","First, focusing on multi-agent cooperation, we review the intrinsic and external motivations that support cooperation among rational agents, and the methods employed to develop effective strategies against diverse opponents.","Second, looking into human-agent cooperation, we discuss the current AI algorithms for cooperating with humans and the human biases towards AI agents.","Third, we review the emergent field of leveraging AI agents to enhance cooperation among humans.","We conclude by discussing future research avenues, such as using large language models, establishing unified theoretical frameworks, revisiting existing theories of human cooperation, and exploring multiple real-world applications."],"url":"http://arxiv.org/abs/2402.17270v1","category":"cs.AI"}
{"created":"2024-02-27 07:11:59","title":"Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue","abstract":"Large Language Models (LLMs) have been demonstrated to generate illegal or unethical responses, particularly when subjected to \"jailbreak.\" Research on jailbreak has highlighted the safety issues of LLMs. However, prior studies have predominantly focused on single-turn dialogue, ignoring the potential complexities and risks presented by multi-turn dialogue, a crucial mode through which humans derive information from LLMs. In this paper, we argue that humans could exploit multi-turn dialogue to induce LLMs into generating harmful information. LLMs may not intend to reject cautionary or borderline unsafe queries, even if each turn is closely served for one malicious purpose in a multi-turn dialogue. Therefore, by decomposing an unsafe query into several sub-queries for multi-turn dialogue, we induced LLMs to answer harmful sub-questions incrementally, culminating in an overall harmful response. Our experiments, conducted across a wide range of LLMs, indicate current inadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our findings expose vulnerabilities of LLMs in complex scenarios involving multi-turn dialogue, presenting new challenges for the safety of LLMs.","sentences":["Large Language Models (LLMs) have been demonstrated to generate illegal or unethical responses, particularly when subjected to \"jailbreak.\"","Research on jailbreak has highlighted the safety issues of LLMs.","However, prior studies have predominantly focused on single-turn dialogue, ignoring the potential complexities and risks presented by multi-turn dialogue, a crucial mode through which humans derive information from LLMs.","In this paper, we argue that humans could exploit multi-turn dialogue to induce LLMs into generating harmful information.","LLMs may not intend to reject cautionary or borderline unsafe queries, even if each turn is closely served for one malicious purpose in a multi-turn dialogue.","Therefore, by decomposing an unsafe query into several sub-queries for multi-turn dialogue, we induced LLMs to answer harmful sub-questions incrementally, culminating in an overall harmful response.","Our experiments, conducted across a wide range of LLMs, indicate current inadequacies in the safety mechanisms of LLMs in multi-turn dialogue.","Our findings expose vulnerabilities of LLMs in complex scenarios involving multi-turn dialogue, presenting new challenges for the safety of LLMs."],"url":"http://arxiv.org/abs/2402.17262v1","category":"cs.CL"}
{"created":"2024-02-27 07:06:28","title":"A new graph-neural-network flavor tagger for Belle II and measurement of $\\sin2\u03c6_1$ in $B^0 \\to J/\u03c8K^0_\\text{S}$ decays","abstract":"We present GFlaT, a new algorithm that uses a graph-neural-network to determine the flavor of neutral $B$ mesons produced in $\\Upsilon(4S)$ decays. It improves previous algorithms by using the information from all charged final-state particles and the relations between them. We evaluate its performance using $B$ decays to flavor-specific hadronic final states reconstructed in a 362 $\\text{fb}^{-1}$ sample of electron-positron collisions collected at the $\\Upsilon(4S)$ resonance with the Belle II detector at the SuperKEKB collider. We achieve an effective tagging efficiency of $(37.40 \\pm 0.43 \\pm 0.36) \\%$, where the first uncertainty is statistical and the second systematic, which is $18\\%$ better than the previous Belle II algorithm. Demonstrating the algorithm, we use $B^{0}\\to J/\\psi K^0_\\text{S}$ decays to measure the mixing-induced and direct $CP$ violation parameters, $S = (0.724 \\pm 0.035 \\pm 0.014)$ and $C = (-0.035 \\pm 0.026 \\pm 0.013)$.","sentences":["We present GFlaT, a new algorithm that uses a graph-neural-network to determine the flavor of neutral $B$ mesons produced in $\\Upsilon(4S)$ decays.","It improves previous algorithms by using the information from all charged final-state particles and the relations between them.","We evaluate its performance using $B$ decays to flavor-specific hadronic final states reconstructed in a 362 $\\text{fb}^{-1}$ sample of electron-positron collisions collected at the $\\Upsilon(4S)$ resonance with the Belle II detector at the SuperKEKB collider.","We achieve an effective tagging efficiency of $(37.40 \\pm 0.43 \\pm 0.36) \\%$, where the first uncertainty is statistical and the second systematic, which is $18\\%$ better than the previous Belle II algorithm.","Demonstrating the algorithm, we use $B^{0}\\to J/\\psi K^0_\\text{S}$ decays to measure the mixing-induced and direct $CP$ violation parameters, $S = (0.724 \\pm 0.035 \\pm 0.014)$ and $C = (-0.035 \\pm 0.026 \\pm 0.013)$."],"url":"http://arxiv.org/abs/2402.17260v1","category":"hep-ex"}
{"created":"2024-02-27 07:03:25","title":"RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences","abstract":"Preference-based Reinforcement Learning (PbRL) avoids the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL algorithms over-reliance on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method incorporates a sample selection-based discriminator to dynamically filter denoised preferences for robust training. To mitigate the accumulated error caused by incorrect selection, we propose to warm start the reward model, which additionally bridges the performance gap during transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the current state-of-the-art PbRL method. Ablation studies further demonstrate that the warm start is crucial for both robustness and feedback-efficiency in limited-feedback cases.","sentences":["Preference-based Reinforcement Learning (PbRL) avoids the need for reward engineering by harnessing human preferences as the reward signal.","However, current PbRL algorithms over-reliance on high-quality feedback from domain experts, which results in a lack of robustness.","In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences.","Our method incorporates a sample selection-based discriminator to dynamically filter denoised preferences for robust training.","To mitigate the accumulated error caused by incorrect selection, we propose to warm start the reward model, which additionally bridges the performance gap during transition from pre-training to online training in PbRL.","Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the current state-of-the-art PbRL method.","Ablation studies further demonstrate that the warm start is crucial for both robustness and feedback-efficiency in limited-feedback cases."],"url":"http://arxiv.org/abs/2402.17257v1","category":"cs.LG"}
{"created":"2024-02-27 06:47:52","title":"Deep Learning-Based Speech and Vision Synthesis to Improve Phishing Attack Detection through a Multi-layer Adaptive Framework","abstract":"The ever-evolving ways attacker continues to im prove their phishing techniques to bypass existing state-of-the-art phishing detection methods pose a mountain of challenges to researchers in both industry and academia research due to the inability of current approaches to detect complex phishing attack. Thus, current anti-phishing methods remain vulnerable to complex phishing because of the increasingly sophistication tactics adopted by attacker coupled with the rate at which new tactics are being developed to evade detection. In this research, we proposed an adaptable framework that combines Deep learning and Randon Forest to read images, synthesize speech from deep-fake videos, and natural language processing at various predictions layered to significantly increase the performance of machine learning models for phishing attack detection.","sentences":["The ever-evolving ways attacker continues to im prove their phishing techniques to bypass existing state-of-the-art phishing detection methods pose a mountain of challenges to researchers in both industry and academia research due to the inability of current approaches to detect complex phishing attack.","Thus, current anti-phishing methods remain vulnerable to complex phishing because of the increasingly sophistication tactics adopted by attacker coupled with the rate at which new tactics are being developed to evade detection.","In this research, we proposed an adaptable framework that combines Deep learning and Randon Forest to read images, synthesize speech from deep-fake videos, and natural language processing at various predictions layered to significantly increase the performance of machine learning models for phishing attack detection."],"url":"http://arxiv.org/abs/2402.17249v1","category":"cs.CR"}
{"created":"2024-02-27 06:31:52","title":"Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation","abstract":"In this work, we share three insights for achieving state-of-the-art aesthetic quality in text-to-image generative models. We focus on three critical aspects for model improvement: enhancing color and contrast, improving generation across multiple aspect ratios, and improving human-centric fine details. First, we delve into the significance of the noise schedule in training a diffusion model, demonstrating its profound impact on realism and visual fidelity. Second, we address the challenge of accommodating various aspect ratios in image generation, emphasizing the importance of preparing a balanced bucketed dataset. Lastly, we investigate the crucial role of aligning model outputs with human preferences, ensuring that generated images resonate with human perceptual expectations. Through extensive analysis and experiments, Playground v2.5 demonstrates state-of-the-art performance in terms of aesthetic quality under various conditions and aspect ratios, outperforming both widely-used open-source models like SDXL and Playground v2, and closed-source commercial systems such as DALLE 3 and Midjourney v5.2. Our model is open-source, and we hope the development of Playground v2.5 provides valuable guidelines for researchers aiming to elevate the aesthetic quality of diffusion-based image generation models.","sentences":["In this work, we share three insights for achieving state-of-the-art aesthetic quality in text-to-image generative models.","We focus on three critical aspects for model improvement: enhancing color and contrast, improving generation across multiple aspect ratios, and improving human-centric fine details.","First, we delve into the significance of the noise schedule in training a diffusion model, demonstrating its profound impact on realism and visual fidelity.","Second, we address the challenge of accommodating various aspect ratios in image generation, emphasizing the importance of preparing a balanced bucketed dataset.","Lastly, we investigate the crucial role of aligning model outputs with human preferences, ensuring that generated images resonate with human perceptual expectations.","Through extensive analysis and experiments, Playground v2.5 demonstrates state-of-the-art performance in terms of aesthetic quality under various conditions and aspect ratios, outperforming both widely-used open-source models like SDXL and Playground v2, and closed-source commercial systems such as DALLE 3 and Midjourney v5.2.","Our model is open-source, and we hope the development of Playground v2.5 provides valuable guidelines for researchers aiming to elevate the aesthetic quality of diffusion-based image generation models."],"url":"http://arxiv.org/abs/2402.17245v1","category":"cs.CV"}
{"created":"2024-02-27 06:25:00","title":"Spike up Prime Interest in Science and Technology through Constructionist Games","abstract":"Robotics sets have been successfully used in elementary and secondary schools in conformance with the 'learning through play' philosophy fostered by LEGO Education, while utilizing the Constructionism didactic approach. Learners discover and acquire knowledge through first-hand tangible experiences, building their own representations in a constructivist learning process. Usual pedagogical goals of the activities include introduction to the principles of control, mechanics, programming, and robotics [1]. They are organized as hands-on learning situations with teamwork cooperation of learners, project-based learning, sharing and presentations of the learners group experiences. Arriving from this tradition, we focus on a slightly different scenarios: employing the robotics sets and the named approaches when learning Physics, Mathematics, Art, Science, and other subjects. In carefully designed projects, learners build interactive models that demonstrate concepts, principles, and phenomena, perform experiments, and modify them in elaboration phases with the aim to connect, create associations and links to the actual underlying theoretical curriculum. In this way, they are collecting practical experiences which are prerequisite to successful learning process. Based on feedback from children, we continue upon two previous sets of activities that focused on Physics and Mathematics, this time with projects built around games. Learners play various games with physical artifacts in the real-world - with the models they build. They acquire skills while playing the games, analyze them, and learn about the underlying principles. They modify the game rules, strategies, create extensions, and interact with each other in an entertaining and engaging settings. This time we have designed the activities together with the children, students of applied robotics seminar, and a student of Applied Informatics.","sentences":["Robotics sets have been successfully used in elementary and secondary schools in conformance with the 'learning through play' philosophy fostered by LEGO Education, while utilizing the Constructionism didactic approach.","Learners discover and acquire knowledge through first-hand tangible experiences, building their own representations in a constructivist learning process.","Usual pedagogical goals of the activities include introduction to the principles of control, mechanics, programming, and robotics [1].","They are organized as hands-on learning situations with teamwork cooperation of learners, project-based learning, sharing and presentations of the learners group experiences.","Arriving from this tradition, we focus on a slightly different scenarios: employing the robotics sets and the named approaches when learning Physics, Mathematics, Art, Science, and other subjects.","In carefully designed projects, learners build interactive models that demonstrate concepts, principles, and phenomena, perform experiments, and modify them in elaboration phases with the aim to connect, create associations and links to the actual underlying theoretical curriculum.","In this way, they are collecting practical experiences which are prerequisite to successful learning process.","Based on feedback from children, we continue upon two previous sets of activities that focused on Physics and Mathematics, this time with projects built around games.","Learners play various games with physical artifacts in the real-world - with the models they build.","They acquire skills while playing the games, analyze them, and learn about the underlying principles.","They modify the game rules, strategies, create extensions, and interact with each other in an entertaining and engaging settings.","This time we have designed the activities together with the children, students of applied robotics seminar, and a student of Applied Informatics."],"url":"http://arxiv.org/abs/2402.17243v1","category":"cs.RO"}
{"created":"2024-02-27 06:24:01","title":"HardTaint: Production-Run Dynamic Taint Analysis via Selective Hardware Tracing","abstract":"Dynamic taint analysis (DTA), as a fundamental analysis technique, is widely used in security, privacy, and diagnosis, etc. As DTA demands to collect and analyze massive taint data online, it suffers extremely high runtime overhead. Over the past decades, numerous attempts have been made to lower the overhead of DTA. Unfortunately, the reductions they achieved are marginal, causing DTA only applicable to the debugging/testing scenarios. In this paper, we propose and implement HardTaint, a system that can realize production-run dynamic taint tracking. HardTaint adopts a hybrid and systematic design which combines static analysis, selective hardware tracing and parallel graph processing techniques. The comprehensive evaluations demonstrate that HardTaint introduces only around 9% runtime overhead which is an order of magnitude lower than the state-of-the-arts, while without sacrificing any taint detection capability.","sentences":["Dynamic taint analysis (DTA), as a fundamental analysis technique, is widely used in security, privacy, and diagnosis, etc.","As DTA demands to collect and analyze massive taint data online, it suffers extremely high runtime overhead.","Over the past decades, numerous attempts have been made to lower the overhead of DTA.","Unfortunately, the reductions they achieved are marginal, causing DTA only applicable to the debugging/testing scenarios.","In this paper, we propose and implement HardTaint, a system that can realize production-run dynamic taint tracking.","HardTaint adopts a hybrid and systematic design which combines static analysis, selective hardware tracing and parallel graph processing techniques.","The comprehensive evaluations demonstrate that HardTaint introduces only around 9% runtime overhead which is an order of magnitude lower than the state-of-the-arts, while without sacrificing any taint detection capability."],"url":"http://arxiv.org/abs/2402.17241v1","category":"cs.CR"}
{"created":"2024-02-27 06:09:48","title":"A Review of Data Mining in Personalized Education: Current Trends and Future Prospects","abstract":"Personalized education, tailored to individual student needs, leverages educational technology and artificial intelligence (AI) in the digital age to enhance learning effectiveness. The integration of AI in educational platforms provides insights into academic performance, learning preferences, and behaviors, optimizing the personal learning process. Driven by data mining techniques, it not only benefits students but also provides educators and institutions with tools to craft customized learning experiences. To offer a comprehensive review of recent advancements in personalized educational data mining, this paper focuses on four primary scenarios: educational recommendation, cognitive diagnosis, knowledge tracing, and learning analysis. This paper presents a structured taxonomy for each area, compiles commonly used datasets, and identifies future research directions, emphasizing the role of data mining in enhancing personalized education and paving the way for future exploration and innovation.","sentences":["Personalized education, tailored to individual student needs, leverages educational technology and artificial intelligence (AI) in the digital age to enhance learning effectiveness.","The integration of AI in educational platforms provides insights into academic performance, learning preferences, and behaviors, optimizing the personal learning process.","Driven by data mining techniques, it not only benefits students but also provides educators and institutions with tools to craft customized learning experiences.","To offer a comprehensive review of recent advancements in personalized educational data mining, this paper focuses on four primary scenarios: educational recommendation, cognitive diagnosis, knowledge tracing, and learning analysis.","This paper presents a structured taxonomy for each area, compiles commonly used datasets, and identifies future research directions, emphasizing the role of data mining in enhancing personalized education and paving the way for future exploration and innovation."],"url":"http://arxiv.org/abs/2402.17236v1","category":"cs.CY"}
{"created":"2024-02-27 05:36:35","title":"Fourier analysis of near-field patterns generated by propagating polaritons","abstract":"Scattering-type scanning near-field optical microscope (s-SNOM) has become an essential tool to study polaritons - quasiparticles of light coupled to collective charge oscillations - via direct probing of their near field with a spatial resolution far beyond the diffraction limit. However, extraction of the polariton complex propagation constant from the near-field images requires subtle considerations that have not received necessary attention so far. In this study, we discuss important yet overlooked aspects of the near-field analysis. First, we experimentally demonstrate that the sample orientation inside the s-SNOM may significantly affect the near-field interference pattern of mid-infrared polaritons, leading to an error in momentum measurement up to 7.7% even for the modes with effective index of 12.5. Second, we establish a methodology to correctly extract the polariton damping rate from the interference fringes depending on their origin - the s-SNOM nano-tip or the material edge. Overall, our work provides a unified framework for the accurate extraction of the polariton momentum and damping from the near-field interference fringes.","sentences":["Scattering-type scanning near-field optical microscope (s-SNOM) has become an essential tool to study polaritons - quasiparticles of light coupled to collective charge oscillations - via direct probing of their near field with a spatial resolution far beyond the diffraction limit.","However, extraction of the polariton complex propagation constant from the near-field images requires subtle considerations that have not received necessary attention so far.","In this study, we discuss important yet overlooked aspects of the near-field analysis.","First, we experimentally demonstrate that the sample orientation inside the s-SNOM may significantly affect the near-field interference pattern of mid-infrared polaritons, leading to an error in momentum measurement up to 7.7% even for the modes with effective index of 12.5.","Second, we establish a methodology to correctly extract the polariton damping rate from the interference fringes depending on their origin - the s-SNOM nano-tip or the material edge.","Overall, our work provides a unified framework for the accurate extraction of the polariton momentum and damping from the near-field interference fringes."],"url":"http://arxiv.org/abs/2402.17225v1","category":"physics.optics"}
{"created":"2024-02-27 05:16:59","title":"Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning","abstract":"Offline safe reinforcement learning (RL) aims to train a constraint satisfaction policy from a fixed dataset. Current state-of-the-art approaches are based on supervised learning with a conditioned policy. However, these approaches fall short in real-world applications that involve complex tasks with rich temporal and logical structures. In this paper, we propose temporal logic Specification-conditioned Decision Transformer (SDT), a novel framework that harnesses the expressive power of signal temporal logic (STL) to specify complex temporal rules that an agent should follow and the sequential modeling capability of Decision Transformer (DT). Empirical evaluations on the DSRL benchmarks demonstrate the better capacity of SDT in learning safe and high-reward policies compared with existing approaches. In addition, SDT shows good alignment with respect to different desired degrees of satisfaction of the STL specification that it is conditioned on.","sentences":["Offline safe reinforcement learning (RL) aims to train a constraint satisfaction policy from a fixed dataset.","Current state-of-the-art approaches are based on supervised learning with a conditioned policy.","However, these approaches fall short in real-world applications that involve complex tasks with rich temporal and logical structures.","In this paper, we propose temporal logic Specification-conditioned Decision Transformer (SDT), a novel framework that harnesses the expressive power of signal temporal logic (STL) to specify complex temporal rules that an agent should follow and the sequential modeling capability of Decision Transformer (DT).","Empirical evaluations on the DSRL benchmarks demonstrate the better capacity of SDT in learning safe and high-reward policies compared with existing approaches.","In addition, SDT shows good alignment with respect to different desired degrees of satisfaction of the STL specification that it is conditioned on."],"url":"http://arxiv.org/abs/2402.17217v1","category":"cs.LG"}
{"created":"2024-02-27 05:14:27","title":"Application of Machine Learning Optimization in Cloud Computing Resource Scheduling and Management","abstract":"In recent years, cloud computing has been widely used. Cloud computing refers to the centralized computing resources, users through the access to the centralized resources to complete the calculation, the cloud computing center will return the results of the program processing to the user. Cloud computing is not only for individual users, but also for enterprise users. By purchasing a cloud server, users do not have to buy a large number of computers, saving computing costs. According to a report by China Economic News Network, the scale of cloud computing in China has reached 209.1 billion yuan. At present, the more mature cloud service providers in China are Ali Cloud, Baidu Cloud, Huawei Cloud and so on. Therefore, this paper proposes an innovative approach to solve complex problems in cloud computing resource scheduling and management using machine learning optimization techniques. Through in-depth study of challenges such as low resource utilization and unbalanced load in the cloud environment, this study proposes a comprehensive solution, including optimization methods such as deep learning and genetic algorithm, to improve system performance and efficiency, and thus bring new breakthroughs and progress in the field of cloud computing resource management.Rational allocation of resources plays a crucial role in cloud computing. In the resource allocation of cloud computing, the cloud computing center has limited cloud resources, and users arrive in sequence. Each user requests the cloud computing center to use a certain number of cloud resources at a specific time.","sentences":["In recent years, cloud computing has been widely used.","Cloud computing refers to the centralized computing resources, users through the access to the centralized resources to complete the calculation, the cloud computing center will return the results of the program processing to the user.","Cloud computing is not only for individual users, but also for enterprise users.","By purchasing a cloud server, users do not have to buy a large number of computers, saving computing costs.","According to a report by China Economic News Network, the scale of cloud computing in China has reached 209.1 billion yuan.","At present, the more mature cloud service providers in China are Ali Cloud, Baidu Cloud, Huawei Cloud and so on.","Therefore, this paper proposes an innovative approach to solve complex problems in cloud computing resource scheduling and management using machine learning optimization techniques.","Through in-depth study of challenges such as low resource utilization and unbalanced load in the cloud environment, this study proposes a comprehensive solution, including optimization methods such as deep learning and genetic algorithm, to improve system performance and efficiency, and thus bring new breakthroughs and progress in the field of cloud computing resource management.","Rational allocation of resources plays a crucial role in cloud computing.","In the resource allocation of cloud computing, the cloud computing center has limited cloud resources, and users arrive in sequence.","Each user requests the cloud computing center to use a certain number of cloud resources at a specific time."],"url":"http://arxiv.org/abs/2402.17216v1","category":"cs.DC"}
{"created":"2024-02-27 05:10:44","title":"VCD: Knowledge Base Guided Visual Commonsense Discovery in Images","abstract":"Visual commonsense contains knowledge about object properties, relationships, and behaviors in visual data. Discovering visual commonsense can provide a more comprehensive and richer understanding of images, and enhance the reasoning and decision-making capabilities of computer vision systems. However, the visual commonsense defined in existing visual commonsense discovery studies is coarse-grained and incomplete. In this work, we draw inspiration from a commonsense knowledge base ConceptNet in natural language processing, and systematically define the types of visual commonsense. Based on this, we introduce a new task, Visual Commonsense Discovery (VCD), aiming to extract fine-grained commonsense of different types contained within different objects in the image. We accordingly construct a dataset (VCDD) from Visual Genome and ConceptNet for VCD, featuring over 100,000 images and 14 million object-commonsense pairs. We furthermore propose a generative model (VCDM) that integrates a vision-language model with instruction tuning to tackle VCD. Automatic and human evaluations demonstrate VCDM's proficiency in VCD, particularly outperforming GPT-4V in implicit commonsense discovery. The value of VCD is further demonstrated by its application to two downstream tasks, including visual commonsense evaluation and visual question answering. The data and code will be made available on GitHub.","sentences":["Visual commonsense contains knowledge about object properties, relationships, and behaviors in visual data.","Discovering visual commonsense can provide a more comprehensive and richer understanding of images, and enhance the reasoning and decision-making capabilities of computer vision systems.","However, the visual commonsense defined in existing visual commonsense discovery studies is coarse-grained and incomplete.","In this work, we draw inspiration from a commonsense knowledge base ConceptNet in natural language processing, and systematically define the types of visual commonsense.","Based on this, we introduce a new task, Visual Commonsense Discovery (VCD), aiming to extract fine-grained commonsense of different types contained within different objects in the image.","We accordingly construct a dataset (VCDD) from Visual Genome and ConceptNet for VCD, featuring over 100,000 images and 14 million object-commonsense pairs.","We furthermore propose a generative model (VCDM) that integrates a vision-language model with instruction tuning to tackle VCD.","Automatic and human evaluations demonstrate VCDM's proficiency in VCD, particularly outperforming GPT-4V in implicit commonsense discovery.","The value of VCD is further demonstrated by its application to two downstream tasks, including visual commonsense evaluation and visual question answering.","The data and code will be made available on GitHub."],"url":"http://arxiv.org/abs/2402.17213v1","category":"cs.CV"}
{"created":"2024-02-27 04:55:03","title":"Measuring Vision-Language STEM Skills of Neural Models","abstract":"We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well below (averaging 54.7%) the performance of elementary students, not to mention near expert-level performance. To understand and increase the performance on our dataset, we teach the models on a training split of our dataset. Even though we observe improved performance, the model performance remains relatively low compared to average elementary students. To solve STEM problems, we will need novel algorithmic innovations from the community.","sentences":["We introduce a new challenge to test the STEM skills of neural models.","The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math).","Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM.","Our dataset features one of the largest and most comprehensive datasets for the challenge.","It includes 448 skills and 1,073,146 questions spanning all STEM subjects.","Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum.","We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark.","Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset.","In fact, these models are still well below (averaging 54.7%) the performance of elementary students, not to mention near expert-level performance.","To understand and increase the performance on our dataset, we teach the models on a training split of our dataset.","Even though we observe improved performance, the model performance remains relatively low compared to average elementary students.","To solve STEM problems, we will need novel algorithmic innovations from the community."],"url":"http://arxiv.org/abs/2402.17205v1","category":"cs.CL"}
{"created":"2024-02-27 04:18:56","title":"The Random Forest Model for Analyzing and Forecasting the US Stock Market in the Context of Smart Finance","abstract":"The stock market is a crucial component of the financial market, playing a vital role in wealth accumulation for investors, financing costs for listed companies, and the stable development of the national macroeconomy. Significant fluctuations in the stock market can damage the interests of stock investors and cause an imbalance in the industrial structure, which can interfere with the macro level development of the national economy. The prediction of stock price trends is a popular research topic in academia. Predicting the three trends of stock pricesrising, sideways, and falling can assist investors in making informed decisions about buying, holding, or selling stocks. Establishing an effective forecasting model for predicting these trends is of substantial practical importance. This paper evaluates the predictive performance of random forest models combined with artificial intelligence on a test set of four stocks using optimal parameters. The evaluation considers both predictive accuracy and time efficiency.","sentences":["The stock market is a crucial component of the financial market, playing a vital role in wealth accumulation for investors, financing costs for listed companies, and the stable development of the national macroeconomy.","Significant fluctuations in the stock market can damage the interests of stock investors and cause an imbalance in the industrial structure, which can interfere with the macro level development of the national economy.","The prediction of stock price trends is a popular research topic in academia.","Predicting the three trends of stock pricesrising, sideways, and falling can assist investors in making informed decisions about buying, holding, or selling stocks.","Establishing an effective forecasting model for predicting these trends is of substantial practical importance.","This paper evaluates the predictive performance of random forest models combined with artificial intelligence on a test set of four stocks using optimal parameters.","The evaluation considers both predictive accuracy and time efficiency."],"url":"http://arxiv.org/abs/2402.17194v1","category":"q-fin.TR"}
{"created":"2024-02-27 04:12:25","title":"AI-Driven Anonymization: Protecting Personal Data Privacy While Leveraging Machine Learning","abstract":"The development of artificial intelligence has significantly transformed people's lives. However, it has also posed a significant threat to privacy and security, with numerous instances of personal information being exposed online and reports of criminal attacks and theft. Consequently, the need to achieve intelligent protection of personal information through machine learning algorithms has become a paramount concern. Artificial intelligence leverages advanced algorithms and technologies to effectively encrypt and anonymize personal data, enabling valuable data analysis and utilization while safeguarding privacy. This paper focuses on personal data privacy protection and the promotion of anonymity as its core research objectives. It achieves personal data privacy protection and detection through the use of machine learning's differential privacy protection algorithm. The paper also addresses existing challenges in machine learning related to privacy and personal data protection, offers improvement suggestions, and analyzes factors impacting datasets to enable timely personal data privacy detection and protection.","sentences":["The development of artificial intelligence has significantly transformed people's lives.","However, it has also posed a significant threat to privacy and security, with numerous instances of personal information being exposed online and reports of criminal attacks and theft.","Consequently, the need to achieve intelligent protection of personal information through machine learning algorithms has become a paramount concern.","Artificial intelligence leverages advanced algorithms and technologies to effectively encrypt and anonymize personal data, enabling valuable data analysis and utilization while safeguarding privacy.","This paper focuses on personal data privacy protection and the promotion of anonymity as its core research objectives.","It achieves personal data privacy protection and detection through the use of machine learning's differential privacy protection algorithm.","The paper also addresses existing challenges in machine learning related to privacy and personal data protection, offers improvement suggestions, and analyzes factors impacting datasets to enable timely personal data privacy detection and protection."],"url":"http://arxiv.org/abs/2402.17191v1","category":"cs.CR"}
{"created":"2024-02-27 04:08:59","title":"An Effective Mixture-Of-Experts Approach For Code-Switching Speech Recognition Leveraging Encoder Disentanglement","abstract":"With the massive developments of end-to-end (E2E) neural networks, recent years have witnessed unprecedented breakthroughs in automatic speech recognition (ASR). However, the codeswitching phenomenon remains a major obstacle that hinders ASR from perfection, as the lack of labeled data and the variations between languages often lead to degradation of ASR performance. In this paper, we focus exclusively on improving the acoustic encoder of E2E ASR to tackle the challenge caused by the codeswitching phenomenon. Our main contributions are threefold: First, we introduce a novel disentanglement loss to enable the lower-layer of the encoder to capture inter-lingual acoustic information while mitigating linguistic confusion at the higher-layer of the encoder. Second, through comprehensive experiments, we verify that our proposed method outperforms the prior-art methods using pretrained dual-encoders, meanwhile having access only to the codeswitching corpus and consuming half of the parameterization. Third, the apparent differentiation of the encoders' output features also corroborates the complementarity between the disentanglement loss and the mixture-of-experts (MoE) architecture.","sentences":["With the massive developments of end-to-end (E2E) neural networks, recent years have witnessed unprecedented breakthroughs in automatic speech recognition (ASR).","However, the codeswitching phenomenon remains a major obstacle that hinders ASR from perfection, as the lack of labeled data and the variations between languages often lead to degradation of ASR performance.","In this paper, we focus exclusively on improving the acoustic encoder of E2E ASR to tackle the challenge caused by the codeswitching phenomenon.","Our main contributions are threefold:","First, we introduce a novel disentanglement loss to enable the lower-layer of the encoder to capture inter-lingual acoustic information while mitigating linguistic confusion at the higher-layer of the encoder.","Second, through comprehensive experiments, we verify that our proposed method outperforms the prior-art methods using pretrained dual-encoders, meanwhile having access only to the codeswitching corpus and consuming half of the parameterization.","Third, the apparent differentiation of the encoders' output features also corroborates the complementarity between the disentanglement loss and the mixture-of-experts (MoE) architecture."],"url":"http://arxiv.org/abs/2402.17189v1","category":"cs.CL"}
{"created":"2024-02-27 03:33:51","title":"Performance analysis of MUSIC-type imaging without diagonal elements of multi-static response matrix","abstract":"Generally, to apply the MUltiple SIgnal Classification (MUSIC) algorithm for the rapid imaging of small inhomogeneities, the complete elements of the multi-static response (MSR) matrix must be collected. However, in real-world applications such as microwave imaging or bistatic measurement configuration, diagonal elements of the MSR matrix are unknown. Nevertheless, it is possible to obtain imaging results using a traditional approach but theoretical reason of the applicability has not been investigated yet. In this paper, we establish mathematical structures of the imaging function of MUSIC from an MSR matrix without diagonal elements in both transverse magnetic (TM) and transverse electric (TE) polarizations. The established structures demonstrate why the shape of the location of small inhomogeneities can be retrieved via MUSIC without the diagonal elements of the MSR matrix. In addition, they reveal the intrinsic properties of imaging and the fundamental limitations. Results of numerical simulations are also provided to support the identified structures.","sentences":["Generally, to apply the MUltiple SIgnal Classification (MUSIC) algorithm for the rapid imaging of small inhomogeneities, the complete elements of the multi-static response (MSR) matrix must be collected.","However, in real-world applications such as microwave imaging or bistatic measurement configuration, diagonal elements of the MSR matrix are unknown.","Nevertheless, it is possible to obtain imaging results using a traditional approach but theoretical reason of the applicability has not been investigated yet.","In this paper, we establish mathematical structures of the imaging function of MUSIC from an MSR matrix without diagonal elements in both transverse magnetic (TM) and transverse electric (TE) polarizations.","The established structures demonstrate why the shape of the location of small inhomogeneities can be retrieved via MUSIC without the diagonal elements of the MSR matrix.","In addition, they reveal the intrinsic properties of imaging and the fundamental limitations.","Results of numerical simulations are also provided to support the identified structures."],"url":"http://arxiv.org/abs/2402.17180v1","category":"math.NA"}
{"created":"2024-02-27 03:30:58","title":"Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models","abstract":"Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.","sentences":["Sora is a text-to-video generative AI model, released by OpenAI in February 2024.","The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world.","Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models.","We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\".","Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing.","We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation.","Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation."],"url":"http://arxiv.org/abs/2402.17177v1","category":"cs.CV"}
{"created":"2024-02-27 03:08:44","title":"LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment","abstract":"For human-centric large-scale scenes, fine-grained modeling for 3D human global pose and shape is significant for scene understanding and can benefit many real-world applications. In this paper, we present LiveHPS, a novel single-LiDAR-based approach for scene-level human pose and shape estimation without any limitation of light conditions and wearable devices. In particular, we design a distillation mechanism to mitigate the distribution-varying effect of LiDAR point clouds and exploit the temporal-spatial geometric and dynamic information existing in consecutive frames to solve the occlusion and noise disturbance. LiveHPS, with its efficient configuration and high-quality output, is well-suited for real-world applications. Moreover, we propose a huge human motion dataset, named FreeMotion, which is collected in various scenarios with diverse human poses, shapes and translations. It consists of multi-modal and multi-view acquisition data from calibrated and synchronized LiDARs, cameras, and IMUs. Extensive experiments on our new dataset and other public datasets demonstrate the SOTA performance and robustness of our approach. We will release our code and dataset soon.","sentences":["For human-centric large-scale scenes, fine-grained modeling for 3D human global pose and shape is significant for scene understanding and can benefit many real-world applications.","In this paper, we present LiveHPS, a novel single-LiDAR-based approach for scene-level human pose and shape estimation without any limitation of light conditions and wearable devices.","In particular, we design a distillation mechanism to mitigate the distribution-varying effect of LiDAR point clouds and exploit the temporal-spatial geometric and dynamic information existing in consecutive frames to solve the occlusion and noise disturbance.","LiveHPS, with its efficient configuration and high-quality output, is well-suited for real-world applications.","Moreover, we propose a huge human motion dataset, named FreeMotion, which is collected in various scenarios with diverse human poses, shapes and translations.","It consists of multi-modal and multi-view acquisition data from calibrated and synchronized LiDARs, cameras, and IMUs.","Extensive experiments on our new dataset and other public datasets demonstrate the SOTA performance and robustness of our approach.","We will release our code and dataset soon."],"url":"http://arxiv.org/abs/2402.17171v1","category":"cs.CV"}
{"created":"2024-02-27 03:03:06","title":"Benchmarking Data Science Agents","abstract":"In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.","sentences":["In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists.","Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing.","Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process.","In this paper, we introduce DSEval -- a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle.","Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness.","Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field."],"url":"http://arxiv.org/abs/2402.17168v1","category":"cs.AI"}
{"created":"2024-02-27 02:54:22","title":"Few-shot adaptation for morphology-independent cell instance segmentation","abstract":"Microscopy data collections are becoming larger and more frequent. Accurate and precise quantitative analysis tools like cell instance segmentation are necessary to benefit from them. This is challenging due to the variability in the data, which requires retraining the segmentation model to maintain high accuracy on new collections. This is needed especially for segmenting cells with elongated and non-convex morphology like bacteria. We propose to reduce the amount of annotation and computing power needed for retraining the model by introducing a few-shot domain adaptation approach that requires annotating only one to five cells of the new data to process and that quickly adapts the model to maintain high accuracy. Our results show a significant boost in accuracy after adaptation to very challenging bacteria datasets.","sentences":["Microscopy data collections are becoming larger and more frequent.","Accurate and precise quantitative analysis tools like cell instance segmentation are necessary to benefit from them.","This is challenging due to the variability in the data, which requires retraining the segmentation model to maintain high accuracy on new collections.","This is needed especially for segmenting cells with elongated and non-convex morphology like bacteria.","We propose to reduce the amount of annotation and computing power needed for retraining the model by introducing a few-shot domain adaptation approach that requires annotating only one to five cells of the new data to process and that quickly adapts the model to maintain high accuracy.","Our results show a significant boost in accuracy after adaptation to very challenging bacteria datasets."],"url":"http://arxiv.org/abs/2402.17165v1","category":"cs.CV"}
{"created":"2024-02-27 02:47:50","title":"Large Language Model for Participatory Urban Planning","abstract":"Participatory urban planning is the mainstream of modern urban planning that involves the active engagement of residents. However, the traditional participatory paradigm requires experienced planning experts and is often time-consuming and costly. Fortunately, the emerging Large Language Models (LLMs) have shown considerable ability to simulate human-like agents, which can be used to emulate the participatory process easily. In this work, we introduce an LLM-based multi-agent collaboration framework for participatory urban planning, which can generate land-use plans for urban regions considering the diverse needs of residents. Specifically, we construct LLM agents to simulate a planner and thousands of residents with diverse profiles and backgrounds. We first ask the planner to carry out an initial land-use plan. To deal with the different facilities needs of residents, we initiate a discussion among the residents in each community about the plan, where residents provide feedback based on their profiles. Furthermore, to improve the efficiency of discussion, we adopt a fishbowl discussion mechanism, where part of the residents discuss and the rest of them act as listeners in each round. Finally, we let the planner modify the plan based on residents' feedback. We deploy our method on two real-world regions in Beijing. Experiments show that our method achieves state-of-the-art performance in residents satisfaction and inclusion metrics, and also outperforms human experts in terms of service accessibility and ecology metrics.","sentences":["Participatory urban planning is the mainstream of modern urban planning that involves the active engagement of residents.","However, the traditional participatory paradigm requires experienced planning experts and is often time-consuming and costly.","Fortunately, the emerging Large Language Models (LLMs) have shown considerable ability to simulate human-like agents, which can be used to emulate the participatory process easily.","In this work, we introduce an LLM-based multi-agent collaboration framework for participatory urban planning, which can generate land-use plans for urban regions considering the diverse needs of residents.","Specifically, we construct LLM agents to simulate a planner and thousands of residents with diverse profiles and backgrounds.","We first ask the planner to carry out an initial land-use plan.","To deal with the different facilities needs of residents, we initiate a discussion among the residents in each community about the plan, where residents provide feedback based on their profiles.","Furthermore, to improve the efficiency of discussion, we adopt a fishbowl discussion mechanism, where part of the residents discuss and the rest of them act as listeners in each round.","Finally, we let the planner modify the plan based on residents' feedback.","We deploy our method on two real-world regions in Beijing.","Experiments show that our method achieves state-of-the-art performance in residents satisfaction and inclusion metrics, and also outperforms human experts in terms of service accessibility and ecology metrics."],"url":"http://arxiv.org/abs/2402.17161v1","category":"cs.AI"}
{"created":"2024-02-27 02:47:09","title":"NocPlace: Nocturnal Visual Place Recognition Using Generative and Inherited Knowledge Transfer","abstract":"Visual Place Recognition (VPR) is crucial in computer vision, aiming to retrieve database images similar to a query image from an extensive collection of known images. However, like many vision-related tasks, learning-based VPR often experiences a decline in performance during nighttime due to the scarcity of nighttime images. Specifically, VPR needs to address the cross-domain problem of night-to-day rather than just the issue of a single nighttime domain. In response to these issues, we present NocPlace, which leverages a generated large-scale, multi-view, nighttime VPR dataset to embed resilience against dazzling lights and extreme darkness in the learned global descriptor. Firstly, we establish a day-night urban scene dataset called NightCities, capturing diverse nighttime scenarios and lighting variations across 60 cities globally. Following this, an unpaired image-to-image translation network is trained on this dataset. Using this trained translation network, we process an existing VPR dataset, thereby obtaining its nighttime version. The NocPlace is then fine-tuned using night-style images, the original labels, and descriptors inherited from the Daytime VPR model. Comprehensive experiments on various nighttime VPR test sets reveal that NocPlace considerably surpasses previous state-of-the-art methods.","sentences":["Visual Place Recognition (VPR) is crucial in computer vision, aiming to retrieve database images similar to a query image from an extensive collection of known images.","However, like many vision-related tasks, learning-based VPR often experiences a decline in performance during nighttime due to the scarcity of nighttime images.","Specifically, VPR needs to address the cross-domain problem of night-to-day rather than just the issue of a single nighttime domain.","In response to these issues, we present NocPlace, which leverages a generated large-scale, multi-view, nighttime VPR dataset to embed resilience against dazzling lights and extreme darkness in the learned global descriptor.","Firstly, we establish a day-night urban scene dataset called NightCities, capturing diverse nighttime scenarios and lighting variations across 60 cities globally.","Following this, an unpaired image-to-image translation network is trained on this dataset.","Using this trained translation network, we process an existing VPR dataset, thereby obtaining its nighttime version.","The NocPlace is then fine-tuned using night-style images, the original labels, and descriptors inherited from the Daytime VPR model.","Comprehensive experiments on various nighttime VPR test sets reveal that NocPlace considerably surpasses previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.17159v1","category":"cs.CV"}
{"created":"2024-02-27 02:41:46","title":"TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence Generation","abstract":"Designing protein sequences with specific biological functions and structural stability is crucial in biology and chemistry. Generative models already demonstrated their capabilities for reliable protein design. However, previous models are limited to the unconditional generation of protein sequences and lack the controllable generation ability that is vital to biological tasks. In this work, we propose TaxDiff, a taxonomic-guided diffusion model for controllable protein sequence generation that combines biological species information with the generative capabilities of diffusion models to generate structurally stable proteins within the sequence space. Specifically, taxonomic control information is inserted into each layer of the transformer block to achieve fine-grained control. The combination of global and local attention ensures the sequence consistency and structural foldability of taxonomic-specific proteins. Extensive experiments demonstrate that TaxDiff can consistently achieve better performance on multiple protein sequence generation benchmarks in both taxonomic-guided controllable generation and unconditional generation. Remarkably, the sequences generated by TaxDiff even surpass those produced by direct-structure-generation models in terms of confidence based on predicted structures and require only a quarter of the time of models based on the diffusion model. The code for generating proteins and training new versions of TaxDiff is available at:https://github.com/Linzy19/TaxDiff.","sentences":["Designing protein sequences with specific biological functions and structural stability is crucial in biology and chemistry.","Generative models already demonstrated their capabilities for reliable protein design.","However, previous models are limited to the unconditional generation of protein sequences and lack the controllable generation ability that is vital to biological tasks.","In this work, we propose TaxDiff, a taxonomic-guided diffusion model for controllable protein sequence generation that combines biological species information with the generative capabilities of diffusion models to generate structurally stable proteins within the sequence space.","Specifically, taxonomic control information is inserted into each layer of the transformer block to achieve fine-grained control.","The combination of global and local attention ensures the sequence consistency and structural foldability of taxonomic-specific proteins.","Extensive experiments demonstrate that TaxDiff can consistently achieve better performance on multiple protein sequence generation benchmarks in both taxonomic-guided controllable generation and unconditional generation.","Remarkably, the sequences generated by TaxDiff even surpass those produced by direct-structure-generation models in terms of confidence based on predicted structures and require only a quarter of the time of models based on the diffusion model.","The code for generating proteins and training new versions of TaxDiff is available at:https://github.com/Linzy19/TaxDiff."],"url":"http://arxiv.org/abs/2402.17156v1","category":"cs.LG"}
{"created":"2024-02-27 02:16:07","title":"Metasql: A Generate-then-Rank Framework for Natural Language to SQL Translation","abstract":"The Natural Language Interface to Databases (NLIDB) empowers non-technical users with database access through intuitive natural language (NL) interactions. Advanced approaches, utilizing neural sequence-to-sequence models or large-scale language models, typically employ auto-regressive decoding to generate unique SQL queries sequentially. While these translation models have greatly improved the overall translation accuracy, surpassing 70% on NLIDB benchmarks, the use of auto-regressive decoding to generate single SQL queries may result in sub-optimal outputs, potentially leading to erroneous translations. In this paper, we propose Metasql, a unified generate-then-rank framework that can be flexibly incorporated with existing NLIDBs to consistently improve their translation accuracy. Metasql introduces query metadata to control the generation of better SQL query candidates and uses learning-to-rank algorithms to retrieve globally optimized queries. Specifically, Metasql first breaks down the meaning of the given NL query into a set of possible query metadata, representing the basic concepts of the semantics. These metadata are then used as language constraints to steer the underlying translation model toward generating a set of candidate SQL queries. Finally, Metasql ranks the candidates to identify the best matching one for the given NL query. Extensive experiments are performed to study Metasql on two public NLIDB benchmarks. The results show that the performance of the translation models can be effectively improved using Metasql.","sentences":["The Natural Language Interface to Databases (NLIDB) empowers non-technical users with database access through intuitive natural language (NL) interactions.","Advanced approaches, utilizing neural sequence-to-sequence models or large-scale language models, typically employ auto-regressive decoding to generate unique SQL queries sequentially.","While these translation models have greatly improved the overall translation accuracy, surpassing 70% on NLIDB benchmarks, the use of auto-regressive decoding to generate single SQL queries may result in sub-optimal outputs, potentially leading to erroneous translations.","In this paper, we propose Metasql, a unified generate-then-rank framework that can be flexibly incorporated with existing NLIDBs to consistently improve their translation accuracy.","Metasql introduces query metadata to control the generation of better SQL query candidates and uses learning-to-rank algorithms to retrieve globally optimized queries.","Specifically, Metasql first breaks down the meaning of the given NL query into a set of possible query metadata, representing the basic concepts of the semantics.","These metadata are then used as language constraints to steer the underlying translation model toward generating a set of candidate SQL queries.","Finally, Metasql ranks the candidates to identify the best matching one for the given NL query.","Extensive experiments are performed to study Metasql on two public NLIDB benchmarks.","The results show that the performance of the translation models can be effectively improved using Metasql."],"url":"http://arxiv.org/abs/2402.17144v1","category":"cs.DB"}
{"created":"2024-02-27 02:05:29","title":"Video as the New Language for Real-World Decision Making","abstract":"Both text and video data are abundant on the internet and support large-scale self-supervised learning through next token or frame prediction. However, they have not been equally leveraged: language models have had significant real-world impact, whereas video generation has remained largely limited to media entertainment. Yet video data captures important information about the physical world that is difficult to express in language. To address this gap, we discuss an under-appreciated opportunity to extend video generation to solve tasks in the real world. We observe how, akin to language, video can serve as a unified interface that can absorb internet knowledge and represent diverse tasks. Moreover, we demonstrate how, like language models, video generation can serve as planners, agents, compute engines, and environment simulators through techniques such as in-context learning, planning and reinforcement learning. We identify major impact opportunities in domains such as robotics, self-driving, and science, supported by recent work that demonstrates how such advanced capabilities in video generation are plausibly within reach. Lastly, we identify key challenges in video generation that mitigate progress. Addressing these challenges will enable video generation models to demonstrate unique value alongside language models in a wider array of AI applications.","sentences":["Both text and video data are abundant on the internet and support large-scale self-supervised learning through next token or frame prediction.","However, they have not been equally leveraged: language models have had significant real-world impact, whereas video generation has remained largely limited to media entertainment.","Yet video data captures important information about the physical world that is difficult to express in language.","To address this gap, we discuss an under-appreciated opportunity to extend video generation to solve tasks in the real world.","We observe how, akin to language, video can serve as a unified interface that can absorb internet knowledge and represent diverse tasks.","Moreover, we demonstrate how, like language models, video generation can serve as planners, agents, compute engines, and environment simulators through techniques such as in-context learning, planning and reinforcement learning.","We identify major impact opportunities in domains such as robotics, self-driving, and science, supported by recent work that demonstrates how such advanced capabilities in video generation are plausibly within reach.","Lastly, we identify key challenges in video generation that mitigate progress.","Addressing these challenges will enable video generation models to demonstrate unique value alongside language models in a wider array of AI applications."],"url":"http://arxiv.org/abs/2402.17139v1","category":"cs.CV"}
{"created":"2024-02-27 01:59:02","title":"Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings","abstract":"Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods. Code for this project is provided at: https://github.com/kvfrans/fre","sentences":["Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner?","In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem.","Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder.","This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples.","We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods.","Code for this project is provided at: https://github.com/kvfrans/fre"],"url":"http://arxiv.org/abs/2402.17135v1","category":"cs.LG"}
{"created":"2024-02-27 01:48:19","title":"OSCaR: Object State Captioning and State Change Representation","abstract":"The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating multimodal large language models (MLLMs). Our experiments demonstrate that while MLLMs show some skill, they lack a full understanding of object state changes. The benchmark includes a fine-tuned model that, despite initial capabilities, requires significant improvements in accuracy and generalization ability for effective understanding of these changes. Our code and dataset are available at https://github.com/nguyennm1024/OSCaR.","sentences":["The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings.","This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language.","Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments.","Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of language.","To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark.","OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections.","It sets a new testbed for evaluating multimodal large language models (MLLMs).","Our experiments demonstrate that while MLLMs show some skill, they lack a full understanding of object state changes.","The benchmark includes a fine-tuned model that, despite initial capabilities, requires significant improvements in accuracy and generalization ability for effective understanding of these changes.","Our code and dataset are available at https://github.com/nguyennm1024/OSCaR."],"url":"http://arxiv.org/abs/2402.17128v1","category":"cs.CV"}
{"created":"2024-02-27 01:27:36","title":"A survey of energies from pure metals to multi-principal element alloys","abstract":"In materials science, a wide range of properties of materials are governed by various types of energies, including thermal, physicochemical, structural, and mechanical energies. In 2005, Dr. Frans Spaepen used crystalline face-centered-cubic (fcc) copper as an example to discuss a variety of phenomena that are associated with energies. Inspired by his pioneering work, we broaden our analysis to include a selection of representative pure metals with fcc, hexagonal close-packed (hcp), and body-centered cubic (bcc) structures. Additionally, we extend our comparison to energies between pure metals and equiatomic binary, ternary, and multi-principal element alloys (sometimes also known as high-entropy alloys). Through an extensive collection of data and calculations, we compile energy tables that provide a comprehensive view of how structure and alloying influence the energy profiles of these metals and alloys. We highlight the significant impact of constituent elements on the energies of alloys compared to pure metals and reveal a notable disparity in mechanical energies among materials in fcc-, hcp- and bcc-structured metals and alloys. Furthermore, we discuss the underlying mechanisms behind these patterns and discuss the implications for structural transformations, providing insights into the broader context of these energy variations.","sentences":["In materials science, a wide range of properties of materials are governed by various types of energies, including thermal, physicochemical, structural, and mechanical energies.","In 2005, Dr. Frans Spaepen used crystalline face-centered-cubic (fcc) copper as an example to discuss a variety of phenomena that are associated with energies.","Inspired by his pioneering work, we broaden our analysis to include a selection of representative pure metals with fcc, hexagonal close-packed (hcp), and body-centered cubic (bcc) structures.","Additionally, we extend our comparison to energies between pure metals and equiatomic binary, ternary, and multi-principal element alloys (sometimes also known as high-entropy alloys).","Through an extensive collection of data and calculations, we compile energy tables that provide a comprehensive view of how structure and alloying influence the energy profiles of these metals and alloys.","We highlight the significant impact of constituent elements on the energies of alloys compared to pure metals and reveal a notable disparity in mechanical energies among materials in fcc-, hcp- and bcc-structured metals and alloys.","Furthermore, we discuss the underlying mechanisms behind these patterns and discuss the implications for structural transformations, providing insights into the broader context of these energy variations."],"url":"http://arxiv.org/abs/2402.17121v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-27 01:19:53","title":"Transparent Image Layer Diffusion using Latent Transparency","abstract":"We present LayerDiffusion, an approach enabling large-scale pretrained latent diffusion models to generate transparent images. The method allows generation of single transparent images or of multiple transparent layers. The method learns a \"latent transparency\" that encodes alpha channel transparency into the latent manifold of a pretrained latent diffusion model. It preserves the production-ready quality of the large diffusion model by regulating the added transparency as a latent offset with minimal changes to the original latent distribution of the pretrained model. In this way, any latent diffusion model can be converted into a transparent image generator by finetuning it with the adjusted latent space. We train the model with 1M transparent image layer pairs collected using a human-in-the-loop collection scheme. We show that latent transparency can be applied to different open source image generators, or be adapted to various conditional control systems to achieve applications like foreground/background-conditioned layer generation, joint layer generation, structural control of layer contents, etc. A user study finds that in most cases (97%) users prefer our natively generated transparent content over previous ad-hoc solutions such as generating and then matting. Users also report the quality of our generated transparent images is comparable to real commercial transparent assets like Adobe Stock.","sentences":["We present LayerDiffusion, an approach enabling large-scale pretrained latent diffusion models to generate transparent images.","The method allows generation of single transparent images or of multiple transparent layers.","The method learns a \"latent transparency\" that encodes alpha channel transparency into the latent manifold of a pretrained latent diffusion model.","It preserves the production-ready quality of the large diffusion model by regulating the added transparency as a latent offset with minimal changes to the original latent distribution of the pretrained model.","In this way, any latent diffusion model can be converted into a transparent image generator by finetuning it with the adjusted latent space.","We train the model with 1M transparent image layer pairs collected using a human-in-the-loop collection scheme.","We show that latent transparency can be applied to different open source image generators, or be adapted to various conditional control systems to achieve applications like foreground/background-conditioned layer generation, joint layer generation, structural control of layer contents, etc.","A user study finds that in most cases (97%) users prefer our natively generated transparent content over previous ad-hoc solutions such as generating and then matting.","Users also report the quality of our generated transparent images is comparable to real commercial transparent assets like Adobe Stock."],"url":"http://arxiv.org/abs/2402.17113v1","category":"cs.CV"}
{"created":"2024-02-27 00:29:33","title":"T-HITL Effectively Addresses Problematic Associations in Image Generation and Maintains Overall Visual Quality","abstract":"Generative AI image models may inadvertently generate problematic representations of people. Past research has noted that millions of users engage daily across the world with these models and that the models, including through problematic representations of people, have the potential to compound and accelerate real-world discrimination and other harms (Bianchi et al, 2023). In this paper, we focus on addressing the generation of problematic associations between demographic groups and semantic concepts that may reflect and reinforce negative narratives embedded in social data. Building on sociological literature (Blumer, 1958) and mapping representations to model behaviors, we have developed a taxonomy to study problematic associations in image generation models. We explore the effectiveness of fine tuning at the model level as a method to address these associations, identifying a potential reduction in visual quality as a limitation of traditional fine tuning. We also propose a new methodology with twice-human-in-the-loop (T-HITL) that promises improvements in both reducing problematic associations and also maintaining visual quality. We demonstrate the effectiveness of T-HITL by providing evidence of three problematic associations addressed by T-HITL at the model level. Our contributions to scholarship are two-fold. By defining problematic associations in the context of machine learning models and generative AI, we introduce a conceptual and technical taxonomy for addressing some of these associations. Finally, we provide a method, T-HITL, that addresses these associations and simultaneously maintains visual quality of image model generations. This mitigation need not be a tradeoff, but rather an enhancement.","sentences":["Generative AI image models may inadvertently generate problematic representations of people.","Past research has noted that millions of users engage daily across the world with these models and that the models, including through problematic representations of people, have the potential to compound and accelerate real-world discrimination and other harms (Bianchi et al, 2023).","In this paper, we focus on addressing the generation of problematic associations between demographic groups and semantic concepts that may reflect and reinforce negative narratives embedded in social data.","Building on sociological literature (Blumer, 1958) and mapping representations to model behaviors, we have developed a taxonomy to study problematic associations in image generation models.","We explore the effectiveness of fine tuning at the model level as a method to address these associations, identifying a potential reduction in visual quality as a limitation of traditional fine tuning.","We also propose a new methodology with twice-human-in-the-loop (T-HITL) that promises improvements in both reducing problematic associations and also maintaining visual quality.","We demonstrate the effectiveness of T-HITL by providing evidence of three problematic associations addressed by T-HITL at the model level.","Our contributions to scholarship are two-fold.","By defining problematic associations in the context of machine learning models and generative AI, we introduce a conceptual and technical taxonomy for addressing some of these associations.","Finally, we provide a method, T-HITL, that addresses these associations and simultaneously maintains visual quality of image model generations.","This mitigation need not be a tradeoff, but rather an enhancement."],"url":"http://arxiv.org/abs/2402.17101v1","category":"cs.CV"}
{"created":"2024-02-27 00:22:18","title":"Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses","abstract":"Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios. Recently, various methods are proposed to check the factual errors in the LLM-generated texts and revise them accordingly, to reduce the hallucination issue. In this paper, we propose Re-Ex, a method of revising LLM-generated texts, which introduces a novel step dubbed as the factual error explanation step. Re-Ex revises the initial response of LLMs using 3-steps: first, external tools are used to get the evidences on the factual errors in the response; second, LLMs are instructed to explain the problematic parts of the response based on the evidences gathered in the first step; finally, LLMs revise the response using the explanation obtained in the second step. In addition to the explanation step, we propose new prompting techniques to reduce the amount of tokens and wall-clock time required for the response revision process. Compared with existing methods including Factool, CoVE, and RARR, Re-Ex provides better revision performance with less time and fewer tokens in multiple benchmarks.","sentences":["Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios.","Recently, various methods are proposed to check the factual errors in the LLM-generated texts and revise them accordingly, to reduce the hallucination issue.","In this paper, we propose Re-Ex, a method of revising LLM-generated texts, which introduces a novel step dubbed as the factual error explanation step.","Re-Ex revises the initial response of LLMs using 3-steps: first, external tools are used to get the evidences on the factual errors in the response; second, LLMs are instructed to explain the problematic parts of the response based on the evidences gathered in the first step; finally, LLMs revise the response using the explanation obtained in the second step.","In addition to the explanation step, we propose new prompting techniques to reduce the amount of tokens and wall-clock time required for the response revision process.","Compared with existing methods including Factool, CoVE, and RARR, Re-Ex provides better revision performance with less time and fewer tokens in multiple benchmarks."],"url":"http://arxiv.org/abs/2402.17097v1","category":"cs.CL"}
{"created":"2024-02-27 00:17:20","title":"Fibre-integrated van der Waals quantum sensor with an optimal cavity interface","abstract":"Integrating quantum materials with fibre optics adds advanced functionalities to a variety of applications, and introduces fibre-based quantum devices such as remote sensors capable of probing multiple physical parameters. However, achieving optimal integration between quantum materials and fibres is challenging, particularly due to difficulties in fabrication of quantum elements with suitable dimensions and an efficient photonic interface to a commercial optical fibre. Here we demonstrate a new modality for a fibre-integrated van der Waals quantum sensor. We design and fabricate a hole-based circular Bragg grating cavity from hexagonal boron nitride (hBN), engineer optically active spin defects within the cavity, and integrate the cavity with an optical fibre using a deterministic pattern transfer technique. The fibre-integrated hBN cavity enables efficient excitation and collection of optical signals from spin defects in hBN, thereby enabling all-fibre integrated quantum sensors. Moreover, we demonstrate remote sensing of a ferromagnetic material and of arbitrary magnetic fields. All in all, the hybrid fibre-based quantum sensing platform may pave the way to a new generation of robust, remote, multi-functional quantum sensors.","sentences":["Integrating quantum materials with fibre optics adds advanced functionalities to a variety of applications, and introduces fibre-based quantum devices such as remote sensors capable of probing multiple physical parameters.","However, achieving optimal integration between quantum materials and fibres is challenging, particularly due to difficulties in fabrication of quantum elements with suitable dimensions and an efficient photonic interface to a commercial optical fibre.","Here we demonstrate a new modality for a fibre-integrated van der Waals quantum sensor.","We design and fabricate a hole-based circular Bragg grating cavity from hexagonal boron nitride (hBN), engineer optically active spin defects within the cavity, and integrate the cavity with an optical fibre using a deterministic pattern transfer technique.","The fibre-integrated hBN cavity enables efficient excitation and collection of optical signals from spin defects in hBN, thereby enabling all-fibre integrated quantum sensors.","Moreover, we demonstrate remote sensing of a ferromagnetic material and of arbitrary magnetic fields.","All in all, the hybrid fibre-based quantum sensing platform may pave the way to a new generation of robust, remote, multi-functional quantum sensors."],"url":"http://arxiv.org/abs/2402.17095v1","category":"quant-ph"}
{"created":"2024-02-26 23:53:34","title":"A Note on Bayesian Networks with Latent Root Variables","abstract":"We characterise the likelihood function computed from a Bayesian network with latent variables as root nodes. We show that the marginal distribution over the remaining, manifest, variables also factorises as a Bayesian network, which we call empirical. A dataset of observations of the manifest variables allows us to quantify the parameters of the empirical Bayesian net. We prove that (i) the likelihood of such a dataset from the original Bayesian network is dominated by the global maximum of the likelihood from the empirical one; and that (ii) such a maximum is attained if and only if the parameters of the Bayesian network are consistent with those of the empirical model.","sentences":["We characterise the likelihood function computed from a Bayesian network with latent variables as root nodes.","We show that the marginal distribution over the remaining, manifest, variables also factorises as a Bayesian network, which we call empirical.","A dataset of observations of the manifest variables allows us to quantify the parameters of the empirical Bayesian net.","We prove that (i) the likelihood of such a dataset from the original Bayesian network is dominated by the global maximum of the likelihood from the empirical one; and that (ii) such a maximum is attained if and only if the parameters of the Bayesian network are consistent with those of the empirical model."],"url":"http://arxiv.org/abs/2402.17087v1","category":"stat.ML"}
{"created":"2024-02-26 23:53:18","title":"An optimal transport model for dynamical shapes, collective motion and cellular aggregates","abstract":"Many biological systems such as cell aggregates, tissues or bacterial colonies behave as unconventional systems of particles that are strongly constrained by volume exclusion and shape interactions. Understanding how these constraints lead to macroscopic self-organized structures is a fundamental question in e.g. developmental biology. To this end, various types of computational models have been developed: phase fields, cellular automata, vertex models, level-set, finite element simulations, etc. We introduce a new framework based on optimal transport theory to model particle systems with arbitrary dynamical shapes and deformability. Our method builds upon the pioneering work of Brenier on incompressible fluids and its recent applications to materials science. It lets us specify the shapes of individual cells and supports a wide range of interaction mechanisms, while automatically taking care of the volume exclusion constraint at an affordable numerical cost. We showcase the versatility of this approach by reproducing several classical systems in computational biology. Our Python code is freely available at: www.github.com/antoinediez/ICeShOT","sentences":["Many biological systems such as cell aggregates, tissues or bacterial colonies behave as unconventional systems of particles that are strongly constrained by volume exclusion and shape interactions.","Understanding how these constraints lead to macroscopic self-organized structures is a fundamental question in e.g. developmental biology.","To this end, various types of computational models have been developed: phase fields, cellular automata, vertex models, level-set, finite element simulations, etc.","We introduce a new framework based on optimal transport theory to model particle systems with arbitrary dynamical shapes and deformability.","Our method builds upon the pioneering work of Brenier on incompressible fluids and its recent applications to materials science.","It lets us specify the shapes of individual cells and supports a wide range of interaction mechanisms, while automatically taking care of the volume exclusion constraint at an affordable numerical cost.","We showcase the versatility of this approach by reproducing several classical systems in computational biology.","Our Python code is freely available at: www.github.com/antoinediez/ICeShOT"],"url":"http://arxiv.org/abs/2402.17086v1","category":"q-bio.QM"}
{"created":"2024-02-26 23:40:33","title":"Deconstructing the Veneer of Simplicity: Co-Designing Introductory Generative AI Workshops with Local Entrepreneurs","abstract":"Generative AI platforms and features are permeating many aspects of work. Entrepreneurs from lean economies in particular are well positioned to outsource tasks to generative AI given limited resources. In this paper, we work to address a growing disparity in use of these technologies by building on a four-year partnership with a local entrepreneurial hub dedicated to equity in tech and entrepreneurship. Together, we co-designed an interactive workshops series aimed to onboard local entrepreneurs to generative AI platforms. Alongside four community-driven and iterative workshops with entrepreneurs across five months, we conducted interviews with 15 local entrepreneurs and community providers. We detail the importance of communal and supportive exposure to generative AI tools for local entrepreneurs, scaffolding actionable use (and supporting non-use), demystifying generative AI technologies by emphasizing entrepreneurial power, while simultaneously deconstructing the veneer of simplicity to address the many operational skills needed for successful application.","sentences":["Generative AI platforms and features are permeating many aspects of work.","Entrepreneurs from lean economies in particular are well positioned to outsource tasks to generative AI given limited resources.","In this paper, we work to address a growing disparity in use of these technologies by building on a four-year partnership with a local entrepreneurial hub dedicated to equity in tech and entrepreneurship.","Together, we co-designed an interactive workshops series aimed to onboard local entrepreneurs to generative AI platforms.","Alongside four community-driven and iterative workshops with entrepreneurs across five months, we conducted interviews with 15 local entrepreneurs and community providers.","We detail the importance of communal and supportive exposure to generative AI tools for local entrepreneurs, scaffolding actionable use (and supporting non-use), demystifying generative AI technologies by emphasizing entrepreneurial power, while simultaneously deconstructing the veneer of simplicity to address the many operational skills needed for successful application."],"url":"http://arxiv.org/abs/2402.17082v1","category":"cs.HC"}
{"created":"2024-02-26 23:15:01","title":"One-Shot Graph Representation Learning Using Hyperdimensional Computing","abstract":"We present a novel, simple, fast, and efficient approach for semi-supervised learning on graphs. The proposed approach takes advantage of hyper-dimensional computing which encodes data samples using random projections into a high dimensional space (HD space for short). Specifically, we propose a Hyper-dimensional Graph Learning (HDGL) algorithm that leverages the injectivity property of the node representations of a family of graph neural networks. HDGL maps node features to the HD space and then uses HD operators such as bundling and binding to aggregate information from the local neighborhood of each node. Results of experiments with widely used benchmark data sets show that HDGL achieves predictive performance that is competitive with the state-of-the-art deep learning methods, without the need for computationally expensive training.","sentences":["We present a novel, simple, fast, and efficient approach for semi-supervised learning on graphs.","The proposed approach takes advantage of hyper-dimensional computing which encodes data samples using random projections into a high dimensional space (HD space for short).","Specifically, we propose a Hyper-dimensional Graph Learning (HDGL) algorithm that leverages the injectivity property of the node representations of a family of graph neural networks.","HDGL maps node features to the HD space and then uses HD operators such as bundling and binding to aggregate information from the local neighborhood of each node.","Results of experiments with widely used benchmark data sets show that HDGL achieves predictive performance that is competitive with the state-of-the-art deep learning methods, without the need for computationally expensive training."],"url":"http://arxiv.org/abs/2402.17073v1","category":"cs.LG"}
{"created":"2024-02-26 23:03:00","title":"Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions","abstract":"Despite the extensive research on training generative adversarial networks (GANs) with limited training data, learning to generate images from long-tailed training distributions remains fairly unexplored. In the presence of imbalanced multi-class training data, GANs tend to favor classes with more samples, leading to the generation of low-quality and less diverse samples in tail classes. In this study, we aim to improve the training of class-conditional GANs with long-tailed data. We propose a straightforward yet effective method for knowledge sharing, allowing tail classes to borrow from the rich information from classes with more abundant training data. More concretely, we propose modifications to existing class-conditional GAN architectures to ensure that the lower-resolution layers of the generator are trained entirely unconditionally while reserving class-conditional generation for the higher-resolution layers. Experiments on several long-tail benchmarks and GAN architectures demonstrate a significant improvement over existing methods in both the diversity and fidelity of the generated images. The code is available at https://github.com/khorrams/utlo.","sentences":["Despite the extensive research on training generative adversarial networks (GANs) with limited training data, learning to generate images from long-tailed training distributions remains fairly unexplored.","In the presence of imbalanced multi-class training data, GANs tend to favor classes with more samples, leading to the generation of low-quality and less diverse samples in tail classes.","In this study, we aim to improve the training of class-conditional GANs with long-tailed data.","We propose a straightforward yet effective method for knowledge sharing, allowing tail classes to borrow from the rich information from classes with more abundant training data.","More concretely, we propose modifications to existing class-conditional GAN architectures to ensure that the lower-resolution layers of the generator are trained entirely unconditionally while reserving class-conditional generation for the higher-resolution layers.","Experiments on several long-tail benchmarks and GAN architectures demonstrate a significant improvement over existing methods in both the diversity and fidelity of the generated images.","The code is available at https://github.com/khorrams/utlo."],"url":"http://arxiv.org/abs/2402.17065v1","category":"cs.CV"}
{"created":"2024-02-26 22:19:30","title":"Reinforcement Learning Based Oscillation Dampening: Scaling up Single-Agent RL algorithms to a 100 AV highway field operational test","abstract":"In this article, we explore the technical details of the reinforcement learning (RL) algorithms that were deployed in the largest field test of automated vehicles designed to smooth traffic flow in history as of 2023, uncovering the challenges and breakthroughs that come with developing RL controllers for automated vehicles. We delve into the fundamental concepts behind RL algorithms and their application in the context of self-driving cars, discussing the developmental process from simulation to deployment in detail, from designing simulators to reward function shaping. We present the results in both simulation and deployment, discussing the flow-smoothing benefits of the RL controller. From understanding the basics of Markov decision processes to exploring advanced techniques such as deep RL, our article offers a comprehensive overview and deep dive of the theoretical foundations and practical implementations driving this rapidly evolving field. We also showcase real-world case studies and alternative research projects that highlight the impact of RL controllers in revolutionizing autonomous driving. From tackling complex urban environments to dealing with unpredictable traffic scenarios, these intelligent controllers are pushing the boundaries of what automated vehicles can achieve. Furthermore, we examine the safety considerations and hardware-focused technical details surrounding deployment of RL controllers into automated vehicles. As these algorithms learn and evolve through interactions with the environment, ensuring their behavior aligns with safety standards becomes crucial. We explore the methodologies and frameworks being developed to address these challenges, emphasizing the importance of building reliable control systems for automated vehicles.","sentences":["In this article, we explore the technical details of the reinforcement learning (RL) algorithms that were deployed in the largest field test of automated vehicles designed to smooth traffic flow in history as of 2023, uncovering the challenges and breakthroughs that come with developing RL controllers for automated vehicles.","We delve into the fundamental concepts behind RL algorithms and their application in the context of self-driving cars, discussing the developmental process from simulation to deployment in detail, from designing simulators to reward function shaping.","We present the results in both simulation and deployment, discussing the flow-smoothing benefits of the RL controller.","From understanding the basics of Markov decision processes to exploring advanced techniques such as deep RL, our article offers a comprehensive overview and deep dive of the theoretical foundations and practical implementations driving this rapidly evolving field.","We also showcase real-world case studies and alternative research projects that highlight the impact of RL controllers in revolutionizing autonomous driving.","From tackling complex urban environments to dealing with unpredictable traffic scenarios, these intelligent controllers are pushing the boundaries of what automated vehicles can achieve.","Furthermore, we examine the safety considerations and hardware-focused technical details surrounding deployment of RL controllers into automated vehicles.","As these algorithms learn and evolve through interactions with the environment, ensuring their behavior aligns with safety standards becomes crucial.","We explore the methodologies and frameworks being developed to address these challenges, emphasizing the importance of building reliable control systems for automated vehicles."],"url":"http://arxiv.org/abs/2402.17050v1","category":"eess.SY"}
{"created":"2024-02-26 22:04:25","title":"An Investigation into the Performances of the State-of-the-art Machine Learning Approaches for Various Cyber-attack Detection: A Survey","abstract":"To secure computers and information systems from attackers taking advantage of vulnerabilities in the system to commit cybercrime, several methods have been proposed for real-time detection of vulnerabilities to improve security around information systems. Of all the proposed methods, machine learning had been the most effective method in securing a system with capabilities ranging from early detection of software vulnerabilities to real-time detection of ongoing compromise in a system. As there are different types of cyberattacks, each of the existing state-of-the-art machine learning models depends on different algorithms for training which also impact their suitability for detection of a particular type of cyberattack. In this research, we analyzed each of the current state-of-theart machine learning models for different types of cyberattack detection from the past 10 years with a major emphasis on the most recent works for comparative study to identify the knowledge gap where work is still needed to be done with regard to detection of each category of cyberattack","sentences":["To secure computers and information systems from attackers taking advantage of vulnerabilities in the system to commit cybercrime, several methods have been proposed for real-time detection of vulnerabilities to improve security around information systems.","Of all the proposed methods, machine learning had been the most effective method in securing a system with capabilities ranging from early detection of software vulnerabilities to real-time detection of ongoing compromise in a system.","As there are different types of cyberattacks, each of the existing state-of-the-art machine learning models depends on different algorithms for training which also impact their suitability for detection of a particular type of cyberattack.","In this research, we analyzed each of the current state-of-theart machine learning models for different types of cyberattack detection from the past 10 years with a major emphasis on the most recent works for comparative study to identify the knowledge gap where work is still needed to be done with regard to detection of each category of cyberattack"],"url":"http://arxiv.org/abs/2402.17045v1","category":"cs.CR"}
{"created":"2024-02-26 21:49:44","title":"Towards Generalizing Inferences from Trials to Target Populations","abstract":"Randomized Controlled Trials (RCTs) are pivotal in generating internally valid estimates with minimal assumptions, serving as a cornerstone for researchers dedicated to advancing causal inference methods. However, extending these findings beyond the experimental cohort to achieve externally valid estimates is crucial for broader scientific inquiry. This paper delves into the forefront of addressing these external validity challenges, encapsulating the essence of a multidisciplinary workshop held at the Institute for Computational and Experimental Research in Mathematics (ICERM), Brown University, in Fall 2023. The workshop congregated experts from diverse fields including social science, medicine, public health, statistics, computer science, and education, to tackle the unique obstacles each discipline faces in extrapolating experimental findings. Our study presents three key contributions: we integrate ongoing efforts, highlighting methodological synergies across fields; provide an exhaustive review of generalizability and transportability based on the workshop's discourse; and identify persistent hurdles while suggesting avenues for future research. By doing so, this paper aims to enhance the collective understanding of the generalizability and transportability of causal effects, fostering cross-disciplinary collaboration and offering valuable insights for researchers working on refining and applying causal inference methods.","sentences":["Randomized Controlled Trials (RCTs) are pivotal in generating internally valid estimates with minimal assumptions, serving as a cornerstone for researchers dedicated to advancing causal inference methods.","However, extending these findings beyond the experimental cohort to achieve externally valid estimates is crucial for broader scientific inquiry.","This paper delves into the forefront of addressing these external validity challenges, encapsulating the essence of a multidisciplinary workshop held at the Institute for Computational and Experimental Research in Mathematics (ICERM), Brown University, in Fall 2023.","The workshop congregated experts from diverse fields including social science, medicine, public health, statistics, computer science, and education, to tackle the unique obstacles each discipline faces in extrapolating experimental findings.","Our study presents three key contributions: we integrate ongoing efforts, highlighting methodological synergies across fields; provide an exhaustive review of generalizability and transportability based on the workshop's discourse; and identify persistent hurdles while suggesting avenues for future research.","By doing so, this paper aims to enhance the collective understanding of the generalizability and transportability of causal effects, fostering cross-disciplinary collaboration and offering valuable insights for researchers working on refining and applying causal inference methods."],"url":"http://arxiv.org/abs/2402.17042v1","category":"stat.ME"}
{"created":"2024-02-26 21:21:30","title":"REFACTOR: Learning to Extract Theorems from Proofs","abstract":"Human mathematicians are often good at recognizing modular and reusable theorems that make complex mathematical results within reach. In this paper, we propose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for training neural networks to mimic this ability in formal mathematical theorem proving. We show on a set of unseen proofs, REFACTOR is able to extract 19.6% of the theorems that humans would use to write the proofs. When applying the model to the existing Metamath library, REFACTOR extracted 16 new theorems. With newly extracted theorems, we show that the existing proofs in the MetaMath database can be refactored. The new theorems are used very frequently after refactoring, with an average usage of 733.5 times, and help shorten the proof lengths. Lastly, we demonstrate that the prover trained on the new-theorem refactored dataset proves more test theorems and outperforms state-of-the-art baselines by frequently leveraging a diverse set of newly extracted theorems. Code can be found at https://github.com/jinpz/refactor.","sentences":["Human mathematicians are often good at recognizing modular and reusable theorems that make complex mathematical results within reach.","In this paper, we propose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for training neural networks to mimic this ability in formal mathematical theorem proving.","We show on a set of unseen proofs, REFACTOR is able to extract 19.6% of the theorems that humans would use to write the proofs.","When applying the model to the existing Metamath library, REFACTOR extracted 16 new theorems.","With newly extracted theorems, we show that the existing proofs in the MetaMath database can be refactored.","The new theorems are used very frequently after refactoring, with an average usage of 733.5 times, and help shorten the proof lengths.","Lastly, we demonstrate that the prover trained on the new-theorem refactored dataset proves more test theorems and outperforms state-of-the-art baselines by frequently leveraging a diverse set of newly extracted theorems.","Code can be found at https://github.com/jinpz/refactor."],"url":"http://arxiv.org/abs/2402.17032v1","category":"cs.AI"}
{"created":"2024-02-26 20:55:47","title":"A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection","abstract":"We tested front-end enhanced neural models where a frozen classifier was prepended by a differentiable and fully convolutional model with a skip connection. By training them using a small learning rate for about one epoch, we obtained models that retained the accuracy of the backbone classifier while being unusually resistant to gradient attacks including APGD and FAB-T attacks from the AutoAttack package, which we attributed to gradient masking. The gradient masking phenomenon is not new, but the degree of masking was quite remarkable for fully differentiable models that did not have gradient-shattering components such as JPEG compression or components that are expected to cause diminishing gradients.   Though black box attacks can be partially effective against gradient masking, they are easily defeated by combining models into randomized ensembles. We estimate that such ensembles achieve near-SOTA AutoAttack accuracy on CIFAR10, CIFAR100, and ImageNet despite having virtually zero accuracy under adaptive attacks. Adversarial training of the backbone classifier can further increase resistance of the front-end enhanced model to gradient attacks. On CIFAR10, the respective randomized ensemble achieved 90.8$\\pm 2.5$% (99% CI) accuracy under AutoAttack while having only 18.2$\\pm 3.6$% accuracy under the adaptive attack.   We do not establish SOTA in adversarial robustness. Instead, we make methodological contributions and further supports the thesis that adaptive attacks designed with the complete knowledge of model architecture are crucial in demonstrating model robustness and that even the so-called white-box gradient attacks can have limited applicability. Although gradient attacks can be complemented with black-box attack such as the SQUARE attack or the zero-order PGD, black-box attacks can be weak against randomized ensembles, e.g., when ensemble models mask gradients.","sentences":["We tested front-end enhanced neural models where a frozen classifier was prepended by a differentiable and fully convolutional model with a skip connection.","By training them using a small learning rate for about one epoch, we obtained models that retained the accuracy of the backbone classifier while being unusually resistant to gradient attacks including APGD and FAB-T attacks from the AutoAttack package, which we attributed to gradient masking.","The gradient masking phenomenon is not new, but the degree of masking was quite remarkable for fully differentiable models that did not have gradient-shattering components such as JPEG compression or components that are expected to cause diminishing gradients.   ","Though black box attacks can be partially effective against gradient masking, they are easily defeated by combining models into randomized ensembles.","We estimate that such ensembles achieve near-SOTA AutoAttack accuracy on CIFAR10, CIFAR100, and ImageNet despite having virtually zero accuracy under adaptive attacks.","Adversarial training of the backbone classifier can further increase resistance of the front-end enhanced model to gradient attacks.","On CIFAR10, the respective randomized ensemble achieved 90.8$\\pm 2.5$% (99% CI) accuracy under AutoAttack while having only 18.2$\\pm 3.6$% accuracy under the adaptive attack.   ","We do not establish SOTA in adversarial robustness.","Instead, we make methodological contributions and further supports the thesis that adaptive attacks designed with the complete knowledge of model architecture are crucial in demonstrating model robustness and that even the so-called white-box gradient attacks can have limited applicability.","Although gradient attacks can be complemented with black-box attack such as the SQUARE attack or the zero-order PGD, black-box attacks can be weak against randomized ensembles, e.g., when ensemble models mask gradients."],"url":"http://arxiv.org/abs/2402.17018v1","category":"cs.LG"}
{"created":"2024-02-26 20:53:12","title":"Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings","abstract":"We introduce a novel suite of state-of-the-art bilingual text embedding models that are designed to support English and another target language. These models are capable of processing lengthy text inputs with up to 8192 tokens, making them highly versatile for a range of natural language processing tasks such as text retrieval, clustering, and semantic textual similarity (STS) calculations.   By focusing on bilingual models and introducing a unique multi-task learning objective, we have significantly improved the model performance on STS tasks, which outperforms the capabilities of existing multilingual models in both target language understanding and cross-lingual evaluation tasks. Moreover, our bilingual models are more efficient, requiring fewer parameters and less memory due to their smaller vocabulary needs. Furthermore, we have expanded the Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and Spanish embedding models. This integration aims to stimulate further research and advancement in text embedding technologies for these languages.","sentences":["We introduce a novel suite of state-of-the-art bilingual text embedding models that are designed to support English and another target language.","These models are capable of processing lengthy text inputs with up to 8192 tokens, making them highly versatile for a range of natural language processing tasks such as text retrieval, clustering, and semantic textual similarity (STS) calculations.   ","By focusing on bilingual models and introducing a unique multi-task learning objective, we have significantly improved the model performance on STS tasks, which outperforms the capabilities of existing multilingual models in both target language understanding and cross-lingual evaluation tasks.","Moreover, our bilingual models are more efficient, requiring fewer parameters and less memory due to their smaller vocabulary needs.","Furthermore, we have expanded the Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and Spanish embedding models.","This integration aims to stimulate further research and advancement in text embedding technologies for these languages."],"url":"http://arxiv.org/abs/2402.17016v1","category":"cs.CL"}
{"created":"2024-02-26 20:42:40","title":"Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset","abstract":"The assessment of explainability in Legal Judgement Prediction (LJP) systems is of paramount importance in building trustworthy and transparent systems, particularly considering the reliance of these systems on factors that may lack legal relevance or involve sensitive attributes. This study delves into the realm of explainability and fairness in LJP models, utilizing Swiss Judgement Prediction (SJP), the only available multilingual LJP dataset. We curate a comprehensive collection of rationales that `support' and `oppose' judgement from legal experts for 108 cases in German, French, and Italian. By employing an occlusion-based explainability approach, we evaluate the explainability performance of state-of-the-art monolingual and multilingual BERT-based LJP models, as well as models developed with techniques such as data augmentation and cross-lingual transfer, which demonstrated prediction performance improvement. Notably, our findings reveal that improved prediction performance does not necessarily correspond to enhanced explainability performance, underscoring the significance of evaluating models from an explainability perspective. Additionally, we introduce a novel evaluation framework, Lower Court Insertion (LCI), which allows us to quantify the influence of lower court information on model predictions, exposing current models' biases.","sentences":["The assessment of explainability in Legal Judgement Prediction (LJP) systems is of paramount importance in building trustworthy and transparent systems, particularly considering the reliance of these systems on factors that may lack legal relevance or involve sensitive attributes.","This study delves into the realm of explainability and fairness in LJP models, utilizing Swiss Judgement Prediction (SJP), the only available multilingual LJP dataset.","We curate a comprehensive collection of rationales that `support' and `oppose' judgement from legal experts for 108 cases in German, French, and Italian.","By employing an occlusion-based explainability approach, we evaluate the explainability performance of state-of-the-art monolingual and multilingual BERT-based LJP models, as well as models developed with techniques such as data augmentation and cross-lingual transfer, which demonstrated prediction performance improvement.","Notably, our findings reveal that improved prediction performance does not necessarily correspond to enhanced explainability performance, underscoring the significance of evaluating models from an explainability perspective.","Additionally, we introduce a novel evaluation framework, Lower Court Insertion (LCI), which allows us to quantify the influence of lower court information on model predictions, exposing current models' biases."],"url":"http://arxiv.org/abs/2402.17013v1","category":"cs.CL"}
{"created":"2024-02-26 20:41:50","title":"Pandora's White-Box: Increased Training Data Leakage in Open LLMs","abstract":"In this paper we undertake a systematic study of privacy attacks against open source Large Language Models (LLMs), where an adversary has access to either the model weights, gradients, or losses, and tries to exploit them to learn something about the underlying training data. Our headline results are the first membership inference attacks (MIAs) against pre-trained LLMs that are able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing that over $50\\%$ (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM in natural settings. We consider varying degrees of access to the underlying model, customization of the language model, and resources available to the attacker. In the pre-trained setting, we propose three new white-box MIAs: an attack based on the gradient norm, a supervised neural network classifier, and a single step loss ratio attack. All outperform existing black-box baselines, and our supervised attack closes the gap between MIA attack success against LLMs and other types of models. In fine-tuning, we find that given access to the loss of the fine-tuned and base models, a fine-tuned loss ratio attack FLoRA is able to achieve near perfect MIA peformance. We then leverage these MIAs to extract fine-tuning data from fine-tuned language models. We find that the pipeline of generating from fine-tuned models prompted with a small snippet of the prefix of each training example, followed by using FLoRa to select the most likely training sample, succeeds the majority of the fine-tuning dataset after only $3$ epochs of fine-tuning. Taken together, these findings show that highly effective MIAs are available in almost all LLM training settings, and highlight that great care must be taken before LLMs are fine-tuned on highly sensitive data and then deployed.","sentences":["In this paper we undertake a systematic study of privacy attacks against open source Large Language Models (LLMs), where an adversary has access to either the model weights, gradients, or losses, and tries to exploit them to learn something about the underlying training data.","Our headline results are the first membership inference attacks (MIAs) against pre-trained LLMs that are able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing that over $50\\%$ (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM in natural settings.","We consider varying degrees of access to the underlying model, customization of the language model, and resources available to the attacker.","In the pre-trained setting, we propose three new white-box MIAs: an attack based on the gradient norm, a supervised neural network classifier, and a single step loss ratio attack.","All outperform existing black-box baselines, and our supervised attack closes the gap between MIA attack success against LLMs and other types of models.","In fine-tuning, we find that given access to the loss of the fine-tuned and base models, a fine-tuned loss ratio attack FLoRA is able to achieve near perfect MIA peformance.","We then leverage these MIAs to extract fine-tuning data from fine-tuned language models.","We find that the pipeline of generating from fine-tuned models prompted with a small snippet of the prefix of each training example, followed by using FLoRa to select the most likely training sample, succeeds the majority of the fine-tuning dataset after only $3$ epochs of fine-tuning.","Taken together, these findings show that highly effective MIAs are available in almost all LLM training settings, and highlight that great care must be taken before LLMs are fine-tuned on highly sensitive data and then deployed."],"url":"http://arxiv.org/abs/2402.17012v1","category":"cs.CR"}
{"created":"2024-02-26 20:35:32","title":"Can Large Language Models Recall Reference Location Like Humans?","abstract":"When completing knowledge-intensive tasks, humans sometimes need not just an answer but also a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the second stage, then locate its position to retrieve a complete passage. Experiments on KILT knowledge-sensitive tasks have verified that LLMs can independently recall reference passage location in various task forms, and the obtained reference significantly assist downstream tasks.","sentences":["When completing knowledge-intensive tasks, humans sometimes need not just an answer","but also a corresponding reference passage for auxiliary reading.","Previous methods required obtaining pre-segmented article chunks through additional retrieval models.","This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position.","We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references.","Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set.","Then, based on the acquired coarse-grained document set, it recalls fine-grained passage.","In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated.","To increase speed, we only recall a short prefix in the second stage, then locate its position to retrieve a complete passage.","Experiments on KILT knowledge-sensitive tasks have verified that LLMs can independently recall reference passage location in various task forms, and the obtained reference significantly assist downstream tasks."],"url":"http://arxiv.org/abs/2402.17010v1","category":"cs.CL"}
{"created":"2024-02-26 20:19:14","title":"Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials","abstract":"Online reinforcement learning (RL) algorithms offer great potential for personalizing treatment for participants in clinical trials. However, deploying an online, autonomous algorithm in the high-stakes healthcare setting makes quality control and data quality especially difficult to achieve. This paper proposes algorithm fidelity as a critical requirement for deploying online RL algorithms in clinical trials. It emphasizes the responsibility of the algorithm to (1) safeguard participants and (2) preserve the scientific utility of the data for post-trial analyses. We also present a framework for pre-deployment planning and real-time monitoring to help algorithm developers and clinical researchers ensure algorithm fidelity. To illustrate our framework's practical application, we present real-world examples from the Oralytics clinical trial. Since Spring 2023, this trial successfully deployed an autonomous, online RL algorithm to personalize behavioral interventions for participants at risk for dental disease.","sentences":["Online reinforcement learning (RL) algorithms offer great potential for personalizing treatment for participants in clinical trials.","However, deploying an online, autonomous algorithm in the high-stakes healthcare setting makes quality control and data quality especially difficult to achieve.","This paper proposes algorithm fidelity as a critical requirement for deploying online RL algorithms in clinical trials.","It emphasizes the responsibility of the algorithm to (1) safeguard participants and (2) preserve the scientific utility of the data for post-trial analyses.","We also present a framework for pre-deployment planning and real-time monitoring to help algorithm developers and clinical researchers ensure algorithm fidelity.","To illustrate our framework's practical application, we present real-world examples from the Oralytics clinical trial.","Since Spring 2023, this trial successfully deployed an autonomous, online RL algorithm to personalize behavioral interventions for participants at risk for dental disease."],"url":"http://arxiv.org/abs/2402.17003v1","category":"cs.LG"}
{"created":"2024-02-26 20:13:58","title":"What Do Language Models Hear? Probing for Auditory Representations in Language Models","abstract":"This work explores whether language models encode meaningfully grounded representations of sounds of objects. We learn a linear probe that retrieves the correct text representation of an object given a snippet of audio related to that object, where the sound representation is given by a pretrained audio model. This probe is trained via a contrastive loss that pushes the language representations and sound representations of an object to be close to one another. After training, the probe is tested on its ability to generalize to objects that were not seen during training. Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects.","sentences":["This work explores whether language models encode meaningfully grounded representations of sounds of objects.","We learn a linear probe that retrieves the correct text representation of an object given a snippet of audio related to that object, where the sound representation is given by a pretrained audio model.","This probe is trained via a contrastive loss that pushes the language representations and sound representations of an object to be close to one another.","After training, the probe is tested on its ability to generalize to objects that were not seen during training.","Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects."],"url":"http://arxiv.org/abs/2402.16998v1","category":"cs.CL"}
{"created":"2024-02-26 20:01:29","title":"Thermodynamic Overfitting and Generalization: Energetic Limits on Predictive Complexity","abstract":"Efficiently harvesting thermodynamic resources requires a precise understanding of their structure. This becomes explicit through the lens of information engines -- thermodynamic engines that use information as fuel. Maximizing the work harvested using available information is a form of physically-instantiated machine learning that drives information engines to develop complex predictive memory to store an environment's temporal correlations. We show that an information engine's complex predictive memory poses both energetic benefits and risks. While increasing memory facilitates detection of hidden patterns in an environment, it also opens the possibility of thermodynamic overfitting, where the engine dissipates additional energy in testing. To address overfitting, we introduce thermodynamic regularizers that incur a cost to engine complexity in training due to the physical constraints on the information engine. We demonstrate that regularized thermodynamic machine learning generalizes effectively. In particular, the physical constraints from which regularizers are derived improve the performance of learned predictive models. This suggests that the laws of physics jointly create the conditions for emergent complexity and predictive intelligence.","sentences":["Efficiently harvesting thermodynamic resources requires a precise understanding of their structure.","This becomes explicit through the lens of information engines -- thermodynamic engines that use information as fuel.","Maximizing the work harvested using available information is a form of physically-instantiated machine learning that drives information engines to develop complex predictive memory to store an environment's temporal correlations.","We show that an information engine's complex predictive memory poses both energetic benefits and risks.","While increasing memory facilitates detection of hidden patterns in an environment, it also opens the possibility of thermodynamic overfitting, where the engine dissipates additional energy in testing.","To address overfitting, we introduce thermodynamic regularizers that incur a cost to engine complexity in training due to the physical constraints on the information engine.","We demonstrate that regularized thermodynamic machine learning generalizes effectively.","In particular, the physical constraints from which regularizers are derived improve the performance of learned predictive models.","This suggests that the laws of physics jointly create the conditions for emergent complexity and predictive intelligence."],"url":"http://arxiv.org/abs/2402.16995v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-26 20:00:57","title":"GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis","abstract":"We introduce GEM3D -- a new deep, topology-aware generative model of 3D shapes. The key ingredient of our method is a neural skeleton-based representation encoding information on both shape topology and geometry. Through a denoising diffusion probabilistic model, our method first generates skeleton-based representations following the Medial Axis Transform (MAT), then generates surfaces through a skeleton-driven neural implicit formulation. The neural implicit takes into account the topological and geometric information stored in the generated skeleton representations to yield surfaces that are more topologically and geometrically accurate compared to previous neural field formulations. We discuss applications of our method in shape synthesis and point cloud reconstruction tasks, and evaluate our method both qualitatively and quantitatively. We demonstrate significantly more faithful surface reconstruction and diverse shape generation results compared to the state-of-the-art, also involving challenging scenarios of reconstructing and synthesizing structurally complex, high-genus shape surfaces from Thingi10K and ShapeNet.","sentences":["We introduce GEM3D -- a new deep, topology-aware generative model of 3D shapes.","The key ingredient of our method is a neural skeleton-based representation encoding information on both shape topology and geometry.","Through a denoising diffusion probabilistic model, our method first generates skeleton-based representations following the Medial Axis Transform (MAT), then generates surfaces through a skeleton-driven neural implicit formulation.","The neural implicit takes into account the topological and geometric information stored in the generated skeleton representations to yield surfaces that are more topologically and geometrically accurate compared to previous neural field formulations.","We discuss applications of our method in shape synthesis and point cloud reconstruction tasks, and evaluate our method both qualitatively and quantitatively.","We demonstrate significantly more faithful surface reconstruction and diverse shape generation results compared to the state-of-the-art, also involving challenging scenarios of reconstructing and synthesizing structurally complex, high-genus shape surfaces from Thingi10K and ShapeNet."],"url":"http://arxiv.org/abs/2402.16994v1","category":"cs.CV"}
{"created":"2024-02-26 19:33:38","title":"Visualizing 2x2 Normal-Form Games: twoxtwogame LaTeX Package","abstract":"Normal-form games with two players, each with two strategies, are the most studied class of games. These so-called 2x2 games are used to model a variety of strategic interactions. They appear in game theory, economics, and artificial intelligence research. However, there lacks tools for describing and visualizing such games. This work introduces a LaTeX package for visualizing 2x2 games. This work has two goals: first, to provide high-quality tools and vector graphic visualizations, suitable for scientific publications. And second, to help promote standardization of names and representations of 2x2 games. The LaTeX package, twoxtwogame, is maintained on GitHub and mirrored on CTAN, and is available under a permissive Apache 2 license.","sentences":["Normal-form games with two players, each with two strategies, are the most studied class of games.","These so-called 2x2 games are used to model a variety of strategic interactions.","They appear in game theory, economics, and artificial intelligence research.","However, there lacks tools for describing and visualizing such games.","This work introduces a LaTeX package for visualizing 2x2 games.","This work has two goals: first, to provide high-quality tools and vector graphic visualizations, suitable for scientific publications.","And second, to help promote standardization of names and representations of 2x2 games.","The LaTeX package, twoxtwogame, is maintained on GitHub and mirrored on CTAN, and is available under a permissive Apache 2 license."],"url":"http://arxiv.org/abs/2402.16985v1","category":"cs.GT"}
{"created":"2024-02-26 19:29:46","title":"Synthesizing Tight Privacy and Accuracy Bounds via Weighted Model Counting","abstract":"Programmatically generating tight differential privacy (DP) bounds is a hard problem. Two core challenges are (1) finding expressive, compact, and efficient encodings of the distributions of DP algorithms, and (2) state space explosion stemming from the multiple quantifiers and relational properties of the DP definition.   We address the first challenge by developing a method for tight privacy and accuracy bound synthesis using weighted model counting on binary decision diagrams, a state of the art technique from the artificial intelligence and automated reasoning communities for exactly computing probability distributions. We address the second challenge by developing a framework for leveraging inherent symmetries in DP algorithms. Our solution benefits from ongoing research in probabilistic programming languages, allowing us to succinctly and expressively represent different DP algorithms with approachable language syntax that can be used by non-experts.   We provide a detailed case study of our solution on the binary randomized response algorithm. We also evaluate an implementation of our solution using the Dice probabilistic programming language for the randomized response and truncated geometric above threshold algorithms. We compare to prior work on exact DP verification using Markov chain probabilistic model checking. Very few existing works consider mechanized analysis of accuracy guarantees for DP algorithms. We additionally provide a detailed analysis using our technique for finding tight accuracy bounds for DP algorithms.","sentences":["Programmatically generating tight differential privacy (DP) bounds is a hard problem.","Two core challenges are (1) finding expressive, compact, and efficient encodings of the distributions of DP algorithms, and (2) state space explosion stemming from the multiple quantifiers and relational properties of the DP definition.   ","We address the first challenge by developing a method for tight privacy and accuracy bound synthesis using weighted model counting on binary decision diagrams, a state of the art technique from the artificial intelligence and automated reasoning communities for exactly computing probability distributions.","We address the second challenge by developing a framework for leveraging inherent symmetries in DP algorithms.","Our solution benefits from ongoing research in probabilistic programming languages, allowing us to succinctly and expressively represent different DP algorithms with approachable language syntax that can be used by non-experts.   ","We provide a detailed case study of our solution on the binary randomized response algorithm.","We also evaluate an implementation of our solution using the Dice probabilistic programming language for the randomized response and truncated geometric above threshold algorithms.","We compare to prior work on exact DP verification using Markov chain probabilistic model checking.","Very few existing works consider mechanized analysis of accuracy guarantees for DP algorithms.","We additionally provide a detailed analysis using our technique for finding tight accuracy bounds for DP algorithms."],"url":"http://arxiv.org/abs/2402.16982v1","category":"cs.CR"}
{"created":"2024-02-26 19:29:23","title":"Non-Euclidean Sliced Optimal Transport Sampling","abstract":"In machine learning and computer graphics, a fundamental task is the approximation of a probability density function through a well-dispersed collection of samples. Providing a formal metric for measuring the distance between probability measures on general spaces, Optimal Transport (OT) emerges as a pivotal theoretical framework within this context. However, the associated computational burden is prohibitive in most real-world scenarios. Leveraging the simple structure of OT in 1D, Sliced Optimal Transport (SOT) has appeared as an efficient alternative to generate samples in Euclidean spaces. This paper pushes the boundaries of SOT utilization in computational geometry problems by extending its application to sample densities residing on more diverse mathematical domains, including the spherical space Sd , the hyperbolic plane Hd , and the real projective plane Pd . Moreover, it ensures the quality of these samples by achieving a blue noise characteristic, regardless of the dimensionality involved. The robustness of our approach is highlighted through its application to various geometry processing tasks, such as the intrinsic blue noise sampling of meshes, as well as the sampling of directions and rotations. These applications collectively underscore the efficacy of our methodology.","sentences":["In machine learning and computer graphics, a fundamental task is the approximation of a probability density function through a well-dispersed collection of samples.","Providing a formal metric for measuring the distance between probability measures on general spaces, Optimal Transport (OT) emerges as a pivotal theoretical framework within this context.","However, the associated computational burden is prohibitive in most real-world scenarios.","Leveraging the simple structure of OT in 1D, Sliced Optimal Transport (SOT) has appeared as an efficient alternative to generate samples in Euclidean spaces.","This paper pushes the boundaries of SOT utilization in computational geometry problems by extending its application to sample densities residing on more diverse mathematical domains, including the spherical space Sd , the hyperbolic plane Hd , and the real projective plane Pd .","Moreover, it ensures the quality of these samples by achieving a blue noise characteristic, regardless of the dimensionality involved.","The robustness of our approach is highlighted through its application to various geometry processing tasks, such as the intrinsic blue noise sampling of meshes, as well as the sampling of directions and rotations.","These applications collectively underscore the efficacy of our methodology."],"url":"http://arxiv.org/abs/2402.16981v1","category":"cs.GR"}
{"created":"2024-02-26 19:29:03","title":"Saliency-Aware Automatic Buddhas Statue Recognition","abstract":"Buddha statues, as a symbol of many religions, have significant cultural implications that are crucial for understanding the culture and history of different regions, and the recognition of Buddha statues is therefore the pivotal link in the field of Buddha study. However, the Buddha statue recognition requires extensive time and effort from knowledgeable professionals, making it a costly task to perform. Convolution neural networks (CNNs) are inherently efficient at processing visual information, but CNNs alone are likely to make inaccurate classification decisions when subjected to the class imbalance problem. Therefore, this paper proposes an end-to-end automatic Buddha statue recognition model based on saliency map sampling. The proposed Grid-Wise Local Self-Attention Module (GLSA) provides extra salient features which can serve to enrich the dataset and allow CNNs to observe in a much more comprehensive way. Eventually, our model is evaluated on a Buddha dataset collected with the aid of Buddha experts and outperforms state-of-the-art networks in terms of Top-1 accuracy by 4.63\\% on average, while only marginally increasing MUL-ADD.","sentences":["Buddha statues, as a symbol of many religions, have significant cultural implications that are crucial for understanding the culture and history of different regions, and the recognition of Buddha statues is therefore the pivotal link in the field of Buddha study.","However, the Buddha statue recognition requires extensive time and effort from knowledgeable professionals, making it a costly task to perform.","Convolution neural networks (CNNs) are inherently efficient at processing visual information, but CNNs alone are likely to make inaccurate classification decisions when subjected to the class imbalance problem.","Therefore, this paper proposes an end-to-end automatic Buddha statue recognition model based on saliency map sampling.","The proposed Grid-Wise Local Self-Attention Module (GLSA) provides extra salient features which can serve to enrich the dataset and allow CNNs to observe in a much more comprehensive way.","Eventually, our model is evaluated on a Buddha dataset collected with the aid of Buddha experts and outperforms state-of-the-art networks in terms of Top-1 accuracy by 4.63\\% on average, while only marginally increasing MUL-ADD."],"url":"http://arxiv.org/abs/2402.16980v1","category":"cs.CV"}
{"created":"2024-02-26 19:19:47","title":"Dealing with Data for RE: Mitigating Challenges using NLP and Generative AI","abstract":"Across the dynamic business landscape today, enterprises face an ever-increasing range of challenges. These include the constantly evolving regulatory environment, the growing demand for personalization within software applications, and the heightened emphasis on governance. In response to these multifaceted demands, large enterprises have been adopting automation that spans from the optimization of core business processes to the enhancement of customer experiences. Indeed, Artificial Intelligence (AI) has emerged as a pivotal element of modern software systems. In this context, data plays an indispensable role. AI-centric software systems based on supervised learning and operating at an industrial scale require large volumes of training data to perform effectively. Moreover, the incorporation of generative AI has led to a growing demand for adequate evaluation benchmarks. Our experience in this field has revealed that the requirement for large datasets for training and evaluation introduces a host of intricate challenges. This book chapter explores the evolving landscape of Software Engineering (SE) in general, and Requirements Engineering (RE) in particular, in this era marked by AI integration. We discuss challenges that arise while integrating Natural Language Processing (NLP) and generative AI into enterprise-critical software systems. The chapter provides practical insights, solutions, and examples to equip readers with the knowledge and tools necessary for effectively building solutions with NLP at their cores. We also reflect on how these text data-centric tasks sit together with the traditional RE process. We also highlight new RE tasks that may be necessary for handling the increasingly important text data-centricity involved in developing software systems.","sentences":["Across the dynamic business landscape today, enterprises face an ever-increasing range of challenges.","These include the constantly evolving regulatory environment, the growing demand for personalization within software applications, and the heightened emphasis on governance.","In response to these multifaceted demands, large enterprises have been adopting automation that spans from the optimization of core business processes to the enhancement of customer experiences.","Indeed, Artificial Intelligence (AI) has emerged as a pivotal element of modern software systems.","In this context, data plays an indispensable role.","AI-centric software systems based on supervised learning and operating at an industrial scale require large volumes of training data to perform effectively.","Moreover, the incorporation of generative AI has led to a growing demand for adequate evaluation benchmarks.","Our experience in this field has revealed that the requirement for large datasets for training and evaluation introduces a host of intricate challenges.","This book chapter explores the evolving landscape of Software Engineering (SE) in general, and Requirements Engineering (RE) in particular, in this era marked by AI integration.","We discuss challenges that arise while integrating Natural Language Processing (NLP) and generative AI into enterprise-critical software systems.","The chapter provides practical insights, solutions, and examples to equip readers with the knowledge and tools necessary for effectively building solutions with NLP at their cores.","We also reflect on how these text data-centric tasks sit together with the traditional RE process.","We also highlight new RE tasks that may be necessary for handling the increasingly important text data-centricity involved in developing software systems."],"url":"http://arxiv.org/abs/2402.16977v1","category":"cs.SE"}
{"created":"2024-02-26 19:16:04","title":"Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections","abstract":"This paper addresses the challenge of leveraging imperfect language models to guide human decision-making in the context of a grounded navigation task. We show that an imperfect instruction generation model can be complemented with an effective communication mechanism to become more successful at guiding humans. The communication mechanism we build comprises models that can detect potential hallucinations in instructions and suggest practical alternatives, and an intuitive interface to present that information to users. We show that this approach reduces the human navigation error by up to 29% with no additional cognitive burden. This result underscores the potential of integrating diverse communication channels into AI systems to compensate for their imperfections and enhance their utility for humans.","sentences":["This paper addresses the challenge of leveraging imperfect language models to guide human decision-making in the context of a grounded navigation task.","We show that an imperfect instruction generation model can be complemented with an effective communication mechanism to become more successful at guiding humans.","The communication mechanism we build comprises models that can detect potential hallucinations in instructions and suggest practical alternatives, and an intuitive interface to present that information to users.","We show that this approach reduces the human navigation error by up to 29% with no additional cognitive burden.","This result underscores the potential of integrating diverse communication channels into AI systems to compensate for their imperfections and enhance their utility for humans."],"url":"http://arxiv.org/abs/2402.16973v1","category":"cs.AI"}
{"created":"2024-02-26 19:06:02","title":"A Survey of Large Language Models in Cybersecurity","abstract":"Large Language Models (LLMs) have quickly risen to prominence due to their ability to perform at or close to the state-of-the-art in a variety of fields while handling natural language. An important field of research is the application of such models at the cybersecurity context. This survey aims to identify where in the field of cybersecurity LLMs have already been applied, the ways in which they are being used and their limitations in the field. Finally, suggestions are made on how to improve such limitations and what can be expected from these systems once these limitations are overcome.","sentences":["Large Language Models (LLMs) have quickly risen to prominence due to their ability to perform at or close to the state-of-the-art in a variety of fields while handling natural language.","An important field of research is the application of such models at the cybersecurity context.","This survey aims to identify where in the field of cybersecurity LLMs have already been applied, the ways in which they are being used and their limitations in the field.","Finally, suggestions are made on how to improve such limitations and what can be expected from these systems once these limitations are overcome."],"url":"http://arxiv.org/abs/2402.16968v1","category":"cs.CR"}
{"created":"2024-02-26 19:01:54","title":"WIPI: A New Web Threat for LLM-Driven Web Agents","abstract":"With the fast development of large language models (LLMs), LLM-driven Web Agents (Web Agents for short) have obtained tons of attention due to their superior capability where LLMs serve as the core part of making decisions like the human brain equipped with multiple web tools to actively interact with external deployed websites. As uncountable Web Agents have been released and such LLM systems are experiencing rapid development and drawing closer to widespread deployment in our daily lives, an essential and pressing question arises: \"Are these Web Agents secure?\". In this paper, we introduce a novel threat, WIPI, that indirectly controls Web Agent to execute malicious instructions embedded in publicly accessible webpages. To launch a successful WIPI works in a black-box environment. This methodology focuses on the form and content of indirect instructions within external webpages, enhancing the efficiency and stealthiness of the attack. To evaluate the effectiveness of the proposed methodology, we conducted extensive experiments using 7 plugin-based ChatGPT Web Agents, 8 Web GPTs, and 3 different open-source Web Agents. The results reveal that our methodology achieves an average attack success rate (ASR) exceeding 90% even in pure black-box scenarios. Moreover, through an ablation study examining various user prefix instructions, we demonstrated that the WIPI exhibits strong robustness, maintaining high performance across diverse prefix instructions.","sentences":["With the fast development of large language models (LLMs), LLM-driven Web Agents (Web Agents for short) have obtained tons of attention due to their superior capability where LLMs serve as the core part of making decisions like the human brain equipped with multiple web tools to actively interact with external deployed websites.","As uncountable Web Agents have been released and such LLM systems are experiencing rapid development and drawing closer to widespread deployment in our daily lives, an essential and pressing question arises: \"Are these Web Agents secure?\".","In this paper, we introduce a novel threat, WIPI, that indirectly controls Web Agent to execute malicious instructions embedded in publicly accessible webpages.","To launch a successful WIPI works in a black-box environment.","This methodology focuses on the form and content of indirect instructions within external webpages, enhancing the efficiency and stealthiness of the attack.","To evaluate the effectiveness of the proposed methodology, we conducted extensive experiments using 7 plugin-based ChatGPT Web Agents, 8 Web GPTs, and 3 different open-source Web Agents.","The results reveal that our methodology achieves an average attack success rate (ASR) exceeding 90% even in pure black-box scenarios.","Moreover, through an ablation study examining various user prefix instructions, we demonstrated that the WIPI exhibits strong robustness, maintaining high performance across diverse prefix instructions."],"url":"http://arxiv.org/abs/2402.16965v1","category":"cs.CR"}
{"created":"2024-02-26 19:00:01","title":"Security of hybrid BB84 with heterodyne detection","abstract":"Quantum key distribution (QKD) promises everlasting security based on the laws of physics. Most common protocols are grouped into two distinct categories based on the degrees of freedom used to carry information, which can be either discrete or continuous, each presenting unique advantages in either performance, feasibility for near-term implementation, and compatibility with existing telecommunications architectures. Recently, hybrid QKD protocols have been introduced to leverage advantages from both categories. In this work we provide a rigorous security proof for a protocol introduced by Qi in 2021, where information is encoded in discrete variables as in the widespread Bennett Brassard 1984 (BB84) protocol but decoded continuously via heterodyne detection. Security proofs for hybrid protocols inherit the same challenges associated with continuous-variable protocols due to unbounded dimensions. Here we successfully address these challenges by exploiting symmetry. Our approach enables truncation of the Hilbert space with precise control of the approximation errors and lead to a tight, semi-analytical expression for the asymptotic key rate under collective attacks. As concrete examples, we apply our theory to compute the key rates under passive attacks, linear loss, and Gaussian noise.","sentences":["Quantum key distribution (QKD) promises everlasting security based on the laws of physics.","Most common protocols are grouped into two distinct categories based on the degrees of freedom used to carry information, which can be either discrete or continuous, each presenting unique advantages in either performance, feasibility for near-term implementation, and compatibility with existing telecommunications architectures.","Recently, hybrid QKD protocols have been introduced to leverage advantages from both categories.","In this work we provide a rigorous security proof for a protocol introduced by Qi in 2021, where information is encoded in discrete variables as in the widespread Bennett Brassard 1984 (BB84) protocol but decoded continuously via heterodyne detection.","Security proofs for hybrid protocols inherit the same challenges associated with continuous-variable protocols due to unbounded dimensions.","Here we successfully address these challenges by exploiting symmetry.","Our approach enables truncation of the Hilbert space with precise control of the approximation errors and lead to a tight, semi-analytical expression for the asymptotic key rate under collective attacks.","As concrete examples, we apply our theory to compute the key rates under passive attacks, linear loss, and Gaussian noise."],"url":"http://arxiv.org/abs/2402.16941v1","category":"quant-ph"}
{"created":"2024-02-26 18:59:33","title":"GROUNDHOG: Grounding Large Language Models to Holistic Segmentation","abstract":"Most multimodal large language models (MLLMs) learn language-to-object grounding through causal language modeling where grounded objects are captured by bounding boxes as sequences of location tokens. This paradigm lacks pixel-level representations that are important for fine-grained visual understanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLM developed by grounding Large Language Models to holistic segmentation. GROUNDHOG incorporates a masked feature extractor and converts extracted features into visual entity tokens for the MLLM backbone, which then connects groundable phrases to unified grounding masks by retrieving and merging the entity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual instruction tuning dataset with Multi-Modal Multi-Grained Grounding, by harvesting a collection of segmentation-grounded datasets with rich annotations. Our experimental results show that GROUNDHOG achieves superior performance on various language grounding tasks without task-specific fine-tuning, and significantly reduces object hallucination. GROUNDHOG also demonstrates better grounding towards complex forms of visual input and provides easy-to-understand diagnosis in failure cases.","sentences":["Most multimodal large language models (MLLMs) learn language-to-object grounding through causal language modeling where grounded objects are captured by bounding boxes as sequences of location tokens.","This paradigm lacks pixel-level representations that are important for fine-grained visual understanding and diagnosis.","In this work, we introduce GROUNDHOG, an MLLM developed by grounding Large Language Models to holistic segmentation.","GROUNDHOG incorporates a masked feature extractor and converts extracted features into visual entity tokens for the MLLM backbone, which then connects groundable phrases to unified grounding masks by retrieving and merging the entity masks.","To train GROUNDHOG, we carefully curated M3G2, a grounded visual instruction tuning dataset with Multi-Modal Multi-Grained Grounding, by harvesting a collection of segmentation-grounded datasets with rich annotations.","Our experimental results show that GROUNDHOG achieves superior performance on various language grounding tasks without task-specific fine-tuning, and significantly reduces object hallucination.","GROUNDHOG also demonstrates better grounding towards complex forms of visual input and provides easy-to-understand diagnosis in failure cases."],"url":"http://arxiv.org/abs/2402.16846v1","category":"cs.CV"}
{"created":"2024-02-26 18:59:31","title":"Neural Operators with Localized Integral and Differential Kernels","abstract":"Neural operators learn mappings between function spaces, which is practical for learning solution operators of PDEs and other scientific modeling applications. Among them, the Fourier neural operator (FNO) is a popular architecture that performs global convolutions in the Fourier space. However, such global operations are often prone to over-smoothing and may fail to capture local details. In contrast, convolutional neural networks (CNN) can capture local features but are limited to training and inference at a single resolution. In this work, we present a principled approach to operator learning that can capture local features under two frameworks by learning differential operators and integral operators with locally supported kernels. Specifically, inspired by stencil methods, we prove that we obtain differential operators under an appropriate scaling of the kernel values of CNNs. To obtain local integral operators, we utilize suitable basis representations for the kernels based on discrete-continuous convolutions. Both these approaches preserve the properties of operator learning and, hence, the ability to predict at any resolution. Adding our layers to FNOs significantly improves their performance, reducing the relative L2-error by 34-72% in our experiments on turbulent 2D Navier-Stokes fluid flow and the spherical shallow water equations.","sentences":["Neural operators learn mappings between function spaces, which is practical for learning solution operators of PDEs and other scientific modeling applications.","Among them, the Fourier neural operator (FNO) is a popular architecture that performs global convolutions in the Fourier space.","However, such global operations are often prone to over-smoothing and may fail to capture local details.","In contrast, convolutional neural networks (CNN) can capture local features but are limited to training and inference at a single resolution.","In this work, we present a principled approach to operator learning that can capture local features under two frameworks by learning differential operators and integral operators with locally supported kernels.","Specifically, inspired by stencil methods, we prove that we obtain differential operators under an appropriate scaling of the kernel values of CNNs.","To obtain local integral operators, we utilize suitable basis representations for the kernels based on discrete-continuous convolutions.","Both these approaches preserve the properties of operator learning and, hence, the ability to predict at any resolution.","Adding our layers to FNOs significantly improves their performance, reducing the relative L2-error by 34-72% in our experiments on turbulent 2D Navier-Stokes fluid flow and the spherical shallow water equations."],"url":"http://arxiv.org/abs/2402.16845v1","category":"cs.LG"}
{"created":"2024-02-26 18:59:28","title":"Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding","abstract":"Large language models (LLMs) have become ubiquitous in practice and are widely used for generation tasks such as translation, summarization and instruction following. However, their enormous size and reliance on autoregressive decoding increase deployment costs and complicate their use in latency-critical applications. In this work, we propose a hybrid approach that combines language models of different sizes to increase the efficiency of autoregressive decoding while maintaining high performance. Our method utilizes a pretrained frozen LLM that encodes all prompt tokens once in parallel, and uses the resulting representations to condition and guide a small language model (SLM), which then generates the response more efficiently. We investigate the combination of encoder-decoder LLMs with both encoder-decoder and decoder-only SLMs from different model families and only require fine-tuning of the SLM. Experiments with various benchmarks show substantial speedups of up to $4\\times$, with minor performance penalties of $1-2\\%$ for translation and summarization tasks compared to the LLM.","sentences":["Large language models (LLMs) have become ubiquitous in practice and are widely used for generation tasks such as translation, summarization and instruction following.","However, their enormous size and reliance on autoregressive decoding increase deployment costs and complicate their use in latency-critical applications.","In this work, we propose a hybrid approach that combines language models of different sizes to increase the efficiency of autoregressive decoding while maintaining high performance.","Our method utilizes a pretrained frozen LLM that encodes all prompt tokens once in parallel, and uses the resulting representations to condition and guide a small language model (SLM), which then generates the response more efficiently.","We investigate the combination of encoder-decoder LLMs with both encoder-decoder and decoder-only SLMs from different model families and only require fine-tuning of the SLM.","Experiments with various benchmarks show substantial speedups of up to $4\\times$, with minor performance penalties of $1-2\\%$ for translation and summarization tasks compared to the LLM."],"url":"http://arxiv.org/abs/2402.16844v1","category":"cs.LG"}
{"created":"2024-02-26 18:59:18","title":"Multi-LoRA Composition for Image Generation","abstract":"Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models for the accurate rendition of specific elements like distinct characters or unique styles in generated images. Nonetheless, existing methods face challenges in effectively composing multiple LoRAs, especially as the number of LoRAs to be integrated grows, thus hindering the creation of complex imagery. In this paper, we study multi-LoRA composition through a decoding-centric perspective. We present two training-free methods: LoRA Switch, which alternates between different LoRAs at each denoising step, and LoRA Composite, which simultaneously incorporates all LoRAs to guide more cohesive image synthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new comprehensive testbed as part of this research. It features a diverse range of LoRA categories with 480 composition sets. Utilizing an evaluation framework based on GPT-4V, our findings demonstrate a clear improvement in performance with our methods over the prevalent baseline, particularly evident when increasing the number of LoRAs in a composition.","sentences":["Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models for the accurate rendition of specific elements like distinct characters or unique styles in generated images.","Nonetheless, existing methods face challenges in effectively composing multiple LoRAs, especially as the number of LoRAs to be integrated grows, thus hindering the creation of complex imagery.","In this paper, we study multi-LoRA composition through a decoding-centric perspective.","We present two training-free methods: LoRA Switch, which alternates between different LoRAs at each denoising step, and LoRA Composite, which simultaneously incorporates all LoRAs to guide more cohesive image synthesis.","To evaluate the proposed approaches, we establish ComposLoRA, a new comprehensive testbed as part of this research.","It features a diverse range of LoRA categories with 480 composition sets.","Utilizing an evaluation framework based on GPT-4V, our findings demonstrate a clear improvement in performance with our methods over the prevalent baseline, particularly evident when increasing the number of LoRAs in a composition."],"url":"http://arxiv.org/abs/2402.16843v1","category":"cs.CV"}
{"created":"2024-02-26 18:57:52","title":"PhyGrasp: Generalizing Robotic Grasping with Physics-informed Large Multimodal Models","abstract":"Robotic grasping is a fundamental aspect of robot functionality, defining how robots interact with objects. Despite substantial progress, its generalizability to counter-intuitive or long-tailed scenarios, such as objects with uncommon materials or shapes, remains a challenge. In contrast, humans can easily apply their intuitive physics to grasp skillfully and change grasps efficiently, even for objects they have never seen before.   This work delves into infusing such physical commonsense reasoning into robotic manipulation. We introduce PhyGrasp, a multimodal large model that leverages inputs from two modalities: natural language and 3D point clouds, seamlessly integrated through a bridge module. The language modality exhibits robust reasoning capabilities concerning the impacts of diverse physical properties on grasping, while the 3D modality comprehends object shapes and parts. With these two capabilities, PhyGrasp is able to accurately assess the physical properties of object parts and determine optimal grasping poses. Additionally, the model's language comprehension enables human instruction interpretation, generating grasping poses that align with human preferences. To train PhyGrasp, we construct a dataset PhyPartNet with 195K object instances with varying physical properties and human preferences, alongside their corresponding language descriptions. Extensive experiments conducted in the simulation and on the real robots demonstrate that PhyGrasp achieves state-of-the-art performance, particularly in long-tailed cases, e.g., about 10% improvement in success rate over GraspNet. Project page: https://sites.google.com/view/phygrasp","sentences":["Robotic grasping is a fundamental aspect of robot functionality, defining how robots interact with objects.","Despite substantial progress, its generalizability to counter-intuitive or long-tailed scenarios, such as objects with uncommon materials or shapes, remains a challenge.","In contrast, humans can easily apply their intuitive physics to grasp skillfully and change grasps efficiently, even for objects they have never seen before.   ","This work delves into infusing such physical commonsense reasoning into robotic manipulation.","We introduce PhyGrasp, a multimodal large model that leverages inputs from two modalities: natural language and 3D point clouds, seamlessly integrated through a bridge module.","The language modality exhibits robust reasoning capabilities concerning the impacts of diverse physical properties on grasping, while the 3D modality comprehends object shapes and parts.","With these two capabilities, PhyGrasp is able to accurately assess the physical properties of object parts and determine optimal grasping poses.","Additionally, the model's language comprehension enables human instruction interpretation, generating grasping poses that align with human preferences.","To train PhyGrasp, we construct a dataset PhyPartNet with 195K object instances with varying physical properties and human preferences, alongside their corresponding language descriptions.","Extensive experiments conducted in the simulation and on the real robots demonstrate that PhyGrasp achieves state-of-the-art performance, particularly in long-tailed cases, e.g., about 10% improvement in success rate over GraspNet.","Project page: https://sites.google.com/view/phygrasp"],"url":"http://arxiv.org/abs/2402.16836v1","category":"cs.RO"}
{"created":"2024-02-26 18:56:48","title":"Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections","abstract":"Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable general-purpose conversations about images with the language modality. As off-the-shelf MLLMs may have limited capabilities on images from domains like dermatology and agriculture, they must be fine-tuned to unlock domain-specific applications. The prevalent architecture of current open-source MLLMs comprises two major modules: an image-language (cross-modal) projection network and a large language model. It is desirable to understand the roles of these two modules in modeling domain-specific visual attributes to inform the design of future models and streamline the interpretability efforts on the current models. To this end, via experiments on 4 datasets and under 2 fine-tuning settings, we find that as the MLLM is fine-tuned, it indeed gains domain-specific visual capabilities, but the updates do not lead to the projection extracting relevant domain-specific visual attributes. Our results indicate that the domain-specific visual attributes are modeled by the LLM, even when only the projection is fine-tuned. Through this study, we offer a potential reinterpretation of the role of cross-modal projections in MLLM architectures. Projection webpage: https://claws-lab.github.io/projection-in-MLLMs/","sentences":["Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable general-purpose conversations about images with the language modality.","As off-the-shelf MLLMs may have limited capabilities on images from domains like dermatology and agriculture, they must be fine-tuned to unlock domain-specific applications.","The prevalent architecture of current open-source MLLMs comprises two major modules: an image-language (cross-modal) projection network and a large language model.","It is desirable to understand the roles of these two modules in modeling domain-specific visual attributes to inform the design of future models and streamline the interpretability efforts on the current models.","To this end, via experiments on 4 datasets and under 2 fine-tuning settings, we find that as the MLLM is fine-tuned, it indeed gains domain-specific visual capabilities, but the updates do not lead to the projection extracting relevant domain-specific visual attributes.","Our results indicate that the domain-specific visual attributes are modeled by the LLM, even when only the projection is fine-tuned.","Through this study, we offer a potential reinterpretation of the role of cross-modal projections in MLLM architectures.","Projection webpage: https://claws-lab.github.io/projection-in-MLLMs/"],"url":"http://arxiv.org/abs/2402.16832v1","category":"cs.CL"}
{"created":"2024-02-26 18:55:13","title":"Training Neural Networks from Scratch with Parallel Low-Rank Adapters","abstract":"The scalability of deep learning models is fundamentally limited by computing resources, memory, and communication. Although methods like low-rank adaptation (LoRA) have reduced the cost of model finetuning, its application in model pre-training remains largely unexplored. This paper explores extending LoRA to model pre-training, identifying the inherent constraints and limitations of standard LoRA in this context. We introduce LoRA-the-Explorer (LTE), a novel bi-level optimization algorithm designed to enable parallel training of multiple low-rank heads across computing nodes, thereby reducing the need for frequent synchronization. Our approach includes extensive experimentation on vision transformers using various vision datasets, demonstrating that LTE is competitive with standard pre-training.","sentences":["The scalability of deep learning models is fundamentally limited by computing resources, memory, and communication.","Although methods like low-rank adaptation (LoRA) have reduced the cost of model finetuning, its application in model pre-training remains largely unexplored.","This paper explores extending LoRA to model pre-training, identifying the inherent constraints and limitations of standard LoRA in this context.","We introduce LoRA-the-Explorer (LTE), a novel bi-level optimization algorithm designed to enable parallel training of multiple low-rank heads across computing nodes, thereby reducing the need for frequent synchronization.","Our approach includes extensive experimentation on vision transformers using various vision datasets, demonstrating that LTE is competitive with standard pre-training."],"url":"http://arxiv.org/abs/2402.16828v1","category":"cs.LG"}
{"created":"2024-02-26 18:50:30","title":"Quantum correlations in the steady state of light-emitter ensembles from perturbation theory","abstract":"The coupling of a quantum system to an environment leads generally to decoherence, and it is detrimental to quantum correlations within the system itself. Yet some forms of quantum correlations can be robust to the presence of an environment - or may even be stabilized by it. Predicting (let alone understanding) them remains arduous, given that the steady state of an open quantum system can be very different from an equilibrium thermodynamic state; and its reconstruction requires generically the numerical solution of the Lindblad equation, which is extremely costly for numerics. Here we focus on the highly relevant situation of ensembles of light emitters undergoing spontaneous decay; and we show that, whenever their Hamiltonian is perturbed away from a U(1) symmetric form, steady-state quantum correlations can be reconstructed via pure-state perturbation theory. Our main result is that in systems of light emitters subject to single-emitter or two-emitter driving, the steady state perturbed away from the U(1) limit generically exhibits spin squeezing; and it has minimal uncertainty for the collective-spin components, revealing that squeezing represents the optimal resource for entanglement-assisted metrology using this state.","sentences":["The coupling of a quantum system to an environment leads generally to decoherence, and it is detrimental to quantum correlations within the system itself.","Yet some forms of quantum correlations can be robust to the presence of an environment - or may even be stabilized by it.","Predicting (let alone understanding) them remains arduous, given that the steady state of an open quantum system can be very different from an equilibrium thermodynamic state; and its reconstruction requires generically the numerical solution of the Lindblad equation, which is extremely costly for numerics.","Here we focus on the highly relevant situation of ensembles of light emitters undergoing spontaneous decay; and we show that, whenever their Hamiltonian is perturbed away from a U(1) symmetric form, steady-state quantum correlations can be reconstructed via pure-state perturbation theory.","Our main result is that in systems of light emitters subject to single-emitter or two-emitter driving, the steady state perturbed away from the U(1) limit generically exhibits spin squeezing; and it has minimal uncertainty for the collective-spin components, revealing that squeezing represents the optimal resource for entanglement-assisted metrology using this state."],"url":"http://arxiv.org/abs/2402.16824v1","category":"quant-ph"}
{"created":"2024-02-26 18:48:27","title":"Language Agents as Optimizable Graphs","abstract":"Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. The code can be found at https://github.com/metauto-ai/gptswarm.","sentences":["Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases.","We unify these approaches by describing LLM-based agents as computational graphs.","The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations.","Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents).","Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization).","Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents.","The code can be found at https://github.com/metauto-ai/gptswarm."],"url":"http://arxiv.org/abs/2402.16823v2","category":"cs.AI"}
{"created":"2024-02-26 18:47:27","title":"Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts","abstract":"As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to user inputs is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem, and uses open-ended search to generate prompts that are both effective and diverse. It can uncover a model's vulnerabilities across a broad range of domains including, in this paper, safety, question answering, and cybersecurity. We also demonstrate that fine-tuning on synthetic data generated by Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting their general capabilities and helpfulness, paving the path to open-ended self-improvement.","sentences":["As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to user inputs is of paramount importance.","Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations.","To address these limitations, we present Rainbow Teaming, a novel approach for producing a diverse collection of adversarial prompts.","Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem, and uses open-ended search to generate prompts that are both effective and diverse.","It can uncover a model's vulnerabilities across a broad range of domains including, in this paper, safety, question answering, and cybersecurity.","We also demonstrate that fine-tuning on synthetic data generated by Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting their general capabilities and helpfulness, paving the path to open-ended self-improvement."],"url":"http://arxiv.org/abs/2402.16822v1","category":"cs.CL"}
{"created":"2024-02-26 18:43:45","title":"Nemotron-4 15B Technical Report","abstract":"We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.","sentences":["We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens.","Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones.","Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks."],"url":"http://arxiv.org/abs/2402.16819v2","category":"cs.CL"}
{"created":"2024-02-26 18:33:13","title":"OncoGPT: A Medical Conversational Model Tailored with Oncology Domain Expertise on a Large Language Model Meta-AI (LLaMA)","abstract":"In the past year, there has been a growing trend in applying Large Language Models (LLMs) to the field of medicine, particularly with the advent of advanced language models such as ChatGPT developed by OpenAI. However, there is limited research on LLMs specifically addressing oncology-related queries. The primary aim of this research was to develop a specialized language model that demonstrates improved accuracy in providing advice related to oncology. We performed an extensive data collection of online question-answer interactions centered around oncology, sourced from reputable doctor-patient platforms. Following data cleaning and anonymization, a dataset comprising over 180K+ oncology-related conversations was established. The conversations were categorized and meticulously reviewed by field specialists and clinicians to ensure precision. Employing the LLaMA model and other selected open-source datasets, we conducted iterative fine-tuning to enhance the model's proficiency in basic medical conversation and specialized oncology knowledge. We observed a substantial enhancement in the model's understanding of genuine patient inquiries and its reliability in offering oncology-related advice through the utilization of real online question-answer interactions in the fine-tuning process. We release database and models to the research community (https://github.com/OncoGPT1).","sentences":["In the past year, there has been a growing trend in applying Large Language Models (LLMs) to the field of medicine, particularly with the advent of advanced language models such as ChatGPT developed by OpenAI.","However, there is limited research on LLMs specifically addressing oncology-related queries.","The primary aim of this research was to develop a specialized language model that demonstrates improved accuracy in providing advice related to oncology.","We performed an extensive data collection of online question-answer interactions centered around oncology, sourced from reputable doctor-patient platforms.","Following data cleaning and anonymization, a dataset comprising over 180K+ oncology-related conversations was established.","The conversations were categorized and meticulously reviewed by field specialists and clinicians to ensure precision.","Employing the LLaMA model and other selected open-source datasets, we conducted iterative fine-tuning to enhance the model's proficiency in basic medical conversation and specialized oncology knowledge.","We observed a substantial enhancement in the model's understanding of genuine patient inquiries and its reliability in offering oncology-related advice through the utilization of real online question-answer interactions in the fine-tuning process.","We release database and models to the research community (https://github.com/OncoGPT1)."],"url":"http://arxiv.org/abs/2402.16810v1","category":"cs.CL"}
{"created":"2024-02-26 18:08:52","title":"If in a Crowdsourced Data Annotation Pipeline, a GPT-4","abstract":"Recent studies indicated GPT-4 outperforms online crowd workers in data labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk). However, these studies were criticized for deviating from standard crowdsourcing practices and emphasizing individual workers' performances over the whole data-annotation process. This paper compared GPT-4 and an ethical and well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces yielded 127,080 labels, which were then used to infer the final labels through eight label-aggregation algorithms. Our evaluation showed that despite best practices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved 83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected via an advanced worker interface for aggregation, 2 out of the 8 algorithms achieved an even higher accuracy (87.5%, 87.0%). Further analysis suggested that, when the crowd's and GPT-4's labeling strengths are complementary, aggregating them could increase labeling accuracy.","sentences":["Recent studies indicated GPT-4 outperforms online crowd workers in data labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk).","However, these studies were criticized for deviating from standard crowdsourcing practices and emphasizing individual workers' performances over the whole data-annotation process.","This paper compared GPT-4 and an ethical and well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments from 200 scholarly articles using the CODA-19 scheme.","Two worker interfaces yielded 127,080 labels, which were then used to infer the final labels through eight label-aggregation algorithms.","Our evaluation showed that despite best practices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved 83.6%.","Interestingly, when combining GPT-4's labels with crowd labels collected via an advanced worker interface for aggregation, 2 out of the 8 algorithms achieved an even higher accuracy (87.5%, 87.0%).","Further analysis suggested that, when the crowd's and GPT-4's labeling strengths are complementary, aggregating them could increase labeling accuracy."],"url":"http://arxiv.org/abs/2402.16795v1","category":"cs.HC"}
{"created":"2024-02-26 18:05:55","title":"Rate-Optimal Rank Aggregation with Private Pairwise Rankings","abstract":"In various real-world scenarios like recommender systems and political surveys, pairwise rankings are commonly collected and utilized for rank aggregation to obtain an overall ranking of items. However, preference rankings can reveal individuals' personal preferences, underscoring the need to protect them before releasing for downstream analysis. In this paper, we address the challenge of preserving privacy while ensuring the utility of rank aggregation based on pairwise rankings generated from the Bradley-Terry-Luce (BTL) model. Using the randomized response mechanism to perturb raw pairwise rankings is a common privacy protection strategy used in practice, but a critical challenge arises because the privatized rankings no longer adhere to the BTL model, resulting in significant bias in downstream rank aggregation tasks. Motivated from this, we propose a debiased randomized response mechanism to protect the raw pairwise rankings, ensuring consistent estimation of true preferences and rankings in downstream rank aggregation. Theoretically, we offer insights into the relationship between overall privacy guarantees and estimation errors from private ranking data, and establish minimax rates for estimation errors. This enables the determination of optimal privacy guarantees that balance consistency in rank aggregation with robust privacy protection. We also investigate convergence rates of expected ranking errors for partial and full ranking recovery, quantifying how privacy protection influences the specification of top-$K$ item sets and complete rankings. Our findings are validated through extensive simulations and a real application.","sentences":["In various real-world scenarios like recommender systems and political surveys, pairwise rankings are commonly collected and utilized for rank aggregation to obtain an overall ranking of items.","However, preference rankings can reveal individuals' personal preferences, underscoring the need to protect them before releasing for downstream analysis.","In this paper, we address the challenge of preserving privacy while ensuring the utility of rank aggregation based on pairwise rankings generated from the Bradley-Terry-Luce (BTL) model.","Using the randomized response mechanism to perturb raw pairwise rankings is a common privacy protection strategy used in practice, but a critical challenge arises because the privatized rankings no longer adhere to the BTL model, resulting in significant bias in downstream rank aggregation tasks.","Motivated from this, we propose a debiased randomized response mechanism to protect the raw pairwise rankings, ensuring consistent estimation of true preferences and rankings in downstream rank aggregation.","Theoretically, we offer insights into the relationship between overall privacy guarantees and estimation errors from private ranking data, and establish minimax rates for estimation errors.","This enables the determination of optimal privacy guarantees that balance consistency in rank aggregation with robust privacy protection.","We also investigate convergence rates of expected ranking errors for partial and full ranking recovery, quantifying how privacy protection influences the specification of top-$K$ item sets and complete rankings.","Our findings are validated through extensive simulations and a real application."],"url":"http://arxiv.org/abs/2402.16792v1","category":"stat.ML"}
{"created":"2024-02-26 18:03:50","title":"Beyond Self-learned Attention: Mitigating Attention Bias in Transformer-based Models Using Attention Guidance","abstract":"Transformer-based models have demonstrated considerable potential for source code modeling tasks in software engineering. However, they are limited by their dependence solely on automatic self-attention weight learning mechanisms. Previous studies have shown that these models overemphasize delimiters added by tokenizers (e.g., [CLS], [SEP]), which may lead to overlooking essential information in the original input source code. To address this challenge, we introduce SyntaGuid, a novel approach that utilizes the observation that attention weights tend to be biased towards specific source code syntax tokens and abstract syntax tree (AST) elements in fine-tuned language models when they make correct predictions. SyntaGuid facilitates the guidance of attention-weight learning, leading to improved model performance on various software engineering tasks. We evaluate the effectiveness of SyntaGuid on multiple tasks and demonstrate that it outperforms existing state-of-the-art models in overall performance without requiring additional data. Experimental result shows that SyntaGuid can improve overall performance up to 3.25% and fix up to 28.3% wrong predictions. Our work represents the first attempt to guide the attention of Transformer-based models towards critical source code tokens during fine-tuning, highlighting the potential for enhancing Transformer-based models in software engineering.","sentences":["Transformer-based models have demonstrated considerable potential for source code modeling tasks in software engineering.","However, they are limited by their dependence solely on automatic self-attention weight learning mechanisms.","Previous studies have shown that these models overemphasize delimiters added by tokenizers (e.g., [CLS], [SEP]), which may lead to overlooking essential information in the original input source code.","To address this challenge, we introduce SyntaGuid, a novel approach that utilizes the observation that attention weights tend to be biased towards specific source code syntax tokens and abstract syntax tree (AST) elements in fine-tuned language models when they make correct predictions.","SyntaGuid facilitates the guidance of attention-weight learning, leading to improved model performance on various software engineering tasks.","We evaluate the effectiveness of SyntaGuid on multiple tasks and demonstrate that it outperforms existing state-of-the-art models in overall performance without requiring additional data.","Experimental result shows that SyntaGuid can improve overall performance up to 3.25% and fix up to 28.3% wrong predictions.","Our work represents the first attempt to guide the attention of Transformer-based models towards critical source code tokens during fine-tuning, highlighting the potential for enhancing Transformer-based models in software engineering."],"url":"http://arxiv.org/abs/2402.16790v1","category":"cs.SE"}
{"created":"2024-02-26 18:01:41","title":"Why Transformers Need Adam: A Hessian Perspective","abstract":"SGD performs worse than Adam by a significant margin on Transformers, but the reason remains unclear. In this work, we provide an explanation of SGD's failure on Transformers through the lens of Hessian: (i) Transformers are ``heterogeneous'': the Hessian spectrum across parameter blocks vary dramatically, a phenomenon we call ``block heterogeneity\"; (ii) Heterogeneity hampers SGD: SGD performs badly on problems with block heterogeneity. To validate that heterogeneity hampers SGD, we check various Transformers, CNNs, MLPs, and quadratic problems, and find that SGD works well on problems without block heterogeneity but performs badly when the heterogeneity exists. Our initial theoretical analysis indicates that SGD fails because it applies one single learning rate for all blocks, which cannot handle the heterogeneity among blocks. The failure could be rescued if we could assign different learning rates across blocks, as designed in Adam.","sentences":["SGD performs worse than Adam by a significant margin on Transformers, but the reason remains unclear.","In this work, we provide an explanation of SGD's failure on Transformers through the lens of Hessian: (i) Transformers are ``heterogeneous'': the Hessian spectrum across parameter blocks vary dramatically, a phenomenon we call ``block heterogeneity\"; (ii) Heterogeneity hampers SGD: SGD performs badly on problems with block heterogeneity.","To validate that heterogeneity hampers SGD, we check various Transformers, CNNs, MLPs, and quadratic problems, and find that SGD works well on problems without block heterogeneity but performs badly when the heterogeneity exists.","Our initial theoretical analysis indicates that SGD fails because it applies one single learning rate for all blocks, which cannot handle the heterogeneity among blocks.","The failure could be rescued if we could assign different learning rates across blocks, as designed in Adam."],"url":"http://arxiv.org/abs/2402.16788v1","category":"cs.LG"}
{"created":"2024-02-26 18:00:49","title":"Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models","abstract":"Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing constrained evaluation paradigm for values and opinions in LLMs and explore more realistic unconstrained evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs.","sentences":["Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires.","Most of this work is motivated by concerns around real-world LLM applications.","For example, politically-biased LLMs may subtly influence society when they are used by millions of people.","Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions.","Motivated by this discrepancy, we challenge the prevailing constrained evaluation paradigm for values and opinions in LLMs and explore more realistic unconstrained evaluations.","As a case study, we focus on the popular Political Compass Test (PCT).","In a systematic review, we find that most prior work using the PCT forces models to comply with the PCT's multiple-choice format.","We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness.","Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting.","We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs."],"url":"http://arxiv.org/abs/2402.16786v1","category":"cs.CL"}
{"created":"2024-02-26 17:56:21","title":"Multiplex measures for higher-order networks","abstract":"A wide variety of complex systems are characterized by interactions of different types involving varying numbers of units. Multiplex hypergraphs serve as a tool to describe such structures, capturing distinct types of higher-order interactions among a collection of units. In this work, we introduce a comprehensive set of measures to describe structural connectivity patterns in multiplex hypergraphs, considering scales from node and hyperedge levels to the system's mesoscale. We validate our measures with three real-world datasets: scientific co-authorship in physics, movie collaborations, and high school interactions. This validation reveals new collaboration patterns, identifies trends within and across movie subfields, and provides insights into daily interaction dynamics. Our framework aims to offer a more nuanced characterization of real-world systems marked by both multiplex and higher-order interactions.","sentences":["A wide variety of complex systems are characterized by interactions of different types involving varying numbers of units.","Multiplex hypergraphs serve as a tool to describe such structures, capturing distinct types of higher-order interactions among a collection of units.","In this work, we introduce a comprehensive set of measures to describe structural connectivity patterns in multiplex hypergraphs, considering scales from node and hyperedge levels to the system's mesoscale.","We validate our measures with three real-world datasets: scientific co-authorship in physics, movie collaborations, and high school interactions.","This validation reveals new collaboration patterns, identifies trends within and across movie subfields, and provides insights into daily interaction dynamics.","Our framework aims to offer a more nuanced characterization of real-world systems marked by both multiplex and higher-order interactions."],"url":"http://arxiv.org/abs/2402.16782v2","category":"physics.soc-ph"}
{"created":"2024-02-26 17:53:15","title":"FedReview: A Review Mechanism for Rejecting Poisoned Updates in Federated Learning","abstract":"Federated learning has recently emerged as a decentralized approach to learn a high-performance model without access to user data. Despite its effectiveness, federated learning gives malicious users opportunities to manipulate the model by uploading poisoned model updates to the server. In this paper, we propose a review mechanism called FedReview to identify and decline the potential poisoned updates in federated learning. Under our mechanism, the server randomly assigns a subset of clients as reviewers to evaluate the model updates on their training datasets in each round. The reviewers rank the model updates based on the evaluation results and count the number of the updates with relatively low quality as the estimated number of poisoned updates. Based on review reports, the server employs a majority voting mechanism to integrate the rankings and remove the potential poisoned updates in the model aggregation process. Extensive evaluation on multiple datasets demonstrate that FedReview can assist the server to learn a well-performed global model in an adversarial environment.","sentences":["Federated learning has recently emerged as a decentralized approach to learn a high-performance model without access to user data.","Despite its effectiveness, federated learning gives malicious users opportunities to manipulate the model by uploading poisoned model updates to the server.","In this paper, we propose a review mechanism called FedReview to identify and decline the potential poisoned updates in federated learning.","Under our mechanism, the server randomly assigns a subset of clients as reviewers to evaluate the model updates on their training datasets in each round.","The reviewers rank the model updates based on the evaluation results and count the number of the updates with relatively low quality as the estimated number of poisoned updates.","Based on review reports, the server employs a majority voting mechanism to integrate the rankings and remove the potential poisoned updates in the model aggregation process.","Extensive evaluation on multiple datasets demonstrate that FedReview can assist the server to learn a well-performed global model in an adversarial environment."],"url":"http://arxiv.org/abs/2402.16934v1","category":"cs.LG"}
{"created":"2024-02-26 17:45:36","title":"A Comprehensive Evaluation of Quantization Strategies for Large Language Models","abstract":"Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings. Quantization techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of LLMs. However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood. Evaluation of quantized LLMs is often limited to language modeling and a few classification tasks, leaving their performance on other benchmarks unclear. To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge \\& capacity, (2) alignment, and (3) efficiency, and conduct extensive experiments across ten diverse benchmarks. Our experimental results indicate that LLMs with 4-bit quantization can retain performance comparable to their non-quantized counterparts, and perplexity can serve as a proxy metric for quantized LLMs on most benchmarks. Furthermore, quantized LLMs with larger parameter scales can outperform smaller LLMs. Despite the memory savings achieved through quantization, it can also slow down the inference speed of LLMs. Consequently, substantial engineering efforts and hardware support are imperative to achieve a balanced optimization of decoding speed and memory consumption in the context of quantized LLMs.","sentences":["Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings.","Quantization techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of LLMs.","However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood.","Evaluation of quantized LLMs is often limited to language modeling and a few classification tasks, leaving their performance on other benchmarks unclear.","To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge \\& capacity, (2) alignment, and (3) efficiency, and conduct extensive experiments across ten diverse benchmarks.","Our experimental results indicate that LLMs with 4-bit quantization can retain performance comparable to their non-quantized counterparts, and perplexity can serve as a proxy metric for quantized LLMs on most benchmarks.","Furthermore, quantized LLMs with larger parameter scales can outperform smaller LLMs.","Despite the memory savings achieved through quantization, it can also slow down the inference speed of LLMs.","Consequently, substantial engineering efforts and hardware support are imperative to achieve a balanced optimization of decoding speed and memory consumption in the context of quantized LLMs."],"url":"http://arxiv.org/abs/2402.16775v1","category":"cs.CL"}
{"created":"2024-02-26 17:35:44","title":"CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks","abstract":"Knowledge-intensive language tasks (KILTs) typically require retrieving relevant documents from trustworthy corpora, e.g., Wikipedia, to produce specific answers. Very recently, a pre-trained generative retrieval model for KILTs, named CorpusBrain, was proposed and reached new state-of-the-art retrieval performance. However, most existing research on KILTs, including CorpusBrain, has predominantly focused on a static document collection, overlooking the dynamic nature of real-world scenarios, where new documents are continuously being incorporated into the source corpus. To address this gap, it is crucial to explore the capability of retrieval models to effectively handle the dynamic retrieval scenario inherent in KILTs.   In this work, we first introduce the continual document learning (CDL) task for KILTs and build a novel benchmark dataset named KILT++ based on the original KILT dataset for evaluation. Then, we conduct a comprehensive study over the use of pre-trained CorpusBrain on KILT++. Unlike the promising results in the stationary scenario, CorpusBrain is prone to catastrophic forgetting in the dynamic scenario, hence hampering the retrieval performance. To alleviate this issue, we propose CorpusBrain++, a continual generative pre-training framework. Empirical results demonstrate the significant effectiveness and remarkable efficiency of CorpusBrain++ in comparison to both traditional and generative IR methods.","sentences":["Knowledge-intensive language tasks (KILTs) typically require retrieving relevant documents from trustworthy corpora, e.g., Wikipedia, to produce specific answers.","Very recently, a pre-trained generative retrieval model for KILTs, named CorpusBrain, was proposed and reached new state-of-the-art retrieval performance.","However, most existing research on KILTs, including CorpusBrain, has predominantly focused on a static document collection, overlooking the dynamic nature of real-world scenarios, where new documents are continuously being incorporated into the source corpus.","To address this gap, it is crucial to explore the capability of retrieval models to effectively handle the dynamic retrieval scenario inherent in KILTs.   ","In this work, we first introduce the continual document learning (CDL) task for KILTs and build a novel benchmark dataset named KILT++ based on the original KILT dataset for evaluation.","Then, we conduct a comprehensive study over the use of pre-trained CorpusBrain on KILT++.","Unlike the promising results in the stationary scenario, CorpusBrain is prone to catastrophic forgetting in the dynamic scenario, hence hampering the retrieval performance.","To alleviate this issue, we propose CorpusBrain++, a continual generative pre-training framework.","Empirical results demonstrate the significant effectiveness and remarkable efficiency of CorpusBrain++ in comparison to both traditional and generative IR methods."],"url":"http://arxiv.org/abs/2402.16767v1","category":"cs.IR"}
{"created":"2024-02-26 17:30:34","title":"Order from chaos: Interplay of development and learning in recurrent networks of structured neurons","abstract":"Behavior can be described as a temporal sequence of actions driven by neural activity. To learn complex sequential patterns in neural networks, memories of past activities need to persist on significantly longer timescales than relaxation times of single-neuron activity. While recurrent networks can produce such long transients, training these networks in a biologically plausible way is challenging. One approach has been reservoir computing, where only weights from a recurrent network to a readout are learned. Other models achieve learning of recurrent synaptic weights using propagated errors. However, their biological plausibility typically suffers from issues with locality, resource allocation or parameter scales and tuning. We suggest that many of these issues can be alleviated by considering dendritic information storage and computation. By applying a fully local, always-on plasticity rule we are able to learn complex sequences in a recurrent network comprised of two populations. Importantly, our model is resource-efficient, enabling the learning of complex sequences using only a small number of neurons. We demonstrate these features in a mock-up of birdsong learning, in which our networks first learn a long, non-Markovian sequence that they can then reproduce robustly despite external disturbances.","sentences":["Behavior can be described as a temporal sequence of actions driven by neural activity.","To learn complex sequential patterns in neural networks, memories of past activities need to persist on significantly longer timescales than relaxation times of single-neuron activity.","While recurrent networks can produce such long transients, training these networks in a biologically plausible way is challenging.","One approach has been reservoir computing, where only weights from a recurrent network to a readout are learned.","Other models achieve learning of recurrent synaptic weights using propagated errors.","However, their biological plausibility typically suffers from issues with locality, resource allocation or parameter scales and tuning.","We suggest that many of these issues can be alleviated by considering dendritic information storage and computation.","By applying a fully local, always-on plasticity rule we are able to learn complex sequences in a recurrent network comprised of two populations.","Importantly, our model is resource-efficient, enabling the learning of complex sequences using only a small number of neurons.","We demonstrate these features in a mock-up of birdsong learning, in which our networks first learn a long, non-Markovian sequence that they can then reproduce robustly despite external disturbances."],"url":"http://arxiv.org/abs/2402.16763v1","category":"q-bio.NC"}
{"created":"2024-02-26 17:28:31","title":"Microscopic pathways of transition from low-density to high-density amorphous phase of water","abstract":"Much attention has been devoted to understanding the microscopic pathways of phase transition between two equilibrium condensed phases (such as liquids and solids). However, the microscopic pathways between non-equilibrium, non-diffusive amorphous (glassy) phases still remain poorly understood. In this work, we have employed computer simulations, persistence homology (a tool rooted in topological data analysis), and machine learning to probe the microscopic pathway of pressure-induced non-equilibrium transition between the low- and high-density amorphous (LDA and HDA, respectively) ice phases of TIP4P/2005 and ST2 water models. Using persistence homology and machine learning, we introduced a new order parameter that unambiguously identifies the LDA and HDA-like local environments. The system transitions continuously and collectively in the order parameter space via a pre-ordered intermediate phase during the compression of the LDA phase. The local order parameter susceptibilities show a maximum near the transition pressure ($P^*$) -- suggesting maximum structural heterogeneities near $P^*$. The HDA-like clusters are structurally ramified and spatially delocalized inside the LDA phase near the transition pressure. We have further investigated the (geometrical) structures and topologies of the LDA and HDA ices formed via different protocols and also studied the dependence of the microscopic pathway of phase transition in the order parameter space on the protocol followed to prepare the initial LDA phase. Finally, the method adopted here to study the microscopic pathways of transition is not restricted to the system under consideration and provides a robust way of probing phase transition pathways involving any two condensed phases at both equilibrium and out-of-equilibrium conditions.","sentences":["Much attention has been devoted to understanding the microscopic pathways of phase transition between two equilibrium condensed phases (such as liquids and solids).","However, the microscopic pathways between non-equilibrium, non-diffusive amorphous (glassy) phases still remain poorly understood.","In this work, we have employed computer simulations, persistence homology (a tool rooted in topological data analysis), and machine learning to probe the microscopic pathway of pressure-induced non-equilibrium transition between the low- and high-density amorphous (LDA and HDA, respectively) ice phases of TIP4P/2005 and ST2 water models.","Using persistence homology and machine learning, we introduced a new order parameter that unambiguously identifies the LDA and HDA-like local environments.","The system transitions continuously and collectively in the order parameter space via a pre-ordered intermediate phase during the compression of the LDA phase.","The local order parameter susceptibilities show a maximum near the transition pressure ($P^*$) -- suggesting maximum structural heterogeneities near $P^*$. The HDA-like clusters are structurally ramified and spatially delocalized inside the LDA phase near the transition pressure.","We have further investigated the (geometrical) structures and topologies of the LDA and HDA ices formed via different protocols and also studied the dependence of the microscopic pathway of phase transition in the order parameter space on the protocol followed to prepare the initial LDA phase.","Finally, the method adopted here to study the microscopic pathways of transition is not restricted to the system under consideration and provides a robust way of probing phase transition pathways involving any two condensed phases at both equilibrium and out-of-equilibrium conditions."],"url":"http://arxiv.org/abs/2402.16761v1","category":"physics.chem-ph"}
{"created":"2024-02-26 17:26:06","title":"The Door and Drawer Reset Mechanisms: Automated Mechanisms for Testing and Data Collection","abstract":"Robotic manipulation in human environments is a challenging problem for researchers and industry alike. In particular, opening doors/drawers can be challenging for robots, as the size, shape, actuation and required force is variable. Because of this, it can be difficult to collect large real-world datasets and to benchmark different control algorithms on the same hardware. In this paper we present two automated testbeds, the Door Reset Mechanism (DORM) and Drawer Reset Mechanism (DWRM), for the purpose of real world testing and data collection. These devices are low-cost, are sensorized, operate with customized variable resistance, and come with open source software. Additionally, we provide a dataset of over 600 grasps using the DORM and DWRM. We use this dataset to highlight how much variability can exist even with the same trial on the same hardware. This data can also serve as a source for real-world noise in simulation environments.","sentences":["Robotic manipulation in human environments is a challenging problem for researchers and industry alike.","In particular, opening doors/drawers can be challenging for robots, as the size, shape, actuation and required force is variable.","Because of this, it can be difficult to collect large real-world datasets and to benchmark different control algorithms on the same hardware.","In this paper we present two automated testbeds, the Door Reset Mechanism (DORM) and Drawer Reset Mechanism (DWRM), for the purpose of real world testing and data collection.","These devices are low-cost, are sensorized, operate with customized variable resistance, and come with open source software.","Additionally, we provide a dataset of over 600 grasps using the DORM and DWRM.","We use this dataset to highlight how much variability can exist even with the same trial on the same hardware.","This data can also serve as a source for real-world noise in simulation environments."],"url":"http://arxiv.org/abs/2402.16759v1","category":"cs.RO"}
{"created":"2024-02-26 17:21:57","title":"Towards Environmental Preference Based Speech Enhancement For Individualised Multi-Modal Hearing Aids","abstract":"Since the advent of Deep Learning (DL), Speech Enhancement (SE) models have performed well under a variety of noise conditions. However, such systems may still introduce sonic artefacts, sound unnatural, and restrict the ability for a user to hear ambient sound which may be of importance. Hearing Aid (HA) users may wish to customise their SE systems to suit their personal preferences and day-to-day lifestyle. In this paper, we introduce a preference learning based SE (PLSE) model for future multi-modal HAs that can contextually exploit audio information to improve listening comfort, based upon the preferences of the user. The proposed system estimates the Signal-to-noise ratio (SNR) as a basic objective speech quality measure which quantifies the relative amount of background noise present in speech, and directly correlates to the intelligibility of the signal. Additionally, to provide contextual information we predict the acoustic scene in which the user is situated. These tasks are achieved via a multi-task DL model, which surpasses the performance of inferring the acoustic scene or SNR separately, by jointly leveraging a shared encoded feature space. These environmental inferences are exploited in a preference elicitation framework, which linearly learns a set of predictive functions to determine the target SNR of an AV (Audio-Visual) SE system. By greatly reducing noise in challenging listening conditions, and by novelly scaling the output of the SE model, we are able to provide HA users with contextually individualised SE. Preliminary results suggest an improvement over the non-individualised baseline model in some participants.","sentences":["Since the advent of Deep Learning (DL), Speech Enhancement (SE) models have performed well under a variety of noise conditions.","However, such systems may still introduce sonic artefacts, sound unnatural, and restrict the ability for a user to hear ambient sound which may be of importance.","Hearing Aid (HA) users may wish to customise their SE systems to suit their personal preferences and day-to-day lifestyle.","In this paper, we introduce a preference learning based SE (PLSE) model for future multi-modal HAs that can contextually exploit audio information to improve listening comfort, based upon the preferences of the user.","The proposed system estimates the Signal-to-noise ratio (SNR) as a basic objective speech quality measure which quantifies the relative amount of background noise present in speech, and directly correlates to the intelligibility of the signal.","Additionally, to provide contextual information we predict the acoustic scene in which the user is situated.","These tasks are achieved via a multi-task DL model, which surpasses the performance of inferring the acoustic scene or SNR separately, by jointly leveraging a shared encoded feature space.","These environmental inferences are exploited in a preference elicitation framework, which linearly learns a set of predictive functions to determine the target SNR of an AV (Audio-Visual) SE system.","By greatly reducing noise in challenging listening conditions, and by novelly scaling the output of the SE model, we are able to provide HA users with contextually individualised SE.","Preliminary results suggest an improvement over the non-individualised baseline model in some participants."],"url":"http://arxiv.org/abs/2402.16757v1","category":"cs.SD"}
{"created":"2024-02-26 17:21:01","title":"Towards Bridging the Gap between Near and Far-Field Characterizations of the Wireless Channel","abstract":"The \"near-field\" propagation modeling of wireless channels is necessary to support sixth-generation (6G) technologies, such as intelligent reflecting surface (IRS), that are enabled by large aperture antennas and higher frequency carriers. As the conventional far-field model proves inadequate in this context, there is a pressing need to explore and bridge the gap between near and far-field propagation models. Although far-field models are simple and provide computationally efficient solutions for many practical applications, near-field models provide the most accurate representation of wireless channels. This paper builds upon the foundations of electromagnetic wave propagation theory to derive near and far-field models as approximations of the Green's function (Maxwell's equations). We characterize the near and far-field models both theoretically and with the help of simulations in a line-of-sight (LOS)-only scenario. In particular, for two key applications in multiantenna systems, namely, beamforming and multiple-access, we showcase the advantages of using the near-field model over the far-field, and present a novel scheduling scheme for multiple-access in the near-field regime. Our findings offer insights into the challenge of incorporating near-field models in practical wireless systems, fostering enhanced performance in future communication technologies.","sentences":["The \"near-field\" propagation modeling of wireless channels is necessary to support sixth-generation (6G) technologies, such as intelligent reflecting surface (IRS), that are enabled by large aperture antennas and higher frequency carriers.","As the conventional far-field model proves inadequate in this context, there is a pressing need to explore and bridge the gap between near and far-field propagation models.","Although far-field models are simple and provide computationally efficient solutions for many practical applications, near-field models provide the most accurate representation of wireless channels.","This paper builds upon the foundations of electromagnetic wave propagation theory to derive near and far-field models as approximations of the Green's function (Maxwell's equations).","We characterize the near and far-field models both theoretically and with the help of simulations in a line-of-sight (LOS)-only scenario.","In particular, for two key applications in multiantenna systems, namely, beamforming and multiple-access, we showcase the advantages of using the near-field model over the far-field, and present a novel scheduling scheme for multiple-access in the near-field regime.","Our findings offer insights into the challenge of incorporating near-field models in practical wireless systems, fostering enhanced performance in future communication technologies."],"url":"http://arxiv.org/abs/2402.16755v1","category":"cs.IT"}
{"created":"2024-02-26 17:20:16","title":"Avoiding Catastrophic Forgetting in Visual Classification Using Human Concept Formation","abstract":"Deep neural networks have excelled in machine learning, particularly in vision tasks, however, they often suffer from catastrophic forgetting when learning new tasks sequentially. In this work, we propose Cobweb4V, a novel visual classification approach that builds on Cobweb, a human like learning system that is inspired by the way humans incrementally learn new concepts over time. In this research, we conduct a comprehensive evaluation, showcasing the proficiency of Cobweb4V in learning visual concepts, requiring less data to achieve effective learning outcomes compared to traditional methods, maintaining stable performance over time, and achieving commendable asymptotic behavior, without catastrophic forgetting effects. These characteristics align with learning strategies in human cognition, positioning Cobweb4V as a promising alternative to neural network approaches.","sentences":["Deep neural networks have excelled in machine learning, particularly in vision tasks, however, they often suffer from catastrophic forgetting when learning new tasks sequentially.","In this work, we propose Cobweb4V, a novel visual classification approach that builds on Cobweb, a human like learning system that is inspired by the way humans incrementally learn new concepts over time.","In this research, we conduct a comprehensive evaluation, showcasing the proficiency of Cobweb4V in learning visual concepts, requiring less data to achieve effective learning outcomes compared to traditional methods, maintaining stable performance over time, and achieving commendable asymptotic behavior, without catastrophic forgetting effects.","These characteristics align with learning strategies in human cognition, positioning Cobweb4V as a promising alternative to neural network approaches."],"url":"http://arxiv.org/abs/2402.16933v1","category":"cs.LG"}
{"created":"2024-02-26 17:16:28","title":"Value Preferences Estimation and Disambiguation in Hybrid Participatory Systems","abstract":"Understanding citizens' values in participatory systems is crucial for citizen-centric policy-making. We envision a hybrid participatory system where participants make choices and provide motivations for those choices, and AI agents estimate their value preferences by interacting with them. We focus on situations where a conflict is detected between participants' choices and motivations, and propose methods for estimating value preferences while addressing detected inconsistencies by interacting with the participants. We operationalize the philosophical stance that \"valuing is deliberatively consequential.\" That is, if a participant's choice is based on a deliberation of value preferences, the value preferences can be observed in the motivation the participant provides for the choice. Thus, we propose and compare value estimation methods that prioritize the values estimated from motivations over the values estimated from choices alone. Then, we introduce a disambiguation strategy that addresses the detected inconsistencies between choices and motivations by directly interacting with the participants. We evaluate the proposed methods on a dataset of a large-scale survey on energy transition. The results show that explicitly addressing inconsistencies between choices and motivations improves the estimation of an individual's value preferences. The disambiguation strategy does not show substantial improvements when compared to similar baselines--however, we discuss how the novelty of the approach can open new research avenues and propose improvements to address the current limitations.","sentences":["Understanding citizens' values in participatory systems is crucial for citizen-centric policy-making.","We envision a hybrid participatory system where participants make choices and provide motivations for those choices, and AI agents estimate their value preferences by interacting with them.","We focus on situations where a conflict is detected between participants' choices and motivations, and propose methods for estimating value preferences while addressing detected inconsistencies by interacting with the participants.","We operationalize the philosophical stance that \"valuing is deliberatively consequential.\"","That is, if a participant's choice is based on a deliberation of value preferences, the value preferences can be observed in the motivation the participant provides for the choice.","Thus, we propose and compare value estimation methods that prioritize the values estimated from motivations over the values estimated from choices alone.","Then, we introduce a disambiguation strategy that addresses the detected inconsistencies between choices and motivations by directly interacting with the participants.","We evaluate the proposed methods on a dataset of a large-scale survey on energy transition.","The results show that explicitly addressing inconsistencies between choices and motivations improves the estimation of an individual's value preferences.","The disambiguation strategy does not show substantial improvements when compared to similar baselines--however, we discuss how the novelty of the approach can open new research avenues and propose improvements to address the current limitations."],"url":"http://arxiv.org/abs/2402.16751v1","category":"cs.AI"}
{"created":"2024-02-26 17:16:16","title":"Optical control and coherent coupling of spin diffusive modes in thermal gases","abstract":"Collective spins in thermal gases are at the core of a multitude of science and technology applications. In most of them, the random thermal motion of the particles is considered detrimental as it is responsible for decoherence and noise. In conditions of diffusive propagation, thermal atoms can potentially occupy various stable spatial modes in a glass cell. Extended or localized, diffusive modes have different magnetic properties, depending on the boundary conditions of the atomic cell, and can react differently to external perturbations. Here we demonstrate that few of these modes can be selectively excited, manipulated, and interrogated in atomic thermal vapours using laser light. In particular, we individuate the conditions for the generation of modes that are exceptionally resilient to undesirable effects introduced by optical pumping, such as light shifts and power-broadening, which are often the dominant sources of systematic errors in atomic magnetometers and co-magnetometers. Moreover, we show that the presence of spatial inhomogeneity in the pump, on top of the random diffusive atomic motion, introduces a coupling that leads to a coherent exchange of excitation between the two longest-lived modes. Our results indicate that systematic engineering of the multi-mode nature of diffusive gases has great potential for improving the performance of quantum technology applications based on alkali-metal thermal gases, and promote these simple experimental systems as versatile tools for quantum information applications.","sentences":["Collective spins in thermal gases are at the core of a multitude of science and technology applications.","In most of them, the random thermal motion of the particles is considered detrimental as it is responsible for decoherence and noise.","In conditions of diffusive propagation, thermal atoms can potentially occupy various stable spatial modes in a glass cell.","Extended or localized, diffusive modes have different magnetic properties, depending on the boundary conditions of the atomic cell, and can react differently to external perturbations.","Here we demonstrate that few of these modes can be selectively excited, manipulated, and interrogated in atomic thermal vapours using laser light.","In particular, we individuate the conditions for the generation of modes that are exceptionally resilient to undesirable effects introduced by optical pumping, such as light shifts and power-broadening, which are often the dominant sources of systematic errors in atomic magnetometers and co-magnetometers.","Moreover, we show that the presence of spatial inhomogeneity in the pump, on top of the random diffusive atomic motion, introduces a coupling that leads to a coherent exchange of excitation between the two longest-lived modes.","Our results indicate that systematic engineering of the multi-mode nature of diffusive gases has great potential for improving the performance of quantum technology applications based on alkali-metal thermal gases, and promote these simple experimental systems as versatile tools for quantum information applications."],"url":"http://arxiv.org/abs/2402.16750v1","category":"quant-ph"}
{"created":"2024-02-26 17:11:11","title":"MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large Multimodal Model","abstract":"With the evolution of storage and communication protocols, ultra-low bitrate image compression has become a highly demanding topic. However, existing compression algorithms must sacrifice either consistency with the ground truth or perceptual quality at ultra-low bitrate. In recent years, the rapid development of the Large Multimodal Model (LMM) has made it possible to balance these two goals. To solve this problem, this paper proposes a method called Multimodal Image Semantic Compression (MISC), which consists of an LMM encoder for extracting the semantic information of the image, a map encoder to locate the region corresponding to the semantic, an image encoder generates an extremely compressed bitstream, and a decoder reconstructs the image based on the above information. Experimental results show that our proposed MISC is suitable for compressing both traditional Natural Sense Images (NSIs) and emerging AI-Generated Images (AIGIs) content. It can achieve optimal consistency and perception results while saving 50% bitrate, which has strong potential applications in the next generation of storage and communication. The code will be released on https://github.com/lcysyzxdxc/MISC.","sentences":["With the evolution of storage and communication protocols, ultra-low bitrate image compression has become a highly demanding topic.","However, existing compression algorithms must sacrifice either consistency with the ground truth or perceptual quality at ultra-low bitrate.","In recent years, the rapid development of the Large Multimodal Model (LMM) has made it possible to balance these two goals.","To solve this problem, this paper proposes a method called Multimodal Image Semantic Compression (MISC), which consists of an LMM encoder for extracting the semantic information of the image, a map encoder to locate the region corresponding to the semantic, an image encoder generates an extremely compressed bitstream, and a decoder reconstructs the image based on the above information.","Experimental results show that our proposed MISC is suitable for compressing both traditional Natural Sense Images (NSIs) and emerging AI-Generated Images (AIGIs) content.","It can achieve optimal consistency and perception results while saving 50% bitrate, which has strong potential applications in the next generation of storage and communication.","The code will be released on https://github.com/lcysyzxdxc/MISC."],"url":"http://arxiv.org/abs/2402.16749v1","category":"cs.CV"}
{"created":"2024-02-26 16:52:46","title":"C-Band Lithium Niobate on Silicon Carbide Surface Acoustic Wave Resonator with Figure-of-Merit of 124 at 6.5 GHz","abstract":"In this work, we demonstrate a C-band shear-horizontal surface acoustic wave (SH-SAW) resonator with high electromechanical coupling (keff2) of 22% and a quality factor (Q) of 565 based on a thin-film lithium niobate (LN) on silicon carbide (SiC) platform, featuring an excellent figure-of-merit (FoM = keff2*Q ) of 124 at 6.5 GHz, the highest FoM reported in this frequency range. The resonator frequency upscaling is achieved through wavelength ($\\lambda$) reduction and the use of thin aluminum (Al) electrodes. The LN/SiC waveguide and synchronous resonator design collectively enable effective acoustic energy confinement for a high FoM, even when the normalized thickness of LN approaches a scale of 0.5$\\lambda$ to 1$\\lambda$. To perform a comprehensive study, we also designed and fabricated five additional resonators, expending the $\\lambda$ studied ranging from 480 to 800 nm, in the same 500 nm-thick transferred Y-cut thin-film LN on SiC. The fabricated SH-SAW resonators, operating from 5 to 8 GHz, experimentally demonstrate a keff2 from 20.3% to 22.9% and a Q from 350 to 575, thereby covering the entire C-band with excellent performance.","sentences":["In this work, we demonstrate a C-band shear-horizontal surface acoustic wave (SH-SAW) resonator with high electromechanical coupling (keff2) of 22% and a quality factor (Q) of 565 based on a thin-film lithium niobate (LN) on silicon carbide (SiC) platform, featuring an excellent figure-of-merit (FoM = keff2*Q ) of 124 at 6.5 GHz, the highest FoM reported in this frequency range.","The resonator frequency upscaling is achieved through wavelength ($\\lambda$) reduction and the use of thin aluminum (Al) electrodes.","The LN/SiC waveguide and synchronous resonator design collectively enable effective acoustic energy confinement for a high FoM, even when the normalized thickness of LN approaches a scale of 0.5$\\lambda$ to 1$\\lambda$. To perform a comprehensive study, we also designed and fabricated five additional resonators, expending the $\\lambda$ studied ranging from 480 to 800 nm, in the same 500 nm-thick transferred Y-cut thin-film LN on SiC.","The fabricated SH-SAW resonators, operating from 5 to 8 GHz, experimentally demonstrate a keff2 from 20.3% to 22.9% and a Q from 350 to 575, thereby covering the entire C-band with excellent performance."],"url":"http://arxiv.org/abs/2402.16732v1","category":"eess.SP"}
{"created":"2024-02-26 16:52:35","title":"Accelerating Graph Neural Networks on Real Processing-In-Memory Systems","abstract":"Graph Neural Networks (GNNs) are emerging ML models to analyze graph-structure data. Graph Neural Network (GNN) execution involves both compute-intensive and memory-intensive kernels, the latter dominates the total time, being significantly bottlenecked by data movement between memory and processors. Processing-In-Memory (PIM) systems can alleviate this data movement bottleneck by placing simple processors near or inside to memory arrays. In this work, we introduce PyGim, an efficient ML framework that accelerates GNNs on real PIM systems. We propose intelligent parallelization techniques for memory-intensive kernels of GNNs tailored for real PIM systems, and develop handy Python API for them. We provide hybrid GNN execution, in which the compute-intensive and memory-intensive kernels are executed in processor-centric and memory-centric computing systems, respectively, to match their algorithmic nature. We extensively evaluate PyGim on a real-world PIM system with 1992 PIM cores using emerging GNN models, and demonstrate that it outperforms its state-of-the-art CPU counterpart on Intel Xeon by on average 3.04x, and achieves higher resource utilization than CPU and GPU systems. Our work provides useful recommendations for software, system and hardware designers. PyGim will be open-sourced to enable the widespread use of PIM systems in GNNs.","sentences":["Graph Neural Networks (GNNs) are emerging ML models to analyze graph-structure data.","Graph Neural Network (GNN) execution involves both compute-intensive and memory-intensive kernels, the latter dominates the total time, being significantly bottlenecked by data movement between memory and processors.","Processing-In-Memory (PIM) systems can alleviate this data movement bottleneck by placing simple processors near or inside to memory arrays.","In this work, we introduce PyGim, an efficient ML framework that accelerates GNNs on real PIM systems.","We propose intelligent parallelization techniques for memory-intensive kernels of GNNs tailored for real PIM systems, and develop handy Python API for them.","We provide hybrid GNN execution, in which the compute-intensive and memory-intensive kernels are executed in processor-centric and memory-centric computing systems, respectively, to match their algorithmic nature.","We extensively evaluate PyGim on a real-world PIM system with 1992 PIM cores using emerging GNN models, and demonstrate that it outperforms its state-of-the-art CPU counterpart on Intel Xeon by on average 3.04x, and achieves higher resource utilization than CPU and GPU systems.","Our work provides useful recommendations for software, system and hardware designers.","PyGim will be open-sourced to enable the widespread use of PIM systems in GNNs."],"url":"http://arxiv.org/abs/2402.16731v1","category":"cs.AR"}
{"created":"2024-02-26 16:48:12","title":"Interpreting Grokked Transformers in Complex Modular Arithmetic","abstract":"Grokking has been actively explored to reveal the mystery of delayed generalization. Identifying interpretable algorithms inside the grokked models is a suggestive hint to understanding its mechanism. In this work, beyond the simplest and well-studied modular addition, we observe the internal circuits learned through grokking in complex modular arithmetic via interpretable reverse engineering, which highlights the significant difference in their dynamics: subtraction poses a strong asymmetry on Transformer; multiplication requires cosine-biased components at all the frequencies in a Fourier domain; polynomials often result in the superposition of the patterns from elementary arithmetic, but clear patterns do not emerge in challenging cases; grokking can easily occur even in higher-degree formulas with basic symmetric and alternating expressions. We also introduce the novel progress measure for modular arithmetic; Fourier Frequency Sparsity and Fourier Coefficient Ratio, which not only indicate the late generalization but also characterize distinctive internal representations of grokked models per modular operation. Our empirical analysis emphasizes the importance of holistic evaluation among various combinations.","sentences":["Grokking has been actively explored to reveal the mystery of delayed generalization.","Identifying interpretable algorithms inside the grokked models is a suggestive hint to understanding its mechanism.","In this work, beyond the simplest and well-studied modular addition, we observe the internal circuits learned through grokking in complex modular arithmetic via interpretable reverse engineering, which highlights the significant difference in their dynamics: subtraction poses a strong asymmetry on Transformer; multiplication requires cosine-biased components at all the frequencies in a Fourier domain; polynomials often result in the superposition of the patterns from elementary arithmetic, but clear patterns do not emerge in challenging cases; grokking can easily occur even in higher-degree formulas with basic symmetric and alternating expressions.","We also introduce the novel progress measure for modular arithmetic; Fourier Frequency Sparsity and Fourier Coefficient Ratio, which not only indicate the late generalization but also characterize distinctive internal representations of grokked models per modular operation.","Our empirical analysis emphasizes the importance of holistic evaluation among various combinations."],"url":"http://arxiv.org/abs/2402.16726v2","category":"cs.LG"}
{"created":"2024-02-26 16:38:22","title":"An Overview of the Development of Stereotactic Body Radiation Therapy","abstract":"Stereotactic body radiation therapy (SBRT) refers to focusing high-energy rays in three-dimensional space on the tumor lesion area, reducing the dose received by surrounding normal tissues, which can effectively improve the local control rate of the tumor and reduce the probability of complications. With the comprehensive development of medical imaging, radiation biology and other disciplines, this less-fractional, high-dose radiotherapy method has been increasingly developed and applied in clinical practice. The background, radio-biological basis, key technologies and main equipment of SBRT are discussed, and its future development direction is prospected.","sentences":["Stereotactic body radiation therapy (SBRT) refers to focusing high-energy rays in three-dimensional space on the tumor lesion area, reducing the dose received by surrounding normal tissues, which can effectively improve the local control rate of the tumor and reduce the probability of complications.","With the comprehensive development of medical imaging, radiation biology and other disciplines, this less-fractional, high-dose radiotherapy method has been increasingly developed and applied in clinical practice.","The background, radio-biological basis, key technologies and main equipment of SBRT are discussed, and its future development direction is prospected."],"url":"http://arxiv.org/abs/2402.16718v1","category":"physics.med-ph"}
{"created":"2024-02-26 16:35:59","title":"CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models","abstract":"Adversarial misuse, particularly through `jailbreaking' that circumvents a model's safety and ethical protocols, poses a significant challenge for Large Language Models (LLMs). This paper delves into the mechanisms behind such successful attacks, introducing a hypothesis for the safety mechanism of aligned LLMs: intent security recognition followed by response generation. Grounded in this hypothesis, we propose CodeChameleon, a novel jailbreak framework based on personalized encryption tactics. To elude the intent security recognition phase, we reformulate tasks into a code completion format, enabling users to encrypt queries using personalized encryption functions. To guarantee response generation functionality, we embed a decryption function within the instructions, which allows the LLM to decrypt and execute the encrypted queries successfully. We conduct extensive experiments on 7 LLMs, achieving state-of-the-art average Attack Success Rate (ASR). Remarkably, our method achieves an 86.6\\% ASR on GPT-4-1106.","sentences":["Adversarial misuse, particularly through `jailbreaking' that circumvents a model's safety and ethical protocols, poses a significant challenge for Large Language Models (LLMs).","This paper delves into the mechanisms behind such successful attacks, introducing a hypothesis for the safety mechanism of aligned LLMs: intent security recognition followed by response generation.","Grounded in this hypothesis, we propose CodeChameleon, a novel jailbreak framework based on personalized encryption tactics.","To elude the intent security recognition phase, we reformulate tasks into a code completion format, enabling users to encrypt queries using personalized encryption functions.","To guarantee response generation functionality, we embed a decryption function within the instructions, which allows the LLM to decrypt and execute the encrypted queries successfully.","We conduct extensive experiments on 7 LLMs, achieving state-of-the-art average Attack Success Rate (ASR).","Remarkably, our method achieves an 86.6\\% ASR on GPT-4-1106."],"url":"http://arxiv.org/abs/2402.16717v1","category":"cs.CL"}
{"created":"2024-02-26 16:34:29","title":"PromptSet: A Programmer's Prompting Dataset","abstract":"The rise of capabilities expressed by large language models has been quickly followed by the integration of the same complex systems into application level logic. Algorithms, programs, systems, and companies are built around structured prompting to black box models where the majority of the design and implementation lies in capturing and quantifying the `agent mode'. The standard way to shape a closed language model is to prime it for a specific task with a tailored prompt, often initially handwritten by a human. The textual prompts co-evolve with the codebase, taking shape over the course of project life as artifacts which must be reviewed and maintained, just as the traditional code files might be. Unlike traditional code, we find that prompts do not receive effective static testing and linting to prevent runtime issues. In this work, we present a novel dataset called PromptSet, with more than 61,000 unique developer prompts used in open source Python programs. We perform analysis on this dataset and introduce the notion of a static linter for prompts. Released with this publication is a HuggingFace dataset and a Github repository to recreate collection and processing efforts, both under the name \\texttt{pisterlabs/promptset}.","sentences":["The rise of capabilities expressed by large language models has been quickly followed by the integration of the same complex systems into application level logic.","Algorithms, programs, systems, and companies are built around structured prompting to black box models where the majority of the design and implementation lies in capturing and quantifying the `agent mode'.","The standard way to shape a closed language model is to prime it for a specific task with a tailored prompt, often initially handwritten by a human.","The textual prompts co-evolve with the codebase, taking shape over the course of project life as artifacts which must be reviewed and maintained, just as the traditional code files might be.","Unlike traditional code, we find that prompts do not receive effective static testing and linting to prevent runtime issues.","In this work, we present a novel dataset called PromptSet, with more than 61,000 unique developer prompts used in open source Python programs.","We perform analysis on this dataset and introduce the notion of a static linter for prompts.","Released with this publication is a HuggingFace dataset and a Github repository to recreate collection and processing efforts, both under the name \\texttt{pisterlabs/promptset}."],"url":"http://arxiv.org/abs/2402.16932v1","category":"cs.SE"}
{"created":"2024-02-26 16:31:28","title":"Quantum linear algebra is all you need for Transformer architectures","abstract":"Generative machine learning methods such as large-language models are revolutionizing the creation of text and images. While these models are powerful they also harness a large amount of computational resources. The transformer is a key component in large language models that aims to generate a suitable completion of a given partial sequence. In this work, we investigate transformer architectures under the lens of fault-tolerant quantum computing. The input model is one where pre-trained weight matrices are given as block encodings to construct the query, key, and value matrices for the transformer. As a first step, we show how to prepare a block encoding of the self-attention matrix, with a row-wise application of the softmax function using the Hadamard product. In addition, we combine quantum subroutines to construct important building blocks in the transformer, the residual connection, layer normalization, and the feed-forward neural network. Our subroutines prepare an amplitude encoding of the transformer output, which can be measured to obtain a prediction. We discuss the potential and challenges for obtaining a quantum advantage.","sentences":["Generative machine learning methods such as large-language models are revolutionizing the creation of text and images.","While these models are powerful they also harness a large amount of computational resources.","The transformer is a key component in large language models that aims to generate a suitable completion of a given partial sequence.","In this work, we investigate transformer architectures under the lens of fault-tolerant quantum computing.","The input model is one where pre-trained weight matrices are given as block encodings to construct the query, key, and value matrices for the transformer.","As a first step, we show how to prepare a block encoding of the self-attention matrix, with a row-wise application of the softmax function using the Hadamard product.","In addition, we combine quantum subroutines to construct important building blocks in the transformer, the residual connection, layer normalization, and the feed-forward neural network.","Our subroutines prepare an amplitude encoding of the transformer output, which can be measured to obtain a prediction.","We discuss the potential and challenges for obtaining a quantum advantage."],"url":"http://arxiv.org/abs/2402.16714v1","category":"quant-ph"}
{"created":"2024-02-26 16:21:53","title":"SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection","abstract":"Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data sets, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also been corroborated in various foundation models and domain-specific tasks. Our findings suggest that longer and more computationally intensive IT data may serve as superior sources of IT, offering valuable insights for future research in this area. Data, code, and scripts are freely available at https://github.com/Blue-Raincoat/SelectIT.","sentences":["Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions.","Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs.","Despite this, common approaches often rely on additional models or data sets, which increases costs and limits widespread adoption.","In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself.","Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources.","Furthermore, we introduce a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset.","Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement.","The robustness of SelectIT has also been corroborated in various foundation models and domain-specific tasks.","Our findings suggest that longer and more computationally intensive IT data may serve as superior sources of IT, offering valuable insights for future research in this area.","Data, code, and scripts are freely available at https://github.com/Blue-Raincoat/SelectIT."],"url":"http://arxiv.org/abs/2402.16705v1","category":"cs.CL"}
{"created":"2024-02-26 16:14:47","title":"Generating Effective Ensembles for Sentiment Analysis","abstract":"In recent years, transformer models have revolutionized Natural Language Processing (NLP), achieving exceptional results across various tasks, including Sentiment Analysis (SA). As such, current state-of-the-art approaches for SA predominantly rely on transformer models alone, achieving impressive accuracy levels on benchmark datasets. In this paper, we show that the key for further improving the accuracy of such ensembles for SA is to include not only transformers, but also traditional NLP models, despite the inferiority of the latter compared to transformer models. However, as we empirically show, this necessitates a change in how the ensemble is constructed, specifically relying on the Hierarchical Ensemble Construction (HEC) algorithm we present. Our empirical studies across eight canonical SA datasets reveal that ensembles incorporating a mix of model types, structured via HEC, significantly outperform traditional ensembles. Finally, we provide a comparative analysis of the performance of the HEC and GPT-4, demonstrating that while GPT-4 closely approaches state-of-the-art SA methods, it remains outperformed by our proposed ensemble strategy.","sentences":["In recent years, transformer models have revolutionized Natural Language Processing (NLP), achieving exceptional results across various tasks, including Sentiment Analysis (SA).","As such, current state-of-the-art approaches for SA predominantly rely on transformer models alone, achieving impressive accuracy levels on benchmark datasets.","In this paper, we show that the key for further improving the accuracy of such ensembles for SA is to include not only transformers, but also traditional NLP models, despite the inferiority of the latter compared to transformer models.","However, as we empirically show, this necessitates a change in how the ensemble is constructed, specifically relying on the Hierarchical Ensemble Construction (HEC) algorithm we present.","Our empirical studies across eight canonical SA datasets reveal that ensembles incorporating a mix of model types, structured via HEC, significantly outperform traditional ensembles.","Finally, we provide a comparative analysis of the performance of the HEC and GPT-4, demonstrating that while GPT-4 closely approaches state-of-the-art SA methods, it remains outperformed by our proposed ensemble strategy."],"url":"http://arxiv.org/abs/2402.16700v1","category":"cs.CL"}
{"created":"2024-02-26 16:09:00","title":"HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization","abstract":"Large language models (LLMs) have made significant progress in generating codes from textual prompts. However, existing benchmarks have mainly concentrated on translating English prompts to multilingual codes or have been constrained to very limited natural languages (NLs). These benchmarks have overlooked the vast landscape of massively multilingual NL to multilingual code, leaving a critical gap in the evaluation of multilingual LLMs. In response, we introduce HumanEval-XL, a massively multilingual code generation benchmark specifically crafted to address this deficiency. HumanEval-XL establishes connections between 23 NLs and 12 programming languages (PLs), and comprises of a collection of 22,080 prompts with an average of 8.33 test cases. By ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a comprehensive evaluation platform for multilingual LLMs, allowing the assessment of the understanding of different NLs. Our work serves as a pioneering step towards filling the void in evaluating NL generalization in the area of multilingual code generation. We make our evaluation code and data publicly available at \\url{https://github.com/FloatAI/HumanEval-XL}.","sentences":["Large language models (LLMs) have made significant progress in generating codes from textual prompts.","However, existing benchmarks have mainly concentrated on translating English prompts to multilingual codes or have been constrained to very limited natural languages (NLs).","These benchmarks have overlooked the vast landscape of massively multilingual NL to multilingual code, leaving a critical gap in the evaluation of multilingual LLMs.","In response, we introduce HumanEval-XL, a massively multilingual code generation benchmark specifically crafted to address this deficiency.","HumanEval-XL establishes connections between 23 NLs and 12 programming languages (PLs), and comprises of a collection of 22,080 prompts with an average of 8.33 test cases.","By ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a comprehensive evaluation platform for multilingual LLMs, allowing the assessment of the understanding of different NLs.","Our work serves as a pioneering step towards filling the void in evaluating NL generalization in the area of multilingual code generation.","We make our evaluation code and data publicly available at \\url{https://github.com/FloatAI/HumanEval-XL}."],"url":"http://arxiv.org/abs/2402.16694v1","category":"cs.CL"}
{"created":"2024-02-26 16:05:33","title":"Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study","abstract":"Recently, pretrained language models based on BERT have been introduced for the French biomedical domain. Although these models have achieved state-of-the-art results on biomedical and clinical NLP tasks, they are constrained by a limited input sequence length of 512 tokens, which poses challenges when applied to clinical notes. In this paper, we present a comparative study of three adaptation strategies for long-sequence models, leveraging the Longformer architecture. We conducted evaluations of these models on 16 downstream tasks spanning both biomedical and clinical domains. Our findings reveal that further pre-training an English clinical model with French biomedical texts can outperform both converting a French biomedical BERT to the Longformer architecture and pre-training a French biomedical Longformer from scratch. The results underscore that long-sequence French biomedical models improve performance across most downstream tasks regardless of sequence length, but BERT based models remain the most efficient for named entity recognition tasks.","sentences":["Recently, pretrained language models based on BERT have been introduced for the French biomedical domain.","Although these models have achieved state-of-the-art results on biomedical and clinical NLP tasks, they are constrained by a limited input sequence length of 512 tokens, which poses challenges when applied to clinical notes.","In this paper, we present a comparative study of three adaptation strategies for long-sequence models, leveraging the Longformer architecture.","We conducted evaluations of these models on 16 downstream tasks spanning both biomedical and clinical domains.","Our findings reveal that further pre-training an English clinical model with French biomedical texts can outperform both converting a French biomedical BERT to the Longformer architecture and pre-training a French biomedical Longformer from scratch.","The results underscore that long-sequence French biomedical models improve performance across most downstream tasks regardless of sequence length, but BERT based models remain the most efficient for named entity recognition tasks."],"url":"http://arxiv.org/abs/2402.16689v1","category":"cs.CL"}
{"created":"2024-02-26 16:02:15","title":"Automated Floodwater Depth Estimation Using Large Multimodal Model for Rapid Flood Mapping","abstract":"Information on the depth of floodwater is crucial for rapid mapping of areas affected by floods. However, previous approaches for estimating floodwater depth, including field surveys, remote sensing, and machine learning techniques, can be time-consuming and resource-intensive. This paper presents an automated and fast approach for estimating floodwater depth from on-site flood photos. A pre-trained large multimodal model, GPT-4 Vision, was used specifically for estimating floodwater. The input data were flooding photos that contained referenced objects, such as street signs, cars, people, and buildings. Using the heights of the common objects as references, the model returned the floodwater depth as the output. Results show that the proposed approach can rapidly provide a consistent and reliable estimation of floodwater depth from flood photos. Such rapid estimation is transformative in flood inundation mapping and assessing the severity of the flood in near-real time, which is essential for effective flood response strategies.","sentences":["Information on the depth of floodwater is crucial for rapid mapping of areas affected by floods.","However, previous approaches for estimating floodwater depth, including field surveys, remote sensing, and machine learning techniques, can be time-consuming and resource-intensive.","This paper presents an automated and fast approach for estimating floodwater depth from on-site flood photos.","A pre-trained large multimodal model, GPT-4 Vision, was used specifically for estimating floodwater.","The input data were flooding photos that contained referenced objects, such as street signs, cars, people, and buildings.","Using the heights of the common objects as references, the model returned the floodwater depth as the output.","Results show that the proposed approach can rapidly provide a consistent and reliable estimation of floodwater depth from flood photos.","Such rapid estimation is transformative in flood inundation mapping and assessing the severity of the flood in near-real time, which is essential for effective flood response strategies."],"url":"http://arxiv.org/abs/2402.16684v1","category":"cs.CV"}
{"created":"2024-02-26 15:40:46","title":"Program-Based Strategy Induction for Reinforcement Learning","abstract":"Typical models of learning assume incremental estimation of continuously-varying decision variables like expected rewards. However, this class of models fails to capture more idiosyncratic, discrete heuristics and strategies that people and animals appear to exhibit. Despite recent advances in strategy discovery using tools like recurrent networks that generalize the classic models, the resulting strategies are often onerous to interpret, making connections to cognition difficult to establish. We use Bayesian program induction to discover strategies implemented by programs, letting the simplicity of strategies trade off against their effectiveness. Focusing on bandit tasks, we find strategies that are difficult or unexpected with classical incremental learning, like asymmetric learning from rewarded and unrewarded trials, adaptive horizon-dependent random exploration, and discrete state switching.","sentences":["Typical models of learning assume incremental estimation of continuously-varying decision variables like expected rewards.","However, this class of models fails to capture more idiosyncratic, discrete heuristics and strategies that people and animals appear to exhibit.","Despite recent advances in strategy discovery using tools like recurrent networks that generalize the classic models, the resulting strategies are often onerous to interpret, making connections to cognition difficult to establish.","We use Bayesian program induction to discover strategies implemented by programs, letting the simplicity of strategies trade off against their effectiveness.","Focusing on bandit tasks, we find strategies that are difficult or unexpected with classical incremental learning, like asymmetric learning from rewarded and unrewarded trials, adaptive horizon-dependent random exploration, and discrete state switching."],"url":"http://arxiv.org/abs/2402.16668v1","category":"cs.LG"}
{"created":"2024-02-26 15:39:52","title":"RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation","abstract":"Generative models have demonstrated considerable potential in software engineering, particularly in tasks such as code generation and debugging. However, their utilization in the domain of code documentation generation remains underexplored. To this end, we introduce RepoAgent, a large language model powered open-source framework aimed at proactively generating, maintaining, and updating code documentation. Through both qualitative and quantitative evaluations, we have validated the effectiveness of our approach, showing that RepoAgent excels in generating high-quality repository-level documentation. The code and results are publicly accessible at https://github.com/OpenBMB/RepoAgent.","sentences":["Generative models have demonstrated considerable potential in software engineering, particularly in tasks such as code generation and debugging.","However, their utilization in the domain of code documentation generation remains underexplored.","To this end, we introduce RepoAgent, a large language model powered open-source framework aimed at proactively generating, maintaining, and updating code documentation.","Through both qualitative and quantitative evaluations, we have validated the effectiveness of our approach, showing that RepoAgent excels in generating high-quality repository-level documentation.","The code and results are publicly accessible at https://github.com/OpenBMB/RepoAgent."],"url":"http://arxiv.org/abs/2402.16667v1","category":"cs.CL"}
{"created":"2024-02-26 15:35:24","title":"LLM-Assisted Multi-Teacher Continual Learning for Visual Question Answering in Robotic Surgery","abstract":"Visual question answering (VQA) can be fundamentally crucial for promoting robotic-assisted surgical education. In practice, the needs of trainees are constantly evolving, such as learning more surgical types, adapting to different robots, and learning new surgical instruments and techniques for one surgery. Therefore, continually updating the VQA system by a sequential data stream from multiple resources is demanded in robotic surgery to address new tasks. In surgical scenarios, the storage cost and patient data privacy often restrict the availability of old data when updating the model, necessitating an exemplar-free continual learning (CL) setup. However, prior studies overlooked two vital problems of the surgical domain: i) large domain shifts from diverse surgical operations collected from multiple departments or clinical centers, and ii) severe data imbalance arising from the uneven presence of surgical instruments or activities during surgical procedures. This paper proposes to address these two problems with a multimodal large language model (LLM) and an adaptive weight assignment methodology. We first develop a new multi-teacher CL framework that leverages a multimodal LLM as the additional teacher. The strong generalization ability of the LLM can bridge the knowledge gap when domain shifts and data imbalances occur. We then put forth a novel data processing method that transforms complex LLM embeddings into logits compatible with our CL framework. We further design an adaptive weight assignment approach that balances the generalization ability of the LLM and the domain expertise of the old CL model. We construct a new dataset for surgical VQA tasks, providing valuable data resources for future research. Extensive experimental results on three datasets demonstrate the superiority of our method to other advanced CL models.","sentences":["Visual question answering (VQA) can be fundamentally crucial for promoting robotic-assisted surgical education.","In practice, the needs of trainees are constantly evolving, such as learning more surgical types, adapting to different robots, and learning new surgical instruments and techniques for one surgery.","Therefore, continually updating the VQA system by a sequential data stream from multiple resources is demanded in robotic surgery to address new tasks.","In surgical scenarios, the storage cost and patient data privacy often restrict the availability of old data when updating the model, necessitating an exemplar-free continual learning (CL) setup.","However, prior studies overlooked two vital problems of the surgical domain: i) large domain shifts from diverse surgical operations collected from multiple departments or clinical centers, and ii) severe data imbalance arising from the uneven presence of surgical instruments or activities during surgical procedures.","This paper proposes to address these two problems with a multimodal large language model (LLM) and an adaptive weight assignment methodology.","We first develop a new multi-teacher CL framework that leverages a multimodal LLM as the additional teacher.","The strong generalization ability of the LLM can bridge the knowledge gap when domain shifts and data imbalances occur.","We then put forth a novel data processing method that transforms complex LLM embeddings into logits compatible with our CL framework.","We further design an adaptive weight assignment approach that balances the generalization ability of the LLM and the domain expertise of the old CL model.","We construct a new dataset for surgical VQA tasks, providing valuable data resources for future research.","Extensive experimental results on three datasets demonstrate the superiority of our method to other advanced CL models."],"url":"http://arxiv.org/abs/2402.16664v1","category":"cs.IR"}
{"created":"2024-02-26 15:34:06","title":"BOXREC: Recommending a Box of Preferred Outfits in Online Shopping","abstract":"Over the past few years, automation of outfit composition has gained much attention from the research community. Most of the existing outfit recommendation systems focus on pairwise item compatibility prediction (using visual and text features) to score an outfit combination having several items, followed by recommendation of top-n outfits or a capsule wardrobe having a collection of outfits based on user's fashion taste. However, none of these consider user's preference of price-range for individual clothing types or an overall shopping budget for a set of items. In this paper, we propose a box recommendation framework - BOXREC - which at first, collects user preferences across different item types (namely, top-wear, bottom-wear and foot-wear) including price-range of each type and a maximum shopping budget for a particular shopping session. It then generates a set of preferred outfits by retrieving all types of preferred items from the database (according to user specified preferences including price-ranges), creates all possible combinations of three preferred items (belonging to distinct item types) and verifies each combination using an outfit scoring framework - BOXREC-OSF. Finally, it provides a box full of fashion items, such that different combinations of the items maximize the number of outfits suitable for an occasion while satisfying maximum shopping budget. Empirical results show superior performance of BOXREC-OSF over the baseline methods.","sentences":["Over the past few years, automation of outfit composition has gained much attention from the research community.","Most of the existing outfit recommendation systems focus on pairwise item compatibility prediction (using visual and text features) to score an outfit combination having several items, followed by recommendation of top-n outfits or a capsule wardrobe having a collection of outfits based on user's fashion taste.","However, none of these consider user's preference of price-range for individual clothing types or an overall shopping budget for a set of items.","In this paper, we propose a box recommendation framework - BOXREC - which at first, collects user preferences across different item types (namely, top-wear, bottom-wear and foot-wear) including price-range of each type and a maximum shopping budget for a particular shopping session.","It then generates a set of preferred outfits by retrieving all types of preferred items from the database (according to user specified preferences including price-ranges), creates all possible combinations of three preferred items (belonging to distinct item types) and verifies each combination using an outfit scoring framework - BOXREC-OSF.","Finally, it provides a box full of fashion items, such that different combinations of the items maximize the number of outfits suitable for an occasion while satisfying maximum shopping budget.","Empirical results show superior performance of BOXREC-OSF over the baseline methods."],"url":"http://arxiv.org/abs/2402.16660v1","category":"cs.IR"}
{"created":"2024-02-26 15:27:47","title":"Enabling robust sensor network design with data processing and optimization making use of local beehive image and video files","abstract":"There is an immediate need for creative ways to improve resource ef iciency given the dynamic nature of robust sensor networks and their increasing reliance on data-driven approaches.One key challenge faced is ef iciently managing large data files collected from sensor networks for example optimal beehive image and video data files. We of er a revolutionary paradigm that uses cutting-edge edge computing techniques to optimize data transmission and storage in order to meet this problem. Our approach encompasses data compression for images and videos, coupled with a data aggregation technique for numerical data. Specifically, we propose a novel compression algorithm that performs better than the traditional Bzip2, in terms of data compression ratio and throughput. We also designed as an addition a data aggregation algorithm that basically performs very well by reducing on the time to process the overhead of individual data packets there by reducing on the network traf ic. A key aspect of our approach is its ability to operate in resource-constrained environments, such as that typically found in a local beehive farm application from where we obtained various datasets. To achieve this, we carefully explore key parameters such as throughput, delay tolerance, compression rate, and data retransmission. This ensures that our approach can meet the unique requirements of robust network management while minimizing the impact on resources. Overall, our study presents and majorly focuses on a holistic solution for optimizing data transmission and processing across robust sensor networks for specifically local beehive image and video data files. Our approach has the potential to significantly improve the ef iciency and ef ectiveness of robust sensor network management, thereby supporting sustainable practices in various IoT applications such as in Bee Hive Data Management.","sentences":["There is an immediate need for creative ways to improve resource ef iciency given the dynamic nature of robust sensor networks and their increasing reliance on data-driven approaches.","One key challenge faced is ef iciently managing large data files collected from sensor networks for example optimal beehive image and video data files.","We of er a revolutionary paradigm that uses cutting-edge edge computing techniques to optimize data transmission and storage in order to meet this problem.","Our approach encompasses data compression for images and videos, coupled with a data aggregation technique for numerical data.","Specifically, we propose a novel compression algorithm that performs better than the traditional Bzip2, in terms of data compression ratio and throughput.","We also designed as an addition a data aggregation algorithm that basically performs very well by reducing on the time to process the overhead of individual data packets there by reducing on the network traf ic.","A key aspect of our approach is its ability to operate in resource-constrained environments, such as that typically found in a local beehive farm application from where we obtained various datasets.","To achieve this, we carefully explore key parameters such as throughput, delay tolerance, compression rate, and data retransmission.","This ensures that our approach can meet the unique requirements of robust network management while minimizing the impact on resources.","Overall, our study presents and majorly focuses on a holistic solution for optimizing data transmission and processing across robust sensor networks for specifically local beehive image and video data files.","Our approach has the potential to significantly improve the ef iciency and ef ectiveness of robust sensor network management, thereby supporting sustainable practices in various IoT applications such as in Bee Hive Data Management."],"url":"http://arxiv.org/abs/2402.16655v1","category":"cs.NI"}
{"created":"2024-02-26 15:26:56","title":"GigaPevt: Multimodal Medical Assistant","abstract":"Building an intelligent and efficient medical assistant is still a challenging AI problem. The major limitation comes from the data modality scarceness, which reduces comprehensive patient perception. This demo paper presents the GigaPevt, the first multimodal medical assistant that combines the dialog capabilities of large language models with specialized medical models. Such an approach shows immediate advantages in dialog quality and metric performance, with a 1.18\\% accuracy improvement in the question-answering task.","sentences":["Building an intelligent and efficient medical assistant is still a challenging AI problem.","The major limitation comes from the data modality scarceness, which reduces comprehensive patient perception.","This demo paper presents the GigaPevt, the first multimodal medical assistant that combines the dialog capabilities of large language models with specialized medical models.","Such an approach shows immediate advantages in dialog quality and metric performance, with a 1.18\\% accuracy improvement in the question-answering task."],"url":"http://arxiv.org/abs/2402.16654v1","category":"cs.AI"}
{"created":"2024-02-26 15:25:59","title":"Evolution of coronal mass ejections with and without sheaths from the inner to the outer heliosphere -- statistical investigation for 1975-2022","abstract":"This study covers a thorough statistical investigation of the evolution of interplanetary coronal mass ejections (ICMEs) with and without sheaths, through a broad heliocentric distance and temporal range. The analysis treats the sheath and magnetic obstacle (MO) separately to gain more insight about their physical properties. In detail, we aim to unravel different characteristics of these structures occurring over the inner and outer heliosphere. The method is based on a large statistical sample of ICMEs probed over different distances in the heliosphere. For this, information about detection times for sheath and MO from 13 individual ICME catalogs were collected and cross-checked. The time information was then combined into a main catalog used as basis for the statistical investigation. The data analysis based on that covers a wealth of spacecraft missions enabling in-situ solar wind measurements from 1975--2022. This allows to study differences between solar cycles. All the structures under study (sheath, MO with and without sheath) show the biggest increase in size together with the largest decrease in density at a distance 0.75 AU. At 1 AU we find different sizes for MOs with and without sheath, with the former being larger. Up to 1 AU, the upstream solar wind shows the strongest pile-up close to the interface with the sheath. For larger distances the pile-up region seems to shift and recedes from that interface further into the upstream solar wind. This might refer to a change in the sheath formation mechanism (driven versus non-driven) with heliocentric distance, suggesting the relevance of the CME propagation and expansion behavior in the outer heliosphere. Comparison to previous studies shows inconsistencies over the solar cycle, which makes more detailed studies necessary to fully understand the evolution of ICME structures.","sentences":["This study covers a thorough statistical investigation of the evolution of interplanetary coronal mass ejections (ICMEs) with and without sheaths, through a broad heliocentric distance and temporal range.","The analysis treats the sheath and magnetic obstacle (MO) separately to gain more insight about their physical properties.","In detail, we aim to unravel different characteristics of these structures occurring over the inner and outer heliosphere.","The method is based on a large statistical sample of ICMEs probed over different distances in the heliosphere.","For this, information about detection times for sheath and MO from 13 individual ICME catalogs were collected and cross-checked.","The time information was then combined into a main catalog used as basis for the statistical investigation.","The data analysis based on that covers a wealth of spacecraft missions enabling in-situ solar wind measurements from 1975--2022.","This allows to study differences between solar cycles.","All the structures under study (sheath, MO with and without sheath) show the biggest increase in size together with the largest decrease in density at a distance 0.75 AU.","At 1 AU we find different sizes for MOs with and without sheath, with the former being larger.","Up to 1 AU, the upstream solar wind shows the strongest pile-up close to the interface with the sheath.","For larger distances the pile-up region seems to shift and recedes from that interface further into the upstream solar wind.","This might refer to a change in the sheath formation mechanism (driven versus non-driven) with heliocentric distance, suggesting the relevance of the CME propagation and expansion behavior in the outer heliosphere.","Comparison to previous studies shows inconsistencies over the solar cycle, which makes more detailed studies necessary to fully understand the evolution of ICME structures."],"url":"http://arxiv.org/abs/2402.16653v1","category":"astro-ph.SR"}
{"created":"2024-02-26 15:23:27","title":"A Comprehensive Survey of Belief Rule Base (BRB) Hybrid Expert system: Bridging Decision Science and Professional Services","abstract":"The Belief Rule Base (BRB) system that adopts a hybrid approach integrating the precision of expert systems with the adaptability of data-driven models. Characterized by its use of if-then rules to accommodate various types of uncertainty through belief degrees, BRB adeptly handles fuzziness, randomness, and ignorance. This semi-quantitative tool excels in processing both numerical data and linguistic knowledge from diverse sources, making it as an indispensable resource in modelling complex nonlinear systems. Notably, BRB's transparent, white-box nature ensures accessibility and clarity for decision-makers and stakeholders, further enhancing its applicability. With its growing adoption in fields ranging from decision-making and reliability evaluation in network security and fault diagnosis, this study aims to explore the evolution and the multifaceted applications of BRB. By analysing its development across different domains, we highlight BRB's potential to revolutionize sectors traditionally resistant to technological disruption, in particular insurance and law.","sentences":["The Belief Rule Base (BRB) system that adopts a hybrid approach integrating the precision of expert systems with the adaptability of data-driven models.","Characterized by its use of if-then rules to accommodate various types of uncertainty through belief degrees, BRB adeptly handles fuzziness, randomness, and ignorance.","This semi-quantitative tool excels in processing both numerical data and linguistic knowledge from diverse sources, making it as an indispensable resource in modelling complex nonlinear systems.","Notably, BRB's transparent, white-box nature ensures accessibility and clarity for decision-makers and stakeholders, further enhancing its applicability.","With its growing adoption in fields ranging from decision-making and reliability evaluation in network security and fault diagnosis, this study aims to explore the evolution and the multifaceted applications of BRB.","By analysing its development across different domains, we highlight BRB's potential to revolutionize sectors traditionally resistant to technological disruption, in particular insurance and law."],"url":"http://arxiv.org/abs/2402.16651v1","category":"cs.AI"}
{"created":"2024-02-26 15:10:56","title":"Towards Open-ended Visual Quality Comparison","abstract":"Comparative settings (e.g. pairwise choice, listwise ranking) have been adopted by a wide range of subjective studies for image quality assessment (IQA), as it inherently standardizes the evaluation criteria across different observers and offer more clear-cut responses. In this work, we extend the edge of emerging large multi-modality models (LMMs) to further advance visual quality comparison into open-ended settings, that 1) can respond to open-range questions on quality comparison; 2) can provide detailed reasonings beyond direct answers. To this end, we propose the Co-Instruct. To train this first-of-its-kind open-source open-ended visual quality comparer, we collect the Co-Instruct-562K dataset, from two sources: (a) LMM-merged single image quality description, (b) GPT-4V \"teacher\" responses on unlabeled data. Furthermore, to better evaluate this setting, we propose the MICBench, the first benchmark on multi-image comparison for LMMs. We demonstrate that Co-Instruct not only achieves 30% higher superior accuracy than state-of-the-art open-source LMMs, but also outperforms GPT-4V (its teacher), on both existing related benchmarks and the proposed MICBench. Our model is published at https://huggingface.co/q-future/co-instruct.","sentences":["Comparative settings (e.g. pairwise choice, listwise ranking) have been adopted by a wide range of subjective studies for image quality assessment (IQA), as it inherently standardizes the evaluation criteria across different observers and offer more clear-cut responses.","In this work, we extend the edge of emerging large multi-modality models (LMMs) to further advance visual quality comparison into open-ended settings, that 1) can respond to open-range questions on quality comparison; 2) can provide detailed reasonings beyond direct answers.","To this end, we propose the Co-Instruct.","To train this first-of-its-kind open-source open-ended visual quality comparer, we collect the Co-Instruct-562K dataset, from two sources: (a) LMM-merged single image quality description, (b) GPT-4V \"teacher\" responses on unlabeled data.","Furthermore, to better evaluate this setting, we propose the MICBench, the first benchmark on multi-image comparison for LMMs.","We demonstrate that Co-Instruct not only achieves 30% higher superior accuracy than state-of-the-art open-source LMMs, but also outperforms GPT-4V (its teacher), on both existing related benchmarks and the proposed MICBench.","Our model is published at https://huggingface.co/q-future/co-instruct."],"url":"http://arxiv.org/abs/2402.16641v1","category":"cs.CV"}
{"created":"2024-02-26 15:05:16","title":"LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language","abstract":"LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to effectively instruct LLMs poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat fragmented optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structured design template, incurring high learning costs and resulting in low reusability. Inspired by structured reusable programming languages, we propose LangGPT, a dual-layer prompt design framework as the programming language for LLMs. LangGPT has an easy-to-learn normative structure and provides an extended structure for migration and reuse. Experiments illustrate that LangGPT significantly enhances the capacity of LLMs to produce responses of superior quality compared to baselines. Moreover, LangGPT has proven effective in guiding LLMs to generate high-quality prompts. We have built a community on LangGPT to facilitate the tuition and sharing of prompt design. We also analyzed the ease of use and reusability of LangGPT through a community user survey.","sentences":["LLMs have demonstrated commendable performance across diverse domains.","Nevertheless, formulating high-quality prompts to effectively instruct LLMs poses a challenge for non-AI experts.","Existing research in prompt engineering suggests somewhat fragmented optimization principles and designs empirically dependent prompt optimizers.","Unfortunately, these endeavors lack a structured design template, incurring high learning costs and resulting in low reusability.","Inspired by structured reusable programming languages, we propose LangGPT, a dual-layer prompt design framework as the programming language for LLMs.","LangGPT has an easy-to-learn normative structure and provides an extended structure for migration and reuse.","Experiments illustrate that LangGPT significantly enhances the capacity of LLMs to produce responses of superior quality compared to baselines.","Moreover, LangGPT has proven effective in guiding LLMs to generate high-quality prompts.","We have built a community on LangGPT to facilitate the tuition and sharing of prompt design.","We also analyzed the ease of use and reusability of LangGPT through a community user survey."],"url":"http://arxiv.org/abs/2402.16929v1","category":"cs.SE"}
{"created":"2024-02-26 15:04:35","title":"Domain Embeddings for Generating Complex Descriptions of Concepts in Italian Language","abstract":"In this work, we propose a Distributional Semantic resource enriched with linguistic and lexical information extracted from electronic dictionaries, designed to address the challenge of bridging the gap between the continuous semantic values represented by distributional vectors and the discrete descriptions offered by general semantics theory. Recently, many researchers have concentrated on the nexus between embeddings and a comprehensive theory of semantics and meaning. This often involves decoding the representation of word meanings in Distributional Models into a set of discrete, manually constructed properties such as semantic primitives or features, using neural decoding techniques. Our approach introduces an alternative strategy grounded in linguistic data. We have developed a collection of domain-specific co-occurrence matrices, derived from two sources: a classification of Italian nouns categorized into 4 semantic traits and 20 concrete noun sub-categories, and a list of Italian verbs classified according to their semantic classes. In these matrices, the co-occurrence values for each word are calculated exclusively with a defined set of words pertinent to a particular lexical domain. The resource comprises 21 domain-specific matrices, one comprehensive matrix, and a Graphical User Interface. Our model facilitates the generation of reasoned semantic descriptions of concepts by selecting matrices directly associated with concrete conceptual knowledge, such as a matrix based on location nouns and the concept of animal habitats. We assessed the utility of the resource through two experiments, achieving promising outcomes in both: the automatic classification of animal nouns and the extraction of animal features.","sentences":["In this work, we propose a Distributional Semantic resource enriched with linguistic and lexical information extracted from electronic dictionaries, designed to address the challenge of bridging the gap between the continuous semantic values represented by distributional vectors and the discrete descriptions offered by general semantics theory.","Recently, many researchers have concentrated on the nexus between embeddings and a comprehensive theory of semantics and meaning.","This often involves decoding the representation of word meanings in Distributional Models into a set of discrete, manually constructed properties such as semantic primitives or features, using neural decoding techniques.","Our approach introduces an alternative strategy grounded in linguistic data.","We have developed a collection of domain-specific co-occurrence matrices, derived from two sources: a classification of Italian nouns categorized into 4 semantic traits and 20 concrete noun sub-categories, and a list of Italian verbs classified according to their semantic classes.","In these matrices, the co-occurrence values for each word are calculated exclusively with a defined set of words pertinent to a particular lexical domain.","The resource comprises 21 domain-specific matrices, one comprehensive matrix, and a Graphical User Interface.","Our model facilitates the generation of reasoned semantic descriptions of concepts by selecting matrices directly associated with concrete conceptual knowledge, such as a matrix based on location nouns and the concept of animal habitats.","We assessed the utility of the resource through two experiments, achieving promising outcomes in both: the automatic classification of animal nouns and the extraction of animal features."],"url":"http://arxiv.org/abs/2402.16632v1","category":"cs.CL"}
{"created":"2024-02-26 15:03:46","title":"GenAINet: Enabling Wireless Collective Intelligence via Knowledge Transfer and Reasoning","abstract":"Generative artificial intelligence (GenAI) and communication networks are expected to have groundbreaking synergies in 6G. Connecting GenAI agents over a wireless network can potentially unleash the power of collective intelligence and pave the way for artificial general intelligence (AGI). However, current wireless networks are designed as a \"data pipe\" and are not suited to accommodate and leverage the power of GenAI. In this paper, we propose the GenAINet framework in which distributed GenAI agents communicate knowledge (high-level concepts or abstracts) to accomplish arbitrary tasks. We first provide a network architecture integrating GenAI capabilities to manage both network protocols and applications. Building on this, we investigate effective communication and reasoning problems by proposing a semantic-native GenAINet. Specifically, GenAI agents extract semantic concepts from multi-modal raw data, build a knowledgebase representing their semantic relations, which is retrieved by GenAI models for planning and reasoning. Under this paradigm, an agent can learn fast from other agents' experience for making better decisions with efficient communications. Furthermore, we conduct two case studies where in wireless device query, we show that extracting and transferring knowledge can improve query accuracy with reduced communication; and in wireless power control, we show that distributed agents can improve decisions via collaborative reasoning. Finally, we address that developing a hierarchical semantic level Telecom world model is a key path towards network of collective intelligence.","sentences":["Generative artificial intelligence (GenAI) and communication networks are expected to have groundbreaking synergies in 6G. Connecting GenAI agents over a wireless network can potentially unleash the power of collective intelligence and pave the way for artificial general intelligence (AGI).","However, current wireless networks are designed as a \"data pipe\" and are not suited to accommodate and leverage the power of GenAI.","In this paper, we propose the GenAINet framework in which distributed GenAI agents communicate knowledge (high-level concepts or abstracts) to accomplish arbitrary tasks.","We first provide a network architecture integrating GenAI capabilities to manage both network protocols and applications.","Building on this, we investigate effective communication and reasoning problems by proposing a semantic-native GenAINet.","Specifically, GenAI agents extract semantic concepts from multi-modal raw data, build a knowledgebase representing their semantic relations, which is retrieved by GenAI models for planning and reasoning.","Under this paradigm, an agent can learn fast from other agents' experience for making better decisions with efficient communications.","Furthermore, we conduct two case studies where in wireless device query, we show that extracting and transferring knowledge can improve query accuracy with reduced communication; and in wireless power control, we show that distributed agents can improve decisions via collaborative reasoning.","Finally, we address that developing a hierarchical semantic level Telecom world model is a key path towards network of collective intelligence."],"url":"http://arxiv.org/abs/2402.16631v1","category":"cs.AI"}
{"created":"2024-02-26 15:01:54","title":"Single Neuromorphic Memristor closely Emulates Multiple Synaptic Mechanisms for Energy Efficient Neural Networks","abstract":"Biological neural networks do not only include long-term memory and weight multiplication capabilities, as commonly assumed in artificial neural networks, but also more complex functions such as short-term memory, short-term plasticity, and meta-plasticity - all collocated within each synapse. Here, we demonstrate memristive nano-devices based on SrTiO3 that inherently emulate all these synaptic functions. These memristors operate in a non-filamentary, low conductance regime, which enables stable and energy efficient operation. They can act as multi-functional hardware synapses in a class of bio-inspired deep neural networks (DNN) that make use of both long- and short-term synaptic dynamics and are capable of meta-learning or \"learning-to-learn\". The resulting bio-inspired DNN is then trained to play the video game Atari Pong, a complex reinforcement learning task in a dynamic environment. Our analysis shows that the energy consumption of the DNN with multi-functional memristive synapses decreases by about two orders of magnitude as compared to a pure GPU implementation. Based on this finding, we infer that memristive devices with a better emulation of the synaptic functionalities do not only broaden the applicability of neuromorphic computing, but could also improve the performance and energy costs of certain artificial intelligence applications.","sentences":["Biological neural networks do not only include long-term memory and weight multiplication capabilities, as commonly assumed in artificial neural networks, but also more complex functions such as short-term memory, short-term plasticity, and meta-plasticity - all collocated within each synapse.","Here, we demonstrate memristive nano-devices based on SrTiO3 that inherently emulate all these synaptic functions.","These memristors operate in a non-filamentary, low conductance regime, which enables stable and energy efficient operation.","They can act as multi-functional hardware synapses in a class of bio-inspired deep neural networks (DNN) that make use of both long- and short-term synaptic dynamics and are capable of meta-learning or \"learning-to-learn\".","The resulting bio-inspired DNN is then trained to play the video game Atari Pong, a complex reinforcement learning task in a dynamic environment.","Our analysis shows that the energy consumption of the DNN with multi-functional memristive synapses decreases by about two orders of magnitude as compared to a pure GPU implementation.","Based on this finding, we infer that memristive devices with a better emulation of the synaptic functionalities do not only broaden the applicability of neuromorphic computing, but could also improve the performance and energy costs of certain artificial intelligence applications."],"url":"http://arxiv.org/abs/2402.16628v1","category":"cs.NE"}
{"created":"2024-02-26 13:49:52","title":"CLAP: Learning Transferable Binary Code Representations with Natural Language Supervision","abstract":"Binary code representation learning has shown significant performance in binary analysis tasks. But existing solutions often have poor transferability, particularly in few-shot and zero-shot scenarios where few or no training samples are available for the tasks. To address this problem, we present CLAP (Contrastive Language-Assembly Pre-training), which employs natural language supervision to learn better representations of binary code (i.e., assembly code) and get better transferability. At the core, our approach boosts superior transfer learning capabilities by effectively aligning binary code with their semantics explanations (in natural language), resulting a model able to generate better embeddings for binary code. To enable this alignment training, we then propose an efficient dataset engine that could automatically generate a large and diverse dataset comprising of binary code and corresponding natural language explanations. We have generated 195 million pairs of binary code and explanations and trained a prototype of CLAP. The evaluations of CLAP across various downstream tasks in binary analysis all demonstrate exceptional performance. Notably, without any task-specific training, CLAP is often competitive with a fully supervised baseline, showing excellent transferability. We release our pre-trained model and code at https://github.com/Hustcw/CLAP.","sentences":["Binary code representation learning has shown significant performance in binary analysis tasks.","But existing solutions often have poor transferability, particularly in few-shot and zero-shot scenarios where few or no training samples are available for the tasks.","To address this problem, we present CLAP (Contrastive Language-Assembly Pre-training), which employs natural language supervision to learn better representations of binary code (i.e., assembly code) and get better transferability.","At the core, our approach boosts superior transfer learning capabilities by effectively aligning binary code with their semantics explanations (in natural language), resulting a model able to generate better embeddings for binary code.","To enable this alignment training, we then propose an efficient dataset engine that could automatically generate a large and diverse dataset comprising of binary code and corresponding natural language explanations.","We have generated 195 million pairs of binary code and explanations and trained a prototype of CLAP.","The evaluations of CLAP across various downstream tasks in binary analysis all demonstrate exceptional performance.","Notably, without any task-specific training, CLAP is often competitive with a fully supervised baseline, showing excellent transferability.","We release our pre-trained model and code at https://github.com/Hustcw/CLAP."],"url":"http://arxiv.org/abs/2402.16928v1","category":"cs.SE"}
{"created":"2024-02-26 13:46:51","title":"Aligning Large Language Models to a Domain-specific Graph Database","abstract":"Graph Databases (Graph DB) are widely applied in various fields, including finance, social networks, and medicine. However, translating Natural Language (NL) into the Graph Query Language (GQL), commonly known as NL2GQL, proves to be challenging due to its inherent complexity and specialized nature. Some approaches have sought to utilize Large Language Models (LLMs) to address analogous tasks like text2SQL. Nevertheless, when it comes to NL2GQL taskson a particular domain, the absence of domain-specific NL-GQL data pairs makes it difficult to establish alignment between LLMs and the graph DB. To address this challenge, we propose a well-defined pipeline. Specifically, we utilize ChatGPT to create NL-GQL data pairs based on the given graph DB with self-instruct. Then, we use the created data to fine-tune LLMs, thereby achieving alignment between LLMs and the graph DB. Additionally, during inference, we propose a method that extracts relevant schema to the queried NL as the input context to guide LLMs for generating accurate GQLs.We evaluate our method on two constructed datasets deriving from graph DBs in finance domain and medicine domain, namely FinGQL and MediGQL. Experimental results demonstrate that our method significantly outperforms a set of baseline methods, with improvements of 5.90 and 6.36 absolute points on EM, and 6.00 and 7.09 absolute points on EX, respectively.","sentences":["Graph Databases (Graph DB) are widely applied in various fields, including finance, social networks, and medicine.","However, translating Natural Language (NL) into the Graph Query Language (GQL), commonly known as NL2GQL, proves to be challenging due to its inherent complexity and specialized nature.","Some approaches have sought to utilize Large Language Models (LLMs) to address analogous tasks like text2SQL.","Nevertheless, when it comes to NL2GQL taskson a particular domain, the absence of domain-specific NL-GQL data pairs makes it difficult to establish alignment between LLMs and the graph DB.","To address this challenge, we propose a well-defined pipeline.","Specifically, we utilize ChatGPT to create NL-GQL data pairs based on the given graph DB with self-instruct.","Then, we use the created data to fine-tune LLMs, thereby achieving alignment between LLMs and the graph DB.","Additionally, during inference, we propose a method that extracts relevant schema to the queried NL as the input context to guide LLMs for generating accurate GQLs.","We evaluate our method on two constructed datasets deriving from graph DBs in finance domain and medicine domain, namely FinGQL and MediGQL.","Experimental results demonstrate that our method significantly outperforms a set of baseline methods, with improvements of 5.90 and 6.36 absolute points on EM, and 6.00 and 7.09 absolute points on EX, respectively."],"url":"http://arxiv.org/abs/2402.16567v1","category":"cs.CL"}
{"created":"2024-02-26 13:43:43","title":"Quick unsupervised hyperspectral dimensionality reduction for earth observation: a comparison","abstract":"Dimensionality reduction can be applied to hyperspectral images so that the most useful data can be extracted and processed more quickly. This is critical in any situation in which data volume exceeds the capacity of the computational resources, particularly in the case of remote sensing platforms (e.g., drones, satellites), but also in the case of multi-year datasets. Moreover, the computational strategies of unsupervised dimensionality reduction often provide the basis for more complicated supervised techniques. Seven unsupervised dimensionality reduction algorithms are tested on hyperspectral data from the HYPSO-1 earth observation satellite. Each particular algorithm is chosen to be representative of a broader collection. The experiments probe the computational complexity, reconstruction accuracy, signal clarity, sensitivity to artifacts, and effects on target detection and classification of the different algorithms. No algorithm consistently outperformed the others across all tests, but some general trends regarding the characteristics of the algorithms did emerge. With half a million pixels, computational time requirements of the methods varied by 5 orders of magnitude, and the reconstruction error varied by about 3 orders of magnitude. A relationship between mutual information and artifact susceptibility was suggested by the tests. The relative performance of the algorithms differed significantly between the target detection and classification tests. Overall, these experiments both show the power of dimensionality reduction and give guidance regarding how to evaluate a technique prior to incorporating it into a processing pipeline.","sentences":["Dimensionality reduction can be applied to hyperspectral images so that the most useful data can be extracted and processed more quickly.","This is critical in any situation in which data volume exceeds the capacity of the computational resources, particularly in the case of remote sensing platforms (e.g., drones, satellites), but also in the case of multi-year datasets.","Moreover, the computational strategies of unsupervised dimensionality reduction often provide the basis for more complicated supervised techniques.","Seven unsupervised dimensionality reduction algorithms are tested on hyperspectral data from the HYPSO-1 earth observation satellite.","Each particular algorithm is chosen to be representative of a broader collection.","The experiments probe the computational complexity, reconstruction accuracy, signal clarity, sensitivity to artifacts, and effects on target detection and classification of the different algorithms.","No algorithm consistently outperformed the others across all tests, but some general trends regarding the characteristics of the algorithms did emerge.","With half a million pixels, computational time requirements of the methods varied by 5 orders of magnitude, and the reconstruction error varied by about 3 orders of magnitude.","A relationship between mutual information and artifact susceptibility was suggested by the tests.","The relative performance of the algorithms differed significantly between the target detection and classification tests.","Overall, these experiments both show the power of dimensionality reduction and give guidance regarding how to evaluate a technique prior to incorporating it into a processing pipeline."],"url":"http://arxiv.org/abs/2402.16566v1","category":"eess.IV"}
{"created":"2024-02-26 13:39:04","title":"Q-FOX Learning: Breaking Tradition in Reinforcement Learning","abstract":"Reinforcement learning (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision. Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms. Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards. In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes' hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning. Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (steps). Q-FOX has been evaluated on two OpenAI Gym environment control tasks: Cart Pole and Frozen Lake. It exposed greater cumulative rewards than HP tuning with other optimizers, such as PSO, GA, Bee, or randomly selected HP. The cumulative reward for the Cart Pole task was 32.08, and for the Frozen Lake task was 0.95. Despite the robustness of Q-FOX, it has limitations. It cannot be used directly in real-word problems before choosing the HP in a simulation environment because its processes work iteratively, making it time-consuming. The results indicate that Q-FOX has played an essential role in HP tuning for RL algorithms to effectively solve different control tasks.","sentences":["Reinforcement learning (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision.","Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms.","Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards.","In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed.","This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes' hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning.","Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (steps).","Q-FOX has been evaluated on two OpenAI Gym environment control tasks: Cart Pole and Frozen Lake.","It exposed greater cumulative rewards than HP tuning with other optimizers, such as PSO, GA, Bee, or randomly selected HP.","The cumulative reward for the Cart Pole task was 32.08, and for the Frozen Lake task was 0.95.","Despite the robustness of Q-FOX, it has limitations.","It cannot be used directly in real-word problems before choosing the HP in a simulation environment because its processes work iteratively, making it time-consuming.","The results indicate that Q-FOX has played an essential role in HP tuning for RL algorithms to effectively solve different control tasks."],"url":"http://arxiv.org/abs/2402.16562v1","category":"cs.LG"}
{"created":"2024-02-26 13:34:57","title":"Open Your Ears to Take a Look: A State-of-the-Art Report on the Integration of Sonification and Visualization","abstract":"The research communities studying visualization and sonification for data display and analysis share exceptionally similar goals, essentially making data of any kind interpretable to humans. One community does so by using visual representations of data, the other community does so by employing auditory (non-speech) representations of data. While the two communities have a lot in common, they developed mostly in parallel over the course of the last few decades. With this STAR, we discuss a collection of work that bridges the borders of the two communities, hence a collection of work that aims to integrate the two techniques to one form of audiovisual display, which we argue to be \"more than the sum of the two.\" We introduce and motivate a classification system applicable to such audiovisual displays and categorize a corpus of 57 academic publications that appeared between 2011 and 2023 in categories such as reading level, dataset type, or evaluation system, to mention a few. The corpus also enables a meta-analysis of the field, including regularly occurring design patterns such as type of visualization and sonification techniques, or the use of visual and auditory channels, and the analysis of a co-author network of the field which shows individual teams without much interconnection. The body of work covered in this STAR also relates to three adjacent topics: audiovisual monitoring, accessibility, and audiovisual data art. These three topics are discussed individually in addition to the systematically conducted part of this research. The findings of this report may be used by researchers from both fields to understand the potentials and challenges of such integrated designs, while inspiring them for future collaboration with experts from the respective other field.","sentences":["The research communities studying visualization and sonification for data display and analysis share exceptionally similar goals, essentially making data of any kind interpretable to humans.","One community does so by using visual representations of data, the other community does so by employing auditory (non-speech) representations of data.","While the two communities have a lot in common, they developed mostly in parallel over the course of the last few decades.","With this STAR, we discuss a collection of work that bridges the borders of the two communities, hence a collection of work that aims to integrate the two techniques to one form of audiovisual display, which we argue to be \"more than the sum of the two.\"","We introduce and motivate a classification system applicable to such audiovisual displays and categorize a corpus of 57 academic publications that appeared between 2011 and 2023 in categories such as reading level, dataset type, or evaluation system, to mention a few.","The corpus also enables a meta-analysis of the field, including regularly occurring design patterns such as type of visualization and sonification techniques, or the use of visual and auditory channels, and the analysis of a co-author network of the field which shows individual teams without much interconnection.","The body of work covered in this STAR also relates to three adjacent topics: audiovisual monitoring, accessibility, and audiovisual data art.","These three topics are discussed individually in addition to the systematically conducted part of this research.","The findings of this report may be used by researchers from both fields to understand the potentials and challenges of such integrated designs, while inspiring them for future collaboration with experts from the respective other field."],"url":"http://arxiv.org/abs/2402.16558v1","category":"cs.HC"}
{"created":"2024-02-27 18:59:29","title":"The Strominger System and Flows by the Ricci Tensor","abstract":"This is a survey on the Strominger system and a geometric flow known as the anomaly flow. We will discuss various aspects of non-K\\\"ahler geometry on Calabi-Yau threefolds. Along the way, we discuss balanced metrics and balanced classes, the Aeppli cohomology class associated to a solution to the Strominger system, the equations of motion of heterotic supergravity, and a version of Ricci flow in this special geometry.","sentences":["This is a survey on the Strominger system and a geometric flow known as the anomaly flow.","We will discuss various aspects of non-K\\\"ahler geometry on Calabi-Yau threefolds.","Along the way, we discuss balanced metrics and balanced classes, the Aeppli cohomology class associated to a solution to the Strominger system, the equations of motion of heterotic supergravity, and a version of Ricci flow in this special geometry."],"url":"http://arxiv.org/abs/2402.17770v1","category":"math.DG"}
{"created":"2024-02-27 18:59:27","title":"Factors that Affect Personalization of Robots for Older Adults","abstract":"We introduce a taxonomy of important factors to consider when designing interactions with an assistive robot in a senior living facility. These factors are derived from our reflection on two field studies and are grouped into the following high-level categories: primary user (residents), care partners, robot, facility and external circumstances. We outline how multiple factors in these categories impact different aspects of personalization, such as adjusting interactions based on the unique needs of a resident or modifying alerts about the robot's status for different care partners. This preliminary taxonomy serves as a framework for considering how to deploy personalized assistive robots in the complex caregiving ecosystem.","sentences":["We introduce a taxonomy of important factors to consider when designing interactions with an assistive robot in a senior living facility.","These factors are derived from our reflection on two field studies and are grouped into the following high-level categories: primary user (residents), care partners, robot, facility and external circumstances.","We outline how multiple factors in these categories impact different aspects of personalization, such as adjusting interactions based on the unique needs of a resident or modifying alerts about the robot's status for different care partners.","This preliminary taxonomy serves as a framework for considering how to deploy personalized assistive robots in the complex caregiving ecosystem."],"url":"http://arxiv.org/abs/2402.17769v1","category":"cs.RO"}
{"created":"2024-02-27 18:56:50","title":"On an abrasion motivated fractal model","abstract":"In this paper, we consider a fractal model motivated by the abrasion of convex polyhedra, where the abrasion is realised by chipping small neighbourhoods of vertices. We study the upper box-counting dimension of the limiting object after infinitely many chipping.","sentences":["In this paper, we consider a fractal model motivated by the abrasion of convex polyhedra, where the abrasion is realised by chipping small neighbourhoods of vertices.","We study the upper box-counting dimension of the limiting object after infinitely many chipping."],"url":"http://arxiv.org/abs/2402.17765v1","category":"math.DS"}
{"created":"2024-02-27 18:38:05","title":"An Eye Gaze Heatmap Analysis of Uncertainty Head-Up Display Designs for Conditional Automated Driving","abstract":"This paper reports results from a high-fidelity driving simulator study (N=215) about a head-up display (HUD) that conveys a conditional automated vehicle's dynamic \"uncertainty\" about the current situation while fallback drivers watch entertaining videos. We compared (between-group) three design interventions: display (a bar visualisation of uncertainty close to the video), interruption (interrupting the video during uncertain situations), and combination (a combination of both), against a baseline (video-only). We visualised eye-tracking data to conduct a heatmap analysis of the four groups' gaze behaviour over time. We found interruptions initiated a phase during which participants interleaved their attention between monitoring and entertainment. This improved monitoring behaviour was more pronounced in combination compared to interruption, suggesting pre-warning interruptions have positive effects. The same addition had negative effects without interruptions (comparing baseline & display). Intermittent interruptions may have safety benefits over placing additional peripheral displays without compromising usability.","sentences":["This paper reports results from a high-fidelity driving simulator study (N=215) about a head-up display (HUD) that conveys a conditional automated vehicle's dynamic \"uncertainty\" about the current situation while fallback drivers watch entertaining videos.","We compared (between-group) three design interventions: display (a bar visualisation of uncertainty close to the video), interruption (interrupting the video during uncertain situations), and combination (a combination of both), against a baseline (video-only).","We visualised eye-tracking data to conduct a heatmap analysis of the four groups' gaze behaviour over time.","We found interruptions initiated a phase during which participants interleaved their attention between monitoring and entertainment.","This improved monitoring behaviour was more pronounced in combination compared to interruption, suggesting pre-warning interruptions have positive effects.","The same addition had negative effects without interruptions (comparing baseline & display).","Intermittent interruptions may have safety benefits over placing additional peripheral displays without compromising usability."],"url":"http://arxiv.org/abs/2402.17751v1","category":"cs.HC"}
{"created":"2024-02-27 18:37:22","title":"Scaling on-chip photonic neural processors using arbitrarily programmable wave propagation","abstract":"On-chip photonic processors for neural networks have potential benefits in both speed and energy efficiency but have not yet reached the scale at which they can outperform electronic processors. The dominant paradigm for designing on-chip photonics is to make networks of relatively bulky discrete components connected by one-dimensional waveguides. A far more compact alternative is to avoid explicitly defining any components and instead sculpt the continuous substrate of the photonic processor to directly perform the computation using waves freely propagating in two dimensions. We propose and demonstrate a device whose refractive index as a function of space, $n(x,z)$, can be rapidly reprogrammed, allowing arbitrary control over the wave propagation in the device. Our device, a 2D-programmable waveguide, combines photoconductive gain with the electro-optic effect to achieve massively parallel modulation of the refractive index of a slab waveguide, with an index modulation depth of $10^{-3}$ and approximately $10^4$ programmable degrees of freedom. We used a prototype device with a functional area of $12\\,\\text{mm}^2$ to perform neural-network inference with up to 49-dimensional input vectors in a single pass, achieving 96% accuracy on vowel classification and 86% accuracy on $7 \\times 7$-pixel MNIST handwritten-digit classification. This is a scale beyond that of previous photonic chips relying on discrete components, illustrating the benefit of the continuous-waves paradigm. In principle, with large enough chip area, the reprogrammability of the device's refractive index distribution enables the reconfigurable realization of any passive, linear photonic circuit or device. This promises the development of more compact and versatile photonic systems for a wide range of applications, including optical processing, smart sensing, spectroscopy, and optical communications.","sentences":["On-chip photonic processors for neural networks have potential benefits in both speed and energy efficiency but have not yet reached the scale at which they can outperform electronic processors.","The dominant paradigm for designing on-chip photonics is to make networks of relatively bulky discrete components connected by one-dimensional waveguides.","A far more compact alternative is to avoid explicitly defining any components and instead sculpt the continuous substrate of the photonic processor to directly perform the computation using waves freely propagating in two dimensions.","We propose and demonstrate a device whose refractive index as a function of space, $n(x,z)$, can be rapidly reprogrammed, allowing arbitrary control over the wave propagation in the device.","Our device, a 2D-programmable waveguide, combines photoconductive gain with the electro-optic effect to achieve massively parallel modulation of the refractive index of a slab waveguide, with an index modulation depth of $10^{-3}$ and approximately $10^4$ programmable degrees of freedom.","We used a prototype device with a functional area of $12\\,\\text{mm}^2$ to perform neural-network inference with up to 49-dimensional input vectors in a single pass, achieving 96% accuracy on vowel classification and 86% accuracy on $7 \\times 7$-pixel MNIST handwritten-digit classification.","This is a scale beyond that of previous photonic chips relying on discrete components, illustrating the benefit of the continuous-waves paradigm.","In principle, with large enough chip area, the reprogrammability of the device's refractive index distribution enables the reconfigurable realization of any passive, linear photonic circuit or device.","This promises the development of more compact and versatile photonic systems for a wide range of applications, including optical processing, smart sensing, spectroscopy, and optical communications."],"url":"http://arxiv.org/abs/2402.17750v1","category":"physics.optics"}
{"created":"2024-02-27 18:22:48","title":"Rose: Efficient and Extensible Autodiff on the Web","abstract":"Automatic differentiation (AD) has become the backbone for a new wave of optimization-driven domains such as computer graphics and machine learning over the past decade. However, existing AD systems face limitations, either lacking support for in-browser development or failing to harness more recent, compilerbased approaches to achieve both expressiveness and size-preserving differentiation. This work introduces Rose, a portable, extensible AD library that runs on the web. Rose allows users to write opaque functions with custom derivatives and supports dynamic construction of AD functions. We integrated Rose into two differentiable simulations and a diagram authoring tool to demonstrate the utility of Rose's design. Finally, we show that Rose is 173x as fast as TensorFlow.js in compiling and running a benchmark suite of optimized diagrams.","sentences":["Automatic differentiation (AD) has become the backbone for a new wave of optimization-driven domains such as computer graphics and machine learning over the past decade.","However, existing AD systems face limitations, either lacking support for in-browser development or failing to harness more recent, compilerbased approaches to achieve both expressiveness and size-preserving differentiation.","This work introduces Rose, a portable, extensible AD library that runs on the web.","Rose allows users to write opaque functions with custom derivatives and supports dynamic construction of AD functions.","We integrated Rose into two differentiable simulations and a diagram authoring tool to demonstrate the utility of Rose's design.","Finally, we show that Rose is 173x as fast as TensorFlow.js in compiling and running a benchmark suite of optimized diagrams."],"url":"http://arxiv.org/abs/2402.17743v1","category":"cs.PL"}
{"created":"2024-02-27 18:14:39","title":"Decompositions of hyperbolic Kac-Moody algebras with respect to imaginary root groups","abstract":"We propose a novel way to define imaginary root subgroups associated with (timelike) imaginary roots of hyperbolic Kac-Moody algebras. Using in an essential way the theory of unitary irreducible representation of covers of the group SO(2,1), these imaginary root subgroups act on the complex Kac-Moody algebra viewed as a Hilbert space. We illustrate our new view on Kac-Moody groups by considering the example of a rank-two hyperbolic algebra that is related to the Fibonacci numbers. We also point out some open issues and new avenues for further research, and briefly discuss the potential relevance of the present results for physics and current attempts at unification.","sentences":["We propose a novel way to define imaginary root subgroups associated with (timelike) imaginary roots of hyperbolic Kac-Moody algebras.","Using in an essential way the theory of unitary irreducible representation of covers of the group SO(2,1), these imaginary root subgroups act on the complex Kac-Moody algebra viewed as a Hilbert space.","We illustrate our new view on Kac-Moody groups by considering the example of a rank-two hyperbolic algebra that is related to the Fibonacci numbers.","We also point out some open issues and new avenues for further research, and briefly discuss the potential relevance of the present results for physics and current attempts at unification."],"url":"http://arxiv.org/abs/2402.17737v1","category":"math.RT"}
{"created":"2024-02-27 17:48:56","title":"An all-frequency stable surface integral equation algorithm for electromagnetism in 3-D unbounded penetrable media: Continuous and fully-discrete model analysis","abstract":"We use the time-harmonic Maxwell partial differential equations (PDEs) to model the wave propagation in 3-D space, which comprises a closed penetrable scatterer and its unbounded free-space complement. Surface integral equations (SIEs) that are equivalent to the time-harmonic Maxwell PDEs provide an efficient framework to directly model the surface electromagnetic fields and hence the RCS.The equivalent SIE system on the interface has the advantages that: (a) it avoids truncation of the unbounded region and the solution exactly satisfies the radiation condition; and (b) the surface-fields solution yields the unknowns in the Maxwell PDEs through surface potential representations of the interior and exterior fields. The Maxwell PDE system has been proven (several decades ago) to be stable for all frequencies, that is, (i) it does not possess eigenfrequencies (it is well-posed); and (ii) it does not suffer from low-frequency. However, weakly-singular SIE reformulations of the PDE satisfying these two properties, subject to a stabilization constraint, were derived and mathematically proven only about a decade ago (see {J. Math. Anal. Appl. 412 (2014) 277-300}). The aim of this article is two-fold: (I) To effect a robust coupling of the stabilization constraint to the weakly singular SIE and use mathematical analysis to establish that the resulting continuous weakly-singular second-kind self-adjoint SIE system (without constraints) retains all-frequency stability; and (II) To apply a fully-discrete spectral algorithm for the all-frequency-stable weakly-singular second-kind SIE, and prove spectral accuracy of the algorithm. We numerically demonstrate the high-order accuracy of the algorithm using several dielectric and absorbing benchmark scatterers with curved surfaces.","sentences":["We use the time-harmonic Maxwell partial differential equations (PDEs) to model the wave propagation in 3-D space, which comprises a closed penetrable scatterer and its unbounded free-space complement.","Surface integral equations (SIEs) that are equivalent to the time-harmonic Maxwell PDEs provide an efficient framework to directly model the surface electromagnetic fields and hence the RCS.The equivalent SIE system on the interface has the advantages that: (a) it avoids truncation of the unbounded region and the solution exactly satisfies the radiation condition; and (b) the surface-fields solution yields the unknowns in the Maxwell PDEs through surface potential representations of the interior and exterior fields.","The Maxwell PDE system has been proven (several decades ago) to be stable for all frequencies, that is, (i) it does not possess eigenfrequencies (it is well-posed); and (ii) it does not suffer from low-frequency.","However, weakly-singular SIE reformulations of the PDE satisfying these two properties, subject to a stabilization constraint, were derived and mathematically proven only about a decade ago (see {J. Math.","Anal.","Appl.","412 (2014) 277-300}).","The aim of this article is two-fold: (I) To effect a robust coupling of the stabilization constraint to the weakly singular SIE and use mathematical analysis to establish that the resulting continuous weakly-singular second-kind self-adjoint SIE system (without constraints) retains all-frequency stability; and (II) To apply a fully-discrete spectral algorithm for the all-frequency-stable weakly-singular second-kind SIE, and prove spectral accuracy of the algorithm.","We numerically demonstrate the high-order accuracy of the algorithm using several dielectric and absorbing benchmark scatterers with curved surfaces."],"url":"http://arxiv.org/abs/2402.17713v1","category":"math.NA"}
{"created":"2024-02-27 17:28:46","title":"Model Free Deep Deterministic Policy Gradient Controller for Setpoint Tracking of Non-minimum Phase Systems","abstract":"Deep Reinforcement Learning (DRL) techniques have received significant attention in control and decision-making algorithms. Most applications involve complex decision-making systems, justified by the algorithms' computational power and cost. While model-based versions are emerging, model-free DRL approaches are intriguing for their independence from models, yet they remain relatively less explored in terms of performance, particularly in applied control. This study conducts a thorough performance analysis comparing the data-driven DRL paradigm with a classical state feedback controller, both designed based on the same cost (reward) function of the linear quadratic regulator (LQR) problem. Twelve additional performance criteria are introduced to assess the controllers' performance, independent of the LQR problem for which they are designed. Two Deep Deterministic Policy Gradient (DDPG)-based controllers are developed, leveraging DDPG's widespread reputation. These controllers are aimed at addressing a challenging setpoint tracking problem in a Non-Minimum Phase (NMP) system. The performance and robustness of the controllers are assessed in the presence of operational challenges, including disturbance, noise, initial conditions, and model uncertainties. The findings suggest that the DDPG controller demonstrates promising behavior under rigorous test conditions. Nevertheless, further improvements are necessary for the DDPG controller to outperform classical methods in all criteria. While DRL algorithms may excel in complex environments owing to the flexibility in the reward function definition, this paper offers practical insights and a comparison framework specifically designed to evaluate these algorithms within the context of control engineering.","sentences":["Deep Reinforcement Learning (DRL) techniques have received significant attention in control and decision-making algorithms.","Most applications involve complex decision-making systems, justified by the algorithms' computational power and cost.","While model-based versions are emerging, model-free DRL approaches are intriguing for their independence from models, yet they remain relatively less explored in terms of performance, particularly in applied control.","This study conducts a thorough performance analysis comparing the data-driven DRL paradigm with a classical state feedback controller, both designed based on the same cost (reward) function of the linear quadratic regulator (LQR) problem.","Twelve additional performance criteria are introduced to assess the controllers' performance, independent of the LQR problem for which they are designed.","Two Deep Deterministic Policy Gradient (DDPG)-based controllers are developed, leveraging DDPG's widespread reputation.","These controllers are aimed at addressing a challenging setpoint tracking problem in a Non-Minimum Phase (NMP) system.","The performance and robustness of the controllers are assessed in the presence of operational challenges, including disturbance, noise, initial conditions, and model uncertainties.","The findings suggest that the DDPG controller demonstrates promising behavior under rigorous test conditions.","Nevertheless, further improvements are necessary for the DDPG controller to outperform classical methods in all criteria.","While DRL algorithms may excel in complex environments owing to the flexibility in the reward function definition, this paper offers practical insights and a comparison framework specifically designed to evaluate these algorithms within the context of control engineering."],"url":"http://arxiv.org/abs/2402.17703v1","category":"eess.SY"}
{"created":"2024-02-27 17:21:10","title":"Learning reduced-order Quadratic-Linear models in Process Engineering using Operator Inference","abstract":"In this work, we address the challenge of efficiently modeling dynamical systems in process engineering. We use reduced-order model learning, specifically operator inference. This is a non-intrusive, data-driven method for learning dynamical systems from time-domain data. The application in our study is carbon dioxide methanation, an important reaction within the Power-to-X framework, to demonstrate its potential. The numerical results show the ability of the reduced-order models constructed with operator inference to provide a reduced yet accurate surrogate solution. This represents an important milestone towards the implementation of fast and reliable digital twin architectures.","sentences":["In this work, we address the challenge of efficiently modeling dynamical systems in process engineering.","We use reduced-order model learning, specifically operator inference.","This is a non-intrusive, data-driven method for learning dynamical systems from time-domain data.","The application in our study is carbon dioxide methanation, an important reaction within the Power-to-X framework, to demonstrate its potential.","The numerical results show the ability of the reduced-order models constructed with operator inference to provide a reduced yet accurate surrogate solution.","This represents an important milestone towards the implementation of fast and reliable digital twin architectures."],"url":"http://arxiv.org/abs/2402.17698v1","category":"math.NA"}
{"created":"2024-02-27 17:05:06","title":"Novel spectral methods for shock capturing and the removal of tygers in computational fluid dynamics","abstract":"Spectral methods yield numerical solutions of the Galerkin-truncated versions of nonlinear partial differential equations involved especially in fluid dynamics. In the presence of discontinuities, such as shocks, spectral approximations develop Gibbs oscillations near the discontinuity. This causes the numerical solution to deviate quickly from the true solution. For spectral approximations of the 1D inviscid Burgers equation, nonlinear wave resonances lead to the formation of tygers in well-resolved areas of the flow, far from the shock. Recently, Besse(to be published) has proposed novel spectral relaxation (SR) and spectral purging (SP) schemes for the removal of tygers and Gibbs oscillations in spectral approximations of nonlinear conservation laws. For the 1D inviscid Burgers equation, it is shown that the novel SR and SP approximations of the solution converge strongly in L2 norm to the entropic weak solution, under an appropriate choice of kernels and related parameters. In this work, we carry out a detailed numerical investigation of SR and SP schemes when applied to the 1D inviscid Burgers equation and report the efficiency of shock capture and the removal of tygers. We then extend our study to systems of nonlinear hyperbolic conservation laws - such as the 2x2 system of the shallow water equations and the standard 3x3 system of 1D compressible Euler equations. For the latter, we generalise the implementation of SR methods to non-periodic problems using Chebyshev polynomials. We then turn to singular flow in the 1D wall approximation of the 3D-axisymmetric wall-bounded incompressible Euler equation. Here, in order to determine the blowup time of the solution, we compare the decay of the width of the analyticity strip, obtained from the pure pseudospectral method, with the improved estimate obtained using the novel spectral relaxation scheme.","sentences":["Spectral methods yield numerical solutions of the Galerkin-truncated versions of nonlinear partial differential equations involved especially in fluid dynamics.","In the presence of discontinuities, such as shocks, spectral approximations develop Gibbs oscillations near the discontinuity.","This causes the numerical solution to deviate quickly from the true solution.","For spectral approximations of the 1D inviscid Burgers equation, nonlinear wave resonances lead to the formation of tygers in well-resolved areas of the flow, far from the shock.","Recently, Besse(to be published) has proposed novel spectral relaxation (SR) and spectral purging (SP) schemes for the removal of tygers and Gibbs oscillations in spectral approximations of nonlinear conservation laws.","For the 1D inviscid Burgers equation, it is shown that the novel SR and SP approximations of the solution converge strongly in L2 norm to the entropic weak solution, under an appropriate choice of kernels and related parameters.","In this work, we carry out a detailed numerical investigation of SR and SP schemes when applied to the 1D inviscid Burgers equation and report the efficiency of shock capture and the removal of tygers.","We then extend our study to systems of nonlinear hyperbolic conservation laws - such as the 2x2 system of the shallow water equations and the standard 3x3 system of 1D compressible Euler equations.","For the latter, we generalise the implementation of SR methods to non-periodic problems using Chebyshev polynomials.","We then turn to singular flow in the 1D wall approximation of the 3D-axisymmetric wall-bounded incompressible Euler equation.","Here, in order to determine the blowup time of the solution, we compare the decay of the width of the analyticity strip, obtained from the pure pseudospectral method, with the improved estimate obtained using the novel spectral relaxation scheme."],"url":"http://arxiv.org/abs/2402.17688v1","category":"math.NA"}
{"created":"2024-02-27 16:59:00","title":"Attosecond Core-level Photoionization Delays of Aromatic Molecules","abstract":"Attosecond photoionization delays are a unique probe of the structure and the electronic dynamics of matter, but valence-shell photoionization dynamics of polyatomic systems are often complicated and challenging to interpret because of the delocalized nature of the electronic wave functions. Here we use attosecond soft-X-ray pulses from LCLS to probe element-specific core-level photoionization delays between N-1s and C-1s core shells of aromatic azabenzene molecules containing one to three nitrogen atoms. Using a self-referenced technique, we measure absolute N-1s photoionization delays of up to 300 as, which increase with the number of nitrogen atoms in the molecule. Comparisons to molecular photoionization calculations reveal the dominant mechanisms to be the increasing trapping of the photoelectron wavefunction in the molecular plane, as well as the symmetry-induced decrease of the coupling strength among the partial waves. This study demonstrates the unique opportunities opened by measurements of core-level photoionization delays for unravelling attosecond electronic dynamics in the building blocks of biomolecules and molecular optoelectronics.","sentences":["Attosecond photoionization delays are a unique probe of the structure and the electronic dynamics of matter, but valence-shell photoionization dynamics of polyatomic systems are often complicated and challenging to interpret because of the delocalized nature of the electronic wave functions.","Here we use attosecond soft-X-ray pulses from LCLS to probe element-specific core-level photoionization delays between N-1s and C-1s core shells of aromatic azabenzene molecules containing one to three nitrogen atoms.","Using a self-referenced technique, we measure absolute N-1s photoionization delays of up to 300 as, which increase with the number of nitrogen atoms in the molecule.","Comparisons to molecular photoionization calculations reveal the dominant mechanisms to be the increasing trapping of the photoelectron wavefunction in the molecular plane, as well as the symmetry-induced decrease of the coupling strength among the partial waves.","This study demonstrates the unique opportunities opened by measurements of core-level photoionization delays for unravelling attosecond electronic dynamics in the building blocks of biomolecules and molecular optoelectronics."],"url":"http://arxiv.org/abs/2402.17685v1","category":"physics.chem-ph"}
{"created":"2024-02-27 16:54:08","title":"MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning for Multimodal Video Captioning","abstract":"To address the problem of catastrophic forgetting due to the invisibility of old categories in sequential input, existing work based on relatively simple categorization tasks has made some progress. In contrast, video captioning is a more complex task in multimodal scenario, which has not been explored in the field of incremental learning. After identifying this stability-plasticity problem when analyzing video with sequential input, we originally propose a method to Mitigate Catastrophic Forgetting in class-incremental learning for multimodal Video Captioning (MCF-VC). As for effectively maintaining good performance on old tasks at the macro level, we design Fine-grained Sensitivity Selection (FgSS) based on the Mask of Linear's Parameters and Fisher Sensitivity to pick useful knowledge from old tasks. Further, in order to better constrain the knowledge characteristics of old and new tasks at the specific feature level, we have created the Two-stage Knowledge Distillation (TsKD), which is able to learn the new task well while weighing the old task. Specifically, we design two distillation losses, which constrain the cross modal semantic information of semantic attention feature map and the textual information of the final outputs respectively, so that the inter-model and intra-model stylized knowledge of the old class is retained while learning the new class. In order to illustrate the ability of our model to resist forgetting, we designed a metric CIDER_t to detect the stage forgetting rate. Our experiments on the public dataset MSR-VTT show that the proposed method significantly resists the forgetting of previous tasks without replaying old samples, and performs well on the new task.","sentences":["To address the problem of catastrophic forgetting due to the invisibility of old categories in sequential input, existing work based on relatively simple categorization tasks has made some progress.","In contrast, video captioning is a more complex task in multimodal scenario, which has not been explored in the field of incremental learning.","After identifying this stability-plasticity problem when analyzing video with sequential input, we originally propose a method to Mitigate Catastrophic Forgetting in class-incremental learning for multimodal Video Captioning (MCF-VC).","As for effectively maintaining good performance on old tasks at the macro level, we design Fine-grained Sensitivity Selection (FgSS) based on the Mask of Linear's Parameters and Fisher Sensitivity to pick useful knowledge from old tasks.","Further, in order to better constrain the knowledge characteristics of old and new tasks at the specific feature level, we have created the Two-stage Knowledge Distillation (TsKD), which is able to learn the new task well while weighing the old task.","Specifically, we design two distillation losses, which constrain the cross modal semantic information of semantic attention feature map and the textual information of the final outputs respectively, so that the inter-model and intra-model stylized knowledge of the old class is retained while learning the new class.","In order to illustrate the ability of our model to resist forgetting, we designed a metric CIDER_t to detect the stage forgetting rate.","Our experiments on the public dataset MSR-VTT show that the proposed method significantly resists the forgetting of previous tasks without replaying old samples, and performs well on the new task."],"url":"http://arxiv.org/abs/2402.17680v1","category":"cs.CV"}
{"created":"2024-02-27 16:48:12","title":"Highly-Efficient Persistent FIFO Queues","abstract":"In this paper, we study the question whether techniques employed, in a conventional system, by state-of-the-art concurrent algorithms to avoid contended hot spots are still efficient for recoverable computing in settings with Non-Volatile Memory (NVM). We focus on concurrent FIFO queues that have two end-points, head and tail, which are highly contended.   We present a persistent FIFO queue implementation that performs a pair of persistence instructions per operation (enqueue or dequeue). The algorithm achieves to perform these instructions on variables of low contention by employing Fetch&Increment and using the state-of-the-art queue implementation by Afek and Morrison (PPoPP'13). These result in performance that is up to 2x faster than state-of-the-art persistent FIFO queue implementations.","sentences":["In this paper, we study the question whether techniques employed, in a conventional system, by state-of-the-art concurrent algorithms to avoid contended hot spots are still efficient for recoverable computing in settings with Non-Volatile Memory (NVM).","We focus on concurrent FIFO queues that have two end-points, head and tail, which are highly contended.   ","We present a persistent FIFO queue implementation that performs a pair of persistence instructions per operation (enqueue or dequeue).","The algorithm achieves to perform these instructions on variables of low contention by employing Fetch&Increment and using the state-of-the-art queue implementation by Afek and Morrison (PPoPP'13).","These result in performance that is up to 2x faster than state-of-the-art persistent FIFO queue implementations."],"url":"http://arxiv.org/abs/2402.17674v1","category":"cs.DC"}
{"created":"2024-02-27 16:42:26","title":"Spectral Gap Superposition States","abstract":"This work introduces a novel NISQ-friendly procedure for estimating spectral gaps in quantum systems. By leveraging Adiabatic Thermalization, we are able to create the Spectral Gap Superposition state, a newly defined quantum state exhibiting observable fluctuations in time that allow for the accurate estimation of any energy gap. Our method is tested by estimating the energy gap between the ground and the first excited state for the 1D and 2D Ising model, the Hydrogen molecule H2 and Helium molecule He2. Despite limiting our circuit design to have at most 40 Trotter steps, our numerical experiments of both noiseless and noisy devices for the presented systems give relative errors in the order of $10^{-2}$ and $10^{-1}$. Further experiments on the IonQ Aria device lead to spectral gap estimations with a relative error of $10^{-2}$ for a 4-site Ising chain, demonstrating the validity of the procedure for NISQ devices and charting a path towards a new way of calculating energy gaps.","sentences":["This work introduces a novel NISQ-friendly procedure for estimating spectral gaps in quantum systems.","By leveraging Adiabatic Thermalization, we are able to create the Spectral Gap Superposition state, a newly defined quantum state exhibiting observable fluctuations in time that allow for the accurate estimation of any energy gap.","Our method is tested by estimating the energy gap between the ground and the first excited state for the 1D and 2D Ising model, the Hydrogen molecule H2 and Helium molecule He2.","Despite limiting our circuit design to have at most 40 Trotter steps, our numerical experiments of both noiseless and noisy devices for the presented systems give relative errors in the order of $10^{-2}$ and $10^{-1}$. Further experiments on the IonQ Aria device lead to spectral gap estimations with a relative error of $10^{-2}$ for a 4-site Ising chain, demonstrating the validity of the procedure for NISQ devices and charting a path towards a new way of calculating energy gaps."],"url":"http://arxiv.org/abs/2402.17668v1","category":"quant-ph"}
{"created":"2024-02-27 16:29:40","title":"Adiabatically-manipulated systems interacting with spin baths beyond the Rotating Wave Approximation","abstract":"The Stimulated Raman Adiabatic Passage on a three-state system interacting with a spin bath is considered focusing on the efficiency of the population transfer. Our analysis is based on the perturbation treatment of the interaction term evaluated beyond the Rotating Wave Approximation, thus focusing on the limit of weak system-bath coupling. The analytical expression of the correction to the efficiency and consequent numerical analysis show that in most of the cases the effects of the environment are negligible, confirming the robustness of the population transfer.","sentences":["The Stimulated Raman Adiabatic Passage on a three-state system interacting with a spin bath is considered focusing on the efficiency of the population transfer.","Our analysis is based on the perturbation treatment of the interaction term evaluated beyond the Rotating Wave Approximation, thus focusing on the limit of weak system-bath coupling.","The analytical expression of the correction to the efficiency and consequent numerical analysis show that in most of the cases the effects of the environment are negligible, confirming the robustness of the population transfer."],"url":"http://arxiv.org/abs/2402.17661v1","category":"quant-ph"}
{"created":"2024-02-27 16:18:40","title":"Computing eigenfrequency sensitivities near exceptional points","abstract":"Exceptional points are spectral degeneracies of non-Hermitian systems where both eigenfrequencies and eigenmodes coalesce. The eigenfrequency sensitivities near an exceptional point are significantly enhanced, whereby they diverge directly at the exceptional point. Capturing this enhanced sensitivity is crucial for the investigation and optimization of exceptional-point-based applications, such as optical sensors. We present a numerical framework, based on contour integration and algorithmic differentiation, to accurately and efficiently compute eigenfrequency sensitivities near exceptional points. We demonstrate the framework to an optical microdisk cavity and derive a semi-analytical solution to validate the numerical results. The computed eigenfrequency sensitivities are used to track the exceptional point along an exceptional surface in the parameter space. The presented framework can be applied to any kind of resonance problem, e.g., with arbitrary geometry or with exceptional points of arbitrary order.","sentences":["Exceptional points are spectral degeneracies of non-Hermitian systems where both eigenfrequencies and eigenmodes coalesce.","The eigenfrequency sensitivities near an exceptional point are significantly enhanced, whereby they diverge directly at the exceptional point.","Capturing this enhanced sensitivity is crucial for the investigation and optimization of exceptional-point-based applications, such as optical sensors.","We present a numerical framework, based on contour integration and algorithmic differentiation, to accurately and efficiently compute eigenfrequency sensitivities near exceptional points.","We demonstrate the framework to an optical microdisk cavity and derive a semi-analytical solution to validate the numerical results.","The computed eigenfrequency sensitivities are used to track the exceptional point along an exceptional surface in the parameter space.","The presented framework can be applied to any kind of resonance problem, e.g., with arbitrary geometry or with exceptional points of arbitrary order."],"url":"http://arxiv.org/abs/2402.17648v1","category":"physics.comp-ph"}
{"created":"2024-02-27 16:16:00","title":"Recurrent chaotic clustering and slow chaos in adaptive networks","abstract":"Adaptive dynamical networks are network systems in which the structure co-evolves and interacts with the dynamical state of the nodes. We study an adaptive dynamical network in which the structure changes on a slower time scale relative to the fast dynamics of the nodes. We identify a phenomenon we refer to as recurrent adaptive chaotic clustering (RACC), in which chaos is observed on a slow time scale, while the fast time scale exhibits regular dynamics. Such slow chaos is further characterized by long (relative to the fast time scale) regimes of frequency clusters or frequency-synchronized dynamics, interrupted by fast jumps between these regimes. We also determine parameter values where the time intervals between jumps are chaotic and show that such a state is robust to changes in parameters and initial conditions.","sentences":["Adaptive dynamical networks are network systems in which the structure co-evolves and interacts with the dynamical state of the nodes.","We study an adaptive dynamical network in which the structure changes on a slower time scale relative to the fast dynamics of the nodes.","We identify a phenomenon we refer to as recurrent adaptive chaotic clustering (RACC), in which chaos is observed on a slow time scale, while the fast time scale exhibits regular dynamics.","Such slow chaos is further characterized by long (relative to the fast time scale) regimes of frequency clusters or frequency-synchronized dynamics, interrupted by fast jumps between these regimes.","We also determine parameter values where the time intervals between jumps are chaotic and show that such a state is robust to changes in parameters and initial conditions."],"url":"http://arxiv.org/abs/2402.17646v1","category":"nlin.AO"}
{"created":"2024-02-27 16:05:21","title":"Dual chiral density wave induced oscillating Casimir effect","abstract":"The Casimir effect is known to be induced from photon fields confined by a small volume, and also its fermionic counterpart has been predicted in a wide range of quantum systems. Here, we investigate what types of Casimir effects can occur from quark fields in dense and thin quark matter. In particular, in the dual chiral density wave, which is a possible ground state of dense quark matter, we find that the Casimir energy oscillates as a function of the thickness of matter. This oscillating Casimir effect is regarded as an analog of that in Weyl semimetals and is attributed to the Weyl points in the momentum space of quark fields. In addition, we show that an oscillation is also induced from the quark Fermi sea, and the total Casimir energy is composed of multiple oscillations.","sentences":["The Casimir effect is known to be induced from photon fields confined by a small volume, and also its fermionic counterpart has been predicted in a wide range of quantum systems.","Here, we investigate what types of Casimir effects can occur from quark fields in dense and thin quark matter.","In particular, in the dual chiral density wave, which is a possible ground state of dense quark matter, we find that the Casimir energy oscillates as a function of the thickness of matter.","This oscillating Casimir effect is regarded as an analog of that in Weyl semimetals and is attributed to the Weyl points in the momentum space of quark fields.","In addition, we show that an oscillation is also induced from the quark Fermi sea, and the total Casimir energy is composed of multiple oscillations."],"url":"http://arxiv.org/abs/2402.17638v1","category":"hep-ph"}
{"created":"2024-02-27 16:00:30","title":"Modelling LAEs in the Epoch of Reionization with OBELISK","abstract":"Lya emitters (LAEs) are particularly useful objects for the study of the Epoch of Reionization. Lya profiles can be used to estimate the amount of ionizing photons that are able to escape the galaxies, and therefore to understand what objects contributed to reionization. However, Lya is a resonant line and its complex radiative transfer effects make the interpretation of the line challenging and require the use of appropriate radiative transfer methods for anything but the simplest gas distributions. With this work we aim to study the properties of simulated LAEs, and the robustness of these inferred properties under a change in the dust model. We also explore the Lyman Continuum (LyC) escape fraction of these galaxies and compare our results with observationally calibrated methods to infer this quantity from the Lya spectrum. We use the radiative transfer code RASCAS to perform synthetic observations of 13 flux-selected galaxies from the Obelisk simulation at redshift z = 6, towards the end of the Epoch of Reionization. Each galaxy was observed in Lya, ionizing and non-ionizing continuum from 48 different viewing angles. We show that the Lya profiles emitted from a galaxy present large variations with a change in viewing angle and that the relation between peak separation and Lya escape fraction is not as strong as previously found, as we find lines of sight with both low peak separation and low escape fraction, due to their dust content. We also show that the properties of the Lya line are reasonably robust under a change in dust model. Lastly, we compare the LyC escape fractions we derive from the simulation to three observationally calibrated methods of inferring this quantity. We determine that none of these relations reproduce the scatter that we find in our sample, and that high escape fraction lines of sight have both low peak separation and low dust extinction in the UV.","sentences":["Lya emitters (LAEs) are particularly useful objects for the study of the Epoch of Reionization.","Lya profiles can be used to estimate the amount of ionizing photons that are able to escape the galaxies, and therefore to understand what objects contributed to reionization.","However, Lya is a resonant line and its complex radiative transfer effects make the interpretation of the line challenging and require the use of appropriate radiative transfer methods for anything but the simplest gas distributions.","With this work we aim to study the properties of simulated LAEs, and the robustness of these inferred properties under a change in the dust model.","We also explore the Lyman Continuum (LyC) escape fraction of these galaxies and compare our results with observationally calibrated methods to infer this quantity from the Lya spectrum.","We use the radiative transfer code RASCAS to perform synthetic observations of 13 flux-selected galaxies from the Obelisk simulation at redshift z = 6, towards the end of the Epoch of Reionization.","Each galaxy was observed in Lya, ionizing and non-ionizing continuum from 48 different viewing angles.","We show that the Lya profiles emitted from a galaxy present large variations with a change in viewing angle and that the relation between peak separation and Lya escape fraction is not as strong as previously found, as we find lines of sight with both low peak separation and low escape fraction, due to their dust content.","We also show that the properties of the Lya line are reasonably robust under a change in dust model.","Lastly, we compare the LyC escape fractions we derive from the simulation to three observationally calibrated methods of inferring this quantity.","We determine that none of these relations reproduce the scatter that we find in our sample, and that high escape fraction lines of sight have both low peak separation and low dust extinction in the UV."],"url":"http://arxiv.org/abs/2402.17635v1","category":"astro-ph.GA"}
{"created":"2024-02-27 15:57:31","title":"Deterministic Cache-Oblivious Funnelselect","abstract":"In the multiple-selection problem one is given an unsorted array $S$ of $N$ elements and an array of $q$ query ranks $r_1<\\cdots<r_q$, and the task is to return, in sorted order, the $q$ elements in $S$ of rank $r_1, \\ldots, r_q$, respectively. The asymptotic deterministic comparison complexity of the problem was settled by Dobkin and Munro [JACM 1981]. In the I/O model an optimal I/O complexity was achieved by Hu et al. [SPAA 2014]. Recently [ESA 2023], we presented a cache-oblivious algorithm with matching I/O complexity, named funnelselect, since it heavily borrows ideas from the cache-oblivious sorting algorithm funnelsort from the seminal paper by Frigo, Leiserson, Prokop and Ramachandran [FOCS 1999]. Funnelselect is inherently randomized as it relies on sampling for cheaply finding many good pivots. In this paper we present deterministic funnelselect, achieving the same optional I/O complexity cache-obliviously without randomization. Our new algorithm essentially replaces a single (in expectation) reversed-funnel computation using random pivots by a recursive algorithm using multiple reversed-funnel computations. To meet the I/O bound, this requires a carefully chosen subproblem size based on the entropy of the sequence of query ranks; deterministic funnelselect thus raises distinct technical challenges not met by randomized funnelselect. The resulting worst-case I/O bound is $O\\bigl(\\sum_{i=1}^{q+1} \\frac{\\Delta_i}{B} \\cdot \\log_{M/B} \\frac{N}{\\Delta_i} + \\frac{N}{B}\\bigr)$, where $B$ is the external memory block size, $M\\geq B^{1+\\epsilon}$ is the internal memory size, for some constant $\\epsilon>0$, and $\\Delta_i = r_{i} - r_{i-1}$ (assuming $r_0=0$ and $r_{q+1}=N + 1$).","sentences":["In the multiple-selection problem one is given an unsorted array $S$ of $N$ elements and an array of $q$ query ranks $r_1<\\cdots<r_q$, and the task is to return, in sorted order, the $q$ elements in $S$ of rank $r_1, \\ldots, r_q$, respectively.","The asymptotic deterministic comparison complexity of the problem was settled by Dobkin and Munro","[JACM 1981].","In the I/O model an optimal I/O complexity was achieved by Hu et al.","[SPAA 2014].","Recently [ESA 2023], we presented a cache-oblivious algorithm with matching I/O complexity, named funnelselect, since it heavily borrows ideas from the cache-oblivious sorting algorithm funnelsort from the seminal paper by Frigo, Leiserson, Prokop and Ramachandran","[FOCS 1999].","Funnelselect is inherently randomized as it relies on sampling for cheaply finding many good pivots.","In this paper we present deterministic funnelselect, achieving the same optional I/O complexity cache-obliviously without randomization.","Our new algorithm essentially replaces a single (in expectation) reversed-funnel computation using random pivots by a recursive algorithm using multiple reversed-funnel computations.","To meet the I/O bound, this requires a carefully chosen subproblem size based on the entropy of the sequence of query ranks; deterministic funnelselect thus raises distinct technical challenges not met by randomized funnelselect.","The resulting worst-case I/O bound is $O\\bigl(\\sum_{i=1}^{q+1} \\frac{\\Delta_i}{B} \\cdot \\log_{M/B} \\frac{N}{\\Delta_i} + \\frac{N}{B}\\bigr)$, where $B$ is the external memory block size, $M\\geq B^{1+\\epsilon}$ is the internal memory size, for some constant $\\epsilon>0$, and $\\Delta_i = r_{i} - r_{i-1}$ (assuming $r_0=0$ and $r_{q+1}=N + 1$)."],"url":"http://arxiv.org/abs/2402.17631v1","category":"cs.DS"}
{"created":"2024-02-27 15:55:08","title":"Arithmetic and Topology of the Hexponential map","abstract":"The modular group $\\operatorname{PSL}_2(\\mathbb{Z})$ acts on the upper-half plane $\\mathbb{HP}$ with quotient the modular orbifold, uniformized by the function $\\mathfrak{j} \\colon \\mathbb{HP}\\to \\mathbb{C}$. We first show that second derived subgroup $\\operatorname{PSL}_2(\\mathbb{Z})''$ corresponds to a $\\mathbb{Z}^2\\rtimes \\mathbb{Z}/6$ Galois cover of the modular orbifold by a hexpunctured plane, uniformized by the hexponential map $\\operatorname{hexp} \\colon \\mathbb{HP} \\to \\mathbb{C} \\setminus (\\omega_0\\mathbb{Z}[j])$, which is a primitive of $C\\eta^4$ where $\\omega_0\\in i\\mathbb{R}$ and $C\\in \\mathbb{R}$ are explicit constants and $\\eta$ is Dedekind eta function. We describe the values of the cusp-compactification $\\partial \\operatorname{hexp}\\colon \\mathbb{QP}^1\\to \\omega_0 \\mathbb{Z}[j]$. After defining the radial-compactification $\\operatorname{Shexp} \\colon \\mathscr{R} \\to \\mathbb{R}/(2\\pi\\mathbb{Z})$, we construct a simple section $\\operatorname{InSh} \\colon \\mathbb{R}/(2\\pi\\mathbb{Z}) \\to \\mathscr{S} \\bmod{\\operatorname{PSL}_2(\\mathbb{Z})'}$ where $\\mathscr{S} \\subset \\mathbb{RP}^1$ is a set of numbers whose continued fraction expansions arise from Sturmian sequences, which contains the set $\\mathscr{M}$ of Markov quadratic irrationals as those numbers arising from periodic Sturmian sequences. Finally we show that the values of $\\operatorname{InSh}$ are either Markov quadratic irrationals or transcendent.","sentences":["The modular group $\\operatorname{PSL}_2(\\mathbb{Z})$ acts on the upper-half plane $\\mathbb{HP}$ with quotient the modular orbifold, uniformized by the function $\\mathfrak{j} \\colon \\mathbb{HP}\\to \\mathbb{C}$. We first show that second derived subgroup $\\operatorname{PSL}_2(\\mathbb{Z})''$ corresponds to a $\\mathbb{Z}^2\\rtimes","\\mathbb{Z}/6$ Galois cover of the modular orbifold by a hexpunctured plane, uniformized by the hexponential map $\\operatorname{hexp} \\colon \\mathbb{HP} \\to \\mathbb{C} \\setminus (\\omega_0\\mathbb{Z}[j])$, which is a primitive of $C\\eta^4$ where $\\omega_0\\in i\\mathbb{R}$ and $C\\in \\mathbb{R}$ are explicit constants and $\\eta$ is Dedekind eta function.","We describe the values of the cusp-compactification $\\partial \\operatorname{hexp}\\colon \\mathbb{QP}^1\\to \\omega_0 \\mathbb{Z}[j]$. After defining the radial-compactification $\\operatorname{Shexp} \\colon \\mathscr{R} \\to \\mathbb{R}/(2\\pi\\mathbb{Z})$, we construct a simple section $\\operatorname{InSh} \\colon \\mathbb{R}/(2\\pi\\mathbb{Z","}) \\to \\mathscr{S} \\bmod{\\operatorname{PSL}_2(\\mathbb{Z})'}$ where $\\mathscr{S} \\subset \\mathbb{RP}^1$ is a set of numbers whose continued fraction expansions arise from Sturmian sequences, which contains the set $\\mathscr{M}$ of Markov quadratic irrationals as those numbers arising from periodic Sturmian sequences.","Finally we show that the values of $\\operatorname{InSh}$ are either Markov quadratic irrationals or transcendent."],"url":"http://arxiv.org/abs/2402.17628v1","category":"math.NT"}
{"created":"2024-02-27 15:53:02","title":"Physics Informed Modeling of Ecosystem Respiration via Dynamic Mode Decomposition with Control Input","abstract":"Ecosystem respiration (Reco) represents a major component of the global carbon cycle, and accurate characterization of its dynamics is essential for a comprehensive understanding of ecosystem-climate interactions and the impacts of climate extremes on the ecosystem. This paper presents a novel data-driven and physics-aware method for estimating Reco dynamics using the dynamic mode decomposition with control input (DMDc) technique, an emerging tool for analyzing nonlinear dynamical systems. The proposed model represents Reco as a state space model with an autonomous component and an exogenous control input. The control input can be any ecosystem driver(s), such as air temperature, soil temperature, or soil water content. This unique modeling approach allows controlled intervention to study the effects of different inputs on the system. Experimental results using Fluxnet2015 data show that the prediction accuracy of Reco dynamics achieved with DMDc is comparable to state-of-the-art methods, making it a promising tool for analyzing the dynamic behavior of different vegetation ecosystems on multi-temporal scales in response to different climatic drivers.","sentences":["Ecosystem respiration (Reco) represents a major component of the global carbon cycle, and accurate characterization of its dynamics is essential for a comprehensive understanding of ecosystem-climate interactions and the impacts of climate extremes on the ecosystem.","This paper presents a novel data-driven and physics-aware method for estimating Reco dynamics using the dynamic mode decomposition with control input (DMDc) technique, an emerging tool for analyzing nonlinear dynamical systems.","The proposed model represents Reco as a state space model with an autonomous component and an exogenous control input.","The control input can be any ecosystem driver(s), such as air temperature, soil temperature, or soil water content.","This unique modeling approach allows controlled intervention to study the effects of different inputs on the system.","Experimental results using Fluxnet2015 data show that the prediction accuracy of Reco dynamics achieved with DMDc is comparable to state-of-the-art methods, making it a promising tool for analyzing the dynamic behavior of different vegetation ecosystems on multi-temporal scales in response to different climatic drivers."],"url":"http://arxiv.org/abs/2402.17625v1","category":"math.DS"}
{"created":"2024-02-27 15:50:19","title":"Bound-state confinement after trap-expansion dynamics in integrable systems","abstract":"Integrable systems possess stable families of quasiparticles, which are composite objects (bound states) of elementary excitations. Motivated by recent quantum computer experiments, we investigate bound-state transport in the spin-$1/2$ anisotropic Heisenberg chain ($XXZ$ chain). Specifically, we consider the sudden vacuum expansion of a finite region $A$ prepared in a non-equilibrium state. In the hydrodynamic regime, if interactions are strong enough, bound states remain confined in the initial region. Bound-state confinement persists until the density of unbound excitations remains finite in the bulk of $A$. Since region $A$ is finite, at asymptotically long times bound states are \"liberated\" after the \"evaporation\" of all the unbound excitations. Fingerprints of confinement are visible in the space-time profiles of local spin-projection operators. To be specific, here we focus on the expansion of the $p$-N\\'eel states, which are obtained by repetition of a unit cell with $p$ up spins followed by $p$ down spins. Upon increasing $p$, the bound-state content is enhanced. In the limit $p\\to\\infty$ one obtains the domain-wall initial state. We show that for $p<4$, only bound states with $n>p$ are confined at large chain anisotropy. For $p\\gtrsim 4$, also bound states with $n=p$ are confined, consistent with the absence of transport in the limit $p\\to\\infty$. The scenario of bound-state confinement leads to a hierarchy of timescales at which bound states of different sizes are liberated, which is also reflected in the dynamics of the von Neumann entropy.","sentences":["Integrable systems possess stable families of quasiparticles, which are composite objects (bound states) of elementary excitations.","Motivated by recent quantum computer experiments, we investigate bound-state transport in the spin-$1/2$ anisotropic Heisenberg chain ($XXZ$ chain).","Specifically, we consider the sudden vacuum expansion of a finite region $A$ prepared in a non-equilibrium state.","In the hydrodynamic regime, if interactions are strong enough, bound states remain confined in the initial region.","Bound-state confinement persists until the density of unbound excitations remains finite in the bulk of $A$.","Since region $A$ is finite, at asymptotically long times bound states are \"liberated\" after the \"evaporation\" of all the unbound excitations.","Fingerprints of confinement are visible in the space-time profiles of local spin-projection operators.","To be specific, here we focus on the expansion of the $p$-N\\'eel states, which are obtained by repetition of a unit cell with $p$ up spins followed by $p$ down spins.","Upon increasing $p$, the bound-state content is enhanced.","In the limit $p\\to\\infty$ one obtains the domain-wall initial state.","We show that for $p<4$, only bound states with $n>p$ are confined at large chain anisotropy.","For $p\\gtrsim 4$, also bound states with $n=p$ are confined, consistent with the absence of transport in the limit $p\\to\\infty$. The scenario of bound-state confinement leads to a hierarchy of timescales at which bound states of different sizes are liberated, which is also reflected in the dynamics of the von Neumann entropy."],"url":"http://arxiv.org/abs/2402.17623v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-27 15:42:33","title":"Neural Automated Writing Evaluation with Corrective Feedback","abstract":"The utilization of technology in second language learning and teaching has become ubiquitous. For the assessment of writing specifically, automated writing evaluation (AWE) and grammatical error correction (GEC) have become immensely popular and effective methods for enhancing writing proficiency and delivering instant and individualized feedback to learners. By leveraging the power of natural language processing (NLP) and machine learning algorithms, AWE and GEC systems have been developed separately to provide language learners with automated corrective feedback and more accurate and unbiased scoring that would otherwise be subject to examiners. In this paper, we propose an integrated system for automated writing evaluation with corrective feedback as a means of bridging the gap between AWE and GEC results for second language learners. This system enables language learners to simulate the essay writing tests: a student writes and submits an essay, and the system returns the assessment of the writing along with suggested grammatical error corrections. Given that automated scoring and grammatical correction are more efficient and cost-effective than human grading, this integrated system would also alleviate the burden of manually correcting innumerable essays.","sentences":["The utilization of technology in second language learning and teaching has become ubiquitous.","For the assessment of writing specifically, automated writing evaluation (AWE) and grammatical error correction (GEC) have become immensely popular and effective methods for enhancing writing proficiency and delivering instant and individualized feedback to learners.","By leveraging the power of natural language processing (NLP) and machine learning algorithms, AWE and GEC systems have been developed separately to provide language learners with automated corrective feedback and more accurate and unbiased scoring that would otherwise be subject to examiners.","In this paper, we propose an integrated system for automated writing evaluation with corrective feedback as a means of bridging the gap between AWE and GEC results for second language learners.","This system enables language learners to simulate the essay writing tests: a student writes and submits an essay, and the system returns the assessment of the writing along with suggested grammatical error corrections.","Given that automated scoring and grammatical correction are more efficient and cost-effective than human grading, this integrated system would also alleviate the burden of manually correcting innumerable essays."],"url":"http://arxiv.org/abs/2402.17613v1","category":"cs.CL"}
{"created":"2024-02-27 15:28:19","title":"Corridor MPC for Multi-Agent Inspection of Orbiting Structures","abstract":"In this work, we propose an extension of the previously introduced Corridor Model Predictive Control scheme for high-order and distributed systems, with an application for on-orbit inspection. To this end, we leverage high order control barrier function (HOCBF) constraints as a suitable control approach to maintain each agent in the formation within a safe corridor from its reference trajectory. The recursive feasibility of the designed MPC scheme is tested numerically, while suitable modifications of the classical HOCBF constraint definition are introduced such that safety is guaranteed both in sampled and continuous time. The designed controller is validated through computer simulation in a realistic inspection scenario of the International Space Station.","sentences":["In this work, we propose an extension of the previously introduced Corridor Model Predictive Control scheme for high-order and distributed systems, with an application for on-orbit inspection.","To this end, we leverage high order control barrier function (HOCBF) constraints as a suitable control approach to maintain each agent in the formation within a safe corridor from its reference trajectory.","The recursive feasibility of the designed MPC scheme is tested numerically, while suitable modifications of the classical HOCBF constraint definition are introduced such that safety is guaranteed both in sampled and continuous time.","The designed controller is validated through computer simulation in a realistic inspection scenario of the International Space Station."],"url":"http://arxiv.org/abs/2402.17596v1","category":"cs.SY"}
{"created":"2024-02-27 15:26:13","title":"Origin of magnetic switching cascades in tetrahedral CoFe nanostructures","abstract":"We present a comprehensive study of small-scale three-dimensional (3D) tetrahedral CoFe nanostructure arrays prepared by focused electron beam-induced deposition (FEBID) and placed in two distinct orientations with respect to the direction of an external magnetic field. Using ultra-sensitive micro-Hall magnetometry we obtain angular-dependent magnetic stray field hysteresis loops that show characteristic cascading magnetic switching close to zero magnetic field. By employing micromagnetic simulations we could reproduce the hysteresis loops and identify characteristic field dependent magnetic configurations including a vortex-type groundstate. From this we derive a coarse-graining macrospin model and show that the complex switching behavior can be explained by the reorientation dynamics of non-interacting uniaxial anisotropic magnetic grains modeled as a superposition of Stoner-Wohlfarth particles.","sentences":["We present a comprehensive study of small-scale three-dimensional (3D) tetrahedral CoFe nanostructure arrays prepared by focused electron beam-induced deposition (FEBID) and placed in two distinct orientations with respect to the direction of an external magnetic field.","Using ultra-sensitive micro-Hall magnetometry we obtain angular-dependent magnetic stray field hysteresis loops that show characteristic cascading magnetic switching close to zero magnetic field.","By employing micromagnetic simulations we could reproduce the hysteresis loops and identify characteristic field dependent magnetic configurations including a vortex-type groundstate.","From this we derive a coarse-graining macrospin model and show that the complex switching behavior can be explained by the reorientation dynamics of non-interacting uniaxial anisotropic magnetic grains modeled as a superposition of Stoner-Wohlfarth particles."],"url":"http://arxiv.org/abs/2402.17594v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-27 15:17:05","title":"Communication-Constrained STL Task Decomposition through Convex Optimization","abstract":"In this work, we propose a method to decompose signal temporal logic (STL) tasks for multi-agent systems subject to constraints imposed by the communication graph. Specifically, we propose to decompose tasks defined over multiple agents which require multi-hop communication, by a set of sub-tasks defined over the states of agents with 1-hop distance over the communication graph. To this end, we parameterize the predicates of the tasks to be decomposed as suitable hyper-rectangles. Then, we show that by solving a constrained convex optimization, optimal parameters maximising the volume of the predicate's super-level sets can be computed for the decomposed tasks. In addition, we provide a formal definition of conflicting conjunctions of tasks for the considered STL fragment and a formal procedure to exclude such conjunctions from the solution set of possible decompositions. The proposed approach is demonstrated through simulations.","sentences":["In this work, we propose a method to decompose signal temporal logic (STL) tasks for multi-agent systems subject to constraints imposed by the communication graph.","Specifically, we propose to decompose tasks defined over multiple agents which require multi-hop communication, by a set of sub-tasks defined over the states of agents with 1-hop distance over the communication graph.","To this end, we parameterize the predicates of the tasks to be decomposed as suitable hyper-rectangles.","Then, we show that by solving a constrained convex optimization, optimal parameters maximising the volume of the predicate's super-level sets can be computed for the decomposed tasks.","In addition, we provide a formal definition of conflicting conjunctions of tasks for the considered STL fragment and a formal procedure to exclude such conjunctions from the solution set of possible decompositions.","The proposed approach is demonstrated through simulations."],"url":"http://arxiv.org/abs/2402.17585v1","category":"eess.SY"}
{"created":"2024-02-27 15:14:19","title":"FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in Large-scale Cloud Systems","abstract":"Postmortem analysis is essential in the management of incidents within cloud systems, which provides valuable insights to improve system's reliability and robustness. At CloudA, fault pattern profiling is performed during the postmortem phase, which involves the classification of incidents' faults into unique categories, referred to as fault pattern. By aggregating and analyzing these fault patterns, engineers can discern common faults, vulnerable components and emerging fault trends. However, this process is currently conducted by manual labeling, which has inherent drawbacks. On the one hand, the sheer volume of incidents means only the most severe ones are analyzed, causing a skewed overview of fault patterns. On the other hand, the complexity of the task demands extensive domain knowledge, which leads to errors and inconsistencies. To address these limitations, we propose an automated approach, named FaultProfIT, for Fault pattern Profiling of Incident Tickets. It leverages hierarchy-guided contrastive learning to train a hierarchy-aware incident encoder and predicts fault patterns with enhanced incident representations. We evaluate FaultProfIT using the production incidents from CloudA. The results demonstrate that FaultProfIT outperforms state-of-the-art methods. Our ablation study and analysis also verify the effectiveness of hierarchy-guided contrastive learning. Additionally, we have deployed FaultProfIT at CloudA for six months. To date, FaultProfIT has analyzed 10,000+ incidents from 30+ cloud services, successfully revealing several fault trends that have informed system improvements.","sentences":["Postmortem analysis is essential in the management of incidents within cloud systems, which provides valuable insights to improve system's reliability and robustness.","At CloudA, fault pattern profiling is performed during the postmortem phase, which involves the classification of incidents' faults into unique categories, referred to as fault pattern.","By aggregating and analyzing these fault patterns, engineers can discern common faults, vulnerable components and emerging fault trends.","However, this process is currently conducted by manual labeling, which has inherent drawbacks.","On the one hand, the sheer volume of incidents means only the most severe ones are analyzed, causing a skewed overview of fault patterns.","On the other hand, the complexity of the task demands extensive domain knowledge, which leads to errors and inconsistencies.","To address these limitations, we propose an automated approach, named FaultProfIT, for Fault pattern Profiling of Incident Tickets.","It leverages hierarchy-guided contrastive learning to train a hierarchy-aware incident encoder and predicts fault patterns with enhanced incident representations.","We evaluate FaultProfIT using the production incidents from CloudA. The results demonstrate that FaultProfIT outperforms state-of-the-art methods.","Our ablation study and analysis also verify the effectiveness of hierarchy-guided contrastive learning.","Additionally, we have deployed FaultProfIT at CloudA for six months.","To date, FaultProfIT has analyzed 10,000+ incidents from 30+ cloud services, successfully revealing several fault trends that have informed system improvements."],"url":"http://arxiv.org/abs/2402.17583v1","category":"cs.SE"}
{"created":"2024-02-27 15:09:39","title":"On the Kaup-Broer-Kupershmidt systems","abstract":"The aim of this paper is to survey and complete, mostly by numerical simulations, results on a remarkable Boussinesq system describing weakly nonlinear, long surface water waves. It is the only member of the so-called (abcd) family of Boussinesq systems known to be completely integrable.","sentences":["The aim of this paper is to survey and complete, mostly by numerical simulations, results on a remarkable Boussinesq system describing weakly nonlinear, long surface water waves.","It is the only member of the so-called (abcd) family of Boussinesq systems known to be completely integrable."],"url":"http://arxiv.org/abs/2402.17576v1","category":"math.AP"}
{"created":"2024-02-27 14:58:49","title":"On the Weierstra\u00df form of infinite dimensional differential algebraic equations","abstract":"The solvability for infinite dimensional differential algebraic equations possessing a resolvent index and a Weierstra{\\ss} form is studied. In particular, the concept of integrated semigroups is used to determine a subset on which solutions exist and are unique. This information is later used for a important class of systems, namely, port-Hamiltonian differential algebraic equations.","sentences":["The solvability for infinite dimensional differential algebraic equations possessing a resolvent index and a Weierstra{\\ss} form is studied.","In particular, the concept of integrated semigroups is used to determine a subset on which solutions exist and are unique.","This information is later used for a important class of systems, namely, port-Hamiltonian differential algebraic equations."],"url":"http://arxiv.org/abs/2402.17560v1","category":"math.AP"}
{"created":"2024-02-27 14:58:35","title":"GraphMatch: Subgraph Query Processing on FPGAs","abstract":"Efficiently finding subgraph embeddings in large graphs is crucial for many application areas like biology and social network analysis. Set intersections are the predominant and most challenging aspect of current join-based subgraph query processing systems for CPUs. Previous work has shown the viability of utilizing FPGAs for acceleration of graph and join processing.   In this work, we propose GraphMatch, the first genearl-purpose stand-alone subgraph query processing accelerator based on worst-case optimal joins (WCOJ) that is fully designed for modern, field programmable gate array (FPGA) hardware. For efficient processing of various graph data sets and query graph patterns, it leverages a novel set intersection approach, called AllCompare, tailor-made for FPGAs. We show that this set intersection approach efficiently solves multi-set intersections in subgraph query processing, superior to CPU-based approaches. Overall, GraphMatch achieves a speedup of over 2.68x and 5.16x, compared to the state-of-the-art systems GraphFlow and RapidMatch, respectively.","sentences":["Efficiently finding subgraph embeddings in large graphs is crucial for many application areas like biology and social network analysis.","Set intersections are the predominant and most challenging aspect of current join-based subgraph query processing systems for CPUs.","Previous work has shown the viability of utilizing FPGAs for acceleration of graph and join processing.   ","In this work, we propose GraphMatch, the first genearl-purpose stand-alone subgraph query processing accelerator based on worst-case optimal joins (WCOJ) that is fully designed for modern, field programmable gate array (FPGA) hardware.","For efficient processing of various graph data sets and query graph patterns, it leverages a novel set intersection approach, called AllCompare, tailor-made for FPGAs.","We show that this set intersection approach efficiently solves multi-set intersections in subgraph query processing, superior to CPU-based approaches.","Overall, GraphMatch achieves a speedup of over 2.68x and 5.16x, compared to the state-of-the-art systems GraphFlow and RapidMatch, respectively."],"url":"http://arxiv.org/abs/2402.17559v1","category":"cs.DB"}
{"created":"2024-02-27 14:43:55","title":"FlipHash: A Constant-Time Consistent Range-Hashing Algorithm","abstract":"Consistent range-hashing is a technique used in distributed systems, either directly or as a subroutine for consistent hashing, commonly to realize an even and stable data distribution over a variable number of resources. We introduce FlipHash, a consistent range-hashing algorithm with constant time complexity and low memory requirements. Like Jump Consistent Hash, FlipHash is intended for applications where resources can be indexed sequentially. Under this condition, it ensures that keys are hashed evenly across resources and that changing the number of resources only causes keys to be remapped from a removed resource or to an added one, but never shuffled across persisted ones. FlipHash differentiates itself with its low computational cost, achieving constant-time complexity. We show that FlipHash beats Jump Consistent Hash's cost, which is logarithmic in the number of resources, both theoretically and in experiments over practical settings.","sentences":["Consistent range-hashing is a technique used in distributed systems, either directly or as a subroutine for consistent hashing, commonly to realize an even and stable data distribution over a variable number of resources.","We introduce FlipHash, a consistent range-hashing algorithm with constant time complexity and low memory requirements.","Like Jump Consistent Hash, FlipHash is intended for applications where resources can be indexed sequentially.","Under this condition, it ensures that keys are hashed evenly across resources and that changing the number of resources only causes keys to be remapped from a removed resource or to an added one, but never shuffled across persisted ones.","FlipHash differentiates itself with its low computational cost, achieving constant-time complexity.","We show that FlipHash beats Jump Consistent Hash's cost, which is logarithmic in the number of resources, both theoretically and in experiments over practical settings."],"url":"http://arxiv.org/abs/2402.17549v1","category":"cs.DS"}
{"created":"2024-02-27 14:30:12","title":"Computational imaging of small-amplitude biperiodic surfaces with negative index material","abstract":"This paper presents an innovative approach to computational acoustic imaging of biperiodic surfaces, exploiting the capabilities of an acoustic superlens to overcome the diffraction limit. We address the challenge of imaging physical entities in complex environments by considering the partial differential equations that govern the physics and solving the corresponding inverse problem. We focus on imaging infinite rough surfaces, specifically 2D diffraction gratings, and propose a method that leverages the transformed field expansion (TFE). We derive a reconstruction formula connecting the Fourier coefficients of the surface and the measured field, demonstrating the potential for unlimited resolution under ideal conditions. We also introduce an approximate discrepancy principle to determine the cut-off frequency for the truncated Fourier series expansion in surface profile reconstruction. Furthermore, we elucidate the resolution enhancement effect of the superlens by deriving the discrete Fourier transform of white Gaussian noise. Our numerical experiments confirm the effectiveness of the proposed method, demonstrating high subwavelength resolution even under slightly non-ideal conditions. This study extends the current understanding of superlens-based imaging and provides a robust framework for future research.","sentences":["This paper presents an innovative approach to computational acoustic imaging of biperiodic surfaces, exploiting the capabilities of an acoustic superlens to overcome the diffraction limit.","We address the challenge of imaging physical entities in complex environments by considering the partial differential equations that govern the physics and solving the corresponding inverse problem.","We focus on imaging infinite rough surfaces, specifically 2D diffraction gratings, and propose a method that leverages the transformed field expansion (TFE).","We derive a reconstruction formula connecting the Fourier coefficients of the surface and the measured field, demonstrating the potential for unlimited resolution under ideal conditions.","We also introduce an approximate discrepancy principle to determine the cut-off frequency for the truncated Fourier series expansion in surface profile reconstruction.","Furthermore, we elucidate the resolution enhancement effect of the superlens by deriving the discrete Fourier transform of white Gaussian noise.","Our numerical experiments confirm the effectiveness of the proposed method, demonstrating high subwavelength resolution even under slightly non-ideal conditions.","This study extends the current understanding of superlens-based imaging and provides a robust framework for future research."],"url":"http://arxiv.org/abs/2402.17543v1","category":"math.AP"}
{"created":"2024-02-27 14:27:05","title":"A heuristic for solving the irregular strip packing problem with quantum optimization","abstract":"We introduce a novel quantum computing heuristic for solving the irregular strip packing problem, a significant challenge in optimizing material usage across various industries. This problem involves arranging a set of irregular polygonal pieces within a fixed-height, rectangular container to minimize waste. Traditional methods heavily rely on manual optimization by specialists, highlighting the complexity and computational difficulty of achieving quasi-optimal layouts. The proposed algorithm employs a quantum-inspired heuristic that decomposes the strip packing problem into two sub-problems: ordering pieces via the traveling salesman problem and spatially arranging them in a rectangle packing problem. This strategy facilitates a novel application of quantum computing to industrial optimization, aiming to minimize waste and enhance material efficiency. Experimental evaluations using both classical and quantum computational methods demonstrate the algorithm's efficacy. We evaluate the algorithm's performance using the quantum approximate optimization algorithm and the quantum alternating operator ansatz, through simulations and real quantum computers, and compare it to classical approaches.","sentences":["We introduce a novel quantum computing heuristic for solving the irregular strip packing problem, a significant challenge in optimizing material usage across various industries.","This problem involves arranging a set of irregular polygonal pieces within a fixed-height, rectangular container to minimize waste.","Traditional methods heavily rely on manual optimization by specialists, highlighting the complexity and computational difficulty of achieving quasi-optimal layouts.","The proposed algorithm employs a quantum-inspired heuristic that decomposes the strip packing problem into two sub-problems: ordering pieces via the traveling salesman problem and spatially arranging them in a rectangle packing problem.","This strategy facilitates a novel application of quantum computing to industrial optimization, aiming to minimize waste and enhance material efficiency.","Experimental evaluations using both classical and quantum computational methods demonstrate the algorithm's efficacy.","We evaluate the algorithm's performance using the quantum approximate optimization algorithm and the quantum alternating operator ansatz, through simulations and real quantum computers, and compare it to classical approaches."],"url":"http://arxiv.org/abs/2402.17542v1","category":"quant-ph"}
{"created":"2024-02-27 14:25:37","title":"Circular THz ratchets in a 2D-modulated Dirac system","abstract":"We report on the observation of the circular ratchet effect excited by terahertz laser radiation in a specially designed two-dimensional metamaterial consisting of a graphene monolayer deposited on a graphite gate patterned with an array of triangular antidots. We show that a periodically driven Dirac fermion system with spatial asymmetry converts the a.c. power into a d.c. current, whose direction reverses when the radiation helicity is switched. The circular ratchet effect is demonstrated for room temperature and a radiation frequency of 2.54 THz. It is shown that the ratchet current magnitude can be controllably tuned by the patterned and uniform back gate voltages. The results are analyzed in the light of the developed microscopic theory considering electronic and plasmonic mechanisms of the ratchet current formation.","sentences":["We report on the observation of the circular ratchet effect excited by terahertz laser radiation in a specially designed two-dimensional metamaterial consisting of a graphene monolayer deposited on a graphite gate patterned with an array of triangular antidots.","We show that a periodically driven Dirac fermion system with spatial asymmetry converts the a.c. power into a d.c. current, whose direction reverses when the radiation helicity is switched.","The circular ratchet effect is demonstrated for room temperature and a radiation frequency of 2.54 THz.","It is shown that the ratchet current magnitude can be controllably tuned by the patterned and uniform back gate voltages.","The results are analyzed in the light of the developed microscopic theory considering electronic and plasmonic mechanisms of the ratchet current formation."],"url":"http://arxiv.org/abs/2402.17540v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-27 14:21:56","title":"Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control","abstract":"Learned sparse retrieval (LSR) is a family of neural methods that encode queries and documents into sparse lexical vectors that can be indexed and retrieved efficiently with an inverted index. We explore the application of LSR to the multi-modal domain, with a focus on text-image retrieval. While LSR has seen success in text retrieval, its application in multimodal retrieval remains underexplored. Current approaches like LexLIP and STAIR require complex multi-step training on massive datasets. Our proposed approach efficiently transforms dense vectors from a frozen dense model into sparse lexical vectors. We address issues of high dimension co-activation and semantic deviation through a new training algorithm, using Bernoulli random variables to control query expansion. Experiments with two dense models (BLIP, ALBEF) and two datasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively reduces co-activation and semantic deviation. Our best-performing sparsified model outperforms state-of-the-art text-image LSR models with a shorter training time and lower GPU memory requirements. Our approach offers an effective solution for training LSR retrieval models in multimodal settings. Our code and model checkpoints are available at github.com/thongnt99/lsr-multimodal","sentences":["Learned sparse retrieval (LSR) is a family of neural methods that encode queries and documents into sparse lexical vectors that can be indexed and retrieved efficiently with an inverted index.","We explore the application of LSR to the multi-modal domain, with a focus on text-image retrieval.","While LSR has seen success in text retrieval, its application in multimodal retrieval remains underexplored.","Current approaches like LexLIP and STAIR require complex multi-step training on massive datasets.","Our proposed approach efficiently transforms dense vectors from a frozen dense model into sparse lexical vectors.","We address issues of high dimension co-activation and semantic deviation through a new training algorithm, using Bernoulli random variables to control query expansion.","Experiments with two dense models (BLIP, ALBEF) and two datasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively reduces co-activation and semantic deviation.","Our best-performing sparsified model outperforms state-of-the-art text-image LSR models with a shorter training time and lower GPU memory requirements.","Our approach offers an effective solution for training LSR retrieval models in multimodal settings.","Our code and model checkpoints are available at github.com/thongnt99/lsr-multimodal"],"url":"http://arxiv.org/abs/2402.17535v1","category":"cs.IR"}
{"created":"2024-02-27 14:05:46","title":"Highway Discretionary Lane-change Decision and Control Using Model Predictive Control","abstract":"To enable vehicles to perform automatic lane change amidst the random traffic flow on highways, this paper introduces a decision-making and control method for vehicle lane-change based on Model Predictive Control (MPC). This approach divides the driving control of vehicles on highways into two parts: lane-change decision and lane-change control, both of which are solved using the MPC method. In the lane-change decision module, the minimum driving costs for each lane are computed and compared by solving the MPC problem to make lane-change decisions. In the lane-change control module, a dynamic bicycle model is incorporated, and a multi-objective cost function is designed to obtain the optimal control inputs for the lane-change process. The proposed lane-change decision and control methods are simulated and validated within the SUMO platform under random highway traffic conditions.","sentences":["To enable vehicles to perform automatic lane change amidst the random traffic flow on highways, this paper introduces a decision-making and control method for vehicle lane-change based on Model Predictive Control (MPC).","This approach divides the driving control of vehicles on highways into two parts: lane-change decision and lane-change control, both of which are solved using the MPC method.","In the lane-change decision module, the minimum driving costs for each lane are computed and compared by solving the MPC problem to make lane-change decisions.","In the lane-change control module, a dynamic bicycle model is incorporated, and a multi-objective cost function is designed to obtain the optimal control inputs for the lane-change process.","The proposed lane-change decision and control methods are simulated and validated within the SUMO platform under random highway traffic conditions."],"url":"http://arxiv.org/abs/2402.17524v1","category":"eess.SY"}
{"created":"2024-02-27 14:05:39","title":"Navigating Complexity: Constrained Portfolio Analysis in High Dimensions with Tracking Error and Weight Constraints","abstract":"This paper analyzes the statistical properties of constrained portfolio formation in a high dimensional portfolio with a large number of assets. Namely, we consider portfolios with tracking error constraints, portfolios with tracking error jointly with weight (equality or inequality) restrictions, and portfolios with only weight restrictions. Tracking error is the portfolio's performance measured against a benchmark (an index usually), {\\color{black}{and weight constraints refers to specific allocation of assets within the portfolio, which often come in the form of regulatory requirement or fund prospectus.}} We show how these portfolios can be estimated consistently in large dimensions, even when the number of assets is larger than the time span of the portfolio. We also provide rate of convergence results for weights of the constrained portfolio, risk of the constrained portfolio and the Sharpe Ratio of the constrained portfolio. To achieve those results we use a new machine learning technique that merges factor models with nodewise regression in statistics. Simulation results and empirics show very good performance of our method.","sentences":["This paper analyzes the statistical properties of constrained portfolio formation in a high dimensional portfolio with a large number of assets.","Namely, we consider portfolios with tracking error constraints, portfolios with tracking error jointly with weight (equality or inequality) restrictions, and portfolios with only weight restrictions.","Tracking error is the portfolio's performance measured against a benchmark (an index usually), {\\color{black}{and weight constraints refers to specific allocation of assets within the portfolio, which often come in the form of regulatory requirement or fund prospectus.}","} We show how these portfolios can be estimated consistently in large dimensions, even when the number of assets is larger than the time span of the portfolio.","We also provide rate of convergence results for weights of the constrained portfolio, risk of the constrained portfolio and the Sharpe Ratio of the constrained portfolio.","To achieve those results we use a new machine learning technique that merges factor models with nodewise regression in statistics.","Simulation results and empirics show very good performance of our method."],"url":"http://arxiv.org/abs/2402.17523v1","category":"q-fin.PM"}
{"created":"2024-02-27 14:05:34","title":"Digital Quantum Simulations of Hong-Ou-Mandel Interference","abstract":"Digital quantum simulation is the process of simulating the dynamics of a physical system by a programmable quantum computer. The universality of quantum computers makes it possible to simulate any quantum system, whether fermionic or bosonic. In this work, we discuss the application of digital quantum simulations to simulate a ubiquitous bosonic system, a beam splitter. To perform the boson-to-qubit mapping, we used the gray code, whose superiority over other encoding schemes has been shown recently. We validated our quantum circuit that mimics the action of a beam splitter by simulating the Hong-Ou-Mandel interference experiment. We simulated the experiment in both quantum simulators and actual quantum backends and were able to observe the HOM interference.","sentences":["Digital quantum simulation is the process of simulating the dynamics of a physical system by a programmable quantum computer.","The universality of quantum computers makes it possible to simulate any quantum system, whether fermionic or bosonic.","In this work, we discuss the application of digital quantum simulations to simulate a ubiquitous bosonic system, a beam splitter.","To perform the boson-to-qubit mapping, we used the gray code, whose superiority over other encoding schemes has been shown recently.","We validated our quantum circuit that mimics the action of a beam splitter by simulating the Hong-Ou-Mandel interference experiment.","We simulated the experiment in both quantum simulators and actual quantum backends and were able to observe the HOM interference."],"url":"http://arxiv.org/abs/2402.17522v1","category":"quant-ph"}
{"created":"2024-02-27 14:04:08","title":"Quantum Computing in Logistics and Supply Chain Management - an Overview","abstract":"The work explores the integration of quantum computing into logistics and supply chain management, emphasising its potential for use in complex optimisation problems. The discussion introduces quantum computing principles, focusing on quantum annealing and gate-based quantum computing, with the Quantum Approximate Optimisation Algorithm and Quantum Annealing as key algorithmic approaches.   The paper provides an overview of quantum approaches to routing, logistic network design, fleet maintenance, cargo loading, prediction, and scheduling problems. Notably, most solutions in the literature are hybrid, combining quantum and classical computing. The conclusion highlights the early stage of quantum computing, emphasising its potential impact on logistics and supply chain optimisation. In the final overview, the literature is categorised, identifying quantum annealing dominance and a need for more research in prediction and machine learning is highlighted. The consensus is that quantum computing has great potential but faces current hardware limitations, necessitating further advancements for practical implementation.","sentences":["The work explores the integration of quantum computing into logistics and supply chain management, emphasising its potential for use in complex optimisation problems.","The discussion introduces quantum computing principles, focusing on quantum annealing and gate-based quantum computing, with the Quantum Approximate Optimisation Algorithm and Quantum Annealing as key algorithmic approaches.   ","The paper provides an overview of quantum approaches to routing, logistic network design, fleet maintenance, cargo loading, prediction, and scheduling problems.","Notably, most solutions in the literature are hybrid, combining quantum and classical computing.","The conclusion highlights the early stage of quantum computing, emphasising its potential impact on logistics and supply chain optimisation.","In the final overview, the literature is categorised, identifying quantum annealing dominance and a need for more research in prediction and machine learning is highlighted.","The consensus is that quantum computing has great potential but faces current hardware limitations, necessitating further advancements for practical implementation."],"url":"http://arxiv.org/abs/2402.17520v1","category":"quant-ph"}
{"created":"2024-02-27 13:54:58","title":"Study of quantum non-locality by CHSH function and its extension in disordered fermions","abstract":"Quantum non-locality is an important concept in quantum physics. In this work, we study the quantum non-locality in a fermion many-body system under quasi-periodic disorders. The Clauser-Horne-Shimony-Holt (CHSH) inequality is systematically investigated, which quantifies quantum non-locality between two sites. We find that the quantum non-locality explicitly characterize the extended and critical phase transitions, and further that in the globally averaged picture of maximum value of the quantum non-locality the CHSH inequality is not broken, but for a local pair in the internal of the system the violation probability of the CHSH inequality becomes sufficiently finite. Further we investigate an extension of the CHSH inequality, Mermin-Klyshko-Svetlichny (MKS) polynomials, which can characterize multipartite quantum non-locality. We also find a similar behavior to the case of CHSH inequality. In particular, in the critical regime and on a transition point, the adjacent three qubit MKS polynomial in a portion of the system exhibits a quantum non-local violation regime with a finite probability.","sentences":["Quantum non-locality is an important concept in quantum physics.","In this work, we study the quantum non-locality in a fermion many-body system under quasi-periodic disorders.","The Clauser-Horne-Shimony-Holt (CHSH) inequality is systematically investigated, which quantifies quantum non-locality between two sites.","We find that the quantum non-locality explicitly characterize the extended and critical phase transitions, and further that in the globally averaged picture of maximum value of the quantum non-locality the CHSH inequality is not broken, but for a local pair in the internal of the system the violation probability of the CHSH inequality becomes sufficiently finite.","Further we investigate an extension of the CHSH inequality, Mermin-Klyshko-Svetlichny (MKS) polynomials, which can characterize multipartite quantum non-locality.","We also find a similar behavior to the case of CHSH inequality.","In particular, in the critical regime and on a transition point, the adjacent three qubit MKS polynomial in a portion of the system exhibits a quantum non-local violation regime with a finite probability."],"url":"http://arxiv.org/abs/2402.17513v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-27 13:47:23","title":"Interactive Multi-Head Self-Attention with Linear Complexity","abstract":"We propose an efficient interactive method for multi-head self-attention via decomposition. For existing methods using multi-head self-attention, the attention operation of each head is computed independently. However, we show that the interactions between cross-heads of the attention matrix enhance the information flow of the attention operation. Considering that the attention matrix of each head can be seen as a feature of networks, it is beneficial to establish connectivity between them to capture interactions better. However, a straightforward approach to capture the interactions between the cross-heads is computationally prohibitive as the complexity grows substantially with the high dimension of an attention matrix. In this work, we propose an effective method to decompose the attention operation into query- and key-less components. This will result in a more manageable size for the attention matrix, specifically for the cross-head interactions. Expensive experimental results show that the proposed cross-head interaction approach performs favorably against existing efficient attention methods and state-of-the-art backbone models.","sentences":["We propose an efficient interactive method for multi-head self-attention via decomposition.","For existing methods using multi-head self-attention, the attention operation of each head is computed independently.","However, we show that the interactions between cross-heads of the attention matrix enhance the information flow of the attention operation.","Considering that the attention matrix of each head can be seen as a feature of networks, it is beneficial to establish connectivity between them to capture interactions better.","However, a straightforward approach to capture the interactions between the cross-heads is computationally prohibitive as the complexity grows substantially with the high dimension of an attention matrix.","In this work, we propose an effective method to decompose the attention operation into query- and key-less components.","This will result in a more manageable size for the attention matrix, specifically for the cross-head interactions.","Expensive experimental results show that the proposed cross-head interaction approach performs favorably against existing efficient attention methods and state-of-the-art backbone models."],"url":"http://arxiv.org/abs/2402.17507v1","category":"cs.CV"}
{"created":"2024-02-27 13:21:32","title":"Few-body bound topological and flat-band states in a Creutz Ladder","abstract":"We investigate the properties of few interacting bosons in a Creutz ladder, which has become a standard model for topological systems, and which can be realised in experiments with cold atoms in optical lattices. At the single-particle level, this system may exhibit a completely flat energy landscape with non-trivial topological properties. In this scenario, we identify topological two-body edge states resulting from the bonding of single-particle edge and flat-band states. We also explore the formation of two- and three-body bound states in the strongly-interacting limit, and we show how these quasi-particles can be engineered to replicate the flat-band and topological features of the original single-particle model. Furthermore, we show that in this geometry perfect Aharonov-Bohm caging of two-body bound states may occur for arbitrary interaction strengths, and we provide numerical evidence that the main features of this effect are preserved in an interacting many-body scenario resulting in many-body Aharonov-Bohm caging.","sentences":["We investigate the properties of few interacting bosons in a Creutz ladder, which has become a standard model for topological systems, and which can be realised in experiments with cold atoms in optical lattices.","At the single-particle level, this system may exhibit a completely flat energy landscape with non-trivial topological properties.","In this scenario, we identify topological two-body edge states resulting from the bonding of single-particle edge and flat-band states.","We also explore the formation of two- and three-body bound states in the strongly-interacting limit, and we show how these quasi-particles can be engineered to replicate the flat-band and topological features of the original single-particle model.","Furthermore, we show that in this geometry perfect Aharonov-Bohm caging of two-body bound states may occur for arbitrary interaction strengths, and we provide numerical evidence that the main features of this effect are preserved in an interacting many-body scenario resulting in many-body Aharonov-Bohm caging."],"url":"http://arxiv.org/abs/2402.17494v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-27 13:18:00","title":"Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?","abstract":"Postoperative risk predictions can inform effective perioperative care management and planning. We aimed to assess whether clinical large language models (LLMs) can predict postoperative risks using clinical texts with various training strategies. The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth Israel Deaconess's MIMIC dataset. Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days. For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia. Three domain adaptation and finetuning strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives; incorporating labels with semi-supervised fine-tuning; and foundational modelling through multi-task learning. Model performance was compared using the area under the receiver operating characteristic curve (AUROC) and the area under the precision recall curve (AUPRC) for classification tasks, and mean squared error (MSE) and R2 for regression tasks. Pre-trained LLMs outperformed traditional word embeddings, with absolute maximal gains of 38.3% for AUROC and 14% for AUPRC. Adapting models further improved performance: (1) self-supervised finetuning by 3.2% for AUROC and 1.5% for AUPRC; (2) semi-supervised finetuning by 1.8% for AUROC and 2% for AUPRC, compared to self-supervised finetuning; (3) foundational modelling by 3.6% for AUROC and 2.6% for AUPRC, compared to self-supervised finetuning. Pre-trained clinical LLMs offer opportunities for postoperative risk predictions in unforeseen data, with peaks in foundational models indicating the potential of task-agnostic learning towards the generalizability of LLMs in perioperative care.","sentences":["Postoperative risk predictions can inform effective perioperative care management and planning.","We aimed to assess whether clinical large language models (LLMs) can predict postoperative risks using clinical texts with various training strategies.","The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021.","Methods were replicated on Beth Israel Deaconess's MIMIC dataset.","Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days.","For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia.","Three domain adaptation and finetuning strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives; incorporating labels with semi-supervised fine-tuning; and foundational modelling through multi-task learning.","Model performance was compared using the area under the receiver operating characteristic curve (AUROC) and the area under the precision recall curve (AUPRC) for classification tasks, and mean squared error (MSE) and R2 for regression tasks.","Pre-trained LLMs outperformed traditional word embeddings, with absolute maximal gains of 38.3% for AUROC and 14% for AUPRC.","Adapting models further improved performance: (1) self-supervised finetuning by 3.2% for AUROC and 1.5% for AUPRC; (2) semi-supervised finetuning by 1.8% for AUROC and 2% for AUPRC, compared to self-supervised finetuning; (3) foundational modelling by 3.6% for AUROC and 2.6% for AUPRC, compared to self-supervised finetuning.","Pre-trained clinical LLMs offer opportunities for postoperative risk predictions in unforeseen data, with peaks in foundational models indicating the potential of task-agnostic learning towards the generalizability of LLMs in perioperative care."],"url":"http://arxiv.org/abs/2402.17493v1","category":"cs.CL"}
{"created":"2024-02-27 13:17:19","title":"Collisional excitation of propyne (CH$_3$CCH) by He atoms","abstract":"A detailed interpretation of the detected emission lines of environments in which propyne (or methyl acetylene, CH$_3$CCH) is observed requires a knowledge of its collisional rate coefficients with the most abundant species in the interstellar medium, He or H$_2$. We present the first three-dimensional potential energy surface (3D-PES) for the CH$_3$CCH-He molecular complex, study the dynamics of the collision, and report the first set of rate coefficients for temperatures up to 100 K for the collisional excitation of the lowest 60 ortho rotational levels and 60 para rotational levels of CH$_3$CCH by He atoms. We computed the 3D-PES with the explicitly correlated coupled-cluster with single-, double-, and perturbative triple-excitation method, in conjunction with the augmented correlation-consistent triple zeta basis set (CCSD(T)-F12a/aug-cc-pVTZ). The 3D-PES was fitted to an analytical function. Scattering computations of pure rotational (de-)excitation of CH$_3$CCH by collision with He atoms were performed and the state-to-state cross sections were computed using the close coupling method for total energies up to 100 cm$^{-1}$ and with the coupled states approximation at higher energy for both ortho and para symmetries of CH$_3$CCH. The PES obtained is caracterized by a large anisotropy and a potential well depth of 51.04 cm$^{-1}$. By thermally averaging the collisional cross sections, we determined quenching rate coefficients for kinetic temperatures up to 100 K. A strong even $\\Delta j$ propensity rule at almost all collision energies exists for CH$_3$CCH-He complex. To evaluate the impact of rate coefficients in the analysis of observations, we carried out non-LTE radiative transfer computations of the excitation temperatures and we demonstrate that LTE conditions are typically not fulfilled for the propyne molecule.","sentences":["A detailed interpretation of the detected emission lines of environments in which propyne (or methyl acetylene, CH$_3$CCH) is observed requires a knowledge of its collisional rate coefficients with the most abundant species in the interstellar medium,","He or H$_2$. We present the first three-dimensional potential energy surface (3D-PES) for the CH$_3$CCH-He molecular complex, study the dynamics of the collision, and report the first set of rate coefficients for temperatures up to 100 K for the collisional excitation of the lowest 60 ortho rotational levels and 60 para rotational levels of CH$_3$CCH by He atoms.","We computed the 3D-PES with the explicitly correlated coupled-cluster with single-, double-, and perturbative triple-excitation method, in conjunction with the augmented correlation-consistent triple zeta basis set (CCSD(T)-F12a/aug-cc-pVTZ).","The 3D-PES was fitted to an analytical function.","Scattering computations of pure rotational (de-)excitation of CH$_3$CCH by collision with He atoms were performed and the state-to-state cross sections were computed using the close coupling method for total energies up to 100 cm$^{-1}$ and with the coupled states approximation at higher energy for both ortho and para symmetries of CH$_3$CCH.","The PES obtained is caracterized by a large anisotropy and a potential well depth of 51.04 cm$^{-1}$. By thermally averaging the collisional cross sections, we determined quenching rate coefficients for kinetic temperatures up to 100 K. A strong even $\\Delta j$ propensity rule at almost all collision energies exists for CH$_3$CCH-He complex.","To evaluate the impact of rate coefficients in the analysis of observations, we carried out non-LTE radiative transfer computations of the excitation temperatures and we demonstrate that LTE conditions are typically not fulfilled for the propyne molecule."],"url":"http://arxiv.org/abs/2402.17491v1","category":"astro-ph.GA"}
{"created":"2024-02-27 13:15:50","title":"SSRESF: Sensitivity-aware Single-particle Radiation Effects Simulation Framework in SoC Platforms based on SVM Algorithm","abstract":"The ever-expanding scale of integrated circuits has brought about a significant rise in the design risks associated with radiation-resistant integrated circuit chips. Traditional single-particle experimental methods, with their iterative design approach, are increasingly ill-suited for the challenges posed by large-scale integrated circuits. In response, this article introduces a novel sensitivity-aware single-particle radiation effects simulation framework tailored for System-on-Chip platforms. Based on SVM algorithm we have implemented fast finding and classification of sensitive circuit nodes. Additionally, the methodology automates soft error analysis across the entire software stack. The study includes practical experiments focusing on RISC-V architecture, encompassing core components, buses, and memory systems. It culminates in the establishment of databases for Single Event Upsets (SEU) and Single Event Transients (SET), showcasing the practical efficacy of the proposed methodology in addressing radiation-induced challenges at the scale of contemporary integrated circuits. Experimental results have shown up to 12.78X speed-up on the basis of achieving 94.58% accuracy.","sentences":["The ever-expanding scale of integrated circuits has brought about a significant rise in the design risks associated with radiation-resistant integrated circuit chips.","Traditional single-particle experimental methods, with their iterative design approach, are increasingly ill-suited for the challenges posed by large-scale integrated circuits.","In response, this article introduces a novel sensitivity-aware single-particle radiation effects simulation framework tailored for System-on-Chip platforms.","Based on SVM algorithm we have implemented fast finding and classification of sensitive circuit nodes.","Additionally, the methodology automates soft error analysis across the entire software stack.","The study includes practical experiments focusing on RISC-V architecture, encompassing core components, buses, and memory systems.","It culminates in the establishment of databases for Single Event Upsets (SEU) and Single Event Transients (SET), showcasing the practical efficacy of the proposed methodology in addressing radiation-induced challenges at the scale of contemporary integrated circuits.","Experimental results have shown up to 12.78X speed-up on the basis of achieving 94.58% accuracy."],"url":"http://arxiv.org/abs/2402.17489v1","category":"cs.AR"}
{"created":"2024-02-27 13:05:31","title":"Wave mechanics in an ionic liquid mixture","abstract":"Experimental measurements of interactions in ionic liquids and concentrated electrolytes over the past decade or so have revealed simultaneous monotonic and oscillatory decay modes. These observations have been hard to interpret using classical theories, which typically allow for just one electrostatic decay mode in electrolytes. Meanwhile, substantial progress in the theoretical description of dielectric response and ion correlations in electrolytes has illuminated the deep connection between density and charge correlations and the multiplicity of decay modes characterising a liquid electrolyte. The challenge in front of us is to build connections between the theoretical expressions for pair correlation functions and the directly measured free energy of interaction between macroscopic surfaces in experiments. Towards this aim, we here present measurements and analysis of the interactions between macroscopic bodies across a fluid mixture of two ionic liquids of widely diverging ionic size. The measured oscillatory interaction forces in the liquid mixtures are significantly more complex than for either of the pure ionic liquids, but can be fitted to a superposition of two oscillatory and one monotonic mode with parameters matching those of the pure liquids. We discuss this empirical finding, which hints at a kind of wave mechanics for interactions in liquid matter.","sentences":["Experimental measurements of interactions in ionic liquids and concentrated electrolytes over the past decade or so have revealed simultaneous monotonic and oscillatory decay modes.","These observations have been hard to interpret using classical theories, which typically allow for just one electrostatic decay mode in electrolytes.","Meanwhile, substantial progress in the theoretical description of dielectric response and ion correlations in electrolytes has illuminated the deep connection between density and charge correlations and the multiplicity of decay modes characterising a liquid electrolyte.","The challenge in front of us is to build connections between the theoretical expressions for pair correlation functions and the directly measured free energy of interaction between macroscopic surfaces in experiments.","Towards this aim, we here present measurements and analysis of the interactions between macroscopic bodies across a fluid mixture of two ionic liquids of widely diverging ionic size.","The measured oscillatory interaction forces in the liquid mixtures are significantly more complex than for either of the pure ionic liquids, but can be fitted to a superposition of two oscillatory and one monotonic mode with parameters matching those of the pure liquids.","We discuss this empirical finding, which hints at a kind of wave mechanics for interactions in liquid matter."],"url":"http://arxiv.org/abs/2402.17480v1","category":"cond-mat.soft"}
{"created":"2024-02-27 13:05:26","title":"Peeling Back the Layers of Extinction of Dusty Galaxies in the Era of JWST: Modelling Joint NIRSpec + MIRI Spectra at rest-frame 1.5 - 28 $\u03bc$m","abstract":"We present an analysis of the combined NIRSpec and MIRI spectra of dusty galaxies between 1.5 - 28 $\\mu$m restframe by implementing a differential extinction model, where the strength of extinction varies across the spectrum as different layers of the obscuring dust are probed. Our model is able to recover a 2D distribution of dust temperature and extinction which allows inference of the physical nature of the dust in these environments. We show that differential extinction is necessary to reproduce the spectra of 4 highly obscured Luminous Infrared Galaxies observed with NIRSpec IFU and MIRI MRS, where simple screen or uniformly mixed dust distributions fail to fit the data. We additionally compare the extinction of HII regions in these galaxies via hydrogen recombination lines, the extinction of molecular gas via the H$_2$ lines, Polycyclic Aromatic Hydrocarbons via the 12.7/11.3 PAH ratio and the stellar continuum. We find that the molecular gas is deeply buried with the HII regions in star-forming regions, with a similar extinction to the hottest dust components. However we find the cooler dust to be less obscured, at a similar extinction to the stellar continuum and PAHs. The nuclei show a complex dust distribution with VV114 NE, NGC 3256 S, IIZw96 SW showing a deeply buried continuum source relative to the molecular gas/HII regions. Additionally, NGC 3256 S, NGC 7469 and VV114 SW show an isolated hot dust component, indicative of AGN heating, where NGC 3256 S and NGC 7469 are previously known AGN.","sentences":["We present an analysis of the combined NIRSpec and MIRI spectra of dusty galaxies between 1.5 - 28 $\\mu$m restframe by implementing a differential extinction model, where the strength of extinction varies across the spectrum as different layers of the obscuring dust are probed.","Our model is able to recover a 2D distribution of dust temperature and extinction which allows inference of the physical nature of the dust in these environments.","We show that differential extinction is necessary to reproduce the spectra of 4 highly obscured Luminous Infrared Galaxies observed with NIRSpec IFU and MIRI MRS, where simple screen or uniformly mixed dust distributions fail to fit the data.","We additionally compare the extinction of HII regions in these galaxies via hydrogen recombination lines, the extinction of molecular gas via the H$_2$ lines, Polycyclic Aromatic Hydrocarbons via the 12.7/11.3 PAH ratio and the stellar continuum.","We find that the molecular gas is deeply buried with the HII regions in star-forming regions, with a similar extinction to the hottest dust components.","However we find the cooler dust to be less obscured, at a similar extinction to the stellar continuum and PAHs.","The nuclei show a complex dust distribution with VV114 NE, NGC 3256 S, IIZw96 SW showing a deeply buried continuum source relative to the molecular gas/HII regions.","Additionally, NGC 3256 S, NGC 7469 and VV114 SW show an isolated hot dust component, indicative of AGN heating, where NGC 3256 S and NGC 7469 are previously known AGN."],"url":"http://arxiv.org/abs/2402.17479v1","category":"astro-ph.GA"}
{"created":"2024-02-27 12:57:55","title":"Mating Siegel and Thurston quadratic polynomials","abstract":"We prove that a quadratic polynomial with a bounded type Siegel disk and a quadratic post-critically finite polynomial are always mateable.","sentences":["We prove that a quadratic polynomial with a bounded type Siegel disk and a quadratic post-critically finite polynomial are always mateable."],"url":"http://arxiv.org/abs/2402.17475v1","category":"math.DS"}
{"created":"2024-02-27 12:53:02","title":"Equilibria and Dynamics of two coupled chains of interacting dipoles","abstract":"We explore the energy transfer dynamics in an array of two chains of identical rigid interacting dipoles. A crossover between two different ground state (GS) equilibrium configurations is observed with varying distance between the two chains of the array. Linearizing around the GS configurations, we verify that interactions up to third nearest neighbors should be accounted for accurately describe the resulting dynamics. Starting with one of the GS, we excite the system by supplying it with an excess energy DK located initially on one of the dipoles. We study the time evolution of the array for different values of the system parameters b and DK. Our focus is hereby on two features of the energy propagation: the redistribution of the excess energy DK among the two chains and the energy localization along each chain. For typical parameter values, the array of dipoles reaches both the equipartition between the chains and the thermal equilibrium from the early stages of the time evolution. Nevertheless, there is a region in parameter space (b,DK) where even up to the long computation time of this study, the array does neither reach energy equipartition nor thermalization between chains. This fact is due to the existence of persistent chaotic breathers.","sentences":["We explore the energy transfer dynamics in an array of two chains of identical rigid interacting dipoles.","A crossover between two different ground state (GS) equilibrium configurations is observed with varying distance between the two chains of the array.","Linearizing around the GS configurations, we verify that interactions up to third nearest neighbors should be accounted for accurately describe the resulting dynamics.","Starting with one of the GS, we excite the system by supplying it with an excess energy DK located initially on one of the dipoles.","We study the time evolution of the array for different values of the system parameters b and DK.","Our focus is hereby on two features of the energy propagation: the redistribution of the excess energy DK among the two chains and the energy localization along each chain.","For typical parameter values, the array of dipoles reaches both the equipartition between the chains and the thermal equilibrium from the early stages of the time evolution.","Nevertheless, there is a region in parameter space (b,DK) where even up to the long computation time of this study, the array does neither reach energy equipartition nor thermalization between chains.","This fact is due to the existence of persistent chaotic breathers."],"url":"http://arxiv.org/abs/2402.17471v1","category":"physics.atm-clus"}
{"created":"2024-02-27 12:49:07","title":"Solitary cluster waves in periodic potentials: Formation, propagation, and soliton-mediated particle transport","abstract":"Transport processes in crowded periodic structures are often mediated by cooperative movements of particles forming clusters. Recent theoretical and experimental studies of driven Brownian motion of hard spheres showed that cluster-mediated transport in one-dimensional periodic potentials can proceed in form of solitary waves. We here give a comprehensive description of these solitons. Fundamental for our analysis is a static presoliton state, which is formed by a periodic arrangements of basic stable clusters. Their size follows from a geometric principle of minimum free space. Adding one particle to the presoliton state gives rise to solitons. We derive the minimal number of particles needed for soliton formation, number of solitons at larger particle numbers, soliton velocities and soliton-mediated particle currents. Incomplete relaxations of the basic clusters are responsible for an effective repulsive soliton-soliton interaction seen in measurements. Our results provide a theoretical basis for describing experiments on cluster-mediated particle transport in periodic potentials.","sentences":["Transport processes in crowded periodic structures are often mediated by cooperative movements of particles forming clusters.","Recent theoretical and experimental studies of driven Brownian motion of hard spheres showed that cluster-mediated transport in one-dimensional periodic potentials can proceed in form of solitary waves.","We here give a comprehensive description of these solitons.","Fundamental for our analysis is a static presoliton state, which is formed by a periodic arrangements of basic stable clusters.","Their size follows from a geometric principle of minimum free space.","Adding one particle to the presoliton state gives rise to solitons.","We derive the minimal number of particles needed for soliton formation, number of solitons at larger particle numbers, soliton velocities and soliton-mediated particle currents.","Incomplete relaxations of the basic clusters are responsible for an effective repulsive soliton-soliton interaction seen in measurements.","Our results provide a theoretical basis for describing experiments on cluster-mediated particle transport in periodic potentials."],"url":"http://arxiv.org/abs/2402.17469v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-27 12:27:18","title":"CLAPSep: Leveraging Contrastive Pre-trained Models for Multi-Modal Query-Conditioned Target Sound Extraction","abstract":"Universal sound separation (USS) aims to extract arbitrary types of sounds from real-world sound recordings. Language-queried target sound extraction (TSE) is an effective approach to achieving USS. Such systems consist of two components: a query network that converts user queries into conditional embeddings, and a separation network that extracts the target sound based on conditional embeddings. Existing methods mainly suffer from two issues: firstly, they require training a randomly initialized model from scratch, lacking the utilization of pre-trained models, and substantial data and computational resources are needed to ensure model convergence; secondly, existing methods need to jointly train a query network and a separation network, which tends to lead to overfitting. To address these issues, we build the CLAPSep model based on contrastive language-audio pre-trained model (CLAP). We achieve this by using a pre-trained text encoder of CLAP as the query network and introducing pre-trained audio encoder weights of CLAP into the separation network to fully utilize the prior knowledge embedded in the pre-trained model to assist in target sound extraction tasks. Extensive experimental results demonstrate that the proposed method saves training resources while ensuring the model's performance and generalizability. Additionally, we explore the model's ability to comprehensively utilize language/audio multi-modal and positive/negative multi-valent user queries, enhancing system performance while providing diversified application modes.","sentences":["Universal sound separation (USS) aims to extract arbitrary types of sounds from real-world sound recordings.","Language-queried target sound extraction (TSE) is an effective approach to achieving USS.","Such systems consist of two components: a query network that converts user queries into conditional embeddings, and a separation network that extracts the target sound based on conditional embeddings.","Existing methods mainly suffer from two issues: firstly, they require training a randomly initialized model from scratch, lacking the utilization of pre-trained models, and substantial data and computational resources are needed to ensure model convergence; secondly, existing methods need to jointly train a query network and a separation network, which tends to lead to overfitting.","To address these issues, we build the CLAPSep model based on contrastive language-audio pre-trained model (CLAP).","We achieve this by using a pre-trained text encoder of CLAP as the query network and introducing pre-trained audio encoder weights of CLAP into the separation network to fully utilize the prior knowledge embedded in the pre-trained model to assist in target sound extraction tasks.","Extensive experimental results demonstrate that the proposed method saves training resources while ensuring the model's performance and generalizability.","Additionally, we explore the model's ability to comprehensively utilize language/audio multi-modal and positive/negative multi-valent user queries, enhancing system performance while providing diversified application modes."],"url":"http://arxiv.org/abs/2402.17455v1","category":"eess.AS"}
{"created":"2024-02-27 12:12:48","title":"Altermagnetism in NiSi with non-collinear spins","abstract":"Recently, a new class of magnetic phenomenon, called altermagnetism, was proposed where the underlying spin configuration resembles antiferromagnetic structure, but the system violates \\textbf{PT} (PT: Parity times Time reversal) symmetry due to the alternation of crystalline symmetry across magnetic ions. Although the original idea was proposed for the collinear spin structure, a recent report by Cheong et al. has suggested that antiferromagnetic materials with non-collinear spin structure and local alternation of crystalline arrangement can also manifest altermagnetism. Besides breaking the \\textbf{PT} symmetry, altermagnetic compounds are also expected to exhibit anomalous Hall effects of odd orders. Here, we discuss possible candidates in this regard. One example is nickel monosilicide, which was recently shown to exhibit high temperature antiferromagnetism with non-collinear spin structure. It fulfills both criteria of breaking the \\textbf{PT} symmetry and linear anomalous Hall effect. In addition to NiSi, we also discuss two other potential experimental venues for the exploration of altermagnetic states.","sentences":["Recently, a new class of magnetic phenomenon, called altermagnetism, was proposed where the underlying spin configuration resembles antiferromagnetic structure, but the system violates \\textbf{PT} (PT: Parity times Time reversal) symmetry due to the alternation of crystalline symmetry across magnetic ions.","Although the original idea was proposed for the collinear spin structure, a recent report by Cheong et al. has suggested that antiferromagnetic materials with non-collinear spin structure and local alternation of crystalline arrangement can also manifest altermagnetism.","Besides breaking the \\textbf{PT} symmetry, altermagnetic compounds are also expected to exhibit anomalous Hall effects of odd orders.","Here, we discuss possible candidates in this regard.","One example is nickel monosilicide, which was recently shown to exhibit high temperature antiferromagnetism with non-collinear spin structure.","It fulfills both criteria of breaking the \\textbf{PT} symmetry and linear anomalous Hall effect.","In addition to NiSi, we also discuss two other potential experimental venues for the exploration of altermagnetic states."],"url":"http://arxiv.org/abs/2402.17451v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-27 11:47:02","title":"Nuclear spin relaxation mediated by donor-bound and free electrons in wide CdTe quantum wells","abstract":"The nuclear spin systems in CdTe/(Cd,Zn)Te and CdTe/(Cd,Mg)Te quantum wells (QW) are studied using a multistage technique combining optical pumping and Hanle effect-based detection. The samples demonstrate drastically different nuclear spin dynamics in zero and weak magnetic fields. In CdTe/(Cd,Zn)Te, the nuclear spin relaxation time is found to strongly increase with the magnetic field, growing from 3 s in zero field to tens of seconds in a field of 25 G. In CdTe/(Cd,Mg)Te the relaxation is an order of magnitude slower, and it is field-independent up to at least 70 G. The differences are attributed to the nuclear spin relaxation being mediated by different kinds of resident electrons in these QWs. In CdTe/(Cd,Mg)Te, a residual electron gas trapped in the QW largely determines the relaxation dynamics. In CdTe/(Cd,Zn)Te, the fast relaxation in zero field is due to interaction with localized donor-bound electrons. Nuclear spin diffusion barriers form around neutral donors when the external magnetic field exceeds the local nuclear field, which is about $B_L\\approx $0.4 G in CdTe. This inhibits nuclear spin diffusion towards the donors, slowing down relaxation. These findings are supported by theoretical modeling. In particular, we show that the formation of the diffusion barrier is made possible by several features specific to CdTe: (i) the large donor binding energy (about 10 meV), (ii) the low abundance of magnetic isotopes (only $\\approx$30% of nuclei have nonzero spin), and (iii) the absence of nuclear quadrupole interactions between nuclei. The two latter properties are also favorable to nuclear spin cooling via optical pumping followed by adiabatic demagnetization. Under non-optimized conditions we have reached sub-microkelvin nuclear spin temperatures in both samples, lower than all previous results obtained in GaAs.","sentences":["The nuclear spin systems in CdTe/(Cd,Zn)Te and CdTe/(Cd,Mg)Te quantum wells (QW) are studied using a multistage technique combining optical pumping and Hanle effect-based detection.","The samples demonstrate drastically different nuclear spin dynamics in zero and weak magnetic fields.","In CdTe/(Cd,Zn)Te, the nuclear spin relaxation time is found to strongly increase with the magnetic field, growing from 3 s in zero field to tens of seconds in a field of 25 G. In CdTe/(Cd,Mg)Te the relaxation is an order of magnitude slower, and it is field-independent up to at least 70 G. The differences are attributed to the nuclear spin relaxation being mediated by different kinds of resident electrons in these QWs.","In CdTe/(Cd,Mg)Te, a residual electron gas trapped in the QW largely determines the relaxation dynamics.","In CdTe/(Cd,Zn)Te, the fast relaxation in zero field is due to interaction with localized donor-bound electrons.","Nuclear spin diffusion barriers form around neutral donors when the external magnetic field exceeds the local nuclear field, which is about $B_L\\approx $0.4 G in CdTe.","This inhibits nuclear spin diffusion towards the donors, slowing down relaxation.","These findings are supported by theoretical modeling.","In particular, we show that the formation of the diffusion barrier is made possible by several features specific to CdTe: (i) the large donor binding energy (about 10 meV), (ii) the low abundance of magnetic isotopes (only $\\approx$30% of nuclei have nonzero spin), and (iii) the absence of nuclear quadrupole interactions between nuclei.","The two latter properties are also favorable to nuclear spin cooling via optical pumping followed by adiabatic demagnetization.","Under non-optimized conditions we have reached sub-microkelvin nuclear spin temperatures in both samples, lower than all previous results obtained in GaAs."],"url":"http://arxiv.org/abs/2402.17435v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-27 11:45:46","title":"Passive Aligning Physical Interaction of Fully-Actuated Aerial Vehicles for Pushing Tasks","abstract":"Recently, the utilization of aerial manipulators for performing pushing tasks in non-destructive testing (NDT) applications has seen significant growth. Such operations entail physical interactions between the aerial robotic system and the environment. End-effectors with multiple contact points are often used for placing NDT sensors in contact with a surface to be inspected. Aligning the NDT sensor and the work surface while preserving contact, requires that all available contact points at the end-effector tip are in contact with the work surface. With a standard full-pose controller, attitude errors often occur due to perturbations caused by modeling uncertainties, sensor noise, and environmental uncertainties. Even small attitude errors can cause a loss of contact points between the end-effector tip and the work surface. To preserve full alignment amidst these uncertainties, we propose a control strategy which selectively deactivates angular motion control and enables direct force control in specific directions. In particular, we derive two essential conditions to be met, such that the robot can passively align with flat work surfaces achieving full alignment through the rotation along non-actively controlled axes. Additionally, these conditions serve as hardware design and control guidelines for effectively integrating the proposed control method for practical usage. Real world experiments are conducted to validate both the control design and the guidelines.","sentences":["Recently, the utilization of aerial manipulators for performing pushing tasks in non-destructive testing (NDT) applications has seen significant growth.","Such operations entail physical interactions between the aerial robotic system and the environment.","End-effectors with multiple contact points are often used for placing NDT sensors in contact with a surface to be inspected.","Aligning the NDT sensor and the work surface while preserving contact, requires that all available contact points at the end-effector tip are in contact with the work surface.","With a standard full-pose controller, attitude errors often occur due to perturbations caused by modeling uncertainties, sensor noise, and environmental uncertainties.","Even small attitude errors can cause a loss of contact points between the end-effector tip and the work surface.","To preserve full alignment amidst these uncertainties, we propose a control strategy which selectively deactivates angular motion control and enables direct force control in specific directions.","In particular, we derive two essential conditions to be met, such that the robot can passively align with flat work surfaces achieving full alignment through the rotation along non-actively controlled axes.","Additionally, these conditions serve as hardware design and control guidelines for effectively integrating the proposed control method for practical usage.","Real world experiments are conducted to validate both the control design and the guidelines."],"url":"http://arxiv.org/abs/2402.17434v1","category":"cs.RO"}
{"created":"2024-02-27 11:32:37","title":"ViTaL: An Advanced Framework for Automated Plant Disease Identification in Leaf Images Using Vision Transformers and Linear Projection For Feature Reduction","abstract":"Our paper introduces a robust framework for the automated identification of diseases in plant leaf images. The framework incorporates several key stages to enhance disease recognition accuracy. In the pre-processing phase, a thumbnail resizing technique is employed to resize images, minimizing the loss of critical image details while ensuring computational efficiency. Normalization procedures are applied to standardize image data before feature extraction. Feature extraction is facilitated through a novel framework built upon Vision Transformers, a state-of-the-art approach in image analysis. Additionally, alternative versions of the framework with an added layer of linear projection and blockwise linear projections are explored. This comparative analysis allows for the evaluation of the impact of linear projection on feature extraction and overall model performance. To assess the effectiveness of the proposed framework, various Convolutional Neural Network (CNN) architectures are utilized, enabling a com- prehensive evaluation of linear projection's influence on key evaluation metrics. The findings demonstrate the efficacy of the proposed framework, with the top- performing model achieving a Hamming loss of 0.054. Furthermore, we propose a novel hardware design specifically tailored for scanning diseased leaves in an omnidirectional fashion. The hardware implementation utilizes a Raspberry Pi Compute Module to address low-memory configurations, ensuring practicality and affordability. This innovative hardware solution enhances the overall feasibility and accessibility of the proposed automated disease identification system. This research contributes to the field of agriculture by offering valuable insights and tools for the early detection and management of plant diseases, potentially leading to improved crop yields and enhanced food security.","sentences":["Our paper introduces a robust framework for the automated identification of diseases in plant leaf images.","The framework incorporates several key stages to enhance disease recognition accuracy.","In the pre-processing phase, a thumbnail resizing technique is employed to resize images, minimizing the loss of critical image details while ensuring computational efficiency.","Normalization procedures are applied to standardize image data before feature extraction.","Feature extraction is facilitated through a novel framework built upon Vision Transformers, a state-of-the-art approach in image analysis.","Additionally, alternative versions of the framework with an added layer of linear projection and blockwise linear projections are explored.","This comparative analysis allows for the evaluation of the impact of linear projection on feature extraction and overall model performance.","To assess the effectiveness of the proposed framework, various Convolutional Neural Network (CNN) architectures are utilized, enabling a com- prehensive evaluation of linear projection's influence on key evaluation metrics.","The findings demonstrate the efficacy of the proposed framework, with the top- performing model achieving a Hamming loss of 0.054.","Furthermore, we propose a novel hardware design specifically tailored for scanning diseased leaves in an omnidirectional fashion.","The hardware implementation utilizes a Raspberry Pi Compute Module to address low-memory configurations, ensuring practicality and affordability.","This innovative hardware solution enhances the overall feasibility and accessibility of the proposed automated disease identification system.","This research contributes to the field of agriculture by offering valuable insights and tools for the early detection and management of plant diseases, potentially leading to improved crop yields and enhanced food security."],"url":"http://arxiv.org/abs/2402.17424v1","category":"cs.CV"}
{"created":"2024-02-27 11:21:30","title":"Comparison of Distances and Entropic Distinguishability Quantifiers for the Detection of Memory Effects","abstract":"We consider a recently introduced framework for the description of memory effects based on quantum state distinguishability quantifiers, in which entropic quantifiers can be included. After briefly presenting the approach, we validate it considering the performance of different quantifiers in the characterization of the reduced dynamics of a two-level system undergoing decoherence. We investigate the different behavior of these quantifiers in the dependence on physical features of the model, such as environmental temperature and coupling strength. It appears that the performance of the different quantifiers conveys the same physical information, though with different sensitivities, thus supporting robustness of the approach.","sentences":["We consider a recently introduced framework for the description of memory effects based on quantum state distinguishability quantifiers, in which entropic quantifiers can be included.","After briefly presenting the approach, we validate it considering the performance of different quantifiers in the characterization of the reduced dynamics of a two-level system undergoing decoherence.","We investigate the different behavior of these quantifiers in the dependence on physical features of the model, such as environmental temperature and coupling strength.","It appears that the performance of the different quantifiers conveys the same physical information, though with different sensitivities, thus supporting robustness of the approach."],"url":"http://arxiv.org/abs/2402.17419v1","category":"quant-ph"}
{"created":"2024-02-27 11:17:46","title":"CARZero: Cross-Attention Alignment for Radiology Zero-Shot Classification","abstract":"The advancement of Zero-Shot Learning in the medical domain has been driven forward by using pre-trained models on large-scale image-text pairs, focusing on image-text alignment. However, existing methods primarily rely on cosine similarity for alignment, which may not fully capture the complex relationship between medical images and reports. To address this gap, we introduce a novel approach called Cross-Attention Alignment for Radiology Zero-Shot Classification (CARZero). Our approach innovatively leverages cross-attention mechanisms to process image and report features, creating a Similarity Representation that more accurately reflects the intricate relationships in medical semantics. This representation is then linearly projected to form an image-text similarity matrix for cross-modality alignment. Additionally, recognizing the pivotal role of prompt selection in zero-shot learning, CARZero incorporates a Large Language Model-based prompt alignment strategy. This strategy standardizes diverse diagnostic expressions into a unified format for both training and inference phases, overcoming the challenges of manual prompt design. Our approach is simple yet effective, demonstrating state-of-the-art performance in zero-shot classification on five official chest radiograph diagnostic test sets, including remarkable results on datasets with long-tail distributions of rare diseases. This achievement is attributed to our new image-text alignment strategy, which effectively addresses the complex relationship between medical images and reports.","sentences":["The advancement of Zero-Shot Learning in the medical domain has been driven forward by using pre-trained models on large-scale image-text pairs, focusing on image-text alignment.","However, existing methods primarily rely on cosine similarity for alignment, which may not fully capture the complex relationship between medical images and reports.","To address this gap, we introduce a novel approach called Cross-Attention Alignment for Radiology Zero-Shot Classification (CARZero).","Our approach innovatively leverages cross-attention mechanisms to process image and report features, creating a Similarity Representation that more accurately reflects the intricate relationships in medical semantics.","This representation is then linearly projected to form an image-text similarity matrix for cross-modality alignment.","Additionally, recognizing the pivotal role of prompt selection in zero-shot learning, CARZero incorporates a Large Language Model-based prompt alignment strategy.","This strategy standardizes diverse diagnostic expressions into a unified format for both training and inference phases, overcoming the challenges of manual prompt design.","Our approach is simple yet effective, demonstrating state-of-the-art performance in zero-shot classification on five official chest radiograph diagnostic test sets, including remarkable results on datasets with long-tail distributions of rare diseases.","This achievement is attributed to our new image-text alignment strategy, which effectively addresses the complex relationship between medical images and reports."],"url":"http://arxiv.org/abs/2402.17417v1","category":"cs.CV"}
{"created":"2024-02-27 11:14:14","title":"Semiclassical approach to spin dynamics of a ferromagnetic S=1 chain","abstract":"Motivated by recent experimental progress in the quasi-one-dimensional quantum magnet NiNb$_2$O$_6$, we study the spin dynamics of an S=1 ferromagnetic Heisenberg chain with single-ion anisotropy by using a semiclassical molecular dynamics approach. This system undergoes a quantum phase transition from a ferromagnetic to a paramagnetic state under a transverse magnetic field, and the magnetic responses reflecting this transition is well described by our semiclassical method. We show that at low-temperature the transverse component of the dynamical structure factor depicts clearly the magnon dispersion, and the longitudinal component exhibits two continua associated with single- and two-magnon excitations, respectively. These spin excitation spectra show interesting temperature dependence as effects of magnon interactions.Our findings shed light on experimental detection of spin excitations in a large class of quasi-one-dimensional magnets.","sentences":["Motivated by recent experimental progress in the quasi-one-dimensional quantum magnet NiNb$_2$O$_6$, we study the spin dynamics of an S=1 ferromagnetic Heisenberg chain with single-ion anisotropy by using a semiclassical molecular dynamics approach.","This system undergoes a quantum phase transition from a ferromagnetic to a paramagnetic state under a transverse magnetic field, and the magnetic responses reflecting this transition is well described by our semiclassical method.","We show that at low-temperature the transverse component of the dynamical structure factor depicts clearly the magnon dispersion, and the longitudinal component exhibits two continua associated with single- and two-magnon excitations, respectively.","These spin excitation spectra show interesting temperature dependence as effects of magnon interactions.","Our findings shed light on experimental detection of spin excitations in a large class of quasi-one-dimensional magnets."],"url":"http://arxiv.org/abs/2402.17416v1","category":"cond-mat.str-el"}
{"created":"2024-02-27 11:10:33","title":"Rate Function Modelling of Quantum Many-Body Adiabaticity","abstract":"The quantum adiabatic theorem is a fundamental result in quantum mechanics, which has a multitude of applications, both theoretical and practical. Here, we investigate the dynamics of adiabatic processes for interacting quantum many-body systems by analysing the properties of observable-free, intensive quantities. In particular, we study the rate function $f(T)$ in dependence of the ramp time $T$, which gives us a complete characterization of the many-body adiabatic fidelity as a function of $T$ and the strength of the parameter displacement $\\Delta \\lambda$. This allows us to control and define the notion of adiabaticity in many-body systems. Several key results in the literature regarding the interplay of the thermodynamic and the adiabatic limit are obtained as inferences from the properties of $f(T)$ in the large $T$ limit.","sentences":["The quantum adiabatic theorem is a fundamental result in quantum mechanics, which has a multitude of applications, both theoretical and practical.","Here, we investigate the dynamics of adiabatic processes for interacting quantum many-body systems by analysing the properties of observable-free, intensive quantities.","In particular, we study the rate function $f(T)$ in dependence of the ramp time $T$, which gives us a complete characterization of the many-body adiabatic fidelity as a function of $T$ and the strength of the parameter displacement $\\Delta \\lambda$.","This allows us to control and define the notion of adiabaticity in many-body systems.","Several key results in the literature regarding the interplay of the thermodynamic and the adiabatic limit are obtained as inferences from the properties of $f(T)$ in the large $T$ limit."],"url":"http://arxiv.org/abs/2402.17415v1","category":"quant-ph"}
{"created":"2024-02-27 10:51:18","title":"Underwater Acoustic Source Seeking Using Time-Difference-of-Arrival Measurements","abstract":"The research presented in this paper is aimed at developing a control algorithm for an autonomous surface system carrying a two-sensor array consisting of two acoustic receivers, capable of measuring the time-difference-of-arrival (TDOA) of a quasiperiodic underwater acoustic signal and utilizing this value to steer the system toward the acoustic source in the horizontal plane. Stability properties of the proposed algorithm are analyzed using the Lie bracket approximation technique. Furthermore, simulation results are presented, where particular attention is given to the relationship between the time difference of arrival measurement noise and the sensor baseline - the distance between the two acoustic receivers. Also, the influence of a constant disturbance caused by sea currents is considered. Finally, experimental results in which the algorithm was deployed on two autonomous surface vehicles, each equipped with a single acoustic receiver, are presented. The algorithm successfully steers the vehicle formation toward the acoustic source, despite the measurement noise and intermittent measurements, thus showing the feasibility of the proposed algorithm in real-life conditions.","sentences":["The research presented in this paper is aimed at developing a control algorithm for an autonomous surface system carrying a two-sensor array consisting of two acoustic receivers, capable of measuring the time-difference-of-arrival (TDOA) of a quasiperiodic underwater acoustic signal and utilizing this value to steer the system toward the acoustic source in the horizontal plane.","Stability properties of the proposed algorithm are analyzed using the Lie bracket approximation technique.","Furthermore, simulation results are presented, where particular attention is given to the relationship between the time difference of arrival measurement noise and the sensor baseline - the distance between the two acoustic receivers.","Also, the influence of a constant disturbance caused by sea currents is considered.","Finally, experimental results in which the algorithm was deployed on two autonomous surface vehicles, each equipped with a single acoustic receiver, are presented.","The algorithm successfully steers the vehicle formation toward the acoustic source, despite the measurement noise and intermittent measurements, thus showing the feasibility of the proposed algorithm in real-life conditions."],"url":"http://arxiv.org/abs/2402.17405v1","category":"cs.RO"}
{"created":"2024-02-27 10:48:56","title":"Beacon, a lightweight deep reinforcement learning benchmark library for flow control","abstract":"Recently, the increasing use of deep reinforcement learning for flow control problems has led to a new area of research, focused on the coupling and the adaptation of the existing algorithms to the control of numerical fluid dynamics environments. Although still in its infancy, the field has seen multiple successes in a short time span, and its fast development pace can certainly be partly imparted to the open-source effort that drives the expansion of the community. Yet, this emerging domain still misses a common ground to (i) ensure the reproducibility of the results, and (ii) offer a proper ad-hoc benchmarking basis. To this end, we propose Beacon, an open-source benchmark library composed of seven lightweight 1D and 2D flow control problems with various characteristics, action and observation space characteristics, and CPU requirements. In this contribution, the seven considered problems are described, and reference control solutions are provided. The sources for the following work are available at https://github.com/jviquerat/beacon.","sentences":["Recently, the increasing use of deep reinforcement learning for flow control problems has led to a new area of research, focused on the coupling and the adaptation of the existing algorithms to the control of numerical fluid dynamics environments.","Although still in its infancy, the field has seen multiple successes in a short time span, and its fast development pace can certainly be partly imparted to the open-source effort that drives the expansion of the community.","Yet, this emerging domain still misses a common ground to (i) ensure the reproducibility of the results, and (ii) offer a proper ad-hoc benchmarking basis.","To this end, we propose Beacon, an open-source benchmark library composed of seven lightweight 1D and 2D flow control problems with various characteristics, action and observation space characteristics, and CPU requirements.","In this contribution, the seven considered problems are described, and reference control solutions are provided.","The sources for the following work are available at https://github.com/jviquerat/beacon."],"url":"http://arxiv.org/abs/2402.17402v1","category":"physics.comp-ph"}
{"created":"2024-02-27 10:47:22","title":"Novel Ternary AgIICoIIIF5 Fluoride: Synthesis, Structure and Magnetic Characteristics","abstract":"We present a new compound in the silver cobalt fluoride system, featuring paramagnetic silver (d9) and high-spin cobalt (d6), synthesized by solid state method in an autoclave under F2 overpressure. Based on powder X ray diffraction, we determined that AgIICoIIIF5 crystallizes in a monoclinic system with space group C2/c. The calculated fundamental band gap falls in the visible range of the electromagnetic spectrum, and the compound has the character of charge-transfer insulator. AgCoF5 is a ferrimagnet with one predominant superexchange magnetic interaction constant between mixed spin cations (Ag ... Co) of minus 62 meV (SCAN result). Magnetometric measurements conducted on a powdered sample allowed the identification of a transition at 128 K, which could indicate magnetic ordering.","sentences":["We present a new compound in the silver cobalt fluoride system, featuring paramagnetic silver (d9) and high-spin cobalt (d6), synthesized by solid state method in an autoclave under F2 overpressure.","Based on powder X ray diffraction, we determined that AgIICoIIIF5 crystallizes in a monoclinic system with space group C2/c.","The calculated fundamental band gap falls in the visible range of the electromagnetic spectrum, and the compound has the character of charge-transfer insulator.","AgCoF5 is a ferrimagnet with one predominant superexchange magnetic interaction constant between mixed spin cations (Ag ... Co) of minus 62 meV (SCAN result).","Magnetometric measurements conducted on a powdered sample allowed the identification of a transition at 128 K, which could indicate magnetic ordering."],"url":"http://arxiv.org/abs/2402.17399v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-27 10:43:51","title":"Electron-beam annealing of Josephson junctions for frequency tuning of quantum processors","abstract":"Superconducting qubits are a promising route to achieving large-scale quantum computers. A key challenge in realising large-scale superconducting quantum processors involves mitigating frequency collisions. In this paper, we present an approach to tuning fixed-frequency qubits with the use of an electron beam to locally anneal the Josephson junction. We demonstrate the ability to both increase and decrease the junction barrier resistance. The technique shows an improvement in wafer scale frequency targetting by assessing the frequency collisions in our qubit architecture. Coherence measurements are also done to evaluate the performance before and after tuning. The tuning process utilises a standard electron beam lithography system, ensuring reproducibility and implementation by any group capable of fabricating these Josephson junctions. This technique has the potential to significantly improve the performance of large-scale quantum computing systems, thereby paving the way for the future of quantum computing.","sentences":["Superconducting qubits are a promising route to achieving large-scale quantum computers.","A key challenge in realising large-scale superconducting quantum processors involves mitigating frequency collisions.","In this paper, we present an approach to tuning fixed-frequency qubits with the use of an electron beam to locally anneal the Josephson junction.","We demonstrate the ability to both increase and decrease the junction barrier resistance.","The technique shows an improvement in wafer scale frequency targetting by assessing the frequency collisions in our qubit architecture.","Coherence measurements are also done to evaluate the performance before and after tuning.","The tuning process utilises a standard electron beam lithography system, ensuring reproducibility and implementation by any group capable of fabricating these Josephson junctions.","This technique has the potential to significantly improve the performance of large-scale quantum computing systems, thereby paving the way for the future of quantum computing."],"url":"http://arxiv.org/abs/2402.17395v1","category":"quant-ph"}
{"created":"2024-02-27 10:12:47","title":"Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control","abstract":"Integral reinforcement learning (IntRL) demands the precise computation of the utility function's integral at its policy evaluation (PEV) stage. This is achieved through quadrature rules, which are weighted sums of utility functions evaluated from state samples obtained in discrete time. Our research reveals a critical yet underexplored phenomenon: the choice of the computational method -- in this case, the quadrature rule -- can significantly impact control performance. This impact is traced back to the fact that computational errors introduced in the PEV stage can affect the policy iteration's convergence behavior, which in turn affects the learned controller. To elucidate how computation impacts control, we draw a parallel between IntRL's policy iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation. In this light, computational error in PEV manifests as an extra error term in each iteration of Newton's method, with its upper bound proportional to the computational error. Further, we demonstrate that when the utility function resides in a reproducing kernel Hilbert space (RKHS), the optimal quadrature is achievable by employing Bayesian quadrature with the RKHS-inducing kernel function. We prove that the local convergence rates for IntRL using the trapezoidal rule and Bayesian quadrature with a Mat\\'ern kernel to be $O(N^{-2})$ and $O(N^{-b})$, where $N$ is the number of evenly-spaced samples and $b$ is the Mat\\'ern kernel's smoothness parameter. These theoretical findings are finally validated by two canonical control tasks.","sentences":["Integral reinforcement learning (IntRL) demands the precise computation of the utility function's integral at its policy evaluation (PEV) stage.","This is achieved through quadrature rules, which are weighted sums of utility functions evaluated from state samples obtained in discrete time.","Our research reveals a critical yet underexplored phenomenon: the choice of the computational method -- in this case, the quadrature rule -- can significantly impact control performance.","This impact is traced back to the fact that computational errors introduced in the PEV stage can affect the policy iteration's convergence behavior, which in turn affects the learned controller.","To elucidate how computation impacts control, we draw a parallel between IntRL's policy iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation.","In this light, computational error in PEV manifests as an extra error term in each iteration of Newton's method, with its upper bound proportional to the computational error.","Further, we demonstrate that when the utility function resides in a reproducing kernel Hilbert space (RKHS), the optimal quadrature is achievable by employing Bayesian quadrature with the RKHS-inducing kernel function.","We prove that the local convergence rates for IntRL using the trapezoidal rule and Bayesian quadrature with a Mat\\'ern kernel to be $O(N^{-2})$ and $O(N^{-b})$, where $N$ is the number of evenly-spaced samples and $b$ is the Mat\\'ern kernel's smoothness parameter.","These theoretical findings are finally validated by two canonical control tasks."],"url":"http://arxiv.org/abs/2402.17375v1","category":"eess.SY"}
{"created":"2024-02-27 10:08:16","title":"Computing Functions of Symmetric Hierarchically Semiseparable Matrices","abstract":"The aim of this work is to develop a fast algorithm for approximating the matrix function $f(A)$ of a square matrix $A$ that is symmetric and has hierarchically semiseparable (HSS) structure. Appearing in a wide variety of applications, often in the context of discretized (fractional) differential and integral operators, HSS matrices have a number of attractive properties facilitating the development of fast algorithms. In this work, we use an unconventional telescopic decomposition of $A$, inspired by recent work of Levitt and Martinsson on approximating an HSS matrix from matrix-vector products with a few random vectors. This telescopic decomposition allows us to approximate $f(A)$ by recursively performing low-rank updates with rational Krylov subspaces while keeping the size of the matrices involved in the rational Krylov subspaces small. In particular, no large-scale linear system needs to be solved, which yields favorable complexity estimates and reduced execution times compared to existing methods, including an existing divide-and-conquer strategy. The advantages of our newly proposed algorithms are demonstrated for a number of examples from the literature, featuring the exponential, the inverse square root, and the sign function of a matrix. Even for matrix inversion, our algorithm exhibits superior performance, even if not specifically designed for this task.","sentences":["The aim of this work is to develop a fast algorithm for approximating the matrix function $f(A)$ of a square matrix $A$ that is symmetric and has hierarchically semiseparable (HSS) structure.","Appearing in a wide variety of applications, often in the context of discretized (fractional) differential and integral operators, HSS matrices have a number of attractive properties facilitating the development of fast algorithms.","In this work, we use an unconventional telescopic decomposition of $A$, inspired by recent work of Levitt and Martinsson on approximating an HSS matrix from matrix-vector products with a few random vectors.","This telescopic decomposition allows us to approximate $f(A)$ by recursively performing low-rank updates with rational Krylov subspaces while keeping the size of the matrices involved in the rational Krylov subspaces small.","In particular, no large-scale linear system needs to be solved, which yields favorable complexity estimates and reduced execution times compared to existing methods, including an existing divide-and-conquer strategy.","The advantages of our newly proposed algorithms are demonstrated for a number of examples from the literature, featuring the exponential, the inverse square root, and the sign function of a matrix.","Even for matrix inversion, our algorithm exhibits superior performance, even if not specifically designed for this task."],"url":"http://arxiv.org/abs/2402.17369v1","category":"math.NA"}
{"created":"2024-02-27 09:51:42","title":"A robust parameterized enhanced shift-splitting preconditioner for three-by-three block saddle point problems","abstract":"This paper proposes a new parameterized enhanced shift-splitting {\\it (PESS)} preconditioner to solve the three-by-three block saddle point problem ({\\it SPP}). In addition, necessary and sufficient criteria are established for the convergence of the proposed {\\it PESS} iterative process for any random initial guess. Furthermore, we meticulously investigate the spectral bounds of the {\\it PESS} preconditioned matrix. Moreover, empirical investigation has been performed for the sensitivity analysis of the system, revealing the robustness of the proposed {\\it PESS} preconditioner. Numerical experiments are carried out to demonstrate the enhanced efficiency and robustness of the proposed {\\it PESS} preconditioner compared to the existing block diagonal and shift-splitting preconditioners.","sentences":["This paper proposes a new parameterized enhanced shift-splitting {\\it (PESS)} preconditioner to solve the three-by-three block saddle point problem ({\\it SPP}).","In addition, necessary and sufficient criteria are established for the convergence of the proposed {\\it PESS} iterative process for any random initial guess.","Furthermore, we meticulously investigate the spectral bounds of the {\\it PESS} preconditioned matrix.","Moreover, empirical investigation has been performed for the sensitivity analysis of the system, revealing the robustness of the proposed {\\it PESS} preconditioner.","Numerical experiments are carried out to demonstrate the enhanced efficiency and robustness of the proposed {\\it PESS} preconditioner compared to the existing block diagonal and shift-splitting preconditioners."],"url":"http://arxiv.org/abs/2402.17357v1","category":"math.NA"}
{"created":"2024-02-27 09:46:53","title":"Comment on \"Mode Structure and Orbital Angular Momentum of Spatiotemporal Optical Vortex (STOV) Pulses\"","abstract":"We report a mathematical error and a misinterpretation in arXiv:2103.03263v4 [Phys. Rev. Lett. {\\bf 127}, 193901 (2021)] that has led to a debate about the nature of the transverse orbital angular momentum (OAM) of spatiotemporal optical vortices (STOVs). The transverse OAM of STOVs evaluated theoretically in that Letter is actually only the intrinsic contribution, while the operators used to evaluate the intrinsic and extrinsic contributions are not Hermitian operators as they may lead to complex-valued expectation values.","sentences":["We report a mathematical error and a misinterpretation in arXiv:2103.03263v4","[Phys. Rev. Lett.","{\\bf 127}, 193901 (2021)] that has led to a debate about the nature of the transverse orbital angular momentum (OAM) of spatiotemporal optical vortices (STOVs).","The transverse OAM of STOVs evaluated theoretically in that Letter is actually only the intrinsic contribution, while the operators used to evaluate the intrinsic and extrinsic contributions are not Hermitian operators as they may lead to complex-valued expectation values."],"url":"http://arxiv.org/abs/2402.17354v1","category":"physics.optics"}
{"created":"2024-02-27 09:38:51","title":"Towards an Enforceable GDPR Specification","abstract":"While Privacy by Design (PbD) is prescribed by modern privacy regulations such as the EU's GDPR, achieving PbD in real software systems is a notoriously difficult task. One emerging technique to realize PbD is Runtime enforcement (RE), in which an enforcer, loaded with a specification of a system's privacy requirements, observes the actions performed by the system and instructs it to perform actions that will ensure compliance with these requirements at all times. To be able to use RE techniques for PbD, privacy regulations first need to be translated into an enforceable specification. In this paper, we report on our ongoing work in formalizing the GDPR. We first present a set of requirements and an iterative methodology for creating enforceable formal specifications of legal provisions. Then, we report on a preliminary case study in which we used our methodology to derive an enforceable specification of part of the GDPR. Our case study suggests that our methodology can be effectively used to develop accurate enforceable specifications.","sentences":["While Privacy by Design (PbD) is prescribed by modern privacy regulations such as the EU's GDPR, achieving PbD in real software systems is a notoriously difficult task.","One emerging technique to realize PbD is Runtime enforcement (RE), in which an enforcer, loaded with a specification of a system's privacy requirements, observes the actions performed by the system and instructs it to perform actions that will ensure compliance with these requirements at all times.","To be able to use RE techniques for PbD, privacy regulations first need to be translated into an enforceable specification.","In this paper, we report on our ongoing work in formalizing the GDPR.","We first present a set of requirements and an iterative methodology for creating enforceable formal specifications of legal provisions.","Then, we report on a preliminary case study in which we used our methodology to derive an enforceable specification of part of the GDPR.","Our case study suggests that our methodology can be effectively used to develop accurate enforceable specifications."],"url":"http://arxiv.org/abs/2402.17350v1","category":"cs.CR"}
{"created":"2024-02-27 09:23:13","title":"Enhanced Bayesian Optimization via Preferential Modeling of Abstract Properties","abstract":"Experimental (design) optimization is a key driver in designing and discovering new products and processes. Bayesian Optimization (BO) is an effective tool for optimizing expensive and black-box experimental design processes. While Bayesian optimization is a principled data-driven approach to experimental optimization, it learns everything from scratch and could greatly benefit from the expertise of its human (domain) experts who often reason about systems at different abstraction levels using physical properties that are not necessarily directly measured (or measurable). In this paper, we propose a human-AI collaborative Bayesian framework to incorporate expert preferences about unmeasured abstract properties into the surrogate modeling to further boost the performance of BO. We provide an efficient strategy that can also handle any incorrect/misleading expert bias in preferential judgments. We discuss the convergence behavior of our proposed framework. Our experimental results involving synthetic functions and real-world datasets show the superiority of our method against the baselines.","sentences":["Experimental (design) optimization is a key driver in designing and discovering new products and processes.","Bayesian Optimization (BO) is an effective tool for optimizing expensive and black-box experimental design processes.","While Bayesian optimization is a principled data-driven approach to experimental optimization, it learns everything from scratch and could greatly benefit from the expertise of its human (domain) experts who often reason about systems at different abstraction levels using physical properties that are not necessarily directly measured (or measurable).","In this paper, we propose a human-AI collaborative Bayesian framework to incorporate expert preferences about unmeasured abstract properties into the surrogate modeling to further boost the performance of BO.","We provide an efficient strategy that can also handle any incorrect/misleading expert bias in preferential judgments.","We discuss the convergence behavior of our proposed framework.","Our experimental results involving synthetic functions and real-world datasets show the superiority of our method against the baselines."],"url":"http://arxiv.org/abs/2402.17343v1","category":"cs.LG"}
{"created":"2024-02-27 09:20:16","title":"A Scalable Multi-Layered Blockchain Architecture for Enhanced EHR Sharing and Drug Supply Chain Management","abstract":"In recent years, the healthcare sector's shift to online platforms has spotlighted challenges concerning data security, privacy, and scalability. Blockchain technology, known for its decentralized, secure, and immutable nature, emerges as a viable solution for these pressing issues. This article presents an innovative Electronic Health Records (EHR) sharing and drug supply chain management framework tailored to address scalability, security, data integrity, traceability, and secure data sharing. The framework introduces five layers and transactions, prioritizing patient-centric healthcare by granting patients comprehensive access control over their health information. This access facilitates smoother processes, such as insurance claims, while maintaining robust security measures. Notably, our implementation of parallelism significantly bolsters scalability and transaction throughput while minimizing network traffic. Performance evaluations conducted through the Caliper benchmark indicate a slight increase in processor consumption during specific transactions, mitigated effectively by parallelization. RAM requirements remain largely stable. Additionally, our approach notably reduces network traffic while tripling transaction throughput. The framework ensures patient privacy, data integrity, access control, and interoperability, aligning with traditional healthcare systems. Moreover, it provides transparency and real-time drug supply monitoring, empowering decision-makers with actionable insights. As healthcare evolves, our framework sets a crucial precedent for innovative, scalable, and secure systems. Future enhancements could focus on scalability, real-world deployment, standardized data formats, reinforced security protocols, privacy preservation, and IoT integration to comply with regulations and meet evolving industry needs.","sentences":["In recent years, the healthcare sector's shift to online platforms has spotlighted challenges concerning data security, privacy, and scalability.","Blockchain technology, known for its decentralized, secure, and immutable nature, emerges as a viable solution for these pressing issues.","This article presents an innovative Electronic Health Records (EHR) sharing and drug supply chain management framework tailored to address scalability, security, data integrity, traceability, and secure data sharing.","The framework introduces five layers and transactions, prioritizing patient-centric healthcare by granting patients comprehensive access control over their health information.","This access facilitates smoother processes, such as insurance claims, while maintaining robust security measures.","Notably, our implementation of parallelism significantly bolsters scalability and transaction throughput while minimizing network traffic.","Performance evaluations conducted through the Caliper benchmark indicate a slight increase in processor consumption during specific transactions, mitigated effectively by parallelization.","RAM requirements remain largely stable.","Additionally, our approach notably reduces network traffic while tripling transaction throughput.","The framework ensures patient privacy, data integrity, access control, and interoperability, aligning with traditional healthcare systems.","Moreover, it provides transparency and real-time drug supply monitoring, empowering decision-makers with actionable insights.","As healthcare evolves, our framework sets a crucial precedent for innovative, scalable, and secure systems.","Future enhancements could focus on scalability, real-world deployment, standardized data formats, reinforced security protocols, privacy preservation, and IoT integration to comply with regulations and meet evolving industry needs."],"url":"http://arxiv.org/abs/2402.17342v1","category":"cs.CR"}
{"created":"2024-02-27 09:10:47","title":"Unified study of viscoelasticity and sound damping in hard and soft amorphous solids","abstract":"Recent research has made significant progress in understanding the non-phonon vibrational states present in amorphous materials. It has been established that their vibrational density of states follows non-Debye scaling laws. Here, we show that the non-Debye scaling laws play a crucial role in determining material properties of a broad range of amorphous solids, from ``hard\" amorphous solids like structural glasses to ``soft\" amorphous solids such as foams and emulsions. We propose a unified framework of viscoelasticity and sound damping for these materials. Although these properties differ significantly between hard and soft amorphous solids, they are determined by the non-Debye scaling laws. We also validate our framework using numerical simulations.","sentences":["Recent research has made significant progress in understanding the non-phonon vibrational states present in amorphous materials.","It has been established that their vibrational density of states follows non-Debye scaling laws.","Here, we show that the non-Debye scaling laws play a crucial role in determining material properties of a broad range of amorphous solids, from ``hard\" amorphous solids like structural glasses to ``soft\" amorphous solids such as foams and emulsions.","We propose a unified framework of viscoelasticity and sound damping for these materials.","Although these properties differ significantly between hard and soft amorphous solids, they are determined by the non-Debye scaling laws.","We also validate our framework using numerical simulations."],"url":"http://arxiv.org/abs/2402.17335v1","category":"cond-mat.soft"}
{"created":"2024-02-27 09:09:30","title":"Aerodynamic Prediction of a CRM High-lift Configuration using a modified three equation turbulence mode","abstract":"Aerodynamic simulations were carried out in the study presented in this paper focusing on the stall performance of the High-Lift Common Research Model obtained from the fourth AIAA High-Lift Prediction Workshop. Various turbulence models of Reynolds-average Navier-Stokes simulations are analyzed. A modified version of the transitional k-(v^2 )-{\\omega} model developed to enhance stall prediction accuracy for high-lift configurations with a nacelle chine. The vortex generator, three-element airfoil, and high-lift model are numerically simulated. The results reveal that implementing a k-(v^2 )-{\\omega} model with separation shear layer fixed notably enhances the stall prediction behavior for both the three-element airfoil and high-lift configuration without affecting the prediction of the vortex strength of a vortex generator. Moreover, incorporating rotation correction into the SPF k-(v^2 )-{\\omega} model improves the prediction of vortex strength and further enhances stall prediction for the high-lift configuration. The relative error in predicting the maximum lift coefficient is less than 5% of the experimental data. The study also investigated the impact of the nacelle chine on the stall behavior of the high-lift configuration. The results demonstrate that the chine vortex can mitigate the adverse effects of the nacelle/pylon vortex system and increase the maximum lift coefficient.","sentences":["Aerodynamic simulations were carried out in the study presented in this paper focusing on the stall performance of the High-Lift Common Research Model obtained from the fourth AIAA High-Lift Prediction Workshop.","Various turbulence models of Reynolds-average Navier-Stokes simulations are analyzed.","A modified version of the transitional k-(v^2 )-{\\omega} model developed to enhance stall prediction accuracy for high-lift configurations with a nacelle chine.","The vortex generator, three-element airfoil, and high-lift model are numerically simulated.","The results reveal that implementing a k-(v^2 )-{\\omega} model with separation shear layer fixed notably enhances the stall prediction behavior for both the three-element airfoil and high-lift configuration without affecting the prediction of the vortex strength of a vortex generator.","Moreover, incorporating rotation correction into the SPF k-(v^2 )-{\\omega} model improves the prediction of vortex strength and further enhances stall prediction for the high-lift configuration.","The relative error in predicting the maximum lift coefficient is less than 5% of the experimental data.","The study also investigated the impact of the nacelle chine on the stall behavior of the high-lift configuration.","The results demonstrate that the chine vortex can mitigate the adverse effects of the nacelle/pylon vortex system and increase the maximum lift coefficient."],"url":"http://arxiv.org/abs/2402.17332v1","category":"physics.flu-dyn"}
{"created":"2024-02-27 09:06:09","title":"Quantum kinetic equation and thermal conductivity tensor for bosons","abstract":"We systematically derive the quantum kinetic equation in full phase space for any quadratic hamiltonian of bosonic fields, including in the absence of translational invariance. This enables the treatment of boundaries, inhomogeneous systems and states with non-trivial textures, such as skyrmions in the context of magnetic bosons. We relate the evolution of the distribution of bosons in phase space to single-electron, band-diagonal, physical quantities such as Berry curvature and energy magnetization, by providing a procedure to \"diagonalize\" the Hamiltonian in phase space, using the formalism of the Moyal product. We obtain exact equations, which can be expanded order by order, for example in the \"smallness\" of the spatial gradients, providing a \"semiclassical\" approximation. In turn, at first order, we recover the usual full Boltzmann equation and give a self-contained and exact derivation of the intrinsic thermal Hall effect of bosons. The formulation clarifies the contribution from \"energy magnetization\" in natural manner, and does not require the inclusion of Luttinger's pseudo-gravitational field to obtain thermal transport quantities.","sentences":["We systematically derive the quantum kinetic equation in full phase space for any quadratic hamiltonian of bosonic fields, including in the absence of translational invariance.","This enables the treatment of boundaries, inhomogeneous systems and states with non-trivial textures, such as skyrmions in the context of magnetic bosons.","We relate the evolution of the distribution of bosons in phase space to single-electron, band-diagonal, physical quantities such as Berry curvature and energy magnetization, by providing a procedure to \"diagonalize\" the Hamiltonian in phase space, using the formalism of the Moyal product.","We obtain exact equations, which can be expanded order by order, for example in the \"smallness\" of the spatial gradients, providing a \"semiclassical\" approximation.","In turn, at first order, we recover the usual full Boltzmann equation and give a self-contained and exact derivation of the intrinsic thermal Hall effect of bosons.","The formulation clarifies the contribution from \"energy magnetization\" in natural manner, and does not require the inclusion of Luttinger's pseudo-gravitational field to obtain thermal transport quantities."],"url":"http://arxiv.org/abs/2402.17329v1","category":"cond-mat.str-el"}
{"created":"2024-02-27 09:02:10","title":"Nodal precession of a hot Jupiter transiting the edge of a late A-type star TOI-1518","abstract":"TOI-1518b, a hot Jupiter around a late A-type star, is one of the few planetary systems that transit the edge of the stellar surface (the impact parameter $b\\sim0.9 $) among hot Jupiters around hot stars (Cabot et al. 2021). The high rotation speed of the host star ($\\sim85$ km s$^{-1}$) and the nearly polar orbit of the planet ($\\sim 120$ deg) may cause a nodal precession. In this study, we report the nodal precession undergone by TOI-1518\\,b. This system is the fourth planetary system in which nodal precession is detected. We investigate the time change in $b$ from the photometric data of TOI-1518 acquired in 2019 and 2022 with TESS and from the spectral transit data of TOI-1518b obtained in 2020 with two high-dispersion spectrographs; CARMENES and EXPRES. We find that the value of $b$ is decreasing with $db/dt=-0.0116\\pm0.0036$\\,year$^{-1}$, indicating that the transit trajectory is moving toward the center of the stellar surface. We also estimate the minimum value of the quadrupole mass moment of TOI-1518 $J_{2,\\mathrm{min}}=4.41\\times 10^{-5}$ and the logarithm of the Love number of TOI-1518 $\\log{k_2}= -2.17\\pm 0.33$ from the nodal precession.","sentences":["TOI-1518b, a hot Jupiter around a late A-type star, is one of the few planetary systems that transit the edge of the stellar surface (the impact parameter $b\\sim0.9 $) among hot Jupiters around hot stars (Cabot et al. 2021).","The high rotation speed of the host star ($\\sim85$ km s$^{-1}$) and the nearly polar orbit of the planet ($\\sim 120$ deg) may cause a nodal precession.","In this study, we report the nodal precession undergone by TOI-1518\\,b.","This system is the fourth planetary system in which nodal precession is detected.","We investigate the time change in $b$ from the photometric data of TOI-1518 acquired in 2019 and 2022 with TESS and from the spectral transit data of TOI-1518b obtained in 2020 with two high-dispersion spectrographs; CARMENES and EXPRES.","We find that the value of $b$ is decreasing with $db/dt=-0.0116\\pm0.0036$\\,year$^{-1}$, indicating that the transit trajectory is moving toward the center of the stellar surface.","We also estimate the minimum value of the quadrupole mass moment of TOI-1518 $J_{2,\\mathrm{min}}=4.41\\times 10^{-5}$ and the logarithm of the Love number of TOI-1518 $\\log{k_2}= -2.17\\pm 0.33$ from the nodal precession."],"url":"http://arxiv.org/abs/2402.17325v1","category":"astro-ph.EP"}
{"created":"2024-02-27 09:01:03","title":"On the Impact and Utility of Single-Exomoon Modeling for Multi-Moon Systems","abstract":"The search for exomoons in time-domain photometric data has to-date generally consisted of fitting transit models that are comprised of a planet hosting a single moon. This simple model has its advantages, but it may not be particularly representative, as most of the major moons in our Solar System are found in multi-moon satellite systems. It is critical that we investigate, then, the impact of applying a single-moon model to systems containing multiple moons, as there is the possibility that utilizing an inaccurate or incomplete model could lead to erroneous conclusions about the system. To that end, in this work we produce a variety of realistic multi-moon light curves, perform standard single-moon model selection, and analyze the impacts that this model choice may have on the search for exomoons. We find that the number of moons in a system fit with a single-moon model generally has little impact on whether we find evidence for a moon in that system, and other system attributes are individually not especially predictive. However, the model parameter solutions for the moon frequently do not match any real moon in the system, instead painting a picture of a ``phantom'' moon. We find no evidence that multi-moon systems yield corresponding multi-modal posteriors. We also find a systematic tendency to overestimate planetary impact parameter and eccentricity, to derive unphysical moon densities, and to infer potentially unphysical limb darkening coefficients. These results will be important to keep in mind in future exomoon search programs.","sentences":["The search for exomoons in time-domain photometric data has to-date generally consisted of fitting transit models that are comprised of a planet hosting a single moon.","This simple model has its advantages, but it may not be particularly representative, as most of the major moons in our Solar System are found in multi-moon satellite systems.","It is critical that we investigate, then, the impact of applying a single-moon model to systems containing multiple moons, as there is the possibility that utilizing an inaccurate or incomplete model could lead to erroneous conclusions about the system.","To that end, in this work we produce a variety of realistic multi-moon light curves, perform standard single-moon model selection, and analyze the impacts that this model choice may have on the search for exomoons.","We find that the number of moons in a system fit with a single-moon model generally has little impact on whether we find evidence for a moon in that system, and other system attributes are individually not especially predictive.","However, the model parameter solutions for the moon frequently do not match any real moon in the system, instead painting a picture of a ``phantom'' moon.","We find no evidence that multi-moon systems yield corresponding multi-modal posteriors.","We also find a systematic tendency to overestimate planetary impact parameter and eccentricity, to derive unphysical moon densities, and to infer potentially unphysical limb darkening coefficients.","These results will be important to keep in mind in future exomoon search programs."],"url":"http://arxiv.org/abs/2402.17324v1","category":"astro-ph.EP"}
{"created":"2024-02-27 08:44:30","title":"Superconducting-transition-temperature dependence of superfluid density and conductivity in pressurized cuprate superconductors","abstract":"What factors fundamentally determine the value of superconducting transition temperature (Tc) in high temperature superconductors has been the subject of intense debate. Following the establishment of an empirical law known as Homes'law, there is a growing consensus in the community that the Tc value of the cuprate superconductors is closely linked to its superfluid density and conductivity. However, all the data supporting this empirical law have been obtained from the ambient-pressure superconductors. In this study, we present the first high-pressure results about the connection of these two quantities with Tc, through the studies on the Bi1.74Pb0.38Sr1.88CuO6+delta and Bi2Sr2CaCu2O8+delta, in which the value of their high-pressure resistivity (the reciprocal of conductivity) is achieved by adopting our newly established method, while the value of superfluid density is extracted using the Homes'law. We highlight that the Tc values are strongly linked the two joint response factors of magnetic field and electric field, i.e. superfluid density and conductivity, respectively, implying that the physics governing the determination of Tc is influenced by the intrinsic electromagnetic fields of the system.","sentences":["What factors fundamentally determine the value of superconducting transition temperature (Tc) in high temperature superconductors has been the subject of intense debate.","Following the establishment of an empirical law known as Homes'law, there is a growing consensus in the community that the Tc value of the cuprate superconductors is closely linked to its superfluid density and conductivity.","However, all the data supporting this empirical law have been obtained from the ambient-pressure superconductors.","In this study, we present the first high-pressure results about the connection of these two quantities with Tc, through the studies on the Bi1.74Pb0.38Sr1.88CuO6+delta and Bi2Sr2CaCu2O8+delta, in which the value of their high-pressure resistivity (the reciprocal of conductivity) is achieved by adopting our newly established method, while the value of superfluid density is extracted using the Homes'law.","We highlight that the Tc values are strongly linked the two joint response factors of magnetic field and electric field, i.e. superfluid density and conductivity, respectively, implying that the physics governing the determination of Tc is influenced by the intrinsic electromagnetic fields of the system."],"url":"http://arxiv.org/abs/2402.17315v1","category":"cond-mat.supr-con"}
{"created":"2024-02-27 08:39:55","title":"The candidates of long-periodic variable sources in 6.7 GHz methanol masers associated with four high-mass star-forming regions","abstract":"Results of the long-term monitoring observations of the 6.7 GHz Class II methanol masers associated with the four high-mass star-forming regions by Hitachi 32-m radio telescope are presented. We detected periodic flux variability in G06.795-0.257, G10.472+0.027, G12.209-0.102, and G13.657-0.599 with the periods of 968, 1624, 1272, and 1266 d, respectively, although the detected period is tentative due to the short monitoring term relative to the estimated period. The facts that the flux variation patterns show the symmetric sine curves and that the luminosities of the central protostar and periods of maser flux variation are consistent with the expected period-luminosity (PL) relation suggest that the mechanisms of maser flux variability of G10.472+0.027 and G12.209-0.102 can be explained by protostellar pulsation instability. From the PL relation, central stars of these two sources are expected to be very high-mass protostars with a mass of 40 M and to have the mass accretion rate of 2*10-2 M yr-1. On the other hand, G06.795-0.257 and G13.657-0.599 have the intermittent variation patterns and have luminosities that are an order of magnitude smaller than those expected from PL relation, suggesting variation mechanisms of these sources originated from binary system. Since almost all the maser features vary with the same period regardless of its geometry, periodic accretion model may be appropriate mechanisms for flux variability in G06.795-0.257 and G13.657-0.599.","sentences":["Results of the long-term monitoring observations of the 6.7 GHz Class II methanol masers associated with the four high-mass star-forming regions by Hitachi 32-m radio telescope are presented.","We detected periodic flux variability in G06.795-0.257, G10.472+0.027, G12.209-0.102, and G13.657-0.599 with the periods of 968, 1624, 1272, and 1266 d, respectively, although the detected period is tentative due to the short monitoring term relative to the estimated period.","The facts that the flux variation patterns show the symmetric sine curves and that the luminosities of the central protostar and periods of maser flux variation are consistent with the expected period-luminosity (PL) relation suggest that the mechanisms of maser flux variability of G10.472+0.027 and G12.209-0.102 can be explained by protostellar pulsation instability.","From the PL relation, central stars of these two sources are expected to be very high-mass protostars with a mass of 40 M and to have the mass accretion rate of 2*10-2 M yr-1.","On the other hand, G06.795-0.257 and G13.657-0.599 have the intermittent variation patterns and have luminosities that are an order of magnitude smaller than those expected from PL relation, suggesting variation mechanisms of these sources originated from binary system.","Since almost all the maser features vary with the same period regardless of its geometry, periodic accretion model may be appropriate mechanisms for flux variability in G06.795-0.257 and G13.657-0.599."],"url":"http://arxiv.org/abs/2402.17314v1","category":"astro-ph.GA"}
{"created":"2024-02-27 08:29:16","title":"Stripes and the Emergence of Charge $\u03c0$-phase Shifts in Isotropically Paired Systems","abstract":"The interplay of spin and motional degrees of freedom forms a key element in explaining stripe formation accompanied by sublattice reversal of local antiferromagnetic ordering in interacting fermionic models. A long-standing question aims to relate pairing to stripe formation, intending to discern the applicability of simple models that observe this phenomenon in understanding cuprate physics. By departing from fermionic statistics, we show that the formation of stripes is rather generic, allowing one to unveil its competition with superfluid behavior. To that end, we use a combination of numerical methods to solve a model of interacting hardcore bosons in ladder geometries, finding that once stripes are formed, either via external pinning or spontaneously, a sublattice reversal ($\\pi$-phase shift) of \\textit{charge} ordering occurs, suppressing the superfluid weight. Lastly, we show that when the Cooper pairs are not local, as in the attractive Hubbard model with finite interactions, auxiliary-field quantum Monte Carlo calculations show evidence of fluctuating stripes, but these are seen to coexist with superfluidity. Our results corroborate the picture that static stripes cannot be reconciled with pairing, unlike the case of fluctuating ones.","sentences":["The interplay of spin and motional degrees of freedom forms a key element in explaining stripe formation accompanied by sublattice reversal of local antiferromagnetic ordering in interacting fermionic models.","A long-standing question aims to relate pairing to stripe formation, intending to discern the applicability of simple models that observe this phenomenon in understanding cuprate physics.","By departing from fermionic statistics, we show that the formation of stripes is rather generic, allowing one to unveil its competition with superfluid behavior.","To that end, we use a combination of numerical methods to solve a model of interacting hardcore bosons in ladder geometries, finding that once stripes are formed, either via external pinning or spontaneously, a sublattice reversal ($\\pi$-phase shift) of \\textit{charge} ordering occurs, suppressing the superfluid weight.","Lastly, we show that when the Cooper pairs are not local, as in the attractive Hubbard model with finite interactions, auxiliary-field quantum Monte Carlo calculations show evidence of fluctuating stripes, but these are seen to coexist with superfluidity.","Our results corroborate the picture that static stripes cannot be reconciled with pairing, unlike the case of fluctuating ones."],"url":"http://arxiv.org/abs/2402.17305v1","category":"cond-mat.str-el"}
{"created":"2024-02-27 08:16:17","title":"Quantum Distance Approximation for Persistence Diagrams","abstract":"Topological Data Analysis methods can be useful for classification and clustering tasks in many different fields as they can provide two dimensional persistence diagrams that summarize important information about the shape of potentially complex and high dimensional data sets. The space of persistence diagrams can be endowed with various metrics such as the Wasserstein distance which admit a statistical structure and allow to use these summaries for machine learning algorithms. However, computing the distance between two persistence diagrams involves finding an optimal way to match the points of the two diagrams and may not always be an easy task for classical computers. In this work we explore the potential of quantum computers to estimate the distance between persistence diagrams, in particular we propose variational quantum algorithms for the Wasserstein distance as well as the $d^{c}_{p}$ distance. Our implementation is a weighted version of the Quantum Approximate Optimization Algorithm that relies on control clauses to encode the constraints of the optimization problem.","sentences":["Topological Data Analysis methods can be useful for classification and clustering tasks in many different fields as they can provide two dimensional persistence diagrams that summarize important information about the shape of potentially complex and high dimensional data sets.","The space of persistence diagrams can be endowed with various metrics such as the Wasserstein distance which admit a statistical structure and allow to use these summaries for machine learning algorithms.","However, computing the distance between two persistence diagrams involves finding an optimal way to match the points of the two diagrams and may not always be an easy task for classical computers.","In this work we explore the potential of quantum computers to estimate the distance between persistence diagrams, in particular we propose variational quantum algorithms for the Wasserstein distance as well as the $d^{c}_{p}$ distance.","Our implementation is a weighted version of the Quantum Approximate Optimization Algorithm that relies on control clauses to encode the constraints of the optimization problem."],"url":"http://arxiv.org/abs/2402.17295v1","category":"quant-ph"}
{"created":"2024-02-27 08:14:50","title":"Advancing Continuous Distribution Generation: An Exponentiated Odds Ratio Generator Approach","abstract":"This paper presents a new methodology for generating continuous statistical distributions, integrating the exponentiated odds ratio within the framework of survival analysis. This new method enhances the flexibility and adaptability of distribution models to effectively address the complexities inherent in contemporary datasets. The core of this advancement is illustrated by introducing a particular subfamily, the \"Type-2 Gumbel Weibull-G Family of Distributions.\" We provide a comprehensive analysis of the mathematical properties of these distributions, encompassing statistical properties such as density functions, moments, hazard rate and quantile functions, R\\'enyi entropy, order statistics, and the concept of stochastic ordering. To establish the robustness of our approach, we apply five distinct methods for parameter estimation. The practical applicability of the Type-2 Gumbel Weibull-G distributions is further supported through the analysis of three real-world datasets. These empirical applications illustrate the exceptional statistical precision of our distributions compared to existing models, thereby reinforcing their significant value in both theoretical and practical statistical applications.","sentences":["This paper presents a new methodology for generating continuous statistical distributions, integrating the exponentiated odds ratio within the framework of survival analysis.","This new method enhances the flexibility and adaptability of distribution models to effectively address the complexities inherent in contemporary datasets.","The core of this advancement is illustrated by introducing a particular subfamily, the \"Type-2 Gumbel Weibull-G Family of Distributions.\"","We provide a comprehensive analysis of the mathematical properties of these distributions, encompassing statistical properties such as density functions, moments, hazard rate and quantile functions, R\\'enyi entropy, order statistics, and the concept of stochastic ordering.","To establish the robustness of our approach, we apply five distinct methods for parameter estimation.","The practical applicability of the Type-2 Gumbel Weibull-G distributions is further supported through the analysis of three real-world datasets.","These empirical applications illustrate the exceptional statistical precision of our distributions compared to existing models, thereby reinforcing their significant value in both theoretical and practical statistical applications."],"url":"http://arxiv.org/abs/2402.17294v1","category":"math.ST"}
{"created":"2024-02-27 08:05:41","title":"Tight Lower Bounds for Block-Structured Integer Programs","abstract":"We study fundamental block-structured integer programs called tree-fold and multi-stage IPs. Tree-fold IPs admit a constraint matrix with independent blocks linked together by few constraints in a recursive pattern; and transposing their constraint matrix yields multi-stage IPs. The state-of-the-art algorithms to solve these IPs have an exponential gap in their running times, making it natural to ask whether this gap is inherent. We answer this question affirmative. Assuming the Exponential Time Hypothesis, we prove lower bounds showing that the exponential difference is necessary, and that the known algorithms are near optimal. Moreover, we prove unconditional lower bounds on the norms of the Graver basis, a fundamental building block of all known algorithms to solve these IPs. This shows that none of the current approaches can be improved beyond this bound.","sentences":["We study fundamental block-structured integer programs called tree-fold and multi-stage IPs.","Tree-fold IPs admit a constraint matrix with independent blocks linked together by few constraints in a recursive pattern; and transposing their constraint matrix yields multi-stage IPs.","The state-of-the-art algorithms to solve these IPs have an exponential gap in their running times, making it natural to ask whether this gap is inherent.","We answer this question affirmative.","Assuming the Exponential Time Hypothesis, we prove lower bounds showing that the exponential difference is necessary, and that the known algorithms are near optimal.","Moreover, we prove unconditional lower bounds on the norms of the Graver basis, a fundamental building block of all known algorithms to solve these IPs.","This shows that none of the current approaches can be improved beyond this bound."],"url":"http://arxiv.org/abs/2402.17290v1","category":"cs.CC"}
{"created":"2024-02-27 07:55:43","title":"Geometry on surfaces, a source for mathematical developments","abstract":"We present a variety of geometrical and combinatorial tools that are used in the study of geometric structures on surfaces: volume, contact, symplectic, complex and almost complex structures. We start with a series of local rigidity results for such structures. Higher-dimensional analogues are also discussed. Some constructions with Riemann surfaces lead, by analogy, to notions that hold for arbitrary fields, and not only the field of complex numbers. The Riemann sphere is also defined using surjective homomorphisms of real algebras from the ring of real univariate polynomials to (arbitrary) fields, in which the field with one element is interpreted as the point at infinity of the Gaussian plane of complex numbers. Several models of the hyperbolic plane and hyperbolic 3-space appear, defined in terms of complex structures on surfaces, and in particular also a rather elementary construction of the hyperbolic plane usingreal monic univariate polynomials of degree two without real roots. Several notions and problems connected with conformal structures in dimension 2 are discussed, including dessins d'enfants, the combinatorial characterization of polynomials and rational maps of the sphere, the type problem, uniformization, quasiconformal mappings, Thurston's characterization of Speiser graphs, stratifications of spaces of monic polynomials, and others. Classical methods and new techniques complement each other. The final version of this paper will appear as a chapter in the Volume Surveys in Geometry. II (ed. A. Papadopoulos), Springer Nature Switzerland, 2024.","sentences":["We present a variety of geometrical and combinatorial tools that are used in the study of geometric structures on surfaces: volume, contact, symplectic, complex and almost complex structures.","We start with a series of local rigidity results for such structures.","Higher-dimensional analogues are also discussed.","Some constructions with Riemann surfaces lead, by analogy, to notions that hold for arbitrary fields, and not only the field of complex numbers.","The Riemann sphere is also defined using surjective homomorphisms of real algebras from the ring of real univariate polynomials to (arbitrary) fields, in which the field with one element is interpreted as the point at infinity of the Gaussian plane of complex numbers.","Several models of the hyperbolic plane and hyperbolic 3-space appear, defined in terms of complex structures on surfaces, and in particular also a rather elementary construction of the hyperbolic plane usingreal monic univariate polynomials of degree two without real roots.","Several notions and problems connected with conformal structures in dimension 2 are discussed, including dessins d'enfants, the combinatorial characterization of polynomials and rational maps of the sphere, the type problem, uniformization, quasiconformal mappings, Thurston's characterization of Speiser graphs, stratifications of spaces of monic polynomials, and others.","Classical methods and new techniques complement each other.","The final version of this paper will appear as a chapter in the Volume Surveys in Geometry.","II (ed. A. Papadopoulos), Springer Nature Switzerland, 2024."],"url":"http://arxiv.org/abs/2402.17283v1","category":"math.CV"}
{"created":"2024-02-27 07:52:25","title":"GAN Based Near-Field Channel Estimation for Extremely Large-Scale MIMO Systems","abstract":"Extremely large-scale multiple-input-multiple-output (XL-MIMO) is a promising technique to achieve ultra-high spectral efficiency for future 6G communications. The mixed line-of-sight (LoS) and non-line-of-sight (NLoS) XL-MIMO near-field channel model is adopted to describe the XL-MIMO near-field channel accurately. In this paper, a generative adversarial network (GAN) variant based channel estimation method is proposed for XL-MIMO systems. Specifically, the GAN variant is developed to simultaneously estimate the LoS and NLoS path components of the XL-MIMO channel. The initially estimated channels instead of the received signals are input into the GAN variant as the conditional input to generate the XL-MIMO channels more efficiently. The GAN variant not only learns the mapping from the initially estimated channels to the XL-MIMO channels but also learns an adversarial loss. Moreover, we combine the adversarial loss with a conventional loss function to ensure the correct direction of training the generator. To further enhance the estimation performance, we investigate the impact of the hyper-parameter of the loss function on the performance of our method. Simulation results show that the proposed method outperforms the existing channel estimation approaches in the adopted channel model. In addition, the proposed method surpasses the Cram$\\acute{\\mathbf{e}}$r-Rao lower bound (CRLB) under low pilot overhead.","sentences":["Extremely large-scale multiple-input-multiple-output (XL-MIMO) is a promising technique to achieve ultra-high spectral efficiency for future 6G communications.","The mixed line-of-sight (LoS) and non-line-of-sight (NLoS) XL-MIMO near-field channel model is adopted to describe the XL-MIMO near-field channel accurately.","In this paper, a generative adversarial network (GAN) variant based channel estimation method is proposed for XL-MIMO systems.","Specifically, the GAN variant is developed to simultaneously estimate the LoS and NLoS path components of the XL-MIMO channel.","The initially estimated channels instead of the received signals are input into the GAN variant as the conditional input to generate the XL-MIMO channels more efficiently.","The GAN variant not only learns the mapping from the initially estimated channels to the XL-MIMO channels but also learns an adversarial loss.","Moreover, we combine the adversarial loss with a conventional loss function to ensure the correct direction of training the generator.","To further enhance the estimation performance, we investigate the impact of the hyper-parameter of the loss function on the performance of our method.","Simulation results show that the proposed method outperforms the existing channel estimation approaches in the adopted channel model.","In addition, the proposed method surpasses the Cram$\\acute{\\mathbf{e}}$r-Rao lower bound (CRLB) under low pilot overhead."],"url":"http://arxiv.org/abs/2402.17281v1","category":"eess.SP"}
{"created":"2024-02-27 07:35:29","title":"Type II Multi-indexed Little $q$-Jacobi and Little $q$-Laguerre Polynomials","abstract":"For the isospectral Darboux transformations of the discrete quantum mechanics with real shifts, there are two methods: type I and type II constructions. Based on the type I construction, the type I multi-indexed little $q$-Jacobi and little $q$-Laguerre orthogonal polynomials were presented in J. Phys. {\\bf A50} (2017) 165204. Based on the type II construction, we present the type II multi-indexed little $q$-Jacobi and little $q$-Laguerre orthogonal polynomials.","sentences":["For the isospectral Darboux transformations of the discrete quantum mechanics with real shifts, there are two methods: type I and type II constructions.","Based on the type I construction, the type I multi-indexed little $q$-Jacobi and little $q$-Laguerre orthogonal polynomials were presented in J. Phys.","{\\bf A50} (2017) 165204.","Based on the type II construction, we present the type II multi-indexed little $q$-Jacobi and little $q$-Laguerre orthogonal polynomials."],"url":"http://arxiv.org/abs/2402.17272v1","category":"math-ph"}
{"created":"2024-02-27 07:25:02","title":"Reinforcement Learning Based Robust Volt/Var Control in Active Distribution Networks With Imprecisely Known Delay","abstract":"Active distribution networks (ADNs) incorporating massive photovoltaic (PV) devices encounter challenges of rapid voltage fluctuations and potential violations. Due to the fluctuation and intermittency of PV generation, the state gap, arising from time-inconsistent states and exacerbated by imprecisely known system delays, significantly impacts the accuracy of voltage control. This paper addresses this challenge by introducing a framework for delay adaptive Volt/Var control (VVC) in the presence of imprecisely known system delays to regulate the reactive power of PV inverters. The proposed approach formulates the voltage control, based on predicted system operation states, as a robust VVC problem. It employs sample selection from the state prediction interval to promptly identify the worst-performing system operation state. Furthermore, we leverage the decentralized partially observable Markov decision process (Dec-POMDP) to reformulate the robust VVC problem. We design Multiple Policy Networks and employ Multiple Policy Networks and Reward Shaping-based Multi-agent Twin Delayed Deep Deterministic Policy Gradient (MPNRS-MATD3) algorithm to efficiently address and solve the Dec-POMDP model-based problem. Simulation results show the delay adaption characteristic of our proposed framework, and the MPNRS-MATD3 outperforms other multi-agent reinforcement learning algorithms in robust voltage control.","sentences":["Active distribution networks (ADNs) incorporating massive photovoltaic (PV) devices encounter challenges of rapid voltage fluctuations and potential violations.","Due to the fluctuation and intermittency of PV generation, the state gap, arising from time-inconsistent states and exacerbated by imprecisely known system delays, significantly impacts the accuracy of voltage control.","This paper addresses this challenge by introducing a framework for delay adaptive Volt/Var control (VVC) in the presence of imprecisely known system delays to regulate the reactive power of PV inverters.","The proposed approach formulates the voltage control, based on predicted system operation states, as a robust VVC problem.","It employs sample selection from the state prediction interval to promptly identify the worst-performing system operation state.","Furthermore, we leverage the decentralized partially observable Markov decision process (Dec-POMDP) to reformulate the robust VVC problem.","We design Multiple Policy Networks and employ Multiple Policy Networks and Reward Shaping-based Multi-agent Twin Delayed Deep Deterministic Policy Gradient (MPNRS-MATD3) algorithm to efficiently address and solve the Dec-POMDP model-based problem.","Simulation results show the delay adaption characteristic of our proposed framework, and the MPNRS-MATD3 outperforms other multi-agent reinforcement learning algorithms in robust voltage control."],"url":"http://arxiv.org/abs/2402.17268v1","category":"eess.SY"}
{"created":"2024-02-27 07:21:09","title":"Mobility edges in non-Hermitian models with slowly varying quasi-periodic disorders","abstract":"We investigate the appearance of mobility edges in a one-dimensional non-Hermitian tight-banding model with alternating hopping constants and slowly varying quasi-periodic on-site potentials. Due to the presence of slowly varying exponent, the parity-time (PT) symmetry of this model is broken and its spectra is complex. It is found that the spectrum of this model can be divided into three different types of patterns depending on the magnitude of the quasi-periodic potential. As the amplitude of the potential increases from small to large, the initially well defined mobility edges become blurred gradually and then eventually disappear for large enough potential. This behavior of the mobility edges is also confirmed by a detailed study of the winding number of the complex spectra of this non-Hermitian model.","sentences":["We investigate the appearance of mobility edges in a one-dimensional non-Hermitian tight-banding model with alternating hopping constants and slowly varying quasi-periodic on-site potentials.","Due to the presence of slowly varying exponent, the parity-time (PT) symmetry of this model is broken and its spectra is complex.","It is found that the spectrum of this model can be divided into three different types of patterns depending on the magnitude of the quasi-periodic potential.","As the amplitude of the potential increases from small to large, the initially well defined mobility edges become blurred gradually and then eventually disappear for large enough potential.","This behavior of the mobility edges is also confirmed by a detailed study of the winding number of the complex spectra of this non-Hermitian model."],"url":"http://arxiv.org/abs/2402.17266v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-27 07:19:50","title":"Explicit Interaction for Fusion-Based Place Recognition","abstract":"Fusion-based place recognition is an emerging technique jointly utilizing multi-modal perception data, to recognize previously visited places in GPS-denied scenarios for robots and autonomous vehicles. Recent fusion-based place recognition methods combine multi-modal features in implicit manners. While achieving remarkable results, they do not explicitly consider what the individual modality affords in the fusion system. Therefore, the benefit of multi-modal feature fusion may not be fully explored. In this paper, we propose a novel fusion-based network, dubbed EINet, to achieve explicit interaction of the two modalities. EINet uses LiDAR ranges to supervise more robust vision features for long time spans, and simultaneously uses camera RGB data to improve the discrimination of LiDAR point clouds. In addition, we develop a new benchmark for the place recognition task based on the nuScenes dataset. To establish this benchmark for future research with comprehensive comparisons, we introduce both supervised and self-supervised training schemes alongside evaluation protocols. We conduct extensive experiments on the proposed benchmark, and the experimental results show that our EINet exhibits better recognition performance as well as solid generalization ability compared to the state-of-the-art fusion-based place recognition approaches. Our open-source code and benchmark are released at: https://github.com/BIT-XJY/EINet.","sentences":["Fusion-based place recognition is an emerging technique jointly utilizing multi-modal perception data, to recognize previously visited places in GPS-denied scenarios for robots and autonomous vehicles.","Recent fusion-based place recognition methods combine multi-modal features in implicit manners.","While achieving remarkable results, they do not explicitly consider what the individual modality affords in the fusion system.","Therefore, the benefit of multi-modal feature fusion may not be fully explored.","In this paper, we propose a novel fusion-based network, dubbed EINet, to achieve explicit interaction of the two modalities.","EINet uses LiDAR ranges to supervise more robust vision features for long time spans, and simultaneously uses camera RGB data to improve the discrimination of LiDAR point clouds.","In addition, we develop a new benchmark for the place recognition task based on the nuScenes dataset.","To establish this benchmark for future research with comprehensive comparisons, we introduce both supervised and self-supervised training schemes alongside evaluation protocols.","We conduct extensive experiments on the proposed benchmark, and the experimental results show that our EINet exhibits better recognition performance as well as solid generalization ability compared to the state-of-the-art fusion-based place recognition approaches.","Our open-source code and benchmark are released at: https://github.com/BIT-XJY/EINet."],"url":"http://arxiv.org/abs/2402.17264v1","category":"cs.CV"}
{"created":"2024-02-27 07:02:10","title":"Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection","abstract":"Out-of-domain (OOD) intent detection aims to examine whether the user's query falls outside the predefined domain of the system, which is crucial for the proper functioning of task-oriented dialogue (TOD) systems. Previous methods address it by fine-tuning discriminative models. Recently, some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, but it is still unclear for their ability on OOD detection task.This paper conducts a comprehensive evaluation of LLMs under various experimental settings, and then outline the strengths and weaknesses of LLMs. We find that LLMs exhibit strong zero-shot and few-shot capabilities, but is still at a disadvantage compared to models fine-tuned with full resource. More deeply, through a series of additional analysis experiments, we discuss and summarize the challenges faced by LLMs and provide guidance for future work including injecting domain knowledge, strengthening knowledge transfer from IND(In-domain) to OOD, and understanding long instructions.","sentences":["Out-of-domain (OOD) intent detection aims to examine whether the user's query falls outside the predefined domain of the system, which is crucial for the proper functioning of task-oriented dialogue (TOD) systems.","Previous methods address it by fine-tuning discriminative models.","Recently, some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, but it is still unclear for their ability on OOD detection task.","This paper conducts a comprehensive evaluation of LLMs under various experimental settings, and then outline the strengths and weaknesses of LLMs.","We find that LLMs exhibit strong zero-shot and few-shot capabilities, but is still at a disadvantage compared to models fine-tuned with full resource.","More deeply, through a series of additional analysis experiments, we discuss and summarize the challenges faced by LLMs and provide guidance for future work including injecting domain knowledge, strengthening knowledge transfer from IND(In-domain) to OOD, and understanding long instructions."],"url":"http://arxiv.org/abs/2402.17256v1","category":"cs.CL"}
{"created":"2024-02-27 06:57:29","title":"The Impact of ionization Morphology and X-ray Heating on the Cosmological 21cm Skew Spectrum","abstract":"The cosmological 21cm signal offers a potential probe of the early Universe and the first ionizing sources. Current experiments probe the spatially-dependent variance (Gaussianity) of the signal through the power spectrum (PS). The signal however is expected to be highly non-Gaussian due to the complex topology of reionization and X-ray heating. We investigate the non-Gaussianities of X-ray heating and reionization, by calculating the skew spectrum (SS) of the 21cm signal using MERAXES, which couples a semi-analytic galaxy population with semi-numerical reionization simulations. The SS is the cross-spectrum of the quadratic temperature brightness field with itself. We generate a set of seven simulations from $z = 30$ to $z = 5$, varying the halo mass threshold for hosting star-formation, the X-ray luminosity per star-formation rate, and the minimum X-ray energy escaping host galaxies. We find the SS is predominantly negative as a function of redshift, transitioning to positive towards the start of reionization, and peaking during the midpoint of reionization. We do not see a negative dip in the SS during reionization, likely due to the specifics of modelling ionization sources. We normalise the SS by the PS during reionization isolating the non-Gaussianities. We find a trough ($k\\sim\\,0.1\\,\\textrm{Mpc}^{-1}$) and peak ($k\\sim\\,0.4-1\\,\\textrm{Mpc}^{-1}$) in the normalised SS during the mid to late periods of reionization. These correlate to the ionization topology, and neutral islands in the IGM. We calculate the cosmic variance of the normalised SS, and find these features are detectable in the absence of foregrounds with the SKA_LOW.","sentences":["The cosmological 21cm signal offers a potential probe of the early Universe and the first ionizing sources.","Current experiments probe the spatially-dependent variance (Gaussianity) of the signal through the power spectrum (PS).","The signal however is expected to be highly non-Gaussian due to the complex topology of reionization and X-ray heating.","We investigate the non-Gaussianities of X-ray heating and reionization, by calculating the skew spectrum (SS) of the 21cm signal using MERAXES, which couples a semi-analytic galaxy population with semi-numerical reionization simulations.","The SS is the cross-spectrum of the quadratic temperature brightness field with itself.","We generate a set of seven simulations from $z = 30$ to $z = 5$, varying the halo mass threshold for hosting star-formation, the X-ray luminosity per star-formation rate, and the minimum X-ray energy escaping host galaxies.","We find the SS is predominantly negative as a function of redshift, transitioning to positive towards the start of reionization, and peaking during the midpoint of reionization.","We do not see a negative dip in the SS during reionization, likely due to the specifics of modelling ionization sources.","We normalise the SS by the PS during reionization isolating the non-Gaussianities.","We find a trough ($k\\sim\\,0.1\\,\\textrm{Mpc}^{-1}$) and peak ($k\\sim\\,0.4-1\\,\\textrm{Mpc}^{-1}$) in the normalised SS during the mid to late periods of reionization.","These correlate to the ionization topology, and neutral islands in the IGM.","We calculate the cosmic variance of the normalised SS, and find these features are detectable in the absence of foregrounds with the SKA_LOW."],"url":"http://arxiv.org/abs/2402.17254v1","category":"astro-ph.CO"}
{"created":"2024-02-27 06:35:01","title":"Synthesizing Particle-in-Cell Simulations Through Learning and GPU Computing for Hybrid Particle Accelerator Beamlines","abstract":"Particle accelerator modeling is an important field of research and development, essential to investigating, designing and operating some of the most complex scientific devices ever built. Kinetic simulations of relativistic, charged particle beams and advanced plasma accelerator elements are often performed with high-fidelity particle-in-cell simulations, some of which fill the largest GPU supercomputers. Start-to-end modeling of a particle accelerator includes many elements and it is desirable to integrate and model advanced accelerator elements fast, in effective models. Traditionally, analytical and reduced-physics models fill this role. The vast data from high-fidelity simulations and power of GPU-accelerated computation open a new opportunity to complement traditional modeling without approximations: surrogate modeling through machine learning. In this paper, we implement, present and benchmark such a data-driven workflow, synthesising a fully GPU-accelerated, conventional-surrogate simulation for hybrid particle accelerator beamlines.","sentences":["Particle accelerator modeling is an important field of research and development, essential to investigating, designing and operating some of the most complex scientific devices ever built.","Kinetic simulations of relativistic, charged particle beams and advanced plasma accelerator elements are often performed with high-fidelity particle-in-cell simulations, some of which fill the largest GPU supercomputers.","Start-to-end modeling of a particle accelerator includes many elements and it is desirable to integrate and model advanced accelerator elements fast, in effective models.","Traditionally, analytical and reduced-physics models fill this role.","The vast data from high-fidelity simulations and power of GPU-accelerated computation open a new opportunity to complement traditional modeling without approximations: surrogate modeling through machine learning.","In this paper, we implement, present and benchmark such a data-driven workflow, synthesising a fully GPU-accelerated, conventional-surrogate simulation for hybrid particle accelerator beamlines."],"url":"http://arxiv.org/abs/2402.17248v1","category":"physics.acc-ph"}
{"created":"2024-02-27 06:33:58","title":"Inverse Optimal Control for Linear Quadratic Tracking with Unknown Target States","abstract":"This paper addresses the inverse optimal control for the linear quadratic tracking problem with a fixed but unknown target state, which aims to estimate the possible triplets comprising the target state, the state weight matrix, and the input weight matrix from observed optimal control input and the corresponding state trajectories. Sufficient conditions have been provided for the unique determination of both the linear quadratic cost function as well as the target state. A computationally efficient and numerically reliable parameter identification algorithm is proposed by equating optimal control strategies with a system of linear equations, and the associated relative error upper bound is derived in terms of data volume and signal-to-noise ratio. Moreover, the proposed inverse optimal control algorithm is applied for the joint cluster coordination and intent identification of a multi-agent system. By incorporating the structural constraint of the Laplace matrix, the relative error upper bound can be reduced accordingly. Finally, the algorithm's efficiency and accuracy are validated by a vehicle-on-a-lever example and a multi-agent formation control example.","sentences":["This paper addresses the inverse optimal control for the linear quadratic tracking problem with a fixed but unknown target state, which aims to estimate the possible triplets comprising the target state, the state weight matrix, and the input weight matrix from observed optimal control input and the corresponding state trajectories.","Sufficient conditions have been provided for the unique determination of both the linear quadratic cost function as well as the target state.","A computationally efficient and numerically reliable parameter identification algorithm is proposed by equating optimal control strategies with a system of linear equations, and the associated relative error upper bound is derived in terms of data volume and signal-to-noise ratio.","Moreover, the proposed inverse optimal control algorithm is applied for the joint cluster coordination and intent identification of a multi-agent system.","By incorporating the structural constraint of the Laplace matrix, the relative error upper bound can be reduced accordingly.","Finally, the algorithm's efficiency and accuracy are validated by a vehicle-on-a-lever example and a multi-agent formation control example."],"url":"http://arxiv.org/abs/2402.17247v1","category":"eess.SY"}
{"created":"2024-02-27 06:13:02","title":"Does Negative Sampling Matter? A Review with Insights into its Theory and Applications","abstract":"Negative sampling has swiftly risen to prominence as a focal point of research, with wide-ranging applications spanning machine learning, computer vision, natural language processing, data mining, and recommender systems. This growing interest raises several critical questions: Does negative sampling really matter? Is there a general framework that can incorporate all existing negative sampling methods? In what fields is it applied? Addressing these questions, we propose a general framework that leverages negative sampling. Delving into the history of negative sampling, we trace the development of negative sampling through five evolutionary paths. We dissect and categorize the strategies used to select negative sample candidates, detailing global, local, mini-batch, hop, and memory-based approaches. Our review categorizes current negative sampling methods into five types: static, hard, GAN-based, Auxiliary-based, and In-batch methods, providing a clear structure for understanding negative sampling. Beyond detailed categorization, we highlight the application of negative sampling in various areas, offering insights into its practical benefits. Finally, we briefly discuss open problems and future directions for negative sampling.","sentences":["Negative sampling has swiftly risen to prominence as a focal point of research, with wide-ranging applications spanning machine learning, computer vision, natural language processing, data mining, and recommender systems.","This growing interest raises several critical questions: Does negative sampling really matter?","Is there a general framework that can incorporate all existing negative sampling methods?","In what fields is it applied?","Addressing these questions, we propose a general framework that leverages negative sampling.","Delving into the history of negative sampling, we trace the development of negative sampling through five evolutionary paths.","We dissect and categorize the strategies used to select negative sample candidates, detailing global, local, mini-batch, hop, and memory-based approaches.","Our review categorizes current negative sampling methods into five types: static, hard, GAN-based, Auxiliary-based, and In-batch methods, providing a clear structure for understanding negative sampling.","Beyond detailed categorization, we highlight the application of negative sampling in various areas, offering insights into its practical benefits.","Finally, we briefly discuss open problems and future directions for negative sampling."],"url":"http://arxiv.org/abs/2402.17238v1","category":"cs.LG"}
{"created":"2024-02-27 06:11:54","title":"Image-Text Matching with Multi-View Attention","abstract":"Existing two-stream models for image-text matching show good performance while ensuring retrieval speed and have received extensive attention from industry and academia. These methods use a single representation to encode image and text separately and get a matching score with cosine similarity or the inner product of vectors. However, the performance of the two-stream model is often sub-optimal. On the one hand, a single representation is challenging to cover complex content comprehensively. On the other hand, in this framework of lack of interaction, it is challenging to match multiple meanings which leads to information being ignored. To address the problems mentioned above and facilitate the performance of the two-stream model, we propose a multi-view attention approach for two-stream image-text matching MVAM (\\textbf{M}ulti-\\textbf{V}iew \\textbf{A}ttention \\textbf{M}odel). It first learns multiple image and text representations by diverse attention heads with different view codes. And then concatenate these representations into one for matching. A diversity objective is also used to promote diversity between attention heads. With this method, models are able to encode images and text from different views and attend to more key points. So we can get representations that contain more information. When doing retrieval tasks, the matching scores between images and texts can be calculated from different aspects, leading to better matching performance. Experiment results on MSCOCO and Flickr30K show that our proposed model brings improvements over existing models. Further case studies show that different attention heads can focus on different contents and finally obtain a more comprehensive representation.","sentences":["Existing two-stream models for image-text matching show good performance while ensuring retrieval speed and have received extensive attention from industry and academia.","These methods use a single representation to encode image and text separately and get a matching score with cosine similarity or the inner product of vectors.","However, the performance of the two-stream model is often sub-optimal.","On the one hand, a single representation is challenging to cover complex content comprehensively.","On the other hand, in this framework of lack of interaction, it is challenging to match multiple meanings which leads to information being ignored.","To address the problems mentioned above and facilitate the performance of the two-stream model, we propose a multi-view attention approach for two-stream image-text matching MVAM (\\textbf{M}ulti-\\textbf{V}iew \\textbf{A}ttention \\textbf{M}odel).","It first learns multiple image and text representations by diverse attention heads with different view codes.","And then concatenate these representations into one for matching.","A diversity objective is also used to promote diversity between attention heads.","With this method, models are able to encode images and text from different views and attend to more key points.","So we can get representations that contain more information.","When doing retrieval tasks, the matching scores between images and texts can be calculated from different aspects, leading to better matching performance.","Experiment results on MSCOCO and Flickr30K show that our proposed model brings improvements over existing models.","Further case studies show that different attention heads can focus on different contents and finally obtain a more comprehensive representation."],"url":"http://arxiv.org/abs/2402.17237v1","category":"cs.CV"}
{"created":"2024-02-27 06:02:31","title":"On the microscopic foundation of thermodynamics and kinetics. Current status and prospects","abstract":"A comparative analysis of two concepts aimed at microscopic substantiation of thermodynamics and kinetics has been performed. The first concept is based on the idea of microscopic reversibility of the dynamics of a system of particles, while macroscopic irreversibility is of statistical origin. The second concept is based on the idea of the initial microscopic irreversibility of dynamics, one of the mechanisms of which is the relativistic retardation of interactions between particles.","sentences":["A comparative analysis of two concepts aimed at microscopic substantiation of thermodynamics and kinetics has been performed.","The first concept is based on the idea of microscopic reversibility of the dynamics of a system of particles, while macroscopic irreversibility is of statistical origin.","The second concept is based on the idea of the initial microscopic irreversibility of dynamics, one of the mechanisms of which is the relativistic retardation of interactions between particles."],"url":"http://arxiv.org/abs/2402.17234v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-27 06:01:56","title":"Hybrid Square Neural ODE Causal Modeling","abstract":"Hybrid models combine mechanistic ODE-based dynamics with flexible and expressive neural network components. Such models have grown rapidly in popularity, especially in scientific domains where such ODE-based modeling offers important interpretability and validated causal grounding (e.g., for counterfactual reasoning). The incorporation of mechanistic models also provides inductive bias in standard blackbox modeling approaches, critical when learning from small datasets or partially observed, complex systems. Unfortunately, as hybrid models become more flexible, the causal grounding provided by the mechanistic model can quickly be lost. We address this problem by leveraging another common source of domain knowledge: ranking of treatment effects for a set of interventions, even if the precise treatment effect is unknown. We encode this information in a causal loss that we combine with the standard predictive loss to arrive at a hybrid loss that biases our learning towards causally valid hybrid models. We demonstrate our ability to achieve a win-win -- state-of-the-art predictive performance and causal validity -- in the challenging task of modeling glucose dynamics during exercise.","sentences":["Hybrid models combine mechanistic ODE-based dynamics with flexible and expressive neural network components.","Such models have grown rapidly in popularity, especially in scientific domains where such ODE-based modeling offers important interpretability and validated causal grounding (e.g., for counterfactual reasoning).","The incorporation of mechanistic models also provides inductive bias in standard blackbox modeling approaches, critical when learning from small datasets or partially observed, complex systems.","Unfortunately, as hybrid models become more flexible, the causal grounding provided by the mechanistic model can quickly be lost.","We address this problem by leveraging another common source of domain knowledge: ranking of treatment effects for a set of interventions, even if the precise treatment effect is unknown.","We encode this information in a causal loss that we combine with the standard predictive loss to arrive at a hybrid loss that biases our learning towards causally valid hybrid models.","We demonstrate our ability to achieve a win-win -- state-of-the-art predictive performance and causal validity -- in the challenging task of modeling glucose dynamics during exercise."],"url":"http://arxiv.org/abs/2402.17233v1","category":"cs.LG"}
{"created":"2024-02-27 05:50:35","title":"MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning","abstract":"Tool-augmented Large Language Models (TALM) are known to enhance the skillset of large language models (LLM), thereby, leading to their improved reasoning abilities across many tasks. While, TALMs have been successfully employed in different question-answering benchmarks, their efficacy on complex mathematical reasoning benchmarks, and the potential complimentary benefits offered by tools for knowledge retrieval and mathematical equation solving, are open research questions. In this work, we present MATHSENSEI, a tool-augmented large language model for mathematical reasoning. Augmented with tools for knowledge retrieval (Bing Web Search), program execution (Python), and symbolic equation solving (Wolfram-Alpha), we study the complimentary benefits of these tools through evaluations on mathematical reasoning datasets. We perform exhaustive ablations on MATH,a popular dataset for evaluating mathematical reasoning on diverse mathematical disciplines. We also conduct experiments involving well-known tool planners to study the impact of tool sequencing on the model performance. MATHSENSEI achieves 13.5% better accuracy over gpt-3.5-turbo with chain-of-thought on the MATH dataset. We further observe that TALMs are not as effective for simpler math word problems (in GSM-8k), and the benefit increases as the complexity and required knowledge increases (progressively over AQuA, MMLU-Math, and higher level complex questions in MATH). The code and data are available at https://github.com/Debrup-61/MathSensei.","sentences":["Tool-augmented Large Language Models (TALM) are known to enhance the skillset of large language models (LLM), thereby, leading to their improved reasoning abilities across many tasks.","While, TALMs have been successfully employed in different question-answering benchmarks, their efficacy on complex mathematical reasoning benchmarks, and the potential complimentary benefits offered by tools for knowledge retrieval and mathematical equation solving, are open research questions.","In this work, we present MATHSENSEI, a tool-augmented large language model for mathematical reasoning.","Augmented with tools for knowledge retrieval (Bing Web Search), program execution (Python), and symbolic equation solving (Wolfram-Alpha), we study the complimentary benefits of these tools through evaluations on mathematical reasoning datasets.","We perform exhaustive ablations on MATH,a popular dataset for evaluating mathematical reasoning on diverse mathematical disciplines.","We also conduct experiments involving well-known tool planners to study the impact of tool sequencing on the model performance.","MATHSENSEI achieves 13.5% better accuracy over gpt-3.5-turbo with chain-of-thought on the MATH dataset.","We further observe that TALMs are not as effective for simpler math word problems (in GSM-8k), and the benefit increases as the complexity and required knowledge increases (progressively over AQuA, MMLU-Math, and higher level complex questions in MATH).","The code and data are available at https://github.com/Debrup-61/MathSensei."],"url":"http://arxiv.org/abs/2402.17231v1","category":"cs.CL"}
{"created":"2024-02-27 05:31:11","title":"Deadzone-Adapted Disturbance Suppression Control for Strict-Feedback Systems","abstract":"In this paper we extend our recently proposed Deadzone-Adapted Disturbance Suppression (DADS) Control approach from systems with matched uncertainties to general systems in parametric strict feedback form. The DADS approach prevents gain and state drift regardless of the size of the disturbance and unknown parameter and achieves an attenuation of the plant output to an assignable small level, despite the presence of persistent disturbances and unknown parameters of arbitrary and unknown bounds. The controller is designed by means of a step-by-step backstepping procedure which can be applied in an algorithmic fashion. Examples are provided which illustrate the efficiency of the DADS controller compared to existing adaptive control schemes.","sentences":["In this paper we extend our recently proposed Deadzone-Adapted Disturbance Suppression (DADS)","Control approach from systems with matched uncertainties to general systems in parametric strict feedback form.","The DADS approach prevents gain and state drift regardless of the size of the disturbance and unknown parameter and achieves an attenuation of the plant output to an assignable small level, despite the presence of persistent disturbances and unknown parameters of arbitrary and unknown bounds.","The controller is designed by means of a step-by-step backstepping procedure which can be applied in an algorithmic fashion.","Examples are provided which illustrate the efficiency of the DADS controller compared to existing adaptive control schemes."],"url":"http://arxiv.org/abs/2402.17222v1","category":"math.OC"}
{"created":"2024-02-27 05:25:05","title":"Blockchain for Finance: A Survey","abstract":"As an innovative technology for enhancing authenticity, security, and risk management, blockchain is being widely adopted in trade and finance systems. The unique capabilities of blockchain, such as immutability and transparency, enable new business models of distributed data storage, point-to-point transactions, and decentralized autonomous organizations. In this paper, we focus on blockchain-based securities trading, in which blockchain technology plays a vital role in financial services as it ultimately lifts trust and frees the need for third-party verification by using consensus-based verification. We investigate the 12 most popular blockchain platforms and elaborate on 6 platforms that are related to finance, seeking to provide a panorama of securities trading practices. Meanwhile, this survey provides a comprehensive summary of blockchain-based securities trading applications. We gather numerous practical applications of blockchain-based securities trading and categorize them into four distinct categories. For each category, we introduce a typical example and explain how blockchain contributes to solving the key problems faced by FinTech companies and researchers. Finally, we provide interesting observations ranging from mainstream blockchain-based financial institutions to security issues of decentralized finance applications, aiming to picture the current blockchain ecosystem in finance.","sentences":["As an innovative technology for enhancing authenticity, security, and risk management, blockchain is being widely adopted in trade and finance systems.","The unique capabilities of blockchain, such as immutability and transparency, enable new business models of distributed data storage, point-to-point transactions, and decentralized autonomous organizations.","In this paper, we focus on blockchain-based securities trading, in which blockchain technology plays a vital role in financial services as it ultimately lifts trust and frees the need for third-party verification by using consensus-based verification.","We investigate the 12 most popular blockchain platforms and elaborate on 6 platforms that are related to finance, seeking to provide a panorama of securities trading practices.","Meanwhile, this survey provides a comprehensive summary of blockchain-based securities trading applications.","We gather numerous practical applications of blockchain-based securities trading and categorize them into four distinct categories.","For each category, we introduce a typical example and explain how blockchain contributes to solving the key problems faced by FinTech companies and researchers.","Finally, we provide interesting observations ranging from mainstream blockchain-based financial institutions to security issues of decentralized finance applications, aiming to picture the current blockchain ecosystem in finance."],"url":"http://arxiv.org/abs/2402.17219v1","category":"cs.CR"}
{"created":"2024-02-27 05:21:39","title":"Viblio: Introducing Credibility Signals and Citations to Video-Sharing Platforms","abstract":"As more users turn to video-sharing platforms like YouTube as an information source, they may consume misinformation despite their best efforts. In this work, we investigate ways that users can better assess the credibility of videos by first exploring how users currently determine credibility using existing signals on platforms and then by introducing and evaluating new credibility-based signals. We conducted 12 contextual inquiry interviews with YouTube users, determining that participants used a combination of existing signals, such as the channel name, the production quality, and prior knowledge, to evaluate credibility, yet sometimes stumbled in their efforts to do so. We then developed Viblio, a prototype system that enables YouTube users to view and add citations and related information while watching a video based on our participants' needs. From an evaluation with 12 people, all participants found Viblio to be intuitive and useful in the process of evaluating a video's credibility and could see themselves using Viblio in the future.","sentences":["As more users turn to video-sharing platforms like YouTube as an information source, they may consume misinformation despite their best efforts.","In this work, we investigate ways that users can better assess the credibility of videos by first exploring how users currently determine credibility using existing signals on platforms and then by introducing and evaluating new credibility-based signals.","We conducted 12 contextual inquiry interviews with YouTube users, determining that participants used a combination of existing signals, such as the channel name, the production quality, and prior knowledge, to evaluate credibility, yet sometimes stumbled in their efforts to do so.","We then developed Viblio, a prototype system that enables YouTube users to view and add citations and related information while watching a video based on our participants' needs.","From an evaluation with 12 people, all participants found Viblio to be intuitive and useful in the process of evaluating a video's credibility and could see themselves using Viblio in the future."],"url":"http://arxiv.org/abs/2402.17218v1","category":"cs.CY"}
{"created":"2024-02-27 05:10:59","title":"CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization","abstract":"In the field of digital content creation, generating high-quality 3D characters from single images is challenging, especially given the complexities of various body poses and the issues of self-occlusion and pose ambiguity. In this paper, we present CharacterGen, a framework developed to efficiently generate 3D characters. CharacterGen introduces a streamlined generation pipeline along with an image-conditioned multi-view diffusion model. This model effectively calibrates input poses to a canonical form while retaining key attributes of the input image, thereby addressing the challenges posed by diverse poses. A transformer-based, generalizable sparse-view reconstruction model is the other core component of our approach, facilitating the creation of detailed 3D models from multi-view images. We also adopt a texture-back-projection strategy to produce high-quality texture maps. Additionally, we have curated a dataset of anime characters, rendered in multiple poses and views, to train and evaluate our model. Our approach has been thoroughly evaluated through quantitative and qualitative experiments, showing its proficiency in generating 3D characters with high-quality shapes and textures, ready for downstream applications such as rigging and animation.","sentences":["In the field of digital content creation, generating high-quality 3D characters from single images is challenging, especially given the complexities of various body poses and the issues of self-occlusion and pose ambiguity.","In this paper, we present CharacterGen, a framework developed to efficiently generate 3D characters.","CharacterGen introduces a streamlined generation pipeline along with an image-conditioned multi-view diffusion model.","This model effectively calibrates input poses to a canonical form while retaining key attributes of the input image, thereby addressing the challenges posed by diverse poses.","A transformer-based, generalizable sparse-view reconstruction model is the other core component of our approach, facilitating the creation of detailed 3D models from multi-view images.","We also adopt a texture-back-projection strategy to produce high-quality texture maps.","Additionally, we have curated a dataset of anime characters, rendered in multiple poses and views, to train and evaluate our model.","Our approach has been thoroughly evaluated through quantitative and qualitative experiments, showing its proficiency in generating 3D characters with high-quality shapes and textures, ready for downstream applications such as rigging and animation."],"url":"http://arxiv.org/abs/2402.17214v1","category":"cs.CV"}
{"created":"2024-02-27 05:09:37","title":"Constraining Planetary Formation Models Using Conditional Occurrences of Various Planet Types","abstract":"We report the conditional occurrences between three planetary types: super-Earths (m sin i $<$ 10 M$_\\oplus$, P $<$ 100 days), warm Jupiters (m sin i $>$ 95 $M_\\oplus$, 10 $<$ P $<$ 100 days), and cold Jupiters (m sin i $>$ 95 M$_\\oplus$, P $>$ 400 days) for sun-like stars. We find that while the occurrence of cold Jupiters in systems with super-Earths is $22.2\\substack{+8.3\\\\-5.4}$$\\%$, compared to $10$$\\%$ for the absolute occurrence rate of cold Jupiters, the occurrence of super-Earths in systems with cold Jupiters is $66.0\\substack{+18.0\\\\-16.0}$$\\%$, compared to $30$$\\%$ for the absolute occurrence rate of super-Earths for sun-like stars. We find that the enhancement of super-Earths in systems with cold Jupiters is evident for sun-like stars, in agreement with several previous studies. We also conduct occurrence studies between warm Jupiters and super-Earths, and between warm Jupiters and cold Jupiters, to consolidate our methods. We conduct an independent observational test to study the effects of cold Jupiters against the inner multiplicity using the well-established giant planet host star metallicity correlation for all transiting planets found to date. The conditional occurrences we find here can be used to constrain the validity of various planetary formation models. The extremely interesting correlations between the super-Earths, cold Jupiters, and warm Jupiters can also be used to understand the formation histories of these planetary types.","sentences":["We report the conditional occurrences between three planetary types: super-Earths (m sin i $<$ 10 M$_\\oplus$, P $<$ 100 days), warm Jupiters (m sin i $>$ 95 $M_\\oplus$, 10 $<$ P $<$ 100 days), and cold Jupiters (m sin i $>$ 95 M$_\\oplus$, P $>$ 400 days) for sun-like stars.","We find that while the occurrence of cold Jupiters in systems with super-Earths is $22.2\\substack{+8.3\\\\-5.4}$$\\%$, compared to $10$$\\%$ for the absolute occurrence rate of cold Jupiters, the occurrence of super-Earths in systems with cold Jupiters is $66.0\\substack{+18.0\\\\-16.0}$$\\%$, compared to $30$$\\%$ for the absolute occurrence rate of super-Earths for sun-like stars.","We find that the enhancement of super-Earths in systems with cold Jupiters is evident for sun-like stars, in agreement with several previous studies.","We also conduct occurrence studies between warm Jupiters and super-Earths, and between warm Jupiters and cold Jupiters, to consolidate our methods.","We conduct an independent observational test to study the effects of cold Jupiters against the inner multiplicity using the well-established giant planet host star metallicity correlation for all transiting planets found to date.","The conditional occurrences we find here can be used to constrain the validity of various planetary formation models.","The extremely interesting correlations between the super-Earths, cold Jupiters, and warm Jupiters can also be used to understand the formation histories of these planetary types."],"url":"http://arxiv.org/abs/2402.17212v1","category":"astro-ph.EP"}
{"created":"2024-02-27 05:04:58","title":"Corner entanglement of a resonating valence bond wavefunction","abstract":"We perform a quantum Monte Carlo simulation of the resonating valence bond wavefunction on a two-dimensional square lattice with periodic boundary conditions. Using two replicas of the system, we calculate the second Renyi entropy on a spatial bipartition with a square geometry. Through a finite-size scaling analysis, we extract the logarithmic correction to the area law due to the presence of the sharp corners in the entangling surface. We find that the coefficient of this logarithm is positive with a value of 0.073 for a single $90^{\\circ}$ corner.","sentences":["We perform a quantum Monte Carlo simulation of the resonating valence bond wavefunction on a two-dimensional square lattice with periodic boundary conditions.","Using two replicas of the system, we calculate the second Renyi entropy on a spatial bipartition with a square geometry.","Through a finite-size scaling analysis, we extract the logarithmic correction to the area law due to the presence of the sharp corners in the entangling surface.","We find that the coefficient of this logarithm is positive with a value of 0.073 for a single $90^{\\circ}$ corner."],"url":"http://arxiv.org/abs/2402.17211v1","category":"cond-mat.str-el"}
{"created":"2024-02-27 04:53:53","title":"Advancing Generative Model Evaluation: A Novel Algorithm for Realistic Image Synthesis and Comparison in OCR System","abstract":"This research addresses a critical challenge in the field of generative models, particularly in the generation and evaluation of synthetic images. Given the inherent complexity of generative models and the absence of a standardized procedure for their comparison, our study introduces a pioneering algorithm to objectively assess the realism of synthetic images. This approach significantly enhances the evaluation methodology by refining the Fr\\'echet Inception Distance (FID) score, allowing for a more precise and subjective assessment of image quality. Our algorithm is particularly tailored to address the challenges in generating and evaluating realistic images of Arabic handwritten digits, a task that has traditionally been near-impossible due to the subjective nature of realism in image generation. By providing a systematic and objective framework, our method not only enables the comparison of different generative models but also paves the way for improvements in their design and output. This breakthrough in evaluation and comparison is crucial for advancing the field of OCR, especially for scripts that present unique complexities, and sets a new standard in the generation and assessment of high-quality synthetic images.","sentences":["This research addresses a critical challenge in the field of generative models, particularly in the generation and evaluation of synthetic images.","Given the inherent complexity of generative models and the absence of a standardized procedure for their comparison, our study introduces a pioneering algorithm to objectively assess the realism of synthetic images.","This approach significantly enhances the evaluation methodology by refining the Fr\\'echet Inception Distance (FID) score, allowing for a more precise and subjective assessment of image quality.","Our algorithm is particularly tailored to address the challenges in generating and evaluating realistic images of Arabic handwritten digits, a task that has traditionally been near-impossible due to the subjective nature of realism in image generation.","By providing a systematic and objective framework, our method not only enables the comparison of different generative models but also paves the way for improvements in their design and output.","This breakthrough in evaluation and comparison is crucial for advancing the field of OCR, especially for scripts that present unique complexities, and sets a new standard in the generation and assessment of high-quality synthetic images."],"url":"http://arxiv.org/abs/2402.17204v1","category":"cs.CV"}
{"created":"2024-02-27 04:50:19","title":"Nonstandard diffeology and generalized functions","abstract":"We introduce a nonstandard extension of the category of diffeological spaces, and demonstrate its application to the study of generalized functions. Just as diffeological spaces are defined as concrete sheaves on the site of Euclidean open sets, our nonstandard diffeological spaces are defined as concrete sheaves on the site of open subsets of nonstandard Euclidean spaces, i.e. finite dimensional vector spaces over (the quasi-asymptotic variant of) Robinson's hyperreal numbers. It is shown that nonstandard diffeological spaces form a category which is enriched over the category of diffeological spaces, is closed under small limits and colimits, and is cartesian closed. Furthermore, it can be shown that the space of nonstandard functions on the extension of a Euclidean open set is a smooth differential algebra that admits an embedding of the differential vector space of Schwartz distributions. Since our algebra of generalized functions comes as a hom-object in a category, it enables not only the multiplication of distributions but also the composition of them. To illustrate the usefulness of this property we show that the homotopy extension property can be established for smooth relative cell complexes by exploiting extended maps.","sentences":["We introduce a nonstandard extension of the category of diffeological spaces, and demonstrate its application to the study of generalized functions.","Just as diffeological spaces are defined as concrete sheaves on the site of Euclidean open sets, our nonstandard diffeological spaces are defined as concrete sheaves on the site of open subsets of nonstandard Euclidean spaces, i.e. finite dimensional vector spaces over (the quasi-asymptotic variant of) Robinson's hyperreal numbers.","It is shown that nonstandard diffeological spaces form a category which is enriched over the category of diffeological spaces, is closed under small limits and colimits, and is cartesian closed.","Furthermore, it can be shown that the space of nonstandard functions on the extension of a Euclidean open set is a smooth differential algebra that admits an embedding of the differential vector space of Schwartz distributions.","Since our algebra of generalized functions comes as a hom-object in a category, it enables not only the multiplication of distributions but also the composition of them.","To illustrate the usefulness of this property we show that the homotopy extension property can be established for smooth relative cell complexes by exploiting extended maps."],"url":"http://arxiv.org/abs/2402.17203v1","category":"math.AT"}
{"created":"2024-02-27 04:37:21","title":"A Decentralized Market Mechanism for Energy Communities under Operating Envelopes","abstract":"We propose an operating envelopes (OEs) aware energy community market mechanism that dynamically charges/rewards its members based on two-part pricing. The OEs are imposed exogenously by a regulated distribution system operator (DSO) on the energy community's revenue meter that is subject to a generalized net energy metering (NEM) tariff design. By formulating the interaction of the community manager and its members as a Stackelberg game, we show that the proposed two-part pricing achieves a Nash equilibrium and maximizes the community's social welfare in a decentralized fashion while ensuring that the community's operation abides by the OEs. The market mechanism conforms with the cost-causation principle and guarantees community members a surplus level no less than their maximum surplus when they autonomously face the DSO. The dynamic and uniform community price is a monotonically decreasing function of the community's aggregate renewable generation. We also analyze the impact of exogenous parameters such as NEM rates and OEs on the value of joining the community. Lastly, through numerical studies, we showcase the community's welfare, pricing, and compare its members' surplus to customers under the DSO and other OEs arrangements.","sentences":["We propose an operating envelopes (OEs) aware energy community market mechanism that dynamically charges/rewards its members based on two-part pricing.","The OEs are imposed exogenously by a regulated distribution system operator (DSO) on the energy community's revenue meter that is subject to a generalized net energy metering (NEM) tariff design.","By formulating the interaction of the community manager and its members as a Stackelberg game, we show that the proposed two-part pricing achieves a Nash equilibrium and maximizes the community's social welfare in a decentralized fashion while ensuring that the community's operation abides by the OEs.","The market mechanism conforms with the cost-causation principle and guarantees community members a surplus level no less than their maximum surplus when they autonomously face the DSO.","The dynamic and uniform community price is a monotonically decreasing function of the community's aggregate renewable generation.","We also analyze the impact of exogenous parameters such as NEM rates and OEs on the value of joining the community.","Lastly, through numerical studies, we showcase the community's welfare, pricing, and compare its members' surplus to customers under the DSO and other OEs arrangements."],"url":"http://arxiv.org/abs/2402.17201v1","category":"eess.SY"}
{"created":"2024-02-27 04:24:24","title":"Anomalous acousto-current within the quantum Hall plateaus","abstract":"We systematically study the acousto-current of two-dimensional electron systems in the integer and fractional quantum Hall regimes using surface acoustic waves. We are able to separate the co-existing acoustic scattering and drag, when phonons induce drag current and tune the electron conductivity, respectively. At large acoustic power, the drag current is finite when the system is compressible and exhibits minima when incompressible quantum Hall effects appear. Surprisingly, it exhibits anomalously large bipolar spikes within the quantum Hall plateaus while it vanishes linearly with reduced acoustic power at compressible phases. The current peaks reverse their polarity at the two flanks of exact integer or fractional fillings, consistent with the opposite electric charge of the quasiparticle/quasihole.","sentences":["We systematically study the acousto-current of two-dimensional electron systems in the integer and fractional quantum Hall regimes using surface acoustic waves.","We are able to separate the co-existing acoustic scattering and drag, when phonons induce drag current and tune the electron conductivity, respectively.","At large acoustic power, the drag current is finite when the system is compressible and exhibits minima when incompressible quantum Hall effects appear.","Surprisingly, it exhibits anomalously large bipolar spikes within the quantum Hall plateaus while it vanishes linearly with reduced acoustic power at compressible phases.","The current peaks reverse their polarity at the two flanks of exact integer or fractional fillings, consistent with the opposite electric charge of the quasiparticle/quasihole."],"url":"http://arxiv.org/abs/2402.17197v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-27 03:58:39","title":"PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning","abstract":"Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems. These modalities provide intuitive semantics that facilitate modality-aware user preference modeling. However, two key challenges in multi-modal recommenders remain unresolved: i) The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT). ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference. To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation. Specifically, PromptMM conducts model compression through distilling u-i edge relationship and multi-modal node content from cumbersome teachers to relieve students from the additional feature reduction parameters. To bridge the semantic gap between multi-modal context and collaborative signals for empowering the overfitting teacher, soft prompt-tuning is introduced to perform student task-adaptive. Additionally, to adjust the impact of inaccuracies in multimedia data, a disentangled multi-modal list-wise distillation is developed with modality-aware re-weighting mechanism. Experiments on real-world data demonstrate PromptMM's superiority over existing techniques. Ablation tests confirm the effectiveness of key components. Additional tests show the efficiency and effectiveness.","sentences":["Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems.","These modalities provide intuitive semantics that facilitate modality-aware user preference modeling.","However, two key challenges in multi-modal recommenders remain unresolved: i)","The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT).","ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference.","To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation.","Specifically, PromptMM conducts model compression through distilling u-i edge relationship and multi-modal node content from cumbersome teachers to relieve students from the additional feature reduction parameters.","To bridge the semantic gap between multi-modal context and collaborative signals for empowering the overfitting teacher, soft prompt-tuning is introduced to perform student task-adaptive.","Additionally, to adjust the impact of inaccuracies in multimedia data, a disentangled multi-modal list-wise distillation is developed with modality-aware re-weighting mechanism.","Experiments on real-world data demonstrate PromptMM's superiority over existing techniques.","Ablation tests confirm the effectiveness of key components.","Additional tests show the efficiency and effectiveness."],"url":"http://arxiv.org/abs/2402.17188v1","category":"cs.IR"}
{"created":"2024-02-27 03:34:58","title":"On the variety of X-states","abstract":"We introduce the notion of an X-state on $n$-qubits. After taking the Zariski closure of the set of X-states in the space of all mixed states, we obtain a complex algebraic variety $\\scr X$ that is equipped with the action of the Lie group of local symmetries $G$. We show that the field of $G$-invariant rational functions on $\\scr X$ is purely transcendental over the complex numbers of degree $2^{2n-1}-n-1$.","sentences":["We introduce the notion of an X-state on $n$-qubits.","After taking the Zariski closure of the set of X-states in the space of all mixed states, we obtain a complex algebraic variety $\\scr X$ that is equipped with the action of the Lie group of local symmetries $G$. We show that the field of $G$-invariant rational functions on $\\scr X$ is purely transcendental over the complex numbers of degree $2^{2n-1}-n-1$."],"url":"http://arxiv.org/abs/2402.17181v1","category":"math-ph"}
{"created":"2024-02-27 03:32:04","title":"NeuralSI: Neural Design of Semantic Interaction for Interactive Deep Learning","abstract":"An increasing number of studies have utilized interactive deep learning as the analytic model of visual analytics systems for complex sensemaking tasks. In these systems, traditional interactive dimensionality reduction (DR) models are commonly utilized to build a bi-directional bridge between high-dimensional deep learning representations and low-dimensional visualizations. While these systems better capture analysts' intents in the context of human-in-the-loop interactive deep learning, traditional DR cannot support several desired properties for visual analytics, including out-of-sample extensions, stability, and real-time inference. To avoid this issue, we propose the neural design framework of semantic interaction for interactive deep learning. In our framework, we replace the traditional DR with a neural projection network and append it to the deep learning model as the task-specific output layer. Therefore, the analytic model (deep learning) and visualization method (interactive DR) form one integrated end-to-end trainable deep neural network. In order to understand the performance of the neural design in comparison to the state-of-the-art, we systematically performed two complementary studies, a human-centered qualitative case study and an algorithm-centered simulation-based quantitative experiment. The results of these studies indicate that the neural design can give semantic interaction systems substantial advantages while still keeping comparable inference ability compared to the state-of-the-art model.","sentences":["An increasing number of studies have utilized interactive deep learning as the analytic model of visual analytics systems for complex sensemaking tasks.","In these systems, traditional interactive dimensionality reduction (DR) models are commonly utilized to build a bi-directional bridge between high-dimensional deep learning representations and low-dimensional visualizations.","While these systems better capture analysts' intents in the context of human-in-the-loop interactive deep learning, traditional DR cannot support several desired properties for visual analytics, including out-of-sample extensions, stability, and real-time inference.","To avoid this issue, we propose the neural design framework of semantic interaction for interactive deep learning.","In our framework, we replace the traditional DR with a neural projection network and append it to the deep learning model as the task-specific output layer.","Therefore, the analytic model (deep learning) and visualization method (interactive DR) form one integrated end-to-end trainable deep neural network.","In order to understand the performance of the neural design in comparison to the state-of-the-art, we systematically performed two complementary studies, a human-centered qualitative case study and an algorithm-centered simulation-based quantitative experiment.","The results of these studies indicate that the neural design can give semantic interaction systems substantial advantages while still keeping comparable inference ability compared to the state-of-the-art model."],"url":"http://arxiv.org/abs/2402.17178v1","category":"cs.HC"}
{"created":"2024-02-27 03:24:54","title":"DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection","abstract":"Model-X knockoff, among various feature selection methods, received much attention recently due to its guarantee on false discovery rate (FDR) control. Subsequent to its introduction in parametric design, knockoff is advanced to handle arbitrary data distributions using deep learning-based generative modeling. However, we observed that current implementations of the deep Model-X knockoff framework exhibit limitations. Notably, the \"swap property\" that knockoffs necessitate frequently encounter challenges on sample level, leading to a diminished selection power. To overcome, we develop \"Deep Dependency Regularized Knockoff (DeepDRK)\", a distribution-free deep learning method that strikes a balance between FDR and power. In DeepDRK, a generative model grounded in a transformer architecture is introduced to better achieve the \"swap property\". Novel efficient regularization techniques are also proposed to reach higher power. Our model outperforms other benchmarks in synthetic, semi-synthetic, and real-world data, especially when sample size is small and data distribution is complex.","sentences":["Model-X knockoff, among various feature selection methods, received much attention recently due to its guarantee on false discovery rate (FDR) control.","Subsequent to its introduction in parametric design, knockoff is advanced to handle arbitrary data distributions using deep learning-based generative modeling.","However, we observed that current implementations of the deep Model-X knockoff framework exhibit limitations.","Notably, the \"swap property\" that knockoffs necessitate frequently encounter challenges on sample level, leading to a diminished selection power.","To overcome, we develop \"Deep Dependency Regularized Knockoff (DeepDRK)\", a distribution-free deep learning method that strikes a balance between FDR and power.","In DeepDRK, a generative model grounded in a transformer architecture is introduced to better achieve the \"swap property\".","Novel efficient regularization techniques are also proposed to reach higher power.","Our model outperforms other benchmarks in synthetic, semi-synthetic, and real-world data, especially when sample size is small and data distribution is complex."],"url":"http://arxiv.org/abs/2402.17176v1","category":"cs.LG"}
{"created":"2024-02-27 03:22:10","title":"On Gaiotto's positivity conjecture","abstract":"We prove a conjecture of D. Gaiotto on positivity of inner products arising in studying Landau-Ginzburg boundary conditions for 3d free hypermultiplets in the 1-dimensional case, and in special cases in higher dimensions.","sentences":["We prove a conjecture of D. Gaiotto on positivity of inner products arising in studying Landau-Ginzburg boundary conditions for 3d free hypermultiplets in the 1-dimensional case, and in special cases in higher dimensions."],"url":"http://arxiv.org/abs/2402.17174v1","category":"math.CA"}
{"created":"2024-02-27 03:00:34","title":"Converse Barrier Certificates for Finite-time Safety Verification of Continuous-time Perturbed Deterministic Systems","abstract":"In this paper, we investigate the problem of verifying the finite-time safety of continuous-time perturbed deterministic systems represented by ordinary differential equations in the presence of measurable disturbances. Given a finite time horizon, if the system is safe, it, starting from a compact initial set, will remain within an open and bounded safe region throughout the specified time horizon, regardless of the disturbances. The main contribution of this work is to uncover that there exists a time-dependent barrier certificate if and only if the system is safe. This barrier certificate satisfies the following conditions: negativity over the initial set at the initial time instant, non-negativity over the boundary of the safe set, and non-increasing behavior along the system dynamics over the specified finite time horizon. The existence problem is explored using a Hamilton-Jacobi differential equation, which has a unique Lipschitz viscosity solution.","sentences":["In this paper, we investigate the problem of verifying the finite-time safety of continuous-time perturbed deterministic systems represented by ordinary differential equations in the presence of measurable disturbances.","Given a finite time horizon, if the system is safe, it, starting from a compact initial set, will remain within an open and bounded safe region throughout the specified time horizon, regardless of the disturbances.","The main contribution of this work is to uncover that there exists a time-dependent barrier certificate if and only if the system is safe.","This barrier certificate satisfies the following conditions: negativity over the initial set at the initial time instant, non-negativity over the boundary of the safe set, and non-increasing behavior along the system dynamics over the specified finite time horizon.","The existence problem is explored using a Hamilton-Jacobi differential equation, which has a unique Lipschitz viscosity solution."],"url":"http://arxiv.org/abs/2402.17167v1","category":"eess.SY"}
{"created":"2024-02-27 02:45:58","title":"A Szemer\u00e9di type theorem for sets of positive density in cut-and-project sets","abstract":"An extension of Szemer\\'edi's Theorem is proved for sets of positive density in cut-and-project sets in general locally compact and second countable abelian groups. As a consequence, we establish a recent conjecture of Klick, Strungaru and Tcaciuc. Furthermore, we show a Szemeredi type theorem for sets of positive density in the set of fractional parts of $\\mathbb{Q}_p$. Via a novel version of Furstenberg's Correspondence principle, which should be of independent interest, we show that our Szemer\\'edi Theorems can be deduced from a general \\emph{transverse} multiple recurrence theorem, which we establish using recent works of Austin.","sentences":["An extension of Szemer\\'edi's Theorem is proved for sets of positive density in cut-and-project sets in general locally compact and second countable abelian groups.","As a consequence, we establish a recent conjecture of Klick, Strungaru and Tcaciuc.","Furthermore, we show a Szemeredi type theorem for sets of positive density in the set of fractional parts of $\\mathbb{Q}_p$. Via a novel version of Furstenberg's Correspondence principle, which should be of independent interest, we show that our Szemer\\'edi Theorems can be deduced from a general \\emph{transverse} multiple recurrence theorem, which we establish using recent works of Austin."],"url":"http://arxiv.org/abs/2402.17158v1","category":"math.DS"}
{"created":"2024-02-27 02:44:40","title":"Generative Learning for Forecasting the Dynamics of Complex Systems","abstract":"We introduce generative models for accelerating simulations of complex systems through learning and evolving their effective dynamics. In the proposed Generative Learning of Effective Dynamics (G-LED), instances of high dimensional data are down sampled to a lower dimensional manifold that is evolved through an auto-regressive attention mechanism. In turn, Bayesian diffusion models, that map this low-dimensional manifold onto its corresponding high-dimensional space, capture the statistics of the system dynamics. We demonstrate the capabilities and drawbacks of G-LED in simulations of several benchmark systems, including the Kuramoto-Sivashinsky (KS) equation, two-dimensional high Reynolds number flow over a backward-facing step, and simulations of three-dimensional turbulent channel flow. The results demonstrate that generative learning offers new frontiers for the accurate forecasting of the statistical properties of complex systems at a reduced computational cost.","sentences":["We introduce generative models for accelerating simulations of complex systems through learning and evolving their effective dynamics.","In the proposed Generative Learning of Effective Dynamics (G-LED), instances of high dimensional data are down sampled to a lower dimensional manifold that is evolved through an auto-regressive attention mechanism.","In turn, Bayesian diffusion models, that map this low-dimensional manifold onto its corresponding high-dimensional space, capture the statistics of the system dynamics.","We demonstrate the capabilities and drawbacks of G-LED in simulations of several benchmark systems, including the Kuramoto-Sivashinsky (KS) equation, two-dimensional high Reynolds number flow over a backward-facing step, and simulations of three-dimensional turbulent channel flow.","The results demonstrate that generative learning offers new frontiers for the accurate forecasting of the statistical properties of complex systems at a reduced computational cost."],"url":"http://arxiv.org/abs/2402.17157v1","category":"cs.LG"}
{"created":"2024-02-27 02:41:40","title":"Acceptor-induced bulk dielectric loss in superconducting circuits on silicon","abstract":"The performance of superconducting quantum circuits is primarily limited by dielectric loss due to interactions with two-level systems (TLS). State-of-the-art circuits with engineered material interfaces are approaching a limit where dielectric loss from bulk substrates plays an important role. However, a microscopic understanding of dielectric loss in crystalline substrates is still lacking. In this work, we show that boron acceptors in silicon constitute a strongly coupled TLS bath for superconducting circuits. We discuss how the electronic structure of boron acceptors leads to an effective TLS response in silicon. We sweep the boron concentration in silicon and demonstrate the bulk dielectric loss limit from boron acceptors. We show that boron-induced dielectric loss can be reduced in a magnetic field due to the spin-orbit structure of boron. This work provides the first detailed microscopic description of a TLS bath for superconducting circuits, and demonstrates the need for ultrahigh purity substrates for next-generation superconducting quantum processors.","sentences":["The performance of superconducting quantum circuits is primarily limited by dielectric loss due to interactions with two-level systems (TLS).","State-of-the-art circuits with engineered material interfaces are approaching a limit where dielectric loss from bulk substrates plays an important role.","However, a microscopic understanding of dielectric loss in crystalline substrates is still lacking.","In this work, we show that boron acceptors in silicon constitute a strongly coupled TLS bath for superconducting circuits.","We discuss how the electronic structure of boron acceptors leads to an effective TLS response in silicon.","We sweep the boron concentration in silicon and demonstrate the bulk dielectric loss limit from boron acceptors.","We show that boron-induced dielectric loss can be reduced in a magnetic field due to the spin-orbit structure of boron.","This work provides the first detailed microscopic description of a TLS bath for superconducting circuits, and demonstrates the need for ultrahigh purity substrates for next-generation superconducting quantum processors."],"url":"http://arxiv.org/abs/2402.17155v1","category":"quant-ph"}
{"created":"2024-02-27 02:37:37","title":"Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations","abstract":"Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute.   Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders''), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data.   HSTU outperforms baselines over synthetic and public datasets by up to 65.8\\% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion parameters, improve metrics in online A/B tests by 12.4\\% and have been deployed on multiple surfaces of a large internet platform with billions of users. More importantly, the model quality of Generative Recommenders empirically scales as a power-law of training compute across three orders of magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for future model developments, and further paves the way for the first foundational models in recommendations.","sentences":["Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis.","Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute.   ","Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems.","We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders''), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data.   ","HSTU outperforms baselines over synthetic and public datasets by up to 65.8\\% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences.","HSTU-based Generative Recommenders, with 1.5 trillion parameters, improve metrics in online A/B tests by 12.4\\% and have been deployed on multiple surfaces of a large internet platform with billions of users.","More importantly, the model quality of Generative Recommenders empirically scales as a power-law of training compute across three orders of magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for future model developments, and further paves the way for the first foundational models in recommendations."],"url":"http://arxiv.org/abs/2402.17152v1","category":"cs.LG"}
{"created":"2024-02-27 02:36:43","title":"Clustering Document Parts: Detecting and Characterizing Influence Campaigns From Documents","abstract":"We propose a novel clustering pipeline to detect and characterize influence campaigns from documents. This approach clusters parts of document, detects clusters that likely reflect an influence campaign, and then identifies documents linked to an influence campaign via their association with the high-influence clusters. Our approach outperforms both the direct document-level classification and the direct document-level clustering approach in predicting if a document is part of an influence campaign. We propose various novel techniques to enhance our pipeline, including using an existing event factuality prediction system to obtain document parts, and aggregating multiple clustering experiments to improve the performance of both cluster and document classification. Classifying documents on the top of clustering not only accurately extracts the parts of the documents that are relevant to influence campaigns, but also capture influence campaigns as a coordinated and holistic phenomenon. Our approach makes possible more fine-grained and interpretable characterizations of influence campaigns from documents.","sentences":["We propose a novel clustering pipeline to detect and characterize influence campaigns from documents.","This approach clusters parts of document, detects clusters that likely reflect an influence campaign, and then identifies documents linked to an influence campaign via their association with the high-influence clusters.","Our approach outperforms both the direct document-level classification and the direct document-level clustering approach in predicting if a document is part of an influence campaign.","We propose various novel techniques to enhance our pipeline, including using an existing event factuality prediction system to obtain document parts, and aggregating multiple clustering experiments to improve the performance of both cluster and document classification.","Classifying documents on the top of clustering not only accurately extracts the parts of the documents that are relevant to influence campaigns, but also capture influence campaigns as a coordinated and holistic phenomenon.","Our approach makes possible more fine-grained and interpretable characterizations of influence campaigns from documents."],"url":"http://arxiv.org/abs/2402.17151v1","category":"cs.CL"}
{"created":"2024-02-27 02:18:55","title":"Target Speaker Extraction by Directly Exploiting Contextual Information in the Time-Frequency Domain","abstract":"In target speaker extraction, many studies rely on the speaker embedding which is obtained from an enrollment of the target speaker and employed as the guidance. However, solely using speaker embedding may not fully utilize the contextual information contained in the enrollment. In this paper, we directly exploit this contextual information in the time-frequency (T-F) domain. Specifically, the T-F representations of the enrollment and the mixed signal are interacted to compute the weighting matrices through an attention mechanism. These weighting matrices reflect the similarity among different frames of the T-F representations and are further employed to obtain the consistent T-F representations of the enrollment. These consistent representations are served as the guidance, allowing for better exploitation of the contextual information. Furthermore, the proposed method achieves the state-of-the-art performance on the benchmark dataset and shows its effectiveness in the complex scenarios.","sentences":["In target speaker extraction, many studies rely on the speaker embedding which is obtained from an enrollment of the target speaker and employed as the guidance.","However, solely using speaker embedding may not fully utilize the contextual information contained in the enrollment.","In this paper, we directly exploit this contextual information in the time-frequency (T-F) domain.","Specifically, the T-F representations of the enrollment and the mixed signal are interacted to compute the weighting matrices through an attention mechanism.","These weighting matrices reflect the similarity among different frames of the T-F representations and are further employed to obtain the consistent T-F representations of the enrollment.","These consistent representations are served as the guidance, allowing for better exploitation of the contextual information.","Furthermore, the proposed method achieves the state-of-the-art performance on the benchmark dataset and shows its effectiveness in the complex scenarios."],"url":"http://arxiv.org/abs/2402.17146v1","category":"eess.AS"}
{"created":"2024-02-27 02:13:32","title":"Energy-Efficient Scheduling with Predictions","abstract":"An important goal of modern scheduling systems is to efficiently manage power usage. In energy-efficient scheduling, the operating system controls the speed at which a machine is processing jobs with the dual objective of minimizing energy consumption and optimizing the quality of service cost of the resulting schedule. Since machine-learned predictions about future requests can often be learned from historical data, a recent line of work on learning-augmented algorithms aims to achieve improved performance guarantees by leveraging predictions. In particular, for energy-efficient scheduling, Bamas et. al. [BamasMRS20] and Antoniadis et. al. [antoniadis2021novel] designed algorithms with predictions for the energy minimization with deadlines problem and achieved an improved competitive ratio when the prediction error is small while also maintaining worst-case bounds even when the prediction error is arbitrarily large.   In this paper, we consider a general setting for energy-efficient scheduling and provide a flexible learning-augmented algorithmic framework that takes as input an offline and an online algorithm for the desired energy-efficient scheduling problem. We show that, when the prediction error is small, this framework gives improved competitive ratios for many different energy-efficient scheduling problems, including energy minimization with deadlines, while also maintaining a bounded competitive ratio regardless of the prediction error. Finally, we empirically demonstrate that this framework achieves an improved performance on real and synthetic datasets.","sentences":["An important goal of modern scheduling systems is to efficiently manage power usage.","In energy-efficient scheduling, the operating system controls the speed at which a machine is processing jobs with the dual objective of minimizing energy consumption and optimizing the quality of service cost of the resulting schedule.","Since machine-learned predictions about future requests can often be learned from historical data, a recent line of work on learning-augmented algorithms aims to achieve improved performance guarantees by leveraging predictions.","In particular, for energy-efficient scheduling, Bamas et. al.","[BamasMRS20] and Antoniadis et.","al.","[antoniadis2021novel] designed algorithms with predictions for the energy minimization with deadlines problem and achieved an improved competitive ratio when the prediction error is small while also maintaining worst-case bounds even when the prediction error is arbitrarily large.   ","In this paper, we consider a general setting for energy-efficient scheduling and provide a flexible learning-augmented algorithmic framework that takes as input an offline and an online algorithm for the desired energy-efficient scheduling problem.","We show that, when the prediction error is small, this framework gives improved competitive ratios for many different energy-efficient scheduling problems, including energy minimization with deadlines, while also maintaining a bounded competitive ratio regardless of the prediction error.","Finally, we empirically demonstrate that this framework achieves an improved performance on real and synthetic datasets."],"url":"http://arxiv.org/abs/2402.17143v1","category":"cs.DS"}
{"created":"2024-02-27 18:48:07","title":"Robustly Learning Single-Index Models via Alignment Sharpness","abstract":"We study the problem of learning Single-Index Models under the $L_2^2$ loss in the agnostic model. We give an efficient learning algorithm, achieving a constant factor approximation to the optimal loss, that succeeds under a range of distributions (including log-concave distributions) and a broad class of monotone and Lipschitz link functions. This is the first efficient constant factor approximate agnostic learner, even for Gaussian data and for any nontrivial class of link functions. Prior work for the case of unknown link function either works in the realizable setting or does not attain constant factor approximation. The main technical ingredient enabling our algorithm and analysis is a novel notion of a local error bound in optimization that we term alignment sharpness and that may be of broader interest.","sentences":["We study the problem of learning Single-Index Models under the $L_2^2$ loss in the agnostic model.","We give an efficient learning algorithm, achieving a constant factor approximation to the optimal loss, that succeeds under a range of distributions (including log-concave distributions) and a broad class of monotone and Lipschitz link functions.","This is the first efficient constant factor approximate agnostic learner, even for Gaussian data and for any nontrivial class of link functions.","Prior work for the case of unknown link function either works in the realizable setting or does not attain constant factor approximation.","The main technical ingredient enabling our algorithm and analysis is a novel notion of a local error bound in optimization that we term alignment sharpness and that may be of broader interest."],"url":"http://arxiv.org/abs/2402.17756v1","category":"cs.LG"}
{"created":"2024-02-27 18:45:31","title":"Increasing the Diversity of Investment Portfolio with Integration of Gamified Components in the FinTech Applications Lifecycle","abstract":"Gamification has the potential to make significant contributions to financial product delivery, Fintech services, and inclusive growth. The integration of gamification into FinTech applications has shown a positive correlation with the social impact theory. Utilizing gamification in a sustainable and effective manner can be crucial for long-term prospects in the FinTech industry. Therefore, it is essential to develop efficient and modern financial software that improves the customer experience. The current literature aims to contribute to this area by highlighting the relationship between interrelated theories and the key factors to consider when designing a gamified element. This study aims to explore the effects of gamification on altering user intention and its significant influence on customer value propositions.","sentences":["Gamification has the potential to make significant contributions to financial product delivery, Fintech services, and inclusive growth.","The integration of gamification into FinTech applications has shown a positive correlation with the social impact theory.","Utilizing gamification in a sustainable and effective manner can be crucial for long-term prospects in the FinTech industry.","Therefore, it is essential to develop efficient and modern financial software that improves the customer experience.","The current literature aims to contribute to this area by highlighting the relationship between interrelated theories and the key factors to consider when designing a gamified element.","This study aims to explore the effects of gamification on altering user intention and its significant influence on customer value propositions."],"url":"http://arxiv.org/abs/2402.17754v1","category":"cs.GT"}
{"created":"2024-02-27 18:25:16","title":"Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using Contrastive Learning and Geometric Unfolding","abstract":"Understanding the cortical organization of the human brain requires interpretable descriptors for distinct structural and functional imaging data. 3D polarized light imaging (3D-PLI) is an imaging modality for visualizing fiber architecture in postmortem brains with high resolution that also captures the presence of cell bodies, for example, to identify hippocampal subfields. The rich texture in 3D-PLI images, however, makes this modality particularly difficult to analyze and best practices for characterizing architectonic patterns still need to be established. In this work, we demonstrate a novel method to analyze the regional organization of the human hippocampus in 3D-PLI by combining recent advances in unfolding methods with deep texture features obtained using a self-supervised contrastive learning approach. We identify clusters in the representations that correspond well with classical descriptions of hippocampal subfields, lending validity to the developed methodology.","sentences":["Understanding the cortical organization of the human brain requires interpretable descriptors for distinct structural and functional imaging data.","3D polarized light imaging (3D-PLI) is an imaging modality for visualizing fiber architecture in postmortem brains with high resolution that also captures the presence of cell bodies, for example, to identify hippocampal subfields.","The rich texture in 3D-PLI images, however, makes this modality particularly difficult to analyze and best practices for characterizing architectonic patterns still need to be established.","In this work, we demonstrate a novel method to analyze the regional organization of the human hippocampus in 3D-PLI by combining recent advances in unfolding methods with deep texture features obtained using a self-supervised contrastive learning approach.","We identify clusters in the representations that correspond well with classical descriptions of hippocampal subfields, lending validity to the developed methodology."],"url":"http://arxiv.org/abs/2402.17744v1","category":"cs.CV"}
{"created":"2024-02-27 18:18:56","title":"Multistable Kuramoto splay states in a crystal of mode-locked laser pulses","abstract":"We demonstrate the existence of a multiplicity of co-existing frequency combs in a harmonically mode-locked laser that we link to the splay phases of the Kuramoto model with short range interactions. These splay states are multistable and the laser may wander between them under the influence of stochastic forces. Consequently, the many pulses circulating in the cavity are not necessarily coherent with each other. We show that this partially disordered state for the phase of the optical field features regular train of pulses in the field intensity, a state that we term an incoherent crystal of optical pulses. We provide evidence that the notion of coherence should be interpreted by comparing the duration of the measurement time with the Kramers' escape time of each splay state. Our results are confirmed experimentally by studying a passively mode-locked vertical external-cavity surface-emitting laser.","sentences":["We demonstrate the existence of a multiplicity of co-existing frequency combs in a harmonically mode-locked laser that we link to the splay phases of the Kuramoto model with short range interactions.","These splay states are multistable and the laser may wander between them under the influence of stochastic forces.","Consequently, the many pulses circulating in the cavity are not necessarily coherent with each other.","We show that this partially disordered state for the phase of the optical field features regular train of pulses in the field intensity, a state that we term an incoherent crystal of optical pulses.","We provide evidence that the notion of coherence should be interpreted by comparing the duration of the measurement time with the Kramers' escape time of each splay state.","Our results are confirmed experimentally by studying a passively mode-locked vertical external-cavity surface-emitting laser."],"url":"http://arxiv.org/abs/2402.17740v1","category":"physics.optics"}
{"created":"2024-02-27 18:06:20","title":"Batched Nonparametric Contextual Bandits","abstract":"We study nonparametric contextual bandits under batch constraints, where the expected reward for each action is modeled as a smooth function of covariates, and the policy updates are made at the end of each batch of observations. We establish a minimax regret lower bound for this setting and propose Batched Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal regret (up to logarithmic factors). In essence, BaSEDB dynamically splits the covariate space into smaller bins, carefully aligning their widths with the batch size. We also show the suboptimality of static binning under batch constraints, highlighting the necessity of dynamic binning. Additionally, our results suggest that a nearly constant number of policy updates can attain optimal regret in the fully online setting.","sentences":["We study nonparametric contextual bandits under batch constraints, where the expected reward for each action is modeled as a smooth function of covariates, and the policy updates are made at the end of each batch of observations.","We establish a minimax regret lower bound for this setting and propose Batched Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal regret (up to logarithmic factors).","In essence, BaSEDB dynamically splits the covariate space into smaller bins, carefully aligning their widths with the batch size.","We also show the suboptimality of static binning under batch constraints, highlighting the necessity of dynamic binning.","Additionally, our results suggest that a nearly constant number of policy updates can attain optimal regret in the fully online setting."],"url":"http://arxiv.org/abs/2402.17732v1","category":"math.ST"}
{"created":"2024-02-27 17:30:33","title":"Transfer Learning Bayesian Optimization to Design Competitor DNA Molecules for Use in Diagnostic Assays","abstract":"With the rise in engineered biomolecular devices, there is an increased need for tailor-made biological sequences. Often, many similar biological sequences need to be made for a specific application meaning numerous, sometimes prohibitively expensive, lab experiments are necessary for their optimization. This paper presents a transfer learning design of experiments workflow to make this development feasible. By combining a transfer learning surrogate model with Bayesian optimization, we show how the total number of experiments can be reduced by sharing information between optimization tasks. We demonstrate the reduction in the number of experiments using data from the development of DNA competitors for use in an amplification-based diagnostic assay. We use cross-validation to compare the predictive accuracy of different transfer learning models, and then compare the performance of the models for both single objective and penalized optimization tasks.","sentences":["With the rise in engineered biomolecular devices, there is an increased need for tailor-made biological sequences.","Often, many similar biological sequences need to be made for a specific application meaning numerous, sometimes prohibitively expensive, lab experiments are necessary for their optimization.","This paper presents a transfer learning design of experiments workflow to make this development feasible.","By combining a transfer learning surrogate model with Bayesian optimization, we show how the total number of experiments can be reduced by sharing information between optimization tasks.","We demonstrate the reduction in the number of experiments using data from the development of DNA competitors for use in an amplification-based diagnostic assay.","We use cross-validation to compare the predictive accuracy of different transfer learning models, and then compare the performance of the models for both single objective and penalized optimization tasks."],"url":"http://arxiv.org/abs/2402.17704v1","category":"q-bio.QM"}
{"created":"2024-02-27 17:01:21","title":"Outlier-Detection for Reactive Machine Learned Potential Energy Surfaces","abstract":"Uncertainty quantification (UQ) to detect samples with large expected errors (outliers) is applied to reactive molecular potential energy surfaces (PESs). Three methods - Ensembles, Deep Evidential Regression (DER), and Gaussian Mixture Models (GMM) - were applied to the H-transfer reaction between ${\\it syn-}$Criegee and vinyl hydroxyperoxide. The results indicate that ensemble models provide the best results for detecting outliers, followed by GMM. For example, from a pool of 1000 structures with the largest uncertainty, the detection quality for outliers is $\\sim 90$ \\% and $\\sim 50$ \\%, respectively, if 25 or 1000 structures with large errors are sought. On the contrary, the limitations of the statistical assumptions of DER greatly impacted its prediction capabilities. Finally, a structure-based indicator was found to be correlated with large average error, which may help to rapidly classify new structures into those that provide an advantage for refining the neural network.","sentences":["Uncertainty quantification (UQ) to detect samples with large expected errors (outliers) is applied to reactive molecular potential energy surfaces (PESs).","Three methods - Ensembles, Deep Evidential Regression (DER), and Gaussian Mixture Models (GMM) - were applied to the H-transfer reaction between ${\\it syn-}$Criegee and vinyl hydroxyperoxide.","The results indicate that ensemble models provide the best results for detecting outliers, followed by GMM.","For example, from a pool of 1000 structures with the largest uncertainty, the detection quality for outliers is $\\sim 90$ \\% and $\\sim 50$ \\%, respectively, if 25 or 1000 structures with large errors are sought.","On the contrary, the limitations of the statistical assumptions of DER greatly impacted its prediction capabilities.","Finally, a structure-based indicator was found to be correlated with large average error, which may help to rapidly classify new structures into those that provide an advantage for refining the neural network."],"url":"http://arxiv.org/abs/2402.17686v1","category":"physics.chem-ph"}
{"created":"2024-02-27 16:57:03","title":"Stochastic expansion for the pricing of Asian options","abstract":"We present closed analytical approximations for the pricing of Asian options with discrete averaging under the Black-Scholes model with time-dependent parameters. The formulae are obtained by using a stochastic Taylor expansion around a log-normal proxy model and are found to be highly accurate in practice.","sentences":["We present closed analytical approximations for the pricing of Asian options with discrete averaging under the Black-Scholes model with time-dependent parameters.","The formulae are obtained by using a stochastic Taylor expansion around a log-normal proxy model and are found to be highly accurate in practice."],"url":"http://arxiv.org/abs/2402.17684v1","category":"q-fin.PR"}
{"created":"2024-02-27 16:56:30","title":"NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents","abstract":"While (large) language models have significantly improved over the last years, they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism. To address this, we propose NextLevelBERT, a Masked Language Model operating not on tokens, but on higher-level semantic representations in the form of text embeddings. We pretrain NextLevelBERT to predict the vector representation of entire masked text chunks and evaluate the effectiveness of the resulting document vectors on three task types: 1) Semantic Textual Similarity via zero-shot document embeddings, 2) Long document classification, 3) Multiple-choice question answering. We find that next level Masked Language Modeling is an effective technique to tackle long-document use cases and can outperform much larger embedding models as long as the required level of detail is not too high. We make model and code available.","sentences":["While (large) language models have significantly improved over the last years, they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism.","To address this, we propose NextLevelBERT, a Masked Language Model operating not on tokens, but on higher-level semantic representations in the form of text embeddings.","We pretrain NextLevelBERT to predict the vector representation of entire masked text chunks and evaluate the effectiveness of the resulting document vectors on three task types: 1) Semantic Textual Similarity via zero-shot document embeddings, 2) Long document classification, 3) Multiple-choice question answering.","We find that next level Masked Language Modeling is an effective technique to tackle long-document use cases and can outperform much larger embedding models as long as the required level of detail is not too high.","We make model and code available."],"url":"http://arxiv.org/abs/2402.17682v1","category":"cs.CL"}
{"created":"2024-02-27 16:42:59","title":"Contractility-driven cell motility against a viscoelastic resistance","abstract":"We study a model of contraction-based cell motility inside a microchannel to investigate the regulation of cell polarization and motion by the mechanical resistance of the environment. A positive feedback between the asymmetry of the acto-myosin cortex density and cell motion gives rise to a spontaneous symmetry breaking beyond a threshold contractility that depends on the resistance of extracellular medium. In highly viscous environments, we predict bistability under moderate contractility, so that symmetry breaking needs to be activated. In a viscoelastic environment, we find periodic oscillations in cortex density and velocity polarization. At the boundary between viscous and viscoelastic environments, the cell may either cross into the viscoelastic medium, bounce back into the viscous medium, or become trapped at the boundary. The different scenarios defined different phase diagram that are confirmed by numerical simulations.","sentences":["We study a model of contraction-based cell motility inside a microchannel to investigate the regulation of cell polarization and motion by the mechanical resistance of the environment.","A positive feedback between the asymmetry of the acto-myosin cortex density and cell motion gives rise to a spontaneous symmetry breaking beyond a threshold contractility that depends on the resistance of extracellular medium.","In highly viscous environments, we predict bistability under moderate contractility, so that symmetry breaking needs to be activated.","In a viscoelastic environment, we find periodic oscillations in cortex density and velocity polarization.","At the boundary between viscous and viscoelastic environments, the cell may either cross into the viscoelastic medium, bounce back into the viscous medium, or become trapped at the boundary.","The different scenarios defined different phase diagram that are confirmed by numerical simulations."],"url":"http://arxiv.org/abs/2402.17669v1","category":"physics.bio-ph"}
{"created":"2024-02-27 16:36:53","title":"Multi-Agent Deep Reinforcement Learning for Distributed Satellite Routing","abstract":"This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL) approach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Each satellite is an independent decision-making agent with a partial knowledge of the environment, and supported by feedback received from the nearby agents. Building on our previous work that introduced a Q-routing solution, the contribution of this paper is to extend it to a deep learning framework able to quickly adapt to the network and traffic changes, and based on two phases: (1) An offline exploration learning phase that relies on a global Deep Neural Network (DNN) to learn the optimal paths at each possible position and congestion level; (2) An online exploitation phase with local, on-board, pre-trained DNNs. Results show that MA-DRL efficiently learns optimal routes offline that are then loaded for an efficient distributed routing online.","sentences":["This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL) approach for routing in Low Earth Orbit Satellite Constellations (LSatCs).","Each satellite is an independent decision-making agent with a partial knowledge of the environment, and supported by feedback received from the nearby agents.","Building on our previous work that introduced a Q-routing solution, the contribution of this paper is to extend it to a deep learning framework able to quickly adapt to the network and traffic changes, and based on two phases: (1) An offline exploration learning phase that relies on a global Deep Neural Network (DNN) to learn the optimal paths at each possible position and congestion level; (2) An online exploitation phase with local, on-board, pre-trained DNNs.","Results show that MA-DRL efficiently learns optimal routes offline that are then loaded for an efficient distributed routing online."],"url":"http://arxiv.org/abs/2402.17666v1","category":"cs.LG"}
{"created":"2024-02-27 16:24:28","title":"Confidence-Aware Multi-Field Model Calibration","abstract":"Accurately predicting the probabilities of user feedback, such as clicks and conversions, is critical for ad ranking and bidding. However, there often exist unwanted mismatches between predicted probabilities and true likelihoods due to the shift of data distributions and intrinsic model biases. Calibration aims to address this issue by post-processing model predictions, and field-aware calibration can adjust model output on different feature field values to satisfy fine-grained advertising demands. Unfortunately, the observed samples corresponding to certain field values can be too limited to make confident calibrations, which may yield bias amplification and online disturbance. In this paper, we propose a confidence-aware multi-field calibration method, which adaptively adjusts the calibration intensity based on the confidence levels derived from sample statistics. It also utilizes multiple feature fields for joint model calibration with awareness of their importance to mitigate the data sparsity effect of a single field. Extensive offline and online experiments show the superiority of our method in boosting advertising performance and reducing prediction deviations.","sentences":["Accurately predicting the probabilities of user feedback, such as clicks and conversions, is critical for ad ranking and bidding.","However, there often exist unwanted mismatches between predicted probabilities and true likelihoods due to the shift of data distributions and intrinsic model biases.","Calibration aims to address this issue by post-processing model predictions, and field-aware calibration can adjust model output on different feature field values to satisfy fine-grained advertising demands.","Unfortunately, the observed samples corresponding to certain field values can be too limited to make confident calibrations, which may yield bias amplification and online disturbance.","In this paper, we propose a confidence-aware multi-field calibration method, which adaptively adjusts the calibration intensity based on the confidence levels derived from sample statistics.","It also utilizes multiple feature fields for joint model calibration with awareness of their importance to mitigate the data sparsity effect of a single field.","Extensive offline and online experiments show the superiority of our method in boosting advertising performance and reducing prediction deviations."],"url":"http://arxiv.org/abs/2402.17655v1","category":"cs.LG"}
{"created":"2024-02-27 16:23:11","title":"Mitigating Distributional Shift in Semantic Segmentation via Uncertainty Estimation from Unlabelled Data","abstract":"Knowing when a trained segmentation model is encountering data that is different to its training data is important. Understanding and mitigating the effects of this play an important part in their application from a performance and assurance perspective - this being a safety concern in applications such as autonomous vehicles (AVs). This work presents a segmentation network that can detect errors caused by challenging test domains without any additional annotation in a single forward pass. As annotation costs limit the diversity of labelled datasets, we use easy-to-obtain, uncurated and unlabelled data to learn to perform uncertainty estimation by selectively enforcing consistency over data augmentation. To this end, a novel segmentation benchmark based on the SAX Dataset is used, which includes labelled test data spanning three autonomous-driving domains, ranging in appearance from dense urban to off-road. The proposed method, named Gamma-SSL, consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark - by up to 10.7% in area under the receiver operating characteristic (ROC) curve and 19.2% in area under the precision-recall (PR) curve in the most challenging of the three scenarios.","sentences":["Knowing when a trained segmentation model is encountering data that is different to its training data is important.","Understanding and mitigating the effects of this play an important part in their application from a performance and assurance perspective - this being a safety concern in applications such as autonomous vehicles (AVs).","This work presents a segmentation network that can detect errors caused by challenging test domains without any additional annotation in a single forward pass.","As annotation costs limit the diversity of labelled datasets, we use easy-to-obtain, uncurated and unlabelled data to learn to perform uncertainty estimation by selectively enforcing consistency over data augmentation.","To this end, a novel segmentation benchmark based on the SAX Dataset is used, which includes labelled test data spanning three autonomous-driving domains, ranging in appearance from dense urban to off-road.","The proposed method, named Gamma-SSL, consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark - by up to 10.7% in area under the receiver operating characteristic (ROC) curve and 19.2% in area under the precision-recall (PR) curve in the most challenging of the three scenarios."],"url":"http://arxiv.org/abs/2402.17653v1","category":"cs.CV"}
{"created":"2024-02-27 16:04:19","title":"Learning the Covariance of Treatment Effects Across Many Weak Experiments","abstract":"When primary objectives are insensitive or delayed, experimenters may instead focus on proxy metrics derived from secondary outcomes. For example, technology companies often infer long-term impacts of product interventions from their effects on weighted indices of short-term user engagement signals. We consider meta-analysis of many historical experiments to learn the covariance of treatment effects on different outcomes, which can support the construction of such proxies. Even when experiments are plentiful and large, if treatment effects are weak, the sample covariance of estimated treatment effects across experiments can be highly biased and remains inconsistent even as more experiments are considered. We overcome this by using techniques inspired by weak instrumental variable analysis, which we show can reliably estimate parameters of interest, even without a structural model. We show the Limited Information Maximum Likelihood (LIML) estimator learns a parameter that is equivalent to fitting total least squares to a transformation of the scatterplot of estimated treatment effects, and that Jackknife Instrumental Variables Estimation (JIVE) learns another parameter that can be computed from the average of Jackknifed covariance matrices across experiments. We also present a total-covariance-based estimator for the latter estimand under homoskedasticity, which we show is equivalent to a $k$-class estimator. We show how these parameters relate to causal quantities and can be used to construct unbiased proxy metrics under a structural model with both direct and indirect effects subject to the INstrument Strength Independent of Direct Effect (INSIDE) assumption of Mendelian randomization. Lastly, we discuss the application of our methods at Netflix.","sentences":["When primary objectives are insensitive or delayed, experimenters may instead focus on proxy metrics derived from secondary outcomes.","For example, technology companies often infer long-term impacts of product interventions from their effects on weighted indices of short-term user engagement signals.","We consider meta-analysis of many historical experiments to learn the covariance of treatment effects on different outcomes, which can support the construction of such proxies.","Even when experiments are plentiful and large, if treatment effects are weak, the sample covariance of estimated treatment effects across experiments can be highly biased and remains inconsistent even as more experiments are considered.","We overcome this by using techniques inspired by weak instrumental variable analysis, which we show can reliably estimate parameters of interest, even without a structural model.","We show the Limited Information Maximum Likelihood (LIML) estimator learns a parameter that is equivalent to fitting total least squares to a transformation of the scatterplot of estimated treatment effects, and that Jackknife Instrumental Variables Estimation (JIVE) learns another parameter that can be computed from the average of Jackknifed covariance matrices across experiments.","We also present a total-covariance-based estimator for the latter estimand under homoskedasticity, which we show is equivalent to a $k$-class estimator.","We show how these parameters relate to causal quantities and can be used to construct unbiased proxy metrics under a structural model with both direct and indirect effects subject to the INstrument Strength Independent of Direct Effect (INSIDE) assumption of Mendelian randomization.","Lastly, we discuss the application of our methods at Netflix."],"url":"http://arxiv.org/abs/2402.17637v1","category":"stat.ME"}
{"created":"2024-02-27 15:58:32","title":"Securing OPEN-RAN Equipment Using Blockchain-Based Supply Chain Verification","abstract":"The disaggregated and multi-vendor nature of OPEN-RAN networks introduces new supply chain security risks, making equipment authenticity and integrity crucial challenges. Robust solutions are needed to mitigate vulnerabilities in manufacturing and integration. This paper puts forth a novel blockchain-based approach to secure OPEN-RAN equipment through its lifecycle. By combining firmware authentication codes, a permissioned blockchain ledger, and equipment node validators, we architect a tamper-resistant ecosystem to track provenance. The outlined design, while conceptual, establishes a foundation and roadmap for future realization. Through careful implementation planning, development of core components like firmware signed hashes and smart contracts, and rigorous performance evaluation, this paper can evolve from concept to practice. There is a vivid potential to make OPEN-RAN supply chains corner to corner secure, igniting further research and real-world deployment.","sentences":["The disaggregated and multi-vendor nature of OPEN-RAN networks introduces new supply chain security risks, making equipment authenticity and integrity crucial challenges.","Robust solutions are needed to mitigate vulnerabilities in manufacturing and integration.","This paper puts forth a novel blockchain-based approach to secure OPEN-RAN equipment through its lifecycle.","By combining firmware authentication codes, a permissioned blockchain ledger, and equipment node validators, we architect a tamper-resistant ecosystem to track provenance.","The outlined design, while conceptual, establishes a foundation and roadmap for future realization.","Through careful implementation planning, development of core components like firmware signed hashes and smart contracts, and rigorous performance evaluation, this paper can evolve from concept to practice.","There is a vivid potential to make OPEN-RAN supply chains corner to corner secure, igniting further research and real-world deployment."],"url":"http://arxiv.org/abs/2402.17632v1","category":"cs.CR"}
{"created":"2024-02-27 15:42:00","title":"Recovered SN Ia rate from simulated LSST images","abstract":"The Legacy Survey of Space and Time (LSST) will revolutionize Time Domain Astronomy by detecting millions of transients. In particular, it is expected to increment the number of type Ia supernovae (SNIa) of a factor of 100 compared to existing samples up to z~1.2. Such a high number of events will dramatically reduce statistical uncertainties in the analysis of SNIa properties and rates. However, the impact of all other sources of uncertainty on the measurement must still be evaluated. The comprehension and reduction of such uncertainties will be fundamental both for cosmology and stellar evolution studies, as measuring the SNIa rate can put constraints on the evolutionary scenarios of different SNIa progenitors. We use simulated data from the DESC Data Challenge 2 (DC2) and LSST Data Preview 0 (DP0) to measure the SNIa rate on a 15 deg2 region of the Wide-Fast-Deep area. We select a sample of SN candidates detected on difference images, associate them to the host galaxy, and retrieve their photometric redshifts (z-phot). Then, we test different light curves classification methods, with and without redshift priors. We discuss how the distribution in redshift measured for the SN candidates changes according to the selected host galaxy and redshift estimate. We measure the SNIa rate analyzing the impact of uncertainties due to z-phot, host galaxy association and classification on the distribution in redshift of the starting sample. We found a 17% average lost fraction of SNIa with respect to the simulated sample. As 10% of the bias is due to the uncertainty on the z-phot alone (which also affects classification when used as a prior), it results to be the major source of uncertainty. We discuss possible reduction of the errors in the measurement of the SNIa rate, including synergies with other surveys, which may help using the rate to discriminate different progenitor models.","sentences":["The Legacy Survey of Space and Time (LSST) will revolutionize Time Domain Astronomy by detecting millions of transients.","In particular, it is expected to increment the number of type Ia supernovae (SNIa) of a factor of 100 compared to existing samples up to z~1.2.","Such a high number of events will dramatically reduce statistical uncertainties in the analysis of SNIa properties and rates.","However, the impact of all other sources of uncertainty on the measurement must still be evaluated.","The comprehension and reduction of such uncertainties will be fundamental both for cosmology and stellar evolution studies, as measuring the SNIa rate can put constraints on the evolutionary scenarios of different SNIa progenitors.","We use simulated data from the DESC Data Challenge 2 (DC2) and LSST Data Preview 0 (DP0) to measure the SNIa rate on a 15 deg2 region of the Wide-Fast-Deep area.","We select a sample of SN candidates detected on difference images, associate them to the host galaxy, and retrieve their photometric redshifts (z-phot).","Then, we test different light curves classification methods, with and without redshift priors.","We discuss how the distribution in redshift measured for the SN candidates changes according to the selected host galaxy and redshift estimate.","We measure the SNIa rate analyzing the impact of uncertainties due to z-phot, host galaxy association and classification on the distribution in redshift of the starting sample.","We found a 17% average lost fraction of SNIa with respect to the simulated sample.","As 10% of the bias is due to the uncertainty on the z-phot alone (which also affects classification when used as a prior), it results to be the major source of uncertainty.","We discuss possible reduction of the errors in the measurement of the SNIa rate, including synergies with other surveys, which may help using the rate to discriminate different progenitor models."],"url":"http://arxiv.org/abs/2402.17612v1","category":"astro-ph.CO"}
{"created":"2024-02-27 15:30:34","title":"Classification Theorem For Positive Critical Points Of Sobolev Trace Inequality","abstract":"We consider the Euler-Lagrange equation of Sobolev trace inequality and prove several classification results. Exploiting the moving sphere method, it has been shown, when $p=2$, positive solutions of Euler-Lagrange equation of Sobolev trace inequality are classified. Since the moving sphere method strongly relies on the symmetries of the equation, in this paper we use asymptotic estimates and two important integral identities to classify positive solutions of Euler-Langrange equation of Sobolev trace inequality under finite energy when $1<p<n$.","sentences":["We consider the Euler-Lagrange equation of Sobolev trace inequality and prove several classification results.","Exploiting the moving sphere method, it has been shown, when $p=2$, positive solutions of Euler-Lagrange equation of Sobolev trace inequality are classified.","Since the moving sphere method strongly relies on the symmetries of the equation, in this paper we use asymptotic estimates and two important integral identities to classify positive solutions of Euler-Langrange equation of Sobolev trace inequality under finite energy when $1<p<n$."],"url":"http://arxiv.org/abs/2402.17602v1","category":"math.AP"}
{"created":"2024-02-27 15:29:31","title":"Sustained Robust Exciton Emission in Suspended Monolayer WSe_2 within the Low Carrier Density Regime for Quantum Emitter Applications","abstract":"The development of semiconductor optoelectronic devices is moving toward low power consumption and miniaturization, especially for high-efficiency quantum emitters. However, most of these quantum sources work at low carrier density region, where the Shockley-Read-Hall recombination may dominant and seriously reduce the emission efficiency. In order to diminish the affection of carrier trapping and sustain a strong photoluminescence emission under low power pumping condition, we investigated on the influence of Suspending to monolayered tungsten diselenide, novel two-dimensional quantum material. Not only the PL intensity, but also the fundamental photoluminescence quantum yield has exhibited a huge, order-scale enhancement through suspending, even surprisingly, we found the PLQY improvement revealed far significantly under small pumping power and came out an exponential increase tendency toward even lower carrier density region. With its strong excitonic effect, suspended WSe_2 offers a solution to reduce carrier trapping and participate in non-radiative processes. Moreover, in the low-power range where SRH recombination dominates, suspended WSe_2 exhibited remarkably higher percentage of excitonic radiation compared to contacted WSe_2. Herein, we quantitatively demonstrate the significance of suspended WSe_2 monolayer at low carrier density region, highlighting its potential for developing compact, low-power quantum emitters in the future.","sentences":["The development of semiconductor optoelectronic devices is moving toward low power consumption and miniaturization, especially for high-efficiency quantum emitters.","However, most of these quantum sources work at low carrier density region, where the Shockley-Read-Hall recombination may dominant and seriously reduce the emission efficiency.","In order to diminish the affection of carrier trapping and sustain a strong photoluminescence emission under low power pumping condition, we investigated on the influence of Suspending to monolayered tungsten diselenide, novel two-dimensional quantum material.","Not only the PL intensity, but also the fundamental photoluminescence quantum yield has exhibited a huge, order-scale enhancement through suspending, even surprisingly, we found the PLQY improvement revealed far significantly under small pumping power and came out an exponential increase tendency toward even lower carrier density region.","With its strong excitonic effect, suspended WSe_2 offers a solution to reduce carrier trapping and participate in non-radiative processes.","Moreover, in the low-power range where SRH recombination dominates, suspended WSe_2 exhibited remarkably higher percentage of excitonic radiation compared to contacted WSe_2.","Herein, we quantitatively demonstrate the significance of suspended WSe_2 monolayer at low carrier density region, highlighting its potential for developing compact, low-power quantum emitters in the future."],"url":"http://arxiv.org/abs/2402.17600v1","category":"physics.optics"}
{"created":"2024-02-27 15:11:04","title":"In-plane ordering and tunable magnetism in Cr-based MXenes","abstract":"MXene, the two-dimensional derivatives of MAX compounds, due to their structural and compositional flexibility, is an ideal family of compounds to study a number of structure-property relations. In this work, we have investigated the tunability of magnetic properties in Cr-based MXenes that have an in-plane ordering arising out of alloying Cr with another non-magnetic transition metal atom. Using Density Functional Theory based calculations we have explored the effects of composition and surface functionalisations on the electronic and magnetic properties of these in-plane ordered MXenes known as i-MXenes. We found that the electronic and magnetic ground states are quite sensitive to the structure and composition. This provides enough tunability in these compounds so that they can be used for practical applications. Our calculated results of magnetic transition temperatures and magnetic anisotropy energies are comparable to many a established two-dimensional magnets. These put together widen the prospect of these i-MXenes for multiple usage as magnetic devices making them attractive for further investigation.","sentences":["MXene, the two-dimensional derivatives of MAX compounds, due to their structural and compositional flexibility, is an ideal family of compounds to study a number of structure-property relations.","In this work, we have investigated the tunability of magnetic properties in Cr-based MXenes that have an in-plane ordering arising out of alloying Cr with another non-magnetic transition metal atom.","Using Density Functional Theory based calculations we have explored the effects of composition and surface functionalisations on the electronic and magnetic properties of these in-plane ordered MXenes known as i-MXenes.","We found that the electronic and magnetic ground states are quite sensitive to the structure and composition.","This provides enough tunability in these compounds so that they can be used for practical applications.","Our calculated results of magnetic transition temperatures and magnetic anisotropy energies are comparable to many a established two-dimensional magnets.","These put together widen the prospect of these i-MXenes for multiple usage as magnetic devices making them attractive for further investigation."],"url":"http://arxiv.org/abs/2402.17577v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-27 15:09:20","title":"Models of hot star decretion disks","abstract":"Massive stars can during their evolution reach the phase of critical (or very rapid, near-critical) rotation when further increase in rotation rate is no longer kinematically allowed. The mass ejection and angular momentum outward transport from such rapidly rotating star's equatorial surface may lead to formation and supports further existence of a circumstellar outflowing (stellar decretion) disk. The efficient mechanism for the outward transport of the mass and angular momentum is provided by the anomalous viscosity. The outer supersonic regions of the disks can extend up to a significantly large distance from the parent star, the exact radial extension is however basically unknown, partly due to the uncertainties in radial variations of temperature and viscosity.   We study in detail the behavior of hydrodynamic quantities, i.e., the evolution of density, radial and azimuthal velocity, and angular momentum loss rate in stellar decretion disks out to extremely distant regions. We investigate the dependence of these physical characteristics on the distribution of temperature and viscosity. We also study the magnetorotational instability, which we regard to be the source of anomalous viscosity in such outflowing disks and to some extent we provide the preliminary models of the two-dimensional radially-vertically correlated distribution of the disk density and temperature.   We developed our own two-dimensional hydrodynamic and magnetohydrodynamic numerical code based on an explicit Eulerian finite difference scheme on staggered grid, including full Navier-Stokes viscosity. We use semianalytic approach to investigate the radial profile of magnetorotational instability, where on the base of the numerical time-dependent hydrodynamic model we analytically study the stability of outflowing disks submerged to the magnetic field of central star.","sentences":["Massive stars can during their evolution reach the phase of critical (or very rapid, near-critical) rotation when further increase in rotation rate is no longer kinematically allowed.","The mass ejection and angular momentum outward transport from such rapidly rotating star's equatorial surface may lead to formation and supports further existence of a circumstellar outflowing (stellar decretion) disk.","The efficient mechanism for the outward transport of the mass and angular momentum is provided by the anomalous viscosity.","The outer supersonic regions of the disks can extend up to a significantly large distance from the parent star, the exact radial extension is however basically unknown, partly due to the uncertainties in radial variations of temperature and viscosity.   ","We study in detail the behavior of hydrodynamic quantities, i.e., the evolution of density, radial and azimuthal velocity, and angular momentum loss rate in stellar decretion disks out to extremely distant regions.","We investigate the dependence of these physical characteristics on the distribution of temperature and viscosity.","We also study the magnetorotational instability, which we regard to be the source of anomalous viscosity in such outflowing disks and to some extent we provide the preliminary models of the two-dimensional radially-vertically correlated distribution of the disk density and temperature.   ","We developed our own two-dimensional hydrodynamic and magnetohydrodynamic numerical code based on an explicit Eulerian finite difference scheme on staggered grid, including full Navier-Stokes viscosity.","We use semianalytic approach to investigate the radial profile of magnetorotational instability, where on the base of the numerical time-dependent hydrodynamic model we analytically study the stability of outflowing disks submerged to the magnetic field of central star."],"url":"http://arxiv.org/abs/2402.17575v1","category":"astro-ph.SR"}
{"created":"2024-02-27 15:09:06","title":"Onset of Air Entrainment by a smooth plunging jet under atmospheric pressure","abstract":"The onset of air entrainment by a smooth vertical liquid jet impacting a pool of the same liquid has been experimentally determined. The ranges of parameters covered complement those considered by Lin & Donnelly (1969). The influence of the jet curvature is clarified. A model based on the viscous stress acting on the interface proves to be in good agreement with the experiments in the limit of very viscous liquids. For less viscous liquids, the critical capillary number happens to be mainly controlled by the ratio of the dynamic viscosity of the air and of the liquid. Besides, the observed evolution of the critical capillary number agrees with the mechanism suggested by Eggers (2001) that is based on the cusp dynamic.","sentences":["The onset of air entrainment by a smooth vertical liquid jet impacting a pool of the same liquid has been experimentally determined.","The ranges of parameters covered complement those considered by Lin & Donnelly (1969).","The influence of the jet curvature is clarified.","A model based on the viscous stress acting on the interface proves to be in good agreement with the experiments in the limit of very viscous liquids.","For less viscous liquids, the critical capillary number happens to be mainly controlled by the ratio of the dynamic viscosity of the air and of the liquid.","Besides, the observed evolution of the critical capillary number agrees with the mechanism suggested by Eggers (2001) that is based on the cusp dynamic."],"url":"http://arxiv.org/abs/2402.17571v1","category":"physics.flu-dyn"}
{"created":"2024-02-27 15:08:14","title":"Backpropagation-Based Analytical Derivatives of EKF Covariance for Active Sensing","abstract":"To enhance accuracy of robot state estimation, perception-aware (or active sensing) methods seek trajectories that minimize uncertainty. To this aim, one possibility is to seek trajectories that minimize the final covariance of an extended Kalman filter (EKF), w.r.t. its control inputs over a given horizon. However, this can be computationally demanding. In this article, we derive novel backpropagation analytical formulas for the derivatives of the final covariance of an EKF w.r.t. its inputs. We then leverage the obtained gradients as an enabling technology to derive perception-aware optimal motion plans. Simulations validate the approach, showcasing improvements in both estimation accuracy and execution time. Experimental results on a real large ground vehicle also support the method.","sentences":["To enhance accuracy of robot state estimation, perception-aware (or active sensing) methods seek trajectories that minimize uncertainty.","To this aim, one possibility is to seek trajectories that minimize the final covariance of an extended Kalman filter (EKF), w.r.t.","its control inputs over a given horizon.","However, this can be computationally demanding.","In this article, we derive novel backpropagation analytical formulas for the derivatives of the final covariance of an EKF w.r.t.","its inputs.","We then leverage the obtained gradients as an enabling technology to derive perception-aware optimal motion plans.","Simulations validate the approach, showcasing improvements in both estimation accuracy and execution time.","Experimental results on a real large ground vehicle also support the method."],"url":"http://arxiv.org/abs/2402.17569v1","category":"cs.RO"}
{"created":"2024-02-27 13:18:00","title":"syren-halofit: A fast, interpretable, high-precision formula for the $\u039b$CDM nonlinear matter power spectrum","abstract":"Rapid and accurate evaluation of the nonlinear matter power spectrum, $P(k)$, as a function of cosmological parameters and redshift is of fundamental importance in cosmology. Analytic approximations provide an interpretable solution, yet current approximations are neither fast nor accurate relative to black-box numerical emulators. We use symbolic regression to obtain simple analytic approximations to the nonlinear scale, $k_\\sigma$, the effective spectral index, $n_{\\rm eff}$, and the curvature, $C$, which are required for the halofit model. We then re-optimise the coefficients of halofit to fit a wide range of cosmologies and redshifts. We then again exploit symbolic regression to explore the space of analytic expressions to fit the residuals between $P(k)$ and the optimised predictions of halofit. All methods are validated against $N$-body simulations. Our symbolic expressions for $k_\\sigma$, $n_{\\rm eff}$ and $C$ have root mean squared fractional errors of 0.8%, 0.2% and 0.3%, respectively, for redshifts below 3 and a wide range of cosmologies. The re-optimised halofit parameters reduce the root mean squared fractional error from 3% to below 2% for wavenumbers $k=9\\times10^{-3}-9 \\, h{\\rm Mpc^{-1}}$. We introduce syren-halofit (symbolic-regression-enhanced halofit), an extension to halofit containing a short symbolic correction which improves this error to 1%. Our method is 2350 and 3170 times faster than current halofit and hmcode implementations, respectively, and 2680 and 64 times faster than EuclidEmulator2 (which requires running class) and the BACCO emulator. We obtain comparable accuracy to EuclidEmulator2 and the BACCO emulator when tested on $N$-body simulations. Our work greatly increases the speed and accuracy of symbolic approximations to $P(k)$, making them significantly faster than their numerical counterparts without loss of accuracy.","sentences":["Rapid and accurate evaluation of the nonlinear matter power spectrum, $P(k)$, as a function of cosmological parameters and redshift is of fundamental importance in cosmology.","Analytic approximations provide an interpretable solution, yet current approximations are neither fast nor accurate relative to black-box numerical emulators.","We use symbolic regression to obtain simple analytic approximations to the nonlinear scale, $k_\\sigma$, the effective spectral index, $n_{\\rm eff}$, and the curvature, $C$, which are required for the halofit model.","We then re-optimise the coefficients of halofit to fit a wide range of cosmologies and redshifts.","We then again exploit symbolic regression to explore the space of analytic expressions to fit the residuals between $P(k)$ and the optimised predictions of halofit.","All methods are validated against $N$-body simulations.","Our symbolic expressions for $k_\\sigma$, $n_{\\rm eff}$ and $C$ have root mean squared fractional errors of 0.8%, 0.2% and 0.3%, respectively, for redshifts below 3 and a wide range of cosmologies.","The re-optimised halofit parameters reduce the root mean squared fractional error from 3% to below 2% for wavenumbers $k=9\\times10^{-3}-9 \\, h{\\rm Mpc^{-1}}$.","We introduce syren-halofit (symbolic-regression-enhanced halofit), an extension to halofit containing a short symbolic correction which improves this error to 1%.","Our method is 2350 and 3170 times faster than current halofit and hmcode implementations, respectively, and 2680 and 64 times faster than EuclidEmulator2 (which requires running class) and the BACCO emulator.","We obtain comparable accuracy to EuclidEmulator2 and the BACCO emulator when tested on $N$-body simulations.","Our work greatly increases the speed and accuracy of symbolic approximations to $P(k)$, making them significantly faster than their numerical counterparts without loss of accuracy."],"url":"http://arxiv.org/abs/2402.17492v1","category":"astro-ph.CO"}
{"created":"2024-02-27 12:42:07","title":"Model X-ray:Detect Backdoored Models via Decision Boundary","abstract":"Deep neural networks (DNNs) have revolutionized various industries, leading to the rise of Machine Learning as a Service (MLaaS). In this paradigm, well-trained models are typically deployed through APIs. However, DNNs are susceptible to backdoor attacks, which pose significant risks to their applications. This vulnerability necessitates a method for users to ascertain whether an API is compromised before usage. Although many backdoor detection methods have been developed, they often operate under the assumption that the defender has access to specific information such as details of the attack, soft predictions from the model API, and even the knowledge of the model parameters, limiting their practicality in MLaaS scenarios. To address it, in this paper, we begin by presenting an intriguing observation: the decision boundary of the backdoored model exhibits a greater degree of closeness than that of the clean model. Simultaneously, if only one single label is infected, a larger portion of the regions will be dominated by the attacked label. Building upon this observation, we propose Model X-ray, a novel backdoor detection approach for MLaaS through the analysis of decision boundaries. Model X-ray can not only identify whether the target API is infected by backdoor attacks but also determine the target attacked label under the all-to-one attack strategy. Importantly, it accomplishes this solely by the hard prediction of clean inputs, regardless of any assumptions about attacks and prior knowledge of the training details of the model. Extensive experiments demonstrated that Model X-ray can be effective for MLaaS across diverse backdoor attacks, datasets, and architectures.","sentences":["Deep neural networks (DNNs) have revolutionized various industries, leading to the rise of Machine Learning as a Service (MLaaS).","In this paradigm, well-trained models are typically deployed through APIs.","However, DNNs are susceptible to backdoor attacks, which pose significant risks to their applications.","This vulnerability necessitates a method for users to ascertain whether an API is compromised before usage.","Although many backdoor detection methods have been developed, they often operate under the assumption that the defender has access to specific information such as details of the attack, soft predictions from the model API, and even the knowledge of the model parameters, limiting their practicality in MLaaS scenarios.","To address it, in this paper, we begin by presenting an intriguing observation: the decision boundary of the backdoored model exhibits a greater degree of closeness than that of the clean model.","Simultaneously, if only one single label is infected, a larger portion of the regions will be dominated by the attacked label.","Building upon this observation, we propose Model X-ray, a novel backdoor detection approach for MLaaS through the analysis of decision boundaries.","Model X-ray can not only identify whether the target API is infected by backdoor attacks but also determine the target attacked label under the all-to-one attack strategy.","Importantly, it accomplishes this solely by the hard prediction of clean inputs, regardless of any assumptions about attacks and prior knowledge of the training details of the model.","Extensive experiments demonstrated that Model X-ray can be effective for MLaaS across diverse backdoor attacks, datasets, and architectures."],"url":"http://arxiv.org/abs/2402.17465v1","category":"cs.CR"}
{"created":"2024-02-27 12:39:22","title":"On the upper and lower covariances under multiple probabilities","abstract":"In this paper, we define the upper (resp. lower) covariance under multiple probabilities via a corresponding max-min-max (resp. min-max-min) optimization problem and the related properties of covariances are obtained. In particular, we propose a fast algorithm of calculation for upper and lower covariances under the finite number of probabilities. As an application, our algorithm can be used to solve a class of quadratic programming problem exactly, and we obtain a probabilistic representation of such quadratic programming problem.","sentences":["In this paper, we define the upper (resp.","lower) covariance under multiple probabilities via a corresponding max-min-max (resp.","min-max-min) optimization problem and the related properties of covariances are obtained.","In particular, we propose a fast algorithm of calculation for upper and lower covariances under the finite number of probabilities.","As an application, our algorithm can be used to solve a class of quadratic programming problem exactly, and we obtain a probabilistic representation of such quadratic programming problem."],"url":"http://arxiv.org/abs/2402.17462v1","category":"math.PR"}
{"created":"2024-02-27 12:28:01","title":"Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning","abstract":"Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit ($\\mu$P and its depth extension), then some hyperparameters - such as the learning rate - exhibit transfer from small to very large models, thus reducing the cost of hyperparameter tuning. From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is remarkably consistent across very different model sizes. In this work, we find empirical evidence that learning rate transfer can be attributed to the fact that under $\\mu$P and its depth extension, the largest eigenvalue of the training loss Hessian (i.e. the sharpness) is largely independent of the width and depth of the network for a sustained period of training time. On the other hand, we show that under the neural tangent kernel (NTK) regime, the sharpness exhibits very different dynamics at different scales, thus preventing learning rate transfer. But what causes these differences in the sharpness dynamics? Through a connection between the spectra of the Hessian and the NTK matrix, we argue that the cause lies in the presence (for $\\mu$P) or progressive absence (for the NTK regime) of feature learning, which results in a different evolution of the NTK, and thus of the sharpness. We corroborate our claims with a substantial suite of experiments, covering a wide range of datasets and architectures: from ResNets and Vision Transformers trained on benchmark vision datasets to Transformers-based language models trained on WikiText","sentences":["Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit ($\\mu$P and its depth extension), then some hyperparameters - such as the learning rate - exhibit transfer from small to very large models, thus reducing the cost of hyperparameter tuning.","From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is remarkably consistent across very different model sizes.","In this work, we find empirical evidence that learning rate transfer can be attributed to the fact that under $\\mu$P and its depth extension, the largest eigenvalue of the training loss Hessian (i.e. the sharpness) is largely independent of the width and depth of the network for a sustained period of training time.","On the other hand, we show that under the neural tangent kernel (NTK) regime, the sharpness exhibits very different dynamics at different scales, thus preventing learning rate transfer.","But what causes these differences in the sharpness dynamics?","Through a connection between the spectra of the Hessian and the NTK matrix, we argue that the cause lies in the presence (for $\\mu$P) or progressive absence (for the NTK regime) of feature learning, which results in a different evolution of the NTK, and thus of the sharpness.","We corroborate our claims with a substantial suite of experiments, covering a wide range of datasets and architectures: from ResNets and Vision Transformers trained on benchmark vision datasets to Transformers-based language models trained on WikiText"],"url":"http://arxiv.org/abs/2402.17457v1","category":"cs.LG"}
{"created":"2024-02-27 11:45:21","title":"Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder","abstract":"Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modules alongside the EEG stream from CET-MAE and further enables an LLM (specifically BART) to decode text from EEG sequences. Comprehensive experiments conducted on the popular text-evoked EEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms the state-of-the-art in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%, respectively. These results indicate significant advancements in the field and underscores the proposed framework's potential to enable more powerful and widespread BCI applications.","sentences":["Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs).","However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding.","To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder.","Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modules alongside the EEG stream from CET-MAE and further enables an LLM (specifically BART) to decode text from EEG sequences.","Comprehensive experiments conducted on the popular text-evoked EEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms the state-of-the-art in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%, respectively.","These results indicate significant advancements in the field and underscores the proposed framework's potential to enable more powerful and widespread BCI applications."],"url":"http://arxiv.org/abs/2402.17433v1","category":"cs.CL"}
{"created":"2024-02-27 11:08:51","title":"Neural Video Compression with Feature Modulation","abstract":"The emerging conditional coding-based neural video codec (NVC) shows superiority over commonly-used residual coding-based codec and the latest NVC already claims to outperform the best traditional codec. However, there still exist critical problems blocking the practicality of NVC. In this paper, we propose a powerful conditional coding-based NVC that solves two critical problems via feature modulation. The first is how to support a wide quality range in a single model. Previous NVC with this capability only supports about 3.8 dB PSNR range on average. To tackle this limitation, we modulate the latent feature of the current frame via the learnable quantization scaler. During the training, we specially design the uniform quantization parameter sampling mechanism to improve the harmonization of encoding and quantization. This results in a better learning of the quantization scaler and helps our NVC support about 11.4 dB PSNR range. The second is how to make NVC still work under a long prediction chain. We expose that the previous SOTA NVC has an obvious quality degradation problem when using a large intra-period setting. To this end, we propose modulating the temporal feature with a periodically refreshing mechanism to boost the quality. %Besides solving the above two problems, we also design a single model that can support both RGB and YUV colorspaces. Notably, under single intra-frame setting, our codec can achieve 29.7\\% bitrate saving over previous SOTA NVC with 16\\% MACs reduction. Our codec serves as a notable landmark in the journey of NVC evolution. The codes are at https://github.com/microsoft/DCVC.","sentences":["The emerging conditional coding-based neural video codec (NVC) shows superiority over commonly-used residual coding-based codec and the latest NVC already claims to outperform the best traditional codec.","However, there still exist critical problems blocking the practicality of NVC.","In this paper, we propose a powerful conditional coding-based NVC that solves two critical problems via feature modulation.","The first is how to support a wide quality range in a single model.","Previous NVC with this capability only supports about 3.8 dB PSNR range on average.","To tackle this limitation, we modulate the latent feature of the current frame via the learnable quantization scaler.","During the training, we specially design the uniform quantization parameter sampling mechanism to improve the harmonization of encoding and quantization.","This results in a better learning of the quantization scaler and helps our NVC support about 11.4 dB PSNR range.","The second is how to make NVC still work under a long prediction chain.","We expose that the previous SOTA NVC has an obvious quality degradation problem when using a large intra-period setting.","To this end, we propose modulating the temporal feature with a periodically refreshing mechanism to boost the quality.","%","Besides solving the above two problems, we also design a single model that can support both RGB and YUV colorspaces.","Notably, under single intra-frame setting, our codec can achieve 29.7\\% bitrate saving over previous SOTA NVC with 16\\% MACs reduction.","Our codec serves as a notable landmark in the journey of NVC evolution.","The codes are at https://github.com/microsoft/DCVC."],"url":"http://arxiv.org/abs/2402.17414v1","category":"cs.CV"}
{"created":"2024-02-27 10:48:22","title":"Quantum entanglement enabled ellipsometer for phase retardance measurement","abstract":"An ellipsometer is a vital precision tool used for measuring optical parameters with wide applications in many fields, including accurate measurements in film thickness, optical constants, structural profiles, etc. However, the precise measurement of photosensitive materials meets huge obstacles because of the excessive input photons, therefore the requirement of enhancing detection accuracy under low incident light intensity is an essential topic in the precision measurement. In this work, by combining a polarization-entangled photon source with a classical transmission-type ellipsometer, the quantum ellipsometer with the PSA (Polarizer-Sample-Analyzer) and the Senarmount method is constructed firstly to measure the phase retardation of the birefringent materials. The experimental results show that the accuracy can reach to nanometer scale at extremely low input intensity, and the stability are within 1% for all specimens tested with a compensator involved. Our work paves the way for precision measurement at low incident light intensity, with potential applications in measuring photosensitive materials, active-biological samples and other remote monitoring scenarios.","sentences":["An ellipsometer is a vital precision tool used for measuring optical parameters with wide applications in many fields, including accurate measurements in film thickness, optical constants, structural profiles, etc.","However, the precise measurement of photosensitive materials meets huge obstacles because of the excessive input photons, therefore the requirement of enhancing detection accuracy under low incident light intensity is an essential topic in the precision measurement.","In this work, by combining a polarization-entangled photon source with a classical transmission-type ellipsometer, the quantum ellipsometer with the PSA (Polarizer-Sample-Analyzer) and the Senarmount method is constructed firstly to measure the phase retardation of the birefringent materials.","The experimental results show that the accuracy can reach to nanometer scale at extremely low input intensity, and the stability are within 1% for all specimens tested with a compensator involved.","Our work paves the way for precision measurement at low incident light intensity, with potential applications in measuring photosensitive materials, active-biological samples and other remote monitoring scenarios."],"url":"http://arxiv.org/abs/2402.17401v1","category":"quant-ph"}
{"created":"2024-02-27 10:11:43","title":"Quasi-Bayesian Estimation and Inference with Control Functions","abstract":"We consider a quasi-Bayesian method that combines a frequentist estimation in the first stage and a Bayesian estimation/inference approach in the second stage. The study is motivated by structural discrete choice models that use the control function methodology to correct for endogeneity bias. In this scenario, the first stage estimates the control function using some frequentist parametric or nonparametric approach. The structural equation in the second stage, associated with certain complicated likelihood functions, can be more conveniently dealt with using a Bayesian approach. This paper studies the asymptotic properties of the quasi-posterior distributions obtained from the second stage. We prove that the corresponding quasi-Bayesian credible set does not have the desired coverage in large samples. Nonetheless, the quasi-Bayesian point estimator remains consistent and is asymptotically equivalent to a frequentist two-stage estimator. We show that one can obtain valid inference by bootstrapping the quasi-posterior that takes into account the first-stage estimation uncertainty.","sentences":["We consider a quasi-Bayesian method that combines a frequentist estimation in the first stage and a Bayesian estimation/inference approach in the second stage.","The study is motivated by structural discrete choice models that use the control function methodology to correct for endogeneity bias.","In this scenario, the first stage estimates the control function using some frequentist parametric or nonparametric approach.","The structural equation in the second stage, associated with certain complicated likelihood functions, can be more conveniently dealt with using a Bayesian approach.","This paper studies the asymptotic properties of the quasi-posterior distributions obtained from the second stage.","We prove that the corresponding quasi-Bayesian credible set does not have the desired coverage in large samples.","Nonetheless, the quasi-Bayesian point estimator remains consistent and is asymptotically equivalent to a frequentist two-stage estimator.","We show that one can obtain valid inference by bootstrapping the quasi-posterior that takes into account the first-stage estimation uncertainty."],"url":"http://arxiv.org/abs/2402.17374v1","category":"econ.EM"}
{"created":"2024-02-27 10:01:01","title":"Causal blind spots when using prediction models for treatment decisions","abstract":"Prediction models are increasingly proposed for guiding treatment decisions, but most fail to address the special role of treatments, leading to inappropriate use. This paper highlights the limitations of using standard prediction models for treatment decision support. We identify 'causal blind spots' in three common approaches to handling treatments in prediction modelling and illustrate potential harmful consequences in several medical applications. We advocate for an extension of guidelines for development, reporting, clinical evaluation and monitoring of prediction models to ensure that the intended use of the model is matched to an appropriate risk estimand. For decision support this requires a shift towards developing predictions under the specific treatment options under consideration ('predictions under interventions'). We argue that this will improve the efficacy of prediction models in guiding treatment decisions and prevent potential negative effects on patient outcomes.","sentences":["Prediction models are increasingly proposed for guiding treatment decisions, but most fail to address the special role of treatments, leading to inappropriate use.","This paper highlights the limitations of using standard prediction models for treatment decision support.","We identify 'causal blind spots' in three common approaches to handling treatments in prediction modelling and illustrate potential harmful consequences in several medical applications.","We advocate for an extension of guidelines for development, reporting, clinical evaluation and monitoring of prediction models to ensure that the intended use of the model is matched to an appropriate risk estimand.","For decision support this requires a shift towards developing predictions under the specific treatment options under consideration ('predictions under interventions').","We argue that this will improve the efficacy of prediction models in guiding treatment decisions and prevent potential negative effects on patient outcomes."],"url":"http://arxiv.org/abs/2402.17366v1","category":"stat.ME"}
{"created":"2024-02-27 09:27:54","title":"Understanding the training of PINNs for unsteady flow past a plunging foil through the lens of input subdomain level loss function gradients","abstract":"Recently immersed boundary method-inspired physics-informed neural networks (PINNs) including the moving boundary-enabled PINNs (MB-PINNs) have shown the ability to accurately reconstruct velocity and recover pressure as a hidden variable for unsteady flow past moving bodies. Considering flow past a plunging foil, MB-PINNs were trained with global physics loss relaxation and also in conjunction with a physics-based undersampling method, obtaining good accuracy. The purpose of this study was to investigate which input spatial subdomain contributes to the training under the effect of physics loss relaxation and physics-based undersampling. In the context of MB-PINNs training, three spatial zones: the moving body, wake, and outer zones were defined. To quantify which spatial zone drives the training, two novel metrics are computed from the zonal loss component gradient statistics and the proportion of sample points in each zone. Results confirm that the learning indeed depends on the combined effect of the zonal loss component gradients and the proportion of points in each zone. Moreover, the dominant input zones are also the ones that have the strongest solution gradients in some sense.","sentences":["Recently immersed boundary method-inspired physics-informed neural networks (PINNs) including the moving boundary-enabled PINNs (MB-PINNs) have shown the ability to accurately reconstruct velocity and recover pressure as a hidden variable for unsteady flow past moving bodies.","Considering flow past a plunging foil, MB-PINNs were trained with global physics loss relaxation and also in conjunction with a physics-based undersampling method, obtaining good accuracy.","The purpose of this study was to investigate which input spatial subdomain contributes to the training under the effect of physics loss relaxation and physics-based undersampling.","In the context of MB-PINNs training, three spatial zones: the moving body, wake, and outer zones were defined.","To quantify which spatial zone drives the training, two novel metrics are computed from the zonal loss component gradient statistics and the proportion of sample points in each zone.","Results confirm that the learning indeed depends on the combined effect of the zonal loss component gradients and the proportion of points in each zone.","Moreover, the dominant input zones are also the ones that have the strongest solution gradients in some sense."],"url":"http://arxiv.org/abs/2402.17346v1","category":"physics.flu-dyn"}
{"created":"2024-02-27 09:11:10","title":"Outdoor Environment Reconstruction with Deep Learning on Radio Propagation Paths","abstract":"Conventional methods for outdoor environment reconstruction rely predominantly on vision-based techniques like photogrammetry and LiDAR, facing limitations such as constrained coverage, susceptibility to environmental conditions, and high computational and energy demands. These challenges are particularly pronounced in applications like augmented reality navigation, especially when integrated with wearable devices featuring constrained computational resources and energy budgets. In response, this paper proposes a novel approach harnessing ambient wireless signals for outdoor environment reconstruction. By analyzing radio frequency (RF) data, the paper aims to deduce the environmental characteristics and digitally reconstruct the outdoor surroundings. Investigating the efficacy of selected deep learning (DL) techniques on the synthetic RF dataset WAIR-D, the study endeavors to address the research gap in this domain. Two DL-driven approaches are evaluated (convolutional U-Net and CLIP+ based on vision transformers), with performance assessed using metrics like intersection-over-union (IoU), Hausdorff distance, and Chamfer distance. The results demonstrate promising performance of the RF-based reconstruction method, paving the way towards lightweight and scalable reconstruction solutions.","sentences":["Conventional methods for outdoor environment reconstruction rely predominantly on vision-based techniques like photogrammetry and LiDAR, facing limitations such as constrained coverage, susceptibility to environmental conditions, and high computational and energy demands.","These challenges are particularly pronounced in applications like augmented reality navigation, especially when integrated with wearable devices featuring constrained computational resources and energy budgets.","In response, this paper proposes a novel approach harnessing ambient wireless signals for outdoor environment reconstruction.","By analyzing radio frequency (RF) data, the paper aims to deduce the environmental characteristics and digitally reconstruct the outdoor surroundings.","Investigating the efficacy of selected deep learning (DL) techniques on the synthetic RF dataset WAIR-D, the study endeavors to address the research gap in this domain.","Two DL-driven approaches are evaluated (convolutional U-Net and CLIP+ based on vision transformers), with performance assessed using metrics like intersection-over-union (IoU), Hausdorff distance, and Chamfer distance.","The results demonstrate promising performance of the RF-based reconstruction method, paving the way towards lightweight and scalable reconstruction solutions."],"url":"http://arxiv.org/abs/2402.17336v1","category":"cs.NI"}
{"created":"2024-02-27 08:00:52","title":"An Interpretable Evaluation of Entropy-based Novelty of Generative Models","abstract":"The massive developments of generative model frameworks and architectures require principled methods for the evaluation of a model's novelty compared to a reference dataset or baseline generative models. While the recent literature has extensively studied the evaluation of the quality, diversity, and generalizability of generative models, the assessment of a model's novelty compared to a baseline model has not been adequately studied in the machine learning community. In this work, we focus on the novelty assessment under multi-modal generative models and attempt to answer the following question: Given the samples of a generative model $\\mathcal{G}$ and a reference dataset $\\mathcal{S}$, how can we discover and count the modes expressed by $\\mathcal{G}$ more frequently than in $\\mathcal{S}$. We introduce a spectral approach to the described task and propose the Kernel-based Entropic Novelty (KEN) score to quantify the mode-based novelty of distribution $P_\\mathcal{G}$ with respect to distribution $P_\\mathcal{S}$. We analytically interpret the behavior of the KEN score under mixture distributions with sub-Gaussian components. Next, we develop a method based on Cholesky decomposition to compute the KEN score from observed samples. We support the KEN-based quantification of novelty by presenting several numerical results on synthetic and real image distributions. Our numerical results indicate the success of the proposed approach in detecting the novel modes and the comparison of state-of-the-art generative models.","sentences":["The massive developments of generative model frameworks and architectures require principled methods for the evaluation of a model's novelty compared to a reference dataset or baseline generative models.","While the recent literature has extensively studied the evaluation of the quality, diversity, and generalizability of generative models, the assessment of a model's novelty compared to a baseline model has not been adequately studied in the machine learning community.","In this work, we focus on the novelty assessment under multi-modal generative models and attempt to answer the following question: Given the samples of a generative model $\\mathcal{G}$ and a reference dataset $\\mathcal{S}$, how can we discover and count the modes expressed by $\\mathcal{G}$ more frequently than in $\\mathcal{S}$. We introduce a spectral approach to the described task and propose the Kernel-based Entropic Novelty (KEN) score to quantify the mode-based novelty of distribution $P_\\mathcal{G}$ with respect to distribution $P_\\mathcal{S}$. We analytically interpret the behavior of the KEN score under mixture distributions with sub-Gaussian components.","Next, we develop a method based on Cholesky decomposition to compute the KEN score from observed samples.","We support the KEN-based quantification of novelty by presenting several numerical results on synthetic and real image distributions.","Our numerical results indicate the success of the proposed approach in detecting the novel modes and the comparison of state-of-the-art generative models."],"url":"http://arxiv.org/abs/2402.17287v1","category":"cs.LG"}
{"created":"2024-02-27 07:56:12","title":"Unitally nondistributive quantales","abstract":"Unitally nondistributive quantales are unital quantales such that the unit is approximable by the totally below relation and does not meet-distribute over arbitrary joins. This special class of nondistributive complete lattices is characterized and each of its members contains at least $7$ elements. Moreover, under mild conditions, every quantale has an extension to a unitally nondistributive quantale by the addition of an isolated unit. As a byproduct of this construction we prove that there exist $30$ non-isomorphic, unitally nondistributive quantales on the set of $7$ elements.","sentences":["Unitally nondistributive quantales are unital quantales such that the unit is approximable by the totally below relation and does not meet-distribute over arbitrary joins.","This special class of nondistributive complete lattices is characterized and each of its members contains at least $7$ elements.","Moreover, under mild conditions, every quantale has an extension to a unitally nondistributive quantale by the addition of an isolated unit.","As a byproduct of this construction we prove that there exist $30$ non-isomorphic, unitally nondistributive quantales on the set of $7$ elements."],"url":"http://arxiv.org/abs/2402.17284v1","category":"math.GN"}
{"created":"2024-02-27 06:29:28","title":"The status and challenges for prostate SBRT treatments in United States proton therapy centers: An NRG Oncology practice survey","abstract":"A survey was designed to inquire about the practice of proton SBRT treatment for prostate cancer. The survey was distributed to all 30 proton therapy centers in the United States that participate in the National Clinical Trial Network in Feb. 2023. The survey focused on usage, patient selection criteria, prescriptions, target contours, dose constraints, treatment plan optimization and evaluation methods, patient-specific QA, and IGRT methods. Results: We received responses from 25 centers (83% participation). Only 8 respondent proton centers (32%) reported performing SBRT of the prostate. The remaining 17 centers cited three primary reasons for not offering this treatment: no clinical need, lack of volumetric imaging, and/or lack of clinical evidence. Only 1 center cited the reduction in overall reimbursement as a concern for not offering prostate SBRT. Several common practices among the 8 centers offering SBRT for the prostate were noted, such as using Hydrogel spacers, fiducial markers, and MRI for target delineation. Most proton centers (87.5%) utilized pencil beam scanning (PBS) delivery and completed Imaging and Radiation Oncology Core (IROC) phantom credentialing. Treatment planning typically used parallel opposed lateral beams, and consistent parameters for setup and range uncertainties were used for plan optimization and robustness evaluation. Measurements-based patient-specific QA, beam delivery every other day, fiducial contours for IGRT, and total doses of 35-40 GyRBE were consistent across all centers. However, there was no consensus on the risk levels for patient selection. Conclusion: Prostate SBRT is used in about 1/3 of proton centers in the US. There was a significant consistency in practices among proton centers treating with proton SBRT. It is possible that the adoption of proton SBRT may become more common if proton SBRT is more commonly offered in clinical trials.","sentences":["A survey was designed to inquire about the practice of proton SBRT treatment for prostate cancer.","The survey was distributed to all 30 proton therapy centers in the United States that participate in the National Clinical Trial Network in Feb. 2023.","The survey focused on usage, patient selection criteria, prescriptions, target contours, dose constraints, treatment plan optimization and evaluation methods, patient-specific QA, and IGRT methods.","Results:","We received responses from 25 centers (83% participation).","Only 8 respondent proton centers (32%) reported performing SBRT of the prostate.","The remaining 17 centers cited three primary reasons for not offering this treatment: no clinical need, lack of volumetric imaging, and/or lack of clinical evidence.","Only 1 center cited the reduction in overall reimbursement as a concern for not offering prostate SBRT.","Several common practices among the 8 centers offering SBRT for the prostate were noted, such as using Hydrogel spacers, fiducial markers, and MRI for target delineation.","Most proton centers (87.5%) utilized pencil beam scanning (PBS) delivery and completed Imaging and Radiation Oncology Core (IROC) phantom credentialing.","Treatment planning typically used parallel opposed lateral beams, and consistent parameters for setup and range uncertainties were used for plan optimization and robustness evaluation.","Measurements-based patient-specific QA, beam delivery every other day, fiducial contours for IGRT, and total doses of 35-40 GyRBE were consistent across all centers.","However, there was no consensus on the risk levels for patient selection.","Conclusion: Prostate SBRT is used in about 1/3 of proton centers in the US.","There was a significant consistency in practices among proton centers treating with proton SBRT.","It is possible that the adoption of proton SBRT may become more common if proton SBRT is more commonly offered in clinical trials."],"url":"http://arxiv.org/abs/2402.17244v1","category":"physics.med-ph"}
{"created":"2024-02-27 05:28:36","title":"On the probability of a Pareto record","abstract":"Given a sequence of independent random vectors taking values in ${\\mathbb R}^d$ and having common continuous distribution $F$, say that the $n^{\\rm \\scriptsize th}$ observation sets a (Pareto) record if it is not dominated (in every coordinate) by any preceding observation. Let $p_n(F) \\equiv p_{n, d}(F)$ denote the probability that the $n^{\\rm \\scriptsize th}$ observation sets a record. There are many interesting questions to address concerning $p_n$ and multivariate records more generally, but this short paper focuses on how $p_n$ varies with $F$, particularly if, under $F$, the coordinates exhibit negative dependence or positive dependence (rather than independence, a more-studied case). We introduce new notions of negative and positive dependence ideally suited for such a study, called negative record-setting probability dependence (NRSPD) and positive record-setting probability dependence (PRSPD), relate these notions to existing notions of dependence, and for fixed $d \\geq 2$ and $n \\geq 1$ prove that the image of the mapping $p_n$ on the domain of NRSPD (respectively, PRSPD) distributions is $[p^*_n, 1]$ (resp., $[n^{-1}, p^*_n]$), where $p^*_n$ is the record-setting probability for any $F$ governing independent coordinates.","sentences":["Given a sequence of independent random vectors taking values in ${\\mathbb R}^d$ and having common continuous distribution $F$, say that the $n^{\\rm \\scriptsize th}$ observation sets a (Pareto) record if it is not dominated (in every coordinate) by any preceding observation.","Let $p_n(F) \\equiv p_{n, d}(F)$ denote the probability that the $n^{\\rm \\scriptsize th}$ observation sets a record.","There are many interesting questions to address concerning $p_n$ and multivariate records more generally, but this short paper focuses on how $p_n$ varies with $F$, particularly if, under $F$, the coordinates exhibit negative dependence or positive dependence (rather than independence, a more-studied case).","We introduce new notions of negative and positive dependence ideally suited for such a study, called negative record-setting probability dependence (NRSPD) and positive record-setting probability dependence (PRSPD), relate these notions to existing notions of dependence, and for fixed $d \\geq 2$ and $n \\geq 1$ prove that the image of the mapping $p_n$ on the domain of NRSPD (respectively, PRSPD) distributions is $[p^*_n, 1]$ (resp., $[n^{-1}, p^*_n]$), where $p^*_n$ is the record-setting probability for any $F$ governing independent coordinates."],"url":"http://arxiv.org/abs/2402.17220v1","category":"math.PR"}
{"created":"2024-02-27 04:55:30","title":"Scalable Identification of Minimum Undesignable RNA Motifs on Loop-Pair Graphs","abstract":"Motivation: RNA design aims to find at least one sequence that folds with the highest probability into a designated target structure, but some structures are undesignable in the sense that no sequence folds into them. Identifying undesignable structures is useful in delineating and understanding the limit of RNA designability, but has received little attention until recently. In addition, existing methods on undesignability are not scalable and not interpretable.   Results: We introduce a novel graph representation and a new general algorithmic framework to efficiently identify undesignable motifs in a secondary structure. The proposed algorithm enumerates minimal motifs based on the loop-pair graph representation of a structure and establishes the undesignability of a motif by proposing rival substructure(s). Our work can also identify unique minimum undesignable motifs across different structures. Our implemented algorithms successfully identify 26 unique minimum undesignable motifs among 18 undesignable puzzles from the benchmark Eterna100. Additionally, our algorithm is so efficient that it scales to natural structures of 16S and 23S Ribosomal RNAs (about 1,500 and 3,000 nucleotides, resp.), and finds all of those structures in the widely used ArchiveII database to be undesignable, with 73 unique minimum undesignable motifs, under the standard Turner energy model in ViennaRNA.","sentences":["Motivation: RNA design aims to find at least one sequence that folds with the highest probability into a designated target structure, but some structures are undesignable in the sense that no sequence folds into them.","Identifying undesignable structures is useful in delineating and understanding the limit of RNA designability, but has received little attention until recently.","In addition, existing methods on undesignability are not scalable and not interpretable.   ","Results: We introduce a novel graph representation and a new general algorithmic framework to efficiently identify undesignable motifs in a secondary structure.","The proposed algorithm enumerates minimal motifs based on the loop-pair graph representation of a structure and establishes the undesignability of a motif by proposing rival substructure(s).","Our work can also identify unique minimum undesignable motifs across different structures.","Our implemented algorithms successfully identify 26 unique minimum undesignable motifs among 18 undesignable puzzles from the benchmark Eterna100.","Additionally, our algorithm is so efficient that it scales to natural structures of 16S and 23S Ribosomal RNAs (about 1,500 and 3,000 nucleotides, resp.), and finds all of those structures in the widely used ArchiveII database to be undesignable, with 73 unique minimum undesignable motifs, under the standard Turner energy model in ViennaRNA."],"url":"http://arxiv.org/abs/2402.17206v1","category":"cs.DS"}
{"created":"2024-02-27 04:37:04","title":"Enhancing Quality of Compressed Images by Mitigating Enhancement Bias Towards Compression Domain","abstract":"Existing quality enhancement methods for compressed images focus on aligning the enhancement domain with the raw domain to yield realistic images. However, these methods exhibit a pervasive enhancement bias towards the compression domain, inadvertently regarding it as more realistic than the raw domain. This bias makes enhanced images closely resemble their compressed counterparts, thus degrading their perceptual quality. In this paper, we propose a simple yet effective method to mitigate this bias and enhance the quality of compressed images. Our method employs a conditional discriminator with the compressed image as a key condition, and then incorporates a domain-divergence regularization to actively distance the enhancement domain from the compression domain. Through this dual strategy, our method enables the discrimination against the compression domain, and brings the enhancement domain closer to the raw domain. Comprehensive quality evaluations confirm the superiority of our method over other state-of-the-art methods without incurring inference overheads.","sentences":["Existing quality enhancement methods for compressed images focus on aligning the enhancement domain with the raw domain to yield realistic images.","However, these methods exhibit a pervasive enhancement bias towards the compression domain, inadvertently regarding it as more realistic than the raw domain.","This bias makes enhanced images closely resemble their compressed counterparts, thus degrading their perceptual quality.","In this paper, we propose a simple yet effective method to mitigate this bias and enhance the quality of compressed images.","Our method employs a conditional discriminator with the compressed image as a key condition, and then incorporates a domain-divergence regularization to actively distance the enhancement domain from the compression domain.","Through this dual strategy, our method enables the discrimination against the compression domain, and brings the enhancement domain closer to the raw domain.","Comprehensive quality evaluations confirm the superiority of our method over other state-of-the-art methods without incurring inference overheads."],"url":"http://arxiv.org/abs/2402.17200v1","category":"cs.CV"}
{"created":"2024-02-27 04:31:07","title":"Ratios conjecture of quartic $L$-functions of prime moduli","abstract":"We apply the method of multiple Dirichlet series to develop $L$-functions ratios conjecture with one shift in both the numerator and denominator in certain ranges for the family of quartic Hecke $L$-functions of prime moduli over the Gaussian field under the generalized Riemann hypothesis. As consequences, we evaluate asymptotically the first moment of central values as well as the one-level density of the same family of $L$-functions.","sentences":["We apply the method of multiple Dirichlet series to develop $L$-functions ratios conjecture with one shift in both the numerator and denominator in certain ranges for the family of quartic Hecke $L$-functions of prime moduli over the Gaussian field under the generalized Riemann hypothesis.","As consequences, we evaluate asymptotically the first moment of central values as well as the one-level density of the same family of $L$-functions."],"url":"http://arxiv.org/abs/2402.17198v1","category":"math.NT"}
{"created":"2024-02-27 04:23:35","title":"Prediction of the SYM-H Index Using a Bayesian Deep Learning Method with Uncertainty Quantification","abstract":"We propose a novel deep learning framework, named SYMHnet, which employs a graph neural network and a bidirectional long short-term memory network to cooperatively learn patterns from solar wind and interplanetary magnetic field parameters for short-term forecasts of the SYM-H index based on 1-minute and 5-minute resolution data. SYMHnet takes, as input, the time series of the parameters' values provided by NASA's Space Science Data Coordinated Archive and predicts, as output, the SYM-H index value at time point t + w hours for a given time point t where w is 1 or 2. By incorporating Bayesian inference into the learning framework, SYMHnet can quantify both aleatoric (data) uncertainty and epistemic (model) uncertainty when predicting future SYM-H indices. Experimental results show that SYMHnet works well at quiet time and storm time, for both 1-minute and 5-minute resolution data. The results also show that SYMHnet generally performs better than related machine learning methods. For example, SYMHnet achieves a forecast skill score (FSS) of 0.343 compared to the FSS of 0.074 of a recent gradient boosting machine (GBM) method when predicting SYM-H indices (1 hour in advance) in a large storm (SYM-H = -393 nT) using 5-minute resolution data. When predicting the SYM-H indices (2 hours in advance) in the large storm, SYMHnet achieves an FSS of 0.553 compared to the FSS of 0.087 of the GBM method. In addition, SYMHnet can provide results for both data and model uncertainty quantification, whereas the related methods cannot.","sentences":["We propose a novel deep learning framework, named SYMHnet, which employs a graph neural network and a bidirectional long short-term memory network to cooperatively learn patterns from solar wind and interplanetary magnetic field parameters for short-term forecasts of the SYM-H index based on 1-minute and 5-minute resolution data.","SYMHnet takes, as input, the time series of the parameters' values provided by NASA's Space Science Data Coordinated Archive and predicts, as output, the SYM-H index value at time point t + w hours for a given time point t where w is 1 or 2.","By incorporating Bayesian inference into the learning framework, SYMHnet can quantify both aleatoric (data) uncertainty and epistemic (model) uncertainty when predicting future SYM-H indices.","Experimental results show that SYMHnet works well at quiet time and storm time, for both 1-minute and 5-minute resolution data.","The results also show that SYMHnet generally performs better than related machine learning methods.","For example, SYMHnet achieves a forecast skill score (FSS) of 0.343 compared to the FSS of 0.074 of a recent gradient boosting machine (GBM) method when predicting SYM-H indices (1 hour in advance) in a large storm (SYM-H = -393 nT) using 5-minute resolution data.","When predicting the SYM-H indices (2 hours in advance) in the large storm, SYMHnet achieves an FSS of 0.553 compared to the FSS of 0.087 of the GBM method.","In addition, SYMHnet can provide results for both data and model uncertainty quantification, whereas the related methods cannot."],"url":"http://arxiv.org/abs/2402.17196v1","category":"astro-ph.IM"}
{"created":"2024-02-27 03:44:55","title":"Inpainting Computational Fluid Dynamics with Deep Learning","abstract":"Fluid data completion is a research problem with high potential benefit for both experimental and computational fluid dynamics. An effective fluid data completion method reduces the required number of sensors in a fluid dynamics experiment, and allows a coarser and more adaptive mesh for a Computational Fluid Dynamics (CFD) simulation. However, the ill-posed nature of the fluid data completion problem makes it prohibitively difficult to obtain a theoretical solution and presents high numerical uncertainty and instability for a data-driven approach (e.g., a neural network model). To address these challenges, we leverage recent advancements in computer vision, employing the vector quantization technique to map both complete and incomplete fluid data spaces onto discrete-valued lower-dimensional representations via a two-stage learning procedure. We demonstrated the effectiveness of our approach on Kolmogorov flow data (Reynolds number: 1000) occluded by masks of different size and arrangement. Experimental results show that our proposed model consistently outperforms benchmark models under different occlusion settings in terms of point-wise reconstruction accuracy as well as turbulent energy spectrum and vorticity distribution.","sentences":["Fluid data completion is a research problem with high potential benefit for both experimental and computational fluid dynamics.","An effective fluid data completion method reduces the required number of sensors in a fluid dynamics experiment, and allows a coarser and more adaptive mesh for a Computational Fluid Dynamics (CFD) simulation.","However, the ill-posed nature of the fluid data completion problem makes it prohibitively difficult to obtain a theoretical solution and presents high numerical uncertainty and instability for a data-driven approach (e.g., a neural network model).","To address these challenges, we leverage recent advancements in computer vision, employing the vector quantization technique to map both complete and incomplete fluid data spaces onto discrete-valued lower-dimensional representations via a two-stage learning procedure.","We demonstrated the effectiveness of our approach on Kolmogorov flow data (Reynolds number: 1000) occluded by masks of different size and arrangement.","Experimental results show that our proposed model consistently outperforms benchmark models under different occlusion settings in terms of point-wise reconstruction accuracy as well as turbulent energy spectrum and vorticity distribution."],"url":"http://arxiv.org/abs/2402.17185v1","category":"cs.LG"}
{"created":"2024-02-27 03:07:16","title":"Globally Convergent Distributed Sequential Quadratic Programming with Overlapping Decomposition and Exact Augmented Lagrangian Merit Function","abstract":"In this paper, we address the problem of solving large-scale graph-structured nonlinear programs (gsNLPs) in a scalable manner. GsNLPs are problems in which the objective and constraint functions are associated with a graph node, and they depend only on the variables of adjacent nodes. This graph-structured formulation encompasses various specific instances, such as dynamic optimization, PDE-constrained optimization, multi-stage stochastic optimization, and optimization over networks. We propose a globally convergent overlapping graph decomposition method for solving large-scale gsNLPs under the standard regularity assumptions and mild conditions on the graph topology. At each iteration step, we use an overlapping graph decomposition to compute an approximate Newton step direction using parallel computations. We then select a suitable step size and update the primal-dual iterates by performing backtracking line search with the exact augmented Lagrangian merit function. By exploiting the exponential decay of sensitivity of gsNLPs, we show that the approximate Newton direction is a descent direction of the augmented Lagrangian merit function, which leads to global convergence and fast local convergence. In particular, global convergence is achieved for sufficiently large overlaps, and the local linear convergence rate improves exponentially in terms of the overlap size. This result matches existing results for dynamic programs. We validate our theory with an elliptic PDE-constrained problem.","sentences":["In this paper, we address the problem of solving large-scale graph-structured nonlinear programs (gsNLPs) in a scalable manner.","GsNLPs are problems in which the objective and constraint functions are associated with a graph node, and they depend only on the variables of adjacent nodes.","This graph-structured formulation encompasses various specific instances, such as dynamic optimization, PDE-constrained optimization, multi-stage stochastic optimization, and optimization over networks.","We propose a globally convergent overlapping graph decomposition method for solving large-scale gsNLPs under the standard regularity assumptions and mild conditions on the graph topology.","At each iteration step, we use an overlapping graph decomposition to compute an approximate Newton step direction using parallel computations.","We then select a suitable step size and update the primal-dual iterates by performing backtracking line search with the exact augmented Lagrangian merit function.","By exploiting the exponential decay of sensitivity of gsNLPs, we show that the approximate Newton direction is a descent direction of the augmented Lagrangian merit function, which leads to global convergence and fast local convergence.","In particular, global convergence is achieved for sufficiently large overlaps, and the local linear convergence rate improves exponentially in terms of the overlap size.","This result matches existing results for dynamic programs.","We validate our theory with an elliptic PDE-constrained problem."],"url":"http://arxiv.org/abs/2402.17170v1","category":"math.OC"}
{"created":"2024-02-27 02:51:58","title":"Withdrawal Success Optimization in a Pooled Annuity Fund","abstract":"Consider a closed pooled annuity fund investing in n assets with discrete-time rebalancing. At time 0, each annuitant makes an initial contribution to the fund, committing to a predetermined schedule of withdrawals. Require annuitants to be homogeneous in the sense that their initial contributions and predetermined withdrawal schedules are identical, and their mortality distributions are identical and independent. Under the forementioned setup, the probability for a particular annuitant to complete the prescribed withdrawals until death is maximized over progressively measurable portfolio weight functions. Applications consider fund portfolios that mix two assets: the S&P Composite Index and an inflation-protected bond. The maximum probability is computed for annually rebalanced schedules consisting of an initial investment and then equal annual withdrawals until death. A considerable increase in the maximum probability is achieved by increasing the number of annuitants initially in the pool. For example, when the per-annuitant initial contribution and annual withdrawal amount are held constant, starting with 20 annuitants instead of just 1 can increase the maximum probability (measured on a scale from 0 to 1) by as much as .15.","sentences":["Consider a closed pooled annuity fund investing in n assets with discrete-time rebalancing.","At time 0, each annuitant makes an initial contribution to the fund, committing to a predetermined schedule of withdrawals.","Require annuitants to be homogeneous in the sense that their initial contributions and predetermined withdrawal schedules are identical, and their mortality distributions are identical and independent.","Under the forementioned setup, the probability for a particular annuitant to complete the prescribed withdrawals until death is maximized over progressively measurable portfolio weight functions.","Applications consider fund portfolios that mix two assets: the S&P Composite Index and an inflation-protected bond.","The maximum probability is computed for annually rebalanced schedules consisting of an initial investment and then equal annual withdrawals until death.","A considerable increase in the maximum probability is achieved by increasing the number of annuitants initially in the pool.","For example, when the per-annuitant initial contribution and annual withdrawal amount are held constant, starting with 20 annuitants instead of just 1 can increase the maximum probability (measured on a scale from 0 to 1) by as much as .15."],"url":"http://arxiv.org/abs/2402.17164v1","category":"q-fin.MF"}
{"created":"2024-02-27 02:50:34","title":"FORECASTOR -- II. Simulating Galaxy Surveys with the Cosmological Advanced Survey Telescope for Optical and UV Research","abstract":"The Cosmological Advanced Survey Telescope for Optical and UV Research (CASTOR) is a planned flagship space telescope, covering the blue-optical and UV part of the spectrum. Here we introduce the CASTOR image simulator, a Python GalSim package-based script capable of generating mock CASTOR images from an input catalogue. We generate example images from the CASTOR Wide, Deep, and Ultra-Deep surveys using simulated light-cones from the Santa Cruz Semi-Analytic Model. We make predictions for the performance of these surveys by comparing galaxies that are extracted from each image using Source Extractor to the input catalogue. We find that the Wide, Deep, and Ultra-Deep surveys will be complete to ~27, 29 and 30 mag, respectively, in the UV, u, and g filters, with the UV-split and u-split filters reaching a shallower depth. With a large area of ~2200 deg$^2$, the Wide survey will detect hundreds of millions of galaxies out to z~4, mostly with $M_\\ast \\gtrsim 10^9 M_\\odot$. The Ultra-Deep survey will probe to z~5, detecting a large fraction of $M_\\ast \\simeq 10^8 M_\\odot$ galaxies. These powerful samples will enable precision measurements of the distribution of star formation in the cosmic web, connecting the growth of stellar mass to the assembly of dark matter halos over two thirds of the history of the Universe, and other core goals of CASTOR's legacy surveys. These image simulations and the tools developed to generate them will be a vital planning tool to estimate CASTOR's performance and iterate the telescope and survey designs prior to launch.","sentences":["The Cosmological Advanced Survey Telescope for Optical and UV Research (CASTOR) is a planned flagship space telescope, covering the blue-optical and UV part of the spectrum.","Here we introduce the CASTOR image simulator, a Python GalSim package-based script capable of generating mock CASTOR images from an input catalogue.","We generate example images from the CASTOR Wide, Deep, and Ultra-Deep surveys using simulated light-cones from the Santa Cruz Semi-Analytic Model.","We make predictions for the performance of these surveys by comparing galaxies that are extracted from each image using Source Extractor to the input catalogue.","We find that the Wide, Deep, and Ultra-Deep surveys will be complete to ~27, 29 and 30 mag, respectively, in the UV, u, and g filters, with the UV-split and u-split filters reaching a shallower depth.","With a large area of ~2200 deg$^2$, the Wide survey will detect hundreds of millions of galaxies out to z~4, mostly with $M_\\ast \\gtrsim 10^9 M_\\odot$. The Ultra-Deep survey will probe to z~5, detecting a large fraction of $M_\\ast \\simeq 10^8 M_\\odot$ galaxies.","These powerful samples will enable precision measurements of the distribution of star formation in the cosmic web, connecting the growth of stellar mass to the assembly of dark matter halos over two thirds of the history of the Universe, and other core goals of CASTOR's legacy surveys.","These image simulations and the tools developed to generate them will be a vital planning tool to estimate CASTOR's performance and iterate the telescope and survey designs prior to launch."],"url":"http://arxiv.org/abs/2402.17163v1","category":"astro-ph.GA"}
{"created":"2024-02-27 02:47:41","title":"Choosing Behind the Veil: Tight Bounds for Identity-Blind Online Algorithms","abstract":"In Bayesian online settings, every element has a value that is drawn from a known underlying distribution, which we refer to as the element's identity. The elements arrive sequentially. Upon the arrival of an element, its value is revealed, and the decision maker needs to decide, immediately and irrevocably, whether to accept it or not. While most previous work has assumed that the decision maker, upon observing the element's value, also becomes aware of its identity -- namely, its distribution -- practical scenarios frequently demand that decisions be made based solely on the element's value, without considering its identity. This necessity arises either from the algorithm's ignorance of the element's identity or due to the pursuit of fairness. We call such algorithms identity-blind algorithms, and propose the identity-blindness gap as a metric to evaluate the performance loss caused by identity-blindness. This gap is defined as the maximum ratio between the expected performance of an identity-blind online algorithm and an optimal online algorithm that knows the arrival order, thus also the identities.   We study the identity-blindness gap in the paradigmatic prophet inequality problem, under the two objectives of maximizing the expected value, and maximizing the probability to obtain the highest value. For the max-expectation objective, the celebrated prophet inequality establishes a single-threshold algorithm that gives at least 1/2 of the offline optimum, thus also an identity-blindness gap of at least 1/2. We show that this bound is tight. For the max-probability objective, while the competitive ratio is tightly 1/e, we provide a deterministic single-threshold algorithm that gives an identity-blindness gap of $\\sim 0.562$ under the assumption that there are no large point masses. Moreover, we show that this bound is tight with respect to deterministic algorithms.","sentences":["In Bayesian online settings, every element has a value that is drawn from a known underlying distribution, which we refer to as the element's identity.","The elements arrive sequentially.","Upon the arrival of an element, its value is revealed, and the decision maker needs to decide, immediately and irrevocably, whether to accept it or not.","While most previous work has assumed that the decision maker, upon observing the element's value, also becomes aware of its identity -- namely, its distribution -- practical scenarios frequently demand that decisions be made based solely on the element's value, without considering its identity.","This necessity arises either from the algorithm's ignorance of the element's identity or due to the pursuit of fairness.","We call such algorithms identity-blind algorithms, and propose the identity-blindness gap as a metric to evaluate the performance loss caused by identity-blindness.","This gap is defined as the maximum ratio between the expected performance of an identity-blind online algorithm and an optimal online algorithm that knows the arrival order, thus also the identities.   ","We study the identity-blindness gap in the paradigmatic prophet inequality problem, under the two objectives of maximizing the expected value, and maximizing the probability to obtain the highest value.","For the max-expectation objective, the celebrated prophet inequality establishes a single-threshold algorithm that gives at least 1/2 of the offline optimum, thus also an identity-blindness gap of at least 1/2.","We show that this bound is tight.","For the max-probability objective, while the competitive ratio is tightly 1/e, we provide a deterministic single-threshold algorithm that gives an identity-blindness gap of $\\sim 0.562$ under the assumption that there are no large point masses.","Moreover, we show that this bound is tight with respect to deterministic algorithms."],"url":"http://arxiv.org/abs/2402.17160v1","category":"cs.GT"}
{"created":"2024-02-27 02:37:53","title":"Exact and efficient phylodynamic simulation from arbitrarily large populations","abstract":"Many biological studies involve inferring the genealogical history of a sample of individuals from a large population and interpreting the reconstructed tree. Such an ascertained tree typically represents only a small part of a comprehensive population tree and is distorted by survivorship and sampling biases. Inferring evolutionary parameters from ascertained trees requires modeling both the underlying population dynamics and the ascertainment process. A crucial component of this phylodynamic modeling involves tree simulation, which is used to benchmark probabilistic inference methods. To simulate an ascertained tree, one must first simulate the full population tree and then prune unobserved lineages. Consequently, the computational cost is determined not by the size of the final simulated tree, but by the size of the population tree in which it is embedded. In most biological scenarios, simulations of the entire population are prohibitively expensive due to computational demands placed on lineages without sampled descendants. Here, we address this challenge by proving that, for any partially ascertained process from a general multi-type birth-death-mutation-sampling (BDMS) model, there exists an equivalent pure birth process (i.e., no death) with mutation and complete sampling. The final trees generated under these processes have exactly the same distribution. Leveraging this property, we propose a highly efficient algorithm for simulating trees under a general BDMS model. Our algorithm scales linearly with the size of the final simulated tree and is independent of the population size, enabling simulations from extremely large populations beyond the reach of current methods but essential for various biological applications. We anticipate that this unprecedented speedup will significantly advance the development of novel inference methods that require extensive training data.","sentences":["Many biological studies involve inferring the genealogical history of a sample of individuals from a large population and interpreting the reconstructed tree.","Such an ascertained tree typically represents only a small part of a comprehensive population tree and is distorted by survivorship and sampling biases.","Inferring evolutionary parameters from ascertained trees requires modeling both the underlying population dynamics and the ascertainment process.","A crucial component of this phylodynamic modeling involves tree simulation, which is used to benchmark probabilistic inference methods.","To simulate an ascertained tree, one must first simulate the full population tree and then prune unobserved lineages.","Consequently, the computational cost is determined not by the size of the final simulated tree, but by the size of the population tree in which it is embedded.","In most biological scenarios, simulations of the entire population are prohibitively expensive due to computational demands placed on lineages without sampled descendants.","Here, we address this challenge by proving that, for any partially ascertained process from a general multi-type birth-death-mutation-sampling (BDMS) model, there exists an equivalent pure birth process (i.e., no death) with mutation and complete sampling.","The final trees generated under these processes have exactly the same distribution.","Leveraging this property, we propose a highly efficient algorithm for simulating trees under a general BDMS model.","Our algorithm scales linearly with the size of the final simulated tree and is independent of the population size, enabling simulations from extremely large populations beyond the reach of current methods but essential for various biological applications.","We anticipate that this unprecedented speedup will significantly advance the development of novel inference methods that require extensive training data."],"url":"http://arxiv.org/abs/2402.17153v1","category":"q-bio.PE"}
{"created":"2024-02-27 02:23:58","title":"On the significance of radiative corrections on measurements of the EMC effect","abstract":"Analyzing global data on the EMC effect, which denotes differences in parton distribution functions in nuclei compared to unbound nucleons, reveals tensions. Precise measurements at Jefferson Lab, studying both x and A dependence, show systematic discrepancies among experiments, making the extraction of the A dependence of the EMC effect sensitive to the selection of datasets. By comparing various methods and assumptions used to calculate radiative corrections, we have identified differences that, while not large, significantly impact the EMC ratios and show that using a consistent radiative correction procedure resolves this discrepancy, leading to a more coherent global picture, and allowing for a more robust extraction of the EMC effect for infinite nuclear matter.","sentences":["Analyzing global data on the EMC effect, which denotes differences in parton distribution functions in nuclei compared to unbound nucleons, reveals tensions.","Precise measurements at Jefferson Lab, studying both x and A dependence, show systematic discrepancies among experiments, making the extraction of the A dependence of the EMC effect sensitive to the selection of datasets.","By comparing various methods and assumptions used to calculate radiative corrections, we have identified differences that, while not large, significantly impact the EMC ratios and show that using a consistent radiative correction procedure resolves this discrepancy, leading to a more coherent global picture, and allowing for a more robust extraction of the EMC effect for infinite nuclear matter."],"url":"http://arxiv.org/abs/2402.17147v1","category":"nucl-ex"}
{"created":"2024-02-27 02:01:24","title":"Integrated Interpolation and Block-term Tensor Decomposition for Spectrum Map Construction","abstract":"This paper addresses the challenge of reconstructing a 3D power spectrum map from sparse, scattered, and incomplete spectrum measurements. It proposes an integrated approach combining interpolation and block-term tensor decomposition (BTD). This approach leverages an interpolation model with the BTD structure to exploit the spatial correlation of power spectrum maps. Additionally, nuclear norm regularization is incorporated to effectively capture the low-rank characteristics. To implement this approach, a novel algorithm that combines alternating regression with singular value thresholding is developed. Analytical justification for the enhancement provided by the BTD structure in interpolating power spectrum maps is provided, yielding several important theoretical insights. The analysis explores the impact of the spectrum on the error in the proposed method and compares it to conventional local polynomial interpolation. Extensive numerical results demonstrate that the proposed method outperforms state-of-the-art methods in terms of signal source separation and power spectrum map construction, and remains stable under off-grid measurements and inhomogeneous measurement topologies.","sentences":["This paper addresses the challenge of reconstructing a 3D power spectrum map from sparse, scattered, and incomplete spectrum measurements.","It proposes an integrated approach combining interpolation and block-term tensor decomposition (BTD).","This approach leverages an interpolation model with the BTD structure to exploit the spatial correlation of power spectrum maps.","Additionally, nuclear norm regularization is incorporated to effectively capture the low-rank characteristics.","To implement this approach, a novel algorithm that combines alternating regression with singular value thresholding is developed.","Analytical justification for the enhancement provided by the BTD structure in interpolating power spectrum maps is provided, yielding several important theoretical insights.","The analysis explores the impact of the spectrum on the error in the proposed method and compares it to conventional local polynomial interpolation.","Extensive numerical results demonstrate that the proposed method outperforms state-of-the-art methods in terms of signal source separation and power spectrum map construction, and remains stable under off-grid measurements and inhomogeneous measurement topologies."],"url":"http://arxiv.org/abs/2402.17138v1","category":"eess.SP"}
{"created":"2024-02-27 01:29:45","title":"Data-driven discovery of interpretable Lagrangian of stochastically excited dynamical systems","abstract":"Exploring the intersection of deterministic and stochastic dynamics, this paper delves into Lagrangian discovery for conservative and non-conservative systems under stochastic excitation. Traditional Lagrangian frameworks, adept at capturing deterministic behavior, are extended to incorporate stochastic excitation. The study critically evaluates recent computational methodologies for learning Lagrangians from observed data, highlighting the limitations in interpretability and the exclusion of stochastic excitation. To address these gaps, an automated data-driven framework is proposed for the simultaneous yet uncoupled discovery of Lagrange densities and the volatility function of stochastic excitation by leveraging the sparse regression. This novel framework offers several advantages over existing approaches. Firstly, it provides an interpretable description of the underlying Lagrange density, allowing for a deeper understanding of system dynamics under stochastic excitations. Secondly, it identifies the interpretable form of the generalized stochastic force, addressing the limitations of existing deterministic approaches. Additionally, the framework demonstrates robustness and versatility through numerical case studies encompassing both stochastic differential equations (SDEs) and stochastic partial differential equations (SPDEs), with results showing almost exact approximations to true system behavior and minimal relative error in derived equations of motion.","sentences":["Exploring the intersection of deterministic and stochastic dynamics, this paper delves into Lagrangian discovery for conservative and non-conservative systems under stochastic excitation.","Traditional Lagrangian frameworks, adept at capturing deterministic behavior, are extended to incorporate stochastic excitation.","The study critically evaluates recent computational methodologies for learning Lagrangians from observed data, highlighting the limitations in interpretability and the exclusion of stochastic excitation.","To address these gaps, an automated data-driven framework is proposed for the simultaneous yet uncoupled discovery of Lagrange densities and the volatility function of stochastic excitation by leveraging the sparse regression.","This novel framework offers several advantages over existing approaches.","Firstly, it provides an interpretable description of the underlying Lagrange density, allowing for a deeper understanding of system dynamics under stochastic excitations.","Secondly, it identifies the interpretable form of the generalized stochastic force, addressing the limitations of existing deterministic approaches.","Additionally, the framework demonstrates robustness and versatility through numerical case studies encompassing both stochastic differential equations (SDEs) and stochastic partial differential equations (SPDEs), with results showing almost exact approximations to true system behavior and minimal relative error in derived equations of motion."],"url":"http://arxiv.org/abs/2402.17122v1","category":"math.DS"}
{"created":"2024-02-27 01:25:52","title":"Creating Suspenseful Stories: Iterative Planning with Large Language Models","abstract":"Automated story generation has been one of the long-standing challenges in NLP. Among all dimensions of stories, suspense is very common in human-written stories but relatively under-explored in AI-generated stories. While recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation. We propose a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology. This theory-grounded method works in a fully zero-shot manner and does not rely on any supervised story corpora. To the best of our knowledge, this paper is the first attempt at suspenseful story generation with LLMs. Extensive human evaluations of the generated suspenseful stories demonstrate the effectiveness of our method.","sentences":["Automated story generation has been one of the long-standing challenges in NLP.","Among all dimensions of stories, suspense is very common in human-written stories but relatively under-explored in AI-generated stories.","While recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation.","We propose a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology.","This theory-grounded method works in a fully zero-shot manner and does not rely on any supervised story corpora.","To the best of our knowledge, this paper is the first attempt at suspenseful story generation with LLMs.","Extensive human evaluations of the generated suspenseful stories demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2402.17119v1","category":"cs.CL"}
{"created":"2024-02-27 01:25:26","title":"Deep Reinforcement Learning (DRL)-based Methods for Serverless Stream Processing Engines: A Vision, Architectural Elements, and Future Directions","abstract":"Streaming applications are becoming widespread across an extensive range of business domains as an increasing number of sources continuously produce data that need to be processed and analysed in real time. Modern businesses are aggressively using streaming data to generate valuable knowledge that can be used to automate processes, help decision-making, optimize resource usage, and ultimately generate revenue for the organization. Despite their increased adoption and tangible benefits, support for the automated deployment and management of streaming applications is yet to emerge. Although a plethora of stream management systems have flooded the open source community in recent years, all of the existing frameworks demand a considerably challenging and lengthy effort from human operators to manually and continuously tune their configuration and deployment environment in order to reach and maintain the desired performance goals. To address these challenges, this article proposes a vision for creating Deep Reinforcement Learning (DRL)-based methods for transforming stream processing engines into self-managed serverless solutions. This will lead to an increase in productivity as engineers can focus on the actual development process, an increase in application performance potentially leading to reduced response times and more accurate and meaningful results, and a considerable decrease in operational costs for organizations.","sentences":["Streaming applications are becoming widespread across an extensive range of business domains as an increasing number of sources continuously produce data that need to be processed and analysed in real time.","Modern businesses are aggressively using streaming data to generate valuable knowledge that can be used to automate processes, help decision-making, optimize resource usage, and ultimately generate revenue for the organization.","Despite their increased adoption and tangible benefits, support for the automated deployment and management of streaming applications is yet to emerge.","Although a plethora of stream management systems have flooded the open source community in recent years, all of the existing frameworks demand a considerably challenging and lengthy effort from human operators to manually and continuously tune their configuration and deployment environment in order to reach and maintain the desired performance goals.","To address these challenges, this article proposes a vision for creating Deep Reinforcement Learning (DRL)-based methods for transforming stream processing engines into self-managed serverless solutions.","This will lead to an increase in productivity as engineers can focus on the actual development process, an increase in application performance potentially leading to reduced response times and more accurate and meaningful results, and a considerable decrease in operational costs for organizations."],"url":"http://arxiv.org/abs/2402.17117v1","category":"cs.DC"}
{"created":"2024-02-27 01:22:08","title":"CharNeRF: 3D Character Generation from Concept Art","abstract":"3D modeling holds significant importance in the realms of AR/VR and gaming, allowing for both artistic creativity and practical applications. However, the process is often time-consuming and demands a high level of skill. In this paper, we present a novel approach to create volumetric representations of 3D characters from consistent turnaround concept art, which serves as the standard input in the 3D modeling industry. While Neural Radiance Field (NeRF) has been a game-changer in image-based 3D reconstruction, to the best of our knowledge, there is no known research that optimizes the pipeline for concept art. To harness the potential of concept art, with its defined body poses and specific view angles, we propose encoding it as priors for our model. We train the network to make use of these priors for various 3D points through a learnable view-direction-attended multi-head self-attention layer. Additionally, we demonstrate that a combination of ray sampling and surface sampling enhances the inference capabilities of our network. Our model is able to generate high-quality 360-degree views of characters. Subsequently, we provide a simple guideline to better leverage our model to extract the 3D mesh. It is important to note that our model's inferencing capabilities are influenced by the training data's characteristics, primarily focusing on characters with a single head, two arms, and two legs. Nevertheless, our methodology remains versatile and adaptable to concept art from diverse subject matters, without imposing any specific assumptions on the data.","sentences":["3D modeling holds significant importance in the realms of AR/VR and gaming, allowing for both artistic creativity and practical applications.","However, the process is often time-consuming and demands a high level of skill.","In this paper, we present a novel approach to create volumetric representations of 3D characters from consistent turnaround concept art, which serves as the standard input in the 3D modeling industry.","While Neural Radiance Field (NeRF) has been a game-changer in image-based 3D reconstruction, to the best of our knowledge, there is no known research that optimizes the pipeline for concept art.","To harness the potential of concept art, with its defined body poses and specific view angles, we propose encoding it as priors for our model.","We train the network to make use of these priors for various 3D points through a learnable view-direction-attended multi-head self-attention layer.","Additionally, we demonstrate that a combination of ray sampling and surface sampling enhances the inference capabilities of our network.","Our model is able to generate high-quality 360-degree views of characters.","Subsequently, we provide a simple guideline to better leverage our model to extract the 3D mesh.","It is important to note that our model's inferencing capabilities are influenced by the training data's characteristics, primarily focusing on characters with a single head, two arms, and two legs.","Nevertheless, our methodology remains versatile and adaptable to concept art from diverse subject matters, without imposing any specific assumptions on the data."],"url":"http://arxiv.org/abs/2402.17115v1","category":"cs.CV"}
{"created":"2024-02-27 01:01:59","title":"Repeated Contracting with Multiple Non-Myopic Agents: Policy Regret and Limited Liability","abstract":"We study a repeated contracting setting in which a Principal adaptively chooses amongst $k$ Agents at each of $T$ rounds. The Agents are non-myopic, and so a mechanism for the Principal induces a $T$-round extensive form game amongst the Agents. We give several results aimed at understanding an under-explored aspect of contract theory -- the game induced when choosing an Agent to contract with. First, we show that this game admits a pure-strategy \\emph{non-responsive} equilibrium amongst the Agents -- informally an equilibrium in which the Agent's actions depend on the history of realized states of nature, but not on the history of each other's actions, and so avoids the complexities of collusion and threats. Next, we show that if the Principal selects Agents using a \\emph{monotone} bandit algorithm, then for any concave contract, in any such equilibrium, the Principal obtains no regret to contracting with the best Agent in hindsight -- not just given their realized actions, but also to the counterfactual world in which they had offered a guaranteed $T$-round contract to the best Agent in hindsight, which would have induced a different sequence of actions. Finally, we show that if the Principal selects Agents using a monotone bandit algorithm which guarantees no swap-regret, then the Principal can additionally offer only limited liability contracts (in which the Agent never needs to pay the Principal) while getting no-regret to the counterfactual world in which she offered a linear contract to the best Agent in hindsight -- despite the fact that linear contracts are not limited liability. We instantiate this theorem by demonstrating the existence of a monotone no swap-regret bandit algorithm, which to our knowledge has not previously appeared in the literature.","sentences":["We study a repeated contracting setting in which a Principal adaptively chooses amongst $k$ Agents at each of $T$ rounds.","The Agents are non-myopic, and so a mechanism for the Principal induces a $T$-round extensive form game amongst the Agents.","We give several results aimed at understanding an under-explored aspect of contract theory -- the game induced when choosing an Agent to contract with.","First, we show that this game admits a pure-strategy \\emph{non-responsive} equilibrium amongst the Agents -- informally an equilibrium in which the Agent's actions depend on the history of realized states of nature, but not on the history of each other's actions, and so avoids the complexities of collusion and threats.","Next, we show that if the Principal selects Agents using a \\emph{monotone} bandit algorithm, then for any concave contract, in any such equilibrium, the Principal obtains no regret to contracting with the best Agent in hindsight -- not just given their realized actions, but also to the counterfactual world in which they had offered a guaranteed $T$-round contract to the best Agent in hindsight, which would have induced a different sequence of actions.","Finally, we show that if the Principal selects Agents using a monotone bandit algorithm which guarantees no swap-regret, then the Principal can additionally offer only limited liability contracts (in which the Agent never needs to pay the Principal) while getting no-regret to the counterfactual world in which she offered a linear contract to the best Agent in hindsight -- despite the fact that linear contracts are not limited liability.","We instantiate this theorem by demonstrating the existence of a monotone no swap-regret bandit algorithm, which to our knowledge has not previously appeared in the literature."],"url":"http://arxiv.org/abs/2402.17108v1","category":"cs.GT"}
{"created":"2024-02-27 00:59:32","title":"Dataset Fairness: Achievable Fairness on Your Data With Utility Guarantees","abstract":"In machine learning fairness, training models which minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy trade-off. The severity of this trade-off fundamentally depends on dataset characteristics such as dataset imbalances or biases. Therefore using a uniform fairness requirement across datasets remains questionable and can often lead to models with substantially low utility. To address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve. Moreover, we quantify the uncertainty in our approximation by introducing confidence intervals around this curve, offering a statistically grounded perspective on the acceptable range of fairness violations for any given accuracy threshold. Our empirical evaluation spanning tabular, image and language datasets underscores that our approach provides practitioners with a principled framework for dataset-specific fairness decisions across various data modalities.","sentences":["In machine learning fairness, training models which minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy trade-off.","The severity of this trade-off fundamentally depends on dataset characteristics such as dataset imbalances or biases.","Therefore using a uniform fairness requirement across datasets remains questionable and can often lead to models with substantially low utility.","To address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees.","By utilizing the You-Only-Train-Once (YOTO) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve.","Moreover, we quantify the uncertainty in our approximation by introducing confidence intervals around this curve, offering a statistically grounded perspective on the acceptable range of fairness violations for any given accuracy threshold.","Our empirical evaluation spanning tabular, image and language datasets underscores that our approach provides practitioners with a principled framework for dataset-specific fairness decisions across various data modalities."],"url":"http://arxiv.org/abs/2402.17106v1","category":"stat.ML"}
{"created":"2024-02-27 00:41:00","title":"Adversarial Perturbations of Physical Signals","abstract":"We investigate the vulnerability of computer-vision-based signal classifiers to adversarial perturbations of their inputs, where the signals and perturbations are subject to physical constraints. We consider a scenario in which a source and interferer emit signals that propagate as waves to a detector, which attempts to classify the source by analyzing the spectrogram of the signal it receives using a pre-trained neural network. By solving PDE-constrained optimization problems, we construct interfering signals that cause the detector to misclassify the source even though the perturbations to the spectrogram of the received signal are nearly imperceptible. Though such problems can have millions of decision variables, we introduce methods to solve them efficiently. Our experiments demonstrate that one can compute effective and physically realizable adversarial perturbations for a variety of machine learning models under various physical conditions.","sentences":["We investigate the vulnerability of computer-vision-based signal classifiers to adversarial perturbations of their inputs, where the signals and perturbations are subject to physical constraints.","We consider a scenario in which a source and interferer emit signals that propagate as waves to a detector, which attempts to classify the source by analyzing the spectrogram of the signal it receives using a pre-trained neural network.","By solving PDE-constrained optimization problems, we construct interfering signals that cause the detector to misclassify the source even though the perturbations to the spectrogram of the received signal are nearly imperceptible.","Though such problems can have millions of decision variables, we introduce methods to solve them efficiently.","Our experiments demonstrate that one can compute effective and physically realizable adversarial perturbations for a variety of machine learning models under various physical conditions."],"url":"http://arxiv.org/abs/2402.17104v1","category":"cs.LG"}
{"created":"2024-02-26 23:50:36","title":"A Proof of the Central Limit Theorem Using the $2$-Wasserstein Metric","abstract":"We prove the Lindeberg--Feller central limit theorem without using characteristic functions or Taylor expansions, but instead by measuring how far a distribution is from the standard normal distribution according to the $2$-Wasserstein metric. This falls under the category of renormalization group methods. The facts we need about the metric are explained and proved in detail. We illustrate the idea on a classical version of the central limit theorem before going into the main proof.","sentences":["We prove the Lindeberg--Feller central limit theorem without using characteristic functions or Taylor expansions, but instead by measuring how far a distribution is from the standard normal distribution according to the $2$-Wasserstein metric.","This falls under the category of renormalization group methods.","The facts we need about the metric are explained and proved in detail.","We illustrate the idea on a classical version of the central limit theorem before going into the main proof."],"url":"http://arxiv.org/abs/2402.17085v1","category":"math.PR"}
{"created":"2024-02-26 23:15:53","title":"Continuous family of surfaces translating by powers of Gauss curvature","abstract":"This paper shows the existence of convex translating surfaces under the flow by the $\\alpha$-th power of Gauss curvature for the sub-affine-critical regime $ 0 < \\alpha < 1/4$. The key aspect of our study is that our ansatz at infinity is the graph of homogeneous functions whose level sets are closed curves shrinking under the flow by the $\\frac{\\alpha}{1-\\alpha}$-th power of curvature. For each ansatz, we construct a family of translating surfaces generated by the Jacobi fields with effective growth rates. Moreover, the construction shows quantitative estimate on the rate of convergence between different translators to each other, which is required to show the continuity of the family. As a result, the family is regarded as a topological manifold. The construction in this paper will become the ground of forthcoming research, where we aim to prove that every translating surface must correspond to one of the solutions obtained herein, classifying translating surfaces and identifying the topology of the moduli space.","sentences":["This paper shows the existence of convex translating surfaces under the flow by the $\\alpha$-th power of Gauss curvature for the sub-affine-critical regime $ 0 <","\\alpha < 1/4$. The key aspect of our study is that our ansatz at infinity is the graph of homogeneous functions whose level sets are closed curves shrinking under the flow by the $\\frac{\\alpha}{1-\\alpha}$-th power of curvature.","For each ansatz, we construct a family of translating surfaces generated by the Jacobi fields with effective growth rates.","Moreover, the construction shows quantitative estimate on the rate of convergence between different translators to each other, which is required to show the continuity of the family.","As a result, the family is regarded as a topological manifold.","The construction in this paper will become the ground of forthcoming research, where we aim to prove that every translating surface must correspond to one of the solutions obtained herein, classifying translating surfaces and identifying the topology of the moduli space."],"url":"http://arxiv.org/abs/2402.17075v1","category":"math.DG"}
{"created":"2024-02-26 23:13:24","title":"Path Planning for a Cooperative Navigation Aid Vehicle to Assist Multiple Agents Intermittently","abstract":"This paper considers the problem of planning a path for a single underwater cooperative navigation aid (CNA) vehicle to intermittently aid a set of N agents to minimize average navigation uncertainty. Both the CNA and agents are modeled as constant-velocity vehicles. The agents traverse along known nominal trajectories and the CNA plans a path to sequentially intercept them. Navigation aiding is modeled by a scalar discrete time Kalman filter. During path planning, the CNA considers surfacing to reduce its own navigation uncertainty. A greedy planning algorithm is proposed that uses a heuristic based on an optimal time-to-aid, overall navigation uncertainty reduction, and transit time, to assign agents to the CNA. The approach is compared to an optimal (exhaustive enumeration) algorithm through a Monte Carlo experiment with randomized agent nominal trajectories and initial navigation uncertainty.","sentences":["This paper considers the problem of planning a path for a single underwater cooperative navigation aid (CNA) vehicle to intermittently aid a set of N agents to minimize average navigation uncertainty.","Both the CNA and agents are modeled as constant-velocity vehicles.","The agents traverse along known nominal trajectories and the CNA plans a path to sequentially intercept them.","Navigation aiding is modeled by a scalar discrete time Kalman filter.","During path planning, the CNA considers surfacing to reduce its own navigation uncertainty.","A greedy planning algorithm is proposed that uses a heuristic based on an optimal time-to-aid, overall navigation uncertainty reduction, and transit time, to assign agents to the CNA.","The approach is compared to an optimal (exhaustive enumeration) algorithm through a Monte Carlo experiment with randomized agent nominal trajectories and initial navigation uncertainty."],"url":"http://arxiv.org/abs/2402.17071v1","category":"eess.SY"}
{"created":"2024-02-26 23:05:02","title":"On Independent Samples Along the Langevin Diffusion and the Unadjusted Langevin Algorithm","abstract":"We study the rate at which the initial and current random variables become independent along a Markov chain, focusing on the Langevin diffusion in continuous time and the Unadjusted Langevin Algorithm (ULA) in discrete time. We measure the dependence between random variables via their mutual information. For the Langevin diffusion, we show the mutual information converges to $0$ exponentially fast when the target is strongly log-concave, and at a polynomial rate when the target is weakly log-concave. These rates are analogous to the mixing time of the Langevin diffusion under similar assumptions. For the ULA, we show the mutual information converges to $0$ exponentially fast when the target is strongly log-concave and smooth. We prove our results by developing the mutual version of the mixing time analyses of these Markov chains. We also provide alternative proofs based on strong data processing inequalities for the Langevin diffusion and the ULA, and by showing regularity results for these processes in mutual information.","sentences":["We study the rate at which the initial and current random variables become independent along a Markov chain, focusing on the Langevin diffusion in continuous time and the Unadjusted Langevin Algorithm (ULA) in discrete time.","We measure the dependence between random variables via their mutual information.","For the Langevin diffusion, we show the mutual information converges to $0$ exponentially fast when the target is strongly log-concave, and at a polynomial rate when the target is weakly log-concave.","These rates are analogous to the mixing time of the Langevin diffusion under similar assumptions.","For the ULA, we show the mutual information converges to $0$ exponentially fast when the target is strongly log-concave and smooth.","We prove our results by developing the mutual version of the mixing time analyses of these Markov chains.","We also provide alternative proofs based on strong data processing inequalities for the Langevin diffusion and the ULA, and by showing regularity results for these processes in mutual information."],"url":"http://arxiv.org/abs/2402.17067v1","category":"math.ST"}
{"created":"2024-02-26 22:41:08","title":"An Analysis of Capacity-Distortion Trade-Offs in Memoryless ISAC Systems","abstract":"This manuscript investigates the information-theoretic limits of integrated sensing and communications (ISAC), aiming for simultaneous reliable communication and precise channel state estimation. We model such a system with a state-dependent discrete memoryless channel (SD-DMC) with present or absent channel feedback and generalized side information at the transmitter and the receiver, where the joint task of message decoding and state estimation is performed at the receiver. The relationship between the achievable communication rate and estimation error, the capacity-distortion (C-D) trade-off, is characterized across different causality levels of the side information. This framework is shown to be capable of modeling various practical scenarios by assigning the side information with different meanings, including monostatic and bistatic radar systems. The analysis is then extended to the two-user degraded broadcast channel, and we derive an achievable C-D region that is tight under certain conditions. To solve the optimization problem arising in the computation of C-D functions/regions, we propose a proximal block coordinate descent (BCD) method, prove its convergence to a stationary point, and derive a stopping criterion. Finally, several representative examples are studied to demonstrate the versatility of our framework and the effectiveness of the proposed algorithm.","sentences":["This manuscript investigates the information-theoretic limits of integrated sensing and communications (ISAC), aiming for simultaneous reliable communication and precise channel state estimation.","We model such a system with a state-dependent discrete memoryless channel (SD-DMC) with present or absent channel feedback and generalized side information at the transmitter and the receiver, where the joint task of message decoding and state estimation is performed at the receiver.","The relationship between the achievable communication rate and estimation error, the capacity-distortion (C-D) trade-off, is characterized across different causality levels of the side information.","This framework is shown to be capable of modeling various practical scenarios by assigning the side information with different meanings, including monostatic and bistatic radar systems.","The analysis is then extended to the two-user degraded broadcast channel, and we derive an achievable C-D region that is tight under certain conditions.","To solve the optimization problem arising in the computation of C-D functions/regions, we propose a proximal block coordinate descent (BCD) method, prove its convergence to a stationary point, and derive a stopping criterion.","Finally, several representative examples are studied to demonstrate the versatility of our framework and the effectiveness of the proposed algorithm."],"url":"http://arxiv.org/abs/2402.17058v1","category":"cs.IT"}
{"created":"2024-02-26 22:37:03","title":"Feasibility analysis of a proposed test of quantum gravity via novel optical magnetometry in xenon","abstract":"We present an analysis of the sensitivity limits of a proposed experimental search for quantum gravity, using a novel approach based on optical magnetometry in the noble gas isotope $^{129}$Xe. The analysis relies on a general uncertainty principle model that is consistent with most formulations of quantum gravity theory, where the canonical uncertainty relations are modified by a leading-order correction term that is linear in momentum. In turn, this correction modifies the magnetic moment of the spin-polarized $^{129}$Xe atoms that are immersed in a magnetic field in the proposed experiment, which results in a velocity-dependent variation of their Larmour frequency, that is detected via two-photon laser spectroscopy. The thermal distribution of atomic velocities, in conjunction with the Doppler effect, is used to scan the interrogating laser over different atomic velocities, and search for a corresponding variation in their Larmor frequencies. We show that the existing bounds on the leading-order quantum gravity correction can be improved by $10^{7}$ with existing technology, where another factor of $10^{2}$ is possible with near-future technical capabilities.","sentences":["We present an analysis of the sensitivity limits of a proposed experimental search for quantum gravity, using a novel approach based on optical magnetometry in the noble gas isotope $^{129}$Xe.","The analysis relies on a general uncertainty principle model that is consistent with most formulations of quantum gravity theory, where the canonical uncertainty relations are modified by a leading-order correction term that is linear in momentum.","In turn, this correction modifies the magnetic moment of the spin-polarized $^{129}$Xe atoms that are immersed in a magnetic field in the proposed experiment, which results in a velocity-dependent variation of their Larmour frequency, that is detected via two-photon laser spectroscopy.","The thermal distribution of atomic velocities, in conjunction with the Doppler effect, is used to scan the interrogating laser over different atomic velocities, and search for a corresponding variation in their Larmor frequencies.","We show that the existing bounds on the leading-order quantum gravity correction can be improved by $10^{7}$ with existing technology, where another factor of $10^{2}$ is possible with near-future technical capabilities."],"url":"http://arxiv.org/abs/2402.17057v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-26 22:12:53","title":"Invariant measures for reducible generalized Bratteli diagrams","abstract":"In 2010, Bezuglyi, Kwiatkowski, Medynets and Solomyak [Ergodic Theory Dynam. Systems 30 (2010), no.4, 973-1007] found a complete description of the set of probability ergodic tail invariant measures on the path space of a standard (classical) stationary reducible Bratteli diagram. It was shown that every distinguished eigenvalue for the incidence matrix determines a probability ergodic invariant measure. In this paper, we show that this result does not hold for stationary reducible generalized Bratteli diagrams. We consider classes of stationary and non-stationary reducible generalized Bratteli diagrams with infinitely many simple standard subdiagrams, in particular, with infinitely many odometers as subdiagrams. We characterize the sets of all probability ergodic invariant measures for such diagrams and study partial orders under which the diagrams can support a Vershik homeomorphism.","sentences":["In 2010, Bezuglyi, Kwiatkowski, Medynets and Solomyak","[Ergodic Theory Dynam.","Systems 30 (2010), no.4, 973-1007] found a complete description of the set of probability ergodic tail invariant measures on the path space of a standard (classical) stationary reducible Bratteli diagram.","It was shown that every distinguished eigenvalue for the incidence matrix determines a probability ergodic invariant measure.","In this paper, we show that this result does not hold for stationary reducible generalized Bratteli diagrams.","We consider classes of stationary and non-stationary reducible generalized Bratteli diagrams with infinitely many simple standard subdiagrams, in particular, with infinitely many odometers as subdiagrams.","We characterize the sets of all probability ergodic invariant measures for such diagrams and study partial orders under which the diagrams can support a Vershik homeomorphism."],"url":"http://arxiv.org/abs/2402.17046v1","category":"math.DS"}
{"created":"2024-02-26 21:45:56","title":"Distinct Optical Excitation Mechanisms of a Coherent Magnon in a van der Waals Antiferromagnet","abstract":"The control of antiferromagnets with ultrashort optical pulses has emerged as a prominent field of research. Tailored laser excitation can launch coherent spin waves at terahertz frequencies, yet a comprehensive description of their generation mechanisms is still lacking despite extensive efforts. Using terahertz emission spectroscopy, we investigate the generation of a coherent magnon mode in the van der Waals antiferromagnet NiPS$_3$ under a range of photoexcitation conditions. By tuning the pump photon energy from transparency to resonant with a $d$-$d$ transition, we reveal a striking change in the coherent magnon's dependence on the pump polarization, indicating two distinct excitation mechanisms. Our findings provide a strategy for the manipulation of magnetic modes via photoexcitation around sub-gap electronic states.","sentences":["The control of antiferromagnets with ultrashort optical pulses has emerged as a prominent field of research.","Tailored laser excitation can launch coherent spin waves at terahertz frequencies, yet a comprehensive description of their generation mechanisms is still lacking despite extensive efforts.","Using terahertz emission spectroscopy, we investigate the generation of a coherent magnon mode in the van der Waals antiferromagnet NiPS$_3$ under a range of photoexcitation conditions.","By tuning the pump photon energy from transparency to resonant with a $d$-$d$ transition, we reveal a striking change in the coherent magnon's dependence on the pump polarization, indicating two distinct excitation mechanisms.","Our findings provide a strategy for the manipulation of magnetic modes via photoexcitation around sub-gap electronic states."],"url":"http://arxiv.org/abs/2402.17041v1","category":"cond-mat.str-el"}
{"created":"2024-02-26 21:44:56","title":"Robust Radiotherapy Planning with Spatially Based Uncertainty Sets","abstract":"Radiotherapy treatment planning is a challenging large-scale optimization problem plagued by uncertainty. Following the robust optimization methodology, we propose a novel, spatially based uncertainty set for robust modeling of radiotherapy planning, producing solutions that are immune to unexpected changes in biological conditions. Our proposed uncertainty set realistically captures biological radiosensitivity patterns that are observed using recent advances in imaging, while its parameters can be personalized for individual patients. We exploit the structure of this set to devise a compact reformulation of the robust model. We develop a row-generation scheme to solve real, large-scale instances of the robust model. This method is then extended to a relaxation-based scheme for enforcing challenging, yet clinically important, dose-volume cardinality constraints. The computational performance of our algorithms, as well as the quality and robustness of the computed treatment plans, are demonstrated on simulated and real imaging data. Based on accepted performance measures, such as minimal target dose and homogeneity, these examples demonstrate that the spatially robust model achieves almost the same performance as the nominal model in the nominal scenario, and otherwise, the spatial model outperforms both the nominal and the box-uncertainty models.","sentences":["Radiotherapy treatment planning is a challenging large-scale optimization problem plagued by uncertainty.","Following the robust optimization methodology, we propose a novel, spatially based uncertainty set for robust modeling of radiotherapy planning, producing solutions that are immune to unexpected changes in biological conditions.","Our proposed uncertainty set realistically captures biological radiosensitivity patterns that are observed using recent advances in imaging, while its parameters can be personalized for individual patients.","We exploit the structure of this set to devise a compact reformulation of the robust model.","We develop a row-generation scheme to solve real, large-scale instances of the robust model.","This method is then extended to a relaxation-based scheme for enforcing challenging, yet clinically important, dose-volume cardinality constraints.","The computational performance of our algorithms, as well as the quality and robustness of the computed treatment plans, are demonstrated on simulated and real imaging data.","Based on accepted performance measures, such as minimal target dose and homogeneity, these examples demonstrate that the spatially robust model achieves almost the same performance as the nominal model in the nominal scenario, and otherwise, the spatial model outperforms both the nominal and the box-uncertainty models."],"url":"http://arxiv.org/abs/2402.17040v1","category":"math.OC"}
{"created":"2024-02-26 21:35:33","title":"Iterated INLA for State and Parameter Estimation in Nonlinear Dynamical Systems","abstract":"Data assimilation (DA) methods use priors arising from differential equations to robustly interpolate and extrapolate data. Popular techniques such as ensemble methods that handle high-dimensional, nonlinear PDE priors focus mostly on state estimation, however can have difficulty learning the parameters accurately. On the other hand, machine learning based approaches can naturally learn the state and parameters, but their applicability can be limited, or produce uncertainties that are hard to interpret. Inspired by the Integrated Nested Laplace Approximation (INLA) method in spatial statistics, we propose an alternative approach to DA based on iteratively linearising the dynamical model. This produces a Gaussian Markov random field at each iteration, enabling one to use INLA to infer the state and parameters. Our approach can be used for arbitrary nonlinear systems, while retaining interpretability, and is furthermore demonstrated to outperform existing methods on the DA task. By providing a more nuanced approach to handling nonlinear PDE priors, our methodology offers improved accuracy and robustness in predictions, especially where data sparsity is prevalent.","sentences":["Data assimilation (DA) methods use priors arising from differential equations to robustly interpolate and extrapolate data.","Popular techniques such as ensemble methods that handle high-dimensional, nonlinear PDE priors focus mostly on state estimation, however can have difficulty learning the parameters accurately.","On the other hand, machine learning based approaches can naturally learn the state and parameters, but their applicability can be limited, or produce uncertainties that are hard to interpret.","Inspired by the Integrated Nested Laplace Approximation (INLA) method in spatial statistics, we propose an alternative approach to DA based on iteratively linearising the dynamical model.","This produces a Gaussian Markov random field at each iteration, enabling one to use INLA to infer the state and parameters.","Our approach can be used for arbitrary nonlinear systems, while retaining interpretability, and is furthermore demonstrated to outperform existing methods on the DA task.","By providing a more nuanced approach to handling nonlinear PDE priors, our methodology offers improved accuracy and robustness in predictions, especially where data sparsity is prevalent."],"url":"http://arxiv.org/abs/2402.17036v1","category":"stat.ML"}
{"created":"2024-02-26 20:59:34","title":"Convergence of the open WASEP stationary measure without Liggett's condition","abstract":"We demonstrate that it is not necessary to assume Liggett's condition in order to obtain convergence of the open ASEP stationary measures to the open KPZ stationary measure. This is equivalent to demonstrating that, under weak asymmetry scaling and appropriate scaling of time and space, the four-parameter Askey-Wilson process converges to a two-parameter continuous dual Hahn process. We conjecture that the convergence of the open ASEP height function process to solutions to the open KPZ equation will hold for a wider range of ASEP parameters than those permitted by Liggett's condition.","sentences":["We demonstrate that it is not necessary to assume Liggett's condition in order to obtain convergence of the open ASEP stationary measures to the open KPZ stationary measure.","This is equivalent to demonstrating that, under weak asymmetry scaling and appropriate scaling of time and space, the four-parameter Askey-Wilson process converges to a two-parameter continuous dual Hahn process.","We conjecture that the convergence of the open ASEP height function process to solutions to the open KPZ equation will hold for a wider range of ASEP parameters than those permitted by Liggett's condition."],"url":"http://arxiv.org/abs/2402.17021v1","category":"math.PR"}
{"created":"2024-02-26 20:56:06","title":"Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling","abstract":"Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 295 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop method to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through an RCT experiment with legal novices on 10 samples from the dataset. We find that LLM-generated stories enhance comprehension of legal concepts and interest in law among non-native speakers compared to only definitions. Moreover, stories consistently help participants relate legal concepts to their lives. Finally, we find that learning with stories shows a higher retention rate for non-native speakers in the follow-up assessment. Our work has strong implications for using LLMs in promoting teaching and learning in the legal field and beyond.","sentences":["Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy.","However, legal documents are often challenging to understand for people without legal backgrounds.","In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts.","We also introduce a new dataset LegalStories, which consists of 295 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs.","To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts.","Furthermore, we use an expert-in-the-loop method to iteratively design multiple-choice questions.","Then, we evaluate the effectiveness of storytelling with LLMs through an RCT experiment with legal novices on 10 samples from the dataset.","We find that LLM-generated stories enhance comprehension of legal concepts and interest in law among non-native speakers compared to only definitions.","Moreover, stories consistently help participants relate legal concepts to their lives.","Finally, we find that learning with stories shows a higher retention rate for non-native speakers in the follow-up assessment.","Our work has strong implications for using LLMs in promoting teaching and learning in the legal field and beyond."],"url":"http://arxiv.org/abs/2402.17019v1","category":"cs.CL"}
{"created":"2024-02-26 20:22:06","title":"Cost of quantum secret key","abstract":"In this paper, we develop the resource theory of quantum secret key. Operating under the assumption that entangled states with zero distillable key do not exist, we define the key cost of a quantum state, and device. We study its properties through the lens of a quantity that we call the key of formation. The main result of our paper is that the regularized key of formation is an upper bound on the key cost of a quantum state. The core protocol underlying this result is privacy dilution, which converts states containing ideal privacy into ones with diluted privacy. Next, we show that the key cost is bounded from below by the regularized relative entropy of entanglement, which implies the irreversibility of the privacy creation-distillation process for a specific class of states. We further focus on mixed-state analogues of pure quantum states in the domain of privacy, and we prove that a number of entanglement measures are equal to each other for these states, similar to the case of pure entangled states. The privacy cost and distillable key in the single-shot regime exhibit a yield-cost relation, and basic consequences for quantum devices are also provided.","sentences":["In this paper, we develop the resource theory of quantum secret key.","Operating under the assumption that entangled states with zero distillable key do not exist, we define the key cost of a quantum state, and device.","We study its properties through the lens of a quantity that we call the key of formation.","The main result of our paper is that the regularized key of formation is an upper bound on the key cost of a quantum state.","The core protocol underlying this result is privacy dilution, which converts states containing ideal privacy into ones with diluted privacy.","Next, we show that the key cost is bounded from below by the regularized relative entropy of entanglement, which implies the irreversibility of the privacy creation-distillation process for a specific class of states.","We further focus on mixed-state analogues of pure quantum states in the domain of privacy, and we prove that a number of entanglement measures are equal to each other for these states, similar to the case of pure entangled states.","The privacy cost and distillable key in the single-shot regime exhibit a yield-cost relation, and basic consequences for quantum devices are also provided."],"url":"http://arxiv.org/abs/2402.17007v1","category":"quant-ph"}
{"created":"2024-02-26 20:20:05","title":"The Interstellar Medium in Dwarf Irregular Galaxies","abstract":"Dwarf irregulars (dIrrs) are among the most common type of galaxy in the Universe. They typically have gas-rich, low surface-brightness, metal-poor, and relatively-thick disks. Here we summarize the current state of our knowledge of the interstellar medium (ISM), including atomic, molecular and ionized gas, along with their dust properties and metals. We also discuss star formation feedback, gas accretion, and mergers with other dwarfs that connect the ISM to the circumgalactic and intergalactic media. We highlight one of the most persistent mysteries: the nature of pervasive gas that is yet undetected as either molecular or cold hydrogen, the ``dark gas''. Here are a few highlights:   1. Significant quantities of HI are in far-outer gas disks.   2. Cold HI in dIrrs would be molecular in the Milky Way, making the chemical properties of star-forming clouds significantly different.   3. Stellar feedback has a much larger impact in dIrrs than in spiral galaxies.   4. The escape fraction of ionizing photons is significant, making dIrrs a plausible source for reionization in the early Universe.   5. Observations suggest a significantly higher abundance of hydrogen (H$_2$ or cold HI) associated with CO in star-forming regions than that traced by the CO alone.","sentences":["Dwarf irregulars (dIrrs) are among the most common type of galaxy in the Universe.","They typically have gas-rich, low surface-brightness, metal-poor, and relatively-thick disks.","Here we summarize the current state of our knowledge of the interstellar medium (ISM), including atomic, molecular and ionized gas, along with their dust properties and metals.","We also discuss star formation feedback, gas accretion, and mergers with other dwarfs that connect the ISM to the circumgalactic and intergalactic media.","We highlight one of the most persistent mysteries: the nature of pervasive gas that is yet undetected as either molecular or cold hydrogen, the ``dark gas''.","Here are a few highlights:   1.","Significant quantities of HI are in far-outer gas disks.   ","2. Cold HI in dIrrs would be molecular in the Milky Way, making the chemical properties of star-forming clouds significantly different.   ","3.","Stellar feedback has a much larger impact in dIrrs than in spiral galaxies.   ","4.","The escape fraction of ionizing photons is significant, making dIrrs a plausible source for reionization in the early Universe.   ","5.","Observations suggest a significantly higher abundance of hydrogen (H$_2$ or cold HI) associated with CO in star-forming regions than that traced by the CO alone."],"url":"http://arxiv.org/abs/2402.17004v1","category":"astro-ph.GA"}
{"created":"2024-02-26 19:48:38","title":"Magnetic filaments: formation, stability, and feedback","abstract":"As well known, magnetic fields in space are distributed very inhomogeneously. Some-times field distributions have forms of filaments with high magnetic field values. As many ob-servations show, such a filamentation takes place in convective cells in the Sun and other astro-physical objects. This effect is associated with the frozenness of the magnetic field into a medium with high conductivity that leads to compression of magnetic field lines and forming magnetic filaments. We show analytically, based on the general analysis, that the magnetic field intensifies in the regions of downward flows in both two-dimensional and three-dimensional convective cells. These regions of the hyperbolic type for magnetic fields play a role of a specific attractor. This analysis was confirmed by numerical simulations for 2D convective cells of the roll-type. Without dissipation the magnetic field grows exponentially in time and does not depend on the aspect ratio between horizontal and vertical scale of the cell. An increase due to compression in the magnetic field in the high conductive plasma is saturated due to the natural limitation associated with dissipative effects when the maximum magnitude of the magnetic field is of the order of the root of the magnetic Reynolds number Rem. For the solar convective zone the mean kinetic energy density exceeds mean magnetic energy density at least for two orders of magnitude that allows one to use the kinematic approximation for the MHD induction equation. In this paper based on the stability analysis we explain why downward flows influence magnetic filaments from making them more flat with orientation along interfaces between convective cells.","sentences":["As well known, magnetic fields in space are distributed very inhomogeneously.","Some-times field distributions have forms of filaments with high magnetic field values.","As many ob-servations show, such a filamentation takes place in convective cells in the Sun and other astro-physical objects.","This effect is associated with the frozenness of the magnetic field into a medium with high conductivity that leads to compression of magnetic field lines and forming magnetic filaments.","We show analytically, based on the general analysis, that the magnetic field intensifies in the regions of downward flows in both two-dimensional and three-dimensional convective cells.","These regions of the hyperbolic type for magnetic fields play a role of a specific attractor.","This analysis was confirmed by numerical simulations for 2D convective cells of the roll-type.","Without dissipation the magnetic field grows exponentially in time and does not depend on the aspect ratio between horizontal and vertical scale of the cell.","An increase due to compression in the magnetic field in the high conductive plasma is saturated due to the natural limitation associated with dissipative effects when the maximum magnitude of the magnetic field is of the order of the root of the magnetic Reynolds number Rem.","For the solar convective zone the mean kinetic energy density exceeds mean magnetic energy density at least for two orders of magnitude that allows one to use the kinematic approximation for the MHD induction equation.","In this paper based on the stability analysis we explain why downward flows influence magnetic filaments from making them more flat with orientation along interfaces between convective cells."],"url":"http://arxiv.org/abs/2402.16989v1","category":"astro-ph.SR"}
{"created":"2024-02-26 19:45:43","title":"Meridional Circulation Streamlined","abstract":"Time-dependent meridional circulation and differential rotation in radiative zones are central open issues in stellar evolution theory. We streamline this challenging problem using the 'downward control principle' of atmospheric science, under a geostrophic f-plane approximation. New insights emerge from this simplified approach, using pressure as the vertical coordinate. We recover the known stellar physics result that the steady-state meridional circulation decays on the length scale (N/f sqrt(Pr))H, assuming molecular viscosity is the dominant drag mechanism. Prior to steady-state, the meridional circulation and the zonal wind (differential rotation) spread together via radiative diffusion, under thermal wind balance. The corresponding (4th-order) hyperdiffusion process is reasonably well approximated by regular (2nd-order) diffusion on scales of order a pressure scale-height. We derive an inhomogeneous diffusion (equiv. advection-diffusion) equation for the zonal flow which admits closed-form time-dependent solutions in a finite depth domain, allowing rapid prototyping of the meridional circulation pattern. In the weak drag limit, we find that the time to rotational steady-state can be longer than the Eddington-Sweet time and be instead determined by the longer drag time. Unless strong enough drag operates, the internal rotation of main-sequence stars may thus never reach steady-state. Streamlined meridional circulation solutions provide leading-order internal rotation profiles for studying the role of fluid/MHD instabilities (or waves) in redistributing angular momentum in the radiative zones of stars. Despite clear geometrical limitations and simplifying assumptions, one might expect our thin-layer geostrophic approach to offer qualitatively useful results to understand deep meridional circulation in stars.","sentences":["Time-dependent meridional circulation and differential rotation in radiative zones are central open issues in stellar evolution theory.","We streamline this challenging problem using the 'downward control principle' of atmospheric science, under a geostrophic f-plane approximation.","New insights emerge from this simplified approach, using pressure as the vertical coordinate.","We recover the known stellar physics result that the steady-state meridional circulation decays on the length scale (N/f sqrt(Pr))H, assuming molecular viscosity is the dominant drag mechanism.","Prior to steady-state, the meridional circulation and the zonal wind (differential rotation) spread together via radiative diffusion, under thermal wind balance.","The corresponding (4th-order) hyperdiffusion process is reasonably well approximated by regular (2nd-order) diffusion on scales of order a pressure scale-height.","We derive an inhomogeneous diffusion (equiv.","advection-diffusion) equation for the zonal flow which admits closed-form time-dependent solutions in a finite depth domain, allowing rapid prototyping of the meridional circulation pattern.","In the weak drag limit, we find that the time to rotational steady-state can be longer than the Eddington-Sweet time and be instead determined by the longer drag time.","Unless strong enough drag operates, the internal rotation of main-sequence stars may thus never reach steady-state.","Streamlined meridional circulation solutions provide leading-order internal rotation profiles for studying the role of fluid/MHD instabilities (or waves) in redistributing angular momentum in the radiative zones of stars.","Despite clear geometrical limitations and simplifying assumptions, one might expect our thin-layer geostrophic approach to offer qualitatively useful results to understand deep meridional circulation in stars."],"url":"http://arxiv.org/abs/2402.16988v1","category":"astro-ph.SR"}
{"created":"2024-02-26 19:09:57","title":"Cluster deep loci and mirror symmetry","abstract":"Affine cluster varieties are covered up to codimension 2 by open algebraic tori. We put forth a general conjecture (based on earlier conversation between Vivek Shende and the last author) characterizing their deep locus, i.e. the complement of all cluster charts, as the locus of points with non-trivial stabilizer under the action of cluster automorphisms. We use the diagrammatics of Demazure weaves to verify the conjecture for skew-symmetric cluster varieties of finite cluster type with arbitrary choice of frozens and for the top open positroid strata of Grassmannians $\\mathrm{Gr}(2,n)$ and $\\mathrm{Gr}(3,n)$. We illustrate how this already has applications in symplectic topology and mirror symmetry, by proving that the Fukaya category of Grassmannians $\\mathrm{Gr}(2,2n+1)$ is split-generated by finitely many Lagrangian tori, and homological mirror symmetry holds with a Landau--Ginzburg model proposed by Rietsch. Finally, we study the geometry of the deep locus, and find that it can be singular and have several irreducible components of different dimensions, but they all are again cluster varieties in our examples in really full rank cases.","sentences":["Affine cluster varieties are covered up to codimension 2 by open algebraic tori.","We put forth a general conjecture (based on earlier conversation between Vivek Shende and the last author) characterizing their deep locus, i.e. the complement of all cluster charts, as the locus of points with non-trivial stabilizer under the action of cluster automorphisms.","We use the diagrammatics of Demazure weaves to verify the conjecture for skew-symmetric cluster varieties of finite cluster type with arbitrary choice of frozens and for the top open positroid strata of Grassmannians $\\mathrm{Gr}(2,n)$ and $\\mathrm{Gr}(3,n)$. We illustrate how this already has applications in symplectic topology and mirror symmetry, by proving that the Fukaya category of Grassmannians $\\mathrm{Gr}(2,2n+1)$ is split-generated by finitely many Lagrangian tori, and homological mirror symmetry holds with a Landau--Ginzburg model proposed by Rietsch.","Finally, we study the geometry of the deep locus, and find that it can be singular and have several irreducible components of different dimensions, but they all are again cluster varieties in our examples in really full rank cases."],"url":"http://arxiv.org/abs/2402.16970v1","category":"math.AG"}
{"created":"2024-02-26 19:00:11","title":"Discovery of Globular Cluster Candidates in the Dwarf Irregular Galaxy IC 2574 Using HST/ACS Imaging","abstract":"We report the discovery of 23 globular cluster (GC) candidates around the relatively isolated dwarf galaxy IC 2574 within the Messier 81 (M81) group, at a distance of 3.86 Mpc. We use observations from the HST Advanced Camera for Surveys (ACS) to analyse the imaging in the F814W and F555W broadband filters. Our GC candidates have luminosities ranging from $-5.9 \\geq M_V \\geq -10.4$ and half-light radii of $1.4 \\leq r_h \\leq 11.5$ pc. We find the total number of GCs ($N_{\\mathrm{GC}})=27\\pm5$ after applying completeness corrections, which implies a specific frequency of $S_N = 4.0\\pm0.8$, consistent with expectations based on its luminosity. The GC system appears to have a bimodal colour distribution, with 30% of the GC candidates having redder colours. We also find 5 objects with extremely blue colours that could be young star clusters linked to an intense star formation episode that occurred in IC 2574 $\\sim$1 Gyr ago. We make an independent measurement of the halo mass of IC 2574 from its kinematic data, which is rare for low mass galaxies, and find log $M_{200} = 10.93 \\pm 0.08$. We place the galaxy on the well-known GC system mass-halo mass relation and find that it agrees well with the observed near-linear relation. IC 2574 has a rich GC population for a dwarf galaxy, which includes an unusually bright $\\omega$ Cen-like GC, making it an exciting nearby laboratory for probing the peculiar efficiency of forming massive GCs in dwarf galaxies.","sentences":["We report the discovery of 23 globular cluster (GC) candidates around the relatively isolated dwarf galaxy IC 2574 within the Messier 81 (M81) group, at a distance of 3.86 Mpc.","We use observations from the HST Advanced Camera for Surveys (ACS) to analyse the imaging in the F814W and F555W broadband filters.","Our GC candidates have luminosities ranging from $-5.9 \\geq M_V \\geq -10.4$ and half-light radii of $1.4 \\leq r_h","\\leq 11.5$ pc.","We find the total number of GCs ($N_{\\mathrm{GC}})=27\\pm5$ after applying completeness corrections, which implies a specific frequency of $S_N = 4.0\\pm0.8$, consistent with expectations based on its luminosity.","The GC system appears to have a bimodal colour distribution, with 30% of the GC candidates having redder colours.","We also find 5 objects with extremely blue colours that could be young star clusters linked to an intense star formation episode that occurred in IC 2574 $\\sim$1 Gyr ago.","We make an independent measurement of the halo mass of IC 2574 from its kinematic data, which is rare for low mass galaxies, and find log $M_{200} = 10.93 \\pm 0.08$. We place the galaxy on the well-known GC system mass-halo mass relation and find that it agrees well with the observed near-linear relation.","IC 2574 has a rich GC population for a dwarf galaxy, which includes an unusually bright $\\omega$ Cen-like GC, making it an exciting nearby laboratory for probing the peculiar efficiency of forming massive GCs in dwarf galaxies."],"url":"http://arxiv.org/abs/2402.16955v1","category":"astro-ph.GA"}
{"created":"2024-02-26 19:00:06","title":"Exploring the sensitivity to non-standard and generalized neutrino interactions through coherent elastic neutrino-nucleus scattering with a NaI detector","abstract":"After the first observation of coherent elastic neutrino-nucleus scattering (CE$\\nu$NS) by the COHERENT collaboration, many efforts are being made to improve the measurement of this process, making it possible to constrain new physics in the neutrino sector. In this paper, we study the sensitivity to non-standard interactions (NSIs) and generalized neutrino interactions (GNIs) of a NaI detector with characteristics similar to the one that is currently being deployed at the Spallation Neutron Source at Oak Ridge National Laboratory. We show that such a detector, whose target nuclei have significantly different proton to neutron ratios (at variance with the current CsI detector), could help to partially break the parameter degeneracies arising from the interference between the Standard Model and NSI contributions to the CE$\\nu$NS cross section, as well as between different NSI parameters. By contrast, only a slight improvement over the current CsI constraints is expected for parameters that do not interfere with the SM contribution. We find that a significant reduction of the background level would make the NaI detector considered in this paper very efficient at breaking degeneracies among NSI parameters.","sentences":["After the first observation of coherent elastic neutrino-nucleus scattering (CE$\\nu$NS) by the COHERENT collaboration, many efforts are being made to improve the measurement of this process, making it possible to constrain new physics in the neutrino sector.","In this paper, we study the sensitivity to non-standard interactions (NSIs) and generalized neutrino interactions (GNIs) of a NaI detector with characteristics similar to the one that is currently being deployed at the Spallation Neutron Source at Oak Ridge National Laboratory.","We show that such a detector, whose target nuclei have significantly different proton to neutron ratios (at variance with the current CsI detector), could help to partially break the parameter degeneracies arising from the interference between the Standard Model and NSI contributions to the CE$\\nu$NS cross section, as well as between different NSI parameters.","By contrast, only a slight improvement over the current CsI constraints is expected for parameters that do not interfere with the SM contribution.","We find that a significant reduction of the background level would make the NaI detector considered in this paper very efficient at breaking degeneracies among NSI parameters."],"url":"http://arxiv.org/abs/2402.16953v1","category":"hep-ph"}
{"created":"2024-02-26 19:00:01","title":"A NIRCam-dark galaxy detected with the MIRI/F1000W filter in the MIDIS/JADES Hubble Ultra Deep Field","abstract":"We report the discovery of Cerberus, an extremely red object detected with the MIRI Deep Imaging Survey (MIDIS) observations in the F1000W filter of the Hubble Ultra Deep Field. The object is detected at S/N~6, with F1000W~27 mag, and it is extremely faint in both the NIRCam data gathered by the JWST Advanced Deep Extragalactic Survey, JADES, with ~30.5 mag $5\\sigma$ upper limits in individual bands, as well as in the MIDIS F560W ultra deep data ($\\sim$29 mag, $5\\sigma$). Analyzing the spectral energy distribution built with individual (low S/N) optical-to-mid-infrared filters and (S/N~5) stacks, we discuss the possible nature of this red NIRCam-dark source using a battery of codes. We discard the possibility of Cerberus being a Solar System body based on the $<$0.016\" proper motion in the 1-year apart JADES and MIDIS observations. A sub-stellar Galactic nature is deemed unlikely, given that the Cerberus' relatively flat NIRCam-to-NIRCam and very red NIRCam-to-MIRI flux ratios are not consistent with any brown dwarf model. The extragalactic nature of Cerberus offers 3 possibilities: (1) A $z\\sim0.4$ galaxy with strong emission from polycyclic aromatic hydrocarbons; the very low inferred stellar mass, $\\mathrm{M}_\\star=10^{5-6}$ M$_\\odot$, makes this possibility highly improbable. (2) A dusty galaxy at $z\\sim4$ with an inferred stellar mass $\\mathrm{M}_\\star\\sim10^{8}$ M$_\\odot$. (3) A galaxy with observational properties similar to those of the reddest little red dots discovered around $z\\sim7$, but Cerberus lying at $z\\sim15$, presenting a spectral energy distribution in the rest-frame optical dominated by emission from a dusty torus or a dusty starburst.","sentences":["We report the discovery of Cerberus, an extremely red object detected with the MIRI Deep Imaging Survey (MIDIS) observations in the F1000W filter of the Hubble Ultra Deep Field.","The object is detected at S/N~6, with F1000W~27 mag, and it is extremely faint in both the NIRCam data gathered by the JWST Advanced Deep Extragalactic Survey, JADES, with ~30.5 mag $5\\sigma$ upper limits in individual bands, as well as in the MIDIS F560W ultra deep data ($\\sim$29 mag, $5\\sigma$).","Analyzing the spectral energy distribution built with individual (low S/N) optical-to-mid-infrared filters and (S/N~5) stacks, we discuss the possible nature of this red NIRCam-dark source using a battery of codes.","We discard the possibility of Cerberus being a Solar System body based on the $<$0.016\" proper motion in the 1-year apart JADES and MIDIS observations.","A sub-stellar Galactic nature is deemed unlikely, given that the Cerberus' relatively flat NIRCam-to-NIRCam and very red NIRCam-to-MIRI flux ratios are not consistent with any brown dwarf model.","The extragalactic nature of Cerberus offers 3 possibilities: (1) A $z\\sim0.4$ galaxy with strong emission from polycyclic aromatic hydrocarbons; the very low inferred stellar mass, $\\mathrm{M}_\\star=10^{5-6}$ M$_\\odot$, makes this possibility highly improbable.","(2) A dusty galaxy at $z\\sim4$ with an inferred stellar mass $\\mathrm{M}_\\star\\sim10^{8}$ M$_\\odot$. (3) A galaxy with observational properties similar to those of the reddest little red dots discovered around $z\\sim7$, but Cerberus lying at $z\\sim15$, presenting a spectral energy distribution in the rest-frame optical dominated by emission from a dusty torus or a dusty starburst."],"url":"http://arxiv.org/abs/2402.16942v1","category":"astro-ph.GA"}
{"created":"2024-02-26 19:00:00","title":"Exact Calculations of Coherent Information for Toric Codes under Decoherence: Identifying the Fundamental Error Threshold","abstract":"The toric code is a canonical example of a topological error-correcting code. Two logical qubits stored within the toric code are robust against local decoherence, ensuring that these qubits can be faithfully retrieved as long as the error rate remains below a certain threshold. Recent studies have explored such a threshold behavior as an intrinsic information-theoretic transition, independent of the decoding protocol. These studies have shown that information-theoretic metrics, calculated using the Renyi (replica) approximation, demonstrate sharp transitions at a specific error rate. However, an exact analytic expression that avoids using the replica trick has not been shown, and the connection between the transition in information-theoretic capacity and the random bond Ising model (RBIM) has only been indirectly established. In this work, we present the first analytic expression for the coherent information of a decohered toric code, thereby establishing a rigorous connection between the fundamental error threshold and the criticality of the RBIM.","sentences":["The toric code is a canonical example of a topological error-correcting code.","Two logical qubits stored within the toric code are robust against local decoherence, ensuring that these qubits can be faithfully retrieved as long as the error rate remains below a certain threshold.","Recent studies have explored such a threshold behavior as an intrinsic information-theoretic transition, independent of the decoding protocol.","These studies have shown that information-theoretic metrics, calculated using the Renyi (replica) approximation, demonstrate sharp transitions at a specific error rate.","However, an exact analytic expression that avoids using the replica trick has not been shown, and the connection between the transition in information-theoretic capacity and the random bond Ising model (RBIM) has only been indirectly established.","In this work, we present the first analytic expression for the coherent information of a decohered toric code, thereby establishing a rigorous connection between the fundamental error threshold and the criticality of the RBIM."],"url":"http://arxiv.org/abs/2402.16937v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-26 18:59:52","title":"InterroGate: Learning to Share, Specialize, and Prune Representations for Multi-task Learning","abstract":"Jointly learning multiple tasks with a unified model can improve accuracy and data efficiency, but it faces the challenge of task interference, where optimizing one task objective may inadvertently compromise the performance of another. A solution to mitigate this issue is to allocate task-specific parameters, free from interference, on top of shared features. However, manually designing such architectures is cumbersome, as practitioners need to balance between the overall performance across all tasks and the higher computational cost induced by the newly added parameters. In this work, we propose \\textit{InterroGate}, a novel multi-task learning (MTL) architecture designed to mitigate task interference while optimizing inference computational efficiency. We employ a learnable gating mechanism to automatically balance the shared and task-specific representations while preserving the performance of all tasks. Crucially, the patterns of parameter sharing and specialization dynamically learned during training, become fixed at inference, resulting in a static, optimized MTL architecture. Through extensive empirical evaluations, we demonstrate SoTA results on three MTL benchmarks using convolutional as well as transformer-based backbones on CelebA, NYUD-v2, and PASCAL-Context.","sentences":["Jointly learning multiple tasks with a unified model can improve accuracy and data efficiency, but it faces the challenge of task interference, where optimizing one task objective may inadvertently compromise the performance of another.","A solution to mitigate this issue is to allocate task-specific parameters, free from interference, on top of shared features.","However, manually designing such architectures is cumbersome, as practitioners need to balance between the overall performance across all tasks and the higher computational cost induced by the newly added parameters.","In this work, we propose \\textit{InterroGate}, a novel multi-task learning (MTL) architecture designed to mitigate task interference while optimizing inference computational efficiency.","We employ a learnable gating mechanism to automatically balance the shared and task-specific representations while preserving the performance of all tasks.","Crucially, the patterns of parameter sharing and specialization dynamically learned during training, become fixed at inference, resulting in a static, optimized MTL architecture.","Through extensive empirical evaluations, we demonstrate SoTA results on three MTL benchmarks using convolutional as well as transformer-based backbones on CelebA, NYUD-v2, and PASCAL-Context."],"url":"http://arxiv.org/abs/2402.16848v1","category":"cs.LG"}
{"created":"2024-02-26 18:57:57","title":"On recurrence for $\\mathbb{Z}^d$-nilsystems","abstract":"We study the topological recurrence phenomenon of actions of locally compact abelian groups on compact metric spaces. We address the popular Katznelson's question in this context and extend recurrence lifting results, known for $\\mathbb{Z}$-actions, to this more general context. In the case $H=\\mathbb{Z}^d$, we develop new techniques to analyze Bohr recurrence sets, and show that Bohr recurrence implies recurrence for a $\\mathbb{Z}^d$-nilsystem $(G/\\Gamma,\\mathbb{Z}^d)$, under the assumption that the connected component of the identity in $G$ is abelian. This family encompasses, for example, all $\\mathbb{Z}^d$-affine nilsystems.","sentences":["We study the topological recurrence phenomenon of actions of locally compact abelian groups on compact metric spaces.","We address the popular Katznelson's question in this context and extend recurrence lifting results, known for $\\mathbb{Z}$-actions, to this more general context.","In the case $H=\\mathbb{Z}^d$, we develop new techniques to analyze Bohr recurrence sets, and show that Bohr recurrence implies recurrence for a $\\mathbb{Z}^d$-nilsystem $(G/\\Gamma,\\mathbb{Z}^d)$, under the assumption that the connected component of the identity in $G$ is abelian.","This family encompasses, for example, all $\\mathbb{Z}^d$-affine nilsystems."],"url":"http://arxiv.org/abs/2402.16838v1","category":"math.DS"}
{"created":"2024-02-26 18:55:15","title":"GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning","abstract":"Embedding models are integral to AI applications like semantic search, personalized recommendations, and retrieval augmented generation for LLMs, necessitating high-quality training data. However, the limited scalability of manual data curation prompts the need for automated methods to ensure data integrity. Traditional unsupervised triplet mining automates training data generation, crucial for embedding model training, yet inadvertently injects biases and noise, thereby degrading model performance. Addressing this, we introduce GISTEmbed, a novel strategy that enhances in-batch negative selection during contrastive training through a guide model. This approach departs from reliance on random sampling and equal utility assumption of batch negatives, significantly reducing noise from data quality issues and improving model fine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB), GISTEmbed showcases consistent performance improvements across various model sizes and achieves state-of-the-art results in select categories. This framework enables significant enhancements for smaller models by leveraging the capabilities of powerful yet resource-intensive large models. GISTEmbed can potentially revolutionize the creation of highly efficient, smaller models, democratizing access to advanced AI technologies. Making these technologies more accessible and cost-effective, especially for applications constrained by resources, significantly expands the impact and accessibility of state-of-the-art AI solutions across diverse sectors.","sentences":["Embedding models are integral to AI applications like semantic search, personalized recommendations, and retrieval augmented generation for LLMs, necessitating high-quality training data.","However, the limited scalability of manual data curation prompts the need for automated methods to ensure data integrity.","Traditional unsupervised triplet mining automates training data generation, crucial for embedding model training, yet inadvertently injects biases and noise, thereby degrading model performance.","Addressing this, we introduce GISTEmbed, a novel strategy that enhances in-batch negative selection during contrastive training through a guide model.","This approach departs from reliance on random sampling and equal utility assumption of batch negatives, significantly reducing noise from data quality issues and improving model fine-tuning.","Benchmarked against the Massive Text Embedding Benchmark (MTEB), GISTEmbed showcases consistent performance improvements across various model sizes and achieves state-of-the-art results in select categories.","This framework enables significant enhancements for smaller models by leveraging the capabilities of powerful yet resource-intensive large models.","GISTEmbed can potentially revolutionize the creation of highly efficient, smaller models, democratizing access to advanced AI technologies.","Making these technologies more accessible and cost-effective, especially for applications constrained by resources, significantly expands the impact and accessibility of state-of-the-art AI solutions across diverse sectors."],"url":"http://arxiv.org/abs/2402.16829v1","category":"cs.LG"}
{"created":"2024-02-26 18:45:05","title":"Entropy solutions in $BV^s$ for a class of triangular systems involving a transport equation","abstract":"In this article, we consider a class of strictly hyperbolic triangular systems involving a transport equation. Such systems are known to create measure solutions for the initial value problem. Adding a stronger transversality assumption on the fields, we are able to obtain solutions in $L^\\infty$ under optimal fractional $BV$ regularity of the initial data. Our results show that the critical fractional regularity is $s=1/3$. We also construct an initial data that is not in $BV^{1/3}$ but for which a blow-up in $L^\\infty$ occurs, proving the optimality of our results.","sentences":["In this article, we consider a class of strictly hyperbolic triangular systems involving a transport equation.","Such systems are known to create measure solutions for the initial value problem.","Adding a stronger transversality assumption on the fields, we are able to obtain solutions in $L^\\infty$ under optimal fractional $BV$ regularity of the initial data.","Our results show that the critical fractional regularity is $s=1/3$. We also construct an initial data that is not in $BV^{1/3}$ but for which a blow-up in $L^\\infty$ occurs, proving the optimality of our results."],"url":"http://arxiv.org/abs/2402.16820v1","category":"math.AP"}
{"created":"2024-02-26 18:34:58","title":"Stopping Bayesian Optimization with Probabilistic Regret Bounds","abstract":"Bayesian optimization is a popular framework for efficiently finding high-quality solutions to difficult problems based on limited prior information. As a rule, these algorithms operate by iteratively choosing what to try next until some predefined budget has been exhausted. We investigate replacing this de facto stopping rule with an $(\\epsilon, \\delta)$-criterion: stop when a solution has been found whose value is within $\\epsilon > 0$ of the optimum with probability at least $1 - \\delta$ under the model. Given access to the prior distribution of problems, we show how to verify this condition in practice using a limited number of draws from the posterior. For Gaussian process priors, we prove that Bayesian optimization with the proposed criterion stops in finite time and returns a point that satisfies the $(\\epsilon, \\delta)$-criterion under mild assumptions. These findings are accompanied by extensive empirical results which demonstrate the strengths and weaknesses of this approach.","sentences":["Bayesian optimization is a popular framework for efficiently finding high-quality solutions to difficult problems based on limited prior information.","As a rule, these algorithms operate by iteratively choosing what to try next until some predefined budget has been exhausted.","We investigate replacing this de facto stopping rule with an $(\\epsilon, \\delta)$-criterion: stop when a solution has been found whose value is within $\\epsilon > 0$ of the optimum with probability at least $1 - \\delta$ under the model.","Given access to the prior distribution of problems, we show how to verify this condition in practice using a limited number of draws from the posterior.","For Gaussian process priors, we prove that Bayesian optimization with the proposed criterion stops in finite time and returns a point that satisfies the $(\\epsilon, \\delta)$-criterion under mild assumptions.","These findings are accompanied by extensive empirical results which demonstrate the strengths and weaknesses of this approach."],"url":"http://arxiv.org/abs/2402.16811v1","category":"stat.ML"}
{"created":"2024-02-26 18:30:19","title":"On tori periods of Weil representations of unitary groups","abstract":"We determine the restriction of Weil representations of unitary groups to maximal tori. In the local case, we show that the Weil representation contains a pair of compatible characters if and only if a root number condition holds. In the global case, we show that a tori period corresponding to a maximal anisotropic torus of the global theta lift of a character does not vanish if and only if the local condition is satisfied everywhere and a central value of an $L$-function does not vanish. Our proof makes use of the seesaw argument and of the well-known theta lifting results from $\\operatorname{U}\\left(1\\right)$ to $\\operatorname{U}\\left(1\\right)$. Our results are used in other papers to construct Arthur packets for $G_2$.","sentences":["We determine the restriction of Weil representations of unitary groups to maximal tori.","In the local case, we show that the Weil representation contains a pair of compatible characters if and only if a root number condition holds.","In the global case, we show that a tori period corresponding to a maximal anisotropic torus of the global theta lift of a character does not vanish if and only if the local condition is satisfied everywhere and a central value of an $L$-function does not vanish.","Our proof makes use of the seesaw argument and of the well-known theta lifting results from $\\operatorname{U}\\left(1\\right)$ to $\\operatorname{U}\\left(1\\right)$. Our results are used in other papers to construct Arthur packets for $G_2$."],"url":"http://arxiv.org/abs/2402.16808v1","category":"math.RT"}
{"created":"2024-02-26 18:29:12","title":"An $^{115}$In$^+$-$^{172}$Yb$^+$ Coulomb crystal clock with $2.5\\times10^{-18}$ systematic uncertainty","abstract":"We present a scalable mixed-species Coulomb crystal clock based on the $^1S_0$ $\\leftrightarrow$ $^3P_0$ transition in $^{115}$In$^+$. $^{172}$Yb$^+$ ions are co-trapped and used for sympathetic cooling. Reproducible interrogation conditions for mixed-species Coulomb crystals are ensured by a conditional preparation sequence with permutation control. We demonstrate clock operation with a 1In$^+$-3Yb$^+$ crystal, achieving a relative systematic uncertainty of $2.5\\times10^{-18}$ and a relative frequency instability of $1.6\\times10^{-15}/\\sqrt{\\tau/1\\;s}$. We report on an absolute frequency measurement with an uncertainty of $1.3\\times10^{-16}$ and optical frequency ratios relative to the $^{171}$Yb$^+$ (E3) and $^{87}$Sr clock transitions with fractional uncertainties of $4.4$ and $4.7$ parts in 10$^{18}$, respectively. The latter are among the most precise measurements of frequency ratios to date and improve upon the previous uncertainty of the $^{115}$In$^+$/$^{87}$Sr ratio by two orders of magnitude. We also demonstrate operation with four $^{115}$In$^+$ clock ions, which reduces the instability to $9.2\\times10^{-16}/\\sqrt{\\tau/1\\;s}$.","sentences":["We present a scalable mixed-species Coulomb crystal clock based on the $^1S_0$ $\\leftrightarrow$ $^3P_0$ transition in $^{115}$In$^+$. $^{172}$Yb$^+$ ions are co-trapped and used for sympathetic cooling.","Reproducible interrogation conditions for mixed-species Coulomb crystals are ensured by a conditional preparation sequence with permutation control.","We demonstrate clock operation with a 1In$^+$-3Yb$^+$ crystal, achieving a relative systematic uncertainty of $2.5\\times10^{-18}$ and a relative frequency instability of $1.6\\times10^{-15}/\\sqrt{\\tau/1\\;s}$. We report on an absolute frequency measurement with an uncertainty of $1.3\\times10^{-16}$ and optical frequency ratios relative to the $^{171}$Yb$^+$ (E3) and $^{87}$Sr clock transitions with fractional uncertainties of $4.4$ and $4.7$ parts in 10$^{18}$, respectively.","The latter are among the most precise measurements of frequency ratios to date and improve upon the previous uncertainty of the $^{115}$In$^+$/$^{87}$Sr ratio by two orders of magnitude.","We also demonstrate operation with four $^{115}$In$^+$ clock ions, which reduces the instability to $9.2\\times10^{-16}/\\sqrt{\\tau/1\\;s}$."],"url":"http://arxiv.org/abs/2402.16807v1","category":"physics.atom-ph"}
{"created":"2024-02-26 18:28:05","title":"Multi-Human Mesh Recovery with Transformers","abstract":"Conventional approaches to human mesh recovery predominantly employ a region-based strategy. This involves initially cropping out a human-centered region as a preprocessing step, with subsequent modeling focused on this zoomed-in image. While effective for single figures, this pipeline poses challenges when dealing with images featuring multiple individuals, as different people are processed separately, often leading to inaccuracies in relative positioning. Despite the advantages of adopting a whole-image-based approach to address this limitation, early efforts in this direction have fallen short in performance compared to recent region-based methods. In this work, we advocate for this under-explored area of modeling all people at once, emphasizing its potential for improved accuracy in multi-person scenarios through considering all individuals simultaneously and leveraging the overall context and interactions. We introduce a new model with a streamlined transformer-based design, featuring three critical design choices: multi-scale feature incorporation, focused attention mechanisms, and relative joint supervision. Our proposed model demonstrates a significant performance improvement, surpassing state-of-the-art region-based and whole-image-based methods on various benchmarks involving multiple individuals.","sentences":["Conventional approaches to human mesh recovery predominantly employ a region-based strategy.","This involves initially cropping out a human-centered region as a preprocessing step, with subsequent modeling focused on this zoomed-in image.","While effective for single figures, this pipeline poses challenges when dealing with images featuring multiple individuals, as different people are processed separately, often leading to inaccuracies in relative positioning.","Despite the advantages of adopting a whole-image-based approach to address this limitation, early efforts in this direction have fallen short in performance compared to recent region-based methods.","In this work, we advocate for this under-explored area of modeling all people at once, emphasizing its potential for improved accuracy in multi-person scenarios through considering all individuals simultaneously and leveraging the overall context and interactions.","We introduce a new model with a streamlined transformer-based design, featuring three critical design choices: multi-scale feature incorporation, focused attention mechanisms, and relative joint supervision.","Our proposed model demonstrates a significant performance improvement, surpassing state-of-the-art region-based and whole-image-based methods on various benchmarks involving multiple individuals."],"url":"http://arxiv.org/abs/2402.16806v1","category":"cs.CV"}
{"created":"2024-02-26 18:24:22","title":"A stochastic perturbation approach to nonlinear bifurcating problems","abstract":"Incorporating probabilistic terms in mathematical models is crucial for capturing and quantifying uncertainties in real-world systems. Indeed, randomness can have a significant impact on the behavior of the problem's solution, and a deeper analysis is needed to obtain more realistic and informative results. On the other hand, the investigation of stochastic models may require great computational resources due to the importance of generating numerous realizations of the system to have meaningful statistics. This makes the development of complexity reduction techniques, such as surrogate models, essential for enabling efficient and scalable simulations. In this work, we exploit polynomial chaos (PC) expansion to study the accuracy of surrogate representations for a bifurcating phenomena in fluid dynamics, namely the Coanda effect, where the stochastic setting gives a different perspective on the non-uniqueness of the solution. Then, its inclusion in the finite element setting is described, arriving to the formulation of the enhanced Spectral Stochastic Finite Element Method (SSFEM). Moreover, we investigate the connections between the deterministic bifurcation diagram and the PC polynomials, underlying their capability in reconstructing the whole solution manifold.","sentences":["Incorporating probabilistic terms in mathematical models is crucial for capturing and quantifying uncertainties in real-world systems.","Indeed, randomness can have a significant impact on the behavior of the problem's solution, and a deeper analysis is needed to obtain more realistic and informative results.","On the other hand, the investigation of stochastic models may require great computational resources due to the importance of generating numerous realizations of the system to have meaningful statistics.","This makes the development of complexity reduction techniques, such as surrogate models, essential for enabling efficient and scalable simulations.","In this work, we exploit polynomial chaos (PC) expansion to study the accuracy of surrogate representations for a bifurcating phenomena in fluid dynamics, namely the Coanda effect, where the stochastic setting gives a different perspective on the non-uniqueness of the solution.","Then, its inclusion in the finite element setting is described, arriving to the formulation of the enhanced Spectral Stochastic Finite Element Method (SSFEM).","Moreover, we investigate the connections between the deterministic bifurcation diagram and the PC polynomials, underlying their capability in reconstructing the whole solution manifold."],"url":"http://arxiv.org/abs/2402.16803v1","category":"math.NA"}
{"created":"2024-02-26 18:19:51","title":"New Prospects for a Causally Local Formulation of Quantum Theory","abstract":"It is difficult to extract reliable criteria for causal locality from the limited ingredients found in textbook quantum theory. In the end, Bell humbly warned that his eponymous theorem was based on criteria that \"should be viewed with the utmost suspicion.\" Remarkably, by stepping outside the wave-function paradigm, one can reformulate quantum theory in terms of old-fashioned configuration spaces together with 'unistochastic' laws. These unistochastic laws take the form of directed conditional probabilities, which turn out to provide a hospitable foundation for encoding microphysical causal relationships. This unistochastic reformulation provides quantum theory with a simpler and more transparent axiomatic foundation, plausibly resolves the measurement problem, and deflates various exotic claims about superposition, interference, and entanglement. Making use of this reformulation, this paper introduces a new principle of causal locality that is intended to improve on Bell's criteria, and shows directly that systems that remain at spacelike separation cannot exert causal influences on each other, according to that new principle. These results therefore lead to a general hidden-variables interpretation of quantum theory that is arguably compatible with causal locality.","sentences":["It is difficult to extract reliable criteria for causal locality from the limited ingredients found in textbook quantum theory.","In the end, Bell humbly warned that his eponymous theorem was based on criteria that \"should be viewed with the utmost suspicion.\"","Remarkably, by stepping outside the wave-function paradigm, one can reformulate quantum theory in terms of old-fashioned configuration spaces together with 'unistochastic' laws.","These unistochastic laws take the form of directed conditional probabilities, which turn out to provide a hospitable foundation for encoding microphysical causal relationships.","This unistochastic reformulation provides quantum theory with a simpler and more transparent axiomatic foundation, plausibly resolves the measurement problem, and deflates various exotic claims about superposition, interference, and entanglement.","Making use of this reformulation, this paper introduces a new principle of causal locality that is intended to improve on Bell's criteria, and shows directly that systems that remain at spacelike separation cannot exert causal influences on each other, according to that new principle.","These results therefore lead to a general hidden-variables interpretation of quantum theory that is arguably compatible with causal locality."],"url":"http://arxiv.org/abs/2402.16935v1","category":"quant-ph"}
{"created":"2024-02-26 18:19:07","title":"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning","abstract":"Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms. We identify that existing benchmarks used for research into open-ended learning fall into one of two categories. Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original. A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90% of the optimal reward. To provide a more compelling challenge we present the main Craftax benchmark, a significant extension of the Crafter mechanics with elements inspired from NetHack. Solving Craftax requires deep exploration, long term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered. We show that existing methods including global and episodic exploration, as well as unsupervised environment design fail to make material progress on the benchmark. We believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources.","sentences":["Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms.","We identify that existing benchmarks used for research into open-ended learning fall into one of two categories.","Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen.","To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original.","A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90% of the optimal reward.","To provide a more compelling challenge we present the main Craftax benchmark, a significant extension of the Crafter mechanics with elements inspired from NetHack.","Solving Craftax requires deep exploration, long term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered.","We show that existing methods including global and episodic exploration, as well as unsupervised environment design fail to make material progress on the benchmark.","We believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources."],"url":"http://arxiv.org/abs/2402.16801v1","category":"cs.LG"}
{"created":"2024-02-26 18:03:15","title":"Entanglement-breaking channels are a quantum memory resource","abstract":"Entanglement-breaking channels (equivalently, measure-and-prepare channels) are an important class of quantum operations noted for their ability to destroy multipartite spatial quantum correlations. Inspired by this property, they have also been employed in defining notions of \"classical memory\", under the assumption that such channels effectively act as a classical resource. We show that, in a single-system multi-time scenario, entanglement-breaking channels are still a quantum memory resource: a qudit going through an entanglement-breaking channel cannot be simulated by a classical system of same dimension. We provide explicit examples of memory-based output generation tasks where entanglement-breaking channels outperform classical memories of the same size. Our results imply that entanglement-breaking channels cannot be generally employed to characterize classical memory effects in temporal scenarios without additional assumptions.","sentences":["Entanglement-breaking channels (equivalently, measure-and-prepare channels) are an important class of quantum operations noted for their ability to destroy multipartite spatial quantum correlations.","Inspired by this property, they have also been employed in defining notions of \"classical memory\", under the assumption that such channels effectively act as a classical resource.","We show that, in a single-system multi-time scenario, entanglement-breaking channels are still a quantum memory resource: a qudit going through an entanglement-breaking channel cannot be simulated by a classical system of same dimension.","We provide explicit examples of memory-based output generation tasks where entanglement-breaking channels outperform classical memories of the same size.","Our results imply that entanglement-breaking channels cannot be generally employed to characterize classical memory effects in temporal scenarios without additional assumptions."],"url":"http://arxiv.org/abs/2402.16789v1","category":"quant-ph"}
{"created":"2024-02-26 18:01:37","title":"Isoperimetric Profiles and Regular Embeddings of locally compact groups","abstract":"In this article we extend the notion of $L^p$-measured subgroups couplings; a quantitative asymmetric version of measure equivalence that was introduced by Delabie, Koivisto, Le Ma\\^itre and Tessera for finitely generated groups, to the setting of locally compact compactly generated unimodular groups. As an example of these couplings; using ideas from Bader and Rosendal, we prove a \"dynamical criteria\" for the existence of regular embeddings between amenable locally compact compactly generated unimodular groups, namely the existence of an $L^\\infty$-measured subgroup coupling that is coarsely $m$ to $1$. We then proceed to prove that the existence of an $L^p$-measured subgroup that is coarsely $m$ to $1$ implies the monotonicity of the $L^p$-isoperimetric profile. We conclude then that the $L^p$-isoperimetric profile is monotonous under regular embeddings, as well as coarse embeddings, between amenable unimodular locally compact compactly generated groups.","sentences":["In this article we extend the notion of $L^p$-measured subgroups couplings; a quantitative asymmetric version of measure equivalence that was introduced by Delabie, Koivisto, Le Ma\\^itre and Tessera for finitely generated groups, to the setting of locally compact compactly generated unimodular groups.","As an example of these couplings; using ideas from Bader and Rosendal, we prove a \"dynamical criteria\" for the existence of regular embeddings between amenable locally compact compactly generated unimodular groups, namely the existence of an $L^\\infty$-measured subgroup coupling that is coarsely $m$ to $1$. We then proceed to prove that the existence of an $L^p$-measured subgroup that is coarsely $m$ to $1$ implies the monotonicity of the $L^p$-isoperimetric profile.","We conclude then that the $L^p$-isoperimetric profile is monotonous under regular embeddings, as well as coarse embeddings, between amenable unimodular locally compact compactly generated groups."],"url":"http://arxiv.org/abs/2402.16787v1","category":"math.GR"}
{"created":"2024-02-26 17:58:34","title":"A Strong Version of the Hilbert Nullstellensatz for slice regular polynomials in several quaternionic variables","abstract":"In this paper we prove a strong version of the Hilbert Nullstellensatz in the ring $\\mathbb H[q_1,\\ldots,q_n]$ of slice regular polynomials in several quaternionic variables. Our proof deeply depends on a detailed analysis of the common zeros of slice regular polynomials which belong to an ideal in $\\mathbb H[q_1,\\ldots,q_n]$. This study motivates the introduction of a new notion of algebraic set in the quaternionic setting, which allows us to define a Zariski-type topology on $\\mathbb H^n$.","sentences":["In this paper we prove a strong version of the Hilbert Nullstellensatz in the ring $\\mathbb H[q_1,\\ldots,q_n]$ of slice regular polynomials in several quaternionic variables.","Our proof deeply depends on a detailed analysis of the common zeros of slice regular polynomials which belong to an ideal in $\\mathbb H[q_1,\\ldots,q_n]$.","This study motivates the introduction of a new notion of algebraic set in the quaternionic setting, which allows us to define a Zariski-type topology on $\\mathbb H^n$."],"url":"http://arxiv.org/abs/2402.16784v2","category":"math.CV"}
{"created":"2024-02-27 18:29:07","title":"LoDIP: Low light phase retrieval with deep image prior","abstract":"Phase retrieval (PR) is a fundamental challenge in scientific imaging, enabling nanoscale techniques like coherent diffractive imaging (CDI). Imaging at low radiation doses becomes important in applications where samples are susceptible to radiation damage. However, most PR methods struggle in low dose scenario due to the presence of very high shot noise. Advancements in the optical data acquisition setup, exemplified by in-situ CDI, have shown potential for low-dose imaging. But these depend on a time series of measurements, rendering them unsuitable for single-image applications. Similarly, on the computational front, data-driven phase retrieval techniques are not readily adaptable to the single-image context. Deep learning based single-image methods, such as deep image prior, have been effective for various imaging tasks but have exhibited limited success when applied to PR. In this work, we propose LoDIP which combines the in-situ CDI setup with the power of implicit neural priors to tackle the problem of single-image low-dose phase retrieval. Quantitative evaluations demonstrate the superior performance of LoDIP on this task as well as applicability to real experimental scenarios.","sentences":["Phase retrieval (PR) is a fundamental challenge in scientific imaging, enabling nanoscale techniques like coherent diffractive imaging (CDI).","Imaging at low radiation doses becomes important in applications where samples are susceptible to radiation damage.","However, most PR methods struggle in low dose scenario due to the presence of very high shot noise.","Advancements in the optical data acquisition setup, exemplified by in-situ CDI, have shown potential for low-dose imaging.","But these depend on a time series of measurements, rendering them unsuitable for single-image applications.","Similarly, on the computational front, data-driven phase retrieval techniques are not readily adaptable to the single-image context.","Deep learning based single-image methods, such as deep image prior, have been effective for various imaging tasks but have exhibited limited success when applied to PR.","In this work, we propose LoDIP which combines the in-situ CDI setup with the power of implicit neural priors to tackle the problem of single-image low-dose phase retrieval.","Quantitative evaluations demonstrate the superior performance of LoDIP on this task as well as applicability to real experimental scenarios."],"url":"http://arxiv.org/abs/2402.17745v1","category":"physics.comp-ph"}
{"created":"2024-02-27 17:36:01","title":"Adaptive quantization with mixed-precision based on low-cost proxy","abstract":"It is critical to deploy complicated neural network models on hardware with limited resources. This paper proposes a novel model quantization method, named the Low-Cost Proxy-Based Adaptive Mixed-Precision Model Quantization (LCPAQ), which contains three key modules. The hardware-aware module is designed by considering the hardware limitations, while an adaptive mixed-precision quantization module is developed to evaluate the quantization sensitivity by using the Hessian matrix and Pareto frontier techniques. Integer linear programming is used to fine-tune the quantization across different layers. Then the low-cost proxy neural architecture search module efficiently explores the ideal quantization hyperparameters. Experiments on the ImageNet demonstrate that the proposed LCPAQ achieves comparable or superior quantization accuracy to existing mixed-precision models. Notably, LCPAQ achieves 1/200 of the search time compared with existing methods, which provides a shortcut in practical quantization use for resource-limited devices.","sentences":["It is critical to deploy complicated neural network models on hardware with limited resources.","This paper proposes a novel model quantization method, named the Low-Cost Proxy-Based Adaptive Mixed-Precision Model Quantization (LCPAQ), which contains three key modules.","The hardware-aware module is designed by considering the hardware limitations, while an adaptive mixed-precision quantization module is developed to evaluate the quantization sensitivity by using the Hessian matrix and Pareto frontier techniques.","Integer linear programming is used to fine-tune the quantization across different layers.","Then the low-cost proxy neural architecture search module efficiently explores the ideal quantization hyperparameters.","Experiments on the ImageNet demonstrate that the proposed LCPAQ achieves comparable or superior quantization accuracy to existing mixed-precision models.","Notably, LCPAQ achieves 1/200 of the search time compared with existing methods, which provides a shortcut in practical quantization use for resource-limited devices."],"url":"http://arxiv.org/abs/2402.17706v1","category":"cs.CV"}
{"created":"2024-02-27 17:26:33","title":"Real-time Low-latency Music Source Separation using Hybrid Spectrogram-TasNet","abstract":"There have been significant advances in deep learning for music demixing in recent years. However, there has been little attention given to how these neural networks can be adapted for real-time low-latency applications, which could be helpful for hearing aids, remixing audio streams and live shows. In this paper, we investigate the various challenges involved in adapting current demixing models in the literature for this use case. Subsequently, inspired by the Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domain Audio Separation Network HS-TasNet, which utilises the advantages of spectral and waveform domains. For a latency of 23 ms, the HS-TasNet obtains an overall signal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increases to 5.55 with additional training data. These results demonstrate the potential of efficient demixing for real-time low-latency music applications.","sentences":["There have been significant advances in deep learning for music demixing in recent years.","However, there has been little attention given to how these neural networks can be adapted for real-time low-latency applications, which could be helpful for hearing aids, remixing audio streams and live shows.","In this paper, we investigate the various challenges involved in adapting current demixing models in the literature for this use case.","Subsequently, inspired by the Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domain Audio Separation Network HS-TasNet, which utilises the advantages of spectral and waveform domains.","For a latency of 23 ms, the HS-TasNet obtains an overall signal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increases to 5.55 with additional training data.","These results demonstrate the potential of efficient demixing for real-time low-latency music applications."],"url":"http://arxiv.org/abs/2402.17701v1","category":"eess.AS"}
{"created":"2024-02-27 17:11:39","title":"Adaptive waveform inversion for transmitted wave data","abstract":"Adaptive Waveform Inversion applied to transient transmitted wave data yields estimates of index of refraction (or wave velocity) similar to those obtained by travel time inversion, provided that the data contain a single smooth wavefront.","sentences":["Adaptive Waveform Inversion applied to transient transmitted wave data yields estimates of index of refraction (or wave velocity) similar to those obtained by travel time inversion, provided that the data contain a single smooth wavefront."],"url":"http://arxiv.org/abs/2402.17696v1","category":"math.OC"}
{"created":"2024-02-27 14:34:14","title":"Adapting Learned Image Codecs to Screen Content via Adjustable Transformations","abstract":"As learned image codecs (LICs) become more prevalent, their low coding efficiency for out-of-distribution data becomes a bottleneck for some applications. To improve the performance of LICs for screen content (SC) images without breaking backwards compatibility, we propose to introduce parameterized and invertible linear transformations into the coding pipeline without changing the underlying baseline codec's operation flow. We design two neural networks to act as prefilters and postfilters in our setup to increase the coding efficiency and help with the recovery from coding artifacts. Our end-to-end trained solution achieves up to 10% bitrate savings on SC compression compared to the baseline LICs while introducing only 1% extra parameters.","sentences":["As learned image codecs (LICs) become more prevalent, their low coding efficiency for out-of-distribution data becomes a bottleneck for some applications.","To improve the performance of LICs for screen content (SC) images without breaking backwards compatibility, we propose to introduce parameterized and invertible linear transformations into the coding pipeline without changing the underlying baseline codec's operation flow.","We design two neural networks to act as prefilters and postfilters in our setup to increase the coding efficiency and help with the recovery from coding artifacts.","Our end-to-end trained solution achieves up to 10% bitrate savings on SC compression compared to the baseline LICs while introducing only 1% extra parameters."],"url":"http://arxiv.org/abs/2402.17544v1","category":"eess.IV"}
{"created":"2024-02-27 14:05:05","title":"AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Point Cloud Analysis","abstract":"Efficient downsampling plays a crucial role in point cloud learning, particularly for large-scale 3D scenes. Existing downsampling methods either require a huge computational burden or sacrifice fine-grained geometric information. This paper presents an advanced sampler that achieves both high accuracy and efficiency. The proposed method utilizes voxel-based sampling as a foundation, but effectively addresses the challenges regarding voxel size determination and the preservation of critical geometric cues. Specifically, we propose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the reference of point-based downsampling ratio. This ensures the sampling results exhibit a favorable distribution for comprehending various 3D objects or scenes. Additionally, we introduce a network compatible with arbitrary voxel sizes for sampling and feature extraction while maintaining high efficiency. Our method achieves state-of-the-art accuracy on the ShapeNetPart and ScanNet benchmarks with promising efficiency. Code will be available at https://github.com/yhc2021/AVS-Net.","sentences":["Efficient downsampling plays a crucial role in point cloud learning, particularly for large-scale 3D scenes.","Existing downsampling methods either require a huge computational burden or sacrifice fine-grained geometric information.","This paper presents an advanced sampler that achieves both high accuracy and efficiency.","The proposed method utilizes voxel-based sampling as a foundation, but effectively addresses the challenges regarding voxel size determination and the preservation of critical geometric cues.","Specifically, we propose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the reference of point-based downsampling ratio.","This ensures the sampling results exhibit a favorable distribution for comprehending various 3D objects or scenes.","Additionally, we introduce a network compatible with arbitrary voxel sizes for sampling and feature extraction while maintaining high efficiency.","Our method achieves state-of-the-art accuracy on the ShapeNetPart and ScanNet benchmarks with promising efficiency.","Code will be available at https://github.com/yhc2021/AVS-Net."],"url":"http://arxiv.org/abs/2402.17521v1","category":"cs.CV"}
{"created":"2024-02-27 13:08:26","title":"Numerical Schemes for 3-Wave Kinetic Equations: A Complete Treatment of the Collision Operator","abstract":"In our previous work, numerical schemes for a simplified version of 3-wave kinetic equations, in which only the simple forward-cascade terms of the collision operators are kept, have been successfully designed, especially to capture the long time dynamics of the equation given the multiple blow-up time phenomenon. In this second work in the series, we propose numerical treatments for the complete 3-wave kinetic equations, in which the complete, much more complicated collision operators are fully considered based on a novel conservative form of the equation. We then derive an implicit finite volume scheme to solve the equation. The new discretization uses an adaptive time-stepping method which allows for the simulations to be carried to very long times. Our computed solutions are compared with previously derived long-time asymptotic estimates for the decay rate of total energy of time-dependent solutions of 3-wave kinetic equations and found to be in excellent agreement.","sentences":["In our previous work, numerical schemes for a simplified version of 3-wave kinetic equations, in which only the simple forward-cascade terms of the collision operators are kept, have been successfully designed, especially to capture the long time dynamics of the equation given the multiple blow-up time phenomenon.","In this second work in the series, we propose numerical treatments for the complete 3-wave kinetic equations, in which the complete, much more complicated collision operators are fully considered based on a novel conservative form of the equation.","We then derive an implicit finite volume scheme to solve the equation.","The new discretization uses an adaptive time-stepping method which allows for the simulations to be carried to very long times.","Our computed solutions are compared with previously derived long-time asymptotic estimates for the decay rate of total energy of time-dependent solutions of 3-wave kinetic equations and found to be in excellent agreement."],"url":"http://arxiv.org/abs/2402.17481v1","category":"math.NA"}
{"created":"2024-02-27 10:47:24","title":"Investigating Continual Pretraining in Large Language Models: Insights and Implications","abstract":"This paper studies the evolving domain of Continual Learning (CL) in large language models (LLMs), with a focus on developing strategies for efficient and sustainable training. Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge and enhancing cross-domain knowledge transfer without relying on domain-specific identification. Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting, our research evaluates the adaptability and capabilities of LLMs to changing data landscapes in practical scenarios. To this end, we introduce a new benchmark designed to measure the adaptability of LLMs to these evolving data environments, offering a comprehensive framework for evaluation. We examine the impact of model size on learning efficacy and forgetting, as well as how the progression and similarity of emerging domains affect the knowledge transfer within these models. Our findings uncover several key insights: (i) when the sequence of domains shows semantic similarity, continual pretraining enables LLMs to better specialize in the current domain compared to stand-alone fine-tuning, (ii) training across a diverse range of domains enhances both backward and forward knowledge transfer, and (iii) smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both forgetting and learning. We posit that our research marks a shift towards establishing a more realistic benchmark for investigating CL in LLMs, and has the potential to play a key role in guiding the direction of future research in the field.","sentences":["This paper studies the evolving domain of Continual Learning (CL) in large language models (LLMs), with a focus on developing strategies for efficient and sustainable training.","Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge and enhancing cross-domain knowledge transfer without relying on domain-specific identification.","Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting, our research evaluates the adaptability and capabilities of LLMs to changing data landscapes in practical scenarios.","To this end, we introduce a new benchmark designed to measure the adaptability of LLMs to these evolving data environments, offering a comprehensive framework for evaluation.","We examine the impact of model size on learning efficacy and forgetting, as well as how the progression and similarity of emerging domains affect the knowledge transfer within these models.","Our findings uncover several key insights: (i) when the sequence of domains shows semantic similarity, continual pretraining enables LLMs to better specialize in the current domain compared to stand-alone fine-tuning, (ii) training across a diverse range of domains enhances both backward and forward knowledge transfer, and (iii) smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both forgetting and learning.","We posit that our research marks a shift towards establishing a more realistic benchmark for investigating CL in LLMs, and has the potential to play a key role in guiding the direction of future research in the field."],"url":"http://arxiv.org/abs/2402.17400v1","category":"cs.CL"}
{"created":"2024-02-27 08:20:45","title":"ArcSin: Adaptive ranged cosine Similarity injected noise for Language-Driven Visual Tasks","abstract":"In this study, we address the challenging task of bridging the modality gap between learning from language and inference for visual tasks, including Visual Question Answering (VQA), Image Captioning (IC) and Visual Entailment (VE). We train models for these tasks in a zero-shot cross-modal transfer setting, a domain where the previous state-of-the-art method relied on the fixed scale noise injection, often compromising the semantic content of the original modality embedding. To combat it, we propose a novel method called Adaptive ranged cosine Similarity injected noise (ArcSin). First, we introduce an innovative adaptive noise scale that effectively generates the textual elements with more variability while preserving the original text feature's integrity. Second, a similarity pool strategy is employed, expanding the domain generalization potential by broadening the overall noise scale. This dual strategy effectively widens the scope of the original domain while safeguarding content integrity. Our empirical results demonstrate that these models closely rival those trained on images in terms of performance. Specifically, our method exhibits substantial improvements over the previous state-of-the-art, achieving gains of 1.9 and 1.1 CIDEr points in S-Cap and M-Cap, respectively. Additionally, we observe increases of 1.5 percentage points (pp), 1.4 pp, and 1.4 pp in accuracy for VQA, VQA-E, and VE, respectively, pushing the boundaries of what is achievable within the constraints of image-trained model benchmarks. The code will be released.","sentences":["In this study, we address the challenging task of bridging the modality gap between learning from language and inference for visual tasks, including Visual Question Answering (VQA), Image Captioning (IC) and Visual Entailment (VE).","We train models for these tasks in a zero-shot cross-modal transfer setting, a domain where the previous state-of-the-art method relied on the fixed scale noise injection, often compromising the semantic content of the original modality embedding.","To combat it, we propose a novel method called Adaptive ranged cosine Similarity injected noise (ArcSin).","First, we introduce an innovative adaptive noise scale that effectively generates the textual elements with more variability while preserving the original text feature's integrity.","Second, a similarity pool strategy is employed, expanding the domain generalization potential by broadening the overall noise scale.","This dual strategy effectively widens the scope of the original domain while safeguarding content integrity.","Our empirical results demonstrate that these models closely rival those trained on images in terms of performance.","Specifically, our method exhibits substantial improvements over the previous state-of-the-art, achieving gains of 1.9 and 1.1 CIDEr points in S-Cap and M-Cap, respectively.","Additionally, we observe increases of 1.5 percentage points (pp), 1.4 pp, and 1.4 pp in accuracy for VQA, VQA-E, and VE, respectively, pushing the boundaries of what is achievable within the constraints of image-trained model benchmarks.","The code will be released."],"url":"http://arxiv.org/abs/2402.17298v1","category":"cs.CV"}
{"created":"2024-02-27 07:53:32","title":"Distribution of number of peaks within a long gamma-ray burst","abstract":"The variety of long duration gamma-ray burst (LGRB) light curves (LCs) encode a wealth of information on how LGRB engines release energy following the collapse of the progenitor star. Attempts to characterise GRB LCs focused on a number of properties, such as the minimum variability timescale, power density spectra (both ensemble average and individual), or with different definitions of variability. In parallel, a characterisation as a stochastic process was pursued by studying the distributions of waiting times, peak flux, fluence of individual peaks within GRB time profiles. Yet, the question remains as to whether the diversity of profiles can be described in terms of a common stochastic process. Here we address this issue by studying for the first time the distribution of the number of peaks in a GRB profile. We used four different GRB catalogues: CGRO/BATSE, Swift/BAT, BeppoSAX/GRBM, and Insight-HXMT. The statistically significant peaks were identified by means of well tested algorithm MEPSA and further selected by applying a set of thresholds on signal-to-noise ratio. We then extracted the corresponding distributions of number of peaks per GRB. Among the different models considered (power-law, simple or stretched exponential) only a mixture of two exponentials models all the observed distributions, suggesting the existence of two distinct behaviours: (i) an average number of 2.1+-0.1 peaks per GRB (\"peak poor\") and accounting for about 80% of the observed population of GRBs; (ii) an average number of 8.3+-1.0 peaks per GRB (\"peak rich\") and accounting for the remaining 20% of the observed population. We associate the class of peak-rich GRBs with the presence of sub-second variability, which seems to be absent among peak-poor GRBs. The two classes could result from two different regimes through which GRB engines release energy or through which energy is dissipated into gamma-rays.","sentences":["The variety of long duration gamma-ray burst (LGRB) light curves (LCs) encode a wealth of information on how LGRB engines release energy following the collapse of the progenitor star.","Attempts to characterise GRB LCs focused on a number of properties, such as the minimum variability timescale, power density spectra (both ensemble average and individual), or with different definitions of variability.","In parallel, a characterisation as a stochastic process was pursued by studying the distributions of waiting times, peak flux, fluence of individual peaks within GRB time profiles.","Yet, the question remains as to whether the diversity of profiles can be described in terms of a common stochastic process.","Here we address this issue by studying for the first time the distribution of the number of peaks in a GRB profile.","We used four different GRB catalogues: CGRO/BATSE, Swift/BAT, BeppoSAX/GRBM, and Insight-HXMT.","The statistically significant peaks were identified by means of well tested algorithm MEPSA and further selected by applying a set of thresholds on signal-to-noise ratio.","We then extracted the corresponding distributions of number of peaks per GRB.","Among the different models considered (power-law, simple or stretched exponential) only a mixture of two exponentials models all the observed distributions, suggesting the existence of two distinct behaviours: (i) an average number of 2.1+-0.1 peaks per GRB (\"peak poor\") and accounting for about 80% of the observed population of GRBs; (ii) an average number of 8.3+-1.0 peaks per GRB (\"peak rich\") and accounting for the remaining 20% of the observed population.","We associate the class of peak-rich GRBs with the presence of sub-second variability, which seems to be absent among peak-poor GRBs.","The two classes could result from two different regimes through which GRB engines release energy or through which energy is dissipated into gamma-rays."],"url":"http://arxiv.org/abs/2402.17282v1","category":"astro-ph.HE"}
{"created":"2024-02-27 07:14:12","title":"Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning","abstract":"Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring pre-trained large language models (LLMs), especially as the models' scale and the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters. However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning. We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential. The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters. This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability. We conduct a theoretical analysis and empirical studies on various NLP tasks. Our experimental results show that, compared to LoRA, MELoRA achieves better performance with 8 times fewer trainable parameters on natural language understanding tasks and 36 times fewer trainable parameters on instruction following tasks, which demonstrates the effectiveness of MELoRA.","sentences":["Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring pre-trained large language models (LLMs), especially as the models' scale and the diversity of tasks increase.","Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters.","However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning.","We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential.","The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters.","This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability.","We conduct a theoretical analysis and empirical studies on various NLP tasks.","Our experimental results show that, compared to LoRA, MELoRA achieves better performance with 8 times fewer trainable parameters on natural language understanding tasks and 36 times fewer trainable parameters on instruction following tasks, which demonstrates the effectiveness of MELoRA."],"url":"http://arxiv.org/abs/2402.17263v1","category":"cs.CL"}
{"created":"2024-02-27 06:32:56","title":"SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion Classification Using 3D Multi-Phase Imaging","abstract":"Automated classification of liver lesions in multi-phase CT and MR scans is of clinical significance but challenging. This study proposes a novel Siamese Dual-Resolution Transformer (SDR-Former) framework, specifically designed for liver lesion classification in 3D multi-phase CT and MR imaging with varying phase counts. The proposed SDR-Former utilizes a streamlined Siamese Neural Network (SNN) to process multi-phase imaging inputs, possessing robust feature representations while maintaining computational efficiency. The weight-sharing feature of the SNN is further enriched by a hybrid Dual-Resolution Transformer (DR-Former), comprising a 3D Convolutional Neural Network (CNN) and a tailored 3D Transformer for processing high- and low-resolution images, respectively. This hybrid sub-architecture excels in capturing detailed local features and understanding global contextual information, thereby, boosting the SNN's feature extraction capabilities. Additionally, a novel Adaptive Phase Selection Module (APSM) is introduced, promoting phase-specific intercommunication and dynamically adjusting each phase's influence on the diagnostic outcome. The proposed SDR-Former framework has been validated through comprehensive experiments on two clinical datasets: a three-phase CT dataset and an eight-phase MR dataset. The experimental results affirm the efficacy of the proposed framework. To support the scientific community, we are releasing our extensive multi-phase MR dataset for liver lesion analysis to the public. This pioneering dataset, being the first publicly available multi-phase MR dataset in this field, also underpins the MICCAI LLD-MMRI Challenge. The dataset is accessible at:https://bit.ly/3IyYlgN.","sentences":["Automated classification of liver lesions in multi-phase CT and MR scans is of clinical significance but challenging.","This study proposes a novel Siamese Dual-Resolution Transformer (SDR-Former) framework, specifically designed for liver lesion classification in 3D multi-phase CT and MR imaging with varying phase counts.","The proposed SDR-Former utilizes a streamlined Siamese Neural Network (SNN) to process multi-phase imaging inputs, possessing robust feature representations while maintaining computational efficiency.","The weight-sharing feature of the SNN is further enriched by a hybrid Dual-Resolution Transformer (DR-Former), comprising a 3D Convolutional Neural Network (CNN) and a tailored 3D Transformer for processing high- and low-resolution images, respectively.","This hybrid sub-architecture excels in capturing detailed local features and understanding global contextual information, thereby, boosting the SNN's feature extraction capabilities.","Additionally, a novel Adaptive Phase Selection Module (APSM) is introduced, promoting phase-specific intercommunication and dynamically adjusting each phase's influence on the diagnostic outcome.","The proposed SDR-Former framework has been validated through comprehensive experiments on two clinical datasets: a three-phase CT dataset and an eight-phase MR dataset.","The experimental results affirm the efficacy of the proposed framework.","To support the scientific community, we are releasing our extensive multi-phase MR dataset for liver lesion analysis to the public.","This pioneering dataset, being the first publicly available multi-phase MR dataset in this field, also underpins the MICCAI LLD-MMRI Challenge.","The dataset is accessible at:https://bit.ly/3IyYlgN."],"url":"http://arxiv.org/abs/2402.17246v1","category":"eess.IV"}
{"created":"2024-02-27 05:42:38","title":"Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology","abstract":"Multiple instance learning (MIL) is the most widely used framework in computational pathology, encompassing sub-typing, diagnosis, prognosis, and more. However, the existing MIL paradigm typically requires an offline instance feature extractor, such as a pre-trained ResNet or a foundation model. This approach lacks the capability for feature fine-tuning within the specific downstream tasks, limiting its adaptability and performance. To address this issue, we propose a Re-embedded Regional Transformer (R$^2$T) for re-embedding the instance features online, which captures fine-grained local features and establishes connections across different regions. Unlike existing works that focus on pre-training powerful feature extractor or designing sophisticated instance aggregator, R$^2$T is tailored to re-embed instance features online. It serves as a portable module that can seamlessly integrate into mainstream MIL models. Extensive experimental results on common computational pathology tasks validate that: 1) feature re-embedding improves the performance of MIL models based on ResNet-50 features to the level of foundation model features, and further enhances the performance of foundation model features; 2) the R$^2$T can introduce more significant performance improvements to various MIL models; 3) R$^2$T-MIL, as an R$^2$T-enhanced AB-MIL, outperforms other latest methods by a large margin. The code is available at:~\\href{https://github.com/DearCaat/RRT-MIL}{https://github.com/DearCaat/RRT-MIL}.","sentences":["Multiple instance learning (MIL) is the most widely used framework in computational pathology, encompassing sub-typing, diagnosis, prognosis, and more.","However, the existing MIL paradigm typically requires an offline instance feature extractor, such as a pre-trained ResNet or a foundation model.","This approach lacks the capability for feature fine-tuning within the specific downstream tasks, limiting its adaptability and performance.","To address this issue, we propose a Re-embedded Regional Transformer (R$^2$T) for re-embedding the instance features online, which captures fine-grained local features and establishes connections across different regions.","Unlike existing works that focus on pre-training powerful feature extractor or designing sophisticated instance aggregator, R$^2$T is tailored to re-embed instance features online.","It serves as a portable module that can seamlessly integrate into mainstream MIL models.","Extensive experimental results on common computational pathology tasks validate that: 1) feature re-embedding improves the performance of MIL models based on ResNet-50 features to the level of foundation model features, and further enhances the performance of foundation model features; 2) the R$^2$T can introduce more significant performance improvements to various MIL models; 3) R$^2$T-MIL, as an R$^2$T-enhanced AB-MIL, outperforms other latest methods by a large margin.","The code is available at:~\\href{https://github.com/DearCaat/RRT-MIL}{https://github.com/DearCaat/RRT-MIL}."],"url":"http://arxiv.org/abs/2402.17228v1","category":"cs.CV"}
{"created":"2024-02-27 05:40:36","title":"Efficient Backpropagation with Variance-Controlled Adaptive Sampling","abstract":"Sampling-based algorithms, which eliminate ''unimportant'' computations during forward and/or back propagation (BP), offer potential solutions to accelerate neural network training. However, since sampling introduces approximations to training, such algorithms may not consistently maintain accuracy across various tasks. In this work, we introduce a variance-controlled adaptive sampling (VCAS) method designed to accelerate BP. VCAS computes an unbiased stochastic gradient with fine-grained layerwise importance sampling in data dimension for activation gradient calculation and leverage score sampling in token dimension for weight gradient calculation. To preserve accuracy, we control the additional variance by learning the sample ratio jointly with model parameters during training. We assessed VCAS on multiple fine-tuning and pre-training tasks in both vision and natural language domains. On all the tasks, VCAS can preserve the original training loss trajectory and validation accuracy with an up to 73.87% FLOPs reduction of BP and 49.58% FLOPs reduction of the whole training process. The implementation is available at https://github.com/thu-ml/VCAS .","sentences":["Sampling-based algorithms, which eliminate ''unimportant'' computations during forward and/or back propagation (BP), offer potential solutions to accelerate neural network training.","However, since sampling introduces approximations to training, such algorithms may not consistently maintain accuracy across various tasks.","In this work, we introduce a variance-controlled adaptive sampling (VCAS) method designed to accelerate BP.","VCAS computes an unbiased stochastic gradient with fine-grained layerwise importance sampling in data dimension for activation gradient calculation and leverage score sampling in token dimension for weight gradient calculation.","To preserve accuracy, we control the additional variance by learning the sample ratio jointly with model parameters during training.","We assessed VCAS on multiple fine-tuning and pre-training tasks in both vision and natural language domains.","On all the tasks, VCAS can preserve the original training loss trajectory and validation accuracy with an up to 73.87% FLOPs reduction of BP and 49.58% FLOPs reduction of the whole training process.","The implementation is available at https://github.com/thu-ml/VCAS ."],"url":"http://arxiv.org/abs/2402.17227v1","category":"cs.LG"}
{"created":"2024-02-27 01:57:02","title":"SAM-DiffSR: Structure-Modulated Diffusion Model for Image Super-Resolution","abstract":"Diffusion-based super-resolution (SR) models have recently garnered significant attention due to their potent restoration capabilities. But conventional diffusion models perform noise sampling from a single distribution, constraining their ability to handle real-world scenes and complex textures across semantic regions. With the success of segment anything model (SAM), generating sufficiently fine-grained region masks can enhance the detail recovery of diffusion-based SR model. However, directly integrating SAM into SR models will result in much higher computational cost. In this paper, we propose the SAM-DiffSR model, which can utilize the fine-grained structure information from SAM in the process of sampling noise to improve the image quality without additional computational cost during inference. In the process of training, we encode structural position information into the segmentation mask from SAM. Then the encoded mask is integrated into the forward diffusion process by modulating it to the sampled noise. This adjustment allows us to independently adapt the noise mean within each corresponding segmentation area. The diffusion model is trained to estimate this modulated noise. Crucially, our proposed framework does NOT change the reverse diffusion process and does NOT require SAM at inference. Experimental results demonstrate the effectiveness of our proposed method, showcasing superior performance in suppressing artifacts, and surpassing existing diffusion-based methods by 0.74 dB at the maximum in terms of PSNR on DIV2K dataset. The code and dataset are available at https://github.com/lose4578/SAM-DiffSR.","sentences":["Diffusion-based super-resolution (SR) models have recently garnered significant attention due to their potent restoration capabilities.","But conventional diffusion models perform noise sampling from a single distribution, constraining their ability to handle real-world scenes and complex textures across semantic regions.","With the success of segment anything model (SAM), generating sufficiently fine-grained region masks can enhance the detail recovery of diffusion-based SR model.","However, directly integrating SAM into SR models will result in much higher computational cost.","In this paper, we propose the SAM-DiffSR model, which can utilize the fine-grained structure information from SAM in the process of sampling noise to improve the image quality without additional computational cost during inference.","In the process of training, we encode structural position information into the segmentation mask from SAM.","Then the encoded mask is integrated into the forward diffusion process by modulating it to the sampled noise.","This adjustment allows us to independently adapt the noise mean within each corresponding segmentation area.","The diffusion model is trained to estimate this modulated noise.","Crucially, our proposed framework does NOT change the reverse diffusion process and does NOT require SAM at inference.","Experimental results demonstrate the effectiveness of our proposed method, showcasing superior performance in suppressing artifacts, and surpassing existing diffusion-based methods by 0.74 dB at the maximum in terms of PSNR on DIV2K dataset.","The code and dataset are available at https://github.com/lose4578/SAM-DiffSR."],"url":"http://arxiv.org/abs/2402.17133v1","category":"cs.CV"}
{"created":"2024-02-27 01:15:28","title":"SwiftCache: Model-Based Learning for Dynamic Content Caching in CDNs","abstract":"We introduce SwiftCache, a \"fresh\" learning-based caching framework designed for content distribution networks (CDNs) featuring distributed front-end local caches and a dynamic back-end database. Users prefer the most recent version of the dynamically updated content, while the local caches lack knowledge of item popularity and refresh rates. We first explore scenarios with requests arriving at a local cache following a Poisson process, whereby we prove that the optimal policy features a threshold-based structure with updates occurring solely at request arrivals. Leveraging these findings, SwiftCache is proposed as a model-based learning framework for dynamic content caching. The simulation demonstrates near-optimal cost for Poisson process arrivals and strong performance with limited cache sizes. For more general environments, we present a model-free Reinforcement Learning (RL) based caching policy without prior statistical assumptions. The model-based policy performs well compared to the model-free policy when the variance of interarrival times remains moderate. However, as the variance increases, RL slightly outperforms model-based learning at the cost of longer training times, and higher computational resource consumption. Model-based learning's adaptability to environmental changes without retraining positions it as a practical choice for dynamic network environments. Distributed edge caches can utilize this approach in a decentralized manner to effectively meet the evolving behaviors of users.","sentences":["We introduce SwiftCache, a \"fresh\" learning-based caching framework designed for content distribution networks (CDNs) featuring distributed front-end local caches and a dynamic back-end database.","Users prefer the most recent version of the dynamically updated content, while the local caches lack knowledge of item popularity and refresh rates.","We first explore scenarios with requests arriving at a local cache following a Poisson process, whereby we prove that the optimal policy features a threshold-based structure with updates occurring solely at request arrivals.","Leveraging these findings, SwiftCache is proposed as a model-based learning framework for dynamic content caching.","The simulation demonstrates near-optimal cost for Poisson process arrivals and strong performance with limited cache sizes.","For more general environments, we present a model-free Reinforcement Learning (RL) based caching policy without prior statistical assumptions.","The model-based policy performs well compared to the model-free policy when the variance of interarrival times remains moderate.","However, as the variance increases, RL slightly outperforms model-based learning at the cost of longer training times, and higher computational resource consumption.","Model-based learning's adaptability to environmental changes without retraining positions it as a practical choice for dynamic network environments.","Distributed edge caches can utilize this approach in a decentralized manner to effectively meet the evolving behaviors of users."],"url":"http://arxiv.org/abs/2402.17111v1","category":"math.OC"}
{"created":"2024-02-26 23:37:59","title":"A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI Judge","abstract":"This study presents an innovative enhancement to retrieval-augmented generation (RAG) systems by seamlessly integrating fine-tuned large language models (LLMs) with vector databases. This integration capitalizes on the combined strengths of structured data retrieval and the nuanced comprehension provided by advanced LLMs. Central to our approach are the LoRA and QLoRA methodologies, which stand at the forefront of model refinement through parameter-efficient fine-tuning and memory optimization. A novel feature of our research is the incorporation of user feedback directly into the training process, ensuring the model's continuous adaptation to user expectations and thus, improving its performance and applicability. Additionally, we introduce a Quantized Influence Measure (QIM) as an innovative \"AI Judge\" mechanism to enhance the precision of result selection, further refining the system's accuracy. Accompanied by an executive diagram and a detailed algorithm for fine-tuning QLoRA, our work provides a comprehensive framework for implementing these advancements within chatbot technologies. This research contributes significant insights into LLM optimization for specific uses and heralds new directions for further development in retrieval-augmented models. Through extensive experimentation and analysis, our findings lay a robust foundation for future advancements in chatbot technology and retrieval systems, marking a significant step forward in the creation of more sophisticated, precise, and user-centric conversational AI systems.","sentences":["This study presents an innovative enhancement to retrieval-augmented generation (RAG) systems by seamlessly integrating fine-tuned large language models (LLMs) with vector databases.","This integration capitalizes on the combined strengths of structured data retrieval and the nuanced comprehension provided by advanced LLMs.","Central to our approach are the LoRA and QLoRA methodologies, which stand at the forefront of model refinement through parameter-efficient fine-tuning and memory optimization.","A novel feature of our research is the incorporation of user feedback directly into the training process, ensuring the model's continuous adaptation to user expectations and thus, improving its performance and applicability.","Additionally, we introduce a Quantized Influence Measure (QIM) as an innovative \"AI Judge\" mechanism to enhance the precision of result selection, further refining the system's accuracy.","Accompanied by an executive diagram and a detailed algorithm for fine-tuning QLoRA, our work provides a comprehensive framework for implementing these advancements within chatbot technologies.","This research contributes significant insights into LLM optimization for specific uses and heralds new directions for further development in retrieval-augmented models.","Through extensive experimentation and analysis, our findings lay a robust foundation for future advancements in chatbot technology and retrieval systems, marking a significant step forward in the creation of more sophisticated, precise, and user-centric conversational AI systems."],"url":"http://arxiv.org/abs/2402.17081v1","category":"cs.IR"}
{"created":"2024-02-26 22:16:06","title":"Extension of the Best Polynomial Operator in Generalized Orlicz spaces","abstract":"In this paper, we consider the best multivalued polynomial approximation operator for functions in an Orlicz Space $L^{\\varphi}(\\Omega)$. We obtain its characterization involving $\\psi^-$ and $\\psi^+$, which are the left and right derivatives functions of $\\varphi$. And then, we extend the operator to $L^{\\psi^+}(\\Omega)$. We also get pointwise convergence of this extension, where the Calder\\'on-Zygmund class $t_m^p (x)$ adapted to $L^{\\psi^+}(\\Omega)$ plays an important role.","sentences":["In this paper, we consider the best multivalued polynomial approximation operator for functions in an Orlicz Space $L^{\\varphi}(\\Omega)$.","We obtain its characterization involving $\\psi^-$ and $\\psi^+$, which are the left and right derivatives functions of $\\varphi$. And then, we extend the operator to $L^{\\psi^+}(\\Omega)$. We also get pointwise convergence of this extension, where the Calder\\'on-Zygmund class $t_m^p (x)$ adapted to $L^{\\psi^+}(\\Omega)$ plays an important role."],"url":"http://arxiv.org/abs/2402.17048v1","category":"math.CA"}
{"created":"2024-02-26 22:04:07","title":"Mind the Gap: Nonlocal Cascades and Preferential Heating in High-$\u03b2$ Alfv\u00e9nic Turbulence","abstract":"Characterizing the thermodynamics of turbulent plasmas is key to decoding observable signatures from astrophysical systems. In magnetohydrodynamic (MHD) turbulence, nonlinear interactions between counter-propagating Alfv\\'en waves cascade energy to smaller spatial scales where dissipation heats the protons and electrons. When the thermal pressure far exceeds the magnetic pressure, linear theory predicts a spectral gap at perpendicular scales near the proton gyroradius where Alfv\\'en waves become non-propagating. For simple models of an MHD turbulent cascade that assume only local nonlinear interactions, the cascade halts at this gap, preventing energy from reaching smaller scales where electron dissipation dominates, leading to an overestimate of the proton heating rate. In this work, we demonstrate that nonlocal contributions to the cascade, specifically large scale shearing and small scale diffusion, can bridge the non-propagating gap, allowing the cascade to continue to smaller scales. We provide an updated functional form for the proton-to-electron heating ratio accounting for this nonlocal energy transfer by evaluating a nonlocal weakened cascade model over a range of temperature and pressure ratios. In plasmas where the thermal pressure dominates the magnetic pressure, we observe that the proton heating is moderated compared to the significant enhancement predicted by local models.","sentences":["Characterizing the thermodynamics of turbulent plasmas is key to decoding observable signatures from astrophysical systems.","In magnetohydrodynamic (MHD) turbulence, nonlinear interactions between counter-propagating Alfv\\'en waves cascade energy to smaller spatial scales where dissipation heats the protons and electrons.","When the thermal pressure far exceeds the magnetic pressure, linear theory predicts a spectral gap at perpendicular scales near the proton gyroradius where Alfv\\'en waves become non-propagating.","For simple models of an MHD turbulent cascade that assume only local nonlinear interactions, the cascade halts at this gap, preventing energy from reaching smaller scales where electron dissipation dominates, leading to an overestimate of the proton heating rate.","In this work, we demonstrate that nonlocal contributions to the cascade, specifically large scale shearing and small scale diffusion, can bridge the non-propagating gap, allowing the cascade to continue to smaller scales.","We provide an updated functional form for the proton-to-electron heating ratio accounting for this nonlocal energy transfer by evaluating a nonlocal weakened cascade model over a range of temperature and pressure ratios.","In plasmas where the thermal pressure dominates the magnetic pressure, we observe that the proton heating is moderated compared to the significant enhancement predicted by local models."],"url":"http://arxiv.org/abs/2402.17044v1","category":"physics.space-ph"}
{"created":"2024-02-26 22:03:24","title":"Traffic Control via Connected and Automated Vehicles: An Open-Road Field Experiment with 100 CAVs","abstract":"The CIRCLES project aims to reduce instabilities in traffic flow, which are naturally occurring phenomena due to human driving behavior. These \"phantom jams\" or \"stop-and-go waves,\"are a significant source of wasted energy. Toward this goal, the CIRCLES project designed a control system referred to as the MegaController by the CIRCLES team, that could be deployed in real traffic. Our field experiment leveraged a heterogeneous fleet of 100 longitudinally-controlled vehicles as Lagrangian traffic actuators, each of which ran a controller with the architecture described in this paper. The MegaController is a hierarchical control architecture, which consists of two main layers. The upper layer is called Speed Planner, and is a centralized optimal control algorithm. It assigns speed targets to the vehicles, conveyed through the LTE cellular network. The lower layer is a control layer, running on each vehicle. It performs local actuation by overriding the stock adaptive cruise controller, using the stock on-board sensors. The Speed Planner ingests live data feeds provided by third parties, as well as data from our own control vehicles, and uses both to perform the speed assignment. The architecture of the speed planner allows for modular use of standard control techniques, such as optimal control, model predictive control, kernel methods and others, including Deep RL, model predictive control and explicit controllers. Depending on the vehicle architecture, all onboard sensing data can be accessed by the local controllers, or only some. Control inputs vary across different automakers, with inputs ranging from torque or acceleration requests for some cars, and electronic selection of ACC set points in others. The proposed architecture allows for the combination of all possible settings proposed above. Most configurations were tested throughout the ramp up to the MegaVandertest.","sentences":["The CIRCLES project aims to reduce instabilities in traffic flow, which are naturally occurring phenomena due to human driving behavior.","These \"phantom jams\" or \"stop-and-go waves,\"are a significant source of wasted energy.","Toward this goal, the CIRCLES project designed a control system referred to as the MegaController by the CIRCLES team, that could be deployed in real traffic.","Our field experiment leveraged a heterogeneous fleet of 100 longitudinally-controlled vehicles as Lagrangian traffic actuators, each of which ran a controller with the architecture described in this paper.","The MegaController is a hierarchical control architecture, which consists of two main layers.","The upper layer is called Speed Planner, and is a centralized optimal control algorithm.","It assigns speed targets to the vehicles, conveyed through the LTE cellular network.","The lower layer is a control layer, running on each vehicle.","It performs local actuation by overriding the stock adaptive cruise controller, using the stock on-board sensors.","The Speed Planner ingests live data feeds provided by third parties, as well as data from our own control vehicles, and uses both to perform the speed assignment.","The architecture of the speed planner allows for modular use of standard control techniques, such as optimal control, model predictive control, kernel methods and others, including Deep RL, model predictive control and explicit controllers.","Depending on the vehicle architecture, all onboard sensing data can be accessed by the local controllers, or only some.","Control inputs vary across different automakers, with inputs ranging from torque or acceleration requests for some cars, and electronic selection of ACC set points in others.","The proposed architecture allows for the combination of all possible settings proposed above.","Most configurations were tested throughout the ramp up to the MegaVandertest."],"url":"http://arxiv.org/abs/2402.17043v1","category":"eess.SY"}
{"created":"2024-02-26 20:57:35","title":"Deep Learning Algorithms Used in Intrusion Detection Systems -- A Review","abstract":"The increase in network attacks has necessitated the development of robust and efficient intrusion detection systems (IDS) capable of identifying malicious activities in real-time. In the last five years, deep learning algorithms have emerged as powerful tools in this domain, offering enhanced detection capabilities compared to traditional methods. This review paper studies recent advancements in the application of deep learning techniques, including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Deep Belief Networks (DBN), Deep Neural Networks (DNN), Long Short-Term Memory (LSTM), autoencoders (AE), Multi-Layer Perceptrons (MLP), Self-Normalizing Networks (SNN) and hybrid models, within network intrusion detection systems. we delve into the unique architectures, training models, and classification methodologies tailored for network traffic analysis and anomaly detection. Furthermore, we analyze the strengths and limitations of each deep learning approach in terms of detection accuracy, computational efficiency, scalability, and adaptability to evolving threats. Additionally, this paper highlights prominent datasets and benchmarking frameworks commonly utilized for evaluating the performance of deep learning-based IDS. This review will provide researchers and industry practitioners with valuable insights into the state-of-the-art deep learning algorithms for enhancing the security framework of network environments through intrusion detection.","sentences":["The increase in network attacks has necessitated the development of robust and efficient intrusion detection systems (IDS) capable of identifying malicious activities in real-time.","In the last five years, deep learning algorithms have emerged as powerful tools in this domain, offering enhanced detection capabilities compared to traditional methods.","This review paper studies recent advancements in the application of deep learning techniques, including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Deep Belief Networks (DBN), Deep Neural Networks (DNN), Long Short-Term Memory (LSTM), autoencoders (AE), Multi-Layer Perceptrons (MLP), Self-Normalizing Networks (SNN) and hybrid models, within network intrusion detection systems.","we delve into the unique architectures, training models, and classification methodologies tailored for network traffic analysis and anomaly detection.","Furthermore, we analyze the strengths and limitations of each deep learning approach in terms of detection accuracy, computational efficiency, scalability, and adaptability to evolving threats.","Additionally, this paper highlights prominent datasets and benchmarking frameworks commonly utilized for evaluating the performance of deep learning-based IDS.","This review will provide researchers and industry practitioners with valuable insights into the state-of-the-art deep learning algorithms for enhancing the security framework of network environments through intrusion detection."],"url":"http://arxiv.org/abs/2402.17020v1","category":"cs.CR"}
{"created":"2024-02-26 19:04:57","title":"Imaging Spectropolarimetry -- A New Observing Mode on the Hubble Space Telescope's Advanced Camera for Surveys","abstract":"Imaging spectropolarimetry is a new observing mode on the Advanced Camera for Surveys (ACS) aboard the Hubble Space Telescope (HST) that was commissioned in Cycle 30 and is available to HST observers starting in Cycle 31 (i.e., from 2023). It is a technique that is accessible from ground-based observatories, but the superb spatial resolution afforded by HST/ACS combined with the slitless nature of HST/ACS grism spectroscopy opens up the possibility of studying polarized extended emission in a way that is not currently possible even with Adaptive Optics facilities on the ground. This mode could help to study interesting targets including (but not limited to) QSOs, AGN and Radio Galaxies, ISM Dust Properties, Pre-Planetary Nebulae, Proto-Planetary and Debris Disks, Supernovae/Supernova Remnants, and Solar System objects. This research note presents the preliminary results from the calibration programs used to calibrate imaging spectropolarimetry on HST/ACS.","sentences":["Imaging spectropolarimetry is a new observing mode on the Advanced Camera for Surveys (ACS) aboard the Hubble Space Telescope (HST) that was commissioned in Cycle 30 and is available to HST observers starting in Cycle 31 (i.e., from 2023).","It is a technique that is accessible from ground-based observatories, but the superb spatial resolution afforded by HST/ACS combined with the slitless nature of HST/ACS grism spectroscopy opens up the possibility of studying polarized extended emission in a way that is not currently possible even with Adaptive Optics facilities on the ground.","This mode could help to study interesting targets including (but not limited to) QSOs, AGN and Radio Galaxies, ISM Dust Properties, Pre-Planetary Nebulae, Proto-Planetary and Debris Disks, Supernovae/Supernova Remnants, and Solar System objects.","This research note presents the preliminary results from the calibration programs used to calibrate imaging spectropolarimetry on HST/ACS."],"url":"http://arxiv.org/abs/2402.16967v1","category":"astro-ph.IM"}
{"created":"2024-02-26 18:59:12","title":"Asymmetry in Low-Rank Adapters of Foundation Models","abstract":"Parameter-efficient fine-tuning optimizes large, pre-trained foundation models by updating a subset of parameters; in this class, Low-Rank Adaptation (LoRA) is particularly effective. Inspired by an effort to investigate the different roles of LoRA matrices during fine-tuning, this paper characterizes and leverages unexpected asymmetry in the importance of low-rank adapter matrices. Specifically, when updating the parameter matrices of a neural network by adding a product $BA$, we observe that the $B$ and $A$ matrices have distinct functions: $A$ extracts features from the input, while $B$ uses these features to create the desired output. Based on this observation, we demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning $A$, and that a random untrained $A$ should perform nearly as well as a fine-tuned one. Using an information-theoretic lens, we also bound the generalization of low-rank adapters, showing that the parameter savings of exclusively training $B$ improves the bound. We support our conclusions with experiments on RoBERTa, BART-Large, LLaMA-2, and ViTs.","sentences":["Parameter-efficient fine-tuning optimizes large, pre-trained foundation models by updating a subset of parameters; in this class, Low-Rank Adaptation (LoRA) is particularly effective.","Inspired by an effort to investigate the different roles of LoRA matrices during fine-tuning, this paper characterizes and leverages unexpected asymmetry in the importance of low-rank adapter matrices.","Specifically, when updating the parameter matrices of a neural network by adding a product $BA$, we observe that the $B$ and $A$ matrices have distinct functions: $A$ extracts features from the input, while $B$ uses these features to create the desired output.","Based on this observation, we demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning $A$, and that a random untrained $A$ should perform nearly as well as a fine-tuned one.","Using an information-theoretic lens, we also bound the generalization of low-rank adapters, showing that the parameter savings of exclusively training $B$ improves the bound.","We support our conclusions with experiments on RoBERTa, BART-Large, LLaMA-2, and ViTs."],"url":"http://arxiv.org/abs/2402.16842v2","category":"cs.LG"}
{"created":"2024-02-26 18:51:15","title":"Weighted Monte Carlo augmented spherical Fourier-Bessel convolutional layers for 3D abdominal organ segmentation","abstract":"Filter-decomposition-based group equivariant convolutional neural networks show promising stability and data efficiency for 3D image feature extraction. However, the existing filter-decomposition-based 3D group equivariant neural networks rely on parameter-sharing designs and are mostly limited to rotation transform groups, where the chosen spherical harmonic filter bases consider only angular orthogonality. These limitations hamper its application to deep neural network architectures for medical image segmentation. To address these issues, this paper describes a non-parameter-sharing affine group equivariant neural network for 3D medical image segmentation based on an adaptive aggregation of Monte Carlo augmented spherical Fourier Bessel filter bases. The efficiency and flexibility of the adopted non-parameter strategy enable for the first time an efficient implementation of 3D affine group equivariant convolutional neural networks for volumetric data. The introduced spherical Bessel Fourier filter basis combines both angular and radial orthogonality for better feature extraction. The 3D image segmentation experiments on two abdominal image sets, BTCV and the NIH Pancreas datasets, show that the proposed methods excel the state-of-the-art 3D neural networks with high training stability and data efficiency. The code will be available at https://github.com/ZhaoWenzhao/WVMS.","sentences":["Filter-decomposition-based group equivariant convolutional neural networks show promising stability and data efficiency for 3D image feature extraction.","However, the existing filter-decomposition-based 3D group equivariant neural networks rely on parameter-sharing designs and are mostly limited to rotation transform groups, where the chosen spherical harmonic filter bases consider only angular orthogonality.","These limitations hamper its application to deep neural network architectures for medical image segmentation.","To address these issues, this paper describes a non-parameter-sharing affine group equivariant neural network for 3D medical image segmentation based on an adaptive aggregation of Monte Carlo augmented spherical Fourier Bessel filter bases.","The efficiency and flexibility of the adopted non-parameter strategy enable for the first time an efficient implementation of 3D affine group equivariant convolutional neural networks for volumetric data.","The introduced spherical Bessel Fourier filter basis combines both angular and radial orthogonality for better feature extraction.","The 3D image segmentation experiments on two abdominal image sets, BTCV and the NIH Pancreas datasets, show that the proposed methods excel the state-of-the-art 3D neural networks with high training stability and data efficiency.","The code will be available at https://github.com/ZhaoWenzhao/WVMS."],"url":"http://arxiv.org/abs/2402.16825v2","category":"cs.CV"}
{"created":"2024-02-26 17:07:51","title":"Asymptotic-preserving and energy stable dynamical low-rank approximation for thermal radiative transfer equations","abstract":"The thermal radiative transfer equations model temperature evolution through a background medium as a result of radiation. When a large number of particles are absorbed in a short time scale, the dynamics tend to a non-linear diffusion-type equation called the Rosseland approximation. The main challenges for constructing numerical schemes that exhibit the correct limiting behavior are posed by the solution's high-dimensional phase space and multi-scale effects. In this work, we propose an asymptotic-preserving and rank-adaptive dynamical low-rank approximation scheme based on the macro-micro decomposition of the particle density and a modified augmented basis-update \\& Galerkin integrator. We show that this scheme, for linear particle emission by the material, dissipates energy over time under a step size restriction that captures the hyperbolic and parabolic CFL conditions. We demonstrate the efficacy of the proposed method in a series of numerical experiments.","sentences":["The thermal radiative transfer equations model temperature evolution through a background medium as a result of radiation.","When a large number of particles are absorbed in a short time scale, the dynamics tend to a non-linear diffusion-type equation called the Rosseland approximation.","The main challenges for constructing numerical schemes that exhibit the correct limiting behavior are posed by the solution's high-dimensional phase space and multi-scale effects.","In this work, we propose an asymptotic-preserving and rank-adaptive dynamical low-rank approximation scheme based on the macro-micro decomposition of the particle density and a modified augmented basis-update \\& Galerkin integrator.","We show that this scheme, for linear particle emission by the material, dissipates energy over time under a step size restriction that captures the hyperbolic and parabolic CFL conditions.","We demonstrate the efficacy of the proposed method in a series of numerical experiments."],"url":"http://arxiv.org/abs/2402.16746v1","category":"math.NA"}
{"created":"2024-02-26 16:50:08","title":"Auto Tuning for OpenMP Dynamic Scheduling applied to FWI","abstract":"Because Full Waveform Inversion (FWI) works with a massive amount of data, its execution requires much time and computational resources, being restricted to large-scale computer systems such as supercomputers. Techniques such as FWI adapt well to parallel computing and can be parallelized in shared memory systems using the application programming interface (API) OpenMP. The management of parallel tasks can be performed through loop schedulers contained in OpenMP. The dynamic scheduler stands out for distributing predefined fixed-size chunk sizes to idle processing cores at runtime. It can better adapt to FWI, where data processing can be irregular. However, the relationship between the size of the chunk size and the runtime is unknown. Optimization techniques can employ meta-heuristics to explore the parameter search space, avoiding testing all possible solutions. Here, we propose a strategy to use the Parameter Auto Tuning for Shared Memory Algorithms (PATSMA), with Coupled Simulated Annealing (CSA) as its optimization method, to automatically adjust the chunk size for the dynamic scheduling of wave propagation, one of the most expensive steps in FWI. Since testing each candidate chunk size in the complete FWI is unpractical, our approach consists of running a PATSMA where the objective function is the runtime of the first time iteration of the first seismic shot of the first FWI iteration. The resulting chunk size is then employed in all wave propagations involved in an FWI. We conducted tests to measure the runtime of an FWI using the proposed autotuning, varying the problem size and running on different computational environments, such as supercomputers and cloud computing instances. The results show that applying the proposed autotuning in an FWI reduces its runtime by up to 70.46% compared to standard OpenMP schedulers.","sentences":["Because Full Waveform Inversion (FWI) works with a massive amount of data, its execution requires much time and computational resources, being restricted to large-scale computer systems such as supercomputers.","Techniques such as FWI adapt well to parallel computing and can be parallelized in shared memory systems using the application programming interface (API) OpenMP.","The management of parallel tasks can be performed through loop schedulers contained in OpenMP.","The dynamic scheduler stands out for distributing predefined fixed-size chunk sizes to idle processing cores at runtime.","It can better adapt to FWI, where data processing can be irregular.","However, the relationship between the size of the chunk size and the runtime is unknown.","Optimization techniques can employ meta-heuristics to explore the parameter search space, avoiding testing all possible solutions.","Here, we propose a strategy to use the Parameter Auto Tuning for Shared Memory Algorithms (PATSMA), with Coupled Simulated Annealing (CSA) as its optimization method, to automatically adjust the chunk size for the dynamic scheduling of wave propagation, one of the most expensive steps in FWI.","Since testing each candidate chunk size in the complete FWI is unpractical, our approach consists of running a PATSMA where the objective function is the runtime of the first time iteration of the first seismic shot of the first FWI iteration.","The resulting chunk size is then employed in all wave propagations involved in an FWI.","We conducted tests to measure the runtime of an FWI using the proposed autotuning, varying the problem size and running on different computational environments, such as supercomputers and cloud computing instances.","The results show that applying the proposed autotuning in an FWI reduces its runtime by up to 70.46% compared to standard OpenMP schedulers."],"url":"http://arxiv.org/abs/2402.16728v1","category":"cs.DC"}
{"created":"2024-02-26 16:48:34","title":"Modeling error correction with Lindblad dynamics and approximate channels","abstract":"We analyze the performance of a quantum error correction code subject to physically-motivated noise modeled by a Lindblad master equation. Working within the code-capacity framework, we consider dissipative and coherent single-qubit terms and two-qubit crosstalk, studying how different approximations of the noise capture the performance of the five-qubit code. A composite-channel approximation where every noise term is considered separately, captures the behavior in many physical cases up to considerably-long timescales, and we analyze its eventual failure due to the effect of noncommuting terms. In contrast, we find that single-qubit approximations do not properly capture the error correction dynamics with two-qubit noise, even for short times. A Pauli approximation going beyond a single-qubit channel, is sensitive to the details of the noise, state, and decoder, and succeeds in many cases at short timescales relative to the noise strength, beyond which it fails. We calculate the code pseudo-threshold emerging within this model, and demonstrate how knowledge of the qubit parameters and connectivity can be used to design better decoders. These results shed light on the performance of error correction codes in the presence of realistic noise and can advance the ongoing efforts toward useful quantum error correction.","sentences":["We analyze the performance of a quantum error correction code subject to physically-motivated noise modeled by a Lindblad master equation.","Working within the code-capacity framework, we consider dissipative and coherent single-qubit terms and two-qubit crosstalk, studying how different approximations of the noise capture the performance of the five-qubit code.","A composite-channel approximation where every noise term is considered separately, captures the behavior in many physical cases up to considerably-long timescales, and we analyze its eventual failure due to the effect of noncommuting terms.","In contrast, we find that single-qubit approximations do not properly capture the error correction dynamics with two-qubit noise, even for short times.","A Pauli approximation going beyond a single-qubit channel, is sensitive to the details of the noise, state, and decoder, and succeeds in many cases at short timescales relative to the noise strength, beyond which it fails.","We calculate the code pseudo-threshold emerging within this model, and demonstrate how knowledge of the qubit parameters and connectivity can be used to design better decoders.","These results shed light on the performance of error correction codes in the presence of realistic noise and can advance the ongoing efforts toward useful quantum error correction."],"url":"http://arxiv.org/abs/2402.16727v1","category":"quant-ph"}
{"created":"2024-02-26 16:24:39","title":"Performance of high-order Godunov-type methods in simulations of astrophysical low Mach number flows","abstract":"High-order Godunov methods for gas dynamics have become a standard tool for simulating different classes of astrophysical flows. Their accuracy is mostly determined by the spatial interpolant used to reconstruct the pair of Riemann states at cell interfaces and by the Riemann solver that computes the interface fluxes. In most Godunov-type methods, these two steps can be treated independently, so that many different schemes can in principle be built from the same numerical framework. In this work, we use our fully compressible Seven-League Hydro (SLH) code to test the accuracy of six reconstruction methods and three approximate Riemann solvers on two- and three-dimensional (2D and 3D) problems involving subsonic flows only. We consider Mach numbers in the range from $10^{-3}$ to $10^{-1}$ in a well-posed, 2D, Kelvin--Helmholtz instability problem and a 3D turbulent convection zone that excites internal gravity waves in an overlying stable layer. We find that (i) there is a spread of almost four orders of magnitude in computational cost per fixed accuracy between the methods tested in this study, with the most performant method being a combination of a \"low-dissipation\" Riemann solver and a sextic reconstruction scheme, (ii) the low-dissipation solver always outperforms conventional Riemann solvers on a fixed grid when the reconstruction scheme is kept the same, (iii) in simulations of turbulent flows, increasing the order of spatial reconstruction reduces the characteristic dissipation length scale achieved on a given grid even if the overall scheme is only second order accurate, (iv) reconstruction methods based on slope-limiting techniques tend to generate artificial, high-frequency acoustic waves during the evolution of the flow, (v) unlimited reconstruction methods introduce oscillations in the thermal stratification near the convective boundary, where the entropy gradient is steep.","sentences":["High-order Godunov methods for gas dynamics have become a standard tool for simulating different classes of astrophysical flows.","Their accuracy is mostly determined by the spatial interpolant used to reconstruct the pair of Riemann states at cell interfaces and by the Riemann solver that computes the interface fluxes.","In most Godunov-type methods, these two steps can be treated independently, so that many different schemes can in principle be built from the same numerical framework.","In this work, we use our fully compressible Seven-League Hydro (SLH) code to test the accuracy of six reconstruction methods and three approximate Riemann solvers on two-","and three-dimensional (2D and 3D) problems involving subsonic flows only.","We consider Mach numbers in the range from $10^{-3}$ to $10^{-1}$ in a well-posed, 2D, Kelvin--Helmholtz instability problem and a 3D turbulent convection zone that excites internal gravity waves in an overlying stable layer.","We find that (i) there is a spread of almost four orders of magnitude in computational cost per fixed accuracy between the methods tested in this study, with the most performant method being a combination of a \"low-dissipation\" Riemann solver and a sextic reconstruction scheme, (ii) the low-dissipation solver always outperforms conventional Riemann solvers on a fixed grid when the reconstruction scheme is kept the same, (iii) in simulations of turbulent flows, increasing the order of spatial reconstruction reduces the characteristic dissipation length scale achieved on a given grid even if the overall scheme is only second order accurate, (iv) reconstruction methods based on slope-limiting techniques tend to generate artificial, high-frequency acoustic waves during the evolution of the flow, (v) unlimited reconstruction methods introduce oscillations in the thermal stratification near the convective boundary, where the entropy gradient is steep."],"url":"http://arxiv.org/abs/2402.16706v1","category":"astro-ph.SR"}
{"created":"2024-02-26 15:59:38","title":"Enhancing Continuous Domain Adaptation with Multi-Path Transfer Curriculum","abstract":"Addressing the large distribution gap between training and testing data has long been a challenge in machine learning, giving rise to fields such as transfer learning and domain adaptation. Recently, Continuous Domain Adaptation (CDA) has emerged as an effective technique, closing this gap by utilizing a series of intermediate domains. This paper contributes a novel CDA method, W-MPOT, which rigorously addresses the domain ordering and error accumulation problems overlooked by previous studies. Specifically, we construct a transfer curriculum over the source and intermediate domains based on Wasserstein distance, motivated by theoretical analysis of CDA. Then we transfer the source model to the target domain through multiple valid paths in the curriculum using a modified version of continuous optimal transport. A bidirectional path consistency constraint is introduced to mitigate the impact of accumulated mapping errors during continuous transfer. We extensively evaluate W-MPOT on multiple datasets, achieving up to 54.1\\% accuracy improvement on multi-session Alzheimer MR image classification and 94.7\\% MSE reduction on battery capacity estimation.","sentences":["Addressing the large distribution gap between training and testing data has long been a challenge in machine learning, giving rise to fields such as transfer learning and domain adaptation.","Recently, Continuous Domain Adaptation (CDA) has emerged as an effective technique, closing this gap by utilizing a series of intermediate domains.","This paper contributes a novel CDA method, W-MPOT, which rigorously addresses the domain ordering and error accumulation problems overlooked by previous studies.","Specifically, we construct a transfer curriculum over the source and intermediate domains based on Wasserstein distance, motivated by theoretical analysis of CDA.","Then we transfer the source model to the target domain through multiple valid paths in the curriculum using a modified version of continuous optimal transport.","A bidirectional path consistency constraint is introduced to mitigate the impact of accumulated mapping errors during continuous transfer.","We extensively evaluate W-MPOT on multiple datasets, achieving up to 54.1\\% accuracy improvement on multi-session Alzheimer MR image classification and 94.7\\% MSE reduction on battery capacity estimation."],"url":"http://arxiv.org/abs/2402.16681v1","category":"cs.LG"}
{"created":"2024-02-26 15:51:45","title":"ConSept: Continual Semantic Segmentation via Adapter-based Vision Transformer","abstract":"In this paper, we delve into the realm of vision transformers for continual semantic segmentation, a problem that has not been sufficiently explored in previous literature. Empirical investigations on the adaptation of existing frameworks to vanilla ViT reveal that incorporating visual adapters into ViTs or fine-tuning ViTs with distillation terms is advantageous for enhancing the segmentation capability of novel classes. These findings motivate us to propose Continual semantic Segmentation via Adapter-based ViT, namely ConSept. Within the simplified architecture of ViT with linear segmentation head, ConSept integrates lightweight attention-based adapters into vanilla ViTs. Capitalizing on the feature adaptation abilities of these adapters, ConSept not only retains superior segmentation ability for old classes, but also attains promising segmentation quality for novel classes. To further harness the intrinsic anti-catastrophic forgetting ability of ConSept and concurrently enhance the segmentation capabilities for both old and new classes, we propose two key strategies: distillation with a deterministic old-classes boundary for improved anti-catastrophic forgetting, and dual dice losses to regularize segmentation maps, thereby improving overall segmentation performance. Extensive experiments show the effectiveness of ConSept on multiple continual semantic segmentation benchmarks under overlapped or disjoint settings. Code will be publicly available at \\url{https://github.com/DongSky/ConSept}.","sentences":["In this paper, we delve into the realm of vision transformers for continual semantic segmentation, a problem that has not been sufficiently explored in previous literature.","Empirical investigations on the adaptation of existing frameworks to vanilla ViT reveal that incorporating visual adapters into ViTs or fine-tuning ViTs with distillation terms is advantageous for enhancing the segmentation capability of novel classes.","These findings motivate us to propose Continual semantic Segmentation via Adapter-based ViT, namely ConSept.","Within the simplified architecture of ViT with linear segmentation head, ConSept integrates lightweight attention-based adapters into vanilla ViTs.","Capitalizing on the feature adaptation abilities of these adapters, ConSept not only retains superior segmentation ability for old classes, but also attains promising segmentation quality for novel classes.","To further harness the intrinsic anti-catastrophic forgetting ability of ConSept and concurrently enhance the segmentation capabilities for both old and new classes, we propose two key strategies: distillation with a deterministic old-classes boundary for improved anti-catastrophic forgetting, and dual dice losses to regularize segmentation maps, thereby improving overall segmentation performance.","Extensive experiments show the effectiveness of ConSept on multiple continual semantic segmentation benchmarks under overlapped or disjoint settings.","Code will be publicly available at \\url{https://github.com/DongSky/ConSept}."],"url":"http://arxiv.org/abs/2402.16674v1","category":"cs.CV"}
{"created":"2024-02-26 15:35:18","title":"UN-SAM: Universal Prompt-Free Segmentation for Generalized Nuclei Images","abstract":"In digital pathology, precise nuclei segmentation is pivotal yet challenged by the diversity of tissue types, staining protocols, and imaging conditions. Recently, the segment anything model (SAM) revealed overwhelming performance in natural scenarios and impressive adaptation to medical imaging. Despite these advantages, the reliance of labor-intensive manual annotation as segmentation prompts severely hinders their clinical applicability, especially for nuclei image analysis containing massive cells where dense manual prompts are impractical. To overcome the limitations of current SAM methods while retaining the advantages, we propose the Universal prompt-free SAM framework for Nuclei segmentation (UN-SAM), by providing a fully automated solution with remarkable generalization capabilities. Specifically, to eliminate the labor-intensive requirement of per-nuclei annotations for prompt, we devise a multi-scale Self-Prompt Generation (SPGen) module to revolutionize clinical workflow by automatically generating high-quality mask hints to guide the segmentation tasks. Moreover, to unleash the generalization capability of SAM across a variety of nuclei images, we devise a Domain-adaptive Tuning Encoder (DT-Encoder) to seamlessly harmonize visual features with domain-common and domain-specific knowledge, and further devise a Domain Query-enhanced Decoder (DQ-Decoder) by leveraging learnable domain queries for segmentation decoding in different nuclei domains. Extensive experiments prove that UN-SAM with exceptional performance surpasses state-of-the-arts in nuclei instance and semantic segmentation, especially the generalization capability in zero-shot scenarios. The source code is available at https://github.com/CUHK-AIM-Group/UN-SAM.","sentences":["In digital pathology, precise nuclei segmentation is pivotal yet challenged by the diversity of tissue types, staining protocols, and imaging conditions.","Recently, the segment anything model (SAM) revealed overwhelming performance in natural scenarios and impressive adaptation to medical imaging.","Despite these advantages, the reliance of labor-intensive manual annotation as segmentation prompts severely hinders their clinical applicability, especially for nuclei image analysis containing massive cells where dense manual prompts are impractical.","To overcome the limitations of current SAM methods while retaining the advantages, we propose the Universal prompt-free SAM framework for Nuclei segmentation (UN-SAM), by providing a fully automated solution with remarkable generalization capabilities.","Specifically, to eliminate the labor-intensive requirement of per-nuclei annotations for prompt, we devise a multi-scale Self-Prompt Generation (SPGen) module to revolutionize clinical workflow by automatically generating high-quality mask hints to guide the segmentation tasks.","Moreover, to unleash the generalization capability of SAM across a variety of nuclei images, we devise a Domain-adaptive Tuning Encoder (DT-Encoder) to seamlessly harmonize visual features with domain-common and domain-specific knowledge, and further devise a Domain Query-enhanced Decoder (DQ-Decoder) by leveraging learnable domain queries for segmentation decoding in different nuclei domains.","Extensive experiments prove that UN-SAM with exceptional performance surpasses state-of-the-arts in nuclei instance and semantic segmentation, especially the generalization capability in zero-shot scenarios.","The source code is available at https://github.com/CUHK-AIM-Group/UN-SAM."],"url":"http://arxiv.org/abs/2402.16663v1","category":"eess.IV"}
{"created":"2024-02-26 15:31:56","title":"Towards Efficient Quantum Computing for Quantum Chemistry: Reducing Circuit Complexity with Transcorrelated and Adaptive Ansatz Techniques","abstract":"The near-term utility of quantum computers is hindered by hardware constraints in the form of noise. One path to achieving noise resilience in hybrid quantum algorithms is to decrease the required circuit depth -- the number of applied gates -- to solve a given problem. This work demonstrates how to reduce circuit depth by combining the transcorrelated (TC) approach with adaptive quantum ans\\\"atze and their implementations in the context of variational quantum imaginary time evolution (AVQITE). The combined TC-AVQITE method is used to calculate ground state energies across the potential energy surfaces of H$_4$, LiH, and H$_2$O. In particular, H$_4$ is a notoriously difficult case where unitary coupled cluster theory, including singles and doubles excitations, fails to provide accurate results. Adding TC yields energies close to the complete basis set (CBS) limit while reducing the number of necessary operators -- and thus circuit depth -- in the adaptive ans\\\"atze. The reduced circuit depth furthermore makes our algorithm more noise-resilient and accelerates convergence. Our study demonstrates that combining the TC method with adaptive ans\\\"atze yields compact, noise-resilient, and easy-to-optimize quantum circuits that yield accurate quantum chemistry results close to the CBS limit.","sentences":["The near-term utility of quantum computers is hindered by hardware constraints in the form of noise.","One path to achieving noise resilience in hybrid quantum algorithms is to decrease the required circuit depth -- the number of applied gates -- to solve a given problem.","This work demonstrates how to reduce circuit depth by combining the transcorrelated (TC) approach with adaptive quantum ans\\\"atze and their implementations in the context of variational quantum imaginary time evolution (AVQITE).","The combined TC-AVQITE method is used to calculate ground state energies across the potential energy surfaces of H$_4$, LiH, and H$_2$O.","In particular, H$_4$ is a notoriously difficult case where unitary coupled cluster theory, including singles and doubles excitations, fails to provide accurate results.","Adding TC yields energies close to the complete basis set (CBS) limit while reducing the number of necessary operators -- and thus circuit depth -- in the adaptive ans\\\"atze.","The reduced circuit depth furthermore makes our algorithm more noise-resilient and accelerates convergence.","Our study demonstrates that combining the TC method with adaptive ans\\\"atze yields compact, noise-resilient, and easy-to-optimize quantum circuits that yield accurate quantum chemistry results close to the CBS limit."],"url":"http://arxiv.org/abs/2402.16659v1","category":"quant-ph"}
{"created":"2024-02-26 15:15:37","title":"Learning Based NMPC Adaptation for Autonomous Driving using Parallelized Digital Twin","abstract":"In this work, we address the problem of transferring an autonomous driving (AD) module from one domain to another, in particular from simulation to the real world (Sim2Real). We propose a data-efficient method for online and on-the-fly learning based adaptation for parametrizable control architectures such that the target closed-loop performance is optimized under several uncertainty sources such as model mismatches, environment changes and task choice. The novelty of the work resides in leveraging black-box optimization enabled by executable digital twins, with data-driven hyper-parameter tuning through derivative-free methods to directly adapt in real-time the AD module. Our proposed method requires a minimal amount of interaction with the real-world in the randomization and online training phase. Specifically, we validate our approach in real-world experiments and show the ability to transfer and safely tune a nonlinear model predictive controller in less than 10 minutes, eliminating the need of day-long manual tuning and hours-long machine learning training phases. Our results show that the online adapted NMPC directly compensates for disturbances, avoids overtuning in simulation and for one specific task, and it generalizes for less than 15cm of tracking accuracy over a multitude of trajectories, and leads to 83% tracking improvement.","sentences":["In this work, we address the problem of transferring an autonomous driving (AD) module from one domain to another, in particular from simulation to the real world (Sim2Real).","We propose a data-efficient method for online and on-the-fly learning based adaptation for parametrizable control architectures such that the target closed-loop performance is optimized under several uncertainty sources such as model mismatches, environment changes and task choice.","The novelty of the work resides in leveraging black-box optimization enabled by executable digital twins, with data-driven hyper-parameter tuning through derivative-free methods to directly adapt in real-time the AD module.","Our proposed method requires a minimal amount of interaction with the real-world in the randomization and online training phase.","Specifically, we validate our approach in real-world experiments and show the ability to transfer and safely tune a nonlinear model predictive controller in less than 10 minutes, eliminating the need of day-long manual tuning and hours-long machine learning training phases.","Our results show that the online adapted NMPC directly compensates for disturbances, avoids overtuning in simulation and for one specific task, and it generalizes for less than 15cm of tracking accuracy over a multitude of trajectories, and leads to 83% tracking improvement."],"url":"http://arxiv.org/abs/2402.16645v1","category":"cs.RO"}
{"created":"2024-02-26 15:10:22","title":"DRSI-Net: Dual-Residual Spatial Interaction Network for Multi-Person Pose Estimation","abstract":"Multi-person pose estimation (MPPE), which aims to locate keypoints for all persons in the frames, is an active research branch of computer vision. Variable human poses and complex scenes make MPPE dependent on both local details and global structures, and the absence of them may cause keypoint feature misalignment. In this case, high-order spatial interactions that can effectively link the local and global information of features are particularly important. However, most methods do not have spatial interactions, and a few methods have low-order spatial interactions but they are difficult to achieve a good balance between accuracy and complexity. To address the above problems, a Dual-Residual Spatial Interaction Network (DRSI-Net) for MPPE with high accuracy and low complexity is proposed in this paper. DRSI-Net recursively performs residual spatial information interactions on neighbor features, so that more useful spatial information can be retained and more similarities can be obtained between shallow and deep extracted features. The channel and spatial dual attention mechanism introduced in the multi-scale feature fusion also helps the network to adaptively focus on features relevant to target keypoints and further refine generated poses. At the same time, by optimizing interactive channel dimensions and dividing gradient flow, the spatial interaction module is designed to be lightweight, which reduces the complexity of the network. According to the experimental results on the COCO dataset, the proposed DRSI-Net outperforms other state-of-the-art methods in both accuracy and complexity.","sentences":["Multi-person pose estimation (MPPE), which aims to locate keypoints for all persons in the frames, is an active research branch of computer vision.","Variable human poses and complex scenes make MPPE dependent on both local details and global structures, and the absence of them may cause keypoint feature misalignment.","In this case, high-order spatial interactions that can effectively link the local and global information of features are particularly important.","However, most methods do not have spatial interactions, and a few methods have low-order spatial interactions but they are difficult to achieve a good balance between accuracy and complexity.","To address the above problems, a Dual-Residual Spatial Interaction Network (DRSI-Net) for MPPE with high accuracy and low complexity is proposed in this paper.","DRSI-Net recursively performs residual spatial information interactions on neighbor features, so that more useful spatial information can be retained and more similarities can be obtained between shallow and deep extracted features.","The channel and spatial dual attention mechanism introduced in the multi-scale feature fusion also helps the network to adaptively focus on features relevant to target keypoints and further refine generated poses.","At the same time, by optimizing interactive channel dimensions and dividing gradient flow, the spatial interaction module is designed to be lightweight, which reduces the complexity of the network.","According to the experimental results on the COCO dataset, the proposed DRSI-Net outperforms other state-of-the-art methods in both accuracy and complexity."],"url":"http://arxiv.org/abs/2402.16640v1","category":"cs.CV"}
{"created":"2024-02-26 15:02:35","title":"SLIPT in Joint Dimming Multi-LED OWC Systems with Rate Splitting Multiple Access","abstract":"Optical wireless communication (OWC) systems with multiple light-emitting diodes (LEDs) have recently been explored to support energy-limited devices via simultaneous lightwave information and power transfer (SLIPT). The energy consumption, however, becomes considerable by increasing the number of incorporated LEDs. This paper proposes a joint dimming (JD) scheme that lowers the consumed power of a SLIPT-enabled OWC system by controlling the number of active LEDs. We further enhance the data rate of this system by utilizing rate splitting multiple access (RSMA). More specifically, we formulate a data rate maximization problem to optimize the beamforming design, LED selection and RSMA rate adaptation that guarantees the power budget of the OWC transmitter, as well as the quality-of-service (QoS) and an energy harvesting level for users. We propose a dynamic resource allocation solution based on proximal policy optimization (PPO) reinforcement learning. In simulations, the optimal dimming level is determined to initiate a trade-off between the data rate and power consumption. It is also verified that RSMA significantly improves the data rate.","sentences":["Optical wireless communication (OWC) systems with multiple light-emitting diodes (LEDs) have recently been explored to support energy-limited devices via simultaneous lightwave information and power transfer (SLIPT).","The energy consumption, however, becomes considerable by increasing the number of incorporated LEDs.","This paper proposes a joint dimming (JD) scheme that lowers the consumed power of a SLIPT-enabled OWC system by controlling the number of active LEDs.","We further enhance the data rate of this system by utilizing rate splitting multiple access (RSMA).","More specifically, we formulate a data rate maximization problem to optimize the beamforming design, LED selection and RSMA rate adaptation that guarantees the power budget of the OWC transmitter, as well as the quality-of-service (QoS) and an energy harvesting level for users.","We propose a dynamic resource allocation solution based on proximal policy optimization (PPO) reinforcement learning.","In simulations, the optimal dimming level is determined to initiate a trade-off between the data rate and power consumption.","It is also verified that RSMA significantly improves the data rate."],"url":"http://arxiv.org/abs/2402.16629v1","category":"cs.IT"}
{"created":"2024-02-26 15:01:16","title":"Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing","abstract":"Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and DDIMs with theoretical derivations, and demonstrate the effectiveness of our model in evaluations with two challenging tasks: text-to-image generation, and text-to-video editing. In each task, our ContextDiff achieves new state-of-the-art performance, significantly enhancing the semantic alignment between text condition and generated samples, as evidenced by quantitative and qualitative evaluations. Our code is available at https://github.com/YangLing0818/ContextDiff","sentences":["Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing.","Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process.","This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results.","To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes.","We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling.","We generalize our contextualized diffusion to both DDPMs and DDIMs with theoretical derivations, and demonstrate the effectiveness of our model in evaluations with two challenging tasks: text-to-image generation, and text-to-video editing.","In each task, our ContextDiff achieves new state-of-the-art performance, significantly enhancing the semantic alignment between text condition and generated samples, as evidenced by quantitative and qualitative evaluations.","Our code is available at https://github.com/YangLing0818/ContextDiff"],"url":"http://arxiv.org/abs/2402.16627v1","category":"cs.CV"}
{"created":"2024-02-26 14:43:17","title":"Structure-Preserving Operator Learning: Modeling the Collision Operator of Kinetic Equations","abstract":"This work explores the application of deep operator learning principles to a problem in statistical physics. Specifically, we consider the linear kinetic equation, consisting of a differential advection operator and an integral collision operator, which is a powerful yet expensive mathematical model for interacting particle systems with ample applications, e.g., in radiation transport. We investigate the capabilities of the Deep Operator network (DeepONet) approach to modelling the high dimensional collision operator of the linear kinetic equation. This integral operator has crucial analytical structures that a surrogate model, e.g., a DeepONet, needs to preserve to enable meaningful physical simulation. We propose several DeepONet modifications to encapsulate essential structural properties of this integral operator in a DeepONet model. To be precise, we adapt the architecture of the trunk-net so the DeepONet has the same collision invariants as the theoretical kinetic collision operator, thus preserving conserved quantities, e.g., mass, of the modeled many-particle system. Further, we propose an entropy-inspired data-sampling method tailored to train the modified DeepONet surrogates without requiring an excessive expensive simulation-based data generation.","sentences":["This work explores the application of deep operator learning principles to a problem in statistical physics.","Specifically, we consider the linear kinetic equation, consisting of a differential advection operator and an integral collision operator, which is a powerful yet expensive mathematical model for interacting particle systems with ample applications, e.g., in radiation transport.","We investigate the capabilities of the Deep Operator network (DeepONet) approach to modelling the high dimensional collision operator of the linear kinetic equation.","This integral operator has crucial analytical structures that a surrogate model, e.g., a DeepONet, needs to preserve to enable meaningful physical simulation.","We propose several DeepONet modifications to encapsulate essential structural properties of this integral operator in a DeepONet model.","To be precise, we adapt the architecture of the trunk-net so the DeepONet has the same collision invariants as the theoretical kinetic collision operator, thus preserving conserved quantities, e.g., mass, of the modeled many-particle system.","Further, we propose an entropy-inspired data-sampling method tailored to train the modified DeepONet surrogates without requiring an excessive expensive simulation-based data generation."],"url":"http://arxiv.org/abs/2402.16613v1","category":"math.NA"}
{"created":"2024-02-26 14:04:54","title":"Semantic Communication-Enabled Wireless Adaptive Panoramic Video Transmission","abstract":"In this paper, we propose an adaptive panoramic video semantic transmission (APVST) network built on the deep joint source-channel coding (Deep JSCC) structure for the efficient end-to-end transmission of panoramic videos. The proposed APVST network can adaptively extract semantic features of panoramic frames and achieve semantic feature encoding. To achieve high spectral efficiency and save bandwidth, we propose a transmission rate control mechanism for the APVST via the entropy model and the latitude adaptive model. Besides, we take weighted-to-spherically-uniform peak signal-to-noise ratio (WS-PSNR) and weighted-to-spherically-uniform structural similarity (WS-SSIM) as distortion evaluation metrics, and propose the weight attention module to fuse the weights with the semantic features to achieve better quality of immersive experiences. Finally, we evaluate our proposed scheme on a panoramic video dataset containing 208 panoramic videos. The simulation results show that the APVST can save up to 20% and 50% on channel bandwidth cost compared with other semantic communication-based and traditional video transmission schemes.","sentences":["In this paper, we propose an adaptive panoramic video semantic transmission (APVST) network built on the deep joint source-channel coding (Deep JSCC) structure for the efficient end-to-end transmission of panoramic videos.","The proposed APVST network can adaptively extract semantic features of panoramic frames and achieve semantic feature encoding.","To achieve high spectral efficiency and save bandwidth, we propose a transmission rate control mechanism for the APVST via the entropy model and the latitude adaptive model.","Besides, we take weighted-to-spherically-uniform peak signal-to-noise ratio (WS-PSNR) and weighted-to-spherically-uniform structural similarity (WS-SSIM) as distortion evaluation metrics, and propose the weight attention module to fuse the weights with the semantic features to achieve better quality of immersive experiences.","Finally, we evaluate our proposed scheme on a panoramic video dataset containing 208 panoramic videos.","The simulation results show that the APVST can save up to 20% and 50% on channel bandwidth cost compared with other semantic communication-based and traditional video transmission schemes."],"url":"http://arxiv.org/abs/2402.16581v1","category":"eess.IV"}
{"created":"2024-02-26 14:04:24","title":"Information-Enriched Selection of Stationary and Non-Stationary Autoregressions using the Adaptive Lasso","abstract":"We propose a novel approach to elicit the weight of a potentially non-stationary regressor in the consistent and oracle-efficient estimation of autoregressive models using the adaptive Lasso. The enhanced weight builds on a statistic that exploits distinct orders in probability of the OLS estimator in time series regressions when the degree of integration differs. We provide theoretical results on the benefit of our approach for detecting stationarity when a tuning criterion selects the $\\ell_1$ penalty parameter. Monte Carlo evidence shows that our proposal is superior to using OLS-based weights, as suggested by Kock [Econom. Theory, 32, 2016, 243-259]. We apply the modified estimator to model selection for German inflation rates after the introduction of the Euro. The results indicate that energy commodity price inflation and headline inflation are best described by stationary autoregressions.","sentences":["We propose a novel approach to elicit the weight of a potentially non-stationary regressor in the consistent and oracle-efficient estimation of autoregressive models using the adaptive Lasso.","The enhanced weight builds on a statistic that exploits distinct orders in probability of the OLS estimator in time series regressions when the degree of integration differs.","We provide theoretical results on the benefit of our approach for detecting stationarity when a tuning criterion selects the $\\ell_1$ penalty parameter.","Monte Carlo evidence shows that our proposal is superior to using OLS-based weights, as suggested by Kock [Econom.","Theory, 32, 2016, 243-259].","We apply the modified estimator to model selection for German inflation rates after the introduction of the Euro.","The results indicate that energy commodity price inflation and headline inflation are best described by stationary autoregressions."],"url":"http://arxiv.org/abs/2402.16580v1","category":"stat.ME"}
{"created":"2024-02-26 13:54:20","title":"Generalized quantum measurement in spin-correlated hyperon-antihyperon decays","abstract":"The rapid developments of Quantum Information Science (QIS) have opened up new avenues for exploring fundamental physics. Quantum nonlocality, a key aspect for distinguishing quantum information from classical one, has undergone extensive examinations in particles' decays through the violation of Bell-type inequalities. Despite these advancements, a comprehensive framework based on quantum information theory for particle interaction is still lacking. Trying to close this gap, we introduce a generalized quantum measurement description for decay processes of spin-1/2 hyperons. We validate this approach by aligning it with established theoretical calculations and apply it to the joint decay of correlated $\\Lambda\\bar{\\Lambda}$ pairs. We employ quantum simulation to observe the violation of CHSH inequalities in hyperon decays. Our generalized measurement description is adaptable and can be extended to a variety of high energy processes, including decays of vector mesons, $J/\\psi,\\psi(2S)\\rightarrow\\Lambda\\bar{\\Lambda}$, in the Beijing Spectrometer III (BESIII) experiment at the Beijing Electron Positron Collider (BEPC). The methodology developed in this study can be applied to quantum correlation and information processing in fundamental interactions.","sentences":["The rapid developments of Quantum Information Science (QIS) have opened up new avenues for exploring fundamental physics.","Quantum nonlocality, a key aspect for distinguishing quantum information from classical one, has undergone extensive examinations in particles' decays through the violation of Bell-type inequalities.","Despite these advancements, a comprehensive framework based on quantum information theory for particle interaction is still lacking.","Trying to close this gap, we introduce a generalized quantum measurement description for decay processes of spin-1/2 hyperons.","We validate this approach by aligning it with established theoretical calculations and apply it to the joint decay of correlated $\\Lambda\\bar{\\Lambda}$ pairs.","We employ quantum simulation to observe the violation of CHSH inequalities in hyperon decays.","Our generalized measurement description is adaptable and can be extended to a variety of high energy processes, including decays of vector mesons, $J/\\psi,\\psi(2S)\\rightarrow\\Lambda\\bar{\\Lambda}$, in the Beijing Spectrometer III (BESIII) experiment at the Beijing Electron Positron Collider (BEPC).","The methodology developed in this study can be applied to quantum correlation and information processing in fundamental interactions."],"url":"http://arxiv.org/abs/2402.16574v1","category":"hep-ph"}
{"created":"2024-02-26 13:39:19","title":"Flexible Robust Beamforming for Multibeam Satellite Downlink using Reinforcement Learning","abstract":"Low Earth Orbit (LEO) satellite-to-handheld connections herald a new era in satellite communications. Space-Division Multiple Access (SDMA) precoding is a method that mitigates interference among satellite beams, boosting spectral efficiency. While optimal SDMA precoding solutions have been proposed for ideal channel knowledge in various scenarios, addressing robust precoding with imperfect channel information has primarily been limited to simplified models. However, these models might not capture the complexity of LEO satellite applications. We use the Soft Actor-Critic (SAC) deep Reinforcement Learning (RL) method to learn robust precoding strategies without the need for explicit insights into the system conditions and imperfections. Our results show flexibility to adapt to arbitrary system configurations while performing strongly in terms of achievable rate and robustness to disruptive influences compared to analytical benchmark precoders.","sentences":["Low Earth Orbit (LEO) satellite-to-handheld connections herald a new era in satellite communications.","Space-Division Multiple Access (SDMA) precoding is a method that mitigates interference among satellite beams, boosting spectral efficiency.","While optimal SDMA precoding solutions have been proposed for ideal channel knowledge in various scenarios, addressing robust precoding with imperfect channel information has primarily been limited to simplified models.","However, these models might not capture the complexity of LEO satellite applications.","We use the Soft Actor-Critic (SAC) deep Reinforcement Learning (RL) method to learn robust precoding strategies without the need for explicit insights into the system conditions and imperfections.","Our results show flexibility to adapt to arbitrary system configurations while performing strongly in terms of achievable rate and robustness to disruptive influences compared to analytical benchmark precoders."],"url":"http://arxiv.org/abs/2402.16563v1","category":"eess.SP"}
{"created":"2024-02-26 13:22:07","title":"Delayed-feedback oscillators replicate the dynamics of multiplex networks: wavefront propagation and stochastic resonance","abstract":"The widespread development and use of neural networks have significantly enriched a wide range of computer algorithms and promise higher speed at lower cost. However, the imitation of neural networks by means of modern computing substrates is highly inefficient, whereas physical realization of large scale networks remains challenging. Fortunately, delayed-feedback oscillators, being much easier to realize experimentally, represent promising candidates for the empirical implementation of neural networks and next generation computing architectures. In the current research, we demonstrate that coupled bistable delayed-feedback oscillators emulate a multilayer network, where one single-layer network is connected to another single-layer network through coupling between replica nodes, i.e. the multiplex network. We show that all the aspects of the multiplexing impact on wavefront propagation and stochastic resonance identified in multilayer networks of bistable oscillators are entirely reproduced in the dynamics of time-delay oscillators. In particular, varying the coupling strength allows suppressing and enhancing the effect of stochastic resonance, as well as controlling the speed and direction of both deterministic and stochastic wavefront propagation. All the considered effects are studied in numerical simulations and confirmed in physical experiments, showing an excellent correspondence and disclosing thereby the robustness of the observed phenomena.","sentences":["The widespread development and use of neural networks have significantly enriched a wide range of computer algorithms and promise higher speed at lower cost.","However, the imitation of neural networks by means of modern computing substrates is highly inefficient, whereas physical realization of large scale networks remains challenging.","Fortunately, delayed-feedback oscillators, being much easier to realize experimentally, represent promising candidates for the empirical implementation of neural networks and next generation computing architectures.","In the current research, we demonstrate that coupled bistable delayed-feedback oscillators emulate a multilayer network, where one single-layer network is connected to another single-layer network through coupling between replica nodes, i.e. the multiplex network.","We show that all the aspects of the multiplexing impact on wavefront propagation and stochastic resonance identified in multilayer networks of bistable oscillators are entirely reproduced in the dynamics of time-delay oscillators.","In particular, varying the coupling strength allows suppressing and enhancing the effect of stochastic resonance, as well as controlling the speed and direction of both deterministic and stochastic wavefront propagation.","All the considered effects are studied in numerical simulations and confirmed in physical experiments, showing an excellent correspondence and disclosing thereby the robustness of the observed phenomena."],"url":"http://arxiv.org/abs/2402.16551v1","category":"nlin.AO"}
{"created":"2024-02-26 12:55:51","title":"Integrating Large Language Models with Graphical Session-Based Recommendation","abstract":"With the rapid development of Large Language Models (LLMs), various explorations have arisen to utilize LLMs capability of context understanding on recommender systems. While pioneering strategies have primarily transformed traditional recommendation tasks into challenges of natural language generation, there has been a relative scarcity of exploration in the domain of session-based recommendation (SBR) due to its specificity. SBR has been primarily dominated by Graph Neural Networks, which have achieved many successful outcomes due to their ability to capture both the implicit and explicit relationships between adjacent behaviors. The structural nature of graphs contrasts with the essence of natural language, posing a significant adaptation gap for LLMs. In this paper, we introduce large language models with graphical Session-Based recommendation, named LLMGR, an effective framework that bridges the aforementioned gap by harmoniously integrating LLMs with Graph Neural Networks (GNNs) for SBR tasks. This integration seeks to leverage the complementary strengths of LLMs in natural language understanding and GNNs in relational data processing, leading to a more powerful session-based recommender system that can understand and recommend items within a session. Moreover, to endow the LLM with the capability to empower SBR tasks, we design a series of prompts for both auxiliary and major instruction tuning tasks. These prompts are crafted to assist the LLM in understanding graph-structured data and align textual information with nodes, effectively translating nuanced user interactions into a format that can be understood and utilized by LLM architectures. Extensive experiments on three real-world datasets demonstrate that LLMGR outperforms several competitive baselines, indicating its effectiveness in enhancing SBR tasks and its potential as a research direction for future exploration.","sentences":["With the rapid development of Large Language Models (LLMs), various explorations have arisen to utilize LLMs capability of context understanding on recommender systems.","While pioneering strategies have primarily transformed traditional recommendation tasks into challenges of natural language generation, there has been a relative scarcity of exploration in the domain of session-based recommendation (SBR) due to its specificity.","SBR has been primarily dominated by Graph Neural Networks, which have achieved many successful outcomes due to their ability to capture both the implicit and explicit relationships between adjacent behaviors.","The structural nature of graphs contrasts with the essence of natural language, posing a significant adaptation gap for LLMs.","In this paper, we introduce large language models with graphical Session-Based recommendation, named LLMGR, an effective framework that bridges the aforementioned gap by harmoniously integrating LLMs with Graph Neural Networks (GNNs) for SBR tasks.","This integration seeks to leverage the complementary strengths of LLMs in natural language understanding and GNNs in relational data processing, leading to a more powerful session-based recommender system that can understand and recommend items within a session.","Moreover, to endow the LLM with the capability to empower SBR tasks, we design a series of prompts for both auxiliary and major instruction tuning tasks.","These prompts are crafted to assist the LLM in understanding graph-structured data and align textual information with nodes, effectively translating nuanced user interactions into a format that can be understood and utilized by LLM architectures.","Extensive experiments on three real-world datasets demonstrate that LLMGR outperforms several competitive baselines, indicating its effectiveness in enhancing SBR tasks and its potential as a research direction for future exploration."],"url":"http://arxiv.org/abs/2402.16539v1","category":"cs.IR"}
{"created":"2024-02-26 12:04:23","title":"Sequential design for surrogate modeling in Bayesian inverse problems","abstract":"Sequential design is a highly active field of research in active learning which provides a general framework for the design of computer experiments to make the most of a low computational budget. It has been widely used to generate efficient surrogate models able to replace complex computer codes, most notably for uncertainty quantification, Bayesian optimization, reliability analysis or model calibration tasks. In this work, a sequential design strategy is developed for Bayesian inverse problems, in which a Gaussian process surrogate model serves as an emulator for a costly computer code. The proposed strategy is based on a goal-oriented I-optimal criterion adapted to the Stepwise Uncertainty Reduction (SUR) paradigm. In SUR strategies, a new design point is chosen by minimizing the expectation of an uncertainty metric with respect to the yet unknown new data point. These methods have attracted increasing interest as they provide an accessible framework for the sequential design of experiments while including almost-sure convergence for the most-widely used metrics. In this paper, a weighted integrated mean square prediction error is introduced and serves as a metric of uncertainty for the newly proposed IP-SUR (Inverse Problem Stepwise Uncertainty Reduction) sequential design strategy derived from SUR methods. This strategy is shown to be tractable for both scalar and multi-output Gaussian process surrogate models with continuous sample paths, and comes with theoretical guarantee for the almost-sure convergence of the metric of uncertainty. The premises of this work are highlighted on various test cases in which the newly derived strategy is compared to other naive and sequential designs (D-optimal designs, Bayes risk minimization).","sentences":["Sequential design is a highly active field of research in active learning which provides a general framework for the design of computer experiments to make the most of a low computational budget.","It has been widely used to generate efficient surrogate models able to replace complex computer codes, most notably for uncertainty quantification, Bayesian optimization, reliability analysis or model calibration tasks.","In this work, a sequential design strategy is developed for Bayesian inverse problems, in which a Gaussian process surrogate model serves as an emulator for a costly computer code.","The proposed strategy is based on a goal-oriented I-optimal criterion adapted to the Stepwise Uncertainty Reduction (SUR) paradigm.","In SUR strategies, a new design point is chosen by minimizing the expectation of an uncertainty metric with respect to the yet unknown new data point.","These methods have attracted increasing interest as they provide an accessible framework for the sequential design of experiments while including almost-sure convergence for the most-widely used metrics.","In this paper, a weighted integrated mean square prediction error is introduced and serves as a metric of uncertainty for the newly proposed IP-SUR (Inverse Problem Stepwise Uncertainty Reduction) sequential design strategy derived from SUR methods.","This strategy is shown to be tractable for both scalar and multi-output Gaussian process surrogate models with continuous sample paths, and comes with theoretical guarantee for the almost-sure convergence of the metric of uncertainty.","The premises of this work are highlighted on various test cases in which the newly derived strategy is compared to other naive and sequential designs (D-optimal designs, Bayes risk minimization)."],"url":"http://arxiv.org/abs/2402.16520v1","category":"stat.ME"}
{"created":"2024-02-26 10:06:41","title":"Matrix weighted modulation spaces","abstract":"Given a matrix-weight $W$ in the Muckenhoupt class $\\mathbf{A}_p(\\mathbb{R}^n)$, $1\\leq p<\\infty$, we introduce corresponding vector-valued continuous and discrete $\\alpha$-modulation spaces $M^{s,\\alpha}_{p,q}(W)$ and $m^{s,\\alpha}_{p,q}(W)$ and prove their equivalence through the use of adapted tight frames. Compatible notions of molecules and almost diagonal matrices are also introduced, and an application to the study of pseudo-differential operators on vector valued spaces is given.","sentences":["Given a matrix-weight $W$ in the Muckenhoupt class $\\mathbf{A}_p(\\mathbb{R}^n)$, $1\\leq p<\\infty$, we introduce corresponding vector-valued continuous and discrete $\\alpha$-modulation spaces $M^{s,\\alpha}_{p,q}(W)$ and $m^{s,\\alpha}_{p,q}(W)$ and prove their equivalence through the use of adapted tight frames.","Compatible notions of molecules and almost diagonal matrices are also introduced, and an application to the study of pseudo-differential operators on vector valued spaces is given."],"url":"http://arxiv.org/abs/2402.16461v1","category":"math.FA"}
{"created":"2024-02-26 09:17:13","title":"Two-stage Information Spreading Evolution on The Control Role of Announcements","abstract":"Modern social media networks have become an important platform for information competition among countries, regions, companies and other parties. This paper utilizes the research method of spread dynamics to investigate the influence of the control role of announcements in social networks on the spreading process. This paper distinguishes two spreading phases using the authentication intervention as a boundary: the unconfirmed spreading phase and the confirmed spreading phase. Based on the actual rules of spreading in online social networks, two kinds of verification results are defined: true information and false information. The Two-stage information spreading dynamics model is developed to analyze the changes in spreading effects due to different validation results. The impact of the intervention time on the overall spread process is analyzed by combining important control factors such as response cost and time-sensitivity. The validity of the model is verified by comparing the model simulation results with real cases and the adaptive capacity experiments. This work is analyzed and visualized from multiple perspectives, providing more quantitative results. The research content will provide a scientific basis for the intervention behavior of information management control by relevant departments or authorities.","sentences":["Modern social media networks have become an important platform for information competition among countries, regions, companies and other parties.","This paper utilizes the research method of spread dynamics to investigate the influence of the control role of announcements in social networks on the spreading process.","This paper distinguishes two spreading phases using the authentication intervention as a boundary: the unconfirmed spreading phase and the confirmed spreading phase.","Based on the actual rules of spreading in online social networks, two kinds of verification results are defined: true information and false information.","The Two-stage information spreading dynamics model is developed to analyze the changes in spreading effects due to different validation results.","The impact of the intervention time on the overall spread process is analyzed by combining important control factors such as response cost and time-sensitivity.","The validity of the model is verified by comparing the model simulation results with real cases and the adaptive capacity experiments.","This work is analyzed and visualized from multiple perspectives, providing more quantitative results.","The research content will provide a scientific basis for the intervention behavior of information management control by relevant departments or authorities."],"url":"http://arxiv.org/abs/2402.16416v1","category":"cs.SI"}
{"created":"2024-02-26 08:59:04","title":"JefiAtten: An Attention Based Neural Network Model for Solving Maxwell's Equations with Charge and Current Sources","abstract":"We present JefiAtten, a novel neural network model employing the attention mechanism to solve Maxwell's equations efficiently. JefiAtten uses self-attention and cross-attention modules to understand the interplay between charge density, current density, and electromagnetic fields. Our results indicate that JefiAtten can generalize well to a range of scenarios, maintaining accuracy across various spatial distribution and handling amplitude variations. The model showcases an improvement in computation speed after training, compared to traditional integral methods. The adaptability of the model suggests potential for broader applications in computational physics, with further refinements to enhance its predictive capabilities and computational efficiency. Our work is a testament to the efficacy of integrating attention mechanisms with numerical simulations, marking a step forward in the quest for data-driven solutions to physical phenomena.","sentences":["We present JefiAtten, a novel neural network model employing the attention mechanism to solve Maxwell's equations efficiently.","JefiAtten uses self-attention and cross-attention modules to understand the interplay between charge density, current density, and electromagnetic fields.","Our results indicate that JefiAtten can generalize well to a range of scenarios, maintaining accuracy across various spatial distribution and handling amplitude variations.","The model showcases an improvement in computation speed after training, compared to traditional integral methods.","The adaptability of the model suggests potential for broader applications in computational physics, with further refinements to enhance its predictive capabilities and computational efficiency.","Our work is a testament to the efficacy of integrating attention mechanisms with numerical simulations, marking a step forward in the quest for data-driven solutions to physical phenomena."],"url":"http://arxiv.org/abs/2402.16920v1","category":"physics.comp-ph"}
{"created":"2024-02-26 08:22:14","title":"Nonlocal-to-local limit in linearized viscoelasticity","abstract":"We study the quasistatic evolution of a linear peridynamic Kelvin-Voigt viscoelastic material. More specifically, we consider the gradient flow of a nonlocal elastic energy with respect to a nonlocal viscous dissipation. Following an evolutionary $\\Gamma$-convergence approach, we prove that the solutions of the nonlocal problem converge to the solution of the local problem, when the peridynamic horizon tends to $0$, that is, in the nonlocal-to-local limit.","sentences":["We study the quasistatic evolution of a linear peridynamic Kelvin-Voigt viscoelastic material.","More specifically, we consider the gradient flow of a nonlocal elastic energy with respect to a nonlocal viscous dissipation.","Following an evolutionary $\\Gamma$-convergence approach, we prove that the solutions of the nonlocal problem converge to the solution of the local problem, when the peridynamic horizon tends to $0$, that is, in the nonlocal-to-local limit."],"url":"http://arxiv.org/abs/2402.16386v1","category":"math.AP"}
{"created":"2024-02-26 07:49:40","title":"Adaptive Online Learning of Separable Path Graph Transforms for Intra-prediction","abstract":"Current video coding standards, including H.264/AVC, HEVC, and VVC, employ discrete cosine transform (DCT), discrete sine transform (DST), and secondary to Karhunen-Loeve transforms (KLTs) decorrelate the intra-prediction residuals. However, the efficiency of these transforms in decorrelation can be limited when the signal has a non-smooth and non-periodic structure, such as those occurring in textures with intricate patterns. This paper introduces a novel adaptive separable path graph-based transform (GBT) that can provide better decorrelation than the DCT for intra-predicted texture data. The proposed GBT is learned in an online scenario with sequential K-means clustering, which groups similar blocks during encoding and decoding to adaptively learn the GBT for the current block from previously reconstructed areas with similar characteristics. A signaling overhead is added to the bitstream of each coding block to indicate the usage of the proposed graph-based transform. We assess the performance of this method combined with H.264/AVC intra-coding tools and demonstrate that it can significantly outperform H.264/AVC DCT for intra-predicted texture data.","sentences":["Current video coding standards, including H.264/AVC, HEVC, and VVC, employ discrete cosine transform (DCT), discrete sine transform (DST), and secondary to Karhunen-Loeve transforms (KLTs) decorrelate the intra-prediction residuals.","However, the efficiency of these transforms in decorrelation can be limited when the signal has a non-smooth and non-periodic structure, such as those occurring in textures with intricate patterns.","This paper introduces a novel adaptive separable path graph-based transform (GBT) that can provide better decorrelation than the DCT for intra-predicted texture data.","The proposed GBT is learned in an online scenario with sequential K-means clustering, which groups similar blocks during encoding and decoding to adaptively learn the GBT for the current block from previously reconstructed areas with similar characteristics.","A signaling overhead is added to the bitstream of each coding block to indicate the usage of the proposed graph-based transform.","We assess the performance of this method combined with H.264/AVC intra-coding tools and demonstrate that it can significantly outperform H.264/AVC DCT for intra-predicted texture data."],"url":"http://arxiv.org/abs/2402.16371v1","category":"eess.IV"}
{"created":"2024-02-26 07:47:50","title":"Unitriangularity of decomposition matrices of the unipotent ${\\ell}$-blocks for simple adjoint exceptional groups","abstract":"In 2020, Brunat-Dudas-Taylor showed that the decomposition matrix of unipotent ${\\ell}$-blocks of a nite reductive group in good characteristic has unitriangular shape, under some conditions on the prime ${\\ell}$, in particular ${\\ell}$ being good. We extend this result to ${\\ell}$ bad by adapting their proof to include the ${\\ell}$-special classes dened by Chaneb.","sentences":["In 2020, Brunat-Dudas-Taylor showed that the decomposition matrix of unipotent ${\\ell}$-blocks of a nite reductive group in good characteristic has unitriangular shape, under some conditions on the prime ${\\ell}$, in particular ${\\ell}$ being good.","We extend this result to ${\\ell}$ bad by adapting their proof to include the ${\\ell}$-special classes dened by Chaneb."],"url":"http://arxiv.org/abs/2402.17616v1","category":"math.RT"}
{"created":"2024-02-26 06:29:05","title":"Personalized Federated Instruction Tuning via Neural Architecture Search","abstract":"Federated Instruction Tuning (FIT) has shown the ability to achieve collaborative model instruction tuning among massive data owners without sharing private data. However, it still faces two key challenges, i.e., data and resource heterogeneity. Due to the varying data distribution and preferences among data owners, FIT cannot adapt to the personalized data of individual owners. Moreover, clients with superior computational abilities are constrained since they need to maintain the same fine-tuning architecture as the weaker clients. To address these issues, we propose a novel Personalized Federated Instruction Tuning (PerFIT) framework based on architecture search. Specifically, PerFIT allows each client to search for a personalized architecture by expanding the trainable parameter space of the global model followed by pruning the parameters to the original state. This procedure allows personalized instruction fine-tuning within expanded parameter spaces, concurrently preserving the same number of trainable parameters. Furthermore, to release the abilities of heterogeneous computational resources and enhance the performance of personalization on local data, we exploit personalized parameter-wise aggregation. The evaluation with multiple LLMs non-IID scenarios demonstrates that compared to the state-of-the-art FIT methods, our approach can achieve up to a 23% decrease in perplexity.","sentences":["Federated Instruction Tuning (FIT) has shown the ability to achieve collaborative model instruction tuning among massive data owners without sharing private data.","However, it still faces two key challenges, i.e., data and resource heterogeneity.","Due to the varying data distribution and preferences among data owners, FIT cannot adapt to the personalized data of individual owners.","Moreover, clients with superior computational abilities are constrained since they need to maintain the same fine-tuning architecture as the weaker clients.","To address these issues, we propose a novel Personalized Federated Instruction Tuning (PerFIT) framework based on architecture search.","Specifically, PerFIT allows each client to search for a personalized architecture by expanding the trainable parameter space of the global model followed by pruning the parameters to the original state.","This procedure allows personalized instruction fine-tuning within expanded parameter spaces, concurrently preserving the same number of trainable parameters.","Furthermore, to release the abilities of heterogeneous computational resources and enhance the performance of personalization on local data, we exploit personalized parameter-wise aggregation.","The evaluation with multiple LLMs non-IID scenarios demonstrates that compared to the state-of-the-art FIT methods, our approach can achieve up to a 23% decrease in perplexity."],"url":"http://arxiv.org/abs/2402.16919v1","category":"cs.LG"}
{"created":"2024-02-26 06:26:21","title":"Efficient calculation of magnetocrystalline anisotropy energy using symmetry-adapted Wannier functions","abstract":"Magnetocrystalline anisotropy, a crucial factor in magnetic properties and applications like magnetoresistive random-access memory, often requires extensive $k$-point mesh in first-principles calculations. In this study, we develop a Wannier orbital tight-binding model incorporating crystal and spin symmetries and utilize time-reversal symmetry to divide magnetization components. This model enables efficient computation of magnetocrystalline anisotropy. Applying this method to $\\mathrm{L1_0}$ $\\mathrm{FePt}$ and $\\mathrm{FeNi}$, we calculate the dependence of the anisotropic energy on $k$-point mesh size, chemical potential, spin-orbit interaction, and magnetization direction. The results validate the practicality of the models to the energy order of $10~[\\mathrm{\\mu eV}/f.u.]$.","sentences":["Magnetocrystalline anisotropy, a crucial factor in magnetic properties and applications like magnetoresistive random-access memory, often requires extensive $k$-point mesh in first-principles calculations.","In this study, we develop a Wannier orbital tight-binding model incorporating crystal and spin symmetries and utilize time-reversal symmetry to divide magnetization components.","This model enables efficient computation of magnetocrystalline anisotropy.","Applying this method to $\\mathrm{L1_0}$ $\\mathrm{FePt}$ and $\\mathrm{FeNi}$, we calculate the dependence of the anisotropic energy on $k$-point mesh size, chemical potential, spin-orbit interaction, and magnetization direction.","The results validate the practicality of the models to the energy order of $10~[\\mathrm{\\mu eV}/f.u.]$."],"url":"http://arxiv.org/abs/2402.16331v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-26 05:50:43","title":"Gradient-Guided Modality Decoupling for Missing-Modality Robustness","abstract":"Multimodal learning with incomplete input data (missing modality) is practical and challenging. In this work, we conduct an in-depth analysis of this challenge and find that modality dominance has a significant negative impact on the model training, greatly degrading the missing modality performance. Motivated by Grad-CAM, we introduce a novel indicator, gradients, to monitor and reduce modality dominance which widely exists in the missing-modality scenario. In aid of this indicator, we present a novel Gradient-guided Modality Decoupling (GMD) method to decouple the dependency on dominating modalities. Specifically, GMD removes the conflicted gradient components from different modalities to achieve this decoupling, significantly improving the performance. In addition, to flexibly handle modal-incomplete data, we design a parameter-efficient Dynamic Sharing (DS) framework which can adaptively switch on/off the network parameters based on whether one modality is available. We conduct extensive experiments on three popular multimodal benchmarks, including BraTS 2018 for medical segmentation, CMU-MOSI, and CMU-MOSEI for sentiment analysis. The results show that our method can significantly outperform the competitors, showing the effectiveness of the proposed solutions. Our code is released here: https://github.com/HaoWang420/Gradient-guided-Modality-Decoupling.","sentences":["Multimodal learning with incomplete input data (missing modality) is practical and challenging.","In this work, we conduct an in-depth analysis of this challenge and find that modality dominance has a significant negative impact on the model training, greatly degrading the missing modality performance.","Motivated by Grad-CAM, we introduce a novel indicator, gradients, to monitor and reduce modality dominance which widely exists in the missing-modality scenario.","In aid of this indicator, we present a novel Gradient-guided Modality Decoupling (GMD) method to decouple the dependency on dominating modalities.","Specifically, GMD removes the conflicted gradient components from different modalities to achieve this decoupling, significantly improving the performance.","In addition, to flexibly handle modal-incomplete data, we design a parameter-efficient Dynamic Sharing (DS) framework which can adaptively switch on/off the network parameters based on whether one modality is available.","We conduct extensive experiments on three popular multimodal benchmarks, including BraTS 2018 for medical segmentation, CMU-MOSI, and CMU-MOSEI for sentiment analysis.","The results show that our method can significantly outperform the competitors, showing the effectiveness of the proposed solutions.","Our code is released here: https://github.com/HaoWang420/Gradient-guided-Modality-Decoupling."],"url":"http://arxiv.org/abs/2402.16318v1","category":"cs.CV"}
{"created":"2024-02-26 04:47:32","title":"m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers","abstract":"Modular neural architectures are gaining increasing attention due to their powerful capability for generalization and sample-efficient adaptation to new domains. However, training modular models, particularly in the early stages, poses challenges due to the optimization difficulties arising from their intrinsic sparse connectivity. Leveraging the knowledge from monolithic models, using techniques such as knowledge distillation, is likely to facilitate the training of modular models and enable them to integrate knowledge from multiple models pretrained on diverse sources. Nevertheless, conventional knowledge distillation approaches are not tailored to modular models and can fail when directly applied due to the unique architectures and the enormous number of parameters involved. Motivated by these challenges, we propose a general module-to-module knowledge distillation (m2mKD) method for transferring knowledge between modules. Our approach involves teacher modules split from a pretrained monolithic model, and student modules of a modular model. m2mKD separately combines these modules with a shared meta model and encourages the student module to mimic the behaviour of the teacher module. We evaluate the effectiveness of m2mKD on two distinct modular neural architectures: Neural Attentive Circuits (NACs) and Vision Mixture-of-Experts (V-MoE). By applying m2mKD to NACs, we achieve significant improvements in IID accuracy on Tiny-ImageNet (up to 5.6%) and OOD robustness on Tiny-ImageNet-R (up to 4.2%). On average, we observe a 1% gain in both ImageNet and ImageNet-R. The V-MoE-Base model trained using m2mKD also achieves 3.5% higher accuracy than end-to-end training on ImageNet. The experimental results demonstrate that our method offers a promising solution for connecting modular networks with pretrained monolithic models. Code is available at https://github.com/kamanphoebe/m2mKD.","sentences":["Modular neural architectures are gaining increasing attention due to their powerful capability for generalization and sample-efficient adaptation to new domains.","However, training modular models, particularly in the early stages, poses challenges due to the optimization difficulties arising from their intrinsic sparse connectivity.","Leveraging the knowledge from monolithic models, using techniques such as knowledge distillation, is likely to facilitate the training of modular models and enable them to integrate knowledge from multiple models pretrained on diverse sources.","Nevertheless, conventional knowledge distillation approaches are not tailored to modular models and can fail when directly applied due to the unique architectures and the enormous number of parameters involved.","Motivated by these challenges, we propose a general module-to-module knowledge distillation (m2mKD) method for transferring knowledge between modules.","Our approach involves teacher modules split from a pretrained monolithic model, and student modules of a modular model.","m2mKD separately combines these modules with a shared meta model and encourages the student module to mimic the behaviour of the teacher module.","We evaluate the effectiveness of m2mKD on two distinct modular neural architectures: Neural Attentive Circuits (NACs) and Vision Mixture-of-Experts (V-MoE).","By applying m2mKD to NACs, we achieve significant improvements in IID accuracy on Tiny-ImageNet (up to 5.6%) and OOD robustness on Tiny-ImageNet-R (up to 4.2%).","On average, we observe a 1% gain in both ImageNet and ImageNet-R. The V-MoE-Base model trained using m2mKD also achieves 3.5% higher accuracy than end-to-end training on ImageNet.","The experimental results demonstrate that our method offers a promising solution for connecting modular networks with pretrained monolithic models.","Code is available at https://github.com/kamanphoebe/m2mKD."],"url":"http://arxiv.org/abs/2402.16918v1","category":"cs.LG"}
{"created":"2024-02-26 03:13:09","title":"Peak finding algorithm for cluster counting with domain adaptation","abstract":"Cluster counting in drift chamber is the most promising breakthrough in particle identification (PID) technique in particle physics experiment. Reconstruction algorithm is one of the key challenges in cluster counting. In this paper, a semi-supervised domain adaptation (DA) algorithm is developed and applied on the peak finding problem in cluster counting. The algorithm uses optimal transport (OT), which provides geometric metric between distributions, to align the samples between the source (simulation) and target (data) samples, and performs semi-supervised learning with the samples in target domain that are partially labeled with the continuous wavelet transform (CWT) algorithm. The model is validated by the pseudo data with labels, which achieves performance close to the fully supervised model. When applying the algorithm on real experimental data, taken at CERN with a 180 GeV/c muon beam, it shows better classification power than the traditional derivative-based algorithm, and the performance is stable for experimental data samples across varying track lengths.","sentences":["Cluster counting in drift chamber is the most promising breakthrough in particle identification (PID) technique in particle physics experiment.","Reconstruction algorithm is one of the key challenges in cluster counting.","In this paper, a semi-supervised domain adaptation (DA) algorithm is developed and applied on the peak finding problem in cluster counting.","The algorithm uses optimal transport (OT), which provides geometric metric between distributions, to align the samples between the source (simulation) and target (data) samples, and performs semi-supervised learning with the samples in target domain that are partially labeled with the continuous wavelet transform (CWT) algorithm.","The model is validated by the pseudo data with labels, which achieves performance close to the fully supervised model.","When applying the algorithm on real experimental data, taken at CERN with a 180 GeV/c muon beam, it shows better classification power than the traditional derivative-based algorithm, and the performance is stable for experimental data samples across varying track lengths."],"url":"http://arxiv.org/abs/2402.16270v1","category":"physics.ins-det"}
{"created":"2024-02-27 18:30:17","title":"A geometric characterization of $\\mathbb{N}$-manifolds and the Frobenius theorem","abstract":"This paper studies graded manifolds with local coordinates concentrated in non-negative degrees. We provide a canonical description of these objects in terms of classical geometric data and, building on this geometric viewpoint, we prove the Frobenius theorem for distributions in this graded setting.","sentences":["This paper studies graded manifolds with local coordinates concentrated in non-negative degrees.","We provide a canonical description of these objects in terms of classical geometric data and, building on this geometric viewpoint, we prove the Frobenius theorem for distributions in this graded setting."],"url":"http://arxiv.org/abs/2402.17746v1","category":"math.DG"}
{"created":"2024-02-27 17:50:47","title":"Attention-based Neural Network Emulators for Multi-Probe Data Vectors Part I: Forecasting the Growth-Geometry split","abstract":"We present a new class of machine-learning emulators that accurately model the cosmic shear, galaxy-galaxy lensing, and galaxy clustering real space correlation functions in the context of Rubin Observatory year one simulated data. To illustrate its capabilities in forecasting models beyond the standard $\\Lambda$CDM, we forecast how well LSST Year 1 data will be able to probe the consistency between geometry $\\Omega^{\\rm geo}_\\mathrm{m}$ and growth $\\Omega^{\\rm growth}_\\mathrm{m}$ dark matter densities in the so-called split $\\Lambda$CDM parameterization. When trained with a few million samples, our emulator shows uniform accuracy across a wide range in an 18-dimensional parameter space. We provide a detailed comparison of three neural network designs, illustrating the importance of adopting state-of-the-art Transformer blocks. Our study also details their performance when computing Bayesian evidence for cosmic shear on three fiducial cosmologies. The transformers-based emulator is always accurate within PolyChord's precision. As an application, we use our emulator to study the degeneracies between dark energy models and growth geometry split parameterizations. We find that the growth-geometry split remains to be a meaningful test of the smooth dark energy assumption.","sentences":["We present a new class of machine-learning emulators that accurately model the cosmic shear, galaxy-galaxy lensing, and galaxy clustering real space correlation functions in the context of Rubin Observatory year one simulated data.","To illustrate its capabilities in forecasting models beyond the standard $\\Lambda$CDM, we forecast how well LSST","Year 1 data will be able to probe the consistency between geometry $\\Omega^{\\rm geo}_\\mathrm{m}$ and growth $\\Omega^{\\rm growth}_\\mathrm{m}$ dark matter densities in the so-called split $\\Lambda$CDM parameterization.","When trained with a few million samples, our emulator shows uniform accuracy across a wide range in an 18-dimensional parameter space.","We provide a detailed comparison of three neural network designs, illustrating the importance of adopting state-of-the-art Transformer blocks.","Our study also details their performance when computing Bayesian evidence for cosmic shear on three fiducial cosmologies.","The transformers-based emulator is always accurate within PolyChord's precision.","As an application, we use our emulator to study the degeneracies between dark energy models and growth geometry split parameterizations.","We find that the growth-geometry split remains to be a meaningful test of the smooth dark energy assumption."],"url":"http://arxiv.org/abs/2402.17716v1","category":"astro-ph.CO"}
{"created":"2024-02-27 17:08:47","title":"A Complete Graphical Language for Linear Optical Circuits with Finite-Photon-Number Sources and Detectors","abstract":"Linear optical circuits can be used to manipulate the quantum states of photons as they pass through components including beam splitters and phase shifters. Those photonic states possess a particularly high level of expressiveness, as they reside within the bosonic Fock space, an infinite-dimensional Hilbert space. However, in the domain of linear optical quantum computation, these basic components may not be sufficient to efficiently perform all computations of interest, such as universal quantum computation. To address this limitation it is common to add auxiliary sources and detectors, which enable projections onto auxiliary photonic states and thus increase the versatility of the processes. In this paper, we introduce the $\\textbf{LO}_{fi}$-calculus, a graphical language to reason on the infinite-dimensional bosonic Fock space with circuits composed of four core elements of linear optics: the phase shifter, the beam splitter, and auxiliary sources and detectors with bounded photon number. We present an equational theory that we prove to be complete: two $\\textbf{LO}_{fi}$-circuits represent the same quantum process if and only if one can be transformed into the other with the rules of the $\\textbf{LO}_{fi}$-calculus. We give a unique and compact universal form for such circuits.","sentences":["Linear optical circuits can be used to manipulate the quantum states of photons as they pass through components including beam splitters and phase shifters.","Those photonic states possess a particularly high level of expressiveness, as they reside within the bosonic Fock space, an infinite-dimensional Hilbert space.","However, in the domain of linear optical quantum computation, these basic components may not be sufficient to efficiently perform all computations of interest, such as universal quantum computation.","To address this limitation it is common to add auxiliary sources and detectors, which enable projections onto auxiliary photonic states and thus increase the versatility of the processes.","In this paper, we introduce the $\\textbf{LO}_{fi}$-calculus, a graphical language to reason on the infinite-dimensional bosonic Fock space with circuits composed of four core elements of linear optics: the phase shifter, the beam splitter, and auxiliary sources and detectors with bounded photon number.","We present an equational theory that we prove to be complete: two $\\textbf{LO}_{fi}$-circuits represent the same quantum process if and only if one can be transformed into the other with the rules of the $\\textbf{LO}_{fi}$-calculus.","We give a unique and compact universal form for such circuits."],"url":"http://arxiv.org/abs/2402.17693v1","category":"quant-ph"}
{"created":"2024-02-27 17:07:25","title":"Expansion dynamics of Bose-Einstein condensates in a synthetic magnetic field","abstract":"We investigate the expansion dynamics of spin-orbit-coupled Bose-Einstein condensates subjected to a synthetic magnetic field, after their release from an external harmonic trap. Our findings reveal that the condensate experiences a spin-dependent rotation and separation due to the rigid-like rotational velocity field, which leads to a spin density deflection. The deflection angle reaches a peak at a time that is inversely related to the frequency of the harmonic trap. When the detuning gradient is below a critical value for vortex nucleation, our analytical results derived from a spinor hydrodynamic theory align closely with numerical results using the coupled Gross-Pitaevskii equations. Beyond this critical value, we also numerically simulated the expansion dynamics of the condensates containing vortices with negative circulation. Our findings highlight the pivotal role of the rigid-like rotational velocity field on the dynamics of the condensate and may stimulate further experimental investigations into the rich superfluid dynamics induced by synthetic magnetic fields.","sentences":["We investigate the expansion dynamics of spin-orbit-coupled Bose-Einstein condensates subjected to a synthetic magnetic field, after their release from an external harmonic trap.","Our findings reveal that the condensate experiences a spin-dependent rotation and separation due to the rigid-like rotational velocity field, which leads to a spin density deflection.","The deflection angle reaches a peak at a time that is inversely related to the frequency of the harmonic trap.","When the detuning gradient is below a critical value for vortex nucleation, our analytical results derived from a spinor hydrodynamic theory align closely with numerical results using the coupled Gross-Pitaevskii equations.","Beyond this critical value, we also numerically simulated the expansion dynamics of the condensates containing vortices with negative circulation.","Our findings highlight the pivotal role of the rigid-like rotational velocity field on the dynamics of the condensate and may stimulate further experimental investigations into the rich superfluid dynamics induced by synthetic magnetic fields."],"url":"http://arxiv.org/abs/2402.17691v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-27 16:27:06","title":"TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations","abstract":"Achieving a balance between computational speed, prediction accuracy, and universal applicability in molecular simulations has been a persistent challenge. This paper presents substantial advancements in the TorchMD-Net software, a pivotal step forward in the shift from conventional force fields to neural network-based potentials. The evolution of TorchMD-Net into a more comprehensive and versatile framework is highlighted, incorporating cutting-edge architectures such as TensorNet. This transformation is achieved through a modular design approach, encouraging customized applications within the scientific community. The most notable enhancement is a significant improvement in computational efficiency, achieving a very remarkable acceleration in the computation of energy and forces for TensorNet models, with performance gains ranging from 2-fold to 10-fold over previous iterations. Other enhancements include highly optimized neighbor search algorithms that support periodic boundary conditions and the smooth integration with existing molecular dynamics frameworks. Additionally, the updated version introduces the capability to integrate physical priors, further enriching its application spectrum and utility in research. The software is available at https://github.com/torchmd/torchmd-net.","sentences":["Achieving a balance between computational speed, prediction accuracy, and universal applicability in molecular simulations has been a persistent challenge.","This paper presents substantial advancements in the TorchMD-Net software, a pivotal step forward in the shift from conventional force fields to neural network-based potentials.","The evolution of TorchMD-Net into a more comprehensive and versatile framework is highlighted, incorporating cutting-edge architectures such as TensorNet.","This transformation is achieved through a modular design approach, encouraging customized applications within the scientific community.","The most notable enhancement is a significant improvement in computational efficiency, achieving a very remarkable acceleration in the computation of energy and forces for TensorNet models, with performance gains ranging from 2-fold to 10-fold over previous iterations.","Other enhancements include highly optimized neighbor search algorithms that support periodic boundary conditions and the smooth integration with existing molecular dynamics frameworks.","Additionally, the updated version introduces the capability to integrate physical priors, further enriching its application spectrum and utility in research.","The software is available at https://github.com/torchmd/torchmd-net."],"url":"http://arxiv.org/abs/2402.17660v1","category":"cs.LG"}
{"created":"2024-02-27 16:12:51","title":"The critical disordered pinning measure","abstract":"In this paper, we study a disordered pinning model induced by a random walk whose increments have a finite fourth moment and vanishing first and third moments. It is known that this model is marginally relevant, and moreover, it undergoes a phase transition in an intermediate disorder regime. We show that, in the critical window, the point-to-point partition functions converge to a unique limiting random measure, which we call the critical disordered pinning measure. We also obtain an analogous result for a continuous counterpart to the pinning model, which is closely related to two other models: one is a critical stochastic Volterra equation that gives rise to a rough volatility model, and the other is a critical stochastic heat equation with multiplicative noise that is white in time and delta in space.","sentences":["In this paper, we study a disordered pinning model induced by a random walk whose increments have a finite fourth moment and vanishing first and third moments.","It is known that this model is marginally relevant, and moreover, it undergoes a phase transition in an intermediate disorder regime.","We show that, in the critical window, the point-to-point partition functions converge to a unique limiting random measure, which we call the critical disordered pinning measure.","We also obtain an analogous result for a continuous counterpart to the pinning model, which is closely related to two other models: one is a critical stochastic Volterra equation that gives rise to a rough volatility model, and the other is a critical stochastic heat equation with multiplicative noise that is white in time and delta in space."],"url":"http://arxiv.org/abs/2402.17642v1","category":"math.PR"}
{"created":"2024-02-27 15:46:51","title":"On the blow-up for a Kuramoto-Velarde type equation","abstract":"It is known that the Kuramoto-Velarde equation is globally well-posed on Sobolev spaces in the case when the parameters $\\gamma_1$ and $\\gamma_2$ involved in the non-linear terms verify $ \\gamma_1=\\frac{\\gamma_1}{2}$ or $\\gamma_2=0$. In the complementary case of these parameters, the global existence or blow-up of solutions is a completely open (and hard) problem. Motivated by this fact, in this work we consider a non-local version of the Kuramoto-Velarde equation. This equation allows us to apply a Fourier-based method and, within the framework $\\gamma_2\\neq \\frac{\\gamma_1}{2}$ and $\\gamma_2\\neq 0$, we show that large values of these parameters yield a blow-up in finite time of solutions in the Sobolev norm.","sentences":["It is known that the Kuramoto-Velarde equation is globally well-posed on Sobolev spaces in the case when the parameters $\\gamma_1$ and $\\gamma_2$ involved in the non-linear terms verify $ \\gamma_1=\\frac{\\gamma_1}{2}$ or $\\gamma_2=0$. In the complementary case of these parameters, the global existence or blow-up of solutions is a completely open (and hard) problem.","Motivated by this fact, in this work we consider a non-local version of the Kuramoto-Velarde equation.","This equation allows us to apply a Fourier-based method and, within the framework $\\gamma_2\\neq \\frac{\\gamma_1}{2}$ and $\\gamma_2\\neq 0$, we show that large values of these parameters yield a blow-up in finite time of solutions in the Sobolev norm."],"url":"http://arxiv.org/abs/2402.17619v1","category":"math.AP"}
{"created":"2024-02-27 15:06:45","title":"Quasi-Classical Gluon Fields and Low's Soft Theorem at Small $x$","abstract":"In the high energy limit, soft gluons can be approximately described by quasi-classical gluon fields. It is well-known that the gluon field is a pure gauge field on the transverse plane at eikonal order. We derived the complete next-to-eikonal order solutions of the classical Yang-Mills equations for soft gluons in the dense nuclear regime. Utilizing these solutions, it is shown that Low's soft theorem at small $x$ can be obtained by considering off-diagonal matrix elements of quasi-classical chromoelectric field between single gluon states in the dilute regime. Furthermore, we extend Low's soft theorem at small $x$ to incorporate the effects of gluon saturation in the dense regime.","sentences":["In the high energy limit, soft gluons can be approximately described by quasi-classical gluon fields.","It is well-known that the gluon field is a pure gauge field on the transverse plane at eikonal order.","We derived the complete next-to-eikonal order solutions of the classical Yang-Mills equations for soft gluons in the dense nuclear regime.","Utilizing these solutions, it is shown that Low's soft theorem at small $x$ can be obtained by considering off-diagonal matrix elements of quasi-classical chromoelectric field between single gluon states in the dilute regime.","Furthermore, we extend Low's soft theorem at small $x$ to incorporate the effects of gluon saturation in the dense regime."],"url":"http://arxiv.org/abs/2402.17568v1","category":"hep-ph"}
{"created":"2024-02-27 15:06:26","title":"Third order estimates and the regularity of the stress field for solutions to $p$-Laplace equations","abstract":"We consider solutions to   $$ - \\Delta_{p} u = f(x) \\quad \\text{in } \\Omega\\, ,$$   when $p$ approaches the semilinear limiting case $p=2$ and we get third order estimates. As a consequence we deduce improved regularity properties of the stress field.","sentences":["We consider solutions to   $$ - \\Delta_{p} u = f(x) \\quad \\text{in } \\Omega\\, ,$$   when $p$ approaches the semilinear limiting case $p=2$","and we get third order estimates.","As a consequence we deduce improved regularity properties of the stress field."],"url":"http://arxiv.org/abs/2402.17566v1","category":"math.AP"}
{"created":"2024-02-27 14:41:09","title":"On geodesic orbit nilmanifolds","abstract":"The paper is devoted to the study of geodesic orbit Riemannian metrics on nilpotent Lie groups. The main result is the construction of continuous families of pairwise non-isomorphic connected and simply connected nilpotent Lie groups, every of which admits geodesic orbit metrics. The minimum dimension of groups in the constructed families is $10$.","sentences":["The paper is devoted to the study of geodesic orbit Riemannian metrics on nilpotent Lie groups.","The main result is the construction of continuous families of pairwise non-isomorphic connected and simply connected nilpotent Lie groups, every of which admits geodesic orbit metrics.","The minimum dimension of groups in the constructed families is $10$."],"url":"http://arxiv.org/abs/2402.17548v1","category":"math.DG"}
{"created":"2024-02-27 14:25:47","title":"Optimal Stopping of BSDEs with Constrained Jumps and Related Double Obstacle PDEs","abstract":"We consider partial differential equations (PDEs) characterized by an upper barrier that depends on the solution itself and a fixed lower barrier, while accommodating a non-local driver. First, we show a Feynman-Kac representation for the PDE when the driver is local. Specifically, we relate the non-linear Snell envelope for an optimal stopping problem, where the underlying process is the first component in the solution to a stopped backward stochastic differential equation (BSDE) with jumps and a constraint on the jumps process, to a viscosity solution for the PDE. Leveraging this Feynman-Kac representation, we subsequently prove existence and uniqueness of viscosity solutions in the non-local setting by employing a contraction argument. In addition, the contraction argument yields existence of a new type of non-linear Snell envelope and extends the theory of probabilistic representation for PDEs.","sentences":["We consider partial differential equations (PDEs) characterized by an upper barrier that depends on the solution itself and a fixed lower barrier, while accommodating a non-local driver.","First, we show a Feynman-Kac representation for the PDE when the driver is local.","Specifically, we relate the non-linear Snell envelope for an optimal stopping problem, where the underlying process is the first component in the solution to a stopped backward stochastic differential equation (BSDE) with jumps and a constraint on the jumps process, to a viscosity solution for the PDE.","Leveraging this Feynman-Kac representation, we subsequently prove existence and uniqueness of viscosity solutions in the non-local setting by employing a contraction argument.","In addition, the contraction argument yields existence of a new type of non-linear Snell envelope and extends the theory of probabilistic representation for PDEs."],"url":"http://arxiv.org/abs/2402.17541v1","category":"math.PR"}
{"created":"2024-02-27 14:01:21","title":"The Strong CP Problem in the Quantum Rotor","abstract":"Recent studies have claimed that the strong CP problem does not occur in QCD, proposing a new order of limits in volume and topological sectors when studying observables on the lattice. In order to shed light on this issue, we study the effect of the topological $\\theta$-term on a simple quantum mechanical rotor that allows a lattice description. The topological susceptibility and the $\\theta$-dependence of the energy spectrum are both computed using local lattice correlation functions. The sign problem is overcome by considering Taylor expansions in $\\theta$ exploiting automatic differentiation methods for Monte Carlo processes. Our findings confirm the conventional wisdom on the strong CP problem.","sentences":["Recent studies have claimed that the strong CP problem does not occur in QCD, proposing a new order of limits in volume and topological sectors when studying observables on the lattice.","In order to shed light on this issue, we study the effect of the topological $\\theta$-term on a simple quantum mechanical rotor that allows a lattice description.","The topological susceptibility and the $\\theta$-dependence of the energy spectrum are both computed using local lattice correlation functions.","The sign problem is overcome by considering Taylor expansions in $\\theta$ exploiting automatic differentiation methods for Monte Carlo processes.","Our findings confirm the conventional wisdom on the strong CP problem."],"url":"http://arxiv.org/abs/2402.17518v1","category":"hep-lat"}
{"created":"2024-02-27 13:08:47","title":"AlignMiF: Geometry-Aligned Multimodal Implicit Field for LiDAR-Camera Joint Synthesis","abstract":"Neural implicit fields have been a de facto standard in novel view synthesis. Recently, there exist some methods exploring fusing multiple modalities within a single field, aiming to share implicit features from different modalities to enhance reconstruction performance. However, these modalities often exhibit misaligned behaviors: optimizing for one modality, such as LiDAR, can adversely affect another, like camera performance, and vice versa. In this work, we conduct comprehensive analyses on the multimodal implicit field of LiDAR-camera joint synthesis, revealing the underlying issue lies in the misalignment of different sensors. Furthermore, we introduce AlignMiF, a geometrically aligned multimodal implicit field with two proposed modules: Geometry-Aware Alignment (GAA) and Shared Geometry Initialization (SGI). These modules effectively align the coarse geometry across different modalities, significantly enhancing the fusion process between LiDAR and camera data. Through extensive experiments across various datasets and scenes, we demonstrate the effectiveness of our approach in facilitating better interaction between LiDAR and camera modalities within a unified neural field. Specifically, our proposed AlignMiF, achieves remarkable improvement over recent implicit fusion methods (+2.01 and +3.11 image PSNR on the KITTI-360 and Waymo datasets) and consistently surpasses single modality performance (13.8% and 14.2% reduction in LiDAR Chamfer Distance on the respective datasets).","sentences":["Neural implicit fields have been a de facto standard in novel view synthesis.","Recently, there exist some methods exploring fusing multiple modalities within a single field, aiming to share implicit features from different modalities to enhance reconstruction performance.","However, these modalities often exhibit misaligned behaviors: optimizing for one modality, such as LiDAR, can adversely affect another, like camera performance, and vice versa.","In this work, we conduct comprehensive analyses on the multimodal implicit field of LiDAR-camera joint synthesis, revealing the underlying issue lies in the misalignment of different sensors.","Furthermore, we introduce AlignMiF, a geometrically aligned multimodal implicit field with two proposed modules: Geometry-Aware Alignment (GAA) and Shared Geometry Initialization (SGI).","These modules effectively align the coarse geometry across different modalities, significantly enhancing the fusion process between LiDAR and camera data.","Through extensive experiments across various datasets and scenes, we demonstrate the effectiveness of our approach in facilitating better interaction between LiDAR and camera modalities within a unified neural field.","Specifically, our proposed AlignMiF, achieves remarkable improvement over recent implicit fusion methods (+2.01 and +3.11 image PSNR on the KITTI-360 and Waymo datasets) and consistently surpasses single modality performance (13.8% and 14.2% reduction in LiDAR Chamfer Distance on the respective datasets)."],"url":"http://arxiv.org/abs/2402.17483v1","category":"cs.CV"}
{"created":"2024-02-27 11:44:11","title":"Higher-loop integrated negative geometries in ABJM","abstract":"In the three-dimensional ${\\cal N}=6$ Chern-Simons matter (ABJM) theory, the integrand for the logarithm of the scattering amplitude admits a decomposition in terms of negative geometries, which implies that all the infrared divergences concentrate in the last loop integration. We compute the infrared-finite functions that arise from performing a three-loop integration over the four-loop integrand for the logarithm of the four-point amplitude, for which we use the method of differential equations. Our results provide a direct computation of the four-loop cusp anomalous dimension of the theory, in agreement with the current all-loop integrability-based proposal. We find an apparent simplicity in the leading singularities of the integrated results, provided one works in the frame in which the unintegrated loop variable goes to infinity. Finally, our results suggest an alternating sign pattern for the integrated negative geometries in the Euclidean region.","sentences":["In the three-dimensional ${\\cal N}=6$ Chern-Simons matter (ABJM) theory, the integrand for the logarithm of the scattering amplitude admits a decomposition in terms of negative geometries, which implies that all the infrared divergences concentrate in the last loop integration.","We compute the infrared-finite functions that arise from performing a three-loop integration over the four-loop integrand for the logarithm of the four-point amplitude, for which we use the method of differential equations.","Our results provide a direct computation of the four-loop cusp anomalous dimension of the theory, in agreement with the current all-loop integrability-based proposal.","We find an apparent simplicity in the leading singularities of the integrated results, provided one works in the frame in which the unintegrated loop variable goes to infinity.","Finally, our results suggest an alternating sign pattern for the integrated negative geometries in the Euclidean region."],"url":"http://arxiv.org/abs/2402.17432v1","category":"hep-th"}
{"created":"2024-02-27 10:22:29","title":"$\\ast$-conformal Einstein solitons on N(k)-contact metric manifolds","abstract":"The main goal of this paper is devoted to N(k)-contact metric manifolds admitting $\\ast$-conformal Einstein soliton and also $\\ast$-conformal gradient Einstein soliton. In this settings the nature of the manifold, and the potential vector field, potential function of solitons are characterized, and conditions for the $\\ast$-conformal Einstein soliton to be expanding, steady, or shrinking are also given. Furthermore, the nature of the potential vector field is evolved when the metric g of N(k)-contact metric manifold satisfies $\\ast$-conformal gradient Einstein soliton. Finally, an illustrative example of a N(k)-contact metric manifold is discussed to verify our findings.","sentences":["The main goal of this paper is devoted to N(k)-contact metric manifolds admitting $\\ast$-conformal Einstein soliton and also $\\ast$-conformal gradient Einstein soliton.","In this settings the nature of the manifold, and the potential vector field, potential function of solitons are characterized, and conditions for the $\\ast$-conformal Einstein soliton to be expanding, steady, or shrinking are also given.","Furthermore, the nature of the potential vector field is evolved when the metric g of N(k)-contact metric manifold satisfies $\\ast$-conformal gradient Einstein soliton.","Finally, an illustrative example of a N(k)-contact metric manifold is discussed to verify our findings."],"url":"http://arxiv.org/abs/2402.17380v1","category":"math.DG"}
{"created":"2024-02-27 10:03:03","title":"Coupled-cluster approach to Coster-Kronig decay and Auger decay in hydrogen sulfide and argon","abstract":"We perform ab initio simulations of the total and partial Auger decay widths of 1s^-1, 2s^-1, and 2p^-1 ionized hydrogen sulfide and 2s^-1 ionized argon with non-Hermitian quantum chemistry. We use coupled cluster theory with single and double substitutions (CCSD) and equation of motion CCSD (EOM-CCSD) and discuss the novel application of (equation of motion-) second order M{\\o}ller-Plesset perturbation theory (MP2). We find good agreement between the methods for the 1s^-1 hole of H2S, whereas for the other holes we can only use the EOM methods. We obtain very large decay widths of the 2s^-1-vacant states due to intense Coster-Kronig transitions with excellent agreement to experiments. The three 2p^-1 holes show completely different spectra because a decay channel is only significant when one of the final holes is spatially aligned with the initial hole. Lastly, we observe that triplet channels are much more important for the 2s^-1 and 2p^-1 holes than for the 1s^-1 hole, for which it is well known that triplet channels only contribute weakly to the total Auger intensity.","sentences":["We perform ab initio simulations of the total and partial Auger decay widths of 1s^-1, 2s^-1, and 2p^-1 ionized hydrogen sulfide and 2s^-1 ionized argon with non-Hermitian quantum chemistry.","We use coupled cluster theory with single and double substitutions (CCSD) and equation of motion CCSD (EOM-CCSD) and discuss the novel application of (equation of motion-) second order M{\\o}ller-Plesset perturbation theory (MP2).","We find good agreement between the methods for the 1s^-1 hole of H2S, whereas for the other holes we can only use the EOM methods.","We obtain very large decay widths of the 2s^-1-vacant states due to intense Coster-Kronig transitions with excellent agreement to experiments.","The three 2p^-1 holes show completely different spectra because a decay channel is only significant when one of the final holes is spatially aligned with the initial hole.","Lastly, we observe that triplet channels are much more important for the 2s^-1 and 2p^-1 holes than for the 1s^-1 hole, for which it is well known that triplet channels only contribute weakly to the total Auger intensity."],"url":"http://arxiv.org/abs/2402.17368v1","category":"physics.chem-ph"}
{"created":"2024-02-27 09:54:15","title":"A granular model for crowd motion and pedestrian flow","abstract":"We study a granular model for congested crowd motion and pedestrian flow. Our approach is based on an approximation through a Hele-Shaw type equation involving a degenerate operator of $p$-Laplacian type and a linear drift, for which we prove existence and uniqueness using nonlinear semigroup methods and the doubling variables technique. Our main result shows that, as $p \\to \\infty$, the weak solutions of the $p-$problem converge to a solution of the congested crowd motion problem.","sentences":["We study a granular model for congested crowd motion and pedestrian flow.","Our approach is based on an approximation through a Hele-Shaw type equation involving a degenerate operator of $p$-Laplacian type and a linear drift, for which we prove existence and uniqueness using nonlinear semigroup methods and the doubling variables technique.","Our main result shows that, as $p \\to \\infty$, the weak solutions of the $p-$problem converge to a solution of the congested crowd motion problem."],"url":"http://arxiv.org/abs/2402.17361v1","category":"math.AP"}
{"created":"2024-02-27 09:44:55","title":"H\u00f6rmander type Fourier multiplier theorem and Nikolskii inequality on quantum tori, and applications","abstract":"In this paper, we study H\\\"ormander type Fourier multiplier theorem and the Nikolskii inequality on quantum tori. On the way to obtain these results, we also prove some classical inequalities such as Paley, Hausdorff-Young, Hausdorff-Young-Paley, Hardy-Littlewood, and Logarithmic Sobolev inequalities on quantum tori. As applications we establish embedding theorems between Sobolev, Besov spaces as well as embeddings between Besov and Wiener and Beurling spaces on quantum tori. We also analyse $\\beta$-versions of Wiener and Beurling spaces and their embeddings, and interpolation properties of all these spaces on quantum tori. As an applications of the analysis, we also derive a version of the Nash inequality, and the time decay for solutions of a heat type equation.","sentences":["In this paper, we study H\\\"ormander type Fourier multiplier theorem and the Nikolskii inequality on quantum tori.","On the way to obtain these results, we also prove some classical inequalities such as Paley, Hausdorff-Young, Hausdorff-Young-Paley, Hardy-Littlewood, and Logarithmic Sobolev inequalities on quantum tori.","As applications we establish embedding theorems between Sobolev, Besov spaces as well as embeddings between Besov and Wiener and Beurling spaces on quantum tori.","We also analyse $\\beta$-versions of Wiener and Beurling spaces and their embeddings, and interpolation properties of all these spaces on quantum tori.","As an applications of the analysis, we also derive a version of the Nash inequality, and the time decay for solutions of a heat type equation."],"url":"http://arxiv.org/abs/2402.17353v1","category":"math.FA"}
{"created":"2024-02-27 09:41:59","title":"ICP-Flow: LiDAR Scene Flow Estimation with ICP","abstract":"Scene flow characterizes the 3D motion between two LiDAR scans captured by an autonomous vehicle at nearby timesteps. Prevalent methods consider scene flow as point-wise unconstrained flow vectors that can be learned by either large-scale training beforehand or time-consuming optimization at inference. However, these methods do not take into account that objects in autonomous driving often move rigidly. We incorporate this rigid-motion assumption into our design, where the goal is to associate objects over scans and then estimate the locally rigid transformations. We propose ICP-Flow, a learning-free flow estimator. The core of our design is the conventional Iterative Closest Point (ICP) algorithm, which aligns the objects over time and outputs the corresponding rigid transformations. Crucially, to aid ICP, we propose a histogram-based initialization that discovers the most likely translation, thus providing a good starting point for ICP. The complete scene flow is then recovered from the rigid transformations. We outperform state-of-the-art baselines, including supervised models, on the Waymo dataset and perform competitively on Argoverse-v2 and nuScenes. Further, we train a feedforward neural network, supervised by the pseudo labels from our model, and achieve top performance among all models capable of real-time inference. We validate the advantage of our model on scene flow estimation with longer temporal gaps, up to 0.5 seconds where other models fail to deliver meaningful results.","sentences":["Scene flow characterizes the 3D motion between two LiDAR scans captured by an autonomous vehicle at nearby timesteps.","Prevalent methods consider scene flow as point-wise unconstrained flow vectors that can be learned by either large-scale training beforehand or time-consuming optimization at inference.","However, these methods do not take into account that objects in autonomous driving often move rigidly.","We incorporate this rigid-motion assumption into our design, where the goal is to associate objects over scans and then estimate the locally rigid transformations.","We propose ICP-Flow, a learning-free flow estimator.","The core of our design is the conventional Iterative Closest Point (ICP) algorithm, which aligns the objects over time and outputs the corresponding rigid transformations.","Crucially, to aid ICP, we propose a histogram-based initialization that discovers the most likely translation, thus providing a good starting point for ICP.","The complete scene flow is then recovered from the rigid transformations.","We outperform state-of-the-art baselines, including supervised models, on the Waymo dataset and perform competitively on Argoverse-v2 and nuScenes.","Further, we train a feedforward neural network, supervised by the pseudo labels from our model, and achieve top performance among all models capable of real-time inference.","We validate the advantage of our model on scene flow estimation with longer temporal gaps, up to 0.5 seconds where other models fail to deliver meaningful results."],"url":"http://arxiv.org/abs/2402.17351v1","category":"cs.CV"}
{"created":"2024-02-27 09:33:51","title":"Ab initio simulations of neutron stars' oblique electrospheres with realistic neutron star parameters","abstract":"Electrospheres are environments with the same origin as pulsars; a highly magnetized rotating neutron star. In pulsars, a cascade of electron-positron pair creation enriches the plasma. The plasma surrounding an electrosphere consists only of particles that have escaped from the neutron star's surface. Electrospheres with a magnetic axis aligned with the rotation axis have been well described for decades. Models of electrospheres with an oblique magnetic axis relative to the rotation axis have resisted most theoretical investigations. Some electrospheres and pulsars have been simulated using particle-in-cell codes, but the numerical constraints did not allow the use of realistic neutron star parameters. We aimed to develop a numerical simulation code optimized for understanding the physics of electrospheres and pulsars, with realistic neutron star parameters. As a first step, presented in this paper, we focused on the simulation of oblique electrospheres with realistic physical parameters. A specific code was developed for the computation of stationary solutions. The resolution of Maxwell's equations was based on spectral methods. Particle motions included their finite inertia. No hypothesis was made in relation to the force-free behavior of the electrospheric plasma. The numerical code is called Pulsar ARoMa (pulsar asymmetric rotating magnetosphere). Various numerical simulations were conducted using realistic neutron star parameters. We find that oblique electrospheres possess the same global structure as aligned force-free electrospheres, with two domes of electrons and a torus of positively charged particles. The domes are not centered on the magnetic axis; nor are they symmetric. Yet, the solutions do not exhibit a force-free behavior. The simulations performed with the Pulsar ARoMa code require modest resources and little computing time. This code will be upgraded for more ambitious investigations into pulsar physics.","sentences":["Electrospheres are environments with the same origin as pulsars; a highly magnetized rotating neutron star.","In pulsars, a cascade of electron-positron pair creation enriches the plasma.","The plasma surrounding an electrosphere consists only of particles that have escaped from the neutron star's surface.","Electrospheres with a magnetic axis aligned with the rotation axis have been well described for decades.","Models of electrospheres with an oblique magnetic axis relative to the rotation axis have resisted most theoretical investigations.","Some electrospheres and pulsars have been simulated using particle-in-cell codes, but the numerical constraints did not allow the use of realistic neutron star parameters.","We aimed to develop a numerical simulation code optimized for understanding the physics of electrospheres and pulsars, with realistic neutron star parameters.","As a first step, presented in this paper, we focused on the simulation of oblique electrospheres with realistic physical parameters.","A specific code was developed for the computation of stationary solutions.","The resolution of Maxwell's equations was based on spectral methods.","Particle motions included their finite inertia.","No hypothesis was made in relation to the force-free behavior of the electrospheric plasma.","The numerical code is called Pulsar ARoMa (pulsar asymmetric rotating magnetosphere).","Various numerical simulations were conducted using realistic neutron star parameters.","We find that oblique electrospheres possess the same global structure as aligned force-free electrospheres, with two domes of electrons and a torus of positively charged particles.","The domes are not centered on the magnetic axis; nor are they symmetric.","Yet, the solutions do not exhibit a force-free behavior.","The simulations performed with the Pulsar ARoMa code require modest resources and little computing time.","This code will be upgraded for more ambitious investigations into pulsar physics."],"url":"http://arxiv.org/abs/2402.17348v1","category":"astro-ph.HE"}
{"created":"2024-02-27 09:33:00","title":"Existence and invariant measure of pullback attractors for 3D Navier-Stokes-Voigt equations with delay","abstract":"In this paper, we study the long-time dynamics of 3D non-autonomous Navier-Stokes-Voigt(NSV) equations with delay. Inspired by [36], we use the contractive function method to prove the pullback D-asymptotical compactness and existence of the pullback attractors. Furthermore, we verify the regularity of pullback attractors by the method in [14, 43, 47] and there exists a unique family of Borel invariant probability measures which is supported by the pullback attractors.","sentences":["In this paper, we study the long-time dynamics of 3D non-autonomous Navier-Stokes-Voigt(NSV) equations with delay.","Inspired by [36], we use the contractive function method to prove the pullback D-asymptotical compactness and existence of the pullback attractors.","Furthermore, we verify the regularity of pullback attractors by the method in [14, 43, 47] and there exists a unique family of Borel invariant probability measures which is supported by the pullback attractors."],"url":"http://arxiv.org/abs/2402.17347v1","category":"math.AP"}
{"created":"2024-02-27 09:02:30","title":"Bypass mechanism of F$_1$-ATPase for asymmetric enzyme kinetics","abstract":"We discovered novel enzyme kinetics of F$_1$-ATPase, a biomolecular motor that synthesizes and hydrolyzes adenosine triphosphate (ATP), using single-molecule experiments and numerical simulations. The enzyme kinetics of F$_1$-ATPase followed the Michaelis-Menten equation in ATP hydrolysis but deviated from it in ATP synthesis, indicating asymmetric enzyme kinetics between ATP synthesis and hydrolysis. Numerical analysis based on a theoretical model revealed a bypass mechanism underlying asymmetric enzyme kinetics. In particular, we found that the origin of the asymmetric enzyme kinetics lies in the asymmetry of the allosterism, not in the asymmetry of potential shapes. The asymmetric enzyme kinetics may suggest that F$_1$-ATPase is designed to sustain the rate of ATP synthesis while suppressing the futile ATP consumption.","sentences":["We discovered novel enzyme kinetics of F$_1$-ATPase, a biomolecular motor that synthesizes and hydrolyzes adenosine triphosphate (ATP), using single-molecule experiments and numerical simulations.","The enzyme kinetics of F$_1$-ATPase followed the Michaelis-Menten equation in ATP hydrolysis but deviated from it in ATP synthesis, indicating asymmetric enzyme kinetics between ATP synthesis and hydrolysis.","Numerical analysis based on a theoretical model revealed a bypass mechanism underlying asymmetric enzyme kinetics.","In particular, we found that the origin of the asymmetric enzyme kinetics lies in the asymmetry of the allosterism, not in the asymmetry of potential shapes.","The asymmetric enzyme kinetics may suggest that F$_1$-ATPase is designed to sustain the rate of ATP synthesis while suppressing the futile ATP consumption."],"url":"http://arxiv.org/abs/2402.17326v1","category":"physics.bio-ph"}
{"created":"2024-02-27 08:50:45","title":"Scaling Supervised Local Learning with Augmented Auxiliary Networks","abstract":"Deep neural networks are typically trained using global error signals that backpropagate (BP) end-to-end, which is not only biologically implausible but also suffers from the update locking problem and requires huge memory consumption. Local learning, which updates each layer independently with a gradient-isolated auxiliary network, offers a promising alternative to address the above problems. However, existing local learning methods are confronted with a large accuracy gap with the BP counterpart, particularly for large-scale networks. This is due to the weak coupling between local layers and their subsequent network layers, as there is no gradient communication across layers. To tackle this issue, we put forward an augmented local learning method, dubbed AugLocal. AugLocal constructs each hidden layer's auxiliary network by uniformly selecting a small subset of layers from its subsequent network layers to enhance their synergy. We also propose to linearly reduce the depth of auxiliary networks as the hidden layer goes deeper, ensuring sufficient network capacity while reducing the computational cost of auxiliary networks. Our extensive experiments on four image classification datasets (i.e., CIFAR-10, SVHN, STL-10, and ImageNet) demonstrate that AugLocal can effectively scale up to tens of local layers with a comparable accuracy to BP-trained networks while reducing GPU memory usage by around 40%. The proposed AugLocal method, therefore, opens up a myriad of opportunities for training high-performance deep neural networks on resource-constrained platforms.Code is available at https://github.com/ChenxiangMA/AugLocal.","sentences":["Deep neural networks are typically trained using global error signals that backpropagate (BP) end-to-end, which is not only biologically implausible but also suffers from the update locking problem and requires huge memory consumption.","Local learning, which updates each layer independently with a gradient-isolated auxiliary network, offers a promising alternative to address the above problems.","However, existing local learning methods are confronted with a large accuracy gap with the BP counterpart, particularly for large-scale networks.","This is due to the weak coupling between local layers and their subsequent network layers, as there is no gradient communication across layers.","To tackle this issue, we put forward an augmented local learning method, dubbed AugLocal.","AugLocal constructs each hidden layer's auxiliary network by uniformly selecting a small subset of layers from its subsequent network layers to enhance their synergy.","We also propose to linearly reduce the depth of auxiliary networks as the hidden layer goes deeper, ensuring sufficient network capacity while reducing the computational cost of auxiliary networks.","Our extensive experiments on four image classification datasets (i.e., CIFAR-10, SVHN, STL-10, and ImageNet) demonstrate that AugLocal can effectively scale up to tens of local layers with a comparable accuracy to BP-trained networks while reducing GPU memory usage by around 40%.","The proposed AugLocal method, therefore, opens up a myriad of opportunities for training high-performance deep neural networks on resource-constrained platforms.","Code is available at https://github.com/ChenxiangMA/AugLocal."],"url":"http://arxiv.org/abs/2402.17318v1","category":"cs.NE"}
{"created":"2024-02-27 08:32:47","title":"Asymptotically constant-free and polynomial-degree-robust a posteriori error estimates for time-harmonic Maxwell's equations","abstract":"We propose a novel a posteriori error estimator for the N\\'ed\\'elec finite element discretization of time-harmonic Maxwell's equations. After the approximation of the electric field is computed, we propose a fully localized algorithm to reconstruct approximations to the electric displacement and the magnetic field, with such approximations respectively fulfilling suitable divergence and curl constraints. These reconstructed fields are in turn used to construct an a posteriori error estimator which is shown to be reliable and efficient. Specifically, the estimator controls the error from above up to a constant that tends to one as the mesh is refined and/or the polynomial degree is increased, and from below up to constant independent of $p$. Both bounds are also fully-robust in the low-frequency regime. The properties of the proposed estimator are illustrated on a set of numerical examples.","sentences":["We propose a novel a posteriori error estimator for the N\\'ed\\'elec finite element discretization of time-harmonic Maxwell's equations.","After the approximation of the electric field is computed, we propose a fully localized algorithm to reconstruct approximations to the electric displacement and the magnetic field, with such approximations respectively fulfilling suitable divergence and curl constraints.","These reconstructed fields are in turn used to construct an a posteriori error estimator which is shown to be reliable and efficient.","Specifically, the estimator controls the error from above up to a constant that tends to one as the mesh is refined and/or the polynomial degree is increased, and from below up to constant independent of $p$. Both bounds are also fully-robust in the low-frequency regime.","The properties of the proposed estimator are illustrated on a set of numerical examples."],"url":"http://arxiv.org/abs/2402.17309v1","category":"math.NA"}
{"created":"2024-02-27 08:21:00","title":"Mesh-Agnostic Decoders for Supercritical Airfoil Prediction and Inverse Design","abstract":"Mesh-agnostic models have advantages in terms of processing unstructured spatial data and incorporating partial differential equations. Recently, they have been widely studied for constructing physics-informed neural networks, but they need to be trained on a case-by-case basis and require large training times. On the other hand, fast prediction and design tools are desired for aerodynamic shape designs, and data-driven mesh-based models have achieved great performance. Therefore, this paper proposes a data-driven mesh-agnostic decoder that combines the fast prediction ability of data-driven models and the flexibility of mesh-agnostic models. The model is denoted by an implicit decoder, which consists of two subnetworks, i.e., ShapeNet and HyperNet. ShapeNet is based on implicit neural representation, and HyperNet is a simple neural network. The implicit decoder is trained for the fast prediction of supercritical airfoils. Different activation functions are compared, and a spatial constraint is proposed to improve the interpretability and generalization ability of the model. Then, the implicit decoder is used together with a mesh-based encoder to build a generative model, which is used for the inverse design of supercritical airfoils with specified physical features.","sentences":["Mesh-agnostic models have advantages in terms of processing unstructured spatial data and incorporating partial differential equations.","Recently, they have been widely studied for constructing physics-informed neural networks, but they need to be trained on a case-by-case basis and require large training times.","On the other hand, fast prediction and design tools are desired for aerodynamic shape designs, and data-driven mesh-based models have achieved great performance.","Therefore, this paper proposes a data-driven mesh-agnostic decoder that combines the fast prediction ability of data-driven models and the flexibility of mesh-agnostic models.","The model is denoted by an implicit decoder, which consists of two subnetworks, i.e., ShapeNet and HyperNet.","ShapeNet is based on implicit neural representation, and HyperNet is a simple neural network.","The implicit decoder is trained for the fast prediction of supercritical airfoils.","Different activation functions are compared, and a spatial constraint is proposed to improve the interpretability and generalization ability of the model.","Then, the implicit decoder is used together with a mesh-based encoder to build a generative model, which is used for the inverse design of supercritical airfoils with specified physical features."],"url":"http://arxiv.org/abs/2402.17299v1","category":"physics.flu-dyn"}
{"created":"2024-02-27 06:53:53","title":"Hardy type inequalities on manifolds with nonnegative Ricci curvature","abstract":"We prove the Hardy inequality, Heisenberg-Pauli-Weyl inequality, Hardy-Sobolev inequality, and Caffarelli-Kohn-Nirenberg (CKN) inequality on manifolds with nonnegative Ricci curvature and Euclidean volume growth, of dimension n>=3.","sentences":["We prove the Hardy inequality, Heisenberg-Pauli-Weyl inequality, Hardy-Sobolev inequality, and Caffarelli-Kohn-Nirenberg (CKN) inequality on manifolds with nonnegative Ricci curvature and Euclidean volume growth, of dimension n>=3."],"url":"http://arxiv.org/abs/2402.17253v1","category":"math.DG"}
{"created":"2024-02-27 05:57:45","title":"Two-scale Neural Networks for Partial Differential Equations with Small Parameters","abstract":"We propose a two-scale neural network method for solving partial differential equations (PDEs) with small parameters using physics-informed neural networks (PINNs). We directly incorporate the small parameters into the architecture of neural networks. The proposed method enables solving PDEs with small parameters in a simple fashion, without adding Fourier features or other computationally taxing searches of truncation parameters. Various numerical examples demonstrate reasonable accuracy in capturing features of large derivatives in the solutions caused by small parameters.","sentences":["We propose a two-scale neural network method for solving partial differential equations (PDEs) with small parameters using physics-informed neural networks (PINNs).","We directly incorporate the small parameters into the architecture of neural networks.","The proposed method enables solving PDEs with small parameters in a simple fashion, without adding Fourier features or other computationally taxing searches of truncation parameters.","Various numerical examples demonstrate reasonable accuracy in capturing features of large derivatives in the solutions caused by small parameters."],"url":"http://arxiv.org/abs/2402.17232v1","category":"math.NA"}
{"created":"2024-02-27 05:04:00","title":"Purified and Unified Steganographic Network","abstract":"Steganography is the art of hiding secret data into the cover media for covert communication. In recent years, more and more deep neural network (DNN)-based steganographic schemes are proposed to train steganographic networks for secret embedding and recovery, which are shown to be promising. Compared with the handcrafted steganographic tools, steganographic networks tend to be large in size. It raises concerns on how to imperceptibly and effectively transmit these networks to the sender and receiver to facilitate the covert communication. To address this issue, we propose in this paper a Purified and Unified Steganographic Network (PUSNet). It performs an ordinary machine learning task in a purified network, which could be triggered into steganographic networks for secret embedding or recovery using different keys. We formulate the construction of the PUSNet into a sparse weight filling problem to flexibly switch between the purified and steganographic networks. We further instantiate our PUSNet as an image denoising network with two steganographic networks concealed for secret image embedding and recovery. Comprehensive experiments demonstrate that our PUSNet achieves good performance on secret image embedding, secret image recovery, and image denoising in a single architecture. It is also shown to be capable of imperceptibly carrying the steganographic networks in a purified network. Code is available at \\url{https://github.com/albblgb/PUSNet}","sentences":["Steganography is the art of hiding secret data into the cover media for covert communication.","In recent years, more and more deep neural network (DNN)-based steganographic schemes are proposed to train steganographic networks for secret embedding and recovery, which are shown to be promising.","Compared with the handcrafted steganographic tools, steganographic networks tend to be large in size.","It raises concerns on how to imperceptibly and effectively transmit these networks to the sender and receiver to facilitate the covert communication.","To address this issue, we propose in this paper a Purified and Unified Steganographic Network (PUSNet).","It performs an ordinary machine learning task in a purified network, which could be triggered into steganographic networks for secret embedding or recovery using different keys.","We formulate the construction of the PUSNet into a sparse weight filling problem to flexibly switch between the purified and steganographic networks.","We further instantiate our PUSNet as an image denoising network with two steganographic networks concealed for secret image embedding and recovery.","Comprehensive experiments demonstrate that our PUSNet achieves good performance on secret image embedding, secret image recovery, and image denoising in a single architecture.","It is also shown to be capable of imperceptibly carrying the steganographic networks in a purified network.","Code is available at \\url{https://github.com/albblgb/PUSNet}"],"url":"http://arxiv.org/abs/2402.17210v1","category":"cs.CR"}
{"created":"2024-02-27 04:33:43","title":"A computational method for angle-resolved photoemission spectra from repeated-slab band structure calculations","abstract":"A versatile method for angle-resolved photoemission spectra (ARPES) calculations is reported within the one-step model of photoemission. The initial states are obtained from a repeated-slab calculation using the projector-augmented wave (PAW) method. ARPES final states are constructed by matching the repeated-slab eigenstates of positive energy with free electron states that satisfy the time-reversed low-energy electron diffraction boundary conditions. Nonphysical solutions of the matching equations, which do not respect the flux conservation, are discarded. The method is applied to surface-normal photoemission from graphene as a function of photon energy from threshold up to 100 eV. The results are compared with independently performed multiple scattering calculations and very good agreement is obtained, provided that the photoemission matrix elements are computed with all-electron waves reconstructed from the PAW pseudo-waves. However, if the pseudo-waves are used directly, the relative intensity between $\\sigma$- and $\\pi$-band emission is wrong by an order of magnitude. The graphene ARPES intensity has a strong photon energy dependence including resonances. The normal emission spectrum from the $\\pi$-band shows a hitherto unreported, sharp resonance at a photon energy of 31 eV. The resonance is due to a 2$D$ interband transitions and highlights the importance of matrix element effects beyond the final state plane-wave approximation.","sentences":["A versatile method for angle-resolved photoemission spectra (ARPES) calculations is reported within the one-step model of photoemission.","The initial states are obtained from a repeated-slab calculation using the projector-augmented wave (PAW) method.","ARPES final states are constructed by matching the repeated-slab eigenstates of positive energy with free electron states that satisfy the time-reversed low-energy electron diffraction boundary conditions.","Nonphysical solutions of the matching equations, which do not respect the flux conservation, are discarded.","The method is applied to surface-normal photoemission from graphene as a function of photon energy from threshold up to 100 eV. The results are compared with independently performed multiple scattering calculations and very good agreement is obtained, provided that the photoemission matrix elements are computed with all-electron waves reconstructed from the PAW pseudo-waves.","However, if the pseudo-waves are used directly, the relative intensity between $\\sigma$- and $\\pi$-band emission is wrong by an order of magnitude.","The graphene ARPES intensity has a strong photon energy dependence including resonances.","The normal emission spectrum from the $\\pi$-band shows a hitherto unreported, sharp resonance at a photon energy of 31 eV. The resonance is due to a 2$D$ interband transitions and highlights the importance of matrix element effects beyond the final state plane-wave approximation."],"url":"http://arxiv.org/abs/2402.17199v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-27 04:18:15","title":"Differentiable Biomechanics Unlocks Opportunities for Markerless Motion Capture","abstract":"Recent developments have created differentiable physics simulators designed for machine learning pipelines that can be accelerated on a GPU. While these can simulate biomechanical models, these opportunities have not been exploited for biomechanics research or markerless motion capture. We show that these simulators can be used to fit inverse kinematics to markerless motion capture data, including scaling the model to fit the anthropomorphic measurements of an individual. This is performed end-to-end with an implicit representation of the movement trajectory, which is propagated through the forward kinematic model to minimize the error from the 3D markers reprojected into the images. The differential optimizer yields other opportunities, such as adding bundle adjustment during trajectory optimization to refine the extrinsic camera parameters or meta-optimization to improve the base model jointly over trajectories from multiple participants. This approach improves the reprojection error from markerless motion capture over prior methods and produces accurate spatial step parameters compared to an instrumented walkway for control and clinical populations.","sentences":["Recent developments have created differentiable physics simulators designed for machine learning pipelines that can be accelerated on a GPU.","While these can simulate biomechanical models, these opportunities have not been exploited for biomechanics research or markerless motion capture.","We show that these simulators can be used to fit inverse kinematics to markerless motion capture data, including scaling the model to fit the anthropomorphic measurements of an individual.","This is performed end-to-end with an implicit representation of the movement trajectory, which is propagated through the forward kinematic model to minimize the error from the 3D markers reprojected into the images.","The differential optimizer yields other opportunities, such as adding bundle adjustment during trajectory optimization to refine the extrinsic camera parameters or meta-optimization to improve the base model jointly over trajectories from multiple participants.","This approach improves the reprojection error from markerless motion capture over prior methods and produces accurate spatial step parameters compared to an instrumented walkway for control and clinical populations."],"url":"http://arxiv.org/abs/2402.17192v1","category":"cs.CV"}
{"created":"2024-02-27 03:36:50","title":"Methylation Operation Wizard (MeOW): Identification of differentially methylated regions in long-read sequencing data","abstract":"Long-read sequencing (LRS) is able to simultaneously capture information about both DNA sequence and modifications, such as CpG methylation in a single sequencing experiment. Here we present Methylation Operation Wizard (MeOW), a program to identify and prioritize differentially methylated regions (DMRs) genome-wide using LRS data. MeOW can be run using either a file containing counts of per-nucleotide methylated CpG sites or with a bam file containing modified base tags.","sentences":["Long-read sequencing (LRS) is able to simultaneously capture information about both DNA sequence and modifications, such as CpG methylation in a single sequencing experiment.","Here we present Methylation Operation Wizard (MeOW), a program to identify and prioritize differentially methylated regions (DMRs) genome-wide using LRS data.","MeOW can be run using either a file containing counts of per-nucleotide methylated CpG sites or with a bam file containing modified base tags."],"url":"http://arxiv.org/abs/2402.17182v1","category":"q-bio.GN"}
{"created":"2024-02-27 02:29:45","title":"Spatial Distribution of Inertial Particles in Turbulent Taylor-Couette Flow","abstract":"This study investigates the spatial distribution of inertial particles in turbulent Taylor-Couette flow. Direct numerical simulations are performed using a one-way coupled Eulerian-Lagrangian approach, with a fixed inner wall Reynolds number of 2500 for the carrier flow, while the particle Stokes number varies from 0.034 to 1 for the dispersed phase. We first examine the issue of preferential concentration of particles near the outer wall region. Employing two-dimensional (2D) Voronoi analysis, we observe a pronounced particle clustering with increasing $St$, particularly evident in regions of low fluid velocity. Additionally, we investigate the concentration balance equation, inspired by the work of johnson et al.(2020), to examine particle radial distribution. We discern the predominant sources of influence, namely biased sampling, turbophoresis, and centrifugal effects. Across all cases, centrifugal force emerges as the primary driver, causing particle migration towards the outer wall. Biased sampling predominantly affects smaller inertial particles, driving them towards the inner wall due to sampling within Taylor rolls with inward radial velocity. Conversely, turbophoresis primarily impacts larger inertial particles, inducing migration towards both walls where turbulent intensity is weaker compared to the bulk. With the revealed physics, our work provides a basis for predicting and controlling particle movement and distribution in industrial applications.","sentences":["This study investigates the spatial distribution of inertial particles in turbulent Taylor-Couette flow.","Direct numerical simulations are performed using a one-way coupled Eulerian-Lagrangian approach, with a fixed inner wall Reynolds number of 2500 for the carrier flow, while the particle Stokes number varies from 0.034 to 1 for the dispersed phase.","We first examine the issue of preferential concentration of particles near the outer wall region.","Employing two-dimensional (2D) Voronoi analysis, we observe a pronounced particle clustering with increasing $St$, particularly evident in regions of low fluid velocity.","Additionally, we investigate the concentration balance equation, inspired by the work of johnson et al.(2020), to examine particle radial distribution.","We discern the predominant sources of influence, namely biased sampling, turbophoresis, and centrifugal effects.","Across all cases, centrifugal force emerges as the primary driver, causing particle migration towards the outer wall.","Biased sampling predominantly affects smaller inertial particles, driving them towards the inner wall due to sampling within Taylor rolls with inward radial velocity.","Conversely, turbophoresis primarily impacts larger inertial particles, inducing migration towards both walls where turbulent intensity is weaker compared to the bulk.","With the revealed physics, our work provides a basis for predicting and controlling particle movement and distribution in industrial applications."],"url":"http://arxiv.org/abs/2402.17149v1","category":"physics.flu-dyn"}
{"created":"2024-02-27 01:53:02","title":"Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers and RNNs Trained with a New Loss Function","abstract":"Glycosylation, a protein modification, has multiple essential functional and structural roles. O-GlcNAcylation, a subtype of glycosylation, has the potential to be an important target for therapeutics, but methods to reliably predict O-GlcNAcylation sites had not been available until 2023; a 2021 review correctly noted that published models were insufficient and failed to generalize. Moreover, many are no longer usable. In 2023, a considerably better RNN model with an F$_1$ score of 36.17% and an MCC of 34.57% on a large dataset was published. This article first sought to improve these metrics using transformer encoders. While transformers displayed high performance on this dataset, their performance was inferior to that of the previously published RNN. We then created a new loss function, which we call the weighted focal differentiable MCC, to improve the performance of classification models. RNN models trained with this new function display superior performance to models trained using the weighted cross-entropy loss; this new function can also be used to fine-tune trained models. A two-cell RNN trained with this loss achieves state-of-the-art performance in O-GlcNAcylation site prediction with an F$_1$ score of 38.82% and an MCC of 38.21% on that large dataset.","sentences":["Glycosylation, a protein modification, has multiple essential functional and structural roles.","O-GlcNAcylation, a subtype of glycosylation, has the potential to be an important target for therapeutics, but methods to reliably predict O-GlcNAcylation sites had not been available until 2023; a 2021 review correctly noted that published models were insufficient and failed to generalize.","Moreover, many are no longer usable.","In 2023, a considerably better RNN model with an F$_1$ score of 36.17% and an MCC of 34.57% on a large dataset was published.","This article first sought to improve these metrics using transformer encoders.","While transformers displayed high performance on this dataset, their performance was inferior to that of the previously published RNN.","We then created a new loss function, which we call the weighted focal differentiable MCC, to improve the performance of classification models.","RNN models trained with this new function display superior performance to models trained using the weighted cross-entropy loss; this new function can also be used to fine-tune trained models.","A two-cell RNN trained with this loss achieves state-of-the-art performance in O-GlcNAcylation site prediction with an F$_1$ score of 38.82% and an MCC of 38.21% on that large dataset."],"url":"http://arxiv.org/abs/2402.17131v1","category":"cs.LG"}
{"created":"2024-02-27 00:36:58","title":"Causal Orthogonalization: Multicollinearity, Economic Interpretability, and the Gram-Schmidt Process","abstract":"This paper considers the problem of interpreting orthogonalization model coefficients. We derive a causal economic interpretation of the Gram-Schmidt orthogonalization process and provide the conditions for its equivalence to total effects from a recursive Directed Acyclic Graph. We extend the Gram-Schmidt process to groups of simultaneous regressors common in economic data sets and derive its finite sample properties, finding its coefficients to be unbiased, stable, and more efficient than those from Ordinary Least Squares. Finally, we apply the estimator to childhood reading comprehension scores, controlling for such highly collinear characteristics as race, education, and income. The model expands Bohren et al.'s decomposition of systemic discrimination into channel-specific effects and improves its coefficient significance levels.","sentences":["This paper considers the problem of interpreting orthogonalization model coefficients.","We derive a causal economic interpretation of the Gram-Schmidt orthogonalization process and provide the conditions for its equivalence to total effects from a recursive Directed Acyclic Graph.","We extend the Gram-Schmidt process to groups of simultaneous regressors common in economic data sets and derive its finite sample properties, finding its coefficients to be unbiased, stable, and more efficient than those from Ordinary Least Squares.","Finally, we apply the estimator to childhood reading comprehension scores, controlling for such highly collinear characteristics as race, education, and income.","The model expands Bohren et al.'s decomposition of systemic discrimination into channel-specific effects and improves its coefficient significance levels."],"url":"http://arxiv.org/abs/2402.17103v1","category":"econ.EM"}
{"created":"2024-02-27 00:29:14","title":"Virial Equation of State for a Granular System","abstract":"The equation of state for an ideal gas is simple, which is $P=nk_{\\rm B}T$. In the case of imperfect gases where mutual interactions among the constituents are important, pressure $P$ can be expressed as the series expansion of density $n$ with appropriate coefficients, known as virial coefficients $B_m$. In this paper, we have obtained the first four virial coefficients for a model interaction potential $\\Phi(r)$ using multidimensional Monte-Carlo integration and importance sampling methods. Next, we perform molecular dynamics simulations with the same $\\Phi(r)$ for a many-particle system to obtain $P$ as a function of $T$ and $n$. We compare our numerical data with the virial equation of state.","sentences":["The equation of state for an ideal gas is simple, which is $P=nk_{\\rm B}T$. In the case of imperfect gases where mutual interactions among the constituents are important, pressure $P$ can be expressed as the series expansion of density $n$ with appropriate coefficients, known as virial coefficients $B_m$. In this paper, we have obtained the first four virial coefficients for a model interaction potential $\\Phi(r)$ using multidimensional Monte-Carlo integration and importance sampling methods.","Next, we perform molecular dynamics simulations with the same $\\Phi(r)$ for a many-particle system to obtain $P$ as a function of $T$ and $n$. We compare our numerical data with the virial equation of state."],"url":"http://arxiv.org/abs/2402.17100v1","category":"cond-mat.soft"}
{"created":"2024-02-26 23:56:34","title":"Renormalization Group flow, Optimal Transport and Diffusion-based Generative Model","abstract":"Diffusion-based generative models represent a forefront direction in generative AI research today. Recent studies in physics have suggested that the renormalization group (RG) can be conceptualized as a diffusion process. This insight motivates us to develop a novel diffusion-based generative model by reversing the momentum-space RG flow. We establish a framework that interprets RG flow as optimal transport gradient flow, which minimizes a functional analogous to the Kullback-Leibler divergence, thereby bridging statistical physics and information theory. Our model applies forward and reverse diffusion processes in Fourier space, exploiting the sparse representation of natural images in this domain to efficiently separate signal from noise and manage image features across scales. By introducing a scale-dependent noise schedule informed by a dispersion relation, the model optimizes denoising performance and image generation in Fourier space, taking advantage of the distinct separation of macro and microscale features. Experimental validations on standard datasets demonstrate the model's capability to generate high-quality images while significantly reducing training time compared to existing image-domain diffusion models. This approach not only enhances our understanding of the generative processes in images but also opens new pathways for research in generative AI, leveraging the convergence of theoretical physics, optimal transport, and machine learning principles.","sentences":["Diffusion-based generative models represent a forefront direction in generative AI research today.","Recent studies in physics have suggested that the renormalization group (RG) can be conceptualized as a diffusion process.","This insight motivates us to develop a novel diffusion-based generative model by reversing the momentum-space RG flow.","We establish a framework that interprets RG flow as optimal transport gradient flow, which minimizes a functional analogous to the Kullback-Leibler divergence, thereby bridging statistical physics and information theory.","Our model applies forward and reverse diffusion processes in Fourier space, exploiting the sparse representation of natural images in this domain to efficiently separate signal from noise and manage image features across scales.","By introducing a scale-dependent noise schedule informed by a dispersion relation, the model optimizes denoising performance and image generation in Fourier space, taking advantage of the distinct separation of macro and microscale features.","Experimental validations on standard datasets demonstrate the model's capability to generate high-quality images while significantly reducing training time compared to existing image-domain diffusion models.","This approach not only enhances our understanding of the generative processes in images but also opens new pathways for research in generative AI, leveraging the convergence of theoretical physics, optimal transport, and machine learning principles."],"url":"http://arxiv.org/abs/2402.17090v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-26 23:37:10","title":"Solving the area-length systems in discrete gravity using homotopy continuation","abstract":"Area variables are intrinsic to connection formulations of general relativity, in contrast to the fundamental length variables prevalent in metric formulations. Within 4D discrete gravity, particularly based on triangulations, the area-length system establishes a relationship between area variables associated with triangles and the edge length variables. This system is comprised of polynomial equations derived from Heron's formula, which relates the area of a triangle to its edge lengths.   Using tools from numerical algebraic geometry, we study the area-length systems. In particular, we show that given the ten triangular areas of a single 4-simplex, there could be up to 64 compatible sets of edge lengths. Moreover, we show that these 64 solutions do not, in general, admit formulae in terms of the areas by analyzing the Galois group, or monodromy group, of the problem. We show that by introducing additional symmetry constraints, it is possible to obtain such formulae for the edge lengths. We take the first steps toward applying our results within discrete quantum gravity, specifically for effective spin foam models.","sentences":["Area variables are intrinsic to connection formulations of general relativity, in contrast to the fundamental length variables prevalent in metric formulations.","Within 4D discrete gravity, particularly based on triangulations, the area-length system establishes a relationship between area variables associated with triangles and the edge length variables.","This system is comprised of polynomial equations derived from Heron's formula, which relates the area of a triangle to its edge lengths.   ","Using tools from numerical algebraic geometry, we study the area-length systems.","In particular, we show that given the ten triangular areas of a single 4-simplex, there could be up to 64 compatible sets of edge lengths.","Moreover, we show that these 64 solutions do not, in general, admit formulae in terms of the areas by analyzing the Galois group, or monodromy group, of the problem.","We show that by introducing additional symmetry constraints, it is possible to obtain such formulae for the edge lengths.","We take the first steps toward applying our results within discrete quantum gravity, specifically for effective spin foam models."],"url":"http://arxiv.org/abs/2402.17080v1","category":"gr-qc"}
{"created":"2024-02-26 22:46:46","title":"Bounds on the Index of an Umbilic Point","abstract":"Umbilics are points of a surface embedded in three space where normal curvatures are independent of direction. The (in)famous Carath\\'{e}odory Conjecture states that a compact simply connected embedded surface has at least two umbilic points. A counterexample to this conjecture would be a surface whose principal foliation has index two at a single umbilic. All (purported) proofs of the Carath\\'{e}odory Conjecture are based on analyses of the index of an umbilic, concluding that it is at most one. This investigation gives a much simpler geometric argument that the index of an umbilic on an analytic surface cannot be an integer larger than one, providing new insight into the Carath\\'{e}odory Conjecture. The results also establish lower bounds for the index of an umbilic based on its degeneracy.","sentences":["Umbilics are points of a surface embedded in three space where normal curvatures are independent of direction.","The (in)famous Carath\\'{e}odory Conjecture states that a compact simply connected embedded surface has at least two umbilic points.","A counterexample to this conjecture would be a surface whose principal foliation has index two at a single umbilic.","All (purported) proofs of the Carath\\'{e}odory Conjecture are based on analyses of the index of an umbilic, concluding that it is at most one.","This investigation gives a much simpler geometric argument that the index of an umbilic on an analytic surface cannot be an integer larger than one, providing new insight into the Carath\\'{e}odory Conjecture.","The results also establish lower bounds for the index of an umbilic based on its degeneracy."],"url":"http://arxiv.org/abs/2402.17060v1","category":"math.DG"}
{"created":"2024-02-26 22:21:28","title":"Wave Propagation in Periodic Plasma Media: A Semiclassical Approach","abstract":"The propagation of electromagnetic waves in unmagnetized periodic plasma media is studied using the semiclassical wave packet approximation. The formalism gives rise to Berry effect terms in the equation of motion. The Berry effect manifests itself as Rytov polarization rotation law and the polarization-dependent Hall effect. The formalism is also applied to the case of non-periodic inhomogeneous plasma media.","sentences":["The propagation of electromagnetic waves in unmagnetized periodic plasma media is studied using the semiclassical wave packet approximation.","The formalism gives rise to Berry effect terms in the equation of motion.","The Berry effect manifests itself as Rytov polarization rotation law and the polarization-dependent Hall effect.","The formalism is also applied to the case of non-periodic inhomogeneous plasma media."],"url":"http://arxiv.org/abs/2402.17051v1","category":"cond-mat.soft"}
{"created":"2024-02-26 21:30:33","title":"Stellarator equilibrium axis-expansion to all orders in distance from the axis for arbitrary plasma beta","abstract":"A systematic theory of the asymptotic expansion of the magnetohydrodynamic (MHD) equilibrium in the distance from the magnetic axis is developed to include arbitrary smooth currents near the magnetic axis. Compared to the vacuum and the force-free system, an additional magnetic differential equation must be solved to obtain the pressure-driven currents. It is shown that there exist variables in which the rest of the MHD system closely mimics the vacuum system. Thus, a unified treatment of MHD fields is possible. The mathematical structure of the near-axis expansions to arbitrary order is examined carefully to show that the double-periodicity of physical quantities in a toroidal domain can be satisfied order by order. The essential role played by the normal form in solving the magnetic differential equations is highlighted. Several explicit examples of vacuum, force-free, and MHD equilibrium in different geometries are presented.","sentences":["A systematic theory of the asymptotic expansion of the magnetohydrodynamic (MHD) equilibrium in the distance from the magnetic axis is developed to include arbitrary smooth currents near the magnetic axis.","Compared to the vacuum and the force-free system, an additional magnetic differential equation must be solved to obtain the pressure-driven currents.","It is shown that there exist variables in which the rest of the MHD system closely mimics the vacuum system.","Thus, a unified treatment of MHD fields is possible.","The mathematical structure of the near-axis expansions to arbitrary order is examined carefully to show that the double-periodicity of physical quantities in a toroidal domain can be satisfied order by order.","The essential role played by the normal form in solving the magnetic differential equations is highlighted.","Several explicit examples of vacuum, force-free, and MHD equilibrium in different geometries are presented."],"url":"http://arxiv.org/abs/2402.17034v1","category":"physics.plasm-ph"}
{"created":"2024-02-26 21:20:10","title":"Stochastic homogenization of a class of quasiconvex and possibly degenerate viscous HJ equations in 1d","abstract":"We prove homogenization for possibly degenerate viscous Hamilton-Jacobi equations with a Hamiltonian of the form $G(p)+V(x,\\omega)$, where $G$ is a quasiconvex, locally Lipschitz function with superlinear growth, the potential $V(x,\\omega)$ is bounded and Lipschitz continuous, and the diffusion coefficient $a(x,\\omega)$ is allowed to vanish on some regions or even on the whole $\\mathbb{R}$. The class of random media we consider is defined by an explicit scaled hill condition on the pair $(a,V)$ which is fulfilled as long as the environment is not ``rigid''.","sentences":["We prove homogenization for possibly degenerate viscous Hamilton-Jacobi equations with a Hamiltonian of the form $G(p)+V(x,\\omega)$, where $G$ is a quasiconvex, locally Lipschitz function with superlinear growth, the potential $V(x,\\omega)$ is bounded and Lipschitz continuous, and the diffusion coefficient $a(x,\\omega)$ is allowed to vanish on some regions or even on the whole $\\mathbb{R}$. The class of random media we consider is defined by an explicit scaled hill condition on the pair $(a,V)$ which is fulfilled as long as the environment is not ``rigid''."],"url":"http://arxiv.org/abs/2402.17031v1","category":"math.AP"}
{"created":"2024-02-26 21:16:22","title":"Transforming Stiffness and Chaos","abstract":"Stiff and chaotic differential equations are challenging for time-stepping numerical methods. For explicit methods, the required time step resolution significantly exceeds the resolution associated with the smoothness of the exact solution for specified accuracy. In order to improve efficiency, the question arises whether transformation to asymptotically stable solutions can be performed, for which neighbouring solutions converge towards each other at a controlled rate. Employing the concept of local Lyapunov exponents, it is demonstrated that chaotic differential equations can be successfully transformed to obtain high accuracy, whereas stiff equations cannot. For instance, the accuracy of explicit fourth order Runge-Kutta solution of the Lorenz chaotic equations can be increased by two orders of magnitude. Alternatively, the time step can be significantly extended with retained accuracy.","sentences":["Stiff and chaotic differential equations are challenging for time-stepping numerical methods.","For explicit methods, the required time step resolution significantly exceeds the resolution associated with the smoothness of the exact solution for specified accuracy.","In order to improve efficiency, the question arises whether transformation to asymptotically stable solutions can be performed, for which neighbouring solutions converge towards each other at a controlled rate.","Employing the concept of local Lyapunov exponents, it is demonstrated that chaotic differential equations can be successfully transformed to obtain high accuracy, whereas stiff equations cannot.","For instance, the accuracy of explicit fourth order Runge-Kutta solution of the Lorenz chaotic equations can be increased by two orders of magnitude.","Alternatively, the time step can be significantly extended with retained accuracy."],"url":"http://arxiv.org/abs/2402.17030v1","category":"math.NA"}
{"created":"2024-02-26 21:16:14","title":"Offline Writer Identification Using Convolutional Neural Network Activation Features","abstract":"Convolutional neural networks (CNNs) have recently become the state-of-the-art tool for large-scale image classification. In this work we propose the use of activation features from CNNs as local descriptors for writer identification. A global descriptor is then formed by means of GMM supervector encoding, which is further improved by normalization with the KL-Kernel. We evaluate our method on two publicly available datasets: the ICDAR 2013 benchmark database and the CVL dataset. While we perform comparably to the state of the art on CVL, our proposed method yields about 0.21 absolute improvement in terms of mAP on the challenging bilingual ICDAR dataset.","sentences":["Convolutional neural networks (CNNs) have recently become the state-of-the-art tool for large-scale image classification.","In this work we propose the use of activation features from CNNs as local descriptors for writer identification.","A global descriptor is then formed by means of GMM supervector encoding, which is further improved by normalization with the KL-Kernel.","We evaluate our method on two publicly available datasets: the ICDAR 2013 benchmark database and the CVL dataset.","While we perform comparably to the state of the art on CVL, our proposed method yields about 0.21 absolute improvement in terms of mAP on the challenging bilingual ICDAR dataset."],"url":"http://arxiv.org/abs/2402.17029v1","category":"cs.CV"}
{"created":"2024-02-26 21:08:43","title":"$K_S^0$ meson production in inelastic p+p interactions at 31, 40 and 80 GeV/c beam momentum measured by NA61/SHINE at the CERN SPS","abstract":"Measurements of $K_S^0$ meson production via its $\\pi^{+} \\pi^{-}$ decay mode in inelastic $\\textit{p+p}$ interactions at incident projectile momenta of 31, 40 and 80 GeV/$c$ ($\\sqrt{s_{NN}}=7.7, 8.8$ and $12.3$ GeV, respectively) are presented. The data were recorded by the NA61/SHINE spectrometer at the CERN Super Proton Synchrotron. Double-differential distributions were obtained in transverse momentum and rapidity. The mean multiplicities of $K_S^0$ mesons were determined to be $(5.95 \\pm 0.19 (stat) \\pm 0.22 (sys)) \\times 10^{-2}$ at 31 GeV/$c$, $(7.61 \\pm 0.13 (stat) \\pm 0.31 (sys)) \\times 10^{-2}$ at 40 GeV/$c$ and $(11.58 \\pm 0.12 (stat) \\pm 0.37 (sys)) \\times 10^{-2}$ at 80 GeV/$c$. The results on $K^{0}_{S}$ production are compared with model calculations (Epos1.99, SMASH 2.0 and PHSD) as well as with published data from other experiments.","sentences":["Measurements of $K_S^0$ meson production via its $\\pi^{+} \\pi^{-}$ decay mode in inelastic $\\textit{p+p}$ interactions at incident projectile momenta of 31, 40 and 80 GeV/$c$ ($\\sqrt{s_{NN}}=7.7, 8.8$ and $12.3$ GeV, respectively) are presented.","The data were recorded by the NA61/SHINE spectrometer at the CERN Super Proton Synchrotron.","Double-differential distributions were obtained in transverse momentum and rapidity.","The mean multiplicities of $K_S^0$ mesons were determined to be $(5.95 \\pm 0.19 (stat) \\pm 0.22 (sys))","\\times 10^{-2}$ at 31 GeV/$c$, $(7.61 \\pm 0.13 (stat) \\pm 0.31 (sys))","\\times 10^{-2}$ at 40 GeV/$c$ and $(11.58 \\pm 0.12 (stat) \\pm 0.37 (sys))","\\times 10^{-2}$ at 80 GeV/$c$. The results on $K^{0}_{S}$ production are compared with model calculations (Epos1.99, SMASH 2.0 and PHSD) as well as with published data from other experiments."],"url":"http://arxiv.org/abs/2402.17025v1","category":"hep-ex"}
{"created":"2024-02-26 21:03:48","title":"Integrating the full four-loop negative geometries and all-loop ladder-type negative geometries in ABJM theory","abstract":"The decomposition of the four-point ABJM amplituhedron into negative geometries produces a compact integrand of logarithmic of amplitudes such that the infrared divergence only comes from the last loop integral, from which we can compute the cusp anomalous dimension of the ABJM theory. In this note, we integrate $L-1$ loop momenta of the $L$-loop negative geometries for all four-loop negative geometries and a special class of all-loop ladder-type negative geometries by a method based on Mellin transformation, and from these finite quantities we extract the corresponding contribution to the cusp anomalous dimension. We find that the infrared divergence of a box-type negative geometry at $L=4$ is weaker than other negative geometries, then only tree-type negative geometries contribute to the cusp anomalous dimension at $L=4$. For the all-loop ladder-type negative geometries, we prove and conjecture some recursive structures as integral equations in Mellin space and find that they cannot contribute zeta values like $\\zeta_3,\\zeta_5$ to the cusp anomalous dimension.","sentences":["The decomposition of the four-point ABJM amplituhedron into negative geometries produces a compact integrand of logarithmic of amplitudes such that the infrared divergence only comes from the last loop integral, from which we can compute the cusp anomalous dimension of the ABJM theory.","In this note, we integrate $L-1$ loop momenta of the $L$-loop negative geometries for all four-loop negative geometries and a special class of all-loop ladder-type negative geometries by a method based on Mellin transformation, and from these finite quantities we extract the corresponding contribution to the cusp anomalous dimension.","We find that the infrared divergence of a box-type negative geometry at $L=4$ is weaker than other negative geometries, then only tree-type negative geometries contribute to the cusp anomalous dimension at $L=4$. For the all-loop ladder-type negative geometries, we prove and conjecture some recursive structures as integral equations in Mellin space and find that they cannot contribute zeta values like $\\zeta_3,\\zeta_5$ to the cusp anomalous dimension."],"url":"http://arxiv.org/abs/2402.17023v1","category":"hep-th"}
{"created":"2024-02-26 20:04:01","title":"Towards Decoding Brain Activity During Passive Listening of Speech","abstract":"The aim of the study is to investigate the complex mechanisms of speech perception and ultimately decode the electrical changes in the brain accruing while listening to speech. We attempt to decode heard speech from intracranial electroencephalographic (iEEG) data using deep learning methods. The goal is to aid the advancement of brain-computer interface (BCI) technology for speech synthesis, and, hopefully, to provide an additional perspective on the cognitive processes of speech perception. This approach diverges from the conventional focus on speech production and instead chooses to investigate neural representations of perceived speech. This angle opened up a complex perspective, potentially allowing us to study more sophisticated neural patterns. Leveraging the power of deep learning models, the research aimed to establish a connection between these intricate neural activities and the corresponding speech sounds. Despite the approach not having achieved a breakthrough yet, the research sheds light on the potential of decoding neural activity during speech perception. Our current efforts can serve as a foundation, and we are optimistic about the potential of expanding and improving upon this work to move closer towards more advanced BCIs, better understanding of processes underlying perceived speech and its relation to spoken speech.","sentences":["The aim of the study is to investigate the complex mechanisms of speech perception and ultimately decode the electrical changes in the brain accruing while listening to speech.","We attempt to decode heard speech from intracranial electroencephalographic (iEEG) data using deep learning methods.","The goal is to aid the advancement of brain-computer interface (BCI) technology for speech synthesis, and, hopefully, to provide an additional perspective on the cognitive processes of speech perception.","This approach diverges from the conventional focus on speech production and instead chooses to investigate neural representations of perceived speech.","This angle opened up a complex perspective, potentially allowing us to study more sophisticated neural patterns.","Leveraging the power of deep learning models, the research aimed to establish a connection between these intricate neural activities and the corresponding speech sounds.","Despite the approach not having achieved a breakthrough yet, the research sheds light on the potential of decoding neural activity during speech perception.","Our current efforts can serve as a foundation, and we are optimistic about the potential of expanding and improving upon this work to move closer towards more advanced BCIs, better understanding of processes underlying perceived speech and its relation to spoken speech."],"url":"http://arxiv.org/abs/2402.16996v1","category":"cs.HC"}
{"created":"2024-02-26 19:52:33","title":"A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data","abstract":"Understanding the structure of real data is paramount in advancing modern deep-learning methodologies. Natural data such as images are believed to be composed of features organised in a hierarchical and combinatorial manner, which neural networks capture during learning. Recent advancements show that diffusion models can generate high-quality images, hinting at their ability to capture this underlying structure. We study this phenomenon in a hierarchical generative model of data. We find that the backward diffusion process acting after a time $t$ is governed by a phase transition at some threshold time, where the probability of reconstructing high-level features, like the class of an image, suddenly drops. Instead, the reconstruction of low-level features, such as specific details of an image, evolves smoothly across the whole diffusion process. This result implies that at times beyond the transition, the class has changed but the generated sample may still be composed of low-level elements of the initial image. We validate these theoretical insights through numerical experiments on class-unconditional ImageNet diffusion models. Our analysis characterises the relationship between time and scale in diffusion models and puts forward generative models as powerful tools to model combinatorial data properties.","sentences":["Understanding the structure of real data is paramount in advancing modern deep-learning methodologies.","Natural data such as images are believed to be composed of features organised in a hierarchical and combinatorial manner, which neural networks capture during learning.","Recent advancements show that diffusion models can generate high-quality images, hinting at their ability to capture this underlying structure.","We study this phenomenon in a hierarchical generative model of data.","We find that the backward diffusion process acting after a time $t$ is governed by a phase transition at some threshold time, where the probability of reconstructing high-level features, like the class of an image, suddenly drops.","Instead, the reconstruction of low-level features, such as specific details of an image, evolves smoothly across the whole diffusion process.","This result implies that at times beyond the transition, the class has changed but the generated sample may still be composed of low-level elements of the initial image.","We validate these theoretical insights through numerical experiments on class-unconditional ImageNet diffusion models.","Our analysis characterises the relationship between time and scale in diffusion models and puts forward generative models as powerful tools to model combinatorial data properties."],"url":"http://arxiv.org/abs/2402.16991v1","category":"stat.ML"}
{"created":"2024-02-26 19:17:23","title":"The Mumford dynamical system and the Gelfand-Dikii recursion","abstract":"In his paper \"The Mumford Dynamical System and Hyperelliptic Kleinian Functions\" (see arXiv:2402.09218), Victor Buchstaber developed the differential-algebraic theory of the Mumford dynamical system. The key object of this theory is the (P,Q)-recursion introduced in his paper. In the present paper, we further develop the theory of (P,Q)-recursion and describe its connections to the Korteweg-de Vries hierarchy, the Lenard operator, and the Gelfand-Dikii recursion.","sentences":["In his paper \"The Mumford Dynamical System and Hyperelliptic Kleinian Functions\" (see arXiv:2402.09218), Victor Buchstaber developed the differential-algebraic theory of the Mumford dynamical system.","The key object of this theory is the (P,Q)-recursion introduced in his paper.","In the present paper, we further develop the theory of (P,Q)-recursion and describe its connections to the Korteweg-de Vries hierarchy, the Lenard operator, and the Gelfand-Dikii recursion."],"url":"http://arxiv.org/abs/2402.16975v1","category":"nlin.SI"}
{"created":"2024-02-26 19:11:07","title":"The Gasing Pangkah Collaboration: I. Asteroseismic Identification and Characterisation of a Rapidly-Rotating Engulfment Candidate","abstract":"We report the discovery and characterisation of TIC 350842552 (\"Zvrk\"), an apparently isolated, rapidly-rotating ($P_\\text{rot} \\sim 99\\ \\mathrm{d}$) red giant observed by TESS in its Southern Continuous Viewing Zone. The star's fast surface rotation is independently verified by the use of p-mode asteroseismology, strong periodicity in TESS and ASAS-SN photometry, and measurements of spectroscopic rotational broadening. A two-component fit to APOGEE spectra indicates a coverage fraction of its surface features consistent with the amplitude of the photometric rotational signal. Variations in the amplitude of its photometric modulations over time suggest the evolution of its surface morphology, and therefore enhanced magnetic activity. We further develop and deploy new asteroseismic techniques to characterise radial differential rotation, and find weak evidence for rotational shear within Zvrk's convective envelope. This feature, in combination with such a high surface rotation rate, is incompatible with models of angular-momentum transport in single-star evolution. Spectroscopic abundance estimates also indicate a high lithium abundance, among other chemical anomalies. Taken together, all of these suggest a planet-ingestion scenario for the formation of this rotational configuration, various models for which we examine in detail.","sentences":["We report the discovery and characterisation of TIC 350842552 (\"Zvrk\"), an apparently isolated, rapidly-rotating ($P_\\text{rot} \\sim 99\\ \\mathrm{d}$) red giant observed by TESS in its Southern Continuous Viewing Zone.","The star's fast surface rotation is independently verified by the use of p-mode asteroseismology, strong periodicity in TESS and ASAS-SN photometry, and measurements of spectroscopic rotational broadening.","A two-component fit to APOGEE spectra indicates a coverage fraction of its surface features consistent with the amplitude of the photometric rotational signal.","Variations in the amplitude of its photometric modulations over time suggest the evolution of its surface morphology, and therefore enhanced magnetic activity.","We further develop and deploy new asteroseismic techniques to characterise radial differential rotation, and find weak evidence for rotational shear within Zvrk's convective envelope.","This feature, in combination with such a high surface rotation rate, is incompatible with models of angular-momentum transport in single-star evolution.","Spectroscopic abundance estimates also indicate a high lithium abundance, among other chemical anomalies.","Taken together, all of these suggest a planet-ingestion scenario for the formation of this rotational configuration, various models for which we examine in detail."],"url":"http://arxiv.org/abs/2402.16971v1","category":"astro-ph.SR"}
{"created":"2024-02-26 19:00:53","title":"Bare mass effects on the reheating process after inflation","abstract":"We consider the effects of a bare mass term for the inflaton, when the inflationary potential takes the form $V(\\phi)= \\lambda \\phi^k$ about its minimum with $k \\ge 4$. We concentrate on $k=4$, but discuss general cases as well. Further, we assume $\\lambda \\phi_{\\rm end}^2 \\gg m_\\phi^2$, where $\\phi_{\\rm end}$ is the inflaton field value when the inflationary expansion ends. We show that the presence of a mass term (which may be present due to radiative corrections or supersymmetry breaking) can significantly alter the reheating process, as the equation of state of the inflaton condensate changes from $w_\\phi=\\frac{1}{3}$ to $w_\\phi=0$ when $\\lambda \\phi^2$ drops below $m_\\phi^2$. We show that for a mass $m_\\phi \\gtrsim T_{\\rm RH}/250$, the mass term will dominate at reheating. We compute the effects on the reheating temperature for cases where reheating is due to inflaton decay (to fermions, scalars, or vectors) or to inflaton scattering (to scalars or vectors). For scattering to scalars and in the absence of a decay, we derive a strong upper limit to the inflaton bare mass $m_\\phi < 350~{\\rm MeV} (T_{\\rm RH}/10^{10}~{\\rm GeV})^{3/5}$, as there is always a residual inflaton background which acts as cold dark matter. We also consider the effect of the bare mass term on the fragmentation of the inflaton condensate.","sentences":["We consider the effects of a bare mass term for the inflaton, when the inflationary potential takes the form $V(\\phi)= \\lambda \\phi^k$ about its minimum with $k \\ge 4$.","We concentrate on $k=4$, but discuss general cases as well.","Further, we assume $\\lambda \\phi_{\\rm end}^2 \\gg m_\\phi^2$, where $\\phi_{\\rm end}$ is the inflaton field value when the inflationary expansion ends.","We show that the presence of a mass term (which may be present due to radiative corrections or supersymmetry breaking) can significantly alter the reheating process, as the equation of state of the inflaton condensate changes from $w_\\phi=\\frac{1}{3}$ to $w_\\phi=0$ when $\\lambda \\phi^2$ drops below $m_\\phi^2$. We show that for a mass $m_\\phi \\gtrsim T_{\\rm RH}/250$, the mass term will dominate at reheating.","We compute the effects on the reheating temperature for cases where reheating is due to inflaton decay (to fermions, scalars, or vectors) or to inflaton scattering (to scalars or vectors).","For scattering to scalars and in the absence of a decay, we derive a strong upper limit to the inflaton bare mass $m_\\phi <","350~{\\rm MeV} (T_{\\rm RH}/10^{10}~{\\rm GeV})^{3/5}$, as there is always a residual inflaton background which acts as cold dark matter.","We also consider the effect of the bare mass term on the fragmentation of the inflaton condensate."],"url":"http://arxiv.org/abs/2402.16958v1","category":"hep-ph"}
{"created":"2024-02-26 19:00:02","title":"Eccentric Mergers in AGN Discs: Influence of the Supermassive Black-Hole on Three-body Interactions","abstract":"There are indications that stellar-origin black holes (BHs) are efficiently paired up in binary black holes (BBHs) in Active Galactic Nuclei (AGN) disc environments, which can undergo interactions with single BHs in the disc. Such binary-single interactions can potentially lead to an exceptionally high fraction of gravitational-wave mergers with measurable eccentricity in LIGO/Virgo/KAGRA. We here take the next important step in this line of studies, by performing post-Newtonian N-body simulations between migrating BBHs and single BHs set in an AGN disc-like configuration with a consistent inclusion of the central supermassive black hole (SMBH) in the equations of motion. With this setup, we study how the fraction of eccentric mergers varies in terms of the initial size of the BBH semi-major axis relative to the Hill sphere, as well as how it depends on the angle between the BBH and the incoming single BH. We find that the fraction of eccentric mergers is still relatively large, even when the interactions are notably influenced by the gravitational field of the nearby SMBH. However, the fraction as a function of the BBH semi-major axis does not follow a smooth functional shape, but instead shows strongly varying features that originate from the underlying phase-space structure. The phase-space further reveals that many of the eccentric mergers are formed through prompt scatterings. Finally, we present the first analytical solution to how the presence of an SMBH in terms of its Hill sphere affects the probability for forming eccentric BBH mergers through chaotic three-body interactions.","sentences":["There are indications that stellar-origin black holes (BHs) are efficiently paired up in binary black holes (BBHs) in Active Galactic Nuclei (AGN) disc environments, which can undergo interactions with single BHs in the disc.","Such binary-single interactions can potentially lead to an exceptionally high fraction of gravitational-wave mergers with measurable eccentricity in LIGO/Virgo/KAGRA.","We here take the next important step in this line of studies, by performing post-Newtonian N-body simulations between migrating BBHs and single BHs set in an AGN disc-like configuration with a consistent inclusion of the central supermassive black hole (SMBH) in the equations of motion.","With this setup, we study how the fraction of eccentric mergers varies in terms of the initial size of the BBH semi-major axis relative to the Hill sphere, as well as how it depends on the angle between the BBH and the incoming single BH.","We find that the fraction of eccentric mergers is still relatively large, even when the interactions are notably influenced by the gravitational field of the nearby SMBH.","However, the fraction as a function of the BBH semi-major axis does not follow a smooth functional shape, but instead shows strongly varying features that originate from the underlying phase-space structure.","The phase-space further reveals that many of the eccentric mergers are formed through prompt scatterings.","Finally, we present the first analytical solution to how the presence of an SMBH in terms of its Hill sphere affects the probability for forming eccentric BBH mergers through chaotic three-body interactions."],"url":"http://arxiv.org/abs/2402.16948v1","category":"astro-ph.GA"}
{"created":"2024-02-26 18:47:08","title":"Numerical Analysis on Neural Network Projected Schemes for Approximating One Dimensional Wasserstein Gradient Flows","abstract":"We provide a numerical analysis and computation of neural network projected schemes for approximating one dimensional Wasserstein gradient flows. We approximate the Lagrangian mapping functions of gradient flows by the class of two-layer neural network functions with ReLU (rectified linear unit) activation functions. The numerical scheme is based on a projected gradient method, namely the Wasserstein natural gradient, where the projection is constructed from the $L^2$ mapping spaces onto the neural network parameterized mapping space. We establish theoretical guarantees for the performance of the neural projected dynamics. We derive a closed-form update for the scheme with well-posedness and explicit consistency guarantee for a particular choice of network structure. General truncation error analysis is also established on the basis of the projective nature of the dynamics. Numerical examples, including gradient drift Fokker-Planck equations, porous medium equations, and Keller-Segel models, verify the accuracy and effectiveness of the proposed neural projected algorithm.","sentences":["We provide a numerical analysis and computation of neural network projected schemes for approximating one dimensional Wasserstein gradient flows.","We approximate the Lagrangian mapping functions of gradient flows by the class of two-layer neural network functions with ReLU (rectified linear unit) activation functions.","The numerical scheme is based on a projected gradient method, namely the Wasserstein natural gradient, where the projection is constructed from the $L^2$ mapping spaces onto the neural network parameterized mapping space.","We establish theoretical guarantees for the performance of the neural projected dynamics.","We derive a closed-form update for the scheme with well-posedness and explicit consistency guarantee for a particular choice of network structure.","General truncation error analysis is also established on the basis of the projective nature of the dynamics.","Numerical examples, including gradient drift Fokker-Planck equations, porous medium equations, and Keller-Segel models, verify the accuracy and effectiveness of the proposed neural projected algorithm."],"url":"http://arxiv.org/abs/2402.16821v1","category":"math.NA"}
{"created":"2024-02-26 18:34:59","title":"Asymptotically non-negative Ricci curvature, elliptic Kato constant and isoperimetric inequalities","abstract":"The ABP method for proving isoperimetric inequalities has been first employed by Cabr\\'e in $\\mathbb{R}^n$, then developed by Brendle, notably in the context of non-compact Riemannian manifolds of non-negative Ricci curvature and positive asymptotic volume ratio. In this paper, we expand upon their approach and prove isoperimetric inequalities (sharp in the limit) in the presence of a small amount of negative curvature. First, we consider smallness of the negative part $\\mathrm{Ric}_-$ of the Ricci curvature in terms of its elliptic Kato constant. Indeed, the Kato constant turns out to control the non-negativity of the ($\\infty$-)Bakry-\\'Emery Ricci-tensor of a suitable conformal deformation of the manifold, and the ABP method can be implemented in this setting. Secondly, we show that the smallness of the Kato constant is implied by a suitable polynomial decay of $\\mathrm{Ric}_-$, provided that the asymptotic volume ratio is positive. To show this latter fact, we enhance techniques elaborated by Li-Tam and Kasue to obtain new estimates of the Green function valid on the whole manifold.","sentences":["The ABP method for proving isoperimetric inequalities has been first employed by Cabr\\'e in $\\mathbb{R}^n$, then developed by Brendle, notably in the context of non-compact Riemannian manifolds of non-negative Ricci curvature and positive asymptotic volume ratio.","In this paper, we expand upon their approach and prove isoperimetric inequalities (sharp in the limit) in the presence of a small amount of negative curvature.","First, we consider smallness of the negative part $\\mathrm{Ric}_-$ of the Ricci curvature in terms of its elliptic Kato constant.","Indeed, the Kato constant turns out to control the non-negativity of the ($\\infty$-)Bakry-\\'Emery Ricci-tensor of a suitable conformal deformation of the manifold, and the ABP method can be implemented in this setting.","Secondly, we show that the smallness of the Kato constant is implied by a suitable polynomial decay of $\\mathrm{Ric}_-$, provided that the asymptotic volume ratio is positive.","To show this latter fact, we enhance techniques elaborated by Li-Tam and Kasue to obtain new estimates of the Green function valid on the whole manifold."],"url":"http://arxiv.org/abs/2402.16812v1","category":"math.DG"}
{"created":"2024-02-26 18:32:50","title":"Motion dynamics of two-dimensional fundamental and vortex solitons in the fractional medium with the cubic-quintic nonlinearity","abstract":"We report results of systematic investigation of dynamics featured by moving two-dimensional (2D) solitons generated by the fractional nonlinear Schroedinger equation (FNLSE) with the cubic-quintic nonlinearity. The motion of solitons is a nontrivial problem, as the fractional diffraction breaks the Galilean invariance of the underlying equation. The addition of the defocusing quintic term to the focusing cubic one is necessary to stabilize the solitons against the collapse. The setting presented here can be implemented in nonlinear optical waveguides emulating the fractional diffraction. Systematic consideration identifies parameters of moving fundamental and vortex solitons (with vorticities 0 and 1 or 2, respectively) and maximum velocities up to which stable solitons persist, for characteristic values of the Levy index which determines the fractionality of the underlying model. Outcomes of collisions between 2D solitons moving in opposite directions are identified too. These are merger of the solitons, quasi-elastic or destructive collisions, and breakup of the two colliding solitons into a quartet of secondary ones.","sentences":["We report results of systematic investigation of dynamics featured by moving two-dimensional (2D) solitons generated by the fractional nonlinear Schroedinger equation (FNLSE) with the cubic-quintic nonlinearity.","The motion of solitons is a nontrivial problem, as the fractional diffraction breaks the Galilean invariance of the underlying equation.","The addition of the defocusing quintic term to the focusing cubic one is necessary to stabilize the solitons against the collapse.","The setting presented here can be implemented in nonlinear optical waveguides emulating the fractional diffraction.","Systematic consideration identifies parameters of moving fundamental and vortex solitons (with vorticities 0 and 1 or 2, respectively) and maximum velocities up to which stable solitons persist, for characteristic values of the Levy index which determines the fractionality of the underlying model.","Outcomes of collisions between 2D solitons moving in opposite directions are identified too.","These are merger of the solitons, quasi-elastic or destructive collisions, and breakup of the two colliding solitons into a quartet of secondary ones."],"url":"http://arxiv.org/abs/2402.16809v2","category":"nlin.PS"}
{"created":"2024-02-26 18:25:17","title":"A parabolic free transmission problem: flat free boundaries are smooth","abstract":"We study a two-phase parabolic free boundary problem motivated by the jump of conductivity in composite materials that undergo a phase transition. Each phase is governed by a heat equation with distinct thermal conductivity, and a transmission-type condition is imposed on the free interface. We establish strong regularity properties of the free boundary: first, we prove that flat free boundaries are $C^{1,\\alpha}$ by means of a linearization technique and compactness arguments. Then we use the Hodograph transform to achieve higher regularity. To this end, we prove a new Harnack-type inequality and develop the Schauder theory for parabolic linear transmission problems.","sentences":["We study a two-phase parabolic free boundary problem motivated by the jump of conductivity in composite materials that undergo a phase transition.","Each phase is governed by a heat equation with distinct thermal conductivity, and a transmission-type condition is imposed on the free interface.","We establish strong regularity properties of the free boundary: first, we prove that flat free boundaries are $C^{1,\\alpha}$ by means of a linearization technique and compactness arguments.","Then we use the Hodograph transform to achieve higher regularity.","To this end, we prove a new Harnack-type inequality and develop the Schauder theory for parabolic linear transmission problems."],"url":"http://arxiv.org/abs/2402.16805v1","category":"math.AP"}
{"created":"2024-02-26 18:00:29","title":"CARTE: pretraining and transfer for tabular learning","abstract":"Pretrained deep-learning models are the go-to solution for images or text. However, for tabular data the standard is still to train tree-based models. Pre-training or transfer is a huge challenge as in general tables have columns about different quantities and naming conventions that vary vastly across sources. Data integration tackles correspondences across multiple sources: schema matching for columns, and entity matching for entries. We propose a neural architecture that does not need such matches. As a result, we can pretrain it on background data that has not been matched. The architecture - CARTE for Context Aware Representation of Table Entries - uses a graph representation of tabular (or relational) data to process tables with different columns, string embeddings of entries and columns names to model an open vocabulary, and a graph-attentional network to contextualize entries with column names and neighboring entries. An extensive benchmark shows that CARTE facilitates learning, outperforming a solid set of baselines including the best tree-based models. CARTE also enables joint learning across tables with unmatched columns, enhancing a small table with bigger ones. CARTE opens the door to large pretrained models embarking information for tabular data.","sentences":["Pretrained deep-learning models are the go-to solution for images or text.","However, for tabular data the standard is still to train tree-based models.","Pre-training or transfer is a huge challenge as in general tables have columns about different quantities and naming conventions that vary vastly across sources.","Data integration tackles correspondences across multiple sources: schema matching for columns, and entity matching for entries.","We propose a neural architecture that does not need such matches.","As a result, we can pretrain it on background data that has not been matched.","The architecture - CARTE for Context Aware Representation of Table Entries - uses a graph representation of tabular (or relational) data to process tables with different columns, string embeddings of entries and columns names to model an open vocabulary, and a graph-attentional network to contextualize entries with column names and neighboring entries.","An extensive benchmark shows that CARTE facilitates learning, outperforming a solid set of baselines including the best tree-based models.","CARTE also enables joint learning across tables with unmatched columns, enhancing a small table with bigger ones.","CARTE opens the door to large pretrained models embarking information for tabular data."],"url":"http://arxiv.org/abs/2402.16785v1","category":"cs.LG"}
{"created":"2024-02-26 17:52:22","title":"Can we distinguish the adiabatic fluctuations and isocurvature fluctuations with pulsar timing arrays?","abstract":"Understanding the nature of primordial fluctuations is critical to our comprehension of the Universe's early stages. While these fluctuations are known to be nearly scale-invariant, quasi-adiabatic, and nearly Gaussian on large scales, their behavior at smaller scales remains less well-defined and may offer insights into new physics. Recent observations by the NANOGrav, PPTA, EPTA, and CPTA collaborations suggest the presence of a stochastic gravitational wave background, which, while consistent with the contribution from supermassive black hole binaries, also opens the possibility of probing new physics. This paper explores whether this signal could stem from primordial isocurvature and adiabatic fluctuations. We adopt parameterized spectra for both types of fluctuations to fit the observations from the latest NANOGrav data. Furthermore, we employ Bayesian analysis to assess the distinguishability of these models in light of current PTA sensitivities. Our findings indicate that with the capabilities, PTAs cannot conclusively differentiate between isocurvature and adiabatic fluctuations.","sentences":["Understanding the nature of primordial fluctuations is critical to our comprehension of the Universe's early stages.","While these fluctuations are known to be nearly scale-invariant, quasi-adiabatic, and nearly Gaussian on large scales, their behavior at smaller scales remains less well-defined and may offer insights into new physics.","Recent observations by the NANOGrav, PPTA, EPTA, and CPTA collaborations suggest the presence of a stochastic gravitational wave background, which, while consistent with the contribution from supermassive black hole binaries, also opens the possibility of probing new physics.","This paper explores whether this signal could stem from primordial isocurvature and adiabatic fluctuations.","We adopt parameterized spectra for both types of fluctuations to fit the observations from the latest NANOGrav data.","Furthermore, we employ Bayesian analysis to assess the distinguishability of these models in light of current PTA sensitivities.","Our findings indicate that with the capabilities, PTAs cannot conclusively differentiate between isocurvature and adiabatic fluctuations."],"url":"http://arxiv.org/abs/2402.16781v1","category":"astro-ph.CO"}
{"created":"2024-02-26 17:49:37","title":"On the Growth of Mistakes in Differentially Private Online Learning: A Lower Bound Perspective","abstract":"In this paper, we provide lower bounds for Differentially Private (DP) Online Learning algorithms. Our result shows that, for a broad class of $(\\varepsilon,\\delta)$-DP online algorithms, for $T$ such that $\\log T\\leq O(1 / \\delta)$, the expected number of mistakes incurred by the algorithm grows as $\\Omega(\\log \\frac{T}{\\delta})$. This matches the upper bound obtained by Golowich and Livni (2021) and is in contrast to non-private online learning where the number of mistakes is independent of $T$. To the best of our knowledge, our work is the first result towards settling lower bounds for DP-Online learning and partially addresses the open question in Sanyal and Ramponi (2022).","sentences":["In this paper, we provide lower bounds for Differentially Private (DP) Online Learning algorithms.","Our result shows that, for a broad class of $(\\varepsilon,\\delta)$-DP online algorithms, for $T$ such that $\\log T\\leq O(1 / \\delta)$, the expected number of mistakes incurred by the algorithm grows as $\\Omega(\\log \\frac{T}{\\delta})$. This matches the upper bound obtained by Golowich and Livni (2021) and is in contrast to non-private online learning where the number of mistakes is independent of $T$. To the best of our knowledge, our work is the first result towards settling lower bounds for DP-Online learning and partially addresses the open question in Sanyal and Ramponi (2022)."],"url":"http://arxiv.org/abs/2402.16778v1","category":"cs.LG"}
{"created":"2024-02-26 17:39:23","title":"Neural Population Geometry and Optimal Coding of Tasks with Shared Latent Structure","abstract":"Humans and animals can recognize latent structures in their environment and apply this information to efficiently navigate the world. Several recent works argue that the brain supports these abilities by forming neural representations that encode such latent structures in flexible, generalizable ways. However, it remains unclear what aspects of neural population activity are contributing to these computational capabilities. Here, we develop an analytical theory linking the mesoscopic statistics of a neural population's activity to generalization performance on a multi-task learning problem. To do this, we rely on a generative model in which different tasks depend on a common, unobserved latent structure and predictions are formed from a linear readout of a neural population's activity. We show that three geometric measures of the population activity determine generalization performance in these settings. Using this theory, we find that experimentally observed factorized (or disentangled) representations naturally emerge as an optimal solution to the multi-task learning problem. We go on to show that when data is scarce, optimal codes compress less informative latent variables, and when data is abundant, optimal codes expand this information in the state space. We validate predictions from our theory using biological and artificial neural network data. Our results therefore tie neural population geometry to the multi-task learning problem and make normative predictions of the structure of population activity in these settings.","sentences":["Humans and animals can recognize latent structures in their environment and apply this information to efficiently navigate the world.","Several recent works argue that the brain supports these abilities by forming neural representations that encode such latent structures in flexible, generalizable ways.","However, it remains unclear what aspects of neural population activity are contributing to these computational capabilities.","Here, we develop an analytical theory linking the mesoscopic statistics of a neural population's activity to generalization performance on a multi-task learning problem.","To do this, we rely on a generative model in which different tasks depend on a common, unobserved latent structure and predictions are formed from a linear readout of a neural population's activity.","We show that three geometric measures of the population activity determine generalization performance in these settings.","Using this theory, we find that experimentally observed factorized (or disentangled) representations naturally emerge as an optimal solution to the multi-task learning problem.","We go on to show that when data is scarce, optimal codes compress less informative latent variables, and when data is abundant, optimal codes expand this information in the state space.","We validate predictions from our theory using biological and artificial neural network data.","Our results therefore tie neural population geometry to the multi-task learning problem and make normative predictions of the structure of population activity in these settings."],"url":"http://arxiv.org/abs/2402.16770v1","category":"q-bio.NC"}
{"created":"2024-02-26 17:19:00","title":"Area preserving Combescure transformations","abstract":"Motivated by the design of flexible nets, we classify all nets of arbitrary size m x n that admit a continuous family of area-preserving Combescure transformations. There are just two different classes. The nets in the first class are special cases of cone nets that have been recently studied by Kilian, Mueller, and Tervooren. The second class consists of Koenigs nets having a Christoffel dual with the same areas of corresponding faces. We apply isotropic metric duality to get a new class of flexible nets in isotropic geometry. We also study the smooth analogs of the introduced classes.","sentences":["Motivated by the design of flexible nets, we classify all nets of arbitrary size m x n that admit a continuous family of area-preserving Combescure transformations.","There are just two different classes.","The nets in the first class are special cases of cone nets that have been recently studied by Kilian, Mueller, and Tervooren.","The second class consists of Koenigs nets having a Christoffel dual with the same areas of corresponding faces.","We apply isotropic metric duality to get a new class of flexible nets in isotropic geometry.","We also study the smooth analogs of the introduced classes."],"url":"http://arxiv.org/abs/2402.16753v1","category":"math.MG"}
{"created":"2024-02-26 17:07:14","title":"On an evolution equation in sub-Finsler geometry","abstract":"We study the gradient flow of an energy with mixed homogeneity which is at the interface of Finsler and sub-Riemannian geometry","sentences":["We study the gradient flow of an energy with mixed homogeneity which is at the interface of Finsler and sub-Riemannian geometry"],"url":"http://arxiv.org/abs/2402.16745v1","category":"math.AP"}
{"created":"2024-02-26 17:02:30","title":"Neural Mesh Fusion: Unsupervised 3D Planar Surface Understanding","abstract":"This paper presents Neural Mesh Fusion (NMF), an efficient approach for joint optimization of polygon mesh from multi-view image observations and unsupervised 3D planar-surface parsing of the scene. In contrast to implicit neural representations, NMF directly learns to deform surface triangle mesh and generate an embedding for unsupervised 3D planar segmentation through gradient-based optimization directly on the surface mesh. The conducted experiments show that NMF obtains competitive results compared to state-of-the-art multi-view planar reconstruction, while not requiring any ground-truth 3D or planar supervision. Moreover, NMF is significantly more computationally efficient compared to implicit neural rendering-based scene reconstruction approaches.","sentences":["This paper presents Neural Mesh Fusion (NMF), an efficient approach for joint optimization of polygon mesh from multi-view image observations and unsupervised 3D planar-surface parsing of the scene.","In contrast to implicit neural representations, NMF directly learns to deform surface triangle mesh and generate an embedding for unsupervised 3D planar segmentation through gradient-based optimization directly on the surface mesh.","The conducted experiments show that NMF obtains competitive results compared to state-of-the-art multi-view planar reconstruction, while not requiring any ground-truth 3D or planar supervision.","Moreover, NMF is significantly more computationally efficient compared to implicit neural rendering-based scene reconstruction approaches."],"url":"http://arxiv.org/abs/2402.16739v1","category":"cs.CV"}
{"created":"2024-02-26 16:43:17","title":"Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2)","abstract":"Real-world autonomous driving (AD) especially urban driving involves many corner cases. The lately released AD simulator CARLA v2 adds 39 common events in the driving scene, and provide more quasi-realistic testbed compared to CARLA v1. It poses new challenge to the community and so far no literature has reported any success on the new scenarios in V2 as existing works mostly have to rely on specific rules for planning yet they cannot cover the more complex cases in CARLA v2. In this work, we take the initiative of directly training a planner and the hope is to handle the corner cases flexibly and effectively, which we believe is also the future of AD. To our best knowledge, we develop the first model-based RL method named Think2Drive for AD, with a world model to learn the transitions of the environment, and then it acts as a neural simulator to train the planner. This paradigm significantly boosts the training efficiency due to the low dimensional state space and parallel computing of tensors in the world model. As a result, Think2Drive is able to run in an expert-level proficiency in CARLA v2 within 3 days of training on a single A6000 GPU, and to our best knowledge, so far there is no reported success (100\\% route completion)on CARLA v2. We also propose CornerCase-Repository, a benchmark that supports the evaluation of driving models by scenarios. Additionally, we propose a new and balanced metric to evaluate the performance by route completion, infraction number, and scenario density, so that the driving score could give more information about the actual driving performance.","sentences":["Real-world autonomous driving (AD) especially urban driving involves many corner cases.","The lately released AD simulator CARLA v2 adds 39 common events in the driving scene, and provide more quasi-realistic testbed compared to CARLA v1.","It poses new challenge to the community and so far no literature has reported any success on the new scenarios in V2 as existing works mostly have to rely on specific rules for planning yet they cannot cover the more complex cases in CARLA v2.","In this work, we take the initiative of directly training a planner and the hope is to handle the corner cases flexibly and effectively, which we believe is also the future of AD.","To our best knowledge, we develop the first model-based RL method named Think2Drive for AD, with a world model to learn the transitions of the environment, and then it acts as a neural simulator to train the planner.","This paradigm significantly boosts the training efficiency due to the low dimensional state space and parallel computing of tensors in the world model.","As a result, Think2Drive is able to run in an expert-level proficiency in CARLA v2 within 3 days of training on a single A6000 GPU, and to our best knowledge, so far there is no reported success (100\\% route completion)on CARLA v2.","We also propose CornerCase-Repository, a benchmark that supports the evaluation of driving models by scenarios.","Additionally, we propose a new and balanced metric to evaluate the performance by route completion, infraction number, and scenario density, so that the driving score could give more information about the actual driving performance."],"url":"http://arxiv.org/abs/2402.16720v1","category":"cs.RO"}
{"created":"2024-02-26 16:42:57","title":"Matrix denoising: Bayes-optimal estimators via low-degree polynomials","abstract":"We consider the additive version of the matrix denoising problem, where a random symmetric matrix $S$ of size $n$ has to be inferred from the observation of $Y=S+Z$, with $Z$ an independent random matrix modeling a noise. For prior distributions of $S$ and $Z$ that are invariant under conjugation by orthogonal matrices we determine, using results from first and second order free probability theory, the Bayes-optimal (in terms of the mean square error) polynomial estimators of degree at most $D$, asymptotically in $n$, and show that as $D$ increases they converge towards the estimator introduced by Bun, Allez, Bouchaud and Potters in [IEEE Transactions on Information Theory {\\bf 62}, 7475 (2016)]. We conjecture that this optimality holds beyond strictly orthogonally invariant priors, and provide partial evidences of this universality phenomenon when $S$ is an arbitrary Wishart matrix and $Z$ is drawn from the Gaussian Orthogonal Ensemble, a case motivated by the related extensive rank matrix factorization problem.","sentences":["We consider the additive version of the matrix denoising problem, where a random symmetric matrix $S$ of size $n$ has to be inferred from the observation of $Y=S+Z$, with $Z$ an independent random matrix modeling a noise.","For prior distributions of $S$ and $Z$ that are invariant under conjugation by orthogonal matrices we determine, using results from first and second order free probability theory, the Bayes-optimal (in terms of the mean square error) polynomial estimators of degree at most $D$, asymptotically in $n$, and show that as $D$ increases they converge towards the estimator introduced by Bun, Allez, Bouchaud and Potters in [IEEE Transactions on Information Theory {\\bf 62}, 7475 (2016)].","We conjecture that this optimality holds beyond strictly orthogonally invariant priors, and provide partial evidences of this universality phenomenon when $S$ is an arbitrary Wishart matrix and $Z$ is drawn from the Gaussian Orthogonal Ensemble, a case motivated by the related extensive rank matrix factorization problem."],"url":"http://arxiv.org/abs/2402.16719v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-26 16:30:23","title":"Action potential propagation properties of a reduced Hodgkin-Huxley type model with ATP Na-K pumps","abstract":"We explore the relationship between the sodium and potassium gating variables in the Hodgkin-Huxley electrophysiology model showing that the transport of sodium and potassium are intrinsically related and may be associated with the ubiquitous ATPase mechanism responsible for energy transport to cells. This reduces the dimension of the original 4-dimensional HH model, decreasing the complexity of the model equations. With a new reduced 2-dimensional HH-type model equations, we have derived the main properties of the propagation speed and width of action potentials for axons with the ATP Na-K pumps as a function of the transmembrane capacity $C_m$ and resistivity $R$ of the gap junctions connecting the Schwann cells of the axon. We show that the action potential propagates along the axon with speed well described by $v(R, C_m)=\\alpha /({C_m R^{\\beta}})$, where $\\alpha $ and $\\beta <1$ are constants independent of the intensity stimulus of the soma. If a current stimulus is introduced at the axon, away from the soma and with an intensity above threshold, two action potential signals propagate in opposite directions of the axon, annihilating the action potential arriving from the soma.","sentences":["We explore the relationship between the sodium and potassium gating variables in the Hodgkin-Huxley electrophysiology model showing that the transport of sodium and potassium are intrinsically related and may be associated with the ubiquitous ATPase mechanism responsible for energy transport to cells.","This reduces the dimension of the original 4-dimensional HH model, decreasing the complexity of the model equations.","With a new reduced 2-dimensional HH-type model equations, we have derived the main properties of the propagation speed and width of action potentials for axons with the ATP Na-K pumps as a function of the transmembrane capacity $C_m$ and resistivity $R$ of the gap junctions connecting the Schwann cells of the axon.","We show that the action potential propagates along the axon with speed well described by $v(R, C_m)=\\alpha /({C_m R^{\\beta}})$, where $\\alpha $ and $\\beta <1$ are constants independent of the intensity stimulus of the soma.","If a current stimulus is introduced at the axon, away from the soma and with an intensity above threshold, two action potential signals propagate in opposite directions of the axon, annihilating the action potential arriving from the soma."],"url":"http://arxiv.org/abs/2402.16711v2","category":"physics.bio-ph"}
{"created":"2024-02-26 16:07:46","title":"Fast Algorithms for Quantile Regression with Selection","abstract":"This paper addresses computational challenges in estimating Quantile Regression with Selection (QRS). The estimation of the parameters that model self-selection requires the estimation of the entire quantile process several times. Moreover, closed-form expressions of the asymptotic variance are too cumbersome, making the bootstrap more convenient to perform inference. Taking advantage of recent advancements in the estimation of quantile regression, along with some specific characteristics of the QRS estimation problem, I propose streamlined algorithms for the QRS estimator. These algorithms significantly reduce computation time through preprocessing techniques and quantile grid reduction for the estimation of the copula and slope parameters. I show the optimization enhancements with some simulations. Lastly, I show how preprocessing methods can improve the precision of the estimates without sacrificing computational efficiency. Hence, they constitute a practical solutions for estimators with non-differentiable and non-convex criterion functions such as those based on copulas.","sentences":["This paper addresses computational challenges in estimating Quantile Regression with Selection (QRS).","The estimation of the parameters that model self-selection requires the estimation of the entire quantile process several times.","Moreover, closed-form expressions of the asymptotic variance are too cumbersome, making the bootstrap more convenient to perform inference.","Taking advantage of recent advancements in the estimation of quantile regression, along with some specific characteristics of the QRS estimation problem, I propose streamlined algorithms for the QRS estimator.","These algorithms significantly reduce computation time through preprocessing techniques and quantile grid reduction for the estimation of the copula and slope parameters.","I show the optimization enhancements with some simulations.","Lastly, I show how preprocessing methods can improve the precision of the estimates without sacrificing computational efficiency.","Hence, they constitute a practical solutions for estimators with non-differentiable and non-convex criterion functions such as those based on copulas."],"url":"http://arxiv.org/abs/2402.16693v1","category":"econ.EM"}
{"created":"2024-02-26 16:01:22","title":"Pentagon relation and Biedenharn-Elliott identity","abstract":"The main subject of the paper is the pentagon relation. This relation can be expressed in different ways. We start with the natural geometric form of the pentagon relation. Then we express it in algebraic form as a family of equations with a set of linear maps as variables. Next, we derive several equivalent forms of the algebraic pentagon relation. These forms can be expressed using the classical notion of 6j-symbols. Finally, we show how to extract a solution of the pentagon relation from any modular category.","sentences":["The main subject of the paper is the pentagon relation.","This relation can be expressed in different ways.","We start with the natural geometric form of the pentagon relation.","Then we express it in algebraic form as a family of equations with a set of linear maps as variables.","Next, we derive several equivalent forms of the algebraic pentagon relation.","These forms can be expressed using the classical notion of 6j-symbols.","Finally, we show how to extract a solution of the pentagon relation from any modular category."],"url":"http://arxiv.org/abs/2402.16682v1","category":"math.GT"}
{"created":"2024-02-26 15:56:41","title":"Friedmann-Robertson-Walker spacetimes from the perspective of geometric algebra","abstract":"The intention of our paper is to provide a pedagogical application of geometric algebra to a particularly well-investigated system: We formulate the geometric and dynamical properties of Friedmann-Robertson-Walker spacetimes within the language of geometric algebra and re-derive the Friedmann-equations as the central cosmological equations. Through the geometric algebra-variant of the Raychaudhuri equations, we comment on the evolution of spacetime volumes, before illustrating conformal flatness as a central property of Friedmann-cosmologies. An important aspect of spacetime symmetries are the associated conservation laws, for which we provide a geometric algebra formulation of the Lie-derivatives, of the Killing equation and of conserved quantities in Friedmann-Robertson-Walker spacetimes. Finally, we discuss the gravitational dynamics of scalar fields, with their particular relevance in cosmology, for cosmic inflation, and for dark energy.","sentences":["The intention of our paper is to provide a pedagogical application of geometric algebra to a particularly well-investigated system: We formulate the geometric and dynamical properties of Friedmann-Robertson-Walker spacetimes within the language of geometric algebra and re-derive the Friedmann-equations as the central cosmological equations.","Through the geometric algebra-variant of the Raychaudhuri equations, we comment on the evolution of spacetime volumes, before illustrating conformal flatness as a central property of Friedmann-cosmologies.","An important aspect of spacetime symmetries are the associated conservation laws, for which we provide a geometric algebra formulation of the Lie-derivatives, of the Killing equation and of conserved quantities in Friedmann-Robertson-Walker spacetimes.","Finally, we discuss the gravitational dynamics of scalar fields, with their particular relevance in cosmology, for cosmic inflation, and for dark energy."],"url":"http://arxiv.org/abs/2402.16680v1","category":"gr-qc"}
{"created":"2024-02-26 15:53:16","title":"Tensor K-matrices for quantum symmetric pairs","abstract":"Let $\\mathfrak{g}$ be a symmetrizable Kac-Moody algebra, $U_q(\\mathfrak{g})$ its quantum group, and $U_q(\\mathfrak{k}) \\subset U_q(\\mathfrak{g})$ a quantum symmetric pair subalgebra determined by a distinguished Lie algebra automorphism $\\theta$. We introduce a category $W_\\theta$ of \\emph{weight} $U_q(\\mathfrak{k})$-modules, which is acted upon by the category of weight $U_q(\\mathfrak{g})$-modules via tensor products. We construct a universal tensor K-matrix $\\mathbb{K}$ (that is, a solution of a reflection equation) in a completion of $U_q(\\mathfrak{k}) \\otimes U_q(\\mathfrak{g})$. This yields a natural operator on any tensor product $M \\otimes V$, where $M\\in W_\\theta$ and $V\\in O_\\theta$, \\ie $V$ is a $U_q(\\mathfrak{g})$-module in category $O$ satisfying an integrability property determined by $\\theta$. $W_\\theta$ is equipped with a canonical structure of a bimodule category over $O_\\theta$ and the action of $\\mathbb{K}$ is encoded by a new categorical structure, which we refer to as a \\emph{boundary} structure on the bimodule category $W_\\theta$. This provides the most comprehensive algebraic framework to date for quantum integrability in the presence of boundaries. In particular, it generalizes a theorem of Kolb which describes a braided module structure on finite-dimensional $U_q(\\mathfrak{k})$-modules when $\\mathfrak{g}$ is finite-dimensional.   We apply our construction to the case of an affine Lie algebra, where it yields a formal tensor K-matrix valued in the endomorphisms of the tensor product of any module in $W_\\theta$ and any finite-dimensional module over the corresponding quantum affine algebra. We prove that this formal series can be normalized to a trigonometric K-matrix if the factors in the tensor product are both finite-dimensional irreducible modules over the quantum affine algebra.","sentences":["Let $\\mathfrak{g}$ be a symmetrizable Kac-Moody algebra, $U_q(\\mathfrak{g})$ its quantum group, and $U_q(\\mathfrak{k})","\\subset U_q(\\mathfrak{g})$ a quantum symmetric pair subalgebra determined by a distinguished Lie algebra automorphism $\\theta$. We introduce a category $W_\\theta$ of \\emph{weight} $U_q(\\mathfrak{k})$-modules, which is acted upon by the category of weight $U_q(\\mathfrak{g})$-modules via tensor products.","We construct a universal tensor K-matrix $\\mathbb{K}$ (that is, a solution of a reflection equation) in a completion of $U_q(\\mathfrak{k})","\\otimes U_q(\\mathfrak{g})$.","This yields a natural operator on any tensor product $M \\otimes V$, where $M\\in W_\\theta$ and $V\\in O_\\theta$, \\ie $V$ is a $U_q(\\mathfrak{g})$-module in category $O$ satisfying an integrability property determined by $\\theta$. $W_\\theta$ is equipped with a canonical structure of a bimodule category over $O_\\theta$ and the action of $\\mathbb{K}$ is encoded by a new categorical structure, which we refer to as a \\emph{boundary} structure on the bimodule category $W_\\theta$.","This provides the most comprehensive algebraic framework to date for quantum integrability in the presence of boundaries.","In particular, it generalizes a theorem of Kolb which describes a braided module structure on finite-dimensional $U_q(\\mathfrak{k})$-modules when $\\mathfrak{g}$ is finite-dimensional.   ","We apply our construction to the case of an affine Lie algebra, where it yields a formal tensor K-matrix valued in the endomorphisms of the tensor product of any module in $W_\\theta$ and any finite-dimensional module over the corresponding quantum affine algebra.","We prove that this formal series can be normalized to a trigonometric K-matrix if the factors in the tensor product are both finite-dimensional irreducible modules over the quantum affine algebra."],"url":"http://arxiv.org/abs/2402.16676v1","category":"math.RT"}
{"created":"2024-02-26 15:48:13","title":"Effective mass approach to memory in non-Markovian systems","abstract":"Recent pioneering experiments on non-Markovian dynamics done e.g. for active matter have demonstrated that our theoretical understanding of this challenging yet hot topic is rather incomplete and there is a wealth of phenomena still awaiting discovery. It is related to the fact that typically for simplification the Markovian approximation is employed and as a consequence the memory is neglected. Therefore methods allowing to study memory effects are extremely valuable. We demonstrate that a non-Markovian system described by the Generalized Langevin Equation (GLE) for a Brownian particle of mass $M$ can be approximated by the memoryless Langevin equation in which the memory effects are correctly reproduced solely via the effective mass $M^*$ of the Brownian particle which is determined only by the form of the memory kernel. Our work lays the foundation for an impactful approach which allows to readily study memory-related corrections to Markovian dynamics.","sentences":["Recent pioneering experiments on non-Markovian dynamics done e.g. for active matter have demonstrated that our theoretical understanding of this challenging yet hot topic is rather incomplete and there is a wealth of phenomena still awaiting discovery.","It is related to the fact that typically for simplification the Markovian approximation is employed and as a consequence the memory is neglected.","Therefore methods allowing to study memory effects are extremely valuable.","We demonstrate that a non-Markovian system described by the Generalized Langevin Equation (GLE) for a Brownian particle of mass $M$ can be approximated by the memoryless Langevin equation in which the memory effects are correctly reproduced solely via the effective mass $M^*$ of the Brownian particle which is determined only by the form of the memory kernel.","Our work lays the foundation for an impactful approach which allows to readily study memory-related corrections to Markovian dynamics."],"url":"http://arxiv.org/abs/2402.16673v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-26 15:44:06","title":"Structure-Preserving Numerical Methods for Two Nonlinear Systems of Dispersive Wave Equations","abstract":"We use the general framework of summation by parts operators to construct conservative, entropy-stable and well-balanced semidiscretizations of two different nonlinear systems of dispersive shallow water equations with varying bathymetry: (i) a variant of the coupled Benjamin-Bona-Mahony (BBM) equations and (ii) a recently proposed model by Sv\\\"ard and Kalisch (2023) with enhanced dispersive behavior. Both models share the property of being conservative in terms of a nonlinear invariant, often interpreted as entropy function. This property is preserved exactly in our novel semidiscretizations. To obtain fully-discrete entropy-stable schemes, we employ the relaxation method. We present improved numerical properties of our schemes in some test cases.","sentences":["We use the general framework of summation by parts operators to construct conservative, entropy-stable and well-balanced semidiscretizations of two different nonlinear systems of dispersive shallow water equations with varying bathymetry: (i) a variant of the coupled Benjamin-Bona-Mahony (BBM) equations and (ii) a recently proposed model by Sv\\\"ard and Kalisch (2023) with enhanced dispersive behavior.","Both models share the property of being conservative in terms of a nonlinear invariant, often interpreted as entropy function.","This property is preserved exactly in our novel semidiscretizations.","To obtain fully-discrete entropy-stable schemes, we employ the relaxation method.","We present improved numerical properties of our schemes in some test cases."],"url":"http://arxiv.org/abs/2402.16669v1","category":"math.NA"}
{"created":"2024-02-27 18:55:17","title":"Massive Activations in Large Language Models","abstract":"We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.","sentences":["We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger).","We call them massive activations.","First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations.","Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs.","Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output.","Last, we also study massive activations in Vision Transformers."],"url":"http://arxiv.org/abs/2402.17762v1","category":"cs.CL"}
{"created":"2024-02-27 17:25:37","title":"RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations","abstract":"Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.","sentences":["Individual neurons participate in the representation of multiple high-level concepts.","To what extent can different interpretability methods successfully disentangle these roles?","To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods.","We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria.","With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations.","We release our benchmark at https://github.com/explanare/ravel."],"url":"http://arxiv.org/abs/2402.17700v1","category":"cs.CL"}
{"created":"2024-02-27 16:53:16","title":"CAD-SIGNet: CAD Language Inference from Point Clouds using Layer-wise Sketch Instance Guided Attention","abstract":"Reverse engineering in the realm of Computer-Aided Design (CAD) has been a longstanding aspiration, though not yet entirely realized. Its primary aim is to uncover the CAD process behind a physical object given its 3D scan. We propose CAD-SIGNet, an end-to-end trainable and auto-regressive architecture to recover the design history of a CAD model represented as a sequence of sketch-and-extrusion from an input point cloud. Our model learns visual-language representations by layer-wise cross-attention between point cloud and CAD language embedding. In particular, a new Sketch instance Guided Attention (SGA) module is proposed in order to reconstruct the fine-grained details of the sketches. Thanks to its auto-regressive nature, CAD-SIGNet not only reconstructs a unique full design history of the corresponding CAD model given an input point cloud but also provides multiple plausible design choices. This allows for an interactive reverse engineering scenario by providing designers with multiple next-step choices along with the design process. Extensive experiments on publicly available CAD datasets showcase the effectiveness of our approach against existing baseline models in two settings, namely, full design history recovery and conditional auto-completion from point clouds.","sentences":["Reverse engineering in the realm of Computer-Aided Design (CAD) has been a longstanding aspiration, though not yet entirely realized.","Its primary aim is to uncover the CAD process behind a physical object given its 3D scan.","We propose CAD-SIGNet, an end-to-end trainable and auto-regressive architecture to recover the design history of a CAD model represented as a sequence of sketch-and-extrusion from an input point cloud.","Our model learns visual-language representations by layer-wise cross-attention between point cloud and CAD language embedding.","In particular, a new Sketch instance Guided Attention (SGA) module is proposed in order to reconstruct the fine-grained details of the sketches.","Thanks to its auto-regressive nature, CAD-SIGNet not only reconstructs a unique full design history of the corresponding CAD model given an input point cloud but also provides multiple plausible design choices.","This allows for an interactive reverse engineering scenario by providing designers with multiple next-step choices along with the design process.","Extensive experiments on publicly available CAD datasets showcase the effectiveness of our approach against existing baseline models in two settings, namely, full design history recovery and conditional auto-completion from point clouds."],"url":"http://arxiv.org/abs/2402.17678v1","category":"cs.CV"}
{"created":"2024-02-27 16:44:09","title":"Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models","abstract":"As foundation models (FMs) continue to shape the landscape of AI, the in-context learning (ICL) paradigm thrives but also encounters issues such as toxicity, hallucination, disparity, adversarial vulnerability, and inconsistency. Ensuring the reliability and responsibility of FMs is crucial for the sustainable development of the AI ecosystem. In this concise overview, we investigate recent advancements in enhancing the reliability and trustworthiness of FMs within ICL frameworks, focusing on four key methodologies, each with its corresponding subgoals. We sincerely hope this paper can provide valuable insights for researchers and practitioners endeavoring to build safe and dependable FMs and foster a stable and consistent ICL environment, thereby unlocking their vast potential.","sentences":["As foundation models (FMs) continue to shape the landscape of AI, the in-context learning (ICL) paradigm thrives but also encounters issues such as toxicity, hallucination, disparity, adversarial vulnerability, and inconsistency.","Ensuring the reliability and responsibility of FMs is crucial for the sustainable development of the AI ecosystem.","In this concise overview, we investigate recent advancements in enhancing the reliability and trustworthiness of FMs within ICL frameworks, focusing on four key methodologies, each with its corresponding subgoals.","We sincerely hope this paper can provide valuable insights for researchers and practitioners endeavoring to build safe and dependable FMs and foster a stable and consistent ICL environment, thereby unlocking their vast potential."],"url":"http://arxiv.org/abs/2402.17671v1","category":"cs.LG"}
{"created":"2024-02-27 15:49:26","title":"Supervised machine learning for microbiomics: bridging the gap between current and best practices","abstract":"Machine learning (ML) is set to accelerate innovations in clinical microbiomics, such as in disease diagnostics and prognostics. This will require high-quality, reproducible, interpretable workflows whose predictive capabilities meet or exceed the high thresholds set for clinical tools by regulatory agencies. Here, we capture a snapshot of current practices in the application of supervised ML to microbiomics data, through an in-depth analysis of 100 peer-reviewed journal articles published in 2021-2022. We apply a data-driven approach to steer discussion of the merits of varied approaches to experimental design, including key considerations such as how to mitigate the effects of small dataset size while avoiding data leakage. We further provide guidance on how to avoid common experimental design pitfalls that can hurt model performance, trustworthiness, and reproducibility. Discussion is accompanied by an interactive online tutorial that demonstrates foundational principles of ML experimental design, tailored to the microbiomics community. Formalizing community best practices for supervised ML in microbiomics is an important step towards improving the success and efficiency of clinical research, to the benefit of patients and other stakeholders.","sentences":["Machine learning (ML) is set to accelerate innovations in clinical microbiomics, such as in disease diagnostics and prognostics.","This will require high-quality, reproducible, interpretable workflows whose predictive capabilities meet or exceed the high thresholds set for clinical tools by regulatory agencies.","Here, we capture a snapshot of current practices in the application of supervised ML to microbiomics data, through an in-depth analysis of 100 peer-reviewed journal articles published in 2021-2022.","We apply a data-driven approach to steer discussion of the merits of varied approaches to experimental design, including key considerations such as how to mitigate the effects of small dataset size while avoiding data leakage.","We further provide guidance on how to avoid common experimental design pitfalls that can hurt model performance, trustworthiness, and reproducibility.","Discussion is accompanied by an interactive online tutorial that demonstrates foundational principles of ML experimental design, tailored to the microbiomics community.","Formalizing community best practices for supervised ML in microbiomics is an important step towards improving the success and efficiency of clinical research, to the benefit of patients and other stakeholders."],"url":"http://arxiv.org/abs/2402.17621v1","category":"q-bio.GN"}
{"created":"2024-02-27 15:37:15","title":"A Large-scale Evaluation of Pretraining Paradigms for the Detection of Defects in Electroluminescence Solar Cell Images","abstract":"Pretraining has been shown to improve performance in many domains, including semantic segmentation, especially in domains with limited labelled data. In this work, we perform a large-scale evaluation and benchmarking of various pretraining methods for Solar Cell Defect Detection (SCDD) in electroluminescence images, a field with limited labelled datasets. We cover supervised training with semantic segmentation, semi-supervised learning, and two self-supervised techniques. We also experiment with both in-distribution and out-of-distribution (OOD) pretraining and observe how this affects downstream performance. The results suggest that supervised training on a large OOD dataset (COCO), self-supervised pretraining on a large OOD dataset (ImageNet), and semi-supervised pretraining (CCT) all yield statistically equivalent performance for mean Intersection over Union (mIoU). We achieve a new state-of-the-art for SCDD and demonstrate that certain pretraining schemes result in superior performance on underrepresented classes. Additionally, we provide a large-scale unlabelled EL image dataset of $22000$ images, and a $642$-image labelled semantic segmentation EL dataset, for further research in developing self- and semi-supervised training techniques in this domain.","sentences":["Pretraining has been shown to improve performance in many domains, including semantic segmentation, especially in domains with limited labelled data.","In this work, we perform a large-scale evaluation and benchmarking of various pretraining methods for Solar Cell Defect Detection (SCDD) in electroluminescence images, a field with limited labelled datasets.","We cover supervised training with semantic segmentation, semi-supervised learning, and two self-supervised techniques.","We also experiment with both in-distribution and out-of-distribution (OOD) pretraining and observe how this affects downstream performance.","The results suggest that supervised training on a large OOD dataset (COCO), self-supervised pretraining on a large OOD dataset (ImageNet), and semi-supervised pretraining (CCT) all yield statistically equivalent performance for mean Intersection over Union (mIoU).","We achieve a new state-of-the-art for SCDD and demonstrate that certain pretraining schemes result in superior performance on underrepresented classes.","Additionally, we provide a large-scale unlabelled EL image dataset of $22000$ images, and a $642$-image labelled semantic segmentation EL dataset, for further research in developing self- and semi-supervised training techniques in this domain."],"url":"http://arxiv.org/abs/2402.17611v1","category":"cs.CV"}
{"created":"2024-02-27 15:22:20","title":"PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive Representation Learning","abstract":"Recently, the application of Contrastive Representation Learning (CRL) in learning with noisy labels (LNL) has shown promising advancements due to its remarkable ability to learn well-distributed representations for better distinguishing noisy labels. However, CRL is mainly used as a pre-training technique, leading to a complicated multi-stage training pipeline. We also observed that trivially combining CRL with supervised LNL methods decreases performance. Using different images from the same class as negative pairs in CRL creates optimization conflicts between CRL and the supervised loss. To address these two issues, we propose an end-to-end PLReMix framework that avoids the complicated pipeline by introducing a Pseudo-Label Relaxed (PLR) contrastive loss to alleviate the conflicts between losses. This PLR loss constructs a reliable negative set of each sample by filtering out its inappropriate negative pairs that overlap at the top k indices of prediction probabilities, leading to more compact semantic clusters than vanilla CRL. Furthermore, a two-dimensional Gaussian Mixture Model (GMM) is adopted to distinguish clean and noisy samples by leveraging semantic information and model outputs simultaneously, which is expanded on the previously widely used one-dimensional form. The PLR loss and a semi-supervised loss are simultaneously applied to train on the GMM divided clean and noisy samples. Experiments on multiple benchmark datasets demonstrate the effectiveness of the proposed method. Our proposed PLR loss is scalable, which can be easily integrated into other LNL methods and boost their performance. Codes will be available.","sentences":["Recently, the application of Contrastive Representation Learning (CRL) in learning with noisy labels (LNL) has shown promising advancements due to its remarkable ability to learn well-distributed representations for better distinguishing noisy labels.","However, CRL is mainly used as a pre-training technique, leading to a complicated multi-stage training pipeline.","We also observed that trivially combining CRL with supervised LNL methods decreases performance.","Using different images from the same class as negative pairs in CRL creates optimization conflicts between CRL and the supervised loss.","To address these two issues, we propose an end-to-end PLReMix framework that avoids the complicated pipeline by introducing a Pseudo-Label Relaxed (PLR) contrastive loss to alleviate the conflicts between losses.","This PLR loss constructs a reliable negative set of each sample by filtering out its inappropriate negative pairs that overlap at the top k indices of prediction probabilities, leading to more compact semantic clusters than vanilla CRL.","Furthermore, a two-dimensional Gaussian Mixture Model (GMM) is adopted to distinguish clean and noisy samples by leveraging semantic information and model outputs simultaneously, which is expanded on the previously widely used one-dimensional form.","The PLR loss and a semi-supervised loss are simultaneously applied to train on the GMM divided clean and noisy samples.","Experiments on multiple benchmark datasets demonstrate the effectiveness of the proposed method.","Our proposed PLR loss is scalable, which can be easily integrated into other LNL methods and boost their performance.","Codes will be available."],"url":"http://arxiv.org/abs/2402.17589v1","category":"cs.CV"}
{"created":"2024-02-27 15:09:20","title":"Hyperdimensional computing: a fast, robust and interpretable paradigm for biological data","abstract":"Advances in bioinformatics are primarily due to new algorithms for processing diverse biological data sources. While sophisticated alignment algorithms have been pivotal in analyzing biological sequences, deep learning has substantially transformed bioinformatics, addressing sequence, structure, and functional analyses. However, these methods are incredibly data-hungry, compute-intensive and hard to interpret. Hyperdimensional computing (HDC) has recently emerged as an intriguing alternative. The key idea is that random vectors of high dimensionality can represent concepts such as sequence identity or phylogeny. These vectors can then be combined using simple operators for learning, reasoning or querying by exploiting the peculiar properties of high-dimensional spaces. Our work reviews and explores the potential of HDC for bioinformatics, emphasizing its efficiency, interpretability, and adeptness in handling multimodal and structured data. HDC holds a lot of potential for various omics data searching, biosignal analysis and health applications.","sentences":["Advances in bioinformatics are primarily due to new algorithms for processing diverse biological data sources.","While sophisticated alignment algorithms have been pivotal in analyzing biological sequences, deep learning has substantially transformed bioinformatics, addressing sequence, structure, and functional analyses.","However, these methods are incredibly data-hungry, compute-intensive and hard to interpret.","Hyperdimensional computing (HDC) has recently emerged as an intriguing alternative.","The key idea is that random vectors of high dimensionality can represent concepts such as sequence identity or phylogeny.","These vectors can then be combined using simple operators for learning, reasoning or querying by exploiting the peculiar properties of high-dimensional spaces.","Our work reviews and explores the potential of HDC for bioinformatics, emphasizing its efficiency, interpretability, and adeptness in handling multimodal and structured data.","HDC holds a lot of potential for various omics data searching, biosignal analysis and health applications."],"url":"http://arxiv.org/abs/2402.17572v1","category":"cs.LG"}
{"created":"2024-02-27 13:49:12","title":"Extreme Miscalibration and the Illusion of Adversarial Robustness","abstract":"Deep learning-based Natural Language Processing (NLP) models are vulnerable to adversarial attacks, where small perturbations can cause a model to misclassify. Adversarial Training (AT) is often used to increase model robustness. However, we have discovered an intriguing phenomenon: deliberately or accidentally miscalibrating models masks gradients in a way that interferes with adversarial attack search methods, giving rise to an apparent increase in robustness. We show that this observed gain in robustness is an illusion of robustness (IOR), and demonstrate how an adversary can perform various forms of test-time temperature calibration to nullify the aforementioned interference and allow the adversarial attack to find adversarial examples. Hence, we urge the NLP community to incorporate test-time temperature scaling into their robustness evaluations to ensure that any observed gains are genuine. Finally, we show how the temperature can be scaled during \\textit{training} to improve genuine robustness.","sentences":["Deep learning-based Natural Language Processing (NLP) models are vulnerable to adversarial attacks, where small perturbations can cause a model to misclassify.","Adversarial Training (AT) is often used to increase model robustness.","However, we have discovered an intriguing phenomenon: deliberately or accidentally miscalibrating models masks gradients in a way that interferes with adversarial attack search methods, giving rise to an apparent increase in robustness.","We show that this observed gain in robustness is an illusion of robustness (IOR), and demonstrate how an adversary can perform various forms of test-time temperature calibration to nullify the aforementioned interference and allow the adversarial attack to find adversarial examples.","Hence, we urge the NLP community to incorporate test-time temperature scaling into their robustness evaluations to ensure that any observed gains are genuine.","Finally, we show how the temperature can be scaled during \\textit{training} to improve genuine robustness."],"url":"http://arxiv.org/abs/2402.17509v1","category":"cs.CL"}
{"created":"2024-02-27 11:50:44","title":"V2C-Long: Longitudinal Cortex Reconstruction with Spatiotemporal Correspondence","abstract":"Reconstructing the cortex from longitudinal MRI is indispensable for analyzing morphological changes in the human brain. Despite the recent disruption of cortical surface reconstruction with deep learning, challenges arising from longitudinal data are still persistent. Especially the lack of strong spatiotemporal point correspondence hinders downstream analyses due to the introduced noise. To address this issue, we present V2C-Long, the first dedicated deep learning-based cortex reconstruction method for longitudinal MRI. In contrast to existing methods, V2C-Long surfaces are directly comparable in a cross-sectional and longitudinal manner. We establish strong inherent spatiotemporal correspondences via a novel composition of two deep mesh deformation networks and fast aggregation of feature-enhanced within-subject templates. The results on internal and external test data demonstrate that V2C-Long yields cortical surfaces with improved accuracy and consistency compared to previous methods. Finally, this improvement manifests in higher sensitivity to regional cortical atrophy in Alzheimer's disease.","sentences":["Reconstructing the cortex from longitudinal MRI is indispensable for analyzing morphological changes in the human brain.","Despite the recent disruption of cortical surface reconstruction with deep learning, challenges arising from longitudinal data are still persistent.","Especially the lack of strong spatiotemporal point correspondence hinders downstream analyses due to the introduced noise.","To address this issue, we present V2C-Long, the first dedicated deep learning-based cortex reconstruction method for longitudinal MRI.","In contrast to existing methods, V2C-Long surfaces are directly comparable in a cross-sectional and longitudinal manner.","We establish strong inherent spatiotemporal correspondences via a novel composition of two deep mesh deformation networks and fast aggregation of feature-enhanced within-subject templates.","The results on internal and external test data demonstrate that V2C-Long yields cortical surfaces with improved accuracy and consistency compared to previous methods.","Finally, this improvement manifests in higher sensitivity to regional cortical atrophy in Alzheimer's disease."],"url":"http://arxiv.org/abs/2402.17438v1","category":"eess.IV"}
{"created":"2024-02-27 09:03:43","title":"Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond","abstract":"We study the data selection problem, whose aim is to select a small representative subset of data that can be used to efficiently train a machine learning model. We present a new data selection approach based on $k$-means clustering and sensitivity sampling. Assuming access to an embedding representation of the data with respect to which the model loss is H\\\"older continuous, our approach provably allows selecting a set of ``typical'' $k + 1/\\varepsilon^2$ elements whose average loss corresponds to the average loss of the whole dataset, up to a multiplicative $(1\\pm\\varepsilon)$ factor and an additive $\\varepsilon \\lambda \\Phi_k$, where $\\Phi_k$ represents the $k$-means cost for the input embeddings and $\\lambda$ is the H\\\"older constant.   We furthermore demonstrate the performance and scalability of our approach on fine-tuning foundation models and show that it outperforms state-of-the-art methods. We also show how it can be applied on linear regression, leading to a new sampling strategy that surprisingly matches the performances of leverage score sampling, while being conceptually simpler and more scalable.","sentences":["We study the data selection problem, whose aim is to select a small representative subset of data that can be used to efficiently train a machine learning model.","We present a new data selection approach based on $k$-means clustering and sensitivity sampling.","Assuming access to an embedding representation of the data with respect to which the model loss is H\\\"older continuous, our approach provably allows selecting a set of ``typical''","$k + 1/\\varepsilon^2$ elements whose average loss corresponds to the average loss of the whole dataset, up to a multiplicative $(1\\pm\\varepsilon)$ factor and an additive $\\varepsilon \\lambda \\Phi_k$, where $\\Phi_k$ represents the $k$-means cost for the input embeddings and $\\lambda$ is the H\\\"older constant.   ","We furthermore demonstrate the performance and scalability of our approach on fine-tuning foundation models and show that it outperforms state-of-the-art methods.","We also show how it can be applied on linear regression, leading to a new sampling strategy that surprisingly matches the performances of leverage score sampling, while being conceptually simpler and more scalable."],"url":"http://arxiv.org/abs/2402.17327v1","category":"cs.LG"}
{"created":"2024-02-27 09:01:03","title":"SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection","abstract":"In the field of class incremental learning (CIL), genera- tive replay has become increasingly prominent as a method to mitigate the catastrophic forgetting, alongside the con- tinuous improvements in generative models. However, its application in class incremental object detection (CIOD) has been significantly limited, primarily due to the com- plexities of scenes involving multiple labels. In this paper, we propose a novel approach called stable diffusion deep generative replay (SDDGR) for CIOD. Our method utilizes a diffusion-based generative model with pre-trained text- to-diffusion networks to generate realistic and diverse syn- thetic images. SDDGR incorporates an iterative refinement strategy to produce high-quality images encompassing old classes. Additionally, we adopt an L2 knowledge distilla- tion technique to improve the retention of prior knowledge in synthetic images. Furthermore, our approach includes pseudo-labeling for old objects within new task images, pre- venting misclassification as background elements. Exten- sive experiments on the COCO 2017 dataset demonstrate that SDDGR significantly outperforms existing algorithms, achieving a new state-of-the-art in various CIOD scenarios. The source code will be made available to the public.","sentences":["In the field of class incremental learning (CIL), genera- tive replay has become increasingly prominent as a method to mitigate the catastrophic forgetting, alongside the con- tinuous improvements in generative models.","However, its application in class incremental object detection (CIOD) has been significantly limited, primarily due to the com- plexities of scenes involving multiple labels.","In this paper, we propose a novel approach called stable diffusion deep generative replay (SDDGR) for CIOD.","Our method utilizes a diffusion-based generative model with pre-trained text- to-diffusion networks to generate realistic and diverse syn- thetic images.","SDDGR incorporates an iterative refinement strategy to produce high-quality images encompassing old classes.","Additionally, we adopt an L2 knowledge distilla- tion technique to improve the retention of prior knowledge in synthetic images.","Furthermore, our approach includes pseudo-labeling for old objects within new task images, pre- venting misclassification as background elements.","Exten- sive experiments on the COCO 2017 dataset demonstrate that SDDGR significantly outperforms existing algorithms, achieving a new state-of-the-art in various CIOD scenarios.","The source code will be made available to the public."],"url":"http://arxiv.org/abs/2402.17323v1","category":"cs.CV"}
{"created":"2024-02-27 08:51:20","title":"A Vanilla Multi-Task Framework for Dense Visual Prediction Solution to 1st VCL Challenge -- Multi-Task Robustness Track","abstract":"In this report, we present our solution to the multi-task robustness track of the 1st Visual Continual Learning (VCL) Challenge at ICCV 2023 Workshop. We propose a vanilla framework named UniNet that seamlessly combines various visual perception algorithms into a multi-task model. Specifically, we choose DETR3D, Mask2Former, and BinsFormer for 3D object detection, instance segmentation, and depth estimation tasks, respectively. The final submission is a single model with InternImage-L backbone, and achieves a 49.6 overall score (29.5 Det mAP, 80.3 mTPS, 46.4 Seg mAP, and 7.93 silog) on SHIFT validation set. Besides, we provide some interesting observations in our experiments which may facilitate the development of multi-task learning in dense visual prediction.","sentences":["In this report, we present our solution to the multi-task robustness track of the 1st Visual Continual Learning (VCL) Challenge at ICCV 2023 Workshop.","We propose a vanilla framework named UniNet that seamlessly combines various visual perception algorithms into a multi-task model.","Specifically, we choose DETR3D, Mask2Former, and BinsFormer for 3D object detection, instance segmentation, and depth estimation tasks, respectively.","The final submission is a single model with InternImage-L backbone, and achieves a 49.6 overall score (29.5 Det mAP, 80.3 mTPS, 46.4 Seg mAP, and 7.93 silog) on SHIFT validation set.","Besides, we provide some interesting observations in our experiments which may facilitate the development of multi-task learning in dense visual prediction."],"url":"http://arxiv.org/abs/2402.17319v1","category":"cs.CV"}
{"created":"2024-02-27 08:49:30","title":"How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation","abstract":"Deep Learning is the state-of-the-art technology for segmenting brain tumours. However, this requires a lot of high-quality data, which is difficult to obtain, especially in the medical field. Therefore, our solutions address this problem by using unconventional mechanisms for data augmentation. Generative adversarial networks and registration are used to massively increase the amount of available samples for training three different deep learning models for brain tumour segmentation, the first task of the BraTS2023 challenge. The first model is the standard nnU-Net, the second is the Swin UNETR and the third is the winning solution of the BraTS 2021 Challenge. The entire pipeline is built on the nnU-Net implementation, except for the generation of the synthetic data. The use of convolutional algorithms and transformers is able to fill each other's knowledge gaps. Using the new metric, our best solution achieves the dice results 0.9005, 0.8673, 0.8509 and HD95 14.940, 14.467, 17.699 (whole tumour, tumour core and enhancing tumour) in the validation set.","sentences":["Deep Learning is the state-of-the-art technology for segmenting brain tumours.","However, this requires a lot of high-quality data, which is difficult to obtain, especially in the medical field.","Therefore, our solutions address this problem by using unconventional mechanisms for data augmentation.","Generative adversarial networks and registration are used to massively increase the amount of available samples for training three different deep learning models for brain tumour segmentation, the first task of the BraTS2023 challenge.","The first model is the standard nnU-Net, the second is the Swin UNETR and the third is the winning solution of the BraTS 2021 Challenge.","The entire pipeline is built on the nnU-Net implementation, except for the generation of the synthetic data.","The use of convolutional algorithms and transformers is able to fill each other's knowledge gaps.","Using the new metric, our best solution achieves the dice results 0.9005, 0.8673, 0.8509 and HD95 14.940, 14.467, 17.699 (whole tumour, tumour core and enhancing tumour) in the validation set."],"url":"http://arxiv.org/abs/2402.17317v1","category":"eess.IV"}
{"created":"2024-02-27 08:31:39","title":"Denoising Diffusion Models for Inpainting of Healthy Brain Tissue","abstract":"This paper is a contribution to the \"BraTS 2023 Local Synthesis of Healthy Brain Tissue via Inpainting Challenge\". The task of this challenge is to transform tumor tissue into healthy tissue in brain magnetic resonance (MR) images. This idea originates from the problem that MR images can be evaluated using automatic processing tools, however, many of these tools are optimized for the analysis of healthy tissue. By solving the given inpainting task, we enable the automatic analysis of images featuring lesions, and further downstream tasks. Our approach builds on denoising diffusion probabilistic models. We use a 2D model that is trained using slices in which healthy tissue was cropped out and is learned to be inpainted again. This allows us to use the ground truth healthy tissue during training. In the sampling stage, we replace the slices containing diseased tissue in the original 3D volume with the slices containing the healthy tissue inpainting. With our approach, we achieve comparable results to the competing methods. On the validation set our model achieves a mean SSIM of 0.7804, a PSNR of 20.3525 and a MSE of 0.0113. In future we plan to extend our 2D model to a 3D model, allowing to inpaint the region of interest as a whole without losing context information of neighboring slices.","sentences":["This paper is a contribution to the \"BraTS 2023 Local Synthesis of Healthy Brain Tissue via Inpainting Challenge\".","The task of this challenge is to transform tumor tissue into healthy tissue in brain magnetic resonance (MR) images.","This idea originates from the problem that MR images can be evaluated using automatic processing tools, however, many of these tools are optimized for the analysis of healthy tissue.","By solving the given inpainting task, we enable the automatic analysis of images featuring lesions, and further downstream tasks.","Our approach builds on denoising diffusion probabilistic models.","We use a 2D model that is trained using slices in which healthy tissue was cropped out and is learned to be inpainted again.","This allows us to use the ground truth healthy tissue during training.","In the sampling stage, we replace the slices containing diseased tissue in the original 3D volume with the slices containing the healthy tissue inpainting.","With our approach, we achieve comparable results to the competing methods.","On the validation set our model achieves a mean SSIM of 0.7804, a PSNR of 20.3525 and a MSE of 0.0113.","In future we plan to extend our 2D model to a 3D model, allowing to inpaint the region of interest as a whole without losing context information of neighboring slices."],"url":"http://arxiv.org/abs/2402.17307v1","category":"eess.IV"}
{"created":"2024-02-27 08:31:12","title":"The Second Round: Diverse Paths Towards Software Engineering","abstract":"In the extant literature, there has been discussion on the drivers and motivations of minorities to enter the software industry. For example, universities have invested in more diverse imagery for years to attract a more diverse pool of students. However, in our research, we consider whether we understand why students choose their current major and how they did in the beginning decided to apply to study software engineering. We were also interested in learning if there could be some signs that would help us in marketing to get more women into tech. We approached the topic via an online survey (N = 78) sent to the university students of software engineering in Finland. Our results show that, on average, women apply later to software engineering studies than men, with statistically significant differences between genders. Additionally, we found that marketing actions have different impacts based on gender: personal guidance in live events or platforms is most influential for women, whereas teachers and social media have a more significant impact on men. The results also indicate two main paths into the field: the traditional linear educational pathway and the adult career change pathway, each significantly varying by gender","sentences":["In the extant literature, there has been discussion on the drivers and motivations of minorities to enter the software industry.","For example, universities have invested in more diverse imagery for years to attract a more diverse pool of students.","However, in our research, we consider whether we understand why students choose their current major and how they did in the beginning decided to apply to study software engineering.","We were also interested in learning if there could be some signs that would help us in marketing to get more women into tech.","We approached the topic via an online survey (N = 78) sent to the university students of software engineering in Finland.","Our results show that, on average, women apply later to software engineering studies than men, with statistically significant differences between genders.","Additionally, we found that marketing actions have different impacts based on gender: personal guidance in live events or platforms is most influential for women, whereas teachers and social media have a more significant impact on men.","The results also indicate two main paths into the field: the traditional linear educational pathway and the adult career change pathway, each significantly varying by gender"],"url":"http://arxiv.org/abs/2402.17306v1","category":"cs.SE"}
{"created":"2024-02-27 08:22:55","title":"VoCo: A Simple-yet-Effective Volume Contrastive Learning Framework for 3D Medical Image Analysis","abstract":"Self-Supervised Learning (SSL) has demonstrated promising results in 3D medical image analysis. However, the lack of high-level semantics in pre-training still heavily hinders the performance of downstream tasks. We observe that 3D medical images contain relatively consistent contextual position information, i.e., consistent geometric relations between different organs, which leads to a potential way for us to learn consistent semantic representations in pre-training. In this paper, we propose a simple-yet-effective Volume Contrast (VoCo) framework to leverage the contextual position priors for pre-training. Specifically, we first generate a group of base crops from different regions while enforcing feature discrepancy among them, where we employ them as class assignments of different regions. Then, we randomly crop sub-volumes and predict them belonging to which class (located at which region) by contrasting their similarity to different base crops, which can be seen as predicting contextual positions of different sub-volumes. Through this pretext task, VoCo implicitly encodes the contextual position priors into model representations without the guidance of annotations, enabling us to effectively improve the performance of downstream tasks that require high-level semantics. Extensive experimental results on six downstream tasks demonstrate the superior effectiveness of VoCo. Code will be available at https://github.com/Luffy03/VoCo.","sentences":["Self-Supervised Learning (SSL) has demonstrated promising results in 3D medical image analysis.","However, the lack of high-level semantics in pre-training still heavily hinders the performance of downstream tasks.","We observe that 3D medical images contain relatively consistent contextual position information, i.e., consistent geometric relations between different organs, which leads to a potential way for us to learn consistent semantic representations in pre-training.","In this paper, we propose a simple-yet-effective Volume Contrast (VoCo) framework to leverage the contextual position priors for pre-training.","Specifically, we first generate a group of base crops from different regions while enforcing feature discrepancy among them, where we employ them as class assignments of different regions.","Then, we randomly crop sub-volumes and predict them belonging to which class (located at which region) by contrasting their similarity to different base crops, which can be seen as predicting contextual positions of different sub-volumes.","Through this pretext task, VoCo implicitly encodes the contextual position priors into model representations without the guidance of annotations, enabling us to effectively improve the performance of downstream tasks that require high-level semantics.","Extensive experimental results on six downstream tasks demonstrate the superior effectiveness of VoCo.","Code will be available at https://github.com/Luffy03/VoCo."],"url":"http://arxiv.org/abs/2402.17300v1","category":"eess.IV"}
{"created":"2024-02-27 08:19:51","title":"Learning Exposure Correction in Dynamic Scenes","abstract":"Capturing videos with wrong exposure usually produces unsatisfactory visual effects. While image exposure correction is a popular topic, the video counterpart is less explored in the literature. Directly applying prior image-based methods to input videos often results in temporal incoherence with low visual quality. Existing research in this area is also limited by the lack of high-quality benchmark datasets. To address these issues, we construct the first real-world paired video dataset, including both underexposure and overexposure dynamic scenes. To achieve spatial alignment, we utilize two DSLR cameras and a beam splitter to simultaneously capture improper and normal exposure videos. In addition, we propose a Video Exposure Correction Network (VECNet) based on Retinex theory, which incorporates a two-stream illumination learning mechanism to enhance the overexposure and underexposure factors, respectively. The estimated multi-frame reflectance and dual-path illumination components are fused at both feature and image levels, leading to visually appealing results. Experimental results demonstrate that the proposed method outperforms existing image exposure correction and underexposed video enhancement methods. The code and dataset will be available soon.","sentences":["Capturing videos with wrong exposure usually produces unsatisfactory visual effects.","While image exposure correction is a popular topic, the video counterpart is less explored in the literature.","Directly applying prior image-based methods to input videos often results in temporal incoherence with low visual quality.","Existing research in this area is also limited by the lack of high-quality benchmark datasets.","To address these issues, we construct the first real-world paired video dataset, including both underexposure and overexposure dynamic scenes.","To achieve spatial alignment, we utilize two DSLR cameras and a beam splitter to simultaneously capture improper and normal exposure videos.","In addition, we propose a Video Exposure Correction Network (VECNet) based on Retinex theory, which incorporates a two-stream illumination learning mechanism to enhance the overexposure and underexposure factors, respectively.","The estimated multi-frame reflectance and dual-path illumination components are fused at both feature and image levels, leading to visually appealing results.","Experimental results demonstrate that the proposed method outperforms existing image exposure correction and underexposed video enhancement methods.","The code and dataset will be available soon."],"url":"http://arxiv.org/abs/2402.17296v1","category":"cs.CV"}
{"created":"2024-02-27 07:28:05","title":"Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition","abstract":"Emotion recognition in conversation (ERC) is a crucial task in natural language processing and affective computing. This paper proposes MultiDAG+CL, a novel approach for Multimodal Emotion Recognition in Conversation (ERC) that employs Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual features within a unified framework. The model is enhanced by Curriculum Learning (CL) to address challenges related to emotional shifts and data imbalance. Curriculum learning facilitates the learning process by gradually presenting training samples in a meaningful order, thereby improving the model's performance in handling emotional variations and data imbalance. Experimental results on the IEMOCAP and MELD datasets demonstrate that the MultiDAG+CL models outperform baseline models.","sentences":["Emotion recognition in conversation (ERC) is a crucial task in natural language processing and affective computing.","This paper proposes MultiDAG+CL, a novel approach for Multimodal Emotion Recognition in Conversation (ERC) that employs Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual features within a unified framework.","The model is enhanced by Curriculum Learning (CL) to address challenges related to emotional shifts and data imbalance.","Curriculum learning facilitates the learning process by gradually presenting training samples in a meaningful order, thereby improving the model's performance in handling emotional variations and data imbalance.","Experimental results on the IEMOCAP and MELD datasets demonstrate that the MultiDAG+CL models outperform baseline models."],"url":"http://arxiv.org/abs/2402.17269v1","category":"cs.LG"}
{"created":"2024-02-27 07:05:22","title":"EDTC: enhance depth of text comprehension in automated audio captioning","abstract":"Modality discrepancies have perpetually posed significant challenges within the realm of Automated Audio Captioning (AAC) and across all multi-modal domains. Facilitating models in comprehending text information plays a pivotal role in establishing a seamless connection between the two modalities of text and audio. While recent research has focused on closing the gap between these two modalities through contrastive learning, it is challenging to bridge the difference between both modalities using only simple contrastive loss. This paper introduces Enhance Depth of Text Comprehension (EDTC), which enhances the model's understanding of text information from three different perspectives. First, we propose a novel fusion module, FUSER, which aims to extract shared semantic information from different audio features through feature fusion. We then introduced TRANSLATOR, a novel alignment module designed to align audio features and text features along the tensor level. Finally, the weights are updated by adding momentum to the twin structure so that the model can learn information about both modalities at the same time. The resulting method achieves state-of-the-art performance on AudioCaps datasets and demonstrates results comparable to the state-of-the-art on Clotho datasets.","sentences":["Modality discrepancies have perpetually posed significant challenges within the realm of Automated Audio Captioning (AAC) and across all multi-modal domains.","Facilitating models in comprehending text information plays a pivotal role in establishing a seamless connection between the two modalities of text and audio.","While recent research has focused on closing the gap between these two modalities through contrastive learning, it is challenging to bridge the difference between both modalities using only simple contrastive loss.","This paper introduces Enhance Depth of Text Comprehension (EDTC), which enhances the model's understanding of text information from three different perspectives.","First, we propose a novel fusion module, FUSER, which aims to extract shared semantic information from different audio features through feature fusion.","We then introduced TRANSLATOR, a novel alignment module designed to align audio features and text features along the tensor level.","Finally, the weights are updated by adding momentum to the twin structure so that the model can learn information about both modalities at the same time.","The resulting method achieves state-of-the-art performance on AudioCaps datasets and demonstrates results comparable to the state-of-the-art on Clotho datasets."],"url":"http://arxiv.org/abs/2402.17259v1","category":"cs.SD"}
{"created":"2024-02-27 07:04:07","title":"Stochastic approximation in infinite dimensions","abstract":"Stochastic Approximation (SA) was introduced in the early 1950's and has been an active area of research for several decades. While the initial focus was on statistical questions, it was seen to have applications to signal processing, convex optimisation. %Over the last decade, there has been a revival of interest in SA as In later years SA has found application in Reinforced Learning (RL) and led to revival of interest.   While bulk of the literature is on SA for the case when the observations are from a finite dimensional Euclidian space, there has been interest in extending the same to infinite dimension. Extension to Hilbert spaces is relatively easier to do, but this is not so when we come to a Banach space - since in the case of a Banach space, even {\\em law of large numbers} is not true in general. We consider some cases where approximation works in a Banach space. Our framework includes case when the Banach space $\\Bb$ is $\\Cb([0,1],\\R^d)$, as well as $\\L^1([0,1],\\R^d)$, the two cases which do not even have the Radon-Nikodym property.","sentences":["Stochastic Approximation (SA) was introduced in the early 1950's and has been an active area of research for several decades.","While the initial focus was on statistical questions, it was seen to have applications to signal processing, convex optimisation.","%Over the last decade, there has been a revival of interest in SA as In later years SA has found application in Reinforced Learning (RL) and led to revival of interest.   ","While bulk of the literature is on SA for the case when the observations are from a finite dimensional Euclidian space, there has been interest in extending the same to infinite dimension.","Extension to Hilbert spaces is relatively easier to do, but this is not so when we come to a Banach space - since in the case of a Banach space, even {\\em law of large numbers} is not true in general.","We consider some cases where approximation works in a Banach space.","Our framework includes case when the Banach space $\\Bb$ is $\\Cb([0,1],\\R^d)$, as well as $\\L^1([0,1],\\R^d)$, the two cases which do not even have the Radon-Nikodym property."],"url":"http://arxiv.org/abs/2402.17258v1","category":"math.ST"}
{"created":"2024-02-27 06:50:31","title":"Context-based and Diversity-driven Specificity in Compositional Zero-Shot Learning","abstract":"Compositional Zero-Shot Learning (CZSL) aims to recognize unseen attribute-object pairs based on a limited set of observed examples. Current CZSL methodologies, despite their advancements, tend to neglect the distinct specificity levels present in attributes. For instance, given images of sliced strawberries, they may fail to prioritize `Sliced-Strawberry' over a generic `Red-Strawberry', despite the former being more informative. They also suffer from ballooning search space when shifting from Close-World (CW) to Open-World (OW) CZSL. To address the issues, we introduce the Context-based and Diversity-driven Specificity learning framework for CZSL (CDS-CZSL). Our framework evaluates the specificity of attributes by considering the diversity of objects they apply to and their related context. This novel approach allows for more accurate predictions by emphasizing specific attribute-object pairs and improves composition filtering in OW-CZSL. We conduct experiments in both CW and OW scenarios, and our model achieves state-of-the-art results across three datasets.","sentences":["Compositional Zero-Shot Learning (CZSL) aims to recognize unseen attribute-object pairs based on a limited set of observed examples.","Current CZSL methodologies, despite their advancements, tend to neglect the distinct specificity levels present in attributes.","For instance, given images of sliced strawberries, they may fail to prioritize `Sliced-Strawberry' over a generic `Red-Strawberry', despite the former being more informative.","They also suffer from ballooning search space when shifting from Close-World (CW) to Open-World (OW) CZSL.","To address the issues, we introduce the Context-based and Diversity-driven Specificity learning framework for CZSL (CDS-CZSL).","Our framework evaluates the specificity of attributes by considering the diversity of objects they apply to and their related context.","This novel approach allows for more accurate predictions by emphasizing specific attribute-object pairs and improves composition filtering in OW-CZSL.","We conduct experiments in both CW and OW scenarios, and our model achieves state-of-the-art results across three datasets."],"url":"http://arxiv.org/abs/2402.17251v1","category":"cs.CV"}
{"created":"2024-02-27 06:05:01","title":"Stochastic Gradient Succeeds for Bandits","abstract":"We show that the \\emph{stochastic gradient} bandit algorithm converges to a \\emph{globally optimal} policy at an $O(1/t)$ rate, even with a \\emph{constant} step size. Remarkably, global convergence of the stochastic gradient bandit algorithm has not been previously established, even though it is an old algorithm known to be applicable to bandits. The new result is achieved by establishing two novel technical findings: first, the noise of the stochastic updates in the gradient bandit algorithm satisfies a strong ``growth condition'' property, where the variance diminishes whenever progress becomes small, implying that additional noise control via diminishing step sizes is unnecessary; second, a form of ``weak exploration'' is automatically achieved through the stochastic gradient updates, since they prevent the action probabilities from decaying faster than $O(1/t)$, thus ensuring that every action is sampled infinitely often with probability $1$. These two findings can be used to show that the stochastic gradient update is already ``sufficient'' for bandits in the sense that exploration versus exploitation is automatically balanced in a manner that ensures almost sure convergence to a global optimum. These novel theoretical findings are further verified by experimental results.","sentences":["We show that the \\emph{stochastic gradient} bandit algorithm converges to a \\emph{globally optimal} policy at an $O(1/t)$ rate, even with a \\emph{constant} step size.","Remarkably, global convergence of the stochastic gradient bandit algorithm has not been previously established, even though it is an old algorithm known to be applicable to bandits.","The new result is achieved by establishing two novel technical findings: first, the noise of the stochastic updates in the gradient bandit algorithm satisfies a strong ``growth condition'' property, where the variance diminishes whenever progress becomes small, implying that additional noise control via diminishing step sizes is unnecessary; second, a form of ``weak exploration'' is automatically achieved through the stochastic gradient updates, since they prevent the action probabilities from decaying faster than $O(1/t)$, thus ensuring that every action is sampled infinitely often with probability $1$. These two findings can be used to show that the stochastic gradient update is already ``sufficient'' for bandits in the sense that exploration versus exploitation is automatically balanced in a manner that ensures almost sure convergence to a global optimum.","These novel theoretical findings are further verified by experimental results."],"url":"http://arxiv.org/abs/2402.17235v1","category":"cs.LG"}
{"created":"2024-02-27 05:48:18","title":"Chain-of-Thought Prompting of Large Language Models for Discovering and Fixing Software Vulnerabilities","abstract":"Security vulnerabilities are increasingly prevalent in modern software and they are widely consequential to our society. Various approaches to defending against these vulnerabilities have been proposed, among which those leveraging deep learning (DL) avoid major barriers with other techniques hence attracting more attention in recent years. However, DL-based approaches face critical challenges including the lack of sizable and quality-labeled task-specific datasets and their inability to generalize well to unseen, real-world scenarios. Lately, large language models (LLMs) have demonstrated impressive potential in various domains by overcoming those challenges, especially through chain-of-thought (CoT) prompting. In this paper, we explore how to leverage LLMs and CoT to address three key software vulnerability analysis tasks: identifying a given type of vulnerabilities, discovering vulnerabilities of any type, and patching detected vulnerabilities. We instantiate the general CoT methodology in the context of these tasks through VSP , our unified, vulnerability-semantics-guided prompting approach, and conduct extensive experiments assessing VSP versus five baselines for the three tasks against three LLMs and two datasets. Results show substantial superiority of our CoT-inspired prompting (553.3%, 36.5%, and 30.8% higher F1 accuracy for vulnerability identification, discovery, and patching, respectively, on CVE datasets) over the baselines. Through in-depth case studies analyzing VSP failures, we also reveal current gaps in LLM/CoT for challenging vulnerability cases, while proposing and validating respective improvements.","sentences":["Security vulnerabilities are increasingly prevalent in modern software and they are widely consequential to our society.","Various approaches to defending against these vulnerabilities have been proposed, among which those leveraging deep learning (DL) avoid major barriers with other techniques hence attracting more attention in recent years.","However, DL-based approaches face critical challenges including the lack of sizable and quality-labeled task-specific datasets and their inability to generalize well to unseen, real-world scenarios.","Lately, large language models (LLMs) have demonstrated impressive potential in various domains by overcoming those challenges, especially through chain-of-thought (CoT) prompting.","In this paper, we explore how to leverage LLMs and CoT to address three key software vulnerability analysis tasks: identifying a given type of vulnerabilities, discovering vulnerabilities of any type, and patching detected vulnerabilities.","We instantiate the general CoT methodology in the context of these tasks through VSP , our unified, vulnerability-semantics-guided prompting approach, and conduct extensive experiments assessing VSP versus five baselines for the three tasks against three LLMs and two datasets.","Results show substantial superiority of our CoT-inspired prompting (553.3%, 36.5%, and 30.8% higher F1 accuracy for vulnerability identification, discovery, and patching, respectively, on CVE datasets) over the baselines.","Through in-depth case studies analyzing VSP failures, we also reveal current gaps in LLM/CoT for challenging vulnerability cases, while proposing and validating respective improvements."],"url":"http://arxiv.org/abs/2402.17230v1","category":"cs.CR"}
{"created":"2024-02-27 05:47:33","title":"Preserving Fairness Generalization in Deepfake Detection","abstract":"Although effective deepfake detection models have been developed in recent years, recent studies have revealed that these models can result in unfair performance disparities among demographic groups, such as race and gender. This can lead to particular groups facing unfair targeting or exclusion from detection, potentially allowing misclassified deepfakes to manipulate public opinion and undermine trust in the model. The existing method for addressing this problem is providing a fair loss function. It shows good fairness performance for intra-domain evaluation but does not maintain fairness for cross-domain testing. This highlights the significance of fairness generalization in the fight against deepfakes. In this work, we propose the first method to address the fairness generalization problem in deepfake detection by simultaneously considering features, loss, and optimization aspects. Our method employs disentanglement learning to extract demographic and domain-agnostic forgery features, fusing them to encourage fair learning across a flattened loss landscape. Extensive experiments on prominent deepfake datasets demonstrate our method's effectiveness, surpassing state-of-the-art approaches in preserving fairness during cross-domain deepfake detection. The code is available at https://github.com/Purdue-M2/Fairness-Generalization","sentences":["Although effective deepfake detection models have been developed in recent years, recent studies have revealed that these models can result in unfair performance disparities among demographic groups, such as race and gender.","This can lead to particular groups facing unfair targeting or exclusion from detection, potentially allowing misclassified deepfakes to manipulate public opinion and undermine trust in the model.","The existing method for addressing this problem is providing a fair loss function.","It shows good fairness performance for intra-domain evaluation but does not maintain fairness for cross-domain testing.","This highlights the significance of fairness generalization in the fight against deepfakes.","In this work, we propose the first method to address the fairness generalization problem in deepfake detection by simultaneously considering features, loss, and optimization aspects.","Our method employs disentanglement learning to extract demographic and domain-agnostic forgery features, fusing them to encourage fair learning across a flattened loss landscape.","Extensive experiments on prominent deepfake datasets demonstrate our method's effectiveness, surpassing state-of-the-art approaches in preserving fairness during cross-domain deepfake detection.","The code is available at https://github.com/Purdue-M2/Fairness-Generalization"],"url":"http://arxiv.org/abs/2402.17229v1","category":"cs.CV"}
{"created":"2024-02-27 05:37:10","title":"Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models","abstract":"Large Language Models (LLMs) have achieved remarkable performance in objective tasks such as open-domain question answering and mathematical reasoning, which can often be solved through recalling learned factual knowledge or chain-of-thought style reasoning. However, we find that the performance of LLMs in subjective tasks is still unsatisfactory, such as metaphor recognition, dark humor detection, etc. Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted reasoning pathway. Based on the characteristics of the tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC (Reasoning in Conversation), a method that focuses on solving subjective tasks through dialogue simulation. The motivation of RiC is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential useful knowledge behind dialogues for giving the final answers. We evaluate both API-based and open-source LLMs including GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental results show that RiC can yield significant improvement compared with various baselines.","sentences":["Large Language Models (LLMs) have achieved remarkable performance in objective tasks such as open-domain question answering and mathematical reasoning, which can often be solved through recalling learned factual knowledge or chain-of-thought style reasoning.","However, we find that the performance of LLMs in subjective tasks is still unsatisfactory, such as metaphor recognition, dark humor detection, etc.","Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted reasoning pathway.","Based on the characteristics of the tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC (Reasoning in Conversation), a method that focuses on solving subjective tasks through dialogue simulation.","The motivation of RiC is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential useful knowledge behind dialogues for giving the final answers.","We evaluate both API-based and open-source LLMs including GPT-4, ChatGPT, and OpenChat across twelve tasks.","Experimental results show that RiC can yield significant improvement compared with various baselines."],"url":"http://arxiv.org/abs/2402.17226v1","category":"cs.CL"}
{"created":"2024-02-27 05:11:14","title":"Multidimensional unstructured sparse recovery via eigenmatrix","abstract":"This note considers the multidimensional unstructured sparse recovery problems. Examples include Fourier inversion and sparse deconvolution. The eigenmatrix is a data-driven construction with desired approximate eigenvalues and eigenvectors proposed for the one-dimensional problems. This note extends the eigenmatrix approach to multidimensional problems. Numerical results are provided to demonstrate the performance of the proposed method.","sentences":["This note considers the multidimensional unstructured sparse recovery problems.","Examples include Fourier inversion and sparse deconvolution.","The eigenmatrix is a data-driven construction with desired approximate eigenvalues and eigenvectors proposed for the one-dimensional problems.","This note extends the eigenmatrix approach to multidimensional problems.","Numerical results are provided to demonstrate the performance of the proposed method."],"url":"http://arxiv.org/abs/2402.17215v1","category":"math.NA"}
{"created":"2024-02-27 05:03:24","title":"Deep Learning-based Kinetic Analysis in Paper-based Analytical Cartridges Integrated with Field-effect Transistors","abstract":"This study explores the fusion of a field-effect transistor (FET), a paper-based analytical cartridge, and the computational power of deep learning (DL) for quantitative biosensing via kinetic analyses. The FET sensors address the low sensitivity challenge observed in paper analytical devices, enabling electrical measurements with kinetic data. The paper-based cartridge eliminates the need for surface chemistry required in FET sensors, ensuring economical operation (cost < $0.15/test). The DL analysis mitigates chronic challenges of FET biosensors such as sample matrix interference, by leveraging kinetic data from target-specific bioreactions. In our proof-of-concept demonstration, our DL-based analyses showcased a coefficient of variation of < 6.46% and a decent concentration measurement correlation with an r2 value of > 0.976 for cholesterol testing when blindly compared to results obtained from a CLIA-certified clinical laboratory. These integrated technologies can create a new generation of FET-based biosensors, potentially transforming point-of-care diagnostics and at-home testing through enhanced accessibility, ease-of-use, and accuracy.","sentences":["This study explores the fusion of a field-effect transistor (FET), a paper-based analytical cartridge, and the computational power of deep learning (DL) for quantitative biosensing via kinetic analyses.","The FET sensors address the low sensitivity challenge observed in paper analytical devices, enabling electrical measurements with kinetic data.","The paper-based cartridge eliminates the need for surface chemistry required in FET sensors, ensuring economical operation (cost < $0.15/test).","The DL analysis mitigates chronic challenges of FET biosensors such as sample matrix interference, by leveraging kinetic data from target-specific bioreactions.","In our proof-of-concept demonstration, our DL-based analyses showcased a coefficient of variation of < 6.46% and a decent concentration measurement correlation with an r2 value of > 0.976 for cholesterol testing when blindly compared to results obtained from a CLIA-certified clinical laboratory.","These integrated technologies can create a new generation of FET-based biosensors, potentially transforming point-of-care diagnostics and at-home testing through enhanced accessibility, ease-of-use, and accuracy."],"url":"http://arxiv.org/abs/2402.17209v1","category":"q-bio.QM"}
{"created":"2024-02-27 04:56:04","title":"Deployment Prior Injection for Run-time Calibratable Object Detection","abstract":"With a strong alignment between the training and test distributions, object relation as a context prior facilitates object detection. Yet, it turns into a harmful but inevitable training set bias upon test distributions that shift differently across space and time. Nevertheless, the existing detectors cannot incorporate deployment context prior during the test phase without parameter update. Such kind of capability requires the model to explicitly learn disentangled representations with respect to context prior. To achieve this, we introduce an additional graph input to the detector, where the graph represents the deployment context prior, and its edge values represent object relations. Then, the detector behavior is trained to bound to the graph with a modified training objective. As a result, during the test phase, any suitable deployment context prior can be injected into the detector via graph edits, hence calibrating, or \"re-biasing\" the detector towards the given prior at run-time without parameter update. Even if the deployment prior is unknown, the detector can self-calibrate using deployment prior approximated using its own predictions. Comprehensive experimental results on the COCO dataset, as well as cross-dataset testing on the Objects365 dataset, demonstrate the effectiveness of the run-time calibratable detector.","sentences":["With a strong alignment between the training and test distributions, object relation as a context prior facilitates object detection.","Yet, it turns into a harmful but inevitable training set bias upon test distributions that shift differently across space and time.","Nevertheless, the existing detectors cannot incorporate deployment context prior during the test phase without parameter update.","Such kind of capability requires the model to explicitly learn disentangled representations with respect to context prior.","To achieve this, we introduce an additional graph input to the detector, where the graph represents the deployment context prior, and its edge values represent object relations.","Then, the detector behavior is trained to bound to the graph with a modified training objective.","As a result, during the test phase, any suitable deployment context prior can be injected into the detector via graph edits, hence calibrating, or \"re-biasing\" the detector towards the given prior at run-time without parameter update.","Even if the deployment prior is unknown, the detector can self-calibrate using deployment prior approximated using its own predictions.","Comprehensive experimental results on the COCO dataset, as well as cross-dataset testing on the Objects365 dataset, demonstrate the effectiveness of the run-time calibratable detector."],"url":"http://arxiv.org/abs/2402.17207v1","category":"cs.CV"}
{"created":"2024-02-27 04:50:13","title":"FedBRB: An Effective Solution to the Small-to-Large Scenario in Device-Heterogeneity Federated Learning","abstract":"Recently, the success of large models has demonstrated the importance of scaling up model size. This has spurred interest in exploring collaborative training of large-scale models from federated learning perspective. Due to computational constraints, many institutions struggle to train a large-scale model locally. Thus, training a larger global model using only smaller local models has become an important scenario (i.e., the \\textbf{small-to-large scenario}). Although recent device-heterogeneity federated learning approaches have started to explore this area, they face limitations in fully covering the parameter space of the global model. In this paper, we propose a method called \\textbf{FedBRB} (\\underline{B}lock-wise \\underline{R}olling and weighted \\underline{B}roadcast) based on the block concept. FedBRB can uses small local models to train all blocks of the large global model, and broadcasts the trained parameters to the entire space for faster information interaction. Experiments demonstrate FedBRB yields substantial performance gains, achieving state-of-the-art results in this scenario. Moreover, FedBRB using only minimal local models can even surpass baselines using larger local models.","sentences":["Recently, the success of large models has demonstrated the importance of scaling up model size.","This has spurred interest in exploring collaborative training of large-scale models from federated learning perspective.","Due to computational constraints, many institutions struggle to train a large-scale model locally.","Thus, training a larger global model using only smaller local models has become an important scenario (i.e., the \\textbf{small-to-large scenario}).","Although recent device-heterogeneity federated learning approaches have started to explore this area, they face limitations in fully covering the parameter space of the global model.","In this paper, we propose a method called \\textbf{FedBRB} (\\underline{B}lock-wise \\underline{R}olling and weighted \\underline{B}roadcast) based on the block concept.","FedBRB can uses small local models to train all blocks of the large global model, and broadcasts the trained parameters to the entire space for faster information interaction.","Experiments demonstrate FedBRB yields substantial performance gains, achieving state-of-the-art results in this scenario.","Moreover, FedBRB using only minimal local models can even surpass baselines using larger local models."],"url":"http://arxiv.org/abs/2402.17202v1","category":"cs.LG"}
{"created":"2024-02-27 04:18:49","title":"When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method","abstract":"While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning -- full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent. We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods.","sentences":["While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited.","To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance.","We consider two types of finetuning -- full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size.","Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent.","We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods."],"url":"http://arxiv.org/abs/2402.17193v1","category":"cs.CL"}
{"created":"2024-02-27 03:33:23","title":"Dual-Space Optimization: Improved Molecule Sequence Design by Latent Prompt Transformer","abstract":"Designing molecules with desirable properties, such as drug-likeliness and high binding affinities towards protein targets, is a challenging problem. In this paper, we propose the Dual-Space Optimization (DSO) method that integrates latent space sampling and data space selection to solve this problem. DSO iteratively updates a latent space generative model and a synthetic dataset in an optimization process that gradually shifts the generative model and the synthetic data towards regions of desired property values. Our generative model takes the form of a Latent Prompt Transformer (LPT) where the latent vector serves as the prompt of a causal transformer. Our extensive experiments demonstrate effectiveness of the proposed method, which sets new performance benchmarks across single-objective, multi-objective and constrained molecule design tasks.","sentences":["Designing molecules with desirable properties, such as drug-likeliness and high binding affinities towards protein targets, is a challenging problem.","In this paper, we propose the Dual-Space Optimization (DSO) method that integrates latent space sampling and data space selection to solve this problem.","DSO iteratively updates a latent space generative model and a synthetic dataset in an optimization process that gradually shifts the generative model and the synthetic data towards regions of desired property values.","Our generative model takes the form of a Latent Prompt Transformer (LPT) where the latent vector serves as the prompt of a causal transformer.","Our extensive experiments demonstrate effectiveness of the proposed method, which sets new performance benchmarks across single-objective, multi-objective and constrained molecule design tasks."],"url":"http://arxiv.org/abs/2402.17179v1","category":"cs.LG"}
{"created":"2024-02-27 03:13:48","title":"Lane2Seq: Towards Unified Lane Detection via Sequence Generation","abstract":"In this paper, we present a novel sequence generation-based framework for lane detection, called Lane2Seq. It unifies various lane detection formats by casting lane detection as a sequence generation task. This is different from previous lane detection methods, which depend on well-designed task-specific head networks and corresponding loss functions. Lane2Seq only adopts a plain transformer-based encoder-decoder architecture with a simple cross-entropy loss. Additionally, we propose a new multi-format model tuning based on reinforcement learning to incorporate the task-specific knowledge into Lane2Seq. Experimental results demonstrate that such a simple sequence generation paradigm not only unifies lane detection but also achieves competitive performance on benchmarks. For example, Lane2Seq gets 97.95\\% and 97.42\\% F1 score on Tusimple and LLAMAS datasets, establishing a new state-of-the-art result for two benchmarks.","sentences":["In this paper, we present a novel sequence generation-based framework for lane detection, called Lane2Seq.","It unifies various lane detection formats by casting lane detection as a sequence generation task.","This is different from previous lane detection methods, which depend on well-designed task-specific head networks and corresponding loss functions.","Lane2Seq only adopts a plain transformer-based encoder-decoder architecture with a simple cross-entropy loss.","Additionally, we propose a new multi-format model tuning based on reinforcement learning to incorporate the task-specific knowledge into Lane2Seq.","Experimental results demonstrate that such a simple sequence generation paradigm not only unifies lane detection but also achieves competitive performance on benchmarks.","For example, Lane2Seq gets 97.95\\% and 97.42\\% F1 score on Tusimple and LLAMAS datasets, establishing a new state-of-the-art result for two benchmarks."],"url":"http://arxiv.org/abs/2402.17172v1","category":"cs.CV"}
{"created":"2024-02-27 02:29:24","title":"Time series generation for option pricing on quantum computers using tensor network","abstract":"Finance, especially option pricing, is a promising industrial field that might benefit from quantum computing. While quantum algorithms for option pricing have been proposed, it is desired to devise more efficient implementations of costly operations in the algorithms, one of which is preparing a quantum state that encodes a probability distribution of the underlying asset price. In particular, in pricing a path-dependent option, we need to generate a state encoding a joint distribution of the underlying asset price at multiple time points, which is more demanding. To address these issues, we propose a novel approach using Matrix Product State (MPS) as a generative model for time series generation. To validate our approach, taking the Heston model as a target, we conduct numerical experiments to generate time series in the model. Our findings demonstrate the capability of the MPS model to generate paths in the Heston model, highlighting its potential for path-dependent option pricing on quantum computers.","sentences":["Finance, especially option pricing, is a promising industrial field that might benefit from quantum computing.","While quantum algorithms for option pricing have been proposed, it is desired to devise more efficient implementations of costly operations in the algorithms, one of which is preparing a quantum state that encodes a probability distribution of the underlying asset price.","In particular, in pricing a path-dependent option, we need to generate a state encoding a joint distribution of the underlying asset price at multiple time points, which is more demanding.","To address these issues, we propose a novel approach using Matrix Product State (MPS) as a generative model for time series generation.","To validate our approach, taking the Heston model as a target, we conduct numerical experiments to generate time series in the model.","Our findings demonstrate the capability of the MPS model to generate paths in the Heston model, highlighting its potential for path-dependent option pricing on quantum computers."],"url":"http://arxiv.org/abs/2402.17148v1","category":"quant-ph"}
{"created":"2024-02-27 01:55:41","title":"A Bayesian Committee Machine Potential for Organic Nitrogen Compounds","abstract":"Large-scale computer simulations of chemical atoms are used in a wide range of applications, including batteries, drugs, and more. However, there is a problem with efficiency as it takes a long time due to the large amount of calculation. To solve these problems, machine learning interatomic potential (ML-IAP) technology is attracting attention as an alternative. ML-IAP not only has high accuracy by faithfully expressing the density functional theory (DFT), but also has the advantage of low computational cost. However, there is a problem that the potential energy changes significantly depending on the environment of each atom, and expansion to a wide range of compounds within a single model is still difficult to build in the case of a kernel-based model. To solve this problem, we would like to develop a universal ML-IAP using this active Bayesian Committee Machine (BCM) potential methodology for carbon-nitrogen-hydrogen (CNH) with various compositions. ML models are trained and generated through first-principles calculations and molecular dynamics simulations for molecules with only CNH. Using long amine structures to test an ML model trained only with short chains, the results show excellent consistency with DFT calculations. Consequently, machine learning-based models for organic molecules not only demonstrate the ability to accurately describe various physical properties but also hold promise for investigating a broad spectrum of diverse materials systems.","sentences":["Large-scale computer simulations of chemical atoms are used in a wide range of applications, including batteries, drugs, and more.","However, there is a problem with efficiency as it takes a long time due to the large amount of calculation.","To solve these problems, machine learning interatomic potential (ML-IAP) technology is attracting attention as an alternative.","ML-IAP not only has high accuracy by faithfully expressing the density functional theory (DFT), but also has the advantage of low computational cost.","However, there is a problem that the potential energy changes significantly depending on the environment of each atom, and expansion to a wide range of compounds within a single model is still difficult to build in the case of a kernel-based model.","To solve this problem, we would like to develop a universal ML-IAP using this active Bayesian Committee Machine (BCM) potential methodology for carbon-nitrogen-hydrogen (CNH) with various compositions.","ML models are trained and generated through first-principles calculations and molecular dynamics simulations for molecules with only CNH.","Using long amine structures to test an ML model trained only with short chains, the results show excellent consistency with DFT calculations.","Consequently, machine learning-based models for organic molecules not only demonstrate the ability to accurately describe various physical properties but also hold promise for investigating a broad spectrum of diverse materials systems."],"url":"http://arxiv.org/abs/2402.17132v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-27 18:12:22","title":"Automated Scheduling of Doppler Exoplanet Observations at Keck Observatory","abstract":"Precise Doppler studies of extrasolar planets require fine-grained control of observational cadence, i.e. the timing of and spacing between observations. We present a novel framework for scheduling a set of Doppler campaigns with different cadence requirements at the W. M. Keck Observatory (WMKO). For a set of observing programs and allocated nights on an instrument, our software optimizes the timing and ordering of ~1000 observations within a given observing semester. We achieve a near-optimal solution in real-time using a hierarchical Integer Linear Programming (ILP) framework. Our scheduling formulation optimizes over the roughly 10^3000 possible orderings. A top level optimization finds the most regular sequence of allocated nights by which to observe each host star in the request catalog based on a frequency specified in the request. A second optimization scheme minimizes the slews and downtime of the instrument. We have assessed our algorithms performance with simulated data and with the real suite of Doppler observations of the California Planet Search in 2023.","sentences":["Precise Doppler studies of extrasolar planets require fine-grained control of observational cadence, i.e. the timing of and spacing between observations.","We present a novel framework for scheduling a set of Doppler campaigns with different cadence requirements at the W. M. Keck Observatory (WMKO).","For a set of observing programs and allocated nights on an instrument, our software optimizes the timing and ordering of ~1000 observations within a given observing semester.","We achieve a near-optimal solution in real-time using a hierarchical Integer Linear Programming (ILP) framework.","Our scheduling formulation optimizes over the roughly 10^3000 possible orderings.","A top level optimization finds the most regular sequence of allocated nights by which to observe each host star in the request catalog based on a frequency specified in the request.","A second optimization scheme minimizes the slews and downtime of the instrument.","We have assessed our algorithms performance with simulated data and with the real suite of Doppler observations of the California Planet Search in 2023."],"url":"http://arxiv.org/abs/2402.17734v1","category":"astro-ph.IM"}
{"created":"2024-02-27 17:57:33","title":"Elliptic Reconstruction and A Posteriori Error Estimates for Parabolic Variational Inequalities","abstract":"Elliptic reconstruction property, originally introduced by Makridakis and Nochetto for linear parabolic problems, is a well-known tool to derive optimal a posteriori error estimates. No such results are known for nonlinear and nonsmooth problems such as parabolic variational inequalities (VIs). This article establishes the elliptic reconstruction property for parabolic VIs and derives a posteriori error estimates in $L^{\\infty}(0,T;L^{2}(\\Omega))$ and $L^{\\infty}(0,T;L^{\\infty}(\\Omega))$, respectively. As an application, the residual-type error estimates are presented.","sentences":["Elliptic reconstruction property, originally introduced by Makridakis and Nochetto for linear parabolic problems, is a well-known tool to derive optimal a posteriori error estimates.","No such results are known for nonlinear and nonsmooth problems such as parabolic variational inequalities (VIs).","This article establishes the elliptic reconstruction property for parabolic VIs and derives a posteriori error estimates in $L^{\\infty}(0,T;L^{2}(\\Omega))$ and $L^{\\infty}(0,T;L^{\\infty}(\\Omega))$, respectively.","As an application, the residual-type error estimates are presented."],"url":"http://arxiv.org/abs/2402.17724v1","category":"math.NA"}
{"created":"2024-02-27 15:57:11","title":"Fine-Grained Natural Language Inference Based Faithfulness Evaluation for Diverse Summarisation Tasks","abstract":"We study existing approaches to leverage off-the-shelf Natural Language Inference (NLI) models for the evaluation of summary faithfulness and argue that these are sub-optimal due to the granularity level considered for premises and hypotheses. That is, the smaller content unit considered as hypothesis is a sentence and premises are made up of a fixed number of document sentences. We propose a novel approach, namely InFusE, that uses a variable premise size and simplifies summary sentences into shorter hypotheses. Departing from previous studies which focus on single short document summarisation, we analyse NLI based faithfulness evaluation for diverse summarisation tasks. We introduce DiverSumm, a new benchmark comprising long form summarisation (long documents and summaries) and diverse summarisation tasks (e.g., meeting and multi-document summarisation). In experiments, InFusE obtains superior performance across the different summarisation tasks. Our code and data are available at https://github.com/HJZnlp/infuse.","sentences":["We study existing approaches to leverage off-the-shelf Natural Language Inference (NLI) models for the evaluation of summary faithfulness and argue that these are sub-optimal due to the granularity level considered for premises and hypotheses.","That is, the smaller content unit considered as hypothesis is a sentence and premises are made up of a fixed number of document sentences.","We propose a novel approach, namely InFusE, that uses a variable premise size and simplifies summary sentences into shorter hypotheses.","Departing from previous studies which focus on single short document summarisation, we analyse NLI based faithfulness evaluation for diverse summarisation tasks.","We introduce DiverSumm, a new benchmark comprising long form summarisation (long documents and summaries) and diverse summarisation tasks (e.g., meeting and multi-document summarisation).","In experiments, InFusE obtains superior performance across the different summarisation tasks.","Our code and data are available at https://github.com/HJZnlp/infuse."],"url":"http://arxiv.org/abs/2402.17630v1","category":"cs.CL"}
{"created":"2024-02-27 15:13:05","title":"A highly efficient computational approach for part-scale microstructure predictions in Ti-6Al-4V additive manufacturing","abstract":"Fast and efficient simulations of metal additive manufacturing (AM) processes are highly relevant to exploring the full potential of this promising manufacturing technique. The microstructure composition plays an important role in characterizing the part quality and deriving mechanical properties. When complete parts are simulated, one often needs to resort to strong simplifications such as layer-wise heating due to the large number of simulated time steps compared to the small time step sizes. This article proposes a scan-resolved approach to the coupled thermo-microstructural problem. Building on a highly efficient thermal model, we discuss the implementation of a phenomenological microstructure model for the evolution of the three main constituents of Ti-6Al-4V: stable $\\alpha_s$-phase, martensite $\\alpha_m$-phase and $\\beta$-phase. The implementation is tailored to modern hardware features using vectorization and fast approximations of transcendental functions. A performance model and numerical examples verify the high degree of optimization. We demonstrate the applicability and predictive power of the approach and the influence of scan strategy and geometry. Depending on the specific example, results can be obtained with moderate computational resources in a few hours to days. The numerical examples include a prediction of the microstructure on the full NIST AM Benchmark cantilever specimen.","sentences":["Fast and efficient simulations of metal additive manufacturing (AM) processes are highly relevant to exploring the full potential of this promising manufacturing technique.","The microstructure composition plays an important role in characterizing the part quality and deriving mechanical properties.","When complete parts are simulated, one often needs to resort to strong simplifications such as layer-wise heating due to the large number of simulated time steps compared to the small time step sizes.","This article proposes a scan-resolved approach to the coupled thermo-microstructural problem.","Building on a highly efficient thermal model, we discuss the implementation of a phenomenological microstructure model for the evolution of the three main constituents of Ti-6Al-4V: stable $\\alpha_s$-phase, martensite $\\alpha_m$-phase and $\\beta$-phase.","The implementation is tailored to modern hardware features using vectorization and fast approximations of transcendental functions.","A performance model and numerical examples verify the high degree of optimization.","We demonstrate the applicability and predictive power of the approach and the influence of scan strategy and geometry.","Depending on the specific example, results can be obtained with moderate computational resources in a few hours to days.","The numerical examples include a prediction of the microstructure on the full NIST AM Benchmark cantilever specimen."],"url":"http://arxiv.org/abs/2402.17580v1","category":"cs.CE"}
{"created":"2024-02-27 15:09:20","title":"HBF MU-MIMO with Interference-Aware Beam Pair Link Allocation for Beyond-5G mm-Wave Networks","abstract":"Hybrid beamforming (HBF) multi-user multiple-input multiple-output (MU-MIMO) is a key technology for unlocking the directional millimeter-wave (mm-wave) nature for spatial multiplexing beyond current codebook-based 5G-NR networks. In order to suppress co-scheduled users' interference, HBF MU-MIMO is predicated on having sufficient radio frequency chains and accurate channel state information (CSI), which can otherwise lead to performance losses due to imperfect interference cancellation. In this work, we propose IABA, a 5G-NR standard-compliant beam pair link (BPL) allocation scheme for mitigating spatial interference in practical HBF MU-MIMO networks. IABA solves the network sum throughput optimization via either a distributed or a centralized BPL allocation using dedicated CSI reference signals for candidate BPL monitoring. We present a comprehensive study of practical multi-cell mm-wave networks and demonstrate that HBF MU-MIMO without interference-aware BPL allocation experiences strong residual interference which limits the achievable network performance. Our results show that IABA offers significant performance gains over the default interference-agnostic 5G-NR BPL allocation, and even allows HBF MU-MIMO to outperform the fully digital MU-MIMO baseline, by facilitating allocation of secondary BPLs other than the strongest BPL found during initial access. We further demonstrate the scalability of IABA with increased gNB antennas and densification for beyond-5G mm-wave networks.","sentences":["Hybrid beamforming (HBF) multi-user multiple-input multiple-output (MU-MIMO) is a key technology for unlocking the directional millimeter-wave (mm-wave) nature for spatial multiplexing beyond current codebook-based 5G-NR networks.","In order to suppress co-scheduled users' interference, HBF MU-MIMO is predicated on having sufficient radio frequency chains and accurate channel state information (CSI), which can otherwise lead to performance losses due to imperfect interference cancellation.","In this work, we propose IABA, a 5G-NR standard-compliant beam pair link (BPL) allocation scheme for mitigating spatial interference in practical HBF MU-MIMO networks.","IABA solves the network sum throughput optimization via either a distributed or a centralized BPL allocation using dedicated CSI reference signals for candidate BPL monitoring.","We present a comprehensive study of practical multi-cell mm-wave networks and demonstrate that HBF MU-MIMO without interference-aware BPL allocation experiences strong residual interference which limits the achievable network performance.","Our results show that IABA offers significant performance gains over the default interference-agnostic 5G-NR BPL allocation, and even allows HBF MU-MIMO to outperform the fully digital MU-MIMO baseline, by facilitating allocation of secondary BPLs other than the strongest BPL found during initial access.","We further demonstrate the scalability of IABA with increased gNB antennas and densification for beyond-5G mm-wave networks."],"url":"http://arxiv.org/abs/2402.17573v1","category":"cs.NI"}
{"created":"2024-02-27 12:55:36","title":"A Holistic Approach for Bitcoin Confirmation Times & Optimal Fee Selection","abstract":"Bitcoin is currently subject to a significant pay-for-speed trade-off. This is caused by lengthy and highly variable transaction confirmation times, especially during times of congestion. Users can reduce their transaction confirmation times by increasing their transaction fee. In this paper, based on the inner workings of Bitcoin, we propose a model-based approach (based on the Cram\\'er-Lundberg model) that can be used to determine the optimal fee, via, for example, the mean or quantiles, and models accurately the confirmation time distribution for a given fee. The proposed model is highly suitable as it arises as the limiting model for the mempool process (that tracks the unconfirmed transactions), which we rigorously show via a fluid limit and we extend this to the diffusion limit (an approximation of the Cram\\'er-Lundberg model for fast computations in highly congested instances). We also propose methods (incorporating the real-time data) to estimate the model parameters, thereby combining model and data-driven approaches. The model-based approach is validated on real-world data and the resulting transaction fees outperform, in most instances, the data-driven ones.","sentences":["Bitcoin is currently subject to a significant pay-for-speed trade-off.","This is caused by lengthy and highly variable transaction confirmation times, especially during times of congestion.","Users can reduce their transaction confirmation times by increasing their transaction fee.","In this paper, based on the inner workings of Bitcoin, we propose a model-based approach (based on the Cram\\'er-Lundberg model) that can be used to determine the optimal fee, via, for example, the mean or quantiles, and models accurately the confirmation time distribution for a given fee.","The proposed model is highly suitable as it arises as the limiting model for the mempool process (that tracks the unconfirmed transactions), which we rigorously show via a fluid limit and we extend this to the diffusion limit (an approximation of the Cram\\'er-Lundberg model for fast computations in highly congested instances).","We also propose methods (incorporating the real-time data) to estimate the model parameters, thereby combining model and data-driven approaches.","The model-based approach is validated on real-world data and the resulting transaction fees outperform, in most instances, the data-driven ones."],"url":"http://arxiv.org/abs/2402.17474v1","category":"math.PR"}
{"created":"2024-02-27 11:40:50","title":"VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction","abstract":"Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed. While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations. To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion. These cells are merged into a complete scene after parallel optimization. We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images. Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering.","sentences":["Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed.","While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations.","To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting.","We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion.","These cells are merged into a complete scene after parallel optimization.","We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images.","Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering."],"url":"http://arxiv.org/abs/2402.17427v1","category":"cs.CV"}
{"created":"2024-02-27 10:27:01","title":"RACP: Risk-Aware Contingency Planning with Multi-Modal Predictions","abstract":"For an autonomous vehicle to operate reliably within real-world traffic scenarios, it is imperative to assess the repercussions of its prospective actions by anticipating the uncertain intentions exhibited by other participants in the traffic environment. Driven by the pronounced multi-modal nature of human driving behavior, this paper presents an approach that leverages Bayesian beliefs over the distribution of potential policies of other road users to construct a novel risk-aware probabilistic motion planning framework. In particular, we propose a novel contingency planner that outputs long-term contingent plans conditioned on multiple possible intents for other actors in the traffic scene. The Bayesian belief is incorporated into the optimization cost function to influence the behavior of the short-term plan based on the likelihood of other agents' policies. Furthermore, a probabilistic risk metric is employed to fine-tune the balance between efficiency and robustness. Through a series of closed-loop safety-critical simulated traffic scenarios shared with human-driven vehicles, we demonstrate the practical efficacy of our proposed approach that can handle multi-vehicle scenarios.","sentences":["For an autonomous vehicle to operate reliably within real-world traffic scenarios, it is imperative to assess the repercussions of its prospective actions by anticipating the uncertain intentions exhibited by other participants in the traffic environment.","Driven by the pronounced multi-modal nature of human driving behavior, this paper presents an approach that leverages Bayesian beliefs over the distribution of potential policies of other road users to construct a novel risk-aware probabilistic motion planning framework.","In particular, we propose a novel contingency planner that outputs long-term contingent plans conditioned on multiple possible intents for other actors in the traffic scene.","The Bayesian belief is incorporated into the optimization cost function to influence the behavior of the short-term plan based on the likelihood of other agents' policies.","Furthermore, a probabilistic risk metric is employed to fine-tune the balance between efficiency and robustness.","Through a series of closed-loop safety-critical simulated traffic scenarios shared with human-driven vehicles, we demonstrate the practical efficacy of our proposed approach that can handle multi-vehicle scenarios."],"url":"http://arxiv.org/abs/2402.17387v1","category":"cs.RO"}
{"created":"2024-02-27 10:02:12","title":"Superconducting Electronic Device Response Linearity Calculation Method Using Discrete Fourier Analysis","abstract":"A method for estimating the linearity of superconducting electronic devices is discussed. The method allows the obtaining of expected spurious-free dynamic range values based on the results of preliminary numerical modeling and solving optimization problems. A description of the method and an analysis of measurement errors are provided.","sentences":["A method for estimating the linearity of superconducting electronic devices is discussed.","The method allows the obtaining of expected spurious-free dynamic range values based on the results of preliminary numerical modeling and solving optimization problems.","A description of the method and an analysis of measurement errors are provided."],"url":"http://arxiv.org/abs/2402.17367v1","category":"cond-mat.supr-con"}
{"created":"2024-02-27 08:10:18","title":"Dielectric Loss due to Charged-Defect Acoustic Phonon Emission","abstract":"The coherence times of state-of-the-art superconducting qubits are limited by bulk dielectric loss, yet the microscopic mechanism leading to this loss is unclear. Here we propose that the experimentally observed loss can be attributed to the presence of charged defects that enable the absorption of electromagnetic radiation by the emission of acoustic phonons. Our explicit derivation of the absorption coefficient for this mechanism allows us to derive a loss tangent of $7.2 \\times 10^{-9}$ for Al$_2$O$_3$, in good agreement with recent high-precision measurements [A. P. Read et al., Phys. Rev. Appl. 19, 034064 (2023)]. We also find that for temperatures well below ~0.2 K, the loss should be independent of temperature, also in agreement with observations. Our investigations show that the loss per defect depends mainly on properties of the host material, and a high-throughput search suggests that diamond, cubic BN, AlN, and SiC are optimal in this respect.","sentences":["The coherence times of state-of-the-art superconducting qubits are limited by bulk dielectric loss, yet the microscopic mechanism leading to this loss is unclear.","Here we propose that the experimentally observed loss can be attributed to the presence of charged defects that enable the absorption of electromagnetic radiation by the emission of acoustic phonons.","Our explicit derivation of the absorption coefficient for this mechanism allows us to derive a loss tangent of $7.2 \\times 10^{-9}$ for Al$_2$O$_3$, in good agreement with recent high-precision measurements [A. P. Read et al., Phys.","Rev. Appl.","19, 034064 (2023)].","We also find that for temperatures well below ~0.2 K, the loss should be independent of temperature, also in agreement with observations.","Our investigations show that the loss per defect depends mainly on properties of the host material, and a high-throughput search suggests that diamond, cubic BN, AlN, and SiC are optimal in this respect."],"url":"http://arxiv.org/abs/2402.17291v1","category":"quant-ph"}
{"created":"2024-02-27 04:57:23","title":"Solving Time-Continuous Stochastic Optimal Control Problems: Algorithm Design and Convergence Analysis of Actor-Critic Flow","abstract":"We propose an actor-critic framework to solve the time-continuous stochastic optimal control problem. A least square temporal difference method is applied to compute the value function for the critic. The policy gradient method is implemented as policy improvement for the actor. Our key contribution lies in establishing the global convergence property of our proposed actor-critic flow, demonstrating a linear rate of convergence. Theoretical findings are further validated through numerical examples, showing the efficacy of our approach in practical applications.","sentences":["We propose an actor-critic framework to solve the time-continuous stochastic optimal control problem.","A least square temporal difference method is applied to compute the value function for the critic.","The policy gradient method is implemented as policy improvement for the actor.","Our key contribution lies in establishing the global convergence property of our proposed actor-critic flow, demonstrating a linear rate of convergence.","Theoretical findings are further validated through numerical examples, showing the efficacy of our approach in practical applications."],"url":"http://arxiv.org/abs/2402.17208v1","category":"math.OC"}
{"created":"2024-02-27 03:24:17","title":"Assessment of Precision and Accuracy of Brain White Matter Microstructure using Combined Diffusion MRI and Relaxometry","abstract":"Joint modeling of diffusion and relaxation has seen growing interest due to its potential to provide complementary information about tissue microstructure. For brain white matter, we designed an optimal diffusion-relaxometry MRI protocol that samples multiple b-values, B-tensor shapes, and echo times (TE). This variable-TE protocol (27 min) has as subsets a fixed-TE protocol (15 min) and a 2-shell dMRI protocol (7 min), both characterizing diffusion only. We assessed the sensitivity, specificity and reproducibility of these protocols with synthetic experiments and in six healthy volunteers. Compared with the fixed-TE protocol, the variable-TE protocol enables estimation of free water fractions while also capturing compartmental $T_2$ relaxation times. Jointly measuring diffusion and relaxation offers increased sensitivity and specificity to microstructure parameters in brain white matter with voxelwise coefficients of variation below 10%.","sentences":["Joint modeling of diffusion and relaxation has seen growing interest due to its potential to provide complementary information about tissue microstructure.","For brain white matter, we designed an optimal diffusion-relaxometry MRI protocol that samples multiple b-values, B-tensor shapes, and echo times (TE).","This variable-TE protocol (27 min) has as subsets a fixed-TE protocol (15 min) and a 2-shell dMRI protocol (7 min), both characterizing diffusion only.","We assessed the sensitivity, specificity and reproducibility of these protocols with synthetic experiments and in six healthy volunteers.","Compared with the fixed-TE protocol, the variable-TE protocol enables estimation of free water fractions while also capturing compartmental $T_2$ relaxation times.","Jointly measuring diffusion and relaxation offers increased sensitivity and specificity to microstructure parameters in brain white matter with voxelwise coefficients of variation below 10%."],"url":"http://arxiv.org/abs/2402.17175v1","category":"physics.med-ph"}
{"created":"2024-02-27 03:21:33","title":"Weighted EF1 and PO Allocations with Few Types of Agents or Chores","abstract":"We investigate the existence of fair and efficient allocations of indivisible chores to asymmetric agents who have unequal entitlements or weights. We consider the fairness notion of weighted envy-freeness up to one chore (wEF1) and the efficiency notion of Pareto-optimality (PO). The existence of EF1 and PO allocations of chores to symmetric agents is a major open problem in discrete fair division, and positive results are known only for certain structured instances. In this paper, we study this problem for a more general setting of asymmetric agents and show that an allocation that is wEF1 and PO exists and can be computed in polynomial time for instances with:   - Three types of agents, where agents with the same type have identical preferences but can have different weights.   - Two types of chores, where the chores can be partitioned into two sets, each containing copies of the same chore. For symmetric agents, our results establish that EF1 and PO allocations exist for three types of agents and also generalize known results for three agents, two types of agents, and two types of chores.   Our algorithms use a weighted picking sequence algorithm as a subroutine; we expect this idea and our analysis to be of independent interest.","sentences":["We investigate the existence of fair and efficient allocations of indivisible chores to asymmetric agents who have unequal entitlements or weights.","We consider the fairness notion of weighted envy-freeness up to one chore (wEF1) and the efficiency notion of Pareto-optimality (PO).","The existence of EF1 and PO allocations of chores to symmetric agents is a major open problem in discrete fair division, and positive results are known only for certain structured instances.","In this paper, we study this problem for a more general setting of asymmetric agents and show that an allocation that is wEF1 and PO exists and can be computed in polynomial time for instances with:   - Three types of agents, where agents with the same type have identical preferences but can have different weights.   ","- Two types of chores, where the chores can be partitioned into two sets, each containing copies of the same chore.","For symmetric agents, our results establish that EF1 and PO allocations exist for three types of agents and also generalize known results for three agents, two types of agents, and two types of chores.   ","Our algorithms use a weighted picking sequence algorithm as a subroutine; we expect this idea and our analysis to be of independent interest."],"url":"http://arxiv.org/abs/2402.17173v1","category":"cs.GT"}
{"created":"2024-02-27 02:39:45","title":"Existence for the Supercooled Stefan Problem in General Dimensions","abstract":"In this paper we prove global-time existence of weak solutions to the supercooled Stefan problem in general space dimensions, starting from a general class of initial domain and data. In addition, our solution is maximal, in the sense that its support is maximal among all comparable weak solutions starting from the same initial data. Our approach is based on a free target optimization problem for Brownian stopping times. While this approach is taken previously in [22], the novel feature of our optimization problem here is that it creates particle dynamics with a prescribed initial domain of general geometry. To achieve this, our main idea is to introduce a superharmonic cost function in the optimization problem, which encourages the target measure to accumulate near the prescribed domain boundary as much as possible. A central ingredient in our proof lies in the usage of dual problem: we prove the dual attainment and use the dual optimal solution to characterize the primal optimal solution. It follows in turn that the underlying particle dynamics yields a solution to the supercooled Stefan problem.","sentences":["In this paper we prove global-time existence of weak solutions to the supercooled Stefan problem in general space dimensions, starting from a general class of initial domain and data.","In addition, our solution is maximal, in the sense that its support is maximal among all comparable weak solutions starting from the same initial data.","Our approach is based on a free target optimization problem for Brownian stopping times.","While this approach is taken previously in [22], the novel feature of our optimization problem here is that it creates particle dynamics with a prescribed initial domain of general geometry.","To achieve this, our main idea is to introduce a superharmonic cost function in the optimization problem, which encourages the target measure to accumulate near the prescribed domain boundary as much as possible.","A central ingredient in our proof lies in the usage of dual problem: we prove the dual attainment and use the dual optimal solution to characterize the primal optimal solution.","It follows in turn that the underlying particle dynamics yields a solution to the supercooled Stefan problem."],"url":"http://arxiv.org/abs/2402.17154v1","category":"math.AP"}
{"created":"2024-02-27 02:10:00","title":"Distributions of Posterior Quantiles via Matching","abstract":"We offer a simple analysis of the problem of choosing a statistical experiment to optimize the induced distribution of posterior medians, or more generally $q$-quantiles for any $q \\in (0,1)$. We show that all implementable distributions of the posterior $q$-quantile are implemented by a single experiment, the $q$-quantile matching experiment, which pools pairs of states across the $q$-quantile of the prior in a positively assortative manner, with weight $q$ on the lower state in each pair. A dense subset of implementable distributions of posterior $q$-quantiles can be uniquely implemented by perturbing the $q$-quantile matching experiment. A linear functional is optimized over distributions of posterior $q$-quantiles by taking the optimal selection from each set of $q$-quantiles induced by the $q$-quantile matching experiment. The $q$-quantile matching experiment is the only experiment that simultaneously implements all implementable distributions of the posterior $q$-quantile.","sentences":["We offer a simple analysis of the problem of choosing a statistical experiment to optimize the induced distribution of posterior medians, or more generally $q$-quantiles for any $q \\in (0,1)$. We show that all implementable distributions of the posterior $q$-quantile are implemented by a single experiment, the $q$-quantile matching experiment, which pools pairs of states across the $q$-quantile of the prior in a positively assortative manner, with weight $q$ on the lower state in each pair.","A dense subset of implementable distributions of posterior $q$-quantiles can be uniquely implemented by perturbing the $q$-quantile matching experiment.","A linear functional is optimized over distributions of posterior $q$-quantiles by taking the optimal selection from each set of $q$-quantiles induced by the $q$-quantile matching experiment.","The $q$-quantile matching experiment is the only experiment that simultaneously implements all implementable distributions of the posterior $q$-quantile."],"url":"http://arxiv.org/abs/2402.17142v1","category":"econ.TH"}
{"created":"2024-02-27 02:06:14","title":"Direct Detection of Dark Photon Dark Matter with the James Webb Space Telescope","abstract":"In this study, we propose an investigation into dark photon dark matter (DPDM) within the infrared frequency band, utilizing highly sensitive infrared light detectors commonly integrated into space telescopes, such as the James Webb Space Telescope (JWST). The presence of DPDM induces electron oscillations in the reflector of these detectors. Consequently, these oscillating electrons can emit monochromatic electromagnetic waves with a frequency almost equivalent to the mass of DPDM. By employing the stationary phase approximation, we can demonstrate that when the size of the reflector significantly exceeds the wavelength of the electromagnetic wave, the contribution to the electromagnetic wave field at a given position primarily stems from the surface unit perpendicular to the relative position vector. This simplification results in the reduction of electromagnetic wave calculations to ray optics. By applying this concept to JWST, our analysis of observational data demonstrates the potential to establish constraints on the kinetic mixing between the photon and dark photon within the range [10, 500] THz. Despite JWST not being optimized for DPDM searches, our findings reveal constraints comparable to those obtained from the XENON1T experiment in the laboratory, as well as astrophysical constraints from solar emission. Additionally, we explore strategies to optimize future experiments specifically designed for DPDM searches.","sentences":["In this study, we propose an investigation into dark photon dark matter (DPDM) within the infrared frequency band, utilizing highly sensitive infrared light detectors commonly integrated into space telescopes, such as the James Webb Space Telescope (JWST).","The presence of DPDM induces electron oscillations in the reflector of these detectors.","Consequently, these oscillating electrons can emit monochromatic electromagnetic waves with a frequency almost equivalent to the mass of DPDM.","By employing the stationary phase approximation, we can demonstrate that when the size of the reflector significantly exceeds the wavelength of the electromagnetic wave, the contribution to the electromagnetic wave field at a given position primarily stems from the surface unit perpendicular to the relative position vector.","This simplification results in the reduction of electromagnetic wave calculations to ray optics.","By applying this concept to JWST, our analysis of observational data demonstrates the potential to establish constraints on the kinetic mixing between the photon and dark photon within the range [10, 500] THz.","Despite JWST not being optimized for DPDM searches, our findings reveal constraints comparable to those obtained from the XENON1T experiment in the laboratory, as well as astrophysical constraints from solar emission.","Additionally, we explore strategies to optimize future experiments specifically designed for DPDM searches."],"url":"http://arxiv.org/abs/2402.17140v1","category":"hep-ph"}
{"created":"2024-02-27 01:45:51","title":"Experimental Study: Enhancing Voice Spoofing Detection Models with wav2vec 2.0","abstract":"Conventional spoofing detection systems have heavily relied on the use of handcrafted features derived from speech data. However, a notable shift has recently emerged towards the direct utilization of raw speech waveforms, as demonstrated by methods like SincNet filters. This shift underscores the demand for more sophisticated audio sample features. Moreover, the success of deep learning models, particularly those utilizing large pretrained wav2vec 2.0 as a featurization front-end, highlights the importance of refined feature encoders. In response, this research assessed the representational capability of wav2vec 2.0 as an audio feature extractor, modifying the size of its pretrained Transformer layers through two key adjustments: (1) selecting a subset of layers starting from the leftmost one and (2) fine-tuning a portion of the selected layers from the rightmost one. We complemented this analysis with five spoofing detection back-end models, with a primary focus on AASIST, enabling us to pinpoint the optimal configuration for the selection and fine-tuning process. In contrast to conventional handcrafted features, our investigation identified several spoofing detection systems that achieve state-of-the-art performance in the ASVspoof 2019 LA dataset. This comprehensive exploration offers valuable insights into feature selection strategies, advancing the field of spoofing detection.","sentences":["Conventional spoofing detection systems have heavily relied on the use of handcrafted features derived from speech data.","However, a notable shift has recently emerged towards the direct utilization of raw speech waveforms, as demonstrated by methods like SincNet filters.","This shift underscores the demand for more sophisticated audio sample features.","Moreover, the success of deep learning models, particularly those utilizing large pretrained wav2vec 2.0 as a featurization front-end, highlights the importance of refined feature encoders.","In response, this research assessed the representational capability of wav2vec 2.0 as an audio feature extractor, modifying the size of its pretrained Transformer layers through two key adjustments: (1) selecting a subset of layers starting from the leftmost one and (2) fine-tuning a portion of the selected layers from the rightmost one.","We complemented this analysis with five spoofing detection back-end models, with a primary focus on AASIST, enabling us to pinpoint the optimal configuration for the selection and fine-tuning process.","In contrast to conventional handcrafted features, our investigation identified several spoofing detection systems that achieve state-of-the-art performance in the ASVspoof 2019 LA dataset.","This comprehensive exploration offers valuable insights into feature selection strategies, advancing the field of spoofing detection."],"url":"http://arxiv.org/abs/2402.17127v1","category":"cs.SD"}
{"created":"2024-02-26 23:21:45","title":"Batch Estimation of a Steady, Uniform, Flow-Field from Ground Velocity and Heading Measurements","abstract":"This paper presents three batch estimation methods that use noisy ground velocity and heading measurements from a vehicle executing a circular orbit (or similar large heading change maneuver) to estimate the speed and direction of a steady, uniform, flow-field. The methods are based on a simple kinematic model of the vehicle's motion and use curve-fitting or nonlinear least-square optimization. A Monte Carlo simulation with randomized flow conditions is used to evaluate the batch estimation methods while varying the measurement noise of the data and the interval of unique heading traversed during the maneuver. The methods are also compared using experimental data obtained with a Bluefin-21 unmanned underwater vehicle performing a series of circular orbit maneuvers over a five hour period in a tide-driven flow.","sentences":["This paper presents three batch estimation methods that use noisy ground velocity and heading measurements from a vehicle executing a circular orbit (or similar large heading change maneuver) to estimate the speed and direction of a steady, uniform, flow-field.","The methods are based on a simple kinematic model of the vehicle's motion and use curve-fitting or nonlinear least-square optimization.","A Monte Carlo simulation with randomized flow conditions is used to evaluate the batch estimation methods while varying the measurement noise of the data and the interval of unique heading traversed during the maneuver.","The methods are also compared using experimental data obtained with a Bluefin-21 unmanned underwater vehicle performing a series of circular orbit maneuvers over a five hour period in a tide-driven flow."],"url":"http://arxiv.org/abs/2402.17078v1","category":"eess.SY"}
{"created":"2024-02-26 23:15:41","title":"Asphalt Concrete Characterization Using Digital Image Correlation: A Systematic Review of Best Practices, Applications, and Future Vision","abstract":"Digital Image Correlation (DIC) is an optical technique that measures displacement and strain by tracking pattern movement in a sequence of captured images during testing. DIC has gained recognition in asphalt pavement engineering since the early 2000s. However, users often perceive the DIC technique as an out-of-box tool and lack a thorough understanding of its operational and measurement principles. This article presents a state-of-art review of DIC as a crucial tool for laboratory testing of asphalt concrete (AC), primarily focusing on the widely utilized 2D-DIC and 3D-DIC techniques. To address frequently asked questions from users, the review thoroughly examines the optimal methods for preparing speckle patterns, configuring single-camera or dual-camera imaging systems, conducting DIC analyses, and exploring various applications. Furthermore, emerging DIC methodologies such as Digital Volume Correlation and deep-learning-based DIC are introduced, highlighting their potential for future applications in pavement engineering. The article also provides a comprehensive and reliable flowchart for implementing DIC in AC characterization. Finally, critical directions for future research are presented.","sentences":["Digital Image Correlation (DIC) is an optical technique that measures displacement and strain by tracking pattern movement in a sequence of captured images during testing.","DIC has gained recognition in asphalt pavement engineering since the early 2000s.","However, users often perceive the DIC technique as an out-of-box tool and lack a thorough understanding of its operational and measurement principles.","This article presents a state-of-art review of DIC as a crucial tool for laboratory testing of asphalt concrete (AC), primarily focusing on the widely utilized 2D-DIC and 3D-DIC techniques.","To address frequently asked questions from users, the review thoroughly examines the optimal methods for preparing speckle patterns, configuring single-camera or dual-camera imaging systems, conducting DIC analyses, and exploring various applications.","Furthermore, emerging DIC methodologies such as Digital Volume Correlation and deep-learning-based DIC are introduced, highlighting their potential for future applications in pavement engineering.","The article also provides a comprehensive and reliable flowchart for implementing DIC in AC characterization.","Finally, critical directions for future research are presented."],"url":"http://arxiv.org/abs/2402.17074v1","category":"cs.CV"}
{"created":"2024-02-26 23:13:47","title":"Proximal Algorithms for a class of abstract convex functions","abstract":"In this paper we analyze a class of nonconvex optimization problem from the viewpoint of abstract convexity. Using the respective generalizations of the subgradient we propose an abstract notion proximal operator and derive a number of algorithms, namely an abstract proximal point method, an abstract forward-backward method and an abstract projected subgradient method. Global convergence results for all algorithms are discussed and numerical examples are given","sentences":["In this paper we analyze a class of nonconvex optimization problem from the viewpoint of abstract convexity.","Using the respective generalizations of the subgradient we propose an abstract notion proximal operator and derive a number of algorithms, namely an abstract proximal point method, an abstract forward-backward method and an abstract projected subgradient method.","Global convergence results for all algorithms are discussed and numerical examples are given"],"url":"http://arxiv.org/abs/2402.17072v1","category":"math.OC"}
{"created":"2024-02-26 22:43:04","title":"On some features of Quadratic Unconstrained Binary Optimization with random coefficients","abstract":"Quadratic Unconstrained Binary Optimization (QUBO or UBQP) is concerned with maximizing/minimizing the quadratic form $H(J, \\eta) = W \\sum_{i,j} J_{i,j} \\eta_{i} \\eta_{j}$ with $J$ a matrix of coefficients, $\\eta \\in \\{0, 1\\}^N$ and $W$ a normalizing constant. In the statistical mechanics literature, QUBO is a lattice gas counterpart to the Sherrington--Kirkpatrick spin glass model. Finding the optima of $H$ is an NP-hard problem. Several problems in combinatorial optimization and data analysis can be mapped to QUBO in a straightforward manner. In the combinatorial optimization literature, random instances of QUBO are often used to test the effectiveness of heuristic algorithms. Here we consider QUBO with random coefficients and show that if the $J_{i,j}$'s have zero mean, then, after proper normalization, the minimum and maximum \\emph{per particle} of $H$ do not depend on the details of the distribution of the couplings and are concentrated around their expected values. Further, with the help of numerical simulations, we give estimates of the minimum and maximum of the objective function and provide some insight into the structure of the minimizer and the maximizer of $H$. We argue that also this structure is rather robust. Our findings hold in the diluted case where each of the $J_{i,j}$'s is allowed to be zero with probability going to $1$ as $N \\to \\infty$ in a suitable way.","sentences":["Quadratic Unconstrained Binary Optimization (QUBO or UBQP) is concerned with maximizing/minimizing the quadratic form $H(J, \\eta) = W \\sum_{i,j} J_{i,j} \\eta_{i} \\eta_{j}$ with $J$ a matrix of coefficients, $\\eta \\in \\{0, 1\\}^N$ and $W$ a normalizing constant.","In the statistical mechanics literature, QUBO is a lattice gas counterpart to the Sherrington--Kirkpatrick spin glass model.","Finding the optima of $H$ is an NP-hard problem.","Several problems in combinatorial optimization and data analysis can be mapped to QUBO in a straightforward manner.","In the combinatorial optimization literature, random instances of QUBO are often used to test the effectiveness of heuristic algorithms.","Here we consider QUBO with random coefficients and show that if the $J_{i,j}$'s have zero mean, then, after proper normalization, the minimum and maximum \\emph{per particle} of $H$ do not depend on the details of the distribution of the couplings and are concentrated around their expected values.","Further, with the help of numerical simulations, we give estimates of the minimum and maximum of the objective function and provide some insight into the structure of the minimizer and the maximizer of $H$. We argue that also this structure is rather robust.","Our findings hold in the diluted case where each of the $J_{i,j}$'s is allowed to be zero with probability going to $1$ as $N \\to \\infty$ in a suitable way."],"url":"http://arxiv.org/abs/2402.17059v1","category":"math.PR"}
{"created":"2024-02-26 21:44:23","title":"Hybrid Feedback Control for Global and Optimal Safe Navigation","abstract":"We propose a hybrid feedback control strategy that safely steers a point-mass robot to a target location optimally from all initial conditions in the n-dimensional Euclidean space with a single spherical obstacle. The robot moves straight to the target when it has a clear line-of-sight to the target location. Otherwise, it engages in an optimal obstacle avoidance maneuver via the shortest path inside the cone enclosing the obstacle and having the robot's position as a vertex. The switching strategy that avoids the undesired equilibria, leading to global asymptotic stability (GAS) of the target location, relies on using two appropriately designed virtual destinations, ensuring control continuity and shortest path generation. Simulation results illustrating the effectiveness of the proposed approach are presented.","sentences":["We propose a hybrid feedback control strategy that safely steers a point-mass robot to a target location optimally from all initial conditions in the n-dimensional Euclidean space with a single spherical obstacle.","The robot moves straight to the target when it has a clear line-of-sight to the target location.","Otherwise, it engages in an optimal obstacle avoidance maneuver via the shortest path inside the cone enclosing the obstacle and having the robot's position as a vertex.","The switching strategy that avoids the undesired equilibria, leading to global asymptotic stability (GAS) of the target location, relies on using two appropriately designed virtual destinations, ensuring control continuity and shortest path generation.","Simulation results illustrating the effectiveness of the proposed approach are presented."],"url":"http://arxiv.org/abs/2402.17038v1","category":"eess.SY"}
{"created":"2024-02-26 21:31:21","title":"Measurement of cabin pressure during a flight with a smartphone","abstract":"An aircraft cabin is used as a laboratory for studying the atmospheric pressure during a flight. All the different steps of the flight: take-off, cruise altitude climbing and landing are monitored with the use of the pressure sensor of a smartphone. Specific details of the atmospheric pressure during take-off and landing are given. Calculations are done for the aircraft nose elevation angle during take-off, with the relation between the optimized cabin pressure for the passenger comfort and the aircraft altitude.","sentences":["An aircraft cabin is used as a laboratory for studying the atmospheric pressure during a flight.","All the different steps of the flight: take-off, cruise altitude climbing and landing are monitored with the use of the pressure sensor of a smartphone.","Specific details of the atmospheric pressure during take-off and landing are given.","Calculations are done for the aircraft nose elevation angle during take-off, with the relation between the optimized cabin pressure for the passenger comfort and the aircraft altitude."],"url":"http://arxiv.org/abs/2402.17035v1","category":"physics.ao-ph"}
{"created":"2024-02-26 19:25:21","title":"An inexact Bregman proximal point method and its acceleration version for unbalanced optimal transport","abstract":"The Unbalanced Optimal Transport (UOT) problem plays increasingly important roles in computational biology, computational imaging and deep learning. Scaling algorithm is widely used to solve UOT due to its convenience and good convergence properties. However, this algorithm has lower accuracy for large regularization parameters, and due to stability issues, small regularization parameters can easily lead to numerical overflow. We address this challenge by developing an inexact Bregman proximal point method for solving UOT. This algorithm approximates the proximal operator using the Scaling algorithm at each iteration. The algorithm (1) converges to the true solution of UOT, (2) has theoretical guarantees and robust regularization parameter selection, (3) mitigates numerical stability issues, and (4) can achieve comparable computational complexity to the Scaling algorithm in specific practice. Building upon this, we develop an accelerated version of inexact Bregman proximal point method for solving UOT by using acceleration techniques of Bregman proximal point method and provide theoretical guarantees and experimental validation of convergence and acceleration.","sentences":["The Unbalanced Optimal Transport (UOT) problem plays increasingly important roles in computational biology, computational imaging and deep learning.","Scaling algorithm is widely used to solve UOT due to its convenience and good convergence properties.","However, this algorithm has lower accuracy for large regularization parameters, and due to stability issues, small regularization parameters can easily lead to numerical overflow.","We address this challenge by developing an inexact Bregman proximal point method for solving UOT.","This algorithm approximates the proximal operator using the Scaling algorithm at each iteration.","The algorithm (1) converges to the true solution of UOT, (2) has theoretical guarantees and robust regularization parameter selection, (3) mitigates numerical stability issues, and (4) can achieve comparable computational complexity to the Scaling algorithm in specific practice.","Building upon this, we develop an accelerated version of inexact Bregman proximal point method for solving UOT by using acceleration techniques of Bregman proximal point method and provide theoretical guarantees and experimental validation of convergence and acceleration."],"url":"http://arxiv.org/abs/2402.16978v1","category":"math.OC"}
{"created":"2024-02-26 19:15:23","title":"Optimal Mechanisms for Consumer Surplus Maximization","abstract":"We consider the problem of designing auctions which maximize consumer surplus (i.e., the social welfare minus the payments charged to the buyers). In the consumer surplus maximization problem, a seller with a set of goods faces a set of strategic buyers with private values, each of whom aims to maximize their own individual utility. The seller, in contrast, aims to allocate the goods in a way which maximizes the total buyer utility. The seller must then elicit the values of the buyers in order to decide what goods to award each buyer. The canonical approach in mechanism design to ensure truthful reporting of the private information is to find appropriate prices to charge each buyer in order to align their objective with the objective of the seller. Indeed, there are many celebrated results to this end when the seller's objective is welfare maximization [Clarke, 1971, Groves, 1973, Vickrey, 1961] or revenue maximization [Myerson, 1981]. However, in the case of consumer surplus maximization the picture is less clear -- using high payments to ensure the highest value bidders are served necessarily decreases their surplus utility, but using low payments may lead the seller into serving lower value bidders.   Our main result in this paper is a framework for designing mechanisms which maximize consumer surplus. We instantiate our framework in a variety of canonical multi-parameter auction settings (i.e., unit-demand bidders with heterogeneous items, multi-unit auctions, and auctions with divisible goods) and use it to design auctions achieving consumer surplus with optimal approximation guarantees against the total social welfare. Along the way, we answer an open question posed by Hartline and Roughgarden [2008], who, to our knowledge, were the first to study the question of consumer surplus approximation guarantees in single-parameter settings, regarding optimal mechanisms for two bidders.","sentences":["We consider the problem of designing auctions which maximize consumer surplus (i.e., the social welfare minus the payments charged to the buyers).","In the consumer surplus maximization problem, a seller with a set of goods faces a set of strategic buyers with private values, each of whom aims to maximize their own individual utility.","The seller, in contrast, aims to allocate the goods in a way which maximizes the total buyer utility.","The seller must then elicit the values of the buyers in order to decide what goods to award each buyer.","The canonical approach in mechanism design to ensure truthful reporting of the private information is to find appropriate prices to charge each buyer in order to align their objective with the objective of the seller.","Indeed, there are many celebrated results to this end when the seller's objective is welfare maximization","[Clarke, 1971, Groves, 1973, Vickrey, 1961] or revenue maximization","[Myerson, 1981].","However, in the case of consumer surplus maximization the picture is less clear -- using high payments to ensure the highest value bidders are served necessarily decreases their surplus utility, but using low payments may lead the seller into serving lower value bidders.   ","Our main result in this paper is a framework for designing mechanisms which maximize consumer surplus.","We instantiate our framework in a variety of canonical multi-parameter auction settings (i.e., unit-demand bidders with heterogeneous items, multi-unit auctions, and auctions with divisible goods) and use it to design auctions achieving consumer surplus with optimal approximation guarantees against the total social welfare.","Along the way, we answer an open question posed by Hartline and Roughgarden [2008], who, to our knowledge, were the first to study the question of consumer surplus approximation guarantees in single-parameter settings, regarding optimal mechanisms for two bidders."],"url":"http://arxiv.org/abs/2402.16972v1","category":"cs.GT"}
{"created":"2024-02-26 19:07:58","title":"Robust Evaluation of Longitudinal Surrogate Markers with Censored Data","abstract":"The development of statistical methods to evaluate surrogate markers is an active area of research. In many clinical settings, the surrogate marker is not simply a single measurement but is instead a longitudinal trajectory of measurements over time, e.g., fasting plasma glucose measured every 6 months for 3 years. In general, available methods developed for the single-surrogate setting cannot accommodate a longitudinal surrogate marker. Furthermore, many of the methods have not been developed for use with primary outcomes that are time-to-event outcomes and/or subject to censoring. In this paper, we propose robust methods to evaluate a longitudinal surrogate marker in a censored time-to-event outcome setting. Specifically, we propose a method to define and estimate the proportion of the treatment effect on a censored primary outcome that is explained by the treatment effect on a longitudinal surrogate marker measured up to time $t_0$. We accommodate both potential censoring of the primary outcome and of the surrogate marker. A simulation study demonstrates good finite-sample performance of our proposed methods. We illustrate our procedures by examining repeated measures of fasting plasma glucose, a surrogate marker for diabetes diagnosis, using data from the Diabetes Prevention Program (DPP).","sentences":["The development of statistical methods to evaluate surrogate markers is an active area of research.","In many clinical settings, the surrogate marker is not simply a single measurement but is instead a longitudinal trajectory of measurements over time, e.g., fasting plasma glucose measured every 6 months for 3 years.","In general, available methods developed for the single-surrogate setting cannot accommodate a longitudinal surrogate marker.","Furthermore, many of the methods have not been developed for use with primary outcomes that are time-to-event outcomes and/or subject to censoring.","In this paper, we propose robust methods to evaluate a longitudinal surrogate marker in a censored time-to-event outcome setting.","Specifically, we propose a method to define and estimate the proportion of the treatment effect on a censored primary outcome that is explained by the treatment effect on a longitudinal surrogate marker measured up to time $t_0$. We accommodate both potential censoring of the primary outcome and of the surrogate marker.","A simulation study demonstrates good finite-sample performance of our proposed methods.","We illustrate our procedures by examining repeated measures of fasting plasma glucose, a surrogate marker for diabetes diagnosis, using data from the Diabetes Prevention Program (DPP)."],"url":"http://arxiv.org/abs/2402.16969v1","category":"stat.ME"}
{"created":"2024-02-26 19:00:22","title":"The outskirts of M33: Tidally induced distortions versus signatures of gas accretion","abstract":"We investigate a possible close encounter between M33 and M31 in the past to understand the role of galaxy-galaxy interactions in shaping the matter distribution in galaxy outskirts. We recovered possible orbital trajectories of M33, M31 and the Milky Way in the past, which are compatible with the Early Third Data Release of the Gaia mission and with mass estimates of Local Group spirals, after tuning mass losses and the dynamical friction term with the help of N-body numerical simulations. A close encounter of M33 and M31 in the past has a low but non-negligible probability. If the two galaxies had been closer in the past, their minimum distance would be of the order of 100 kpc or larger, and this happened earlier than 3 Gyr ago. During this encounter, 35-40% of the dark matter mass of M33 might have been removed from the halo due to tidal stripping. A detailed comparison of the results of test-particle simulations with the observed disk warp or with the spatial distribution of candidate dark satellites of M33 suggests that a closer passage of M33 around M31 cannot, however, be responsible for the observed morphological features. We suggest that more recent gas accretion events, possibly from a cosmic filament, might cause the misalignment of the outer disk of M33 after the rapid inner disk formation.","sentences":["We investigate a possible close encounter between M33 and M31 in the past to understand the role of galaxy-galaxy interactions in shaping the matter distribution in galaxy outskirts.","We recovered possible orbital trajectories of M33, M31 and the Milky Way in the past, which are compatible with the Early Third Data Release of the Gaia mission and with mass estimates of Local Group spirals, after tuning mass losses and the dynamical friction term with the help of N-body numerical simulations.","A close encounter of M33 and M31 in the past has a low but non-negligible probability.","If the two galaxies had been closer in the past, their minimum distance would be of the order of 100 kpc or larger, and this happened earlier than 3 Gyr ago.","During this encounter, 35-40% of the dark matter mass of M33 might have been removed from the halo due to tidal stripping.","A detailed comparison of the results of test-particle simulations with the observed disk warp or with the spatial distribution of candidate dark satellites of M33 suggests that a closer passage of M33 around M31 cannot, however, be responsible for the observed morphological features.","We suggest that more recent gas accretion events, possibly from a cosmic filament, might cause the misalignment of the outer disk of M33 after the rapid inner disk formation."],"url":"http://arxiv.org/abs/2402.16957v1","category":"astro-ph.GA"}
{"created":"2024-02-26 18:59:35","title":"The Art of Staying Ahead of Deadlines: Improved Algorithms for the Minimum Tardy Processing Time","abstract":"We study the fundamental scheduling problem $1\\|\\sum p_jU_j$. Given a set of $n$ jobs with processing times $p_j$ and deadlines $d_j$, the problem is to select a subset of jobs such that the total processing time is maximized without violating the deadlines. In the midst of a flourishing line of research, Fischer and Wennmann have recently devised the sought-after $\\widetilde O(P)$-time algorithm, where $P = \\sum p_j$ is the total processing time of all jobs. This running time is optimal as it matches conditional lower bounds based on popular conjectures.   However, $P$ is not the sole parameter one could parameterize the running time by. Indeed, they explicitly leave open the question of whether a running time of $\\widetilde O(n + \\max d_j)$ or even $\\widetilde O(n + \\max p_j)$ is possible. In this work, we show, somewhat surprisingly, that by a refined implementation of their original algorithm, one can obtain the asked-for $\\widetilde O(n + \\max d_j)$-time algorithm.","sentences":["We study the fundamental scheduling problem $1\\|\\sum p_jU_j$. Given a set of $n$ jobs with processing times $p_j$ and deadlines $d_j$, the problem is to select a subset of jobs such that the total processing time is maximized without violating the deadlines.","In the midst of a flourishing line of research, Fischer and Wennmann have recently devised the sought-after $\\widetilde O(P)$-time algorithm, where $P = \\sum p_j$ is the total processing time of all jobs.","This running time is optimal as it matches conditional lower bounds based on popular conjectures.   ","However, $P$ is not the sole parameter one could parameterize the running time by.","Indeed, they explicitly leave open the question of whether a running time of $\\widetilde O(n + \\max d_j)$ or even $\\widetilde O(n + \\max p_j)$ is possible.","In this work, we show, somewhat surprisingly, that by a refined implementation of their original algorithm, one can obtain the asked-for $\\widetilde O(n + \\max d_j)$-time algorithm."],"url":"http://arxiv.org/abs/2402.16847v1","category":"cs.DS"}
{"created":"2024-02-26 18:59:08","title":"Unveiling Intrinsic Many-Body Complexity by Compressing Single-Body Triviality","abstract":"The simultaneous treatment of static and dynamical correlations in strongly-correlated electron systems is a critical challenge. In particular, finding a universal scheme for identifying a single-particle orbital basis that minimizes the representational complexity of the many-body wavefunction is a formidable and longstanding problem. As a substantial contribution towards its solution, we show that the total orbital correlation actually reveals and quantifies the intrinsic complexity of the wavefunction,once it is minimized via orbital rotations. To demonstrate the power of this concept in practice, an iterative scheme is proposed to optimize the orbitals by minimizing the total orbital correlation calculated by the tailored coupled cluster singles and doubles (TCCSD) ansatz. The optimized orbitals enable the limited TCCSD ansatz to capture more non-trivial information of the many-body wavefunction, indicated by the improved wavefunction and energy. An initial application of this scheme shows great improvement of TCCSD in predicting the singlet ground state potential energy curves of the strongly correlated C$_{\\rm 2}$ and Cr$_{\\rm 2}$ molecule.","sentences":["The simultaneous treatment of static and dynamical correlations in strongly-correlated electron systems is a critical challenge.","In particular, finding a universal scheme for identifying a single-particle orbital basis that minimizes the representational complexity of the many-body wavefunction is a formidable and longstanding problem.","As a substantial contribution towards its solution, we show that the total orbital correlation actually reveals and quantifies the intrinsic complexity of the wavefunction,once it is minimized via orbital rotations.","To demonstrate the power of this concept in practice, an iterative scheme is proposed to optimize the orbitals by minimizing the total orbital correlation calculated by the tailored coupled cluster singles and doubles (TCCSD) ansatz.","The optimized orbitals enable the limited TCCSD ansatz to capture more non-trivial information of the many-body wavefunction, indicated by the improved wavefunction and energy.","An initial application of this scheme shows great improvement of TCCSD in predicting the singlet ground state potential energy curves of the strongly correlated C$_{\\rm 2}$ and Cr$_{\\rm 2}$ molecule."],"url":"http://arxiv.org/abs/2402.16841v1","category":"quant-ph"}
{"created":"2024-02-26 18:54:35","title":"A Survey on Data Selection for Language Models","abstract":"A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required.   Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data selection practices has become concentrated within a few organizations, many of which do not openly share their findings and methodologies.   To narrow this gap in knowledge, we present a comprehensive review of existing literature on data selection methods and related research areas, providing a taxonomy of existing approaches. By describing the current landscape of research, this work aims to accelerate progress in data selection by establishing an entry point for new and established researchers. Additionally, throughout this review we draw attention to noticeable holes in the literature and conclude the paper by proposing promising avenues for future research.","sentences":["A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training.","However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary.","Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required.   ","Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points.","The promise of improved data selection methods has caused the volume of research in the area to rapidly expand.","However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research.","Consequently, knowledge of effective data selection practices has become concentrated within a few organizations, many of which do not openly share their findings and methodologies.   ","To narrow this gap in knowledge, we present a comprehensive review of existing literature on data selection methods and related research areas, providing a taxonomy of existing approaches.","By describing the current landscape of research, this work aims to accelerate progress in data selection by establishing an entry point for new and established researchers.","Additionally, throughout this review we draw attention to noticeable holes in the literature and conclude the paper by proposing promising avenues for future research."],"url":"http://arxiv.org/abs/2402.16827v1","category":"cs.CL"}
{"created":"2024-02-26 18:54:15","title":"Disentangled 3D Scene Generation with Layout Learning","abstract":"We introduce a method to generate 3D scenes that are disentangled into their component objects. This disentanglement is unsupervised, relying only on the knowledge of a large pretrained text-to-image model. Our key insight is that objects can be discovered by finding parts of a 3D scene that, when rearranged spatially, still produce valid configurations of the same scene. Concretely, our method jointly optimizes multiple NeRFs from scratch - each representing its own object - along with a set of layouts that composite these objects into scenes. We then encourage these composited scenes to be in-distribution according to the image generator. We show that despite its simplicity, our approach successfully generates 3D scenes decomposed into individual objects, enabling new capabilities in text-to-3D content creation. For results and an interactive demo, see our project page at https://dave.ml/layoutlearning/","sentences":["We introduce a method to generate 3D scenes that are disentangled into their component objects.","This disentanglement is unsupervised, relying only on the knowledge of a large pretrained text-to-image model.","Our key insight is that objects can be discovered by finding parts of a 3D scene that, when rearranged spatially, still produce valid configurations of the same scene.","Concretely, our method jointly optimizes multiple NeRFs from scratch - each representing its own object - along with a set of layouts that composite these objects into scenes.","We then encourage these composited scenes to be in-distribution according to the image generator.","We show that despite its simplicity, our approach successfully generates 3D scenes decomposed into individual objects, enabling new capabilities in text-to-3D content creation.","For results and an interactive demo, see our project page at https://dave.ml/layoutlearning/"],"url":"http://arxiv.org/abs/2402.16936v1","category":"cs.CV"}
{"created":"2024-02-26 18:17:34","title":"Finite element schemes with tangential motion for fourth order geometric curve evolutions in arbitrary codimension","abstract":"We introduce novel finite element schemes for curve diffusion and elastic flow in arbitrary codimension. The schemes are based on a variational form of a system that includes a specifically chosen tangential motion. We derive optimal $L^2$- and $H^1$-error bounds for continuous-in-time semidiscrete finite element approximations that use piecewise linear elements. In addition, we consider fully discrete schemes and, in the case of curve diffusion, prove unconditional stability for it. Finally, we present several numerical simulations, including some convergence experiments that confirm the derived error bounds. The presented simulations suggest that the tangential motion leads to equidistribution in practice.","sentences":["We introduce novel finite element schemes for curve diffusion and elastic flow in arbitrary codimension.","The schemes are based on a variational form of a system that includes a specifically chosen tangential motion.","We derive optimal $L^2$- and $H^1$-error bounds for continuous-in-time semidiscrete finite element approximations that use piecewise linear elements.","In addition, we consider fully discrete schemes and, in the case of curve diffusion, prove unconditional stability for it.","Finally, we present several numerical simulations, including some convergence experiments that confirm the derived error bounds.","The presented simulations suggest that the tangential motion leads to equidistribution in practice."],"url":"http://arxiv.org/abs/2402.16799v1","category":"math.NA"}
{"created":"2024-02-26 18:07:27","title":"Failures and Successes of Cross-Validation for Early-Stopped Gradient Descent","abstract":"We analyze the statistical properties of generalized cross-validation (GCV) and leave-one-out cross-validation (LOOCV) applied to early-stopped gradient descent (GD) in high-dimensional least squares regression. We prove that GCV is generically inconsistent as an estimator of the prediction risk of early-stopped GD, even for a well-specified linear model with isotropic features. In contrast, we show that LOOCV converges uniformly along the GD trajectory to the prediction risk. Our theory requires only mild assumptions on the data distribution and does not require the underlying regression function to be linear. Furthermore, by leveraging the individual LOOCV errors, we construct consistent estimators for the entire prediction error distribution along the GD trajectory and consistent estimators for a wide class of error functionals. This in particular enables the construction of pathwise prediction intervals based on GD iterates that have asymptotically correct nominal coverage conditional on the training data.","sentences":["We analyze the statistical properties of generalized cross-validation (GCV) and leave-one-out cross-validation (LOOCV) applied to early-stopped gradient descent (GD) in high-dimensional least squares regression.","We prove that GCV is generically inconsistent as an estimator of the prediction risk of early-stopped GD, even for a well-specified linear model with isotropic features.","In contrast, we show that LOOCV converges uniformly along the GD trajectory to the prediction risk.","Our theory requires only mild assumptions on the data distribution and does not require the underlying regression function to be linear.","Furthermore, by leveraging the individual LOOCV errors, we construct consistent estimators for the entire prediction error distribution along the GD trajectory and consistent estimators for a wide class of error functionals.","This in particular enables the construction of pathwise prediction intervals based on GD iterates that have asymptotically correct nominal coverage conditional on the training data."],"url":"http://arxiv.org/abs/2402.16793v1","category":"math.ST"}
{"created":"2024-02-26 17:58:16","title":"Conditional optimal sets and the quantization coefficients for some uniform distributions","abstract":"Bucklew and Wise (1982) showed that the quantization dimension of an absolutely continuous probability measure on a given Euclidean space is constant and equals the Euclidean dimension of the space, and the quantization coefficient exists as a finite positive number. By giving different examples, in this paper, we have shown that the quantization coefficients for absolutely continuous probability measures defined on the same Euclidean space can be different. We have taken uniform distribution as a prototype of an absolutely continuous probability measure. In addition, we have also calculated the conditional optimal sets of $n$-points and the $n$th conditional quantization errors for the uniform distributions in constrained and unconstrained scenarios.","sentences":["Bucklew and Wise (1982) showed that the quantization dimension of an absolutely continuous probability measure on a given Euclidean space is constant and equals the Euclidean dimension of the space, and the quantization coefficient exists as a finite positive number.","By giving different examples, in this paper, we have shown that the quantization coefficients for absolutely continuous probability measures defined on the same Euclidean space can be different.","We have taken uniform distribution as a prototype of an absolutely continuous probability measure.","In addition, we have also calculated the conditional optimal sets of $n$-points and the $n$th conditional quantization errors for the uniform distributions in constrained and unconstrained scenarios."],"url":"http://arxiv.org/abs/2402.16783v1","category":"math.PR"}
{"created":"2024-02-26 17:33:30","title":"Debiased LASSO under Poisson-Gauss Model","abstract":"Quantifying uncertainty in high-dimensional sparse linear regression is a fundamental task in statistics that arises in various applications. One of the most successful methods for quantifying uncertainty is the debiased LASSO, which has a solid theoretical foundation but is restricted to settings where the noise is purely additive. Motivated by real-world applications, we study the so-called Poisson inverse problem with additive Gaussian noise and propose a debiased LASSO algorithm that only requires $n \\gg s\\log^2p$ samples, which is optimal up to a logarithmic factor.","sentences":["Quantifying uncertainty in high-dimensional sparse linear regression is a fundamental task in statistics that arises in various applications.","One of the most successful methods for quantifying uncertainty is the debiased LASSO, which has a solid theoretical foundation but is restricted to settings where the noise is purely additive.","Motivated by real-world applications, we study the so-called Poisson inverse problem with additive Gaussian noise and propose a debiased LASSO algorithm that only requires $n \\gg s\\log^2p$ samples, which is optimal up to a logarithmic factor."],"url":"http://arxiv.org/abs/2402.16764v1","category":"math.ST"}
{"created":"2024-02-26 17:19:54","title":"Ambiguity Function Shaping in FMCW Automotive Radar","abstract":"Frequency-modulated continuous wave (FMCW) radar with inter-chirp coding produces high side-lobes in the Doppler and range dimensions of the radar's ambiguity function. The high side-lobes may cause miss-detection due to masking between targets that are at similar range and have large received power difference, as is often the case in automotive scenarios. In this paper, we develop a novel code optimization method that attenuates the side-lobes of the radar's ambiguity function. In particular, we introduce a framework for designing radar transmit sequences by shaping the radar Ambiguity Function (AF) to a desired structure. The proposed approach suppresses the average amplitude of the AF of the transmitted signal in regions of interest by efficiently tackling a longstanding optimization problem. The optimization criterion is quartic in nature with respect to the radar transmit code. A cyclic iterative algorithm is introduced that recasts the quartic problem as a unimodular quadratic problem (UQP) which can be tackled using power-method-like iterations (PMLI). Our numerical results demonstrate the effectiveness of the proposed algorithm in designing sequences with desired AF which is of great interest to the future generations of automotive radar sensors.","sentences":["Frequency-modulated continuous wave (FMCW) radar with inter-chirp coding produces high side-lobes in the Doppler and range dimensions of the radar's ambiguity function.","The high side-lobes may cause miss-detection due to masking between targets that are at similar range and have large received power difference, as is often the case in automotive scenarios.","In this paper, we develop a novel code optimization method that attenuates the side-lobes of the radar's ambiguity function.","In particular, we introduce a framework for designing radar transmit sequences by shaping the radar Ambiguity Function (AF) to a desired structure.","The proposed approach suppresses the average amplitude of the AF of the transmitted signal in regions of interest by efficiently tackling a longstanding optimization problem.","The optimization criterion is quartic in nature with respect to the radar transmit code.","A cyclic iterative algorithm is introduced that recasts the quartic problem as a unimodular quadratic problem (UQP) which can be tackled using power-method-like iterations (PMLI).","Our numerical results demonstrate the effectiveness of the proposed algorithm in designing sequences with desired AF which is of great interest to the future generations of automotive radar sensors."],"url":"http://arxiv.org/abs/2402.16754v1","category":"eess.SP"}
{"created":"2024-02-26 17:09:18","title":"Enhancing Hypergradients Estimation: A Study of Preconditioning and Reparameterization","abstract":"Bilevel optimization aims to optimize an outer objective function that depends on the solution to an inner optimization problem. It is routinely used in Machine Learning, notably for hyperparameter tuning. The conventional method to compute the so-called hypergradient of the outer problem is to use the Implicit Function Theorem (IFT). As a function of the error of the inner problem resolution, we study the error of the IFT method. We analyze two strategies to reduce this error: preconditioning the IFT formula and reparameterizing the inner problem. We give a detailed account of the impact of these two modifications on the error, highlighting the role played by higher-order derivatives of the functionals at stake. Our theoretical findings explain when super efficiency, namely reaching an error on the hypergradient that depends quadratically on the error on the inner problem, is achievable and compare the two approaches when this is impossible. Numerical evaluations on hyperparameter tuning for regression problems substantiate our theoretical findings.","sentences":["Bilevel optimization aims to optimize an outer objective function that depends on the solution to an inner optimization problem.","It is routinely used in Machine Learning, notably for hyperparameter tuning.","The conventional method to compute the so-called hypergradient of the outer problem is to use the Implicit Function Theorem (IFT).","As a function of the error of the inner problem resolution, we study the error of the IFT method.","We analyze two strategies to reduce this error: preconditioning the IFT formula and reparameterizing the inner problem.","We give a detailed account of the impact of these two modifications on the error, highlighting the role played by higher-order derivatives of the functionals at stake.","Our theoretical findings explain when super efficiency, namely reaching an error on the hypergradient that depends quadratically on the error on the inner problem, is achievable and compare the two approaches when this is impossible.","Numerical evaluations on hyperparameter tuning for regression problems substantiate our theoretical findings."],"url":"http://arxiv.org/abs/2402.16748v1","category":"cs.LG"}
{"created":"2024-02-26 16:47:18","title":"Structure-dependent fragmentation risk of rubble-pile asteroids on low-impacts","abstract":"A key strategy in defending against stray asteroids is deflection from a collision trajectory by a low-momentum impact. This is to avoid a potential rain of large hazardous fragments, which high-momentum blasts may generate. Using a proof-of-principle numerical model, we show that even low-momentum impacts on rubble pile asteroids (RPAs), which most asteroids are, may lead to their fracturing into large fragments because of their internal non-uniform chain-like stress distribution. Since stress chains occur in three- and two-dimensional (2D) loose aggregates, we study low-momentum impacts on gravity-aggregated clusters in 2D. We establish that stresses are indeed supported by chains and show that the post-impact dynamic shear stress and displacement rates are highest along the chains, increasing significantly the risk of fracturing there. Our simulations suggest that this phenomenon is independent of the cluster's particle size distribution. We conclude that future studies must be carried out to quantify the relations between the risk of fracturing and stress chain statistics in RPAs.","sentences":["A key strategy in defending against stray asteroids is deflection from a collision trajectory by a low-momentum impact.","This is to avoid a potential rain of large hazardous fragments, which high-momentum blasts may generate.","Using a proof-of-principle numerical model, we show that even low-momentum impacts on rubble pile asteroids (RPAs), which most asteroids are, may lead to their fracturing into large fragments because of their internal non-uniform chain-like stress distribution.","Since stress chains occur in three- and two-dimensional (2D) loose aggregates, we study low-momentum impacts on gravity-aggregated clusters in 2D. We establish that stresses are indeed supported by chains and show that the post-impact dynamic shear stress and displacement rates are highest along the chains, increasing significantly the risk of fracturing there.","Our simulations suggest that this phenomenon is independent of the cluster's particle size distribution.","We conclude that future studies must be carried out to quantify the relations between the risk of fracturing and stress chain statistics in RPAs."],"url":"http://arxiv.org/abs/2402.16723v1","category":"astro-ph.EP"}
{"created":"2024-02-26 16:47:03","title":"All-optical polarization scrambler based on polarization beam splitting with amplified fiber ring","abstract":"Optical-fiber-based polarization scramblers can reduce the impact of polarization sensitive performance of various optical fiber systems. Here, we propose a simple and efficient polarization scrambler based on an all optical Mach-Zehnder structure by combining polarization beam splitter and amplified fiber ring. To totally decoherence one polarization splitted beam, a fiber ring together with an amplifier are incorporated. The ratio of two orthogonal beams can be controlled by varying the amplification factor, and we observe different evolution trajectories of the output state of polarizations on Poincare sphere. When the amplification factor exceeds a certain threshold, the scrambler system exhibits chaotical behavior. A commercial single wavelength laser with linewidth of 3 MHz is utilized to characterize the scrambling performance. We found that when the sampling rate is 1.6 MSa/s, a scrambling speed up to 2000 krad/s can be obtained for the average degree of polarization being less than 0.1. We also exploit these chaotic polarization fluctuations to generate random binary number, indicating that the proposed technique is a good candidate for random bit generator.","sentences":["Optical-fiber-based polarization scramblers can reduce the impact of polarization sensitive performance of various optical fiber systems.","Here, we propose a simple and efficient polarization scrambler based on an all optical Mach-Zehnder structure by combining polarization beam splitter and amplified fiber ring.","To totally decoherence one polarization splitted beam, a fiber ring together with an amplifier are incorporated.","The ratio of two orthogonal beams can be controlled by varying the amplification factor, and we observe different evolution trajectories of the output state of polarizations on Poincare sphere.","When the amplification factor exceeds a certain threshold, the scrambler system exhibits chaotical behavior.","A commercial single wavelength laser with linewidth of 3 MHz is utilized to characterize the scrambling performance.","We found that when the sampling rate is 1.6 MSa/s, a scrambling speed up to 2000 krad/s can be obtained for the average degree of polarization being less than 0.1.","We also exploit these chaotic polarization fluctuations to generate random binary number, indicating that the proposed technique is a good candidate for random bit generator."],"url":"http://arxiv.org/abs/2402.16722v1","category":"physics.optics"}
{"created":"2024-02-26 16:35:54","title":"A General Bayesian Algorithm for the Autonomous Alignment of Beamlines","abstract":"Autonomous methods to align beamlines can decrease the amount of time spent on diagnostics, and also uncover better global optima leading to better beam quality. The alignment of these beamlines is a high-dimensional, expensive-to-sample optimization problem involving the simultaneous treatment of many optical elements with correlated and nonlinear dynamics. Bayesian optimization is a strategy of efficient global optimization that has proved successful in similar regimes in a wide variety of beamline alignment applications, though it has typically been implemented for particular beamlines and optimization tasks. In this paper, we present a basic formulation of Bayesian inference and Gaussian process models as they relate to multiobjective Bayesian optimization, as well as the practical challenges presented by beamline alignment. We show that the same general implementation of Bayesian optimization with special consideration for beamline alignment can quickly learn the dynamics of particular beamlines in an online fashion through hyperparameter fitting with no prior information. We present the implementation of a concise software framework for beamline alignment and test it on four different optimization problems for experiments at x-ray beamlines of the National Synchrotron Light Source II and the Advanced Light Source and an electron beam at the Accelerator Test Facility, along with benchmarking on a simulated digital twin. We discuss new applications of the framework, and the potential for a unified approach to beamline alignment at synchrotron facilities.","sentences":["Autonomous methods to align beamlines can decrease the amount of time spent on diagnostics, and also uncover better global optima leading to better beam quality.","The alignment of these beamlines is a high-dimensional, expensive-to-sample optimization problem involving the simultaneous treatment of many optical elements with correlated and nonlinear dynamics.","Bayesian optimization is a strategy of efficient global optimization that has proved successful in similar regimes in a wide variety of beamline alignment applications, though it has typically been implemented for particular beamlines and optimization tasks.","In this paper, we present a basic formulation of Bayesian inference and Gaussian process models as they relate to multiobjective Bayesian optimization, as well as the practical challenges presented by beamline alignment.","We show that the same general implementation of Bayesian optimization with special consideration for beamline alignment can quickly learn the dynamics of particular beamlines in an online fashion through hyperparameter fitting with no prior information.","We present the implementation of a concise software framework for beamline alignment and test it on four different optimization problems for experiments at x-ray beamlines of the National Synchrotron Light Source II and the Advanced Light Source and an electron beam at the Accelerator Test Facility, along with benchmarking on a simulated digital twin.","We discuss new applications of the framework, and the potential for a unified approach to beamline alignment at synchrotron facilities."],"url":"http://arxiv.org/abs/2402.16716v1","category":"physics.acc-ph"}
{"created":"2024-02-26 16:30:58","title":"Scalable Robust Sparse Principal Component Analysis","abstract":"In this work, we propose an optimization framework for estimating a sparse robust one-dimensional subspace. Our objective is to minimize both the representation error and the penalty, in terms of the l1-norm criterion. Given that the problem is NP-hard, we introduce a linear relaxation-based approach. Additionally, we present a novel fitting procedure, utilizing simple ratios and sorting techniques. The proposed algorithm demonstrates a worst-case time complexity of $O(n^2 m \\log n)$ and, in certain instances, achieves global optimality for the sparse robust subspace, thereby exhibiting polynomial time efficiency. Compared to extant methodologies, the proposed algorithm finds the subspace with the lowest discordance, offering a smoother trade-off between sparsity and fit. Its architecture affords scalability, evidenced by a 16-fold improvement in computational speeds for matrices of 2000x2000 over CPU version. Furthermore, this method is distinguished by several advantages, including its independence from initialization and deterministic and replicable procedures. Furthermore, this method is distinguished by several advantages, including its independence from initialization and deterministic and replicable procedures. The real-world example demonstrates the effectiveness of algorithm in achieving meaningful sparsity, underscoring its precise and useful application across various domains.","sentences":["In this work, we propose an optimization framework for estimating a sparse robust one-dimensional subspace.","Our objective is to minimize both the representation error and the penalty, in terms of the l1-norm criterion.","Given that the problem is NP-hard, we introduce a linear relaxation-based approach.","Additionally, we present a novel fitting procedure, utilizing simple ratios and sorting techniques.","The proposed algorithm demonstrates a worst-case time complexity of $O(n^2 m \\log n)$","and, in certain instances, achieves global optimality for the sparse robust subspace, thereby exhibiting polynomial time efficiency.","Compared to extant methodologies, the proposed algorithm finds the subspace with the lowest discordance, offering a smoother trade-off between sparsity and fit.","Its architecture affords scalability, evidenced by a 16-fold improvement in computational speeds for matrices of 2000x2000 over CPU version.","Furthermore, this method is distinguished by several advantages, including its independence from initialization and deterministic and replicable procedures.","Furthermore, this method is distinguished by several advantages, including its independence from initialization and deterministic and replicable procedures.","The real-world example demonstrates the effectiveness of algorithm in achieving meaningful sparsity, underscoring its precise and useful application across various domains."],"url":"http://arxiv.org/abs/2402.16712v1","category":"stat.ML"}
{"created":"2024-02-26 16:27:08","title":"Cost Aware Best Arm Identification","abstract":"In this paper, we study a best arm identification problem with dual objects. In addition to the classic reward, each arm is associated with a cost distribution and the goal is to identify the largest reward arm using the minimum expected cost. We call it \\emph{Cost Aware Best Arm Identification} (CABAI), which captures the separation of testing and implementation phases in product development pipelines and models the objective shift between phases, i.e., cost for testing and reward for implementation. We first derive an theoretic lower bound for CABAI and propose an algorithm called $\\mathsf{CTAS}$ to match it asymptotically. To reduce the computation of $\\mathsf{CTAS}$, we further propose a low-complexity algorithm called CO, based on a square-root rule, which proves optimal in simplified two-armed models and generalizes surprisingly well in numerical experiments. Our results show (i) ignoring the heterogeneous action cost results in sub-optimality in practice, and (ii) low-complexity algorithms deliver near-optimal performance over a wide range of problems.","sentences":["In this paper, we study a best arm identification problem with dual objects.","In addition to the classic reward, each arm is associated with a cost distribution and the goal is to identify the largest reward arm using the minimum expected cost.","We call it \\emph{Cost Aware Best Arm Identification} (CABAI), which captures the separation of testing and implementation phases in product development pipelines and models the objective shift between phases, i.e., cost for testing and reward for implementation.","We first derive an theoretic lower bound for CABAI and propose an algorithm called $\\mathsf{CTAS}$ to match it asymptotically.","To reduce the computation of $\\mathsf{CTAS}$, we further propose a low-complexity algorithm called CO, based on a square-root rule, which proves optimal in simplified two-armed models and generalizes surprisingly well in numerical experiments.","Our results show (i) ignoring the heterogeneous action cost results in sub-optimality in practice, and (ii) low-complexity algorithms deliver near-optimal performance over a wide range of problems."],"url":"http://arxiv.org/abs/2402.16710v1","category":"cs.LG"}
{"created":"2024-02-26 16:13:09","title":"SwarmPRM: Probabilistic Roadmap Motion Planning for Swarm Robotic Systems","abstract":"Swarm robotic systems consisting of large-scale cooperative agents hold promise for performing autonomous tasks in diverse fields. However, existing planning strategies for swarm robotic systems often encounter a trade-off between scalability and solution quality. We introduce here SwarmPRM, a hierarchical, highly scalable, computationally efficient, and risk-aware sampling-based motion planning approach for swarm robotic systems, which is asymptotically optimal under mild assumptions. We employ probability density functions (PDFs) to represent the swarm's macroscopic state and utilize optimal mass transport (OMT) theory to measure the swarm's cost to go. A risk-aware Gaussian roadmap is constructed wherein each node encapsulates a distinct PDF and conditional-value-at-risk (CVaR) is employed to assess the collision risk, facilitating the generation of macroscopic PDFs in Wasserstein-GMM space. Extensive simulations demonstrate that the proposed approach outperforms state-of-the-art methods in terms of computational efficiency and the average travelling distance.","sentences":["Swarm robotic systems consisting of large-scale cooperative agents hold promise for performing autonomous tasks in diverse fields.","However, existing planning strategies for swarm robotic systems often encounter a trade-off between scalability and solution quality.","We introduce here SwarmPRM, a hierarchical, highly scalable, computationally efficient, and risk-aware sampling-based motion planning approach for swarm robotic systems, which is asymptotically optimal under mild assumptions.","We employ probability density functions (PDFs) to represent the swarm's macroscopic state and utilize optimal mass transport (OMT) theory to measure the swarm's cost to go.","A risk-aware Gaussian roadmap is constructed wherein each node encapsulates a distinct PDF and conditional-value-at-risk (CVaR) is employed to assess the collision risk, facilitating the generation of macroscopic PDFs in Wasserstein-GMM space.","Extensive simulations demonstrate that the proposed approach outperforms state-of-the-art methods in terms of computational efficiency and the average travelling distance."],"url":"http://arxiv.org/abs/2402.16699v1","category":"cs.RO"}
{"created":"2024-02-26 16:06:24","title":"Comparing resource requirements of noisy quantum simulation algorithms for the Tavis-Cummings model","abstract":"Fault-tolerant quantum computers could facilitate the simulation of quantum systems unfeasible for classical computation. However, the noisy intermediate-scale quantum (NISQ) devices of the present and near term are limited and their utilisation requires additional strategies. These include quantum error mitigation (QEM) for alleviating device noise, and variational quantum algorithms (VQAs) which combine classical optimization with short-depth, parameterized quantum circuits. We compare two such methods: zero-noise extrapolation (ZNE) with noise amplification by circuit folding, and incremental structural learning (ISL), a type of circuit recompiling VQA. These are applied to Trotterized time-evolution of the Tavis--Cummings model (TCM) under a noise simulation. Since both methods add circuit evaluation overhead, it is of interest to see how they compare both in the accuracy of the dynamics they produce, and in terms of the quantum resources used. Additionally, noisy recompilation of time-evolution circuits with ISL has not previously been explored. We find that while ISL achieves lower error than ZNE for smaller system sizes, it fails to produce correct dynamics for 4 qubits, where ZNE is superior. Diverging resource requirements for ISL and ZNE are observed, with ISL achieving low circuit depths at the cost of a large number of circuit evaluations.","sentences":["Fault-tolerant quantum computers could facilitate the simulation of quantum systems unfeasible for classical computation.","However, the noisy intermediate-scale quantum (NISQ) devices of the present and near term are limited and their utilisation requires additional strategies.","These include quantum error mitigation (QEM) for alleviating device noise, and variational quantum algorithms (VQAs) which combine classical optimization with short-depth, parameterized quantum circuits.","We compare two such methods: zero-noise extrapolation (ZNE) with noise amplification by circuit folding, and incremental structural learning (ISL), a type of circuit recompiling VQA.","These are applied to Trotterized time-evolution of the Tavis--Cummings model (TCM) under a noise simulation.","Since both methods add circuit evaluation overhead, it is of interest to see how they compare both in the accuracy of the dynamics they produce, and in terms of the quantum resources used.","Additionally, noisy recompilation of time-evolution circuits with ISL has not previously been explored.","We find that while ISL achieves lower error than ZNE for smaller system sizes, it fails to produce correct dynamics for 4 qubits, where ZNE is superior.","Diverging resource requirements for ISL and ZNE are observed, with ISL achieving low circuit depths at the cost of a large number of circuit evaluations."],"url":"http://arxiv.org/abs/2402.16692v1","category":"quant-ph"}
{"created":"2024-02-26 15:14:38","title":"TrustMol: Trustworthy Inverse Molecular Design via Alignment with Molecular Dynamics","abstract":"Data-driven generation of molecules with desired properties, also known as inverse molecular design (IMD), has attracted significant attention in recent years. Despite the significant progress in the accuracy and diversity of solutions, existing IMD methods lag behind in terms of trustworthiness. The root issue is that the design process of these methods is increasingly more implicit and indirect, and this process is also isolated from the native forward process (NFP), the ground-truth function that models the molecular dynamics. Following this insight, we propose TrustMol, an IMD method built to be trustworthy. For this purpose, TrustMol relies on a set of technical novelties including a new variational autoencoder network. Moreover, we propose a latent-property pairs acquisition method to effectively navigate the complexities of molecular latent optimization, a process that seems intuitive yet challenging due to the high-frequency and discontinuous nature of molecule space. TrustMol also integrates uncertainty-awareness into molecular latent optimization. These lead to improvements in both explainability and reliability of the IMD process. We validate the trustworthiness of TrustMol through a wide range of experiments.","sentences":["Data-driven generation of molecules with desired properties, also known as inverse molecular design (IMD), has attracted significant attention in recent years.","Despite the significant progress in the accuracy and diversity of solutions, existing IMD methods lag behind in terms of trustworthiness.","The root issue is that the design process of these methods is increasingly more implicit and indirect, and this process is also isolated from the native forward process (NFP), the ground-truth function that models the molecular dynamics.","Following this insight, we propose TrustMol, an IMD method built to be trustworthy.","For this purpose, TrustMol relies on a set of technical novelties including a new variational autoencoder network.","Moreover, we propose a latent-property pairs acquisition method to effectively navigate the complexities of molecular latent optimization, a process that seems intuitive yet challenging due to the high-frequency and discontinuous nature of molecule space.","TrustMol also integrates uncertainty-awareness into molecular latent optimization.","These lead to improvements in both explainability and reliability of the IMD process.","We validate the trustworthiness of TrustMol through a wide range of experiments."],"url":"http://arxiv.org/abs/2402.16930v1","category":"physics.chem-ph"}
{"created":"2024-02-26 15:09:56","title":"Differentiable Particle Filtering using Optimal Placement Resampling","abstract":"Particle filters are a frequent choice for inference tasks in nonlinear and non-Gaussian state-space models. They can either be used for state inference by approximating the filtering distribution or for parameter inference by approximating the marginal data (observation) likelihood. A good proposal distribution and a good resampling scheme are crucial to obtain low variance estimates. However, traditional methods like multinomial resampling introduce nondifferentiability in PF-based loss functions for parameter estimation, prohibiting gradient-based learning tasks. This work proposes a differentiable resampling scheme by deterministic sampling from an empirical cumulative distribution function. We evaluate our method on parameter inference tasks and proposal learning.","sentences":["Particle filters are a frequent choice for inference tasks in nonlinear and non-Gaussian state-space models.","They can either be used for state inference by approximating the filtering distribution or for parameter inference by approximating the marginal data (observation) likelihood.","A good proposal distribution and a good resampling scheme are crucial to obtain low variance estimates.","However, traditional methods like multinomial resampling introduce nondifferentiability in PF-based loss functions for parameter estimation, prohibiting gradient-based learning tasks.","This work proposes a differentiable resampling scheme by deterministic sampling from an empirical cumulative distribution function.","We evaluate our method on parameter inference tasks and proposal learning."],"url":"http://arxiv.org/abs/2402.16639v1","category":"cs.LG"}
{"created":"2024-02-26 15:09:17","title":"The structure is the message: preserving experimental context through tensor decomposition","abstract":"Recent biological studies have been revolutionized in scale and granularity by multiplex and high-throughput assays. Profiling cell responses across several experimental parameters, such as perturbations, time, and genetic contexts, leads to richer and more generalizable findings. However, these multidimensional datasets necessitate a reevaluation of the conventional methods for their representation and analysis. Traditionally, experimental parameters are merged to flatten the data into a two-dimensional matrix, sacrificing crucial experiment context reflected by the structure. As Marshall McLuhan famously stated, \"The medium is the message.\" In this work, we propose that the experiment structure is the medium in which subsequent analysis is performed, and the optimal choice of data representation must reflect the experiment structure. We introduce tensor-structured analyses and decompositions to preserve this information. We contend that tensor methods are poised to become integral to the biomedical data sciences toolkit.","sentences":["Recent biological studies have been revolutionized in scale and granularity by multiplex and high-throughput assays.","Profiling cell responses across several experimental parameters, such as perturbations, time, and genetic contexts, leads to richer and more generalizable findings.","However, these multidimensional datasets necessitate a reevaluation of the conventional methods for their representation and analysis.","Traditionally, experimental parameters are merged to flatten the data into a two-dimensional matrix, sacrificing crucial experiment context reflected by the structure.","As Marshall McLuhan famously stated, \"The medium is the message.\"","In this work, we propose that the experiment structure is the medium in which subsequent analysis is performed, and the optimal choice of data representation must reflect the experiment structure.","We introduce tensor-structured analyses and decompositions to preserve this information.","We contend that tensor methods are poised to become integral to the biomedical data sciences toolkit."],"url":"http://arxiv.org/abs/2402.16638v1","category":"q-bio.QM"}
{"created":"2024-02-26 14:53:39","title":"Generalized sparsity-promoting solvers for Bayesian inverse problems: Versatile sparsifying transforms and unknown noise variances","abstract":"Bayesian hierarchical models can provide efficient algorithms for finding sparse solutions to ill-posed inverse problems. The models typically comprise a conditionally Gaussian prior model for the unknown which is augmented by a generalized gamma hyper-prior model for variance hyper-parameters. This investigation generalizes these models and their efficient maximum a posterior (MAP) estimation using the iterative alternating sequential (IAS) algorithm in two ways: (1) General sparsifying transforms: Diverging from conventional methods, our approach permits the use of sparsifying transformations with nontrivial kernels; (2) Unknown noise variances: We treat the noise variance as a random variable that is estimated during the inference procedure. This is important in applications where the noise estimate cannot be accurately estimated a priori. Remarkably, these augmentations neither significantly burden the computational expense of the algorithm nor compromise its efficacy. We include convexity and convergence analysis for the method and demonstrate its efficacy in several numerical experiments.","sentences":["Bayesian hierarchical models can provide efficient algorithms for finding sparse solutions to ill-posed inverse problems.","The models typically comprise a conditionally Gaussian prior model for the unknown which is augmented by a generalized gamma hyper-prior model for variance hyper-parameters.","This investigation generalizes these models and their efficient maximum a posterior (MAP) estimation using the iterative alternating sequential (IAS) algorithm in two ways: (1) General sparsifying transforms: Diverging from conventional methods, our approach permits the use of sparsifying transformations with nontrivial kernels; (2) Unknown noise variances: We treat the noise variance as a random variable that is estimated during the inference procedure.","This is important in applications where the noise estimate cannot be accurately estimated a priori.","Remarkably, these augmentations neither significantly burden the computational expense of the algorithm nor compromise its efficacy.","We include convexity and convergence analysis for the method and demonstrate its efficacy in several numerical experiments."],"url":"http://arxiv.org/abs/2402.16623v1","category":"math.NA"}
{"created":"2024-02-26 14:47:49","title":"IM-based Pilot-assisted Channel Estimation for FTN Signaling HF Communications","abstract":"This paper investigates doubly-selective (i.e., time- and frequency-selective) channel estimation in faster-than-Nyquist (FTN) signaling HF communications. In particular, we propose a novel IM-based channel estimation algorithm for FTN signaling HF communications including pilot sequence placement (PSP) and pilot sequence location identification (PSLI) algorithms. At the transmitter, we propose the PSP algorithm that utilizes the locations of pilot sequences to carry additional information bits, thereby improving the SE of HF communications. HF channels have two non-zero independent fading paths with specific fixed delay spread and frequency spread characteristics as outlined in the Union Radio communication Sector (ITU-R) F.1487 and F.520. Having said that, based on the aforementioned properties of the HF channels and the favorable auto-correlation characteristics of the optimal pilot sequence, we propose a novel PSLI algorithm that effectively identifies the pilot sequence location within a given frame at the receiver. This is achieved by showing that the square of the absolute value of the cross-correlation between the received symbols and the pilot sequence consists of a scaled version of the square of the absolute value of the auto-correlation of the pilot sequence weighted by the gain of the corresponding HF channel path. Simulation results show very low pilot sequence location identification errors for HF channels. Our simulation results show a 6 dB improvement in the MSE of the channel estimation as well as about 3.5 dB BER improvement of FTN signaling along with an enhancement in SE compared to the method in [1]. We also achieved an enhancement in SE compared to the work in [2] while maintaining comparable MSE of the channel estimation and BER performance.","sentences":["This paper investigates doubly-selective (i.e., time- and frequency-selective) channel estimation in faster-than-Nyquist (FTN) signaling HF communications.","In particular, we propose a novel IM-based channel estimation algorithm for FTN signaling HF communications including pilot sequence placement (PSP) and pilot sequence location identification (PSLI) algorithms.","At the transmitter, we propose the PSP algorithm that utilizes the locations of pilot sequences to carry additional information bits, thereby improving the SE of HF communications.","HF channels have two non-zero independent fading paths with specific fixed delay spread and frequency spread characteristics as outlined in the Union Radio communication Sector (ITU-R) F.1487 and F.520.","Having said that, based on the aforementioned properties of the HF channels and the favorable auto-correlation characteristics of the optimal pilot sequence, we propose a novel PSLI algorithm that effectively identifies the pilot sequence location within a given frame at the receiver.","This is achieved by showing that the square of the absolute value of the cross-correlation between the received symbols and the pilot sequence consists of a scaled version of the square of the absolute value of the auto-correlation of the pilot sequence weighted by the gain of the corresponding HF channel path.","Simulation results show very low pilot sequence location identification errors for HF channels.","Our simulation results show a 6 dB improvement in the MSE of the channel estimation as well as about 3.5 dB BER improvement of FTN signaling along with an enhancement in SE compared to the method in [1].","We also achieved an enhancement in SE compared to the work in [2] while maintaining comparable MSE of the channel estimation and BER performance."],"url":"http://arxiv.org/abs/2402.16618v1","category":"cs.IT"}
{"created":"2024-02-26 14:47:13","title":"Quantum process tomography of structured optical gates with convolutional neural networks","abstract":"The characterization of a unitary gate is experimentally accomplished via Quantum Process Tomography, which combines the outcomes of different projective measurements to reconstruct the underlying operator. The process matrix is typically extracted from maximum-likelihood estimation. Recently, optimization strategies based on evolutionary and machine-learning techniques have been proposed. Here, we investigate a deep-learning approach that allows for fast and accurate reconstructions of space-dependent SU(2) operators, only processing a minimal set of measurements. We train a convolutional neural network based on a scalable U-Net architecture to process entire experimental images in parallel. Synthetic processes are reconstructed with average fidelity above 90%. The performance of our routine is experimentally validated on complex polarization transformations. Our approach further expands the toolbox of data-driven approaches to Quantum Process Tomography and shows promise in the real-time characterization of complex optical gates.","sentences":["The characterization of a unitary gate is experimentally accomplished via Quantum Process Tomography, which combines the outcomes of different projective measurements to reconstruct the underlying operator.","The process matrix is typically extracted from maximum-likelihood estimation.","Recently, optimization strategies based on evolutionary and machine-learning techniques have been proposed.","Here, we investigate a deep-learning approach that allows for fast and accurate reconstructions of space-dependent SU(2) operators, only processing a minimal set of measurements.","We train a convolutional neural network based on a scalable U-Net architecture to process entire experimental images in parallel.","Synthetic processes are reconstructed with average fidelity above 90%.","The performance of our routine is experimentally validated on complex polarization transformations.","Our approach further expands the toolbox of data-driven approaches to Quantum Process Tomography and shows promise in the real-time characterization of complex optical gates."],"url":"http://arxiv.org/abs/2402.16616v1","category":"quant-ph"}
{"created":"2024-02-26 14:43:20","title":"Thermodynamic stability and vibrational properties of multi-alkali antimonides","abstract":"Modern advances in generating ultrabright electron beams have unlocked unprecedented experimental advances based on synchrotron radiation. Current challenges lie in improving the quality of electron sources with novel photocathode materials such as alkali-based semiconductors. To unleash their potential, a detailed characterization and prediction of their fundamental properties is essential. In this work, we employ density functional theory combined with machine learning techniques to probe the thermodynamic stability of various alkali antimonide crystals, emphasizing the role of the approximations taken for the exchange-correlation potential. Our results reveal that the SCAN functional offers an optimal trade-off between accuracy and computational costs to describe the vibrational properties of these materials. Furthermore, it is found that systems with a higher concentration of Cs atoms exhibit enhanced anharmonicities, which are accurately predicted and characterized with the employed methodology.","sentences":["Modern advances in generating ultrabright electron beams have unlocked unprecedented experimental advances based on synchrotron radiation.","Current challenges lie in improving the quality of electron sources with novel photocathode materials such as alkali-based semiconductors.","To unleash their potential, a detailed characterization and prediction of their fundamental properties is essential.","In this work, we employ density functional theory combined with machine learning techniques to probe the thermodynamic stability of various alkali antimonide crystals, emphasizing the role of the approximations taken for the exchange-correlation potential.","Our results reveal that the SCAN functional offers an optimal trade-off between accuracy and computational costs to describe the vibrational properties of these materials.","Furthermore, it is found that systems with a higher concentration of Cs atoms exhibit enhanced anharmonicities, which are accurately predicted and characterized with the employed methodology."],"url":"http://arxiv.org/abs/2402.16614v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-26 14:40:15","title":"GEA: Reconstructing Expressive 3D Gaussian Avatar from Monocular Video","abstract":"This paper presents GEA, a novel method for creating expressive 3D avatars with high-fidelity reconstructions of body and hands based on 3D Gaussians. The key contributions are twofold. First, we design a two-stage pose estimation method to obtain an accurate SMPL-X pose from input images, providing a correct mapping between the pixels of a training image and the SMPL-X model. It uses an attention-aware network and an optimization scheme to align the normal and silhouette between the estimated SMPL-X body and the real body in the image. Second, we propose an iterative re-initialization strategy to handle unbalanced aggregation and initialization bias faced by Gaussian representation. This strategy iteratively redistributes the avatar's Gaussian points, making it evenly distributed near the human body surface by applying meshing, resampling and re-Gaussian operations. As a result, higher-quality rendering can be achieved. Extensive experimental analyses validate the effectiveness of the proposed model, demonstrating that it achieves state-of-the-art performance in photorealistic novel view synthesis while offering fine-grained control over the human body and hand pose. Project page: https://3d-aigc.github.io/GEA/.","sentences":["This paper presents GEA, a novel method for creating expressive 3D avatars with high-fidelity reconstructions of body and hands based on 3D Gaussians.","The key contributions are twofold.","First, we design a two-stage pose estimation method to obtain an accurate SMPL-X pose from input images, providing a correct mapping between the pixels of a training image and the SMPL-X model.","It uses an attention-aware network and an optimization scheme to align the normal and silhouette between the estimated SMPL-X body and the real body in the image.","Second, we propose an iterative re-initialization strategy to handle unbalanced aggregation and initialization bias faced by Gaussian representation.","This strategy iteratively redistributes the avatar's Gaussian points, making it evenly distributed near the human body surface by applying meshing, resampling and re-Gaussian operations.","As a result, higher-quality rendering can be achieved.","Extensive experimental analyses validate the effectiveness of the proposed model, demonstrating that it achieves state-of-the-art performance in photorealistic novel view synthesis while offering fine-grained control over the human body and hand pose.","Project page: https://3d-aigc.github.io/GEA/."],"url":"http://arxiv.org/abs/2402.16607v1","category":"cs.CV"}
{"created":"2024-02-26 14:30:34","title":"Saddle Point Search Algorithms for Variational Density Functional Calculations of Excited Electronic States with Self-Interaction Correction","abstract":"Excited electronic states of molecules and solids play a fundamental role in fields such as catalysis and electronics. In electronic structure calculations, excited states typically correspond to saddle points on the surface described by the variation of the energy as a function of the electronic degrees of freedom. A direct optimization algorithm based on generalized mode following is presented for density functional calculations of excited states. While conventional direct optimization methods based on quasi-Newton algorithms usually converge to the stationary point closest to the initial guess, even minima, the generalized mode following approach systematically targets a saddle point of a specific order l by following the l lowest eigenvectors of the electronic Hessian up in energy. This approach thereby recasts the challenging saddle point search as a minimization, enabling the use of efficient and robust minimization algorithms. The initial guess orbitals and the saddle point order of the target excited state solution are evaluated by performing an initial step of constrained optimization freezing the electronic degrees of freedom involved in the excitation. In the context of Kohn-Sham density functional calculations, typical approximations to the exchange-and-correlation functional suffer from a self-interaction error. The Perdew and Zunger self-interaction correction can alleviate this problem, but makes the energy variant to unitary transformations in the occupied orbital space, introducing a large amount of unphysical solutions that do not fully minimize the self-interaction error. An extension of the generalized mode following method is proposed that ensures convergence to the solution minimizing the self-interaction error.","sentences":["Excited electronic states of molecules and solids play a fundamental role in fields such as catalysis and electronics.","In electronic structure calculations, excited states typically correspond to saddle points on the surface described by the variation of the energy as a function of the electronic degrees of freedom.","A direct optimization algorithm based on generalized mode following is presented for density functional calculations of excited states.","While conventional direct optimization methods based on quasi-Newton algorithms usually converge to the stationary point closest to the initial guess, even minima, the generalized mode following approach systematically targets a saddle point of a specific order l by following the l lowest eigenvectors of the electronic Hessian up in energy.","This approach thereby recasts the challenging saddle point search as a minimization, enabling the use of efficient and robust minimization algorithms.","The initial guess orbitals and the saddle point order of the target excited state solution are evaluated by performing an initial step of constrained optimization freezing the electronic degrees of freedom involved in the excitation.","In the context of Kohn-Sham density functional calculations, typical approximations to the exchange-and-correlation functional suffer from a self-interaction error.","The Perdew and Zunger self-interaction correction can alleviate this problem, but makes the energy variant to unitary transformations in the occupied orbital space, introducing a large amount of unphysical solutions that do not fully minimize the self-interaction error.","An extension of the generalized mode following method is proposed that ensures convergence to the solution minimizing the self-interaction error."],"url":"http://arxiv.org/abs/2402.16601v1","category":"physics.chem-ph"}
{"created":"2024-02-26 14:27:06","title":"Semantic change detection for Slovene language: a novel dataset and an approach based on optimal transport","abstract":"In this paper, we focus on the detection of semantic changes in Slovene, a less resourced Slavic language with two million speakers. Detecting and tracking semantic changes provides insights into the evolution of the language caused by changes in society and culture. Recently, several systems have been proposed to aid in this study, but all depend on manually annotated gold standard datasets for evaluation. In this paper, we present the first Slovene dataset for evaluating semantic change detection systems, which contains aggregated semantic change scores for 104 target words obtained from more than 3000 manually annotated sentence pairs. We evaluate several existing semantic change detection methods on this dataset and also propose a novel approach based on optimal transport that improves on the existing state-of-the-art systems with an error reduction rate of 22.8%.","sentences":["In this paper, we focus on the detection of semantic changes in Slovene, a less resourced Slavic language with two million speakers.","Detecting and tracking semantic changes provides insights into the evolution of the language caused by changes in society and culture.","Recently, several systems have been proposed to aid in this study, but all depend on manually annotated gold standard datasets for evaluation.","In this paper, we present the first Slovene dataset for evaluating semantic change detection systems, which contains aggregated semantic change scores for 104 target words obtained from more than 3000 manually annotated sentence pairs.","We evaluate several existing semantic change detection methods on this dataset and also propose a novel approach based on optimal transport that improves on the existing state-of-the-art systems with an error reduction rate of 22.8%."],"url":"http://arxiv.org/abs/2402.16596v1","category":"cs.CL"}
{"created":"2024-02-26 14:14:32","title":"Sounding-Based Evaluation of Multi-Sensor ISAC Networks for Drone Applications: Measurement and Simulation Perspectives","abstract":"With the upcoming multitude of commercial and public applications envisioned in the mobile 6G radio landscape using unmanned aerial vehicles (UAVs), integrated sensing and communication (ISAC) plays a key role to enable the detection and localization of passive objects with radar sensing, while optimizing the utilization of scarce resources. To explore the potential of future ISAC architectures with UAVs as mobile nodes in distributed multi-sensor networks, the system's fundamental capability to detect static and dynamic objects that reveal themselves by their bi-static back-scattering needs to be evaluated. Therefore, this paper addresses simulation- and measurement based data acquisition methods to gather knowledge about the bistatic reflectivity of single objects including their Micro-Doppler signature for object identification as well as the influence of multipath propagation in different environments on the localization accuracy and radar tracking performance. We show exemplary results from simulation models, bi-static reflectivity measurements in laboratory environment and real-flight channel sounding experiments in selected scenarios showcasing the potential of synthetic and measured data sets for development and evaluation of ISAC algorithms. The presented measurement data sets are publicly available to encourage the academic RF community to validate future algorithms using realistic scenarios alongside simulations models.","sentences":["With the upcoming multitude of commercial and public applications envisioned in the mobile 6G radio landscape using unmanned aerial vehicles (UAVs), integrated sensing and communication (ISAC) plays a key role to enable the detection and localization of passive objects with radar sensing, while optimizing the utilization of scarce resources.","To explore the potential of future ISAC architectures with UAVs as mobile nodes in distributed multi-sensor networks, the system's fundamental capability to detect static and dynamic objects that reveal themselves by their bi-static back-scattering needs to be evaluated.","Therefore, this paper addresses simulation- and measurement based data acquisition methods to gather knowledge about the bistatic reflectivity of single objects including their Micro-Doppler signature for object identification as well as the influence of multipath propagation in different environments on the localization accuracy and radar tracking performance.","We show exemplary results from simulation models, bi-static reflectivity measurements in laboratory environment and real-flight channel sounding experiments in selected scenarios showcasing the potential of synthetic and measured data sets for development and evaluation of ISAC algorithms.","The presented measurement data sets are publicly available to encourage the academic RF community to validate future algorithms using realistic scenarios alongside simulations models."],"url":"http://arxiv.org/abs/2402.16591v1","category":"eess.SP"}
{"created":"2024-02-26 14:11:20","title":"Isogeometric analysis of the Laplace eigenvalue problem on circular sectors: Regularity properties, graded meshes & variational crimes","abstract":"The Laplace eigenvalue problem on circular sectors has eigenfunctions with corner singularities. Standard methods may produce suboptimal approximation results. To address this issue, a novel numerical algorithm that enhances standard isogeometric analysis is proposed in this paper by using a single-patch graded mesh refinement scheme. Numerical tests demonstrate optimal convergence rates for both the eigenvalues and eigenfunctions. Furthermore, the results show that smooth splines possess a superior approximation constant compared to their $C^0$-continuous counterparts for the lower part of the Laplace spectrum. This is an extension of previous findings about excellent spectral approximation properties of smooth splines on rectangular domains to circular sectors. In addition, graded meshes prove to be particularly advantageous for an accurate approximation of a limited number of eigenvalues. The novel algorithm applied here has a drawback in the singularity of the isogeometric parameterization. It results in some basis functions not belonging to the solution space of the corresponding weak problem, which is considered a variational crime. However, the approach proves to be robust. Finally, a hierarchical mesh structure is presented to avoid anisotropic elements, omit redundant degrees of freedom and keep the number of basis functions contributing to the variational crime constant, independent of the mesh size. Numerical results validate the effectiveness of hierarchical mesh grading for the simulation of eigenfunctions with and without corner singularities.","sentences":["The Laplace eigenvalue problem on circular sectors has eigenfunctions with corner singularities.","Standard methods may produce suboptimal approximation results.","To address this issue, a novel numerical algorithm that enhances standard isogeometric analysis is proposed in this paper by using a single-patch graded mesh refinement scheme.","Numerical tests demonstrate optimal convergence rates for both the eigenvalues and eigenfunctions.","Furthermore, the results show that smooth splines possess a superior approximation constant compared to their $C^0$-continuous counterparts for the lower part of the Laplace spectrum.","This is an extension of previous findings about excellent spectral approximation properties of smooth splines on rectangular domains to circular sectors.","In addition, graded meshes prove to be particularly advantageous for an accurate approximation of a limited number of eigenvalues.","The novel algorithm applied here has a drawback in the singularity of the isogeometric parameterization.","It results in some basis functions not belonging to the solution space of the corresponding weak problem, which is considered a variational crime.","However, the approach proves to be robust.","Finally, a hierarchical mesh structure is presented to avoid anisotropic elements, omit redundant degrees of freedom and keep the number of basis functions contributing to the variational crime constant, independent of the mesh size.","Numerical results validate the effectiveness of hierarchical mesh grading for the simulation of eigenfunctions with and without corner singularities."],"url":"http://arxiv.org/abs/2402.16589v1","category":"math.NA"}
{"created":"2024-02-26 13:55:34","title":"Optimal shapes for positivity preserving","abstract":"We are looking for an optimal convex domain on which the boundary value problem $$\\left\\{\\begin{array}{cc}(-\\Delta)^2 u_\\gamma-\\gamma\\Delta u_\\gamma = f,& \\mbox{ in }\\Omega\\\\ u_\\gamma=\\partial_\\nu u_\\gamma=0,& \\mbox{ on }\\partial\\Omega\\end{array}\\right.$$ admits a nonnegative solution for the most $\\gamma$, if $f$ is a given nonnegative function.","sentences":["We are looking for an optimal convex domain on which the boundary value problem $$\\left\\{\\begin{array}{cc}(-\\Delta)^2 u_\\gamma-\\gamma\\Delta u_\\gamma = f,& \\mbox{ in }\\Omega\\\\ u_\\gamma=\\partial_\\nu u_\\gamma=0,& \\mbox{ on }\\partial\\Omega\\end{array}\\right.$$ admits a nonnegative solution for the most $\\gamma$, if $f$ is a given nonnegative function."],"url":"http://arxiv.org/abs/2402.16575v1","category":"math.AP"}
{"created":"2024-02-26 13:48:44","title":"Searching a Lightweight Network Architecture for Thermal Infrared Pedestrian Tracking","abstract":"Manually-designed network architectures for thermal infrared pedestrian tracking (TIR-PT) require substantial effort from human experts. Neural networks with ResNet backbones are popular for TIR-PT. However, TIR-PT is a tracking task and more challenging than classification and detection. This paper makes an early attempt to search an optimal network architecture for TIR-PT automatically, employing single-bottom and dual-bottom cells as basic search units and incorporating eight operation candidates within the search space. To expedite the search process, a random channel selection strategy is employed prior to assessing operation candidates. Classification, batch hard triplet, and center loss are jointly used to retrain the searched architecture. The outcome is a high-performance network architecture that is both parameter- and computation-efficient. Extensive experiments proved the effectiveness of the automated method.","sentences":["Manually-designed network architectures for thermal infrared pedestrian tracking (TIR-PT) require substantial effort from human experts.","Neural networks with ResNet backbones are popular for TIR-PT.","However, TIR-PT is a tracking task and more challenging than classification and detection.","This paper makes an early attempt to search an optimal network architecture for TIR-PT automatically, employing single-bottom and dual-bottom cells as basic search units and incorporating eight operation candidates within the search space.","To expedite the search process, a random channel selection strategy is employed prior to assessing operation candidates.","Classification, batch hard triplet, and center loss are jointly used to retrain the searched architecture.","The outcome is a high-performance network architecture that is both parameter- and computation-efficient.","Extensive experiments proved the effectiveness of the automated method."],"url":"http://arxiv.org/abs/2402.16570v1","category":"cs.CV"}
{"created":"2024-02-26 13:43:25","title":"Partial Rankings of Optimizers","abstract":"We introduce a framework for benchmarking optimizers according to multiple criteria over various test functions. Based on a recently introduced union-free generic depth function for partial orders/rankings, it fully exploits the ordinal information and allows for incomparability. Our method describes the distribution of all partial orders/rankings, avoiding the notorious shortcomings of aggregation. This permits to identify test functions that produce central or outlying rankings of optimizers and to assess the quality of benchmarking suites.","sentences":["We introduce a framework for benchmarking optimizers according to multiple criteria over various test functions.","Based on a recently introduced union-free generic depth function for partial orders/rankings, it fully exploits the ordinal information and allows for incomparability.","Our method describes the distribution of all partial orders/rankings, avoiding the notorious shortcomings of aggregation.","This permits to identify test functions that produce central or outlying rankings of optimizers and to assess the quality of benchmarking suites."],"url":"http://arxiv.org/abs/2402.16565v1","category":"cs.LG"}
{"created":"2024-02-26 13:32:38","title":"A randomized algorithm for simultaneously diagonalizing symmetric matrices by congruence","abstract":"A family of symmetric matrices $A_1,\\ldots, A_d$ is SDC (simultaneous diagonalization by congruence) if there is an invertible matrix $X$ such that every $X^T A_k X$ is diagonal. In this work, a novel randomized SDC (RSDC) algorithm is proposed that reduces SDC to a generalized eigenvalue problem by considering two (random) linear combinations of the family. We establish exact recovery: RSDC achieves diagonalization with probability $1$ if the family is exactly SDC. Under a mild regularity assumption, robust recovery is also established: Given a family that is $\\epsilon$-close to SDC then RSDC diagonalizes, with high probability, the family up to an error of norm $\\mathcal{O}(\\epsilon)$. Under a positive definiteness assumption, which often holds in applications, stronger results are established, including a bound on the condition number of the transformation matrix. For practical use, we suggest to combine RSDC with an optimization algorithm. The performance of the resulting method is verified for synthetic data, image separation and EEG analysis tasks. It turns out that our newly developed method outperforms existing optimization-based methods in terms of efficiency while achieving a comparable level of accuracy.","sentences":["A family of symmetric matrices $A_1,\\ldots, A_d$ is SDC (simultaneous diagonalization by congruence) if there is an invertible matrix $X$ such that every $X^T A_k X$ is diagonal.","In this work, a novel randomized SDC (RSDC) algorithm is proposed that reduces SDC to a generalized eigenvalue problem by considering two (random) linear combinations of the family.","We establish exact recovery: RSDC achieves diagonalization with probability $1$ if the family is exactly SDC.","Under a mild regularity assumption, robust recovery is also established: Given a family that is $\\epsilon$-close to SDC then RSDC diagonalizes, with high probability, the family up to an error of norm $\\mathcal{O}(\\epsilon)$. Under a positive definiteness assumption, which often holds in applications, stronger results are established, including a bound on the condition number of the transformation matrix.","For practical use, we suggest to combine RSDC with an optimization algorithm.","The performance of the resulting method is verified for synthetic data, image separation and EEG analysis tasks.","It turns out that our newly developed method outperforms existing optimization-based methods in terms of efficiency while achieving a comparable level of accuracy."],"url":"http://arxiv.org/abs/2402.16557v1","category":"math.NA"}
{"created":"2024-02-26 13:26:34","title":"Contracts with Inspections","abstract":"In the classical principal-agent hidden-action model, a principal delegates the execution of a costly task to an agent for which he can choose among actions with different costs and different success probabilities to accomplish the task. To incentivize the agent to exert effort, the principal can commit to a contract, which is the amount of payment based on the task's success. A crucial assumption of this model is that the principal can only base the payment on the outcome but not on the agent's chosen action.   In this work, we relax the hidden-action assumption and introduce a new model where the principal is allowed to inspect subsets of actions at some cost that depends on the inspected subset. If the principal discovers that the agent did not select the agreed-upon action through the inspection, the principal can withhold payment. This relaxation of the model introduces a broader strategy space for the principal, who now faces a tradeoff between positive incentives (increasing payment) and negative incentives (increasing inspection).   We show how to find the best deterministic incentive-compatible inspection scheme for all monotone inspection cost functions. We then turn to randomized inspection schemes and show that one can efficiently find the best randomized incentive-compatible inspection scheme when the inspection cost function is submodular. We complement this result by showing that it is impossible to efficiently find the optimal randomized inspection scheme for the more general case of XOS inspection cost functions.","sentences":["In the classical principal-agent hidden-action model, a principal delegates the execution of a costly task to an agent for which he can choose among actions with different costs and different success probabilities to accomplish the task.","To incentivize the agent to exert effort, the principal can commit to a contract, which is the amount of payment based on the task's success.","A crucial assumption of this model is that the principal can only base the payment on the outcome but not on the agent's chosen action.   ","In this work, we relax the hidden-action assumption and introduce a new model where the principal is allowed to inspect subsets of actions at some cost that depends on the inspected subset.","If the principal discovers that the agent did not select the agreed-upon action through the inspection, the principal can withhold payment.","This relaxation of the model introduces a broader strategy space for the principal, who now faces a tradeoff between positive incentives (increasing payment) and negative incentives (increasing inspection).   ","We show how to find the best deterministic incentive-compatible inspection scheme for all monotone inspection cost functions.","We then turn to randomized inspection schemes and show that one can efficiently find the best randomized incentive-compatible inspection scheme when the inspection cost function is submodular.","We complement this result by showing that it is impossible to efficiently find the optimal randomized inspection scheme for the more general case of XOS inspection cost functions."],"url":"http://arxiv.org/abs/2402.16553v1","category":"cs.GT"}
{"created":"2024-02-26 13:01:45","title":"Model-based deep reinforcement learning for accelerated learning from flow simulations","abstract":"In recent years, deep reinforcement learning has emerged as a technique to solve closed-loop flow control problems. Employing simulation-based environments in reinforcement learning enables a priori end-to-end optimization of the control system, provides a virtual testbed for safety-critical control applications, and allows to gain a deep understanding of the control mechanisms. While reinforcement learning has been applied successfully in a number of rather simple flow control benchmarks, a major bottleneck toward real-world applications is the high computational cost and turnaround time of flow simulations. In this contribution, we demonstrate the benefits of model-based reinforcement learning for flow control applications. Specifically, we optimize the policy by alternating between trajectories sampled from flow simulations and trajectories sampled from an ensemble of environment models. The model-based learning reduces the overall training time by up to $85\\%$ for the fluidic pinball test case. Even larger savings are expected for more demanding flow simulations.","sentences":["In recent years, deep reinforcement learning has emerged as a technique to solve closed-loop flow control problems.","Employing simulation-based environments in reinforcement learning enables a priori end-to-end optimization of the control system, provides a virtual testbed for safety-critical control applications, and allows to gain a deep understanding of the control mechanisms.","While reinforcement learning has been applied successfully in a number of rather simple flow control benchmarks, a major bottleneck toward real-world applications is the high computational cost and turnaround time of flow simulations.","In this contribution, we demonstrate the benefits of model-based reinforcement learning for flow control applications.","Specifically, we optimize the policy by alternating between trajectories sampled from flow simulations and trajectories sampled from an ensemble of environment models.","The model-based learning reduces the overall training time by up to $85\\%$ for the fluidic pinball test case.","Even larger savings are expected for more demanding flow simulations."],"url":"http://arxiv.org/abs/2402.16543v1","category":"physics.flu-dyn"}
{"created":"2024-02-26 12:59:20","title":"Integer Programming Using A Single Atom","abstract":"Integer programming (IP), as the name suggests is an integer-variable-based approach commonly used to formulate real-world optimization problems with constraints. Currently, quantum algorithms reformulate the IP into an unconstrained form through the use of binary variables, which is an indirect and resource-consuming way of solving it. We develop an algorithm that maps and solves an IP problem in its original form to any quantum system that possesses a large number of accessible internal degrees of freedom which can be controlled with sufficient accuracy. Using a single Rydberg atom as an example, we associate the integer values to electronic states belonging to different manifolds and implement a selective superposition of these different states to solve the full IP problem. The optimal solution is found within 2-40{\\mu}s for a few prototypical IP problems with up to eight variables and up to four constraints including a non-linear IP problem, which is usually harder to solve with classical algorithms when compared with linear IP problems. Our algorithm for solving IP is benchmarked using the Branch & Bound approach and it outperforms the classical algorithm in terms of the number of steps needed to converge and carries the potential to improve the bounds provided by the classical algorithm for larger problems.","sentences":["Integer programming (IP), as the name suggests is an integer-variable-based approach commonly used to formulate real-world optimization problems with constraints.","Currently, quantum algorithms reformulate the IP into an unconstrained form through the use of binary variables, which is an indirect and resource-consuming way of solving it.","We develop an algorithm that maps and solves an IP problem in its original form to any quantum system that possesses a large number of accessible internal degrees of freedom which can be controlled with sufficient accuracy.","Using a single Rydberg atom as an example, we associate the integer values to electronic states belonging to different manifolds and implement a selective superposition of these different states to solve the full IP problem.","The optimal solution is found within 2-40{\\mu}s for a few prototypical IP problems with up to eight variables and up to four constraints including a non-linear IP problem, which is usually harder to solve with classical algorithms when compared with linear IP problems.","Our algorithm for solving IP is benchmarked using the Branch & Bound approach and it outperforms the classical algorithm in terms of the number of steps needed to converge and carries the potential to improve the bounds provided by the classical algorithm for larger problems."],"url":"http://arxiv.org/abs/2402.16541v1","category":"quant-ph"}
{"created":"2024-02-26 12:51:43","title":"Tests of Macrorealism in Discrete and Continuous Variable Systems","abstract":"I study several aspects of tests of macrorealism (MR), which for a given data set serves to give a quantitative signal of the presence of a specific notion of non-classical behaviour. The insufficiency of classical understanding underpins both the paradoxes of quantum mechanics, its future technological promise, and so these tests are of interest both foundationally and pragmatically. I derive generalisations of the Leggett-Garg (LG) inequalities and Fine's theorem, which together establish the necessary and sufficient conditions for macrorealism. First, I extend these conditions to tests involving an arbitrary number of measurement times. Secondly, I generalise them beyond the standard dichotomic variable, to systems described by many-valued variables. I also perform a quantum mechanical analysis examining the interplay of different conditions of MR. I then develop the theoretical framework to support tests of macrorealism in continuous variable systems, where I define variables based on coarse-grainings of position. I calculate temporal correlators for general bound systems, and analyse LG violations within the quantum harmonic oscillator (QHO), in its energy eigenstates and coherent states. I analyse the precise physical mechanisms underpinning the violations in terms of probability currents, Bohm trajectories. Staying within continuous variable systems, we outline a different approach to meeting the invasiveness requirement of LG tests. Reasoning that we may approximately non-invasively measure whether a particle crosses the axis, we measure an object which is related to the standard correlators, and derive a set of macrorealistic inequalities for these modified correlators. We demonstrate violations of these modified LG inequalities for several states within the QHO.","sentences":["I study several aspects of tests of macrorealism (MR), which for a given data set serves to give a quantitative signal of the presence of a specific notion of non-classical behaviour.","The insufficiency of classical understanding underpins both the paradoxes of quantum mechanics, its future technological promise, and so these tests are of interest both foundationally and pragmatically.","I derive generalisations of the Leggett-Garg (LG) inequalities and Fine's theorem, which together establish the necessary and sufficient conditions for macrorealism.","First, I extend these conditions to tests involving an arbitrary number of measurement times.","Secondly, I generalise them beyond the standard dichotomic variable, to systems described by many-valued variables.","I also perform a quantum mechanical analysis examining the interplay of different conditions of MR.","I then develop the theoretical framework to support tests of macrorealism in continuous variable systems, where I define variables based on coarse-grainings of position.","I calculate temporal correlators for general bound systems, and analyse LG violations within the quantum harmonic oscillator (QHO), in its energy eigenstates and coherent states.","I analyse the precise physical mechanisms underpinning the violations in terms of probability currents, Bohm trajectories.","Staying within continuous variable systems, we outline a different approach to meeting the invasiveness requirement of LG tests.","Reasoning that we may approximately non-invasively measure whether a particle crosses the axis, we measure an object which is related to the standard correlators, and derive a set of macrorealistic inequalities for these modified correlators.","We demonstrate violations of these modified LG inequalities for several states within the QHO."],"url":"http://arxiv.org/abs/2402.16537v1","category":"quant-ph"}
{"created":"2024-02-26 12:26:06","title":"On the directional asymptotic approach in optimization theory","abstract":"As a starting point of our research, we show that, for a fixed order $\\gamma\\geq 1$, each local minimizer of a rather general nonsmooth optimization problem in Euclidean spaces is either M-stationary in the classical sense (corresponding to stationarity of order $1$), satisfies stationarity conditions in terms of a coderivative construction of order $\\gamma$, or is asymptotically stationary with respect to a critical direction as well as order $\\gamma$ in a certain sense. By ruling out the latter case with a constraint qualification not stronger than directional metric subregularity, we end up with new necessary optimality conditions comprising a mixture of limiting variational tools of orders $1$ and $\\gamma$. These abstract findings are carved out for the broad class of geometric constraints and $\\gamma:=2$, and visualized by examples from complementarity-constrained and nonlinear semidefinite optimization. As a byproduct of the particular setting $\\gamma:=1$, our general approach yields new so-called directional asymptotic regularity conditions which serve as constraint qualifications guaranteeing M-stationarity of local minimizers. We compare these new regularity conditions with standard constraint qualifications from nonsmooth optimization. Further, we extend directional concepts of pseudo- and quasi-normality to arbitrary set-valued mappings. It is shown that these properties provide sufficient conditions for the validity of directional asymptotic regularity. Finally, a novel coderivative-like variational tool is used to construct sufficient conditions for the presence of directional asymptotic regularity. For geometric constraints, it is illustrated that all appearing objects can be calculated in terms of initial problem data.","sentences":["As a starting point of our research, we show that, for a fixed order $\\gamma\\geq 1$, each local minimizer of a rather general nonsmooth optimization problem in Euclidean spaces is either M-stationary in the classical sense (corresponding to stationarity of order $1$), satisfies stationarity conditions in terms of a coderivative construction of order $\\gamma$, or is asymptotically stationary with respect to a critical direction as well as order $\\gamma$ in a certain sense.","By ruling out the latter case with a constraint qualification not stronger than directional metric subregularity, we end up with new necessary optimality conditions comprising a mixture of limiting variational tools of orders $1$ and $\\gamma$.","These abstract findings are carved out for the broad class of geometric constraints and $\\gamma:=2$, and visualized by examples from complementarity-constrained and nonlinear semidefinite optimization.","As a byproduct of the particular setting $\\gamma:=1$, our general approach yields new so-called directional asymptotic regularity conditions which serve as constraint qualifications guaranteeing M-stationarity of local minimizers.","We compare these new regularity conditions with standard constraint qualifications from nonsmooth optimization.","Further, we extend directional concepts of pseudo- and quasi-normality to arbitrary set-valued mappings.","It is shown that these properties provide sufficient conditions for the validity of directional asymptotic regularity.","Finally, a novel coderivative-like variational tool is used to construct sufficient conditions for the presence of directional asymptotic regularity.","For geometric constraints, it is illustrated that all appearing objects can be calculated in terms of initial problem data."],"url":"http://arxiv.org/abs/2402.16530v1","category":"math.OC"}
{"created":"2024-02-26 12:25:56","title":"Black hole in a combined magnetic field: ionized accretion disks in the jetlike and looplike configurations","abstract":"Magnetic fields surrounding black holes are responsible for various astrophysical phenomena related to accretion processes and relativistic jets. Depending on the source, the configuration of the field lines may differ significantly, affecting the trajectories of charged particles and the corresponding observables. Usually, the magnetic fields around black holes are modeled within a single source or current generating the field. However, magnetic fields can have more than a single origin, being a combination of different fields, such as, e.g., that of an accretion disk and external large-scale or Galactic ones. In this paper, we propose a combined magnetic field solution given by the superposition of the uniform and Blandford-Znajek split-monopole magnetic fields in a strong gravity regime of the Schwarzschild black hole. We show that when the combined magnetic field components are aligned, the resulting field is of a paraboloidal jetlike shape. Such a configuration is supported by relativistic jet observations and is often utilized in general relativistic magnetohydrodynamical simulations. In the opposite orientation of the two field components, we observe looplike field structures magnetically connecting the black hole with an accretion disk and the magnetic null points, which can be related to the regions of magnetic reconnection. In the combined magnetic field configurations, we analyze the dynamics of charged particles, study their stability conditions, and find the locations of stable off-equatorial structures close to the symmetry axis. We consider an ionization of Keplerian accretion disk as a particular scenario of particle scattering. From the numerical experiments, we conclude that charged particles in the jetlike combination show a strong tendency to escape from the black hole. In contrast, the looplike combination supports accretion of charged particles into the black hole.","sentences":["Magnetic fields surrounding black holes are responsible for various astrophysical phenomena related to accretion processes and relativistic jets.","Depending on the source, the configuration of the field lines may differ significantly, affecting the trajectories of charged particles and the corresponding observables.","Usually, the magnetic fields around black holes are modeled within a single source or current generating the field.","However, magnetic fields can have more than a single origin, being a combination of different fields, such as, e.g., that of an accretion disk and external large-scale or Galactic ones.","In this paper, we propose a combined magnetic field solution given by the superposition of the uniform and Blandford-Znajek split-monopole magnetic fields in a strong gravity regime of the Schwarzschild black hole.","We show that when the combined magnetic field components are aligned, the resulting field is of a paraboloidal jetlike shape.","Such a configuration is supported by relativistic jet observations and is often utilized in general relativistic magnetohydrodynamical simulations.","In the opposite orientation of the two field components, we observe looplike field structures magnetically connecting the black hole with an accretion disk and the magnetic null points, which can be related to the regions of magnetic reconnection.","In the combined magnetic field configurations, we analyze the dynamics of charged particles, study their stability conditions, and find the locations of stable off-equatorial structures close to the symmetry axis.","We consider an ionization of Keplerian accretion disk as a particular scenario of particle scattering.","From the numerical experiments, we conclude that charged particles in the jetlike combination show a strong tendency to escape from the black hole.","In contrast, the looplike combination supports accretion of charged particles into the black hole."],"url":"http://arxiv.org/abs/2402.16529v1","category":"astro-ph.HE"}
