{"created":"2024-04-25 17:59:59","title":"The Black-Hole Masses of High-Redshift QSOs","abstract":"Observations of high-redshift quasars frequently promote suggestions of large black hole masses, whose presence so early in cosmic time is not easily explicable. I consider the parallel with ultraluminous X-ray sources (ULXs) -- now known to be stellar-mass black hole (and neutron star) binaries apparently radiating far above their Eddington luminosities $L_{\\rm Edd}$. The true luminosity in ULXs is actually only of order $L_{\\rm Edd}$, for {\\it stellar-mass} accretors, but has a very anisotropic (`beamed') component, plus a near-isotropic component of similar luminosity but much lower specific intensity. Observers viewing ULXs from within the beam but assuming spherical symmetry deduce a luminosity $\\gg L_{\\rm Edd}$. These features appear because the accretors are fed mass at highly super-Eddington rates, most of it expelled in high-speed ($v >0.2c$) outflows from the accretion disc.   I show that in similarly-beamed AGN, emission-line properties would be essentially the same as in unbeamed sources, but standard virial mass indicators unusable because velocity widths are dominated by the outflows, not bound motions about the black holes. In an ensemble of this kind the apparently most luminous systems are always the most distant, but have the lowest black hole masses. Interpreting observations of this ensemble without knowing that they are beamed leads instead to very high black hole mass estimates. The analogy with ULXs therefore suggests that high-redshift quasars might actually have central black hole masses which could have grown from stellar values within the lookback time. I consider how one might test these ideas observationally.","sentences":["Observations of high-redshift quasars frequently promote suggestions of large black hole masses, whose presence so early in cosmic time is not easily explicable.","I consider the parallel with ultraluminous X-ray sources (ULXs) -- now known to be stellar-mass black hole (and neutron star) binaries apparently radiating far above their Eddington luminosities $L_{\\rm Edd}$. The true luminosity in ULXs is actually only of order $L_{\\rm Edd}$, for {\\it stellar-mass} accretors, but has a very anisotropic (`beamed') component, plus a near-isotropic component of similar luminosity but much lower specific intensity.","Observers viewing ULXs from within the beam but assuming spherical symmetry deduce a luminosity $\\gg L_{\\rm Edd}$. These features appear because the accretors are fed mass at highly super-Eddington rates, most of it expelled in high-speed ($v >0.2c$) outflows from the accretion disc.   ","I show that in similarly-beamed AGN, emission-line properties would be essentially the same as in unbeamed sources, but standard virial mass indicators unusable because velocity widths are dominated by the outflows, not bound motions about the black holes.","In an ensemble of this kind the apparently most luminous systems are always the most distant, but have the lowest black hole masses.","Interpreting observations of this ensemble without knowing that they are beamed leads instead to very high black hole mass estimates.","The analogy with ULXs therefore suggests that high-redshift quasars might actually have central black hole masses which could have grown from stellar values within the lookback time.","I consider how one might test these ideas observationally."],"url":"http://arxiv.org/abs/2404.16832v1","category":"astro-ph.GA"}
{"created":"2024-04-25 17:59:46","title":"ResVR: Joint Rescaling and Viewport Rendering of Omnidirectional Images","abstract":"With the advent of virtual reality technology, omnidirectional image (ODI) rescaling techniques are increasingly embraced for reducing transmitted and stored file sizes while preserving high image quality. Despite this progress, current ODI rescaling methods predominantly focus on enhancing the quality of images in equirectangular projection (ERP) format, which overlooks the fact that the content viewed on head mounted displays (HMDs) is actually a rendered viewport instead of an ERP image. In this work, we emphasize that focusing solely on ERP quality results in inferior viewport visual experiences for users. Thus, we propose ResVR, which is the first comprehensive framework for the joint Rescaling and Viewport Rendering of ODIs. ResVR allows obtaining LR ERP images for transmission while rendering high-quality viewports for users to watch on HMDs. In our ResVR, a novel discrete pixel sampling strategy is developed to tackle the complex mapping between the viewport and ERP, enabling end-to-end training of ResVR pipeline. Furthermore, a spherical pixel shape representation technique is innovatively derived from spherical differentiation to significantly improve the visual quality of rendered viewports. Extensive experiments demonstrate that our ResVR outperforms existing methods in viewport rendering tasks across different fields of view, resolutions, and view directions while keeping a low transmission overhead.","sentences":["With the advent of virtual reality technology, omnidirectional image (ODI) rescaling techniques are increasingly embraced for reducing transmitted and stored file sizes while preserving high image quality.","Despite this progress, current ODI rescaling methods predominantly focus on enhancing the quality of images in equirectangular projection (ERP) format, which overlooks the fact that the content viewed on head mounted displays (HMDs) is actually a rendered viewport instead of an ERP image.","In this work, we emphasize that focusing solely on ERP quality results in inferior viewport visual experiences for users.","Thus, we propose ResVR, which is the first comprehensive framework for the joint Rescaling and Viewport Rendering of ODIs.","ResVR allows obtaining LR ERP images for transmission while rendering high-quality viewports for users to watch on HMDs.","In our ResVR, a novel discrete pixel sampling strategy is developed to tackle the complex mapping between the viewport and ERP, enabling end-to-end training of ResVR pipeline.","Furthermore, a spherical pixel shape representation technique is innovatively derived from spherical differentiation to significantly improve the visual quality of rendered viewports.","Extensive experiments demonstrate that our ResVR outperforms existing methods in viewport rendering tasks across different fields of view, resolutions, and view directions while keeping a low transmission overhead."],"url":"http://arxiv.org/abs/2404.16825v1","category":"cs.CV"}
{"created":"2024-04-25 17:58:16","title":"Ordered and disordered stealthy hyperuniform point patterns across spatial dimensions","abstract":"In previous work [Phys. Rev. X 5, 021020 (2015)], it was shown that stealthy hyperuniform systems can be regarded as hard spheres in Fourier-space in the sense that the the structure factor is exactly zero in a spherical region around the origin in analogy with the pair-correlation function of real-space hard spheres. In this work, we exploit this correspondence to confirm that the densest Fourier-space hard-sphere system is that of a Bravais lattice. This is in contrast to real-space hard-spheres, whose densest configuration is conjectured to be disordered. We also extend the virial series previously suggested for disordered stealthy hyperuniform systems to higher dimensions in order to predict spatial decorrelation as function of dimension. This prediction is then borne out by numerical simulations of disordered stealthy hyperuniform ground states in dimensions $d=2$-$8$.","sentences":["In previous work [Phys. Rev. X 5, 021020 (2015)], it was shown that stealthy hyperuniform systems can be regarded as hard spheres in Fourier-space in the sense that the the structure factor is exactly zero in a spherical region around the origin in analogy with the pair-correlation function of real-space hard spheres.","In this work, we exploit this correspondence to confirm that the densest Fourier-space hard-sphere system is that of a Bravais lattice.","This is in contrast to real-space hard-spheres, whose densest configuration is conjectured to be disordered.","We also extend the virial series previously suggested for disordered stealthy hyperuniform systems to higher dimensions in order to predict spatial decorrelation as function of dimension.","This prediction is then borne out by numerical simulations of disordered stealthy hyperuniform ground states in dimensions $d=2$-$8$."],"url":"http://arxiv.org/abs/2404.16819v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-25 17:54:54","title":"The Spectrum of $\\mathbb{Q}$-Isotropic Binary Quadratic Forms","abstract":"We give a complete list of the points in the spectrum $$\\mathcal{Z}=\\{\\inf_{(x,y)\\in\\Lambda,xy\\neq0}{\\left\\vert xy\\right\\vert},\\,\\text{$\\Lambda$ is a unimodular rational lattice of $\\mathbb{R}^2$}\\}$$ above $\\frac{1}{3}.$ We further show that the set of limit points of $\\mathcal{Z}$ with values larger than $\\frac{1}{3},$ is equal to the set $\\{\\frac{2m}{\\sqrt{9m^2-4}+3m},\\text{ where $m$ is a Markoff number}\\}$.","sentences":["We give a complete list of the points in the spectrum $$\\mathcal{Z}=\\{\\inf_{(x,y)\\in\\Lambda,xy\\neq0}{\\left\\vert xy\\right\\vert},\\,\\text{$\\Lambda$ is a unimodular rational lattice of $\\mathbb{R}^2$}\\}$$ above $\\frac{1}{3}.$ We further show that the set of limit points of $\\mathcal{Z}$ with values larger than $\\frac{1}{3},$ is equal to the set $\\{\\frac{2m}{\\sqrt{9m^2-4}+3m},\\text{ where $m$ is a Markoff number}\\}$."],"url":"http://arxiv.org/abs/2404.16810v1","category":"math.NT"}
{"created":"2024-04-25 17:53:26","title":"Enhancing nanocrystal superlattice self-assembly near a metastable liquid binodal","abstract":"Bottom-up assembly of nanocrystals (NCs) into ordered arrays, or superlattices (SLs), is a promising route to design materials with new functionalities, but the degree of control over assembly into functional structures remains challenging. Using electrostatics, rather than density, to tune the interactions between semiconductor NCs, we watch self-assembly proceeding through a metastable liquid phase. We systematically investigate the phase behavior as a function of quench conditions in situ and in real time using small angle X-ray scattering (SAXS). Through quantitative fitting to colloid, liquid, and SL models, we extract the time evolution of each phase and the system phase diagram, which we find to be consistent with short-range attractive interactions. Using the phase diagram's predictive power, we establish control of the self-assembly rate over three orders of magnitude, and identify one- and two-step self-assembly regimes, with only the latter implicating the metastable liquid as an intermediate. Importantly, the presence of the metastable liquid increases SL formation rates relative to the equivalent one-step pathway, and SL order counterintuitively increases with the rate, revealing a highly desirable and generalizable kinetic strategy to promote and enhance ordered assembly.","sentences":["Bottom-up assembly of nanocrystals (NCs) into ordered arrays, or superlattices (SLs), is a promising route to design materials with new functionalities, but the degree of control over assembly into functional structures remains challenging.","Using electrostatics, rather than density, to tune the interactions between semiconductor NCs, we watch self-assembly proceeding through a metastable liquid phase.","We systematically investigate the phase behavior as a function of quench conditions in situ and in real time using small angle X-ray scattering (SAXS).","Through quantitative fitting to colloid, liquid, and SL models, we extract the time evolution of each phase and the system phase diagram, which we find to be consistent with short-range attractive interactions.","Using the phase diagram's predictive power, we establish control of the self-assembly rate over three orders of magnitude, and identify one-","and two-step self-assembly regimes, with only the latter implicating the metastable liquid as an intermediate.","Importantly, the presence of the metastable liquid increases SL formation rates relative to the equivalent one-step pathway, and SL order counterintuitively increases with the rate, revealing a highly desirable and generalizable kinetic strategy to promote and enhance ordered assembly."],"url":"http://arxiv.org/abs/2404.16808v1","category":"cond-mat.soft"}
{"created":"2024-04-25 17:52:10","title":"Simple tunable phase-locked lasers for quantum technologies","abstract":"In a wide range of quantum technology applications, ranging from atomic clocks to the creation of ultracold or quantum degenerate samples for atom interferometry, optimal laser sources are critical. In particular, two phase-locked laser sources with a precise difference frequency are needed for efficient coherent population trapping (CPT) clocks, gray molasses laser cooling, or driving Raman transitions. Here we show how a simple cost-effective laser diode can selectively amplify only one sideband of a fiber-electrooptically-modulated seed laser to produce moderate-power phase-locked light with sub-Hz relative linewidth and tunable difference frequencies up to $\\approx 15\\,$GHz. The architecture is readily scalable to multiple phase-locked lasers and could conceivably be used for future on-chip compact phase-locked laser systems for quantum technologies.","sentences":["In a wide range of quantum technology applications, ranging from atomic clocks to the creation of ultracold or quantum degenerate samples for atom interferometry, optimal laser sources are critical.","In particular, two phase-locked laser sources with a precise difference frequency are needed for efficient coherent population trapping (CPT) clocks, gray molasses laser cooling, or driving Raman transitions.","Here we show how a simple cost-effective laser diode can selectively amplify only one sideband of a fiber-electrooptically-modulated seed laser to produce moderate-power phase-locked light with sub-Hz relative linewidth and tunable difference frequencies up to $\\approx 15\\,$GHz.","The architecture is readily scalable to multiple phase-locked lasers and could conceivably be used for future on-chip compact phase-locked laser systems for quantum technologies."],"url":"http://arxiv.org/abs/2404.16806v1","category":"physics.atom-ph"}
{"created":"2024-04-25 17:46:55","title":"The Directed Landscape is a Black Noise","abstract":"We show that the directed landscape is a black noise in the sense of Tsirelson and Vershik. As a corollary, we show that for any microscopic system in which the height profile converges in law to the directed landscape, the driving noise is asymptotically independent of the height profile. This decoupling result provides one answer to the question of what happens to the driving noise in the limit under the KPZ scaling, and illustrates a type of noise sensitivity for systems in the KPZ universality class. Such decoupling and sensitivity phenomena are not present in the intermediate-disorder or weak-asymmetry regime, thus illustrating a contrast from the weak KPZ scaling regime. Along the way, we prove a strong mixing property for the directed landscape on a bounded time interval under spatial shifts, with a mixing rate $\\alpha(N)\\leq Ce^{-dN^3}$ for some $C,d>0$.","sentences":["We show that the directed landscape is a black noise in the sense of Tsirelson and Vershik.","As a corollary, we show that for any microscopic system in which the height profile converges in law to the directed landscape, the driving noise is asymptotically independent of the height profile.","This decoupling result provides one answer to the question of what happens to the driving noise in the limit under the KPZ scaling, and illustrates a type of noise sensitivity for systems in the KPZ universality class.","Such decoupling and sensitivity phenomena are not present in the intermediate-disorder or weak-asymmetry regime, thus illustrating a contrast from the weak KPZ scaling regime.","Along the way, we prove a strong mixing property for the directed landscape on a bounded time interval under spatial shifts, with a mixing rate $\\alpha(N)\\leq Ce^{-dN^3}$ for some $C,d>0$."],"url":"http://arxiv.org/abs/2404.16801v1","category":"math.PR"}
{"created":"2024-04-25 17:39:52","title":"A Communication- and Memory-Aware Model for Load Balancing Tasks","abstract":"While load balancing in distributed-memory computing has been well-studied, we present an innovative approach to this problem: a unified, reduced-order model that combines three key components to describe \"work\" in a distributed system: computation, communication, and memory. Our model enables an optimizer to explore complex tradeoffs in task placement, such as increased parallelism at the expense of data replication, which increases memory usage. We propose a fully distributed, heuristic-based load balancing optimization algorithm, and demonstrate that it quickly finds close-to-optimal solutions. We formalize the complex optimization problem as a mixed-integer linear program, and compare it to our strategy. Finally, we show that when applied to an electromagnetics code, our approach obtains up to 2.3x speedups for the imbalanced execution.","sentences":["While load balancing in distributed-memory computing has been well-studied, we present an innovative approach to this problem: a unified, reduced-order model that combines three key components to describe \"work\" in a distributed system: computation, communication, and memory.","Our model enables an optimizer to explore complex tradeoffs in task placement, such as increased parallelism at the expense of data replication, which increases memory usage.","We propose a fully distributed, heuristic-based load balancing optimization algorithm, and demonstrate that it quickly finds close-to-optimal solutions.","We formalize the complex optimization problem as a mixed-integer linear program, and compare it to our strategy.","Finally, we show that when applied to an electromagnetics code, our approach obtains up to 2.3x speedups for the imbalanced execution."],"url":"http://arxiv.org/abs/2404.16793v1","category":"cs.DC"}
{"created":"2024-04-25 17:34:40","title":"Enhancing Quality of Experience in Telecommunication Networks: A Review of Frameworks and Machine Learning Algorithms","abstract":"The Internet service provider industry is currently experiencing intense competition as companies strive to provide top-notch services to their customers. Providers are introducing cutting-edge technologies to enhance service quality, understanding that their survival depends on the level of service they offer. However, evaluating service quality is a complex task. A crucial aspect of this evaluation lies in understanding user experience, which significantly impacts the success and reputation of a service or product. Ensuring a seamless and positive user experience is essential for attracting and retaining customers. To date, much effort has been devoted to developing tools for measuring Quality of Experience (QoE), which incorporate both subjective and objective criteria. These tools, available in closed and open-source formats, are accessible to organizations and contribute to improving user experience quality. This review article delves into recent research and initiatives aimed at creating frameworks for assessing user QoE. It also explores the integration of machine learning algorithms to enhance these tools for future advancements. Additionally, the article examines current challenges and envisions future directions in the development of these measurement tools.","sentences":["The Internet service provider industry is currently experiencing intense competition as companies strive to provide top-notch services to their customers.","Providers are introducing cutting-edge technologies to enhance service quality, understanding that their survival depends on the level of service they offer.","However, evaluating service quality is a complex task.","A crucial aspect of this evaluation lies in understanding user experience, which significantly impacts the success and reputation of a service or product.","Ensuring a seamless and positive user experience is essential for attracting and retaining customers.","To date, much effort has been devoted to developing tools for measuring Quality of Experience (QoE), which incorporate both subjective and objective criteria.","These tools, available in closed and open-source formats, are accessible to organizations and contribute to improving user experience quality.","This review article delves into recent research and initiatives aimed at creating frameworks for assessing user QoE. It also explores the integration of machine learning algorithms to enhance these tools for future advancements.","Additionally, the article examines current challenges and envisions future directions in the development of these measurement tools."],"url":"http://arxiv.org/abs/2404.16787v1","category":"cs.NI"}
{"created":"2024-04-25 17:19:18","title":"Threshold and frequency properties of a cold ytterbium laser","abstract":"We investigate properties of the lasing action observed on the 1S0--3P1 intercombination transition of ytterbium atoms that are laser-cooled and -trapped inside a high-finesse cavity. The dressing of the atomic states on the 1S0--1P1 transition by the magneto-optical trap (MOT) laser light allows the coupled atom-cavity system to lase, via a two-photon transition, on the same line on which it is pumped. The observation and basic description of this phenomenon was presented earlier by Gothe et al. [Phys. Rev. A 99, 013415 (2019)]. In the current work, we focus on a detailed analysis of the lasing threshold and frequency properties and perform a comparison to our theoretical models.","sentences":["We investigate properties of the lasing action observed on the 1S0--3P1 intercombination transition of ytterbium atoms that are laser-cooled and -trapped inside a high-finesse cavity.","The dressing of the atomic states on the 1S0--1P1 transition by the magneto-optical trap (MOT) laser light allows the coupled atom-cavity system to lase, via a two-photon transition, on the same line on which it is pumped.","The observation and basic description of this phenomenon was presented earlier by Gothe et al.","[Phys. Rev.","A 99, 013415 (2019)].","In the current work, we focus on a detailed analysis of the lasing threshold and frequency properties and perform a comparison to our theoretical models."],"url":"http://arxiv.org/abs/2404.16765v1","category":"quant-ph"}
{"created":"2024-04-25 17:19:13","title":"Dataset of Quotation Attribution in German News Articles","abstract":"Extracting who says what to whom is a crucial part in analyzing human communication in today's abundance of data such as online news articles. Yet, the lack of annotated data for this task in German news articles severely limits the quality and usability of possible systems. To remedy this, we present a new, freely available, creative-commons-licensed dataset for quotation attribution in German news articles based on WIKINEWS. The dataset provides curated, high-quality annotations across 1000 documents (250,000 tokens) in a fine-grained annotation schema enabling various downstream uses for the dataset. The annotations not only specify who said what but also how, in which context, to whom and define the type of quotation. We specify our annotation schema, describe the creation of the dataset and provide a quantitative analysis. Further, we describe suitable evaluation metrics, apply two existing systems for quotation attribution, discuss their results to evaluate the utility of our dataset and outline use cases of our dataset in downstream tasks.","sentences":["Extracting who says what to whom is a crucial part in analyzing human communication in today's abundance of data such as online news articles.","Yet, the lack of annotated data for this task in German news articles severely limits the quality and usability of possible systems.","To remedy this, we present a new, freely available, creative-commons-licensed dataset for quotation attribution in German news articles based on WIKINEWS.","The dataset provides curated, high-quality annotations across 1000 documents (250,000 tokens) in a fine-grained annotation schema enabling various downstream uses for the dataset.","The annotations not only specify who said what but also how, in which context, to whom and define the type of quotation.","We specify our annotation schema, describe the creation of the dataset and provide a quantitative analysis.","Further, we describe suitable evaluation metrics, apply two existing systems for quotation attribution, discuss their results to evaluate the utility of our dataset and outline use cases of our dataset in downstream tasks."],"url":"http://arxiv.org/abs/2404.16764v1","category":"cs.CL"}
{"created":"2024-04-25 17:17:32","title":"Analysis of Ethanol Blending Effects on Auto-Ignition and Heat Release in n-Heptane/Ethanol Non-Premixed Flames","abstract":"This study delves into the auto-ignition temperature of n-heptane and ethanol mixtures within a counterflow flame configuration under low strain rate, with a particular focus on the impact of ethanol blending on heat release rates. Employing the sensitivity analysis method inspired by Zurada's sensitivity approach for neural network, this study identifies the group of critical species influencing the heat release rate. Further analysis concentration change reveals the intricate interactions among these various radicals across different temperature zones. It is found that, in n-heptane dominant mixtures, inhibition of low-temperature chemistry (LTC) caused by additional ethanol, impacts heat release rate at high temperature zone through diffusion effect of specific radicals such as CH2O, C2H4, C3H6 and H2O2. For ethanol-dominant mixtures, an increase in heat release rate was observed with higher ethanol fraction. Further concentration change analysis elucidated it is primarily attributed to the decomposition of ethanol and its subsequent reactions. This research underscores the significance of incorporating both chemical kinetics and species diffusion effects when analyzing the counterflow configuration of complex fuel mixtures.","sentences":["This study delves into the auto-ignition temperature of n-heptane and ethanol mixtures within a counterflow flame configuration under low strain rate, with a particular focus on the impact of ethanol blending on heat release rates.","Employing the sensitivity analysis method inspired by Zurada's sensitivity approach for neural network, this study identifies the group of critical species influencing the heat release rate.","Further analysis concentration change reveals the intricate interactions among these various radicals across different temperature zones.","It is found that, in n-heptane dominant mixtures, inhibition of low-temperature chemistry (LTC) caused by additional ethanol, impacts heat release rate at high temperature zone through diffusion effect of specific radicals such as CH2O, C2H4, C3H6 and H2O2.","For ethanol-dominant mixtures, an increase in heat release rate was observed with higher ethanol fraction.","Further concentration change analysis elucidated it is primarily attributed to the decomposition of ethanol and its subsequent reactions.","This research underscores the significance of incorporating both chemical kinetics and species diffusion effects when analyzing the counterflow configuration of complex fuel mixtures."],"url":"http://arxiv.org/abs/2404.16762v1","category":"physics.chem-ph"}
{"created":"2024-04-25 17:15:17","title":"Compact almost automorphic dynamics of non-autonomous differential equations with exponential dichotomy and applications to biological models with delay","abstract":"In the present work, we prove that, if $A(\\cdot)$ is a compact almost automorphic matrix and the system $$x'(t) = A(t)x(t)\\, ,$$ possesses an exponential dichotomy with Green function $G(\\cdot, \\cdot)$, then its associated system $$y'(t) = B(t)y(t)\\, ,$$ where $B(\\cdot) \\in H(A)$ (the hull of $A(\\cdot)$) also possesses an exponential dichotomy. Moreover, the Green function $G(\\cdot, \\cdot)$ is compact Bi-almost automorphic in $\\mathbb{R}^2$, this implies that $G(\\cdot, \\cdot)$ is $\\Delta_2$ - like uniformly continuous, where $\\Delta_2$ is the principal diagonal of $\\mathbb{R}^2$, an important ingredient in the proof of invariance of the compact almost automorphic function space under convolution product with kernel $G(\\cdot, \\cdot)$. Finally, we study the existence of a positive compact almost automorphic solution of non-autonomous differential equations of biological interest having non-linear harvesting terms and mixed delays.","sentences":["In the present work, we prove that, if $A(\\cdot)$ is a compact almost automorphic matrix and the system $$x'(t)","= A(t)x(t)\\, ,$$ possesses an exponential dichotomy with Green function $G(\\cdot, \\cdot)$, then its associated system $$y'(t) = B(t)y(t)\\, ,$$ where $B(\\cdot) \\in H(A)$ (the hull of $A(\\cdot)$) also possesses an exponential dichotomy.","Moreover, the Green function $G(\\cdot, \\cdot)$ is compact Bi-almost automorphic in $\\mathbb{R}^2$, this implies that $G(\\cdot, \\cdot)$ is $\\Delta_2$ - like uniformly continuous, where $\\Delta_2$ is the principal diagonal of $\\mathbb{R}^2$, an important ingredient in the proof of invariance of the compact almost automorphic function space under convolution product with kernel $G(\\cdot,","\\cdot)$. Finally, we study the existence of a positive compact almost automorphic solution of non-autonomous differential equations of biological interest having non-linear harvesting terms and mixed delays."],"url":"http://arxiv.org/abs/2404.16758v1","category":"math.DS"}
{"created":"2024-04-25 17:12:32","title":"Second-order adiabatic expansions of heat and charge currents with nonequilibrium Green's functions","abstract":"Due to technological needs, nanoscale heat management, energy conversion and quantum thermodynamics have become key areas of research, putting heat pumps and nanomotors center stage. The treatment of these particular systems often requires the use of adiabatic expansions in terms of the frequency of the external driving or the velocity of some classical degree of freedom. However, due to the difficulty of getting the expressions, most works have only explored first-order terms. Despite this, adiabatic expansions have allowed the study of intriguing phenomena such as adiabatic quantum pumps and motors, or electronic friction. Here, we use nonequilibrium Green's functions, within a Schwinger-Keldysh approach, to develop second-order expressions for the energy, heat, and charge currents. We illustrate, through two simple models, how the obtained formulas produce physically consistent results, and allow for the thermodynamic study of unexplored phenomena, such as second-order monoparametric pumping.","sentences":["Due to technological needs, nanoscale heat management, energy conversion and quantum thermodynamics have become key areas of research, putting heat pumps and nanomotors center stage.","The treatment of these particular systems often requires the use of adiabatic expansions in terms of the frequency of the external driving or the velocity of some classical degree of freedom.","However, due to the difficulty of getting the expressions, most works have only explored first-order terms.","Despite this, adiabatic expansions have allowed the study of intriguing phenomena such as adiabatic quantum pumps and motors, or electronic friction.","Here, we use nonequilibrium Green's functions, within a Schwinger-Keldysh approach, to develop second-order expressions for the energy, heat, and charge currents.","We illustrate, through two simple models, how the obtained formulas produce physically consistent results, and allow for the thermodynamic study of unexplored phenomena, such as second-order monoparametric pumping."],"url":"http://arxiv.org/abs/2404.16757v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-25 17:05:55","title":"On the global dynamics of a forest model with monotone positive feedback and memory","abstract":"We continue to study (see arXiv:2401.08618, https://doi.org/10.48550/arXiv.2401.08618) a renewal equation $\\phi(t)=\\frak F\\phi_t$ proposed in [C. Barril et al., J. Math. Biology, https://doi.org/10.1007/s00285-024-02084-x] to model trees growth. This time we are considering the case when the per capita reproduction rate $\\beta(x)$ is a non-monotone (unimodal) function of tree's height $x$. Note that the height of some species of trees can impact negatively seed viability, in a kind of autogamy depression. Similarly to previous works, it is also assumed that the growth rate $g(x)$ of an individual of height $x$ is a strictly decreasing function. Here we analyse the connection between dynamics of the associated one-dimensional map $F(b)= {\\frak F}b,$ $b \\in {\\mathbb R}_+$, and the delayed (hence infinite-dimensional) model $\\phi(t)=\\frak F\\phi_t$. Our key observation is that this model is of monotone positive feedback type since $F$ is strictly increasing on ${\\mathbb R}_+$ independently on the monotonicity properties of $\\beta$.","sentences":["We continue to study (see arXiv:2401.08618, https://doi.org/10.48550/arXiv.2401.08618) a renewal equation $\\phi(t)=\\frak F\\phi_t$ proposed in [C. Barril et al., J. Math.","Biology, https://doi.org/10.1007/s00285-024-02084-x] to model trees growth.","This time we are considering the case when the per capita reproduction rate $\\beta(x)$ is a non-monotone (unimodal) function of tree's height $x$.","Note that the height of some species of trees can impact negatively seed viability, in a kind of autogamy depression.","Similarly to previous works, it is also assumed that the growth rate $g(x)$ of an individual of height $x$ is a strictly decreasing function.","Here we analyse the connection between dynamics of the associated one-dimensional map $F(b)= {\\frak F}b,$ $b \\in {\\mathbb R}_+$, and the delayed (hence infinite-dimensional) model $\\phi(t)=\\frak F\\phi_t$.","Our key observation is that this model is of monotone positive feedback type since $F$ is strictly increasing on ${\\mathbb R}_+$ independently on the monotonicity properties of $\\beta$."],"url":"http://arxiv.org/abs/2404.16749v1","category":"math.DS"}
{"created":"2024-04-25 17:04:42","title":"Characterizing Solar Center-to-Limb Radial-Velocity Variability with SDO","abstract":"Stellar photospheric inhomogeneities are a significant source of noise which currently precludes the discovery of Earth-mass planets orbiting Sun-like stars with the radial-velocity (RV) method. To complement several previous studies which have used ground- and spaced-based facilities to characterize the RV of the Sun, we here characterize the center-to-limb variability (CLV) of solar RVs arising from various solar-surface inhomogeneities observed by SDO/HMI and SDO/AIA. By using various SDO observables to classify pixels and calculate line-of-sight velocities as a function of pixel classification and limb angle, we show that each identified feature type, including the umbrae and penumbrae of sunspots, quiet-Sun magnetoconvective cells, magnetic network, and plage, exhibit distinct and complex CLV signatures, including a notable limb-angle dependence in the observed suppression of convective blueshift for magnetically active regions. We discuss the observed distributions of velocities by identified region type and limb angle, offer interpretations of the physical phenomena that shape these distributions, and emphasize the need to understand the RV signatures of these regions as astrophysical signals, rather than simple (un)correlated noise processes.","sentences":["Stellar photospheric inhomogeneities are a significant source of noise which currently precludes the discovery of Earth-mass planets orbiting Sun-like stars with the radial-velocity (RV) method.","To complement several previous studies which have used ground- and spaced-based facilities to characterize the RV of the Sun, we here characterize the center-to-limb variability (CLV) of solar RVs arising from various solar-surface inhomogeneities observed by SDO/HMI and SDO/AIA.","By using various SDO observables to classify pixels and calculate line-of-sight velocities as a function of pixel classification and limb angle, we show that each identified feature type, including the umbrae and penumbrae of sunspots, quiet-Sun magnetoconvective cells, magnetic network, and plage, exhibit distinct and complex CLV signatures, including a notable limb-angle dependence in the observed suppression of convective blueshift for magnetically active regions.","We discuss the observed distributions of velocities by identified region type and limb angle, offer interpretations of the physical phenomena that shape these distributions, and emphasize the need to understand the RV signatures of these regions as astrophysical signals, rather than simple (un)correlated noise processes."],"url":"http://arxiv.org/abs/2404.16747v1","category":"astro-ph.SR"}
{"created":"2024-04-25 17:00:08","title":"JITScanner: Just-in-Time Executable Page Check in the Linux Operating System","abstract":"Modern malware poses a severe threat to cybersecurity, continually evolving in sophistication. To combat this threat, researchers and security professionals continuously explore advanced techniques for malware detection and analysis. Dynamic analysis, a prevalent approach, offers advantages over static analysis by enabling observation of runtime behavior and detecting obfuscated or encrypted code used to evade detection. However, executing programs within a controlled environment can be resource-intensive, often necessitating compromises, such as limiting sandboxing to an initial period. In our article, we propose an alternative method for dynamic executable analysis: examining the presence of malicious signatures within executable virtual pages precisely when their current content, including any updates over time, is accessed for instruction fetching. Our solution, named JITScanner, is developed as a Linux-oriented package built upon a Loadable Kernel Module (LKM). It integrates a user-level component that communicates efficiently with the LKM using scalable multi-processor/core technology. JITScanner's effectiveness in detecting malware programs and its minimal intrusion in normal runtime scenarios have been extensively tested, with the experiment results detailed in this article. These experiments affirm the viability of our approach, showcasing JITScanner's capability to effectively identify malware while minimizing runtime overhead.","sentences":["Modern malware poses a severe threat to cybersecurity, continually evolving in sophistication.","To combat this threat, researchers and security professionals continuously explore advanced techniques for malware detection and analysis.","Dynamic analysis, a prevalent approach, offers advantages over static analysis by enabling observation of runtime behavior and detecting obfuscated or encrypted code used to evade detection.","However, executing programs within a controlled environment can be resource-intensive, often necessitating compromises, such as limiting sandboxing to an initial period.","In our article, we propose an alternative method for dynamic executable analysis: examining the presence of malicious signatures within executable virtual pages precisely when their current content, including any updates over time, is accessed for instruction fetching.","Our solution, named JITScanner, is developed as a Linux-oriented package built upon a Loadable Kernel Module (LKM).","It integrates a user-level component that communicates efficiently with the LKM using scalable multi-processor/core technology.","JITScanner's effectiveness in detecting malware programs and its minimal intrusion in normal runtime scenarios have been extensively tested, with the experiment results detailed in this article.","These experiments affirm the viability of our approach, showcasing JITScanner's capability to effectively identify malware while minimizing runtime overhead."],"url":"http://arxiv.org/abs/2404.16744v1","category":"cs.CR"}
{"created":"2024-04-25 16:50:59","title":"Bayesian Nonparametric Inference in McKean-Vlasov models","abstract":"We consider nonparametric statistical inference on a periodic interaction potential $W$ from noisy discrete space-time measurements of solutions $\\rho=\\rho_W$ of the nonlinear McKean-Vlasov equation, describing the probability density of the mean field limit of an interacting particle system. We show how Gaussian process priors assigned to $W$ give rise to posterior mean estimators that exhibit fast convergence rates for the implied estimated densities $\\bar \\rho$ towards $\\rho_W$. We further show that if the initial condition $\\phi$ is not too smooth and satisfies a standard deconvolvability condition, then one can consistently infer the potential $W$ itself at convergence rates $N^{-\\theta}$ for appropriate $\\theta>0$, where $N$ is the number of measurements. The exponent $\\theta$ can be taken to approach $1/2$ as the regularity of $W$ increases corresponding to `near-parametric' models.","sentences":["We consider nonparametric statistical inference on a periodic interaction potential $W$ from noisy discrete space-time measurements of solutions $\\rho=\\rho_W$ of the nonlinear McKean-Vlasov equation, describing the probability density of the mean field limit of an interacting particle system.","We show how Gaussian process priors assigned to $W$ give rise to posterior mean estimators that exhibit fast convergence rates for the implied estimated densities $\\bar \\rho$ towards $\\rho_W$. We further show that if the initial condition $\\phi$ is not too smooth and satisfies a standard deconvolvability condition, then one can consistently infer the potential $W$ itself at convergence rates $N^{-\\theta}$ for appropriate $\\theta>0$, where $N$ is the number of measurements.","The exponent $\\theta$ can be taken to approach $1/2$ as the regularity of $W$ increases corresponding to `near-parametric' models."],"url":"http://arxiv.org/abs/2404.16742v1","category":"math.ST"}
{"created":"2024-04-25 16:50:54","title":"Parameterized Complexity of Efficient Sortation","abstract":"A crucial challenge arising in the design of large-scale logistical networks is to optimize parcel sortation for routing. We study this problem under the recent graph-theoretic formalization of Van Dyk, Klause, Koenemann and Megow (IPCO 2024). The problem asks - given an input digraph D (the fulfillment network) together with a set of commodities represented as source-sink tuples - for a minimum-outdegree subgraph H of the transitive closure of D that contains a source-sink route for each of the commodities. Given the underlying motivation, we study two variants of the problem which differ in whether the routes for the commodities are assumed to be given, or can be chosen arbitrarily.   We perform a thorough parameterized analysis of the complexity of both problems. Our results concentrate on three fundamental parameterizations of the problem: (1) When attempting to parameterize by the target outdegree of H, we show that the problems are paraNP-hard even in highly restricted cases; (2) When parameterizing by the number of commodities, we utilize Ramsey-type arguments, kernelization and treewidth reduction techniques to obtain parameterized algorithms for both problems; (3) When parameterizing by the structure of D, we establish fixed-parameter tractability for both problems w.r.t. treewidth, maximum degree and the maximum routing length. We combine this with lower bounds which show that omitting any of the three parameters results in paraNP-hardness.","sentences":["A crucial challenge arising in the design of large-scale logistical networks is to optimize parcel sortation for routing.","We study this problem under the recent graph-theoretic formalization of Van Dyk, Klause, Koenemann and Megow (IPCO 2024).","The problem asks - given an input digraph D (the fulfillment network) together with a set of commodities represented as source-sink tuples - for a minimum-outdegree subgraph H of the transitive closure of D that contains a source-sink route for each of the commodities.","Given the underlying motivation, we study two variants of the problem which differ in whether the routes for the commodities are assumed to be given, or can be chosen arbitrarily.   ","We perform a thorough parameterized analysis of the complexity of both problems.","Our results concentrate on three fundamental parameterizations of the problem: (1) When attempting to parameterize by the target outdegree of H, we show that the problems are paraNP-hard even in highly restricted cases; (2) When parameterizing by the number of commodities, we utilize Ramsey-type arguments, kernelization and treewidth reduction techniques to obtain parameterized algorithms for both problems; (3) When parameterizing by the structure of D, we establish fixed-parameter tractability for both problems w.r.t",". treewidth, maximum degree and the maximum routing length.","We combine this with lower bounds which show that omitting any of the three parameters results in paraNP-hardness."],"url":"http://arxiv.org/abs/2404.16741v1","category":"cs.DS"}
{"created":"2024-04-25 16:43:25","title":"Uniform Substitution for Differential Refinement Logic","abstract":"This paper introduces a uniform substitution calculus for differential refinement logic dRL. The logic dRL extends the differential dynamic logic dL such that one can simultaneously reason about properties of and relations between hybrid systems. Refinements is useful e.g. for simplifying proofs by relating a concrete hybrid system to an abstract one from which the property can be proved more easily. Uniform substitution is the key to parsimonious prover microkernels. It enables the verbatim use of single axiom formulas instead of axiom schemata with soundness-critical side conditions scattered across the proof calculus. The uniform substitution rule can then be used to instantiate all axioms soundly. Access to differential variables in dRL enables more control over the notion of refinement, which is shown to be decidable on a fragment of hybrid programs.","sentences":["This paper introduces a uniform substitution calculus for differential refinement logic dRL.","The logic dRL extends the differential dynamic logic dL such that one can simultaneously reason about properties of and relations between hybrid systems.","Refinements is useful e.g. for simplifying proofs by relating a concrete hybrid system to an abstract one from which the property can be proved more easily.","Uniform substitution is the key to parsimonious prover microkernels.","It enables the verbatim use of single axiom formulas instead of axiom schemata with soundness-critical side conditions scattered across the proof calculus.","The uniform substitution rule can then be used to instantiate all axioms soundly.","Access to differential variables in dRL enables more control over the notion of refinement, which is shown to be decidable on a fragment of hybrid programs."],"url":"http://arxiv.org/abs/2404.16734v1","category":"cs.LO"}
{"created":"2024-04-25 16:41:57","title":"Non-asymptotic Global Convergence Analysis of BFGS with the Armijo-Wolfe Line Search","abstract":"In this paper, we establish the first explicit and non-asymptotic global convergence analysis of the BFGS method when deployed with an inexact line search scheme that satisfies the Armijo-Wolfe conditions. We show that BFGS achieves a global convergence rate of $(1-\\frac{1}{\\kappa})^k$ for $\\mu$-strongly convex functions with $L$-Lipschitz gradients, where $\\kappa=\\frac{L}{\\mu}$ denotes the condition number. Furthermore, if the objective function's Hessian is Lipschitz, BFGS with the Armijo-Wolfe line search achieves a linear convergence rate only determined by the line search parameters and independent of the condition number. These results hold for any initial point $x_0$ and any symmetric positive definite initial Hessian approximation matrix $B_0$, although the choice of $B_0$ affects the iteration count required to attain these rates. Specifically, we show that for $B_0 = LI$, the rate of $O((1-\\frac{1}{\\kappa})^k)$ appears from the first iteration, while for $B_0 = \\mu I$, it takes $d\\log \\kappa$ iterations. Conversely, the condition number-independent linear convergence rate for $B_0 = LI$ occurs after $O\\left(\\kappa\\left(d +\\frac{M \\sqrt{f(x_0)-f(x_*)}}{\\mu^{3/2}}\\right)\\right)$ iterations, whereas for $B_0 = \\mu I$, it holds after $O\\left(\\frac{M \\sqrt{f(x_0)-f(x_*)}}{\\mu^{3/2}}\\left(d\\log \\kappa + \\kappa\\right)\\right)$ iterations. Here, $d$ denotes the dimension of the problem, $M$ is the Lipschitz parameter of the Hessian, and $x_*$ denotes the optimal solution. We further leverage these global linear convergence results to characterize the overall iteration complexity of BFGS when deployed with the Armijo-Wolfe line search.","sentences":["In this paper, we establish the first explicit and non-asymptotic global convergence analysis of the BFGS method when deployed with an inexact line search scheme that satisfies the Armijo-Wolfe conditions.","We show that BFGS achieves a global convergence rate of $(1-\\frac{1}{\\kappa})^k$ for $\\mu$-strongly convex functions with $L$-Lipschitz gradients, where $\\kappa=\\frac{L}{\\mu}$ denotes the condition number.","Furthermore, if the objective function's Hessian is Lipschitz, BFGS with the Armijo-Wolfe line search achieves a linear convergence rate only determined by the line search parameters and independent of the condition number.","These results hold for any initial point $x_0$ and any symmetric positive definite initial Hessian approximation matrix $B_0$, although the choice of $B_0$ affects the iteration count required to attain these rates.","Specifically, we show that for $B_0 = LI$, the rate of $O((1-\\frac{1}{\\kappa})^k)$ appears from the first iteration, while for $B_0 = \\mu I$, it takes $d\\log \\kappa$ iterations.","Conversely, the condition number-independent linear convergence rate for $B_0 = LI$ occurs after $O\\left(\\kappa\\left(d +\\frac{M \\sqrt{f(x_0)-f(x_*)}}{\\mu^{3/2}}\\right)\\right)$ iterations, whereas for $B_0 = \\mu I$, it holds after $O\\left(\\frac{M \\sqrt{f(x_0)-f(x_*)}}{\\mu^{3/2}}\\left(d\\log \\kappa + \\kappa\\right)\\right)$ iterations.","Here, $d$ denotes the dimension of the problem, $M$ is the Lipschitz parameter of the Hessian, and $x_*$ denotes the optimal solution.","We further leverage these global linear convergence results to characterize the overall iteration complexity of BFGS when deployed with the Armijo-Wolfe line search."],"url":"http://arxiv.org/abs/2404.16731v1","category":"math.OC"}
{"created":"2024-04-25 16:41:12","title":"Finch: Sparse and Structured Array Programming with Control Flow","abstract":"From FORTRAN to NumPy, arrays have revolutionized how we express computation. However, arrays in these, and almost all prominent systems, can only handle dense rectilinear integer grids. Real world arrays often contain underlying structure, such as sparsity, runs of repeated values, or symmetry. Support for structured data is fragmented and incomplete. Existing frameworks limit the array structures and program control flow they support to better simplify the problem.   In this work, we propose a new programming language, Finch, which supports both flexible control flow and diverse data structures. Finch facilitates a programming model which resolves the challenges of computing over structured arrays by combining control flow and data structures into a common representation where they can be co-optimized. Finch automatically specializes control flow to data so that performance engineers can focus on experimenting with many algorithms. Finch supports a familiar programming language of loops, statements, ifs, breaks, etc., over a wide variety of array structures, such as sparsity, run-length-encoding, symmetry, triangles, padding, or blocks. Finch reliably utilizes the key properties of structure, such as structural zeros, repeated values, or clustered non-zeros. We show that this leads to dramatic speedups in operations such as SpMV and SpGEMM, image processing, graph analytics, and a high-level tensor operator fusion interface.","sentences":["From FORTRAN to NumPy, arrays have revolutionized how we express computation.","However, arrays in these, and almost all prominent systems, can only handle dense rectilinear integer grids.","Real world arrays often contain underlying structure, such as sparsity, runs of repeated values, or symmetry.","Support for structured data is fragmented and incomplete.","Existing frameworks limit the array structures and program control flow they support to better simplify the problem.   ","In this work, we propose a new programming language, Finch, which supports both flexible control flow and diverse data structures.","Finch facilitates a programming model which resolves the challenges of computing over structured arrays by combining control flow and data structures into a common representation where they can be co-optimized.","Finch automatically specializes control flow to data so that performance engineers can focus on experimenting with many algorithms.","Finch supports a familiar programming language of loops, statements, ifs, breaks, etc., over a wide variety of array structures, such as sparsity, run-length-encoding, symmetry, triangles, padding, or blocks.","Finch reliably utilizes the key properties of structure, such as structural zeros, repeated values, or clustered non-zeros.","We show that this leads to dramatic speedups in operations such as SpMV and SpGEMM, image processing, graph analytics, and a high-level tensor operator fusion interface."],"url":"http://arxiv.org/abs/2404.16730v1","category":"cs.MS"}
{"created":"2024-04-25 16:34:26","title":"Clique Is Hard on Average for Sherali-Adams with Bounded Coefficients","abstract":"We prove that Sherali-Adams with polynomially bounded coefficients requires proofs of size $n^{\\Omega(d)}$ to rule out the existence of an $n^{\\Theta(1)}$-clique in Erd\\H{o}s-R\\'{e}nyi random graphs whose maximum clique is of size $d\\leq 2\\log n$. This lower bound is tight up to the multiplicative constant in the exponent. We obtain this result by introducing a technique inspired by pseudo-calibration which may be of independent interest. The technique involves defining a measure on monomials that precisely captures the contribution of a monomial to a refutation. This measure intuitively captures progress and should have further applications in proof complexity.","sentences":["We prove that Sherali-Adams with polynomially bounded coefficients requires proofs of size $n^{\\Omega(d)}$ to rule out the existence of an $n^{\\Theta(1)}$-clique in Erd\\H{o}s-R\\'{e}nyi random graphs whose maximum clique is of size $d\\leq 2\\log n$. This lower bound is tight up to the multiplicative constant in the exponent.","We obtain this result by introducing a technique inspired by pseudo-calibration which may be of independent interest.","The technique involves defining a measure on monomials that precisely captures the contribution of a monomial to a refutation.","This measure intuitively captures progress and should have further applications in proof complexity."],"url":"http://arxiv.org/abs/2404.16722v1","category":"cs.CC"}
{"created":"2024-04-25 16:22:12","title":"Distributed MPC for PWA Systems Based on Switching ADMM","abstract":"This paper presents a novel approach for distributed model predictive control (MPC) for piecewise affine (PWA) systems. Existing approaches rely on solving mixed-integer optimization problems, requiring significant computation power or time. We propose a distributed MPC scheme that requires solving only convex optimization problems. The key contribution is a novel method, based on the alternating direction method of multipliers, for solving the non-convex optimal control problem that arises due to the PWA dynamics. We present a distributed MPC scheme, leveraging this method, that explicitly accounts for the coupling between subsystems by reaching agreement on the values of coupled states. Stability and recursive feasibility are shown under additional assumptions on the underlying system. Two numerical examples are provided, in which the proposed controller is shown to significantly improve the CPU time and closed-loop performance over existing state-of-the-art approaches.","sentences":["This paper presents a novel approach for distributed model predictive control (MPC) for piecewise affine (PWA) systems.","Existing approaches rely on solving mixed-integer optimization problems, requiring significant computation power or time.","We propose a distributed MPC scheme that requires solving only convex optimization problems.","The key contribution is a novel method, based on the alternating direction method of multipliers, for solving the non-convex optimal control problem that arises due to the PWA dynamics.","We present a distributed MPC scheme, leveraging this method, that explicitly accounts for the coupling between subsystems by reaching agreement on the values of coupled states.","Stability and recursive feasibility are shown under additional assumptions on the underlying system.","Two numerical examples are provided, in which the proposed controller is shown to significantly improve the CPU time and closed-loop performance over existing state-of-the-art approaches."],"url":"http://arxiv.org/abs/2404.16712v1","category":"math.OC"}
{"created":"2024-04-25 16:17:23","title":"Understanding Reliability from a Regression Perspective","abstract":"Reliability is an important quantification of measurement precision based on a latent variable measurement model. Inspired by McDonald (2011), we present a regression framework of reliability, placing emphasis on whether latent or observed scores serve as the regression outcome. Our theory unifies two extant perspectives of reliability: (a) classical test theory (measurement decomposition), and (b) optimal prediction of latent scores (prediction decomposition). Importantly, reliability should be treated as a property of the observed score under a measurement decomposition, but a property of the latent score under a prediction decomposition. To facilitate the evaluation and interpretation of distinct reliability coefficients for complex measurement models, we introduce a Monte Carlo approach for approximate calculation of reliability. We illustrate the proposed computational procedure with an empirical data analysis, which concerns measuring susceptibility and severity of depressive symptoms using a two-dimensional item response theory model. We conclude with a discussion on computing reliability coefficients and outline future avenues of research.","sentences":["Reliability is an important quantification of measurement precision based on a latent variable measurement model.","Inspired by McDonald (2011), we present a regression framework of reliability, placing emphasis on whether latent or observed scores serve as the regression outcome.","Our theory unifies two extant perspectives of reliability: (a) classical test theory (measurement decomposition), and (b) optimal prediction of latent scores (prediction decomposition).","Importantly, reliability should be treated as a property of the observed score under a measurement decomposition, but a property of the latent score under a prediction decomposition.","To facilitate the evaluation and interpretation of distinct reliability coefficients for complex measurement models, we introduce a Monte Carlo approach for approximate calculation of reliability.","We illustrate the proposed computational procedure with an empirical data analysis, which concerns measuring susceptibility and severity of depressive symptoms using a two-dimensional item response theory model.","We conclude with a discussion on computing reliability coefficients and outline future avenues of research."],"url":"http://arxiv.org/abs/2404.16709v1","category":"stat.ME"}
{"created":"2024-04-25 16:07:55","title":"Fidelity and criticality in the nonreciprocal Aubry-Andr{\u00e9}-Harper model","abstract":"We study the critical behaviors of the ground and first excited states in the one-dimensional nonreciprocal Aubry-Andr{\\'e}-Harper model using both the self-normal and biorthogonal fidelity susceptibilities. We demonstrate that fidelity susceptibilities serve as a probe for the phase transition in the nonreciprocal AAH model. For ground states, characterized by real eigenenergies across the entire regime, both fidelity susceptibilities near the critical points scale as $N^{2}$, akin to the Hermitian AAH model. However, for the first-excited states, where $\\mathcal{PT}$ transitions occur, the fidelity susceptibilities exhibit distinct scaling laws, contingent upon whether the lattice consists of even or odd sites. For even lattices, the self-normal fidelity susceptibilities near the critical points continue to scale as $N^{2}$. For odd lattices, the biorthogonal fidelity susceptibilities diverge, while the self-normal fidelity susceptibilities exhibit linear behavior, indicating a novel scaling law.","sentences":["We study the critical behaviors of the ground and first excited states in the one-dimensional nonreciprocal Aubry-Andr{\\'e}-Harper model using both the self-normal and biorthogonal fidelity susceptibilities.","We demonstrate that fidelity susceptibilities serve as a probe for the phase transition in the nonreciprocal AAH model.","For ground states, characterized by real eigenenergies across the entire regime, both fidelity susceptibilities near the critical points scale as $N^{2}$, akin to the Hermitian AAH model.","However, for the first-excited states, where $\\mathcal{PT}$ transitions occur, the fidelity susceptibilities exhibit distinct scaling laws, contingent upon whether the lattice consists of even or odd sites.","For even lattices, the self-normal fidelity susceptibilities near the critical points continue to scale as $N^{2}$. For odd lattices, the biorthogonal fidelity susceptibilities diverge, while the self-normal fidelity susceptibilities exhibit linear behavior, indicating a novel scaling law."],"url":"http://arxiv.org/abs/2404.16704v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-25 16:02:44","title":"On the Streaming Complexity of Expander Decomposition","abstract":"In this paper we study the problem of finding $(\\epsilon, \\phi)$-expander decompositions of a graph in the streaming model, in particular for dynamic streams of edge insertions and deletions. The goal is to partition the vertex set so that every component induces a $\\phi$-expander, while the number of inter-cluster edges is only an $\\epsilon$ fraction of the total volume. It was recently shown that there exists a simple algorithm to construct a $(O(\\phi \\log n), \\phi)$-expander decomposition of an $n$-vertex graph using $\\widetilde{O}(n/\\phi^2)$ bits of space [Filtser, Kapralov, Makarov, ITCS'23]. This result calls for understanding the extent to which a dependence in space on the sparsity parameter $\\phi$ is inherent. We move towards answering this question on two fronts. We prove that a $(O(\\phi \\log n), \\phi)$-expander decomposition can be found using $\\widetilde{O}(n)$ space, for every $\\phi$. At the core of our result is the first streaming algorithm for computing boundary-linked expander decompositions, a recently introduced strengthening of the classical notion [Goranci et al., SODA'21]. The key advantage is that a classical sparsifier [Fung et al., STOC'11], with size independent of $\\phi$, preserves the cuts inside the clusters of a boundary-linked expander decomposition within a multiplicative error. Notable algorithmic applications use sequences of expander decompositions, in particular one often repeatedly computes a decomposition of the subgraph induced by the inter-cluster edges (e.g., the seminal work of Spielman and Teng on spectral sparsifiers [Spielman, Teng, SIAM Journal of Computing 40(4)], or the recent maximum flow breakthrough [Chen et al., FOCS'22], among others). We prove that any streaming algorithm that computes a sequence of $(O(\\phi \\log n), \\phi)$-expander decompositions requires ${\\widetilde{\\Omega}}(n/\\phi)$ bits of space, even in insertion only streams.","sentences":["In this paper we study the problem of finding $(\\epsilon, \\phi)$-expander decompositions of a graph in the streaming model, in particular for dynamic streams of edge insertions and deletions.","The goal is to partition the vertex set so that every component induces a $\\phi$-expander, while the number of inter-cluster edges is only an $\\epsilon$ fraction of the total volume.","It was recently shown that there exists a simple algorithm to construct a $(O(\\phi \\log n), \\phi)$-expander decomposition of an $n$-vertex graph using $\\widetilde{O}(n/\\phi^2)$ bits of space [Filtser, Kapralov, Makarov, ITCS'23].","This result calls for understanding the extent to which a dependence in space on the sparsity parameter $\\phi$ is inherent.","We move towards answering this question on two fronts.","We prove that a $(O(\\phi \\log n), \\phi)$-expander decomposition can be found using $\\widetilde{O}(n)$ space, for every $\\phi$. At the core of our result is the first streaming algorithm for computing boundary-linked expander decompositions, a recently introduced strengthening of the classical notion","[Goranci et al., SODA'21].","The key advantage is that a classical sparsifier","[Fung et al., STOC'11], with size independent of $\\phi$, preserves the cuts inside the clusters of a boundary-linked expander decomposition within a multiplicative error.","Notable algorithmic applications use sequences of expander decompositions, in particular one often repeatedly computes a decomposition of the subgraph induced by the inter-cluster edges (e.g., the seminal work of Spielman and Teng on spectral sparsifiers [Spielman, Teng, SIAM Journal of Computing 40(4)], or the recent maximum flow breakthrough [Chen et al., FOCS'22], among others).","We prove that any streaming algorithm that computes a sequence of $(O(\\phi \\log n), \\phi)$-expander decompositions requires ${\\widetilde{\\Omega}}(n/\\phi)$ bits of space, even in insertion only streams."],"url":"http://arxiv.org/abs/2404.16701v1","category":"cs.DS"}
{"created":"2024-04-25 16:01:14","title":"Dimensional Crossover of Microscopic Magnetic Metasurfaces for Magnetic Field Amplification","abstract":"Transformation optics applied to low frequency magnetic systems has been recently implemented to design magnetic field concentrators and cloaks with superior performance. Although this achievement has been amply demonstrated theoretically and experimentally in bulk 3D macrostructures, the performance of these devices at low dimensions remains an open question. In this work, we numerically investigate the non-monotonic evolution of the gain of a magnetic metamaterial field concentrator as the axial dimension is progressively shrunk. In particular, we show that in planar structures the role played by the diamagnetic components becomes negligible, whereas the paramagnetic elements increase their magnetic field channeling efficiency. This is further demonstrated experimentally by tracking the gain of superconductor-ferromagnet concentrators through the superconducting transition. Interestingly, for thicknesses where the diamagnetic petals play an important role for the concentration gain, they also help to reduce the stray field of the concentrator, thus limiting the perturbation of the external field (invisibility). Our findings establish a roadmap and set clear geometrical limits for designing low dimensional magnetic field concentrators.","sentences":["Transformation optics applied to low frequency magnetic systems has been recently implemented to design magnetic field concentrators and cloaks with superior performance.","Although this achievement has been amply demonstrated theoretically and experimentally in bulk 3D macrostructures, the performance of these devices at low dimensions remains an open question.","In this work, we numerically investigate the non-monotonic evolution of the gain of a magnetic metamaterial field concentrator as the axial dimension is progressively shrunk.","In particular, we show that in planar structures the role played by the diamagnetic components becomes negligible, whereas the paramagnetic elements increase their magnetic field channeling efficiency.","This is further demonstrated experimentally by tracking the gain of superconductor-ferromagnet concentrators through the superconducting transition.","Interestingly, for thicknesses where the diamagnetic petals play an important role for the concentration gain, they also help to reduce the stray field of the concentrator, thus limiting the perturbation of the external field (invisibility).","Our findings establish a roadmap and set clear geometrical limits for designing low dimensional magnetic field concentrators."],"url":"http://arxiv.org/abs/2404.16700v1","category":"physics.app-ph"}
{"created":"2024-04-25 15:42:10","title":"Reusing Deep Learning Models: Challenges and Directions in Software Engineering","abstract":"Deep neural networks (DNNs) achieve state-of-the-art performance in many areas, including computer vision, system configuration, and question-answering. However, DNNs are expensive to develop, both in intellectual effort (e.g., devising new architectures) and computational costs (e.g., training). Reusing DNNs is a promising direction to amortize costs within a company and across the computing industry. As with any new technology, however, there are many challenges in reusing DNNs. These challenges include both missing technical capabilities and missing engineering practices.   This vision paper describes challenges in current approaches to DNN re-use. We summarize studies of re-use failures across the spectrum of re-use techniques, including conceptual (e.g., reusing based on a research paper), adaptation (e.g., re-using by building on an existing implementation), and deployment (e.g., direct re-use on a new device). We outline possible advances that would improve each kind of re-use.","sentences":["Deep neural networks (DNNs) achieve state-of-the-art performance in many areas, including computer vision, system configuration, and question-answering.","However, DNNs are expensive to develop, both in intellectual effort (e.g., devising new architectures) and computational costs (e.g., training).","Reusing DNNs is a promising direction to amortize costs within a company and across the computing industry.","As with any new technology, however, there are many challenges in reusing DNNs.","These challenges include both missing technical capabilities and missing engineering practices.   ","This vision paper describes challenges in current approaches to DNN re-use.","We summarize studies of re-use failures across the spectrum of re-use techniques, including conceptual (e.g., reusing based on a research paper), adaptation (e.g., re-using by building on an existing implementation), and deployment (e.g., direct re-use on a new device).","We outline possible advances that would improve each kind of re-use."],"url":"http://arxiv.org/abs/2404.16688v1","category":"cs.SE"}
{"created":"2024-04-25 15:33:15","title":"Monolithic two-level Schwarz preconditioner for Biot's consolidation model in two space dimensions","abstract":"This paper addresses the construction and analysis of a class of domain decomposition methods for the iterative solution of the quasi-static Biot problem in three-field formulation. The considered discrete model arises from time discretization by the implicit Euler method and space discretization by a family of strongly mass-conserving methods exploiting $H^{div}$-conforming approximations of the solid displacement and fluid flux fields. For the resulting saddle-point problem, we construct monolithic overlapping domain decomposition (DD) methods whose analysis relies on a transformation into an equivalent symmetric positive definite system and on stable decompositions of the involved finite element spaces under proper problem-dependent norms. Numerical results on two-dimensional test problems are in accordance with the provided theoretical uniform convergence estimates for the two-level multiplicative Schwarz method.","sentences":["This paper addresses the construction and analysis of a class of domain decomposition methods for the iterative solution of the quasi-static Biot problem in three-field formulation.","The considered discrete model arises from time discretization by the implicit Euler method and space discretization by a family of strongly mass-conserving methods exploiting $H^{div}$-conforming approximations of the solid displacement and fluid flux fields.","For the resulting saddle-point problem, we construct monolithic overlapping domain decomposition (DD) methods whose analysis relies on a transformation into an equivalent symmetric positive definite system and on stable decompositions of the involved finite element spaces under proper problem-dependent norms.","Numerical results on two-dimensional test problems are in accordance with the provided theoretical uniform convergence estimates for the two-level multiplicative Schwarz method."],"url":"http://arxiv.org/abs/2404.16684v1","category":"math.NA"}
{"created":"2024-04-25 15:28:08","title":"The azimuthal distribution of ejecta mass from oblique impacts into sand","abstract":"We measure ejecta mass as a function of azimuthal and impact angle for 104 m/s oblique impacts into sand. We find that the ejecta mass distribution is strongly sensitive to azimuthal angle with up to 8 times more mass in ejecta on the downrange side compared to the uprange side. Crater radii, measured from the site of impact, are measured at different impact and azimuthal angles. Crater ejecta scaling laws are modified to depend on azimuthal and impact angle. We find that crater radii are sensitive to both impact and azimuthal angle but the ejecta mass as a function of both angles can be estimated from the cube of the crater radius without an additional angular dependent function. The ejecta distributions are relevant for processes that depend upon the integrated properties of intermediate velocity impacts occurring in the outer solar system and possibly during planetesimal formation.","sentences":["We measure ejecta mass as a function of azimuthal and impact angle for 104 m/s oblique impacts into sand.","We find that the ejecta mass distribution is strongly sensitive to azimuthal angle with up to 8 times more mass in ejecta on the downrange side compared to the uprange side.","Crater radii, measured from the site of impact, are measured at different impact and azimuthal angles.","Crater ejecta scaling laws are modified to depend on azimuthal and impact angle.","We find that crater radii are sensitive to both impact and azimuthal angle but the ejecta mass as a function of both angles can be estimated from the cube of the crater radius without an additional angular dependent function.","The ejecta distributions are relevant for processes that depend upon the integrated properties of intermediate velocity impacts occurring in the outer solar system and possibly during planetesimal formation."],"url":"http://arxiv.org/abs/2404.16677v1","category":"astro-ph.EP"}
{"created":"2024-04-25 15:20:47","title":"RUMOR: Reinforcement learning for Understanding a Model of the Real World for Navigation in Dynamic Environments","abstract":"Autonomous navigation in dynamic environments is a complex but essential task for autonomous robots, with recent deep reinforcement learning approaches showing promising results. However, the complexity of the real world makes it infeasible to train agents in every possible scenario configuration. Moreover, existing methods typically overlook factors such as robot kinodynamic constraints, or assume perfect knowledge of the environment. In this work, we present RUMOR, a novel planner for differential-drive robots that uses deep reinforcement learning to navigate in highly dynamic environments. Unlike other end-to-end DRL planners, it uses a descriptive robocentric velocity space model to extract the dynamic environment information, enhancing training effectiveness and scenario interpretation. Additionally, we propose an action space that inherently considers robot kinodynamics and train it in a simulator that reproduces the real world problematic aspects, reducing the gap between the reality and simulation. We extensively compare RUMOR with other state-of-the-art approaches, demonstrating a better performance, and provide a detailed analysis of the results. Finally, we validate RUMOR's performance in real-world settings by deploying it on a ground robot. Our experiments, conducted in crowded scenarios and unseen environments, confirm the algorithm's robustness and transferability.","sentences":["Autonomous navigation in dynamic environments is a complex but essential task for autonomous robots, with recent deep reinforcement learning approaches showing promising results.","However, the complexity of the real world makes it infeasible to train agents in every possible scenario configuration.","Moreover, existing methods typically overlook factors such as robot kinodynamic constraints, or assume perfect knowledge of the environment.","In this work, we present RUMOR, a novel planner for differential-drive robots that uses deep reinforcement learning to navigate in highly dynamic environments.","Unlike other end-to-end DRL planners, it uses a descriptive robocentric velocity space model to extract the dynamic environment information, enhancing training effectiveness and scenario interpretation.","Additionally, we propose an action space that inherently considers robot kinodynamics and train it in a simulator that reproduces the real world problematic aspects, reducing the gap between the reality and simulation.","We extensively compare RUMOR with other state-of-the-art approaches, demonstrating a better performance, and provide a detailed analysis of the results.","Finally, we validate RUMOR's performance in real-world settings by deploying it on a ground robot.","Our experiments, conducted in crowded scenarios and unseen environments, confirm the algorithm's robustness and transferability."],"url":"http://arxiv.org/abs/2404.16672v1","category":"cs.RO"}
{"created":"2024-04-25 15:15:26","title":"A candidate period of 4.605 day for FRB 20121102A and one possible implication of its origin","abstract":"A firm establishment of the presence or the lack of periodicity in repeating Fast Radio Bursts (FRBs) is crucial for determining their origins. Here we compile 1145 radio bursts of FRB 20121102A with fluence larger than 0.15 Jy ms from observations using the Five-hundredmeter Aperture Spherical radio Telescope, Arecibo Observatory, Green Bank Telescope, Effelsberg Telescope, MeerKAT Telescope, Lovell Telescope, Deep Space Network 70 m radio telescopes, Very Large Array, and the Westerbork Synthesis Radio Telescope spanning the time interval of MJD 57175-58776. A quasi-period of $157.1_{-4.8}^{+5.2}$ day and a candidate quasi-period of $4.605_{-0.010}^{+0.003}$ day are found through the phase-folding probability binomial analysis. The former is consistent with previous findings and the latter is new. The 4.605 day periodicity is more obvious in high-energy bursts with fluence larger than $10^{38}$ erg. The presence of these (candidate) quasi-periods, together with the corresponding width of burst accumulation in the phase space, are consistent with the bursts' originating from a binary degenerate star system with a close-by planet around the primary neutron star.","sentences":["A firm establishment of the presence or the lack of periodicity in repeating Fast Radio Bursts (FRBs) is crucial for determining their origins.","Here we compile 1145 radio bursts of FRB 20121102A with fluence larger than 0.15 Jy ms from observations using the Five-hundredmeter Aperture Spherical radio Telescope, Arecibo Observatory, Green Bank Telescope, Effelsberg Telescope, MeerKAT Telescope, Lovell Telescope, Deep Space Network 70 m radio telescopes, Very Large Array, and the Westerbork Synthesis Radio Telescope spanning the time interval of MJD 57175-58776.","A quasi-period of $157.1_{-4.8}^{+5.2}$ day and a candidate quasi-period of $4.605_{-0.010}^{+0.003}$ day are found through the phase-folding probability binomial analysis.","The former is consistent with previous findings and the latter is new.","The 4.605 day periodicity is more obvious in high-energy bursts with fluence larger than $10^{38}$ erg.","The presence of these (candidate) quasi-periods, together with the corresponding width of burst accumulation in the phase space, are consistent with the bursts' originating from a binary degenerate star system with a close-by planet around the primary neutron star."],"url":"http://arxiv.org/abs/2404.16669v1","category":"astro-ph.HE"}
{"created":"2024-04-25 15:11:50","title":"The First Estimation of the Ambipolar Diffusivity Coefficient from Multi-Scale Observations of the Class 0/I Protostar, HOPS-370","abstract":"Protostars are born in magnetized environments. As a consequence, the formation of protostellar disks can be suppressed by the magnetic field efficiently removing angular momentum of the infalling material. Non-ideal MHD effects are proposed to as one way to allow protostellar disks to form. Thus, it is important to understand their contributions in observations of protostellar systems. We derive an analytical equation to estimate the ambipolar diffusivity coefficient at the edge of the protostellar disk in the Class 0/I protostar, HOPS-370, for the first time, under the assumption that the disk radius is set by ambipolar diffusion. Using previous results of the protostellar mass, disk mass, disk radius, density and temperature profiles and magnetic field strength, we estimate the ambipolar diffusivity coefficient to be $1.7^{+1.5}_{-1.4}\\times10^{19}\\,\\mathrm{cm^{2}\\,s^{-1}}$. We quantify the contribution of ambipolar diffusion by estimating its dimensionless Els\\\"{a}sser number to be $\\sim1.7^{+1.0}_{-1.0}$, indicating its dynamical importance in this region. We compare to chemical calculations of the ambipolar diffusivity coefficient using the Non-Ideal magnetohydrodynamics Coefficients and Ionisation Library (NICIL), which is consistent with our results. In addition, we compare our derived ambipolar diffusivity coefficient to the diffusivity coefficients for Ohmic dissipation and the Hall effect, and find ambipolar diffusion is dominant in our density regime. These results demonstrate a new methodology to understand non-ideal MHD effects in observations of protostellar disks. More detailed modeling of the magnetic field, envelope and microphysics, along with a larger sample of protostellar systems is needed to further understand the contributions of non-ideal MHD.","sentences":["Protostars are born in magnetized environments.","As a consequence, the formation of protostellar disks can be suppressed by the magnetic field efficiently removing angular momentum of the infalling material.","Non-ideal MHD effects are proposed to as one way to allow protostellar disks to form.","Thus, it is important to understand their contributions in observations of protostellar systems.","We derive an analytical equation to estimate the ambipolar diffusivity coefficient at the edge of the protostellar disk in the Class 0/I protostar, HOPS-370, for the first time, under the assumption that the disk radius is set by ambipolar diffusion.","Using previous results of the protostellar mass, disk mass, disk radius, density and temperature profiles and magnetic field strength, we estimate the ambipolar diffusivity coefficient to be $1.7^{+1.5}_{-1.4}\\times10^{19}\\,\\mathrm{cm^{2}\\,s^{-1}}$. We quantify the contribution of ambipolar diffusion by estimating its dimensionless Els\\\"{a}sser number to be $\\sim1.7^{+1.0}_{-1.0}$, indicating its dynamical importance in this region.","We compare to chemical calculations of the ambipolar diffusivity coefficient using the Non-Ideal magnetohydrodynamics Coefficients and Ionisation Library (NICIL), which is consistent with our results.","In addition, we compare our derived ambipolar diffusivity coefficient to the diffusivity coefficients for Ohmic dissipation and the Hall effect, and find ambipolar diffusion is dominant in our density regime.","These results demonstrate a new methodology to understand non-ideal MHD effects in observations of protostellar disks.","More detailed modeling of the magnetic field, envelope and microphysics, along with a larger sample of protostellar systems is needed to further understand the contributions of non-ideal MHD."],"url":"http://arxiv.org/abs/2404.16668v1","category":"astro-ph.SR"}
{"created":"2024-04-25 15:00:34","title":"Computing Hamiltonian Paths with Partial Order Restrictions","abstract":"When solving the Hamiltonian path problem it seems natural to be given additional precedence constraints for the order in which the vertices are visited. For example one could decide whether a Hamiltonian path exists for a fixed starting point, or that some vertices are visited before another vertex. We consider the problem of finding a Hamiltonian path that observes all precedence constraints given in a partial order on the vertex set. We show that this problem is $\\mathsf{NP}$-complete even if restricted to complete bipartite graphs and posets of height 2. In contrast, for posets of width $k$ there is an $\\mathcal{O}(k^2 n^k)$ algorithm for arbitrary graphs with $n$ vertices. We show that it is unlikely that the running time of this algorithm can be improved significantly, i.e., there is no $f(k) n^{o(k)}$ time algorithm under the assumption of the Exponential Time Hypothesis. Furthermore, for the class of outerplanar graphs, we give an $\\mathcal{O}(n^2)$ algorithm for arbitrary posets.","sentences":["When solving the Hamiltonian path problem it seems natural to be given additional precedence constraints for the order in which the vertices are visited.","For example one could decide whether a Hamiltonian path exists for a fixed starting point, or that some vertices are visited before another vertex.","We consider the problem of finding a Hamiltonian path that observes all precedence constraints given in a partial order on the vertex set.","We show that this problem is $\\mathsf{NP}$-complete even if restricted to complete bipartite graphs and posets of height 2.","In contrast, for posets of width $k$ there is an $\\mathcal{O}(k^2 n^k)$ algorithm for arbitrary graphs with $n$ vertices.","We show that it is unlikely that the running time of this algorithm can be improved significantly, i.e., there is no $f(k) n^{o(k)}$ time algorithm under the assumption of the Exponential Time Hypothesis.","Furthermore, for the class of outerplanar graphs, we give an $\\mathcal{O}(n^2)$ algorithm for arbitrary posets."],"url":"http://arxiv.org/abs/2404.16662v1","category":"cs.DM"}
{"created":"2024-04-25 14:44:23","title":"Obstruction classes for moduli spaces of sheaves and Lagrangian fibrations","abstract":"We investigate obstruction classes of moduli spaces of sheaves on K3 surfaces. We extend previous results by Caldararu, explicitly determining the obstruction class and its order in the Brauer group. Our main theorem establishes a short exact sequence relating the Brauer group of the moduli space to that of the underlying K3 surface. This provides a criterion for when the moduli space is fine, generalising well-known results for K3 surfaces. Additionally, we explore applications to Ogg-Shafarevich theory for Beauville-Mukai systems. Furthermore, we investigate birational equivalences of Beauville-Mukai systems on elliptic K3 surfaces, presenting a complete characterisation of such equivalences.","sentences":["We investigate obstruction classes of moduli spaces of sheaves on K3 surfaces.","We extend previous results by Caldararu, explicitly determining the obstruction class and its order in the Brauer group.","Our main theorem establishes a short exact sequence relating the Brauer group of the moduli space to that of the underlying K3 surface.","This provides a criterion for when the moduli space is fine, generalising well-known results for K3 surfaces.","Additionally, we explore applications to Ogg-Shafarevich theory for Beauville-Mukai systems.","Furthermore, we investigate birational equivalences of Beauville-Mukai systems on elliptic K3 surfaces, presenting a complete characterisation of such equivalences."],"url":"http://arxiv.org/abs/2404.16652v1","category":"math.AG"}
{"created":"2024-04-25 14:42:12","title":"Evolutionary Large Language Models for Hardware Security: A Comparative Survey","abstract":"Automating hardware (HW) security vulnerability detection and mitigation during the design phase is imperative for two reasons: (i) It must be before chip fabrication, as post-fabrication fixes can be costly or even impractical; (ii) The size and complexity of modern HW raise concerns about unknown vulnerabilities compromising CIA triad. While Large Language Models (LLMs) can revolutionize both HW design and testing processes, within the semiconductor context, LLMs can be harnessed to automatically rectify security-relevant vulnerabilities inherent in HW designs. This study explores the seeds of LLM integration in register transfer level (RTL) designs, focusing on their capacity for autonomously resolving security-related vulnerabilities. The analysis involves comparing methodologies, assessing scalability, interpretability, and identifying future research directions. Potential areas for exploration include developing specialized LLM architectures for HW security tasks and enhancing model performance with domain-specific knowledge, leading to reliable automated security measurement and risk mitigation associated with HW vulnerabilities.","sentences":["Automating hardware (HW) security vulnerability detection and mitigation during the design phase is imperative for two reasons: (i) It must be before chip fabrication, as post-fabrication fixes can be costly or even impractical; (ii) The size and complexity of modern HW raise concerns about unknown vulnerabilities compromising CIA triad.","While Large Language Models (LLMs) can revolutionize both HW design and testing processes, within the semiconductor context, LLMs can be harnessed to automatically rectify security-relevant vulnerabilities inherent in HW designs.","This study explores the seeds of LLM integration in register transfer level (RTL) designs, focusing on their capacity for autonomously resolving security-related vulnerabilities.","The analysis involves comparing methodologies, assessing scalability, interpretability, and identifying future research directions.","Potential areas for exploration include developing specialized LLM architectures for HW security tasks and enhancing model performance with domain-specific knowledge, leading to reliable automated security measurement and risk mitigation associated with HW vulnerabilities."],"url":"http://arxiv.org/abs/2404.16651v1","category":"cs.CR"}
{"created":"2024-04-25 14:41:53","title":"Design optimization of advanced tow-steered composites with manufacturing constraints","abstract":"Tow steering technologies, such as Automated fiber placement, enable the fabrication of composite laminates with curvilinear fiber, tow, or tape paths. Designers may therefore tailor tow orientations locally according to the expected local stress state within a structure, such that strong and stiff orientations of the tow are (for example) optimized to provide maximal mechanical benefit. Tow path optimization can be an effective tool in automating this design process, yet has a tendency to create complex designs that may be challenging to manufacture. In the context of tow steering, these complexities can manifest in defects such as tow wrinkling, gaps, overlaps. In this work, we implement manufacturing constraints within the tow path optimization formulation to restrict the minimum tow turning radius and the maximum density of gaps between and overlaps of tows. This is achieved by bounding the local value of the curl and divergence of the vector field associated with the tow orientations. The resulting local constraints are effectively enforced in the optimization framework through the Augmented Lagrangian method. The resulting optimization methodology is demonstrated by designing 2D and 3D structures with optimized tow orientation paths that maximize stiffness (minimize compliance) considering various levels of manufacturing restrictions. The optimized tow paths are shown to be structurally efficient and to respect imposed manufacturing constraints. As expected, the more geometrical complexity that can be achieved by the feedstock tow and placement technology, the higher the stiffness of the resulting optimized design.","sentences":["Tow steering technologies, such as Automated fiber placement, enable the fabrication of composite laminates with curvilinear fiber, tow, or tape paths.","Designers may therefore tailor tow orientations locally according to the expected local stress state within a structure, such that strong and stiff orientations of the tow are (for example) optimized to provide maximal mechanical benefit.","Tow path optimization can be an effective tool in automating this design process, yet has a tendency to create complex designs that may be challenging to manufacture.","In the context of tow steering, these complexities can manifest in defects such as tow wrinkling, gaps, overlaps.","In this work, we implement manufacturing constraints within the tow path optimization formulation to restrict the minimum tow turning radius and the maximum density of gaps between and overlaps of tows.","This is achieved by bounding the local value of the curl and divergence of the vector field associated with the tow orientations.","The resulting local constraints are effectively enforced in the optimization framework through the Augmented Lagrangian method.","The resulting optimization methodology is demonstrated by designing 2D and 3D structures with optimized tow orientation paths that maximize stiffness (minimize compliance) considering various levels of manufacturing restrictions.","The optimized tow paths are shown to be structurally efficient and to respect imposed manufacturing constraints.","As expected, the more geometrical complexity that can be achieved by the feedstock tow and placement technology, the higher the stiffness of the resulting optimized design."],"url":"http://arxiv.org/abs/2404.16650v1","category":"cs.CE"}
{"created":"2024-04-25 14:41:01","title":"Kalman-based approaches for online estimation of bioreactor dynamics from fluorescent reporter measurements","abstract":"We address online estimation of microbial growth dynamics in bioreactors from measurements of a fluorescent reporter protein synthesized along with microbial growth. We consider an extended version of standard growth models that accounts for the dynamics of reporter synthesis. We develop state estimation from sampled, noisy measurements in the cases of known and unknown growth rate functions. Leveraging conservation laws and regularized estimation techniques, we reduce these nonlinear estimation problems to linear time-varying ones, and solve them via Kalman filtering. We establish convergence results in absence of noise and show performance on noisy data in simulation.","sentences":["We address online estimation of microbial growth dynamics in bioreactors from measurements of a fluorescent reporter protein synthesized along with microbial growth.","We consider an extended version of standard growth models that accounts for the dynamics of reporter synthesis.","We develop state estimation from sampled, noisy measurements in the cases of known and unknown growth rate functions.","Leveraging conservation laws and regularized estimation techniques, we reduce these nonlinear estimation problems to linear time-varying ones, and solve them via Kalman filtering.","We establish convergence results in absence of noise and show performance on noisy data in simulation."],"url":"http://arxiv.org/abs/2404.16649v1","category":"math.OC"}
{"created":"2024-04-25 14:35:16","title":"Improving TAS Adaptability with a Variable Temperature Threshold","abstract":"Thermal-Aware Scheduling (TAS) provides methods to manage the thermal dissipation of a computing chip during task execution. These methods aim to avoid issues such as accelerated aging of the device, premature failure and degraded chip performance. In this work, we implement a new TAS algorithm, VTF-TAS, which makes use of a variable temperature threshold to control task execution and thermal dissipation. To enable adequate execution of the tasks to reach their deadlines, this threshold is managed based on the theory of fluid scheduling. Using an evaluation methodology as described in POD-TAS, we evaluate VTF-TAS using a set of 4 benchmarks from the COMBS benchmark suite to examine its ability to minimize chip temperature throughout schedule execution. Through our evaluation, we demonstrate that this new algorithm is able to adaptively manage the temperature threshold such that the peak temperature during schedule execution is lower than POD-TAS, with no requirement for an expensive search procedure to obtain an optimal threshold for scheduling.","sentences":["Thermal-Aware Scheduling (TAS) provides methods to manage the thermal dissipation of a computing chip during task execution.","These methods aim to avoid issues such as accelerated aging of the device, premature failure and degraded chip performance.","In this work, we implement a new TAS algorithm, VTF-TAS, which makes use of a variable temperature threshold to control task execution and thermal dissipation.","To enable adequate execution of the tasks to reach their deadlines, this threshold is managed based on the theory of fluid scheduling.","Using an evaluation methodology as described in POD-TAS, we evaluate VTF-TAS using a set of 4 benchmarks from the COMBS benchmark suite to examine its ability to minimize chip temperature throughout schedule execution.","Through our evaluation, we demonstrate that this new algorithm is able to adaptively manage the temperature threshold such that the peak temperature during schedule execution is lower than POD-TAS, with no requirement for an expensive search procedure to obtain an optimal threshold for scheduling."],"url":"http://arxiv.org/abs/2404.16646v1","category":"eess.SY"}
{"created":"2024-04-25 14:34:10","title":"Explanations in Everyday Software Systems: Towards a Taxonomy for Explainability Needs","abstract":"Modern software systems are becoming increasingly complex and opaque. The integration of explanations within software has shown the potential to address this opacity and can make the system more understandable to end-users. As a result, explainability has gained much traction as a non-functional requirement of complex systems. Understanding what type of system requires what types of explanations is necessary to facilitate the inclusion of explainability in early software design processes. In order to specify explainability requirements, an explainability taxonomy that applies to a variety of different software types is needed. In this paper, we present the results of an online survey with 84 participants. We asked the participants to state their questions and confusions concerning their three most recently used software systems and elicited both explicit and implicit explainability needs from their statements. These needs were coded by three researchers. In total, we identified and classified 315 explainability needs from the survey answers. Drawing from a large pool of explainability needs and our coding procedure, we present two major contributions of this work: 1) a taxonomy for explainability needs in everyday software systems and 2) an overview of how the need for explanations differs between different types of software systems.","sentences":["Modern software systems are becoming increasingly complex and opaque.","The integration of explanations within software has shown the potential to address this opacity and can make the system more understandable to end-users.","As a result, explainability has gained much traction as a non-functional requirement of complex systems.","Understanding what type of system requires what types of explanations is necessary to facilitate the inclusion of explainability in early software design processes.","In order to specify explainability requirements, an explainability taxonomy that applies to a variety of different software types is needed.","In this paper, we present the results of an online survey with 84 participants.","We asked the participants to state their questions and confusions concerning their three most recently used software systems and elicited both explicit and implicit explainability needs from their statements.","These needs were coded by three researchers.","In total, we identified and classified 315 explainability needs from the survey answers.","Drawing from a large pool of explainability needs and our coding procedure, we present two major contributions of this work: 1) a taxonomy for explainability needs in everyday software systems and 2) an overview of how the need for explanations differs between different types of software systems."],"url":"http://arxiv.org/abs/2404.16644v1","category":"cs.SE"}
{"created":"2024-04-25 14:22:59","title":"Inverse scattering for repulsive potential and strong singular interactions","abstract":"In a previous work of 2014 on a quantum system governed by the repulsive Hamiltonian, the author proved uniqueness for short-range interactions described by a scattering operator consisting of regular and singular parts. In this paper, the singular part is assumed to have much stronger singularities and the same uniqueness theorem is proved. By applying the time-dependent method invented by Enss and Weder in 1995, the high-velocity limit for a wider class of the scattering operator with stronger singularities also uniquely determines uniquely the interactions of a multi-dimensional system.","sentences":["In a previous work of 2014 on a quantum system governed by the repulsive Hamiltonian, the author proved uniqueness for short-range interactions described by a scattering operator consisting of regular and singular parts.","In this paper, the singular part is assumed to have much stronger singularities and the same uniqueness theorem is proved.","By applying the time-dependent method invented by Enss and Weder in 1995, the high-velocity limit for a wider class of the scattering operator with stronger singularities also uniquely determines uniquely the interactions of a multi-dimensional system."],"url":"http://arxiv.org/abs/2404.16634v1","category":"math-ph"}
{"created":"2024-04-25 14:21:15","title":"Introducing Systems Thinking as a Framework for Teaching and Assessing Threat Modeling Competency","abstract":"Computing systems face diverse and substantial cybersecurity threats. To mitigate these cybersecurity threats, software engineers need to be competent in the skill of threat modeling. In industry and academia, there are many frameworks for teaching threat modeling, but our analysis of these frameworks suggests that (1) these approaches tend to be focused on component-level analysis rather than educating students to reason holistically about a system's cybersecurity, and (2) there is no rubric for assessing a student's threat modeling competency. To address these concerns, we propose using systems thinking in conjunction with popular and industry-standard threat modeling frameworks like STRIDE for teaching and assessing threat modeling competency. Prior studies suggest a holistic approach, like systems thinking, can help understand and mitigate cybersecurity threats. Thus, we developed and piloted two novel rubrics - one for assessing STRIDE threat modeling performance and the other for assessing systems thinking performance while conducting STRIDE.   To conduct this study, we piloted the two rubrics mentioned above to assess threat model artifacts of students enrolled in an upper-level software engineering course at Purdue University in Fall 2021, Spring 2023, and Fall 2023. Students who had both systems thinking and STRIDE instruction identified and attempted to mitigate component-level as well as systems-level threats. Students with only STRIDE instruction tended to focus on identifying and mitigating component-level threats and discounted system-level threats. We contribute to engineering education by: (1) describing a new rubric for assessing threat modeling based on systems thinking; (2) identifying trends and blindspots in students' threat modeling approach; and (3) envisioning the benefits of integrating systems thinking in threat modeling teaching and assessment.","sentences":["Computing systems face diverse and substantial cybersecurity threats.","To mitigate these cybersecurity threats, software engineers need to be competent in the skill of threat modeling.","In industry and academia, there are many frameworks for teaching threat modeling, but our analysis of these frameworks suggests that (1) these approaches tend to be focused on component-level analysis rather than educating students to reason holistically about a system's cybersecurity, and (2) there is no rubric for assessing a student's threat modeling competency.","To address these concerns, we propose using systems thinking in conjunction with popular and industry-standard threat modeling frameworks like STRIDE for teaching and assessing threat modeling competency.","Prior studies suggest a holistic approach, like systems thinking, can help understand and mitigate cybersecurity threats.","Thus, we developed and piloted two novel rubrics - one for assessing STRIDE threat modeling performance and the other for assessing systems thinking performance while conducting STRIDE.   ","To conduct this study, we piloted the two rubrics mentioned above to assess threat model artifacts of students enrolled in an upper-level software engineering course at Purdue University in Fall 2021, Spring 2023, and Fall 2023.","Students who had both systems thinking and STRIDE instruction identified and attempted to mitigate component-level as well as systems-level threats.","Students with only STRIDE instruction tended to focus on identifying and mitigating component-level threats and discounted system-level threats.","We contribute to engineering education by: (1) describing a new rubric for assessing threat modeling based on systems thinking; (2) identifying trends and blindspots in students' threat modeling approach; and (3) envisioning the benefits of integrating systems thinking in threat modeling teaching and assessment."],"url":"http://arxiv.org/abs/2404.16632v1","category":"cs.CR"}
{"created":"2024-04-25 14:16:36","title":"Implementing and Optimizing the Scaled Dot-Product Attention on Streaming Dataflow","abstract":"Transformer models serve as the backbone of many state-ofthe-art language models, and most use the scaled dot-product attention (SDPA) mechanism to capture relationships between tokens. However, the straightforward implementation of SDPA has quadratic compute and memory complexity with respect to the sequence length. On processor architectures such as GPUs and TPUs, there is a robust body of prior work. However, little work has been performed on non-processor architectures.In this work, we show how the architecture and execution model of Streaming Dataflow Accelerators can help tackle this challenge. We first define abstract hardware that adopts a streaming execution model, and we implement a cycle-accurate simulator of the abstract hardware using the Dataflow Abstract Machine simulation framework. Second, we implement the naive SDPA algorithm on this abstract hardware and show it requires linear (O(N)) intermediate memory. Third, we then modify the naive algorithm, taking inspiration from prior processor-oriented works, by reordering the multiplication and division operations. Finally, we map the modified algorithm to abstract hardware, and confirm that the implementation computes SDPA at full throughput while only using a constant amount (O(1)) of intermediate memory.","sentences":["Transformer models serve as the backbone of many state-ofthe-art language models, and most use the scaled dot-product attention (SDPA) mechanism to capture relationships between tokens.","However, the straightforward implementation of SDPA has quadratic compute and memory complexity with respect to the sequence length.","On processor architectures such as GPUs and TPUs, there is a robust body of prior work.","However, little work has been performed on non-processor architectures.","In this work, we show how the architecture and execution model of Streaming Dataflow Accelerators can help tackle this challenge.","We first define abstract hardware that adopts a streaming execution model, and we implement a cycle-accurate simulator of the abstract hardware using the Dataflow Abstract Machine simulation framework.","Second, we implement the naive SDPA algorithm on this abstract hardware and show it requires linear (O(N))","intermediate memory.","Third, we then modify the naive algorithm, taking inspiration from prior processor-oriented works, by reordering the multiplication and division operations.","Finally, we map the modified algorithm to abstract hardware, and confirm that the implementation computes SDPA at full throughput while only using a constant amount (O(1)) of intermediate memory."],"url":"http://arxiv.org/abs/2404.16629v1","category":"cs.AR"}
{"created":"2024-04-25 14:09:22","title":"Development of parallel programs on shared data-structures -- Revised version","abstract":"A syntax-directed formal system for the development of totally correct programs with respect to an unfair shared-state parallel while-language is proposed. The system can be understood as a compositional reformulation of the Owicki/Gries method for verification of parallel programs. Auxiliary variables are used both as a specification tool to eliminate undesirable implementations, and as a verification tool to make it possible to prove that an already finished program satisfies a particular specification. Auxiliary variables may be of any sort, and it is up to the user to define the auxiliary structure he prefers. Moreover, the auxiliary structure is only a part of the logic. This means that auxiliary variables do not have to be implemented as if they were ordinary programming variables. The system is proved sound and relatively complete with respect to an operational semantics and employed to develop three nontrivial algorithms: the Dining-Philosophers, the Bubble-Lattice-Sort and the Set-Partition algorithms. Finally, a related method for the development of (possibly nonterminating) programs with respect to four properties is described. This approach is then used to develop Dekker's algorithm.","sentences":["A syntax-directed formal system for the development of totally correct programs with respect to an unfair shared-state parallel while-language is proposed.","The system can be understood as a compositional reformulation of the Owicki/Gries method for verification of parallel programs.","Auxiliary variables are used both as a specification tool to eliminate undesirable implementations, and as a verification tool to make it possible to prove that an already finished program satisfies a particular specification.","Auxiliary variables may be of any sort, and it is up to the user to define the auxiliary structure he prefers.","Moreover, the auxiliary structure is only a part of the logic.","This means that auxiliary variables do not have to be implemented as if they were ordinary programming variables.","The system is proved sound and relatively complete with respect to an operational semantics and employed to develop three nontrivial algorithms: the Dining-Philosophers, the Bubble-Lattice-Sort and the Set-Partition algorithms.","Finally, a related method for the development of (possibly nonterminating) programs with respect to four properties is described.","This approach is then used to develop Dekker's algorithm."],"url":"http://arxiv.org/abs/2404.16624v1","category":"cs.FL"}
{"created":"2024-04-25 14:02:25","title":"The THU-HCSI Multi-Speaker Multi-Lingual Few-Shot Voice Cloning System for LIMMITS'24 Challenge","abstract":"This paper presents the multi-speaker multi-lingual few-shot voice cloning system developed by THU-HCSI team for LIMMITS'24 Challenge. To achieve high speaker similarity and naturalness in both mono-lingual and cross-lingual scenarios, we build the system upon YourTTS and add several enhancements. For further improving speaker similarity and speech quality, we introduce speaker-aware text encoder and flow-based decoder with Transformer blocks. In addition, we denoise the few-shot data, mix up them with pre-training data, and adopt a speaker-balanced sampling strategy to guarantee effective fine-tuning for target speakers. The official evaluations in track 1 show that our system achieves the best speaker similarity MOS of 4.25 and obtains considerable naturalness MOS of 3.97.","sentences":["This paper presents the multi-speaker multi-lingual few-shot voice cloning system developed by THU-HCSI team for LIMMITS'24 Challenge.","To achieve high speaker similarity and naturalness in both mono-lingual and cross-lingual scenarios, we build the system upon YourTTS and add several enhancements.","For further improving speaker similarity and speech quality, we introduce speaker-aware text encoder and flow-based decoder with Transformer blocks.","In addition, we denoise the few-shot data, mix up them with pre-training data, and adopt a speaker-balanced sampling strategy to guarantee effective fine-tuning for target speakers.","The official evaluations in track 1 show that our system achieves the best speaker similarity MOS of 4.25 and obtains considerable","naturalness MOS of 3.97."],"url":"http://arxiv.org/abs/2404.16619v1","category":"cs.SD"}
{"created":"2024-04-25 13:51:01","title":"Towards Symbiotic SAGIN Through Inter-operator Resource and Service Sharing: Joint Orchestration of User Association and Radio Resources","abstract":"The space-air-ground integrated network (SAGIN) is a pivotal architecture to support ubiquitous connectivity in the upcoming 6G era. Inter-operator resource and service sharing is a promising way to realize such a huge network, utilizing resources efficiently and reducing construction costs. Given the rationality of operators, the configuration of resources and services in SAGIN should focus on both the overall system performance and individual benefits of operators. Motivated by emerging symbiotic communication facilitating mutual benefits across different radio systems, we investigate the resource and service sharing in SAGIN from a symbiotic communication perspective in this paper. In particular, we consider a SAGIN consisting of a ground network operator (GNO) and a satellite network operator (SNO). Specifically, we aim to maximize the weighted sum rate (WSR) of the whole SAGIN by jointly optimizing the user association, resource allocation, and beamforming. Besides, we introduce a sharing coefficient to characterize the revenue of operators. Operators may suffer revenue loss when only focusing on maximizing the WSR. In pursuit of mutual benefits, we propose a mutual benefit constraint (MBC) to ensure that each operator obtains revenue gains. Then, we develop a centralized algorithm based on the successive convex approximation (SCA) method. Considering that the centralized algorithm is difficult to implement, we propose a distributed algorithm based on Lagrangian dual decomposition and the consensus alternating direction method of multipliers (ADMM). Finally, we provide extensive numerical simulations to demonstrate the effectiveness of the two proposed algorithms, and the distributed optimization algorithm can approach the performance of the centralized one.","sentences":["The space-air-ground integrated network (SAGIN) is a pivotal architecture to support ubiquitous connectivity in the upcoming 6G era.","Inter-operator resource and service sharing is a promising way to realize such a huge network, utilizing resources efficiently and reducing construction costs.","Given the rationality of operators, the configuration of resources and services in SAGIN should focus on both the overall system performance and individual benefits of operators.","Motivated by emerging symbiotic communication facilitating mutual benefits across different radio systems, we investigate the resource and service sharing in SAGIN from a symbiotic communication perspective in this paper.","In particular, we consider a SAGIN consisting of a ground network operator (GNO) and a satellite network operator (SNO).","Specifically, we aim to maximize the weighted sum rate (WSR) of the whole SAGIN by jointly optimizing the user association, resource allocation, and beamforming.","Besides, we introduce a sharing coefficient to characterize the revenue of operators.","Operators may suffer revenue loss when only focusing on maximizing the WSR.","In pursuit of mutual benefits, we propose a mutual benefit constraint (MBC) to ensure that each operator obtains revenue gains.","Then, we develop a centralized algorithm based on the successive convex approximation (SCA) method.","Considering that the centralized algorithm is difficult to implement, we propose a distributed algorithm based on Lagrangian dual decomposition and the consensus alternating direction method of multipliers (ADMM).","Finally, we provide extensive numerical simulations to demonstrate the effectiveness of the two proposed algorithms, and the distributed optimization algorithm can approach the performance of the centralized one."],"url":"http://arxiv.org/abs/2404.16611v1","category":"cs.IT"}
{"created":"2024-04-25 13:48:10","title":"A holographic bottom-up approach to $\u03a3$ baryons","abstract":"In this work, we discuss the description of neutral $\\Sigma$ baryons with $I(J^P)=1(1/2^+)$ and $I(J^P)=1(3/2^+)$ using two bottom-up approaches: the deformed background and the static dilaton. In both models, we consider a non-linear Regge trajectory extension motivated by the \\emph{strange} nature that $\\Sigma$ baryons have. We found that both models describe these systems with an RMS error smaller than 10 $\\%$. We also perform a configurational entropy calculation in both models to discuss hadronic stability.","sentences":["In this work, we discuss the description of neutral $\\Sigma$ baryons with $I(J^P)=1(1/2^+)$ and $I(J^P)=1(3/2^+)$ using two bottom-up approaches: the deformed background and the static dilaton.","In both models, we consider a non-linear Regge trajectory extension motivated by the \\emph{strange} nature that $\\Sigma$ baryons have.","We found that both models describe these systems with an RMS error smaller than 10 $\\%$. We also perform a configurational entropy calculation in both models to discuss hadronic stability."],"url":"http://arxiv.org/abs/2404.16608v1","category":"hep-ph"}
{"created":"2024-04-25 13:26:51","title":"Electronic structure, self-doping, and superconducting instability in the alternating single-layer trilayer stacking nickelates La$_3$Ni$_2$O$_7$","abstract":"Motivated by the recently proposed alternating single-layer trilayer stacking structure for the nickelate La$_3$Ni$_2$O$_7$, we comprehensively study this system using {\\it ab initio} and random-phase approximation techniques. Our analysis unveils similarities between this novel La$_3$Ni$_2$O$_7$ structure and other Ruddlesden-Popper nickelate superconductors, such as a similar charge-transfer gap value and orbital-selective behavior of the $e_g$ orbitals. However, different from other Ruddlesden-Popper nickelate superconductors, we do not observe any obvious reconstruction of the Fermi surface from ambient conditions (Cmmm phase) to high pressures (P4/mmm phase). Pressure primarily increases the bandwidths of the Ni $e_g$ bands, suggesting an enhancement of the itinerant properties of those $e_g$ states. Furthermore, the $d_{3z^2-r^2}$ orbital also has a layer-selective behavior because the antibonding-bonding-nonbonding splitting can only be obtained in the trilayer. In addition, we observe a \"self-doping\" effect from the trilayer to the single-layer sublattices and this effect will be enhanced by overall electron doping. Moreover, we find a leading $d_{x^2-y^2}$-wave pairing state that is restricted to the single-layer. Because the effective coupling between the single layers is very weak -- due to the non-superconducting trilayer in between -- this suggests that the superconducting transition temperature $T_c$ in this structure should be much lower than in the bilayer structure.","sentences":["Motivated by the recently proposed alternating single-layer trilayer stacking structure for the nickelate La$_3$Ni$_2$O$_7$, we comprehensively study this system using {\\it ab initio} and random-phase approximation techniques.","Our analysis unveils similarities between this novel La$_3$Ni$_2$O$_7$ structure and other Ruddlesden-Popper nickelate superconductors, such as a similar charge-transfer gap value and orbital-selective behavior of the $e_g$ orbitals.","However, different from other Ruddlesden-Popper nickelate superconductors, we do not observe any obvious reconstruction of the Fermi surface from ambient conditions (Cmmm phase) to high pressures (P4/mmm phase).","Pressure primarily increases the bandwidths of the Ni $e_g$ bands, suggesting an enhancement of the itinerant properties of those $e_g$ states.","Furthermore, the $d_{3z^2-r^2}$ orbital also has a layer-selective behavior because the antibonding-bonding-nonbonding splitting can only be obtained in the trilayer.","In addition, we observe a \"self-doping\" effect from the trilayer to the single-layer sublattices and this effect will be enhanced by overall electron doping.","Moreover, we find a leading $d_{x^2-y^2}$-wave pairing state that is restricted to the single-layer.","Because the effective coupling between the single layers is very weak -- due to the non-superconducting trilayer in between -- this suggests that the superconducting transition temperature $T_c$ in this structure should be much lower than in the bilayer structure."],"url":"http://arxiv.org/abs/2404.16600v1","category":"cond-mat.supr-con"}
{"created":"2024-04-25 13:25:06","title":"Uncovering Data Across Continua: An Introduction to Functional Data Analysis","abstract":"In a world increasingly awash with data, the need to extract meaningful insights from data has never been more crucial. Functional Data Analysis (FDA) goes beyond traditional data points, treating data as dynamic, continuous functions, capturing ever-changing phenomena nuances. This article introduces FDA, merging statistics with real-world complexity, ideal for those with mathematical skills but no FDA background.","sentences":["In a world increasingly awash with data, the need to extract meaningful insights from data has never been more crucial.","Functional Data Analysis (FDA) goes beyond traditional data points, treating data as dynamic, continuous functions, capturing ever-changing phenomena nuances.","This article introduces FDA, merging statistics with real-world complexity, ideal for those with mathematical skills but no FDA background."],"url":"http://arxiv.org/abs/2404.16598v1","category":"math.ST"}
{"created":"2024-04-25 13:14:48","title":"Uninterrupted Maximum Flow on Signalized Traffic Networks","abstract":"This paper describes a traffic signal control procedure that allows motorists who travel at a recommended speed on suburban arterial two-way roads with a common cycle-time to make every traffic signal. A road-to-traveler-feedback-device advises motorists how fast they should travel to do this. Signalized arterial roads where vehicles that travel at the recommended speed make every traffic signal are termed Ride-the-Green-Wave (RGW) roads. Left-turn-arounds allow vehicles to turn left from one two-way RGW-road to an intersecting/orthogonal two-way RGW-road while allowing maximum flow on the intersecting RGW-roads. In addition to introducing novel traffic signal control strategies, the methods presented in this paper have implications for: road network design, public transport control, connected and automated vehicles and environmental impacts.","sentences":["This paper describes a traffic signal control procedure that allows motorists who travel at a recommended speed on suburban arterial two-way roads with a common cycle-time to make every traffic signal.","A road-to-traveler-feedback-device advises motorists how fast they should travel to do this.","Signalized arterial roads where vehicles that travel at the recommended speed make every traffic signal are termed Ride-the-Green-Wave (RGW) roads.","Left-turn-arounds allow vehicles to turn left from one two-way RGW-road to an intersecting/orthogonal two-way RGW-road while allowing maximum flow on the intersecting RGW-roads.","In addition to introducing novel traffic signal control strategies, the methods presented in this paper have implications for: road network design, public transport control, connected and automated vehicles and environmental impacts."],"url":"http://arxiv.org/abs/2404.16592v1","category":"eess.SY"}
{"created":"2024-04-25 13:13:30","title":"SB-ETAS: using simulation based inference for scalable, likelihood-free inference for the ETAS model of earthquake occurrences","abstract":"Performing Bayesian inference for the Epidemic-Type Aftershock Sequence (ETAS) model of earthquakes typically requires MCMC sampling using the likelihood function or estimating the latent branching structure. These tasks have computational complexity $O(n^2)$ with the number of earthquakes and therefore do not scale well with new enhanced catalogs, which can now contain an order of $10^6$ events. On the other hand, simulation from the ETAS model can be done more quickly $O(n \\log n )$. We present SB-ETAS: simulation-based inference for the ETAS model. This is an approximate Bayesian method which uses Sequential Neural Posterior Estimation (SNPE), a machine learning based algorithm for learning posterior distributions from simulations. SB-ETAS can successfully approximate ETAS posterior distributions on shorter catalogues where it is computationally feasible to compare with MCMC sampling. Furthermore, the scaling of SB-ETAS makes it feasible to fit to very large earthquake catalogs, such as one for Southern California dating back to 1932. SB-ETAS can find Bayesian estimates of ETAS parameters for this catalog in less than 10 hours on a standard laptop, which would have taken over 2 weeks using MCMC. Looking beyond the standard ETAS model, this simulation based framework would allow earthquake modellers to define and infer parameters for much more complex models that have intractable likelihood functions.","sentences":["Performing Bayesian inference for the Epidemic-Type Aftershock Sequence (ETAS) model of earthquakes typically requires MCMC sampling using the likelihood function or estimating the latent branching structure.","These tasks have computational complexity $O(n^2)$ with the number of earthquakes and therefore do not scale well with new enhanced catalogs, which can now contain an order of $10^6$ events.","On the other hand, simulation from the ETAS model can be done more quickly $O(n \\log n )$.","We present SB-ETAS: simulation-based inference for the ETAS model.","This is an approximate Bayesian method which uses Sequential Neural Posterior Estimation (SNPE), a machine learning based algorithm for learning posterior distributions from simulations.","SB-ETAS can successfully approximate ETAS posterior distributions on shorter catalogues where it is computationally feasible to compare with MCMC sampling.","Furthermore, the scaling of SB-ETAS makes it feasible to fit to very large earthquake catalogs, such as one for Southern California dating back to 1932.","SB-ETAS can find Bayesian estimates of ETAS parameters for this catalog in less than 10 hours on a standard laptop, which would have taken over 2 weeks using MCMC.","Looking beyond the standard ETAS model, this simulation based framework would allow earthquake modellers to define and infer parameters for much more complex models that have intractable likelihood functions."],"url":"http://arxiv.org/abs/2404.16590v1","category":"stat.AP"}
{"created":"2024-04-25 13:13:21","title":"Proving Behavioural Apartness","abstract":"Bisimilarity is a central notion for coalgebras. In recent work, Geuvers and Jacobs suggest to focus on apartness, which they define by dualising coalgebraic bisimulations. This yields the possibility of finite proofs of distinguishability for a wide variety of state-based systems.   We propose behavioural apartness, defined by dualising behavioural equivalence rather than bisimulations. A motivating example is the subdistribution functor, where the proof system based on bisimilarity requires an infinite quantification over couplings, whereas behavioural apartness instantiates to a finite rule. In addition, we provide optimised proof rules for behavioural apartness and show their use in several examples.","sentences":["Bisimilarity is a central notion for coalgebras.","In recent work, Geuvers and Jacobs suggest to focus on apartness, which they define by dualising coalgebraic bisimulations.","This yields the possibility of finite proofs of distinguishability for a wide variety of state-based systems.   ","We propose behavioural apartness, defined by dualising behavioural equivalence rather than bisimulations.","A motivating example is the subdistribution functor, where the proof system based on bisimilarity requires an infinite quantification over couplings, whereas behavioural apartness instantiates to a finite rule.","In addition, we provide optimised proof rules for behavioural apartness and show their use in several examples."],"url":"http://arxiv.org/abs/2404.16588v1","category":"cs.LO"}
{"created":"2024-04-25 13:08:25","title":"Reduced density matrix formulation of quantum linear response","abstract":"The prediction of spectral properties via linear response (LR) theory is an important tool in quantum chemistry for understanding photo-induced processes in molecular systems. With the advances of quantum computing, we recently adapted this method for near-term quantum hardware using a truncated active space approximation with orbital rotation, named quantum linear response (qLR). In an effort to reduce the classic cost of this hybrid approach, we here derive and implement a reduced density matrix (RDM) driven approach of qLR. This allows for the calculation of spectral properties of moderately sized molecules with much larger basis sets than so far possible. We report qLR results for benzene and $R$-methyloxirane with a cc-pVTZ basis set and study the effect of shot noise on the valence and oxygen K-edge absorption spectra of H$_2$O in the cc-pVTZ basis.","sentences":["The prediction of spectral properties via linear response (LR) theory is an important tool in quantum chemistry for understanding photo-induced processes in molecular systems.","With the advances of quantum computing, we recently adapted this method for near-term quantum hardware using a truncated active space approximation with orbital rotation, named quantum linear response (qLR).","In an effort to reduce the classic cost of this hybrid approach, we here derive and implement a reduced density matrix (RDM) driven approach of qLR.","This allows for the calculation of spectral properties of moderately sized molecules with much larger basis sets than so far possible.","We report qLR results for benzene and $R$-methyloxirane with a cc-pVTZ basis set and study the effect of shot noise on the valence and oxygen K-edge absorption spectra of H$_2$O in the cc-pVTZ basis."],"url":"http://arxiv.org/abs/2404.16586v1","category":"physics.chem-ph"}
{"created":"2024-04-25 12:56:23","title":"Directional intermodular coupling enriches functional complexity in biological neuronal networks","abstract":"Hierarchically modular organization is a canonical network topology that is evolutionarily conserved in the nervous systems of animals. Within the network, neurons form directional connections defined by the growth of their axonal terminals. However, this topology is dissimilar to the network formed by dissociated neurons in culture because they form randomly connected networks on homogeneous substrates. In this study, we fabricated microfluidic devices to reconstitute hierarchically modular neuronal networks in culture (in vitro) and investigated how non-random structures, such as directional connectivity between modules, affect global network dynamics. Embedding directional connections in a pseudo-feedforward manner suppressed excessive synchrony in cultured neuronal networks and enhanced the integration-segregation balance. Modeling the behavior of biological neuronal networks using spiking neural networks (SNNs) further revealed that modularity and directionality cooperate to shape such network dynamics. Finally, we demonstrate that for a given network topology, the statistics of network dynamics, such as global network activation, correlation coefficient, and functional complexity, can be analytically predicted based on eigendecomposition of the transition matrix in the state-transition model. Hence, the integration of bioengineering and cell culture technologies enables us not only to reconstitute complex network circuitry in the nervous system but also to understand the structure-function relationships in biological neuronal networks by bridging theoretical modeling with in vitro experiments.","sentences":["Hierarchically modular organization is a canonical network topology that is evolutionarily conserved in the nervous systems of animals.","Within the network, neurons form directional connections defined by the growth of their axonal terminals.","However, this topology is dissimilar to the network formed by dissociated neurons in culture because they form randomly connected networks on homogeneous substrates.","In this study, we fabricated microfluidic devices to reconstitute hierarchically modular neuronal networks in culture (in vitro) and investigated how non-random structures, such as directional connectivity between modules, affect global network dynamics.","Embedding directional connections in a pseudo-feedforward manner suppressed excessive synchrony in cultured neuronal networks and enhanced the integration-segregation balance.","Modeling the behavior of biological neuronal networks using spiking neural networks (SNNs) further revealed that modularity and directionality cooperate to shape such network dynamics.","Finally, we demonstrate that for a given network topology, the statistics of network dynamics, such as global network activation, correlation coefficient, and functional complexity, can be analytically predicted based on eigendecomposition of the transition matrix in the state-transition model.","Hence, the integration of bioengineering and cell culture technologies enables us not only to reconstitute complex network circuitry in the nervous system but also to understand the structure-function relationships in biological neuronal networks by bridging theoretical modeling with in vitro experiments."],"url":"http://arxiv.org/abs/2404.16582v1","category":"q-bio.NC"}
{"created":"2024-04-25 12:46:22","title":"Stokes-Brinkman-Darcy models for fluid-porous systems: derivation, analysis and validation","abstract":"Flow interaction between a plain-fluid region in contact with a porous layer attracted significant attention from modelling and analysis sides due to numerous applications in biology, environment and industry. In the most widely used coupled model, fluid flow is described by the Stokes equations in the free-flow domain and Darcy's law in the porous medium, and complemented by the appropriate interface conditions. However, traditional coupling concepts are restricted, with a few exceptions, to one-dimensional flows parallel to the fluid-porous interface. In this work, we use an alternative approach to model interaction between the plain-fluid domain and porous medium by considering a transition zone, and propose the full- and hybrid-dimensional Stokes-Brinkman-Darcy models. In the first case, the equi-dimensional Brinkman equations are considered in the transition region, and the appropriate interface conditions are set on the top and bottom of the transition zone. In the latter case, we perform a dimensional model reduction by averaging the Brinkman equations in the normal direction and using the proposed transmission conditions. The well-posedness of both coupled problems is proved, and some numerical simulations are carried out in order to validate the concepts.","sentences":["Flow interaction between a plain-fluid region in contact with a porous layer attracted significant attention from modelling and analysis sides due to numerous applications in biology, environment and industry.","In the most widely used coupled model, fluid flow is described by the Stokes equations in the free-flow domain and Darcy's law in the porous medium, and complemented by the appropriate interface conditions.","However, traditional coupling concepts are restricted, with a few exceptions, to one-dimensional flows parallel to the fluid-porous interface.","In this work, we use an alternative approach to model interaction between the plain-fluid domain and porous medium by considering a transition zone, and propose the full- and hybrid-dimensional Stokes-Brinkman-Darcy models.","In the first case, the equi-dimensional Brinkman equations are considered in the transition region, and the appropriate interface conditions are set on the top and bottom of the transition zone.","In the latter case, we perform a dimensional model reduction by averaging the Brinkman equations in the normal direction and using the proposed transmission conditions.","The well-posedness of both coupled problems is proved, and some numerical simulations are carried out in order to validate the concepts."],"url":"http://arxiv.org/abs/2404.16577v1","category":"math.NA"}
{"created":"2024-04-25 12:32:01","title":"Nucleation transitions in polycontextural networks towards consensus","abstract":"Recently, we proposed polycontextural networks as a model of evolving systems of interacting beliefs. Here, we present an analysis of the phase transition as well as the scaling properties. The model contains interacting agents that strive for consensus, each with only subjective perception. Depending on a parameter that governs how responsive the agents are to changing their belief systems the model exhibits a phase transition that mediates between an active phase where the agents constantly change their beliefs and a frozen phase, where almost no changes appear. We observe the build-up of convention-aligned clusters only in the intermediate regime of diverging susceptibility. Here, we analyze in detail the behavior of polycontextural networks close to this transition. We provide an analytical estimate of the critical point and show that the scaling properties and the space-time structure of these clusters show self-similar behavior. Our results not only contribute to a better understanding of the emergence of consensus in systems of distributed beliefs but also show that polycontextural networks are models, motivated by social systems, where susceptibility -- the sensitivity to change own beliefs -- drives the growth of consensus clusters.","sentences":["Recently, we proposed polycontextural networks as a model of evolving systems of interacting beliefs.","Here, we present an analysis of the phase transition as well as the scaling properties.","The model contains interacting agents that strive for consensus, each with only subjective perception.","Depending on a parameter that governs how responsive the agents are to changing their belief systems the model exhibits a phase transition that mediates between an active phase where the agents constantly change their beliefs and a frozen phase, where almost no changes appear.","We observe the build-up of convention-aligned clusters only in the intermediate regime of diverging susceptibility.","Here, we analyze in detail the behavior of polycontextural networks close to this transition.","We provide an analytical estimate of the critical point and show that the scaling properties and the space-time structure of these clusters show self-similar behavior.","Our results not only contribute to a better understanding of the emergence of consensus in systems of distributed beliefs but also show that polycontextural networks are models, motivated by social systems, where susceptibility -- the sensitivity to change own beliefs -- drives the growth of consensus clusters."],"url":"http://arxiv.org/abs/2404.16569v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-25 12:29:04","title":"J-PLUS: Bayesian object classification with a strum of BANNJOS","abstract":"With its 12 optical filters, the Javalambre-Photometric Local Universe Survey (J-PLUS) provides an unprecedented multicolor view of the local Universe. The third data release (DR3) covers 3,192 deg$^2$ and contains 47.4 million objects. However, the classification algorithms currently implemented in its pipeline are deterministic and based solely on the sources morphology. Our goal is classify the sources identified in the J-PLUS DR3 images into stars, quasi-stellar objects (QSOs), and galaxies. For this task, we present BANNJOS, a machine learning pipeline that uses Bayesian neural networks to provide the probability distribution function (PDF) of the classification. BANNJOS is trained on photometric, astrometric, and morphological data from J-PLUS DR3, Gaia DR3, and CatWISE2020, using over 1.2 million objects with spectroscopic classification from SDSS DR18, LAMOST DR9, DESI EDR, and Gaia DR3. Results are validated using $1.4 10^5$ objects and cross-checked against theoretical model predictions. BANNJOS outperforms all previous classifiers in terms of accuracy, precision, and completeness across the entire magnitude range. It delivers over 95% accuracy for objects brighter than $r = 21.5$ mag, and ~90% accuracy for those up to $r = 22$ mag, where J-PLUS completeness is < 25%. BANNJOS is also the first object classifier to provide the full probability distribution function (PDF) of the classification, enabling precise object selection for high purity or completeness, and for identifying objects with complex features, like active galactic nuclei with resolved host galaxies. BANNJOS has effectively classified J-PLUS sources into around 20 million galaxies, 1 million QSOs, and 26 million stars, with full PDFs for each, which allow for later refinement of the sample. The upcoming J-PAS survey, with its 56 color bands, will further enhance BANNJOS's ability to detail each source's nature.","sentences":["With its 12 optical filters, the Javalambre-Photometric Local Universe Survey (J-PLUS) provides an unprecedented multicolor view of the local Universe.","The third data release (DR3) covers 3,192 deg$^2$ and contains 47.4 million objects.","However, the classification algorithms currently implemented in its pipeline are deterministic and based solely on the sources morphology.","Our goal is classify the sources identified in the J-PLUS DR3 images into stars, quasi-stellar objects (QSOs), and galaxies.","For this task, we present BANNJOS, a machine learning pipeline that uses Bayesian neural networks to provide the probability distribution function (PDF) of the classification.","BANNJOS is trained on photometric, astrometric, and morphological data from J-PLUS DR3, Gaia DR3, and CatWISE2020, using over 1.2 million objects with spectroscopic classification from SDSS DR18, LAMOST DR9, DESI EDR, and Gaia DR3.","Results are validated using $1.4 10^5$ objects and cross-checked against theoretical model predictions.","BANNJOS outperforms all previous classifiers in terms of accuracy, precision, and completeness across the entire magnitude range.","It delivers over 95% accuracy for objects brighter than $r = 21.5$ mag, and ~90% accuracy for those up to $r = 22$ mag, where J-PLUS completeness is < 25%.","BANNJOS is also the first object classifier to provide the full probability distribution function (PDF) of the classification, enabling precise object selection for high purity or completeness, and for identifying objects with complex features, like active galactic nuclei with resolved host galaxies.","BANNJOS has effectively classified J-PLUS sources into around 20 million galaxies, 1 million QSOs, and 26 million stars, with full PDFs for each, which allow for later refinement of the sample.","The upcoming J-PAS survey, with its 56 color bands, will further enhance BANNJOS's ability to detail each source's nature."],"url":"http://arxiv.org/abs/2404.16567v1","category":"astro-ph.GA"}
{"created":"2024-04-25 12:16:22","title":"Conductivity of lattice bosons at high temperatures","abstract":"Quantum simulations are quickly becoming an indispensable tool for studying particle transport in correlated lattice models. One of the central topics in the study of transport is the bad-metal behavior, characterized by the direct current (dc) resistivity linear in temperature. In the fermionic Hubbard model, optical conductivity has been studied extensively, and a recent optical lattice experiment has demonstrated bad metal behavior in qualitative agreement with theory. Far less is known about transport in the bosonic Hubbard model. We investigate the conductivity in the Bose-Hubbard model, and focus on the regime of strong interactions and high-temperatures. We use numerically exact calculations for small lattice sizes. At weak tunneling, we find multiple peaks in the optical conductivity that stem from the Hubbard bands present in the many-body spectrum. This feature slowly washes out as the tunneling rate gets stronger. At high temperature, we identify a regime of $T$-linear resistivity, as expected. When the interactions are very strong, the leading inverse-temperature coefficient in conductivity is proportional to the tunneling amplitude. As the tunneling becomes stronger, this dependence takes quadratic form. At very strong coupling and half filling, we identify a separate linear resistivity regime at lower temperature, corresponding to the hard-core boson regime. Additionally, we unexpectedly observe that at half filling, in a big part of the phase diagram, conductivity is an increasing function of the coupling constant before it saturates at the hard-core-boson result. We explain this feature based on the analysis of the many-body energy spectrum and the contributions to conductivity of individual eigenstates of the system.","sentences":["Quantum simulations are quickly becoming an indispensable tool for studying particle transport in correlated lattice models.","One of the central topics in the study of transport is the bad-metal behavior, characterized by the direct current (dc) resistivity linear in temperature.","In the fermionic Hubbard model, optical conductivity has been studied extensively, and a recent optical lattice experiment has demonstrated bad metal behavior in qualitative agreement with theory.","Far less is known about transport in the bosonic Hubbard model.","We investigate the conductivity in the Bose-Hubbard model, and focus on the regime of strong interactions and high-temperatures.","We use numerically exact calculations for small lattice sizes.","At weak tunneling, we find multiple peaks in the optical conductivity that stem from the Hubbard bands present in the many-body spectrum.","This feature slowly washes out as the tunneling rate gets stronger.","At high temperature, we identify a regime of $T$-linear resistivity, as expected.","When the interactions are very strong, the leading inverse-temperature coefficient in conductivity is proportional to the tunneling amplitude.","As the tunneling becomes stronger, this dependence takes quadratic form.","At very strong coupling and half filling, we identify a separate linear resistivity regime at lower temperature, corresponding to the hard-core boson regime.","Additionally, we unexpectedly observe that at half filling, in a big part of the phase diagram, conductivity is an increasing function of the coupling constant before it saturates at the hard-core-boson result.","We explain this feature based on the analysis of the many-body energy spectrum and the contributions to conductivity of individual eigenstates of the system."],"url":"http://arxiv.org/abs/2404.16559v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-25 12:04:31","title":"Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System","abstract":"In this paper, we propose a novel approach to address the problem of camera and radar sensor fusion for 3D object detection in autonomous vehicle perception systems. Our approach builds on recent advances in deep learning and leverages the strengths of both sensors to improve object detection performance. Precisely, we extract 2D features from camera images using a state-of-the-art deep learning architecture and then apply a novel Cross-Domain Spatial Matching (CDSM) transformation method to convert these features into 3D space. We then fuse them with extracted radar data using a complementary fusion strategy to produce a final 3D object representation. To demonstrate the effectiveness of our approach, we evaluate it on the NuScenes dataset. We compare our approach to both single-sensor performance and current state-of-the-art fusion methods. Our results show that the proposed approach achieves superior performance over single-sensor solutions and could directly compete with other top-level fusion methods.","sentences":["In this paper, we propose a novel approach to address the problem of camera and radar sensor fusion for 3D object detection in autonomous vehicle perception systems.","Our approach builds on recent advances in deep learning and leverages the strengths of both sensors to improve object detection performance.","Precisely, we extract 2D features from camera images using a state-of-the-art deep learning architecture and then apply a novel Cross-Domain Spatial Matching (CDSM) transformation method to convert these features into 3D space.","We then fuse them with extracted radar data using a complementary fusion strategy to produce a final 3D object representation.","To demonstrate the effectiveness of our approach, we evaluate it on the NuScenes dataset.","We compare our approach to both single-sensor performance and current state-of-the-art fusion methods.","Our results show that the proposed approach achieves superior performance over single-sensor solutions and could directly compete with other top-level fusion methods."],"url":"http://arxiv.org/abs/2404.16548v1","category":"cs.CV"}
{"created":"2024-04-25 12:00:27","title":"On CR maps between hyperquadrics and Winkelmann hypersurfaces","abstract":"In this paper, we study CR maps between hyperquadrics and Winkelmann hypersurfaces. Based on a previous study on the CR Ahlfors derivative of Lamel-Son and a recent result of Huang-Lu-Tang-Xiao on CR maps between hyperquadrics, we prove that a transversal CR map from a hyperquadric into a hyperquadric or a Winkelmann hypersurface extends to a local holomorphic isometric embedding with respect to certain K\\\"ahler metrics if and only if the Hermitian part of its CR Ahlfors derivative vanishes on an open set of the source. Our proof is based on relating the geometric rank of a CR map into a hyperquadric and its CR Ahlfors derivative.","sentences":["In this paper, we study CR maps between hyperquadrics and Winkelmann hypersurfaces.","Based on a previous study on the CR Ahlfors derivative of Lamel-Son and a recent result of Huang-Lu-Tang-Xiao on CR maps between hyperquadrics, we prove that a transversal CR map from a hyperquadric into a hyperquadric or a Winkelmann hypersurface extends to a local holomorphic isometric embedding with respect to certain K\\\"ahler metrics if and only if the Hermitian part of its CR Ahlfors derivative vanishes on an open set of the source.","Our proof is based on relating the geometric rank of a CR map into a hyperquadric and its CR Ahlfors derivative."],"url":"http://arxiv.org/abs/2404.16543v1","category":"math.CV"}
{"created":"2024-04-25 11:47:28","title":"AMEP: The Active Matter Evaluation Package for Python","abstract":"The Active Matter Evaluation Package (AMEP) is a Python library for analyzing simulation data of particle-based and continuum simulations. It provides a powerful and simple interface for handling large data sets and for calculating and visualizing a broad variety of observables that are relevant to active matter systems. Examples range from the mean-square displacement and the structure factor to cluster-size distributions, binder cumulants, and growth exponents. AMEP is written in pure Python and is based on powerful libraries such as NumPy, SciPy, Matplotlib, and scikit-image. Computationally expensive methods are parallelized and optimized to run efficiently on workstations, laptops, and high-performance computing architectures, and an HDF5-based data format is used in the backend to store and handle simulation data as well as analysis results. AMEP provides the first comprehensive framework for analyzing simulation results of both particle-based and continuum simulations (as well as experimental data) of active matter systems. In particular, AMEP also allows it to analyze simulations that combine particle-based and continuum techniques such as used to study the motion of bacteria in chemical fields or for modeling particle motion in a flow field. AMEP is available at https://amepproject.de and can be installed via conda and pip.","sentences":["The Active Matter Evaluation Package (AMEP) is a Python library for analyzing simulation data of particle-based and continuum simulations.","It provides a powerful and simple interface for handling large data sets and for calculating and visualizing a broad variety of observables that are relevant to active matter systems.","Examples range from the mean-square displacement and the structure factor to cluster-size distributions, binder cumulants, and growth exponents.","AMEP is written in pure Python and is based on powerful libraries such as NumPy, SciPy, Matplotlib, and scikit-image.","Computationally expensive methods are parallelized and optimized to run efficiently on workstations, laptops, and high-performance computing architectures, and an HDF5-based data format is used in the backend to store and handle simulation data as well as analysis results.","AMEP provides the first comprehensive framework for analyzing simulation results of both particle-based and continuum simulations (as well as experimental data) of active matter systems.","In particular, AMEP also allows it to analyze simulations that combine particle-based and continuum techniques such as used to study the motion of bacteria in chemical fields or for modeling particle motion in a flow field.","AMEP is available at https://amepproject.de and can be installed via conda and pip."],"url":"http://arxiv.org/abs/2404.16533v1","category":"cond-mat.soft"}
{"created":"2024-04-25 11:42:59","title":"Nonlinear dynamics of a hanging string with a freely pivoting attached mass","abstract":"We show that the natural resonant frequency of a suspended flexible string is significantly modified (by one order of magnitude) by adding a freely pivoting attached mass at its lower end. This articulated system then exhibits complex nonlinear dynamics such as bending oscillations, similar to those of a swing becoming slack, thereby strongly modifying the system resonance that is found to be controlled by the length of the pivoting mass. The dynamics is experimentally studied using a remote and noninvasive magnetic parametric forcing. To do so, a permanent magnet is suspended by a flexible string above a vertically oscillating conductive plate. Harmonic and period-doubling instabilities are experimentally reported and are modeled using the Hill equation, leading to analytical solutions that accurately describe the experimentally observed tonguelike instability curves.","sentences":["We show that the natural resonant frequency of a suspended flexible string is significantly modified (by one order of magnitude) by adding a freely pivoting attached mass at its lower end.","This articulated system then exhibits complex nonlinear dynamics such as bending oscillations, similar to those of a swing becoming slack, thereby strongly modifying the system resonance that is found to be controlled by the length of the pivoting mass.","The dynamics is experimentally studied using a remote and noninvasive magnetic parametric forcing.","To do so, a permanent magnet is suspended by a flexible string above a vertically oscillating conductive plate.","Harmonic and period-doubling instabilities are experimentally reported and are modeled using the Hill equation, leading to analytical solutions that accurately describe the experimentally observed tonguelike instability curves."],"url":"http://arxiv.org/abs/2404.16531v1","category":"nlin.PS"}
{"created":"2024-04-25 11:42:43","title":"On the Political Economy of Link-based Web Search","abstract":"Web search engines arguably form the most popular data-driven systems in contemporary society. They wield a considerable power by functioning as gatekeepers of the Web, with most user journeys on the Web beginning with them. Starting from the late 1990s, search engines have been dominated by the paradigm of link-based web search. In this paper, we critically analyze the political economy of the paradigm of link-based web search, drawing upon insights and methodologies from critical political economy. We draw several insights on how link-based web search has led to phenomena that favor capital through long-term structural changes on the Web, and how it has led to accentuating unpaid digital labor and ecologically unsustainable practices, among several others. We show how contemporary observations on the degrading quality of link-based web search can be traced back to the internal contradictions with the paradigm, and how such socio-technical phenomena may lead to a disutility of the link-based web search model. Our contribution is primarily on enhancing the understanding of the political economy of link-based web search, and laying bare the phenomena at work, and implicitly catalyze the search for alternative models.","sentences":["Web search engines arguably form the most popular data-driven systems in contemporary society.","They wield a considerable power by functioning as gatekeepers of the Web, with most user journeys on the Web beginning with them.","Starting from the late 1990s, search engines have been dominated by the paradigm of link-based web search.","In this paper, we critically analyze the political economy of the paradigm of link-based web search, drawing upon insights and methodologies from critical political economy.","We draw several insights on how link-based web search has led to phenomena that favor capital through long-term structural changes on the Web, and how it has led to accentuating unpaid digital labor and ecologically unsustainable practices, among several others.","We show how contemporary observations on the degrading quality of link-based web search can be traced back to the internal contradictions with the paradigm, and how such socio-technical phenomena may lead to a disutility of the link-based web search model.","Our contribution is primarily on enhancing the understanding of the political economy of link-based web search, and laying bare the phenomena at work, and implicitly catalyze the search for alternative models."],"url":"http://arxiv.org/abs/2404.16530v1","category":"cs.CY"}
{"created":"2024-04-25 11:42:32","title":"Vision-based robot manipulation of transparent liquid containers in a laboratory setting","abstract":"Laboratory processes involving small volumes of solutions and active ingredients are often performed manually due to challenges in automation, such as high initial costs, semi-structured environments and protocol variability. In this work, we develop a flexible and cost-effective approach to address this gap by introducing a vision-based system for liquid volume estimation and a simulation-driven pouring method particularly designed for containers with small openings. We evaluate both components individually, followed by an applied real-world integration of cell culture automation using a UR5 robotic arm. Our work is fully reproducible: we share our code at at \\url{https://github.com/DaniSchober/LabLiquidVision} and the newly introduced dataset LabLiquidVolume is available at https://data.dtu.dk/articles/dataset/LabLiquidVision/25103102.","sentences":["Laboratory processes involving small volumes of solutions and active ingredients are often performed manually due to challenges in automation, such as high initial costs, semi-structured environments and protocol variability.","In this work, we develop a flexible and cost-effective approach to address this gap by introducing a vision-based system for liquid volume estimation and a simulation-driven pouring method particularly designed for containers with small openings.","We evaluate both components individually, followed by an applied real-world integration of cell culture automation using a UR5 robotic arm.","Our work is fully reproducible: we share our code at at \\url{https://github.com/DaniSchober/LabLiquidVision} and the newly introduced dataset LabLiquidVolume is available at https://data.dtu.dk/articles/dataset/LabLiquidVision/25103102."],"url":"http://arxiv.org/abs/2404.16529v1","category":"cs.RO"}
{"created":"2024-04-25 11:38:30","title":"An efficient approach for searching three-body periodic orbits passing through Eulerian configuration","abstract":"A new efficient approach for searching three-body periodic equal-mass collisionless orbits passing through Eulerian configuration is presented. The approach is based on a symmetry property of the solutions at the half period. Depending on two previously established symmetry types on the shape sphere, each solution is presented by one or two distinct initial conditions (one or two points in the search domain). A numerical search based on Newton's method on a relatively coarse search grid for solutions with relatively small scale-invariant periods $T^{*}<70$ is conducted. The linear systems at each Newton's iteration are computed by high order high precision Taylor series method. The search produced 12,431 initial conditions (i.c.s) corresponding to 6,333 distinct solutions. In addition to passing through the Eulerian configuration, 35 of the solutions are also free-fall ones. Although most of the found solutions are new, all linearly stable solutions among them (only 7) are old ones. Particular attention is paid to the details of the high precision computations and the analysis of accuracy. All i.c.s are given with 100 correct digits.","sentences":["A new efficient approach for searching three-body periodic equal-mass collisionless orbits passing through Eulerian configuration is presented.","The approach is based on a symmetry property of the solutions at the half period.","Depending on two previously established symmetry types on the shape sphere, each solution is presented by one or two distinct initial conditions (one or two points in the search domain).","A numerical search based on Newton's method on a relatively coarse search grid for solutions with relatively small scale-invariant periods $T^{*}<70$ is conducted.","The linear systems at each Newton's iteration are computed by high order high precision Taylor series method.","The search produced 12,431 initial conditions (i.c.s) corresponding to 6,333 distinct solutions.","In addition to passing through the Eulerian configuration, 35 of the solutions are also free-fall ones.","Although most of the found solutions are new, all linearly stable solutions among them (only 7) are old ones.","Particular attention is paid to the details of the high precision computations and the analysis of accuracy.","All i.c.s are given with 100 correct digits."],"url":"http://arxiv.org/abs/2404.16526v1","category":"physics.class-ph"}
{"created":"2024-04-25 11:35:26","title":"Anomalous Directed Percolation on a Dynamic Network using Rydberg Facilitation","abstract":"The facilitation of Rydberg excitations in a gas of atoms provides an ideal model system to study epidemic evolution on (dynamic) networks and self organization of complex systems to the critical point of a non-equilibrium phase transition. Using Monte-Carlo simulations and a machine learning algorithm we show that the universality class of this phase transition can be tuned. The classes include directed percolation (DP), the most common class in short-range spreading models, and mean-field (MF) behavior, but also different types of anomalous directed percolation (ADP), characterized by rare long-range excitation processes. In a frozen gas, ground state atoms that can facilitate each other form a static network, for which we predict DP universality. Atomic motion then turns the network into a dynamic one with long-range (Levy-flight type) excitations. This leads to continuously varying critical exponents corresponding to the ADP universality class, eventually reaching MF behavior. These findings also explain the recently observed critical exponent of Rydberg facilitation in an ultra-cold gas experiment [Helmrich et al., Nature 577, 481 (2020)], which was in between DP and MF values.","sentences":["The facilitation of Rydberg excitations in a gas of atoms provides an ideal model system to study epidemic evolution on (dynamic) networks and self organization of complex systems to the critical point of a non-equilibrium phase transition.","Using Monte-Carlo simulations and a machine learning algorithm we show that the universality class of this phase transition can be tuned.","The classes include directed percolation (DP), the most common class in short-range spreading models, and mean-field (MF) behavior, but also different types of anomalous directed percolation (ADP), characterized by rare long-range excitation processes.","In a frozen gas, ground state atoms that can facilitate each other form a static network, for which we predict DP universality.","Atomic motion then turns the network into a dynamic one with long-range (Levy-flight type) excitations.","This leads to continuously varying critical exponents corresponding to the ADP universality class, eventually reaching MF behavior.","These findings also explain the recently observed critical exponent of Rydberg facilitation in an ultra-cold gas experiment [Helmrich et al., Nature 577, 481 (2020)], which was in between DP and MF values."],"url":"http://arxiv.org/abs/2404.16523v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-25 11:23:11","title":"Topological properties of finite-size heterostructures of magnetic topological insulators and superconductors","abstract":"Heterostructures of magnetic topological insulators (MTIs) and superconductors (SCs) in two-dimensional (2D) slab and one-dimensional (1D) nanoribbon geometries have been predicted to host, respectively, chiral Majorana edge states (CMESs) and Majorana bound states (MBSs). We study the topological properties of such MTI/SC heterostructures upon variation of the geometry from wide slabs to quasi-1D nanoribbon systems and as a function of the chemical potential, the magnetic doping, and the induced superconducting pairing potential. To do so, we construct effective symmetry-constrained low-energy Hamiltonians accounting for the real-space confinement. For a nanoribbon geometry with finite width and length, we observe different phases characterized by CMESs, MBSs, as well as coexisting CMESs and MBSs, as the chemical potential, the magnetic doping and/or the width are varied.","sentences":["Heterostructures of magnetic topological insulators (MTIs) and superconductors (SCs) in two-dimensional (2D) slab and one-dimensional (1D) nanoribbon geometries have been predicted to host, respectively, chiral Majorana edge states (CMESs) and Majorana bound states (MBSs).","We study the topological properties of such MTI/SC heterostructures upon variation of the geometry from wide slabs to quasi-1D nanoribbon systems and as a function of the chemical potential, the magnetic doping, and the induced superconducting pairing potential.","To do so, we construct effective symmetry-constrained low-energy Hamiltonians accounting for the real-space confinement.","For a nanoribbon geometry with finite width and length, we observe different phases characterized by CMESs, MBSs, as well as coexisting CMESs and MBSs, as the chemical potential, the magnetic doping and/or the width are varied."],"url":"http://arxiv.org/abs/2404.16520v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-25 11:15:26","title":"Adaptive Learning-based Model Predictive Control for Uncertain Interconnected Systems: A Set Membership Identification Approach","abstract":"We propose a novel adaptive learning-based model predictive control (MPC) scheme for interconnected systems which can be decomposed into several smaller dynamically coupled subsystems with uncertain coupling. The proposed scheme is mainly divided into two main online phases; a learning phase and an adaptation phase. Set membership identification is used in the learning phase to learn an uncertainty set that contains the coupling strength using online data. In the adaptation phase, rigid tube-based robust MPC is used to compute the optimal predicted states and inputs. Besides computing the optimal trajectories, the MPC ingredients are adapted in the adaptation phase taking the learnt uncertainty set into account. These MPC ingredients include the prestabilizing controller, the rigid tube, the tightened constraints and the terminal ingredients. The recursive feasibility of the proposed scheme as well as the stability of the corresponding closed-loop system are discussed. The developed scheme is compared in simulations to existing schemes including robust, adaptive and learning-based MPC.","sentences":["We propose a novel adaptive learning-based model predictive control (MPC) scheme for interconnected systems which can be decomposed into several smaller dynamically coupled subsystems with uncertain coupling.","The proposed scheme is mainly divided into two main online phases; a learning phase and an adaptation phase.","Set membership identification is used in the learning phase to learn an uncertainty set that contains the coupling strength using online data.","In the adaptation phase, rigid tube-based robust MPC is used to compute the optimal predicted states and inputs.","Besides computing the optimal trajectories, the MPC ingredients are adapted in the adaptation phase taking the learnt uncertainty set into account.","These MPC ingredients include the prestabilizing controller, the rigid tube, the tightened constraints and the terminal ingredients.","The recursive feasibility of the proposed scheme as well as the stability of the corresponding closed-loop system are discussed.","The developed scheme is compared in simulations to existing schemes including robust, adaptive and learning-based MPC."],"url":"http://arxiv.org/abs/2404.16514v1","category":"eess.SY"}
{"created":"2024-04-25 11:01:40","title":"Semantic-aware Next-Best-View for Multi-DoFs Mobile System in Search-and-Acquisition based Visual Perception","abstract":"Efficient visual perception using mobile systems is crucial, particularly in unknown environments such as search and rescue operations, where swift and comprehensive perception of objects of interest is essential. In such real-world applications, objects of interest are often situated in complex environments, making the selection of the 'Next Best' view based solely on maximizing visibility gain suboptimal. Semantics, providing a higher-level interpretation of perception, should significantly contribute to the selection of the next viewpoint for various perception tasks. In this study, we formulate a novel information gain that integrates both visibility gain and semantic gain in a unified form to select the semantic-aware Next-Best-View. Additionally, we design an adaptive strategy with termination criterion to support a two-stage search-and-acquisition manoeuvre on multiple objects of interest aided by a multi-degree-of-freedoms (Multi-DoFs) mobile system. Several semantically relevant reconstruction metrics, including perspective directivity and region of interest (ROI)-to-full reconstruction volume ratio, are introduced to evaluate the performance of the proposed approach. Simulation experiments demonstrate the advantages of the proposed approach over existing methods, achieving improvements of up to 27.13% for the ROI-to-full reconstruction volume ratio and a 0.88234 average perspective directivity. Furthermore, the planned motion trajectory exhibits better perceiving coverage toward the target.","sentences":["Efficient visual perception using mobile systems is crucial, particularly in unknown environments such as search and rescue operations, where swift and comprehensive perception of objects of interest is essential.","In such real-world applications, objects of interest are often situated in complex environments, making the selection of the 'Next Best' view based solely on maximizing visibility gain suboptimal.","Semantics, providing a higher-level interpretation of perception, should significantly contribute to the selection of the next viewpoint for various perception tasks.","In this study, we formulate a novel information gain that integrates both visibility gain and semantic gain in a unified form to select the semantic-aware Next-Best-View.","Additionally, we design an adaptive strategy with termination criterion to support a two-stage search-and-acquisition manoeuvre on multiple objects of interest aided by a multi-degree-of-freedoms (Multi-DoFs) mobile system.","Several semantically relevant reconstruction metrics, including perspective directivity and region of interest (ROI)-to-full reconstruction volume ratio, are introduced to evaluate the performance of the proposed approach.","Simulation experiments demonstrate the advantages of the proposed approach over existing methods, achieving improvements of up to 27.13% for the ROI-to-full reconstruction volume ratio and a 0.88234 average perspective directivity.","Furthermore, the planned motion trajectory exhibits better perceiving coverage toward the target."],"url":"http://arxiv.org/abs/2404.16507v1","category":"cs.CV"}
{"created":"2024-04-25 10:52:28","title":"A Prototypical Expert-Driven Approach Towards Capability-Based Monitoring of Automated Driving Systems","abstract":"Supervising the safe operation of automated vehicles is a key requirement in order to unleash their full potential in future transportation systems. In particular, previous publications have argued that SAE Level 4 vehicles should be aware of their capabilities at runtime to make appropriate behavioral decisions. In this paper, we present a framework that enables the implementation of an online capability monitor. We derive a graphical system model that captures the relationships between the quality of system elements across different architectural views. In an expert-driven approach, we parameterize Bayesian Networks based on this structure using Fuzzy Logic. Using the online monitor, we infer the quality of the system's capabilities based on technical measurements acquired at runtime. Our approach is demonstrated in the context of the UNICAR.agil research project in an urban example scenario.","sentences":["Supervising the safe operation of automated vehicles is a key requirement in order to unleash their full potential in future transportation systems.","In particular, previous publications have argued that SAE Level 4 vehicles should be aware of their capabilities at runtime to make appropriate behavioral decisions.","In this paper, we present a framework that enables the implementation of an online capability monitor.","We derive a graphical system model that captures the relationships between the quality of system elements across different architectural views.","In an expert-driven approach, we parameterize Bayesian Networks based on this structure using Fuzzy Logic.","Using the online monitor, we infer the quality of the system's capabilities based on technical measurements acquired at runtime.","Our approach is demonstrated in the context of the UNICAR.agil research project in an urban example scenario."],"url":"http://arxiv.org/abs/2404.16502v1","category":"eess.SY"}
{"created":"2024-04-25 10:45:04","title":"Violation of Bell inequalities in an analogue black hole","abstract":"Signals of entanglement and nonlocality are quantitatively evaluated at zero and finite temperature in an analogue black hole realized in the flow of a quasi one-dimensional Bose-Einstein condensate. The violation of Lorentz invariance inherent to this analog system opens the prospect to observe 3-mode quantum correlations and we study the corresponding violation of bipartite and tripartite Bell inequalities. It is shown that the long wavelength modes of the system are maximally entangled, in the sense that they realize a superposition of continuous variable versions of Greenberger-Horne-Zeilinger states whose entanglement resists partial tracing.","sentences":["Signals of entanglement and nonlocality are quantitatively evaluated at zero and finite temperature in an analogue black hole realized in the flow of a quasi one-dimensional Bose-Einstein condensate.","The violation of Lorentz invariance inherent to this analog system opens the prospect to observe 3-mode quantum correlations and we study the corresponding violation of bipartite and tripartite Bell inequalities.","It is shown that the long wavelength modes of the system are maximally entangled, in the sense that they realize a superposition of continuous variable versions of Greenberger-Horne-Zeilinger states whose entanglement resists partial tracing."],"url":"http://arxiv.org/abs/2404.16497v1","category":"quant-ph"}
{"created":"2024-04-25 10:41:12","title":"Probabilistic Multi-Layer Perceptrons for Wind Farm Condition Monitoring","abstract":"We provide a condition monitoring system for wind farms, based on normal behaviour modelling using a probabilistic multi-layer perceptron with transfer learning via fine-tuning. The model predicts the output power of the wind turbine under normal behaviour based on features retrieved from supervisory control and data acquisition (SCADA) systems. Its advantages are that (i) it can be trained with SCADA data of at least a few years, (ii) it can incorporate all SCADA data of all wind turbines in a wind farm as features, (iii) it assumes that the output power follows a normal density with heteroscedastic variance and (iv) it can predict the output of one wind turbine by borrowing strength from the data of all other wind turbines in a farm. Probabilistic guidelines for condition monitoring are given via a CUSUM control chart. We illustrate the performance of our model in a real SCADA data example which provides evidence that it outperforms other probabilistic prediction models.","sentences":["We provide a condition monitoring system for wind farms, based on normal behaviour modelling using a probabilistic multi-layer perceptron with transfer learning via fine-tuning.","The model predicts the output power of the wind turbine under normal behaviour based on features retrieved from supervisory control and data acquisition (SCADA) systems.","Its advantages are that (i) it can be trained with SCADA data of at least a few years, (ii) it can incorporate all SCADA data of all wind turbines in a wind farm as features, (iii) it assumes that the output power follows a normal density with heteroscedastic variance and (iv) it can predict the output of one wind turbine by borrowing strength from the data of all other wind turbines in a farm.","Probabilistic guidelines for condition monitoring are given via a CUSUM control chart.","We illustrate the performance of our model in a real SCADA data example which provides evidence that it outperforms other probabilistic prediction models."],"url":"http://arxiv.org/abs/2404.16496v1","category":"cs.LG"}
{"created":"2024-04-25 10:32:24","title":"On the topology of concurrent systems","abstract":"Higher-dimensional automata, i.e., pointed labeled precubical sets, are a powerful combinatorial-topological model for concurrent systems. In this paper, we show that for every (nonempty) connected polyhedron there exists a shared-variable system such that the higher-dimensional automaton modeling the state space of the system has the homotopy type of the polyhedron.","sentences":["Higher-dimensional automata, i.e., pointed labeled precubical sets, are a powerful combinatorial-topological model for concurrent systems.","In this paper, we show that for every (nonempty) connected polyhedron there exists a shared-variable system such that the higher-dimensional automaton modeling the state space of the system has the homotopy type of the polyhedron."],"url":"http://arxiv.org/abs/2404.16492v1","category":"cs.FL"}
{"created":"2024-04-25 10:28:06","title":"Ascent and Descent of Weighted Composition Operators on Lorentz spaces","abstract":"The aim of this article is to detect the ascent and descent of weighted composition operators on Lorentz spaces. We investigate the conditions on the measurable transformation $T$ and the complex-valued measurable function $u$ defined on measure space $(X, \\mathcal{A}, \\mu)$ that cause the weighted composition operators on Lorentz space $L(p, q)$, $1 < p \\leq \\infty, 1 \\leq q \\leq \\infty$ to have finite or infinite ascent (descent). We also give a number of examples in support of our findings.","sentences":["The aim of this article is to detect the ascent and descent of weighted composition operators on Lorentz spaces.","We investigate the conditions on the measurable transformation $T$ and the complex-valued measurable function $u$ defined on measure space $(X, \\mathcal{A}, \\mu)$ that cause the weighted composition operators on Lorentz space $L(p, q)$, $1 < p \\leq \\infty, 1 \\leq q \\leq \\infty$ to have finite or infinite ascent (descent).","We also give a number of examples in support of our findings."],"url":"http://arxiv.org/abs/2404.16491v1","category":"math.FA"}
{"created":"2024-04-25 10:26:18","title":"Cost-Driven Data Replication with Predictions","abstract":"This paper studies an online replication problem for distributed data access. The goal is to dynamically create and delete data copies in a multi-server system as time passes to minimize the total storage and network cost of serving access requests. We study the problem in the emergent learning-augmented setting, assuming simple binary predictions about inter-request times at individual servers. We develop an online algorithm and prove that it is ($\\frac{5+\\alpha}{3}$)-consistent (competitiveness under perfect predictions) and ($1 + \\frac{1}{\\alpha}$)-robust (competitiveness under terrible predictions), where $\\alpha \\in (0, 1]$ is a hyper-parameter representing the level of distrust in the predictions. We also study the impact of mispredictions on the competitive ratio of the proposed algorithm and adapt it to achieve a bounded robustness while retaining its consistency. We further establish a lower bound of $\\frac{3}{2}$ on the consistency of any deterministic learning-augmented algorithm. Experimental evaluations are carried out to evaluate our algorithms using real data access traces.","sentences":["This paper studies an online replication problem for distributed data access.","The goal is to dynamically create and delete data copies in a multi-server system as time passes to minimize the total storage and network cost of serving access requests.","We study the problem in the emergent learning-augmented setting, assuming simple binary predictions about inter-request times at individual servers.","We develop an online algorithm and prove that it is ($\\frac{5+\\alpha}{3}$)-consistent (competitiveness under perfect predictions) and ($1 + \\frac{1}{\\alpha}$)-robust (competitiveness under terrible predictions), where $\\alpha \\in (0, 1]$ is a hyper-parameter representing the level of distrust in the predictions.","We also study the impact of mispredictions on the competitive ratio of the proposed algorithm and adapt it to achieve a bounded robustness while retaining its consistency.","We further establish a lower bound of $\\frac{3}{2}$ on the consistency of any deterministic learning-augmented algorithm.","Experimental evaluations are carried out to evaluate our algorithms using real data access traces."],"url":"http://arxiv.org/abs/2404.16489v1","category":"cs.DS"}
{"created":"2024-04-25 10:18:16","title":"OpenIVM: a SQL-to-SQL Compiler for Incremental Computations","abstract":"This demonstration presents a new Open Source SQL-to-SQL compiler for Incremental View Maintenance (IVM). While previous systems, such as DBToaster, implemented computational functionality for IVM in a separate system, the core principle of OpenIVM is to make use of existing SQL query processing engines and perform all IVM computations via SQL. This approach enables the integration of IVM in these systems without code duplication. Also, it eases its use in cross-system IVM, i.e. to orchestrate an HTAP system in which one (OLTP) DBMS provides insertions/updates/deletes (deltas), which are propagated using SQL into another (OLAP) DBMS, hosting materialized views. Our system compiles view definitions into SQL to eventually propagate deltas into the table that materializes the view, following the principles of DBSP. Under the hood, OpenIVM uses the DuckDB library to compile (parse, transform, optimize) the materialized view maintenance logic. We demonstrate OpenIVM in action (i) as the core of a DuckDB extension module that adds IVM functionality to it and (ii) powering cross-system IVM for HTAP, with PostgreSQL handling updates on base tables and DuckDB hosting materialized views on these.","sentences":["This demonstration presents a new Open Source SQL-to-SQL compiler for Incremental View Maintenance (IVM).","While previous systems, such as DBToaster, implemented computational functionality for IVM in a separate system, the core principle of OpenIVM is to make use of existing SQL query processing engines and perform all IVM computations via SQL.","This approach enables the integration of IVM in these systems without code duplication.","Also, it eases its use in cross-system IVM, i.e. to orchestrate an HTAP system in which one (OLTP) DBMS provides insertions/updates/deletes (deltas), which are propagated using SQL into another (OLAP) DBMS, hosting materialized views.","Our system compiles view definitions into SQL to eventually propagate deltas into the table that materializes the view, following the principles of DBSP.","Under the hood, OpenIVM uses the DuckDB library to compile (parse, transform, optimize)","the materialized view maintenance logic.","We demonstrate OpenIVM in action (i) as the core of a DuckDB extension module that adds IVM functionality to it and (ii) powering cross-system IVM for HTAP, with PostgreSQL handling updates on base tables and DuckDB hosting materialized views on these."],"url":"http://arxiv.org/abs/2404.16486v1","category":"cs.DB"}
{"created":"2024-04-25 10:12:31","title":"Leveraging Pretrained Latent Representations for Few-Shot Imitation Learning on a Dexterous Robotic Hand","abstract":"In the context of imitation learning applied to dexterous robotic hands, the high complexity of the systems makes learning complex manipulation tasks challenging. However, the numerous datasets depicting human hands in various different tasks could provide us with better knowledge regarding human hand motion. We propose a method to leverage multiple large-scale task-agnostic datasets to obtain latent representations that effectively encode motion subtrajectories that we included in a transformer-based behavior cloning method. Our results demonstrate that employing latent representations yields enhanced performance compared to conventional behavior cloning methods, particularly regarding resilience to errors and noise in perception and proprioception. Furthermore, the proposed approach solely relies on human demonstrations, eliminating the need for teleoperation and, therefore, accelerating the data acquisition process. Accurate inverse kinematics for fingertip retargeting ensures precise transfer from human hand data to the robot, facilitating effective learning and deployment of manipulation policies. Finally, the trained policies have been successfully transferred to a real-world 23Dof robotic system.","sentences":["In the context of imitation learning applied to dexterous robotic hands, the high complexity of the systems makes learning complex manipulation tasks challenging.","However, the numerous datasets depicting human hands in various different tasks could provide us with better knowledge regarding human hand motion.","We propose a method to leverage multiple large-scale task-agnostic datasets to obtain latent representations that effectively encode motion subtrajectories that we included in a transformer-based behavior cloning method.","Our results demonstrate that employing latent representations yields enhanced performance compared to conventional behavior cloning methods, particularly regarding resilience to errors and noise in perception and proprioception.","Furthermore, the proposed approach solely relies on human demonstrations, eliminating the need for teleoperation and, therefore, accelerating the data acquisition process.","Accurate inverse kinematics for fingertip retargeting ensures precise transfer from human hand data to the robot, facilitating effective learning and deployment of manipulation policies.","Finally, the trained policies have been successfully transferred to a real-world 23Dof robotic system."],"url":"http://arxiv.org/abs/2404.16483v1","category":"cs.RO"}
{"created":"2024-04-25 10:00:16","title":"A Novel Channel Coding Scheme for Digital Multiple Access Computing","abstract":"In this paper, we consider the ChannelComp framework, which facilitates the computation of desired functions by multiple transmitters over a common receiver using digital modulations across a multiple access channel. While ChannelComp currently offers a broad framework for computation by designing digital constellations for over-the-air computation and employing symbol-level encoding, encoding the repeated transmissions of the same symbol and using the corresponding received sequence may significantly improve the computation performance and reduce the encoding complexity. In this paper, we propose an enhancement involving the encoding of the repetitive transmission of the same symbol at each transmitter over multiple time slots and the design of constellation diagrams, with the aim of minimizing computational errors. We frame this enhancement as an optimization problem, which jointly identifies the constellation diagram and the channel code for repetition, which we call ReChCompCode. To manage the computational complexity of the optimization, we divide it into two tractable subproblems. Through numerical experiments, we evaluate the performance of ReChCompCode. The simulation results reveal that ReChCompCode can reduce the computation error by approximately up to 30 dB compared to standard ChannelComp, particularly for product functions.","sentences":["In this paper, we consider the ChannelComp framework, which facilitates the computation of desired functions by multiple transmitters over a common receiver using digital modulations across a multiple access channel.","While ChannelComp currently offers a broad framework for computation by designing digital constellations for over-the-air computation and employing symbol-level encoding, encoding the repeated transmissions of the same symbol and using the corresponding received sequence may significantly improve the computation performance and reduce the encoding complexity.","In this paper, we propose an enhancement involving the encoding of the repetitive transmission of the same symbol at each transmitter over multiple time slots and the design of constellation diagrams, with the aim of minimizing computational errors.","We frame this enhancement as an optimization problem, which jointly identifies the constellation diagram and the channel code for repetition, which we call ReChCompCode.","To manage the computational complexity of the optimization, we divide it into two tractable subproblems.","Through numerical experiments, we evaluate the performance of ReChCompCode.","The simulation results reveal that ReChCompCode can reduce the computation error by approximately up to 30 dB compared to standard ChannelComp, particularly for product functions."],"url":"http://arxiv.org/abs/2404.16476v1","category":"eess.SP"}
{"created":"2024-04-25 09:54:21","title":"A finite-time quantum Otto engine with tunnel coupled one-dimensional Bose gases","abstract":"We undertake a theoretical study of a finite-time quantum Otto engine cycle driven by inter-particle interactions in a weakly interacting one-dimensional Bose gas in the quasicondensate regime. Utilizing a $c$-field approach, we simulate the entire Otto cycle, i.e. the two work strokes and the two equilibration strokes. More specifically, the interaction-induced work strokes are modelled by treating the working fluid as an isolated quantum many-body system undergoing unitary evolution. The equilibration strokes, on the other hand, are modelled by treating the working fluid as an open quantum system tunnel-coupled to another quasicondensate which acts as either the hot or cold reservoir, albeit of finite size. We find that, unlike a uniform 1D Bose gas, a harmonically trapped quasicondensate cannot operate purely as a \\emph{heat} engine; instead, the engine operation is enabled by additional \\emph{chemical} work performed on the working fluid, facilitated by the inflow of particles from the hot reservoir. The microscopic treatment of dynamics during equilibration strokes enables us to evaluate the characteristic operational time scales of this Otto chemical engine, crucial for characterizing its power output, without any \\emph{ad hoc} assumptions about typical thermalization timescales. We analyse the performance and quantify the figures of merit of the proposed Otto chemical engine, finding that it offers a favourable trade-off between efficiency and power output, particularly when the interaction-induced work strokes are implemented via a sudden quench. We further demonstrate that in the sudden quench regime, the engine operates with an efficiency close to the near-adiabatic (near maximum efficiency) limit, while concurrently achieving maximum power output.","sentences":["We undertake a theoretical study of a finite-time quantum Otto engine cycle driven by inter-particle interactions in a weakly interacting one-dimensional Bose gas in the quasicondensate regime.","Utilizing a $c$-field approach, we simulate the entire Otto cycle, i.e. the two work strokes and the two equilibration strokes.","More specifically, the interaction-induced work strokes are modelled by treating the working fluid as an isolated quantum many-body system undergoing unitary evolution.","The equilibration strokes, on the other hand, are modelled by treating the working fluid as an open quantum system tunnel-coupled to another quasicondensate which acts as either the hot or cold reservoir, albeit of finite size.","We find that, unlike a uniform 1D Bose gas, a harmonically trapped quasicondensate cannot operate purely as a \\emph{heat} engine; instead, the engine operation is enabled by additional \\emph{chemical} work performed on the working fluid, facilitated by the inflow of particles from the hot reservoir.","The microscopic treatment of dynamics during equilibration strokes enables us to evaluate the characteristic operational time scales of this Otto chemical engine, crucial for characterizing its power output, without any \\emph{ad hoc} assumptions about typical thermalization timescales.","We analyse the performance and quantify the figures of merit of the proposed Otto chemical engine, finding that it offers a favourable trade-off between efficiency and power output, particularly when the interaction-induced work strokes are implemented via a sudden quench.","We further demonstrate that in the sudden quench regime, the engine operates with an efficiency close to the near-adiabatic (near maximum efficiency) limit, while concurrently achieving maximum power output."],"url":"http://arxiv.org/abs/2404.16470v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-25 09:48:37","title":"Epidemic risk perception and social interactions lead to awareness cascades on multiplex networks","abstract":"The course of an epidemic is not only shaped by infection transmission over face-to-face contacts, but also by preventive behaviour caused by risk perception and social interactions. This study explores the dynamics of coupled awareness and biological infection spread within a two-layer multiplex network framework. One layer embodies face-to-face contacts, with a biological infection transmission following a simple contagion model, the SIR process. Awareness, modelled by the linear threshold model, a complex contagion, spreads over a social layer and induces behaviour that lowers the chance of a biological infection occurring. It may be provoked by the presence of either aware or infectious neighbours. We introduce a novel model combining these influences through a convex combination, creating a continuum between pure social contagion and local risk perception. Simulation of the model shows distinct effects arising from the awareness sources. Also, for convex combinations where both input sources are of importance, awareness cascades that are not attributable to only one of these sources, emerge. Under these conditions, the combination of a small-world face-to-face and a scale-free social layer, but not vice versa, make that the extent of the infections decreases with increasing transmission probability.","sentences":["The course of an epidemic is not only shaped by infection transmission over face-to-face contacts, but also by preventive behaviour caused by risk perception and social interactions.","This study explores the dynamics of coupled awareness and biological infection spread within a two-layer multiplex network framework.","One layer embodies face-to-face contacts, with a biological infection transmission following a simple contagion model, the SIR process.","Awareness, modelled by the linear threshold model, a complex contagion, spreads over a social layer and induces behaviour that lowers the chance of a biological infection occurring.","It may be provoked by the presence of either aware or infectious neighbours.","We introduce a novel model combining these influences through a convex combination, creating a continuum between pure social contagion and local risk perception.","Simulation of the model shows distinct effects arising from the awareness sources.","Also, for convex combinations where both input sources are of importance, awareness cascades that are not attributable to only one of these sources, emerge.","Under these conditions, the combination of a small-world face-to-face and a scale-free social layer, but not vice versa, make that the extent of the infections decreases with increasing transmission probability."],"url":"http://arxiv.org/abs/2404.16466v1","category":"physics.soc-ph"}
{"created":"2024-04-25 09:48:09","title":"In Situ Characterisation of Graphene Growth on Liquid Copper-Gallium Alloys: Paving the Path for Cost-Effective Synthesis","abstract":"Liquid metal catalysts (LMCats), primarily molten copper, have demonstrated their efficiency in the chemical vapour deposition (CVD) approach for synthesising high-quality, large-area graphene. However, their high melting temperatures limit broader applications. Reducing the temperature of graphene production on LMCats would lead to a more efficient and cost-effective process. Here, we investigated the effects of alloying copper with a low-melting temperature metal on graphene growth in real-time. We examined a set of liquid copper-gallium alloy systems using two complementary in situ techniques: radiation-mode optical microscopy and synchrotron X-ray reflectivity (XRR). Microscopy observations revealed reduced catalytic activity and graphene quality degradation in compositions with gallium domination. The XRR confirmed the formation of single-layer graphene on alloys with up to 60 wt% of gallium. Additionally, we detected a systematic increase in adsorption height on the alloys' surface, suggesting a weaker graphene adhesion on gallium. These findings propose a trade-off between layer quality and production cost reduction is feasible. Our results offer insights into the CVD synthesis of graphene on bimetallic liquid surfaces and underscore the potential of gallium-copper alloys for enabling the direct transfer of graphene from a liquid substrate, thereby addressing the limitations imposed by high melting temperatures of conventional LMCats.","sentences":["Liquid metal catalysts (LMCats), primarily molten copper, have demonstrated their efficiency in the chemical vapour deposition (CVD) approach for synthesising high-quality, large-area graphene.","However, their high melting temperatures limit broader applications.","Reducing the temperature of graphene production on LMCats would lead to a more efficient and cost-effective process.","Here, we investigated the effects of alloying copper with a low-melting temperature metal on graphene growth in real-time.","We examined a set of liquid copper-gallium alloy systems using two complementary in situ techniques: radiation-mode optical microscopy and synchrotron X-ray reflectivity (XRR).","Microscopy observations revealed reduced catalytic activity and graphene quality degradation in compositions with gallium domination.","The XRR confirmed the formation of single-layer graphene on alloys with up to 60 wt% of gallium.","Additionally, we detected a systematic increase in adsorption height on the alloys' surface, suggesting a weaker graphene adhesion on gallium.","These findings propose a trade-off between layer quality and production cost reduction is feasible.","Our results offer insights into the CVD synthesis of graphene on bimetallic liquid surfaces and underscore the potential of gallium-copper alloys for enabling the direct transfer of graphene from a liquid substrate, thereby addressing the limitations imposed by high melting temperatures of conventional LMCats."],"url":"http://arxiv.org/abs/2404.16465v1","category":"physics.app-ph"}
{"created":"2024-04-25 09:47:30","title":"Quantum-assisted trustworthiness for the Quantum Internet","abstract":"Device redundancy is one of the most well-known mechanisms in distributed systems to increase the overall system fault tolerance and, consequently, trustworthiness. Existing algorithms in this regard aim to exchange a significant number of messages among nodes to identify and agree which communication links or nodes are faulty. This approach greatly degrades the performance of those wireless communication networks exposed to limited available bandwidth and/or energy consumption due to messages flooding. Lately, quantum-assisted mechanisms have been envisaged as an appealing alternative to improve the performance in this kind of communication networks and have been shown to obtain levels of performance close to the ones achieved in ideal conditions. The purpose of this paper is to further explore this approach by using super-additivity and superposed quantum trajectories in quantum Internet to obtain a higher system trustworthiness. More specifically, the wireless communication network that supports the permafrost telemetry service for the Antarctica together with five operational modes (three of them using classical techniques and two of them using quantum-assisted mechanisms) have been simulated. Obtained results show that the new quantum-assisted mechanisms can increase the system performance by up to a 28%.","sentences":["Device redundancy is one of the most well-known mechanisms in distributed systems to increase the overall system fault tolerance and, consequently, trustworthiness.","Existing algorithms in this regard aim to exchange a significant number of messages among nodes to identify and agree which communication links or nodes are faulty.","This approach greatly degrades the performance of those wireless communication networks exposed to limited available bandwidth and/or energy consumption due to messages flooding.","Lately, quantum-assisted mechanisms have been envisaged as an appealing alternative to improve the performance in this kind of communication networks and have been shown to obtain levels of performance close to the ones achieved in ideal conditions.","The purpose of this paper is to further explore this approach by using super-additivity and superposed quantum trajectories in quantum Internet to obtain a higher system trustworthiness.","More specifically, the wireless communication network that supports the permafrost telemetry service for the Antarctica together with five operational modes (three of them using classical techniques and two of them using quantum-assisted mechanisms) have been simulated.","Obtained results show that the new quantum-assisted mechanisms can increase the system performance by up to a 28%."],"url":"http://arxiv.org/abs/2404.16463v1","category":"quant-ph"}
{"created":"2024-04-25 09:39:22","title":"Impacts of Energetic Particles from T Tauri Flares on Inner Protoplanetary Discs","abstract":"T Tauri stars are known to be magnetically active stars subject to strong flares observed in X-rays. These flares are likely due to intense magnetic reconnection events during which a part of the stored magnetic energy is converted into kinetic energy of supra-thermal particles. Since T Tauri stars are surrounded by an accretion disc, these particles may influence the disc dynamics and chemistry. This work continues on a previous stationary model, which showed that energetic particles accelerated during flares can produce a strong ionisation rate at high column densities in the inner accretion disc. The present model includes non-stationary sequences of flaring events sampled by a Chandra X-ray survey of nearby young stellar objects. We calculate the averaged ionisation rate expected in a radius range from 0.08 to 0.6 au from the central star. We confirm that energetic particles produced by the flares dominate the ionisation of the disc up to column densities of $10^{25}~\\rm{cm^{-2}}$. We further study the main consequences of this additional source of ionisation on the viscosity, the accretion rate, the volumetric heating rate and the chemical complexity of inner protoplanetary discs.","sentences":["T Tauri stars are known to be magnetically active stars subject to strong flares observed in X-rays.","These flares are likely due to intense magnetic reconnection events during which a part of the stored magnetic energy is converted into kinetic energy of supra-thermal particles.","Since T Tauri stars are surrounded by an accretion disc, these particles may influence the disc dynamics and chemistry.","This work continues on a previous stationary model, which showed that energetic particles accelerated during flares can produce a strong ionisation rate at high column densities in the inner accretion disc.","The present model includes non-stationary sequences of flaring events sampled by a Chandra X-ray survey of nearby young stellar objects.","We calculate the averaged ionisation rate expected in a radius range from 0.08 to 0.6 au from the central star.","We confirm that energetic particles produced by the flares dominate the ionisation of the disc up to column densities of $10^{25}~\\rm{cm^{-2}}$. We further study the main consequences of this additional source of ionisation on the viscosity, the accretion rate, the volumetric heating rate and the chemical complexity of inner protoplanetary discs."],"url":"http://arxiv.org/abs/2404.16459v1","category":"astro-ph.HE"}
{"created":"2024-04-25 09:38:11","title":"On an infinite commuting ODE system associated to a simple Lie algebra","abstract":"Inspired by a recent work of Dubrovin [7], for each simple Lie algebra $\\mathfrak{g}$, we introduce an infinite family of pairwise commuting ODEs and define their $\\tau$-functions. We show that these $\\tau$-functions can be identified with the $\\tau$-functions for the Drinfeld--Sokolov hierarchy of $\\mathfrak{g}$-type. Explicit examples for $\\mathfrak{g}=A_1$ and $A_2$ are provided, which are connected to the KdV hierarchy and the Boussinesq hierarchy respectively.","sentences":["Inspired by a recent work of Dubrovin","[7], for each simple Lie algebra $\\mathfrak{g}$, we introduce an infinite family of pairwise commuting ODEs and define their $\\tau$-functions.","We show that these $\\tau$-functions can be identified with the $\\tau$-functions for the Drinfeld--Sokolov hierarchy of $\\mathfrak{g}$-type.","Explicit examples for $\\mathfrak{g}=A_1$ and $A_2$ are provided, which are connected to the KdV hierarchy and the Boussinesq hierarchy respectively."],"url":"http://arxiv.org/abs/2404.16458v1","category":"nlin.SI"}
{"created":"2024-04-25 09:34:34","title":"Stabilizing quantum simulations of lattice gauge theories by dissipation","abstract":"Simulations of lattice gauge theories on noisy quantum hardware inherently suffer from violations of the gauge symmetry due to coherent and incoherent errors of the underlying physical system that implements the simulation. These gauge violations cause the simulations to become unphysical requiring the result of the simulation to be discarded. We investigate an active correction scheme that relies on detecting gauge violations locally and subsequently correcting them by dissipatively driving the system back into the physical gauge sector. We show that the correction scheme not only ensures the protection of the gauge symmetry, but it also leads to a longer validity of the simulation results even within the gauge-invariant sector. Finally, we discuss further applications of the scheme such as preparation of the many-body ground state of the simulated system.","sentences":["Simulations of lattice gauge theories on noisy quantum hardware inherently suffer from violations of the gauge symmetry due to coherent and incoherent errors of the underlying physical system that implements the simulation.","These gauge violations cause the simulations to become unphysical requiring the result of the simulation to be discarded.","We investigate an active correction scheme that relies on detecting gauge violations locally and subsequently correcting them by dissipatively driving the system back into the physical gauge sector.","We show that the correction scheme not only ensures the protection of the gauge symmetry, but it also leads to a longer validity of the simulation results even within the gauge-invariant sector.","Finally, we discuss further applications of the scheme such as preparation of the many-body ground state of the simulated system."],"url":"http://arxiv.org/abs/2404.16454v1","category":"quant-ph"}
{"created":"2024-04-25 09:30:19","title":"Unconditional correctness of recent quantum algorithms for factoring and computing discrete logarithms","abstract":"In 1994, Shor introduced his famous quantum algorithm to factor integers and compute discrete logarithms in polynomial time. In 2023, Regev proposed a multi-dimensional version of Shor's algorithm that requires far fewer quantum gates. His algorithm relies on a number-theoretic conjecture on the elements in $(\\mathbb{Z}/N\\mathbb{Z})^{\\times}$ that can be written as short products of very small prime numbers. We prove a version of this conjecture using tools from analytic number theory such as zero-density estimates. As a result, we obtain an unconditional proof of correctness of this improved quantum algorithm and of subsequent variants.","sentences":["In 1994, Shor introduced his famous quantum algorithm to factor integers and compute discrete logarithms in polynomial time.","In 2023, Regev proposed a multi-dimensional version of Shor's algorithm that requires far fewer quantum gates.","His algorithm relies on a number-theoretic conjecture on the elements in $(\\mathbb{Z}/N\\mathbb{Z})^{\\times}$ that can be written as short products of very small prime numbers.","We prove a version of this conjecture using tools from analytic number theory such as zero-density estimates.","As a result, we obtain an unconditional proof of correctness of this improved quantum algorithm and of subsequent variants."],"url":"http://arxiv.org/abs/2404.16450v1","category":"math.NT"}
{"created":"2024-04-25 09:29:10","title":"Hardy decomposition of higher order Lipschitz classes by polymonogenic functions","abstract":"In this paper we find a decomposition of higher order Lipschitz functions into the traces of a polymonogenic function and solve a related Riemann-Hilbert problem. Our approach lies in using a cliffordian Cauchy-type operator, which behaves as an involution operator on higher order Lipschitz spaces. The result obtained is a multidimensional sharpened version of the Hardy decomposition of H\\\"older continuous functions on a simple closed curve in the complex plane.","sentences":["In this paper we find a decomposition of higher order Lipschitz functions into the traces of a polymonogenic function and solve a related Riemann-Hilbert problem.","Our approach lies in using a cliffordian Cauchy-type operator, which behaves as an involution operator on higher order Lipschitz spaces.","The result obtained is a multidimensional sharpened version of the Hardy decomposition of H\\\"older continuous functions on a simple closed curve in the complex plane."],"url":"http://arxiv.org/abs/2404.16447v1","category":"math.CV"}
{"created":"2024-04-25 09:22:31","title":"Tightening I/O Lower Bounds through the Hourglass Dependency Pattern","abstract":"When designing an algorithm, one cares about arithmetic/computational complexity, but data movement (I/O) complexity plays an increasingly important role that highly impacts performance and energy consumption. For a given algorithm and a given I/O model, scheduling strategies such as loop tiling can reduce the required I/O down to a limit, called the I/O complexity, inherent to the algorithm itself. The objective of I/O complexity analysis is to compute, for a given program, its minimal I/O requirement among all valid schedules. We consider a sequential execution model with two memories, an infinite one, and a small one of size S on which the computations retrieve and produce data. The I/O is the number of reads and writes between the two memories. We identify a common \"hourglass pattern\" in the dependency graphs of several common linear algebra kernels. Using the properties of this pattern, we mathematically prove tighter lower bounds on their I/O complexity, which improves the previous state-of-the-art bound by a parametric ratio. This proof was integrated inside the IOLB automatic lower bound derivation tool.","sentences":["When designing an algorithm, one cares about arithmetic/computational complexity, but data movement (I/O) complexity plays an increasingly important role that highly impacts performance and energy consumption.","For a given algorithm and a given I/O model, scheduling strategies such as loop tiling can reduce the required I/O down to a limit, called the I/O complexity, inherent to the algorithm itself.","The objective of I/O complexity analysis is to compute, for a given program, its minimal I/O requirement among all valid schedules.","We consider a sequential execution model with two memories, an infinite one, and a small one of size S on which the computations retrieve and produce data.","The I/O is the number of reads and writes between the two memories.","We identify a common \"hourglass pattern\" in the dependency graphs of several common linear algebra kernels.","Using the properties of this pattern, we mathematically prove tighter lower bounds on their I/O complexity, which improves the previous state-of-the-art bound by a parametric ratio.","This proof was integrated inside the IOLB automatic lower bound derivation tool."],"url":"http://arxiv.org/abs/2404.16443v1","category":"cs.CC"}
{"created":"2024-04-25 09:20:34","title":"Quantification of 2D Interfaces: Quality of heterostructures, and what is inside a nanobubble","abstract":"Trapped materials at the interfaces of two-dimensional heterostructures (HS) lead to reduced coupling between the layers, resulting in degraded optoelectronic performance and device variability. Further, nanobubbles can form at the interface during transfer or after annealing. The question of what is inside a nanobubble, i.e. the trapped material, remains unanswered, limiting the studies and applications of these nanobubble systems. In this work, we report two key advances. Firstly, we quantify the interface quality using RAW-format optical imaging, and distinguish between ideal and non-ideal interfaces. The HS-substrate ratio value is calculated using a transfer matrix model, and is able to detect the presence of trapped layers. The second key advance is identification of water as the trapped material inside a nanobubble. To the best of our knowledge, this is the first study to show that optical imaging alone can quantify interface quality, and find the type of trapped material inside spontaneously formed nanobubbles. We also define a quality index parameter to quantify the interface quality of HS. Quantitative measurement of the interface will help answer the question whether annealing is necessary during HS preparation, and will enable creation of complex HS with small twist angles. Identification of the trapped materials will pave the way towards using nanobubbles for novel optical and engineering applications.","sentences":["Trapped materials at the interfaces of two-dimensional heterostructures (HS) lead to reduced coupling between the layers, resulting in degraded optoelectronic performance and device variability.","Further, nanobubbles can form at the interface during transfer or after annealing.","The question of what is inside a nanobubble, i.e. the trapped material, remains unanswered, limiting the studies and applications of these nanobubble systems.","In this work, we report two key advances.","Firstly, we quantify the interface quality using RAW-format optical imaging, and distinguish between ideal and non-ideal interfaces.","The HS-substrate ratio value is calculated using a transfer matrix model, and is able to detect the presence of trapped layers.","The second key advance is identification of water as the trapped material inside a nanobubble.","To the best of our knowledge, this is the first study to show that optical imaging alone can quantify interface quality, and find the type of trapped material inside spontaneously formed nanobubbles.","We also define a quality index parameter to quantify the interface quality of HS.","Quantitative measurement of the interface will help answer the question whether annealing is necessary during HS preparation, and will enable creation of complex HS with small twist angles.","Identification of the trapped materials will pave the way towards using nanobubbles for novel optical and engineering applications."],"url":"http://arxiv.org/abs/2404.16441v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-25 09:20:22","title":"Toeplitz Operators on Weighted Bergman Spaces over Tubular Domains","abstract":"In this paper, we mainly study the necessary and sufficient conditions for the boundedness and compactness of Toeplitz operators on weighted Bergman spaces over a tubular domains by using the Carlson measures on tubular domains. We also give some related results about Carlson measures.","sentences":["In this paper, we mainly study the necessary and sufficient conditions for the boundedness and compactness of Toeplitz operators on weighted Bergman spaces over a tubular domains by using the Carlson measures on tubular domains.","We also give some related results about Carlson measures."],"url":"http://arxiv.org/abs/2404.16439v1","category":"math.CV"}
{"created":"2024-04-25 09:10:53","title":"Ionic self-phoresis maps onto correlation-induced self-phoresis","abstract":"We re-examine the self-phoresis of a particle that releases(removes) pairs of ions into(from) the electrolyte solution. We show analytically that in the linear regime the mathematical description of this system maps onto that of the correlation-induced (self-)chemophoresis (CICP). This connection provides a unifying perspective of the two phenomena, within which one recovers and extends recent predictions as particular instances of CICP. Conversely, ion-phoretic particles are identified as candidates for experimental investigations into the rich variety of motility patterns predicted by CICP.","sentences":["We re-examine the self-phoresis of a particle that releases(removes) pairs of ions into(from) the electrolyte solution.","We show analytically that in the linear regime the mathematical description of this system maps onto that of the correlation-induced (self-)chemophoresis (CICP).","This connection provides a unifying perspective of the two phenomena, within which one recovers and extends recent predictions as particular instances of CICP.","Conversely, ion-phoretic particles are identified as candidates for experimental investigations into the rich variety of motility patterns predicted by CICP."],"url":"http://arxiv.org/abs/2404.16435v1","category":"cond-mat.soft"}
{"created":"2024-04-25 09:07:38","title":"Global existence of a strong solution to the initial value problem for the Nernst-Planck-Navier-Stokes system in $\\mathbb{R}^N$","abstract":"We study the existence of a strong solution to the initial value problem for the Nernst-Planck-Navier-Stokes (NPNS) system in $\\mathbb{R}^N, N\\geq 3$. We obtain a global in-time strong solution without any smallness assumptions on the initial data.","sentences":["We study the existence of a strong solution to the initial value problem for the Nernst-Planck-Navier-Stokes (NPNS) system in $\\mathbb{R}^N, N\\geq 3$.","We obtain a global in-time strong solution without any smallness assumptions on the initial data."],"url":"http://arxiv.org/abs/2404.16433v1","category":"math.AP"}
{"created":"2024-04-25 09:05:39","title":"Secure Coded Distributed Computing","abstract":"In this paper, we consider two critical aspects of security in the \\textit{distributed computing (DC)} model: \\textit{secure data shuffling} and \\textit{secure coded computing}. It is imperative that any external entity overhearing the transmissions does not gain any information about the \\textit{intermediate values (IVs)} exchanged during the shuffling phase of the DC model. Our approach ensures IV confidentiality during data shuffling. Moreover, each node in the system must be able to recover the IVs necessary for computing its output functions but must also remain oblivious to the IVs associated with output functions not assigned to it. We design secure DC methods and establish achievable limits on the tradeoffs between the communication and computation loads to contribute to the advancement of secure data processing in distributed systems.","sentences":["In this paper, we consider two critical aspects of security in the \\textit{distributed computing (DC)} model: \\textit{secure data shuffling} and \\textit{secure coded computing}.","It is imperative that any external entity overhearing the transmissions does not gain any information about the \\textit{intermediate values (IVs)} exchanged during the shuffling phase of the DC model.","Our approach ensures IV confidentiality during data shuffling.","Moreover, each node in the system must be able to recover the IVs necessary for computing its output functions but must also remain oblivious to the IVs associated with output functions not assigned to it.","We design secure DC methods and establish achievable limits on the tradeoffs between the communication and computation loads to contribute to the advancement of secure data processing in distributed systems."],"url":"http://arxiv.org/abs/2404.16431v1","category":"cs.IT"}
{"created":"2024-04-25 08:50:00","title":"Magnetocapacitance oscillations dominated by giant Rashba spin orbit interaction in InAs/GaSb quantum wells separated by AlSb barrier","abstract":"We observed magnetocapacitance oscillations in InAs/GaSb quantum wells separated by a $20$\\,nm AlSb middle barrier. By realizing independent ohmic contacts for electrons in InAs and holes in the GaSb layer, we found an out-of-plane oscillatory response in capacitance representing the density of states of this system. We were able to tune the charge carrier densities by applying a DC bias voltage, identifying the formation of beating signatures for forward bias. The coexistence of two distinguishable two dimensional charge carrier systems of unequal densities was verified. The corresponding Landau phase diagram presents distinct features originating from the two observed densities. A giant Rashba coefficient ranging from $430-612$\\,meV$\\text{\\AA}$ and large \\textit{g}-factor value underlines the influence of spin orbit interaction.","sentences":["We observed magnetocapacitance oscillations in InAs/GaSb quantum wells separated by a $20$\\,nm AlSb middle barrier.","By realizing independent ohmic contacts for electrons in InAs and holes in the GaSb layer, we found an out-of-plane oscillatory response in capacitance representing the density of states of this system.","We were able to tune the charge carrier densities by applying a DC bias voltage, identifying the formation of beating signatures for forward bias.","The coexistence of two distinguishable two dimensional charge carrier systems of unequal densities was verified.","The corresponding Landau phase diagram presents distinct features originating from the two observed densities.","A giant Rashba coefficient ranging from $430-612$\\,meV$\\text{\\AA}$ and large \\textit{g}-factor value underlines the influence of spin orbit interaction."],"url":"http://arxiv.org/abs/2404.16419v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-25 08:47:52","title":"A quantitative theory for heterogeneous combustion of nonvolatile metal particles in the diffusion-limited regime","abstract":"The paper presents an analytical theory quantitatively describing the heterogeneous combustion of nonvolatile (metal) particles in the diffusion-limited regime. It is assumed that the particle is suspended in an unconfined, isobaric, quiescent gaseous mixture and the chemisorption of the oxygen takes place evenly on the particle surface. The exact solution of the particle burn time is derived from the conservation equations of the gas-phase described in a spherical coordinate system with the utilization of constant thermophysical properties, evaluated at a reference film layer. This solution inherently takes the Stefan flow into account. The approximate expression of the time-dependent particle temperature is solved from the conservation of the particle enthalpy by neglecting the higher order terms in the Taylor expansion of the product of the transient particle density and diameter squared. Coupling the solutions for the burn time and time-dependent particle temperature provides quantitative results when initial and boundary conditions are specified. The theory is employed to predict the burn time and temperature of micro-sized iron particles, which are then compared with measurements, as the first validation case. The theoretical burn time agrees with the experiments almost perfectly at both low and high oxygen levels. The calculated particle temperature matches the measurements fairly well at relatively low oxygen mole fractions, whereas the theory overpredict the particle peak temperature due to the neglect of evaporation and the possible transition of the combustion regime.","sentences":["The paper presents an analytical theory quantitatively describing the heterogeneous combustion of nonvolatile (metal) particles in the diffusion-limited regime.","It is assumed that the particle is suspended in an unconfined, isobaric, quiescent gaseous mixture and the chemisorption of the oxygen takes place evenly on the particle surface.","The exact solution of the particle burn time is derived from the conservation equations of the gas-phase described in a spherical coordinate system with the utilization of constant thermophysical properties, evaluated at a reference film layer.","This solution inherently takes the Stefan flow into account.","The approximate expression of the time-dependent particle temperature is solved from the conservation of the particle enthalpy by neglecting the higher order terms in the Taylor expansion of the product of the transient particle density and diameter squared.","Coupling the solutions for the burn time and time-dependent particle temperature provides quantitative results when initial and boundary conditions are specified.","The theory is employed to predict the burn time and temperature of micro-sized iron particles, which are then compared with measurements, as the first validation case.","The theoretical burn time agrees with the experiments almost perfectly at both low and high oxygen levels.","The calculated particle temperature matches the measurements fairly well at relatively low oxygen mole fractions, whereas the theory overpredict the particle peak temperature due to the neglect of evaporation and the possible transition of the combustion regime."],"url":"http://arxiv.org/abs/2404.16415v1","category":"physics.flu-dyn"}
{"created":"2024-04-25 08:46:08","title":"Validating a lutetium frequency reference","abstract":"We review our progress in developing a frequency reference with singly ionized lutetium and give estimates of the levels of inaccuracy we expect to achieve in the near future with both the $^1S_0\\leftrightarrow{}^3D_1$ and $^1S_0\\leftrightarrow{}^3D_2$ transitions. Based on established experimental results, we show that inaccuracies at the low $10^{-19}$ level are readily achievable for the $^1S_0\\leftrightarrow{}^3D_1$ transition, and the frequency ratio between the two transitions is limited almost entirely by the BBR shift. We argue that the frequency ratio measured within the one apparatus provides a well-defined metric to compare and establish the performance of remotely located systems. For the measurement of an in situ frequency ratio, relativistic shifts drop out and both transitions experience the same electromagnetic environment. Consequently, the uncertainty budget for the ratio is practically identical to the uncertainty budgets for the individual transitions. If the ratios for two or more systems disagree we can be certain at least one of the clock assessments is incorrect. If they agree, subsequent comparisons on one transition would only differ by relativistic effects. Since motional effects are easily assessed and typically small for a heavy ion, only the differential gravitational red-shift will significantly contribute and this can be confirmed by comparison on the second transition.","sentences":["We review our progress in developing a frequency reference with singly ionized lutetium and give estimates of the levels of inaccuracy we expect to achieve in the near future with both the $^1S_0\\leftrightarrow{}^3D_1$ and $^1S_0\\leftrightarrow{}^3D_2$ transitions.","Based on established experimental results, we show that inaccuracies at the low $10^{-19}$ level are readily achievable for the $^1S_0\\leftrightarrow{}^3D_1$ transition, and the frequency ratio between the two transitions is limited almost entirely by the BBR shift.","We argue that the frequency ratio measured within the one apparatus provides a well-defined metric to compare and establish the performance of remotely located systems.","For the measurement of an in situ frequency ratio, relativistic shifts drop out and both transitions experience the same electromagnetic environment.","Consequently, the uncertainty budget for the ratio is practically identical to the uncertainty budgets for the individual transitions.","If the ratios for two or more systems disagree we can be certain at least one of the clock assessments is incorrect.","If they agree, subsequent comparisons on one transition would only differ by relativistic effects.","Since motional effects are easily assessed and typically small for a heavy ion, only the differential gravitational red-shift will significantly contribute and this can be confirmed by comparison on the second transition."],"url":"http://arxiv.org/abs/2404.16414v1","category":"physics.atom-ph"}
{"created":"2024-04-25 08:42:16","title":"Distributed Matrix Pencil Formulations for Prescribed-Time Leader-Following Consensus of MASs with Unknown Sensor Sensitivity","abstract":"In this paper, we address the problem of prescribed-time leader-following consensus of heterogeneous multi-agent systems (MASs) in the presence of unknown sensor sensitivity. Under a connected undirected topology, we propose a time-varying dual observer/controller design framework that makes use of regular local and inaccurate feedback to achieve consensus tracking within a prescribed time. In particular, the developed analysis framework is applicable to MASs equipped with sensors of different sensitivities. One of the design innovations involves constructing a distributed matrix pencil formulation based on worst-case sensors, yielding control parameters with sufficient robustness yet relatively low conservatism. Another novelty is the construction of the control gains, which consists of the product of a proportional coefficient obtained from the matrix pencil formulation and a classic time-varying function that grows to infinity or a novel bounded time-varying function. Furthermore, it is possible to extend the prescribed-time distributed protocol to infinite time domain by introducing the bounded time-varying gain technique without sacrificing the ultimate control accuracy, and the corresponding technical proof is comprehensive. The effectiveness of the method is demonstrated through a group of 5 single-link robot manipulators.","sentences":["In this paper, we address the problem of prescribed-time leader-following consensus of heterogeneous multi-agent systems (MASs) in the presence of unknown sensor sensitivity.","Under a connected undirected topology, we propose a time-varying dual observer/controller design framework that makes use of regular local and inaccurate feedback to achieve consensus tracking within a prescribed time.","In particular, the developed analysis framework is applicable to MASs equipped with sensors of different sensitivities.","One of the design innovations involves constructing a distributed matrix pencil formulation based on worst-case sensors, yielding control parameters with sufficient robustness yet relatively low conservatism.","Another novelty is the construction of the control gains, which consists of the product of a proportional coefficient obtained from the matrix pencil formulation and a classic time-varying function that grows to infinity or a novel bounded time-varying function.","Furthermore, it is possible to extend the prescribed-time distributed protocol to infinite time domain by introducing the bounded time-varying gain technique without sacrificing the ultimate control accuracy, and the corresponding technical proof is comprehensive.","The effectiveness of the method is demonstrated through a group of 5 single-link robot manipulators."],"url":"http://arxiv.org/abs/2404.16412v1","category":"eess.SY"}
{"created":"2024-04-25 08:37:55","title":"Detecting self-organising patterns in crowd motion: Effect of optimisation algorithms","abstract":"The escalating process of urbanization has raised concerns about incidents arising from overcrowding, necessitating a deep understanding of large human crowd behavior and the development of effective crowd management strategies. This study employs computational methods to analyze real-world crowd behaviors, emphasizing self-organizing patterns. Notably, the intersection of two streams of individuals triggers the spontaneous emergence of striped patterns, validated through both simulations and live human experiments. Addressing a gap in computational methods for studying these patterns, previous research utilized the pattern-matching technique, employing the Nelder-Mead Simplex algorithm for fitting a two-dimensional sinusoidal function to pedestrian coordinates. This paper advances the pattern-matching procedure by introducing Simulated Annealing as the optimization algorithm and employing a two-dimensional square wave for data fitting. The amalgamation of Simulated Annealing and the square wave significantly enhances pattern fitting quality, validated through statistical hypothesis tests. The study concludes by outlining potential applications of this method across diverse scenarios.","sentences":["The escalating process of urbanization has raised concerns about incidents arising from overcrowding, necessitating a deep understanding of large human crowd behavior and the development of effective crowd management strategies.","This study employs computational methods to analyze real-world crowd behaviors, emphasizing self-organizing patterns.","Notably, the intersection of two streams of individuals triggers the spontaneous emergence of striped patterns, validated through both simulations and live human experiments.","Addressing a gap in computational methods for studying these patterns, previous research utilized the pattern-matching technique, employing the Nelder-Mead Simplex algorithm for fitting a two-dimensional sinusoidal function to pedestrian coordinates.","This paper advances the pattern-matching procedure by introducing Simulated Annealing as the optimization algorithm and employing a two-dimensional square wave for data fitting.","The amalgamation of Simulated Annealing and the square wave significantly enhances pattern fitting quality, validated through statistical hypothesis tests.","The study concludes by outlining potential applications of this method across diverse scenarios."],"url":"http://arxiv.org/abs/2404.16410v1","category":"math.OC"}
{"created":"2024-04-25 08:33:08","title":"Lost in Recursion: Mining Rich Event Semantics in Knowledge Graphs","abstract":"Our world is shaped by events of various complexity. This includes both small-scale local events like local farmer markets and large complex events like political and military conflicts. The latter are typically not observed directly but through the lenses of intermediaries like newspapers or social media. In other words, we do not witness the unfolding of such events directly but are confronted with narratives surrounding them. Such narratives capture different aspects of a complex event and may also differ with respect to the narrator. Thus, they provide a rich semantics concerning real-world events. In this paper, we show how narratives concerning complex events can be constructed and utilized. We provide a formal representation of narratives based on recursive nodes to represent multiple levels of detail and discuss how narratives can be bound to event-centric knowledge graphs. Additionally, we provide an algorithm based on incremental prompting techniques that mines such narratives from texts to account for different perspectives on complex events. Finally, we show the effectiveness and future research directions in a proof of concept.","sentences":["Our world is shaped by events of various complexity.","This includes both small-scale local events like local farmer markets and large complex events like political and military conflicts.","The latter are typically not observed directly but through the lenses of intermediaries like newspapers or social media.","In other words, we do not witness the unfolding of such events directly but are confronted with narratives surrounding them.","Such narratives capture different aspects of a complex event and may also differ with respect to the narrator.","Thus, they provide a rich semantics concerning real-world events.","In this paper, we show how narratives concerning complex events can be constructed and utilized.","We provide a formal representation of narratives based on recursive nodes to represent multiple levels of detail and discuss how narratives can be bound to event-centric knowledge graphs.","Additionally, we provide an algorithm based on incremental prompting techniques that mines such narratives from texts to account for different perspectives on complex events.","Finally, we show the effectiveness and future research directions in a proof of concept."],"url":"http://arxiv.org/abs/2404.16405v1","category":"cs.CL"}
{"created":"2024-04-25 08:28:50","title":"Anomalous magnetic transition in a disordered quasicrystal approximant with heavy-fermion nature","abstract":"Quasicrystal approximant (CexY1-x)Cd6 (0 < x < 1) forms a network of corner-sharing octahedra. We report that (Ce0.8Y0.2)Cd6 exhibits an anomalous magnetic transition which can be classified neither into the conventional static magnetic ordering nor into spin glasses. The anomalous transition is characterized by the coexistence of a static order and a frequency-dependent sharp positive anomaly in the 3rd-harmonic susceptibility. Based on the investigation of the reference systems CeCd6 and (Ce0.05Y0.95)Cd6, we speculate that the anomalous transition could be induced by disorder in the possible frustrated Ce-network in the presence of the Kondo effect.","sentences":["Quasicrystal approximant (CexY1-x)Cd6 (0 < x < 1) forms a network of corner-sharing octahedra.","We report that (Ce0.8Y0.2)Cd6 exhibits an anomalous magnetic transition which can be classified neither into the conventional static magnetic ordering nor into spin glasses.","The anomalous transition is characterized by the coexistence of a static order and a frequency-dependent sharp positive anomaly in the 3rd-harmonic susceptibility.","Based on the investigation of the reference systems CeCd6 and (Ce0.05Y0.95)Cd6, we speculate that the anomalous transition could be induced by disorder in the possible frustrated Ce-network in the presence of the Kondo effect."],"url":"http://arxiv.org/abs/2404.16402v1","category":"cond-mat.str-el"}
{"created":"2024-04-25 08:00:12","title":"Speed limits and thermodynamic uncertainty relations for quantum systems governed by non-Hermitian Hamiltonian","abstract":"Non-Hermitian Hamiltonians play a crucial role in the description of open quantum systems and nonequilibrium dynamics. In this paper, we derive trade-off relations for systems governed by non-Hermitian Hamiltonians, focusing on the Margolus-Levitin and Mandelstam-Tamm bounds, which are quantum speed limits originally derived in isolated quantum dynamics. We extend these bounds to the case of non-Hermitian Hamiltonians and derive additional bounds on the ratio of the standard deviation to the mean of an observable, which take the same form as the thermodynamic uncertainty relation. As an example, we apply these bounds to the continuous measurement formalism in open quantum dynamics, where the dynamics is described by discontinuous jumps and smooth evolution induced by the non-Hermitian Hamiltonian. Our work provides a unified perspective on the quantum speed limit and thermodynamic uncertainty relations in open quantum dynamics from the viewpoint of the non-Hermitian Hamiltonian, extending the results of previous studies.","sentences":["Non-Hermitian Hamiltonians play a crucial role in the description of open quantum systems and nonequilibrium dynamics.","In this paper, we derive trade-off relations for systems governed by non-Hermitian Hamiltonians, focusing on the Margolus-Levitin and Mandelstam-Tamm bounds, which are quantum speed limits originally derived in isolated quantum dynamics.","We extend these bounds to the case of non-Hermitian Hamiltonians and derive additional bounds on the ratio of the standard deviation to the mean of an observable, which take the same form as the thermodynamic uncertainty relation.","As an example, we apply these bounds to the continuous measurement formalism in open quantum dynamics, where the dynamics is described by discontinuous jumps and smooth evolution induced by the non-Hermitian Hamiltonian.","Our work provides a unified perspective on the quantum speed limit and thermodynamic uncertainty relations in open quantum dynamics from the viewpoint of the non-Hermitian Hamiltonian, extending the results of previous studies."],"url":"http://arxiv.org/abs/2404.16392v1","category":"quant-ph"}
{"created":"2024-04-25 07:45:16","title":"A Multivariate to Bivariate Reduction for Noncommutative Rank and Related Results","abstract":"We study the noncommutative rank problem, ncRANK, of computing the rank of matrices with linear entries in $n$ noncommuting variables and the problem of noncommutative Rational Identity Testing, RIT, which is to decide if a given rational formula in $n$ noncommuting variables is zero on its domain of definition. Motivated by the question whether these problems have deterministic NC algorithms, we revisit their interrelationship from a parallel complexity point of view. We show the following results:   1. Based on Cohn's embedding theorem \\cite{Co90,Cohnfir} we show deterministic NC reductions from multivariate ncRANK to bivariate ncRANK and from multivariate RIT to bivariate RIT.   2. We obtain a deterministic NC-Turing reduction from bivariate $\\RIT$ to bivariate ncRANK, thereby proving that a deterministic NC algorithm for bivariate ncRANK would imply that both multivariate RIT and multivariate ncRANK are in deterministic NC.","sentences":["We study the noncommutative rank problem, ncRANK, of computing the rank of matrices with linear entries in $n$ noncommuting variables and the problem of noncommutative Rational Identity Testing, RIT, which is to decide if a given rational formula in $n$ noncommuting variables is zero on its domain of definition.","Motivated by the question whether these problems have deterministic NC algorithms, we revisit their interrelationship from a parallel complexity point of view.","We show the following results:   1.","Based on Cohn's embedding theorem \\cite{Co90,Cohnfir} we show deterministic NC reductions from multivariate ncRANK to bivariate ncRANK and from multivariate RIT to bivariate RIT.   2.","We obtain a deterministic NC-Turing reduction from bivariate $\\RIT$ to bivariate ncRANK, thereby proving that a deterministic NC algorithm for bivariate ncRANK would imply that both multivariate RIT and multivariate ncRANK are in deterministic NC."],"url":"http://arxiv.org/abs/2404.16382v1","category":"cs.CC"}
{"created":"2024-04-25 07:11:51","title":"Does carrier localization affect the anomalous Hall effect?","abstract":"The effect of carrier localization due to electron-electron interaction in anomalous Hall effect is elusive and there are contradictory results in the literature. To address the issue, we report here the detailed transport study including the Hall measurements on $\\beta$-Mn type cubic compound Co$_7$Zn$_7$Mn$_6$ with chiral crystal structure, which lacks global mirror symmetry. The alloy orders magnetically below $T_c$ = 204 K, and reported to show spin glass state at low temperature. The longitudinal resistivity ($\\rho_{xx}$) shows a pronounced upturn below $T_{min}$ = 75 K, which is found to be associated with carrier localization due to quantum interference effect. The upturn in $\\rho_{xx}$ shows a $T^{1/2}$ dependence and it is practically insensitive to the externally applied magnetic field, which indicate that electron-electron interaction is primarily responsible for the low-$T$ upturn. The studied sample shows considerable value of anomalous Hall effect below $T_c$. We found that the localization effect is present in the ordinary Hall coefficient ($R_0$), but we failed to observe any signature of localization in the anomalous Hall resistivity or conductivity. The absence of localization effect in the anomalous Hall effect in Co$_7$Zn$_7$Mn$_6$ may be due to large carrier density, and it warrants further theoretical investigations, particularly with systems having broken mirror symmetry.","sentences":["The effect of carrier localization due to electron-electron interaction in anomalous Hall effect is elusive and there are contradictory results in the literature.","To address the issue, we report here the detailed transport study including the Hall measurements on $\\beta$-Mn type cubic compound Co$_7$Zn$_7$Mn$_6$ with chiral crystal structure, which lacks global mirror symmetry.","The alloy orders magnetically below $T_c$ = 204 K, and reported to show spin glass state at low temperature.","The longitudinal resistivity ($\\rho_{xx}$) shows a pronounced upturn below $T_{min}$ = 75 K, which is found to be associated with carrier localization due to quantum interference effect.","The upturn in $\\rho_{xx}$ shows a $T^{1/2}$ dependence and it is practically insensitive to the externally applied magnetic field, which indicate that electron-electron interaction is primarily responsible for the low-$T$ upturn.","The studied sample shows considerable value of anomalous Hall effect below $T_c$. We found that the localization effect is present in the ordinary Hall coefficient ($R_0$), but we failed to observe any signature of localization in the anomalous Hall resistivity or conductivity.","The absence of localization effect in the anomalous Hall effect in Co$_7$Zn$_7$Mn$_6$ may be due to large carrier density, and it warrants further theoretical investigations, particularly with systems having broken mirror symmetry."],"url":"http://arxiv.org/abs/2404.16368v1","category":"cond-mat.str-el"}
{"created":"2024-04-25 06:42:32","title":"Evolutionary Causal Discovery with Relative Impact Stratification for Interpretable Data Analysis","abstract":"This study proposes Evolutionary Causal Discovery (ECD) for causal discovery that tailors response variables, predictor variables, and corresponding operators to research datasets. Utilizing genetic programming for variable relationship parsing, the method proceeds with the Relative Impact Stratification (RIS) algorithm to assess the relative impact of predictor variables on the response variable, facilitating expression simplification and enhancing the interpretability of variable relationships. ECD proposes an expression tree to visualize the RIS results, offering a differentiated depiction of unknown causal relationships compared to conventional causal discovery. The ECD method represents an evolution and augmentation of existing causal discovery methods, providing an interpretable approach for analyzing variable relationships in complex systems, particularly in healthcare settings with Electronic Health Record (EHR) data. Experiments on both synthetic and real-world EHR datasets demonstrate the efficacy of ECD in uncovering patterns and mechanisms among variables, maintaining high accuracy and stability across different noise levels. On the real-world EHR dataset, ECD reveals the intricate relationships between the response variable and other predictive variables, aligning with the results of structural equation modeling and shapley additive explanations analyses.","sentences":["This study proposes Evolutionary Causal Discovery (ECD) for causal discovery that tailors response variables, predictor variables, and corresponding operators to research datasets.","Utilizing genetic programming for variable relationship parsing, the method proceeds with the Relative Impact Stratification (RIS) algorithm to assess the relative impact of predictor variables on the response variable, facilitating expression simplification and enhancing the interpretability of variable relationships.","ECD proposes an expression tree to visualize the RIS results, offering a differentiated depiction of unknown causal relationships compared to conventional causal discovery.","The ECD method represents an evolution and augmentation of existing causal discovery methods, providing an interpretable approach for analyzing variable relationships in complex systems, particularly in healthcare settings with Electronic Health Record (EHR) data.","Experiments on both synthetic and real-world EHR datasets demonstrate the efficacy of ECD in uncovering patterns and mechanisms among variables, maintaining high accuracy and stability across different noise levels.","On the real-world EHR dataset, ECD reveals the intricate relationships between the response variable and other predictive variables, aligning with the results of structural equation modeling and shapley additive explanations analyses."],"url":"http://arxiv.org/abs/2404.16361v1","category":"cs.LG"}
{"created":"2024-04-25 06:36:00","title":"Reverse engineering the brain input: Network control theory to identify cognitive task-related control nodes","abstract":"The human brain receives complex inputs when performing cognitive tasks, which range from external inputs via the senses to internal inputs from other brain regions. However, the explicit inputs to the brain during a cognitive task remain unclear. Here, we present an input identification framework for reverse engineering the control nodes and the corresponding inputs to the brain. The framework is verified with synthetic data generated by a predefined linear system, indicating it can robustly reconstruct data and recover the inputs. Then we apply the framework to the real motor-task fMRI data from 200 human subjects. Our results show that the model with sparse inputs can reconstruct neural dynamics in motor tasks ($EV=0.779$) and the identified 28 control nodes largely overlap with the motor system. Underpinned by network control theory, our framework offers a general tool for understanding brain inputs.","sentences":["The human brain receives complex inputs when performing cognitive tasks, which range from external inputs via the senses to internal inputs from other brain regions.","However, the explicit inputs to the brain during a cognitive task remain unclear.","Here, we present an input identification framework for reverse engineering the control nodes and the corresponding inputs to the brain.","The framework is verified with synthetic data generated by a predefined linear system, indicating it can robustly reconstruct data and recover the inputs.","Then we apply the framework to the real motor-task fMRI data from 200 human subjects.","Our results show that the model with sparse inputs can reconstruct neural dynamics in motor tasks ($EV=0.779$) and the identified 28 control nodes largely overlap with the motor system.","Underpinned by network control theory, our framework offers a general tool for understanding brain inputs."],"url":"http://arxiv.org/abs/2404.16357v1","category":"q-bio.NC"}
{"created":"2024-04-25 06:20:58","title":"A Graphical Calculus for Stable Curvature Invariants","abstract":"In this article we develop a graphical calculus for stable invariants of Riemannian manifolds akin to the graphical calculus for Rozansky-Witten invariants for hyperk\\\"ahler manifolds; based on interpreting trivalent graphs with colored edges as stably invariant polynomials on the space of algebraic curvature tensors. In this graphical calculus we describe explicitly the Pfaffian polynomials central to the Theorem of Chern-Gau{\\ss}-Bonnet and the normalized moment polynomials calculating the moments of sectional curvature considered as a random variable on the Gra{\\ss}mannian of planes. Eventually we illustrate the power of this graphical calculus by deriving a curvature identity for compact Einstein manifolds of dimensions greater than 2 involving the Euler characteristic, the third moment of sectional curvature and the $L^2$--norm of the covariant derivative of the curvature tensor. A model implementation of this calculus for the computer algebra system Maxima is available for download under http://www.matcuer.unam.mx/~gw/CurvGraphs.mac.","sentences":["In this article we develop a graphical calculus for stable invariants of Riemannian manifolds akin to the graphical calculus for Rozansky-Witten invariants for hyperk\\\"ahler manifolds; based on interpreting trivalent graphs with colored edges as stably invariant polynomials on the space of algebraic curvature tensors.","In this graphical calculus we describe explicitly the Pfaffian polynomials central to the Theorem of Chern-Gau{\\ss}-Bonnet and the normalized moment polynomials calculating the moments of sectional curvature considered as a random variable on the Gra{\\ss}mannian of planes.","Eventually we illustrate the power of this graphical calculus by deriving a curvature identity for compact Einstein manifolds of dimensions greater than 2 involving the Euler characteristic, the third moment of sectional curvature and the $L^2$--norm of the covariant derivative of the curvature tensor.","A model implementation of this calculus for the computer algebra system Maxima is available for download under http://www.matcuer.unam.mx/~gw/CurvGraphs.mac."],"url":"http://arxiv.org/abs/2404.16355v1","category":"math.DG"}
{"created":"2024-04-25 06:08:43","title":"QREChem: Quantum Resource Estimation Software for Chemistry Applications","abstract":"As quantum hardware continues to improve, more and more application scientists have entered the field of quantum computing. However, even with the rapid improvements in the last few years, quantum devices, especially for quantum chemistry applications, still struggle to perform calculations that classical computers could not calculate. In lieu of being able to perform specific calculations, it is important have a systematic way of estimating the resources necessary to tackle specific problems. Standard arguments about computational complexity provide hope that quantum computers will be useful for problems in quantum chemistry but obscure the true impact of many algorithmic overheads. These overheads will ultimately determine the precise point when quantum computers will perform better than classical computers. We have developed QREChem to provide logical resource estimates for ground state energy estimation in quantum chemistry through a Trotter-based quantum phase estimation approach. QREChem provides resource estimates which include the specific overheads inherent to problems in quantum chemistry by including heuristic estimates of the number of Trotter steps and number of necessary ancilla, allowing for more accurate estimates of the total number of gates. We utilize QREChem to provide logical resource estimates for a variety of small molecules in various basis sets, obtaining estimates in the range of $10^7-10^{15}$ for total number of T gates. We also determine estimates for the FeMoco molecule and compare all estimates to other resource estimation tools.","sentences":["As quantum hardware continues to improve, more and more application scientists have entered the field of quantum computing.","However, even with the rapid improvements in the last few years, quantum devices, especially for quantum chemistry applications, still struggle to perform calculations that classical computers could not calculate.","In lieu of being able to perform specific calculations, it is important have a systematic way of estimating the resources necessary to tackle specific problems.","Standard arguments about computational complexity provide hope that quantum computers will be useful for problems in quantum chemistry but obscure the true impact of many algorithmic overheads.","These overheads will ultimately determine the precise point when quantum computers will perform better than classical computers.","We have developed QREChem to provide logical resource estimates for ground state energy estimation in quantum chemistry through a Trotter-based quantum phase estimation approach.","QREChem provides resource estimates which include the specific overheads inherent to problems in quantum chemistry by including heuristic estimates of the number of Trotter steps and number of necessary ancilla, allowing for more accurate estimates of the total number of gates.","We utilize QREChem to provide logical resource estimates for a variety of small molecules in various basis sets, obtaining estimates in the range of $10^7-10^{15}$ for total number of T gates.","We also determine estimates for the FeMoco molecule and compare all estimates to other resource estimation tools."],"url":"http://arxiv.org/abs/2404.16351v1","category":"cs.ET"}
{"created":"2024-04-25 05:59:44","title":"More Asymmetry Yields Faster Matrix Multiplication","abstract":"We present a new improvement on the laser method for designing fast matrix multiplication algorithms. The new method further develops the recent advances by [Duan, Wu, Zhou FOCS 2023] and [Vassilevska Williams, Xu, Xu, Zhou SODA 2024]. Surprisingly the new improvement is achieved by incorporating more asymmetry in the analysis, circumventing a fundamental tool of prior work that requires two of the three dimensions to be treated identically. The method yields a new bound on the square matrix multiplication exponent $$\\omega<2.371339,$$ improved from the previous bound of $\\omega<2.371552$. We also improve the bounds of the exponents for multiplying rectangular matrices of various shapes.","sentences":["We present a new improvement on the laser method for designing fast matrix multiplication algorithms.","The new method further develops the recent advances by [Duan, Wu, Zhou FOCS 2023] and [Vassilevska Williams, Xu, Xu, Zhou SODA 2024].","Surprisingly the new improvement is achieved by incorporating more asymmetry in the analysis, circumventing a fundamental tool of prior work that requires two of the three dimensions to be treated identically.","The method yields a new bound on the square matrix multiplication exponent $$\\omega<2.371339,$$ improved from the previous bound of $\\omega<2.371552$. We also improve the bounds of the exponents for multiplying rectangular matrices of various shapes."],"url":"http://arxiv.org/abs/2404.16349v1","category":"cs.DS"}
{"created":"2024-04-25 05:59:28","title":"Enhancing Arterial Blood Flow Simulations through Physics-Informed Neural Networks","abstract":"This study introduces a computational approach leveraging physics-informed neural networks (PINNs) for the efficient computation of arterial blood flows, particularly focusing on solving the incompressible Navier-Stokes equations by using the domain decomposition technique. Unlike conventional computational fluid dynamics methods, PINNs offer advantages by eliminating the need for discretized meshes and enabling the direct solution of partial differential equations (PDEs). In this paper, we propose the weighted Extended Physics-Informed Neural Networks (WXPINNs) and weighted Conservative Physics-Informed Neural Networks (WCPINNs), tailored for detailed hemodynamic simulations based on generalized space-time domain decomposition techniques. The inclusion of multiple neural networks enhances the representation capacity of the weighted PINN methods. Furthermore, the weighted PINNs can be efficiently trained in parallel computing frameworks by employing separate neural networks for each sub-domain. We show that PINNs simulation results circumvent backflow instabilities, underscoring a notable advantage of employing PINNs over traditional numerical methods to solve such complex blood flow models. They naturally address such challenges within their formulations. The presented numerical results demonstrate that the proposed weighted PINNs outperform traditional PINNs settings, where sub-PINNs are applied to each subdomain separately. This study contributes to the integration of deep learning methodologies with fluid mechanics, paving the way for accurate and efficient high-fidelity simulations in biomedical applications, particularly in modeling arterial blood flow.","sentences":["This study introduces a computational approach leveraging physics-informed neural networks (PINNs) for the efficient computation of arterial blood flows, particularly focusing on solving the incompressible Navier-Stokes equations by using the domain decomposition technique.","Unlike conventional computational fluid dynamics methods, PINNs offer advantages by eliminating the need for discretized meshes and enabling the direct solution of partial differential equations (PDEs).","In this paper, we propose the weighted Extended Physics-Informed Neural Networks (WXPINNs) and weighted Conservative Physics-Informed Neural Networks (WCPINNs), tailored for detailed hemodynamic simulations based on generalized space-time domain decomposition techniques.","The inclusion of multiple neural networks enhances the representation capacity of the weighted PINN methods.","Furthermore, the weighted PINNs can be efficiently trained in parallel computing frameworks by employing separate neural networks for each sub-domain.","We show that PINNs simulation results circumvent backflow instabilities, underscoring a notable advantage of employing PINNs over traditional numerical methods to solve such complex blood flow models.","They naturally address such challenges within their formulations.","The presented numerical results demonstrate that the proposed weighted PINNs outperform traditional PINNs settings, where sub-PINNs are applied to each subdomain separately.","This study contributes to the integration of deep learning methodologies with fluid mechanics, paving the way for accurate and efficient high-fidelity simulations in biomedical applications, particularly in modeling arterial blood flow."],"url":"http://arxiv.org/abs/2404.16347v1","category":"math.NA"}
{"created":"2024-04-25 05:37:53","title":"Imaging Tunable Luttinger Liquid Systems in van der Waals Heterostructures","abstract":"One-dimensional (1D) interacting electrons are often described as a Luttinger liquid1-4 having properties that are intrinsically different from Fermi liquids in higher dimensions5,6. 1D electrons in materials systems exhibit exotic quantum phenomena that can be tuned by both intra- and inter-1D-chain electronic interactions, but their experimental characterization can be challenging. Here we demonstrate that layer-stacking domain walls (DWs) in van der Waals heterostructures form a broadly tunable Luttinger liquid system including both isolated and coupled arrays. We have imaged the evolution of DW Luttinger liquids under different interaction regimes tuned by electron density using a novel scanning tunneling microscopy (STM) technique. Single DWs at low carrier density are highly susceptible to Wigner crystallization consistent with a spin-incoherent Luttinger liquid, while at intermediate densities dimerized Wigner crystals form due to an enhanced magneto-elastic coupling. Periodic arrays of DWs exhibit an interplay between intra- and inter-chain interactions that gives rise to new quantum phases. At low electron densities inter-chain interactions are dominant and induce a 2D electron crystal composed of phased-locked 1D Wigner crystal in a staggered configuration. Increased electron density causes intra-chain fluctuation potentials to dominate, leading to an electronic smectic liquid crystal phase where electrons are ordered with algebraical correlation decay along the chain direction but disordered between chains. Our work shows that layer-stacking DWs in 2D heterostructures offers new opportunities to explore Luttinger liquid physics.","sentences":["One-dimensional (1D) interacting electrons are often described as a Luttinger liquid1-4 having properties that are intrinsically different from Fermi liquids in higher dimensions5,6.","1D electrons in materials systems exhibit exotic quantum phenomena that can be tuned by both intra- and inter-1D-chain electronic interactions, but their experimental characterization can be challenging.","Here we demonstrate that layer-stacking domain walls (DWs) in van der Waals heterostructures form a broadly tunable Luttinger liquid system including both isolated and coupled arrays.","We have imaged the evolution of DW Luttinger liquids under different interaction regimes tuned by electron density using a novel scanning tunneling microscopy (STM) technique.","Single DWs at low carrier density are highly susceptible to Wigner crystallization consistent with a spin-incoherent Luttinger liquid, while at intermediate densities dimerized Wigner crystals form due to an enhanced magneto-elastic coupling.","Periodic arrays of DWs exhibit an interplay between intra- and inter-chain interactions that gives rise to new quantum phases.","At low electron densities inter-chain interactions are dominant and induce a 2D electron crystal composed of phased-locked 1D Wigner crystal in a staggered configuration.","Increased electron density causes intra-chain fluctuation potentials to dominate, leading to an electronic smectic liquid crystal phase where electrons are ordered with algebraical correlation decay along the chain direction but disordered between chains.","Our work shows that layer-stacking DWs in 2D heterostructures offers new opportunities to explore Luttinger liquid physics."],"url":"http://arxiv.org/abs/2404.16344v1","category":"cond-mat.str-el"}
{"created":"2024-04-25 04:31:41","title":"On Approximating the Dynamic and Discrete Network Flow Problem","abstract":"We examine the dynamic network flow problem under the assumption that the flow consists of discrete units. The dynamic network flow problem is commonly addressed in the context of developing evacuation plans, where the flow is typically treated as a continuous quantity. However, real-world scenarios often involve moving groups, such as families, as single units. We demonstrate that solving the dynamic flow problem with this consideration is APX-hard. Conversely, we present a PTAS for instances where the base graph is a path with a constant number of nodes. We introduce a `ready time' constraint to the minsum bin packing problem, meaning certain items cannot be placed in specific bins, develop a PTAS for this modified problem, and apply our algorithms to the discrete and dynamic flow problem.","sentences":["We examine the dynamic network flow problem under the assumption that the flow consists of discrete units.","The dynamic network flow problem is commonly addressed in the context of developing evacuation plans, where the flow is typically treated as a continuous quantity.","However, real-world scenarios often involve moving groups, such as families, as single units.","We demonstrate that solving the dynamic flow problem with this consideration is APX-hard.","Conversely, we present a PTAS for instances where the base graph is a path with a constant number of nodes.","We introduce a `ready time' constraint to the minsum bin packing problem, meaning certain items cannot be placed in specific bins, develop a PTAS for this modified problem, and apply our algorithms to the discrete and dynamic flow problem."],"url":"http://arxiv.org/abs/2404.16329v1","category":"cs.DS"}
{"created":"2024-04-25 04:23:40","title":"NeuroKoopman Dynamic Causal Discovery","abstract":"In many real-world applications where the system dynamics has an underlying interdependency among its variables (such as power grid, economics, neuroscience, omics networks, environmental ecosystems, and others), one is often interested in knowing whether the past values of one time series influences the future of another, known as Granger causality, and the associated underlying dynamics. This paper introduces a Koopman-inspired framework that leverages neural networks for data-driven learning of the Koopman bases, termed NeuroKoopman Dynamic Causal Discovery (NKDCD), for reliably inferring the Granger causality along with the underlying nonlinear dynamics. NKDCD employs an autoencoder architecture that lifts the nonlinear dynamics to a higher dimension using data-learned bases, where the lifted time series can be reliably modeled linearly. The lifting function, the linear Granger causality lag matrices, and the projection function (from lifted space to base space) are all represented as multilayer perceptrons and are all learned simultaneously in one go. NKDCD also utilizes sparsity-inducing penalties on the weights of the lag matrices, encouraging the model to select only the needed causal dependencies within the data. Through extensive testing on practically applicable datasets, it is shown that the NKDCD outperforms the existing nonlinear Granger causality discovery approaches.","sentences":["In many real-world applications where the system dynamics has an underlying interdependency among its variables (such as power grid, economics, neuroscience, omics networks, environmental ecosystems, and others), one is often interested in knowing whether the past values of one time series influences the future of another, known as Granger causality, and the associated underlying dynamics.","This paper introduces a Koopman-inspired framework that leverages neural networks for data-driven learning of the Koopman bases, termed NeuroKoopman Dynamic Causal Discovery (NKDCD), for reliably inferring the Granger causality along with the underlying nonlinear dynamics.","NKDCD employs an autoencoder architecture that lifts the nonlinear dynamics to a higher dimension using data-learned bases, where the lifted time series can be reliably modeled linearly.","The lifting function, the linear Granger causality lag matrices, and the projection function (from lifted space to base space) are all represented as multilayer perceptrons and are all learned simultaneously in one go.","NKDCD also utilizes sparsity-inducing penalties on the weights of the lag matrices, encouraging the model to select only the needed causal dependencies within the data.","Through extensive testing on practically applicable datasets, it is shown that the NKDCD outperforms the existing nonlinear Granger causality discovery approaches."],"url":"http://arxiv.org/abs/2404.16326v1","category":"cs.LG"}
{"created":"2024-04-25 17:56:45","title":"Meta-Transfer Derm-Diagnosis: Exploring Few-Shot Learning and Transfer Learning for Skin Disease Classification in Long-Tail Distribution","abstract":"Addressing the challenges of rare diseases is difficult, especially with the limited number of reference images and a small patient population. This is more evident in rare skin diseases, where we encounter long-tailed data distributions that make it difficult to develop unbiased and broadly effective models. The diverse ways in which image datasets are gathered and their distinct purposes also add to these challenges. Our study conducts a detailed examination of the benefits and drawbacks of episodic and conventional training methodologies, adopting a few-shot learning approach alongside transfer learning. We evaluated our models using the ISIC2018, Derm7pt, and SD-198 datasets. With minimal labeled examples, our models showed substantial information gains and better performance compared to previously trained models. Our research emphasizes the improved ability to represent features in DenseNet121 and MobileNetV2 models, achieved by using pre-trained models on ImageNet to increase similarities within classes. Moreover, our experiments, ranging from 2-way to 5-way classifications with up to 10 examples, showed a growing success rate for traditional transfer learning methods as the number of examples increased. The addition of data augmentation techniques significantly improved our transfer learning based model performance, leading to higher performances than existing methods, especially in the SD-198 and ISIC2018 datasets. All source code related to this work will be made publicly available soon at the provided URL.","sentences":["Addressing the challenges of rare diseases is difficult, especially with the limited number of reference images and a small patient population.","This is more evident in rare skin diseases, where we encounter long-tailed data distributions that make it difficult to develop unbiased and broadly effective models.","The diverse ways in which image datasets are gathered and their distinct purposes also add to these challenges.","Our study conducts a detailed examination of the benefits and drawbacks of episodic and conventional training methodologies, adopting a few-shot learning approach alongside transfer learning.","We evaluated our models using the ISIC2018, Derm7pt, and SD-198 datasets.","With minimal labeled examples, our models showed substantial information gains and better performance compared to previously trained models.","Our research emphasizes the improved ability to represent features in DenseNet121 and MobileNetV2 models, achieved by using pre-trained models on ImageNet to increase similarities within classes.","Moreover, our experiments, ranging from 2-way to 5-way classifications with up to 10 examples, showed a growing success rate for traditional transfer learning methods as the number of examples increased.","The addition of data augmentation techniques significantly improved our transfer learning based model performance, leading to higher performances than existing methods, especially in the SD-198 and ISIC2018 datasets.","All source code related to this work will be made publicly available soon at the provided URL."],"url":"http://arxiv.org/abs/2404.16814v1","category":"cs.CV"}
{"created":"2024-04-25 17:49:51","title":"Non-supersymmetric duality cascade of QCD(BF) via semiclassics on $\\mathbb{R}^2\\times T^2$ with the baryon-'t Hooft flux","abstract":"We study the phase diagrams of the bifundamental QCD (QCD(BF)) of different ranks, which is the $4$d $SU(N_1) \\times SU(N_2)$ gauge theory coupled with a bifundamental Dirac fermion. After discussing the anomaly constraints on possible vacuum structures, we apply a novel semiclassical approach on $\\mathbb{R}^2\\times T^2$ with the baryon-'t Hooft flux to obtain the concrete dynamics. The $2$d effective theory is derived by the dilute gas approximation of center vortices, and it serves as the basis for determining the phase diagram of the model under the assumption of adiabatic continuity. As an application, we justify the non-supersymmetric duality cascade between different QCD(BF), which has been conjectured in the large-${N}$ argument. Combined with the semiclassics and the large-$N_{1,2}$ limit, we construct the explicit duality map from the parent theory, $SU(N_1) \\times SU(N_2)$ QCD(BF), to the daughter theory, $SU(N_1) \\times SU(N_2-N_1)$ QCD(BF), including the correspondence of the coupling constants. We numerically examine the validity of the duality also for finite $N_{1,2}$ within our semiclassics, finding a remarkable agreement of the phase diagrams between the parent and daughter sides.","sentences":["We study the phase diagrams of the bifundamental QCD (QCD(BF)) of different ranks, which is the $4$d $SU(N_1)","\\times SU(N_2)$ gauge theory coupled with a bifundamental Dirac fermion.","After discussing the anomaly constraints on possible vacuum structures, we apply a novel semiclassical approach on $\\mathbb{R}^2\\times T^2$ with the baryon-'t Hooft flux to obtain the concrete dynamics.","The $2$d effective theory is derived by the dilute gas approximation of center vortices, and it serves as the basis for determining the phase diagram of the model under the assumption of adiabatic continuity.","As an application, we justify the non-supersymmetric duality cascade between different QCD(BF), which has been conjectured in the large-${N}$ argument.","Combined with the semiclassics and the large-$N_{1,2}$ limit, we construct the explicit duality map from the parent theory, $SU(N_1)","\\times SU(N_2)$ QCD(BF), to the daughter theory, $SU(N_1)","\\times SU(N_2-N_1)$ QCD(BF), including the correspondence of the coupling constants.","We numerically examine the validity of the duality also for finite $N_{1,2}$ within our semiclassics, finding a remarkable agreement of the phase diagrams between the parent and daughter sides."],"url":"http://arxiv.org/abs/2404.16803v1","category":"hep-th"}
{"created":"2024-04-25 17:49:28","title":"Transformer-Based Local Feature Matching for Multimodal Image Registration","abstract":"Ultrasound imaging is a cost-effective and radiation-free modality for visualizing anatomical structures in real-time, making it ideal for guiding surgical interventions. However, its limited field-of-view, speckle noise, and imaging artifacts make it difficult to interpret the images for inexperienced users. In this paper, we propose a new 2D ultrasound to 3D CT registration method to improve surgical guidance during ultrasound-guided interventions. Our approach adopts a dense feature matching method called LoFTR to our multimodal registration problem. We learn to predict dense coarse-to-fine correspondences using a Transformer-based architecture to estimate a robust rigid transformation between a 2D ultrasound frame and a CT scan. Additionally, a fully differentiable pose estimation method is introduced, optimizing LoFTR on pose estimation error during training. Experiments conducted on a multimodal dataset of ex vivo porcine kidneys demonstrate the method's promising results for intraoperative, trackerless ultrasound pose estimation. By mapping 2D ultrasound frames into the 3D CT volume space, the method provides intraoperative guidance, potentially improving surgical workflows and image interpretation.","sentences":["Ultrasound imaging is a cost-effective and radiation-free modality for visualizing anatomical structures in real-time, making it ideal for guiding surgical interventions.","However, its limited field-of-view, speckle noise, and imaging artifacts make it difficult to interpret the images for inexperienced users.","In this paper, we propose a new 2D ultrasound to 3D CT registration method to improve surgical guidance during ultrasound-guided interventions.","Our approach adopts a dense feature matching method called LoFTR to our multimodal registration problem.","We learn to predict dense coarse-to-fine correspondences using a Transformer-based architecture to estimate a robust rigid transformation between a 2D ultrasound frame and a CT scan.","Additionally, a fully differentiable pose estimation method is introduced, optimizing LoFTR on pose estimation error during training.","Experiments conducted on a multimodal dataset of ex vivo porcine kidneys demonstrate the method's promising results for intraoperative, trackerless ultrasound pose estimation.","By mapping 2D ultrasound frames into the 3D CT volume space, the method provides intraoperative guidance, potentially improving surgical workflows and image interpretation."],"url":"http://arxiv.org/abs/2404.16802v1","category":"eess.IV"}
{"created":"2024-04-25 17:40:50","title":"Structure-Preserving Oscillation-Eliminating Discontinuous Galerkin Schemes for Ideal MHD Equations: Locally Divergence-Free and Positivity-Preserving","abstract":"This paper develops structure-preserving, oscillation-eliminating discontinuous Galerkin (OEDG) schemes for ideal magnetohydrodynamics (MHD), as a sequel to our recent work [Peng, Sun, and Wu, OEDG: Oscillation-eliminating discontinuous Galerkin method for hyperbolic conservation laws, 2023]. The schemes are based on a locally divergence-free (LDF) oscillation-eliminating (OE) procedure to suppress spurious oscillations while maintaining many of the good properties of original DG schemes, such as conservation, local compactness, and optimal convergence rates. The OE procedure is built on the solution operator of a novel damping equation -- a simple linear ordinary differential equation (ODE) whose exact solution can be exactly formulated. Because this OE procedure does not interfere with DG spatial discretization and RK stage update, it can be easily incorporated to existing DG codes as an independent module. These features make the proposed LDF OEDG schemes highly efficient and easy to implement.In addition, we present a positivity-preserving (PP) analysis of the LDF OEDG schemes on Cartesian meshes via the optimal convex decomposition technique and the geometric quasi-linearization (GQL) approach. Efficient PP LDF OEDG schemes are obtained with the HLL flux under a condition accessible by the simple local scaling PP limiter.Several one- and two-dimensional MHD tests confirm the accuracy, effectiveness, and robustness of the proposed structure-preserving OEDG schemes.","sentences":["This paper develops structure-preserving, oscillation-eliminating discontinuous Galerkin (OEDG) schemes for ideal magnetohydrodynamics (MHD), as a sequel to our recent work [Peng, Sun, and Wu, OEDG: Oscillation-eliminating discontinuous Galerkin method for hyperbolic conservation laws, 2023].","The schemes are based on a locally divergence-free (LDF) oscillation-eliminating (OE) procedure to suppress spurious oscillations while maintaining many of the good properties of original DG schemes, such as conservation, local compactness, and optimal convergence rates.","The OE procedure is built on the solution operator of a novel damping equation -- a simple linear ordinary differential equation (ODE) whose exact solution can be exactly formulated.","Because this OE procedure does not interfere with DG spatial discretization and RK stage update, it can be easily incorporated to existing DG codes as an independent module.","These features make the proposed LDF OEDG schemes highly efficient and easy to implement.","In addition, we present a positivity-preserving (PP) analysis of the LDF OEDG schemes on Cartesian meshes via the optimal convex decomposition technique and the geometric quasi-linearization (GQL) approach.","Efficient PP LDF OEDG schemes are obtained with the HLL flux under a condition accessible by the simple local scaling PP limiter.","Several one-","and two-dimensional MHD tests confirm the accuracy, effectiveness, and robustness of the proposed structure-preserving OEDG schemes."],"url":"http://arxiv.org/abs/2404.16794v1","category":"math.NA"}
{"created":"2024-04-25 17:26:58","title":"Estimating Metocean Environments Associated with Extreme Structural Response","abstract":"Extreme value analysis (EVA) uses data to estimate long-term extreme environmental conditions for variables such as significant wave height and period, for the design of marine structures. Together with models for the short-term evolution of the ocean environment and for wave-structure interaction, EVA provides a basis for full probabilistic design analysis. Environmental contours provide an alternate approach to estimating structural integrity, without requiring structural knowledge. These contour methods also exploit statistical models, including EVA, but avoid the need for structural modelling by making what are believed to be conservative assumptions about the shape of the structural failure boundary in the environment space. These assumptions, however, may not always be appropriate, or may lead to unnecessary wasted resources from over design. We introduce a methodology for full probabilistic analysis to estimate the joint probability density of the environment, conditional on the occurrence of an extreme structural response, for simple structures. We use this conditional density of the environment as a basis to assess the performance of different environmental contour methods. We demonstrate the difficulty of estimating the contour boundary in the environment space for typical data samples, as well as the dependence of the performance of the environmental contour on the structure being considered.","sentences":["Extreme value analysis (EVA) uses data to estimate long-term extreme environmental conditions for variables such as significant wave height and period, for the design of marine structures.","Together with models for the short-term evolution of the ocean environment and for wave-structure interaction, EVA provides a basis for full probabilistic design analysis.","Environmental contours provide an alternate approach to estimating structural integrity, without requiring structural knowledge.","These contour methods also exploit statistical models, including EVA, but avoid the need for structural modelling by making what are believed to be conservative assumptions about the shape of the structural failure boundary in the environment space.","These assumptions, however, may not always be appropriate, or may lead to unnecessary wasted resources from over design.","We introduce a methodology for full probabilistic analysis to estimate the joint probability density of the environment, conditional on the occurrence of an extreme structural response, for simple structures.","We use this conditional density of the environment as a basis to assess the performance of different environmental contour methods.","We demonstrate the difficulty of estimating the contour boundary in the environment space for typical data samples, as well as the dependence of the performance of the environmental contour on the structure being considered."],"url":"http://arxiv.org/abs/2404.16775v1","category":"stat.ME"}
{"created":"2024-04-25 17:23:31","title":"Pseudogap phase as fluctuating pair density wave","abstract":"The physical nature of pseudogap phase is one of the most important and intriguing problems towards understanding the key mechanism of high temperature superconductivity in cuprates. Theoretically, the square-lattice $t$-$J$ model is widely believed to be the simplest toy model that captures the essential physics of cuprate superconductors. We employ the Grassmann tensor product state approach to investigate uniform states in the underdoped ($\\delta \\lesssim 0.1$) region. In addition to the previously known uniform $d$-wave state, we discover a strongly fluctuating pair density wave (PDW) state with wave vector $Q = (\\pi, \\pi)$. This fluctuating PDW state weakly breaks the $C_4$ rotational symmetry of the square lattice and has a lower or comparable energy to the $d$-wave state (depending on doping and the $t/J$ ratio), making it a promising candidate state for describing the pseudogap phase.","sentences":["The physical nature of pseudogap phase is one of the most important and intriguing problems towards understanding the key mechanism of high temperature superconductivity in cuprates.","Theoretically, the square-lattice $t$-$J$ model is widely believed to be the simplest toy model that captures the essential physics of cuprate superconductors.","We employ the Grassmann tensor product state approach to investigate uniform states in the underdoped ($\\delta \\lesssim 0.1$) region.","In addition to the previously known uniform $d$-wave state, we discover a strongly fluctuating pair density wave (PDW) state with wave vector $Q = (\\pi, \\pi)$.","This fluctuating PDW state weakly breaks the $C_4$ rotational symmetry of the square lattice and has a lower or comparable energy to the $d$-wave state (depending on doping and the $t/J$ ratio), making it a promising candidate state for describing the pseudogap phase."],"url":"http://arxiv.org/abs/2404.16770v1","category":"cond-mat.str-el"}
{"created":"2024-04-25 17:00:24","title":"Estimating the Number of Components in Finite Mixture Models via Variational Approximation","abstract":"This work introduces a new method for selecting the number of components in finite mixture models (FMMs) using variational Bayes, inspired by the large-sample properties of the Evidence Lower Bound (ELBO) derived from mean-field (MF) variational approximation. Specifically, we establish matching upper and lower bounds for the ELBO without assuming conjugate priors, suggesting the consistency of model selection for FMMs based on maximizing the ELBO. As a by-product of our proof, we demonstrate that the MF approximation inherits the stable behavior (benefited from model singularity) of the posterior distribution, which tends to eliminate the extra components under model misspecification where the number of mixture components is over-specified. This stable behavior also leads to the $n^{-1/2}$ convergence rate for parameter estimation, up to a logarithmic factor, under this model overspecification. Empirical experiments are conducted to validate our theoretical findings and compare with other state-of-the-art methods for selecting the number of components in FMMs.","sentences":["This work introduces a new method for selecting the number of components in finite mixture models (FMMs) using variational Bayes, inspired by the large-sample properties of the Evidence Lower Bound (ELBO) derived from mean-field (MF) variational approximation.","Specifically, we establish matching upper and lower bounds for the ELBO without assuming conjugate priors, suggesting the consistency of model selection for FMMs based on maximizing the ELBO.","As a by-product of our proof, we demonstrate that the MF approximation inherits the stable behavior (benefited from model singularity) of the posterior distribution, which tends to eliminate the extra components under model misspecification where the number of mixture components is over-specified.","This stable behavior also leads to the $n^{-1/2}$ convergence rate for parameter estimation, up to a logarithmic factor, under this model overspecification.","Empirical experiments are conducted to validate our theoretical findings and compare with other state-of-the-art methods for selecting the number of components in FMMs."],"url":"http://arxiv.org/abs/2404.16746v1","category":"stat.ME"}
{"created":"2024-04-25 16:42:28","title":"The MOPYS project: A survey of 70 planets in search of extended He I and H atmospheres. No evidence of enhanced evaporation in young planets","abstract":"During the first Gyr of their life, exoplanet atmospheres suffer from different atmospheric escape phenomena that can strongly affect the shape and morphology of the exoplanet itself. These processes can be studied with Ly$\\alpha$, H$\\alpha$ and/or He I triplet observations. We present high-resolution spectroscopy observations from CARMENES and GIARPS checking for He I and H$\\alpha$ signals in 20 exoplanetary atmospheres: V1298Tau c, K2-100b, HD63433b, HD63433c, HD73583b, HD73583c, K2-77b, TOI-2076b, TOI-2048b, HD235088b, TOI-1807b, TOI-1136d, TOI-1268b, TOI-1683b, TOI-2018b, MASCARA-2b, WASP-189b, TOI-2046b, TOI-1431b, and HAT-P-57b. We report two new high-resolution spectroscopy He I detections for TOI-1268b and TOI-2018b, and an H$\\alpha$ detection for TOI-1136d. The MOPYS (Measuring Out-flows in Planets orbiting Young Stars) project aims to understand the evaporating phenomena and test their predictions from the current observations. We compiled a list of 70 exoplanets with He I and/or H$\\alpha$ observations, from this work and the literature, and we considered the He I and H$\\alpha$ results as proxy for atmospheric escape. Our principal results are that 0.1-1Gyr-old planets do not exhibit more He I or H$\\alpha$ detections than older planets, and evaporation signals are more frequent for planets orbiting $\\sim$1-3Gyr-old stars. We provide new constrains to the cosmic shoreline, the empirical division between rocky planets and planets with atmosphere, by using the evaporation detections and explore the capabilities of a new dimensionless parameter, $R_{\\rm He}/R_{\\rm Hill}$, to explain the He I triplet detections. Furthermore, we present a statistically significant upper boundary for the He I triplet detections in the $T_{\\rm eq}$ vs $\\rho_{\\rm p}$ parameter space. Planets located above that boundary are unlikely to show He I absorption signals.","sentences":["During the first Gyr of their life, exoplanet atmospheres suffer from different atmospheric escape phenomena that can strongly affect the shape and morphology of the exoplanet itself.","These processes can be studied with Ly$\\alpha$, H$\\alpha$ and/or He I triplet observations.","We present high-resolution spectroscopy observations from CARMENES and GIARPS checking for He I and H$\\alpha$ signals in 20 exoplanetary atmospheres: V1298Tau c, K2-100b, HD63433b, HD63433c, HD73583b, HD73583c, K2-77b, TOI-2076b, TOI-2048b, HD235088b, TOI-1807b, TOI-1136d, TOI-1268b, TOI-1683b, TOI-2018b, MASCARA-2b, WASP-189b, TOI-2046b, TOI-1431b, and HAT-P-57b.","We report two new high-resolution spectroscopy He I detections for TOI-1268b and TOI-2018b, and an H$\\alpha$ detection for TOI-1136d.","The MOPYS (Measuring Out-flows in Planets orbiting Young Stars) project aims to understand the evaporating phenomena and test their predictions from the current observations.","We compiled a list of 70 exoplanets with He I and/or H$\\alpha$ observations, from this work and the literature, and we considered the He I and H$\\alpha$ results as proxy for atmospheric escape.","Our principal results are that 0.1-1Gyr-old planets do not exhibit more He I or H$\\alpha$ detections than older planets, and evaporation signals are more frequent for planets orbiting $\\sim$1-3Gyr-old stars.","We provide new constrains to the cosmic shoreline, the empirical division between rocky planets and planets with atmosphere, by using the evaporation detections and explore the capabilities of a new dimensionless parameter, $R_{\\rm He}/R_{\\rm Hill}$, to explain the He I triplet detections.","Furthermore, we present a statistically significant upper boundary for the He I triplet detections in the $T_{\\rm eq}$ vs $\\rho_{\\rm p}$ parameter space.","Planets located above that boundary are unlikely to show He I absorption signals."],"url":"http://arxiv.org/abs/2404.16732v1","category":"astro-ph.EP"}
{"created":"2024-04-25 15:59:01","title":"High-Coherence Kerr-cat qubit in 2D architecture","abstract":"The Kerr-cat qubit is a bosonic qubit in which multi-photon Schrodinger cat states are stabilized by applying a two-photon drive to an oscillator with a Kerr nonlinearity. The suppressed bit-flip rate with increasing cat size makes this qubit a promising candidate to implement quantum error correction codes tailored for noise-biased qubits. However, achieving strong light-matter interactions necessary for stabilizing and controlling this qubit has traditionally required strong microwave drives that heat the qubit and degrade its performance. In contrast, increasing the coupling to the drive port removes the need for strong drives at the expense of large Purcell decay. By integrating an effective band-block filter on-chip, we overcome this trade-off and realize a Kerr-cat qubit in a scalable 2D superconducting circuit with high coherence. This filter provides 30 dB of isolation at the qubit frequency with negligible attenuation at the frequencies required for stabilization and readout. We experimentally demonstrate quantum non-demolition readout fidelity of 99.6% for a cat with 8 photons. Also, to have high-fidelity universal control over this qubit, we combine fast Rabi oscillations with a new demonstration of the X(90) gate through phase modulation of the stabilization drive. Finally, the lifetime in this architecture is examined as a function of the cat size of up to 10 photons in the oscillator achieving a bit-flip time higher than 1 ms and only a linear decrease in the phase-flip time, in good agreement with the theoretical analysis of the circuit. Our qubit shows promise as a building block for fault-tolerant quantum processors with a small footprint.","sentences":["The Kerr-cat qubit is a bosonic qubit in which multi-photon Schrodinger cat states are stabilized by applying a two-photon drive to an oscillator with a Kerr nonlinearity.","The suppressed bit-flip rate with increasing cat size makes this qubit a promising candidate to implement quantum error correction codes tailored for noise-biased qubits.","However, achieving strong light-matter interactions necessary for stabilizing and controlling this qubit has traditionally required strong microwave drives that heat the qubit and degrade its performance.","In contrast, increasing the coupling to the drive port removes the need for strong drives at the expense of large Purcell decay.","By integrating an effective band-block filter on-chip, we overcome this trade-off and realize a Kerr-cat qubit in a scalable 2D superconducting circuit with high coherence.","This filter provides 30 dB of isolation at the qubit frequency with negligible attenuation at the frequencies required for stabilization and readout.","We experimentally demonstrate quantum non-demolition readout fidelity of 99.6% for a cat with 8 photons.","Also, to have high-fidelity universal control over this qubit, we combine fast Rabi oscillations with a new demonstration of the X(90) gate through phase modulation of the stabilization drive.","Finally, the lifetime in this architecture is examined as a function of the cat size of up to 10 photons in the oscillator achieving a bit-flip time higher than 1 ms and only a linear decrease in the phase-flip time, in good agreement with the theoretical analysis of the circuit.","Our qubit shows promise as a building block for fault-tolerant quantum processors with a small footprint."],"url":"http://arxiv.org/abs/2404.16697v1","category":"quant-ph"}
{"created":"2024-04-25 15:16:53","title":"Magnetic Resonance Frequency Shift Caused by Nonuniform Field and Boundary Relaxation","abstract":"Magnetic field inhomogeneity is usually detrimental to magnetic resonance (MR) experiments. It is widely recognized that a nonuniform magnetic field can lead to an increase in the resonance line width, as well as a reduction in sensitivity and spectral resolution. However, nonuniform magnetic field can also cause shift in resonance frequency, which received far less attention. In this work, we investigate the frequency shift under arbitrary nonuniform magnetic field and boundary relaxation by applying perturbation theory to the Torrey equation. Several compact frequency shift formulas are reported. We find that the frequency shift is mainly determined by $B_z$ distribution (rather than the transverse field components in previous study) and has important dependence on boundary relaxation. Furthermore, due to the difference of boundary relaxation and high order perturbation correction, this frequency shift is spin-species dependent, which implies a systematic error in many MR based precision measurements such as NMR gyroscope and comagnetometers. This insight provides a potential tool for understanding the unexplained isotope shifts in recent NMR gyroscope and new physics searching experiments that utilize comagnetometers. Finally, we propose a new tool for wall interaction research based on the frequency shift's dependency on boundary relaxation.","sentences":["Magnetic field inhomogeneity is usually detrimental to magnetic resonance (MR) experiments.","It is widely recognized that a nonuniform magnetic field can lead to an increase in the resonance line width, as well as a reduction in sensitivity and spectral resolution.","However, nonuniform magnetic field can also cause shift in resonance frequency, which received far less attention.","In this work, we investigate the frequency shift under arbitrary nonuniform magnetic field and boundary relaxation by applying perturbation theory to the Torrey equation.","Several compact frequency shift formulas are reported.","We find that the frequency shift is mainly determined by $B_z$ distribution (rather than the transverse field components in previous study) and has important dependence on boundary relaxation.","Furthermore, due to the difference of boundary relaxation and high order perturbation correction, this frequency shift is spin-species dependent, which implies a systematic error in many MR based precision measurements such as NMR gyroscope and comagnetometers.","This insight provides a potential tool for understanding the unexplained isotope shifts in recent NMR gyroscope and new physics searching experiments that utilize comagnetometers.","Finally, we propose a new tool for wall interaction research based on the frequency shift's dependency on boundary relaxation."],"url":"http://arxiv.org/abs/2404.16671v1","category":"quant-ph"}
{"created":"2024-04-25 15:06:58","title":"PhyRecon: Physically Plausible Neural Scene Reconstruction","abstract":"While neural implicit representations have gained popularity in multi-view 3D reconstruction, previous work struggles to yield physically plausible results, thereby limiting their applications in physics-demanding domains like embodied AI and robotics. The lack of plausibility originates from both the absence of physics modeling in the existing pipeline and their inability to recover intricate geometrical structures. In this paper, we introduce PhyRecon, which stands as the first approach to harness both differentiable rendering and differentiable physics simulation to learn implicit surface representations. Our framework proposes a novel differentiable particle-based physical simulator seamlessly integrated with the neural implicit representation. At its core is an efficient transformation between SDF-based implicit representation and explicit surface points by our proposed algorithm, Surface Points Marching Cubes (SP-MC), enabling differentiable learning with both rendering and physical losses. Moreover, we model both rendering and physical uncertainty to identify and compensate for the inconsistent and inaccurate monocular geometric priors. The physical uncertainty additionally enables a physics-guided pixel sampling to enhance the learning of slender structures. By amalgamating these techniques, our model facilitates efficient joint modeling with appearance, geometry, and physics. Extensive experiments demonstrate that PhyRecon significantly outperforms all state-of-the-art methods in terms of reconstruction quality. Our reconstruction results also yield superior physical stability, verified by Isaac Gym, with at least a 40% improvement across all datasets, opening broader avenues for future physics-based applications.","sentences":["While neural implicit representations have gained popularity in multi-view 3D reconstruction, previous work struggles to yield physically plausible results, thereby limiting their applications in physics-demanding domains like embodied AI and robotics.","The lack of plausibility originates from both the absence of physics modeling in the existing pipeline and their inability to recover intricate geometrical structures.","In this paper, we introduce PhyRecon, which stands as the first approach to harness both differentiable rendering and differentiable physics simulation to learn implicit surface representations.","Our framework proposes a novel differentiable particle-based physical simulator seamlessly integrated with the neural implicit representation.","At its core is an efficient transformation between SDF-based implicit representation and explicit surface points by our proposed algorithm, Surface Points Marching Cubes (SP-MC), enabling differentiable learning with both rendering and physical losses.","Moreover, we model both rendering and physical uncertainty to identify and compensate for the inconsistent and inaccurate monocular geometric priors.","The physical uncertainty additionally enables a physics-guided pixel sampling to enhance the learning of slender structures.","By amalgamating these techniques, our model facilitates efficient joint modeling with appearance, geometry, and physics.","Extensive experiments demonstrate that PhyRecon significantly outperforms all state-of-the-art methods in terms of reconstruction quality.","Our reconstruction results also yield superior physical stability, verified by Isaac Gym, with at least a 40% improvement across all datasets, opening broader avenues for future physics-based applications."],"url":"http://arxiv.org/abs/2404.16666v1","category":"cs.CV"}
{"created":"2024-04-25 13:39:25","title":"High-Order regularity for fully nonlinear elliptic transmission problems under weak convexity assumption","abstract":"This paper studies Schauder theory to transmission problems modelled by fully nonlinear uniformly elliptic equations of second order. We focus on operators F that fails to be concave or convex in the space of symmetric matrices. In a first scenario, it is considered that F enjoys a small ellipticity aperture. In our second case, we study regularity results where the convexity of the superlevel (or sublevel) sets is verified, implying that the operator F is quasiconcave (or quasiconvex).","sentences":["This paper studies Schauder theory to transmission problems modelled by fully nonlinear uniformly elliptic equations of second order.","We focus on operators F that fails to be concave or convex in the space of symmetric matrices.","In a first scenario, it is considered that F enjoys a small ellipticity aperture.","In our second case, we study regularity results where the convexity of the superlevel (or sublevel) sets is verified, implying that the operator F is quasiconcave (or quasiconvex)."],"url":"http://arxiv.org/abs/2404.16602v1","category":"math.AP"}
{"created":"2024-04-25 13:14:37","title":"The hunt of PeVatrons as the origin of the most energetic photons observed in our Galaxy","abstract":"Ultrarelativistic particles called cosmic rays permeate the Milky Way, propagating through the Galactic turbulent magnetic fields. The mechanisms under which these particles increase their energy can be reasonably described by current theories of acceleration and propagation of cosmic rays. There are, however, still many open questions as to how to reach petaelectronvolt (PeV) energies, the maximum energy believed to be attained in our Galaxy, and in which astrophysical sources (dubbed {\\it PeVatrons}) this ultra-high energy acceleration happens. In this article, we describe the theoretical conditions for plasma acceleration to these energies, and the Galactic sources in which these conditions are possible. These theoretical predictions are then confronted with the latest experimental results, summarising the state-of-the-art of our current knowledge of PeVatrons. We finally describe the prospects to keep advancing the understanding of these elusive objects, still unidentified more than one hundred years after the discovery of cosmic rays.","sentences":["Ultrarelativistic particles called cosmic rays permeate the Milky Way, propagating through the Galactic turbulent magnetic fields.","The mechanisms under which these particles increase their energy can be reasonably described by current theories of acceleration and propagation of cosmic rays.","There are, however, still many open questions as to how to reach petaelectronvolt (PeV) energies, the maximum energy believed to be attained in our Galaxy, and in which astrophysical sources (dubbed {\\it PeVatrons}) this ultra-high energy acceleration happens.","In this article, we describe the theoretical conditions for plasma acceleration to these energies, and the Galactic sources in which these conditions are possible.","These theoretical predictions are then confronted with the latest experimental results, summarising the state-of-the-art of our current knowledge of PeVatrons.","We finally describe the prospects to keep advancing the understanding of these elusive objects, still unidentified more than one hundred years after the discovery of cosmic rays."],"url":"http://arxiv.org/abs/2404.16591v1","category":"astro-ph.HE"}
{"created":"2024-04-25 12:34:23","title":"MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth Estimation of Endoscopic Images","abstract":"Photometric constraint is indispensable for self-supervised monocular depth estimation. It involves warping a source image onto a target view using estimated depth&pose, and then minimizing the difference between the warped and target images. However, the endoscopic built-in light causes significant brightness fluctuations, and thus makes the photometric constraint unreliable. Previous efforts only mitigate this relying on extra models to calibrate image brightness. In this paper, we propose MonoPCC to address the brightness inconsistency radically by reshaping the photometric constraint into a cycle form. Instead of only warping the source image, MonoPCC constructs a closed loop consisting of two opposite forward-backward warping paths: from target to source and then back to target. Thus, the target image finally receives an image cycle-warped from itself, which naturally makes the constraint invariant to brightness changes. Moreover, MonoPCC transplants the source image's phase-frequency into the intermediate warped image to avoid structure lost, and also stabilizes the training via an exponential moving average (EMA) strategy to avoid frequent changes in the forward warping. The comprehensive and extensive experimental results on three datasets demonstrate that our proposed MonoPCC shows a great robustness to the brightness inconsistency, and exceeds other state-of-the-arts by reducing the absolute relative error by at least 7.27%.","sentences":["Photometric constraint is indispensable for self-supervised monocular depth estimation.","It involves warping a source image onto a target view using estimated depth&pose, and then minimizing the difference between the warped and target images.","However, the endoscopic built-in light causes significant brightness fluctuations, and thus makes the photometric constraint unreliable.","Previous efforts only mitigate this relying on extra models to calibrate image brightness.","In this paper, we propose MonoPCC to address the brightness inconsistency radically by reshaping the photometric constraint into a cycle form.","Instead of only warping the source image, MonoPCC constructs a closed loop consisting of two opposite forward-backward warping paths: from target to source and then back to target.","Thus, the target image finally receives an image cycle-warped from itself, which naturally makes the constraint invariant to brightness changes.","Moreover, MonoPCC transplants the source image's phase-frequency into the intermediate warped image to avoid structure lost, and also stabilizes the training via an exponential moving average (EMA) strategy to avoid frequent changes in the forward warping.","The comprehensive and extensive experimental results on three datasets demonstrate that our proposed MonoPCC shows a great robustness to the brightness inconsistency, and exceeds other state-of-the-arts by reducing the absolute relative error by at least 7.27%."],"url":"http://arxiv.org/abs/2404.16571v1","category":"cs.CV"}
{"created":"2024-04-25 10:52:08","title":"360SFUDA++: Towards Source-free UDA for Panoramic Segmentation by Learning Reliable Category Prototypes","abstract":"In this paper, we address the challenging source-free unsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semantic segmentation, given only a pinhole image pre-trained model (i.e., source) and unlabeled panoramic images (i.e., target). Tackling this problem is non-trivial due to three critical challenges: 1) semantic mismatches from the distinct Field-of-View (FoV) between domains, 2) style discrepancies inherent in the UDA problem, and 3) inevitable distortion of the panoramic images. To tackle these problems, we propose 360SFUDA++ that effectively extracts knowledge from the source pinhole model with only unlabeled panoramic images and transfers the reliable knowledge to the target panoramic domain. Specifically, we first utilize Tangent Projection (TP) as it has less distortion and meanwhile slits the equirectangular projection (ERP) to patches with fixed FoV projection (FFP) to mimic the pinhole images. Both projections are shown effective in extracting knowledge from the source model. However, as the distinct projections make it less possible to directly transfer knowledge between domains, we then propose Reliable Panoramic Prototype Adaptation Module (RP2AM) to transfer knowledge at both prediction and prototype levels. RP$^2$AM selects the confident knowledge and integrates panoramic prototypes for reliable knowledge adaptation. Moreover, we introduce Cross-projection Dual Attention Module (CDAM), which better aligns the spatial and channel characteristics across projections at the feature level between domains. Both knowledge extraction and transfer processes are synchronously updated to reach the best performance. Extensive experiments on the synthetic and real-world benchmarks, including outdoor and indoor scenarios, demonstrate that our 360SFUDA++ achieves significantly better performance than prior SFUDA methods.","sentences":["In this paper, we address the challenging source-free unsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semantic segmentation, given only a pinhole image pre-trained model (i.e., source) and unlabeled panoramic images (i.e., target).","Tackling this problem is non-trivial due to three critical challenges: 1) semantic mismatches from the distinct Field-of-View (FoV) between domains, 2) style discrepancies inherent in the UDA problem, and 3) inevitable distortion of the panoramic images.","To tackle these problems, we propose 360SFUDA++ that effectively extracts knowledge from the source pinhole model with only unlabeled panoramic images and transfers the reliable knowledge to the target panoramic domain.","Specifically, we first utilize Tangent Projection (TP) as it has less distortion and meanwhile slits the equirectangular projection (ERP) to patches with fixed FoV projection (FFP) to mimic the pinhole images.","Both projections are shown effective in extracting knowledge from the source model.","However, as the distinct projections make it less possible to directly transfer knowledge between domains, we then propose Reliable Panoramic Prototype Adaptation Module (RP2AM) to transfer knowledge at both prediction and prototype levels.","RP$^2$AM selects the confident knowledge and integrates panoramic prototypes for reliable knowledge adaptation.","Moreover, we introduce Cross-projection Dual Attention Module (CDAM), which better aligns the spatial and channel characteristics across projections at the feature level between domains.","Both knowledge extraction and transfer processes are synchronously updated to reach the best performance.","Extensive experiments on the synthetic and real-world benchmarks, including outdoor and indoor scenarios, demonstrate that our 360SFUDA++ achieves significantly better performance than prior SFUDA methods."],"url":"http://arxiv.org/abs/2404.16501v1","category":"cs.CV"}
{"created":"2024-04-25 10:21:35","title":"Two-Dimensional Eclipse Mapping of the Hot Jupiter WASP-43b with JWST MIRI/LRS","abstract":"We present eclipse maps of the two-dimensional thermal emission from the dayside of the hot Jupiter WASP-43b, derived from an observation of a phase curve with the JWST MIRI/LRS instrument. The observed eclipse shapes deviate significantly from those expected for a planet emitting uniformly over its surface. We fit a map to this deviation, constructed from spherical harmonics up to order $\\ell_{\\rm max}=2$, alongside the planetary, orbital, stellar, and systematic parameters. This yields a map with a meridionally-averaged eastward hot-spot shift of $(7.75 \\pm 0.36)^{\\circ}$, with no significant degeneracy between the map and the additional parameters. We show the latitudinal and longitudinal contributions of the day-side emission structure to the eclipse shape, finding a latitudinal signal of $\\sim$200 ppm and a longitudinal signal of $\\sim$250 ppm. To investigate the sensitivity of the map to the method, we fix the non-mapping parameters and derive an \"eigenmap\" fitted with an optimised number of orthogonal phase curves, which yields a similar map to the $\\ell_{\\rm max}=2$ map. We also fit a map up to $\\ell_{\\rm max}=3$, which shows a smaller hot-spot shift, with a larger uncertainty. These maps are similar to those produced by atmospheric simulations. We conclude that there is a significant mapping signal which constrains the spherical harmonic components of our model up to $\\ell_{\\rm max}=2$. Alternative mapping models may derive different structures with smaller-scale features; we suggest that further observations of WASP-43b and other planets will drive the development of more robust methods and more accurate maps.","sentences":["We present eclipse maps of the two-dimensional thermal emission from the dayside of the hot Jupiter WASP-43b, derived from an observation of a phase curve with the JWST MIRI/LRS instrument.","The observed eclipse shapes deviate significantly from those expected for a planet emitting uniformly over its surface.","We fit a map to this deviation, constructed from spherical harmonics up to order $\\ell_{\\rm max}=2$, alongside the planetary, orbital, stellar, and systematic parameters.","This yields a map with a meridionally-averaged eastward hot-spot shift of $(7.75 \\pm 0.36)^{\\circ}$, with no significant degeneracy between the map and the additional parameters.","We show the latitudinal and longitudinal contributions of the day-side emission structure to the eclipse shape, finding a latitudinal signal of $\\sim$200 ppm and a longitudinal signal of $\\sim$250 ppm.","To investigate the sensitivity of the map to the method, we fix the non-mapping parameters and derive an \"eigenmap\" fitted with an optimised number of orthogonal phase curves, which yields a similar map to the $\\ell_{\\rm max}=2$ map.","We also fit a map up to $\\ell_{\\rm max}=3$, which shows a smaller hot-spot shift, with a larger uncertainty.","These maps are similar to those produced by atmospheric simulations.","We conclude that there is a significant mapping signal which constrains the spherical harmonic components of our model up to $\\ell_{\\rm max}=2$. Alternative mapping models may derive different structures with smaller-scale features; we suggest that further observations of WASP-43b and other planets will drive the development of more robust methods and more accurate maps."],"url":"http://arxiv.org/abs/2404.16488v1","category":"astro-ph.EP"}
{"created":"2024-04-25 10:12:42","title":"Real-Time 4K Super-Resolution of Compressed AVIF Images. AIS 2024 Challenge Survey","abstract":"This paper introduces a novel benchmark as part of the AIS 2024 Real-Time Image Super-Resolution (RTSR) Challenge, which aims to upscale compressed images from 540p to 4K resolution (4x factor) in real-time on commercial GPUs. For this, we use a diverse test set containing a variety of 4K images ranging from digital art to gaming and photography. The images are compressed using the modern AVIF codec, instead of JPEG. All the proposed methods improve PSNR fidelity over Lanczos interpolation, and process images under 10ms. Out of the 160 participants, 25 teams submitted their code and models. The solutions present novel designs tailored for memory-efficiency and runtime on edge devices. This survey describes the best solutions for real-time SR of compressed high-resolution images.","sentences":["This paper introduces a novel benchmark as part of the AIS 2024 Real-Time Image Super-Resolution (RTSR) Challenge, which aims to upscale compressed images from 540p to 4K resolution (4x factor) in real-time on commercial GPUs.","For this, we use a diverse test set containing a variety of 4K images ranging from digital art to gaming and photography.","The images are compressed using the modern AVIF codec, instead of JPEG.","All the proposed methods improve PSNR fidelity over Lanczos interpolation, and process images under 10ms.","Out of the 160 participants, 25 teams submitted their code and models.","The solutions present novel designs tailored for memory-efficiency and runtime on edge devices.","This survey describes the best solutions for real-time SR of compressed high-resolution images."],"url":"http://arxiv.org/abs/2404.16484v1","category":"cs.CV"}
{"created":"2024-04-25 10:04:36","title":"The Impact of Social Environment and Interaction Focus on User Experience and Social Acceptability of an Augmented Reality Game","abstract":"One of the most promising technologies inside the Extended Reality (XR) spectrum is Augmented Reality. This technology is already in people's pockets regarding Mobile Augmented Reality with their smartphones. The scientific community still needs answers about how humans could and should interact in environments where perceived stimuli are different from fully physical or digital circumstances. Moreover, it is still being determined if people accept these new technologies in different social environments and interaction settings or if some obstacles could exist. This paper explores the impact of the Social Environment and the Focus of social interaction on users while playing a location-based augmented reality game, measuring it with user experience and social acceptance indicators. An empirical study in a within-subject fashion was performed in different social environments and under different settings of social interaction focus with N = 28 participants compiling self-reported questionnaires after playing a Scavenger Hunt in Augmented Reality. The measures from two different Social Environments (Crowded vs. Uncrowded) resulted in statistically relevant mean differences with indicators from the Social Acceptability dimension. Moreover, the analyses show statistically relevant differences between the variances from different degrees of Social Interaction Focus with Overall Social Presence, Perceived Psychological Engagement, Perceived Attentional Engagement, and Perceived Emotional Contagion. The results suggest that a location-based AR game played in different social environments and settings can influence the user experience's social dimension. Therefore, they should be carefully considered while designing immersive technological experiences in public spaces involving social interactions between players.","sentences":["One of the most promising technologies inside the Extended Reality (XR) spectrum is Augmented Reality.","This technology is already in people's pockets regarding Mobile Augmented Reality with their smartphones.","The scientific community still needs answers about how humans could and should interact in environments where perceived stimuli are different from fully physical or digital circumstances.","Moreover, it is still being determined if people accept these new technologies in different social environments and interaction settings or if some obstacles could exist.","This paper explores the impact of the Social Environment and the Focus of social interaction on users while playing a location-based augmented reality game, measuring it with user experience and social acceptance indicators.","An empirical study in a within-subject fashion was performed in different social environments and under different settings of social interaction focus with N = 28 participants compiling self-reported questionnaires after playing a Scavenger Hunt in Augmented Reality.","The measures from two different Social Environments (Crowded vs. Uncrowded) resulted in statistically relevant mean differences with indicators from the Social Acceptability dimension.","Moreover, the analyses show statistically relevant differences between the variances from different degrees of Social Interaction Focus with Overall Social Presence, Perceived Psychological Engagement, Perceived Attentional Engagement, and Perceived Emotional Contagion.","The results suggest that a location-based AR game played in different social environments and settings can influence the user experience's social dimension.","Therefore, they should be carefully considered while designing immersive technological experiences in public spaces involving social interactions between players."],"url":"http://arxiv.org/abs/2404.16479v1","category":"cs.HC"}
{"created":"2024-04-25 09:25:17","title":"Wavefunction collapse driven by non-Hermitian disturbance","abstract":"In the context of the measurement problem, we propose to model the interaction between a quantum particle and an \"apparatus\" through a non-Hermitian Hamiltonian term. We simulate the time evolution of a normalized quantum state split into two spin components (via a Stern-Gerlach experiment) and that undergoes a wave-function collapse driven by a non-Hermitian Hatano-Nelson Hamiltonian. We further analyze how the strength and other parameters of the non-Hermitian perturbation influence the time-to-collapse of the wave function obtained under a Schr\\\"{o}dinger-type evolution. We finally discuss a thought experiment where manipulation of the apparatus could challenge standard quantum mechanics predictions.","sentences":["In the context of the measurement problem, we propose to model the interaction between a quantum particle and an \"apparatus\" through a non-Hermitian Hamiltonian term.","We simulate the time evolution of a normalized quantum state split into two spin components (via a Stern-Gerlach experiment) and that undergoes a wave-function collapse driven by a non-Hermitian Hatano-Nelson Hamiltonian.","We further analyze how the strength and other parameters of the non-Hermitian perturbation influence the time-to-collapse of the wave function obtained under a Schr\\\"{o}dinger-type evolution.","We finally discuss a thought experiment where manipulation of the apparatus could challenge standard quantum mechanics predictions."],"url":"http://arxiv.org/abs/2404.16445v1","category":"quant-ph"}
{"created":"2024-04-25 08:58:30","title":"On algebraic independence of Taylor coefficients of certain Anderson-Thakur series","abstract":"We study algebraic independence problem for the Taylor coefficients of the Anderson-Thakur series arisen as deformation series of positive characteristic multiple zeta values (abbreviated as MZV's). These Taylor coefficients are simply specialization of hyperderivatives of the Anderson-Thakur series. We consider the prolongation of t-motives associated with MZV's, and then determine the dimension of the t-motivic Galois groups in question under certain hypothesis. By using Papanikolas' theory, it enables us to obtain the desired algebraic independence result.","sentences":["We study algebraic independence problem for the Taylor coefficients of the Anderson-Thakur series arisen as deformation series of positive characteristic multiple zeta values (abbreviated as MZV's).","These Taylor coefficients are simply specialization of hyperderivatives of the Anderson-Thakur series.","We consider the prolongation of t-motives associated with MZV's, and then determine the dimension of the t-motivic Galois groups in question under certain hypothesis.","By using Papanikolas' theory, it enables us to obtain the desired algebraic independence result."],"url":"http://arxiv.org/abs/2404.16427v1","category":"math.NT"}
{"created":"2024-04-25 08:52:25","title":"Robust Fine-tuning for Pre-trained 3D Point Cloud Models","abstract":"This paper presents a robust fine-tuning method designed for pre-trained 3D point cloud models, to enhance feature robustness in downstream fine-tuned models. We highlight the limitations of current fine-tuning methods and the challenges of learning robust models. The proposed method, named Weight-Space Ensembles for Fine-Tuning then Linear Probing (WiSE-FT-LP), integrates the original pre-training and fine-tuning models through weight space integration followed by Linear Probing. This approach significantly enhances the performance of downstream fine-tuned models under distribution shifts, improving feature robustness while maintaining high performance on the target distribution. We apply this robust fine-tuning method to mainstream 3D point cloud pre-trained models and evaluate the quality of model parameters and the degradation of downstream task performance. Experimental results demonstrate the effectiveness of WiSE-FT-LP in enhancing model robustness, effectively balancing downstream task performance and model feature robustness without altering the model structures.","sentences":["This paper presents a robust fine-tuning method designed for pre-trained 3D point cloud models, to enhance feature robustness in downstream fine-tuned models.","We highlight the limitations of current fine-tuning methods and the challenges of learning robust models.","The proposed method, named Weight-Space Ensembles for Fine-Tuning then Linear Probing (WiSE-FT-LP), integrates the original pre-training and fine-tuning models through weight space integration followed by Linear Probing.","This approach significantly enhances the performance of downstream fine-tuned models under distribution shifts, improving feature robustness while maintaining high performance on the target distribution.","We apply this robust fine-tuning method to mainstream 3D point cloud pre-trained models and evaluate the quality of model parameters and the degradation of downstream task performance.","Experimental results demonstrate the effectiveness of WiSE-FT-LP in enhancing model robustness, effectively balancing downstream task performance and model feature robustness without altering the model structures."],"url":"http://arxiv.org/abs/2404.16422v1","category":"cs.CV"}
{"created":"2024-04-25 08:49:08","title":"Learning Discriminative Spatio-temporal Representations for Semi-supervised Action Recognition","abstract":"Semi-supervised action recognition aims to improve spatio-temporal reasoning ability with a few labeled data in conjunction with a large amount of unlabeled data. Albeit recent advancements, existing powerful methods are still prone to making ambiguous predictions under scarce labeled data, embodied as the limitation of distinguishing different actions with similar spatio-temporal information. In this paper, we approach this problem by empowering the model two aspects of capability, namely discriminative spatial modeling and temporal structure modeling for learning discriminative spatio-temporal representations. Specifically, we propose an Adaptive Contrastive Learning~(ACL) strategy. It assesses the confidence of all unlabeled samples by the class prototypes of the labeled data, and adaptively selects positive-negative samples from a pseudo-labeled sample bank to construct contrastive learning. Additionally, we introduce a Multi-scale Temporal Learning~(MTL) strategy. It could highlight informative semantics from long-term clips and integrate them into the short-term clip while suppressing noisy information. Afterwards, both of these two new techniques are integrated in a unified framework to encourage the model to make accurate predictions. Extensive experiments on UCF101, HMDB51 and Kinetics400 show the superiority of our method over prior state-of-the-art approaches.","sentences":["Semi-supervised action recognition aims to improve spatio-temporal reasoning ability with a few labeled data in conjunction with a large amount of unlabeled data.","Albeit recent advancements, existing powerful methods are still prone to making ambiguous predictions under scarce labeled data, embodied as the limitation of distinguishing different actions with similar spatio-temporal information.","In this paper, we approach this problem by empowering the model two aspects of capability, namely discriminative spatial modeling and temporal structure modeling for learning discriminative spatio-temporal representations.","Specifically, we propose an Adaptive Contrastive Learning~(ACL) strategy.","It assesses the confidence of all unlabeled samples by the class prototypes of the labeled data, and adaptively selects positive-negative samples from a pseudo-labeled sample bank to construct contrastive learning.","Additionally, we introduce a Multi-scale Temporal Learning~(MTL) strategy.","It could highlight informative semantics from long-term clips and integrate them into the short-term clip while suppressing noisy information.","Afterwards, both of these two new techniques are integrated in a unified framework to encourage the model to make accurate predictions.","Extensive experiments on UCF101, HMDB51 and Kinetics400 show the superiority of our method over prior state-of-the-art approaches."],"url":"http://arxiv.org/abs/2404.16416v1","category":"cs.CV"}
{"created":"2024-04-25 08:29:33","title":"Ground state properties and bubble structure of the isotopic chains of Z = 125 and 126 using the relativistic mean-field formalism","abstract":"The ground state properties of Z = 125 and 126 nuclei are investigated, taking the isotopic series from the proton to neutron drip-lines. This analysis is conducted using the relativistic mean-field approach with NL3 and the Relativistic-Hartree-Bogoliubov model with DD-ME2 parameterization. The bulk properties under examination include the binding energy, the neutron separation energies, the differential variation of the separation energy, the quadrupole deformation parameter $\\beta_2$, and the single-particle energy. We observed the stability at N = 172 and 184 over the isotopic chain for both parameter sets. The quadrupole deformation parameter reveals a shape transition from prolate to spherical and back to prolate with mass number. No signature of a super- and/or hyper-deformed structure is found over the isotopic chain. Furthermore, the analysis is extended to examine the bubble structure, revealing a bubble/semi-bubble structure for a few neutron-rich isotopes.","sentences":["The ground state properties of Z = 125 and 126 nuclei are investigated, taking the isotopic series from the proton to neutron drip-lines.","This analysis is conducted using the relativistic mean-field approach with NL3 and the Relativistic-Hartree-Bogoliubov model with DD-ME2 parameterization.","The bulk properties under examination include the binding energy, the neutron separation energies, the differential variation of the separation energy, the quadrupole deformation parameter $\\beta_2$, and the single-particle energy.","We observed the stability at N = 172 and 184 over the isotopic chain for both parameter sets.","The quadrupole deformation parameter reveals a shape transition from prolate to spherical and back to prolate with mass number.","No signature of a super- and/or hyper-deformed structure is found over the isotopic chain.","Furthermore, the analysis is extended to examine the bubble structure, revealing a bubble/semi-bubble structure for a few neutron-rich isotopes."],"url":"http://arxiv.org/abs/2404.16403v1","category":"nucl-th"}
{"created":"2024-04-25 08:12:42","title":"Phase and morphology of water-ice grains formed in a cryogenic laboratory plasma","abstract":"Grains of ice are formed spontaneously when water vapor is injected into a weakly-ionized laboratory plasma in which the background gas has been cooled to cryogenic temperatures comparable to those of deep space. These ice grains are levitated indefinitely within the plasma so that their time evolution can be observed under free-floating conditions. Using microscope imaging, ice grains are shown to have a spindle-like fractal structure and grow over time. Both crystalline and amorphous phases of ice are observed using Fourier Transform Infrared (FTIR) spectroscopy. A mix of crystalline and amorphous grains coexist under certain thermal conditions and a linear mixing model is used on the ice absorption band surrounding 3.2 microns to examine the ice phase composition and its temporal stability. The extinction spectrum is also affected by inelastic scattering as grains grow, and characteristic grain radii are obtained from Mie scattering theory and compared to size measurements from direct imaging. Observations are used to compare possible ice nucleation mechanisms, and it is concluded that nucleation is likely catalyzed by ions, as ice does not nucleate in absence of plasma and impurities are not detected. Ice grain properties and infrared extinction spectra show similarity to observations of some astrophysical ices observed in protoplanetary disks, implying that the fractal morphology of the ice and observed processes of homogeneous ice nucleation could occur as well in such astrophysical environments with weakly-ionized conditions.","sentences":["Grains of ice are formed spontaneously when water vapor is injected into a weakly-ionized laboratory plasma in which the background gas has been cooled to cryogenic temperatures comparable to those of deep space.","These ice grains are levitated indefinitely within the plasma so that their time evolution can be observed under free-floating conditions.","Using microscope imaging, ice grains are shown to have a spindle-like fractal structure and grow over time.","Both crystalline and amorphous phases of ice are observed using Fourier Transform Infrared (FTIR) spectroscopy.","A mix of crystalline and amorphous grains coexist under certain thermal conditions and a linear mixing model is used on the ice absorption band surrounding 3.2 microns to examine the ice phase composition and its temporal stability.","The extinction spectrum is also affected by inelastic scattering as grains grow, and characteristic grain radii are obtained from Mie scattering theory and compared to size measurements from direct imaging.","Observations are used to compare possible ice nucleation mechanisms, and it is concluded that nucleation is likely catalyzed by ions, as ice does not nucleate in absence of plasma and impurities are not detected.","Ice grain properties and infrared extinction spectra show similarity to observations of some astrophysical ices observed in protoplanetary disks, implying that the fractal morphology of the ice and observed processes of homogeneous ice nucleation could occur as well in such astrophysical environments with weakly-ionized conditions."],"url":"http://arxiv.org/abs/2404.16396v1","category":"physics.plasm-ph"}
{"created":"2024-04-25 07:59:20","title":"Low-mass dark sector searches with deuteron photodisintegration","abstract":"Recent years have seen much activity in searches for dark-sector messenger particles in the 10-100 MeV mass range, especially in view of a potential new light boson conjectured by the ATOMKI collaboration, X17. Under the assumption that the messenger particle has definite parity and either zero or unit spin, quite stringent bounds already exist on its coupling to electrons and protons. Equally stringent bounds on the neutron coupling do not exist yet, but are nonetheless desirable. We explore how measurements of deuteron photodisintegration with a quasi-free neutron can yield bounds on the neutron coupling, and compute projections for a potential measurement at the lowenergy high-intensity electron scattering experiment MAGIX@MESA. The projected bounds are found to be competitive for an axial-vector or pseudoscalar scenario, but not for a vector or scalar scenario.","sentences":["Recent years have seen much activity in searches for dark-sector messenger particles in the 10-100 MeV mass range, especially in view of a potential new light boson conjectured by the ATOMKI collaboration, X17.","Under the assumption that the messenger particle has definite parity and either zero or unit spin, quite stringent bounds already exist on its coupling to electrons and protons.","Equally stringent bounds on the neutron coupling do not exist yet, but are nonetheless desirable.","We explore how measurements of deuteron photodisintegration with a quasi-free neutron can yield bounds on the neutron coupling, and compute projections for a potential measurement at the lowenergy high-intensity electron scattering experiment MAGIX@MESA.","The projected bounds are found to be competitive for an axial-vector or pseudoscalar scenario, but not for a vector or scalar scenario."],"url":"http://arxiv.org/abs/2404.16390v1","category":"hep-ph"}
{"created":"2024-04-25 07:56:12","title":"Revisiting Restarts of CDCL: Should the Search Information be Preserved?","abstract":"SAT solvers are indispensable in formal verification for hardware and software with many important applications. CDCL is the most widely used framework for modern SAT solvers, and restart is an essential technique of CDCL. When restarting, CDCL solvers cancel the current variable assignment while maintaining the branching order, variable phases, and learnt clauses. This type of restart is referred to as warm restart in this paper. Although different restart policies have been studied, there is no study on whether such information should be kept after restarts. This work addresses this question and finds some interesting observations.   This paper indicates that under this popular warm restart scheme, there is a substantial variation in run-time with different randomized initial orders and phases, which motivates us to forget some learned information periodically to prevent being stuck in a disadvantageous search space. We propose a new type of restart called cold restart, which differs from previous restarts by forgetting some of the learned information. Experiments show that modern CDCL solvers can benefit from periodically conducting cold restarts. Based on the analysis of the cold-restart strategies, we develop a parallel SAT solver. Both the sequential and parallel versions of cold restart are more suitable for satisfiable instances, which suggests that existing CDCL heuristics for information management should be revised if one hopes to construct a satisfiable-oriented SAT solver.","sentences":["SAT solvers are indispensable in formal verification for hardware and software with many important applications.","CDCL is the most widely used framework for modern SAT solvers, and restart is an essential technique of CDCL.","When restarting, CDCL solvers cancel the current variable assignment while maintaining the branching order, variable phases, and learnt clauses.","This type of restart is referred to as warm restart in this paper.","Although different restart policies have been studied, there is no study on whether such information should be kept after restarts.","This work addresses this question and finds some interesting observations.   ","This paper indicates that under this popular warm restart scheme, there is a substantial variation in run-time with different randomized initial orders and phases, which motivates us to forget some learned information periodically to prevent being stuck in a disadvantageous search space.","We propose a new type of restart called cold restart, which differs from previous restarts by forgetting some of the learned information.","Experiments show that modern CDCL solvers can benefit from periodically conducting cold restarts.","Based on the analysis of the cold-restart strategies, we develop a parallel SAT solver.","Both the sequential and parallel versions of cold restart are more suitable for satisfiable instances, which suggests that existing CDCL heuristics for information management should be revised if one hopes to construct a satisfiable-oriented SAT solver."],"url":"http://arxiv.org/abs/2404.16387v1","category":"cs.LO"}
{"created":"2024-04-25 06:36:56","title":"Unraveling cell-cell communication with NicheNet by inferring active ligands from transcriptomics data","abstract":"Ligand-receptor interactions constitute a fundamental mechanism of cell-cell communication and signaling. NicheNet is a well-established computational tool that infers ligand-receptor interactions that potentially regulate gene expression changes in receiver cell populations. Whereas the original publication delves into the algorithm and validation, this paper describes a best practices workflow cultivated over four years of experience and user feedback. Starting from the input single-cell expression matrix, we describe a \"sender-agnostic\" approach which considers ligands from the entire microenvironment, and a \"sender-focused\" approach which only considers ligands from cell populations of interest. As output, users will obtain a list of prioritized ligands and their potential target genes, along with multiple visualizations. In NicheNet v2, we have updated the data sources and implemented a downstream procedure for prioritizing cell-type-specific ligand-receptor pairs. Although a standard NicheNet analysis takes less than 10 minutes to run, users often invest additional time in making decisions about the approach and parameters that best suit their biological question. This paper serves to aid in this decision-making process by describing the most appropriate workflow for common experimental designs like case-control and cell differentiation studies. Finally, in addition to the step-by-step description of the code, we also provide wrapper functions that enable the analysis to be run in one line of code, thus tailoring the workflow to users at all levels of computational proficiency.","sentences":["Ligand-receptor interactions constitute a fundamental mechanism of cell-cell communication and signaling.","NicheNet is a well-established computational tool that infers ligand-receptor interactions that potentially regulate gene expression changes in receiver cell populations.","Whereas the original publication delves into the algorithm and validation, this paper describes a best practices workflow cultivated over four years of experience and user feedback.","Starting from the input single-cell expression matrix, we describe a \"sender-agnostic\" approach which considers ligands from the entire microenvironment, and a \"sender-focused\" approach which only considers ligands from cell populations of interest.","As output, users will obtain a list of prioritized ligands and their potential target genes, along with multiple visualizations.","In NicheNet v2, we have updated the data sources and implemented a downstream procedure for prioritizing cell-type-specific ligand-receptor pairs.","Although a standard NicheNet analysis takes less than 10 minutes to run, users often invest additional time in making decisions about the approach and parameters that best suit their biological question.","This paper serves to aid in this decision-making process by describing the most appropriate workflow for common experimental designs like case-control and cell differentiation studies.","Finally, in addition to the step-by-step description of the code, we also provide wrapper functions that enable the analysis to be run in one line of code, thus tailoring the workflow to users at all levels of computational proficiency."],"url":"http://arxiv.org/abs/2404.16358v1","category":"q-bio.CB"}
{"created":"2024-04-25 04:29:25","title":"Distributionally Robust Safe Screening","abstract":"In this study, we propose a method Distributionally Robust Safe Screening (DRSS), for identifying unnecessary samples and features within a DR covariate shift setting. This method effectively combines DR learning, a paradigm aimed at enhancing model robustness against variations in data distribution, with safe screening (SS), a sparse optimization technique designed to identify irrelevant samples and features prior to model training. The core concept of the DRSS method involves reformulating the DR covariate-shift problem as a weighted empirical risk minimization problem, where the weights are subject to uncertainty within a predetermined range. By extending the SS technique to accommodate this weight uncertainty, the DRSS method is capable of reliably identifying unnecessary samples and features under any future distribution within a specified range. We provide a theoretical guarantee of the DRSS method and validate its performance through numerical experiments on both synthetic and real-world datasets.","sentences":["In this study, we propose a method Distributionally Robust Safe Screening (DRSS), for identifying unnecessary samples and features within a DR covariate shift setting.","This method effectively combines DR learning, a paradigm aimed at enhancing model robustness against variations in data distribution, with safe screening (SS), a sparse optimization technique designed to identify irrelevant samples and features prior to model training.","The core concept of the DRSS method involves reformulating the DR covariate-shift problem as a weighted empirical risk minimization problem, where the weights are subject to uncertainty within a predetermined range.","By extending the SS technique to accommodate this weight uncertainty, the DRSS method is capable of reliably identifying unnecessary samples and features under any future distribution within a specified range.","We provide a theoretical guarantee of the DRSS method and validate its performance through numerical experiments on both synthetic and real-world datasets."],"url":"http://arxiv.org/abs/2404.16328v1","category":"stat.ML"}
{"created":"2024-04-25 04:21:56","title":"Improved impedance inversion by deep learning and iterated graph Laplacian","abstract":"Deep learning techniques have shown significant potential in many applications through recent years. The achieved results often outperform traditional techniques. However, the quality of a neural network highly depends on the used training data. Noisy, insufficient, or biased training data leads to suboptimal results.   We present a hybrid method that combines deep learning with iterated graph Laplacian and show its application in acoustic impedance inversion which is a routine procedure in seismic explorations. A neural network is used to obtain a first approximation of the underlying acoustic impedance and construct a graph Laplacian matrix from this approximation. Afterwards, we use a Tikhonov-like variational method to solve the impedance inversion problem where the regularizer is based on the constructed graph Laplacian. The obtained solution can be shown to be more accurate and stable with respect to noise than the initial guess obtained by the neural network. This process can be iterated several times, each time constructing a new graph Laplacian matrix from the most recent reconstruction. The method converges after only a few iterations returning a much more accurate reconstruction.   We demonstrate the potential of our method on two different datasets and under various levels of noise. We use two different neural networks that have been introduced in previous works. The experiments show that our approach improves the reconstruction quality in the presence of noise.","sentences":["Deep learning techniques have shown significant potential in many applications through recent years.","The achieved results often outperform traditional techniques.","However, the quality of a neural network highly depends on the used training data.","Noisy, insufficient, or biased training data leads to suboptimal results.   ","We present a hybrid method that combines deep learning with iterated graph Laplacian and show its application in acoustic impedance inversion which is a routine procedure in seismic explorations.","A neural network is used to obtain a first approximation of the underlying acoustic impedance and construct a graph Laplacian matrix from this approximation.","Afterwards, we use a Tikhonov-like variational method to solve the impedance inversion problem where the regularizer is based on the constructed graph Laplacian.","The obtained solution can be shown to be more accurate and stable with respect to noise than the initial guess obtained by the neural network.","This process can be iterated several times, each time constructing a new graph Laplacian matrix from the most recent reconstruction.","The method converges after only a few iterations returning a much more accurate reconstruction.   ","We demonstrate the potential of our method on two different datasets and under various levels of noise.","We use two different neural networks that have been introduced in previous works.","The experiments show that our approach improves the reconstruction quality in the presence of noise."],"url":"http://arxiv.org/abs/2404.16324v1","category":"math.NA"}
{"created":"2024-04-25 03:59:34","title":"Pattern runs on matter: The free monad monad as a module over the cofree comonad comonad","abstract":"Interviews run on people, programs run on operating systems, voting schemes run on voters, games run on players. Each of these is an example of the abstraction pattern runs on matter. Pattern determines the decision tree that governs how a situation can unfold, while matter responds with decisions at each juncture.   In this article, we will give a straightforward and concrete construction of the free monad monad for $(\\mathbf{Poly}, \\mathbin{\\triangleleft}, \\mathcal{y})$, the category of polynomial functors with the substitution monoidal product. Although the free monad has been well-studied in other contexts, the construction we give is streamlined and explicitly illustrates how the free monad represents terminating decision trees. We will also explore the naturally arising interaction between the free monad and cofree comonad. Again, while the interaction itself is known, the perspective we take is the free monad as a module over the cofree comonad. Lastly, we will give four applications of the module action to interviews, computer programs, voting, and games. In each example, we will see how the free monad represents pattern, the cofree comonad represents matter, and the module action represents runs on.","sentences":["Interviews run on people, programs run on operating systems, voting schemes run on voters, games run on players.","Each of these is an example of the abstraction pattern runs on matter.","Pattern determines the decision tree that governs how a situation can unfold, while matter responds with decisions at each juncture.   ","In this article, we will give a straightforward and concrete construction of the free monad monad for $(\\mathbf{Poly}, \\mathbin{\\triangleleft}, \\mathcal{y})$, the category of polynomial functors with the substitution monoidal product.","Although the free monad has been well-studied in other contexts, the construction we give is streamlined and explicitly illustrates how the free monad represents terminating decision trees.","We will also explore the naturally arising interaction between the free monad and cofree comonad.","Again, while the interaction itself is known, the perspective we take is the free monad as a module over the cofree comonad.","Lastly, we will give four applications of the module action to interviews, computer programs, voting, and games.","In each example, we will see how the free monad represents pattern, the cofree comonad represents matter, and the module action represents runs on."],"url":"http://arxiv.org/abs/2404.16321v1","category":"math.CT"}
{"created":"2024-04-25 03:56:55","title":"Periodic homogenisation for singular PDEs with generalised Besov spaces","abstract":"We consider periodic homogenisation problems for gPAM, $\\Phi^{4}_{3}$ and modified KPZ equations. Using Littlewood-Paley blocks, we establish para-products and commutator estimates for generalised Besov spaces associated with self-adjoint elliptic operators. With this, we show that, the homogenisation and renormalisation procedures commute in aforementioned models under suitable assumptions.","sentences":["We consider periodic homogenisation problems for gPAM, $\\Phi^{4}_{3}$ and modified KPZ equations.","Using Littlewood-Paley blocks, we establish para-products and commutator estimates for generalised Besov spaces associated with self-adjoint elliptic operators.","With this, we show that, the homogenisation and renormalisation procedures commute in aforementioned models under suitable assumptions."],"url":"http://arxiv.org/abs/2404.16320v1","category":"math.AP"}
{"created":"2024-04-25 03:44:45","title":"Parallel and (Nearly) Work-Efficient Dynamic Programming","abstract":"The idea of dynamic programming (DP), proposed by Bellman in the 1950s, is one of the most important algorithmic techniques. However, in parallel, many fundamental and sequentially simple problems become more challenging, and open to a (nearly) work-efficient solution (i.e., the work is off by at most a polylogarithmic factor over the best sequential solution). In fact, sequential DP algorithms employ many advanced optimizations such as decision monotonicity or special data structures, and achieve better work than straightforward solutions. Many such optimizations are inherently sequential, which creates extra challenges for a parallel algorithm to achieve the same work bound.   The goal of this paper is to achieve (nearly) work-efficient parallel DP algorithms by parallelizing classic, highly-optimized and practical sequential algorithms. We show a general framework called the Cordon Algorithm for parallel DP algorithms, and use it to solve several classic problems. Our selection of problems includes Longest Increasing Subsequence (LIS), sparse Longest Common Subsequence (LCS), convex/concave generalized Least Weight Subsequence (LWS), Optimal Alphabetic Tree (OAT), and more. We show how the Cordon Algorithm can be used to achieve the same level of optimization as the sequential algorithms, and achieve good parallelism. Many of our algorithms are conceptually simple, and we show some experimental results as proofs-of-concept.","sentences":["The idea of dynamic programming (DP), proposed by Bellman in the 1950s, is one of the most important algorithmic techniques.","However, in parallel, many fundamental and sequentially simple problems become more challenging, and open to a (nearly) work-efficient solution (i.e., the work is off by at most a polylogarithmic factor over the best sequential solution).","In fact, sequential DP algorithms employ many advanced optimizations such as decision monotonicity or special data structures, and achieve better work than straightforward solutions.","Many such optimizations are inherently sequential, which creates extra challenges for a parallel algorithm to achieve the same work bound.   ","The goal of this paper is to achieve (nearly) work-efficient parallel DP algorithms by parallelizing classic, highly-optimized and practical sequential algorithms.","We show a general framework called the Cordon Algorithm for parallel DP algorithms, and use it to solve several classic problems.","Our selection of problems includes Longest Increasing Subsequence (LIS), sparse Longest Common Subsequence (LCS), convex/concave generalized Least Weight Subsequence (LWS), Optimal Alphabetic Tree (OAT), and more.","We show how the Cordon Algorithm can be used to achieve the same level of optimization as the sequential algorithms, and achieve good parallelism.","Many of our algorithms are conceptually simple, and we show some experimental results as proofs-of-concept."],"url":"http://arxiv.org/abs/2404.16314v1","category":"cs.DS"}
{"created":"2024-04-25 03:25:52","title":"Robot Swarm Control Based on Smoothed Particle Hydrodynamics for Obstacle-Unaware Navigation","abstract":"Robot swarms hold immense potential for performing complex tasks far beyond the capabilities of individual robots. However, the challenge in unleashing this potential is the robots' limited sensory capabilities, which hinder their ability to detect and adapt to unknown obstacles in real-time. To overcome this limitation, we introduce a novel robot swarm control method with an indirect obstacle detector using a smoothed particle hydrodynamics (SPH) model. The indirect obstacle detector can predict the collision with an obstacle and its collision point solely from the robot's velocity information. This approach enables the swarm to effectively and accurately navigate environments without the need for explicit obstacle detection, significantly enhancing their operational robustness and efficiency. Our method's superiority is quantitatively validated through a comparative analysis, showcasing its significant navigation and pattern formation improvements under obstacle-unaware conditions.","sentences":["Robot swarms hold immense potential for performing complex tasks far beyond the capabilities of individual robots.","However, the challenge in unleashing this potential is the robots' limited sensory capabilities, which hinder their ability to detect and adapt to unknown obstacles in real-time.","To overcome this limitation, we introduce a novel robot swarm control method with an indirect obstacle detector using a smoothed particle hydrodynamics (SPH) model.","The indirect obstacle detector can predict the collision with an obstacle and its collision point solely from the robot's velocity information.","This approach enables the swarm to effectively and accurately navigate environments without the need for explicit obstacle detection, significantly enhancing their operational robustness and efficiency.","Our method's superiority is quantitatively validated through a comparative analysis, showcasing its significant navigation and pattern formation improvements under obstacle-unaware conditions."],"url":"http://arxiv.org/abs/2404.16309v1","category":"cs.RO"}
{"created":"2024-04-25 02:54:11","title":"CFMW: Cross-modality Fusion Mamba for Multispectral Object Detection under Adverse Weather Conditions","abstract":"Cross-modality images that integrate visible-infrared spectra cues can provide richer complementary information for object detection. Despite this, existing visible-infrared object detection methods severely degrade in severe weather conditions. This failure stems from the pronounced sensitivity of visible images to environmental perturbations, such as rain, haze, and snow, which frequently cause false negatives and false positives in detection. To address this issue, we introduce a novel and challenging task, termed visible-infrared object detection under adverse weather conditions. To foster this task, we have constructed a new Severe Weather Visible-Infrared Dataset (SWVID) with diverse severe weather scenes. Furthermore, we introduce the Cross-modality Fusion Mamba with Weather-removal (CFMW) to augment detection accuracy in adverse weather conditions. Thanks to the proposed Weather Removal Diffusion Model (WRDM) and Cross-modality Fusion Mamba (CFM) modules, CFMW is able to mine more essential information of pedestrian features in cross-modality fusion, thus could transfer to other rarer scenarios with high efficiency and has adequate availability on those platforms with low computing power. To the best of our knowledge, this is the first study that targeted improvement and integrated both Diffusion and Mamba modules in cross-modality object detection, successfully expanding the practical application of this type of model with its higher accuracy and more advanced architecture. Extensive experiments on both well-recognized and self-created datasets conclusively demonstrate that our CFMW achieves state-of-the-art detection performance, surpassing existing benchmarks. The dataset and source code will be made publicly available at https://github.com/lhy-zjut/CFMW.","sentences":["Cross-modality images that integrate visible-infrared spectra cues can provide richer complementary information for object detection.","Despite this, existing visible-infrared object detection methods severely degrade in severe weather conditions.","This failure stems from the pronounced sensitivity of visible images to environmental perturbations, such as rain, haze, and snow, which frequently cause false negatives and false positives in detection.","To address this issue, we introduce a novel and challenging task, termed visible-infrared object detection under adverse weather conditions.","To foster this task, we have constructed a new Severe Weather Visible-Infrared Dataset (SWVID) with diverse severe weather scenes.","Furthermore, we introduce the Cross-modality Fusion Mamba with Weather-removal (CFMW) to augment detection accuracy in adverse weather conditions.","Thanks to the proposed Weather Removal Diffusion Model (WRDM) and Cross-modality Fusion Mamba (CFM) modules, CFMW is able to mine more essential information of pedestrian features in cross-modality fusion, thus could transfer to other rarer scenarios with high efficiency and has adequate availability on those platforms with low computing power.","To the best of our knowledge, this is the first study that targeted improvement and integrated both Diffusion and Mamba modules in cross-modality object detection, successfully expanding the practical application of this type of model with its higher accuracy and more advanced architecture.","Extensive experiments on both well-recognized and self-created datasets conclusively demonstrate that our CFMW achieves state-of-the-art detection performance, surpassing existing benchmarks.","The dataset and source code will be made publicly available at https://github.com/lhy-zjut/CFMW."],"url":"http://arxiv.org/abs/2404.16302v1","category":"cs.CV"}
{"created":"2024-04-25 02:51:55","title":"Style Adaptation for Domain-adaptive Semantic Segmentation","abstract":"Unsupervised Domain Adaptation (UDA) refers to the method that utilizes annotated source domain data and unlabeled target domain data to train a model capable of generalizing to the target domain data. Domain discrepancy leads to a significant decrease in the performance of general network models trained on the source domain data when applied to the target domain. We introduce a straightforward approach to mitigate the domain discrepancy, which necessitates no additional parameter calculations and seamlessly integrates with self-training-based UDA methods. Through the transfer of the target domain style to the source domain in the latent feature space, the model is trained to prioritize the target domain style during the decision-making process. We tackle the problem at both the image-level and shallow feature map level by transferring the style information from the target domain to the source domain data. As a result, we obtain a model that exhibits superior performance on the target domain. Our method yields remarkable enhancements in the state-of-the-art performance for synthetic-to-real UDA tasks. For example, our proposed method attains a noteworthy UDA performance of 76.93 mIoU on the GTA->Cityscapes dataset, representing a notable improvement of +1.03 percentage points over the previous state-of-the-art results.","sentences":["Unsupervised Domain Adaptation (UDA) refers to the method that utilizes annotated source domain data and unlabeled target domain data to train a model capable of generalizing to the target domain data.","Domain discrepancy leads to a significant decrease in the performance of general network models trained on the source domain data when applied to the target domain.","We introduce a straightforward approach to mitigate the domain discrepancy, which necessitates no additional parameter calculations and seamlessly integrates with self-training-based UDA methods.","Through the transfer of the target domain style to the source domain in the latent feature space, the model is trained to prioritize the target domain style during the decision-making process.","We tackle the problem at both the image-level and shallow feature map level by transferring the style information from the target domain to the source domain data.","As a result, we obtain a model that exhibits superior performance on the target domain.","Our method yields remarkable enhancements in the state-of-the-art performance for synthetic-to-real UDA tasks.","For example, our proposed method attains a noteworthy UDA performance of 76.93 mIoU on the GTA->Cityscapes dataset, representing a notable improvement of +1.03 percentage points over the previous state-of-the-art results."],"url":"http://arxiv.org/abs/2404.16301v1","category":"cs.CV"}
{"created":"2024-04-25 02:14:07","title":"Differentially Private Federated Learning: Servers Trustworthiness, Estimation, and Statistical Inference","abstract":"Differentially private federated learning is crucial for maintaining privacy in distributed environments. This paper investigates the challenges of high-dimensional estimation and inference under the constraints of differential privacy. First, we study scenarios involving an untrusted central server, demonstrating the inherent difficulties of accurate estimation in high-dimensional problems. Our findings indicate that the tight minimax rates depends on the high-dimensionality of the data even with sparsity assumptions. Second, we consider a scenario with a trusted central server and introduce a novel federated estimation algorithm tailored for linear regression models. This algorithm effectively handles the slight variations among models distributed across different machines. We also propose methods for statistical inference, including coordinate-wise confidence intervals for individual parameters and strategies for simultaneous inference. Extensive simulation experiments support our theoretical advances, underscoring the efficacy and reliability of our approaches.","sentences":["Differentially private federated learning is crucial for maintaining privacy in distributed environments.","This paper investigates the challenges of high-dimensional estimation and inference under the constraints of differential privacy.","First, we study scenarios involving an untrusted central server, demonstrating the inherent difficulties of accurate estimation in high-dimensional problems.","Our findings indicate that the tight minimax rates depends on the high-dimensionality of the data even with sparsity assumptions.","Second, we consider a scenario with a trusted central server and introduce a novel federated estimation algorithm tailored for linear regression models.","This algorithm effectively handles the slight variations among models distributed across different machines.","We also propose methods for statistical inference, including coordinate-wise confidence intervals for individual parameters and strategies for simultaneous inference.","Extensive simulation experiments support our theoretical advances, underscoring the efficacy and reliability of our approaches."],"url":"http://arxiv.org/abs/2404.16287v1","category":"stat.ML"}
{"created":"2024-04-25 02:13:31","title":"Willmore-type inequalities for closed hypersurfaces in weighted manifolds","abstract":"In this paper, we prove some Willmore-type inequalities for closed hypersurfaces in weighted manifolds with nonnegative Bakry-\\'Emery Ricci curvature. In particular, we give a sharp Willmore-like inequality in shrinking gradient Ricci solitons. These results can be regarded as generalizations of Agostiniani-Fogagnolo-Mazzieri's Willmore-type inequality in weighted manifolds. As applications, we derive some isoperimetric type inequalities under certain existence assumptions of isoperimetric regions in weighted manifolds.","sentences":["In this paper, we prove some Willmore-type inequalities for closed hypersurfaces in weighted manifolds with nonnegative Bakry-\\'Emery Ricci curvature.","In particular, we give a sharp Willmore-like inequality in shrinking gradient Ricci solitons.","These results can be regarded as generalizations of Agostiniani-Fogagnolo-Mazzieri's Willmore-type inequality in weighted manifolds.","As applications, we derive some isoperimetric type inequalities under certain existence assumptions of isoperimetric regions in weighted manifolds."],"url":"http://arxiv.org/abs/2404.16286v1","category":"math.DG"}
{"created":"2024-04-25 01:56:00","title":"Andes: Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services","abstract":"The advent of large language models (LLMs) has transformed text-based services, enabling capabilities ranging from real-time translation to AI-driven chatbots. However, existing serving systems primarily focus on optimizing server-side aggregate metrics like token generation throughput, ignoring individual user experience with streamed text. As a result, under high and/or bursty load, a significant number of users can receive unfavorable service quality or poor Quality-of-Experience (QoE). In this paper, we first formally define QoE of text streaming services, where text is delivered incrementally and interactively to users, by considering the end-to-end token delivery process throughout the entire interaction with the user. Thereafter, we propose Andes, a QoE-aware serving system that enhances user experience for LLM-enabled text streaming services. At its core, Andes strategically allocates contended GPU resources among multiple requests over time to optimize their QoE. Our evaluations demonstrate that, compared to the state-of-the-art LLM serving systems like vLLM, Andes improves the average QoE by up to 3.2$\\times$ under high request rate, or alternatively, it attains up to 1.6$\\times$ higher request rate while preserving high QoE.","sentences":["The advent of large language models (LLMs) has transformed text-based services, enabling capabilities ranging from real-time translation to AI-driven chatbots.","However, existing serving systems primarily focus on optimizing server-side aggregate metrics like token generation throughput, ignoring individual user experience with streamed text.","As a result, under high and/or bursty load, a significant number of users can receive unfavorable service quality or poor Quality-of-Experience (QoE).","In this paper, we first formally define QoE of text streaming services, where text is delivered incrementally and interactively to users, by considering the end-to-end token delivery process throughout the entire interaction with the user.","Thereafter, we propose Andes, a QoE-aware serving system that enhances user experience for LLM-enabled text streaming services.","At its core, Andes strategically allocates contended GPU resources among multiple requests over time to optimize their QoE. Our evaluations demonstrate that, compared to the state-of-the-art LLM serving systems like vLLM, Andes improves the average QoE by up to 3.2$\\times$ under high request rate, or alternatively, it attains up to 1.6$\\times$ higher request rate while preserving high QoE."],"url":"http://arxiv.org/abs/2404.16283v1","category":"cs.DC"}
{"created":"2024-04-25 01:54:30","title":"Adaptive tracking control for non-periodic reference signals under quantized observations","abstract":"This paper considers an adaptive tracking control problem for stochastic regression systems with multi-threshold quantized observations. Different from the existing studies for periodic reference signals, the reference signal in this paper is non-periodic. Its main difficulty is how to ensure that the designed controller satisfies the uniformly bounded and excitation conditions that guarantee the convergence of the estimation in the controller under non-periodic signal conditions. This paper designs two backward-shifted polynomials with time-varying parameters and a special projection structure, which break through periodic limitations and establish the convergence and tracking properties. To be specific, the adaptive tracking control law can achieve asymptotically optimal tracking for the non-periodic reference signal; Besides, the proposed estimation algorithm is proved to converge to the true values in almost sure and mean square sense, and the convergence speed can reach $O\\left(\\frac{1}{k}\\right)$ under suitable conditions. Finally, the effectiveness of the proposed adaptive tracking control scheme is verified through a simulation.","sentences":["This paper considers an adaptive tracking control problem for stochastic regression systems with multi-threshold quantized observations.","Different from the existing studies for periodic reference signals, the reference signal in this paper is non-periodic.","Its main difficulty is how to ensure that the designed controller satisfies the uniformly bounded and excitation conditions that guarantee the convergence of the estimation in the controller under non-periodic signal conditions.","This paper designs two backward-shifted polynomials with time-varying parameters and a special projection structure, which break through periodic limitations and establish the convergence and tracking properties.","To be specific, the adaptive tracking control law can achieve asymptotically optimal tracking for the non-periodic reference signal; Besides, the proposed estimation algorithm is proved to converge to the true values in almost sure and mean square sense, and the convergence speed can reach $O\\left(\\frac{1}{k}\\right)$ under suitable conditions.","Finally, the effectiveness of the proposed adaptive tracking control scheme is verified through a simulation."],"url":"http://arxiv.org/abs/2404.16282v1","category":"eess.SY"}
{"created":"2024-04-25 01:47:19","title":"Quantum Imaginarity-Mixedness Trade-off: Characterizing Maximally Imaginary Mixed States","abstract":"We investigate the trade-off relations between imaginarity and mixedness in arbitrary $d$-dimensional quantum systems. For given mixedness, a quantum state with maximum imaginarity is defined to be a \"maximally imaginary mixed state\" (MIMS). By using the $l_{1}$ norm of imaginarity and the normalized linear entropy, we conclusively identify the MIMSs for both qubit and qutrit systems. For high-dimensional quantum systems, we present a comprehensive class of MIMSs, which also gives rise to complementarity relations between the $1$-norm of imaginarity and the $1$-norm of mixedness, as well as between the relative entropy of imaginarity and the von Neumann entropy. Furthermore, we examine the evolution of the trade-off relation for single-qubit states under four specific Markovian channels: bit flip channel, phase damping channel, depolarizing channel and amplitude damping channel.","sentences":["We investigate the trade-off relations between imaginarity and mixedness in arbitrary $d$-dimensional quantum systems.","For given mixedness, a quantum state with maximum imaginarity is defined to be a \"maximally imaginary mixed state\" (MIMS).","By using the $l_{1}$ norm of imaginarity and the normalized linear entropy, we conclusively identify the MIMSs for both qubit and qutrit systems.","For high-dimensional quantum systems, we present a comprehensive class of MIMSs, which also gives rise to complementarity relations between the $1$-norm of imaginarity and the $1$-norm of mixedness, as well as between the relative entropy of imaginarity and the von Neumann entropy.","Furthermore, we examine the evolution of the trade-off relation for single-qubit states under four specific Markovian channels: bit flip channel, phase damping channel, depolarizing channel and amplitude damping channel."],"url":"http://arxiv.org/abs/2404.16279v1","category":"quant-ph"}
{"created":"2024-04-25 01:37:36","title":"Tuning the electronic and magnetic properties of NiBr$_2$ via pressure","abstract":"Transition metal dihalides (MX$_2$, M= transition metal, X= halide) have attracted much attention recently due to their intriguing low-dimensional magnetic properties. Particular focus has been placed in this family in the context of multiferroicity -- a common occurrence in MX$_2$ compounds that adopt non-collinear magnetic structures. One example of helimagnetic multiferroic material in the dihalide family is represented by NiBr$_2$. Here, we study the evolution of the electronic structure and magnetic properties of this material under pressure using first-principles calculations combined with Monte Carlo simulations. Our results indicate there is significant magnetic frustration in NiBr$_2$ due to the competing interactions arising from its underlying triangular lattice. This magnetic frustration increases with pressure and is at the origin of the helimagnetic order. Further, pressure causes a sizable increase in the interlayer interactions. Our Monte Carlo simulations show that a large (3-fold) increase in the helimagnetic transition temperature can be achieved at pressures of around 15 GPa. This indicates that hydrostatic pressure can indeed be used as a tuning knob to increase the magnetic transition temperature of NiBr$_2$.","sentences":["Transition metal dihalides (MX$_2$, M= transition metal, X= halide) have attracted much attention recently due to their intriguing low-dimensional magnetic properties.","Particular focus has been placed in this family in the context of multiferroicity -- a common occurrence in MX$_2$ compounds that adopt non-collinear magnetic structures.","One example of helimagnetic multiferroic material in the dihalide family is represented by NiBr$_2$. Here, we study the evolution of the electronic structure and magnetic properties of this material under pressure using first-principles calculations combined with Monte Carlo simulations.","Our results indicate there is significant magnetic frustration in NiBr$_2$ due to the competing interactions arising from its underlying triangular lattice.","This magnetic frustration increases with pressure and is at the origin of the helimagnetic order.","Further, pressure causes a sizable increase in the interlayer interactions.","Our Monte Carlo simulations show that a large (3-fold) increase in the helimagnetic transition temperature can be achieved at pressures of around 15 GPa.","This indicates that hydrostatic pressure can indeed be used as a tuning knob to increase the magnetic transition temperature of NiBr$_2$."],"url":"http://arxiv.org/abs/2404.16278v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-25 00:36:32","title":"Expected Time-Optimal Control: a Particle MPC-based Approach via Sequential Convex Programming","abstract":"In this paper, we consider the problem of minimum-time optimal control for a dynamical system with initial state uncertainties and propose a sequential convex programming (SCP) solution framework. We seek to minimize the expected terminal (mission) time, which is an essential capability for planetary exploration missions where ground rovers have to carry out scientific tasks efficiently within the mission timelines in uncertain environments. Our main contribution is to convert the underlying stochastic optimal control problem into a deterministic, numerically tractable, optimal control problem. To this end, the proposed solution framework combines two strategies from previous methods: i) a partial model predictive control with consensus horizon approach and ii) a sum-of-norm cost, a temporally strictly increasing weighted-norm, promoting minimum-time trajectories. Our contribution is to adopt these formulations into an SCP solution framework and obtain a numerically tractable stochastic control algorithm. We then demonstrate the resulting control method in multiple applications: i) a closed-loop linear system as a representative result (a spacecraft double integrator model), ii) an open-loop linear system (the same model), and then iii) a nonlinear system (Dubin's car).","sentences":["In this paper, we consider the problem of minimum-time optimal control for a dynamical system with initial state uncertainties and propose a sequential convex programming (SCP) solution framework.","We seek to minimize the expected terminal (mission) time, which is an essential capability for planetary exploration missions where ground rovers have to carry out scientific tasks efficiently within the mission timelines in uncertain environments.","Our main contribution is to convert the underlying stochastic optimal control problem into a deterministic, numerically tractable, optimal control problem.","To this end, the proposed solution framework combines two strategies from previous methods: i) a partial model predictive control with consensus horizon approach and ii) a sum-of-norm cost, a temporally strictly increasing weighted-norm, promoting minimum-time trajectories.","Our contribution is to adopt these formulations into an SCP solution framework and obtain a numerically tractable stochastic control algorithm.","We then demonstrate the resulting control method in multiple applications: i) a closed-loop linear system as a representative result (a spacecraft double integrator model), ii) an open-loop linear system (the same model), and then iii) a nonlinear system (Dubin's car)."],"url":"http://arxiv.org/abs/2404.16269v1","category":"math.OC"}
{"created":"2024-04-25 00:30:28","title":"Dynamic PageRank: Algorithms and Lower Bounds","abstract":"We consider the PageRank problem in the dynamic setting, where the goal is to explicitly maintain an approximate PageRank vector $\\pi \\in \\mathbb{R}^n$ for a graph under a sequence of edge insertions and deletions. Our main result is a complete characterization of the complexity of dynamic PageRank maintenance for both multiplicative and additive ($L_1$) approximations.   First, we establish matching lower and upper bounds for maintaining additive approximate PageRank in both incremental and decremental settings. In particular, we demonstrate that in the worst-case $(1/\\alpha)^{\\Theta(\\log \\log n)}$ update time is necessary and sufficient for this problem, where $\\alpha$ is the desired additive approximation. On the other hand, we demonstrate that the commonly employed ForwardPush approach performs substantially worse than this optimal runtime. Specifically, we show that ForwardPush requires $\\Omega(n^{1-\\delta})$ time per update on average, for any $\\delta > 0$, even in the incremental setting.   For multiplicative approximations, however, we demonstrate that the situation is significantly more challenging. Specifically, we prove that any algorithm that explicitly maintains a constant factor multiplicative approximation of the PageRank vector of a directed graph must have amortized update time $\\Omega(n^{1-\\delta})$, for any $\\delta > 0$, even in the incremental setting, thereby resolving a 13-year old open question of Bahmani et al.~(VLDB 2010). This sharply contrasts with the undirected setting, where we show that $\\rm{poly}\\ \\log n$ update time is feasible, even in the fully dynamic setting under oblivious adversary.","sentences":["We consider the PageRank problem in the dynamic setting, where the goal is to explicitly maintain an approximate PageRank vector $\\pi \\in \\mathbb{R}^n$ for a graph under a sequence of edge insertions and deletions.","Our main result is a complete characterization of the complexity of dynamic PageRank maintenance for both multiplicative and additive ($L_1$) approximations.   ","First, we establish matching lower and upper bounds for maintaining additive approximate PageRank in both incremental and decremental settings.","In particular, we demonstrate that in the worst-case $(1/\\alpha)^{\\Theta(\\log \\log n)}$ update time is necessary and sufficient for this problem, where $\\alpha$ is the desired additive approximation.","On the other hand, we demonstrate that the commonly employed ForwardPush approach performs substantially worse than this optimal runtime.","Specifically, we show that ForwardPush requires $\\Omega(n^{1-\\delta})$ time per update on average, for any $\\delta > 0$, even in the incremental setting.   ","For multiplicative approximations, however, we demonstrate that the situation is significantly more challenging.","Specifically, we prove that any algorithm that explicitly maintains a constant factor multiplicative approximation of the PageRank vector of a directed graph must have amortized update time $\\Omega(n^{1-\\delta})$, for any $\\delta > 0$, even in the incremental setting, thereby resolving a 13-year old open question of Bahmani et al.~(VLDB 2010).","This sharply contrasts with the undirected setting, where we show that $\\rm{poly}\\ \\log n$ update time is feasible, even in the fully dynamic setting under oblivious adversary."],"url":"http://arxiv.org/abs/2404.16267v1","category":"cs.DS"}
{"created":"2024-04-25 00:29:32","title":"Deci-Hz gravitational waves from the self-interacting axion cloud around the rotating stellar mass black hole","abstract":"Gravitational waves from condensates of ultra-light particles, such as axion, around rotating black holes are a promising probe to search for unknown physics. For this purpose, we need to characterize the signal to detect the gravitational waves, which requires tracking the evolution of the condensates, including various effects. The axion self-interaction causes the non-linear coupling between the superradiant modes, resulting in complicated branching of evolution. Most studies so far have considered evolution under the non-relativistic approximation or the two-mode approximation. In this paper, we numerically investigate the evolution of the axion condensate without these approximations, taking higher multipole modes into account. We also investigate the possible signature in gravitational waves from the condensate. We show that the higher multipole modes are excited, leading to the gravitational wave signal by the transition of the axion between different levels. The most prominent signal of gravitational waves arises from the transition between modes with their angular quantum numbers different by two. The gravitational wave signal is emitted in the deci-Hz band for stellar mass black holes, which might be observable with the future gravitational wave detectors.","sentences":["Gravitational waves from condensates of ultra-light particles, such as axion, around rotating black holes are a promising probe to search for unknown physics.","For this purpose, we need to characterize the signal to detect the gravitational waves, which requires tracking the evolution of the condensates, including various effects.","The axion self-interaction causes the non-linear coupling between the superradiant modes, resulting in complicated branching of evolution.","Most studies so far have considered evolution under the non-relativistic approximation or the two-mode approximation.","In this paper, we numerically investigate the evolution of the axion condensate without these approximations, taking higher multipole modes into account.","We also investigate the possible signature in gravitational waves from the condensate.","We show that the higher multipole modes are excited, leading to the gravitational wave signal by the transition of the axion between different levels.","The most prominent signal of gravitational waves arises from the transition between modes with their angular quantum numbers different by two.","The gravitational wave signal is emitted in the deci-Hz band for stellar mass black holes, which might be observable with the future gravitational wave detectors."],"url":"http://arxiv.org/abs/2404.16265v1","category":"gr-qc"}
{"created":"2024-04-24 23:46:37","title":"Stabilization of hyperbolic reaction-diffusion systems on directed networks through the generalized Routh-Hurwitz criterion for complex polynomials","abstract":"The study of dynamical systems on complex networks is of paramount importance in engineering, given that many natural and artificial systems find a natural embedding on discrete topologies. For instance, power grids, chemical reactors and the brain, to name a few, can be modeled through the network formalism by considering elementary units coupled via the links. In recent years, scholars have developed numerical and theoretical tools to study the stability of those coupled systems when subjected to perturbations. In such framework, it was found that asymmetric couplings enhance the possibilities for such systems to become unstable. Moreover, in this scenario the polynomials whose stability needs to be studied bear complex coefficients, which makes the analysis more difficult. In this work, we put to use a recent extension of the well-known Routh-Hurwitz stability criterion, allowing to treat the complex coefficient case. Then, using the Brusselator model as a case study, we discuss the stability conditions and the regions of parameters when the networked system remains stable.","sentences":["The study of dynamical systems on complex networks is of paramount importance in engineering, given that many natural and artificial systems find a natural embedding on discrete topologies.","For instance, power grids, chemical reactors and the brain, to name a few, can be modeled through the network formalism by considering elementary units coupled via the links.","In recent years, scholars have developed numerical and theoretical tools to study the stability of those coupled systems when subjected to perturbations.","In such framework, it was found that asymmetric couplings enhance the possibilities for such systems to become unstable.","Moreover, in this scenario the polynomials whose stability needs to be studied bear complex coefficients, which makes the analysis more difficult.","In this work, we put to use a recent extension of the well-known Routh-Hurwitz stability criterion, allowing to treat the complex coefficient case.","Then, using the Brusselator model as a case study, we discuss the stability conditions and the regions of parameters when the networked system remains stable."],"url":"http://arxiv.org/abs/2404.16252v1","category":"math.OC"}
{"created":"2024-04-24 22:58:42","title":"Synergizing Privacy and Utility in Data Analytics Through Advanced Information Theorization","abstract":"This study develops a novel framework for privacy-preserving data analytics, addressing the critical challenge of balancing data utility with privacy concerns. We introduce three sophisticated algorithms: a Noise-Infusion Technique tailored for high-dimensional image data, a Variational Autoencoder (VAE) for robust feature extraction while masking sensitive attributes and an Expectation Maximization (EM) approach optimized for structured data privacy. Applied to datasets such as Modified MNIST and CelebrityA, our methods significantly reduce mutual information between sensitive attributes and transformed data, thereby enhancing privacy. Our experimental results confirm that these approaches achieve superior privacy protection and retain high utility, making them viable for practical applications where both aspects are crucial. The research contributes to the field by providing a flexible and effective strategy for deploying privacy-preserving algorithms across various data types and establishing new benchmarks for utility and confidentiality in data analytics.","sentences":["This study develops a novel framework for privacy-preserving data analytics, addressing the critical challenge of balancing data utility with privacy concerns.","We introduce three sophisticated algorithms: a Noise-Infusion Technique tailored for high-dimensional image data, a Variational Autoencoder (VAE) for robust feature extraction while masking sensitive attributes and an Expectation Maximization (EM) approach optimized for structured data privacy.","Applied to datasets such as Modified MNIST and CelebrityA, our methods significantly reduce mutual information between sensitive attributes and transformed data, thereby enhancing privacy.","Our experimental results confirm that these approaches achieve superior privacy protection and retain high utility, making them viable for practical applications where both aspects are crucial.","The research contributes to the field by providing a flexible and effective strategy for deploying privacy-preserving algorithms across various data types and establishing new benchmarks for utility and confidentiality in data analytics."],"url":"http://arxiv.org/abs/2404.16241v1","category":"cs.CR"}
{"created":"2024-04-24 22:45:27","title":"K-core attack, equilibrium K-core, and kinetically constrained spin system","abstract":"Kinetically constrained spin systems are toy models of supercooled liquids and amorphous solids. In this Perspective, we revisit the prototypical Fredrickson-Andersen (FA) kinetically constrained model from the viewpoint of K-core combinatorial optimization. Each kinetic cluster of the FA system, containing all the mutually visitable microscopic occupation configurations, is exactly the solution space of a specific instance of the K-core attack problem. The whole set of different jammed occupation patterns of the FA system is the configuration space of an equilibrium K-core problem. Based on recent theoretical results achieved on the K-core attack and equilibrium K-core problems, we discuss the thermodynamic spin glass phase transitions and the maximum occupation density of the fully unfrozen FA kinetic cluster, and the minimum occupation density and extreme vulnerability of the partially frozen (jammed) kinetic clusters. The equivalence between K-core attack and the fully unfrozen FA kinetic cluster also implies a new way of sampling K-core attack solutions.","sentences":["Kinetically constrained spin systems are toy models of supercooled liquids and amorphous solids.","In this Perspective, we revisit the prototypical Fredrickson-Andersen (FA) kinetically constrained model from the viewpoint of K-core combinatorial optimization.","Each kinetic cluster of the FA system, containing all the mutually visitable microscopic occupation configurations, is exactly the solution space of a specific instance of the K-core attack problem.","The whole set of different jammed occupation patterns of the FA system is the configuration space of an equilibrium K-core problem.","Based on recent theoretical results achieved on the K-core attack and equilibrium K-core problems, we discuss the thermodynamic spin glass phase transitions and the maximum occupation density of the fully unfrozen FA kinetic cluster, and the minimum occupation density and extreme vulnerability of the partially frozen (jammed) kinetic clusters.","The equivalence between K-core attack and the fully unfrozen FA kinetic cluster also implies a new way of sampling K-core attack solutions."],"url":"http://arxiv.org/abs/2404.16237v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-24 22:24:52","title":"SECO: Secure Inference With Model Splitting Across Multi-Server Hierarchy","abstract":"In the context of prediction-as-a-service, concerns about the privacy of the data and the model have been brought up and tackled via secure inference protocols. These protocols are built up by using single or multiple cryptographic tools designed under a variety of different security assumptions.   In this paper, we introduce SECO, a secure inference protocol that enables a user holding an input data vector and multiple server nodes deployed with a split neural network model to collaboratively compute the prediction, without compromising either party's data privacy. We extend prior work on secure inference that requires the entire neural network model to be located on a single server node, to a multi-server hierarchy, where the user communicates to a gateway server node, which in turn communicates to remote server nodes. The inference task is split across the server nodes and must be performed over an encrypted copy of the data vector.   We adopt multiparty homomorphic encryption and multiparty garbled circuit schemes, making the system secure against dishonest majority of semi-honest servers as well as protecting the partial model structure from the user. We evaluate SECO on multiple models, achieving the reduction of computation and communication cost for the user, making the protocol applicable to user's devices with limited resources.","sentences":["In the context of prediction-as-a-service, concerns about the privacy of the data and the model have been brought up and tackled via secure inference protocols.","These protocols are built up by using single or multiple cryptographic tools designed under a variety of different security assumptions.   ","In this paper, we introduce SECO, a secure inference protocol that enables a user holding an input data vector and multiple server nodes deployed with a split neural network model to collaboratively compute the prediction, without compromising either party's data privacy.","We extend prior work on secure inference that requires the entire neural network model to be located on a single server node, to a multi-server hierarchy, where the user communicates to a gateway server node, which in turn communicates to remote server nodes.","The inference task is split across the server nodes and must be performed over an encrypted copy of the data vector.   ","We adopt multiparty homomorphic encryption and multiparty garbled circuit schemes, making the system secure against dishonest majority of semi-honest servers as well as protecting the partial model structure from the user.","We evaluate SECO on multiple models, achieving the reduction of computation and communication cost for the user, making the protocol applicable to user's devices with limited resources."],"url":"http://arxiv.org/abs/2404.16232v1","category":"cs.CR"}
{"created":"2024-04-24 22:02:42","title":"Which statistical hypotheses are afflicted with false confidence?","abstract":"The false confidence theorem establishes that, for any data-driven, precise-probabilistic method for uncertainty quantification, there exists (non-trivial) false hypotheses to which the method tends to assign high confidence. This raises concerns about the reliability of these widely-used methods, and shines new light on the consonant belief function-based methods that are provably immune to false confidence. But an existence result alone is insufficient. Towards a partial answer to the title question, I show that, roughly, complements of convex hypotheses are afflicted by false confidence.","sentences":["The false confidence theorem establishes that, for any data-driven, precise-probabilistic method for uncertainty quantification, there exists (non-trivial) false hypotheses to which the method tends to assign high confidence.","This raises concerns about the reliability of these widely-used methods, and shines new light on the consonant belief function-based methods that are provably immune to false confidence.","But an existence result alone is insufficient.","Towards a partial answer to the title question, I show that, roughly, complements of convex hypotheses are afflicted by false confidence."],"url":"http://arxiv.org/abs/2404.16228v1","category":"math.ST"}
{"created":"2024-04-24 21:56:06","title":"Tractable Conjunctive Queries over Static and Dynamic Relations","abstract":"We investigate the evaluation of conjunctive queries over static and dynamic relations. While static relations are given as input and do not change, dynamic relations are subject to inserts and deletes.   We characterise syntactically three classes of queries that admit constant update time and constant enumeration delay. We call such queries tractable. Depending on the class, the preprocessing time is linear, polynomial, or exponential (under data complexity, so the query size is constant).   To decide whether a query is tractable, it does not suffice to analyse separately the sub-query over the static relations and the sub-query over the dynamic relations. Instead, we need to take the interaction between the static and the dynamic relations into account. Even when the sub-query over the dynamic relations is not tractable, the overall query can become tractable if the dynamic relations are sufficiently constrained by the static ones.","sentences":["We investigate the evaluation of conjunctive queries over static and dynamic relations.","While static relations are given as input and do not change, dynamic relations are subject to inserts and deletes.   ","We characterise syntactically three classes of queries that admit constant update time and constant enumeration delay.","We call such queries tractable.","Depending on the class, the preprocessing time is linear, polynomial, or exponential (under data complexity, so the query size is constant).   ","To decide whether a query is tractable, it does not suffice to analyse separately the sub-query over the static relations and the sub-query over the dynamic relations.","Instead, we need to take the interaction between the static and the dynamic relations into account.","Even when the sub-query over the dynamic relations is not tractable, the overall query can become tractable if the dynamic relations are sufficiently constrained by the static ones."],"url":"http://arxiv.org/abs/2404.16224v1","category":"cs.DB"}
{"created":"2024-04-24 21:35:12","title":"Can Increasing the Hit Ratio Hurt Cache Throughput?","abstract":"Software caches are an intrinsic component of almost every computer system. Consequently, caching algorithms, particularly eviction policies, are the topic of many papers. Almost all these prior papers evaluate the caching algorithm based on its hit ratio, namely the fraction of requests that are found in the cache, as opposed to disk. The hit ratio is viewed as a proxy for traditional performance metrics like system throughput or response time. Intuitively it makes sense that higher hit ratio should lead to higher throughput (and lower response time), since more requests are found in the cache (low access time) as opposed to the disk (high access time).   This paper challenges this intuition. We show that increasing the hit ratio can actually hurt the throughput (and response time) for many caching algorithms. Our investigation follows a three-pronged approach involving (i) queueing modeling and analysis, (ii) implementation and measurement, and (iii) simulation to validate the accuracy of the queueing model. We also show that the phenomenon of throughput decreasing at higher hit ratios is likely to be more pronounced in future systems, where the trend is towards faster disks and higher numbers of cores per CPU.","sentences":["Software caches are an intrinsic component of almost every computer system.","Consequently, caching algorithms, particularly eviction policies, are the topic of many papers.","Almost all these prior papers evaluate the caching algorithm based on its hit ratio, namely the fraction of requests that are found in the cache, as opposed to disk.","The hit ratio is viewed as a proxy for traditional performance metrics like system throughput or response time.","Intuitively it makes sense that higher hit ratio should lead to higher throughput (and lower response time), since more requests are found in the cache (low access time) as opposed to the disk (high access time).   ","This paper challenges this intuition.","We show that increasing the hit ratio can actually hurt the throughput (and response time) for many caching algorithms.","Our investigation follows a three-pronged approach involving (i) queueing modeling and analysis, (ii) implementation and measurement, and (iii) simulation to validate the accuracy of the queueing model.","We also show that the phenomenon of throughput decreasing at higher hit ratios is likely to be more pronounced in future systems, where the trend is towards faster disks and higher numbers of cores per CPU."],"url":"http://arxiv.org/abs/2404.16219v1","category":"cs.PF"}
{"created":"2024-04-24 21:02:14","title":"AIS 2024 Challenge on Video Quality Assessment of User-Generated Content: Methods and Results","abstract":"This paper reviews the AIS 2024 Video Quality Assessment (VQA) Challenge, focused on User-Generated Content (UGC). The aim of this challenge is to gather deep learning-based methods capable of estimating the perceptual quality of UGC videos. The user-generated videos from the YouTube UGC Dataset include diverse content (sports, games, lyrics, anime, etc.), quality and resolutions. The proposed methods must process 30 FHD frames under 1 second. In the challenge, a total of 102 participants registered, and 15 submitted code and models. The performance of the top-5 submissions is reviewed and provided here as a survey of diverse deep models for efficient video quality assessment of user-generated content.","sentences":["This paper reviews the AIS 2024 Video Quality Assessment (VQA) Challenge, focused on User-Generated Content (UGC).","The aim of this challenge is to gather deep learning-based methods capable of estimating the perceptual quality of UGC videos.","The user-generated videos from the YouTube UGC Dataset include diverse content (sports, games, lyrics, anime, etc.), quality and resolutions.","The proposed methods must process 30 FHD frames under 1 second.","In the challenge, a total of 102 participants registered, and 15 submitted code and models.","The performance of the top-5 submissions is reviewed and provided here as a survey of diverse deep models for efficient video quality assessment of user-generated content."],"url":"http://arxiv.org/abs/2404.16205v1","category":"cs.CV"}
{"created":"2024-04-24 20:33:38","title":"Structural investigation of the quasi-one-dimensional topological insulator Bi$_4$I$_4$","abstract":"The bismuth-halide Bi$_4$I$_4$ undergoes a structural transition around $T_P\\sim 300$K, which separates a high-temperature $\\beta$ phase ($T>T_P$) from a low-temperature $\\alpha$ phase ($T<T_P$). $\\alpha$ and $\\beta$ phases are suggested to host electronic band structures with distinct topological classifications. Rapid quenching was reported to stabilize a metastable $\\beta$-Bi$_4$I$_4$ at $T<T_P$, making possible a comparative study of the physical properties of the two phases in the same low-temperature range. In this work, we present a structural investigation of the Bi$_4$I$_4$ before and after quenching together with electrical resistivity measurements. We found that rapid cooling does not consistently lead to a metastable $\\beta$-Bi$_4$I$_4$, and a quick transition to $\\alpha$-Bi$_4$I$_4$ is observed. As a result, the comparison of putative signatures of different topologies attributed to a specific structural phase should be carefully considered. The observed phase instability is accompanied by an increase in iodine vacancies and by a change in the temperature dependence of electrical resistivity, pointing to native defects as a possible origin of our finding. Density functional theory (DFT) calculations support the scenario that iodine vacancies, together with bismuth antisites and interstitials, are among the defects that are more likely to occur in Bi$_4$I$_4$ during the growth.","sentences":["The bismuth-halide Bi$_4$I$_4$ undergoes a structural transition around $T_P\\sim 300$K, which separates a high-temperature $\\beta$ phase ($T>T_P$) from a low-temperature $\\alpha$ phase ($T<T_P$).","$\\alpha$ and $\\beta$ phases are suggested to host electronic band structures with distinct topological classifications.","Rapid quenching was reported to stabilize a metastable $\\beta$-Bi$_4$I$_4$ at $T<T_P$, making possible a comparative study of the physical properties of the two phases in the same low-temperature range.","In this work, we present a structural investigation of the Bi$_4$I$_4$ before and after quenching together with electrical resistivity measurements.","We found that rapid cooling does not consistently lead to a metastable $\\beta$-Bi$_4$I$_4$, and a quick transition to $\\alpha$-Bi$_4$I$_4$ is observed.","As a result, the comparison of putative signatures of different topologies attributed to a specific structural phase should be carefully considered.","The observed phase instability is accompanied by an increase in iodine vacancies and by a change in the temperature dependence of electrical resistivity, pointing to native defects as a possible origin of our finding.","Density functional theory (DFT) calculations support the scenario that iodine vacancies, together with bismuth antisites and interstitials, are among the defects that are more likely to occur in Bi$_4$I$_4$ during the growth."],"url":"http://arxiv.org/abs/2404.16194v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-24 20:27:08","title":"Spanwise Control Authority of Synthetic Jets on a Stalled Airfoil","abstract":"This study investigates the aerodynamic effects of low- and high-frequency synthetic jet control strategies on a National Advisory Committee for Aeronautics (NACA) 0025 airfoil. Visualizations and measurements are employed to assess the stability of the flow, focusing on the shear layer and wake dynamics under two forcing frequencies. High-frequency actuation is found to induce steadier flow reattachment and more favorable aerodynamic characteristics compared to low-frequency control. Flow structures resulting from high-frequency actuation, notably vortex rings, are identified and their significance in flow control is evaluated. Furthermore, the spanwise control authority is analyzed, revealing variations in aerodynamic stability away from the midspan. Insights from modal analysis provide additional understanding of flow structures and their evolution across different spanwise planes.","sentences":["This study investigates the aerodynamic effects of low- and high-frequency synthetic jet control strategies on a National Advisory Committee for Aeronautics (NACA) 0025 airfoil.","Visualizations and measurements are employed to assess the stability of the flow, focusing on the shear layer and wake dynamics under two forcing frequencies.","High-frequency actuation is found to induce steadier flow reattachment and more favorable aerodynamic characteristics compared to low-frequency control.","Flow structures resulting from high-frequency actuation, notably vortex rings, are identified and their significance in flow control is evaluated.","Furthermore, the spanwise control authority is analyzed, revealing variations in aerodynamic stability away from the midspan.","Insights from modal analysis provide additional understanding of flow structures and their evolution across different spanwise planes."],"url":"http://arxiv.org/abs/2404.16190v1","category":"physics.flu-dyn"}
{"created":"2024-04-24 20:19:14","title":"The impact of different magnetic braking prescriptions on the evolution of LMXBs","abstract":"We revisit the evolution of low-mass close binary systems under different magnetic braking (MB) prescriptions. We study binaries with a neutron star accretor. During mass transfer episodes, these systems emit X-rays and are known as Low Mass X-ray Binaries (LMXBs). When mass transfer stops, they can be observed as binary pulsars. Additionally, some of these systems can experience mass transfer while having orbital periods of less than 1 hr, thus evolving into ultracompact X-ray binaries (UCXBs). The evolution of LMXBs depends on their capability to lose angular momentum and maintain stable mass transfer. Among the angular momentum loss mechanisms, MB is one important, and still uncertain phenomenon. The standard MB prescription faces some problems when calculating LMXB evolution, leading to, e.g., a fine-tuning problem in the formation of UCXBs. Recent studies proposed new MB prescriptions, yielding diverse outcomes. Here, we investigate the effects of three novel MB prescriptions on the evolution of LMXBs using our stellar code. We found that all MB prescriptions considered allow the formation of binaries with orbital periods spanning from less than one hour to more than tens of days. Remarkably, our results enable the occurrence of wide systems even for the MB law that causes the strongest angular momentum losses and very high mass transfer rates. We found that models computed with the strongest MB prescription reach the UCXB state starting from a wider initial orbital period interval. Finally, we discuss and compare our results with observations and previous studies performed on this topic.","sentences":["We revisit the evolution of low-mass close binary systems under different magnetic braking (MB) prescriptions.","We study binaries with a neutron star accretor.","During mass transfer episodes, these systems emit X-rays and are known as Low Mass X-ray Binaries (LMXBs).","When mass transfer stops, they can be observed as binary pulsars.","Additionally, some of these systems can experience mass transfer while having orbital periods of less than 1 hr, thus evolving into ultracompact X-ray binaries (UCXBs).","The evolution of LMXBs depends on their capability to lose angular momentum and maintain stable mass transfer.","Among the angular momentum loss mechanisms, MB is one important, and still uncertain phenomenon.","The standard MB prescription faces some problems when calculating LMXB evolution, leading to, e.g., a fine-tuning problem in the formation of UCXBs.","Recent studies proposed new MB prescriptions, yielding diverse outcomes.","Here, we investigate the effects of three novel MB prescriptions on the evolution of LMXBs using our stellar code.","We found that all MB prescriptions considered allow the formation of binaries with orbital periods spanning from less than one hour to more than tens of days.","Remarkably, our results enable the occurrence of wide systems even for the MB law that causes the strongest angular momentum losses and very high mass transfer rates.","We found that models computed with the strongest MB prescription reach the UCXB state starting from a wider initial orbital period interval.","Finally, we discuss and compare our results with observations and previous studies performed on this topic."],"url":"http://arxiv.org/abs/2404.16185v1","category":"astro-ph.HE"}
{"created":"2024-04-24 20:10:10","title":"Blind Federated Learning without initial model","abstract":"Federated learning is an emerging machine learning approach that allows the construction of a model between several participants who hold their own private data. This method is secure and privacy-preserving, suitable for training a machine learning model using sensitive data from different sources, such as hospitals. In this paper, the authors propose two innovative methodologies for Particle Swarm Optimisation-based federated learning of Fuzzy Cognitive Maps in a privacy-preserving way. In addition, one relevant contribution this research includes is the lack of an initial model in the federated learning process, making it effectively blind. This proposal is tested with several open datasets, improving both accuracy and precision.","sentences":["Federated learning is an emerging machine learning approach that allows the construction of a model between several participants who hold their own private data.","This method is secure and privacy-preserving, suitable for training a machine learning model using sensitive data from different sources, such as hospitals.","In this paper, the authors propose two innovative methodologies for Particle Swarm Optimisation-based federated learning of Fuzzy Cognitive Maps in a privacy-preserving way.","In addition, one relevant contribution this research includes is the lack of an initial model in the federated learning process, making it effectively blind.","This proposal is tested with several open datasets, improving both accuracy and precision."],"url":"http://arxiv.org/abs/2404.16180v1","category":"cs.LG"}
{"created":"2024-04-24 20:03:01","title":"An analysis of the effects of sharing research data, code, and preprints on citations","abstract":"Calls to make scientific research more open have gained traction with a range of societal stakeholders. Open Science practices include but are not limited to the early sharing of results via preprints and openly sharing outputs such as data and code to make research more reproducible and extensible. Existing evidence shows that adopting Open Science practices has effects in several domains. In this study, we investigate whether adopting one or more Open Science practices leads to significantly higher citations for an associated publication, which is one form of academic impact. We use a novel dataset known as Open Science Indicators, produced by PLOS and DataSeer, which includes all PLOS publications from 2018 to 2023 as well as a comparison group sampled from the PMC Open Access Subset. In total, we analyze circa 122'000 publications. We calculate publication and author-level citation indicators and use a broad set of control variables to isolate the effect of Open Science Indicators on received citations. We show that Open Science practices are adopted to different degrees across scientific disciplines. We find that the early release of a publication as a preprint correlates with a significant positive citation advantage of about 20.2% on average. We also find that sharing data in an online repository correlates with a smaller yet still positive citation advantage of 4.3% on average. However, we do not find a significant citation advantage for sharing code. Further research is needed on additional or alternative measures of impact beyond citations. Our results are likely to be of interest to researchers, as well as publishers, research funders, and policymakers.","sentences":["Calls to make scientific research more open have gained traction with a range of societal stakeholders.","Open Science practices include but are not limited to the early sharing of results via preprints and openly sharing outputs such as data and code to make research more reproducible and extensible.","Existing evidence shows that adopting Open Science practices has effects in several domains.","In this study, we investigate whether adopting one or more Open Science practices leads to significantly higher citations for an associated publication, which is one form of academic impact.","We use a novel dataset known as Open Science Indicators, produced by PLOS and DataSeer, which includes all PLOS publications from 2018 to 2023 as well as a comparison group sampled from the PMC Open Access Subset.","In total, we analyze circa 122'000 publications.","We calculate publication and author-level citation indicators and use a broad set of control variables to isolate the effect of Open Science Indicators on received citations.","We show that Open Science practices are adopted to different degrees across scientific disciplines.","We find that the early release of a publication as a preprint correlates with a significant positive citation advantage of about 20.2% on average.","We also find that sharing data in an online repository correlates with a smaller yet still positive citation advantage of 4.3% on average.","However, we do not find a significant citation advantage for sharing code.","Further research is needed on additional or alternative measures of impact beyond citations.","Our results are likely to be of interest to researchers, as well as publishers, research funders, and policymakers."],"url":"http://arxiv.org/abs/2404.16171v1","category":"cs.DL"}
{"created":"2024-04-24 19:41:19","title":"Double Robust Variance Estimation","abstract":"Doubly robust estimators have gained popularity in the field of causal inference due to their ability to provide consistent point estimates when either an outcome or exposure model is correctly specified. However, the influence function based variance estimator frequently used with doubly robust estimators is only consistent when both the outcome and exposure models are correctly specified. Here, use of M-estimation and the empirical sandwich variance estimator for doubly robust point and variance estimation is demonstrated. Simulation studies illustrate the properties of the influence function based and empirical sandwich variance estimators. Estimators are applied to data from the Improving Pregnancy Outcomes with Progesterone (IPOP) trial to estimate the effect of maternal anemia on birth weight among women with HIV. In the example, birth weights if all women had anemia were estimated to be lower than birth weights if no women had anemia, though estimates were imprecise. Variance estimates were more stable under varying model specifications for the empirical sandwich variance estimator than the influence function based variance estimator.","sentences":["Doubly robust estimators have gained popularity in the field of causal inference due to their ability to provide consistent point estimates when either an outcome or exposure model is correctly specified.","However, the influence function based variance estimator frequently used with doubly robust estimators is only consistent when both the outcome and exposure models are correctly specified.","Here, use of M-estimation and the empirical sandwich variance estimator for doubly robust point and variance estimation is demonstrated.","Simulation studies illustrate the properties of the influence function based and empirical sandwich variance estimators.","Estimators are applied to data from the Improving Pregnancy Outcomes with Progesterone (IPOP) trial to estimate the effect of maternal anemia on birth weight among women with HIV.","In the example, birth weights if all women had anemia were estimated to be lower than birth weights if no women had anemia, though estimates were imprecise.","Variance estimates were more stable under varying model specifications for the empirical sandwich variance estimator than the influence function based variance estimator."],"url":"http://arxiv.org/abs/2404.16166v1","category":"stat.ME"}
{"created":"2024-04-24 19:40:23","title":"Comparative Analysis of Information Theoretic and Statistical Methods for Line Parameter Estimation","abstract":"Recent studies indicate that the noise characteristics of phasor measurement units (PMUs) can be more accurately described by non-Gaussian distributions. Consequently, estimation techniques based on Gaussian noise assumptions may produce poor results with PMU data. This paper considers the PMU based line parameter estimation (LPE) problem, and investigates the performance of four state-of-the-art techniques in solving this problem in presence of non-Gaussian measurement noise. The rigorous comparative analysis highlights the merits and demerits of each technique w.r.t. the LPE problem, and identifies conditions under which they are expected to give good results.","sentences":["Recent studies indicate that the noise characteristics of phasor measurement units (PMUs) can be more accurately described by non-Gaussian distributions.","Consequently, estimation techniques based on Gaussian noise assumptions may produce poor results with PMU data.","This paper considers the PMU based line parameter estimation (LPE) problem, and investigates the performance of four state-of-the-art techniques in solving this problem in presence of non-Gaussian measurement noise.","The rigorous comparative analysis highlights the merits and demerits of each technique w.r.t.","the LPE problem, and identifies conditions under which they are expected to give good results."],"url":"http://arxiv.org/abs/2404.16165v1","category":"eess.SP"}
{"created":"2024-04-24 19:38:56","title":"The Trembling-Hand Problem for LTLf Planning","abstract":"Consider an agent acting to achieve its temporal goal, but with a \"trembling hand\". In this case, the agent may mistakenly instruct, with a certain (typically small) probability, actions that are not intended due to faults or imprecision in its action selection mechanism, thereby leading to possible goal failure. We study the trembling-hand problem in the context of reasoning about actions and planning for temporally extended goals expressed in Linear Temporal Logic on finite traces (LTLf), where we want to synthesize a strategy (aka plan) that maximizes the probability of satisfying the LTLf goal in spite of the trembling hand. We consider both deterministic and nondeterministic (adversarial) domains. We propose solution techniques for both cases by relying respectively on Markov Decision Processes and on Markov Decision Processes with Set-valued Transitions with LTLf objectives, where the set-valued probabilistic transitions capture both the nondeterminism from the environment and the possible action instruction errors from the agent. We formally show the correctness of our solution techniques and demonstrate their effectiveness experimentally through a proof-of-concept implementation.","sentences":["Consider an agent acting to achieve its temporal goal, but with a \"trembling hand\".","In this case, the agent may mistakenly instruct, with a certain (typically small) probability, actions that are not intended due to faults or imprecision in its action selection mechanism, thereby leading to possible goal failure.","We study the trembling-hand problem in the context of reasoning about actions and planning for temporally extended goals expressed in Linear Temporal Logic on finite traces (LTLf), where we want to synthesize a strategy (aka plan) that maximizes the probability of satisfying the LTLf goal in spite of the trembling hand.","We consider both deterministic and nondeterministic (adversarial) domains.","We propose solution techniques for both cases by relying respectively on Markov Decision Processes and on Markov Decision Processes with Set-valued Transitions with LTLf objectives, where the set-valued probabilistic transitions capture both the nondeterminism from the environment and the possible action instruction errors from the agent.","We formally show the correctness of our solution techniques and demonstrate their effectiveness experimentally through a proof-of-concept implementation."],"url":"http://arxiv.org/abs/2404.16163v1","category":"cs.RO"}
{"created":"2024-04-24 19:16:20","title":"Rethinking Grant-Free Protocol in mMTC","abstract":"This paper revisits the identity detection problem under the current grant-free protocol in massive machine-type communications (mMTC) by asking the following question: for stable identity detection performance, is it enough to permit active devices to transmit preambles without any handshaking with the base station (BS)? Specifically, in the current grant-free protocol, the BS blindly allocates a fixed length of preamble to devices for identity detection as it lacks the prior information on the number of active devices $K$. However, in practice, $K$ varies dynamically over time, resulting in degraded identity detection performance especially when $K$ is large. Consequently, the current grant-free protocol fails to ensure stable identity detection performance. To address this issue, we propose a two-stage communication protocol which consists of estimation of $K$ in Phase I and detection of identities of active devices in Phase II. The preamble length for identity detection in Phase II is dynamically allocated based on the estimated $K$ in Phase I through a table lookup manner such that the identity detection performance could always be better than a predefined threshold. In addition, we design an algorithm for estimating $K$ in Phase I, and exploit the estimated $K$ to reduce the computational complexity of the identity detector in Phase II. Numerical results demonstrate the effectiveness of the proposed two-stage communication protocol and algorithms.","sentences":["This paper revisits the identity detection problem under the current grant-free protocol in massive machine-type communications (mMTC) by asking the following question: for stable identity detection performance, is it enough to permit active devices to transmit preambles without any handshaking with the base station (BS)?","Specifically, in the current grant-free protocol, the BS blindly allocates a fixed length of preamble to devices for identity detection as it lacks the prior information on the number of active devices $K$. However, in practice, $K$ varies dynamically over time, resulting in degraded identity detection performance especially when $K$ is large.","Consequently, the current grant-free protocol fails to ensure stable identity detection performance.","To address this issue, we propose a two-stage communication protocol which consists of estimation of $K$ in Phase I and detection of identities of active devices in Phase II.","The preamble length for identity detection in Phase II is dynamically allocated based on the estimated $K$ in Phase I through a table lookup manner such that the identity detection performance could always be better than a predefined threshold.","In addition, we design an algorithm for estimating $K$ in Phase I, and exploit the estimated $K$ to reduce the computational complexity of the identity detector in Phase II.","Numerical results demonstrate the effectiveness of the proposed two-stage communication protocol and algorithms."],"url":"http://arxiv.org/abs/2404.16152v1","category":"cs.IT"}
{"created":"2024-04-24 19:02:41","title":"Superposition of configurations and scanning","abstract":"We endow the cohomology of configuration spaces of a manifold with a product arising from superposing configurations. We prove that, under the scanning isomorphism, this product corresponds to the cup-product of the section space of the standard scanning bundle of the manifold.","sentences":["We endow the cohomology of configuration spaces of a manifold with a product arising from superposing configurations.","We prove that, under the scanning isomorphism, this product corresponds to the cup-product of the section space of the standard scanning bundle of the manifold."],"url":"http://arxiv.org/abs/2404.16145v1","category":"math.AT"}
{"created":"2024-04-24 18:50:56","title":"Learned Pulse Shaping Design for PAPR Reduction in DFT-s-OFDM","abstract":"High peak-to-average power ratio (PAPR) is one of the main factors limiting cell coverage for cellular systems, especially in the uplink direction. Discrete Fourier transform spread orthogonal frequency-domain multiplexing (DFT-s-OFDM) with spectrally-extended frequency-domain spectrum shaping (FDSS) is one of the efficient techniques deployed to lower the PAPR of the uplink waveforms. In this work, we propose a machine learning-based framework to determine the FDSS filter, optimizing a tradeoff between the symbol error rate (SER), the PAPR, and the spectral flatness requirements. Our end-to-end optimization framework considers multiple important design constraints, including the Nyquist zero-ISI (inter-symbol interference) condition. The numerical results show that learned FDSS filters lower the PAPR compared to conventional baselines, with minimal SER degradation. Tuning the parameters of the optimization also helps us understand the fundamental limitations and characteristics of the FDSS filters for PAPR reduction.","sentences":["High peak-to-average power ratio (PAPR) is one of the main factors limiting cell coverage for cellular systems, especially in the uplink direction.","Discrete Fourier transform spread orthogonal frequency-domain multiplexing (DFT-s-OFDM) with spectrally-extended frequency-domain spectrum shaping (FDSS) is one of the efficient techniques deployed to lower the PAPR of the uplink waveforms.","In this work, we propose a machine learning-based framework to determine the FDSS filter, optimizing a tradeoff between the symbol error rate (SER), the PAPR, and the spectral flatness requirements.","Our end-to-end optimization framework considers multiple important design constraints, including the Nyquist zero-ISI (inter-symbol interference) condition.","The numerical results show that learned FDSS filters lower the PAPR compared to conventional baselines, with minimal SER degradation.","Tuning the parameters of the optimization also helps us understand the fundamental limitations and characteristics of the FDSS filters for PAPR reduction."],"url":"http://arxiv.org/abs/2404.16137v1","category":"cs.IT"}
{"created":"2024-04-24 18:40:45","title":"Quantitative Characterization of Retinal Features in Translated OCTA","abstract":"Purpose: This study explores the feasibility of using generative machine learning (ML) to translate Optical Coherence Tomography (OCT) images into Optical Coherence Tomography Angiography (OCTA) images, potentially bypassing the need for specialized OCTA hardware. Methods: The method involved implementing a generative adversarial network framework that includes a 2D vascular segmentation model and a 2D OCTA image translation model. The study utilizes a public dataset of 500 patients, divided into subsets based on resolution and disease status, to validate the quality of TR-OCTA images. The validation employs several quality and quantitative metrics to compare the translated images with ground truth OCTAs (GT-OCTA). We then quantitatively characterize vascular features generated in TR-OCTAs with GT-OCTAs to assess the feasibility of using TR-OCTA for objective disease diagnosis. Result: TR-OCTAs showed high image quality in both 3 and 6 mm datasets (high-resolution, moderate structural similarity and contrast quality compared to GT-OCTAs). There were slight discrepancies in vascular metrics, especially in diseased patients. Blood vessel features like tortuosity and vessel perimeter index showed a better trend compared to density features which are affected by local vascular distortions. Conclusion: This study presents a promising solution to the limitations of OCTA adoption in clinical practice by using vascular features from TR-OCTA for disease detection. Translation relevance: This study has the potential to significantly enhance the diagnostic process for retinal diseases by making detailed vascular imaging more widely available and reducing dependency on costly OCTA equipment.","sentences":["Purpose:","This study explores the feasibility of using generative machine learning (ML) to translate Optical Coherence Tomography (OCT) images into Optical Coherence Tomography Angiography (OCTA) images, potentially bypassing the need for specialized OCTA hardware.","Methods: The method involved implementing a generative adversarial network framework that includes a 2D vascular segmentation model and a 2D OCTA image translation model.","The study utilizes a public dataset of 500 patients, divided into subsets based on resolution and disease status, to validate the quality of TR-OCTA images.","The validation employs several quality and quantitative metrics to compare the translated images with ground truth OCTAs (GT-OCTA).","We then quantitatively characterize vascular features generated in TR-OCTAs with GT-OCTAs to assess the feasibility of using TR-OCTA for objective disease diagnosis.","Result:","TR-OCTAs showed high image quality in both 3 and 6 mm datasets (high-resolution, moderate structural similarity and contrast quality compared to GT-OCTAs).","There were slight discrepancies in vascular metrics, especially in diseased patients.","Blood vessel features like tortuosity and vessel perimeter index showed a better trend compared to density features which are affected by local vascular distortions.","Conclusion: This study presents a promising solution to the limitations of OCTA adoption in clinical practice by using vascular features from TR-OCTA for disease detection.","Translation relevance: This study has the potential to significantly enhance the diagnostic process for retinal diseases by making detailed vascular imaging more widely available and reducing dependency on costly OCTA equipment."],"url":"http://arxiv.org/abs/2404.16133v1","category":"cs.CV"}
{"created":"2024-04-24 18:11:31","title":"Joint operation of a fast-charging EV hub with a stand-alone independent battery storage system under fairness considerations","abstract":"The need for larger-scale fast-charging electric vehicle (EV) hubs is on the rise due to the growth in EV adoption. Another area of power infrastructure growth is the proliferation of independently operated stand-alone battery storage systems (BSS), which is fueled by improvements and cost reductions in battery technology. Many possible uses of the stand-alone BSS are being explored including participation in the energy and ancillary markets, load balancing for renewable generations, and supporting large-scale load-consuming entities like hospitals. In this paper, we study a novel usage of the stand-alone BSS whereby in addition to participating in the electricity reserve market, it allows an EV hub to use a part of its storage capacity, when profitable. The hub uses the BSS storage capacity for arbitrage consequently reducing its operating cost. We formulate this joint operation as a bi-objective optimization model. We then reformulate it into a second-order cone Nash bargaining problem, the solution of which guarantees fairness to both the hub and the BSS. A sample numerical case study is formulated using actual prices of electricity and simulated data for the reserve market and EV charging demand. The Nash bargaining solution shows that both participants can benefit from the joint operation.","sentences":["The need for larger-scale fast-charging electric vehicle (EV) hubs is on the rise due to the growth in EV adoption.","Another area of power infrastructure growth is the proliferation of independently operated stand-alone battery storage systems (BSS), which is fueled by improvements and cost reductions in battery technology.","Many possible uses of the stand-alone BSS are being explored including participation in the energy and ancillary markets, load balancing for renewable generations, and supporting large-scale load-consuming entities like hospitals.","In this paper, we study a novel usage of the stand-alone BSS whereby in addition to participating in the electricity reserve market, it allows an EV hub to use a part of its storage capacity, when profitable.","The hub uses the BSS storage capacity for arbitrage consequently reducing its operating cost.","We formulate this joint operation as a bi-objective optimization model.","We then reformulate it into a second-order cone Nash bargaining problem, the solution of which guarantees fairness to both the hub and the BSS.","A sample numerical case study is formulated using actual prices of electricity and simulated data for the reserve market and EV charging demand.","The Nash bargaining solution shows that both participants can benefit from the joint operation."],"url":"http://arxiv.org/abs/2404.16113v1","category":"eess.SY"}
{"created":"2024-04-24 18:06:01","title":"Forcing, Transition Algebras, and Calculi","abstract":"We bring forward a logical system of transition algebras that enhances many-sorted first-order logic using features from dynamic logics. The sentences we consider include compositions, unions, and transitive closures of transition relations, which are treated similarly to the actions used in dynamic logics in order to define necessity and possibility operators. This leads to a higher degree of expressivity than that of many-sorted first-order logic. For example, one can finitely axiomatize both the finiteness and the reachability of models, neither of which are ordinarily possible in many-sorted first-order logic. We introduce syntactic entailment and study basic properties such as compactness and completeness, showing that the latter does not hold when standard finitary proof rules are used. Consequently, we define proof rules having both finite and countably infinite premises, and we provide conditions under which completeness can be proved. To that end, we generalize the forcing method introduced in model theory by Robinson from a single signature to a category of signatures, and we apply it to obtain a completeness result for signatures that are at most countable.","sentences":["We bring forward a logical system of transition algebras that enhances many-sorted first-order logic using features from dynamic logics.","The sentences we consider include compositions, unions, and transitive closures of transition relations, which are treated similarly to the actions used in dynamic logics in order to define necessity and possibility operators.","This leads to a higher degree of expressivity than that of many-sorted first-order logic.","For example, one can finitely axiomatize both the finiteness and the reachability of models, neither of which are ordinarily possible in many-sorted first-order logic.","We introduce syntactic entailment and study basic properties such as compactness and completeness, showing that the latter does not hold when standard finitary proof rules are used.","Consequently, we define proof rules having both finite and countably infinite premises, and we provide conditions under which completeness can be proved.","To that end, we generalize the forcing method introduced in model theory by Robinson from a single signature to a category of signatures, and we apply it to obtain a completeness result for signatures that are at most countable."],"url":"http://arxiv.org/abs/2404.16111v1","category":"cs.LO"}
{"created":"2024-04-24 18:00:05","title":"A Cepheid systematics-free test of $H_0$ to $\\lesssim2.5\\%$ accuracy using SH0ES photometry","abstract":"The recent SH0ES determination of the Hubble constant, $H_0=73.04\\pm1.04$ km/s/Mpc, deviates significantly by $\\approx5\\sigma$ from the \\textit{Planck} value, stimulating discussions on cosmological model extensions. To minimize statistical uncertainty and mitigate sensitivity to systematic errors in any single anchor distance determination, SH0ES combines Cepheids from various observations, including those from Type Ia supernova (SNe Ia) host galaxies, NGC 4258, and closer galaxies (MW, LMC, SMC, and M31), although this mixed sample may introduce unknown or subtle systematic errors due to comparing distant and closer Cepheids. To address this, we propose a subset excluding Cepheids from the closer galaxies, retaining only the NGC 4258 water megamasers as a single anchor, circumventing potential systematic errors associated with observational methods and reduction techniques. Focusing solely on these Cepheids yields competitive statistical errors, approximately $2.5\\%$, sufficient to identify a $\\approx3\\sigma$ tension with the \\textit{Planck} $H_0$ value. Our approach offers an opportunity to utilize optical photometry with systematic uncertainty smaller than the statistical uncertainty, potentially achieving higher precision than NIR photometry, given the lower optical background. However, currently the optical photometry sample's fidelity does not match that of NIR photometry. The significant Hubble tension obtained is unrelated to Cepheids and we discuss other options.","sentences":["The recent SH0ES determination of the Hubble constant, $H_0=73.04\\pm1.04$ km/s/Mpc, deviates significantly by $\\approx5\\sigma$ from the \\textit{Planck} value, stimulating discussions on cosmological model extensions.","To minimize statistical uncertainty and mitigate sensitivity to systematic errors in any single anchor distance determination, SH0ES combines Cepheids from various observations, including those from Type Ia supernova (SNe Ia) host galaxies, NGC 4258, and closer galaxies (MW, LMC, SMC, and M31), although this mixed sample may introduce unknown or subtle systematic errors due to comparing distant and closer Cepheids.","To address this, we propose a subset excluding Cepheids from the closer galaxies, retaining only the NGC 4258 water megamasers as a single anchor, circumventing potential systematic errors associated with observational methods and reduction techniques.","Focusing solely on these Cepheids yields competitive statistical errors, approximately $2.5\\%$, sufficient to identify a $\\approx3\\sigma$ tension with the \\textit{Planck} $H_0$ value.","Our approach offers an opportunity to utilize optical photometry with systematic uncertainty smaller than the statistical uncertainty, potentially achieving higher precision than NIR photometry, given the lower optical background.","However, currently the optical photometry sample's fidelity does not match that of NIR photometry.","The significant Hubble tension obtained is unrelated to Cepheids and we discuss other options."],"url":"http://arxiv.org/abs/2404.16102v1","category":"astro-ph.CO"}
{"created":"2024-04-24 18:00:03","title":"Multivariate Fidelities","abstract":"The main contribution of our paper is to introduce a number of multivariate quantum fidelities and show that they satisfy several desirable properties that are natural extensions of those of the Uhlmann and Holevo fidelities. We propose three variants that reduce to the average pairwise fidelity for commuting states: average pairwise $z$-fidelities, the multivariate semi-definite programming (SDP) fidelity, and a multivariate fidelity inspired by an existing secrecy measure. The second one is obtained by extending the SDP formulation of the Uhlmann fidelity to more than two states. All three of these variants satisfy the following properties: (i) reduction to multivariate classical fidelities for commuting states, (ii) the data-processing inequality, (iii) invariance under permutations of the states, (iv) its values are in the interval $[0,1]$; they are faithful, that is, their values are equal to one if and only if all the states are equal, and they satisfy orthogonality, that is their values are equal to zero if and only if the states are mutually orthogonal to each other, (v) direct-sum property, (vi) joint concavity, and (vii) uniform continuity bounds under certain conditions. Furthermore, we establish inequalities relating these different variants, indeed clarifying that all these definitions coincide with the average pairwise fidelity for commuting states. Lastly, we introduce another multivariate fidelity called multivariate log-Euclidean fidelity, which is a quantum generalization of the Matusita multivariate fidelity. We also show that it satisfies most of the desirable properties listed above, it is a function of a multivariate log-Euclidean divergence, and has an operational interpretation in terms of quantum hypothesis testing with an arbitrarily varying null hypothesis.","sentences":["The main contribution of our paper is to introduce a number of multivariate quantum fidelities and show that they satisfy several desirable properties that are natural extensions of those of the Uhlmann and Holevo fidelities.","We propose three variants that reduce to the average pairwise fidelity for commuting states: average pairwise $z$-fidelities, the multivariate semi-definite programming (SDP) fidelity, and a multivariate fidelity inspired by an existing secrecy measure.","The second one is obtained by extending the SDP formulation of the Uhlmann fidelity to more than two states.","All three of these variants satisfy the following properties: (i) reduction to multivariate classical fidelities for commuting states, (ii) the data-processing inequality, (iii) invariance under permutations of the states, (iv) its values are in the interval $[0,1]$; they are faithful, that is, their values are equal to one if and only if all the states are equal, and they satisfy orthogonality, that is their values are equal to zero if and only if the states are mutually orthogonal to each other, (v) direct-sum property, (vi) joint concavity, and (vii) uniform continuity bounds under certain conditions.","Furthermore, we establish inequalities relating these different variants, indeed clarifying that all these definitions coincide with the average pairwise fidelity for commuting states.","Lastly, we introduce another multivariate fidelity called multivariate log-Euclidean fidelity, which is a quantum generalization of the Matusita multivariate fidelity.","We also show that it satisfies most of the desirable properties listed above, it is a function of a multivariate log-Euclidean divergence, and has an operational interpretation in terms of quantum hypothesis testing with an arbitrarily varying null hypothesis."],"url":"http://arxiv.org/abs/2404.16101v1","category":"quant-ph"}
{"created":"2024-04-24 18:00:01","title":"A dark standard siren measurement of the Hubble constant following LIGO/Virgo/KAGRA O4a","abstract":"We present a new constraint on the Hubble constant ($H_0$) from the standard dark siren method using a sample of $5$ well-covered gravitational wave (GW) alerts reported during the first part of the fourth LIGO/Virgo/KAGRA observing runs in combination with standard dark sirens from the first three runs. Our methodology relies on the galaxy catalog method alone. We use the full probability density estimation of photometric redshifts derived by a deep learning method using the DESI Legacy Survey and DELVE galaxy catalogs. We add the constraints from the binary black hole mergers candidates S231226av, S231206cc, S230919bj, S230627c, and S230922g to the sample of standard dark sirens analyzed in Alfradique et al. (2024). We combine the $H_0$ posterior for $5$ new standard sirens with other $10$ previous events (3 with updated posteriors), finding $H_0 = 69.9^{+13.3}_{-12.0}~{\\rm km~s^{-1}~Mpc^{-1}}$ (68% Highest Density Interval) with the catalog method alone. This result represents an improvement of $\\sim 23\\%$ comparing the new $15$ dark siren constrain with the previous $10$ dark siren constraint, and a reduction in uncertainty of $\\sim 40\\%$ from the combination of $15$ dark and bright sirens compared with the GW170817 bright siren alone. The combination of dark and bright siren GW170817 with recent jet constraints yields $H_0$ of $68.0^{+4.3}_{-3.8}~{\\rm km~s^{-1}~Mpc^{-1}}$, a $\\sim 6\\%$ precision from Standard Sirens, reducing the previous constraint uncertainty by $\\sim 10\\%$ .","sentences":["We present a new constraint on the Hubble constant ($H_0$) from the standard dark siren method using a sample of $5$ well-covered gravitational wave (GW) alerts reported during the first part of the fourth LIGO/Virgo/KAGRA observing runs in combination with standard dark sirens from the first three runs.","Our methodology relies on the galaxy catalog method alone.","We use the full probability density estimation of photometric redshifts derived by a deep learning method using the DESI Legacy Survey and DELVE galaxy catalogs.","We add the constraints from the binary black hole mergers candidates S231226av, S231206cc, S230919bj, S230627c, and S230922g to the sample of standard dark sirens analyzed in Alfradique et al. (2024).","We combine the $H_0$ posterior for $5$ new standard sirens with other $10$ previous events (3 with updated posteriors), finding $H_0 = 69.9^{+13.3}_{-12.0}~{\\rm km~s^{-1}~Mpc^{-1}}$ (68% Highest Density Interval) with the catalog method alone.","This result represents an improvement of $\\sim 23\\%$ comparing the new $15$ dark siren constrain with the previous $10$ dark siren constraint, and a reduction in uncertainty of $\\sim 40\\%$ from the combination of $15$ dark and bright sirens compared with the GW170817 bright siren alone.","The combination of dark and bright siren GW170817 with recent jet constraints yields $H_0$ of $68.0^{+4.3}_{-3.8}~{\\rm km~s^{-1}~Mpc^{-1}}$, a $\\sim 6\\%$ precision from Standard Sirens, reducing the previous constraint uncertainty by $\\sim 10\\%$ ."],"url":"http://arxiv.org/abs/2404.16092v1","category":"astro-ph.CO"}
{"created":"2024-04-24 18:00:01","title":"K-theoretic Global Symmetry in String-constructed QFT and T-duality","abstract":"We propose that generalized symmetries in some string-constructed QFTs are given by twisted K-theory, so as to be manifestly consistent under T-duality. We thus have \\textit{even-form} and \\textit{odd-form} symmetries determined by $K^*_N(\\partial X)$, the twisted K-theory as D-brane charges on the asymptotic boundary $\\partial X$ of internal geometry $X$ with twist class $N$. For these QFTs, \"\\textit{$p$-form symmetries}\" are no longer separately well-defined for individual $p$, but instead mixed up. We discuss 6D ADE-type (2,0) SCFTs and some 6d (1,0) LSTs as examples, and demonstrate their twisted K-theoretic symmetries to be consistent with T-duality.","sentences":["We propose that generalized symmetries in some string-constructed QFTs are given by twisted K-theory, so as to be manifestly consistent under T-duality.","We thus have \\textit{even-form} and \\textit{odd-form} symmetries determined by $K^*_N(\\partial X)$, the twisted K-theory as D-brane charges on the asymptotic boundary $\\partial X$ of internal geometry $X$ with twist class $N$. For these QFTs, \"\\textit{$p$-form symmetries}\" are no longer separately well-defined for individual $p$, but instead mixed up.","We discuss 6D ADE-type (2,0) SCFTs and some 6d (1,0) LSTs as examples, and demonstrate their twisted K-theoretic symmetries to be consistent with T-duality."],"url":"http://arxiv.org/abs/2404.16097v1","category":"hep-th"}
{"created":"2024-04-24 17:59:48","title":"Cantor: Inspiring Multimodal Chain-of-Thought of MLLM","abstract":"With the advent of large language models(LLMs) enhanced by the chain-of-thought(CoT) methodology, visual reasoning problem is usually decomposed into manageable sub-tasks and tackled sequentially with various external tools. However, such a paradigm faces the challenge of the potential \"determining hallucinations\" in decision-making due to insufficient visual information and the limitation of low-level perception tools that fail to provide abstract summaries necessary for comprehensive reasoning. We argue that converging visual context acquisition and logical reasoning is pivotal for tackling visual reasoning tasks. This paper delves into the realm of multimodal CoT to solve intricate visual reasoning tasks with multimodal large language models(MLLMs) and their cognitive capability. To this end, we propose an innovative multimodal CoT framework, termed Cantor, characterized by a perception-decision architecture. Cantor first acts as a decision generator and integrates visual inputs to analyze the image and problem, ensuring a closer alignment with the actual context. Furthermore, Cantor leverages the advanced cognitive functions of MLLMs to perform as multifaceted experts for deriving higher-level information, enhancing the CoT generation process. Our extensive experiments demonstrate the efficacy of the proposed framework, showing significant improvements in multimodal CoT performance across two complex visual reasoning datasets, without necessitating fine-tuning or ground-truth rationales. Project Page: https://ggg0919.github.io/cantor/ .","sentences":["With the advent of large language models(LLMs) enhanced by the chain-of-thought(CoT) methodology, visual reasoning problem is usually decomposed into manageable sub-tasks and tackled sequentially with various external tools.","However, such a paradigm faces the challenge of the potential \"determining hallucinations\" in decision-making due to insufficient visual information and the limitation of low-level perception tools that fail to provide abstract summaries necessary for comprehensive reasoning.","We argue that converging visual context acquisition and logical reasoning is pivotal for tackling visual reasoning tasks.","This paper delves into the realm of multimodal CoT to solve intricate visual reasoning tasks with multimodal large language models(MLLMs) and their cognitive capability.","To this end, we propose an innovative multimodal CoT framework, termed Cantor, characterized by a perception-decision architecture.","Cantor first acts as a decision generator and integrates visual inputs to analyze the image and problem, ensuring a closer alignment with the actual context.","Furthermore, Cantor leverages the advanced cognitive functions of MLLMs to perform as multifaceted experts for deriving higher-level information, enhancing the CoT generation process.","Our extensive experiments demonstrate the efficacy of the proposed framework, showing significant improvements in multimodal CoT performance across two complex visual reasoning datasets, without necessitating fine-tuning or ground-truth rationales.","Project Page: https://ggg0919.github.io/cantor/ ."],"url":"http://arxiv.org/abs/2404.16033v1","category":"cs.CV"}
{"created":"2024-04-24 17:59:36","title":"Studying Large Language Model Behaviors Under Realistic Knowledge Conflicts","abstract":"Retrieval-augmented generation (RAG) mitigates many problems of fully parametric language models, such as temporal degradation, hallucinations, and lack of grounding. In RAG, the model's knowledge can be updated from documents provided in context. This leads to cases of conflict between the model's parametric knowledge and the contextual information, where the model may not always update its knowledge. Previous work studied knowledge conflicts by creating synthetic documents that contradict the model's correct parametric answers. We present a framework for studying knowledge conflicts in a realistic setup. We update incorrect parametric knowledge using real conflicting documents. This reflects how knowledge conflicts arise in practice. In this realistic scenario, we find that knowledge updates fail less often than previously reported. In cases where the models still fail to update their answers, we find a parametric bias: the incorrect parametric answer appearing in context makes the knowledge update likelier to fail. These results suggest that the factual parametric knowledge of LLMs can negatively influence their reading abilities and behaviors. Our code is available at https://github.com/kortukov/realistic_knowledge_conflicts/.","sentences":["Retrieval-augmented generation (RAG) mitigates many problems of fully parametric language models, such as temporal degradation, hallucinations, and lack of grounding.","In RAG, the model's knowledge can be updated from documents provided in context.","This leads to cases of conflict between the model's parametric knowledge and the contextual information, where the model may not always update its knowledge.","Previous work studied knowledge conflicts by creating synthetic documents that contradict the model's correct parametric answers.","We present a framework for studying knowledge conflicts in a realistic setup.","We update incorrect parametric knowledge using real conflicting documents.","This reflects how knowledge conflicts arise in practice.","In this realistic scenario, we find that knowledge updates fail less often than previously reported.","In cases where the models still fail to update their answers, we find a parametric bias: the incorrect parametric answer appearing in context makes the knowledge update likelier to fail.","These results suggest that the factual parametric knowledge of LLMs can negatively influence their reading abilities and behaviors.","Our code is available at https://github.com/kortukov/realistic_knowledge_conflicts/."],"url":"http://arxiv.org/abs/2404.16032v1","category":"cs.LG"}
{"created":"2024-04-24 17:42:32","title":"How to Make Money From Fresh Data: Subscription Strategies in Age-Based Systems","abstract":"We consider a communication system consisting of a server that tracks and publishes updates about a time-varying data source or event, and a gossip network of users interested in closely tracking the event. The timeliness of the information is measured through the version age of information. The users wish to have their expected version ages remain below a threshold, and have the option to either rely on gossip from their neighbors or subscribe to the server directly to follow updates about the event if the former option does not meet the timeliness requirements. The server wishes to maximize its profit by increasing the number of subscribers and reducing costs associated with the frequent sampling of the event. We model the problem setup as a Stackelberg game between the server and the users, where the server commits to a frequency of sampling the event, and the users make decisions on whether to subscribe or not. As an initial work, we focus on directed networks with unidirectional flow of information and obtain the optimal equilibrium strategies for all the players. We provide simulation results to confirm the theoretical findings and provide additional insights.","sentences":["We consider a communication system consisting of a server that tracks and publishes updates about a time-varying data source or event, and a gossip network of users interested in closely tracking the event.","The timeliness of the information is measured through the version age of information.","The users wish to have their expected version ages remain below a threshold, and have the option to either rely on gossip from their neighbors or subscribe to the server directly to follow updates about the event if the former option does not meet the timeliness requirements.","The server wishes to maximize its profit by increasing the number of subscribers and reducing costs associated with the frequent sampling of the event.","We model the problem setup as a Stackelberg game between the server and the users, where the server commits to a frequency of sampling the event, and the users make decisions on whether to subscribe or not.","As an initial work, we focus on directed networks with unidirectional flow of information and obtain the optimal equilibrium strategies for all the players.","We provide simulation results to confirm the theoretical findings and provide additional insights."],"url":"http://arxiv.org/abs/2404.16009v1","category":"cs.IT"}
{"created":"2024-04-24 17:30:37","title":"Remarks on Landau-Siegel zeros","abstract":"For certain families of $L$-functions, we prove that if each $L$-function in the family has only real zeros in a fixed yet arbitrarily small neighborhood of $s=1$, then one may considerably improve upon the known results on Landau-Siegel zeros. Sarnak and the third author proved a similar result under much more restrictive hypotheses.","sentences":["For certain families of $L$-functions, we prove that if each $L$-function in the family has only real zeros in a fixed yet arbitrarily small neighborhood of $s=1$, then one may considerably improve upon the known results on Landau-Siegel zeros.","Sarnak and the third author proved a similar result under much more restrictive hypotheses."],"url":"http://arxiv.org/abs/2404.16003v1","category":"math.NT"}
{"created":"2024-04-24 17:26:17","title":"BeSound: Bluetooth-Based Position Estimation Enhancing with Cross-Modality Distillation","abstract":"Smart factories leverage advanced technologies to optimize manufacturing processes and enhance efficiency. Implementing worker tracking systems, primarily through camera-based methods, ensures accurate monitoring. However, concerns about worker privacy and technology protection make it necessary to explore alternative approaches. We propose a non-visual, scalable solution using Bluetooth Low Energy (BLE) and ultrasound coordinates. BLE position estimation offers a very low-power and cost-effective solution, as the technology is available on smartphones and is scalable due to the large number of smartphone users, facilitating worker localization and safety protocol transmission. Ultrasound signals provide faster response times and higher accuracy but require custom hardware, increasing costs. To combine the benefits of both modalities, we employ knowledge distillation (KD) from ultrasound signals to BLE RSSI data. Once the student model is trained, the model only takes as inputs the BLE-RSSI data for inference, retaining the advantages of ubiquity and low cost of BLE RSSI. We tested our approach using data from an experiment with twelve participants in a smart factory test bed environment. We obtained an increase of 11.79% in the F1-score compared to the baseline (target model without KD and trained with BLE-RSSI data only).","sentences":["Smart factories leverage advanced technologies to optimize manufacturing processes and enhance efficiency.","Implementing worker tracking systems, primarily through camera-based methods, ensures accurate monitoring.","However, concerns about worker privacy and technology protection make it necessary to explore alternative approaches.","We propose a non-visual, scalable solution using Bluetooth Low Energy (BLE) and ultrasound coordinates.","BLE position estimation offers a very low-power and cost-effective solution, as the technology is available on smartphones and is scalable due to the large number of smartphone users, facilitating worker localization and safety protocol transmission.","Ultrasound signals provide faster response times and higher accuracy but require custom hardware, increasing costs.","To combine the benefits of both modalities, we employ knowledge distillation (KD) from ultrasound signals to BLE RSSI data.","Once the student model is trained, the model only takes as inputs the BLE-RSSI data for inference, retaining the advantages of ubiquity and low cost of BLE RSSI.","We tested our approach using data from an experiment with twelve participants in a smart factory test bed environment.","We obtained an increase of 11.79% in the F1-score compared to the baseline (target model without KD and trained with BLE-RSSI data only)."],"url":"http://arxiv.org/abs/2404.15999v1","category":"cs.LG"}
{"created":"2024-04-24 17:19:07","title":"Reissner-Nordstr\u00f6m black holes surrounded by perfect fluid dark matter: testing the viability of weak gravity conjecture and weak cosmic censorship conjecture simultaneously","abstract":"A possible violation of the weak gravity conjecture (WGC) by cosmic censorship is one of the major challenges in the field of general relativity. However, in this paper, we explore the possibility of reconciling the WGC and the WCCC by considering Reissner-Nordstr\\\"om (R-N) black holes embedded in perfect fluid dark matter (PFDM) in asymptotically flat spacetimes. These two conjectures are seemingly unrelated, but a recent proposal suggested that they are connected surprisingly. In particular, We argue a promising class of valid counterexamples to the WCCC in the four-dimensional Einstein-Maxwell theory, considering a charged black hole when WGC is present. We demonstrate that by imposing certain constraints on the parameters of the metric, the WGC and the WCCC can be compatible. Furthermore, we investigate the properties of the charged black hole in the presence of PFDM for $Q > M$ and present some intriguing figures to test the validity of the WGC and the WCCC simultaneously. When PFDM is absent ($\\gamma=0$), the RN black hole either has two event horizons if $Q^2/M^2\\leq 1$ or none if $Q^2/M^2> 1$. The second scenario results in a naked singularity, which contradicts the WCCC. But when PFDM is present ($\\gamma\\neq 0$), the RN black hole has event horizons with regard to Q and M. This implies that the singularity is always covered, and the WGC and the WCCC are fulfilled. Furthermore, we demonstrate that there is a critical value of $\\gamma$, called $\\gamma_{ext}$, that makes the RN black hole extremal when $\\gamma=\\gamma_{ext}$. In this situation, the black hole has an event horizon, and the WGC and the WCCC are still fulfilled. We infer that PFDM can make the WGC and the WCCC compatible with the RN black hole and that the WGC and the WCCC agree with each other when PFDM is present.","sentences":["A possible violation of the weak gravity conjecture (WGC) by cosmic censorship is one of the major challenges in the field of general relativity.","However, in this paper, we explore the possibility of reconciling the WGC and the WCCC by considering Reissner-Nordstr\\\"om (R-N) black holes embedded in perfect fluid dark matter (PFDM) in asymptotically flat spacetimes.","These two conjectures are seemingly unrelated, but a recent proposal suggested that they are connected surprisingly.","In particular, We argue a promising class of valid counterexamples to the WCCC in the four-dimensional Einstein-Maxwell theory, considering a charged black hole when WGC is present.","We demonstrate that by imposing certain constraints on the parameters of the metric, the WGC and the WCCC can be compatible.","Furthermore, we investigate the properties of the charged black hole in the presence of PFDM for $Q > M$ and present some intriguing figures to test the validity of the WGC and the WCCC simultaneously.","When PFDM is absent ($\\gamma=0$), the RN black hole either has two event horizons if $Q^2/M^2\\leq 1$ or none if $Q^2/M^2>","1$.","The second scenario results in a naked singularity, which contradicts the WCCC.","But when PFDM is present ($\\gamma\\neq 0$), the RN black hole has event horizons with regard to Q and M.","This implies that the singularity is always covered, and the WGC and the WCCC are fulfilled.","Furthermore, we demonstrate that there is a critical value of $\\gamma$, called $\\gamma_{ext}$, that makes the RN black hole extremal when $\\gamma=\\gamma_{ext}$. In this situation, the black hole has an event horizon, and the WGC and the WCCC are still fulfilled.","We infer that PFDM can make the WGC and the WCCC compatible with the RN black hole and that the WGC and the WCCC agree with each other when PFDM is present."],"url":"http://arxiv.org/abs/2404.15998v1","category":"gr-qc"}
{"created":"2024-04-24 17:10:35","title":"Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach","abstract":"Large language models (LLMs) are highly capable of many tasks but they can sometimes generate unreliable or inaccurate outputs. To tackle this issue, this paper studies the problem of uncertainty estimation and calibration for LLMs. We begin by formulating the uncertainty estimation problem for LLMs and then propose a supervised approach that takes advantage of the labeled datasets and estimates the uncertainty of the LLMs' responses. Based on the formulation, we illustrate the difference between the uncertainty estimation for LLMs and that for standard ML models and explain why the hidden activations of the LLMs contain uncertainty information. Our designed approach effectively demonstrates the benefits of utilizing hidden activations for enhanced uncertainty estimation across various tasks and shows robust transferability in out-of-distribution settings. Moreover, we distinguish the uncertainty estimation task from the uncertainty calibration task and show that a better uncertainty estimation mode leads to a better calibration performance. In practice, our method is easy to implement and is adaptable to different levels of model transparency including black box, grey box, and white box, each demonstrating strong performance based on the accessibility of the LLM's internal mechanisms.","sentences":["Large language models (LLMs) are highly capable of many tasks but they can sometimes generate unreliable or inaccurate outputs.","To tackle this issue, this paper studies the problem of uncertainty estimation and calibration for LLMs.","We begin by formulating the uncertainty estimation problem for LLMs and then propose a supervised approach that takes advantage of the labeled datasets and estimates the uncertainty of the LLMs' responses.","Based on the formulation, we illustrate the difference between the uncertainty estimation for LLMs and that for standard ML models and explain why the hidden activations of the LLMs contain uncertainty information.","Our designed approach effectively demonstrates the benefits of utilizing hidden activations for enhanced uncertainty estimation across various tasks and shows robust transferability in out-of-distribution settings.","Moreover, we distinguish the uncertainty estimation task from the uncertainty calibration task and show that a better uncertainty estimation mode leads to a better calibration performance.","In practice, our method is easy to implement and is adaptable to different levels of model transparency including black box, grey box, and white box, each demonstrating strong performance based on the accessibility of the LLM's internal mechanisms."],"url":"http://arxiv.org/abs/2404.15993v1","category":"cs.LG"}
{"created":"2024-04-24 16:52:23","title":"Learning deep Koopman operators with convex stability constraints","abstract":"In this paper, we present a novel sufficient condition for the stability of discrete-time linear systems that can be represented as a set of piecewise linear constraints, which make them suitable for quadratic programming optimization problems. More specifically, we tackle the problem of imposing asymptotic stability to a Koopman matrix learned from data during iterative gradient descent optimization processes. We show that this sufficient condition can be decoupled by rows of the system matrix, and propose a control barrier function-based projected gradient descent to enforce gradual evolution towards the stability set by running an optimization-in-the-loop during the iterative learning process. We compare the performance of our algorithm with other two recent approaches in the literature, and show that we get close to state-of-the-art performance while providing the added flexibility of allowing the optimization problem to be further customized for specific applications.","sentences":["In this paper, we present a novel sufficient condition for the stability of discrete-time linear systems that can be represented as a set of piecewise linear constraints, which make them suitable for quadratic programming optimization problems.","More specifically, we tackle the problem of imposing asymptotic stability to a Koopman matrix learned from data during iterative gradient descent optimization processes.","We show that this sufficient condition can be decoupled by rows of the system matrix, and propose a control barrier function-based projected gradient descent to enforce gradual evolution towards the stability set by running an optimization-in-the-loop during the iterative learning process.","We compare the performance of our algorithm with other two recent approaches in the literature, and show that we get close to state-of-the-art performance while providing the added flexibility of allowing the optimization problem to be further customized for specific applications."],"url":"http://arxiv.org/abs/2404.15978v1","category":"eess.SY"}
{"created":"2024-04-24 16:38:15","title":"Interpretable Clustering with the Distinguishability Criterion","abstract":"Cluster analysis is a popular unsupervised learning tool used in many disciplines to identify heterogeneous sub-populations within a sample. However, validating cluster analysis results and determining the number of clusters in a data set remains an outstanding problem. In this work, we present a global criterion called the Distinguishability criterion to quantify the separability of identified clusters and validate inferred cluster configurations. Our computational implementation of the Distinguishability criterion corresponds to the Bayes risk of a randomized classifier under the 0-1 loss. We propose a combined loss function-based computational framework that integrates the Distinguishability criterion with many commonly used clustering procedures, such as hierarchical clustering, k-means, and finite mixture models. We present these new algorithms as well as the results from comprehensive data analysis based on simulation studies and real data applications.","sentences":["Cluster analysis is a popular unsupervised learning tool used in many disciplines to identify heterogeneous sub-populations within a sample.","However, validating cluster analysis results and determining the number of clusters in a data set remains an outstanding problem.","In this work, we present a global criterion called the Distinguishability criterion to quantify the separability of identified clusters and validate inferred cluster configurations.","Our computational implementation of the Distinguishability criterion corresponds to the Bayes risk of a randomized classifier under the 0-1 loss.","We propose a combined loss function-based computational framework that integrates the Distinguishability criterion with many commonly used clustering procedures, such as hierarchical clustering, k-means, and finite mixture models.","We present these new algorithms as well as the results from comprehensive data analysis based on simulation studies and real data applications."],"url":"http://arxiv.org/abs/2404.15967v2","category":"stat.ML"}
{"created":"2024-04-24 16:31:10","title":"Showcasing Automated Vehicle Prototypes: A Collaborative Release Process to Manage and Communicate Risk","abstract":"The development and deployment of automated vehicles pose major challenges for manufacturers to this day. Whilst central questions, like the issue of ensuring a sufficient level of safety, remain unanswered, prototypes are increasingly finding their way into public traffic in urban areas. Although safety concepts for prototypes are addressed in literature, published work hardly contains any dedicated considerations on a systematic release for their operation. In this paper, we propose an incremental release process for public demonstrations of prototypes' automated driving functionality. We explicate release process requirements, derive process design decisions, and define stakeholder tasks. Furthermore, we reflect on practical insights gained through implementing the release process as part of the UNICAR$agil$ research project, in which four prototypes based on novel vehicle concepts were built and demonstrated to the public. One observation is the improved quality of internal risk communication, achieved by dismantling information asymmetries between stakeholders. Design conflicts are disclosed - providing a contribution to nurture transparency and, thereby, supporting a valid basis for release decisions. We argue that our release process meets two important requirements, as the results suggest its applicability to the domain of automated driving and its scalability to different vehicle concepts and organizational structures.","sentences":["The development and deployment of automated vehicles pose major challenges for manufacturers to this day.","Whilst central questions, like the issue of ensuring a sufficient level of safety, remain unanswered, prototypes are increasingly finding their way into public traffic in urban areas.","Although safety concepts for prototypes are addressed in literature, published work hardly contains any dedicated considerations on a systematic release for their operation.","In this paper, we propose an incremental release process for public demonstrations of prototypes' automated driving functionality.","We explicate release process requirements, derive process design decisions, and define stakeholder tasks.","Furthermore, we reflect on practical insights gained through implementing the release process as part of the UNICAR$agil$ research project, in which four prototypes based on novel vehicle concepts were built and demonstrated to the public.","One observation is the improved quality of internal risk communication, achieved by dismantling information asymmetries between stakeholders.","Design conflicts are disclosed - providing a contribution to nurture transparency and, thereby, supporting a valid basis for release decisions.","We argue that our release process meets two important requirements, as the results suggest its applicability to the domain of automated driving and its scalability to different vehicle concepts and organizational structures."],"url":"http://arxiv.org/abs/2404.15962v1","category":"eess.SY"}
{"created":"2024-04-24 16:25:52","title":"Explainable AI models for predicting liquefaction-induced lateral spreading","abstract":"Earthquake-induced liquefaction can cause substantial lateral spreading, posing threats to infrastructure. Machine learning (ML) can improve lateral spreading prediction models by capturing complex soil characteristics and site conditions. However, the \"black box\" nature of ML models can hinder their adoption in critical decision-making. This study addresses this limitation by using SHapley Additive exPlanations (SHAP) to interpret an eXtreme Gradient Boosting (XGB) model for lateral spreading prediction, trained on data from the 2011 Christchurch Earthquake. SHAP analysis reveals the factors driving the model's predictions, enhancing transparency and allowing for comparison with established engineering knowledge. The results demonstrate that the XGB model successfully identifies the importance of soil characteristics derived from Cone Penetration Test (CPT) data in predicting lateral spreading, validating its alignment with domain understanding. This work highlights the value of explainable machine learning for reliable and informed decision-making in geotechnical engineering and hazard assessment.","sentences":["Earthquake-induced liquefaction can cause substantial lateral spreading, posing threats to infrastructure.","Machine learning (ML) can improve lateral spreading prediction models by capturing complex soil characteristics and site conditions.","However, the \"black box\" nature of ML models can hinder their adoption in critical decision-making.","This study addresses this limitation by using SHapley Additive exPlanations (SHAP) to interpret an eXtreme Gradient Boosting (XGB) model for lateral spreading prediction, trained on data from the 2011 Christchurch Earthquake.","SHAP analysis reveals the factors driving the model's predictions, enhancing transparency and allowing for comparison with established engineering knowledge.","The results demonstrate that the XGB model successfully identifies the importance of soil characteristics derived from Cone Penetration Test (CPT) data in predicting lateral spreading, validating its alignment with domain understanding.","This work highlights the value of explainable machine learning for reliable and informed decision-making in geotechnical engineering and hazard assessment."],"url":"http://arxiv.org/abs/2404.15959v1","category":"physics.geo-ph"}
{"created":"2024-04-24 16:12:23","title":"Parameterized Algorithms for Coordinated Motion Planning: Minimizing Energy","abstract":"We study the parameterized complexity of a generalization of the coordinated motion planning problem on graphs, where the goal is to route a specified subset of a given set of $k$ robots to their destinations with the aim of minimizing the total energy (i.e., the total length traveled). We develop novel techniques to push beyond previously-established results that were restricted to solid grids.   We design a fixed-parameter additive approximation algorithm for this problem parameterized by $k$ alone. This result, which is of independent interest, allows us to prove the following two results pertaining to well-studied coordinated motion planning problems: (1) A fixed-parameter algorithm, parameterized by $k$, for routing a single robot to its destination while avoiding the other robots, which is related to the famous Rush-Hour Puzzle; and (2) a fixed-parameter algorithm, parameterized by $k$ plus the treewidth of the input graph, for the standard \\textsc{Coordinated Motion Planning} (CMP) problem in which we need to route all the $k$ robots to their destinations. The latter of these results implies, among others, the fixed-parameter tractability of CMP parameterized by $k$ on graphs of bounded outerplanarity, which include bounded-height subgrids.   We complement the above results with a lower bound which rules out the fixed-parameter tractability for CMP when parameterized by the total energy. This contrasts the recently-obtained tractability of the problem on solid grids under the same parameterization. As our final result, we strengthen the aforementioned fixed-parameter tractability to hold not only on solid grids but all graphs of bounded local treewidth -- a class including, among others, all graphs of bounded genus.","sentences":["We study the parameterized complexity of a generalization of the coordinated motion planning problem on graphs, where the goal is to route a specified subset of a given set of $k$ robots to their destinations with the aim of minimizing the total energy (i.e., the total length traveled).","We develop novel techniques to push beyond previously-established results that were restricted to solid grids.   ","We design a fixed-parameter additive approximation algorithm for this problem parameterized by $k$ alone.","This result, which is of independent interest, allows us to prove the following two results pertaining to well-studied coordinated motion planning problems: (1) A fixed-parameter algorithm, parameterized by $k$, for routing a single robot to its destination while avoiding the other robots, which is related to the famous Rush-Hour Puzzle; and (2) a fixed-parameter algorithm, parameterized by $k$ plus the treewidth of the input graph, for the standard \\textsc{Coordinated Motion Planning} (CMP) problem in which we need to route all the $k$ robots to their destinations.","The latter of these results implies, among others, the fixed-parameter tractability of CMP parameterized by $k$ on graphs of bounded outerplanarity, which include bounded-height subgrids.   ","We complement the above results with a lower bound which rules out the fixed-parameter tractability for CMP when parameterized by the total energy.","This contrasts the recently-obtained tractability of the problem on solid grids under the same parameterization.","As our final result, we strengthen the aforementioned fixed-parameter tractability to hold not only on solid grids but all graphs of bounded local treewidth -- a class including, among others, all graphs of bounded genus."],"url":"http://arxiv.org/abs/2404.15950v1","category":"cs.DM"}
{"created":"2024-04-24 16:08:51","title":"Adapted Lie splitting method for convection-diffusion problems with singular convective term","abstract":"Splitting methods are a widely used numerical scheme for solving convection-diffusion problems. However, they may lose stability in some situations, particularly when applied to convection-diffusion problems in the presence of an unbounded convective term. In this paper, we propose a new splitting method, called the \"Adapted Lie splitting method\", which successfully overcomes the observed instability in certain cases. Assuming that the unbounded coefficient belongs to a suitable Lorentz space, we show that the adapted Lie splitting converges to first-order under the analytic semigroup framework. Furthermore, we provide numerical experiments to illustrate our newly proposed splitting approach.","sentences":["Splitting methods are a widely used numerical scheme for solving convection-diffusion problems.","However, they may lose stability in some situations, particularly when applied to convection-diffusion problems in the presence of an unbounded convective term.","In this paper, we propose a new splitting method, called the \"Adapted Lie splitting method\", which successfully overcomes the observed instability in certain cases.","Assuming that the unbounded coefficient belongs to a suitable Lorentz space, we show that the adapted Lie splitting converges to first-order under the analytic semigroup framework.","Furthermore, we provide numerical experiments to illustrate our newly proposed splitting approach."],"url":"http://arxiv.org/abs/2404.15947v1","category":"math.NA"}
{"created":"2024-04-24 15:54:35","title":"Accurate Direct Positioning in Distributed MIMO Using Delay-Doppler Channel Measurements","abstract":"Distributed multiple-input multiple-output (D-MIMO) is a promising technology for simultaneous communication and positioning. However, phase synchronization between multiple access points in D-MIMO is challenging, which requires methods that function without the need for phase synchronization. We therefore present a method for D-MIMO that performs direct positioning of a moving device based on the delay-Doppler characteristics of the channel state information (CSI). Our method relies on particle-filter-based Bayesian inference with a state-space model. We use recent measurements from a sub-6 GHz D-MIMO OFDM system in an industrial environment to demonstrate centimeter accuracy under partial line-of-sight (LoS) conditions and decimeter accuracy under full non-LoS.","sentences":["Distributed multiple-input multiple-output (D-MIMO) is a promising technology for simultaneous communication and positioning.","However, phase synchronization between multiple access points in D-MIMO is challenging, which requires methods that function without the need for phase synchronization.","We therefore present a method for D-MIMO that performs direct positioning of a moving device based on the delay-Doppler characteristics of the channel state information (CSI).","Our method relies on particle-filter-based Bayesian inference with a state-space model.","We use recent measurements from a sub-6 GHz D-MIMO OFDM system in an industrial environment to demonstrate centimeter accuracy under partial line-of-sight (LoS) conditions and decimeter accuracy under full non-LoS."],"url":"http://arxiv.org/abs/2404.15936v1","category":"eess.SP"}
{"created":"2024-04-24 15:22:07","title":"Single-Atom Verification of the Optimal Trade-Off Between Speed and Cost in Shortcuts to Adiabaticity","abstract":"The approach of shortcuts to adiabaticity enables the effective execution of adiabatic dynamics in quantum information processing with enhanced speed. Owing to the inherent trade-off between dynamical speed and the cost associated with the transitionless driving field, executing arbitrarily fast operations becomes impractical. To understand the accurate interplay between speed and energetic cost in this process, we propose theoretically and verify experimentally a new trade-off, which is characterized by a tightly optimized bound within $s$-parameterized phase spaces. Our experiment is carried out in a single ultracold $^{40}$Ca$^{+}$ ion trapped in a harmonic potential. By exactly operating the quantum states of the ion, we execute the Landau-Zener model as an example, where the quantum speed limit as well as the cost are governed by the spectral gap. We witness that our proposed trade-off is indeed tight in scenarios involving both initially pure and initially mixed states. Our work helps understanding the fundamental constraints in shortcuts to adiabaticity and illuminates the potential of under-utilized phase spaces that have been traditionally overlooked.","sentences":["The approach of shortcuts to adiabaticity enables the effective execution of adiabatic dynamics in quantum information processing with enhanced speed.","Owing to the inherent trade-off between dynamical speed and the cost associated with the transitionless driving field, executing arbitrarily fast operations becomes impractical.","To understand the accurate interplay between speed and energetic cost in this process, we propose theoretically and verify experimentally a new trade-off, which is characterized by a tightly optimized bound within $s$-parameterized phase spaces.","Our experiment is carried out in a single ultracold $^{40}$Ca$^{+}$ ion trapped in a harmonic potential.","By exactly operating the quantum states of the ion, we execute the Landau-Zener model as an example, where the quantum speed limit as well as the cost are governed by the spectral gap.","We witness that our proposed trade-off is indeed tight in scenarios involving both initially pure and initially mixed states.","Our work helps understanding the fundamental constraints in shortcuts to adiabaticity and illuminates the potential of under-utilized phase spaces that have been traditionally overlooked."],"url":"http://arxiv.org/abs/2404.15922v1","category":"quant-ph"}
{"created":"2024-04-24 15:16:06","title":"An Element-Wise Weights Aggregation Method for Federated Learning","abstract":"Federated learning (FL) is a powerful Machine Learning (ML) paradigm that enables distributed clients to collaboratively learn a shared global model while keeping the data on the original device, thereby preserving privacy. A central challenge in FL is the effective aggregation of local model weights from disparate and potentially unbalanced participating clients. Existing methods often treat each client indiscriminately, applying a single proportion to the entire local model. However, it is empirically advantageous for each weight to be assigned a specific proportion. This paper introduces an innovative Element-Wise Weights Aggregation Method for Federated Learning (EWWA-FL) aimed at optimizing learning performance and accelerating convergence speed. Unlike traditional FL approaches, EWWA-FL aggregates local weights to the global model at the level of individual elements, thereby allowing each participating client to make element-wise contributions to the learning process. By taking into account the unique dataset characteristics of each client, EWWA-FL enhances the robustness of the global model to different datasets while also achieving rapid convergence. The method is flexible enough to employ various weighting strategies. Through comprehensive experiments, we demonstrate the advanced capabilities of EWWA-FL, showing significant improvements in both accuracy and convergence speed across a range of backbones and benchmarks.","sentences":["Federated learning (FL) is a powerful Machine Learning (ML) paradigm that enables distributed clients to collaboratively learn a shared global model while keeping the data on the original device, thereby preserving privacy.","A central challenge in FL is the effective aggregation of local model weights from disparate and potentially unbalanced participating clients.","Existing methods often treat each client indiscriminately, applying a single proportion to the entire local model.","However, it is empirically advantageous for each weight to be assigned a specific proportion.","This paper introduces an innovative Element-Wise Weights Aggregation Method for Federated Learning (EWWA-FL) aimed at optimizing learning performance and accelerating convergence speed.","Unlike traditional FL approaches, EWWA-FL aggregates local weights to the global model at the level of individual elements, thereby allowing each participating client to make element-wise contributions to the learning process.","By taking into account the unique dataset characteristics of each client, EWWA-FL enhances the robustness of the global model to different datasets while also achieving rapid convergence.","The method is flexible enough to employ various weighting strategies.","Through comprehensive experiments, we demonstrate the advanced capabilities of EWWA-FL, showing significant improvements in both accuracy and convergence speed across a range of backbones and benchmarks."],"url":"http://arxiv.org/abs/2404.15919v1","category":"cs.LG"}
{"created":"2024-04-24 15:10:53","title":"Detecting Disjoint Shortest Paths in Linear Time and More","abstract":"In the $k$-Disjoint Shortest Paths ($k$-DSP) problem, we are given a weighted graph $G$ on $n$ nodes and $m$ edges with specified source vertices $s_1, \\dots, s_k$, and target vertices $t_1, \\dots, t_k$, and are tasked with determining if $G$ contains vertex-disjoint $(s_i,t_i)$-shortest paths. For any constant $k$, it is known that $k$-DSP can be solved in polynomial time over undirected graphs and directed acyclic graphs (DAGs). However, the exact time complexity of $k$-DSP remains mysterious, with large gaps between the fastest known algorithms and best conditional lower bounds. In this paper, we obtain faster algorithms for important cases of $k$-DSP, and present better conditional lower bounds for $k$-DSP and its variants.   Previous work solved 2-DSP over weighted undirected graphs in $O(n^7)$ time, and weighted DAGs in $O(mn)$ time. For the main result of this paper, we present linear time algorithms for solving 2-DSP on weighted undirected graphs and DAGs. Our algorithms are algebraic however, and so only solve the detection rather than search version of 2-DSP.   For lower bounds, prior work implied that $k$-Clique can be reduced to $2k$-DSP in DAGs and undirected graphs with $O((kn)^2)$ nodes. We improve this reduction, by showing how to reduce from $k$-Clique to $k$-DSP in DAGs and undirected graphs with $O((kn)^2)$ nodes. A variant of $k$-DSP is the $k$-Disjoint Paths ($k$-DP) problem, where the solution paths no longer need to be shortest paths. Previous work reduced from $k$-Clique to $p$-DP in DAGs with $O(kn)$ nodes, for $p= k + k(k-1)/2$. We improve this by showing a reduction from $k$-Clique to $p$-DP, for $p=k + \\lfloor k^2/4\\rfloor$. Under the $k$-Clique Hypothesis from fine-grained complexity, our results establish better conditional lower bounds for $k$-DSP for all $k\\ge 4$, and better conditional lower bounds for $p$-DP for all $p\\le 4031$.","sentences":["In the $k$-Disjoint Shortest Paths ($k$-DSP) problem, we are given a weighted graph $G$ on $n$ nodes and $m$ edges with specified source vertices $s_1, \\dots, s_k$, and target vertices $t_1, \\dots, t_k$, and are tasked with determining if $G$ contains vertex-disjoint $(s_i,t_i)$-shortest paths.","For any constant $k$, it is known that $k$-DSP can be solved in polynomial time over undirected graphs and directed acyclic graphs (DAGs).","However, the exact time complexity of $k$-DSP remains mysterious, with large gaps between the fastest known algorithms and best conditional lower bounds.","In this paper, we obtain faster algorithms for important cases of $k$-DSP, and present better conditional lower bounds for $k$-DSP and its variants.   ","Previous work solved 2-DSP over weighted undirected graphs in $O(n^7)$ time, and weighted DAGs in $O(mn)$ time.","For the main result of this paper, we present linear time algorithms for solving 2-DSP on weighted undirected graphs and DAGs.","Our algorithms are algebraic however, and so only solve the detection rather than search version of 2-DSP.   ","For lower bounds, prior work implied that $k$-Clique can be reduced to $2k$-DSP in DAGs and undirected graphs with $O((kn)^2)$ nodes.","We improve this reduction, by showing how to reduce from $k$-Clique to $k$-DSP in DAGs and undirected graphs with $O((kn)^2)$ nodes.","A variant of $k$-DSP is the $k$-Disjoint Paths ($k$-DP) problem, where the solution paths no longer need to be shortest paths.","Previous work reduced from $k$-Clique to $p$-DP in DAGs with $O(kn)$ nodes, for $p= k + k(k-1)/2$. We improve this by showing a reduction from $k$-Clique to $p$-DP, for $p=k + \\lfloor k^2/4\\rfloor$. Under the $k$-Clique Hypothesis from fine-grained complexity, our results establish better conditional lower bounds for $k$-DSP for all $k\\ge 4$, and better conditional lower bounds for $p$-DP for all $p\\le 4031$."],"url":"http://arxiv.org/abs/2404.15916v1","category":"cs.DS"}
{"created":"2024-04-24 15:04:18","title":"Kepler main-sequence solar-like stars: surface rotation and magnetic-activity evolution","abstract":"While the mission's primary goal was focused on exoplanet detection and characterization, Kepler made and continues to make extraordinary advances in stellar physics. Stellar rotation and magnetic activity are no exceptions. Kepler allowed for these properties to be determined for tens of thousands of stars from the main sequence up to the red giant branch. From photometry, this can be achieved by investigating the brightness fluctuations due to active regions, which cause surface inhomogeneities, or through asteroseismology as oscillation modes are sensitive to rotation and magnetic fields. This review summarizes the rotation and magnetic activity properties of the single main-sequence solar-like stars within the Kepler field. We contextualize the Kepler sample by comparing it to known transitions in the stellar rotation and magnetic-activity evolution, such as the convergence to the rotation sequence (from the saturated to the unsaturated regime of magnetic activity) and the Vaughan-Preston gap. While reviewing the publicly available data, we also uncover one interesting finding related to the intermediate-rotation gap seen in Kepler and other surveys. We find evidence for this rotation gap in previous ground-based data for the X-ray luminosity. Understanding the complex evolution and interplay between rotation and magnetic activity in solar-like stars is crucial, as it sheds light on fundamental processes governing stellar evolution, including the evolution of our own Sun.","sentences":["While the mission's primary goal was focused on exoplanet detection and characterization, Kepler made and continues to make extraordinary advances in stellar physics.","Stellar rotation and magnetic activity are no exceptions.","Kepler allowed for these properties to be determined for tens of thousands of stars from the main sequence up to the red giant branch.","From photometry, this can be achieved by investigating the brightness fluctuations due to active regions, which cause surface inhomogeneities, or through asteroseismology as oscillation modes are sensitive to rotation and magnetic fields.","This review summarizes the rotation and magnetic activity properties of the single main-sequence solar-like stars within the Kepler field.","We contextualize the Kepler sample by comparing it to known transitions in the stellar rotation and magnetic-activity evolution, such as the convergence to the rotation sequence (from the saturated to the unsaturated regime of magnetic activity) and the Vaughan-Preston gap.","While reviewing the publicly available data, we also uncover one interesting finding related to the intermediate-rotation gap seen in Kepler and other surveys.","We find evidence for this rotation gap in previous ground-based data for the X-ray luminosity.","Understanding the complex evolution and interplay between rotation and magnetic activity in solar-like stars is crucial, as it sheds light on fundamental processes governing stellar evolution, including the evolution of our own Sun."],"url":"http://arxiv.org/abs/2404.15911v1","category":"astro-ph.SR"}
{"created":"2024-04-24 14:41:14","title":"Quantum metrology in a driven-dissipation down-conversion system beyond the parametric approximation","abstract":"We investigate quantum metrology in a degenerate down-conversion system composed of a pump mode and two degenerate signal modes. In the conventional parametric approximation, the pump mode is assumed to be constant, not a quantum operator. We obtain the measurement precision of the coupling strength between the pump mode and two degenerate signal modes beyond the parametric approximation. Without a dissipation, the super-Heisenberg limit can be obtained when the initial state is the direct product of classical state and quantum state. This does not require the use of entanglement resources which are not easy to prepare. When the pump mode suffers from a single-photon dissipation, the measurement uncertainty of the coupling strength is close to 0 as the coupling strength approaches 0 with a coherent driving. The direct photon detection is proved to be the optimal measurement. This result has not been changed when the signal modes suffer from the two-photon dissipation. When the signal modes also suffer from the single-mode dissipation, the information of the coupling strength can still be obtained in the steady state. In addition, the measurement uncertainty of the coupling strength can also be close to 0 and become independent of noise temperature as the critical point between the normal and superradiance phase approaches. Finally, we show that a driven-dissipation down-conversion system can be used as a precise quantum sensor to measure the driving strength.","sentences":["We investigate quantum metrology in a degenerate down-conversion system composed of a pump mode and two degenerate signal modes.","In the conventional parametric approximation, the pump mode is assumed to be constant, not a quantum operator.","We obtain the measurement precision of the coupling strength between the pump mode and two degenerate signal modes beyond the parametric approximation.","Without a dissipation, the super-Heisenberg limit can be obtained when the initial state is the direct product of classical state and quantum state.","This does not require the use of entanglement resources which are not easy to prepare.","When the pump mode suffers from a single-photon dissipation, the measurement uncertainty of the coupling strength is close to 0 as the coupling strength approaches 0 with a coherent driving.","The direct photon detection is proved to be the optimal measurement.","This result has not been changed when the signal modes suffer from the two-photon dissipation.","When the signal modes also suffer from the single-mode dissipation, the information of the coupling strength can still be obtained in the steady state.","In addition, the measurement uncertainty of the coupling strength can also be close to 0 and become independent of noise temperature as the critical point between the normal and superradiance phase approaches.","Finally, we show that a driven-dissipation down-conversion system can be used as a precise quantum sensor to measure the driving strength."],"url":"http://arxiv.org/abs/2404.15898v1","category":"quant-ph"}
{"created":"2024-04-24 14:31:49","title":"The generalized method of separation of variables for diffusion-influenced reactions: Irreducible Cartesian tensors technique","abstract":"Motivated through various applications of the trapping diffusion-influenced reactions theory in physics, chemistry and biology, this paper deals with irreducible Cartesian tensors (ICT) technique within the scope of the generalized method of separation of variables (GMSV). Presenting a survey from the basic concepts of the theory, we spotlight the distinctive features of the above approach against known in literature similar techniques. The classical solution to the stationary diffusion equation under appropriate boundary conditions is represented as a series in the ICT. By means of proved translation addition theorem we straightforwardly reduce the general boundary value diffusion problem for $N$ spherical sinks to the corresponding resolving infinite system of linear algebraic equations with respect to unknown tensor coefficients. These coefficients comprise explicit dependence on the arbitrary three-dimensional configurations of $N$ sinks with different radii and surface reactivities. Specific application of the ICT technique is illustrated by some numerical calculations for the reactions, occurring in the arrays of two and three spherical sinks. For the first time we treat the statement and solution to the above reaction-diffusion problem, applying the smooth manifold concept.","sentences":["Motivated through various applications of the trapping diffusion-influenced reactions theory in physics, chemistry and biology, this paper deals with irreducible Cartesian tensors (ICT) technique within the scope of the generalized method of separation of variables (GMSV).","Presenting a survey from the basic concepts of the theory, we spotlight the distinctive features of the above approach against known in literature similar techniques.","The classical solution to the stationary diffusion equation under appropriate boundary conditions is represented as a series in the ICT.","By means of proved translation addition theorem we straightforwardly reduce the general boundary value diffusion problem for $N$ spherical sinks to the corresponding resolving infinite system of linear algebraic equations with respect to unknown tensor coefficients.","These coefficients comprise explicit dependence on the arbitrary three-dimensional configurations of $N$ sinks with different radii and surface reactivities.","Specific application of the ICT technique is illustrated by some numerical calculations for the reactions, occurring in the arrays of two and three spherical sinks.","For the first time we treat the statement and solution to the above reaction-diffusion problem, applying the smooth manifold concept."],"url":"http://arxiv.org/abs/2404.15893v1","category":"physics.chem-ph"}
{"created":"2024-04-24 14:07:07","title":"The Robotic MAAO 0.7m Telescope System: Performance and Standard Photometric System","abstract":"We introduce a 0.7m telescope system at the Miryang Arirang Astronomical Observatory (MAAO), a public observatory in Miryang, Korea. System integration and a scheduling program enable the 0.7m telescope system to operate completely robotically during nighttime, eliminating the need for human intervention. Using the 0.7m telescope system, we obtain atmospheric extinction coefficients and the zero-point magnitudes by observing standard stars. As a result, we find that atmospheric extinctions are moderate but they can sometimes increase depending on the weather conditions. The measured 5-sigma limiting magnitudes reach down to BVRI=19.4-19.6 AB mag for a point source with a total integrated time of 10 minutes under clear weather conditions, demonstrating comparable performance with other observational facilities operating under similar specifications and sky conditions. We expect that the newly established MAAO 0.7m telescope system will contribute significantly to the observational studies of astronomy. Particularly, with its capability for robotic observations, this system, although its primary duty is for public viewing, can be extensively used for the time-series observation of transients.","sentences":["We introduce a 0.7m telescope system at the Miryang Arirang Astronomical Observatory (MAAO), a public observatory in Miryang, Korea.","System integration and a scheduling program enable the 0.7m telescope system to operate completely robotically during nighttime, eliminating the need for human intervention.","Using the 0.7m telescope system, we obtain atmospheric extinction coefficients and the zero-point magnitudes by observing standard stars.","As a result, we find that atmospheric extinctions are moderate but they can sometimes increase depending on the weather conditions.","The measured 5-sigma limiting magnitudes reach down to BVRI=19.4-19.6 AB mag for a point source with a total integrated time of 10 minutes under clear weather conditions, demonstrating comparable performance with other observational facilities operating under similar specifications and sky conditions.","We expect that the newly established MAAO 0.7m telescope system will contribute significantly to the observational studies of astronomy.","Particularly, with its capability for robotic observations, this system, although its primary duty is for public viewing, can be extensively used for the time-series observation of transients."],"url":"http://arxiv.org/abs/2404.15884v1","category":"astro-ph.IM"}
{"created":"2024-04-24 13:50:27","title":"Machine Learning for Pre/Post Flight UAV Rotor Defect Detection Using Vibration Analysis","abstract":"Unmanned Aerial Vehicles (UAVs) will be critical infrastructural components of future smart cities. In order to operate efficiently, UAV reliability must be ensured by constant monitoring for faults and failures. To this end, the work presented in this paper leverages signal processing and Machine Learning (ML) methods to analyze the data of a comprehensive vibrational analysis to determine the presence of rotor blade defects during pre and post-flight operation. With the help of dimensionality reduction techniques, the Random Forest algorithm exhibited the best performance and detected defective rotor blades perfectly. Additionally, a comprehensive analysis of the impact of various feature subsets is presented to gain insight into the factors affecting the model's classification decision process.","sentences":["Unmanned Aerial Vehicles (UAVs) will be critical infrastructural components of future smart cities.","In order to operate efficiently, UAV reliability must be ensured by constant monitoring for faults and failures.","To this end, the work presented in this paper leverages signal processing and Machine Learning (ML) methods to analyze the data of a comprehensive vibrational analysis to determine the presence of rotor blade defects during pre and post-flight operation.","With the help of dimensionality reduction techniques, the Random Forest algorithm exhibited the best performance and detected defective rotor blades perfectly.","Additionally, a comprehensive analysis of the impact of various feature subsets is presented to gain insight into the factors affecting the model's classification decision process."],"url":"http://arxiv.org/abs/2404.15880v1","category":"eess.SP"}
{"created":"2024-04-24 13:42:26","title":"Effective Unsupervised Constrained Text Generation based on Perturbed Masking","abstract":"Unsupervised constrained text generation aims to generate text under a given set of constraints without any supervised data. Current state-of-the-art methods stochastically sample edit positions and actions, which may cause unnecessary search steps. In this paper, we propose PMCTG to improve effectiveness by searching for the best edit position and action in each step. Specifically, PMCTG extends perturbed masking technique to effectively search for the most incongruent token to edit. Then it introduces four multi-aspect scoring functions to select edit action to further reduce search difficulty. Since PMCTG does not require supervised data, it could be applied to different generation tasks. We show that under the unsupervised setting, PMCTG achieves new state-of-the-art results in two representative tasks, namely keywords-to-sentence generation and paraphrasing.","sentences":["Unsupervised constrained text generation aims to generate text under a given set of constraints without any supervised data.","Current state-of-the-art methods stochastically sample edit positions and actions, which may cause unnecessary search steps.","In this paper, we propose PMCTG to improve effectiveness by searching for the best edit position and action in each step.","Specifically, PMCTG extends perturbed masking technique to effectively search for the most incongruent token to edit.","Then it introduces four multi-aspect scoring functions to select edit action to further reduce search difficulty.","Since PMCTG does not require supervised data, it could be applied to different generation tasks.","We show that under the unsupervised setting, PMCTG achieves new state-of-the-art results in two representative tasks, namely keywords-to-sentence generation and paraphrasing."],"url":"http://arxiv.org/abs/2404.15877v1","category":"cs.CL"}
{"created":"2024-04-24 13:40:12","title":"Extension of Hayward black hole in $f(R)$ gravity coupled with a scalar field","abstract":"This study looks into regular solutions in a theory of gravity called $f(R)$ gravity, which also involves a scalar field. The $f(R)$ theory changes Einstein's ideas by adding a new function related to something called the Ricci scalar. This lets us tweak the equations that describe how gravity works. Adding a scalar field makes the theory more interesting, giving us more ways to investigate and understand it. { The main goal of this research is to create regular black holes using a combination of $f(R)$ gravitational theory and a scalar field.} Regular solutions don't have any singularities, which are points where certain physical quantities, like invariants, become really big or undefined. { In this context, we find two regular black hole solutions by using a spherical space with either an equal or unequal approach.} For the solutions where we use the equal approach, we figure out the shape of $f(R)$ and how it changes, along with its first and second derivatives. We demonstrate that Hayward's solution in this theory stays steady because all the shapes of $f(R)$ and their first and second derivatives are positive. Next, we focus on the case where the metric isn't equal and figure out the black hole solution. We also find out what $f(R)$ and the scalar field look like in this situation. We demonstrate that the solution in this case is a broader version of the Hayward solution. When certain conditions are met, we end up back at the scenario where the metrics are equal. We also prove that this model is stable because $f(R)$, along with its first and second derivatives, are all positive. { We analyze the trajectories of these black hole solutions and determine the forms of their conserved quantities that remain same along those trajectories.","sentences":["This study looks into regular solutions in a theory of gravity called $f(R)$ gravity, which also involves a scalar field.","The $f(R)$ theory changes Einstein's ideas by adding a new function related to something called the Ricci scalar.","This lets us tweak the equations that describe how gravity works.","Adding a scalar field makes the theory more interesting, giving us more ways to investigate and understand it.","{","The main goal of this research is to create regular black holes using a combination of $f(R)$ gravitational theory and a scalar field.}","Regular solutions don't have any singularities, which are points where certain physical quantities, like invariants, become really big or undefined.","{ In this context, we find two regular black hole solutions by using a spherical space with either an equal or unequal approach.}","For the solutions where we use the equal approach, we figure out the shape of $f(R)$ and how it changes, along with its first and second derivatives.","We demonstrate that Hayward's solution in this theory stays steady because all the shapes of $f(R)$ and their first and second derivatives are positive.","Next, we focus on the case where the metric isn't equal and figure out the black hole solution.","We also find out what $f(R)$ and the scalar field look like in this situation.","We demonstrate that the solution in this case is a broader version of the Hayward solution.","When certain conditions are met, we end up back at the scenario where the metrics are equal.","We also prove that this model is stable because $f(R)$, along with its first and second derivatives, are all positive.","{ We analyze the trajectories of these black hole solutions and determine the forms of their conserved quantities that remain same along those trajectories."],"url":"http://arxiv.org/abs/2404.16081v1","category":"gr-qc"}
{"created":"2024-04-24 13:36:45","title":"Sufficent Conditions for the preservation of Path-Connectedness in an arbitrary metric space","abstract":"It is proven that if $ (X,d) $ is an arbitrary metric space and $ U $ is a path-connected subset of $ X $ with $M:=\\{x_i:\\ i\\in\\{1,2,\\dots,k\\}\\}\\subset int(U) $, then the property of path-connectedness is also preserved in the resulting set $ U\\setminus M, $ provided that the boundary of each open ball of X is a non-empty and path-connected set. Moreover, under appropriate conditions we extend the above result in the case where the set $ M $ is countably infinite. As a consequence these results maintain path-connectedness for domains with holes.","sentences":["It is proven that if $ (X,d) $ is an arbitrary metric space and $ U $ is a path-connected subset of $ X $ with $M:=\\{x_i:\\ i\\in\\{1,2,\\dots,k\\}\\}\\subset int(U) $, then the property of path-connectedness is also preserved in the resulting set $ U\\setminus M, $ provided that the boundary of each open ball of X is a non-empty and path-connected set.","Moreover, under appropriate conditions we extend the above result in the case where the set $ M $ is countably infinite.","As a consequence these results maintain path-connectedness for domains with holes."],"url":"http://arxiv.org/abs/2404.15871v1","category":"math.GN"}
{"created":"2024-04-24 13:25:47","title":"Truncated quantum observables and their semiclassical limit","abstract":"For quantum observables $H$ truncated on the range of orthogonal projections $\\Pi_N$ of rank $N$, we study the corresponding Weyl symbol in the phase space in the semiclassical limit of vanishing Planck constant $\\hbar\\to0$ and large quantum number $N\\to\\infty$, with $\\hbar N$ fixed. Under certain assumptions, we prove the $L^2$- convergence of the Weyl symbols to a symbol truncated (hence, in general discontinuous) on the classically permitted region in phase space. As an illustration of the general theorems we analyse truncated observables for the harmonic oscillator and for a free particle in a one-dimensional box. In the latter case, we also compute the microscopic pointwise limit of the symbols near the boundary of the classically permitted region.","sentences":["For quantum observables $H$ truncated on the range of orthogonal projections $\\Pi_N$ of rank $N$, we study the corresponding Weyl symbol in the phase space in the semiclassical limit of vanishing Planck constant $\\hbar\\to0$ and large quantum number $N\\to\\infty$, with $\\hbar N$ fixed.","Under certain assumptions, we prove the $L^2$- convergence of the Weyl symbols to a symbol truncated (hence, in general discontinuous) on the classically permitted region in phase space.","As an illustration of the general theorems we analyse truncated observables for the harmonic oscillator and for a free particle in a one-dimensional box.","In the latter case, we also compute the microscopic pointwise limit of the symbols near the boundary of the classically permitted region."],"url":"http://arxiv.org/abs/2404.15863v1","category":"math-ph"}
{"created":"2024-04-24 13:23:29","title":"Modeling liquefaction-induced runout of a tailings dam using a hybrid finite element and material point method approach","abstract":"Tailings dams impound large amounts of saturated soil which can be highly susceptible to liquefaction. Liquefaction results in a severe loss of strength in the retained soil and potentially failure of the dam. If the dam is breached, a massive debris flow of liquefied soil is then released with potentially disastrous consequences downstream. Numerical models are frequently utilized to predict the liquefaction response of tailings dams and the potential runout, and these analyses inform engineering decisions regarding hazard avoidance and mitigation. The Finite Element Method (FEM) is a widespread tool which excels at modeling liquefaction triggering and initial movements, but it quickly loses accuracy when modeling large deformations due to mesh distortion. Conversely, the Material Point Method (MPM), a hybrid Eulerian-Lagrangian method, employs particles that move freely across a background grid and can account for large deformations without losing accuracy. However, issues with the accuracy of MPM's stress distributions and limits associated with the available boundary conditions impair its ability to predict liquefaction initiation. In this paper, we utilize a sequential hybridization of the FEM and MPM methods as a superior alternative to either individually. To demonstrate the efficacy of this hybrid method to simulate the entire process of tailings dam failures from initiation to runout, we model the 1978 Mochikoshi Tailings Dam failure.","sentences":["Tailings dams impound large amounts of saturated soil which can be highly susceptible to liquefaction.","Liquefaction results in a severe loss of strength in the retained soil and potentially failure of the dam.","If the dam is breached, a massive debris flow of liquefied soil is then released with potentially disastrous consequences downstream.","Numerical models are frequently utilized to predict the liquefaction response of tailings dams and the potential runout, and these analyses inform engineering decisions regarding hazard avoidance and mitigation.","The Finite Element Method (FEM) is a widespread tool which excels at modeling liquefaction triggering and initial movements, but it quickly loses accuracy when modeling large deformations due to mesh distortion.","Conversely, the Material Point Method (MPM), a hybrid Eulerian-Lagrangian method, employs particles that move freely across a background grid and can account for large deformations without losing accuracy.","However, issues with the accuracy of MPM's stress distributions and limits associated with the available boundary conditions impair its ability to predict liquefaction initiation.","In this paper, we utilize a sequential hybridization of the FEM and MPM methods as a superior alternative to either individually.","To demonstrate the efficacy of this hybrid method to simulate the entire process of tailings dam failures from initiation to runout, we model the 1978 Mochikoshi Tailings Dam failure."],"url":"http://arxiv.org/abs/2404.15860v1","category":"physics.geo-ph"}
{"created":"2024-04-24 13:14:09","title":"CONNECTION: COvert chaNnel NEtwork attaCk Through bIt-rate mOdulatioN","abstract":"Covert channel networks are a well-known method for circumventing the security measures organizations put in place to protect their networks from adversarial attacks. This paper introduces a novel method based on bit-rate modulation for implementing covert channels between devices connected over a wide area network. This attack can be exploited to exfiltrate sensitive information from a machine (i.e., covert sender) and stealthily transfer it to a covert receiver while evading network security measures and detection systems. We explain how to implement this threat, focusing specifically on covert channel networks and their potential security risks to network information transmission. The proposed method leverages bit-rate modulation, where a high bit rate represents a '1' and a low bit rate represents a '0', enabling covert communication. We analyze the key metrics associated with covert channels, including robustness in the presence of legitimate traffic and other interference, bit-rate capacity, and bit error rate. Experiments demonstrate the good performance of this attack, which achieved 5 bps with excellent robustness and a channel capacity of up to 0.9239 bps/Hz under different noise sources. Therefore, we show that bit-rate modulation effectively violates network security and compromises sensitive data.","sentences":["Covert channel networks are a well-known method for circumventing the security measures organizations put in place to protect their networks from adversarial attacks.","This paper introduces a novel method based on bit-rate modulation for implementing covert channels between devices connected over a wide area network.","This attack can be exploited to exfiltrate sensitive information from a machine (i.e., covert sender) and stealthily transfer it to a covert receiver while evading network security measures and detection systems.","We explain how to implement this threat, focusing specifically on covert channel networks and their potential security risks to network information transmission.","The proposed method leverages bit-rate modulation, where a high bit rate represents a '1' and a low bit rate represents a '0', enabling covert communication.","We analyze the key metrics associated with covert channels, including robustness in the presence of legitimate traffic and other interference, bit-rate capacity, and bit error rate.","Experiments demonstrate the good performance of this attack, which achieved 5 bps with excellent robustness and a channel capacity of up to 0.9239 bps/Hz under different noise sources.","Therefore, we show that bit-rate modulation effectively violates network security and compromises sensitive data."],"url":"http://arxiv.org/abs/2404.15858v1","category":"cs.CR"}
{"created":"2024-04-24 13:13:06","title":"Algebraic structure of the renormalization group in the renormalizable QFT theories","abstract":"We consider the group formed by finite renormalizations as an infinite-dimensional Lie group. It is demonstrated that for the finite renormalization of the gauge coupling constant its generators $\\hat L_n$ with $n\\ge 1$ satisfy the commutation relations of the Witt algebra and, therefore, form its subalgebra. The commutation relations are also written for the more general case when finite renormalizations are made for both the coupling constant and matter fields. We also construct the generator of the Abelian subgroup corresponding to the changes of the renormalization scale. The explicit expressions for the renormalization group generators are written in the case when they act on the $\\beta$-function and the anomalous dimension. It is explained how the finite changes of these functions under the finite renormalizations can be obtained with the help of the exponential map.","sentences":["We consider the group formed by finite renormalizations as an infinite-dimensional Lie group.","It is demonstrated that for the finite renormalization of the gauge coupling constant its generators $\\hat L_n$ with $n\\ge 1$ satisfy the commutation relations of the Witt algebra and, therefore, form its subalgebra.","The commutation relations are also written for the more general case when finite renormalizations are made for both the coupling constant and matter fields.","We also construct the generator of the Abelian subgroup corresponding to the changes of the renormalization scale.","The explicit expressions for the renormalization group generators are written in the case when they act on the $\\beta$-function and the anomalous dimension.","It is explained how the finite changes of these functions under the finite renormalizations can be obtained with the help of the exponential map."],"url":"http://arxiv.org/abs/2404.15856v1","category":"hep-th"}
{"created":"2024-04-24 13:11:51","title":"Taking Bi-Intuitionistic Logic First-Order: A Proof-Theoretic Investigation via Polytree Sequents","abstract":"It is well-known that extending the Hilbert axiomatic system for first-order intuitionistic logic with an exclusion operator, that is dual to implication, collapses the domains in the model into a constant domain. This makes it a very challenging problem to find a sound and complete proof system for first-order bi-intuitionistic logic with non-constant domains, that is also conservative over first-order intuitionistic logic. We solve this problem by presenting the first sound and complete proof system for first-order bi-intuitionistic logic with increasing domains. We formalize our proof system in a labeled polytree sequent calculus (a notational variant of nested sequents), and prove that it enjoys cut-elimination and is conservative over first-order intuitionistic logic. A key feature of our calculus is an explicit eigenvariable context, which allows us to control precisely the scope of free variables in a polytree structure. Semantically this context can be seen as encoding a notion of Scott's existence predicate for intuitionistic logic. This turns out to be crucial to avoid the collapse of domains and to prove the completeness of our proof system. The explicit consideration of the variable context in a formula sheds light on a previously overlooked dependency between the residuation principle and the existence predicate in the first-order setting, that may help explain the difficulty in obtaining a complete proof system for first-order bi-intuitionistic logic.","sentences":["It is well-known that extending the Hilbert axiomatic system for first-order intuitionistic logic with an exclusion operator, that is dual to implication, collapses the domains in the model into a constant domain.","This makes it a very challenging problem to find a sound and complete proof system for first-order bi-intuitionistic logic with non-constant domains, that is also conservative over first-order intuitionistic logic.","We solve this problem by presenting the first sound and complete proof system for first-order bi-intuitionistic logic with increasing domains.","We formalize our proof system in a labeled polytree sequent calculus (a notational variant of nested sequents), and prove that it enjoys cut-elimination and is conservative over first-order intuitionistic logic.","A key feature of our calculus is an explicit eigenvariable context, which allows us to control precisely the scope of free variables in a polytree structure.","Semantically this context can be seen as encoding a notion of Scott's existence predicate for intuitionistic logic.","This turns out to be crucial to avoid the collapse of domains and to prove the completeness of our proof system.","The explicit consideration of the variable context in a formula sheds light on a previously overlooked dependency between the residuation principle and the existence predicate in the first-order setting, that may help explain the difficulty in obtaining a complete proof system for first-order bi-intuitionistic logic."],"url":"http://arxiv.org/abs/2404.15855v1","category":"cs.LO"}
{"created":"2024-04-24 13:10:35","title":"CLAD: Robust Audio Deepfake Detection Against Manipulation Attacks with Contrastive Learning","abstract":"The increasing prevalence of audio deepfakes poses significant security threats, necessitating robust detection methods. While existing detection systems exhibit promise, their robustness against malicious audio manipulations remains underexplored. To bridge the gap, we undertake the first comprehensive study of the susceptibility of the most widely adopted audio deepfake detectors to manipulation attacks. Surprisingly, even manipulations like volume control can significantly bypass detection without affecting human perception. To address this, we propose CLAD (Contrastive Learning-based Audio deepfake Detector) to enhance the robustness against manipulation attacks. The key idea is to incorporate contrastive learning to minimize the variations introduced by manipulations, therefore enhancing detection robustness. Additionally, we incorporate a length loss, aiming to improve the detection accuracy by clustering real audios more closely in the feature space. We comprehensively evaluated the most widely adopted audio deepfake detection models and our proposed CLAD against various manipulation attacks. The detection models exhibited vulnerabilities, with FAR rising to 36.69%, 31.23%, and 51.28% under volume control, fading, and noise injection, respectively. CLAD enhanced robustness, reducing the FAR to 0.81% under noise injection and consistently maintaining an FAR below 1.63% across all tests. Our source code and documentation are available in the artifact repository (https://github.com/CLAD23/CLAD).","sentences":["The increasing prevalence of audio deepfakes poses significant security threats, necessitating robust detection methods.","While existing detection systems exhibit promise, their robustness against malicious audio manipulations remains underexplored.","To bridge the gap, we undertake the first comprehensive study of the susceptibility of the most widely adopted audio deepfake detectors to manipulation attacks.","Surprisingly, even manipulations like volume control can significantly bypass detection without affecting human perception.","To address this, we propose CLAD (Contrastive Learning-based Audio deepfake Detector) to enhance the robustness against manipulation attacks.","The key idea is to incorporate contrastive learning to minimize the variations introduced by manipulations, therefore enhancing detection robustness.","Additionally, we incorporate a length loss, aiming to improve the detection accuracy by clustering real audios more closely in the feature space.","We comprehensively evaluated the most widely adopted audio deepfake detection models and our proposed CLAD against various manipulation attacks.","The detection models exhibited vulnerabilities, with FAR rising to 36.69%, 31.23%, and 51.28% under volume control, fading, and noise injection, respectively.","CLAD enhanced robustness, reducing the FAR to 0.81% under noise injection and consistently maintaining an FAR below 1.63% across all tests.","Our source code and documentation are available in the artifact repository (https://github.com/CLAD23/CLAD)."],"url":"http://arxiv.org/abs/2404.15854v1","category":"cs.CR"}
{"created":"2024-04-24 12:59:54","title":"Porting Large Language Models to Mobile Devices for Question Answering","abstract":"Deploying Large Language Models (LLMs) on mobile devices makes all the capabilities of natural language processing available on the device. An important use case of LLMs is question answering, which can provide accurate and contextually relevant answers to a wide array of user queries. We describe how we managed to port state of the art LLMs to mobile devices, enabling them to operate natively on the device. We employ the llama.cpp framework, a flexible and self-contained C++ framework for LLM inference. We selected a 6-bit quantized version of the Orca-Mini-3B model with 3 billion parameters and present the correct prompt format for this model. Experimental results show that LLM inference runs in interactive speed on a Galaxy S21 smartphone and that the model delivers high-quality answers to user queries related to questions from different subjects like politics, geography or history.","sentences":["Deploying Large Language Models (LLMs) on mobile devices makes all the capabilities of natural language processing available on the device.","An important use case of LLMs is question answering, which can provide accurate and contextually relevant answers to a wide array of user queries.","We describe how we managed to port state of the art LLMs to mobile devices, enabling them to operate natively on the device.","We employ the llama.cpp framework, a flexible and self-contained C++ framework for LLM inference.","We selected a 6-bit quantized version of the Orca-Mini-3B model with 3 billion parameters and present the correct prompt format for this model.","Experimental results show that LLM inference runs in interactive speed on a Galaxy S21 smartphone and that the model delivers high-quality answers to user queries related to questions from different subjects like politics, geography or history."],"url":"http://arxiv.org/abs/2404.15851v1","category":"cs.CV"}
{"created":"2024-04-24 12:51:14","title":"From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models","abstract":"It is imperative for Large language models (LLMs) to follow instructions with elaborate requirements (i.e. Complex Instructions Following). Yet, it remains under-explored how to enhance the ability of LLMs to follow complex instructions with multiple constraints. To bridge the gap, we initially study what training data is effective in enhancing complex constraints following abilities. We found that training LLMs with instructions containing multiple constraints enhances their understanding of complex instructions, especially those with lower complexity levels. The improvement can even generalize to compositions of out-of-domain constraints. Additionally, we further propose methods addressing how to obtain and utilize the effective training data. Finally, we conduct extensive experiments to prove the effectiveness of our methods in terms of overall performance, training efficiency, and generalization abilities under four settings.","sentences":["It is imperative for Large language models (LLMs) to follow instructions with elaborate requirements (i.e. Complex Instructions Following).","Yet, it remains under-explored how to enhance the ability of LLMs to follow complex instructions with multiple constraints.","To bridge the gap, we initially study what training data is effective in enhancing complex constraints following abilities.","We found that training LLMs with instructions containing multiple constraints enhances their understanding of complex instructions, especially those with lower complexity levels.","The improvement can even generalize to compositions of out-of-domain constraints.","Additionally, we further propose methods addressing how to obtain and utilize the effective training data.","Finally, we conduct extensive experiments to prove the effectiveness of our methods in terms of overall performance, training efficiency, and generalization abilities under four settings."],"url":"http://arxiv.org/abs/2404.15846v1","category":"cs.CL"}
{"created":"2024-04-25 17:56:21","title":"ESG: Pipeline-Conscious Efficient Scheduling of DNN Workflows on Serverless Platforms with Shareable GPUs","abstract":"Recent years have witnessed increasing interest in machine learning inferences on serverless computing for its auto-scaling and cost effective properties. Existing serverless computing, however, lacks effective job scheduling methods to handle the schedule space dramatically expanded by GPU sharing, task batching, and inter-task relations. Prior solutions have dodged the issue by neglecting some important factors, leaving some large performance potential locked. This paper presents ESG, a new scheduling algorithm that directly addresses the difficulties. ESG treats sharable GPU as a first-order factor in scheduling. It employs an optimality-guided adaptive method by combining A*-search and a novel dual-blade pruning to dramatically prune the scheduling space without compromising the quality. It further introduces a novel method, dominator-based SLO distribution, to ensure the scalability of the scheduler. The results show that ESG can significantly improve the SLO hit rates 61%-80% while saving 47%-187% costs over prior work.","sentences":["Recent years have witnessed increasing interest in machine learning inferences on serverless computing for its auto-scaling and cost effective properties.","Existing serverless computing, however, lacks effective job scheduling methods to handle the schedule space dramatically expanded by GPU sharing, task batching, and inter-task relations.","Prior solutions have dodged the issue by neglecting some important factors, leaving some large performance potential locked.","This paper presents ESG, a new scheduling algorithm that directly addresses the difficulties.","ESG treats sharable GPU as a first-order factor in scheduling.","It employs an optimality-guided adaptive method by combining A*-search and a novel dual-blade pruning to dramatically prune the scheduling space without compromising the quality.","It further introduces a novel method, dominator-based SLO distribution, to ensure the scalability of the scheduler.","The results show that ESG can significantly improve the SLO hit rates 61%-80% while saving 47%-187% costs over prior work."],"url":"http://arxiv.org/abs/2404.16812v1","category":"cs.DC"}
{"created":"2024-04-25 17:24:35","title":"ConKeD++ -- Improving descriptor learning for retinal image registration: A comprehensive study of contrastive losses","abstract":"Self-supervised contrastive learning has emerged as one of the most successful deep learning paradigms. In this regard, it has seen extensive use in image registration and, more recently, in the particular field of medical image registration. In this work, we propose to test and extend and improve a state-of-the-art framework for color fundus image registration, ConKeD. Using the ConKeD framework we test multiple loss functions, adapting them to the framework and the application domain. Furthermore, we evaluate our models using the standarized benchmark dataset FIRE as well as several datasets that have never been used before for color fundus registration, for which we are releasing the pairing data as well as a standardized evaluation approach. Our work demonstrates state-of-the-art performance across all datasets and metrics demonstrating several advantages over current SOTA color fundus registration methods","sentences":["Self-supervised contrastive learning has emerged as one of the most successful deep learning paradigms.","In this regard, it has seen extensive use in image registration and, more recently, in the particular field of medical image registration.","In this work, we propose to test and extend and improve a state-of-the-art framework for color fundus image registration, ConKeD. Using the ConKeD framework we test multiple loss functions, adapting them to the framework and the application domain.","Furthermore, we evaluate our models using the standarized benchmark dataset FIRE as well as several datasets that have never been used before for color fundus registration, for which we are releasing the pairing data as well as a standardized evaluation approach.","Our work demonstrates state-of-the-art performance across all datasets and metrics demonstrating several advantages over current SOTA color fundus registration methods"],"url":"http://arxiv.org/abs/2404.16773v1","category":"cs.CV"}
{"created":"2024-04-25 17:09:14","title":"TokenHMR: Advancing Human Mesh Recovery with a Tokenized Pose Representation","abstract":"We address the problem of regressing 3D human pose and shape from a single image, with a focus on 3D accuracy. The current best methods leverage large datasets of 3D pseudo-ground-truth (p-GT) and 2D keypoints, leading to robust performance. With such methods, we observe a paradoxical decline in 3D pose accuracy with increasing 2D accuracy. This is caused by biases in the p-GT and the use of an approximate camera projection model. We quantify the error induced by current camera models and show that fitting 2D keypoints and p-GT accurately causes incorrect 3D poses. Our analysis defines the invalid distances within which minimizing 2D and p-GT losses is detrimental. We use this to formulate a new loss Threshold-Adaptive Loss Scaling (TALS) that penalizes gross 2D and p-GT losses but not smaller ones. With such a loss, there are many 3D poses that could equally explain the 2D evidence. To reduce this ambiguity we need a prior over valid human poses but such priors can introduce unwanted bias. To address this, we exploit a tokenized representation of human pose and reformulate the problem as token prediction. This restricts the estimated poses to the space of valid poses, effectively providing a uniform prior. Extensive experiments on the EMDB and 3DPW datasets show that our reformulated keypoint loss and tokenization allows us to train on in-the-wild data while improving 3D accuracy over the state-of-the-art. Our models and code are available for research at https://tokenhmr.is.tue.mpg.de.","sentences":["We address the problem of regressing 3D human pose and shape from a single image, with a focus on 3D accuracy.","The current best methods leverage large datasets of 3D pseudo-ground-truth (p-GT) and 2D keypoints, leading to robust performance.","With such methods, we observe a paradoxical decline in 3D pose accuracy with increasing 2D accuracy.","This is caused by biases in the p-GT and the use of an approximate camera projection model.","We quantify the error induced by current camera models and show that fitting 2D keypoints and p-GT accurately causes incorrect 3D poses.","Our analysis defines the invalid distances within which minimizing 2D and p-GT losses is detrimental.","We use this to formulate a new loss Threshold-Adaptive Loss Scaling (TALS) that penalizes gross 2D and p-GT losses but not smaller ones.","With such a loss, there are many 3D poses that could equally explain the 2D evidence.","To reduce this ambiguity we need a prior over valid human poses but such priors can introduce unwanted bias.","To address this, we exploit a tokenized representation of human pose and reformulate the problem as token prediction.","This restricts the estimated poses to the space of valid poses, effectively providing a uniform prior.","Extensive experiments on the EMDB and 3DPW datasets show that our reformulated keypoint loss and tokenization allows us to train on in-the-wild data while improving 3D accuracy over the state-of-the-art.","Our models and code are available for research at https://tokenhmr.is.tue.mpg.de."],"url":"http://arxiv.org/abs/2404.16752v1","category":"cs.CV"}
{"created":"2024-04-25 14:08:02","title":"Layered List Labeling","abstract":"The list-labeling problem is one of the most basic and well-studied algorithmic primitives in data structures, with an extensive literature spanning upper bounds, lower bounds, and data management applications. The classical algorithm for this problem, dating back to 1981, has amortized cost $O(\\log^2 n)$. Subsequent work has led to improvements in three directions: \\emph{low-latency} (worst-case) bounds; \\emph{high-throughput} (expected) bounds; and (adaptive) bounds for \\emph{important workloads}.   Perhaps surprisingly, these three directions of research have remained almost entirely disjoint -- this is because, so far, the techniques that allow for progress in one direction have forced worsening bounds in the others. Thus there would appear to be a tension between worst-case, adaptive, and expected bounds. List labeling has been proposed for use in databases at least as early as PODS'99, but a database needs good throughput, response time, and needs to adapt to common workloads (e.g., bulk loads), and no current list-labeling algorithm achieve good bounds for all three.   We show that this tension is not fundamental. In fact, with the help of new data-structural techniques, one can actually \\emph{combine} any three list-labeling solutions in order to cherry-pick the best worst-case, adaptive, and expected bounds from each of them.","sentences":["The list-labeling problem is one of the most basic and well-studied algorithmic primitives in data structures, with an extensive literature spanning upper bounds, lower bounds, and data management applications.","The classical algorithm for this problem, dating back to 1981, has amortized cost $O(\\log^2 n)$.","Subsequent work has led to improvements in three directions: \\emph{low-latency} (worst-case) bounds; \\emph{high-throughput} (expected) bounds; and (adaptive) bounds for \\emph{important workloads}.   ","Perhaps surprisingly, these three directions of research have remained almost entirely disjoint -- this is because, so far, the techniques that allow for progress in one direction have forced worsening bounds in the others.","Thus there would appear to be a tension between worst-case, adaptive, and expected bounds.","List labeling has been proposed for use in databases at least as early as PODS'99, but a database needs good throughput, response time, and needs to adapt to common workloads (e.g., bulk loads), and no current list-labeling algorithm achieve good bounds for all three.   ","We show that this tension is not fundamental.","In fact, with the help of new data-structural techniques, one can actually \\emph{combine} any three list-labeling solutions in order to cherry-pick the best worst-case, adaptive, and expected bounds from each of them."],"url":"http://arxiv.org/abs/2404.16623v1","category":"cs.DS"}
{"created":"2024-04-25 13:53:42","title":"Stochastic Dissipative Euler's equations for a free body","abstract":"Intrinsic thermal fluctuations within a real solid challenge the rigid body assumption that is central to Euler's equations for the motion of a free body. Recently, we have introduced a dissipative and stochastic version of Euler's equations in a thermodynamically consistent way (European Journal of Mechanics - A/Solids 103, 105184 (2024)). This framework describes the evolution of both orientation and shape of a free body, incorporating internal thermal fluctuations and their concomitant dissipative mechanisms. In the present work, we demonstrate that, in the absence of angular momentum, the theory predicts that principal axis unit vectors of a body undergo an anisotropic Brownian motion on the unit sphere, with the anisotropy arising from the body's varying moments of inertia. The resulting equilibrium time correlation function of the principal eigenvectors decays exponentially. This theoretical prediction is confirmed in molecular dynamics simulations of small bodies. The comparison of theory and equilibrium MD simulations allow us to measure the orientational diffusion tensor. We then use this information in the Stochastic Dissipative Euler's Equations, to describe a non-equilibrium situation of a body spinning around the unstable intermediate axis. The agreement between theory and simulations is excellent, offering a validation of the theoretical framework.","sentences":["Intrinsic thermal fluctuations within a real solid challenge the rigid body assumption that is central to Euler's equations for the motion of a free body.","Recently, we have introduced a dissipative and stochastic version of Euler's equations in a thermodynamically consistent way (European Journal of Mechanics - A/Solids 103, 105184 (2024)).","This framework describes the evolution of both orientation and shape of a free body, incorporating internal thermal fluctuations and their concomitant dissipative mechanisms.","In the present work, we demonstrate that, in the absence of angular momentum, the theory predicts that principal axis unit vectors of a body undergo an anisotropic Brownian motion on the unit sphere, with the anisotropy arising from the body's varying moments of inertia.","The resulting equilibrium time correlation function of the principal eigenvectors decays exponentially.","This theoretical prediction is confirmed in molecular dynamics simulations of small bodies.","The comparison of theory and equilibrium MD simulations allow us to measure the orientational diffusion tensor.","We then use this information in the Stochastic Dissipative Euler's Equations, to describe a non-equilibrium situation of a body spinning around the unstable intermediate axis.","The agreement between theory and simulations is excellent, offering a validation of the theoretical framework."],"url":"http://arxiv.org/abs/2404.16613v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-25 12:59:58","title":"Stability of Navier-Stokes equations with a free surface","abstract":"We consider the viscous incompressible fluids in a three-dimensional horizontally periodic domain bounded below by a fixed smooth boundary and above by a free moving surface. The fluid dynamics are governed by the Navier-Stokes equations with the effect of gravity and surface tension on the free surface. We develop a global well-posedness theory by a nonlinear energy method in low regular Sobolev spaces with several techniques, including: the horizontal energy-dissipation estimates, a new tripled bootstrap argument inspired by Guo and Tice [Arch. Ration. Mech. Anal.(2018)]. Moreover, the solution decays asymptotically to the equilibrium in an exponential rate.","sentences":["We consider the viscous incompressible fluids in a three-dimensional horizontally periodic domain bounded below by a fixed smooth boundary and above by a free moving surface.","The fluid dynamics are governed by the Navier-Stokes equations with the effect of gravity and surface tension on the free surface.","We develop a global well-posedness theory by a nonlinear energy method in low regular Sobolev spaces with several techniques, including: the horizontal energy-dissipation estimates, a new tripled bootstrap argument inspired by Guo and Tice","[Arch.","Ration.","Mech.","Anal.(2018)].","Moreover, the solution decays asymptotically to the equilibrium in an exponential rate."],"url":"http://arxiv.org/abs/2404.16585v1","category":"math.AP"}
{"created":"2024-04-25 12:02:20","title":"Image registration based automated lesion correspondence pipeline for longitudinal CT data","abstract":"Patients diagnosed with metastatic breast cancer (mBC) typically undergo several radiographic assessments during their treatment. mBC often involves multiple metastatic lesions in different organs, it is imperative to accurately track and assess these lesions to gain a comprehensive understanding of the disease's response to treatment. Computerized analysis methods that rely on lesion-level tracking have often used manual matching of corresponding lesions, a time-consuming process that is prone to errors. This paper introduces an automated lesion correspondence algorithm designed to precisely track both targets' lesions and non-targets' lesions in longitudinal data. Here we demonstrate the applicability of our algorithm on the anonymized data from two Phase III trials. The dataset contains imaging data of patients for different follow-up timepoints and the radiologist annotations for the patients enrolled in the trials. Target and non-target lesions are annotated by either one or two groups of radiologists. To facilitate accurate tracking, we have developed a registration-assisted lesion correspondence algorithm. The algorithm employs a sequential two-step pipeline: (a) Firstly, an adaptive Hungarian algorithm is used to establish correspondence among lesions within a single volumetric image series which have been annotated by multiple radiologists at a specific timepoint. (b) Secondly, after establishing correspondence and assigning unique names to the lesions, three-dimensional rigid registration is applied to various image series at the same timepoint. Registration is followed by ongoing lesion correspondence based on the adaptive Hungarian algorithm and updating lesion names for accurate tracking. Validation of our automated lesion correspondence algorithm is performed through triaxial plots based on axial, sagittal, and coronal views, confirming its efficacy in matching lesions.","sentences":["Patients diagnosed with metastatic breast cancer (mBC) typically undergo several radiographic assessments during their treatment.","mBC often involves multiple metastatic lesions in different organs, it is imperative to accurately track and assess these lesions to gain a comprehensive understanding of the disease's response to treatment.","Computerized analysis methods that rely on lesion-level tracking have often used manual matching of corresponding lesions, a time-consuming process that is prone to errors.","This paper introduces an automated lesion correspondence algorithm designed to precisely track both targets' lesions and non-targets' lesions in longitudinal data.","Here we demonstrate the applicability of our algorithm on the anonymized data from two Phase III trials.","The dataset contains imaging data of patients for different follow-up timepoints and the radiologist annotations for the patients enrolled in the trials.","Target and non-target lesions are annotated by either one or two groups of radiologists.","To facilitate accurate tracking, we have developed a registration-assisted lesion correspondence algorithm.","The algorithm employs a sequential two-step pipeline: (a) Firstly, an adaptive Hungarian algorithm is used to establish correspondence among lesions within a single volumetric image series which have been annotated by multiple radiologists at a specific timepoint.","(b) Secondly, after establishing correspondence and assigning unique names to the lesions, three-dimensional rigid registration is applied to various image series at the same timepoint.","Registration is followed by ongoing lesion correspondence based on the adaptive Hungarian algorithm and updating lesion names for accurate tracking.","Validation of our automated lesion correspondence algorithm is performed through triaxial plots based on axial, sagittal, and coronal views, confirming its efficacy in matching lesions."],"url":"http://arxiv.org/abs/2404.16544v1","category":"eess.IV"}
{"created":"2024-04-25 11:51:43","title":"Local training and enrichment based on a residual localization strategy","abstract":"To efficiently tackle parametrized multi and/or large scale problems, we propose an adaptive localized model order reduction framework combining both local offline training and local online enrichment with localized error control. For the latter, we adapt the residual localization strategy introduced in [Buhr, Engwer, Ohlberger, Rave, SIAM J. Sci. Comput., 2017] which allows to derive a localized a posteriori error estimator that can be employed to adaptively enrich the reduced solution space locally where needed. Numerical experiments demonstrate the potential of the proposed approach.","sentences":["To efficiently tackle parametrized multi and/or large scale problems, we propose an adaptive localized model order reduction framework combining both local offline training and local online enrichment with localized error control.","For the latter, we adapt the residual localization strategy introduced in [Buhr, Engwer, Ohlberger, Rave, SIAM J. Sci.","Comput., 2017] which allows to derive a localized a posteriori error estimator that can be employed to adaptively enrich the reduced solution space locally where needed.","Numerical experiments demonstrate the potential of the proposed approach."],"url":"http://arxiv.org/abs/2404.16537v1","category":"math.NA"}
{"created":"2024-04-25 07:55:47","title":"Promoting CNNs with Cross-Architecture Knowledge Distillation for Efficient Monocular Depth Estimation","abstract":"Recently, the performance of monocular depth estimation (MDE) has been significantly boosted with the integration of transformer models. However, the transformer models are usually computationally-expensive, and their effectiveness in light-weight models are limited compared to convolutions. This limitation hinders their deployment on resource-limited devices. In this paper, we propose a cross-architecture knowledge distillation method for MDE, dubbed DisDepth, to enhance efficient CNN models with the supervision of state-of-the-art transformer models. Concretely, we first build a simple framework of convolution-based MDE, which is then enhanced with a novel local-global convolution module to capture both local and global information in the image. To effectively distill valuable information from the transformer teacher and bridge the gap between convolution and transformer features, we introduce a method to acclimate the teacher with a ghost decoder. The ghost decoder is a copy of the student's decoder, and adapting the teacher with the ghost decoder aligns the features to be student-friendly while preserving their original performance. Furthermore, we propose an attentive knowledge distillation loss that adaptively identifies features valuable for depth estimation. This loss guides the student to focus more on attentive regions, improving its performance. Extensive experiments on KITTI and NYU Depth V2 datasets demonstrate the effectiveness of DisDepth. Our method achieves significant improvements on various efficient backbones, showcasing its potential for efficient monocular depth estimation.","sentences":["Recently, the performance of monocular depth estimation (MDE) has been significantly boosted with the integration of transformer models.","However, the transformer models are usually computationally-expensive, and their effectiveness in light-weight models are limited compared to convolutions.","This limitation hinders their deployment on resource-limited devices.","In this paper, we propose a cross-architecture knowledge distillation method for MDE, dubbed DisDepth, to enhance efficient CNN models with the supervision of state-of-the-art transformer models.","Concretely, we first build a simple framework of convolution-based MDE, which is then enhanced with a novel local-global convolution module to capture both local and global information in the image.","To effectively distill valuable information from the transformer teacher and bridge the gap between convolution and transformer features, we introduce a method to acclimate the teacher with a ghost decoder.","The ghost decoder is a copy of the student's decoder, and adapting the teacher with the ghost decoder aligns the features to be student-friendly while preserving their original performance.","Furthermore, we propose an attentive knowledge distillation loss that adaptively identifies features valuable for depth estimation.","This loss guides the student to focus more on attentive regions, improving its performance.","Extensive experiments on KITTI and NYU Depth V2 datasets demonstrate the effectiveness of DisDepth.","Our method achieves significant improvements on various efficient backbones, showcasing its potential for efficient monocular depth estimation."],"url":"http://arxiv.org/abs/2404.16386v1","category":"cs.CV"}
{"created":"2024-04-25 06:41:58","title":"An Improved Graph Pooling Network for Skeleton-Based Action Recognition","abstract":"Pooling is a crucial operation in computer vision, yet the unique structure of skeletons hinders the application of existing pooling strategies to skeleton graph modelling. In this paper, we propose an Improved Graph Pooling Network, referred to as IGPN. The main innovations include: Our method incorporates a region-awareness pooling strategy based on structural partitioning. The correlation matrix of the original feature is used to adaptively adjust the weight of information in different regions of the newly generated features, resulting in more flexible and effective processing. To prevent the irreversible loss of discriminative information, we propose a cross fusion module and an information supplement module to provide block-level and input-level information respectively. As a plug-and-play structure, the proposed operation can be seamlessly combined with existing GCN-based models. We conducted extensive evaluations on several challenging benchmarks, and the experimental results indicate the effectiveness of our proposed solutions. For example, in the cross-subject evaluation of the NTU-RGB+D 60 dataset, IGPN achieves a significant improvement in accuracy compared to the baseline while reducing Flops by nearly 70%; a heavier version has also been introduced to further boost accuracy.","sentences":["Pooling is a crucial operation in computer vision, yet the unique structure of skeletons hinders the application of existing pooling strategies to skeleton graph modelling.","In this paper, we propose an Improved Graph Pooling Network, referred to as IGPN.","The main innovations include: Our method incorporates a region-awareness pooling strategy based on structural partitioning.","The correlation matrix of the original feature is used to adaptively adjust the weight of information in different regions of the newly generated features, resulting in more flexible and effective processing.","To prevent the irreversible loss of discriminative information, we propose a cross fusion module and an information supplement module to provide block-level and input-level information respectively.","As a plug-and-play structure, the proposed operation can be seamlessly combined with existing GCN-based models.","We conducted extensive evaluations on several challenging benchmarks, and the experimental results indicate the effectiveness of our proposed solutions.","For example, in the cross-subject evaluation of the NTU-RGB+D 60 dataset, IGPN achieves a significant improvement in accuracy compared to the baseline while reducing Flops by nearly 70%; a heavier version has also been introduced to further boost accuracy."],"url":"http://arxiv.org/abs/2404.16359v1","category":"cs.CV"}
{"created":"2024-04-25 04:18:59","title":"DIG3D: Marrying Gaussian Splatting with Deformable Transformer for Single Image 3D Reconstruction","abstract":"In this paper, we study the problem of 3D reconstruction from a single-view RGB image and propose a novel approach called DIG3D for 3D object reconstruction and novel view synthesis. Our method utilizes an encoder-decoder framework which generates 3D Gaussians in decoder with the guidance of depth-aware image features from encoder. In particular, we introduce the use of deformable transformer, allowing efficient and effective decoding through 3D reference point and multi-layer refinement adaptations. By harnessing the benefits of 3D Gaussians, our approach offers an efficient and accurate solution for 3D reconstruction from single-view images. We evaluate our method on the ShapeNet SRN dataset, getting PSNR of 24.21 and 24.98 in car and chair dataset, respectively. The result outperforming the recent method by around 2.25%, demonstrating the effectiveness of our method in achieving superior results.","sentences":["In this paper, we study the problem of 3D reconstruction from a single-view RGB image and propose a novel approach called DIG3D for 3D object reconstruction and novel view synthesis.","Our method utilizes an encoder-decoder framework which generates 3D Gaussians in decoder with the guidance of depth-aware image features from encoder.","In particular, we introduce the use of deformable transformer, allowing efficient and effective decoding through 3D reference point and multi-layer refinement adaptations.","By harnessing the benefits of 3D Gaussians, our approach offers an efficient and accurate solution for 3D reconstruction from single-view images.","We evaluate our method on the ShapeNet SRN dataset, getting PSNR of 24.21 and 24.98 in car and chair dataset, respectively.","The result outperforming the recent method by around 2.25%, demonstrating the effectiveness of our method in achieving superior results."],"url":"http://arxiv.org/abs/2404.16323v1","category":"cs.CV"}
{"created":"2024-04-25 03:56:29","title":"The MAGPI Survey: Evolution of radial trends in star formation activity across cosmic time","abstract":"Using adaptive optics with the Multi-Unit Spectroscopic Explorer (MUSE) on the Very Large Telescope (VLT), the Middle Ages Galaxy Properties with Integral Field Spectroscopy (MAGPI) survey allows us to study the spatially resolved Universe at a crucial time of ~4 Gyr ago ($z$ ~ 0.3) when simulations predict the greatest diversity in evolutionary pathways for galaxies. We investigate the radial trends in the star formation (SF) activity and luminosity-weighted stellar ages as a function of offset from the star-forming main sequence (SFMS) for a total of 294 galaxies. Using both H$\\alpha$ emission and the 4000 Angstrom break (i.e., D4000) as star formation rate (SFR) tracers, we find overall flat radial profiles for galaxies lying on and above the SFMS, suggestive of physical processes that enhance/regulate SF throughout the entire galaxy disc. However, for galaxies lying below the SFMS, we find positive gradients in SF suggestive of inside-out quenching. Placing our results in context with results from other redshift regimes suggests an evolution in radial trends at $z$ ~ 0.3 for SF galaxies above the SFMS, from uniformly enhanced SF at $z$ ~ 1 and $z$ ~ 0.3 to centrally enhanced SF at $z$ ~ 0 (when averaged over a wide range of mass). We also capture higher local SFRs for galaxies below the SFMS compared to that of $z$ ~ 0, which can be explained by a larger population of quenched satellites in the local Universe and/or different treatments of limitations set by the D4000-sSFR relation.","sentences":["Using adaptive optics with the Multi-Unit Spectroscopic Explorer (MUSE) on the Very Large Telescope (VLT), the Middle Ages Galaxy Properties with Integral Field Spectroscopy (MAGPI) survey allows us to study the spatially resolved Universe at a crucial time of ~4 Gyr ago ($z$ ~ 0.3) when simulations predict the greatest diversity in evolutionary pathways for galaxies.","We investigate the radial trends in the star formation (SF) activity and luminosity-weighted stellar ages as a function of offset from the star-forming main sequence (SFMS) for a total of 294 galaxies.","Using both H$\\alpha$ emission and the 4000 Angstrom break (i.e., D4000) as star formation rate (SFR) tracers, we find overall flat radial profiles for galaxies lying on and above the SFMS, suggestive of physical processes that enhance/regulate SF throughout the entire galaxy disc.","However, for galaxies lying below the SFMS, we find positive gradients in SF suggestive of inside-out quenching.","Placing our results in context with results from other redshift regimes suggests an evolution in radial trends at $z$ ~ 0.3 for SF galaxies above the SFMS, from uniformly enhanced SF at $z$ ~ 1 and $z$ ~ 0.3 to centrally enhanced SF at $z$ ~ 0","(when averaged over a wide range of mass).","We also capture higher local SFRs for galaxies below the SFMS compared to that of $z$ ~ 0, which can be explained by a larger population of quenched satellites in the local Universe and/or different treatments of limitations set by the D4000-sSFR relation."],"url":"http://arxiv.org/abs/2404.16319v1","category":"astro-ph.GA"}
{"created":"2024-04-25 03:22:48","title":"Boosting Model Resilience via Implicit Adversarial Data Augmentation","abstract":"Data augmentation plays a pivotal role in enhancing and diversifying training data. Nonetheless, consistently improving model performance in varied learning scenarios, especially those with inherent data biases, remains challenging. To address this, we propose to augment the deep features of samples by incorporating their adversarial and anti-adversarial perturbation distributions, enabling adaptive adjustment in the learning difficulty tailored to each sample's specific characteristics. We then theoretically reveal that our augmentation process approximates the optimization of a surrogate loss function as the number of augmented copies increases indefinitely. This insight leads us to develop a meta-learning-based framework for optimizing classifiers with this novel loss, introducing the effects of augmentation while bypassing the explicit augmentation process. We conduct extensive experiments across four common biased learning scenarios: long-tail learning, generalized long-tail learning, noisy label learning, and subpopulation shift learning. The empirical results demonstrate that our method consistently achieves state-of-the-art performance, highlighting its broad adaptability.","sentences":["Data augmentation plays a pivotal role in enhancing and diversifying training data.","Nonetheless, consistently improving model performance in varied learning scenarios, especially those with inherent data biases, remains challenging.","To address this, we propose to augment the deep features of samples by incorporating their adversarial and anti-adversarial perturbation distributions, enabling adaptive adjustment in the learning difficulty tailored to each sample's specific characteristics.","We then theoretically reveal that our augmentation process approximates the optimization of a surrogate loss function as the number of augmented copies increases indefinitely.","This insight leads us to develop a meta-learning-based framework for optimizing classifiers with this novel loss, introducing the effects of augmentation while bypassing the explicit augmentation process.","We conduct extensive experiments across four common biased learning scenarios: long-tail learning, generalized long-tail learning, noisy label learning, and subpopulation shift learning.","The empirical results demonstrate that our method consistently achieves state-of-the-art performance, highlighting its broad adaptability."],"url":"http://arxiv.org/abs/2404.16307v1","category":"cs.LG"}
{"created":"2024-04-25 01:12:31","title":"Velocity-Based Monte Carlo Fluids","abstract":"We present a velocity-based Monte Carlo fluid solver that overcomes the limitations of its existing vorticity-based counterpart. Because the velocity-based formulation is more commonly used in graphics, our Monte Carlo solver can be readily extended with various techniques from the fluid simulation literature. We derive our method by solving the Navier-Stokes equations via operator splitting and designing a pointwise Monte Carlo estimator for each substep. We reformulate the projection and diffusion steps as integration problems based on the recently introduced walk-on-boundary technique [Sugimoto et al. 2023]. We transform the volume integral arising from the source term of the pressure Poisson equation into a form more amenable to practical numerical evaluation. Our resulting velocity-based formulation allows for the proper simulation of scenes that the prior vorticity-based Monte Carlo method [Rioux-Lavoie and Sugimoto et al. 2022] either simulates incorrectly or cannot support. We demonstrate that our method can easily incorporate advancements drawn from conventional non-Monte Carlo methods by showing how one can straightforwardly add buoyancy effects, divergence control capabilities, and numerical dissipation reduction methods, such as advection-reflection and PIC/FLIP methods.","sentences":["We present a velocity-based Monte Carlo fluid solver that overcomes the limitations of its existing vorticity-based counterpart.","Because the velocity-based formulation is more commonly used in graphics, our Monte Carlo solver can be readily extended with various techniques from the fluid simulation literature.","We derive our method by solving the Navier-Stokes equations via operator splitting and designing a pointwise Monte Carlo estimator for each substep.","We reformulate the projection and diffusion steps as integration problems based on the recently introduced walk-on-boundary technique [Sugimoto et al. 2023].","We transform the volume integral arising from the source term of the pressure Poisson equation into a form more amenable to practical numerical evaluation.","Our resulting velocity-based formulation allows for the proper simulation of scenes that the prior vorticity-based Monte Carlo method","[Rioux-Lavoie and Sugimoto et al. 2022]","either simulates incorrectly or cannot support.","We demonstrate that our method can easily incorporate advancements drawn from conventional non-Monte Carlo methods by showing how one can straightforwardly add buoyancy effects, divergence control capabilities, and numerical dissipation reduction methods, such as advection-reflection and PIC/FLIP methods."],"url":"http://arxiv.org/abs/2404.16274v1","category":"cs.GR"}
{"created":"2024-04-25 00:34:52","title":"Lacunarity Pooling Layers for Plant Image Classification using Texture Analysis","abstract":"Pooling layers (e.g., max and average) may overlook important information encoded in the spatial arrangement of pixel intensity and/or feature values. We propose a novel lacunarity pooling layer that aims to capture the spatial heterogeneity of the feature maps by evaluating the variability within local windows. The layer operates at multiple scales, allowing the network to adaptively learn hierarchical features. The lacunarity pooling layer can be seamlessly integrated into any artificial neural network architecture. Experimental results demonstrate the layer's effectiveness in capturing intricate spatial patterns, leading to improved feature extraction capabilities. The proposed approach holds promise in various domains, especially in agricultural image analysis tasks. This work contributes to the evolving landscape of artificial neural network architectures by introducing a novel pooling layer that enriches the representation of spatial features. Our code is publicly available.","sentences":["Pooling layers (e.g., max and average) may overlook important information encoded in the spatial arrangement of pixel intensity and/or feature values.","We propose a novel lacunarity pooling layer that aims to capture the spatial heterogeneity of the feature maps by evaluating the variability within local windows.","The layer operates at multiple scales, allowing the network to adaptively learn hierarchical features.","The lacunarity pooling layer can be seamlessly integrated into any artificial neural network architecture.","Experimental results demonstrate the layer's effectiveness in capturing intricate spatial patterns, leading to improved feature extraction capabilities.","The proposed approach holds promise in various domains, especially in agricultural image analysis tasks.","This work contributes to the evolving landscape of artificial neural network architectures by introducing a novel pooling layer that enriches the representation of spatial features.","Our code is publicly available."],"url":"http://arxiv.org/abs/2404.16268v1","category":"cs.CV"}
{"created":"2024-04-25 00:13:00","title":"Interpreting Answers to Yes-No Questions in Dialogues from Multiple Domains","abstract":"People often answer yes-no questions without explicitly saying yes, no, or similar polar keywords. Figuring out the meaning of indirect answers is challenging, even for large language models. In this paper, we investigate this problem working with dialogues from multiple domains. We present new benchmarks in three diverse domains: movie scripts, tennis interviews, and airline customer service. We present an approach grounded on distant supervision and blended training to quickly adapt to a new dialogue domain. Experimental results show that our approach is never detrimental and yields F1 improvements as high as 11-34%.","sentences":["People often answer yes-no questions without explicitly saying yes, no, or similar polar keywords.","Figuring out the meaning of indirect answers is challenging, even for large language models.","In this paper, we investigate this problem working with dialogues from multiple domains.","We present new benchmarks in three diverse domains: movie scripts, tennis interviews, and airline customer service.","We present an approach grounded on distant supervision and blended training to quickly adapt to a new dialogue domain.","Experimental results show that our approach is never detrimental and yields F1 improvements as high as 11-34%."],"url":"http://arxiv.org/abs/2404.16262v1","category":"cs.CL"}
{"created":"2024-04-24 23:53:48","title":"On standing wave in perturbed anti-de Sitter spacetimes with a naked singularity","abstract":"In the framework of black hole perturbation theory, this work investigates the standing wave solutions in Reissner-Nordtsr\\\"om (RN) anti-de Sitter (AdS) spacetimes with a naked singularity. These solutions can be viewed as a specific class of quasinormal modes exhibiting distinct characteristics. The imaginary parts of their frequencies are numerically vanishing, allowing them to persist over an extended period. Besides, these modes are predominantly stationary in terms of the evolution of spacetime waveforms. The numerical calculations are carried out employing the finite difference method, and the quasinormal frequencies extracted by the Prony method are shown to be consistent with those obtained using the matrix method. The obtained waveforms and quasinormal frequencies are shown to be drastically different from those of an extreme RN-AdS black hole. As the quasinormal modes are primarily dissipative, the non-dissipative standing waves are attributed to the nature that the singularity can neither be a sink nor a source of the gravitational system.","sentences":["In the framework of black hole perturbation theory, this work investigates the standing wave solutions in Reissner-Nordtsr\\\"om (RN) anti-de Sitter (AdS) spacetimes with a naked singularity.","These solutions can be viewed as a specific class of quasinormal modes exhibiting distinct characteristics.","The imaginary parts of their frequencies are numerically vanishing, allowing them to persist over an extended period.","Besides, these modes are predominantly stationary in terms of the evolution of spacetime waveforms.","The numerical calculations are carried out employing the finite difference method, and the quasinormal frequencies extracted by the Prony method are shown to be consistent with those obtained using the matrix method.","The obtained waveforms and quasinormal frequencies are shown to be drastically different from those of an extreme RN-AdS black hole.","As the quasinormal modes are primarily dissipative, the non-dissipative standing waves are attributed to the nature that the singularity can neither be a sink nor a source of the gravitational system."],"url":"http://arxiv.org/abs/2404.16254v1","category":"gr-qc"}
{"created":"2024-04-24 21:21:50","title":"An Analysis of Recent Advances in Deepfake Image Detection in an Evolving Threat Landscape","abstract":"Deepfake or synthetic images produced using deep generative models pose serious risks to online platforms. This has triggered several research efforts to accurately detect deepfake images, achieving excellent performance on publicly available deepfake datasets. In this work, we study 8 state-of-the-art detectors and argue that they are far from being ready for deployment due to two recent developments. First, the emergence of lightweight methods to customize large generative models, can enable an attacker to create many customized generators (to create deepfakes), thereby substantially increasing the threat surface. We show that existing defenses fail to generalize well to such \\emph{user-customized generative models} that are publicly available today. We discuss new machine learning approaches based on content-agnostic features, and ensemble modeling to improve generalization performance against user-customized models. Second, the emergence of \\textit{vision foundation models} -- machine learning models trained on broad data that can be easily adapted to several downstream tasks -- can be misused by attackers to craft adversarial deepfakes that can evade existing defenses. We propose a simple adversarial attack that leverages existing foundation models to craft adversarial samples \\textit{without adding any adversarial noise}, through careful semantic manipulation of the image content. We highlight the vulnerabilities of several defenses against our attack, and explore directions leveraging advanced foundation models and adversarial training to defend against this new threat.","sentences":["Deepfake or synthetic images produced using deep generative models pose serious risks to online platforms.","This has triggered several research efforts to accurately detect deepfake images, achieving excellent performance on publicly available deepfake datasets.","In this work, we study 8 state-of-the-art detectors and argue that they are far from being ready for deployment due to two recent developments.","First, the emergence of lightweight methods to customize large generative models, can enable an attacker to create many customized generators (to create deepfakes), thereby substantially increasing the threat surface.","We show that existing defenses fail to generalize well to such \\emph{user-customized generative models} that are publicly available today.","We discuss new machine learning approaches based on content-agnostic features, and ensemble modeling to improve generalization performance against user-customized models.","Second, the emergence of \\textit{vision foundation models} -- machine learning models trained on broad data that can be easily adapted to several downstream tasks -- can be misused by attackers to craft adversarial deepfakes that can evade existing defenses.","We propose a simple adversarial attack that leverages existing foundation models to craft adversarial samples \\textit{without adding any adversarial noise}, through careful semantic manipulation of the image content.","We highlight the vulnerabilities of several defenses against our attack, and explore directions leveraging advanced foundation models and adversarial training to defend against this new threat."],"url":"http://arxiv.org/abs/2404.16212v1","category":"cs.CR"}
{"created":"2024-04-24 20:58:50","title":"Entanglement-Based Artificial Topology: Neighboring Remote Network Nodes","abstract":"Entanglement is unanimously recognized as the key communication resource of the Quantum Internet. Yet, the possibility of implementing novel network functionalities by exploiting the marvels of entanglement has been poorly investigated so far, by mainly restricting the attention to bipartite entanglement. Conversely, in this paper, we aim at exploiting multipartite entanglement as inter-network resource. Specifically, we consider the interconnection of different Quantum Local Area Networks (QLANs), and we show that multipartite entanglement allows to dynamically generate an inter-QLAN artificial topology, by means of local operations only, that overcomes the limitations of the physical QLAN topologies. To this aim, we first design the multipartite entangled state to be distributed within each QLAN. Then, we show how such a state can be engineered to: i) interconnect nodes belonging to different QLANs, and ii) dynamically adapt to different inter-QLAN traffic patterns. Our contribution aims at providing the network engineering community with a hands-on guideline towards the concept of artificial topology and artificial neighborhood.","sentences":["Entanglement is unanimously recognized as the key communication resource of the Quantum Internet.","Yet, the possibility of implementing novel network functionalities by exploiting the marvels of entanglement has been poorly investigated so far, by mainly restricting the attention to bipartite entanglement.","Conversely, in this paper, we aim at exploiting multipartite entanglement as inter-network resource.","Specifically, we consider the interconnection of different Quantum Local Area Networks (QLANs), and we show that multipartite entanglement allows to dynamically generate an inter-QLAN artificial topology, by means of local operations only, that overcomes the limitations of the physical QLAN topologies.","To this aim, we first design the multipartite entangled state to be distributed within each QLAN.","Then, we show how such a state can be engineered to: i) interconnect nodes belonging to different QLANs, and ii) dynamically adapt to different inter-QLAN traffic patterns.","Our contribution aims at providing the network engineering community with a hands-on guideline towards the concept of artificial topology and artificial neighborhood."],"url":"http://arxiv.org/abs/2404.16204v1","category":"quant-ph"}
{"created":"2024-04-24 20:31:15","title":"Fusion of Domain-Adapted Vision and Language Models for Medical Visual Question Answering","abstract":"Vision-language models, while effective in general domains and showing strong performance in diverse multi-modal applications like visual question-answering (VQA), struggle to maintain the same level of effectiveness in more specialized domains, e.g., medical. We propose a medical vision-language model that integrates large vision and language models adapted for the medical domain. This model goes through three stages of parameter-efficient training using three separate biomedical and radiology multi-modal visual and text datasets. The proposed model achieves state-of-the-art performance on the SLAKE 1.0 medical VQA (MedVQA) dataset with an overall accuracy of 87.5% and demonstrates strong performance on another MedVQA dataset, VQA-RAD, achieving an overall accuracy of 73.2%.","sentences":["Vision-language models, while effective in general domains and showing strong performance in diverse multi-modal applications like visual question-answering (VQA), struggle to maintain the same level of effectiveness in more specialized domains, e.g., medical.","We propose a medical vision-language model that integrates large vision and language models adapted for the medical domain.","This model goes through three stages of parameter-efficient training using three separate biomedical and radiology multi-modal visual and text datasets.","The proposed model achieves state-of-the-art performance on the SLAKE 1.0 medical VQA (MedVQA) dataset with an overall accuracy of 87.5% and demonstrates strong performance on another MedVQA dataset, VQA-RAD, achieving an overall accuracy of 73.2%."],"url":"http://arxiv.org/abs/2404.16192v1","category":"cs.CL"}
{"created":"2024-04-24 19:20:15","title":"A Comparative Analysis of Adversarial Robustness for Quantum and Classical Machine Learning Models","abstract":"Quantum machine learning (QML) continues to be an area of tremendous interest from research and industry. While QML models have been shown to be vulnerable to adversarial attacks much in the same manner as classical machine learning models, it is still largely unknown how to compare adversarial attacks on quantum versus classical models. In this paper, we show how to systematically investigate the similarities and differences in adversarial robustness of classical and quantum models using transfer attacks, perturbation patterns and Lipschitz bounds. More specifically, we focus on classification tasks on a handcrafted dataset that allows quantitative analysis for feature attribution. This enables us to get insight, both theoretically and experimentally, on the robustness of classification networks. We start by comparing typical QML model architectures such as amplitude and re-upload encoding circuits with variational parameters to a classical ConvNet architecture. Next, we introduce a classical approximation of QML circuits (originally obtained with Random Fourier Features sampling but adapted in this work to fit a trainable encoding) and evaluate this model, denoted Fourier network, in comparison to other architectures. Our findings show that this Fourier network can be seen as a \"middle ground\" on the quantum-classical boundary. While adversarial attacks successfully transfer across this boundary in both directions, we also show that regularization helps quantum networks to be more robust, which has direct impact on Lipschitz bounds and transfer attacks.","sentences":["Quantum machine learning (QML) continues to be an area of tremendous interest from research and industry.","While QML models have been shown to be vulnerable to adversarial attacks much in the same manner as classical machine learning models, it is still largely unknown how to compare adversarial attacks on quantum versus classical models.","In this paper, we show how to systematically investigate the similarities and differences in adversarial robustness of classical and quantum models using transfer attacks, perturbation patterns and Lipschitz bounds.","More specifically, we focus on classification tasks on a handcrafted dataset that allows quantitative analysis for feature attribution.","This enables us to get insight, both theoretically and experimentally, on the robustness of classification networks.","We start by comparing typical QML model architectures such as amplitude and re-upload encoding circuits with variational parameters to a classical ConvNet architecture.","Next, we introduce a classical approximation of QML circuits (originally obtained with Random Fourier Features sampling but adapted in this work to fit a trainable encoding) and evaluate this model, denoted Fourier network, in comparison to other architectures.","Our findings show that this Fourier network can be seen as a \"middle ground\" on the quantum-classical boundary.","While adversarial attacks successfully transfer across this boundary in both directions, we also show that regularization helps quantum networks to be more robust, which has direct impact on Lipschitz bounds and transfer attacks."],"url":"http://arxiv.org/abs/2404.16154v1","category":"cs.LG"}
{"created":"2024-04-24 19:10:54","title":"$\u0394$ADAPT-VQE: Toward Accurate Calculation of Excitation Energies on Quantum Computers for BODIPY Molecules With Application in Photodynamic Therapy","abstract":"Quantum chemistry simulations offer a cost-effective way for computational design of BODIPY photosensitizers with potential use in photodynamic therapy (PDT). However, accurate predictions of photophysical properties, such as excitation energies, pose a challenge for the popular time-dependent density functional theory (TDDFT) and equation-of-motion coupled cluster with singles and doubles (EOM-CCSD) methods. By contrast, reliable descriptions can be achieved by multi-reference quantum chemistry methods, though unfortunately, their computational cost grows exponentially with the number of correlated electrons. Alternatively, quantum computing holds a great potential for exact simulation of photophysical properties in a computationally more efficient way. To this end, we introduce the state-specific $\\Delta$UCCSD-VQE (unitary coupled cluster with singles and doubles variational quantum eigensolver) and $\\Delta$ADAPT-VQE methods in which the electronically excited state is calculated via a non-Aufbau electronic configuration. The accuracy and capability of the developed methods are assessed against experimentally determined excitation energies for six BODIPY-derivatives. We show that the proposed methods predict accurate vertical excitation energies that are not only in good agreement with experimental reference data but also outperform popular quantum chemistry methods, such as TDDFT and EOM-CCSD. Spurred by its impressive performance and simplicity, we are confident that $\\Delta$ADAPT will emerge as the method of choice for guiding the rational design of photosensitizers for PDT and photocatalysis in the era of near-term quantum computing.","sentences":["Quantum chemistry simulations offer a cost-effective way for computational design of BODIPY photosensitizers with potential use in photodynamic therapy (PDT).","However, accurate predictions of photophysical properties, such as excitation energies, pose a challenge for the popular time-dependent density functional theory (TDDFT) and equation-of-motion coupled cluster with singles and doubles (EOM-CCSD) methods.","By contrast, reliable descriptions can be achieved by multi-reference quantum chemistry methods, though unfortunately, their computational cost grows exponentially with the number of correlated electrons.","Alternatively, quantum computing holds a great potential for exact simulation of photophysical properties in a computationally more efficient way.","To this end, we introduce the state-specific $\\Delta$UCCSD-VQE (unitary coupled cluster with singles and doubles variational quantum eigensolver) and $\\Delta$ADAPT-VQE methods in which the electronically excited state is calculated via a non-Aufbau electronic configuration.","The accuracy and capability of the developed methods are assessed against experimentally determined excitation energies for six BODIPY-derivatives.","We show that the proposed methods predict accurate vertical excitation energies that are not only in good agreement with experimental reference data but also outperform popular quantum chemistry methods, such as TDDFT and EOM-CCSD.","Spurred by its impressive performance and simplicity, we are confident that $\\Delta$ADAPT will emerge as the method of choice for guiding the rational design of photosensitizers for PDT and photocatalysis in the era of near-term quantum computing."],"url":"http://arxiv.org/abs/2404.16149v1","category":"physics.chem-ph"}
{"created":"2024-04-24 18:57:30","title":"A Survey on Intermediate Fusion Methods for Collaborative Perception Categorized by Real World Challenges","abstract":"This survey analyzes intermediate fusion methods in collaborative perception for autonomous driving, categorized by real-world challenges. We examine various methods, detailing their features and the evaluation metrics they employ. The focus is on addressing challenges like transmission efficiency, localization errors, communication disruptions, and heterogeneity. Moreover, we explore strategies to counter adversarial attacks and defenses, as well as approaches to adapt to domain shifts. The objective is to present an overview of how intermediate fusion methods effectively meet these diverse challenges, highlighting their role in advancing the field of collaborative perception in autonomous driving.","sentences":["This survey analyzes intermediate fusion methods in collaborative perception for autonomous driving, categorized by real-world challenges.","We examine various methods, detailing their features and the evaluation metrics they employ.","The focus is on addressing challenges like transmission efficiency, localization errors, communication disruptions, and heterogeneity.","Moreover, we explore strategies to counter adversarial attacks and defenses, as well as approaches to adapt to domain shifts.","The objective is to present an overview of how intermediate fusion methods effectively meet these diverse challenges, highlighting their role in advancing the field of collaborative perception in autonomous driving."],"url":"http://arxiv.org/abs/2404.16139v1","category":"cs.CV"}
{"created":"2024-04-24 18:49:37","title":"3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement","abstract":"In the field of 3D Human Pose Estimation (HPE), accurately estimating human pose, especially in scenarios with occlusions, is a significant challenge. This work identifies and addresses a gap in the current state of the art in 3D HPE concerning the scarcity of data and strategies for handling occlusions. We introduce our novel BlendMimic3D dataset, designed to mimic real-world situations where occlusions occur for seamless integration in 3D HPE algorithms. Additionally, we propose a 3D pose refinement block, employing a Graph Convolutional Network (GCN) to enhance pose representation through a graph model. This GCN block acts as a plug-and-play solution, adaptable to various 3D HPE frameworks without requiring retraining them. By training the GCN with occluded data from BlendMimic3D, we demonstrate significant improvements in resolving occluded poses, with comparable results for non-occluded ones. Project web page is available at https://blendmimic3d.github.io/BlendMimic3D/.","sentences":["In the field of 3D Human Pose Estimation (HPE), accurately estimating human pose, especially in scenarios with occlusions, is a significant challenge.","This work identifies and addresses a gap in the current state of the art in 3D HPE concerning the scarcity of data and strategies for handling occlusions.","We introduce our novel BlendMimic3D dataset, designed to mimic real-world situations where occlusions occur for seamless integration in 3D HPE algorithms.","Additionally, we propose a 3D pose refinement block, employing a Graph Convolutional Network (GCN) to enhance pose representation through a graph model.","This GCN block acts as a plug-and-play solution, adaptable to various 3D HPE frameworks without requiring retraining them.","By training the GCN with occluded data from BlendMimic3D, we demonstrate significant improvements in resolving occluded poses, with comparable results for non-occluded ones.","Project web page is available at https://blendmimic3d.github.io/BlendMimic3D/."],"url":"http://arxiv.org/abs/2404.16136v1","category":"cs.CV"}
{"created":"2024-04-24 18:49:07","title":"Performant near-term quantum combinatorial optimization","abstract":"We present a variational quantum algorithm for solving combinatorial optimization problems with linear-depth circuits. Our algorithm uses an ansatz composed of Hamiltonian generators designed to control each term in the target combinatorial function, along with parameter updates following a modified version of quantum imaginary time evolution. We evaluate this ansatz in numerical simulations that target solutions to the MAXCUT problem. The state evolution is shown to closely mimic imaginary time evolution, and its optimal-solution convergence is further improved using adaptive transformations of the classical Hamiltonian spectrum, while resources are minimized by pruning optimized gates that are close to the identity. With these innovations, the algorithm consistently converges to optimal solutions, with interesting highly-entangled dynamics along the way. This performant and resource-minimal approach is a promising candidate for potential quantum computational advantages on near-term quantum computing hardware.","sentences":["We present a variational quantum algorithm for solving combinatorial optimization problems with linear-depth circuits.","Our algorithm uses an ansatz composed of Hamiltonian generators designed to control each term in the target combinatorial function, along with parameter updates following a modified version of quantum imaginary time evolution.","We evaluate this ansatz in numerical simulations that target solutions to the MAXCUT problem.","The state evolution is shown to closely mimic imaginary time evolution, and its optimal-solution convergence is further improved using adaptive transformations of the classical Hamiltonian spectrum, while resources are minimized by pruning optimized gates that are close to the identity.","With these innovations, the algorithm consistently converges to optimal solutions, with interesting highly-entangled dynamics along the way.","This performant and resource-minimal approach is a promising candidate for potential quantum computational advantages on near-term quantum computing hardware."],"url":"http://arxiv.org/abs/2404.16135v1","category":"quant-ph"}
{"created":"2024-04-24 18:40:27","title":"Implementation of Immersed Boundaries through Volume Penalization in the Industrial Aeronautical Solver CODA","abstract":"In this work, we present the implementation and validation of the immersed boundary volume penalization methods in the CFD solver CODA. Our goal is the modelling and simulation of turbulent fluid flowsin complex 3D aerodynamic configuration through the numerical solution of the Reynolds-Averaged Navier-Stokes equations using the Spalart-Allmaras turbulent model. To do that, an efficient immerse boundary method, together with an efficient preprocessing tool for the construction of unstructured hexahedral meshes with adaptive mesh refinement around immersed geometries has been developed. We report several numerical computations, including subsonic flow past a NACA0012 airfoil, subsonic flow past an MDA30P30N multi-element airfoil and subsonic flow around the NASA high-lift CRM aircraft. These simulations have been performed in the CODA solver with a second-order finite volume scheme as spatial discretization and an implicit backward-Euler scheme based on the matrix-free GMRES block-Jacobi iterative method. The reported numerical simulations are in very good agreement with their corresponding experimental data.","sentences":["In this work, we present the implementation and validation of the immersed boundary volume penalization methods in the CFD solver CODA.","Our goal is the modelling and simulation of turbulent fluid flowsin complex 3D aerodynamic configuration through the numerical solution of the Reynolds-Averaged Navier-Stokes equations using the Spalart-Allmaras turbulent model.","To do that, an efficient immerse boundary method, together with an efficient preprocessing tool for the construction of unstructured hexahedral meshes with adaptive mesh refinement around immersed geometries has been developed.","We report several numerical computations, including subsonic flow past a NACA0012 airfoil, subsonic flow past an MDA30P30N multi-element airfoil and subsonic flow around the NASA high-lift CRM aircraft.","These simulations have been performed in the CODA solver with a second-order finite volume scheme as spatial discretization and an implicit backward-Euler scheme based on the matrix-free GMRES block-Jacobi iterative method.","The reported numerical simulations are in very good agreement with their corresponding experimental data."],"url":"http://arxiv.org/abs/2404.16132v1","category":"physics.flu-dyn"}
{"created":"2024-04-24 18:00:00","title":"Constant-depth preparation of matrix product states with adaptive quantum circuits","abstract":"Adaptive quantum circuits, which combine local unitary gates, midcircuit measurements, and feedforward operations, have recently emerged as a promising avenue for efficient state preparation, particularly on near-term quantum devices limited to shallow-depth circuits. Matrix product states (MPS) comprise a significant class of many-body entangled states, efficiently describing the ground states of one-dimensional gapped local Hamiltonians and finding applications in a number of recent quantum algorithms. Recently, it was shown that the AKLT state -- a paradigmatic example of an MPS -- can be exactly prepared with an adaptive quantum circuit of constant-depth, an impossible feat with local unitary gates due to its nonzero correlation length [Smith et al., PRX Quantum 4, 020315 (2023)]. In this work, we broaden the scope of this approach and demonstrate that a diverse class of MPS can be exactly prepared using constant-depth adaptive quantum circuits, outperforming optimal preparation protocols that rely on unitary circuits alone. We show that this class includes short- and long-ranged entangled MPS, symmetry-protected topological (SPT) and symmetry-broken states, MPS with finite Abelian, non-Abelian, and continuous symmetries, resource states for MBQC, and families of states with tunable correlation length. Moreover, we illustrate the utility of our framework for designing constant-depth sampling protocols, such as for random MPS or for generating MPS in a particular SPT phase. We present sufficient conditions for particular MPS to be preparable in constant time, with global on-site symmetry playing a pivotal role. Altogether, this work demonstrates the immense promise of adaptive quantum circuits for efficiently preparing many-body entangled states and provides explicit algorithms that outperform known protocols to prepare an essential class of states.","sentences":["Adaptive quantum circuits, which combine local unitary gates, midcircuit measurements, and feedforward operations, have recently emerged as a promising avenue for efficient state preparation, particularly on near-term quantum devices limited to shallow-depth circuits.","Matrix product states (MPS) comprise a significant class of many-body entangled states, efficiently describing the ground states of one-dimensional gapped local Hamiltonians and finding applications in a number of recent quantum algorithms.","Recently, it was shown that the AKLT state -- a paradigmatic example of an MPS -- can be exactly prepared with an adaptive quantum circuit of constant-depth, an impossible feat with local unitary gates due to its nonzero correlation length","[Smith et al., PRX Quantum 4, 020315 (2023)].","In this work, we broaden the scope of this approach and demonstrate that a diverse class of MPS can be exactly prepared using constant-depth adaptive quantum circuits, outperforming optimal preparation protocols that rely on unitary circuits alone.","We show that this class includes short- and long-ranged entangled MPS, symmetry-protected topological (SPT) and symmetry-broken states, MPS with finite Abelian, non-Abelian, and continuous symmetries, resource states for MBQC, and families of states with tunable correlation length.","Moreover, we illustrate the utility of our framework for designing constant-depth sampling protocols, such as for random MPS or for generating MPS in a particular SPT phase.","We present sufficient conditions for particular MPS to be preparable in constant time, with global on-site symmetry playing a pivotal role.","Altogether, this work demonstrates the immense promise of adaptive quantum circuits for efficiently preparing many-body entangled states and provides explicit algorithms that outperform known protocols to prepare an essential class of states."],"url":"http://arxiv.org/abs/2404.16083v1","category":"quant-ph"}
{"created":"2024-04-24 17:43:14","title":"Broad Angle Resolver for THz Band","abstract":"Terahertz (THz) communication systems hold immense potential for high-speed data transfer across various domains yet face challenges due to directionality constraints because of free space path loss. To address this, directional beams are commonly employed in THz technology. With the usage directional beams, it is important to track the transmitting device to keep the link connectivity. Leaky Parallel Plate Waveguides (LPPWs) were introduced to tackle this challenge. But traditional LPPW uses unique angle-frequency relationship which requires broad range of frequencies. This study proposes a novel approach to mitigate these constraints, balancing directionality with reduced directional gain to enhance LPPW adaptability. The proposed device can accurately determine the receiving angle of a beam by analyzing unique features extracted from the dual peak outputs. Experimentation and simulations reveal that the device allows for a broader angle of acceptance and calculation of the received angle.","sentences":["Terahertz (THz) communication systems hold immense potential for high-speed data transfer across various domains yet face challenges due to directionality constraints because of free space path loss.","To address this, directional beams are commonly employed in THz technology.","With the usage directional beams, it is important to track the transmitting device to keep the link connectivity.","Leaky Parallel Plate Waveguides (LPPWs) were introduced to tackle this challenge.","But traditional LPPW uses unique angle-frequency relationship which requires broad range of frequencies.","This study proposes a novel approach to mitigate these constraints, balancing directionality with reduced directional gain to enhance LPPW adaptability.","The proposed device can accurately determine the receiving angle of a beam by analyzing unique features extracted from the dual peak outputs.","Experimentation and simulations reveal that the device allows for a broader angle of acceptance and calculation of the received angle."],"url":"http://arxiv.org/abs/2404.16010v1","category":"physics.optics"}
{"created":"2024-04-24 16:43:54","title":"Shared Boundary Interfaces: can one fit all? A controlled study on virtual reality vs touch-screen interfaces on persons with Neurodevelopmental Disorders","abstract":"Technology presents a significant educational opportunity, particularly in enhancing emotional engagement and expanding learning and educational prospects for individuals with Neurodevelopmental Disorders (NDD). Virtual reality emerges as a promising tool for addressing such disorders, complemented by numerous touchscreen applications that have shown efficacy in fostering education and learning abilities. VR and touchscreen technologies represent diverse interface modalities. This study primarily investigates which interface, VR or touchscreen, more effectively facilitates food education for individuals with NDD. We compared learning outcomes via pre- and post-exposure questionnaires. To this end, we developed GEA, a dual-interface, user-friendly web application for Food Education, adaptable for either immersive use in a head-mounted display (HMD) or non-immersive use on a tablet. A controlled study was conducted to determine which interface better promotes learning. Over three sessions, the experimental group engaged with all GEA games in VR (condition A), while the control group interacted with the same games on a tablet (condition B). Results indicated a significant increase in post-questionnaire scores across subjects, averaging a 46% improvement. This enhancement was notably consistent between groups, with VR and Tablet groups showing 42% and 41% improvements, respectively.","sentences":["Technology presents a significant educational opportunity, particularly in enhancing emotional engagement and expanding learning and educational prospects for individuals with Neurodevelopmental Disorders (NDD).","Virtual reality emerges as a promising tool for addressing such disorders, complemented by numerous touchscreen applications that have shown efficacy in fostering education and learning abilities.","VR and touchscreen technologies represent diverse interface modalities.","This study primarily investigates which interface, VR or touchscreen, more effectively facilitates food education for individuals with NDD.","We compared learning outcomes via pre- and post-exposure questionnaires.","To this end, we developed GEA, a dual-interface, user-friendly web application for Food Education, adaptable for either immersive use in a head-mounted display (HMD) or non-immersive use on a tablet.","A controlled study was conducted to determine which interface better promotes learning.","Over three sessions, the experimental group engaged with all GEA games in VR (condition A), while the control group interacted with the same games on a tablet (condition B).","Results indicated a significant increase in post-questionnaire scores across subjects, averaging a 46% improvement.","This enhancement was notably consistent between groups, with VR and Tablet groups showing 42% and 41% improvements, respectively."],"url":"http://arxiv.org/abs/2404.15970v1","category":"cs.HC"}
{"created":"2024-04-24 16:24:43","title":"Platooning of Heterogeneous Vehicles with Actuation Delays: Theoretical and Experimental Results","abstract":"In this paper we present a prediction-based Cooperative Adaptive Cruise Controller for vehicles with actuation delay, applicable within heterogeneous platoons. We provide a stability analysis for the discrete-time implementation of this controller, which shows the effect of the used sampling times and can be used for selecting appropriate controller gains. The theoretical results are validated by means of experiments using full scale vehicles. This is an extended version of a paper with the same title (submitted to IFAC TDS 2024). Additional mathematical details are provided in this extended version.","sentences":["In this paper we present a prediction-based Cooperative Adaptive Cruise Controller for vehicles with actuation delay, applicable within heterogeneous platoons.","We provide a stability analysis for the discrete-time implementation of this controller, which shows the effect of the used sampling times and can be used for selecting appropriate controller gains.","The theoretical results are validated by means of experiments using full scale vehicles.","This is an extended version of a paper with the same title (submitted to IFAC TDS 2024).","Additional mathematical details are provided in this extended version."],"url":"http://arxiv.org/abs/2404.15958v1","category":"eess.SY"}
{"created":"2024-04-24 16:23:34","title":"A Survey on Visual Mamba","abstract":"State space models (SSMs) with selection mechanisms and hardware-aware architectures, namely Mamba, have recently demonstrated significant promise in long-sequence modeling. Since the self-attention mechanism in transformers has quadratic complexity with image size and increasing computational demands, the researchers are now exploring how to adapt Mamba for computer vision tasks. This paper is the first comprehensive survey aiming to provide an in-depth analysis of Mamba models in the field of computer vision. It begins by exploring the foundational concepts contributing to Mamba's success, including the state space model framework, selection mechanisms, and hardware-aware design. Next, we review these vision mamba models by categorizing them into foundational ones and enhancing them with techniques such as convolution, recurrence, and attention to improve their sophistication. We further delve into the widespread applications of Mamba in vision tasks, which include their use as a backbone in various levels of vision processing. This encompasses general visual tasks, Medical visual tasks (e.g., 2D / 3D segmentation, classification, and image registration, etc.), and Remote Sensing visual tasks. We specially introduce general visual tasks from two levels: High/Mid-level vision (e.g., Object detection, Segmentation, Video classification, etc.) and Low-level vision (e.g., Image super-resolution, Image restoration, Visual generation, etc.). We hope this endeavor will spark additional interest within the community to address current challenges and further apply Mamba models in computer vision.","sentences":["State space models (SSMs) with selection mechanisms and hardware-aware architectures, namely Mamba, have recently demonstrated significant promise in long-sequence modeling.","Since the self-attention mechanism in transformers has quadratic complexity with image size and increasing computational demands, the researchers are now exploring how to adapt Mamba for computer vision tasks.","This paper is the first comprehensive survey aiming to provide an in-depth analysis of Mamba models in the field of computer vision.","It begins by exploring the foundational concepts contributing to Mamba's success, including the state space model framework, selection mechanisms, and hardware-aware design.","Next, we review these vision mamba models by categorizing them into foundational ones and enhancing them with techniques such as convolution, recurrence, and attention to improve their sophistication.","We further delve into the widespread applications of Mamba in vision tasks, which include their use as a backbone in various levels of vision processing.","This encompasses general visual tasks, Medical visual tasks (e.g., 2D / 3D segmentation, classification, and image registration, etc.), and Remote Sensing visual tasks.","We specially introduce general visual tasks from two levels: High/Mid-level vision (e.g., Object detection, Segmentation, Video classification, etc.) and Low-level vision (e.g., Image super-resolution, Image restoration, Visual generation, etc.).","We hope this endeavor will spark additional interest within the community to address current challenges and further apply Mamba models in computer vision."],"url":"http://arxiv.org/abs/2404.15956v1","category":"cs.CV"}
{"created":"2024-04-24 16:14:13","title":"SPHINCS_BSSN: Numerical Relativity with Particles","abstract":"In this book chapter we describe the {\\em Lagrangian} numerical relativity code \\sphi. This code evolves spacetimes in full General Relativity by integrating the BSSN equations on structured meshes with a simple dynamical mesh refinement strategy. The fluid is evolved by means of freely moving Lagrangian particles, that are evolved using a modern Smooth Particle Hydrodynamics (SPH) formulation. To robustly and accurately capture shocks, our code uses artificial dissipation terms, but, similar to Finite Volume schemes, we apply a slope-limited reconstruction within the dissipative terms and we use in addition time-dependent dissipation parameters, so that dissipation is only applied where needed. The technically most complicated, but absolutely crucial part of the methodology, is the coupling between the particles and the mesh. For the mapping of the energy-momentum tensor $T_{\\mu\\nu}$ from the particles to the mesh, we use a sophisticated combination of \"Local Regression Estimate\" (LRE) method and a \"multi-dimensional optimal order detection\" (MOOD) approach which we describe in some detail. The mapping of the metric quantities from the grid to the particles is achieved by a quintic Hermite interpolation. Apart from giving an introduction to our numerical methods, we demonstrate the accurate working of our code by presenting a set of representative relativistic hydrodynamics tests. We begin with a relativistic shock tube test, then compare the frequencies of a fully relativistic neutron star with reference values from the literature and, finally, we present full-blown merger simulations of irrotational binary systems, one case where a central remnant survives and another where a black hole forms, and of a binary where only one of the stars is rapidly spinning.","sentences":["In this book chapter we describe the {\\em Lagrangian} numerical relativity code \\sphi.","This code evolves spacetimes in full General Relativity by integrating the BSSN equations on structured meshes with a simple dynamical mesh refinement strategy.","The fluid is evolved by means of freely moving Lagrangian particles, that are evolved using a modern Smooth Particle Hydrodynamics (SPH) formulation.","To robustly and accurately capture shocks, our code uses artificial dissipation terms, but, similar to Finite Volume schemes, we apply a slope-limited reconstruction within the dissipative terms and we use in addition time-dependent dissipation parameters, so that dissipation is only applied where needed.","The technically most complicated, but absolutely crucial part of the methodology, is the coupling between the particles and the mesh.","For the mapping of the energy-momentum tensor $T_{\\mu\\nu}$ from the particles to the mesh, we use a sophisticated combination of \"Local Regression Estimate\" (LRE) method and a \"multi-dimensional optimal order detection\" (MOOD) approach which we describe in some detail.","The mapping of the metric quantities from the grid to the particles is achieved by a quintic Hermite interpolation.","Apart from giving an introduction to our numerical methods, we demonstrate the accurate working of our code by presenting a set of representative relativistic hydrodynamics tests.","We begin with a relativistic shock tube test, then compare the frequencies of a fully relativistic neutron star with reference values from the literature and, finally, we present full-blown merger simulations of irrotational binary systems, one case where a central remnant survives and another where a black hole forms, and of a binary where only one of the stars is rapidly spinning."],"url":"http://arxiv.org/abs/2404.15952v1","category":"gr-qc"}
{"created":"2024-04-24 14:24:57","title":"Sketch2Human: Deep Human Generation with Disentangled Geometry and Appearance Control","abstract":"Geometry- and appearance-controlled full-body human image generation is an interesting but challenging task. Existing solutions are either unconditional or dependent on coarse conditions (e.g., pose, text), thus lacking explicit geometry and appearance control of body and garment. Sketching offers such editing ability and has been adopted in various sketch-based face generation and editing solutions. However, directly adapting sketch-based face generation to full-body generation often fails to produce high-fidelity and diverse results due to the high complexity and diversity in the pose, body shape, and garment shape and texture. Recent geometrically controllable diffusion-based methods mainly rely on prompts to generate appearance and it is hard to balance the realism and the faithfulness of their results to the sketch when the input is coarse. This work presents Sketch2Human, the first system for controllable full-body human image generation guided by a semantic sketch (for geometry control) and a reference image (for appearance control). Our solution is based on the latent space of StyleGAN-Human with inverted geometry and appearance latent codes as input. Specifically, we present a sketch encoder trained with a large synthetic dataset sampled from StyleGAN-Human's latent space and directly supervised by sketches rather than real images. Considering the entangled information of partial geometry and texture in StyleGAN-Human and the absence of disentangled datasets, we design a novel training scheme that creates geometry-preserved and appearance-transferred training data to tune a generator to achieve disentangled geometry and appearance control. Although our method is trained with synthetic data, it can handle hand-drawn sketches as well. Qualitative and quantitative evaluations demonstrate the superior performance of our method to state-of-the-art methods.","sentences":["Geometry- and appearance-controlled full-body human image generation is an interesting but challenging task.","Existing solutions are either unconditional or dependent on coarse conditions (e.g., pose, text), thus lacking explicit geometry and appearance control of body and garment.","Sketching offers such editing ability and has been adopted in various sketch-based face generation and editing solutions.","However, directly adapting sketch-based face generation to full-body generation often fails to produce high-fidelity and diverse results due to the high complexity and diversity in the pose, body shape, and garment shape and texture.","Recent geometrically controllable diffusion-based methods mainly rely on prompts to generate appearance and it is hard to balance the realism and the faithfulness of their results to the sketch when the input is coarse.","This work presents Sketch2Human, the first system for controllable full-body human image generation guided by a semantic sketch (for geometry control) and a reference image (for appearance control).","Our solution is based on the latent space of StyleGAN-Human with inverted geometry and appearance latent codes as input.","Specifically, we present a sketch encoder trained with a large synthetic dataset sampled from StyleGAN-Human's latent space and directly supervised by sketches rather than real images.","Considering the entangled information of partial geometry and texture in StyleGAN-Human and the absence of disentangled datasets, we design a novel training scheme that creates geometry-preserved and appearance-transferred training data to tune a generator to achieve disentangled geometry and appearance control.","Although our method is trained with synthetic data, it can handle hand-drawn sketches as well.","Qualitative and quantitative evaluations demonstrate the superior performance of our method to state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.15889v1","category":"cs.CV"}
{"created":"2024-04-24 13:24:38","title":"The genuinely multipartite nonlocality of graph states is model-dependent","abstract":"Bell's theorem proves that some quantum state correlations can only be explained by bipartite non-classical resources. The notion of genuinely multipartite nonlocality (GMNL) was later introduced to conceptualize the fact that nonclassical resources involving more than two parties in a nontrivial way may be needed to account for some quantum correlations. In this letter, we first recall the contradictions inherent to the historical definition of GMNL. Second, we turn to one of its redefinitions, called Local-Operations-and-Shared-Randomness GMNL (LOSR-GMNL), proving that all caterpillar graph states (including cluster states) have this second property. Finally, we conceptualize a third, alternative definition, which we call Local-Operations-and-Neighbour-Communication GMNL (LONC-GMNL), that is adapted to situations in which short-range communication between some parties might occur. We show that cluster states do not have this third property, while GHZ states do. Beyond its technical content, our letter illustrates that rigorous conceptual work is needed before applying the concepts of genuinely multipartite nonlocality, genuine multipartite entanglement or entanglement depth to benchmark the nonclassicality of some experimentally-produced quantum system. We note that most experimental works still use witnesses based on the historical definitions of these notions, which fail to reject models based on bipartite resources.","sentences":["Bell's theorem proves that some quantum state correlations can only be explained by bipartite non-classical resources.","The notion of genuinely multipartite nonlocality (GMNL) was later introduced to conceptualize the fact that nonclassical resources involving more than two parties in a nontrivial way may be needed to account for some quantum correlations.","In this letter, we first recall the contradictions inherent to the historical definition of GMNL.","Second, we turn to one of its redefinitions, called Local-Operations-and-Shared-Randomness GMNL (LOSR-GMNL), proving that all caterpillar graph states (including cluster states) have this second property.","Finally, we conceptualize a third, alternative definition, which we call Local-Operations-and-Neighbour-Communication GMNL (LONC-GMNL), that is adapted to situations in which short-range communication between some parties might occur.","We show that cluster states do not have this third property, while GHZ states do.","Beyond its technical content, our letter illustrates that rigorous conceptual work is needed before applying the concepts of genuinely multipartite nonlocality, genuine multipartite entanglement or entanglement depth to benchmark the nonclassicality of some experimentally-produced quantum system.","We note that most experimental works still use witnesses based on the historical definitions of these notions, which fail to reject models based on bipartite resources."],"url":"http://arxiv.org/abs/2404.15861v1","category":"quant-ph"}
{"created":"2024-04-24 11:41:28","title":"Vision Transformer-based Adversarial Domain Adaptation","abstract":"Unsupervised domain adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain. The most recent UDA methods always resort to adversarial training to yield state-of-the-art results and a dominant number of existing UDA methods employ convolutional neural networks (CNNs) as feature extractors to learn domain invariant features. Vision transformer (ViT) has attracted tremendous attention since its emergence and has been widely used in various computer vision tasks, such as image classification, object detection, and semantic segmentation, yet its potential in adversarial domain adaptation has never been investigated. In this paper, we fill this gap by employing the ViT as the feature extractor in adversarial domain adaptation. Moreover, we empirically demonstrate that ViT can be a plug-and-play component in adversarial domain adaptation, which means directly replacing the CNN-based feature extractor in existing UDA methods with the ViT-based feature extractor can easily obtain performance improvement. The code is available at https://github.com/LluckyYH/VT-ADA.","sentences":["Unsupervised domain adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain.","The most recent UDA methods always resort to adversarial training to yield state-of-the-art results and a dominant number of existing UDA methods employ convolutional neural networks (CNNs) as feature extractors to learn domain invariant features.","Vision transformer (ViT) has attracted tremendous attention since its emergence and has been widely used in various computer vision tasks, such as image classification, object detection, and semantic segmentation, yet its potential in adversarial domain adaptation has never been investigated.","In this paper, we fill this gap by employing the ViT as the feature extractor in adversarial domain adaptation.","Moreover, we empirically demonstrate that ViT can be a plug-and-play component in adversarial domain adaptation, which means directly replacing the CNN-based feature extractor in existing UDA methods with the ViT-based feature extractor can easily obtain performance improvement.","The code is available at https://github.com/LluckyYH/VT-ADA."],"url":"http://arxiv.org/abs/2404.15817v1","category":"cs.CV"}
{"created":"2024-04-24 09:02:24","title":"SRAGAN: Saliency Regularized and Attended Generative Adversarial Network for Chinese Ink-wash Painting Generation","abstract":"This paper handles the problem of converting real pictures into traditional Chinese ink-wash paintings, i.e., Chinese ink-wash painting style transfer. Though this problem could be realized by a wide range of image-to-image translation models, a notable issue with all these methods is that the original image content details could be easily erased or corrupted due to transfer of ink-wash style elements. To solve or ameliorate this issue, we propose to incorporate saliency detection into the unpaired image-to-image translation framework to regularize content information of the generated paintings. The saliency map is utilized for content regularization from two aspects, both explicitly and implicitly: (\\romannumeral1) we propose saliency IOU (SIOU) loss to explicitly regularize saliency consistency before and after stylization; (\\romannumeral2) we propose saliency adaptive normalization (SANorm) which implicitly enhances content integrity of the generated paintings by injecting saliency information to the generator network to guide painting generation. Besides, we also propose saliency attended discriminator network which harnesses saliency mask to focus generative adversarial attention onto salient image regions, it contributes to producing finer ink-wash stylization effect for salient objects of images. Qualitative and quantitative experiments consistently demonstrate superiority of our model over related advanced methods for Chinese ink-wash painting style transfer.","sentences":["This paper handles the problem of converting real pictures into traditional Chinese ink-wash paintings, i.e., Chinese ink-wash painting style transfer.","Though this problem could be realized by a wide range of image-to-image translation models, a notable issue with all these methods is that the original image content details could be easily erased or corrupted due to transfer of ink-wash style elements.","To solve or ameliorate this issue, we propose to incorporate saliency detection into the unpaired image-to-image translation framework to regularize content information of the generated paintings.","The saliency map is utilized for content regularization from two aspects, both explicitly and implicitly: (\\romannumeral1) we propose saliency IOU (SIOU) loss to explicitly regularize saliency consistency before and after stylization; (\\romannumeral2) we propose saliency adaptive normalization (SANorm) which implicitly enhances content integrity of the generated paintings by injecting saliency information to the generator network to guide painting generation.","Besides, we also propose saliency attended discriminator network which harnesses saliency mask to focus generative adversarial attention onto salient image regions, it contributes to producing finer ink-wash stylization effect for salient objects of images.","Qualitative and quantitative experiments consistently demonstrate superiority of our model over related advanced methods for Chinese ink-wash painting style transfer."],"url":"http://arxiv.org/abs/2404.15743v1","category":"cs.CV"}
{"created":"2024-04-24 08:53:04","title":"Low thermal boundary resistance at bonded GaN/diamond interface by controlling ultrathin heterogeneous amorphous layer","abstract":"Thermal boundary resistance (TBR) in semiconductor-on-diamond structure bottlenecks efficient heat dissipation in electronic devices. In this study, to reduce the TBR between GaN and diamond, surface-activated bonding with a hybrid SiOx-Ar ion source was applied to achieve an ultrathin interfacial layer. The simultaneous surface activation and slow deposition of the SiOx binder layer enabled precise control over layer thickness (2.5-5.3 nm) and formation of an amorphous heterogeneous nanostructure comprising a SiOx region between two inter-diffusion regions. Crucially, the 2.5-nm-thick interfacial layer achieved a TBR of 8.3 m2-W/GW, a record low for direct-bonded GaN/diamond interface. A remarkable feature is that the TBR is extremely sensitive to the interfacial thickness; rapidly increasing to 34 m2-K/GW on doubling the thickness to 5.3 nm. Theoretical analysis revealed the origin of this increase: a diamond/SiOx interdiffusion layer extend the vibrational frequency, far-exceeding that of crystalline diamond, which increases the lattice vibrational mismatch and suppresses phonon transmission.","sentences":["Thermal boundary resistance (TBR) in semiconductor-on-diamond structure bottlenecks efficient heat dissipation in electronic devices.","In this study, to reduce the TBR between GaN and diamond, surface-activated bonding with a hybrid SiOx-Ar ion source was applied to achieve an ultrathin interfacial layer.","The simultaneous surface activation and slow deposition of the SiOx binder layer enabled precise control over layer thickness (2.5-5.3 nm) and formation of an amorphous heterogeneous nanostructure comprising a SiOx region between two inter-diffusion regions.","Crucially, the 2.5-nm-thick interfacial layer achieved a TBR of 8.3 m2-W/GW, a record low for direct-bonded GaN/diamond interface.","A remarkable feature is that the TBR is extremely sensitive to the interfacial thickness; rapidly increasing to 34 m2-K/GW on doubling the thickness to 5.3 nm.","Theoretical analysis revealed the origin of this increase: a diamond/SiOx interdiffusion layer extend the vibrational frequency, far-exceeding that of crystalline diamond, which increases the lattice vibrational mismatch and suppresses phonon transmission."],"url":"http://arxiv.org/abs/2404.15738v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-24 08:52:40","title":"No Train but Gain: Language Arithmetic for training-free Language Adapters enhancement","abstract":"Modular deep learning is the state-of-the-art solution for lifting the curse of multilinguality, preventing the impact of negative interference and enabling cross-lingual performance in Multilingual Pre-trained Language Models. However, a trade-off of this approach is the reduction in positive transfer learning from closely related languages. In response, we introduce a novel method called language arithmetic, which enables training-free post-processing to address this limitation. Inspired by the task arithmetic framework, we apply learning via addition to the language adapters, transitioning the framework from a multi-task to a multilingual setup. The effectiveness of the proposed solution is demonstrated on three downstream tasks in a MAD-X-based set of cross-lingual schemes, acting as a post-processing procedure. Language arithmetic consistently improves the baselines with significant gains in the most challenging cases of zero-shot and low-resource applications. Our code and models are available at https://github.com/mklimasz/language-arithmetic .","sentences":["Modular deep learning is the state-of-the-art solution for lifting the curse of multilinguality, preventing the impact of negative interference and enabling cross-lingual performance in Multilingual Pre-trained Language Models.","However, a trade-off of this approach is the reduction in positive transfer learning from closely related languages.","In response, we introduce a novel method called language arithmetic, which enables training-free post-processing to address this limitation.","Inspired by the task arithmetic framework, we apply learning via addition to the language adapters, transitioning the framework from a multi-task to a multilingual setup.","The effectiveness of the proposed solution is demonstrated on three downstream tasks in a MAD-X-based set of cross-lingual schemes, acting as a post-processing procedure.","Language arithmetic consistently improves the baselines with significant gains in the most challenging cases of zero-shot and low-resource applications.","Our code and models are available at https://github.com/mklimasz/language-arithmetic ."],"url":"http://arxiv.org/abs/2404.15737v1","category":"cs.CL"}
{"created":"2024-04-24 08:30:33","title":"Local convergence rates for Wasserstein gradient flows and McKean-Vlasov equations with multiple stationary solutions","abstract":"Non-linear versions of log-Sobolev inequalities, that link a free energy to its dissipation along the corresponding Wasserstein gradient flow (i.e. corresponds to Polyak-Lojasiewicz inequalities in this context), are known to provide global exponential long-time convergence to the free energy minimizers, and have been shown to hold in various contexts. However they cannot hold when the free energy admits critical points which are not global minimizers, which is for instance the case of the granular media equation in a double-well potential with quadratic attractive interaction at low temperature. This work addresses such cases, extending the general arguments when a log-Sobolev inequality only holds locally and, as an example, establishing such local inequalities for the granular media equation with quadratic interaction either in the one-dimensional symmetric double-well case or in higher dimension in the low temperature regime. The method provides quantitative convergence rates for initial conditions in a Wasserstein ball around the stationary solutions. The same analysis is carried out for the kinetic counterpart of the gradient flow, i.e. the corresponding Vlasov-Fokker-Planck equation. The local exponential convergence to stationary solutions for the mean-field equations, both elliptic and kinetic, is shown to induce for the corresponding particle systems a fast (i.e. uniform in the number or particles) decay of the particle system free energy toward the level of the non-linear limit.","sentences":["Non-linear versions of log-Sobolev inequalities, that link a free energy to its dissipation along the corresponding Wasserstein gradient flow (i.e. corresponds to Polyak-Lojasiewicz inequalities in this context), are known to provide global exponential long-time convergence to the free energy minimizers, and have been shown to hold in various contexts.","However they cannot hold when the free energy admits critical points which are not global minimizers, which is for instance the case of the granular media equation in a double-well potential with quadratic attractive interaction at low temperature.","This work addresses such cases, extending the general arguments when a log-Sobolev inequality only holds locally and, as an example, establishing such local inequalities for the granular media equation with quadratic interaction either in the one-dimensional symmetric double-well case or in higher dimension in the low temperature regime.","The method provides quantitative convergence rates for initial conditions in a Wasserstein ball around the stationary solutions.","The same analysis is carried out for the kinetic counterpart of the gradient flow, i.e. the corresponding Vlasov-Fokker-Planck equation.","The local exponential convergence to stationary solutions for the mean-field equations, both elliptic and kinetic, is shown to induce for the corresponding particle systems a fast (i.e. uniform in the number or particles) decay of the particle system free energy toward the level of the non-linear limit."],"url":"http://arxiv.org/abs/2404.15725v1","category":"math.AP"}
{"created":"2024-04-25 17:52:03","title":"Accelerated inference on accelerated cosmic expansion: New constraints on axion-like early dark energy with DESI BAO and ACT DR6 CMB lensing","abstract":"The early dark energy (EDE) extension to $\\Lambda$CDM has been proposed as a candidate scenario to resolve the \"Hubble tension\". We present new constraints on the EDE model by incorporating new data from the Dark Energy Spectroscopic Instrument (DESI) Baryon Acoustic Oscillation (BAO) survey and CMB lensing measurements from the Atacama Cosmology Telescope (ACT) DR6 and \\textit{Planck} NPIPE data. We do not find evidence for EDE. The maximum fractional contribution of EDE to the total energy density is $f_\\mathrm{EDE}< 0.091 \\; (95\\% \\; \\mathrm{CL} )$ from our baseline combination of \\textit{Planck} CMB, CMB lensing, and DESI BAO. Our strongest constraints on EDE come from the combination of \\textit{Planck} CMB and CMB lensing alone, yielding $f_\\mathrm{EDE}< 0.070 \\; (95\\% \\; \\mathrm{CL} )$. We also explore extensions of $\\Lambda$CDM beyond the EDE parameters by treating the total neutrino mass as a free parameter, finding $\\sum m_\\nu < 0.096 \\,\\, {\\rm eV} \\; (95\\% \\; \\mathrm{CL} )$ and $f_\\mathrm{EDE}< 0.087 \\; (95\\% \\; \\mathrm{CL} )$. For the first time in EDE analyses, we perform Bayesian parameter estimation using neural network emulators of cosmological observables, which are on the order of a hundred times faster than full Boltzmann solutions.","sentences":["The early dark energy (EDE) extension to $\\Lambda$CDM has been proposed as a candidate scenario to resolve the \"Hubble tension\".","We present new constraints on the EDE model by incorporating new data from the Dark Energy Spectroscopic Instrument (DESI)","Baryon Acoustic Oscillation (BAO) survey and CMB lensing measurements from the Atacama Cosmology Telescope (ACT) DR6 and \\textit{Planck} NPIPE data.","We do not find evidence for EDE.","The maximum fractional contribution of EDE to the total energy density is $f_\\mathrm{EDE}< 0.091 \\; (95\\% \\; \\mathrm{CL} )$ from our baseline combination of \\textit{Planck} CMB, CMB lensing, and DESI BAO.","Our strongest constraints on EDE come from the combination of \\textit{Planck} CMB and CMB lensing alone, yielding $f_\\mathrm{EDE}< 0.070 \\; (95\\% \\; \\mathrm{CL} )$.","We also explore extensions of $\\Lambda$CDM beyond the EDE parameters by treating the total neutrino mass as a free parameter, finding $\\sum m_\\nu < 0.096 \\,\\, {\\rm eV} \\; (95\\% \\; \\mathrm{CL} )$ and $f_\\mathrm{EDE}< 0.087 \\; (95\\% \\; \\mathrm{CL} )$.","For the first time in EDE analyses, we perform Bayesian parameter estimation using neural network emulators of cosmological observables, which are on the order of a hundred times faster than full Boltzmann solutions."],"url":"http://arxiv.org/abs/2404.16805v1","category":"astro-ph.CO"}
{"created":"2024-04-25 17:40:52","title":"In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization","abstract":"With the increasing computational costs associated with deep learning, automated hyperparameter optimization methods, strongly relying on black-box Bayesian optimization (BO), face limitations. Freeze-thaw BO offers a promising grey-box alternative, strategically allocating scarce resources incrementally to different configurations. However, the frequent surrogate model updates inherent to this approach pose challenges for existing methods, requiring retraining or fine-tuning their neural network surrogates online, introducing overhead, instability, and hyper-hyperparameters. In this work, we propose FT-PFN, a novel surrogate for Freeze-thaw style BO. FT-PFN is a prior-data fitted network (PFN) that leverages the transformers' in-context learning ability to efficiently and reliably do Bayesian learning curve extrapolation in a single forward pass. Our empirical analysis across three benchmark suites shows that the predictions made by FT-PFN are more accurate and 10-100 times faster than those of the deep Gaussian process and deep ensemble surrogates used in previous work. Furthermore, we show that, when combined with our novel acquisition mechanism (MFPI-random), the resulting in-context freeze-thaw BO method (ifBO), yields new state-of-the-art performance in the same three families of deep learning HPO benchmarks considered in prior work.","sentences":["With the increasing computational costs associated with deep learning, automated hyperparameter optimization methods, strongly relying on black-box Bayesian optimization (BO), face limitations.","Freeze-thaw BO offers a promising grey-box alternative, strategically allocating scarce resources incrementally to different configurations.","However, the frequent surrogate model updates inherent to this approach pose challenges for existing methods, requiring retraining or fine-tuning their neural network surrogates online, introducing overhead, instability, and hyper-hyperparameters.","In this work, we propose FT-PFN, a novel surrogate for Freeze-thaw style BO.","FT-PFN is a prior-data fitted network (PFN) that leverages the transformers' in-context learning ability to efficiently and reliably do Bayesian learning curve extrapolation in a single forward pass.","Our empirical analysis across three benchmark suites shows that the predictions made by FT-PFN are more accurate and 10-100 times faster than those of the deep Gaussian process and deep ensemble surrogates used in previous work.","Furthermore, we show that, when combined with our novel acquisition mechanism (MFPI-random), the resulting in-context freeze-thaw BO method (ifBO), yields new state-of-the-art performance in the same three families of deep learning HPO benchmarks considered in prior work."],"url":"http://arxiv.org/abs/2404.16795v1","category":"cs.LG"}
{"created":"2024-04-25 17:38:37","title":"Rectifying submanifolds of Riemannian manifolds with anti-torqued axis","abstract":"In this paper we study rectifying submanifolds of a Riemannian manifold endowed with an anti-torqued vector field. For this, we first determine a necessary and sufficient condition for the ambient space to admit such a vector field. Then we characterize submanifolds for which an anti-torqued vector field is always assumed to be tangent or normal. A similar characterization is also done in the case of the torqued vector fields. Finally, we obtain that the rectifying submanifolds with anti-torqued axis are the warped products whose warping function is a first integration of the conformal scalar of the axis.","sentences":["In this paper we study rectifying submanifolds of a Riemannian manifold endowed with an anti-torqued vector field.","For this, we first determine a necessary and sufficient condition for the ambient space to admit such a vector field.","Then we characterize submanifolds for which an anti-torqued vector field is always assumed to be tangent or normal.","A similar characterization is also done in the case of the torqued vector fields.","Finally, we obtain that the rectifying submanifolds with anti-torqued axis are the warped products whose warping function is a first integration of the conformal scalar of the axis."],"url":"http://arxiv.org/abs/2404.16788v1","category":"math.DG"}
{"created":"2024-04-25 17:23:29","title":"Multi-scale modeling of Snail-mediated response to hypoxia in tumor progression","abstract":"Tumor cell migration within the microenvironment is a crucial aspect for cancer progression and, in this context, hypoxia has a significant role. An inadequate oxygen supply acts as an environmental stressor inducing migratory bias and phenotypic changes. In this paper, we propose a novel multi-scale mathematical model to analyze the pivotal role of Snail protein expression in the cellular responses to hypoxia. Starting from the description of single-cell dynamics driven by the Snail protein, we construct the corresponding kinetic transport equation that describes the evolution of the cell distribution. Subsequently, we employ proper scaling arguments to formally derive the equations for the statistical moments of the cell distribution, which govern the macroscopic tumor dynamics. Numerical simulations of the model are performed in various scenarios with biological relevance to provide insights into the role of the multiple tactic terms, the impact of Snail expression on cell proliferation, and the emergence of hypoxia-induced migration patterns. Moreover, quantitative comparison with experimental data shows the model's reliability in measuring the impact of Snail transcription on cell migratory potential. Through our findings, we shed light on the potential of our mathematical framework in advancing the understanding of the biological mechanisms driving tumor progression.","sentences":["Tumor cell migration within the microenvironment is a crucial aspect for cancer progression and, in this context, hypoxia has a significant role.","An inadequate oxygen supply acts as an environmental stressor inducing migratory bias and phenotypic changes.","In this paper, we propose a novel multi-scale mathematical model to analyze the pivotal role of Snail protein expression in the cellular responses to hypoxia.","Starting from the description of single-cell dynamics driven by the Snail protein, we construct the corresponding kinetic transport equation that describes the evolution of the cell distribution.","Subsequently, we employ proper scaling arguments to formally derive the equations for the statistical moments of the cell distribution, which govern the macroscopic tumor dynamics.","Numerical simulations of the model are performed in various scenarios with biological relevance to provide insights into the role of the multiple tactic terms, the impact of Snail expression on cell proliferation, and the emergence of hypoxia-induced migration patterns.","Moreover, quantitative comparison with experimental data shows the model's reliability in measuring the impact of Snail transcription on cell migratory potential.","Through our findings, we shed light on the potential of our mathematical framework in advancing the understanding of the biological mechanisms driving tumor progression."],"url":"http://arxiv.org/abs/2404.16769v1","category":"q-bio.CB"}
{"created":"2024-04-25 17:06:21","title":"Hydrodynamics of a Discrete Conservation Law","abstract":"The Riemann problem for the discrete conservation law $2 \\dot{u}_n + u^2_{n+1} - u^2_{n-1} = 0$ is classified using Whitham modulation theory, a quasi-continuum approximation, and numerical simulations. A surprisingly elaborate set of solutions to this simple discrete regularization of the inviscid Burgers' equation is obtained. In addition to discrete analogues of well-known dispersive hydrodynamic solutions -- rarefaction waves (RWs) and dispersive shock waves (DSWs) -- additional unsteady solution families and finite time blow-up are observed. Two solution types exhibit no known conservative continuum correlates: (i) a counterpropagating DSW and RW solution separated by a symmetric, stationary shock and (ii) an unsteady shock emitting two counter-propagating periodic wavetrains with the same frequency connected to a partial DSW or a RW. Another class of solutions called traveling DSWs, (iii), consists of a partial DSW connected to a traveling wave comprised of a periodic wavetrain with a rapid transition to a constant. Portions of solutions (ii) and (iii) are interpreted as shock solutions of the Whitham modulation equations.","sentences":["The Riemann problem for the discrete conservation law $2 \\dot{u}_n + u^2_{n+1} - u^2_{n-1} = 0$ is classified using Whitham modulation theory, a quasi-continuum approximation, and numerical simulations.","A surprisingly elaborate set of solutions to this simple discrete regularization of the inviscid Burgers' equation is obtained.","In addition to discrete analogues of well-known dispersive hydrodynamic solutions -- rarefaction waves (RWs) and dispersive shock waves (DSWs) -- additional unsteady solution families and finite time blow-up are observed.","Two solution types exhibit no known conservative continuum correlates: (i) a counterpropagating DSW and RW solution separated by a symmetric, stationary shock and (ii) an unsteady shock emitting two counter-propagating periodic wavetrains with the same frequency connected to a partial DSW or a RW.","Another class of solutions called traveling DSWs, (iii), consists of a partial DSW connected to a traveling wave comprised of a periodic wavetrain with a rapid transition to a constant.","Portions of solutions (ii) and (iii) are interpreted as shock solutions of the Whitham modulation equations."],"url":"http://arxiv.org/abs/2404.16750v1","category":"nlin.PS"}
{"created":"2024-04-25 16:47:53","title":"Superconducting Klein and anti-Klein tunneling in Weyl junctions","abstract":"Klein tunneling is an old topic in relativistic quantum physics, and has been observed recently in graphene where massless particles reside. Here, we propose a new heterostructure platform for Klein tunneling to occur, which consists of a Weyl-semimetal-based normal state/superconductor (NS) junction. By developing a Blonder-Tinkham-Klapwijk-like theory, we find that Klein tunneling occurs at normal incidence, which can lead to differential conductance doubling. If the (single) Weyl semimeltals are replaced by double Weyl semimetals, anti-Klein tunneling will take place of Klein tunneling. Our work provides a theoretical guide for the detection of (anti-)Klein tunneling in three-dimensional chiral NS junctions.","sentences":["Klein tunneling is an old topic in relativistic quantum physics, and has been observed recently in graphene where massless particles reside.","Here, we propose a new heterostructure platform for Klein tunneling to occur, which consists of a Weyl-semimetal-based normal state/superconductor (NS) junction.","By developing a Blonder-Tinkham-Klapwijk-like theory, we find that Klein tunneling occurs at normal incidence, which can lead to differential conductance doubling.","If the (single) Weyl semimeltals are replaced by double Weyl semimetals, anti-Klein tunneling will take place of Klein tunneling.","Our work provides a theoretical guide for the detection of (anti-)Klein tunneling in three-dimensional chiral NS junctions."],"url":"http://arxiv.org/abs/2404.16738v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-25 16:07:55","title":"Conformal para quaternionic contact curvature and the local flatness theorem","abstract":"A tensor invariant is defined on a para quaternionic contact manifold in terms of the curvature and torsion of the canonical para quaternionic connection involving derivatives up to third order of the contact form. This tensor, called para quaternionic contact conformal curvature, is similar to the Weyl conformal curvature in Riemannian geometry, the Chern-Moser tensor in CR geometry, the para contact curvature in para CR geometry and to the quaternionic contact conformal curvature in quaternionic contact geometry.   It is shown that a para quaternionic contact manifold is locally para quaternionic contact conformal to the standard flat para quaternionic contact structure on the para quaternionic Heisenberg group, or equivalently, to the standard para 3-Sasakian structure on the para quaternionic pseudo-sphere iff the para quaternionic contact conformal curvature vanishes.","sentences":["A tensor invariant is defined on a para quaternionic contact manifold in terms of the curvature and torsion of the canonical para quaternionic connection involving derivatives up to third order of the contact form.","This tensor, called para quaternionic contact conformal curvature, is similar to the Weyl conformal curvature in Riemannian geometry, the Chern-Moser tensor in CR geometry, the para contact curvature in para CR geometry and to the quaternionic contact conformal curvature in quaternionic contact geometry.   ","It is shown that a para quaternionic contact manifold is locally para quaternionic contact conformal to the standard flat para quaternionic contact structure on the para quaternionic Heisenberg group, or equivalently, to the standard para 3-Sasakian structure on the para quaternionic pseudo-sphere iff the para quaternionic contact conformal curvature vanishes."],"url":"http://arxiv.org/abs/2404.16703v1","category":"math.DG"}
{"created":"2024-04-25 15:06:48","title":"A new way of deriving implicit Runge-Kutta methods based on repeated integrals","abstract":"Runge-Kutta methods have an irreplaceable position among numerical methods designed to solve ordinary differential equations. Especially, implicit ones are suitable for approximating solutions of stiff initial value problems. We propose a new way of deriving coefficients of implicit Runge-Kutta methods. This approach based on repeated integrals yields both new and well-known Butcher's tableaux. We discuss the properties of newly derived methods and compare them with standard collocation implicit Runge-Kutta methods in a series of numerical experiments. In particular, we observe higher accuracy and higher experimental order of convergence of some newly derived methods.","sentences":["Runge-Kutta methods have an irreplaceable position among numerical methods designed to solve ordinary differential equations.","Especially, implicit ones are suitable for approximating solutions of stiff initial value problems.","We propose a new way of deriving coefficients of implicit Runge-Kutta methods.","This approach based on repeated integrals yields both new and well-known Butcher's tableaux.","We discuss the properties of newly derived methods and compare them with standard collocation implicit Runge-Kutta methods in a series of numerical experiments.","In particular, we observe higher accuracy and higher experimental order of convergence of some newly derived methods."],"url":"http://arxiv.org/abs/2404.16665v1","category":"math.NA"}
{"created":"2024-04-25 14:29:39","title":"Higher H\u00f6lder regularity for a subquadratic nonlocal parabolic equation","abstract":"In this paper, we are concerned with the H\\\"older regularity for solutions of the nonlocal evolutionary equation $$ \\partial_t u+(-\\Delta_p)^s u = 0. $$ Here, $(-\\Delta_p)^s$ is the fractional $p$-Laplacian, $0<s<1$ and $1<p<2$. We establish H\\\"older regularity with explicit H\\\"older exponents. We also include the inhomogeneous equation with a bounded inhomogeneity. In some cases, the obtained H\\\"older exponents are almost sharp. Our results complement the previous results for the superquadratic case when $p\\geq 2$.","sentences":["In this paper, we are concerned with the H\\\"older regularity for solutions of the nonlocal evolutionary equation $$ \\partial_t u+(-\\Delta_p)^s u = 0.","$$ Here, $(-\\Delta_p)^s$ is the fractional $p$-Laplacian, $0<s<1$ and $1<p<2$. We establish H\\\"older regularity with explicit H\\\"older exponents.","We also include the inhomogeneous equation with a bounded inhomogeneity.","In some cases, the obtained H\\\"older exponents are almost sharp.","Our results complement the previous results for the superquadratic case when $p\\geq 2$."],"url":"http://arxiv.org/abs/2404.16640v1","category":"math.AP"}
{"created":"2024-04-25 14:09:41","title":"Long-term stellar activity of M dwarfs: A combined K2 and TESS study of two early M-type stars","abstract":"Studies of the rotation and activity of M type stars are essential to enhance our understanding of stellar dynamos and angular momentum evolution. Using the outstanding photometric capabilities of space telescopes rotation signals even with low amplitudes can be investigated in up to now unrivaled detail. By combining data of K2 and the TESS prime mission the star spot activity of M dwarfs can be monitored on half a decade timescale. In the framework of our study on the rotation-activity relation for bright and nearby M dwarfs we also aim at an investigation of the long-term activity. While K2 was observing fields distributed around the ecliptic plane, the TESS prime mission was oriented along a line of ecliptic longitude with one camera centered on an ecliptic pole. Due to these different observing strategies, the overlap between K2 and the TESS prime mission is marginal. However, 45 stars from our sample were observed with both missions of which two early M-type stars that fulfill our selection criteria, EPIC 202059229 and EPIC 245919787, were analyzed in more detail. We found that for both stars the rotation period did not change while the rotational phase did change for EPIC 245919787 by ~0.2. The amplitude of the spot induced variability changed for both stars but more significant for EPIC 245919787. By comparing the cumulative flare frequency distributions we found that the flare activity for EPIC 202059229 is unchanged while it slightly changes for EPIC 245919787 between the K2 and TESS epochs. Using a combination of light curves from K2 and TESS that span a baseline up to 4.5 years we could measure significant differential rotation for EPIC 245919787. Furthermore, we show that combining missions like K2 and TESS is a promising method for detecting stellar activity cycles.","sentences":["Studies of the rotation and activity of M type stars are essential to enhance our understanding of stellar dynamos and angular momentum evolution.","Using the outstanding photometric capabilities of space telescopes rotation signals even with low amplitudes can be investigated in up to now unrivaled detail.","By combining data of K2 and the TESS prime mission the star spot activity of M dwarfs can be monitored on half a decade timescale.","In the framework of our study on the rotation-activity relation for bright and nearby M dwarfs we also aim at an investigation of the long-term activity.","While K2 was observing fields distributed around the ecliptic plane, the TESS prime mission was oriented along a line of ecliptic longitude with one camera centered on an ecliptic pole.","Due to these different observing strategies, the overlap between K2 and the TESS prime mission is marginal.","However, 45 stars from our sample were observed with both missions of which two early M-type stars that fulfill our selection criteria, EPIC 202059229 and EPIC 245919787, were analyzed in more detail.","We found that for both stars the rotation period did not change while the rotational phase did change for EPIC 245919787 by ~0.2.","The amplitude of the spot induced variability changed for both stars but more significant for EPIC 245919787.","By comparing the cumulative flare frequency distributions we found that the flare activity for EPIC 202059229 is unchanged while it slightly changes for EPIC 245919787 between the K2 and TESS epochs.","Using a combination of light curves from K2 and TESS that span a baseline up to 4.5 years we could measure significant differential rotation for EPIC 245919787.","Furthermore, we show that combining missions like K2 and TESS is a promising method for detecting stellar activity cycles."],"url":"http://arxiv.org/abs/2404.16625v1","category":"astro-ph.SR"}
{"created":"2024-04-25 13:41:18","title":"A Mathematical Framework for Spatio-Temporal Control in Industrial Drying","abstract":"We introduce two models of industrial drying - a simplified one-equation model, and a detailed three-equation model. The purpose of the simplified model is rigorous validation of numerical methods for PDE-constrained optimal control. The purpose of the detailed model is to be able to predict and control the behaviour of an industrial disk drier. For both models, we introduce a fully validated numerical method to compute the optimal source term to maintain the outlet temperature as close as possible to the set-point temperature. By performing simulations using realistic parameters for industrial driers, we illustrate potential applications of the method.","sentences":["We introduce two models of industrial drying - a simplified one-equation model, and a detailed three-equation model.","The purpose of the simplified model is rigorous validation of numerical methods for PDE-constrained optimal control.","The purpose of the detailed model is to be able to predict and control the behaviour of an industrial disk drier.","For both models, we introduce a fully validated numerical method to compute the optimal source term to maintain the outlet temperature as close as possible to the set-point temperature.","By performing simulations using realistic parameters for industrial driers, we illustrate potential applications of the method."],"url":"http://arxiv.org/abs/2404.16604v1","category":"math.OC"}
{"created":"2024-04-25 13:14:59","title":"Andreev reflection, Andreev states, and long ballistic SNS junction","abstract":"The analysis in the present paper is based on the most known concept introduced by the brilliant physicist Alexander Andreev: Andreev bound states in a normal metal sandwiched between two superconductors. The paper presents results of direct calculations of {\\em ab initio} expressions for the currents in a long ballistic SNS junction. The expressions are expanded in $1/L$ ($L$ is the thickness of the normal layer). The main contribution $\\propto 1/L$ to the current agrees with the results obtained in the past, but the analysis suggests a new physical picture of the charge transport through the junction free from the problem with the charge conservation law. The saw-tooth current-phase relation at $T=0$ directly follows from the Galilean invariance of the Bogolyubov-de Gennes equations proved in the paper. The proof is valid for any variation of the energy gap in space if the Andreev reflection is the only scattering process. The respective roles of the contributions of bound and continuum states to the current are clarified. They depend on the junction dimensionality.","sentences":["The analysis in the present paper is based on the most known concept introduced by the brilliant physicist Alexander Andreev:","Andreev bound states in a normal metal sandwiched between two superconductors.","The paper presents results of direct calculations of {\\em ab initio} expressions for the currents in a long ballistic SNS junction.","The expressions are expanded in $1/L$ ($L$ is the thickness of the normal layer).","The main contribution $\\propto 1/L$ to the current agrees with the results obtained in the past, but the analysis suggests a new physical picture of the charge transport through the junction free from the problem with the charge conservation law.","The saw-tooth current-phase relation at $T=0$ directly follows from the Galilean invariance of the Bogolyubov-de Gennes equations proved in the paper.","The proof is valid for any variation of the energy gap in space if the Andreev reflection is the only scattering process.","The respective roles of the contributions of bound and continuum states to the current are clarified.","They depend on the junction dimensionality."],"url":"http://arxiv.org/abs/2404.16593v1","category":"cond-mat.supr-con"}
{"created":"2024-04-25 12:07:41","title":"Surprisingly Strong Performance Prediction with Neural Graph Features","abstract":"Performance prediction has been a key part of the neural architecture search (NAS) process, allowing to speed up NAS algorithms by avoiding resource-consuming network training. Although many performance predictors correlate well with ground truth performance, they require training data in the form of trained networks. Recently, zero-cost proxies have been proposed as an efficient method to estimate network performance without any training. However, they are still poorly understood, exhibit biases with network properties, and their performance is limited. Inspired by the drawbacks of zero-cost proxies, we propose neural graph features (GRAF), simple to compute properties of architectural graphs. GRAF offers fast and interpretable performance prediction while outperforming zero-cost proxies and other common encodings. In combination with other zero-cost proxies, GRAF outperforms most existing performance predictors at a fraction of the cost.","sentences":["Performance prediction has been a key part of the neural architecture search (NAS) process, allowing to speed up NAS algorithms by avoiding resource-consuming network training.","Although many performance predictors correlate well with ground truth performance, they require training data in the form of trained networks.","Recently, zero-cost proxies have been proposed as an efficient method to estimate network performance without any training.","However, they are still poorly understood, exhibit biases with network properties, and their performance is limited.","Inspired by the drawbacks of zero-cost proxies, we propose neural graph features (GRAF), simple to compute properties of architectural graphs.","GRAF offers fast and interpretable performance prediction while outperforming zero-cost proxies and other common encodings.","In combination with other zero-cost proxies, GRAF outperforms most existing performance predictors at a fraction of the cost."],"url":"http://arxiv.org/abs/2404.16551v1","category":"cs.LG"}
{"created":"2024-04-25 11:27:58","title":"A Deep Learning-Driven Pipeline for Differentiating Hypertrophic Cardiomyopathy from Cardiac Amyloidosis Using 2D Multi-View Echocardiography","abstract":"Hypertrophic cardiomyopathy (HCM) and cardiac amyloidosis (CA) are both heart conditions that can progress to heart failure if untreated. They exhibit similar echocardiographic characteristics, often leading to diagnostic challenges. This paper introduces a novel multi-view deep learning approach that utilizes 2D echocardiography for differentiating between HCM and CA. The method begins by classifying 2D echocardiography data into five distinct echocardiographic views: apical 4-chamber, parasternal long axis of left ventricle, parasternal short axis at levels of the mitral valve, papillary muscle, and apex. It then extracts features of each view separately and combines five features for disease classification. A total of 212 patients diagnosed with HCM, and 30 patients diagnosed with CA, along with 200 individuals with normal cardiac function(Normal), were enrolled in this study from 2018 to 2022. This approach achieved a precision, recall of 0.905, and micro-F1 score of 0.904, demonstrating its effectiveness in accurately identifying HCM and CA using a multi-view analysis.","sentences":["Hypertrophic cardiomyopathy (HCM) and cardiac amyloidosis (CA) are both heart conditions that can progress to heart failure if untreated.","They exhibit similar echocardiographic characteristics, often leading to diagnostic challenges.","This paper introduces a novel multi-view deep learning approach that utilizes 2D echocardiography for differentiating between HCM and CA.","The method begins by classifying 2D echocardiography data into five distinct echocardiographic views: apical 4-chamber, parasternal long axis of left ventricle, parasternal short axis at levels of the mitral valve, papillary muscle, and apex.","It then extracts features of each view separately and combines five features for disease classification.","A total of 212 patients diagnosed with HCM, and 30 patients diagnosed with CA, along with 200 individuals with normal cardiac function(Normal), were enrolled in this study from 2018 to 2022.","This approach achieved a precision, recall of 0.905, and micro-F1 score of 0.904, demonstrating its effectiveness in accurately identifying HCM and CA using a multi-view analysis."],"url":"http://arxiv.org/abs/2404.16522v1","category":"eess.IV"}
{"created":"2024-04-25 09:29:14","title":"Inverse Spectral Problems for Collapsing Manifolds II: Quantitative Stability of Reconstruction for Orbifolds","abstract":"We consider the inverse problem of determining the metric-measure structure of collapsing manifolds from local measurements of spectral data. In the part I of the paper, we proved the uniqueness of the inverse problem and a continuity result for the stability in the closure of Riemannian manifolds with bounded diameter and sectional curvature in the measured Gromov-Hausdorff topology. In this paper we show that when the collapse of dimension is $1$-dimensional, it is possible to obtain quantitative stability of the inverse problem for Riemannian orbifolds. The proof is based on an improved version of the quantitative unique continuation for the wave operator on Riemannian manifolds by removing assumptions on the covariant derivatives of the curvature tensor.","sentences":["We consider the inverse problem of determining the metric-measure structure of collapsing manifolds from local measurements of spectral data.","In the part I of the paper, we proved the uniqueness of the inverse problem and a continuity result for the stability in the closure of Riemannian manifolds with bounded diameter and sectional curvature in the measured Gromov-Hausdorff topology.","In this paper we show that when the collapse of dimension is $1$-dimensional, it is possible to obtain quantitative stability of the inverse problem for Riemannian orbifolds.","The proof is based on an improved version of the quantitative unique continuation for the wave operator on Riemannian manifolds by removing assumptions on the covariant derivatives of the curvature tensor."],"url":"http://arxiv.org/abs/2404.16448v1","category":"math.AP"}
{"created":"2024-04-25 09:16:23","title":"Exponential decay for fractional Schr\u00f6dinger parabolic problems","abstract":"We discuss exponential decay in $L^p(R^N)$, $1\\leq p\\leq \\infty$,of solutions of a fractional Schr\\\"odinger parabolic equation with a locally uniformly integrable potential. The exponential type of the semigroup of solutions is considered and its independence in of $1\\leq p\\leq \\infty$ is addressed. We characterise a large class of potentials for which solutions decay exponentially.","sentences":["We discuss exponential decay in $L^p(R^N)$, $1\\leq p\\leq \\infty$,of solutions of a fractional Schr\\\"odinger parabolic equation with a locally uniformly integrable potential.","The exponential type of the semigroup of solutions is considered and its independence in of $1\\leq p\\leq \\infty$ is addressed.","We characterise a large class of potentials for which solutions decay exponentially."],"url":"http://arxiv.org/abs/2404.16438v1","category":"math.AP"}
{"created":"2024-04-25 09:02:11","title":"Depth Supervised Neural Surface Reconstruction from Airborne Imagery","abstract":"While originally developed for novel view synthesis, Neural Radiance Fields (NeRFs) have recently emerged as an alternative to multi-view stereo (MVS). Triggered by a manifold of research activities, promising results have been gained especially for texture-less, transparent, and reflecting surfaces, while such scenarios remain challenging for traditional MVS-based approaches. However, most of these investigations focus on close-range scenarios, with studies for airborne scenarios still missing. For this task, NeRFs face potential difficulties at areas of low image redundancy and weak data evidence, as often found in street canyons, facades or building shadows. Furthermore, training such networks is computationally expensive. Thus, the aim of our work is twofold: First, we investigate the applicability of NeRFs for aerial image blocks representing different characteristics like nadir-only, oblique and high-resolution imagery. Second, during these investigations we demonstrate the benefit of integrating depth priors from tie-point measures, which are provided during presupposed Bundle Block Adjustment. Our work is based on the state-of-the-art framework VolSDF, which models 3D scenes by signed distance functions (SDFs), since this is more applicable for surface reconstruction compared to the standard volumetric representation in vanilla NeRFs. For evaluation, the NeRF-based reconstructions are compared to results of a publicly available benchmark dataset for airborne images.","sentences":["While originally developed for novel view synthesis, Neural Radiance Fields (NeRFs) have recently emerged as an alternative to multi-view stereo (MVS).","Triggered by a manifold of research activities, promising results have been gained especially for texture-less, transparent, and reflecting surfaces, while such scenarios remain challenging for traditional MVS-based approaches.","However, most of these investigations focus on close-range scenarios, with studies for airborne scenarios still missing.","For this task, NeRFs face potential difficulties at areas of low image redundancy and weak data evidence, as often found in street canyons, facades or building shadows.","Furthermore, training such networks is computationally expensive.","Thus, the aim of our work is twofold:","First, we investigate the applicability of NeRFs for aerial image blocks representing different characteristics like nadir-only, oblique and high-resolution imagery.","Second, during these investigations we demonstrate the benefit of integrating depth priors from tie-point measures, which are provided during presupposed Bundle Block Adjustment.","Our work is based on the state-of-the-art framework VolSDF, which models 3D scenes by signed distance functions (SDFs), since this is more applicable for surface reconstruction compared to the standard volumetric representation in vanilla NeRFs.","For evaluation, the NeRF-based reconstructions are compared to results of a publicly available benchmark dataset for airborne images."],"url":"http://arxiv.org/abs/2404.16429v1","category":"cs.CV"}
{"created":"2024-04-25 08:56:35","title":"Instanton's Insertions to arbitrary non flat Connections in $\\mathbb{R}^4$","abstract":"Given a connection $A$ on a $SU(2)$-bundle $P$ over $\\mathbb{R}^4$ with finite Yang-Mills energy $YM(A)$ and nonzero curvature $F_A(0)$ at the origin, and given $\\rho>0$ small enough, we construct a new connection $\\hat A$ on a bundle $\\hat P$ of different Chern class ($|c_2(A)-c_2(\\hat A)|=8\\pi^2$), in such a way that $\\hat A$ is gauge equivalent to $A$ in $\\mathbb{R}^4\\setminus B_\\rho(0)$ and $$YM(\\hat A)\\le YM(A)+8\\pi^2-\\varepsilon_0\\rho^4|F_A(0)|^2$$ for a universal constant $\\varepsilon_0>0$.","sentences":["Given a connection $A$ on a $SU(2)$-bundle $P$ over $\\mathbb{R}^4$ with finite Yang-Mills energy $YM(A)$ and nonzero curvature $F_A(0)$ at the origin, and given $\\rho>0$ small enough, we construct a new connection $\\hat A$ on a bundle $\\hat P$ of different Chern class ($|c_2(A)-c_2(\\hat A)|=8\\pi^2$), in such a way that $\\hat A$ is gauge equivalent to $A$ in $\\mathbb{R}^4\\setminus B_\\rho(0)$ and $$YM(\\hat A)\\le YM(A)+8\\pi^2-\\varepsilon_0\\rho^4|F_A(0)|^2$$ for a universal constant $\\varepsilon_0>0$."],"url":"http://arxiv.org/abs/2404.16426v1","category":"math.DG"}
{"created":"2024-04-25 07:45:43","title":"One-bubble nodal blow-up for asymptotically critical stationary Schr\u00f6dinger-type equations","abstract":"We investigate in this work families $(u_\\epsilon)_{\\epsilon >0}$ of sign-changing blowing-up solutions of asymptotically critical stationary nonlinear Schr\\\"odinger equations of the following type: $$\\Delta_g u_\\epsilon + h_\\epsilon u_\\epsilon = |u_{\\epsilon}|^{p_\\epsilon-2} u_\\epsilon $$ in a closed manifold $(M,g)$, where $h_\\epsilon$ converges to $h$ in $C^1(M)$. Assuming that $(u_\\epsilon)_{\\epsilon >0}$ blows-up as \\emph{a single sign-changing bubble}, we obtain necessary conditions for blow-up that constrain the localisation of blow-up points and exhibit a strong interaction between $h$, the geometry of $(M,g)$ and the bubble itself. These conditions are new and are a consequence of the sign-changing nature of $u_\\epsilon$.","sentences":["We investigate in this work families $(u_\\epsilon)_{\\epsilon >0}$ of sign-changing blowing-up solutions of asymptotically critical stationary nonlinear Schr\\\"odinger equations of the following type: $$\\Delta_g u_\\epsilon + h_\\epsilon u_\\epsilon = |u_{\\epsilon}|^{p_\\epsilon-2} u_\\epsilon $$ in a closed manifold $(M,g)$, where $h_\\epsilon$ converges to $h$ in $C^1(M)$. Assuming that $(u_\\epsilon)_{\\epsilon >0}$ blows-up as \\emph{a single sign-changing bubble}, we obtain necessary conditions for blow-up that constrain the localisation of blow-up points and exhibit a strong interaction between $h$, the geometry of $(M,g)$ and the bubble itself.","These conditions are new and are a consequence of the sign-changing nature of $u_\\epsilon$."],"url":"http://arxiv.org/abs/2404.16384v1","category":"math.AP"}
{"created":"2024-04-25 06:54:32","title":"Feature graph construction with static features for malware detection","abstract":"Malware can greatly compromise the integrity and trustworthiness of information and is in a constant state of evolution. Existing feature fusion-based detection methods generally overlook the correlation between features. And mere concatenation of features will reduce the model's characterization ability, lead to low detection accuracy. Moreover, these methods are susceptible to concept drift and significant degradation of the model. To address those challenges, we introduce a feature graph-based malware detection method, MFGraph, to characterize applications by learning feature-to-feature relationships to achieve improved detection accuracy while mitigating the impact of concept drift. In MFGraph, we construct a feature graph using static features extracted from binary PE files, then apply a deep graph convolutional network to learn the representation of the feature graph. Finally, we employ the representation vectors obtained from the output of a three-layer perceptron to differentiate between benign and malicious software. We evaluated our method on the EMBER dataset, and the experimental results demonstrate that it achieves an AUC score of 0.98756 on the malware detection task, outperforming other baseline models. Furthermore, the AUC score of MFGraph decreases by only 5.884% in one year, indicating that it is the least affected by concept drift.","sentences":["Malware can greatly compromise the integrity and trustworthiness of information and is in a constant state of evolution.","Existing feature fusion-based detection methods generally overlook the correlation between features.","And mere concatenation of features will reduce the model's characterization ability, lead to low detection accuracy.","Moreover, these methods are susceptible to concept drift and significant degradation of the model.","To address those challenges, we introduce a feature graph-based malware detection method, MFGraph, to characterize applications by learning feature-to-feature relationships to achieve improved detection accuracy while mitigating the impact of concept drift.","In MFGraph, we construct a feature graph using static features extracted from binary PE files, then apply a deep graph convolutional network to learn the representation of the feature graph.","Finally, we employ the representation vectors obtained from the output of a three-layer perceptron to differentiate between benign and malicious software.","We evaluated our method on the EMBER dataset, and the experimental results demonstrate that it achieves an AUC score of 0.98756 on the malware detection task, outperforming other baseline models.","Furthermore, the AUC score of MFGraph decreases by only 5.884% in one year, indicating that it is the least affected by concept drift."],"url":"http://arxiv.org/abs/2404.16362v1","category":"cs.CR"}
{"created":"2024-04-25 06:42:01","title":"Preparing matrix product states via fusion: constraints and extensions","abstract":"In the era of noisy, intermediate-scale quantum (NISQ) devices, the efficient preparation of many-body resource states is a task of paramount importance. In this paper we focus on the deterministic preparation of matrix-product states (MPS) in constant depth by utilizing measurements and classical communication to fuse smaller states into larger ones. We place strong constraints on the MPS that can be prepared using this method, which we refer to as MPS fusion. Namely, we establish that it is necessary for the MPS to have a flat entanglement spectrum. Using the recently introduced split-index MPS (SIMPS) representation, we then introduce a family of states that belong to interesting phases of matter protected by non-onsite symmetries and serve as resources for long-range quantum teleportation, but which lie beyond the scope of ordinary MPS fusion. It is shown constructively that these states can be prepared in constant depth using a broader class of measurement-assisted protocols, which we dub SIMPS fusion. Even in cases when MPS fusion is possible, using SIMPS fusion can give rise to significantly reduced resource overhead. Our results therefore simultaneously establish the boundaries of conventional MPS fusion and push the envelope of which states can be prepared using measurement-assisted protocols.","sentences":["In the era of noisy, intermediate-scale quantum (NISQ) devices, the efficient preparation of many-body resource states is a task of paramount importance.","In this paper we focus on the deterministic preparation of matrix-product states (MPS) in constant depth by utilizing measurements and classical communication to fuse smaller states into larger ones.","We place strong constraints on the MPS that can be prepared using this method, which we refer to as MPS fusion.","Namely, we establish that it is necessary for the MPS to have a flat entanglement spectrum.","Using the recently introduced split-index MPS (SIMPS) representation, we then introduce a family of states that belong to interesting phases of matter protected by non-onsite symmetries and serve as resources for long-range quantum teleportation, but which lie beyond the scope of ordinary MPS fusion.","It is shown constructively that these states can be prepared in constant depth using a broader class of measurement-assisted protocols, which we dub SIMPS fusion.","Even in cases when MPS fusion is possible, using SIMPS fusion can give rise to significantly reduced resource overhead.","Our results therefore simultaneously establish the boundaries of conventional MPS fusion and push the envelope of which states can be prepared using measurement-assisted protocols."],"url":"http://arxiv.org/abs/2404.16360v1","category":"quant-ph"}
{"created":"2024-04-25 06:11:59","title":"Rigorous derivation of a Hele-Shaw type model and its non-symmetric traveling wave solution","abstract":"In this paper, we consider a Hele-Shaw model that describes tumor growth subject to nutrient supply. This model was recently studied in \\cite{feng2022tumor} via asymptotic analysis. Our contributions are twofold: Firstly, we provide a rigorous derivation of this Hele-Shaw model by taking the incompressible limit of the porous medium reaction-diffusion equation, which solidifies the mathematical foundations of the model. Secondly, from a bifurcation theory perspective, we prove the existence of non-symmetric traveling wave solutions to the model, which reflect the intrinsic boundary instability in tumor growth dynamics.","sentences":["In this paper, we consider a Hele-Shaw model that describes tumor growth subject to nutrient supply.","This model was recently studied in \\cite{feng2022tumor} via asymptotic analysis.","Our contributions are twofold: Firstly, we provide a rigorous derivation of this Hele-Shaw model by taking the incompressible limit of the porous medium reaction-diffusion equation, which solidifies the mathematical foundations of the model.","Secondly, from a bifurcation theory perspective, we prove the existence of non-symmetric traveling wave solutions to the model, which reflect the intrinsic boundary instability in tumor growth dynamics."],"url":"http://arxiv.org/abs/2404.16353v1","category":"math.AP"}
{"created":"2024-04-25 03:45:43","title":"The Weil Correspondence and Universal Special Geometry","abstract":"The Weil correspondence states that the datum of a Seiberg-Witten differential is equivalent to an algebraic group extension of the integrable system associated to the Seiberg-Witten geometry. Remarkably this group extension represents quantum consistent couplings for the $\\mathcal{N}=2$ QFT if and only if the extension is anti-affine in the algebro-geometric sense. The universal special geometry is the algebraic integrable system whose Lagrangian fibers are the anti-affine extension groups; it is defined over a base $\\mathscr{B}$ parametrized by the Coulomb coordinates and the couplings. On the total space of the universal geometry there is a canonical (holomorphic) Euler differential. The ordinary Seiberg-Witten geometries at fixed couplings are symplectic quotients of the universal one, and the Seiberg-Witten differential arises as the reduction of the Euler one in accordance with the Weil correspondence. This universal viewpoint allows to study geometrically the flavor symmetry of the $\\mathcal{N}=2$ SCFT in terms of the Mordell-Weil lattice (with N\\'eron-Tate height) of the Albanese variety $A_\\mathbb{L}$ of the universal geometry seen as a quasi-Abelian variety $Y_\\mathbb{L}$ defined over the function field $\\mathbb{L}\\equiv\\mathbb{C}(\\mathscr{B})$.","sentences":["The Weil correspondence states that the datum of a Seiberg-Witten differential is equivalent to an algebraic group extension of the integrable system associated to the Seiberg-Witten geometry.","Remarkably this group extension represents quantum consistent couplings for the $\\mathcal{N}=2$ QFT if and only if the extension is anti-affine in the algebro-geometric sense.","The universal special geometry is the algebraic integrable system whose Lagrangian fibers are the anti-affine extension groups; it is defined over a base $\\mathscr{B}$ parametrized by the Coulomb coordinates and the couplings.","On the total space of the universal geometry there is a canonical (holomorphic) Euler differential.","The ordinary Seiberg-Witten geometries at fixed couplings are symplectic quotients of the universal one, and the Seiberg-Witten differential arises as the reduction of the Euler one in accordance with the Weil correspondence.","This universal viewpoint allows to study geometrically the flavor symmetry of the $\\mathcal{N}=2$ SCFT in terms of the Mordell-Weil lattice (with N\\'eron-Tate height) of the Albanese variety $A_\\mathbb{L}$ of the universal geometry seen as a quasi-Abelian variety $Y_\\mathbb{L}$ defined over the function field $\\mathbb{L}\\equiv\\mathbb{C}(\\mathscr{B})$."],"url":"http://arxiv.org/abs/2404.16316v1","category":"hep-th"}
{"created":"2024-04-25 02:48:16","title":"Reinforcement Learning with Generative Models for Compact Support Sets","abstract":"Foundation models contain a wealth of information from their vast number of training samples. However, most prior arts fail to extract this information in a precise and efficient way for small sample sizes. In this work, we propose a framework utilizing reinforcement learning as a control for foundation models, allowing for the granular generation of small, focused synthetic support sets to augment the performance of neural network models on real data classification tasks. We first allow a reinforcement learning agent access to a novel context based dictionary; the agent then uses this dictionary with a novel prompt structure to form and optimize prompts as inputs to generative models, receiving feedback based on a reward function combining the change in validation accuracy and entropy. A support set is formed this way over several exploration steps. Our framework produced excellent results, increasing classification accuracy by significant margins for no additional labelling or data cost.","sentences":["Foundation models contain a wealth of information from their vast number of training samples.","However, most prior arts fail to extract this information in a precise and efficient way for small sample sizes.","In this work, we propose a framework utilizing reinforcement learning as a control for foundation models, allowing for the granular generation of small, focused synthetic support sets to augment the performance of neural network models on real data classification tasks.","We first allow a reinforcement learning agent access to a novel context based dictionary; the agent then uses this dictionary with a novel prompt structure to form and optimize prompts as inputs to generative models, receiving feedback based on a reward function combining the change in validation accuracy and entropy.","A support set is formed this way over several exploration steps.","Our framework produced excellent results, increasing classification accuracy by significant margins for no additional labelling or data cost."],"url":"http://arxiv.org/abs/2404.16300v1","category":"cs.LG"}
{"created":"2024-04-25 02:40:14","title":"The role of conjugacy in the dynamics of time of arrival operators","abstract":"The construction of time of arrival (TOA) operators canonically conjugate to the system Hamiltonian entails finding the solution of a specific second-order partial differential equation called the time kernel equation (TKE). An expanded iterative solution of the TKE has been obtained recently in [Eur. Phys. J. Plus \\textbf{138}, 153 (2023)] but is generally intractable to be useful for arbitrary nonlinear potentials. In this work, we provide an exact analytic solution of the TKE for a special class of potentials satisfying a specific separability condition. The solution enables us to investigate the time evolution of the eigenfunctions of the conjugacy-preserving TOA operators (CPTOA) by coarse graining methods and spatial confinement. We show that the eigenfunctions of the constructed operator exhibit unitary arrival at the intended arrival point at a time equal to their corresponding eigenvalue. Moreover, we examine whether there is a discernible difference in the dynamics between the TOA operators constructed by quantization and those independent of quantization for specific interaction potentials. We find that the CPTOA operator possesses better unitary dynamics over the Weyl-quantized one within numerical accuracy. This allows us determine the role of the canonical commutation relation between time and energy on the observed dynamics of time of arrival operators.","sentences":["The construction of time of arrival (TOA) operators canonically conjugate to the system Hamiltonian entails finding the solution of a specific second-order partial differential equation called the time kernel equation (TKE).","An expanded iterative solution of the TKE has been obtained recently in [Eur. Phys.","J. Plus \\textbf{138}, 153 (2023)]","but is generally intractable to be useful for arbitrary nonlinear potentials.","In this work, we provide an exact analytic solution of the TKE for a special class of potentials satisfying a specific separability condition.","The solution enables us to investigate the time evolution of the eigenfunctions of the conjugacy-preserving TOA operators (CPTOA) by coarse graining methods and spatial confinement.","We show that the eigenfunctions of the constructed operator exhibit unitary arrival at the intended arrival point at a time equal to their corresponding eigenvalue.","Moreover, we examine whether there is a discernible difference in the dynamics between the TOA operators constructed by quantization and those independent of quantization for specific interaction potentials.","We find that the CPTOA operator possesses better unitary dynamics over the Weyl-quantized one within numerical accuracy.","This allows us determine the role of the canonical commutation relation between time and energy on the observed dynamics of time of arrival operators."],"url":"http://arxiv.org/abs/2404.16298v1","category":"quant-ph"}
{"created":"2024-04-25 02:22:54","title":"Duality for differential modules over complete non-archimedean valuation field of characteristic zero","abstract":"Let $K$ be a complete non-archimedean valuation field of characteristic $0$, with non-trivial valuation, equipped with (possibly multiple) commuting bounded derivations. We prove a decomposition theorem for finite differential modules over $K$, where decompositions regarding the extrinsic subsidiary $\\partial$-generic radii of convergence in the sense of Kedlaya-Xiao. Our result is a refinement of a previous decomposition theorem due to Kedlaya and Xiao. As a key step in the proof, we prove a decomposition theorem in a stronger form in the case where $K$ is equipped with a single derivation. To achieve this goal, we construct an object $f_{0*}L_0$ representing the usual dual functor and study some filtrations of $f_{0*}L_0$, which is used to construct the direct summands appearing in our decomposition theorem.","sentences":["Let $K$ be a complete non-archimedean valuation field of characteristic $0$, with non-trivial valuation, equipped with (possibly multiple) commuting bounded derivations.","We prove a decomposition theorem for finite differential modules over $K$, where decompositions regarding the extrinsic subsidiary $\\partial$-generic radii of convergence in the sense of Kedlaya-Xiao.","Our result is a refinement of a previous decomposition theorem due to Kedlaya and Xiao.","As a key step in the proof, we prove a decomposition theorem in a stronger form in the case where $K$ is equipped with a single derivation.","To achieve this goal, we construct an object $f_{0*}L_0$ representing the usual dual functor and study some filtrations of $f_{0*}L_0$, which is used to construct the direct summands appearing in our decomposition theorem."],"url":"http://arxiv.org/abs/2404.16291v1","category":"math.NT"}
{"created":"2024-04-25 02:22:30","title":"On the temporal estimates for the incompressible Navier-Stokes equations and Hall-magnetohydrodynamic equations","abstract":"In this paper, we derive decay rates of the solutions to the incompressible Navier-Stokes equations and Hall-magnetohydrodynamic equations. We first improve the decay rate of weak solutions of these equations by refining the Fourier splitting method with initial data in the space of pseudo-measures. We also deal with these equations with initial data in Lei-Lin spaces and find decay rates of solutions in Lei-Lin spaces.","sentences":["In this paper, we derive decay rates of the solutions to the incompressible Navier-Stokes equations and Hall-magnetohydrodynamic equations.","We first improve the decay rate of weak solutions of these equations by refining the Fourier splitting method with initial data in the space of pseudo-measures.","We also deal with these equations with initial data in Lei-Lin spaces and find decay rates of solutions in Lei-Lin spaces."],"url":"http://arxiv.org/abs/2404.16290v1","category":"math.AP"}
{"created":"2024-04-25 01:53:21","title":"Timely Communications for Remote Inference","abstract":"In this paper, we analyze the impact of data freshness on remote inference systems, where a pre-trained neural network infers a time-varying target (e.g., the locations of vehicles and pedestrians) based on features (e.g., video frames) observed at a sensing node (e.g., a camera). One might expect that the performance of a remote inference system degrades monotonically as the feature becomes stale. Using an information-theoretic analysis, we show that this is true if the feature and target data sequence can be closely approximated as a Markov chain, whereas it is not true if the data sequence is far from Markovian. Hence, the inference error is a function of Age of Information (AoI), where the function could be non-monotonic. To minimize the inference error in real-time, we propose a new \"selection-from-buffer\" model for sending the features, which is more general than the \"generate-at-will\" model used in earlier studies. In addition, we design low-complexity scheduling policies to improve inference performance. For single-source, single-channel systems, we provide an optimal scheduling policy. In multi-source, multi-channel systems, the scheduling problem becomes a multi-action restless multi-armed bandit problem. For this setting, we design a new scheduling policy by integrating Whittle index-based source selection and duality-based feature selection-from-buffer algorithms. This new scheduling policy is proven to be asymptotically optimal. These scheduling results hold for minimizing general AoI functions (monotonic or non-monotonic). Data-driven evaluations demonstrate the significant advantages of our proposed scheduling policies.","sentences":["In this paper, we analyze the impact of data freshness on remote inference systems, where a pre-trained neural network infers a time-varying target (e.g., the locations of vehicles and pedestrians) based on features (e.g., video frames) observed at a sensing node (e.g., a camera).","One might expect that the performance of a remote inference system degrades monotonically as the feature becomes stale.","Using an information-theoretic analysis, we show that this is true if the feature and target data sequence can be closely approximated as a Markov chain, whereas it is not true if the data sequence is far from Markovian.","Hence, the inference error is a function of Age of Information (AoI), where the function could be non-monotonic.","To minimize the inference error in real-time, we propose a new \"selection-from-buffer\" model for sending the features, which is more general than the \"generate-at-will\" model used in earlier studies.","In addition, we design low-complexity scheduling policies to improve inference performance.","For single-source, single-channel systems, we provide an optimal scheduling policy.","In multi-source, multi-channel systems, the scheduling problem becomes a multi-action restless multi-armed bandit problem.","For this setting, we design a new scheduling policy by integrating Whittle index-based source selection and duality-based feature selection-from-buffer algorithms.","This new scheduling policy is proven to be asymptotically optimal.","These scheduling results hold for minimizing general AoI functions (monotonic or non-monotonic).","Data-driven evaluations demonstrate the significant advantages of our proposed scheduling policies."],"url":"http://arxiv.org/abs/2404.16281v1","category":"cs.NI"}
{"created":"2024-04-25 01:01:06","title":"True random number generation using metastable 1T' molybdenum ditelluride","abstract":"True random numbers play a critical role in secure cryptography. The generation relies on a stable and readily extractable entropy source. Here, from solution-processed structurally metastable 1T' MoTe2, we prove stable output of featureless, stochastic, and yet stable conductance noise at a broad temperature (down to 15 K) with minimal power consumption (down to 0.05 micro-W). Our characterizations and statistical analysis of the characteristics of the conductance noise suggest that the noise arises from the volatility of the stochastic polarization of the underlying ferroelectric dipoles in the 1T' MoTe2. Further, as proved in our experiments and indicated by our Monte Carlo simulation, the ferroelectric dipole polarization is a reliable entropy source with the stochastic polarization persistent and stable over time. Exploiting the conductance noise, we achieve the generation of true random numbers and demonstrate their use in common cryptographic applications, for example, password generation and data encryption. Besides, particularly, we show a privacy safeguarding approach to sensitive data that can be critical for the cryptography of neural networks. We believe our work will bring insights into the understanding of the metastable 1T' MoTe2 and, more importantly, underpin its great potential in secure cryptography.","sentences":["True random numbers play a critical role in secure cryptography.","The generation relies on a stable and readily extractable entropy source.","Here, from solution-processed structurally metastable 1T' MoTe2, we prove stable output of featureless, stochastic, and yet stable conductance noise at a broad temperature (down to 15 K) with minimal power consumption (down to 0.05 micro-W).","Our characterizations and statistical analysis of the characteristics of the conductance noise suggest that the noise arises from the volatility of the stochastic polarization of the underlying ferroelectric dipoles in the 1T' MoTe2.","Further, as proved in our experiments and indicated by our Monte Carlo simulation, the ferroelectric dipole polarization is a reliable entropy source with the stochastic polarization persistent and stable over time.","Exploiting the conductance noise, we achieve the generation of true random numbers and demonstrate their use in common cryptographic applications, for example, password generation and data encryption.","Besides, particularly, we show a privacy safeguarding approach to sensitive data that can be critical for the cryptography of neural networks.","We believe our work will bring insights into the understanding of the metastable 1T' MoTe2 and, more importantly, underpin its great potential in secure cryptography."],"url":"http://arxiv.org/abs/2404.16271v1","category":"cs.CR"}
{"created":"2024-04-25 00:30:03","title":"A Multi-objective Optimization Benchmark Test Suite for Real-time Semantic Segmentation","abstract":"As one of the emerging challenges in Automated Machine Learning, the Hardware-aware Neural Architecture Search (HW-NAS) tasks can be treated as black-box multi-objective optimization problems (MOPs). An important application of HW-NAS is real-time semantic segmentation, which plays a pivotal role in autonomous driving scenarios. The HW-NAS for real-time semantic segmentation inherently needs to balance multiple optimization objectives, including model accuracy, inference speed, and hardware-specific considerations. Despite its importance, benchmarks have yet to be developed to frame such a challenging task as multi-objective optimization. To bridge the gap, we introduce a tailored streamline to transform the task of HW-NAS for real-time semantic segmentation into standard MOPs. Building upon the streamline, we present a benchmark test suite, CitySeg/MOP, comprising fifteen MOPs derived from the Cityscapes dataset. The CitySeg/MOP test suite is integrated into the EvoXBench platform to provide seamless interfaces with various programming languages (e.g., Python and MATLAB) for instant fitness evaluations. We comprehensively assessed the CitySeg/MOP test suite on various multi-objective evolutionary algorithms, showcasing its versatility and practicality. Source codes are available at https://github.com/EMI-Group/evoxbench.","sentences":["As one of the emerging challenges in Automated Machine Learning, the Hardware-aware Neural Architecture Search (HW-NAS) tasks can be treated as black-box multi-objective optimization problems (MOPs).","An important application of HW-NAS is real-time semantic segmentation, which plays a pivotal role in autonomous driving scenarios.","The HW-NAS for real-time semantic segmentation inherently needs to balance multiple optimization objectives, including model accuracy, inference speed, and hardware-specific considerations.","Despite its importance, benchmarks have yet to be developed to frame such a challenging task as multi-objective optimization.","To bridge the gap, we introduce a tailored streamline to transform the task of HW-NAS for real-time semantic segmentation into standard MOPs.","Building upon the streamline, we present a benchmark test suite, CitySeg/MOP, comprising fifteen MOPs derived from the Cityscapes dataset.","The CitySeg/MOP test suite is integrated into the EvoXBench platform to provide seamless interfaces with various programming languages (e.g., Python and MATLAB) for instant fitness evaluations.","We comprehensively assessed the CitySeg/MOP test suite on various multi-objective evolutionary algorithms, showcasing its versatility and practicality.","Source codes are available at https://github.com/EMI-Group/evoxbench."],"url":"http://arxiv.org/abs/2404.16266v1","category":"cs.CV"}
{"created":"2024-04-24 23:56:03","title":"Enhancing Privacy in Face Analytics Using Fully Homomorphic Encryption","abstract":"Modern face recognition systems utilize deep neural networks to extract salient features from a face. These features denote embeddings in latent space and are often stored as templates in a face recognition system. These embeddings are susceptible to data leakage and, in some cases, can even be used to reconstruct the original face image. To prevent compromising identities, template protection schemes are commonly employed. However, these schemes may still not prevent the leakage of soft biometric information such as age, gender and race. To alleviate this issue, we propose a novel technique that combines Fully Homomorphic Encryption (FHE) with an existing template protection scheme known as PolyProtect. We show that the embeddings can be compressed and encrypted using FHE and transformed into a secure PolyProtect template using polynomial transformation, for additional protection. We demonstrate the efficacy of the proposed approach through extensive experiments on multiple datasets. Our proposed approach ensures irreversibility and unlinkability, effectively preventing the leakage of soft biometric attributes from face embeddings without compromising recognition accuracy.","sentences":["Modern face recognition systems utilize deep neural networks to extract salient features from a face.","These features denote embeddings in latent space and are often stored as templates in a face recognition system.","These embeddings are susceptible to data leakage and, in some cases, can even be used to reconstruct the original face image.","To prevent compromising identities, template protection schemes are commonly employed.","However, these schemes may still not prevent the leakage of soft biometric information such as age, gender and race.","To alleviate this issue, we propose a novel technique that combines Fully Homomorphic Encryption (FHE) with an existing template protection scheme known as PolyProtect.","We show that the embeddings can be compressed and encrypted using FHE and transformed into a secure PolyProtect template using polynomial transformation, for additional protection.","We demonstrate the efficacy of the proposed approach through extensive experiments on multiple datasets.","Our proposed approach ensures irreversibility and unlinkability, effectively preventing the leakage of soft biometric attributes from face embeddings without compromising recognition accuracy."],"url":"http://arxiv.org/abs/2404.16255v1","category":"cs.CR"}
{"created":"2024-04-24 23:19:54","title":"Computationally Efficient Molecular Integrals of Solid Harmonic Gaussian Orbitals Using Quantum Entanglement of Angular Momentum","abstract":"Vector-coupling and vector-uncoupling schemes in the quantum theory of angular momentum correspond to unitary Clebsch-Gordan transformations that operate on quantum angular momentum states and thereby control their degree of entanglement. The addition of quantum angular momentum from this transformation is suitable for reducing the degree of entanglement of quantum angular momentum, leading to simple and effective calculations of the molecular integrals of solid harmonic Gaussian orbitals (SHGO). Even with classical computers, the speed-up ratio in the evaluation of molecular nuclear Coulomb integrals with SHGOs can be up to four orders of magnitude for atomic orbitals with high angular momentum quantum number. Thus, the less entanglement there is for a quantum system the easier it is to simulate, and molecular integrals with SHGOs are shown to be particularly well-suited for quantum computing. High-efficiency quantum circuits previously developed for unitary and cascading Clebsch-Gordan transformations of angular momentum states can be applied to the differential and product rules of solid harmonics to efficiently compute two-electron Coulomb integrals ubiquitous in quantum chemistry. Combined with such quantum circuits and variational quantum eigensolver algorithms, the high computational efficiency of molecular integrals in solid harmonic bases unveiled in this paper may open an avenue for accelerating full quantum computing chemistry.","sentences":["Vector-coupling and vector-uncoupling schemes in the quantum theory of angular momentum correspond to unitary Clebsch-Gordan transformations that operate on quantum angular momentum states and thereby control their degree of entanglement.","The addition of quantum angular momentum from this transformation is suitable for reducing the degree of entanglement of quantum angular momentum, leading to simple and effective calculations of the molecular integrals of solid harmonic Gaussian orbitals (SHGO).","Even with classical computers, the speed-up ratio in the evaluation of molecular nuclear Coulomb integrals with SHGOs can be up to four orders of magnitude for atomic orbitals with high angular momentum quantum number.","Thus, the less entanglement there is for a quantum system the easier it is to simulate, and molecular integrals with SHGOs are shown to be particularly well-suited for quantum computing.","High-efficiency quantum circuits previously developed for unitary and cascading Clebsch-Gordan transformations of angular momentum states can be applied to the differential and product rules of solid harmonics to efficiently compute two-electron Coulomb integrals ubiquitous in quantum chemistry.","Combined with such quantum circuits and variational quantum eigensolver algorithms, the high computational efficiency of molecular integrals in solid harmonic bases unveiled in this paper may open an avenue for accelerating full quantum computing chemistry."],"url":"http://arxiv.org/abs/2404.16245v1","category":"quant-ph"}
{"created":"2024-04-24 22:53:50","title":"Synthesis of layered gold tellurides AuSbTe and Au$_2$Te$_3$ and their semiconducting and metallic behavior","abstract":"Previous studies on natural samples of pampaloite (AuSbTe) revealed the crystal structure of a potentially cleavable and/or exfoliable material, while studies on natural and synthetic montbrayite (Sb-containing Au$_2$Te$_3$) claimed various chemical compositions for this low symmetry compound. Few investigations of synthetic samples have been reported for both materials, leaving much of their chemical, thermal and electronic characteristics unknown. Here, we investigate the stability, electronic properties and synthesis of the gold antimony tellurides AuSbTe and Au$_{1.9}$Sb$_{0.46}$Te$_{2.64}$ (montbrayite). Differential thermal analysis and $\\textit{in situ}$ powder x-ray diffraction revealed that AuSbTe is incongruently melting, while Au$_{1.9}$Sb$_{0.46}$Te$_{2.64}$ is congruently melting. Calculations of the band structures and four-point resistivity measurements showed that AuSbTe is a semiconductor and Au$_{1.9}$Sb$_{0.46}$Te$_{2.64}$ a metal. Various synthesis attempts confirmed the limited stable chemical composition of Au$_{1.9}$Sb$_{0.46}$Te$_{2.64}$, identified successful methods to synthesize both compounds, and highlighted the challenges associated with single crystal synthesis of AuSbTe.","sentences":["Previous studies on natural samples of pampaloite (AuSbTe) revealed the crystal structure of a potentially cleavable and/or exfoliable material, while studies on natural and synthetic montbrayite (Sb-containing Au$_2$Te$_3$) claimed various chemical compositions for this low symmetry compound.","Few investigations of synthetic samples have been reported for both materials, leaving much of their chemical, thermal and electronic characteristics unknown.","Here, we investigate the stability, electronic properties and synthesis of the gold antimony tellurides AuSbTe and Au$_{1.9}$Sb$_{0.46}$Te$_{2.64}$ (montbrayite).","Differential thermal analysis and $\\textit{in situ}$ powder x-ray diffraction revealed that AuSbTe is incongruently melting, while Au$_{1.9}$Sb$_{0.46}$Te$_{2.64}$ is congruently melting.","Calculations of the band structures and four-point resistivity measurements showed that AuSbTe is a semiconductor and Au$_{1.9}$Sb$_{0.46}$Te$_{2.64}$ a metal.","Various synthesis attempts confirmed the limited stable chemical composition of Au$_{1.9}$Sb$_{0.46}$Te$_{2.64}$, identified successful methods to synthesize both compounds, and highlighted the challenges associated with single crystal synthesis of AuSbTe."],"url":"http://arxiv.org/abs/2404.16239v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-24 22:24:17","title":"A proof theory of (omega-)context-free languages, via non-wellfounded proofs","abstract":"We investigate the proof theory of regular expressions with fixed points, construed as a notation for (omega-)context-free grammars. Starting with a hypersequential system for regular expressions due to Das and Pous, we define its extension by least fixed points and prove soundness and completeness of its non-wellfounded proofs for the standard language model. From here we apply proof-theoretic techniques to recover an infinitary axiomatisation of the resulting equational theory, complete for inclusions of context-free languages. Finally, we extend our syntax by greatest fixed points, now computing omega-context-free languages. We show the soundness and completeness of the corresponding system using a mixture of proof-theoretic and game-theoretic techniques.","sentences":["We investigate the proof theory of regular expressions with fixed points, construed as a notation for (omega-)context-free grammars.","Starting with a hypersequential system for regular expressions due to Das and Pous, we define its extension by least fixed points and prove soundness and completeness of its non-wellfounded proofs for the standard language model.","From here we apply proof-theoretic techniques to recover an infinitary axiomatisation of the resulting equational theory, complete for inclusions of context-free languages.","Finally, we extend our syntax by greatest fixed points, now computing omega-context-free languages.","We show the soundness and completeness of the corresponding system using a mixture of proof-theoretic and game-theoretic techniques."],"url":"http://arxiv.org/abs/2404.16231v1","category":"cs.LO"}
{"created":"2024-04-24 21:43:15","title":"NeRF-XL: Scaling NeRFs with Multiple GPUs","abstract":"We present NeRF-XL, a principled method for distributing Neural Radiance Fields (NeRFs) across multiple GPUs, thus enabling the training and rendering of NeRFs with an arbitrarily large capacity. We begin by revisiting existing multi-GPU approaches, which decompose large scenes into multiple independently trained NeRFs, and identify several fundamental issues with these methods that hinder improvements in reconstruction quality as additional computational resources (GPUs) are used in training. NeRF-XL remedies these issues and enables the training and rendering of NeRFs with an arbitrary number of parameters by simply using more hardware. At the core of our method lies a novel distributed training and rendering formulation, which is mathematically equivalent to the classic single-GPU case and minimizes communication between GPUs. By unlocking NeRFs with arbitrarily large parameter counts, our approach is the first to reveal multi-GPU scaling laws for NeRFs, showing improvements in reconstruction quality with larger parameter counts and speed improvements with more GPUs. We demonstrate the effectiveness of NeRF-XL on a wide variety of datasets, including the largest open-source dataset to date, MatrixCity, containing 258K images covering a 25km^2 city area.","sentences":["We present NeRF-XL, a principled method for distributing Neural Radiance Fields (NeRFs) across multiple GPUs, thus enabling the training and rendering of NeRFs with an arbitrarily large capacity.","We begin by revisiting existing multi-GPU approaches, which decompose large scenes into multiple independently trained NeRFs, and identify several fundamental issues with these methods that hinder improvements in reconstruction quality as additional computational resources (GPUs) are used in training.","NeRF-XL remedies these issues and enables the training and rendering of NeRFs with an arbitrary number of parameters by simply using more hardware.","At the core of our method lies a novel distributed training and rendering formulation, which is mathematically equivalent to the classic single-GPU case and minimizes communication between GPUs.","By unlocking NeRFs with arbitrarily large parameter counts, our approach is the first to reveal multi-GPU scaling laws for NeRFs, showing improvements in reconstruction quality with larger parameter counts and speed improvements with more GPUs.","We demonstrate the effectiveness of NeRF-XL on a wide variety of datasets, including the largest open-source dataset to date, MatrixCity, containing 258K images covering a 25km^2 city area."],"url":"http://arxiv.org/abs/2404.16221v1","category":"cs.CV"}
{"created":"2024-04-24 20:57:07","title":"Optimized higher-order photon state classification by machine learning","abstract":"The classification of higher-order photon emission becomes important with more methods being developed for deterministic multiphoton generation. The widely-used second-order correlation g(2) is not sufficient to determine the quantum purity of higher photon Fock states. Traditional characterization methods require a large amount of photon detection events which leads to increased measurement and computation time. Here, we demonstrate a Machine Learning model based on a 2D Convolutional Neural Network (CNN) for rapid classification of multiphoton Fock states up to |3> with an overall accuracy of 94%. By fitting the g(3) correlation with simulated photon detection events, the model exhibits efficient performance particularly with sparse correlation data, with 800 co-detection events to achieve an accuracy of 90%. Using the proposed experimental setup, this CNN classifier opens up the possibility for quasi real-time classification of higher photon states, which holds broad applications in quantum technologies.","sentences":["The classification of higher-order photon emission becomes important with more methods being developed for deterministic multiphoton generation.","The widely-used second-order correlation g(2) is not sufficient to determine the quantum purity of higher photon Fock states.","Traditional characterization methods require a large amount of photon detection events which leads to increased measurement and computation time.","Here, we demonstrate a Machine Learning model based on a 2D Convolutional Neural Network (CNN) for rapid classification of multiphoton Fock states up to |3> with an overall accuracy of 94%.","By fitting the g(3) correlation with simulated photon detection events, the model exhibits efficient performance particularly with sparse correlation data, with 800 co-detection events to achieve an accuracy of 90%.","Using the proposed experimental setup, this CNN classifier opens up the possibility for quasi real-time classification of higher photon states, which holds broad applications in quantum technologies."],"url":"http://arxiv.org/abs/2404.16203v1","category":"quant-ph"}
{"created":"2024-04-24 20:26:34","title":"Structure Preserving PINN for Solving Time Dependent PDEs with Periodic Boundary","abstract":"We present a structure preserving PINN for solving a series of time dependent PDEs with periodic boundary. Our method can incorporate the periodic boundary condition as the natural output of any deep neural net, hence significantly improving the training accuracy of baseline PINN. Together with mini-batching and other PINN variants (SA-PINN, RBA-PINN, etc.), our structure preserving PINN can even handle stiff PDEs for modeling a wide range of convection-diffusion and reaction-diffusion processes. We demonstrate the effectiveness of our PINNs on various PDEs from Allen Cahn, Gray Scott to nonlinear Schrodinger.","sentences":["We present a structure preserving PINN for solving a series of time dependent PDEs with periodic boundary.","Our method can incorporate the periodic boundary condition as the natural output of any deep neural net, hence significantly improving the training accuracy of baseline PINN.","Together with mini-batching and other PINN variants (SA-PINN, RBA-PINN, etc.), our structure preserving PINN can even handle stiff PDEs for modeling a wide range of convection-diffusion and reaction-diffusion processes.","We demonstrate the effectiveness of our PINNs on various PDEs from Allen Cahn, Gray Scott to nonlinear Schrodinger."],"url":"http://arxiv.org/abs/2404.16189v1","category":"math.NA"}
{"created":"2024-04-24 20:21:54","title":"Deep Learning Interatomic Potential Connects Molecular Structural Ordering to Macroscale Properties of Polyacrylonitrile (PAN) Polymer","abstract":"Polyacrylonitrile (PAN) is an important commercial polymer, bearing atactic stereochemistry resulting from nonselective radical polymerization. As such, an accurate, fundamental understanding of governing interactions among PAN molecular units are indispensable to advance the design principles of final products at reduced processability costs. While ab initio molecular dynamics (AIMD) simulations can provide the necessary accuracy for treating key interactions in polar polymers such as dipole-dipole interactions and hydrogen bonding, and analyzing their influence on molecular orientation, their implementation is limited to small molecules only. Herein, we show that the neural network interatomic potentials (NNIP) that are trained on the small-scale AIMD data (acquired for oligomers) can be efficiently employed to examine the structures/properties at large scales (polymers). NNIP provides critical insight into intra- and interchain hydrogen bonding and dipolar correlations, and accurately predicts the amorphous bulk PAN structure validated by modeling the experimental X-ray structure factor. Furthermore, the NNIP-predicted PAN properties such as density and elastic modulus are in good agreement with their experimental values. Overall, the trend in the elastic modulus is found to correlate strongly with the PAN structural orientations encoded in Hermans orientation factor. This study enables the ability to predict the structure-property relations for PAN and analogs with sustainable ab initio accuracy across scales.","sentences":["Polyacrylonitrile (PAN) is an important commercial polymer, bearing atactic stereochemistry resulting from nonselective radical polymerization.","As such, an accurate, fundamental understanding of governing interactions among PAN molecular units are indispensable to advance the design principles of final products at reduced processability costs.","While ab initio molecular dynamics (AIMD) simulations can provide the necessary accuracy for treating key interactions in polar polymers such as dipole-dipole interactions and hydrogen bonding, and analyzing their influence on molecular orientation, their implementation is limited to small molecules only.","Herein, we show that the neural network interatomic potentials (NNIP) that are trained on the small-scale AIMD data (acquired for oligomers) can be efficiently employed to examine the structures/properties at large scales (polymers).","NNIP provides critical insight into intra- and interchain hydrogen bonding and dipolar correlations, and accurately predicts the amorphous bulk PAN structure validated by modeling the experimental X-ray structure factor.","Furthermore, the NNIP-predicted PAN properties such as density and elastic modulus are in good agreement with their experimental values.","Overall, the trend in the elastic modulus is found to correlate strongly with the PAN structural orientations encoded in Hermans orientation factor.","This study enables the ability to predict the structure-property relations for PAN and analogs with sustainable ab initio accuracy across scales."],"url":"http://arxiv.org/abs/2404.16187v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-24 20:04:49","title":"A note on the lifespan of solutions to the semilinear wave equation with weighted nonlinearity","abstract":"We investigate the lifespan of solutions to a specific variant of the semilinear wave equation, which incorporates weighted nonlinearity $$ u_{tt}-u_{xx} =|x|^\\alpha |u|^p, \\quad\\mbox{for}\\;\\;\\; (t,x)\\in (0,\\infty)\\times\\mathbb{R}, $$ where $p>1$, $\\alpha\\in\\mathbb{R}$. We explore the behavior of solutions for small initial data, considering the influence of weighted nonlinearities on the lifespan.","sentences":["We investigate the lifespan of solutions to a specific variant of the semilinear wave equation, which incorporates weighted nonlinearity $$ u_{tt}-u_{xx} =|x|^\\alpha |u|^p, \\quad\\mbox{for}\\;\\;\\; (t,x)\\in (0,\\infty)\\times\\mathbb{R}, $$ where $p>1$, $\\alpha\\in\\mathbb{R}$. We explore the behavior of solutions for small initial data, considering the influence of weighted nonlinearities on the lifespan."],"url":"http://arxiv.org/abs/2404.16173v1","category":"math.AP"}
{"created":"2024-04-24 19:25:15","title":"Convergence of stochastic integrals with applications to transport equations and conservation laws with noise","abstract":"Convergence of stochastic integrals driven by Wiener processes $W_n$, with $W_n \\to W$ almost surely in $C_t$, is crucial in analyzing SPDEs. Our focus is on the convergence of the form $\\int_0^T V_n\\, \\mathrm{d} W_n \\to \\int_0^T V\\, \\mathrm{d} W$, where $\\{V_n\\}$ is bounded in $L^p(\\Omega \\times [0,T];X)$ for a Banach space $X$ and some finite $p > 2$. This is challenging when $V_n$ converges to $V$ weakly in the temporal variable. We supply convergence results to handle stochastic integral limits when strong temporal convergence is lacking. A key tool is a uniform mean $L^1$ time translation estimate on $V_n$, an estimate that is easily verified in many SPDEs. However, this estimate alone does not guarantee strong compactness of $(\\omega,t)\\mapsto V_n(\\omega,t)$. Our findings, especially pertinent to equations exhibiting singular behavior, are substantiated by establishing several stability results for stochastic transport equations and conservation laws.","sentences":["Convergence of stochastic integrals driven by Wiener processes $W_n$, with $W_n \\to W$ almost surely in $C_t$, is crucial in analyzing SPDEs.","Our focus is on the convergence of the form $\\int_0^T V_n\\, \\mathrm{d} W_n \\to \\int_0^T V\\, \\mathrm{d} W$, where $\\{V_n\\}$ is bounded in $L^p(\\Omega \\times","[0,T];X)$ for a Banach space $X$ and some finite $p > 2$.","This is challenging when","$V_n$ converges to $V$ weakly in the temporal variable.","We supply convergence results to handle stochastic integral limits when strong temporal convergence is lacking.","A key tool is a uniform mean $L^1$ time translation estimate on $V_n$, an estimate that is easily verified in many SPDEs.","However, this estimate alone does not guarantee strong compactness of $(\\omega,t)\\mapsto V_n(\\omega,t)$. Our findings, especially pertinent to equations exhibiting singular behavior, are substantiated by establishing several stability results for stochastic transport equations and conservation laws."],"url":"http://arxiv.org/abs/2404.16157v1","category":"math.PR"}
{"created":"2024-04-24 19:06:00","title":"Static Blackhole with Cosmological Influence: Whittaker Solutions","abstract":"In this article, we investigate the impact of cosmological parameters on black holes using an exact solution to Einstein's equations that satisfies the Whittaker equation of state. We examine a spherically symmetric black hole in the background of a static Einstein Universe with a perfect fluid source with the cosmological constant. This solution is characterized by two independent parameters, namely the size of the universe~($R$) and the cosmological constant~(Lambda), representing the cosmological influences. We explore phenomena such as periastron precession and the scattering of massless scalar fields to determine how these cosmological parameters affect the physics around black holes.","sentences":["In this article, we investigate the impact of cosmological parameters on black holes using an exact solution to Einstein's equations that satisfies the Whittaker equation of state.","We examine a spherically symmetric black hole in the background of a static Einstein Universe with a perfect fluid source with the cosmological constant.","This solution is characterized by two independent parameters, namely the size of the universe~($R$) and the cosmological constant~(Lambda), representing the cosmological influences.","We explore phenomena such as periastron precession and the scattering of massless scalar fields to determine how these cosmological parameters affect the physics around black holes."],"url":"http://arxiv.org/abs/2404.16146v1","category":"gr-qc"}
{"created":"2024-04-24 18:58:00","title":"Machine-Learned Closure of URANS for Stably Stratified Turbulence: Connecting Physical Timescales & Data Hyperparameters of Deep Time-Series Models","abstract":"We develop time-series machine learning (ML) methods for closure modeling of the Unsteady Reynolds Averaged Navier Stokes (URANS) equations applied to stably stratified turbulence (SST). SST is strongly affected by fine balances between forces and becomes more anisotropic in time for decaying cases. Moreover, there is a limited understanding of the physical phenomena described by some of the terms in the URANS equations. Rather than attempting to model each term separately, it is attractive to explore the capability of machine learning to model groups of terms, i.e., to directly model the force balances. We consider decaying SST which are homogeneous and stably stratified by a uniform density gradient, enabling dimensionality reduction. We consider two time-series ML models: Long Short-Term Memory (LSTM) and Neural Ordinary Differential Equation (NODE). Both models perform accurately and are numerically stable in a posteriori tests. Furthermore, we explore the data requirements of the ML models by extracting physically relevant timescales of the complex system. We find that the ratio of the timescales of the minimum information required by the ML models to accurately capture the dynamics of the SST corresponds to the Reynolds number of the flow. The current framework provides the backbone to explore the capability of such models to capture the dynamics of higher-dimensional complex SST flows.","sentences":["We develop time-series machine learning (ML) methods for closure modeling of the Unsteady Reynolds Averaged Navier Stokes (URANS) equations applied to stably stratified turbulence (SST).","SST is strongly affected by fine balances between forces and becomes more anisotropic in time for decaying cases.","Moreover, there is a limited understanding of the physical phenomena described by some of the terms in the URANS equations.","Rather than attempting to model each term separately, it is attractive to explore the capability of machine learning to model groups of terms, i.e., to directly model the force balances.","We consider decaying SST which are homogeneous and stably stratified by a uniform density gradient, enabling dimensionality reduction.","We consider two time-series ML models: Long Short-Term Memory (LSTM) and Neural Ordinary Differential Equation (NODE).","Both models perform accurately and are numerically stable in a posteriori tests.","Furthermore, we explore the data requirements of the ML models by extracting physically relevant timescales of the complex system.","We find that the ratio of the timescales of the minimum information required by the ML models to accurately capture the dynamics of the SST corresponds to the Reynolds number of the flow.","The current framework provides the backbone to explore the capability of such models to capture the dynamics of higher-dimensional complex SST flows."],"url":"http://arxiv.org/abs/2404.16141v1","category":"physics.flu-dyn"}
{"created":"2024-04-24 18:57:54","title":"Organizing Physics with Open Energy-Driven Systems","abstract":"Organizing physics has been a long-standing preoccupation of applied category theory, going back at least to Lawvere. We contribute to this research thread by noticing that Hamiltonian mechanics and gradient descent depend crucially on a consistent choice of transformation -- which we call a reaction structure -- from the cotangent bundle to the tangent bundle. We then construct a compositional theory of reaction structures. Reaction-based systems offer a different perspective on composition in physics than port-Hamiltonian systems or open classical mechanics, in that reaction-based composition does not create any new constraints that must be solved for algebraically.   The technical contributions of this paper are the development of symmetric monoidal categories of open energy-driven systems and open differential equations, and a functor between them, functioning as a \"functorial semantics\" for reaction structures. This approach echoes what has previously been done for open games and open gradient-based learners, and in fact subsumes the latter. We then illustrate our theory by constructing an $n$-fold pendulum as a composite of $n$-many pendula.","sentences":["Organizing physics has been a long-standing preoccupation of applied category theory, going back at least to Lawvere.","We contribute to this research thread by noticing that Hamiltonian mechanics and gradient descent depend crucially on a consistent choice of transformation -- which we call a reaction structure -- from the cotangent bundle to the tangent bundle.","We then construct a compositional theory of reaction structures.","Reaction-based systems offer a different perspective on composition in physics than port-Hamiltonian systems or open classical mechanics, in that reaction-based composition does not create any new constraints that must be solved for algebraically.   ","The technical contributions of this paper are the development of symmetric monoidal categories of open energy-driven systems and open differential equations, and a functor between them, functioning as a \"functorial semantics\" for reaction structures.","This approach echoes what has previously been done for open games and open gradient-based learners, and in fact subsumes the latter.","We then illustrate our theory by constructing an $n$-fold pendulum as a composite of $n$-many pendula."],"url":"http://arxiv.org/abs/2404.16140v1","category":"math.CT"}
{"created":"2024-04-24 18:45:50","title":"Power Failure Cascade Prediction using Graph Neural Networks","abstract":"We consider the problem of predicting power failure cascades due to branch failures. We propose a flow-free model based on graph neural networks that predicts grid states at every generation of a cascade process given an initial contingency and power injection values. We train the proposed model using a cascade sequence data pool generated from simulations. We then evaluate our model at various levels of granularity. We present several error metrics that gauge the model's ability to predict the failure size, the final grid state, and the failure time steps of each branch within the cascade. We benchmark the graph neural network model against influence models. We show that, in addition to being generic over randomly scaled power injection values, the graph neural network model outperforms multiple influence models that are built specifically for their corresponding loading profiles. Finally, we show that the proposed model reduces the computational time by almost two orders of magnitude.","sentences":["We consider the problem of predicting power failure cascades due to branch failures.","We propose a flow-free model based on graph neural networks that predicts grid states at every generation of a cascade process given an initial contingency and power injection values.","We train the proposed model using a cascade sequence data pool generated from simulations.","We then evaluate our model at various levels of granularity.","We present several error metrics that gauge the model's ability to predict the failure size, the final grid state, and the failure time steps of each branch within the cascade.","We benchmark the graph neural network model against influence models.","We show that, in addition to being generic over randomly scaled power injection values, the graph neural network model outperforms multiple influence models that are built specifically for their corresponding loading profiles.","Finally, we show that the proposed model reduces the computational time by almost two orders of magnitude."],"url":"http://arxiv.org/abs/2404.16134v1","category":"eess.SY"}
{"created":"2024-04-24 18:20:03","title":"Boomerang effect in classical stochastic models","abstract":"The phenomenon of Anderson localization, occurring in a disordered medium, significantly influences the dynamics of quantum particles. A fascinating manifestation of this is the \"quantum boomerang effect\" (QBE), observed when a quantum particle, propelled with a finite initial velocity, reverses its average trajectory, eventually halting at its starting point. This effect has recently been demonstrated in an experiment replicating the quantum kicked-rotor model. This research delves into the classical analog of QBE. We uncover evidence of a similar effect in classical systems, characterized by the absence of typical diffusion processes. Our investigation encompasses both simplified probabilistic models and more complex phenomenological models that link classical with quantum mechanics. The results indicate that the boomerang effect is not confined to the quantum realm and may also be present in diverse systems exhibiting subdiffusive behavior.","sentences":["The phenomenon of Anderson localization, occurring in a disordered medium, significantly influences the dynamics of quantum particles.","A fascinating manifestation of this is the \"quantum boomerang effect\" (QBE), observed when a quantum particle, propelled with a finite initial velocity, reverses its average trajectory, eventually halting at its starting point.","This effect has recently been demonstrated in an experiment replicating the quantum kicked-rotor model.","This research delves into the classical analog of QBE.","We uncover evidence of a similar effect in classical systems, characterized by the absence of typical diffusion processes.","Our investigation encompasses both simplified probabilistic models and more complex phenomenological models that link classical with quantum mechanics.","The results indicate that the boomerang effect is not confined to the quantum realm and may also be present in diverse systems exhibiting subdiffusive behavior."],"url":"http://arxiv.org/abs/2404.16119v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-24 18:12:34","title":"Electric and magnetic waveguides in graphene: quantum and classical","abstract":"Electric and magnetic waveguides are considered in planar Dirac materials like graphene as well as their classical version for relativistic particles of zero mass and electric charge. In order to solve the Dirac-Weyl equation analytically, we have assumed the displacement symmetry of the system along a direction. In these conditions we have examined the rest of symmetries relevant each type, magnetic or electric system, which will determine their similarities and differences. We have worked out waveguides with square profile in detail to show up some of the most interesting features also in quantum and classical complementary contexts. All the results have been visualized along a series of representative graphics showing explicitly the main properties for both types of waveguides.","sentences":["Electric and magnetic waveguides are considered in planar Dirac materials like graphene as well as their classical version for relativistic particles of zero mass and electric charge.","In order to solve the Dirac-Weyl equation analytically, we have assumed the displacement symmetry of the system along a direction.","In these conditions we have examined the rest of symmetries relevant each type, magnetic or electric system, which will determine their similarities and differences.","We have worked out waveguides with square profile in detail to show up some of the most interesting features also in quantum and classical complementary contexts.","All the results have been visualized along a series of representative graphics showing explicitly the main properties for both types of waveguides."],"url":"http://arxiv.org/abs/2404.16114v1","category":"math-ph"}
{"created":"2024-04-24 18:00:01","title":"Resonant Reheating","abstract":"We investigate a novel reheating scenario proceeding through $s$-channel inflaton annihilation, mediated by a massive scalar. If the inflaton $\\phi$ oscillates around the minimum of a monomial potential $\\propto \\phi^{n}$, we reveal the emergence of resonance phenomena originating from the dynamic evolution of the inflaton mass for $n>2$. Consequently, a resonance appears in both the radiation and the temperature evolution during the reheating process. By solving the coupled Boltzmann equations, we present solutions for radiation and temperature. We find non-trivial temperature characteristics during reheating, depending on the value of $n$ and the masses of the inflaton and mediator. Some phenomenological aspects of the model are explored. As a concrete example, we show that the same mediator participates in the genesis of dark matter, modifying the standard freeze-in dynamics. In addition, we demonstrate that the resonant reheating scenario could be tested by next-generation low- and high-frequency gravitational wave detectors.","sentences":["We investigate a novel reheating scenario proceeding through $s$-channel inflaton annihilation, mediated by a massive scalar.","If the inflaton $\\phi$ oscillates around the minimum of a monomial potential $\\propto \\phi^{n}$, we reveal the emergence of resonance phenomena originating from the dynamic evolution of the inflaton mass for $n>2$. Consequently, a resonance appears in both the radiation and the temperature evolution during the reheating process.","By solving the coupled Boltzmann equations, we present solutions for radiation and temperature.","We find non-trivial temperature characteristics during reheating, depending on the value of $n$ and the masses of the inflaton and mediator.","Some phenomenological aspects of the model are explored.","As a concrete example, we show that the same mediator participates in the genesis of dark matter, modifying the standard freeze-in dynamics.","In addition, we demonstrate that the resonant reheating scenario could be tested by next-generation low- and high-frequency gravitational wave detectors."],"url":"http://arxiv.org/abs/2404.16090v1","category":"hep-ph"}
{"created":"2024-04-24 18:00:01","title":"OmniLearn: A Method to Simultaneously Facilitate All Jet Physics Tasks","abstract":"Machine learning has become an essential tool in jet physics. Due to their complex, high-dimensional nature, jets can be explored holistically by neural networks in ways that are not possible manually. However, innovations in all areas of jet physics are proceeding in parallel. We show that specially constructed machine learning models trained for a specific jet classification task can improve the accuracy, precision, or speed of all other jet physics tasks. This is demonstrated by training on a particular multiclass classification task and then using the learned representation for different classification tasks, for datasets with a different (full) detector simulation, for jets from a different collision system ($pp$ versus $ep$), for generative models, for likelihood ratio estimation, and for anomaly detection. Our OmniLearn approach is thus a foundation model and is made publicly available for use in any area where state-of-the-art precision is required for analyses involving jets and their substructure.","sentences":["Machine learning has become an essential tool in jet physics.","Due to their complex, high-dimensional nature, jets can be explored holistically by neural networks in ways that are not possible manually.","However, innovations in all areas of jet physics are proceeding in parallel.","We show that specially constructed machine learning models trained for a specific jet classification task can improve the accuracy, precision, or speed of all other jet physics tasks.","This is demonstrated by training on a particular multiclass classification task and then using the learned representation for different classification tasks, for datasets with a different (full) detector simulation, for jets from a different collision system ($pp$ versus $ep$), for generative models, for likelihood ratio estimation, and for anomaly detection.","Our OmniLearn approach is thus a foundation model and is made publicly available for use in any area where state-of-the-art precision is required for analyses involving jets and their substructure."],"url":"http://arxiv.org/abs/2404.16091v1","category":"hep-ph"}
{"created":"2024-04-24 18:00:01","title":"Self-Gravitating Matter in Stationary and Axisymmetric Black Hole Spacetimes","abstract":"All black holes (BHs) in nature are expected to be described by the Kerr vacuum solution of general relativity. However, the Kerr solution comes with several difficulties such as the existence of Cauchy horizons, curvature singularities, and causality-violating regions. Attempts to resolve some of these issues include phenomenological BH models, which typically contain nontrivial matter content. We introduce a simple framework here to examine the properties of matter in such phenomenological models for a broad class of stationary and axisymmetric spinning BH spacetimes, generated from nonspinning seed solutions via a metric ansatz. We apply this framework to a representative set of spinning BH spacetimes and the non-spinning seeds from which they are derived. The models span different types of matter - fluids, scalar fields, electromagnetic fields. For each model, we calculate the timelike four-velocity of the matter and thereby identify the rest frame of the matter, both outside and inside the horizon. We then examine the spatial distribution of the matter rest-frame energy density $\\epsilon$ and the principal pressures. This provides a complete picture of how the matter moves, what its material properties are, and whether it obeys the classical energy conditions. Notably, at a horizon, the normal component of the pressure always satisfies $p_n = -\\epsilon$. We also investigate the expansions of the principal null congruences and explore the Hawking mass profiles of these spacetimes. These provide glimpses into the geometry of the stationary BH exterior as well as the nonstationary interior cosmology. The axisymmetric metric ansatz we work with can be used to generate new spinning solutions from a variety of nonspinning seeds. The matter in these models often satisfies the weak energy condition, at least in the BH exterior, and some models exhibit non-rigid, differential rotation.","sentences":["All black holes (BHs) in nature are expected to be described by the Kerr vacuum solution of general relativity.","However, the Kerr solution comes with several difficulties such as the existence of Cauchy horizons, curvature singularities, and causality-violating regions.","Attempts to resolve some of these issues include phenomenological BH models, which typically contain nontrivial matter content.","We introduce a simple framework here to examine the properties of matter in such phenomenological models for a broad class of stationary and axisymmetric spinning BH spacetimes, generated from nonspinning seed solutions via a metric ansatz.","We apply this framework to a representative set of spinning BH spacetimes and the non-spinning seeds from which they are derived.","The models span different types of matter - fluids, scalar fields, electromagnetic fields.","For each model, we calculate the timelike four-velocity of the matter and thereby identify the rest frame of the matter, both outside and inside the horizon.","We then examine the spatial distribution of the matter rest-frame energy density $\\epsilon$ and the principal pressures.","This provides a complete picture of how the matter moves, what its material properties are, and whether it obeys the classical energy conditions.","Notably, at a horizon, the normal component of the pressure always satisfies $p_n = -\\epsilon$. We also investigate the expansions of the principal null congruences and explore the Hawking mass profiles of these spacetimes.","These provide glimpses into the geometry of the stationary BH exterior as well as the nonstationary interior cosmology.","The axisymmetric metric ansatz we work with can be used to generate new spinning solutions from a variety of nonspinning seeds.","The matter in these models often satisfies the weak energy condition, at least in the BH exterior, and some models exhibit non-rigid, differential rotation."],"url":"http://arxiv.org/abs/2404.16093v1","category":"gr-qc"}
{"created":"2024-04-24 18:00:01","title":"Long-range multipartite entanglement near measurement-induced transitions","abstract":"Measurements profoundly impact quantum systems, and can be used to create new states of matter out of equilibrium. Here, we investigate the multipartite entanglement structure that emerges in quantum circuits involving unitaries and measurements. We describe how a balance between measurements and unitary evolution can lead to multipartite entanglement spreading to distances far greater than what is found in non-monitored systems, thus evading the usual fate of entanglement. We introduce a graphical representation based on spanning graphs that allows to infer the evolution of genuine multipartite entanglement for general subregions. We exemplify our findings on circuits that realize a 1d measurement-induced dynamical phase transition, where we find genuine 3-party entanglement at all separations. The 2- and 4-party cases are also covered with examples. Finally, we discuss how our approach can provide fundamental insights regarding entanglement dynamics for a wide class of quantum circuits and architectures.","sentences":["Measurements profoundly impact quantum systems, and can be used to create new states of matter out of equilibrium.","Here, we investigate the multipartite entanglement structure that emerges in quantum circuits involving unitaries and measurements.","We describe how a balance between measurements and unitary evolution can lead to multipartite entanglement spreading to distances far greater than what is found in non-monitored systems, thus evading the usual fate of entanglement.","We introduce a graphical representation based on spanning graphs that allows to infer the evolution of genuine multipartite entanglement for general subregions.","We exemplify our findings on circuits that realize a 1d measurement-induced dynamical phase transition, where we find genuine 3-party entanglement at all separations.","The 2- and 4-party cases are also covered with examples.","Finally, we discuss how our approach can provide fundamental insights regarding entanglement dynamics for a wide class of quantum circuits and architectures."],"url":"http://arxiv.org/abs/2404.16095v1","category":"quant-ph"}
{"created":"2024-04-24 18:00:00","title":"Cosmological constraints from weak lensing scattering transform using HSC Y1 data","abstract":"As weak lensing surveys go deeper, there is an increasing need for reliable characterization of non-Gaussian structures at small angular scales. Here we present the first cosmological constraints with weak lensing scattering transform, a statistical estimator that combines efficiency, robustness, and interpretability. With the Hyper Suprime-Cam survey (HSC) year 1 data, we obtain $\\Omega_\\text{m}=0.29_{-0.03}^{+0.04}$, $S_8\\equiv \\sigma_8(\\Omega_\\text{m}/0.3)^{0.5}=0.83\\pm0.02$, and intrinsic alignment strength $A_\\text{IA}=1.0\\pm0.4$ through simulation-based forward modeling. Our constraints are consistent with those derived from Planck. The error bar of $\\Omega_\\text{m}$ is 2 times tighter than that obtained from the power spectrum when the same scale range is used. This constraining power is on par with that of convolutional neural networks, suggesting that further investment in spatial information extraction may not yield substantial benefits.   We also point out an internal tension of $S_8$ estimates linked to a redshift bin around z ~ 1 in the HSC data. We found that discarding that bin leads to a consistent decrease of $S_8$ from 0.83 to 0.79, for all statistical estimators. We argue that photometric redshift estimation is now the main limitation in the estimation of $S_8$ using HSC. This limitation is likely to affect other ground-based weak lensing surveys reaching redshifts greater than one. Alternative redshift estimation techniques, like clustering redshifts, may help alleviate this limitation.","sentences":["As weak lensing surveys go deeper, there is an increasing need for reliable characterization of non-Gaussian structures at small angular scales.","Here we present the first cosmological constraints with weak lensing scattering transform, a statistical estimator that combines efficiency, robustness, and interpretability.","With the Hyper Suprime-Cam survey (HSC) year 1 data, we obtain $\\Omega_\\text{m}=0.29_{-0.03}^{+0.04}$, $S_8\\equiv \\sigma_8(\\Omega_\\text{m}/0.3)^{0.5}=0.83\\pm0.02$, and intrinsic alignment strength $A_\\text{IA}=1.0\\pm0.4$ through simulation-based forward modeling.","Our constraints are consistent with those derived from Planck.","The error bar of $\\Omega_\\text{m}$ is 2 times tighter than that obtained from the power spectrum when the same scale range is used.","This constraining power is on par with that of convolutional neural networks, suggesting that further investment in spatial information extraction may not yield substantial benefits.   ","We also point out an internal tension of $S_8$ estimates linked to a redshift bin around z ~ 1 in the HSC data.","We found that discarding that bin leads to a consistent decrease of $S_8$ from 0.83 to 0.79, for all statistical estimators.","We argue that photometric redshift estimation is now the main limitation in the estimation of $S_8$ using HSC.","This limitation is likely to affect other ground-based weak lensing surveys reaching redshifts greater than one.","Alternative redshift estimation techniques, like clustering redshifts, may help alleviate this limitation."],"url":"http://arxiv.org/abs/2404.16085v1","category":"astro-ph.CO"}
{"created":"2024-04-24 18:00:00","title":"Statistical Mechanics of Stochastic Quantum Control: $d$-adic R\u00e9nyi Circuits","abstract":"The dynamics of quantum information in many-body systems with large onsite Hilbert space dimension admits an enlightening description in terms of effective statistical mechanics models. Motivated by this fact, we reveal a connection between three separate models: the classically chaotic $d$-adic R\\'{e}nyi map with stochastic control, a quantum analog of this map for qudits, and a Potts model on a random graph. The classical model and its quantum analog share a transition between chaotic and controlled phases, driven by a randomly applied control map that attempts to order the system. In the quantum model, the control map necessitates measurements that concurrently drive a phase transition in the entanglement content of the late-time steady state. To explore the interplay of the control and entanglement transitions, we derive an effective Potts model from the quantum model and use it to probe information-theoretic quantities that witness both transitions. The entanglement transition is found to be in the bond-percolation universality class, consistent with other measurement-induced phase transitions, while the control transition is governed by a classical random walk. These two phase transitions merge as a function of model parameters, consistent with behavior observed in previous small-size numerical studies of the quantum model.","sentences":["The dynamics of quantum information in many-body systems with large onsite Hilbert space dimension admits an enlightening description in terms of effective statistical mechanics models.","Motivated by this fact, we reveal a connection between three separate models: the classically chaotic $d$-adic R\\'{e}nyi map with stochastic control, a quantum analog of this map for qudits, and a Potts model on a random graph.","The classical model and its quantum analog share a transition between chaotic and controlled phases, driven by a randomly applied control map that attempts to order the system.","In the quantum model, the control map necessitates measurements that concurrently drive a phase transition in the entanglement content of the late-time steady state.","To explore the interplay of the control and entanglement transitions, we derive an effective Potts model from the quantum model and use it to probe information-theoretic quantities that witness both transitions.","The entanglement transition is found to be in the bond-percolation universality class, consistent with other measurement-induced phase transitions, while the control transition is governed by a classical random walk.","These two phase transitions merge as a function of model parameters, consistent with behavior observed in previous small-size numerical studies of the quantum model."],"url":"http://arxiv.org/abs/2404.16087v1","category":"quant-ph"}
{"created":"2024-04-24 17:15:19","title":"Asymptotically Fair and Truthful Allocation of Public Goods","abstract":"We study the fair and truthful allocation of m divisible public items among n agents, each with distinct preferences for the items. To aggregate agents' preferences fairly, we follow the literature on the fair allocation of public goods and aim to find a core solution. For divisible items, a core solution always exists and can be calculated efficiently by maximizing the Nash welfare objective. However, such a solution is easily manipulated; agents might have incentives to misreport their preferences. To mitigate this, the current state-of-the-art finds an approximate core solution with high probability while ensuring approximate truthfulness. However, this approach has two main limitations. First, due to several approximations, the approximation error in the core could grow with n, resulting in a non-asymptotic core solution. This limitation is particularly significant as public-good allocation mechanisms are frequently applied in scenarios involving a large number of agents, such as the allocation of public tax funds for municipal projects. Second, implementing the current approach for practical applications proves to be a highly nontrivial task. To address these limitations, we introduce PPGA, a (differentially) Private Public-Good Allocation algorithm, and show that it attains asymptotic truthfulness and finds an asymptotic core solution with high probability. Additionally, to demonstrate the practical applicability of our algorithm, we implement PPGA and empirically study its properties using municipal participatory budgeting data.","sentences":["We study the fair and truthful allocation of m divisible public items among n agents, each with distinct preferences for the items.","To aggregate agents' preferences fairly, we follow the literature on the fair allocation of public goods and aim to find a core solution.","For divisible items, a core solution always exists and can be calculated efficiently by maximizing the Nash welfare objective.","However, such a solution is easily manipulated; agents might have incentives to misreport their preferences.","To mitigate this, the current state-of-the-art finds an approximate core solution with high probability while ensuring approximate truthfulness.","However, this approach has two main limitations.","First, due to several approximations, the approximation error in the core could grow with n, resulting in a non-asymptotic core solution.","This limitation is particularly significant as public-good allocation mechanisms are frequently applied in scenarios involving a large number of agents, such as the allocation of public tax funds for municipal projects.","Second, implementing the current approach for practical applications proves to be a highly nontrivial task.","To address these limitations, we introduce PPGA, a (differentially) Private Public-Good Allocation algorithm, and show that it attains asymptotic truthfulness and finds an asymptotic core solution with high probability.","Additionally, to demonstrate the practical applicability of our algorithm, we implement PPGA and empirically study its properties using municipal participatory budgeting data."],"url":"http://arxiv.org/abs/2404.15996v1","category":"cs.GT"}
{"created":"2024-04-24 17:12:20","title":"A proof of Vishik's nonuniqueness Theorem for the forced 2D Euler equation","abstract":"We give a simpler proof of Vishik's nonuniqueness Theorem for the forced 2D Euler equation in the vorticity class $L^1\\cap L^p$ with $2<p<\\infty$. The main simplification is an alternative construction of a smooth and compactly supported unstable vortex, which is split into two steps: Firstly, we construct a piecewise constant unstable vortex, and secondly, we find a regularization through a fixed point argument. This simpler structure of the unstable vortex yields a simplification of the other parts of Vishik's proof.","sentences":["We give a simpler proof of Vishik's nonuniqueness Theorem for the forced 2D Euler equation in the vorticity class $L^1\\cap L^p$ with $2<p<\\infty$. The main simplification is an alternative construction of a smooth and compactly supported unstable vortex, which is split into two steps: Firstly, we construct a piecewise constant unstable vortex, and secondly, we find a regularization through a fixed point argument.","This simpler structure of the unstable vortex yields a simplification of the other parts of Vishik's proof."],"url":"http://arxiv.org/abs/2404.15995v1","category":"math.AP"}
{"created":"2024-04-24 17:01:46","title":"NLO thermal corrections to dark matter annihilation cross sections: a novel approach","abstract":"The dark matter relic density has been increasingly accurately measured by successive generations of experiments. The Boltzmann equation determines the yields using the dark matter annihilation cross section as one of the inputs; the accurate computation of the latter including thermal contributions thus assumes importance. We report here the next-to-leading order (NLO) thermal corrections to the cross sections for (Majorana) dark matter annihilation to standard model fermions: $\\chi \\chi \\to f \\overline{f}$, via charged scalars. We use a novel approach, utilising the technique of Grammer and Yennie, extended to thermal field theories, where the cancellation of soft infra-red divergences occurs naturally. We present the NLO thermal cross sections in full detail for both the relativistic case as well as in the non-relativistic limit. Our independent calculation verifies earlier results where the leading contribution at order ${\\cal{O}}(T^2)$ was shown to be proportional to the square of the fermion mass in the non-relativistic limit, just as at leading order. We find that the ${\\cal{O}}(T^4)$ contributions have the same dependence on the fermion mass as well.","sentences":["The dark matter relic density has been increasingly accurately measured by successive generations of experiments.","The Boltzmann equation determines the yields using the dark matter annihilation cross section as one of the inputs; the accurate computation of the latter including thermal contributions thus assumes importance.","We report here the next-to-leading order (NLO) thermal corrections to the cross sections for (Majorana) dark matter annihilation to standard model fermions: $\\chi \\chi \\to f \\overline{f}$, via charged scalars.","We use a novel approach, utilising the technique of Grammer and Yennie, extended to thermal field theories, where the cancellation of soft infra-red divergences occurs naturally.","We present the NLO thermal cross sections in full detail for both the relativistic case as well as in the non-relativistic limit.","Our independent calculation verifies earlier results where the leading contribution at order ${\\cal{O}}(T^2)$ was shown to be proportional to the square of the fermion mass in the non-relativistic limit, just as at leading order.","We find that the ${\\cal{O}}(T^4)$ contributions have the same dependence on the fermion mass as well."],"url":"http://arxiv.org/abs/2404.15987v1","category":"hep-ph"}
{"created":"2024-04-24 17:00:32","title":"Frozen stars: Black hole mimickers sourced by a string fluid","abstract":"The frozen star is a non-singular, ultracompact object that, to an external observer, looks exactly like a Schwarzschild black hole, but with a different interior geometry and matter composition. The frozen star needs to be sourced by an extremely anisotropic fluid, for which the sum of the radial pressure and energy density is either vanishing or perturbatively small. Here, we show that this matter can be identified with the string fluid resulting from the decay of an unstable $D$-brane or a brane-antibrane system at the end of open-string tachyon condensation. The string fluid corresponds to flux tubes emanating from the center and ending at the Schwarzschild radius of the star. The effective Lagrangian for this fluid can be recast into a Born-Infeld form. When the fluid Lagrangian is coupled to that of Einstein's Gravity, the static, spherically symmetric solutions of the equations of motion are shown to be the same as those describing the frozen star model. Frozen stars can therefore be viewed as gravitationally back-reacted BIons. The Born-Infeld Lagrangian provides a complete set of equations that describe the dynamics of the frozen star in a generic state, which is not necessarily static nor spherically symmetric. Additionally, this description provides a new physical perspective on the structure of the frozen star in terms of the corresponding electric fields and charges. The electric field is sourced by a point-like charge at the center of the star, while its outer layer is equal and oppositely charged. The electric force between the charges is offset because the mass of the star is fixed.","sentences":["The frozen star is a non-singular, ultracompact object that, to an external observer, looks exactly like a Schwarzschild black hole, but with a different interior geometry and matter composition.","The frozen star needs to be sourced by an extremely anisotropic fluid, for which the sum of the radial pressure and energy density is either vanishing or perturbatively small.","Here, we show that this matter can be identified with the string fluid resulting from the decay of an unstable $D$-brane or a brane-antibrane system at the end of open-string tachyon condensation.","The string fluid corresponds to flux tubes emanating from the center and ending at the Schwarzschild radius of the star.","The effective Lagrangian for this fluid can be recast into a Born-Infeld form.","When the fluid Lagrangian is coupled to that of Einstein's Gravity, the static, spherically symmetric solutions of the equations of motion are shown to be the same as those describing the frozen star model.","Frozen stars can therefore be viewed as gravitationally back-reacted BIons.","The Born-Infeld Lagrangian provides a complete set of equations that describe the dynamics of the frozen star in a generic state, which is not necessarily static nor spherically symmetric.","Additionally, this description provides a new physical perspective on the structure of the frozen star in terms of the corresponding electric fields and charges.","The electric field is sourced by a point-like charge at the center of the star, while its outer layer is equal and oppositely charged.","The electric force between the charges is offset because the mass of the star is fixed."],"url":"http://arxiv.org/abs/2404.15985v1","category":"hep-th"}
{"created":"2024-04-24 16:58:38","title":"Cosmological constraints on time-varying cosmological terms: A study of FLRW universe models with $\u039b(t)$CDM cosmology","abstract":"This paper explores models of the FLRW universe that incorporate a time-varying cosmological term $\\Lambda(t)$. Specifically, we assume a power-law form for the cosmological term as a function of the scale factor: $\\Lambda(t)=\\Lambda_{0} a(t)^{-\\alpha}$, where $\\Lambda_{0}$ represents the present value of the cosmological term. Then, we derive an exact solution to Einstein's field equations within the framework of $\\Lambda(t)$CDM cosmology and determine the best-fit values of the model parameters using the combined $H(z)$ + SNe Ia dataset and MCMC analysis. Moreover, the deceleration parameter demonstrates the accelerating behavior of the universe, highlighting the transition redshift $z_{tr}$, at which the expansion shifts from deceleration to acceleration, with confidence levels of $1-\\sigma$ and $2-\\sigma$. In addition, we analyze the behavior of the Hubble parameter, jerk parameter, and $Om(z)$ diagnostic. Our analysis leads us to the conclusion that the $\\Lambda(t)$CDM model is consistent with present-day observations.","sentences":["This paper explores models of the FLRW universe that incorporate a time-varying cosmological term $\\Lambda(t)$. Specifically, we assume a power-law form for the cosmological term as a function of the scale factor: $\\Lambda(t)=\\Lambda_{0} a(t)^{-\\alpha}$, where $\\Lambda_{0}$ represents the present value of the cosmological term.","Then, we derive an exact solution to Einstein's field equations within the framework of $\\Lambda(t)$CDM cosmology and determine the best-fit values of the model parameters using the combined $H(z)$ + SNe Ia dataset and MCMC analysis.","Moreover, the deceleration parameter demonstrates the accelerating behavior of the universe, highlighting the transition redshift $z_{tr}$, at which the expansion shifts from deceleration to acceleration, with confidence levels of $1-\\sigma$ and $2-\\sigma$.","In addition, we analyze the behavior of the Hubble parameter, jerk parameter, and $Om(z)$ diagnostic.","Our analysis leads us to the conclusion that the $\\Lambda(t)$CDM model is consistent with present-day observations."],"url":"http://arxiv.org/abs/2404.15982v1","category":"astro-ph.CO"}
{"created":"2024-04-24 16:54:39","title":"On the Fourier analysis in the SO(3) space : EquiLoPO Network","abstract":"Analyzing volumetric data with rotational invariance or equivariance is an active topic in current research. Existing deep-learning approaches utilize either group convolutional networks limited to discrete rotations or steerable convolutional networks with constrained filter structures. This work proposes a novel equivariant neural network architecture that achieves analytical Equivariance to Local Pattern Orientation on the continuous SO(3) group while allowing unconstrained trainable filters - EquiLoPO Network. Our key innovations are a group convolutional operation leveraging irreducible representations as the Fourier basis and a local activation function in the SO(3) space that provides a well-defined mapping from input to output functions, preserving equivariance. By integrating these operations into a ResNet-style architecture, we propose a model that overcomes the limitations of prior methods. A comprehensive evaluation on diverse 3D medical imaging datasets from MedMNIST3D demonstrates the effectiveness of our approach, which consistently outperforms state of the art. This work suggests the benefits of true rotational equivariance on SO(3) and flexible unconstrained filters enabled by the local activation function, providing a flexible framework for equivariant deep learning on volumetric data with potential applications across domains. Our code is publicly available at \\url{https://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPO/-/tree/main/EquiLoPO}.","sentences":["Analyzing volumetric data with rotational invariance or equivariance is an active topic in current research.","Existing deep-learning approaches utilize either group convolutional networks limited to discrete rotations or steerable convolutional networks with constrained filter structures.","This work proposes a novel equivariant neural network architecture that achieves analytical Equivariance to Local Pattern Orientation on the continuous SO(3) group while allowing unconstrained trainable filters - EquiLoPO Network.","Our key innovations are a group convolutional operation leveraging irreducible representations as the Fourier basis and a local activation function in the SO(3) space that provides a well-defined mapping from input to output functions, preserving equivariance.","By integrating these operations into a ResNet-style architecture, we propose a model that overcomes the limitations of prior methods.","A comprehensive evaluation on diverse 3D medical imaging datasets from MedMNIST3D demonstrates the effectiveness of our approach, which consistently outperforms state of the art.","This work suggests the benefits of true rotational equivariance on SO(3) and flexible unconstrained filters enabled by the local activation function, providing a flexible framework for equivariant deep learning on volumetric data with potential applications across domains.","Our code is publicly available at \\url{https://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPO/-/tree/main/EquiLoPO}."],"url":"http://arxiv.org/abs/2404.15979v1","category":"cs.CV"}
{"created":"2024-04-24 16:52:15","title":"Kinetic Model for Dark Energy -- Dark Matter Interaction: Scenario for the Hubble Tension","abstract":"We analyze a model for Dark Energy - Dark Matter interaction, based on a decaying process of the former constituents into the latter ones. The dynamical equations are constructed following a kinetic formulation, which separates the interacting fluctuations from equilibrium distribution of the both the species. The emerging dynamical picture consists of coupled equations, which are specialized in the case of a Dark Energy equation of state parameter: we deal with a modified Lambda Cold Dark Matter model, which is investigated versus a possible interpretation of the Hubble tension. We compare our model with data corresponding to 6 points of the expansion rate from Type Ia Supernovae. We show that, the proposed model suitably fits data according to a value of the Hubble constant compatible with the SHOES Collaboration measurement. The tension is solved because, essentially for redshift greater than one, the correction to the Lambda Cold Dark Matter model vanishes and its presence does not affect the Planck measurements.","sentences":["We analyze a model for Dark Energy - Dark Matter interaction, based on a decaying process of the former constituents into the latter ones.","The dynamical equations are constructed following a kinetic formulation, which separates the interacting fluctuations from equilibrium distribution of the both the species.","The emerging dynamical picture consists of coupled equations, which are specialized in the case of a Dark Energy equation of state parameter: we deal with a modified Lambda Cold Dark Matter model, which is investigated versus a possible interpretation of the Hubble tension.","We compare our model with data corresponding to 6 points of the expansion rate from Type Ia Supernovae.","We show that, the proposed model suitably fits data according to a value of the Hubble constant compatible with the SHOES Collaboration measurement.","The tension is solved because, essentially for redshift greater than one, the correction to the Lambda Cold Dark Matter model vanishes and its presence does not affect the Planck measurements."],"url":"http://arxiv.org/abs/2404.15977v1","category":"gr-qc"}
{"created":"2024-04-24 16:35:49","title":"Choquard equations with critical exponential nonlinearities in the zero mass case","abstract":"We investigate Choquard equations in $\\mathbb R^N$ driven by a weighted $N$-Laplace operator and with polynomial kernel and zero mass. Since the setting is limiting for the Sobolev embedding, we work with nonlinearities which may grow up to the critical exponential. We establish existence of a positive solution by variational methods, completing the analysis in [Romani, ArXiv preprint 2023], where the case of a logarithmic kernel was considered.","sentences":["We investigate Choquard equations in $\\mathbb R^N$ driven by a weighted $N$-Laplace operator and with polynomial kernel and zero mass.","Since the setting is limiting for the Sobolev embedding, we work with nonlinearities which may grow up to the critical exponential.","We establish existence of a positive solution by variational methods, completing the analysis in [Romani, ArXiv preprint 2023], where the case of a logarithmic kernel was considered."],"url":"http://arxiv.org/abs/2404.15965v1","category":"math.AP"}
{"created":"2024-04-24 16:33:54","title":"Complex Stochastic Optimal Control Foundation of Quantum Mechanics","abstract":"Recent studies have expanded the use of the stochastic Hamilton Jacobi Bellman (HJB) equation to include complex variables for deriving quantum mechanical equations. However, these studies typically assume that it is valid to apply the HJB equation directly to complex numbers, an approach that overlooks the fundamental problem of comparing complex numbers to find optimal controls. This paper addresses how to properly apply the HJB equation in the context of complex variables. Our findings significantly reevaluate the stochastic movement of quantum particles, directly influenced by the Cauchy Riemann theorem. These insights not only deepen our understanding of quantum dynamics but also enhance the mathematical rigor of the framework for applying stochastic optimal control in quantum mechanics.","sentences":["Recent studies have expanded the use of the stochastic Hamilton Jacobi Bellman (HJB) equation to include complex variables for deriving quantum mechanical equations.","However, these studies typically assume that it is valid to apply the HJB equation directly to complex numbers, an approach that overlooks the fundamental problem of comparing complex numbers to find optimal controls.","This paper addresses how to properly apply the HJB equation in the context of complex variables.","Our findings significantly reevaluate the stochastic movement of quantum particles, directly influenced by the Cauchy Riemann theorem.","These insights not only deepen our understanding of quantum dynamics but also enhance the mathematical rigor of the framework for applying stochastic optimal control in quantum mechanics."],"url":"http://arxiv.org/abs/2404.15964v1","category":"quant-ph"}
{"created":"2024-04-24 16:19:11","title":"Mixed Supervised Graph Contrastive Learning for Recommendation","abstract":"Recommender systems (RecSys) play a vital role in online platforms, offering users personalized suggestions amidst vast information. Graph contrastive learning aims to learn from high-order collaborative filtering signals with unsupervised augmentation on the user-item bipartite graph, which predominantly relies on the multi-task learning framework involving both the pair-wise recommendation loss and the contrastive loss. This decoupled design can cause inconsistent optimization direction from different losses, which leads to longer convergence time and even sub-optimal performance. Besides, the self-supervised contrastive loss falls short in alleviating the data sparsity issue in RecSys as it learns to differentiate users/items from different views without providing extra supervised collaborative filtering signals during augmentations. In this paper, we propose Mixed Supervised Graph Contrastive Learning for Recommendation (MixSGCL) to address these concerns. MixSGCL originally integrates the training of recommendation and unsupervised contrastive losses into a supervised contrastive learning loss to align the two tasks within one optimization direction. To cope with the data sparsity issue, instead unsupervised augmentation, we further propose node-wise and edge-wise mixup to mine more direct supervised collaborative filtering signals based on existing user-item interactions. Extensive experiments on three real-world datasets demonstrate that MixSGCL surpasses state-of-the-art methods, achieving top performance on both accuracy and efficiency. It validates the effectiveness of MixSGCL with our coupled design on supervised graph contrastive learning.","sentences":["Recommender systems (RecSys) play a vital role in online platforms, offering users personalized suggestions amidst vast information.","Graph contrastive learning aims to learn from high-order collaborative filtering signals with unsupervised augmentation on the user-item bipartite graph, which predominantly relies on the multi-task learning framework involving both the pair-wise recommendation loss and the contrastive loss.","This decoupled design can cause inconsistent optimization direction from different losses, which leads to longer convergence time and even sub-optimal performance.","Besides, the self-supervised contrastive loss falls short in alleviating the data sparsity issue in RecSys as it learns to differentiate users/items from different views without providing extra supervised collaborative filtering signals during augmentations.","In this paper, we propose Mixed Supervised Graph Contrastive Learning for Recommendation (MixSGCL) to address these concerns.","MixSGCL originally integrates the training of recommendation and unsupervised contrastive losses into a supervised contrastive learning loss to align the two tasks within one optimization direction.","To cope with the data sparsity issue, instead unsupervised augmentation, we further propose node-wise and edge-wise mixup to mine more direct supervised collaborative filtering signals based on existing user-item interactions.","Extensive experiments on three real-world datasets demonstrate that MixSGCL surpasses state-of-the-art methods, achieving top performance on both accuracy and efficiency.","It validates the effectiveness of MixSGCL with our coupled design on supervised graph contrastive learning."],"url":"http://arxiv.org/abs/2404.15954v1","category":"cs.IR"}
{"created":"2024-04-24 16:17:15","title":"Population synthesis of Galactic pulsars with machine learning","abstract":"This thesis work represents the first efforts to combine population synthesis studies of the Galactic isolated neutron stars with deep-learning techniques with the aim of better understanding neutron-star birth properties and evolution. In particular, we develop a flexible population-synthesis framework to model the dynamical and magneto-rotational evolution of neutron stars, their emission in radio and their detection with radio telescopes. We first study the feasibility of using deep neural networks to infer the dynamical properties at birth and then explore a simulation-based inference approach to predict the birth magnetic-field and spin-period distributions and the late-time magnetic-field decay for the observed radio pulsar population. Our results for the birth magneto-rotational properties agree with the findings of previous works while we constrain the late-time evolution of the magnetic field in neutron stars for the first time. Moreover, this thesis also studies possible scenarios to explain the puzzling nature of recently discovered periodic radio sources with very long periods of the order of thousands of seconds. In particular, by assuming a neutron-star origin, we study the spin-period evolution of a newborn neutron star interacting with a supernova fallback disk and find that the combination of strong, magnetar-like magnetic fields and moderate accretion rates can lead to very large spin periods on timescales of ten thousands of years. Moreover, we perform population synthesis studies to assess the possibility for these sources to be either neutron stars or magnetic white dwarfs emitting coherently through magnetic dipolar losses. These discoveries have opened up a new perspective on the neutron-star population and have started to question our current understanding of how coherent radio emission is produced in pulsar magnetospheres.","sentences":["This thesis work represents the first efforts to combine population synthesis studies of the Galactic isolated neutron stars with deep-learning techniques with the aim of better understanding neutron-star birth properties and evolution.","In particular, we develop a flexible population-synthesis framework to model the dynamical and magneto-rotational evolution of neutron stars, their emission in radio and their detection with radio telescopes.","We first study the feasibility of using deep neural networks to infer the dynamical properties at birth and then explore a simulation-based inference approach to predict the birth magnetic-field and spin-period distributions and the late-time magnetic-field decay for the observed radio pulsar population.","Our results for the birth magneto-rotational properties agree with the findings of previous works while we constrain the late-time evolution of the magnetic field in neutron stars for the first time.","Moreover, this thesis also studies possible scenarios to explain the puzzling nature of recently discovered periodic radio sources with very long periods of the order of thousands of seconds.","In particular, by assuming a neutron-star origin, we study the spin-period evolution of a newborn neutron star interacting with a supernova fallback disk and find that the combination of strong, magnetar-like magnetic fields and moderate accretion rates can lead to very large spin periods on timescales of ten thousands of years.","Moreover, we perform population synthesis studies to assess the possibility for these sources to be either neutron stars or magnetic white dwarfs emitting coherently through magnetic dipolar losses.","These discoveries have opened up a new perspective on the neutron-star population and have started to question our current understanding of how coherent radio emission is produced in pulsar magnetospheres."],"url":"http://arxiv.org/abs/2404.15953v1","category":"astro-ph.HE"}
{"created":"2024-04-24 16:02:00","title":"Anomalous random flights and time-fractional run-and-tumble equations","abstract":"Random flights (also called run-and-tumble walks or transport processes) represent finite velocity random motions changing direction at any Poissonian time. These models in d-dimension, can be studied giving a general formulation of the problem valid at any spatial dimension. The aim of this paper is to extend this general analysis to time-fractional processes arising from a non-local generalization of the kinetic equations. The probabilistic interpretation of the solution of the time-fractional equations leads to a time-changed version of the original transport processes. The obtained results provides a clear picture of the role played by the time-fractional derivatives in this kind of random motions. They displayed an anomalous behavior and are useful to describe several complex systems arising in statistical physics and biology. In particular, we focus on the one-dimensional random flight, called telegraph process, studying the time-fractional version of the classical telegraph equation and providing a suitable interpretation of its stochastic solutions.","sentences":["Random flights (also called run-and-tumble walks or transport processes) represent finite velocity random motions changing direction at any Poissonian time.","These models in d-dimension, can be studied giving a general formulation of the problem valid at any spatial dimension.","The aim of this paper is to extend this general analysis to time-fractional processes arising from a non-local generalization of the kinetic equations.","The probabilistic interpretation of the solution of the time-fractional equations leads to a time-changed version of the original transport processes.","The obtained results provides a clear picture of the role played by the time-fractional derivatives in this kind of random motions.","They displayed an anomalous behavior and are useful to describe several complex systems arising in statistical physics and biology.","In particular, we focus on the one-dimensional random flight, called telegraph process, studying the time-fractional version of the classical telegraph equation and providing a suitable interpretation of its stochastic solutions."],"url":"http://arxiv.org/abs/2404.15941v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-24 15:41:14","title":"Probing Hidden Leptonic Scalar Portals using the NA64 Experiment at CERN","abstract":"In this study, we demonstrate the potential of the NA64 experiment at CERN SPS to search for New Physics processes involving $e\\rightarrow\\mu$ transitions after the collision of 100 GeV electrons with target nuclei. A new Dark Sector leptonic portal in which a scalar boson $\\varphi$ could be produced in the lepton-flavor-changing bremsstrahlung-like reaction, $eN\\rightarrow \\mu N\\varphi$, is used as benchmark process. In this work, we develop a realistic Monte Carlo simulation of the NA64 experimental setup implementing the differential and total production cross-section computed at exact tree-level and applying the Weisz\\\"{a}cker-Williams phase space approximation. Using this framework, we investigate the main background sources and calculate the expected sensitivity of the experiment. The results indicate that with minor setup optimization, NA64 can probe a large fraction of the available parameter space compatible with the muon $g-2$ anomaly and the Dark Matter relic predictions in the context of a new Dark Sector leptonic portal with $10^{11}$ EOT. This result paves the way to the exploration of lepton-flavour-changing transitions in NA64.","sentences":["In this study, we demonstrate the potential of the NA64 experiment at CERN SPS to search for New Physics processes involving $e\\rightarrow\\mu$ transitions after the collision of 100 GeV electrons with target nuclei.","A new Dark Sector leptonic portal in which a scalar boson $\\varphi$ could be produced in the lepton-flavor-changing bremsstrahlung-like reaction, $eN\\rightarrow \\mu N\\varphi$, is used as benchmark process.","In this work, we develop a realistic Monte Carlo simulation of the NA64 experimental setup implementing the differential and total production cross-section computed at exact tree-level and applying the Weisz\\\"{a}cker-Williams phase space approximation.","Using this framework, we investigate the main background sources and calculate the expected sensitivity of the experiment.","The results indicate that with minor setup optimization, NA64 can probe a large fraction of the available parameter space compatible with the muon $g-2$ anomaly and the Dark Matter relic predictions in the context of a new Dark Sector leptonic portal with $10^{11}$ EOT.","This result paves the way to the exploration of lepton-flavour-changing transitions in NA64."],"url":"http://arxiv.org/abs/2404.15931v1","category":"hep-ph"}
{"created":"2024-04-24 15:39:53","title":"Skew bracoids and the Yang-Baxter equation","abstract":"Skew braces provide an algebraic framework for studying bijective nondegenerate solutions of the set-theoretic Yang-Baxter equation. We show that left skew bracoids, recently introduced by two of the authors, can be used to obtain right nondegenerate solutions, and give a variety of examples arising from various methods of constructing left skew bracoids. We compare the solutions we obtain with the left nondegenerate solutions obtained via (left cancellative) left semibraces, and establish a correspondence between left semibraces and a class of left skew bracoids.","sentences":["Skew braces provide an algebraic framework for studying bijective nondegenerate solutions of the set-theoretic Yang-Baxter equation.","We show that left skew bracoids, recently introduced by two of the authors, can be used to obtain right nondegenerate solutions, and give a variety of examples arising from various methods of constructing left skew bracoids.","We compare the solutions we obtain with the left nondegenerate solutions obtained via (left cancellative) left semibraces, and establish a correspondence between left semibraces and a class of left skew bracoids."],"url":"http://arxiv.org/abs/2404.15929v2","category":"math.RA"}
{"created":"2024-04-24 15:20:40","title":"Algebraic intersection for hyperbolic surfaces","abstract":"We show that the algebraic intersection form of hyperbolic surfaces of genus $g$ has a minimum in the moduli space and that the minimum grows in the order $(\\log g)^{-2}$ in terms of the genus. We also describe the asymptotic behavior of the algebraic intersection form in the moduli space as the homologically systolic length goes to zero.","sentences":["We show that the algebraic intersection form of hyperbolic surfaces of genus $g$ has a minimum in the moduli space and that the minimum grows in the order $(\\log g)^{-2}$ in terms of the genus.","We also describe the asymptotic behavior of the algebraic intersection form in the moduli space as the homologically systolic length goes to zero."],"url":"http://arxiv.org/abs/2404.15921v1","category":"math.GT"}
{"created":"2024-04-24 15:12:25","title":"Perception and Localization of Macular Degeneration Applying Convolutional Neural Network, ResNet and Grad-CAM","abstract":"A well-known retinal disease that feels blurry visions to the affected patients is Macular Degeneration. This research is based on classifying the healthy and macular degeneration fundus with localizing the affected region of the fundus. A CNN architecture and CNN with ResNet architecture (ResNet50, ResNet50v2, ResNet101, ResNet101v2, ResNet152, ResNet152v2) as the backbone are used to classify the two types of fundus. The data are split into three categories including (a) Training set is 90% and Testing set is 10% (b) Training set is 80% and Testing set is 20%, (c) Training set is 50% and Testing set is 50%. After the training, the best model has been selected from the evaluation metrics. Among the models, CNN with backbone of ResNet50 performs best which gives the training accuracy of 98.7\\% for 90\\% train and 10\\% test data split. With this model, we have performed the Grad-CAM visualization to get the region of affected area of fundus.","sentences":["A well-known retinal disease that feels blurry visions to the affected patients is Macular Degeneration.","This research is based on classifying the healthy and macular degeneration fundus with localizing the affected region of the fundus.","A CNN architecture and CNN with ResNet architecture (ResNet50, ResNet50v2, ResNet101, ResNet101v2, ResNet152, ResNet152v2) as the backbone are used to classify the two types of fundus.","The data are split into three categories including (a) Training set is 90% and Testing set is 10% (b) Training set is 80% and Testing set is 20%, (c) Training set is 50% and Testing set is 50%.","After the training, the best model has been selected from the evaluation metrics.","Among the models, CNN with backbone of ResNet50 performs best which gives the training accuracy of 98.7\\% for 90\\% train and 10\\% test data split.","With this model, we have performed the Grad-CAM visualization to get the region of affected area of fundus."],"url":"http://arxiv.org/abs/2404.15918v1","category":"eess.IV"}
{"created":"2024-04-24 14:48:16","title":"The classical and quantum particle on a flag manifold","abstract":"In the present paper we consider two related problems, i.e. the description of geodesics and the calculation of the spectrum of the Laplace-Beltrami operator on a flag manifold. We show that there exists a family of invariant metrics such that both problems can be solved simply and explicitly. In order to determine the spectrum of the Laplace-Beltrami operator, we construct natural, finite-dimensional approximations (of spin chain type) to the Hilbert space of functions on a flag manifold.","sentences":["In the present paper we consider two related problems, i.e. the description of geodesics and the calculation of the spectrum of the Laplace-Beltrami operator on a flag manifold.","We show that there exists a family of invariant metrics such that both problems can be solved simply and explicitly.","In order to determine the spectrum of the Laplace-Beltrami operator, we construct natural, finite-dimensional approximations (of spin chain type) to the Hilbert space of functions on a flag manifold."],"url":"http://arxiv.org/abs/2404.15900v1","category":"hep-th"}
{"created":"2024-04-24 14:21:18","title":"Differential equations on a $k$-dimensional torus: Poincar\u00e9 type results","abstract":"Ordinary differential equations of the first order on the torus have been investigated in detail by H. Poincar\\'e, P. Bohl and A. Denjoy. P. Bohl, back in 1916, emphasised the importance of the transfer of the results for the order $k=1$ to the case $k>1$, adding at the same time: \"However, any attempt to do so would be hopeless\". The following more than hundred years have only confirmed Bohl's forecast. It became clear that a new approach to this problem is needed.   In this paper, we propose a new (non-Hamiltonian) and promising approach. We use Hamiltonians, that is, ordinary differential systems of equations of the first order, only for heuristics. In the main scheme and corresponding proofs we do not use these systems. Instead of differential systems, we study sets of continuous vector functions $\\phi(t,\\eta)$ satisfying certain important conditions. Limit sets and left and right rotation vectors appear in the case $k>1$. Some of our results are new even in the case $k=1$.","sentences":["Ordinary differential equations of the first order on the torus have been investigated in detail by H. Poincar\\'e, P. Bohl and A. Denjoy.","P. Bohl, back in 1916, emphasised the importance of the transfer of the results for the order $k=1$ to the case $k>1$, adding at the same time: \"However, any attempt to do so would be hopeless\".","The following more than hundred years have only confirmed Bohl's forecast.","It became clear that a new approach to this problem is needed.   ","In this paper, we propose a new (non-Hamiltonian) and promising approach.","We use Hamiltonians, that is, ordinary differential systems of equations of the first order, only for heuristics.","In the main scheme and corresponding proofs we do not use these systems.","Instead of differential systems, we study sets of continuous vector functions $\\phi(t,\\eta)$ satisfying certain important conditions.","Limit sets and left and right rotation vectors appear in the case $k>1$. Some of our results are new even in the case $k=1$."],"url":"http://arxiv.org/abs/2404.15887v1","category":"math.CA"}
{"created":"2024-04-24 14:12:37","title":"Inverse modified scattering and polyhomogeneous expansions for the Vlasov--Poisson system","abstract":"We give a new proof of well posedness of the inverse modified scattering problem for the Vlasov--Poisson system: for every suitable scattering profile there exists a solution of Vlasov--Poisson which disperses and scatters, in a modified sense, to this profile. Further, as a consequence of the proof, the solutions are shown to admit a polyhomogeneous expansion, to any finite but arbitrarily high order, with coefficients given explicitly in terms of the scattering profile. The proof does not exploit the full ellipticity of the Poisson equation.","sentences":["We give a new proof of well posedness of the inverse modified scattering problem for the Vlasov--Poisson system: for every suitable scattering profile there exists a solution of Vlasov--Poisson which disperses and scatters, in a modified sense, to this profile.","Further, as a consequence of the proof, the solutions are shown to admit a polyhomogeneous expansion, to any finite but arbitrarily high order, with coefficients given explicitly in terms of the scattering profile.","The proof does not exploit the full ellipticity of the Poisson equation."],"url":"http://arxiv.org/abs/2404.15885v1","category":"math.AP"}
{"created":"2024-04-24 13:45:43","title":"Simulating unsteady fluid flows on a superconducting quantum processor","abstract":"Recent advancements of intermediate-scale quantum processors have triggered tremendous interest in the exploration of practical quantum advantage. The simulation of fluid dynamics, a highly challenging problem in classical physics but vital for practical applications, emerges as a good candidate for showing quantum utility. Here, we report an experiment on the digital simulation of unsteady flows, which consists of quantum encoding, evolution, and detection of flow states, with a superconducting quantum processor. The quantum algorithm is based on the Hamiltonian simulation using the hydrodynamic formulation of the Schr\\\"odinger equation. With the median fidelities of 99.97% and 99.67% for parallel single- and two-qubit gates respectively, we simulate the dynamics of a two-dimensional (2D) compressible diverging flow and a 2D decaying vortex with ten qubits. The experimental results well capture the temporal evolution of averaged density and momentum profiles, and qualitatively reproduce spatial flow fields with moderate noises. This work demonstrates the potential of quantum computing in simulating more complex flows, such as turbulence, for practical applications.","sentences":["Recent advancements of intermediate-scale quantum processors have triggered tremendous interest in the exploration of practical quantum advantage.","The simulation of fluid dynamics, a highly challenging problem in classical physics but vital for practical applications, emerges as a good candidate for showing quantum utility.","Here, we report an experiment on the digital simulation of unsteady flows, which consists of quantum encoding, evolution, and detection of flow states, with a superconducting quantum processor.","The quantum algorithm is based on the Hamiltonian simulation using the hydrodynamic formulation of the Schr\\\"odinger equation.","With the median fidelities of 99.97% and 99.67% for parallel single- and two-qubit gates respectively, we simulate the dynamics of a two-dimensional (2D) compressible diverging flow and a 2D decaying vortex with ten qubits.","The experimental results well capture the temporal evolution of averaged density and momentum profiles, and qualitatively reproduce spatial flow fields with moderate noises.","This work demonstrates the potential of quantum computing in simulating more complex flows, such as turbulence, for practical applications."],"url":"http://arxiv.org/abs/2404.15878v1","category":"quant-ph"}
{"created":"2024-04-24 13:41:59","title":"Quasi-equilibrium chemical evolution in starless cores","abstract":"The chemistry of H2O, CO and other small molecular species in an isolated pre-stellar core, L1544, has been assessed in the context of a comprehensive gas-grain chemical model, coupled to an empirically constrained physical/dynamical model. Our main findings are (i) that the chemical network remains in near equilibrium as the core evolves towards star formation and the molecular abundances change in response to the evolving physical conditions. The gas-phase abundances at any time can be calculated accurately with equilibrium chemistry, and the concept of chemical clocks is meaningless in molecular clouds with similar conditions and dynamical time scales, and (ii) A comparison of the results of complex and simple chemical networks indicates that the abundances of the dominant oxygen and carbon species, H2O, CO, C, and C+ are reasonably approximated by simple networks. In chemical equilibrium, the time-dependent differential terms vanish and a simple network reduces to a few algebraic equations. This allows rapid calculation of the abundances most responsible for spectral line radiative cooling in molecular clouds with long dynamical time scales. The dust ice mantles are highly structured and the ice layers retain a memory of the gas-phase abundances at the time of their deposition. A complex (gas-phase and gas-grain) chemical structure therefore exists, with cosmic-ray induced processes dominating in the inner regions. The inferred H2O abundance profiles for L1544 require that the outer parts of the core and also any medium exterior to the core are essentially transparent to the interstellar radiation field.","sentences":["The chemistry of H2O, CO and other small molecular species in an isolated pre-stellar core, L1544, has been assessed in the context of a comprehensive gas-grain chemical model, coupled to an empirically constrained physical/dynamical model.","Our main findings are (i) that the chemical network remains in near equilibrium as the core evolves towards star formation and the molecular abundances change in response to the evolving physical conditions.","The gas-phase abundances at any time can be calculated accurately with equilibrium chemistry, and the concept of chemical clocks is meaningless in molecular clouds with similar conditions and dynamical time scales, and (ii) A comparison of the results of complex and simple chemical networks indicates that the abundances of the dominant oxygen and carbon species, H2O, CO, C, and C+ are reasonably approximated by simple networks.","In chemical equilibrium, the time-dependent differential terms vanish and a simple network reduces to a few algebraic equations.","This allows rapid calculation of the abundances most responsible for spectral line radiative cooling in molecular clouds with long dynamical time scales.","The dust ice mantles are highly structured and the ice layers retain a memory of the gas-phase abundances at the time of their deposition.","A complex (gas-phase and gas-grain) chemical structure therefore exists, with cosmic-ray induced processes dominating in the inner regions.","The inferred H2O abundance profiles for L1544 require that the outer parts of the core and also any medium exterior to the core are essentially transparent to the interstellar radiation field."],"url":"http://arxiv.org/abs/2404.15876v1","category":"astro-ph.SR"}
{"created":"2024-04-24 13:38:36","title":"Confinement-induced resonance from the generalized Gross-Pitaevskii equations","abstract":"The confinement-induced resonances for trapped bosons in the cigar-shaped and pancake geometries are studied within the generalized Gross-Pitaevskii equations, which are a simplified version of the Hartree-Fock-Bogoliubov approximation. Although the Hartree-Fock-Bogoliubov method is considered applicable only for small interparticle interactions, the resonance denominators for the chemical potential are obtained in both quasi-one and quasi-two dimensions. A useful integral representation of the one-particle Green's function are found for the cylindrical confinement. We find the position of a smoothed resonance for the chemical potential in the pancake geometry at positive scattering length.","sentences":["The confinement-induced resonances for trapped bosons in the cigar-shaped and pancake geometries are studied within the generalized Gross-Pitaevskii equations, which are a simplified version of the Hartree-Fock-Bogoliubov approximation.","Although the Hartree-Fock-Bogoliubov method is considered applicable only for small interparticle interactions, the resonance denominators for the chemical potential are obtained in both quasi-one and quasi-two dimensions.","A useful integral representation of the one-particle Green's function are found for the cylindrical confinement.","We find the position of a smoothed resonance for the chemical potential in the pancake geometry at positive scattering length."],"url":"http://arxiv.org/abs/2404.15872v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-24 13:36:28","title":"Scattering Cross Sections of Magnetized Particles within Intense Electromagnetic Waves: Application to Fast Radio Bursts","abstract":"Recently, Beloborodov suggested that there exists a resonance phenomenon between an extremely intense electromagnetic wave and internal magnetized particles. The particles exchange energy with the wave at frequent resonance events and then reach the radiation reaction limit immediately. This process greatly enhances the scattering cross section of the particles. Note that these results only involve an extraordinary (X) mode wave. In this paper, we focus on an intense ordinary (O) mode wave propagating through magnetized particles and compare it with the case of the X-mode wave. Our result shows that the scattering cross section of the particles in the O-mode wave is significantly smaller than that in the X-mode wave. This has important implications for the transparency of a fast radio burst (FRB) inside the magnetosphere of a magnetar. We argue that there is a strong scattering region in the stellar magnetosphere, within which an O-mode wave is more transparent than an X-mode wave for an FRB.","sentences":["Recently, Beloborodov suggested that there exists a resonance phenomenon between an extremely intense electromagnetic wave and internal magnetized particles.","The particles exchange energy with the wave at frequent resonance events and then reach the radiation reaction limit immediately.","This process greatly enhances the scattering cross section of the particles.","Note that these results only involve an extraordinary (X) mode wave.","In this paper, we focus on an intense ordinary (O) mode wave propagating through magnetized particles and compare it with the case of the X-mode wave.","Our result shows that the scattering cross section of the particles in the O-mode wave is significantly smaller than that in the X-mode wave.","This has important implications for the transparency of a fast radio burst (FRB) inside the magnetosphere of a magnetar.","We argue that there is a strong scattering region in the stellar magnetosphere, within which an O-mode wave is more transparent than an X-mode wave for an FRB."],"url":"http://arxiv.org/abs/2404.15870v1","category":"astro-ph.HE"}
{"created":"2024-04-24 13:32:48","title":"Renormalization group of topological scattering networks","abstract":"Exploring and understanding topological phases in systems with strong distributed disorder requires developing fundamentally new approaches to replace traditional tools such as topological band theory. Here, we present a general real-space renormalization group (RG) approach for scattering models, which is capable of dealing with strong distributed disorder without relying on the renormalization of Hamiltonians or wave functions. Such scheme, based on a block-scattering transformation combined with a replica strategy, is applied for a comprehensive study of strongly disordered unitary scattering networks with localized bulk states, uncovering a connection between topological physics and critical behavior. Our RG scheme leads to topological flow diagrams that unveil how the microscopic competition between reflection and non-reciprocity leads to the large-scale emergence of macroscopic scattering attractors, corresponding to trivial and topological insulators. Our findings are confirmed by a scaling analysis of the localization length (LL) and critical exponents, and experimentally validated. The results not only shed light on the fundamental understanding of topological phase transitions and scaling properties in strongly disordered regimes, but also pave the way for practical applications in modern topological condensed-matter and photonics, where disorder may be seen as a useful design degree of freedom, and no longer as a hindrance.","sentences":["Exploring and understanding topological phases in systems with strong distributed disorder requires developing fundamentally new approaches to replace traditional tools such as topological band theory.","Here, we present a general real-space renormalization group (RG) approach for scattering models, which is capable of dealing with strong distributed disorder without relying on the renormalization of Hamiltonians or wave functions.","Such scheme, based on a block-scattering transformation combined with a replica strategy, is applied for a comprehensive study of strongly disordered unitary scattering networks with localized bulk states, uncovering a connection between topological physics and critical behavior.","Our RG scheme leads to topological flow diagrams that unveil how the microscopic competition between reflection and non-reciprocity leads to the large-scale emergence of macroscopic scattering attractors, corresponding to trivial and topological insulators.","Our findings are confirmed by a scaling analysis of the localization length (LL) and critical exponents, and experimentally validated.","The results not only shed light on the fundamental understanding of topological phase transitions and scaling properties in strongly disordered regimes, but also pave the way for practical applications in modern topological condensed-matter and photonics, where disorder may be seen as a useful design degree of freedom, and no longer as a hindrance."],"url":"http://arxiv.org/abs/2404.15866v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-24 13:28:29","title":"Effects of gravitational waves on electromagnetic fields","abstract":"We focus on the interaction of a plane gravitational wave with electromagnetic fields and we describe this interaction in the proper detector frame where, thanks to the introduction of Fermi coordinates, it is possible to refer to directly measurable quantities. The presence of a gravitational field can be addressed in terms of an effective electromagnetic medium and, within this framework, we show that the coupling of pre-existing electromagnetic fields with the gravitational field of the wave gives rise to new effective currents. To assess the impact of these effects, we solve Maxwell's equations for some standard configurations of the electric and magnetic fields.","sentences":["We focus on the interaction of a plane gravitational wave with electromagnetic fields and we describe this interaction in the proper detector frame where, thanks to the introduction of Fermi coordinates, it is possible to refer to directly measurable quantities.","The presence of a gravitational field can be addressed in terms of an effective electromagnetic medium and, within this framework, we show that the coupling of pre-existing electromagnetic fields with the gravitational field of the wave gives rise to new effective currents.","To assess the impact of these effects, we solve Maxwell's equations for some standard configurations of the electric and magnetic fields."],"url":"http://arxiv.org/abs/2404.15864v1","category":"gr-qc"}
{"created":"2024-04-24 12:42:12","title":"Partial Renormalization of Quasiparticle Interactions","abstract":"Nonlocal effective interactions are inherent to non-relativistic quantum many-body systems, but their systematic resummation poses a significant challenge known as the ``vertex problem\" in many-body perturbation theory. We introduce a renormalization scheme based on a projection-based renormalization condition that selectively resums the most essential nonlocal contributions to the effective interaction vertex, avoiding the computational complexity of the full vertex function. This enables us to derive a renormalized Feynman diagrammatic series with large parameters canceled by counter-diagrams, efficiently generated using a perturbative expansion of the parquet equations and computed using a diagrammatic Monte Carlo algorithm. Applying our approach to a 3D Yukawa Fermi liquid, we demonstrate that the renormalized perturbation theory remains predictive even in the strongly correlated regime and uncover significant sign cancellations between different channels contributing to the scattering amplitude. Our work establishes a novel framework for investigating strong correlations in quantum many-body systems, offering a systematic approach to explore nonlocal theories for challenging systems like the electron liquid in material science.","sentences":["Nonlocal effective interactions are inherent to non-relativistic quantum many-body systems, but their systematic resummation poses a significant challenge known as the ``vertex problem\" in many-body perturbation theory.","We introduce a renormalization scheme based on a projection-based renormalization condition that selectively resums the most essential nonlocal contributions to the effective interaction vertex, avoiding the computational complexity of the full vertex function.","This enables us to derive a renormalized Feynman diagrammatic series with large parameters canceled by counter-diagrams, efficiently generated using a perturbative expansion of the parquet equations and computed using a diagrammatic Monte Carlo algorithm.","Applying our approach to a 3D Yukawa Fermi liquid, we demonstrate that the renormalized perturbation theory remains predictive even in the strongly correlated regime and uncover significant sign cancellations between different channels contributing to the scattering amplitude.","Our work establishes a novel framework for investigating strong correlations in quantum many-body systems, offering a systematic approach to explore nonlocal theories for challenging systems like the electron liquid in material science."],"url":"http://arxiv.org/abs/2404.15844v1","category":"cond-mat.str-el"}
{"created":"2024-04-24 12:29:50","title":"Normalized solutions of $L^2$-supercritical NLS equations on noncompact metric graphs","abstract":"We consider the existence of normalized solutions to nonlinear Schr\\\"odinger equations on noncompact metric graphs in the $L^2$ supercritical regime. For sufficiently small prescribed mass ($L^2$ norm), we prove existence of positive solutions on two classes of graphs: periodic graphs, and noncompact graphs with finitely many edges and suitable topological assumptions. Our approach is based on mountain pass techniques. A key point to overcome the serious lack of compactness is to show that all solutions with small mass have positive energy. To complement our analysis, we prove that this is no longer true, in general, for large masses. To the best of our knowledge, these are the first results with an $L^2$ supercritical nonlinearity extended on the whole graph and unraveling the role of topology in the existence of solutions.","sentences":["We consider the existence of normalized solutions to nonlinear Schr\\\"odinger equations on noncompact metric graphs in the $L^2$ supercritical regime.","For sufficiently small prescribed mass ($L^2$ norm), we prove existence of positive solutions on two classes of graphs: periodic graphs, and noncompact graphs with finitely many edges and suitable topological assumptions.","Our approach is based on mountain pass techniques.","A key point to overcome the serious lack of compactness is to show that all solutions with small mass have positive energy.","To complement our analysis, we prove that this is no longer true, in general, for large masses.","To the best of our knowledge, these are the first results with an $L^2$ supercritical nonlinearity extended on the whole graph and unraveling the role of topology in the existence of solutions."],"url":"http://arxiv.org/abs/2404.15841v1","category":"math.AP"}
{"created":"2024-04-24 12:16:37","title":"Empirical Analysis of the Dynamic Binary Value Problem with IOHprofiler","abstract":"Optimization problems in dynamic environments have recently been the source of several theoretical studies. One of these problems is the monotonic Dynamic Binary Value problem, which theoretically has high discriminatory power between different Genetic Algorithms. Given this theoretical foundation, we integrate several versions of this problem into the IOHprofiler benchmarking framework. Using this integration, we perform several large-scale benchmarking experiments to both recreate theoretical results on moderate dimensional problems and investigate aspects of GA's performance which have not yet been studied theoretically. Our results highlight some of the many synergies between theory and benchmarking and offer a platform through which further research into dynamic optimization problems can be performed.","sentences":["Optimization problems in dynamic environments have recently been the source of several theoretical studies.","One of these problems is the monotonic Dynamic Binary Value problem, which theoretically has high discriminatory power between different Genetic Algorithms.","Given this theoretical foundation, we integrate several versions of this problem into the IOHprofiler benchmarking framework.","Using this integration, we perform several large-scale benchmarking experiments to both recreate theoretical results on moderate dimensional problems and investigate aspects of GA's performance which have not yet been studied theoretically.","Our results highlight some of the many synergies between theory and benchmarking and offer a platform through which further research into dynamic optimization problems can be performed."],"url":"http://arxiv.org/abs/2404.15837v1","category":"cs.NE"}
{"created":"2024-04-24 12:14:54","title":"Employing Two-Dimensional Word Embedding for Difficult Tabular Data Stream Classification","abstract":"Rapid technological advances are inherently linked to the increased amount of data, a substantial portion of which can be interpreted as data stream, capable of exhibiting the phenomenon of concept drift and having a high imbalance ratio. Consequently, developing new approaches to classifying difficult data streams is a rapidly growing research area. At the same time, the proliferation of deep learning and transfer learning, as well as the success of convolutional neural networks in computer vision tasks, have contributed to the emergence of a new research trend, namely Multi-Dimensional Encoding (MDE), focusing on transforming tabular data into a homogeneous form of a discrete digital signal. This paper proposes Streaming Super Tabular Machine Learning (SSTML), thereby exploring for the first time the potential of MDE in the difficult data stream classification task. SSTML encodes consecutive data chunks into an image representation using the STML algorithm and then performs a single ResNet-18 training epoch. Experiments conducted on synthetic and real data streams have demonstrated the ability of SSTML to achieve classification quality statistically significantly superior to state-of-the-art algorithms while maintaining comparable processing time.","sentences":["Rapid technological advances are inherently linked to the increased amount of data, a substantial portion of which can be interpreted as data stream, capable of exhibiting the phenomenon of concept drift and having a high imbalance ratio.","Consequently, developing new approaches to classifying difficult data streams is a rapidly growing research area.","At the same time, the proliferation of deep learning and transfer learning, as well as the success of convolutional neural networks in computer vision tasks, have contributed to the emergence of a new research trend, namely Multi-Dimensional Encoding (MDE), focusing on transforming tabular data into a homogeneous form of a discrete digital signal.","This paper proposes Streaming Super Tabular Machine Learning (SSTML), thereby exploring for the first time the potential of MDE in the difficult data stream classification task.","SSTML encodes consecutive data chunks into an image representation using the STML algorithm and then performs a single ResNet-18 training epoch.","Experiments conducted on synthetic and real data streams have demonstrated the ability of SSTML to achieve classification quality statistically significantly superior to state-of-the-art algorithms while maintaining comparable processing time."],"url":"http://arxiv.org/abs/2404.15836v1","category":"cs.LG"}
{"created":"2024-04-24 12:11:33","title":"OpTC -- A Toolchain for Deployment of Neural Networks on AURIX TC3xx Microcontrollers","abstract":"The AURIX 2xx and 3xx families of TriCore microcontrollers are widely used in the automotive industry and, recently, also in applications that involve machine learning tasks. Yet, these applications are mainly engineered manually, and only little tool support exists for bringing neural networks to TriCore microcontrollers. Thus, we propose OpTC, an end-to-end toolchain for automatic compression, conversion, code generation, and deployment of neural networks on TC3xx microcontrollers. OpTC supports various types of neural networks and provides compression using layer-wise pruning based on sensitivity analysis for a given neural network. The flexibility in supporting different types of neural networks, such as multi-layer perceptrons (MLP), convolutional neural networks (CNN), and recurrent neural networks (RNN), is shown in case studies for a TC387 microcontroller. Automotive applications for predicting the temperature in electric motors and detecting anomalies are thereby used to demonstrate the effectiveness and the wide range of applications supported by OpTC.","sentences":["The AURIX 2xx and 3xx families of TriCore microcontrollers are widely used in the automotive industry and, recently, also in applications that involve machine learning tasks.","Yet, these applications are mainly engineered manually, and only little tool support exists for bringing neural networks to TriCore microcontrollers.","Thus, we propose OpTC, an end-to-end toolchain for automatic compression, conversion, code generation, and deployment of neural networks on TC3xx microcontrollers.","OpTC supports various types of neural networks and provides compression using layer-wise pruning based on sensitivity analysis for a given neural network.","The flexibility in supporting different types of neural networks, such as multi-layer perceptrons (MLP), convolutional neural networks (CNN), and recurrent neural networks (RNN), is shown in case studies for a TC387 microcontroller.","Automotive applications for predicting the temperature in electric motors and detecting anomalies are thereby used to demonstrate the effectiveness and the wide range of applications supported by OpTC."],"url":"http://arxiv.org/abs/2404.15833v1","category":"cs.LG"}
{"created":"2024-04-24 11:13:28","title":"Observational signature of Lorentz violation in Kalb-Ramond field model and Bumblebee model: A comprehensive comparative study","abstract":"This article is devoted to the comparative study of the effects of the Lorentz symmetry violation (LV) arising in Kalb-Ramond (KR) and Bumblebee (BM) field models. We study optical appearance with accretion, Quasinormal modes, ringdown waveforms, Hawking radiation, and weak gravitational lensing. The horizon radius, photon radius, and the critical impact parameter for KR BHs decrease with the LV parameter $\\a$. In contrast, they remain independent of the BM parameter $\\b$ and have values the same as those for \\s BH. We find that a KR BH is brighter than a \\s or BM BH for static and infalling accretion. The BM BH, on the other hand, is brighter than a \\s BH when the accretion is static but becomes darker for an infalling accretion. Our investigation into quasinormal modes (QNMs) and ringdown waveforms provides deeper insight into the difference in observational imprints of LV parameters. It reveals that GWs emitted by KR BHs have larger frequencies and decay faster than those emitted by \\s or BM BHs for scalar and electromagnetic perturbations. We then study the greybody factor (GF) and power emitted for both BHs. The Hawking temperature is higher for a KR BM and lower for a BM BH than a \\s BH. It also reveals that the transmission probability decreases with $\\a$ and $\\b$. A comparison of GFs for KR and BM BHs reveals that the transmission probability is higher for BM BH. We also study the effect of LV on the power emitted in the form of Hawking radiation. Power received by an asymptotic observer is larger for a KR BH. We obtain higher-order corrections in the deflection angle and graphically illustrate the impact of $\\a$ and $\\b$. We observe that a light ray gets deflected most from its path when passing by a \\s BH, and the deflection is least when it passes by a KR BH. Our study conclusively shows that we can differentiate between KR and BM BHs based on astrophysical observations.","sentences":["This article is devoted to the comparative study of the effects of the Lorentz symmetry violation (LV) arising in Kalb-Ramond (KR) and Bumblebee (BM) field models.","We study optical appearance with accretion, Quasinormal modes, ringdown waveforms, Hawking radiation, and weak gravitational lensing.","The horizon radius, photon radius, and the critical impact parameter for KR BHs decrease with the LV parameter $\\a$.","In contrast, they remain independent of the BM parameter $\\b$ and have values the same as those for \\s BH.","We find that a KR BH is brighter than a \\s or BM BH for static and infalling accretion.","The BM BH, on the other hand, is brighter than a \\s BH when the accretion is static but becomes darker for an infalling accretion.","Our investigation into quasinormal modes (QNMs) and ringdown waveforms provides deeper insight into the difference in observational imprints of LV parameters.","It reveals that GWs emitted by KR BHs have larger frequencies and decay faster than those emitted by \\s or BM BHs for scalar and electromagnetic perturbations.","We then study the greybody factor (GF) and power emitted for both BHs.","The Hawking temperature is higher for a KR BM and lower for a BM BH than a \\s BH.","It also reveals that the transmission probability decreases with $\\a$ and $\\b$. A comparison of GFs for KR and BM BHs reveals that the transmission probability is higher for BM BH.","We also study the effect of LV on the power emitted in the form of Hawking radiation.","Power received by an asymptotic observer is larger for a KR BH.","We obtain higher-order corrections in the deflection angle and graphically illustrate the impact of $\\a$ and $\\b$. We observe that a light ray gets deflected most from its path when passing by a \\s BH, and the deflection is least when it passes by a KR BH.","Our study conclusively shows that we can differentiate between KR and BM BHs based on astrophysical observations."],"url":"http://arxiv.org/abs/2404.15808v1","category":"gr-qc"}
{"created":"2024-04-24 10:32:59","title":"Orbits around a black bounce spacetime","abstract":"In this work, the trajectories of particles around a black bounce spacetime are considered, with the Simpson-Visser model serving as an example. Trajectories for massless and massive particles are obtained through the study of null and time-like geodesics. As the Simpson-Visser solution is derived via the Einstein equations for a source involving nonlinear electrodynamics and a scalar field, photon trajectories are investigated by considering an effective metric in which photons follow null geodesics. The stability of circular orbits is analyzed by examining the behavior of maxima and minima of the effective potential associated with geodesics. It is also studied what type of geodesic photons follow when the usual metric is considered instead of the effective one. The main focus of this work is to obtain corrections to the trajectories of photons when considering that the solution arises from nonlinear electrodynamics.","sentences":["In this work, the trajectories of particles around a black bounce spacetime are considered, with the Simpson-Visser model serving as an example.","Trajectories for massless and massive particles are obtained through the study of null and time-like geodesics.","As the Simpson-Visser solution is derived via the Einstein equations for a source involving nonlinear electrodynamics and a scalar field, photon trajectories are investigated by considering an effective metric in which photons follow null geodesics.","The stability of circular orbits is analyzed by examining the behavior of maxima and minima of the effective potential associated with geodesics.","It is also studied what type of geodesic photons follow when the usual metric is considered instead of the effective one.","The main focus of this work is to obtain corrections to the trajectories of photons when considering that the solution arises from nonlinear electrodynamics."],"url":"http://arxiv.org/abs/2404.15792v1","category":"gr-qc"}
{"created":"2024-04-24 10:31:53","title":"Error estimates of a regularized finite difference method for the Logarithmic Schr\u00f6dinger equation with Dirac delta potential","abstract":"In this paper, we introduce a conservative Crank-Nicolson-type finite difference schemes for the regularized logarithmic Schr\\\"{o}dinger equation (RLSE) with Dirac delta potential in 1D. The regularized logarithmic Schr\\\"{o}dinger equation with a small regularized parameter $0<\\eps \\ll 1$ is adopted to approximate the logarithmic Schr\\\"{o}dinger equation (LSE) with linear convergence rate $O(\\eps)$. The numerical method can be used to avoid numerical blow-up and/or to suppress round-off error due to the logarithmic nonlinearity in LSE. Then, by using domain-decomposition technique, we can transform the original problem into an interface problem. Different treatments on the interface conditions lead to different discrete schemes and it turns out that a simple discrete approximation of the Dirac potential coincides with one of the conservative finite difference schemes. The optimal $H^1$ error estimates and the conservative properties of the finite difference schemes are investigated. The Crank-Nicolson finite difference methods enjoy the second-order convergence rate in time and space. Numerical examples are provided to support our analysis and show the accuracy and efficiency of the numerical method.","sentences":["In this paper, we introduce a conservative Crank-Nicolson-type finite difference schemes for the regularized logarithmic Schr\\\"{o}dinger equation (RLSE) with Dirac delta potential in 1D.","The regularized logarithmic Schr\\\"{o}dinger equation with a small regularized parameter $0<\\eps \\ll 1$ is adopted to approximate the logarithmic Schr\\\"{o}dinger equation (LSE) with linear convergence rate $O(\\eps)$. The numerical method can be used to avoid numerical blow-up and/or to suppress round-off error due to the logarithmic nonlinearity in LSE.","Then, by using domain-decomposition technique, we can transform the original problem into an interface problem.","Different treatments on the interface conditions lead to different discrete schemes and it turns out that a simple discrete approximation of the Dirac potential coincides with one of the conservative finite difference schemes.","The optimal $H^1$ error estimates and the conservative properties of the finite difference schemes are investigated.","The Crank-Nicolson finite difference methods enjoy the second-order convergence rate in time and space.","Numerical examples are provided to support our analysis and show the accuracy and efficiency of the numerical method."],"url":"http://arxiv.org/abs/2404.15791v1","category":"math.NA"}
{"created":"2024-04-24 10:28:54","title":"MotionMaster: Training-free Camera Motion Transfer For Video Generation","abstract":"The emergence of diffusion models has greatly propelled the progress in image and video generation. Recently, some efforts have been made in controllable video generation, including text-to-video generation and video motion control, among which camera motion control is an important topic. However, existing camera motion control methods rely on training a temporal camera module, and necessitate substantial computation resources due to the large amount of parameters in video generation models. Moreover, existing methods pre-define camera motion types during training, which limits their flexibility in camera control. Therefore, to reduce training costs and achieve flexible camera control, we propose COMD, a novel training-free video motion transfer model, which disentangles camera motions and object motions in source videos and transfers the extracted camera motions to new videos. We first propose a one-shot camera motion disentanglement method to extract camera motion from a single source video, which separates the moving objects from the background and estimates the camera motion in the moving objects region based on the motion in the background by solving a Poisson equation. Furthermore, we propose a few-shot camera motion disentanglement method to extract the common camera motion from multiple videos with similar camera motions, which employs a window-based clustering technique to extract the common features in temporal attention maps of multiple videos. Finally, we propose a motion combination method to combine different types of camera motions together, enabling our model a more controllable and flexible camera control. Extensive experiments demonstrate that our training-free approach can effectively decouple camera-object motion and apply the decoupled camera motion to a wide range of controllable video generation tasks, achieving flexible and diverse camera motion control.","sentences":["The emergence of diffusion models has greatly propelled the progress in image and video generation.","Recently, some efforts have been made in controllable video generation, including text-to-video generation and video motion control, among which camera motion control is an important topic.","However, existing camera motion control methods rely on training a temporal camera module, and necessitate substantial computation resources due to the large amount of parameters in video generation models.","Moreover, existing methods pre-define camera motion types during training, which limits their flexibility in camera control.","Therefore, to reduce training costs and achieve flexible camera control, we propose COMD, a novel training-free video motion transfer model, which disentangles camera motions and object motions in source videos and transfers the extracted camera motions to new videos.","We first propose a one-shot camera motion disentanglement method to extract camera motion from a single source video, which separates the moving objects from the background and estimates the camera motion in the moving objects region based on the motion in the background by solving a Poisson equation.","Furthermore, we propose a few-shot camera motion disentanglement method to extract the common camera motion from multiple videos with similar camera motions, which employs a window-based clustering technique to extract the common features in temporal attention maps of multiple videos.","Finally, we propose a motion combination method to combine different types of camera motions together, enabling our model a more controllable and flexible camera control.","Extensive experiments demonstrate that our training-free approach can effectively decouple camera-object motion and apply the decoupled camera motion to a wide range of controllable video generation tasks, achieving flexible and diverse camera motion control."],"url":"http://arxiv.org/abs/2404.15789v1","category":"cs.CV"}
{"created":"2024-04-24 10:20:55","title":"Violent relaxation in one-dimensional self-gravitating system: deviation from the Vlasov limit due to finite-$N$ effect","abstract":"We investigate the effect of a finite particle number $N$ on the violent relaxation leading to the Quasi-Stationary State (QSS) in a one-dimensional self-gravitating system. From the theoretical point of view, we demonstrate that the local Poissonian fluctuations embedded in the initial state give rise to an additional term proportional to $1/N$ in the Vlasov equation. This term designates the strength of the local mean-field variations by fluctuations. Because it is of the mean-field origin, we interpret it differently from the known collision term in the way that it effects the violent relaxation stage. Its role is to deviate the distribution function from the Vlasov limit, in the collisionless manner, at a rate proportional to $1/N$ while the violent relaxation is progressing. This hypothesis is tested by inspecting the QSSs in simulations of various $N$. We observe that the core phase-space density can exceed the limiting density deduced from the Vlasov equation and its deviation degree is in accordance with the $1/N$ estimate. This indicates the deviation from the standard mean-field approximation of the violent relaxation process by that $1/N$ term. In conclusion, the finite-$N$ effect has a significant contribution to the QSS apart from that it plays a role in the collisional stage that takes place long after. The conventional collisionless Vlasov equation might not be able to describe the violent relaxation of a system of particles properly without the correction term of the local finite-$N$ fluctuations.","sentences":["We investigate the effect of a finite particle number $N$ on the violent relaxation leading to the Quasi-Stationary State (QSS) in a one-dimensional self-gravitating system.","From the theoretical point of view, we demonstrate that the local Poissonian fluctuations embedded in the initial state give rise to an additional term proportional to $1/N$ in the Vlasov equation.","This term designates the strength of the local mean-field variations by fluctuations.","Because it is of the mean-field origin, we interpret it differently from the known collision term in the way that it effects the violent relaxation stage.","Its role is to deviate the distribution function from the Vlasov limit, in the collisionless manner, at a rate proportional to $1/N$ while the violent relaxation is progressing.","This hypothesis is tested by inspecting the QSSs in simulations of various $N$. We observe that the core phase-space density can exceed the limiting density deduced from the Vlasov equation and its deviation degree is in accordance with the $1/N$ estimate.","This indicates the deviation from the standard mean-field approximation of the violent relaxation process by that $1/N$ term.","In conclusion, the finite-$N$ effect has a significant contribution to the QSS apart from that it plays a role in the collisional stage that takes place long after.","The conventional collisionless Vlasov equation might not be able to describe the violent relaxation of a system of particles properly without the correction term of the local finite-$N$ fluctuations."],"url":"http://arxiv.org/abs/2404.15787v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-24 10:12:43","title":"An Empirical Study of Aegis","abstract":"Bit flipping attacks are one class of attacks on neural networks with numerous defense mechanisms invented to mitigate its potency. Due to the importance of ensuring the robustness of these defense mechanisms, we perform an empirical study on the Aegis framework. We evaluate the baseline mechanisms of Aegis on low-entropy data (MNIST), and we evaluate a pre-trained model with the mechanisms fine-tuned on MNIST. We also compare the use of data augmentation to the robustness training of Aegis, and how Aegis performs under other adversarial attacks, such as the generation of adversarial examples. We find that both the dynamic-exit strategy and robustness training of Aegis has some drawbacks. In particular, we see drops in accuracy when testing on perturbed data, and on adversarial examples, as compared to baselines. Moreover, we found that the dynamic exit-strategy loses its uniformity when tested on simpler datasets. The code for this project is available on GitHub.","sentences":["Bit flipping attacks are one class of attacks on neural networks with numerous defense mechanisms invented to mitigate its potency.","Due to the importance of ensuring the robustness of these defense mechanisms, we perform an empirical study on the Aegis framework.","We evaluate the baseline mechanisms of Aegis on low-entropy data (MNIST), and we evaluate a pre-trained model with the mechanisms fine-tuned on MNIST.","We also compare the use of data augmentation to the robustness training of Aegis, and how Aegis performs under other adversarial attacks, such as the generation of adversarial examples.","We find that both the dynamic-exit strategy and robustness training of Aegis has some drawbacks.","In particular, we see drops in accuracy when testing on perturbed data, and on adversarial examples, as compared to baselines.","Moreover, we found that the dynamic exit-strategy loses its uniformity when tested on simpler datasets.","The code for this project is available on GitHub."],"url":"http://arxiv.org/abs/2404.15784v1","category":"cs.LG"}
{"created":"2024-04-24 09:55:08","title":"Unconditional well-posedness for the nonlinear Schr\u00f6dinger equation in Bessel potential spaces","abstract":"The Cauchy problem for the nonlinear Schr\\\"odinger equation is called unconditionally well posed in a data space $E$ if it is well posed in the usual sense and the solution is unique in the space $C([0,T]; E)$. In this paper, this notion of unconditional well-posedness is redefined so that it covers $L^p$-based Sobolev spaces as data space $E$ and it is equivalent to the usual one when $E$ is an $L^2$-based Sobolev space $H^s$. Next, based on this definition, it is shown that the Cauchy problem for the 1D cubic NLS is unconditionally well posed in Bessel potential spaces $H^s_p$ for $4/3<p\\le 2$ under certain regularity assumptions on $s$.","sentences":["The Cauchy problem for the nonlinear Schr\\\"odinger equation is called unconditionally well posed in a data space $E$ if it is well posed in the usual sense and the solution is unique in the space $C([0,T];","E)$.","In this paper, this notion of unconditional well-posedness is redefined so that it covers $L^p$-based Sobolev spaces as data space $E$ and it is equivalent to the usual one when $E$ is an $L^2$-based Sobolev space $H^s$. Next, based on this definition, it is shown that the Cauchy problem for the 1D cubic NLS is unconditionally well posed in Bessel potential spaces $H^s_p$ for $4/3<p\\le 2$ under certain regularity assumptions on $s$."],"url":"http://arxiv.org/abs/2404.15775v1","category":"math.AP"}
{"created":"2024-04-24 09:41:23","title":"Isomonodromy and Painlev\u00e9 type equations, Case Studies","abstract":"There is an abundance of equations of Painlev\\'e type besides the classical Painlev\\'e equations. Classifications have been computed by the Japanese school. Here we consider Painlev\\'e type equations induced by isomonodromic families of linear ODE's having at most $z=0$ and $z=\\infty$ as singularities. Requiring that the formal data at the singularities produce isomonodromic families parametrized by a single variable $t$ leads to a small list of hierarchies of cases. The study of these cases involves Stokes matrices and moduli for linear ODE's on the projective line.   Case studies reveal interesting families of linear ODE's and Painlev\\'e type equations. However, rather often the complexity (especially of the Lax pair) is too high for either the computations or for the output. Apart from classical Painlev\\'e equations one rediscovers work of M.~Mazzocco, M.~Noumi and Y.~Yamada. A hierarchy, probably new, related to the classical $P_3(D_8)$, is discovered. Finally, an amusing ``companion'' of $P_1$ is presented.","sentences":["There is an abundance of equations of Painlev\\'e type besides the classical Painlev\\'e equations.","Classifications have been computed by the Japanese school.","Here we consider Painlev\\'e type equations induced by isomonodromic families of linear ODE's having at most $z=0$ and $z=\\infty$ as singularities.","Requiring that the formal data at the singularities produce isomonodromic families parametrized by a single variable $t$ leads to a small list of hierarchies of cases.","The study of these cases involves Stokes matrices and moduli for linear ODE's on the projective line.   ","Case studies reveal interesting families of linear ODE's and Painlev\\'e type equations.","However, rather often the complexity (especially of the Lax pair) is too high for either the computations or for the output.","Apart from classical Painlev\\'e equations one rediscovers work of M.~Mazzocco, M.~Noumi and Y.~Yamada.","A hierarchy, probably new, related to the classical $P_3(D_8)$, is discovered.","Finally, an amusing ``companion'' of $P_1$ is presented."],"url":"http://arxiv.org/abs/2404.15767v1","category":"math.CA"}
{"created":"2024-04-24 09:25:16","title":"Exploring Machine Learning Algorithms for Infection Detection Using GC-IMS Data: A Preliminary Study","abstract":"The developing field of enhanced diagnostic techniques in the diagnosis of infectious diseases, constitutes a crucial domain in modern healthcare. By utilizing Gas Chromatography-Ion Mobility Spectrometry (GC-IMS) data and incorporating machine learning algorithms into one platform, our research aims to tackle the ongoing issue of precise infection identification. Inspired by these difficulties, our goals consist of creating a strong data analytics process, enhancing machine learning (ML) models, and performing thorough validation for clinical applications. Our research contributes to the emerging field of advanced diagnostic technologies by integrating Gas Chromatography-Ion Mobility Spectrometry (GC-IMS) data and machine learning algorithms within a unified Laboratory Information Management System (LIMS) platform. Preliminary trials demonstrate encouraging levels of accuracy when employing various ML algorithms to differentiate between infected and non-infected samples. Continuing endeavors are currently concentrated on enhancing the effectiveness of the model, investigating techniques to clarify its functioning, and incorporating many types of data to further support the early detection of diseases.","sentences":["The developing field of enhanced diagnostic techniques in the diagnosis of infectious diseases, constitutes a crucial domain in modern healthcare.","By utilizing Gas Chromatography-Ion Mobility Spectrometry (GC-IMS) data and incorporating machine learning algorithms into one platform, our research aims to tackle the ongoing issue of precise infection identification.","Inspired by these difficulties, our goals consist of creating a strong data analytics process, enhancing machine learning (ML) models, and performing thorough validation for clinical applications.","Our research contributes to the emerging field of advanced diagnostic technologies by integrating Gas Chromatography-Ion Mobility Spectrometry (GC-IMS) data and machine learning algorithms within a unified Laboratory Information Management System (LIMS) platform.","Preliminary trials demonstrate encouraging levels of accuracy when employing various ML algorithms to differentiate between infected and non-infected samples.","Continuing endeavors are currently concentrated on enhancing the effectiveness of the model, investigating techniques to clarify its functioning, and incorporating many types of data to further support the early detection of diseases."],"url":"http://arxiv.org/abs/2404.15757v1","category":"cs.LG"}
{"created":"2024-04-24 09:18:16","title":"Convolutional Coded Poisson Receivers","abstract":"In this paper, we present a framework for convolutional coded Poisson receivers (CCPRs) that incorporates spatially coupled methods into the architecture of coded Poisson receivers (CPRs). We use density evolution equations to track the packet decoding process with the successive interference cancellation (SIC) technique. We derive outer bounds for the stability region of CPRs when the underlying channel can be modeled by a $\\phi$-ALOHA receiver. The stability region is the set of loads that every packet can be successfully received with a probability of 1. Our outer bounds extend those of the spatially-coupled Irregular Repetition Slotted ALOHA (IRSA) protocol and apply to channel models with multiple traffic classes. For CCPRs with a single class of users, the stability region is reduced to an interval. Therefore, it can be characterized by a percolation threshold. We study the potential threshold by the potential function of the base CPR used for constructing a CCPR. In addition, we prove that the CCPR is stable under a technical condition for the window size. For the multiclass scenario, we recursively evaluate the density evolution equations to determine the boundaries of the stability region. Numerical results demonstrate that the stability region of CCPRs can be enlarged compared to that of CPRs by leveraging the spatially-coupled method. Moreover, the stability region of CCPRs is close to our outer bounds when the window size is large.","sentences":["In this paper, we present a framework for convolutional coded Poisson receivers (CCPRs) that incorporates spatially coupled methods into the architecture of coded Poisson receivers (CPRs).","We use density evolution equations to track the packet decoding process with the successive interference cancellation (SIC) technique.","We derive outer bounds for the stability region of CPRs when the underlying channel can be modeled by a $\\phi$-ALOHA receiver.","The stability region is the set of loads that every packet can be successfully received with a probability of 1.","Our outer bounds extend those of the spatially-coupled Irregular Repetition Slotted ALOHA (IRSA) protocol and apply to channel models with multiple traffic classes.","For CCPRs with a single class of users, the stability region is reduced to an interval.","Therefore, it can be characterized by a percolation threshold.","We study the potential threshold by the potential function of the base CPR used for constructing a CCPR.","In addition, we prove that the CCPR is stable under a technical condition for the window size.","For the multiclass scenario, we recursively evaluate the density evolution equations to determine the boundaries of the stability region.","Numerical results demonstrate that the stability region of CCPRs can be enlarged compared to that of CPRs by leveraging the spatially-coupled method.","Moreover, the stability region of CCPRs is close to our outer bounds when the window size is large."],"url":"http://arxiv.org/abs/2404.15756v1","category":"cs.IT"}
{"created":"2024-04-24 09:10:25","title":"The homogeneous generalized Ricci flow","abstract":"We develop a framework inspired by Lauret's \"bracket flow\" to study the generalized Ricci flow, as introduced by Streets, on discrete quotients of Lie groups. As a first application, we establish global existence on solvmanifolds in arbitrary dimensions, a result which is new even for the pluriclosed flow. We also define a notion of generalized Ricci soliton on exact Courant algebroids that is geometrically meaningful and allows for non-trivial expanding examples. On nilmanifolds, we show that these solitons arise as rescaled limits of the generalized Ricci flow, provided the initial metrics have \"harmonic torsion\", and we classify them in low dimensions. Finally, we provide a new formula for the generalized Ricci curvature of invariant generalized metrics in terms of a moment map for the action of a non-reductive real Lie group.","sentences":["We develop a framework inspired by Lauret's \"bracket flow\" to study the generalized Ricci flow, as introduced by Streets, on discrete quotients of Lie groups.","As a first application, we establish global existence on solvmanifolds in arbitrary dimensions, a result which is new even for the pluriclosed flow.","We also define a notion of generalized Ricci soliton on exact Courant algebroids that is geometrically meaningful and allows for non-trivial expanding examples.","On nilmanifolds, we show that these solitons arise as rescaled limits of the generalized Ricci flow, provided the initial metrics have \"harmonic torsion\", and we classify them in low dimensions.","Finally, we provide a new formula for the generalized Ricci curvature of invariant generalized metrics in terms of a moment map for the action of a non-reductive real Lie group."],"url":"http://arxiv.org/abs/2404.15749v1","category":"math.DG"}
{"created":"2024-04-24 09:01:50","title":"Generalizing the SINDy approach with nested neural networks","abstract":"Symbolic Regression (SR) is a widely studied field of research that aims to infer symbolic expressions from data. A popular approach for SR is the Sparse Identification of Nonlinear Dynamical Systems (\\sindy) framework, which uses sparse regression to identify governing equations from data. This study introduces an enhanced method, Nested SINDy, that aims to increase the expressivity of the SINDy approach thanks to a nested structure. Indeed, traditional symbolic regression and system identification methods often fail with complex systems that cannot be easily described analytically. Nested SINDy builds on the SINDy framework by introducing additional layers before and after the core SINDy layer. This allows the method to identify symbolic representations for a wider range of systems, including those with compositions and products of functions. We demonstrate the ability of the Nested SINDy approach to accurately find symbolic expressions for simple systems, such as basic trigonometric functions, and sparse (false but accurate) analytical representations for more complex systems. Our results highlight Nested SINDy's potential as a tool for symbolic regression, surpassing the traditional SINDy approach in terms of expressivity. However, we also note the challenges in the optimization process for Nested SINDy and suggest future research directions, including the designing of a more robust methodology for the optimization process. This study proves that Nested SINDy can effectively discover symbolic representations of dynamical systems from data, offering new opportunities for understanding complex systems through data-driven methods.","sentences":["Symbolic Regression (SR) is a widely studied field of research that aims to infer symbolic expressions from data.","A popular approach for SR is the Sparse Identification of Nonlinear Dynamical Systems (\\sindy) framework, which uses sparse regression to identify governing equations from data.","This study introduces an enhanced method, Nested SINDy, that aims to increase the expressivity of the SINDy approach thanks to a nested structure.","Indeed, traditional symbolic regression and system identification methods often fail with complex systems that cannot be easily described analytically.","Nested SINDy builds on the SINDy framework by introducing additional layers before and after the core SINDy layer.","This allows the method to identify symbolic representations for a wider range of systems, including those with compositions and products of functions.","We demonstrate the ability of the Nested SINDy approach to accurately find symbolic expressions for simple systems, such as basic trigonometric functions, and sparse (false but accurate) analytical representations for more complex systems.","Our results highlight Nested SINDy's potential as a tool for symbolic regression, surpassing the traditional SINDy approach in terms of expressivity.","However, we also note the challenges in the optimization process for Nested SINDy and suggest future research directions, including the designing of a more robust methodology for the optimization process.","This study proves that Nested SINDy can effectively discover symbolic representations of dynamical systems from data, offering new opportunities for understanding complex systems through data-driven methods."],"url":"http://arxiv.org/abs/2404.15742v1","category":"math.NA"}
{"created":"2024-04-24 08:39:14","title":"MD-NOMAD: Mixture density nonlinear manifold decoder for emulating stochastic differential equations and uncertainty propagation","abstract":"We propose a neural operator framework, termed mixture density nonlinear manifold decoder (MD-NOMAD), for stochastic simulators. Our approach leverages an amalgamation of the pointwise operator learning neural architecture nonlinear manifold decoder (NOMAD) with mixture density-based methods to estimate conditional probability distributions for stochastic output functions. MD-NOMAD harnesses the ability of probabilistic mixture models to estimate complex probability and the high-dimensional scalability of pointwise neural operator NOMAD. We conduct empirical assessments on a wide array of stochastic ordinary and partial differential equations and present the corresponding results, which highlight the performance of the proposed framework.","sentences":["We propose a neural operator framework, termed mixture density nonlinear manifold decoder (MD-NOMAD), for stochastic simulators.","Our approach leverages an amalgamation of the pointwise operator learning neural architecture nonlinear manifold decoder (NOMAD) with mixture density-based methods to estimate conditional probability distributions for stochastic output functions.","MD-NOMAD harnesses the ability of probabilistic mixture models to estimate complex probability and the high-dimensional scalability of pointwise neural operator NOMAD.","We conduct empirical assessments on a wide array of stochastic ordinary and partial differential equations and present the corresponding results, which highlight the performance of the proposed framework."],"url":"http://arxiv.org/abs/2404.15731v1","category":"cs.LG"}
{"created":"2024-04-24 08:37:13","title":"Gradformer: Graph Transformer with Exponential Decay","abstract":"Graph Transformers (GTs) have demonstrated their advantages across a wide range of tasks. However, the self-attention mechanism in GTs overlooks the graph's inductive biases, particularly biases related to structure, which are crucial for the graph tasks. Although some methods utilize positional encoding and attention bias to model inductive biases, their effectiveness is still suboptimal analytically. Therefore, this paper presents Gradformer, a method innovatively integrating GT with the intrinsic inductive bias by applying an exponential decay mask to the attention matrix. Specifically, the values in the decay mask matrix diminish exponentially, correlating with the decreasing node proximities within the graph structure. This design enables Gradformer to retain its ability to capture information from distant nodes while focusing on the graph's local details. Furthermore, Gradformer introduces a learnable constraint into the decay mask, allowing different attention heads to learn distinct decay masks. Such an design diversifies the attention heads, enabling a more effective assimilation of diverse structural information within the graph. Extensive experiments on various benchmarks demonstrate that Gradformer consistently outperforms the Graph Neural Network and GT baseline models in various graph classification and regression tasks. Additionally, Gradformer has proven to be an effective method for training deep GT models, maintaining or even enhancing accuracy compared to shallow models as the network deepens, in contrast to the significant accuracy drop observed in other GT models.Codes are available at \\url{https://github.com/LiuChuang0059/Gradformer}.","sentences":["Graph Transformers (GTs) have demonstrated their advantages across a wide range of tasks.","However, the self-attention mechanism in GTs overlooks the graph's inductive biases, particularly biases related to structure, which are crucial for the graph tasks.","Although some methods utilize positional encoding and attention bias to model inductive biases, their effectiveness is still suboptimal analytically.","Therefore, this paper presents Gradformer, a method innovatively integrating GT with the intrinsic inductive bias by applying an exponential decay mask to the attention matrix.","Specifically, the values in the decay mask matrix diminish exponentially, correlating with the decreasing node proximities within the graph structure.","This design enables Gradformer to retain its ability to capture information from distant nodes while focusing on the graph's local details.","Furthermore, Gradformer introduces a learnable constraint into the decay mask, allowing different attention heads to learn distinct decay masks.","Such an design diversifies the attention heads, enabling a more effective assimilation of diverse structural information within the graph.","Extensive experiments on various benchmarks demonstrate that Gradformer consistently outperforms the Graph Neural Network and GT baseline models in various graph classification and regression tasks.","Additionally, Gradformer has proven to be an effective method for training deep GT models, maintaining or even enhancing accuracy compared to shallow models as the network deepens, in contrast to the significant accuracy drop observed in other GT models.","Codes are available at \\url{https://github.com/LiuChuang0059/Gradformer}."],"url":"http://arxiv.org/abs/2404.15729v1","category":"cs.LG"}
{"created":"2024-04-24 08:32:21","title":"A least squares approach to Whitney forms","abstract":"In this work we describe and test the construction of least squares Whitney forms based on weights. If, on the one hand, the relevance of such a family of differential forms is nowadays clear in numerical analysis, on the other hand the selection of performing sets of supports (hence of weights) for projecting onto high order Whitney forms turns often to be a rough task. As an account of this, it is worth mentioning that Runge-like phenomena have been observed but still not resolved completely. We hence move away from sharp results on unisolvence and consider a least squares approach, obtaining results that are consistent with the nodal literature and making some steps towards the resolution of the aforementioned Runge phenomenon for high order Whitney forms.","sentences":["In this work we describe and test the construction of least squares Whitney forms based on weights.","If, on the one hand, the relevance of such a family of differential forms is nowadays clear in numerical analysis, on the other hand the selection of performing sets of supports (hence of weights) for projecting onto high order Whitney forms turns often to be a rough task.","As an account of this, it is worth mentioning that Runge-like phenomena have been observed but still not resolved completely.","We hence move away from sharp results on unisolvence and consider a least squares approach, obtaining results that are consistent with the nodal literature and making some steps towards the resolution of the aforementioned Runge phenomenon for high order Whitney forms."],"url":"http://arxiv.org/abs/2404.15727v1","category":"math.NA"}
{"created":"2024-04-25 17:58:09","title":"Boosting Unsupervised Semantic Segmentation with Principal Mask Proposals","abstract":"Unsupervised semantic segmentation aims to automatically partition images into semantically meaningful regions by identifying global categories within an image corpus without any form of annotation. Building upon recent advances in self-supervised representation learning, we focus on how to leverage these large pre-trained models for the downstream task of unsupervised segmentation. We present PriMaPs - Principal Mask Proposals - decomposing images into semantically meaningful masks based on their feature representation. This allows us to realize unsupervised semantic segmentation by fitting class prototypes to PriMaPs with a stochastic expectation-maximization algorithm, PriMaPs-EM. Despite its conceptual simplicity, PriMaPs-EM leads to competitive results across various pre-trained backbone models, including DINO and DINOv2, and across datasets, such as Cityscapes, COCO-Stuff, and Potsdam-3. Importantly, PriMaPs-EM is able to boost results when applied orthogonally to current state-of-the-art unsupervised semantic segmentation pipelines.","sentences":["Unsupervised semantic segmentation aims to automatically partition images into semantically meaningful regions by identifying global categories within an image corpus without any form of annotation.","Building upon recent advances in self-supervised representation learning, we focus on how to leverage these large pre-trained models for the downstream task of unsupervised segmentation.","We present PriMaPs - Principal Mask Proposals - decomposing images into semantically meaningful masks based on their feature representation.","This allows us to realize unsupervised semantic segmentation by fitting class prototypes to PriMaPs with a stochastic expectation-maximization algorithm, PriMaPs-EM.","Despite its conceptual simplicity, PriMaPs-EM leads to competitive results across various pre-trained backbone models, including DINO and DINOv2, and across datasets, such as Cityscapes, COCO-Stuff, and Potsdam-3.","Importantly, PriMaPs-EM is able to boost results when applied orthogonally to current state-of-the-art unsupervised semantic segmentation pipelines."],"url":"http://arxiv.org/abs/2404.16818v1","category":"cs.CV"}
{"created":"2024-04-25 17:30:38","title":"Registration by Regression (RbR): a framework for interpretable and flexible atlas registration","abstract":"In human neuroimaging studies, atlas registration enables mapping MRI scans to a common coordinate frame, which is necessary to aggregate data from multiple subjects. Machine learning registration methods have achieved excellent speed and accuracy but lack interpretability. More recently, keypoint-based methods have been proposed to tackle this issue, but their accuracy is still subpar, particularly when fitting nonlinear transforms. Here we propose Registration by Regression (RbR), a novel atlas registration framework that is highly robust and flexible, conceptually simple, and can be trained with cheaply obtained data. RbR predicts the (x,y,z) atlas coordinates for every voxel of the input scan (i.e., every voxel is a keypoint), and then uses closed-form expressions to quickly fit transforms using a wide array of possible deformation models, including affine and nonlinear (e.g., Bspline, Demons, invertible diffeomorphic models, etc.). Robustness is provided by the large number of voxels informing the registration and can be further increased by robust estimators like RANSAC. Experiments on independent public datasets show that RbR yields more accurate registration than competing keypoint approaches, while providing full control of the deformation model.","sentences":["In human neuroimaging studies, atlas registration enables mapping MRI scans to a common coordinate frame, which is necessary to aggregate data from multiple subjects.","Machine learning registration methods have achieved excellent speed and accuracy but lack interpretability.","More recently, keypoint-based methods have been proposed to tackle this issue, but their accuracy is still subpar, particularly when fitting nonlinear transforms.","Here we propose Registration by Regression (RbR), a novel atlas registration framework that is highly robust and flexible, conceptually simple, and can be trained with cheaply obtained data.","RbR predicts the (x,y,z) atlas coordinates for every voxel of the input scan (i.e., every voxel is a keypoint), and then uses closed-form expressions to quickly fit transforms using a wide array of possible deformation models, including affine and nonlinear (e.g., Bspline, Demons, invertible diffeomorphic models, etc.).","Robustness is provided by the large number of voxels informing the registration and can be further increased by robust estimators like RANSAC.","Experiments on independent public datasets show that RbR yields more accurate registration than competing keypoint approaches, while providing full control of the deformation model."],"url":"http://arxiv.org/abs/2404.16781v1","category":"cs.CV"}
{"created":"2024-04-25 16:39:32","title":"History repeats itself: A Baseline for Temporal Knowledge Graph Forecasting","abstract":"Temporal Knowledge Graph (TKG) Forecasting aims at predicting links in Knowledge Graphs for future timesteps based on a history of Knowledge Graphs. To this day, standardized evaluation protocols and rigorous comparison across TKG models are available, but the importance of simple baselines is often neglected in the evaluation, which prevents researchers from discerning actual and fictitious progress. We propose to close this gap by designing an intuitive baseline for TKG Forecasting based on predicting recurring facts. Compared to most TKG models, it requires little hyperparameter tuning and no iterative training. Further, it can help to identify failure modes in existing approaches. The empirical findings are quite unexpected: compared to 11 methods on five datasets, our baseline ranks first or third in three of them, painting a radically different picture of the predictive quality of the state of the art.","sentences":["Temporal Knowledge Graph (TKG) Forecasting aims at predicting links in Knowledge Graphs for future timesteps based on a history of Knowledge Graphs.","To this day, standardized evaluation protocols and rigorous comparison across TKG models are available, but the importance of simple baselines is often neglected in the evaluation, which prevents researchers from discerning actual and fictitious progress.","We propose to close this gap by designing an intuitive baseline for TKG Forecasting based on predicting recurring facts.","Compared to most TKG models, it requires little hyperparameter tuning and no iterative training.","Further, it can help to identify failure modes in existing approaches.","The empirical findings are quite unexpected: compared to 11 methods on five datasets, our baseline ranks first or third in three of them, painting a radically different picture of the predictive quality of the state of the art."],"url":"http://arxiv.org/abs/2404.16726v1","category":"cs.LG"}
{"created":"2024-04-25 16:37:58","title":"Tverberg's theorem and multi-class support vector machines","abstract":"We show how, using linear-algebraic tools developed to prove Tverberg's theorem in combinatorial geometry, we can design new models of multi-class support vector machines (SVMs). These supervised learning protocols require fewer conditions to classify sets of points, and can be computed using existing binary SVM algorithms in higher-dimensional spaces, including soft-margin SVM algorithms. We describe how the theoretical guarantees of standard support vector machines transfer to these new classes of multi-class support vector machines. We give a new simple proof of a geometric characterization of support vectors for largest margin SVMs by Veelaert.","sentences":["We show how, using linear-algebraic tools developed to prove Tverberg's theorem in combinatorial geometry, we can design new models of multi-class support vector machines (SVMs).","These supervised learning protocols require fewer conditions to classify sets of points, and can be computed using existing binary SVM algorithms in higher-dimensional spaces, including soft-margin SVM algorithms.","We describe how the theoretical guarantees of standard support vector machines transfer to these new classes of multi-class support vector machines.","We give a new simple proof of a geometric characterization of support vectors for largest margin SVMs by Veelaert."],"url":"http://arxiv.org/abs/2404.16724v1","category":"cs.LG"}
{"created":"2024-04-25 14:10:52","title":"Incorporating Lexical and Syntactic Knowledge for Unsupervised Cross-Lingual Transfer","abstract":"Unsupervised cross-lingual transfer involves transferring knowledge between languages without explicit supervision. Although numerous studies have been conducted to improve performance in such tasks by focusing on cross-lingual knowledge, particularly lexical and syntactic knowledge, current approaches are limited as they only incorporate syntactic or lexical information. Since each type of information offers unique advantages and no previous attempts have combined both, we attempt to explore the potential of this approach. In this paper, we present a novel framework called \"Lexicon-Syntax Enhanced Multilingual BERT\" that combines both lexical and syntactic knowledge. Specifically, we use Multilingual BERT (mBERT) as the base model and employ two techniques to enhance its learning capabilities. The code-switching technique is used to implicitly teach the model lexical alignment information, while a syntactic-based graph attention network is designed to help the model encode syntactic structure. To integrate both types of knowledge, we input code-switched sequences into both the syntactic module and the mBERT base model simultaneously. Our extensive experimental results demonstrate this framework can consistently outperform all baselines of zero-shot cross-lingual transfer, with the gains of 1.0~3.7 points on text classification, named entity recognition (ner), and semantic parsing tasks. Keywords:cross-lingual transfer, lexicon, syntax, code-switching, graph attention network","sentences":["Unsupervised cross-lingual transfer involves transferring knowledge between languages without explicit supervision.","Although numerous studies have been conducted to improve performance in such tasks by focusing on cross-lingual knowledge, particularly lexical and syntactic knowledge, current approaches are limited as they only incorporate syntactic or lexical information.","Since each type of information offers unique advantages and no previous attempts have combined both, we attempt to explore the potential of this approach.","In this paper, we present a novel framework called \"Lexicon-Syntax Enhanced Multilingual BERT\" that combines both lexical and syntactic knowledge.","Specifically, we use Multilingual BERT (mBERT) as the base model and employ two techniques to enhance its learning capabilities.","The code-switching technique is used to implicitly teach the model lexical alignment information, while a syntactic-based graph attention network is designed to help the model encode syntactic structure.","To integrate both types of knowledge, we input code-switched sequences into both the syntactic module and the mBERT base model simultaneously.","Our extensive experimental results demonstrate this framework can consistently outperform all baselines of zero-shot cross-lingual transfer, with the gains of 1.0~3.7 points on text classification, named entity recognition (ner), and semantic parsing tasks.","Keywords:cross-lingual transfer, lexicon, syntax, code-switching, graph attention network"],"url":"http://arxiv.org/abs/2404.16627v1","category":"cs.CL"}
{"created":"2024-04-25 13:56:05","title":"Robust Capped lp-Norm Support Vector Ordinal Regression","abstract":"Ordinal regression is a specialized supervised problem where the labels show an inherent order. The order distinguishes it from normal multi-class problem. Support Vector Ordinal Regression, as an outstanding ordinal regression model, is widely used in many ordinal regression tasks. However, like most supervised learning algorithms, the design of SVOR is based on the assumption that the training data are real and reliable, which is difficult to satisfy in real-world data. In many practical applications, outliers are frequently present in the training set, potentially leading to misguide the learning process, such that the performance is non-optimal. In this paper, we propose a novel capped $\\ell_{p}$-norm loss function that is theoretically robust to both light and heavy outliers. The capped $\\ell_{p}$-norm loss can help the model detect and eliminate outliers during training process. Adhering to this concept, we introduce a new model, Capped $\\ell_{p}$-Norm Support Vector Ordinal Regression(CSVOR), that is robust to outliers. CSVOR uses a weight matrix to detect and eliminate outliers during the training process to improve the robustness to outliers. Moreover, a Re-Weighted algorithm algorithm which is illustrated convergence by our theoretical results is proposed to effectively minimize the corresponding problem. Extensive experimental results demonstrate that our model outperforms state-of-the-art(SOTA) methods, particularly in the presence of outliers.","sentences":["Ordinal regression is a specialized supervised problem where the labels show an inherent order.","The order distinguishes it from normal multi-class problem.","Support Vector Ordinal Regression, as an outstanding ordinal regression model, is widely used in many ordinal regression tasks.","However, like most supervised learning algorithms, the design of SVOR is based on the assumption that the training data are real and reliable, which is difficult to satisfy in real-world data.","In many practical applications, outliers are frequently present in the training set, potentially leading to misguide the learning process, such that the performance is non-optimal.","In this paper, we propose a novel capped $\\ell_{p}$-norm loss function that is theoretically robust to both light and heavy outliers.","The capped $\\ell_{p}$-norm loss can help the model detect and eliminate outliers during training process.","Adhering to this concept, we introduce a new model, Capped $\\ell_{p}$-Norm Support Vector Ordinal Regression(CSVOR), that is robust to outliers.","CSVOR uses a weight matrix to detect and eliminate outliers during the training process to improve the robustness to outliers.","Moreover, a Re-Weighted algorithm algorithm which is illustrated convergence by our theoretical results is proposed to effectively minimize the corresponding problem.","Extensive experimental results demonstrate that our model outperforms state-of-the-art(SOTA) methods, particularly in the presence of outliers."],"url":"http://arxiv.org/abs/2404.16616v1","category":"cs.LG"}
{"created":"2024-04-25 12:36:19","title":"Exploring Internal Numeracy in Language Models: A Case Study on ALBERT","abstract":"It has been found that Transformer-based language models have the ability to perform basic quantitative reasoning. In this paper, we propose a method for studying how these models internally represent numerical data, and use our proposal to analyze the ALBERT family of language models. Specifically, we extract the learned embeddings these models use to represent tokens that correspond to numbers and ordinals, and subject these embeddings to Principal Component Analysis (PCA). PCA results reveal that ALBERT models of different sizes, trained and initialized separately, consistently learn to use the axes of greatest variation to represent the approximate ordering of various numerical concepts. Numerals and their textual counterparts are represented in separate clusters, but increase along the same direction in 2D space. Our findings illustrate that language models, trained purely to model text, can intuit basic mathematical concepts, opening avenues for NLP applications that intersect with quantitative reasoning.","sentences":["It has been found that Transformer-based language models have the ability to perform basic quantitative reasoning.","In this paper, we propose a method for studying how these models internally represent numerical data, and use our proposal to analyze the ALBERT family of language models.","Specifically, we extract the learned embeddings these models use to represent tokens that correspond to numbers and ordinals, and subject these embeddings to Principal Component Analysis (PCA).","PCA results reveal that ALBERT models of different sizes, trained and initialized separately, consistently learn to use the axes of greatest variation to represent the approximate ordering of various numerical concepts.","Numerals and their textual counterparts are represented in separate clusters, but increase along the same direction in 2D space.","Our findings illustrate that language models, trained purely to model text, can intuit basic mathematical concepts, opening avenues for NLP applications that intersect with quantitative reasoning."],"url":"http://arxiv.org/abs/2404.16574v1","category":"cs.CL"}
{"created":"2024-04-25 12:35:27","title":"Multi-Scale Representations by Varying Window Attention for Semantic Segmentation","abstract":"Multi-scale learning is central to semantic segmentation. We visualize the effective receptive field (ERF) of canonical multi-scale representations and point out two risks in learning them: scale inadequacy and field inactivation. A novel multi-scale learner, varying window attention (VWA), is presented to address these issues. VWA leverages the local window attention (LWA) and disentangles LWA into the query window and context window, allowing the context's scale to vary for the query to learn representations at multiple scales. However, varying the context to large-scale windows (enlarging ratio R) can significantly increase the memory footprint and computation cost (R^2 times larger than LWA). We propose a simple but professional re-scaling strategy to zero the extra induced cost without compromising performance. Consequently, VWA uses the same cost as LWA to overcome the receptive limitation of the local window. Furthermore, depending on VWA and employing various MLPs, we introduce a multi-scale decoder (MSD), VWFormer, to improve multi-scale representations for semantic segmentation. VWFormer achieves efficiency competitive with the most compute-friendly MSDs, like FPN and MLP decoder, but performs much better than any MSDs. For instance, using nearly half of UPerNet's computation, VWFormer outperforms it by 1.0%-2.5% mIoU on ADE20K. With little extra overhead, ~10G FLOPs, Mask2Former armed with VWFormer improves by 1.0%-1.3%.","sentences":["Multi-scale learning is central to semantic segmentation.","We visualize the effective receptive field (ERF) of canonical multi-scale representations and point out two risks in learning them: scale inadequacy and field inactivation.","A novel multi-scale learner, varying window attention (VWA), is presented to address these issues.","VWA leverages the local window attention (LWA) and disentangles LWA into the query window and context window, allowing the context's scale to vary for the query to learn representations at multiple scales.","However, varying the context to large-scale windows (enlarging ratio R) can significantly increase the memory footprint and computation cost (R^2 times larger than LWA).","We propose a simple but professional re-scaling strategy to zero the extra induced cost without compromising performance.","Consequently, VWA uses the same cost as LWA to overcome the receptive limitation of the local window.","Furthermore, depending on VWA and employing various MLPs, we introduce a multi-scale decoder (MSD), VWFormer, to improve multi-scale representations for semantic segmentation.","VWFormer achieves efficiency competitive with the most compute-friendly MSDs, like FPN and MLP decoder, but performs much better than any MSDs.","For instance, using nearly half of UPerNet's computation, VWFormer outperforms it by 1.0%-2.5% mIoU on ADE20K. With little extra overhead, ~10G FLOPs, Mask2Former armed with VWFormer improves by 1.0%-1.3%."],"url":"http://arxiv.org/abs/2404.16573v1","category":"cs.CV"}
{"created":"2024-04-25 12:27:59","title":"PyRadar: Towards Automatically Retrieving and Validating Source Code Repository Information for PyPI Packages","abstract":"A package's source code repository records the development history of the package, providing indispensable information for the use and risk monitoring of the package. However, a package release often misses its source code repository due to the separation of the package's development platform from its distribution platform. Existing tools retrieve the release's repository information from its metadata, which suffers from two limitations: the metadata may not contain or contain wrong information. Our analysis shows that existing tools can only retrieve repository information for up to 70.5% of PyPI releases. To address the limitations, this paper proposes PyRadar, a novel framework that utilizes the metadata and source distribution to retrieve and validate the repository information for PyPI releases. We start with an empirical study to compare four existing tools on 4,227,425 PyPI releases and analyze phantom files (files appearing in the release's distribution but not in the release's repository) in 14,375 correct package-repository links and 2,064 incorrect links. Based on the findings, we design PyRadar with three components, i.e., Metadata-based Retriever, Source Code Repository Validator, and Source Code-based Retriever. In particular, the Metadata-based Retriever combines best practices of existing tools and successfully retrieves repository information from the metadata for 72.1% of PyPI releases. The Source Code Repository Validator applies common machine learning algorithms on six crafted features and achieves an AUC of up to 0.995. The Source Code-based Retriever queries World of Code with the SHA-1 hashes of all Python files in the release's source distribution and retrieves repository information for 90.2% of packages in our dataset with an accuracy of 0.970. Both practitioners and researchers can employ the PyRadar to better use PyPI packages.","sentences":["A package's source code repository records the development history of the package, providing indispensable information for the use and risk monitoring of the package.","However, a package release often misses its source code repository due to the separation of the package's development platform from its distribution platform.","Existing tools retrieve the release's repository information from its metadata, which suffers from two limitations: the metadata may not contain or contain wrong information.","Our analysis shows that existing tools can only retrieve repository information for up to 70.5% of PyPI releases.","To address the limitations, this paper proposes PyRadar, a novel framework that utilizes the metadata and source distribution to retrieve and validate the repository information for PyPI releases.","We start with an empirical study to compare four existing tools on 4,227,425 PyPI releases and analyze phantom files (files appearing in the release's distribution but not in the release's repository) in 14,375 correct package-repository links and 2,064 incorrect links.","Based on the findings, we design PyRadar with three components, i.e., Metadata-based Retriever, Source Code Repository Validator, and Source Code-based Retriever.","In particular, the Metadata-based Retriever combines best practices of existing tools and successfully retrieves repository information from the metadata for 72.1% of PyPI releases.","The Source Code Repository Validator applies common machine learning algorithms on six crafted features and achieves an AUC of up to 0.995.","The Source Code-based Retriever queries World of Code with the SHA-1 hashes of all Python files in the release's source distribution and retrieves repository information for 90.2% of packages in our dataset with an accuracy of 0.970.","Both practitioners and researchers can employ the PyRadar to better use PyPI packages."],"url":"http://arxiv.org/abs/2404.16565v1","category":"cs.SE"}
{"created":"2024-04-25 11:36:10","title":"3D deep learning for enhanced atom probe tomography analysis of nanoscale microstructures","abstract":"Quantitative analysis of microstructural features on the nanoscale, including precipitates, local chemical orderings (LCOs) or structural defects (e.g. stacking faults) plays a pivotal role in understanding the mechanical and physical responses of engineering materials. Atom probe tomography (APT), known for its exceptional combination of chemical sensitivity and sub-nanometer resolution, primarily identifies microstructures through compositional segregations. However, this fails when there is no significant segregation, as can be the case for LCOs and stacking faults. Here, we introduce a 3D deep learning approach, AtomNet, designed to process APT point cloud data at the single-atom level for nanoscale microstructure extraction, simultaneously considering compositional and structural information. AtomNet is showcased in segmenting L12-type nanoprecipitates from the matrix in an AlLiMg alloy, irrespective of crystallographic orientations, which outperforms previous methods. AtomNet also allows for 3D imaging of L10-type LCOs in an AuCu alloy, a challenging task for conventional analysis due to their small size and subtle compositional differences. Finally, we demonstrate the use of AtomNet for revealing 2D stacking faults in a Co-based superalloy, without any defected training data, expanding the capabilities of APT for automated exploration of hidden microstructures. AtomNet pushes the boundaries of APT analysis, and holds promise in establishing precise quantitative microstructure-property relationships across a diverse range of metallic materials.","sentences":["Quantitative analysis of microstructural features on the nanoscale, including precipitates, local chemical orderings (LCOs) or structural defects (e.g. stacking faults) plays a pivotal role in understanding the mechanical and physical responses of engineering materials.","Atom probe tomography (APT), known for its exceptional combination of chemical sensitivity and sub-nanometer resolution, primarily identifies microstructures through compositional segregations.","However, this fails when there is no significant segregation, as can be the case for LCOs and stacking faults.","Here, we introduce a 3D deep learning approach, AtomNet, designed to process APT point cloud data at the single-atom level for nanoscale microstructure extraction, simultaneously considering compositional and structural information.","AtomNet is showcased in segmenting L12-type nanoprecipitates from the matrix in an AlLiMg alloy, irrespective of crystallographic orientations, which outperforms previous methods.","AtomNet also allows for 3D imaging of L10-type LCOs in an AuCu alloy, a challenging task for conventional analysis due to their small size and subtle compositional differences.","Finally, we demonstrate the use of AtomNet for revealing 2D stacking faults in a Co-based superalloy, without any defected training data, expanding the capabilities of APT for automated exploration of hidden microstructures.","AtomNet pushes the boundaries of APT analysis, and holds promise in establishing precise quantitative microstructure-property relationships across a diverse range of metallic materials."],"url":"http://arxiv.org/abs/2404.16524v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-25 10:57:50","title":"Efficient algorithms for regularized Poisson Non-negative Matrix Factorization","abstract":"We consider the problem of regularized Poisson Non-negative Matrix Factorization (NMF) problem, encompassing various regularization terms such as Lipschitz and relatively smooth functions, alongside linear constraints. This problem holds significant relevance in numerous Machine Learning applications, particularly within the domain of physical linear unmixing problems. A notable challenge arises from the main loss term in the Poisson NMF problem being a KL divergence, which is non-Lipschitz, rendering traditional gradient descent-based approaches inefficient. In this contribution, we explore the utilization of Block Successive Upper Minimization (BSUM) to overcome this challenge. We build approriate majorizing function for Lipschitz and relatively smooth functions, and show how to introduce linear constraints into the problem. This results in the development of two novel algorithms for regularized Poisson NMF. We conduct numerical simulations to showcase the effectiveness of our approach.","sentences":["We consider the problem of regularized Poisson Non-negative Matrix Factorization (NMF) problem, encompassing various regularization terms such as Lipschitz and relatively smooth functions, alongside linear constraints.","This problem holds significant relevance in numerous Machine Learning applications, particularly within the domain of physical linear unmixing problems.","A notable challenge arises from the main loss term in the Poisson NMF problem being a KL divergence, which is non-Lipschitz, rendering traditional gradient descent-based approaches inefficient.","In this contribution, we explore the utilization of Block Successive Upper Minimization (BSUM) to overcome this challenge.","We build approriate majorizing function for Lipschitz and relatively smooth functions, and show how to introduce linear constraints into the problem.","This results in the development of two novel algorithms for regularized Poisson NMF.","We conduct numerical simulations to showcase the effectiveness of our approach."],"url":"http://arxiv.org/abs/2404.16505v1","category":"cs.LG"}
{"created":"2024-04-25 09:07:19","title":"Point-JEPA: A Joint Embedding Predictive Architecture for Self-Supervised Learning on Point Cloud","abstract":"Recent advancements in self-supervised learning in the point cloud domain have demonstrated significant potential. However, these methods often suffer from drawbacks, including lengthy pre-training time, the necessity of reconstruction in the input space, or the necessity of additional modalities. In order to address these issues, we introduce Point-JEPA, a joint embedding predictive architecture designed specifically for point cloud data. To this end, we introduce a sequencer that orders point cloud tokens to efficiently compute and utilize tokens proximity based on their indices during target and context selection. The sequencer also allows shared computations of the tokens proximity between context and target selection, further improving the efficiency. Experimentally, our method achieves competitive results with state-of-the-art methods while avoiding the reconstruction in the input space or additional modality.","sentences":["Recent advancements in self-supervised learning in the point cloud domain have demonstrated significant potential.","However, these methods often suffer from drawbacks, including lengthy pre-training time, the necessity of reconstruction in the input space, or the necessity of additional modalities.","In order to address these issues, we introduce Point-JEPA, a joint embedding predictive architecture designed specifically for point cloud data.","To this end, we introduce a sequencer that orders point cloud tokens to efficiently compute and utilize tokens proximity based on their indices during target and context selection.","The sequencer also allows shared computations of the tokens proximity between context and target selection, further improving the efficiency.","Experimentally, our method achieves competitive results with state-of-the-art methods while avoiding the reconstruction in the input space or additional modality."],"url":"http://arxiv.org/abs/2404.16432v1","category":"cs.CV"}
{"created":"2024-04-25 07:10:29","title":"Learning Syntax Without Planting Trees: Understanding When and Why Transformers Generalize Hierarchically","abstract":"Transformers trained on natural language data have been shown to learn its hierarchical structure and generalize to sentences with unseen syntactic structures without explicitly encoding any structural bias. In this work, we investigate sources of inductive bias in transformer models and their training that could cause such generalization behavior to emerge. We extensively experiment with transformer models trained on multiple synthetic datasets and with different training objectives and show that while other objectives e.g. sequence-to-sequence modeling, prefix language modeling, often failed to lead to hierarchical generalization, models trained with the language modeling objective consistently learned to generalize hierarchically. We then conduct pruning experiments to study how transformers trained with the language modeling objective encode hierarchical structure. When pruned, we find joint existence of subnetworks within the model with different generalization behaviors (subnetworks corresponding to hierarchical structure and linear order). Finally, we take a Bayesian perspective to further uncover transformers' preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and whether the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization.","sentences":["Transformers trained on natural language data have been shown to learn its hierarchical structure and generalize to sentences with unseen syntactic structures without explicitly encoding any structural bias.","In this work, we investigate sources of inductive bias in transformer models and their training that could cause such generalization behavior to emerge.","We extensively experiment with transformer models trained on multiple synthetic datasets and with different training objectives and show that while other objectives e.g. sequence-to-sequence modeling, prefix language modeling, often failed to lead to hierarchical generalization, models trained with the language modeling objective consistently learned to generalize hierarchically.","We then conduct pruning experiments to study how transformers trained with the language modeling objective encode hierarchical structure.","When pruned, we find joint existence of subnetworks within the model with different generalization behaviors (subnetworks corresponding to hierarchical structure and linear order).","Finally, we take a Bayesian perspective to further uncover transformers' preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and whether the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization."],"url":"http://arxiv.org/abs/2404.16367v1","category":"cs.CL"}
{"created":"2024-04-25 05:59:42","title":"Dual Expert Distillation Network for Generalized Zero-Shot Learning","abstract":"Zero-shot learning has consistently yielded remarkable progress via modeling nuanced one-to-one visual-attribute correlation. Existing studies resort to refining a uniform mapping function to align and correlate the sample regions and subattributes, ignoring two crucial issues: 1) the inherent asymmetry of attributes; and 2) the unutilized channel information. This paper addresses these issues by introducing a simple yet effective approach, dubbed Dual Expert Distillation Network (DEDN), where two experts are dedicated to coarse- and fine-grained visual-attribute modeling, respectively. Concretely, one coarse expert, namely cExp, has a complete perceptual scope to coordinate visual-attribute similarity metrics across dimensions, and moreover, another fine expert, namely fExp, consists of multiple specialized subnetworks, each corresponds to an exclusive set of attributes. Two experts cooperatively distill from each other to reach a mutual agreement during training. Meanwhile, we further equip DEDN with a newly designed backbone network, i.e., Dual Attention Network (DAN), which incorporates both region and channel attention information to fully exploit and leverage visual semantic knowledge. Experiments on various benchmark datasets indicate a new state-of-the-art.","sentences":["Zero-shot learning has consistently yielded remarkable progress via modeling nuanced one-to-one visual-attribute correlation.","Existing studies resort to refining a uniform mapping function to align and correlate the sample regions and subattributes, ignoring two crucial issues: 1) the inherent asymmetry of attributes; and 2) the unutilized channel information.","This paper addresses these issues by introducing a simple yet effective approach, dubbed Dual Expert Distillation Network (DEDN), where two experts are dedicated to coarse- and fine-grained visual-attribute modeling, respectively.","Concretely, one coarse expert, namely cExp, has a complete perceptual scope to coordinate visual-attribute similarity metrics across dimensions, and moreover, another fine expert, namely fExp, consists of multiple specialized subnetworks, each corresponds to an exclusive set of attributes.","Two experts cooperatively distill from each other to reach a mutual agreement during training.","Meanwhile, we further equip DEDN with a newly designed backbone network, i.e., Dual Attention Network (DAN), which incorporates both region and channel attention information to fully exploit and leverage visual semantic knowledge.","Experiments on various benchmark datasets indicate a new state-of-the-art."],"url":"http://arxiv.org/abs/2404.16348v1","category":"cs.CV"}
{"created":"2024-04-25 04:53:43","title":"FedStyle: Style-Based Federated Learning Crowdsourcing Framework for Art Commissions","abstract":"The unique artistic style is crucial to artists' occupational competitiveness, yet prevailing Art Commission Platforms rarely support style-based retrieval. Meanwhile, the fast-growing generative AI techniques aggravate artists' concerns about releasing personal artworks to public platforms. To achieve artistic style-based retrieval without exposing personal artworks, we propose FedStyle, a style-based federated learning crowdsourcing framework. It allows artists to train local style models and share model parameters rather than artworks for collaboration. However, most artists possess a unique artistic style, resulting in severe model drift among them. FedStyle addresses such extreme data heterogeneity by having artists learn their abstract style representations and align with the server, rather than merely aggregating model parameters lacking semantics. Besides, we introduce contrastive learning to meticulously construct the style representation space, pulling artworks with similar styles closer and keeping different ones apart in the embedding space. Extensive experiments on the proposed datasets demonstrate the superiority of FedStyle.","sentences":["The unique artistic style is crucial to artists' occupational competitiveness, yet prevailing Art Commission Platforms rarely support style-based retrieval.","Meanwhile, the fast-growing generative AI techniques aggravate artists' concerns about releasing personal artworks to public platforms.","To achieve artistic style-based retrieval without exposing personal artworks, we propose FedStyle, a style-based federated learning crowdsourcing framework.","It allows artists to train local style models and share model parameters rather than artworks for collaboration.","However, most artists possess a unique artistic style, resulting in severe model drift among them.","FedStyle addresses such extreme data heterogeneity by having artists learn their abstract style representations and align with the server, rather than merely aggregating model parameters lacking semantics.","Besides, we introduce contrastive learning to meticulously construct the style representation space, pulling artworks with similar styles closer and keeping different ones apart in the embedding space.","Extensive experiments on the proposed datasets demonstrate the superiority of FedStyle."],"url":"http://arxiv.org/abs/2404.16336v1","category":"cs.LG"}
{"created":"2024-04-25 03:46:53","title":"FLAASH: Flexible Accelerator Architecture for Sparse High-Order Tensor Contraction","abstract":"Tensors play a vital role in machine learning (ML) and often exhibit properties best explored while maintaining high-order. Efficiently performing ML computations requires taking advantage of sparsity, but generalized hardware support is challenging. This paper introduces FLAASH, a flexible and modular accelerator design for sparse tensor contraction that achieves over 25x speedup for a deep learning workload. Our architecture performs sparse high-order tensor contraction by distributing sparse dot products, or portions thereof, to numerous Sparse Dot Product Engines (SDPEs). Memory structure and job distribution can be customized, and we demonstrate a simple approach as a proof of concept. We address the challenges associated with control flow to navigate data structures, high-order representation, and high-sparsity handling. The effectiveness of our approach is demonstrated through various evaluations, showcasing significant speedup as sparsity and order increase.","sentences":["Tensors play a vital role in machine learning (ML) and often exhibit properties best explored while maintaining high-order.","Efficiently performing ML computations requires taking advantage of sparsity, but generalized hardware support is challenging.","This paper introduces FLAASH, a flexible and modular accelerator design for sparse tensor contraction that achieves over 25x speedup for a deep learning workload.","Our architecture performs sparse high-order tensor contraction by distributing sparse dot products, or portions thereof, to numerous Sparse Dot Product Engines (SDPEs).","Memory structure and job distribution can be customized, and we demonstrate a simple approach as a proof of concept.","We address the challenges associated with control flow to navigate data structures, high-order representation, and high-sparsity handling.","The effectiveness of our approach is demonstrated through various evaluations, showcasing significant speedup as sparsity and order increase."],"url":"http://arxiv.org/abs/2404.16317v1","category":"cs.AR"}
{"created":"2024-04-25 02:23:11","title":"One Noise to Rule Them All: Learning a Unified Model of Spatially-Varying Noise Patterns","abstract":"Procedural noise is a fundamental component of computer graphics pipelines, offering a flexible way to generate textures that exhibit \"natural\" random variation. Many different types of noise exist, each produced by a separate algorithm. In this paper, we present a single generative model which can learn to generate multiple types of noise as well as blend between them. In addition, it is capable of producing spatially-varying noise blends despite not having access to such data for training. These features are enabled by training a denoising diffusion model using a novel combination of data augmentation and network conditioning techniques. Like procedural noise generators, the model's behavior is controllable via interpretable parameters and a source of randomness. We use our model to produce a variety of visually compelling noise textures. We also present an application of our model to improving inverse procedural material design; using our model in place of fixed-type noise nodes in a procedural material graph results in higher-fidelity material reconstructions without needing to know the type of noise in advance.","sentences":["Procedural noise is a fundamental component of computer graphics pipelines, offering a flexible way to generate textures that exhibit \"natural\" random variation.","Many different types of noise exist, each produced by a separate algorithm.","In this paper, we present a single generative model which can learn to generate multiple types of noise as well as blend between them.","In addition, it is capable of producing spatially-varying noise blends despite not having access to such data for training.","These features are enabled by training a denoising diffusion model using a novel combination of data augmentation and network conditioning techniques.","Like procedural noise generators, the model's behavior is controllable via interpretable parameters and a source of randomness.","We use our model to produce a variety of visually compelling noise textures.","We also present an application of our model to improving inverse procedural material design; using our model in place of fixed-type noise nodes in a procedural material graph results in higher-fidelity material reconstructions without needing to know the type of noise in advance."],"url":"http://arxiv.org/abs/2404.16292v1","category":"cs.GR"}
{"created":"2024-04-25 02:21:27","title":"Deep Joint CSI Feedback and Multiuser Precoding for MIMO OFDM Systems","abstract":"The design of precoding plays a crucial role in achieving a high downlink sum-rate in multiuser multiple-input multiple-output (MIMO) orthogonal frequency-division multiplexing (OFDM) systems. In this correspondence, we propose a deep learning based joint CSI feedback and multiuser precoding method in frequency division duplex systems, aiming at maximizing the downlink sum-rate performance in an end-to-end manner. Specifically, the eigenvectors of the CSI matrix are compressed using deep joint source-channel coding techniques. This compression method enhances the resilience of the feedback CSI information against degradation in the feedback channel. A joint multiuser precoding module and a power allocation module are designed to adjust the precoding direction and the precoding power for users based on the feedback CSI information. Experimental results demonstrate that the downlink sum-rate can be significantly improved by using the proposed method, especially in scenarios with low signal-to-noise ratio and low feedback overhead.","sentences":["The design of precoding plays a crucial role in achieving a high downlink sum-rate in multiuser multiple-input multiple-output (MIMO) orthogonal frequency-division multiplexing (OFDM) systems.","In this correspondence, we propose a deep learning based joint CSI feedback and multiuser precoding method in frequency division duplex systems, aiming at maximizing the downlink sum-rate performance in an end-to-end manner.","Specifically, the eigenvectors of the CSI matrix are compressed using deep joint source-channel coding techniques.","This compression method enhances the resilience of the feedback CSI information against degradation in the feedback channel.","A joint multiuser precoding module and a power allocation module are designed to adjust the precoding direction and the precoding power for users based on the feedback CSI information.","Experimental results demonstrate that the downlink sum-rate can be significantly improved by using the proposed method, especially in scenarios with low signal-to-noise ratio and low feedback overhead."],"url":"http://arxiv.org/abs/2404.16289v1","category":"cs.IT"}
{"created":"2024-04-25 01:33:55","title":"Causally Inspired Regularization Enables Domain General Representations","abstract":"Given a causal graph representing the data-generating process shared across different domains/distributions, enforcing sufficient graph-implied conditional independencies can identify domain-general (non-spurious) feature representations. For the standard input-output predictive setting, we categorize the set of graphs considered in the literature into two distinct groups: (i) those in which the empirical risk minimizer across training domains gives domain-general representations and (ii) those where it does not. For the latter case (ii), we propose a novel framework with regularizations, which we demonstrate are sufficient for identifying domain-general feature representations without a priori knowledge (or proxies) of the spurious features. Empirically, our proposed method is effective for both (semi) synthetic and real-world data, outperforming other state-of-the-art methods in average and worst-domain transfer accuracy.","sentences":["Given a causal graph representing the data-generating process shared across different domains/distributions, enforcing sufficient graph-implied conditional independencies can identify domain-general (non-spurious) feature representations.","For the standard input-output predictive setting, we categorize the set of graphs considered in the literature into two distinct groups: (i) those in which the empirical risk minimizer across training domains gives domain-general representations and (ii) those where it does not.","For the latter case (ii), we propose a novel framework with regularizations, which we demonstrate are sufficient for identifying domain-general feature representations without a priori knowledge (or proxies) of the spurious features.","Empirically, our proposed method is effective for both (semi) synthetic and real-world data, outperforming other state-of-the-art methods in average and worst-domain transfer accuracy."],"url":"http://arxiv.org/abs/2404.16277v1","category":"cs.LG"}
{"created":"2024-04-24 21:07:59","title":"Foundations of automatic feature extraction at LHC--point clouds and graphs","abstract":"Deep learning algorithms will play a key role in the upcoming runs of the Large Hadron Collider (LHC), helping bolster various fronts ranging from fast and accurate detector simulations to physics analysis probing possible deviations from the Standard Model. The game-changing feature of these new algorithms is the ability to extract relevant information from high-dimensional input spaces, often regarded as \"replacing the expert\" in designing physics-intuitive variables. While this may seem true at first glance, it is far from reality. Existing research shows that physics-inspired feature extractors have many advantages beyond improving the qualitative understanding of the extracted features. In this review, we systematically explore automatic feature extraction from a phenomenological viewpoint and the motivation for physics-inspired architectures. We also discuss how prior knowledge from physics results in the naturalness of the point cloud representation and discuss graph-based applications to LHC phenomenology.","sentences":["Deep learning algorithms will play a key role in the upcoming runs of the Large Hadron Collider (LHC), helping bolster various fronts ranging from fast and accurate detector simulations to physics analysis probing possible deviations from the Standard Model.","The game-changing feature of these new algorithms is the ability to extract relevant information from high-dimensional input spaces, often regarded as \"replacing the expert\" in designing physics-intuitive variables.","While this may seem true at first glance, it is far from reality.","Existing research shows that physics-inspired feature extractors have many advantages beyond improving the qualitative understanding of the extracted features.","In this review, we systematically explore automatic feature extraction from a phenomenological viewpoint and the motivation for physics-inspired architectures.","We also discuss how prior knowledge from physics results in the naturalness of the point cloud representation and discuss graph-based applications to LHC phenomenology."],"url":"http://arxiv.org/abs/2404.16207v1","category":"hep-ph"}
{"created":"2024-04-24 20:09:21","title":"S2DEVFMAP: Self-Supervised Learning Framework with Dual Ensemble Voting Fusion for Maximizing Anomaly Prediction in Timeseries","abstract":"Anomaly detection plays a crucial role in industrial settings, particularly in maintaining the reliability and optimal performance of cooling systems. Traditional anomaly detection methods often face challenges in handling diverse data characteristics and variations in noise levels, resulting in limited effectiveness. And yet traditional anomaly detection often relies on application of single models. This work proposes a novel, robust approach using five heterogeneous independent models combined with a dual ensemble fusion of voting techniques. Diverse models capture various system behaviors, while the fusion strategy maximizes detection effectiveness and minimizes false alarms. Each base autoencoder model learns a unique representation of the data, leveraging their complementary strengths to improve anomaly detection performance. To increase the effectiveness and reliability of final anomaly prediction, dual ensemble technique is applied. This approach outperforms in maximizing the coverage of identifying anomalies. Experimental results on a real-world dataset of industrial cooling system data demonstrate the effectiveness of the proposed approach. This approach can be extended to other industrial applications where anomaly detection is critical for ensuring system reliability and preventing potential malfunctions.","sentences":["Anomaly detection plays a crucial role in industrial settings, particularly in maintaining the reliability and optimal performance of cooling systems.","Traditional anomaly detection methods often face challenges in handling diverse data characteristics and variations in noise levels, resulting in limited effectiveness.","And yet traditional anomaly detection often relies on application of single models.","This work proposes a novel, robust approach using five heterogeneous independent models combined with a dual ensemble fusion of voting techniques.","Diverse models capture various system behaviors, while the fusion strategy maximizes detection effectiveness and minimizes false alarms.","Each base autoencoder model learns a unique representation of the data, leveraging their complementary strengths to improve anomaly detection performance.","To increase the effectiveness and reliability of final anomaly prediction, dual ensemble technique is applied.","This approach outperforms in maximizing the coverage of identifying anomalies.","Experimental results on a real-world dataset of industrial cooling system data demonstrate the effectiveness of the proposed approach.","This approach can be extended to other industrial applications where anomaly detection is critical for ensuring system reliability and preventing potential malfunctions."],"url":"http://arxiv.org/abs/2404.16179v1","category":"cs.LG"}
{"created":"2024-04-24 19:57:09","title":"Interpretable Machine Learning Models for Predicting the Next Targets of Activist Funds","abstract":"This work develops a predictive model to identify potential targets of activist investment funds, which strategically acquire significant corporate stakes to drive operational and strategic improvements and enhance shareholder value. Predicting these targets is crucial for companies to mitigate intervention risks, for activists to select optimal targets, and for investors to capitalize on associated stock price gains. Our analysis utilizes data from the Russell 3000 index from 2016 to 2022. We tested 123 variations of models using different data imputation, oversampling, and machine learning methods, achieving a top AUC-ROC of 0.782. This demonstrates the model's effectiveness in identifying likely targets of activist funds. We applied the Shapley value method to determine the most influential factors in a company's susceptibility to activist investment. This interpretative approach provides clear insights into the driving forces behind activist targeting. Our model offers stakeholders a strategic tool for proactive corporate governance and investment strategy, enhancing understanding of the dynamics of activist investing.","sentences":["This work develops a predictive model to identify potential targets of activist investment funds, which strategically acquire significant corporate stakes to drive operational and strategic improvements and enhance shareholder value.","Predicting these targets is crucial for companies to mitigate intervention risks, for activists to select optimal targets, and for investors to capitalize on associated stock price gains.","Our analysis utilizes data from the Russell 3000 index from 2016 to 2022.","We tested 123 variations of models using different data imputation, oversampling, and machine learning methods, achieving a top AUC-ROC of 0.782.","This demonstrates the model's effectiveness in identifying likely targets of activist funds.","We applied the Shapley value method to determine the most influential factors in a company's susceptibility to activist investment.","This interpretative approach provides clear insights into the driving forces behind activist targeting.","Our model offers stakeholders a strategic tool for proactive corporate governance and investment strategy, enhancing understanding of the dynamics of activist investing."],"url":"http://arxiv.org/abs/2404.16169v1","category":"cs.CE"}
{"created":"2024-04-24 19:25:58","title":"The Feasibility of Implementing Large-Scale Transformers on Multi-FPGA Platforms","abstract":"FPGAs are rarely mentioned when discussing the implementation of large machine learning applications, such as Large Language Models (LLMs), in the data center. There has been much evidence showing that single FPGAs can be competitive with GPUs in performance for some computations, especially for low latency, and often much more efficient when power is considered. This suggests that there is merit to exploring the use of multiple FPGAs for large machine learning applications. The challenge with using multiple FPGAs is that there is no commonly-accepted flow for developing and deploying multi-FPGA applications, i.e., there are no tools to describe a large application, map it to multiple FPGAs and then deploy the application on a multi-FPGA platform. In this paper, we explore the feasibility of implementing large transformers using multiple FPGAs by developing a scalable multi-FPGA platform and some tools to map large applications to the platform. We validate our approach by designing an efficient multi-FPGA version of the I-BERT transformer and implement one encoder using six FPGAs as a working proof-of-concept to show that our platform and tools work. Based on our proof-of-concept prototype and the estimations of performance using the latest FPGAs compared to GPUs, we conclude that there can be a place for FPGAs in the world of large machine learning applications. We demonstrate a promising first step that shows that with the right infrastructure and tools it is reasonable to continue to explore the possible benefits of using FPGAs for applications such as LLMs.","sentences":["FPGAs are rarely mentioned when discussing the implementation of large machine learning applications, such as Large Language Models (LLMs), in the data center.","There has been much evidence showing that single FPGAs can be competitive with GPUs in performance for some computations, especially for low latency, and often much more efficient when power is considered.","This suggests that there is merit to exploring the use of multiple FPGAs for large machine learning applications.","The challenge with using multiple FPGAs is that there is no commonly-accepted flow for developing and deploying multi-FPGA applications, i.e., there are no tools to describe a large application, map it to multiple FPGAs and then deploy the application on a multi-FPGA platform.","In this paper, we explore the feasibility of implementing large transformers using multiple FPGAs by developing a scalable multi-FPGA platform and some tools to map large applications to the platform.","We validate our approach by designing an efficient multi-FPGA version of the I-BERT transformer and implement one encoder using six FPGAs as a working proof-of-concept to show that our platform and tools work.","Based on our proof-of-concept prototype and the estimations of performance using the latest FPGAs compared to GPUs, we conclude that there can be a place for FPGAs in the world of large machine learning applications.","We demonstrate a promising first step that shows that with the right infrastructure and tools it is reasonable to continue to explore the possible benefits of using FPGAs for applications such as LLMs."],"url":"http://arxiv.org/abs/2404.16158v1","category":"cs.AR"}
{"created":"2024-04-24 19:24:53","title":"Guardians of the Quantum GAN","abstract":"Quantum Generative Adversarial Networks (qGANs) are at the forefront of image-generating quantum machine learning models. To accommodate the growing demand for Noisy Intermediate-Scale Quantum (NISQ) devices to train and infer quantum machine learning models, the number of third-party vendors offering quantum hardware as a service is expected to rise. This expansion introduces the risk of untrusted vendors potentially stealing proprietary information from the quantum machine learning models. To address this concern we propose a novel watermarking technique that exploits the noise signature embedded during the training phase of qGANs as a non-invasive watermark. The watermark is identifiable in the images generated by the qGAN allowing us to trace the specific quantum hardware used during training hence providing strong proof of ownership. To further enhance the security robustness, we propose the training of qGANs on a sequence of multiple quantum hardware, embedding a complex watermark comprising the noise signatures of all the training hardware that is difficult for adversaries to replicate. We also develop a machine learning classifier to extract this watermark robustly, thereby identifying the training hardware (or the suite of hardware) from the images generated by the qGAN validating the authenticity of the model. We note that the watermark signature is robust against inferencing on hardware different than the hardware that was used for training. We obtain watermark extraction accuracy of 100% and ~90% for training the qGAN on individual and multiple quantum hardware setups (and inferencing on different hardware), respectively. Since parameter evolution during training is strongly modulated by quantum noise, the proposed watermark can be extended to other quantum machine learning models as well.","sentences":["Quantum Generative Adversarial Networks (qGANs) are at the forefront of image-generating quantum machine learning models.","To accommodate the growing demand for Noisy Intermediate-Scale Quantum (NISQ) devices to train and infer quantum machine learning models, the number of third-party vendors offering quantum hardware as a service is expected to rise.","This expansion introduces the risk of untrusted vendors potentially stealing proprietary information from the quantum machine learning models.","To address this concern we propose a novel watermarking technique that exploits the noise signature embedded during the training phase of qGANs as a non-invasive watermark.","The watermark is identifiable in the images generated by the qGAN allowing us to trace the specific quantum hardware used during training hence providing strong proof of ownership.","To further enhance the security robustness, we propose the training of qGANs on a sequence of multiple quantum hardware, embedding a complex watermark comprising the noise signatures of all the training hardware that is difficult for adversaries to replicate.","We also develop a machine learning classifier to extract this watermark robustly, thereby identifying the training hardware (or the suite of hardware) from the images generated by the qGAN validating the authenticity of the model.","We note that the watermark signature is robust against inferencing on hardware different than the hardware that was used for training.","We obtain watermark extraction accuracy of 100% and ~90% for training the qGAN on individual and multiple quantum hardware setups (and inferencing on different hardware), respectively.","Since parameter evolution during training is strongly modulated by quantum noise, the proposed watermark can be extended to other quantum machine learning models as well."],"url":"http://arxiv.org/abs/2404.16156v1","category":"quant-ph"}
{"created":"2024-04-24 19:22:45","title":"Does SAM dream of EIG? Characterizing Interactive Segmenter Performance using Expected Information Gain","abstract":"We introduce an assessment procedure for interactive segmentation models. Based on concepts from Bayesian Experimental Design, the procedure measures a model's understanding of point prompts and their correspondence with the desired segmentation mask. We show that Oracle Dice index measurements are insensitive or even misleading in measuring this property. We demonstrate the use of the proposed procedure on three interactive segmentation models and subsets of two large image segmentation datasets.","sentences":["We introduce an assessment procedure for interactive segmentation models.","Based on concepts from Bayesian Experimental Design, the procedure measures a model's understanding of point prompts and their correspondence with the desired segmentation mask.","We show that Oracle Dice index measurements are insensitive or even misleading in measuring this property.","We demonstrate the use of the proposed procedure on three interactive segmentation models and subsets of two large image segmentation datasets."],"url":"http://arxiv.org/abs/2404.16155v1","category":"cs.CV"}
{"created":"2024-04-24 18:52:30","title":"Logic Dynamic Movement Primitives for Long-horizon Manipulation Tasks in Dynamic Environments","abstract":"Learning from Demonstration (LfD) stands as an efficient framework for imparting human-like skills to robots. Nevertheless, designing an LfD framework capable of seamlessly imitating, generalizing, and reacting to disturbances for long-horizon manipulation tasks in dynamic environments remains a challenge. To tackle this challenge, we present Logic Dynamic Movement Primitives (Logic-DMP), which combines Task and Motion Planning (TAMP) with an optimal control formulation of DMP, allowing us to incorporate motion-level via-point specifications and to handle task-level variations or disturbances in dynamic environments. We conduct a comparative analysis of our proposed approach against several baselines, evaluating its generalization ability and reactivity across three long-horizon manipulation tasks. Our experiment demonstrates the fast generalization and reactivity of Logic-DMP for handling task-level variants and disturbances in long-horizon manipulation tasks.","sentences":["Learning from Demonstration (LfD) stands as an efficient framework for imparting human-like skills to robots.","Nevertheless, designing an LfD framework capable of seamlessly imitating, generalizing, and reacting to disturbances for long-horizon manipulation tasks in dynamic environments remains a challenge.","To tackle this challenge, we present Logic Dynamic Movement Primitives (Logic-DMP), which combines Task and Motion Planning (TAMP) with an optimal control formulation of DMP, allowing us to incorporate motion-level via-point specifications and to handle task-level variations or disturbances in dynamic environments.","We conduct a comparative analysis of our proposed approach against several baselines, evaluating its generalization ability and reactivity across three long-horizon manipulation tasks.","Our experiment demonstrates the fast generalization and reactivity of Logic-DMP for handling task-level variants and disturbances in long-horizon manipulation tasks."],"url":"http://arxiv.org/abs/2404.16138v1","category":"cs.RO"}
{"created":"2024-04-24 18:39:18","title":"Combinatorial Approximations for Cluster Deletion: Simpler, Faster, and Better","abstract":"Cluster deletion is an NP-hard graph clustering objective with applications in computational biology and social network analysis, where the goal is to delete a minimum number of edges to partition a graph into cliques. We first provide a tighter analysis of two previous approximation algorithms, improving their approximation guarantees from 4 to 3. Moreover, we show that both algorithms can be derandomized in a surprisingly simple way, by greedily taking a vertex of maximum degree in an auxiliary graph and forming a cluster around it. One of these algorithms relies on solving a linear program. Our final contribution is to design a new and purely combinatorial approach for doing so that is far more scalable in theory and practice.","sentences":["Cluster deletion is an NP-hard graph clustering objective with applications in computational biology and social network analysis, where the goal is to delete a minimum number of edges to partition a graph into cliques.","We first provide a tighter analysis of two previous approximation algorithms, improving their approximation guarantees from 4 to 3.","Moreover, we show that both algorithms can be derandomized in a surprisingly simple way, by greedily taking a vertex of maximum degree in an auxiliary graph and forming a cluster around it.","One of these algorithms relies on solving a linear program.","Our final contribution is to design a new and purely combinatorial approach for doing so that is far more scalable in theory and practice."],"url":"http://arxiv.org/abs/2404.16131v1","category":"cs.DS"}
{"created":"2024-04-24 18:31:48","title":"Comparison of static and dynamic random forests models for EHR data in the presence of competing risks: predicting central line-associated bloodstream infection","abstract":"Prognostic outcomes related to hospital admissions typically do not suffer from censoring, and can be modeled either categorically or as time-to-event. Competing events are common but often ignored. We compared the performance of random forest (RF) models to predict the risk of central line-associated bloodstream infections (CLABSI) using different outcome operationalizations. We included data from 27478 admissions to the University Hospitals Leuven, covering 30862 catheter episodes (970 CLABSI, 1466 deaths and 28426 discharges) to build static and dynamic RF models for binary (CLABSI vs no CLABSI), multinomial (CLABSI, discharge, death or no event), survival (time to CLABSI) and competing risks (time to CLABSI, discharge or death) outcomes to predict the 7-day CLABSI risk. We evaluated model performance across 100 train/test splits. Performance of binary, multinomial and competing risks models was similar: AUROC was 0.74 for baseline predictions, rose to 0.78 for predictions at day 5 in the catheter episode, and decreased thereafter. Survival models overestimated the risk of CLABSI (E:O ratios between 1.2 and 1.6), and had AUROCs about 0.01 lower than other models. Binary and multinomial models had lowest computation times. Models including multiple outcome events (multinomial and competing risks) display a different internal structure compared to binary and survival models. In the absence of censoring, complex modelling choices do not considerably improve the predictive performance compared to a binary model for CLABSI prediction in our studied settings. Survival models censoring the competing events at their time of occurrence should be avoided.","sentences":["Prognostic outcomes related to hospital admissions typically do not suffer from censoring, and can be modeled either categorically or as time-to-event.","Competing events are common but often ignored.","We compared the performance of random forest (RF) models to predict the risk of central line-associated bloodstream infections (CLABSI) using different outcome operationalizations.","We included data from 27478 admissions to the University Hospitals Leuven, covering 30862 catheter episodes (970 CLABSI, 1466 deaths and 28426 discharges) to build static and dynamic RF models for binary (CLABSI vs no CLABSI), multinomial (CLABSI, discharge, death or no event), survival (time to CLABSI) and competing risks (time to CLABSI, discharge or death) outcomes to predict the 7-day CLABSI risk.","We evaluated model performance across 100 train/test splits.","Performance of binary, multinomial and competing risks models was similar: AUROC was 0.74 for baseline predictions, rose to 0.78 for predictions at day 5 in the catheter episode, and decreased thereafter.","Survival models overestimated the risk of CLABSI (E:O ratios between 1.2 and 1.6), and had AUROCs about 0.01 lower than other models.","Binary and multinomial models had lowest computation times.","Models including multiple outcome events (multinomial and competing risks) display a different internal structure compared to binary and survival models.","In the absence of censoring, complex modelling choices do not considerably improve the predictive performance compared to a binary model for CLABSI prediction in our studied settings.","Survival models censoring the competing events at their time of occurrence should be avoided."],"url":"http://arxiv.org/abs/2404.16127v1","category":"cs.LG"}
{"created":"2024-04-24 18:21:08","title":"Securing Hybrid Wireless Body Area Networks (HyWBAN): Advancements in Semantic Communications and Jamming Techniques","abstract":"This paper explores novel strategies to strengthen the security of Hybrid Wireless Body Area Networks (HyWBANs), essential in smart healthcare and Internet of Things (IoT) applications. Recognizing the vulnerability of HyWBAN to sophisticated cyber-attacks, we propose an innovative combination of semantic communications and jamming receivers. This dual-layered security mechanism protects against unauthorized access and data breaches, particularly in scenarios involving in-body to on-body communication channels. We conduct comprehensive laboratory measurements to understand hybrid (radio and optical) communication propagation through biological tissues and utilize these insights to refine a dataset for training a Deep Learning (DL) model. These models, in turn, generate semantic concepts linked to cryptographic keys for enhanced data confidentiality and integrity using a jamming receiver. The proposed model demonstrates a significant reduction in energy consumption compared to traditional cryptographic methods, like Elliptic Curve Diffie-Hellman (ECDH), especially when supplemented with jamming. Our approach addresses the primary security concerns and sets the baseline for future secure biomedical communication systems advancements.","sentences":["This paper explores novel strategies to strengthen the security of Hybrid Wireless Body Area Networks (HyWBANs), essential in smart healthcare and Internet of Things (IoT) applications.","Recognizing the vulnerability of HyWBAN to sophisticated cyber-attacks, we propose an innovative combination of semantic communications and jamming receivers.","This dual-layered security mechanism protects against unauthorized access and data breaches, particularly in scenarios involving in-body to on-body communication channels.","We conduct comprehensive laboratory measurements to understand hybrid (radio and optical) communication propagation through biological tissues and utilize these insights to refine a dataset for training a Deep Learning (DL) model.","These models, in turn, generate semantic concepts linked to cryptographic keys for enhanced data confidentiality and integrity using a jamming receiver.","The proposed model demonstrates a significant reduction in energy consumption compared to traditional cryptographic methods, like Elliptic Curve Diffie-Hellman (ECDH), especially when supplemented with jamming.","Our approach addresses the primary security concerns and sets the baseline for future secure biomedical communication systems advancements."],"url":"http://arxiv.org/abs/2404.16120v1","category":"cs.CR"}
{"created":"2024-04-24 17:59:11","title":"Editable Image Elements for Controllable Synthesis","abstract":"Diffusion models have made significant advances in text-guided synthesis tasks. However, editing user-provided images remains challenging, as the high dimensional noise input space of diffusion models is not naturally suited for image inversion or spatial editing. In this work, we propose an image representation that promotes spatial editing of input images using a diffusion model. Concretely, we learn to encode an input into \"image elements\" that can faithfully reconstruct an input image. These elements can be intuitively edited by a user, and are decoded by a diffusion model into realistic images. We show the effectiveness of our representation on various image editing tasks, such as object resizing, rearrangement, dragging, de-occlusion, removal, variation, and image composition. Project page: https://jitengmu.github.io/Editable_Image_Elements/","sentences":["Diffusion models have made significant advances in text-guided synthesis tasks.","However, editing user-provided images remains challenging, as the high dimensional noise input space of diffusion models is not naturally suited for image inversion or spatial editing.","In this work, we propose an image representation that promotes spatial editing of input images using a diffusion model.","Concretely, we learn to encode an input into \"image elements\" that can faithfully reconstruct an input image.","These elements can be intuitively edited by a user, and are decoded by a diffusion model into realistic images.","We show the effectiveness of our representation on various image editing tasks, such as object resizing, rearrangement, dragging, de-occlusion, removal, variation, and image composition.","Project page: https://jitengmu.github.io/Editable_Image_Elements/"],"url":"http://arxiv.org/abs/2404.16029v1","category":"cs.CV"}
{"created":"2024-04-24 17:57:18","title":"ORBIT-Surgical: An Open-Simulation Framework for Learning Surgical Augmented Dexterity","abstract":"Physics-based simulations have accelerated progress in robot learning for driving, manipulation, and locomotion. Yet, a fast, accurate, and robust surgical simulation environment remains a challenge. In this paper, we present ORBIT-Surgical, a physics-based surgical robot simulation framework with photorealistic rendering in NVIDIA Omniverse. We provide 14 benchmark surgical tasks for the da Vinci Research Kit (dVRK) and Smart Tissue Autonomous Robot (STAR) which represent common subtasks in surgical training. ORBIT-Surgical leverages GPU parallelization to train reinforcement learning and imitation learning algorithms to facilitate study of robot learning to augment human surgical skills. ORBIT-Surgical also facilitates realistic synthetic data generation for active perception tasks. We demonstrate ORBIT-Surgical sim-to-real transfer of learned policies onto a physical dVRK robot. Project website: orbit-surgical.github.io","sentences":["Physics-based simulations have accelerated progress in robot learning for driving, manipulation, and locomotion.","Yet, a fast, accurate, and robust surgical simulation environment remains a challenge.","In this paper, we present ORBIT-Surgical, a physics-based surgical robot simulation framework with photorealistic rendering in NVIDIA Omniverse.","We provide 14 benchmark surgical tasks for the da Vinci Research Kit (dVRK) and Smart Tissue Autonomous Robot (STAR) which represent common subtasks in surgical training.","ORBIT-Surgical leverages GPU parallelization to train reinforcement learning and imitation learning algorithms to facilitate study of robot learning to augment human surgical skills.","ORBIT-Surgical also facilitates realistic synthetic data generation for active perception tasks.","We demonstrate ORBIT-Surgical sim-to-real transfer of learned policies onto a physical dVRK robot.","Project website: orbit-surgical.github.io"],"url":"http://arxiv.org/abs/2404.16027v1","category":"cs.RO"}
{"created":"2024-04-24 17:35:29","title":"Unimodal and Multimodal Sensor Fusion for Wearable Activity Recognition","abstract":"Combining different sensing modalities with multiple positions helps form a unified perception and understanding of complex situations such as human behavior. Hence, human activity recognition (HAR) benefits from combining redundant and complementary information (Unimodal/Multimodal). Even so, it is not an easy task. It requires a multidisciplinary approach, including expertise in sensor technologies, signal processing, data fusion algorithms, and domain-specific knowledge. This Ph.D. work employs sensing modalities such as inertial, pressure (audio and atmospheric pressure), and textile capacitive sensing for HAR. The scenarios explored are gesture and hand position tracking, facial and head pattern recognition, and body posture and gesture recognition. The selected wearable devices and sensing modalities are fully integrated with machine learning-based algorithms, some of which are implemented in the embedded device, on the edge, and tested in real-time.","sentences":["Combining different sensing modalities with multiple positions helps form a unified perception and understanding of complex situations such as human behavior.","Hence, human activity recognition (HAR) benefits from combining redundant and complementary information (Unimodal/Multimodal).","Even so, it is not an easy task.","It requires a multidisciplinary approach, including expertise in sensor technologies, signal processing, data fusion algorithms, and domain-specific knowledge.","This Ph.D. work employs sensing modalities such as inertial, pressure (audio and atmospheric pressure), and textile capacitive sensing for HAR.","The scenarios explored are gesture and hand position tracking, facial and head pattern recognition, and body posture and gesture recognition.","The selected wearable devices and sensing modalities are fully integrated with machine learning-based algorithms, some of which are implemented in the embedded device, on the edge, and tested in real-time."],"url":"http://arxiv.org/abs/2404.16005v1","category":"cs.LG"}
{"created":"2024-04-24 17:27:57","title":"A comprehensive and easy-to-use multi-domain multi-task medical imaging meta-dataset (MedIMeta)","abstract":"While the field of medical image analysis has undergone a transformative shift with the integration of machine learning techniques, the main challenge of these techniques is often the scarcity of large, diverse, and well-annotated datasets. Medical images vary in format, size, and other parameters and therefore require extensive preprocessing and standardization, for usage in machine learning. Addressing these challenges, we introduce the Medical Imaging Meta-Dataset (MedIMeta), a novel multi-domain, multi-task meta-dataset. MedIMeta contains 19 medical imaging datasets spanning 10 different domains and encompassing 54 distinct medical tasks, all of which are standardized to the same format and readily usable in PyTorch or other ML frameworks. We perform a technical validation of MedIMeta, demonstrating its utility through fully supervised and cross-domain few-shot learning baselines.","sentences":["While the field of medical image analysis has undergone a transformative shift with the integration of machine learning techniques, the main challenge of these techniques is often the scarcity of large, diverse, and well-annotated datasets.","Medical images vary in format, size, and other parameters and therefore require extensive preprocessing and standardization, for usage in machine learning.","Addressing these challenges, we introduce the Medical Imaging Meta-Dataset (MedIMeta), a novel multi-domain, multi-task meta-dataset.","MedIMeta contains 19 medical imaging datasets spanning 10 different domains and encompassing 54 distinct medical tasks, all of which are standardized to the same format and readily usable in PyTorch or other ML frameworks.","We perform a technical validation of MedIMeta, demonstrating its utility through fully supervised and cross-domain few-shot learning baselines."],"url":"http://arxiv.org/abs/2404.16000v1","category":"cs.CV"}
{"created":"2024-04-24 17:06:52","title":"HDDGAN: A Heterogeneous Dual-Discriminator Generative Adversarial Network for Infrared and Visible Image Fusion","abstract":"Infrared and visible image fusion (IVIF) aims to preserve thermal radiation information from infrared images while integrating texture details from visible images, enabling the capture of important features and hidden details of subjects in complex scenes and disturbed environments. Consequently, IVIF offers distinct advantages in practical applications such as video surveillance, night navigation, and target recognition. However, prevailing methods often face challenges in simultaneously capturing thermal region features and detailed information due to the disparate characteristics of infrared and visible images. Consequently, fusion outcomes frequently entail a compromise between thermal target area information and texture details. In this study, we introduce a novel heterogeneous dual-discriminator generative adversarial network (HDDGAN) to address this issue. Specifically, the generator is structured as a multi-scale skip-connected structure, facilitating the extraction of essential features from different source images. To enhance the information representation ability of the fusion result, an attention mechanism is employed to construct the information fusion layer within the generator, leveraging the disparities between the source images. Moreover, recognizing the distinct learning requirements of information in infrared and visible images, we design two discriminators with differing structures. This approach aims to guide the model to learn salient information from infrared images while simultaneously capturing detailed information from visible images. Extensive experiments conducted on various public datasets demonstrate the superiority of our proposed HDDGAN over other state-of-the-art (SOTA) algorithms, highlighting its enhanced potential for practical applications.","sentences":["Infrared and visible image fusion (IVIF) aims to preserve thermal radiation information from infrared images while integrating texture details from visible images, enabling the capture of important features and hidden details of subjects in complex scenes and disturbed environments.","Consequently, IVIF offers distinct advantages in practical applications such as video surveillance, night navigation, and target recognition.","However, prevailing methods often face challenges in simultaneously capturing thermal region features and detailed information due to the disparate characteristics of infrared and visible images.","Consequently, fusion outcomes frequently entail a compromise between thermal target area information and texture details.","In this study, we introduce a novel heterogeneous dual-discriminator generative adversarial network (HDDGAN) to address this issue.","Specifically, the generator is structured as a multi-scale skip-connected structure, facilitating the extraction of essential features from different source images.","To enhance the information representation ability of the fusion result, an attention mechanism is employed to construct the information fusion layer within the generator, leveraging the disparities between the source images.","Moreover, recognizing the distinct learning requirements of information in infrared and visible images, we design two discriminators with differing structures.","This approach aims to guide the model to learn salient information from infrared images while simultaneously capturing detailed information from visible images.","Extensive experiments conducted on various public datasets demonstrate the superiority of our proposed HDDGAN over other state-of-the-art (SOTA) algorithms, highlighting its enhanced potential for practical applications."],"url":"http://arxiv.org/abs/2404.15992v1","category":"cs.CV"}
{"created":"2024-04-24 16:26:37","title":"Training Attention Skills in Individuals with Neurodevelopmental Disorders using Virtual Reality and Eye-tracking technology","abstract":"Neurodevelopmental disorders (NDD), encompassing conditions like Intellectual Disability, Attention Deficit Hyperactivity Disorder, and Autism Spectrum Disorder, present challenges across various cognitive capacities. Attention deficits are often common in individuals with NDD due to the sensory system dysfunction that characterizes these disorders. Consequently, limited attention capability can affect the overall quality of life and the ability to transfer knowledge from one circumstance to another. The literature has increasingly recognized the potential benefits of virtual reality (VR) in supporting NDD learning and rehabilitation due to its interactive and engaging nature, which is critical for consistent practice. In previous studies, we explored the usage of a VR application called Wildcard to enhance attention skills in persons with NDD. The application has been redesigned in this study, exploiting eye-tracking technology to enable novel and more fine-grade interactions. A four-week experiment with 38 NDD participants was conducted to evaluate its usability and effectiveness in improving Visual Attention Skills. Results show the usability and effectiveness of Wildcard in enhancing attention skills, advocating for continued exploration of VR and eye-tracking technology's potential in NDD interventions.","sentences":["Neurodevelopmental disorders (NDD), encompassing conditions like Intellectual Disability, Attention Deficit Hyperactivity Disorder, and Autism Spectrum Disorder, present challenges across various cognitive capacities.","Attention deficits are often common in individuals with NDD due to the sensory system dysfunction that characterizes these disorders.","Consequently, limited attention capability can affect the overall quality of life and the ability to transfer knowledge from one circumstance to another.","The literature has increasingly recognized the potential benefits of virtual reality (VR) in supporting NDD learning and rehabilitation due to its interactive and engaging nature, which is critical for consistent practice.","In previous studies, we explored the usage of a VR application called Wildcard to enhance attention skills in persons with NDD.","The application has been redesigned in this study, exploiting eye-tracking technology to enable novel and more fine-grade interactions.","A four-week experiment with 38 NDD participants was conducted to evaluate its usability and effectiveness in improving Visual Attention Skills.","Results show the usability and effectiveness of Wildcard in enhancing attention skills, advocating for continued exploration of VR and eye-tracking technology's potential in NDD interventions."],"url":"http://arxiv.org/abs/2404.15960v1","category":"cs.HC"}
{"created":"2024-04-24 16:19:31","title":"Beyond Deepfake Images: Detecting AI-Generated Videos","abstract":"Recent advances in generative AI have led to the development of techniques to generate visually realistic synthetic video. While a number of techniques have been developed to detect AI-generated synthetic images, in this paper we show that synthetic image detectors are unable to detect synthetic videos. We demonstrate that this is because synthetic video generators introduce substantially different traces than those left by image generators. Despite this, we show that synthetic video traces can be learned, and used to perform reliable synthetic video detection or generator source attribution even after H.264 re-compression. Furthermore, we demonstrate that while detecting videos from new generators through zero-shot transferability is challenging, accurate detection of videos from a new generator can be achieved through few-shot learning.","sentences":["Recent advances in generative AI have led to the development of techniques to generate visually realistic synthetic video.","While a number of techniques have been developed to detect AI-generated synthetic images, in this paper we show that synthetic image detectors are unable to detect synthetic videos.","We demonstrate that this is because synthetic video generators introduce substantially different traces than those left by image generators.","Despite this, we show that synthetic video traces can be learned, and used to perform reliable synthetic video detection or generator source attribution even after H.264 re-compression.","Furthermore, we demonstrate that while detecting videos from new generators through zero-shot transferability is challenging, accurate detection of videos from a new generator can be achieved through few-shot learning."],"url":"http://arxiv.org/abs/2404.15955v1","category":"cs.CV"}
{"created":"2024-04-24 15:58:44","title":"Microstructural features governing fracture of a two-dimensional amorphous solid identified by machine learning","abstract":"Brittle fracturing of materials is common in natural and industrial processes over a variety of length scales. Knowledge of individual particle dynamics is vital to obtain deeper insight into the atomistic processes governing crack propagation in such materials, yet it is challenging to obtain these details in experiments. We propose an experimental approach where isotropic dilational strain is applied to a densely packed monolayer of attractive colloidal microspheres, resulting in fracture. Using brightfield microscopy and particle tracking, we examine the microstructural evolution of the monolayer during fracturing. Furthermore, using a quantified representation of the microstructure in combination with a machine learning algorithm, we calculate the likelihood of regions of the monolayer to be on a crack line, which we term Weakness. From this analysis, we identify the most important contributions to crack propagation and find that local density is more important than orientational order. Our methodology and results provide a basis for further research on microscopic processes during the fracturing process.","sentences":["Brittle fracturing of materials is common in natural and industrial processes over a variety of length scales.","Knowledge of individual particle dynamics is vital to obtain deeper insight into the atomistic processes governing crack propagation in such materials, yet it is challenging to obtain these details in experiments.","We propose an experimental approach where isotropic dilational strain is applied to a densely packed monolayer of attractive colloidal microspheres, resulting in fracture.","Using brightfield microscopy and particle tracking, we examine the microstructural evolution of the monolayer during fracturing.","Furthermore, using a quantified representation of the microstructure in combination with a machine learning algorithm, we calculate the likelihood of regions of the monolayer to be on a crack line, which we term Weakness.","From this analysis, we identify the most important contributions to crack propagation and find that local density is more important than orientational order.","Our methodology and results provide a basis for further research on microscopic processes during the fracturing process."],"url":"http://arxiv.org/abs/2404.15938v1","category":"cond-mat.soft"}
{"created":"2024-04-24 15:38:22","title":"Generalization Measures for Zero-Shot Cross-Lingual Transfer","abstract":"A model's capacity to generalize its knowledge to interpret unseen inputs with different characteristics is crucial to build robust and reliable machine learning systems. Language model evaluation tasks lack information metrics about model generalization and their applicability in a new setting is measured using task and language-specific downstream performance, which is often lacking in many languages and tasks. In this paper, we explore a set of efficient and reliable measures that could aid in computing more information related to the generalization capability of language models in cross-lingual zero-shot settings. In addition to traditional measures such as variance in parameters after training and distance from initialization, we also measure the effectiveness of sharpness in loss landscape in capturing the success in cross-lingual transfer and propose a novel and stable algorithm to reliably compute the sharpness of a model optimum that correlates to generalization.","sentences":["A model's capacity to generalize its knowledge to interpret unseen inputs with different characteristics is crucial to build robust and reliable machine learning systems.","Language model evaluation tasks lack information metrics about model generalization and their applicability in a new setting is measured using task and language-specific downstream performance, which is often lacking in many languages and tasks.","In this paper, we explore a set of efficient and reliable measures that could aid in computing more information related to the generalization capability of language models in cross-lingual zero-shot settings.","In addition to traditional measures such as variance in parameters after training and distance from initialization, we also measure the effectiveness of sharpness in loss landscape in capturing the success in cross-lingual transfer and propose a novel and stable algorithm to reliably compute the sharpness of a model optimum that correlates to generalization."],"url":"http://arxiv.org/abs/2404.15928v1","category":"cs.CL"}
{"created":"2024-04-24 15:37:12","title":"Inside the echo chamber: Linguistic underpinnings of misinformation on Twitter","abstract":"Social media users drive the spread of misinformation online by sharing posts that include erroneous information or commenting on controversial topics with unsubstantiated arguments often in earnest. Work on echo chambers has suggested that users' perspectives are reinforced through repeated interactions with like-minded peers, promoted by homophily and bias in information diffusion. Building on long-standing interest in the social bases of language and linguistic underpinnings of social behavior, this work explores how conversations around misinformation are mediated through language use. We compare a number of linguistic measures, e.g., in-/out-group cues, readability, and discourse connectives, within and across topics of conversation and user communities. Our findings reveal increased presence of group identity signals and processing fluency within echo chambers during discussions of misinformation. We discuss the specific character of these broader trends across topics and examine contextual influences.","sentences":["Social media users drive the spread of misinformation online by sharing posts that include erroneous information or commenting on controversial topics with unsubstantiated arguments often in earnest.","Work on echo chambers has suggested that users' perspectives are reinforced through repeated interactions with like-minded peers, promoted by homophily and bias in information diffusion.","Building on long-standing interest in the social bases of language and linguistic underpinnings of social behavior, this work explores how conversations around misinformation are mediated through language use.","We compare a number of linguistic measures, e.g., in-/out-group cues, readability, and discourse connectives, within and across topics of conversation and user communities.","Our findings reveal increased presence of group identity signals and processing fluency within echo chambers during discussions of misinformation.","We discuss the specific character of these broader trends across topics and examine contextual influences."],"url":"http://arxiv.org/abs/2404.15925v1","category":"cs.SI"}
{"created":"2024-04-25 13:55:03","title":"Derandomization with Pseudorandomness","abstract":"Derandomization techniques are often used within advanced randomized algorithms. In particular, pseudorandom objects, such as hash families and expander graphs, are key components of such algorithms, but their verification presents a challenge. This work shows how such algorithms can be expressed and verified in Isabelle and presents a pseudorandom objects library that abstracts away the involved deep algebraic/analytic results. Moreover, it presents examples that show how the library eases and enables the verification of advanced randomized algorithms. Highlighting the value of this framework is that it was recently used to verify the optimal-space distinct elements algorithm by Blasiok from 2018, which relies on the combination of many derandomization techniques to achieve its optimality.","sentences":["Derandomization techniques are often used within advanced randomized algorithms.","In particular, pseudorandom objects, such as hash families and expander graphs, are key components of such algorithms, but their verification presents a challenge.","This work shows how such algorithms can be expressed and verified in Isabelle and presents a pseudorandom objects library that abstracts away the involved deep algebraic/analytic results.","Moreover, it presents examples that show how the library eases and enables the verification of advanced randomized algorithms.","Highlighting the value of this framework is that it was recently used to verify the optimal-space distinct elements algorithm by Blasiok from 2018, which relies on the combination of many derandomization techniques to achieve its optimality."],"url":"http://arxiv.org/abs/2404.16614v1","category":"cs.LO"}
{"created":"2024-04-25 12:52:57","title":"A New Two-Sided Sketching Algorithm for Large-Scale Tensor Decomposition Based on Discrete Cosine Transformation","abstract":"Large tensors are frequently encountered in various fields such as computer vision, scientific simulations, sensor networks, and data mining. However, these tensors are often too large for convenient processing, transfer, or storage. Fortunately, they typically exhibit a low-rank structure that can be leveraged through tensor decomposition. Despite this, performing large-scale tensor decomposition can be time-consuming. Sketching is a useful technique to reduce the dimensionality of the data. In this study, we introduce a novel two-sided sketching method based on the $t$-product decomposition and the discrete cosine transformation. We conduct a thorough theoretical analysis to assess the approximation error of the proposed method. Specifically, we enhance the algorithm with power iteration to achieve more precise approximate solutions. Extensive numerical experiments and comparisons on low-rank approximation of color images and grayscale videos illustrate the efficiency and effectiveness of the proposed approach in terms of both CPU time and approximation accuracy.","sentences":["Large tensors are frequently encountered in various fields such as computer vision, scientific simulations, sensor networks, and data mining.","However, these tensors are often too large for convenient processing, transfer, or storage.","Fortunately, they typically exhibit a low-rank structure that can be leveraged through tensor decomposition.","Despite this, performing large-scale tensor decomposition can be time-consuming.","Sketching is a useful technique to reduce the dimensionality of the data.","In this study, we introduce a novel two-sided sketching method based on the $t$-product decomposition and the discrete cosine transformation.","We conduct a thorough theoretical analysis to assess the approximation error of the proposed method.","Specifically, we enhance the algorithm with power iteration to achieve more precise approximate solutions.","Extensive numerical experiments and comparisons on low-rank approximation of color images and grayscale videos illustrate the efficiency and effectiveness of the proposed approach in terms of both CPU time and approximation accuracy."],"url":"http://arxiv.org/abs/2404.16580v1","category":"math.OC"}
{"created":"2024-04-25 12:03:00","title":"Implementation of matrix compression in the coupling of JOREK to realistic 3D conducting wall structures","abstract":"JOREK is an advanced non-linear simulation code for studying MHD instabilities in magnetically confined fusion plasmas and their control and/or mitigation. A free-boundary and resistive wall extension was introduced via coupling to the STARWALL and CARIDDI codes, both able to provide dense response matrices describing the electromagnetic interactions between plasma and conducting structures. For detailed CAD representations of the conducting structures and high resolutions for the plasma region, memory and computing time limitations restrict the possibility of simulating the ITER tokamak. In the present work, the Singular Value Decomposition provided by routines from the ScaLAPACK library has been successfully applied to compress some of the dense response matrices and thus optimize memory usage. This is demonstrated for simulations of Tearing Mode and Vertical Displacement Event instabilities. An outlook to future applications on large production cases and further extensions of the method are discussed.","sentences":["JOREK is an advanced non-linear simulation code for studying MHD instabilities in magnetically confined fusion plasmas and their control and/or mitigation.","A free-boundary and resistive wall extension was introduced via coupling to the STARWALL and CARIDDI codes, both able to provide dense response matrices describing the electromagnetic interactions between plasma and conducting structures.","For detailed CAD representations of the conducting structures and high resolutions for the plasma region, memory and computing time limitations restrict the possibility of simulating the ITER tokamak.","In the present work, the Singular Value Decomposition provided by routines from the ScaLAPACK library has been successfully applied to compress some of the dense response matrices and thus optimize memory usage.","This is demonstrated for simulations of Tearing Mode and Vertical Displacement Event instabilities.","An outlook to future applications on large production cases and further extensions of the method are discussed."],"url":"http://arxiv.org/abs/2404.16546v1","category":"physics.plasm-ph"}
{"created":"2024-04-25 11:55:29","title":"Approximation Algorithm of Minimum All-Ones Problem for Arbitrary Graphs","abstract":"Let $G=(V, E)$ be a graph and let each vertex of $G$ has a lamp and a button. Each button can be of $\\sigma^+$-type or $\\sigma$-type.   Assume that initially some lamps are on and others are off. The button on vertex $x$ is of $\\sigma^+$-type ($\\sigma$-type, respectively) if pressing the button changes the lamp states on $x$ and on its neighbors in $G$ (the lamp states on the neighbors of $x$ only, respectively). Assume that there is a set $X\\subseteq V$ such that pressing buttons on vertices of $X$ lights all lamps on vertices of $G$. In particular, it is known to hold when initially all lamps are off and all buttons are of $\\sigma^+$-type.   Finding such a set $X$ of the smallest size is NP-hard even if initially all lamps are off and all buttons are of $\\sigma^+$-type. Using a linear algebraic approach we design a polynomial-time approximation algorithm for the problem such that for the set $X$ constructed by the algorithm, we have $|X|\\le \\min\\{r,(|V|+{\\rm opt})/2\\},$ where $r$ is the rank of a (modified) adjacent matrix of $G$ and ${\\rm opt}$ is the size of an optimal solution to the problem.   To the best of our knowledge, this is the first polynomial-time approximation algorithm for the problem with a nontrivial approximation guarantee.","sentences":["Let $G=(V, E)$ be a graph and let each vertex of $G$ has a lamp and a button.","Each button can be of $\\sigma^+$-type or $\\sigma$-type.   ","Assume that initially some lamps are on and others are off.","The button on vertex $x$ is of $\\sigma^+$-type ($\\sigma$-type, respectively) if pressing the button changes the lamp states on $x$ and on its neighbors in $G$ (the lamp states on the neighbors of $x$ only, respectively).","Assume that there is a set $X\\subseteq V$ such that pressing buttons on vertices of $X$ lights all lamps on vertices of $G$. In particular, it is known to hold when initially all lamps are off and all buttons are of $\\sigma^+$-type.   ","Finding such a set $X$ of the smallest size is NP-hard even if initially all lamps are off and all buttons are of $\\sigma^+$-type.","Using a linear algebraic approach we design a polynomial-time approximation algorithm for the problem such that for the set $X$ constructed by the algorithm, we have $|X|\\le \\min\\{r,(|V|+{\\rm opt})/2\\},$ where $r$ is the rank of a (modified) adjacent matrix of $G$ and ${\\rm opt}$ is the size of an optimal solution to the problem.   ","To the best of our knowledge, this is the first polynomial-time approximation algorithm for the problem with a nontrivial approximation guarantee."],"url":"http://arxiv.org/abs/2404.16540v1","category":"cs.DS"}
{"created":"2024-04-25 10:40:09","title":"Theoretical Insights into Inorganic Antiperovskite Nitrides (X$_3$NA; X = Mg, Sr, Ca, Ba; A = Sb, As): An Emerging Class of Materials for Photovoltaics","abstract":"Antiperovskite nitrides are potential candidates for applications harvesting solar light. With a comprehensive state-of-the-art approach combining hybrid density-functional theory, many-body perturbation theory, the Wannier-Mott model, density-functional perturbation theory, and the Feynman polaron model, we explore excitonic and polaronic effects in X$_3$NA (X: Mg, Ca, Sr, Ba, A = Sb, As). For all of them, we uncover a significant influence of the ionic dielectric screening on the static dielectric constant. Small exciton binding energies, weak electron-phonon coupling, and high charge-carrier mobilities facilitate enhanced charge transport in Mg$_3$NSb, Sr$_3$NSb, and Ba$_3$NSb. Our results highlight the potential of these nitrides as optimal candidates for efficient photovoltaic absorbers.","sentences":["Antiperovskite nitrides are potential candidates for applications harvesting solar light.","With a comprehensive state-of-the-art approach combining hybrid density-functional theory, many-body perturbation theory, the Wannier-Mott model, density-functional perturbation theory, and the Feynman polaron model, we explore excitonic and polaronic effects in X$_3$NA (X: Mg, Ca, Sr, Ba, A = Sb, As).","For all of them, we uncover a significant influence of the ionic dielectric screening on the static dielectric constant.","Small exciton binding energies, weak electron-phonon coupling, and high charge-carrier mobilities facilitate enhanced charge transport in Mg$_3$NSb, Sr$_3$NSb, and Ba$_3$NSb.","Our results highlight the potential of these nitrides as optimal candidates for efficient photovoltaic absorbers."],"url":"http://arxiv.org/abs/2404.16494v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-25 10:18:01","title":"Concentration estimates for SPDEs driven by fractional Brownian motion","abstract":"The main goal of this work is to provide sample-path estimates for the solution of slowly time-dependent SPDEs perturbed by a cylindrical fractional Brownian motion. Our strategy is similar to the approach by Berglund and Nader for space-time white noise. However, the setting of fractional Brownian motion does not allow us to use any martingale methods. Using instead optimal estimates for the probability that the supremum of a Gaussian process exceeds a certain level, we derive concentration estimates for the solution of the SPDE, provided that the Hurst index $H$ of the fractional Brownian motion satisfies $H>\\frac14$. As a by-product, we also obtain concentration estimates for one-dimensional fractional SDEs valid for any $H\\in(0,1)$.","sentences":["The main goal of this work is to provide sample-path estimates for the solution of slowly time-dependent SPDEs perturbed by a cylindrical fractional Brownian motion.","Our strategy is similar to the approach by Berglund and Nader for space-time white noise.","However, the setting of fractional Brownian motion does not allow us to use any martingale methods.","Using instead optimal estimates for the probability that the supremum of a Gaussian process exceeds a certain level, we derive concentration estimates for the solution of the SPDE, provided that the Hurst index $H$ of the fractional Brownian motion satisfies $H>\\frac14$. As a by-product, we also obtain concentration estimates for one-dimensional fractional SDEs valid for any $H\\in(0,1)$."],"url":"http://arxiv.org/abs/2404.16485v1","category":"math.PR"}
{"created":"2024-04-25 07:27:04","title":"Revisiting Seismicity Criticality: A New Framework for Bias Correction of Statistical Seismology Model Calibrations","abstract":"The Epidemic-Type Aftershock Sequences (ETAS) model and its variants effectively capture the space-time clustering of seismicity, setting the standard for earthquake forecasting. Accurate unbiased ETAS calibration is thus crucial. But we identify three sources of bias, (i) boundary effects, (ii) finite-size effects, and (iii) censorship, which are often overlooked or misinterpreted, causing errors in seismic analysis and predictions. By employing an ETAS model variant with variable spatial background rates, we propose a method to correct for these biases, focusing on the branching ratio n, a key indicator of earthquake triggering potential. Our approach quantifies the variation in the apparent branching ratio (napp) with increased cut-off magnitude (Mco) above the optimal cut-off (Mcobest). The napp(Mco) function yields insights superior to traditional point estimates. We validate our method using synthetic earthquake catalogs, accurately recovering the true branching ratio (ntrue) after correcting biases with napp(Mco). Additionally, our method introduces a refined estimation of the minimum triggering magnitude (m0), a crucial parameter in the ETAS model. Applying our framework to the earthquake catalogs of California, New Zealand, and the China Seismic Experimental Site (CSES) in Sichuan and Yunnan provinces, we find that seismicity hovers away from the critical point, nc = 1, remaining distinctly subcritical, however with values tending to be larger than recent reports that do not consider the above biases. It is interesting that, m0 is found around 4 for California, 3 for New Zealand and 2 for CSES, suggesting that many small triggered earthquakes may not be fertile. Understanding seismicity's critical state significantly enhances our comprehension of seismic patterns, aftershock predictability, and informs earthquake risk mitigation and management strategies.","sentences":["The Epidemic-Type Aftershock Sequences (ETAS) model and its variants effectively capture the space-time clustering of seismicity, setting the standard for earthquake forecasting.","Accurate unbiased ETAS calibration is thus crucial.","But we identify three sources of bias, (i) boundary effects, (ii) finite-size effects, and (iii) censorship, which are often overlooked or misinterpreted, causing errors in seismic analysis and predictions.","By employing an ETAS model variant with variable spatial background rates, we propose a method to correct for these biases, focusing on the branching ratio n, a key indicator of earthquake triggering potential.","Our approach quantifies the variation in the apparent branching ratio (napp) with increased cut-off magnitude (Mco) above the optimal cut-off (Mcobest).","The napp(Mco) function yields insights superior to traditional point estimates.","We validate our method using synthetic earthquake catalogs, accurately recovering the true branching ratio (ntrue) after correcting biases with napp(Mco).","Additionally, our method introduces a refined estimation of the minimum triggering magnitude (m0), a crucial parameter in the ETAS model.","Applying our framework to the earthquake catalogs of California, New Zealand, and the China Seismic Experimental Site (CSES) in Sichuan and Yunnan provinces, we find that seismicity hovers away from the critical point, nc = 1, remaining distinctly subcritical, however with values tending to be larger than recent reports that do not consider the above biases.","It is interesting that, m0 is found around 4 for California, 3 for New Zealand and 2 for CSES, suggesting that many small triggered earthquakes may not be fertile.","Understanding seismicity's critical state significantly enhances our comprehension of seismic patterns, aftershock predictability, and informs earthquake risk mitigation and management strategies."],"url":"http://arxiv.org/abs/2404.16374v1","category":"physics.geo-ph"}
{"created":"2024-04-25 05:15:24","title":"Vertex Ranking of Degenerate Graphs","abstract":"An $\\ell$-vertex-ranking of a graph $G$ is a colouring of the vertices of $G$ with integer colours so that in any connected subgraph $H$ of $G$ with diameter at most $\\ell$, there is a vertex in $H$ whose colour is larger than that of every other vertex in $H$. The $\\ell$-vertex-ranking number, $\\chi_{\\ell-\\mathrm{vr}}(G)$, of $G$ is the minimum integer $k$ such that $G$ has an $\\ell$-vertex-ranking using $k$ colours. We prove that, for any fixed $d$ and $\\ell$, every $d$-degenerate $n$-vertex graph $G$ satisfies $\\chi_{\\ell-\\mathrm{vr}}(G)= O(n^{1-2/(\\ell+1)}\\log n)$ if $\\ell$ is even and $\\chi_{\\ell-\\mathrm{vr}}(G)= O(n^{1-2/\\ell}\\log n)$ if $\\ell$ is odd. The case $\\ell=2$ resolves (up to the $\\log n$ factor) an open problem posed by \\citet{karpas.neiman.ea:on} and the cases $\\ell\\in\\{2,3\\}$ are asymptotically optimal (up to the $\\log n$ factor).","sentences":["An $\\ell$-vertex-ranking of a graph $G$ is a colouring of the vertices of $G$ with integer colours so that in any connected subgraph $H$ of $G$ with diameter at most $\\ell$, there is a vertex in $H$ whose colour is larger than that of every other vertex in $H$. The $\\ell$-vertex-ranking number, $\\chi_{\\ell-\\mathrm{vr}}(G)$, of $G$ is the minimum integer $k$ such that $G$ has an $\\ell$-vertex-ranking using $k$ colours.","We prove that, for any fixed $d$ and $\\ell$, every $d$-degenerate $n$-vertex graph $G$ satisfies $\\chi_{\\ell-\\mathrm{vr}}(G)= O(n^{1-2/(\\ell+1)}\\log","n)$","if $\\ell$ is even and $\\chi_{\\ell-\\mathrm{vr}}(G)= O(n^{1-2/\\ell}\\log n)$ if $\\ell$ is odd.","The case $\\ell=2$ resolves (up to the $\\log n$ factor) an open problem posed by \\citet{karpas.neiman.ea:on} and the cases $\\ell\\in\\{2,3\\}$ are asymptotically optimal (up to the $\\log n$ factor)."],"url":"http://arxiv.org/abs/2404.16340v1","category":"math.CO"}
{"created":"2024-04-25 04:00:52","title":"Bridging Speed and Accuracy to Approximate $K$-Nearest Neighbor Search","abstract":"Approximate K-Nearest Neighbor (AKNN) search in high-dimensional spaces is a critical yet challenging problem. The efficiency of AKNN search largely depends on the computation of distances, a process that significantly affects the runtime. To improve computational efficiency, existing work often opts for estimating approximate distances rather than computing exact distances, at the cost of reduced AKNN search accuracy. The recent method of ADSampling has attempted to mitigate this problem by using random projection for distance approximations and adjusting these approximations based on error bounds to improve accuracy. However, ADSampling faces limitations in effectiveness and generality, mainly due to the suboptimality of its distance approximations and its heavy reliance on random projection matrices to obtain error bounds. In this study, we propose a new method that uses an optimal orthogonal projection instead of random projection, thereby providing improved distance approximations. Moreover, our method uses error quantiles instead of error bounds for approximation adjustment, and the derivation of error quantiles can be made independent of the projection matrix, thus extending the generality of our approach. Extensive experiments confirm the superior efficiency and effectiveness of the proposed method. In particular, compared to the state-of-the-art method of ADSampling, our method achieves a speedup of 1.6 to 2.1 times on real datasets with almost no loss of accuracy.","sentences":["Approximate K-Nearest Neighbor (AKNN) search in high-dimensional spaces is a critical yet challenging problem.","The efficiency of AKNN search largely depends on the computation of distances, a process that significantly affects the runtime.","To improve computational efficiency, existing work often opts for estimating approximate distances rather than computing exact distances, at the cost of reduced AKNN search accuracy.","The recent method of ADSampling has attempted to mitigate this problem by using random projection for distance approximations and adjusting these approximations based on error bounds to improve accuracy.","However, ADSampling faces limitations in effectiveness and generality, mainly due to the suboptimality of its distance approximations and its heavy reliance on random projection matrices to obtain error bounds.","In this study, we propose a new method that uses an optimal orthogonal projection instead of random projection, thereby providing improved distance approximations.","Moreover, our method uses error quantiles instead of error bounds for approximation adjustment, and the derivation of error quantiles can be made independent of the projection matrix, thus extending the generality of our approach.","Extensive experiments confirm the superior efficiency and effectiveness of the proposed method.","In particular, compared to the state-of-the-art method of ADSampling, our method achieves a speedup of 1.6 to 2.1 times on real datasets with almost no loss of accuracy."],"url":"http://arxiv.org/abs/2404.16322v1","category":"cs.DB"}
{"created":"2024-04-25 03:21:11","title":"TI2V-Zero: Zero-Shot Image Conditioning for Text-to-Video Diffusion Models","abstract":"Text-conditioned image-to-video generation (TI2V) aims to synthesize a realistic video starting from a given image (e.g., a woman's photo) and a text description (e.g., \"a woman is drinking water.\"). Existing TI2V frameworks often require costly training on video-text datasets and specific model designs for text and image conditioning. In this paper, we propose TI2V-Zero, a zero-shot, tuning-free method that empowers a pretrained text-to-video (T2V) diffusion model to be conditioned on a provided image, enabling TI2V generation without any optimization, fine-tuning, or introducing external modules. Our approach leverages a pretrained T2V diffusion foundation model as the generative prior. To guide video generation with the additional image input, we propose a \"repeat-and-slide\" strategy that modulates the reverse denoising process, allowing the frozen diffusion model to synthesize a video frame-by-frame starting from the provided image. To ensure temporal continuity, we employ a DDPM inversion strategy to initialize Gaussian noise for each newly synthesized frame and a resampling technique to help preserve visual details. We conduct comprehensive experiments on both domain-specific and open-domain datasets, where TI2V-Zero consistently outperforms a recent open-domain TI2V model. Furthermore, we show that TI2V-Zero can seamlessly extend to other tasks such as video infilling and prediction when provided with more images. Its autoregressive design also supports long video generation.","sentences":["Text-conditioned image-to-video generation (TI2V) aims to synthesize a realistic video starting from a given image (e.g., a woman's photo) and a text description (e.g., \"a woman is drinking water.\").","Existing TI2V frameworks often require costly training on video-text datasets and specific model designs for text and image conditioning.","In this paper, we propose TI2V-Zero, a zero-shot, tuning-free method that empowers a pretrained text-to-video (T2V) diffusion model to be conditioned on a provided image, enabling TI2V generation without any optimization, fine-tuning, or introducing external modules.","Our approach leverages a pretrained T2V diffusion foundation model as the generative prior.","To guide video generation with the additional image input, we propose a \"repeat-and-slide\" strategy that modulates the reverse denoising process, allowing the frozen diffusion model to synthesize a video frame-by-frame starting from the provided image.","To ensure temporal continuity, we employ a DDPM inversion strategy to initialize Gaussian noise for each newly synthesized frame and a resampling technique to help preserve visual details.","We conduct comprehensive experiments on both domain-specific and open-domain datasets, where TI2V-Zero consistently outperforms a recent open-domain TI2V model.","Furthermore, we show that TI2V-Zero can seamlessly extend to other tasks such as video infilling and prediction when provided with more images.","Its autoregressive design also supports long video generation."],"url":"http://arxiv.org/abs/2404.16306v1","category":"cs.CV"}
{"created":"2024-04-25 01:01:35","title":"Heavy Dark Matter in White Dwarfs: Multiple-Scattering Capture and Thermalization","abstract":"We present an improved treatment for the scattering of heavy dark matter from the ion constituents of a white dwarf. In the heavy dark matter regime, multiple collisions are required for the dark matter to become gravitationally captured. Our treatment incorporates all relevant physical effects including the dark matter trajectories, nuclear form factors, and radial profiles for the white dwarf escape velocity and target number densities. Our capture rates differ by orders of magnitude from previous estimates, which have typically used approximations developed for dark matter scattering in the Earth. We also compute the time for the dark matter to thermalize in the center of the white dwarf, including in-medium effects such as phonon emission and absorption from the ionic lattice in the case where the star has a crystallized core. We find much shorter thermalization timescales than previously estimated, especially if the white dwarf core has crystallized. We illustrate the importance of our improved approach by determining the cross section required for accumulated asymmetric dark matter to self-gravitate.","sentences":["We present an improved treatment for the scattering of heavy dark matter from the ion constituents of a white dwarf.","In the heavy dark matter regime, multiple collisions are required for the dark matter to become gravitationally captured.","Our treatment incorporates all relevant physical effects including the dark matter trajectories, nuclear form factors, and radial profiles for the white dwarf escape velocity and target number densities.","Our capture rates differ by orders of magnitude from previous estimates, which have typically used approximations developed for dark matter scattering in the Earth.","We also compute the time for the dark matter to thermalize in the center of the white dwarf, including in-medium effects such as phonon emission and absorption from the ionic lattice in the case where the star has a crystallized core.","We find much shorter thermalization timescales than previously estimated, especially if the white dwarf core has crystallized.","We illustrate the importance of our improved approach by determining the cross section required for accumulated asymmetric dark matter to self-gravitate."],"url":"http://arxiv.org/abs/2404.16272v1","category":"hep-ph"}
{"created":"2024-04-24 22:39:27","title":"Rigorous Formalization of Orbital Functionals: Addressing the Noninteracting $v$-Representability Problem","abstract":"Functionals that explicitly depend on occupied, unoccupied, or fractionally-occupied orbitals are rigorously formalized using Clifford algebras, and a variational principle is established that facilitates orbital (and occupation) optimization as a formal implementation method. Theoretically, these methodologies circumvent the limitations encountered in the original Kohn-Sham and related methods, particularly when the interacting system's electron density does not match that of any noninteracting reference system. This work redefines orbital (and occupation) functionals from a novel perspective, positioning them not merely as extensions of traditional density functionals, but as superior, rigorous alternatives.","sentences":["Functionals that explicitly depend on occupied, unoccupied, or fractionally-occupied orbitals are rigorously formalized using Clifford algebras, and a variational principle is established that facilitates orbital (and occupation) optimization as a formal implementation method.","Theoretically, these methodologies circumvent the limitations encountered in the original Kohn-Sham and related methods, particularly when the interacting system's electron density does not match that of any noninteracting reference system.","This work redefines orbital (and occupation) functionals from a novel perspective, positioning them not merely as extensions of traditional density functionals, but as superior, rigorous alternatives."],"url":"http://arxiv.org/abs/2404.16236v1","category":"quant-ph"}
{"created":"2024-04-24 22:01:31","title":"Optimal entanglement generation in optomechanical systems via Krotov control of covariance matrix dynamics","abstract":"We investigated the optimal control of a continuous variable system, focusing on entanglement generation in an optomechanical system without utilizing Fock basis cutoffs. Using the Krotov algorithm to optimize the dynamics of the covariance matrix, we illustrated how to design a control objective function to manipulate the dynamics of the system to generate a desirable target state. We showed that entanglement between the macroscopic mechanical mirror and the quantum optical cavity can be reliably generated through imposing the control on the detuning of the external laser field. It has be shown that the control may be still achieved when imposing spectral constraints on the external field to restrict it to low-frequency components. In addition, we systematically studies the effects of quantum control on non-Markovian open system dynamics. We observed that memory effects can play a beneficial role in mitigating the detrimental impact of environmental noises. Specifically, the entanglement generated shows reduced decay in the presence of these memory effects.","sentences":["We investigated the optimal control of a continuous variable system, focusing on entanglement generation in an optomechanical system without utilizing Fock basis cutoffs.","Using the Krotov algorithm to optimize the dynamics of the covariance matrix, we illustrated how to design a control objective function to manipulate the dynamics of the system to generate a desirable target state.","We showed that entanglement between the macroscopic mechanical mirror and the quantum optical cavity can be reliably generated through imposing the control on the detuning of the external laser field.","It has be shown that the control may be still achieved when imposing spectral constraints on the external field to restrict it to low-frequency components.","In addition, we systematically studies the effects of quantum control on non-Markovian open system dynamics.","We observed that memory effects can play a beneficial role in mitigating the detrimental impact of environmental noises.","Specifically, the entanglement generated shows reduced decay in the presence of these memory effects."],"url":"http://arxiv.org/abs/2404.16227v1","category":"quant-ph"}
{"created":"2024-04-24 21:32:59","title":"Fault-Tolerant Bounded Flow Preservers","abstract":"Given a directed graph $G = (V, E)$ with $n$ vertices, $m$ edges and a designated source vertex $s\\in V$, we consider the question of finding a sparse subgraph $H$ of $G$ that preserves the flow from $s$ up to a given threshold $\\lambda$ even after failure of $k$ edges. We refer to such subgraphs as $(\\lambda,k)$-fault-tolerant bounded-flow-preserver ($(\\lambda,k)$-FT-BFP). Formally, for any $F \\subseteq E$ of at most $k$ edges and any $v\\in V$, the $(s, v)$-max-flow in $H \\setminus F$ is equal to $(s, v)$-max-flow in $G \\setminus F$, if the latter is bounded by $\\lambda$, and at least $\\lambda$ otherwise. Our contributions are summarized as follows:   1. We provide a polynomial time algorithm that given any graph $G$ constructs a $(\\lambda,k)$-FT-BFP of $G$ with at most $\\lambda 2^kn$ edges.   2. We also prove a matching lower bound of $\\Omega(\\lambda 2^kn)$ on the size of $(\\lambda,k)$-FT-BFP. In particular, we show that for every $\\lambda,k,n\\geq 1$, there exists an $n$-vertex directed graph whose optimal $(\\lambda,k)$-FT-BFP contains $\\Omega(\\min\\{2^k\\lambda n,n^2\\})$ edges.   3. Furthermore, we show that the problem of computing approximate $(\\lambda,k)$-FT-BFP is NP-hard for any approximation ratio that is better than $O(\\log(\\lambda^{-1} n))$.","sentences":["Given a directed graph $G = (V, E)$ with $n$ vertices, $m$ edges and a designated source vertex $s\\in V$, we consider the question of finding a sparse subgraph $H$ of $G$ that preserves the flow from $s$ up to a given threshold $\\lambda$ even after failure of $k$ edges.","We refer to such subgraphs as $(\\lambda,k)$-fault-tolerant bounded-flow-preserver ($(\\lambda,k)$-FT-BFP).","Formally, for any $F \\subseteq E$ of at most $k$ edges and any $v\\in V$, the $(s, v)$-max-flow in $H \\setminus F$ is equal to $(s, v)$-max-flow in $G \\setminus F$, if the latter is bounded by $\\lambda$, and at least $\\lambda$ otherwise.","Our contributions are summarized as follows:   1.","We provide a polynomial time algorithm that given any graph $G$ constructs a $(\\lambda,k)$-FT-BFP of $G$ with at most $\\lambda 2^kn$ edges.   ","2.","We also prove a matching lower bound of $\\Omega(\\lambda 2^kn)$ on the size of $(\\lambda,k)$-FT-BFP.","In particular, we show that for every $\\lambda,k,n\\geq 1$, there exists an $n$-vertex directed graph whose optimal $(\\lambda,k)$-FT-BFP contains $\\Omega(\\min\\{2^k\\lambda n,n^2\\})$ edges.   ","3.","Furthermore, we show that the problem of computing approximate $(\\lambda,k)$-FT-BFP is NP-hard for any approximation ratio that is better than $O(\\log(\\lambda^{-1} n))$."],"url":"http://arxiv.org/abs/2404.16217v1","category":"cs.DS"}
{"created":"2024-04-24 20:37:44","title":"On Hybrid Gene Regulatory Networks","abstract":"In this work, we study a class of hybrid dynamical systems called hybrid gene regulatory networks (HGRNs) which was proposed to model gene regulatory networks. In HGRNs, there exist well-behaved trajectories that reach a fixed point or converge to a limit cycle, as well as chaotic trajectories that behave non-periodic or indeterministic. In our work, we investigate these irregular behaviors of HGRNs and present theoretical results about the decidability of the reachability problem, the probability of indeterministic behavior of HGRNs, and chaos especially in 2-dimensional HGRNs.","sentences":["In this work, we study a class of hybrid dynamical systems called hybrid gene regulatory networks (HGRNs) which was proposed to model gene regulatory networks.","In HGRNs, there exist well-behaved trajectories that reach a fixed point or converge to a limit cycle, as well as chaotic trajectories that behave non-periodic or indeterministic.","In our work, we investigate these irregular behaviors of HGRNs and present theoretical results about the decidability of the reachability problem, the probability of indeterministic behavior of HGRNs, and chaos especially in 2-dimensional HGRNs."],"url":"http://arxiv.org/abs/2404.16197v1","category":"q-bio.MN"}
{"created":"2024-04-24 19:01:02","title":"A Two-Phase Infinite/Finite Low-Level Memory Model","abstract":"This paper provides a novel approach to reconciling complex low-level memory model features, such as pointer--integer casts, with desired refinements that are needed to justify the correctness of program transformations. The idea is to use a \"two-phased\" memory model, one with and unbounded memory and corresponding unbounded integer type, and one with a finite memory; the connection between the two levels is made explicit by our notion of refinement that handles out-of-memory behaviors. This approach allows for more optimizations to be performed and establishes a clear boundary between the idealized semantics of a program and the implementation of that program on finite hardware.   To demonstrate the utility of this idea in practice, we instantiate the two-phase memory model in the context of Zakowski et al.'s VIR semantics, yielding infinite and finite memory models of LLVM IR, including low-level features like undef and bitcast. Both the infinite and finite models, which act as specifications, can provably be refined to executable reference interpreters. The semantics justify optimizations, such as dead-alloca-elimination, that were previously impossible or difficult to prove correct.","sentences":["This paper provides a novel approach to reconciling complex low-level memory model features, such as pointer--integer casts, with desired refinements that are needed to justify the correctness of program transformations.","The idea is to use a \"two-phased\" memory model, one with and unbounded memory and corresponding unbounded integer type, and one with a finite memory; the connection between the two levels is made explicit by our notion of refinement that handles out-of-memory behaviors.","This approach allows for more optimizations to be performed and establishes a clear boundary between the idealized semantics of a program and the implementation of that program on finite hardware.   ","To demonstrate the utility of this idea in practice, we instantiate the two-phase memory model in the context of Zakowski et al.'s VIR semantics, yielding infinite and finite memory models of LLVM IR, including low-level features like undef and bitcast.","Both the infinite and finite models, which act as specifications, can provably be refined to executable reference interpreters.","The semantics justify optimizations, such as dead-alloca-elimination, that were previously impossible or difficult to prove correct."],"url":"http://arxiv.org/abs/2404.16143v1","category":"cs.PL"}
{"created":"2024-04-24 18:27:43","title":"Generalized Optimization Modulo Theories","abstract":"Optimization Modulo Theories (OMT) has emerged as an important extension of the highly successful Satisfiability Modulo Theories (SMT) paradigm. The OMT problem requires solving an SMT problem with the restriction that the solution must be optimal with respect to a given objective function. We introduce a generalization of the OMT problem where, in particular, objective functions can range over partially ordered sets. We provide a formalization of and an abstract calculus for the generalized OMT problem and prove their key correctness properties. Generalized OMT extends previous work on OMT in several ways. First, in contrast to many current OMT solvers, our calculus is theory-agnostic, enabling the optimization of queries over any theories or combinations thereof. Second, our formalization unifies both single- and multi-objective optimization problems, allowing us to study them both in a single framework and facilitating the use of objective functions that are not supported by existing OMT approaches. Finally, our calculus is sufficiently general to fully capture a wide variety of current OMT approaches (each of which can be realized as a specific strategy for rule application in the calculus) and to support the exploration of new search strategies. Much like the original abstract DPLL(T) calculus for SMT, our Generalized OMT calculus is designed to establish a theoretical foundation for understanding and research and to serve as a framework for studying variations of and extensions to existing OMT methodologies.","sentences":["Optimization Modulo Theories (OMT) has emerged as an important extension of the highly successful Satisfiability Modulo Theories (SMT) paradigm.","The OMT problem requires solving an SMT problem with the restriction that the solution must be optimal with respect to a given objective function.","We introduce a generalization of the OMT problem where, in particular, objective functions can range over partially ordered sets.","We provide a formalization of and an abstract calculus for the generalized OMT problem and prove their key correctness properties.","Generalized OMT extends previous work on OMT in several ways.","First, in contrast to many current OMT solvers, our calculus is theory-agnostic, enabling the optimization of queries over any theories or combinations thereof.","Second, our formalization unifies both single- and multi-objective optimization problems, allowing us to study them both in a single framework and facilitating the use of objective functions that are not supported by existing OMT approaches.","Finally, our calculus is sufficiently general to fully capture a wide variety of current OMT approaches (each of which can be realized as a specific strategy for rule application in the calculus) and to support the exploration of new search strategies.","Much like the original abstract DPLL(T) calculus for SMT, our Generalized OMT calculus is designed to establish a theoretical foundation for understanding and research and to serve as a framework for studying variations of and extensions to existing OMT methodologies."],"url":"http://arxiv.org/abs/2404.16122v1","category":"cs.LO"}
{"created":"2024-04-24 18:18:56","title":"Act as a Honeytoken Generator! An Investigation into Honeytoken Generation with Large Language Models","abstract":"With the increasing prevalence of security incidents, the adoption of deception-based defense strategies has become pivotal in cyber security. This work addresses the challenge of scalability in designing honeytokens, a key component of such defense mechanisms. The manual creation of honeytokens is a tedious task. Although automated generators exists, they often lack versatility, being specialized for specific types of honeytokens, and heavily rely on suitable training datasets. To overcome these limitations, this work systematically investigates the approach of utilizing Large Language Models (LLMs) to create a variety of honeytokens. Out of the seven different honeytoken types created in this work, such as configuration files, databases, and log files, two were used to evaluate the optimal prompt. The generation of robots.txt files and honeywords was used to systematically test 210 different prompt structures, based on 16 prompt building blocks. Furthermore, all honeytokens were tested across different state-of-the-art LLMs to assess the varying performance of different models. Prompts performing optimally on one LLMs do not necessarily generalize well to another. Honeywords generated by GPT-3.5 were found to be less distinguishable from real passwords compared to previous methods of automated honeyword generation. Overall, the findings of this work demonstrate that generic LLMs are capable of creating a wide array of honeytokens using the presented prompt structures.","sentences":["With the increasing prevalence of security incidents, the adoption of deception-based defense strategies has become pivotal in cyber security.","This work addresses the challenge of scalability in designing honeytokens, a key component of such defense mechanisms.","The manual creation of honeytokens is a tedious task.","Although automated generators exists, they often lack versatility, being specialized for specific types of honeytokens, and heavily rely on suitable training datasets.","To overcome these limitations, this work systematically investigates the approach of utilizing Large Language Models (LLMs) to create a variety of honeytokens.","Out of the seven different honeytoken types created in this work, such as configuration files, databases, and log files, two were used to evaluate the optimal prompt.","The generation of robots.txt files and honeywords was used to systematically test 210 different prompt structures, based on 16 prompt building blocks.","Furthermore, all honeytokens were tested across different state-of-the-art LLMs to assess the varying performance of different models.","Prompts performing optimally on one LLMs do not necessarily generalize well to another.","Honeywords generated by GPT-3.5 were found to be less distinguishable from real passwords compared to previous methods of automated honeyword generation.","Overall, the findings of this work demonstrate that generic LLMs are capable of creating a wide array of honeytokens using the presented prompt structures."],"url":"http://arxiv.org/abs/2404.16118v1","category":"cs.CR"}
{"created":"2024-04-24 17:59:32","title":"Emergence of rapid solidification microstructure in additive manufacturing of a Magnesium alloy","abstract":"Bioresorbable Mg-based alloys with low density, low elastic modulus, and excellent biocompatibility are outstanding candidates for temporary orthopedic implants. Coincidentally, metal additive manufacturing (AM) is disrupting the biomedical sector by providing fast access to patient-customized implants. Due to the high cooling rates associated with fusion-based AM techniques, they are often described as rapid solidification processes. However, conclusive observations or rapid solidification in metal AM -- attested by drastic microstructural changes induced by solute trapping, kinetic undercooling, or morphological transitions of the solid-liquid interface -- are scarce. Here we study the formation of banded microstructures during laser powder-bed fusion (LPBF) of a biomedical-grade Magnesium-rare earth alloy, combining advanced characterization and state-of-the-art thermal and phase-field modeling. Our experiments unambiguously identify microstructures as the result of an oscillatory banding instability known from other rapid solidification processes. Our simulations confirm that LPBF-relevant solidification conditions strongly promote the development of banded microstructures in a Mg-Nd alloy. Simulations also allow us to peer into the sub-micrometer nanosecond-scale details of the solid-liquid interface evolution giving rise to the distinctive banded patterns. Since rapidly solidified Mg alloys may exhibit significantly different mechanical and corrosion response compared to their cast counterparts, the ability to predict the emergence of rapid solidification microstructures (and to correlate them with local solidification conditions) may open new pathways for the design of bioresorbable orthopedic implants, not only fitted geometrically to each patient, but also optimized with locally-tuned mechanical and corrosion properties.","sentences":["Bioresorbable Mg-based alloys with low density, low elastic modulus, and excellent biocompatibility are outstanding candidates for temporary orthopedic implants.","Coincidentally, metal additive manufacturing (AM) is disrupting the biomedical sector by providing fast access to patient-customized implants.","Due to the high cooling rates associated with fusion-based AM techniques, they are often described as rapid solidification processes.","However, conclusive observations or rapid solidification in metal AM -- attested by drastic microstructural changes induced by solute trapping, kinetic undercooling, or morphological transitions of the solid-liquid interface -- are scarce.","Here we study the formation of banded microstructures during laser powder-bed fusion (LPBF) of a biomedical-grade Magnesium-rare earth alloy, combining advanced characterization and state-of-the-art thermal and phase-field modeling.","Our experiments unambiguously identify microstructures as the result of an oscillatory banding instability known from other rapid solidification processes.","Our simulations confirm that LPBF-relevant solidification conditions strongly promote the development of banded microstructures in a Mg-Nd alloy.","Simulations also allow us to peer into the sub-micrometer nanosecond-scale details of the solid-liquid interface evolution giving rise to the distinctive banded patterns.","Since rapidly solidified Mg alloys may exhibit significantly different mechanical and corrosion response compared to their cast counterparts, the ability to predict the emergence of rapid solidification microstructures (and to correlate them with local solidification conditions) may open new pathways for the design of bioresorbable orthopedic implants, not only fitted geometrically to each patient, but also optimized with locally-tuned mechanical and corrosion properties."],"url":"http://arxiv.org/abs/2404.16031v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-24 17:56:44","title":"On the Emergence of Ergodic Dynamics in Unique Games","abstract":"The Unique Games Conjecture (UGC) constitutes a highly dynamic subarea within computational complexity theory, intricately linked to the outstanding P versus NP problem. Despite multiple insightful results in the past few years, a proof for the conjecture remains elusive. In this work, we construct a novel dynamical systems-based approach for studying unique games and, more generally, the field of computational complexity. We propose a family of dynamical systems whose equilibria correspond to solutions of unique games and prove that unsatisfiable instances lead to ergodic dynamics. Moreover, as the instance hardness increases, the weight of the invariant measure in the vicinity of the optimal assignments scales polynomially, sub-exponentially, or exponentially depending on the value gap. We numerically reproduce a previously hypothesized hardness plot associated with the UGC. Our results indicate that the UGC is likely true, subject to our proposed conjectures that link dynamical systems theory with computational complexity.","sentences":["The Unique Games Conjecture (UGC) constitutes a highly dynamic subarea within computational complexity theory, intricately linked to the outstanding P versus NP problem.","Despite multiple insightful results in the past few years, a proof for the conjecture remains elusive.","In this work, we construct a novel dynamical systems-based approach for studying unique games and, more generally, the field of computational complexity.","We propose a family of dynamical systems whose equilibria correspond to solutions of unique games and prove that unsatisfiable instances lead to ergodic dynamics.","Moreover, as the instance hardness increases, the weight of the invariant measure in the vicinity of the optimal assignments scales polynomially, sub-exponentially, or exponentially depending on the value gap.","We numerically reproduce a previously hypothesized hardness plot associated with the UGC.","Our results indicate that the UGC is likely true, subject to our proposed conjectures that link dynamical systems theory with computational complexity."],"url":"http://arxiv.org/abs/2404.16024v1","category":"math.DS"}
{"created":"2024-04-24 17:53:14","title":"Universal Adversarial Triggers Are Not Universal","abstract":"Recent work has developed optimization procedures to find token sequences, called adversarial triggers, which can elicit unsafe responses from aligned language models. These triggers are believed to be universally transferable, i.e., a trigger optimized on one model can jailbreak other models. In this paper, we concretely show that such adversarial triggers are not universal. We extensively investigate trigger transfer amongst 13 open models and observe inconsistent transfer. Our experiments further reveal a significant difference in robustness to adversarial triggers between models Aligned by Preference Optimization (APO) and models Aligned by Fine-Tuning (AFT). We find that APO models are extremely hard to jailbreak even when the trigger is optimized directly on the model. On the other hand, while AFT models may appear safe on the surface, exhibiting refusals to a range of unsafe instructions, we show that they are highly susceptible to adversarial triggers. Lastly, we observe that most triggers optimized on AFT models also generalize to new unsafe instructions from five diverse domains, further emphasizing their vulnerability. Overall, our work highlights the need for more comprehensive safety evaluations for aligned language models.","sentences":["Recent work has developed optimization procedures to find token sequences, called adversarial triggers, which can elicit unsafe responses from aligned language models.","These triggers are believed to be universally transferable, i.e., a trigger optimized on one model can jailbreak other models.","In this paper, we concretely show that such adversarial triggers are not universal.","We extensively investigate trigger transfer amongst 13 open models and observe inconsistent transfer.","Our experiments further reveal a significant difference in robustness to adversarial triggers between models Aligned by Preference Optimization (APO) and models Aligned by Fine-Tuning (AFT).","We find that APO models are extremely hard to jailbreak even when the trigger is optimized directly on the model.","On the other hand, while AFT models may appear safe on the surface, exhibiting refusals to a range of unsafe instructions, we show that they are highly susceptible to adversarial triggers.","Lastly, we observe that most triggers optimized on AFT models also generalize to new unsafe instructions from five diverse domains, further emphasizing their vulnerability.","Overall, our work highlights the need for more comprehensive safety evaluations for aligned language models."],"url":"http://arxiv.org/abs/2404.16020v1","category":"cs.CL"}
{"created":"2024-04-24 17:29:03","title":"Benchmarking a heuristic Floquet adiabatic algorithm for the Max-Cut problem","abstract":"According to the adiabatic theorem of quantum mechanics, a system initially in the ground state of a Hamiltonian remains in the ground state if one slowly changes the Hamiltonian. This can be used in principle to solve hard problems on quantum computers. Generically, however, implementation of this Hamiltonian dynamics on digital quantum computers requires scaling Trotter step size with system size and simulation time, which incurs a large gate count. In this work, we argue that for classical optimization problems, the adiabatic evolution can be performed with a fixed, finite Trotter step. This \"Floquet adiabatic evolution\" reduces by several orders of magnitude the gate count compared to the usual, continuous-time adiabatic evolution. We give numerical evidence using matrix-product-state simulations that it can optimally solve the Max-Cut problem on $3$-regular graphs in a large number of instances, with surprisingly low runtime, even with bond dimensions as low as $D=2$. Extrapolating our numerical results, we estimate the resources needed for a quantum computer to compete with classical exact or approximate solvers for this specific problem.","sentences":["According to the adiabatic theorem of quantum mechanics, a system initially in the ground state of a Hamiltonian remains in the ground state if one slowly changes the Hamiltonian.","This can be used in principle to solve hard problems on quantum computers.","Generically, however, implementation of this Hamiltonian dynamics on digital quantum computers requires scaling Trotter step size with system size and simulation time, which incurs a large gate count.","In this work, we argue that for classical optimization problems, the adiabatic evolution can be performed with a fixed, finite Trotter step.","This \"Floquet adiabatic evolution\" reduces by several orders of magnitude the gate count compared to the usual, continuous-time adiabatic evolution.","We give numerical evidence using matrix-product-state simulations that it can optimally solve the Max-Cut problem on $3$-regular graphs in a large number of instances, with surprisingly low runtime, even with bond dimensions as low as $D=2$. Extrapolating our numerical results, we estimate the resources needed for a quantum computer to compete with classical exact or approximate solvers for this specific problem."],"url":"http://arxiv.org/abs/2404.16001v1","category":"quant-ph"}
{"created":"2024-04-24 17:01:05","title":"Seed Selection in the Heterogeneous Moran Process","abstract":"The Moran process is a classic stochastic process that models the rise and takeover of novel traits in network-structured populations. In biological terms, a set of mutants, each with fitness $m\\in(0,\\infty)$ invade a population of residents with fitness $1$. Each agent reproduces at a rate proportional to its fitness and each offspring replaces a random network neighbor. The process ends when the mutants either fixate (take over the whole population) or go extinct. The fixation probability measures the success of the invasion. To account for environmental heterogeneity, we study a generalization of the Standard process, called the Heterogeneous Moran process. Here, the fitness of each agent is determined both by its type (resident/mutant) and the node it occupies. We study the natural optimization problem of seed selection: given a budget $k$, which $k$ agents should initiate the mutant invasion to maximize the fixation probability? We show that the problem is strongly inapproximable: it is $\\mathbf{NP}$-hard to distinguish between maximum fixation probability 0 and 1. We then focus on mutant-biased networks, where each node exhibits at least as large mutant fitness as resident fitness. We show that the problem remains $\\mathbf{NP}$-hard, but the fixation probability becomes submodular, and thus the optimization problem admits a greedy $(1-1/e)$-approximation. An experimental evaluation of the greedy algorithm along with various heuristics on real-world data sets corroborates our results.","sentences":["The Moran process is a classic stochastic process that models the rise and takeover of novel traits in network-structured populations.","In biological terms, a set of mutants, each with fitness $m\\in(0,\\infty)$ invade a population of residents with fitness $1$. Each agent reproduces at a rate proportional to its fitness and each offspring replaces a random network neighbor.","The process ends when the mutants either fixate (take over the whole population) or go extinct.","The fixation probability measures the success of the invasion.","To account for environmental heterogeneity, we study a generalization of the Standard process, called the Heterogeneous Moran process.","Here, the fitness of each agent is determined both by its type (resident/mutant) and the node it occupies.","We study the natural optimization problem of seed selection: given a budget $k$, which $k$ agents should initiate the mutant invasion to maximize the fixation probability?","We show that the problem is strongly inapproximable: it is $\\mathbf{NP}$-hard to distinguish between maximum fixation probability 0 and 1.","We then focus on mutant-biased networks, where each node exhibits at least as large mutant fitness as resident fitness.","We show that the problem remains $\\mathbf{NP}$-hard, but the fixation probability becomes submodular, and thus the optimization problem admits a greedy $(1-1/e)$-approximation.","An experimental evaluation of the greedy algorithm along with various heuristics on real-world data sets corroborates our results."],"url":"http://arxiv.org/abs/2404.15986v1","category":"cs.DS"}
{"created":"2024-04-24 15:53:32","title":"Gravitational Faraday and spin-Hall effects of light: Local description","abstract":"A gravitational field can cause a rotation of the polarisation plane of light. This phenomenon is known as the gravitational Faraday effect. It arises due to different spin-orbit interaction of left- and right-handed circularly polarised components of light. Such an interaction also causes transverse displacement in the light trajectory, in opposite directions for each component. This phenomenon is known as the gravitational spin-Hall effect of light. We study these effects in a local inertial frame in arbitrary vacuum space-time and show that they are observer dependent and arise due to interaction of light polarisation with a local gravitomagnetic field measured by observer. Thus, to address the effects to a gravitational field alone, one has to consider zero angular momentum observers.","sentences":["A gravitational field can cause a rotation of the polarisation plane of light.","This phenomenon is known as the gravitational Faraday effect.","It arises due to different spin-orbit interaction of left- and right-handed circularly polarised components of light.","Such an interaction also causes transverse displacement in the light trajectory, in opposite directions for each component.","This phenomenon is known as the gravitational spin-Hall effect of light.","We study these effects in a local inertial frame in arbitrary vacuum space-time and show that they are observer dependent and arise due to interaction of light polarisation with a local gravitomagnetic field measured by observer.","Thus, to address the effects to a gravitational field alone, one has to consider zero angular momentum observers."],"url":"http://arxiv.org/abs/2404.15934v1","category":"gr-qc"}
{"created":"2024-04-24 15:44:34","title":"Symmetries of spatial correlators of light and heavy mesons in high temperature lattice QCD","abstract":"The spatial $z$-correlators of meson operators in $N_f=2+1+1$ lattice QCD with optimal domain-wall quarks at the physical point are studied for seven temperatures in the range of 190-1540 MeV. The meson operators include a complete set of Dirac bilinears (scalar, pseudoscalar, vector, axial vector, tensor vector, and axial-tensor vector), and each for six flavor combinations ($\\bar u d$, $\\bar u s$, $\\bar s s$, $\\bar u c$, $\\bar s c$, and $\\bar c c$). In Ref. \\cite{Chiu:2023hnm}, we focused on the meson correlators of $u$ and $d$ quarks, and discussed their implications for the effective restoration of $SU(2)_L \\times SU(2)_R$ and $U(1)_A$ chiral symmetries, as well as the emergence of approximate $SU(2)_{CS}$ chiral spin symmetry. In this work, we extend our study to meson correlators of six flavor contents, and first observe the hierarchical restoration of chiral symmetries in QCD, from $SU(2)_L \\times SU(2)_R \\times U(1)_A $ to $SU(3)_L \\times SU(3)_R \\times U(1)_A $, and to $SU(4)_L \\times SU(4)_R \\times U(1)_A $, as the temperature is increased from 190 MeV to 1540 MeV. Moreover, we compare the temperature windows for the emergence of the approximate $SU(2)_{CS}$ symmetry in light and heavy vector mesons, and find that the temperature windows are dominated by the $(\\bar u c, \\bar s c, \\bar c c)$ sectors.","sentences":["The spatial $z$-correlators of meson operators in $N_f=2+1+1$ lattice QCD with optimal domain-wall quarks at the physical point are studied for seven temperatures in the range of 190-1540 MeV.","The meson operators include a complete set of Dirac bilinears (scalar, pseudoscalar, vector, axial vector, tensor vector, and axial-tensor vector), and each for six flavor combinations ($\\bar u d$, $\\bar u s$, $\\bar s s$, $\\bar u c$, $\\bar s c$, and $\\bar c c$).","In Ref. \\cite{Chiu:2023hnm}, we focused on the meson correlators of $u$ and $d$ quarks, and discussed their implications for the effective restoration of $SU(2)_L \\times SU(2)_R$ and $U(1)_A$ chiral symmetries, as well as the emergence of approximate $SU(2)_{CS}$ chiral spin symmetry.","In this work, we extend our study to meson correlators of six flavor contents, and first observe the hierarchical restoration of chiral symmetries in QCD, from $SU(2)_L \\times SU(2)_R \\times U(1)_A $ to $SU(3)_L \\times SU(3)_R \\times U(1)_A $, and to $SU(4)_L \\times SU(4)_R \\times U(1)_A $, as the temperature is increased from 190 MeV to 1540 MeV.","Moreover, we compare the temperature windows for the emergence of the approximate $SU(2)_{CS}$ symmetry in light and heavy vector mesons, and find that the temperature windows are dominated by the $(\\bar u c, \\bar s c, \\bar c c)$ sectors."],"url":"http://arxiv.org/abs/2404.15932v1","category":"hep-lat"}
{"created":"2024-04-24 15:37:15","title":"Microwave Technologies in Experiments for Detection of Dark Matter Axions","abstract":"This article reviews different microwave technologies used in dark matter axion detection experiments with resonant cavities. The general concepts of the experiment are presented and ways to optimize the design parameters of microwave resonators are discussed. Additionally, different frequency tuning systems are described. Finally, research lines where microwave engineering can contribute to this kind of axion detection are presented.","sentences":["This article reviews different microwave technologies used in dark matter axion detection experiments with resonant cavities.","The general concepts of the experiment are presented and ways to optimize the design parameters of microwave resonators are discussed.","Additionally, different frequency tuning systems are described.","Finally, research lines where microwave engineering can contribute to this kind of axion detection are presented."],"url":"http://arxiv.org/abs/2404.15926v1","category":"physics.ins-det"}
{"created":"2024-04-24 15:12:20","title":"Hardness and Tight Approximations of Demand Strip Packing","abstract":"We settle the pseudo-polynomial complexity of the Demand Strip Packing (DSP) problem: Given a strip of fixed width and a set of items with widths and heights, the items must be placed inside the strip with the objective of minimizing the peak height. This problem has gained significant scientific interest due to its relevance in smart grids[Deppert et al.\\ APPROX'21, G\\'alvez et al.\\ APPROX'21]. Smart Grids are a modern form of electrical grid that provide opportunities for optimization. They are forecast to impact the future of energy provision significantly. Algorithms running in pseudo-polynomial time lend themselves to these applications as considered time intervals, such as days, are small. Moreover, such algorithms can provide superior approximation guarantees over those running in polynomial time. Consequently, they evoke scientific interest in related problems.   We prove that Demand Strip Packing is strongly NP-hard for approximation ratios below $5/4$. Through this proof, we provide novel insights into the relation of packing and scheduling problems. Using these insights, we show a series of frameworks that solve both Demand Strip Packing and Parallel Task Scheduling optimally when increasing the strip's width or number of machines. Such alterations to problems are known as resource augmentation. Applications are found when penalty costs are prohibitively large. Finally, we provide a pseudo-polynomial time approximation algorithm for DSP with an approximation ratio of $(5/4+\\varepsilon)$, which is nearly optimal assuming $P\\neq NP$. The construction of this algorithm provides several insights into the structure of DSP solutions and uses novel techniques to restructure optimal solutions.","sentences":["We settle the pseudo-polynomial complexity of the Demand Strip Packing (DSP) problem: Given a strip of fixed width and a set of items with widths and heights, the items must be placed inside the strip with the objective of minimizing the peak height.","This problem has gained significant scientific interest due to its relevance in smart grids[Deppert et al.\\ APPROX'21, G\\'alvez et al.\\","APPROX'21].","Smart Grids are a modern form of electrical grid that provide opportunities for optimization.","They are forecast to impact the future of energy provision significantly.","Algorithms running in pseudo-polynomial time lend themselves to these applications as considered time intervals, such as days, are small.","Moreover, such algorithms can provide superior approximation guarantees over those running in polynomial time.","Consequently, they evoke scientific interest in related problems.   ","We prove that Demand Strip Packing is strongly NP-hard for approximation ratios below $5/4$. Through this proof, we provide novel insights into the relation of packing and scheduling problems.","Using these insights, we show a series of frameworks that solve both Demand Strip Packing and Parallel Task Scheduling optimally when increasing the strip's width or number of machines.","Such alterations to problems are known as resource augmentation.","Applications are found when penalty costs are prohibitively large.","Finally, we provide a pseudo-polynomial time approximation algorithm for DSP with an approximation ratio of $(5/4+\\varepsilon)$, which is nearly optimal assuming $P\\neq NP$.","The construction of this algorithm provides several insights into the structure of DSP solutions and uses novel techniques to restructure optimal solutions."],"url":"http://arxiv.org/abs/2404.15917v1","category":"cs.DS"}
{"created":"2024-04-24 13:33:50","title":"A Generalization of Relative Entropy to Count Vectors and its Concentration Property","abstract":"We introduce a new generalization of relative entropy to non-negative vectors with sums $\\gt 1$. We show in a purely combinatorial setting, with no probabilistic considerations, that in the presence of linear constraints defining a convex polytope, a concentration phenomenon arises for this generalized relative entropy, and we quantify the concentration precisely. We also present a probabilistic formulation, and extend the concentration results to it. In addition, we provide a number of simplifications and improvements to our previous work, notably in dualizing the optimization problem, in the concentration with respect to $\\ell_{\\infty}$ distance, and in the relationship to generalized KL-divergence. A number of our results apply to general compact convex sets, not necessarily polyhedral.","sentences":["We introduce a new generalization of relative entropy to non-negative vectors with sums $\\gt 1$.","We show in a purely combinatorial setting, with no probabilistic considerations, that in the presence of linear constraints defining a convex polytope, a concentration phenomenon arises for this generalized relative entropy, and we quantify the concentration precisely.","We also present a probabilistic formulation, and extend the concentration results to it.","In addition, we provide a number of simplifications and improvements to our previous work, notably in dualizing the optimization problem, in the concentration with respect to $\\ell_{\\infty}$ distance, and in the relationship to generalized KL-divergence.","A number of our results apply to general compact convex sets, not necessarily polyhedral."],"url":"http://arxiv.org/abs/2404.15867v1","category":"cs.IT"}
{"created":"2024-04-24 13:13:22","title":"Optimizing Energy Efficiency of 5G RedCap Beam Management for Smart Agriculture Applications","abstract":"Beam management in 5G NR involves the transmission and reception of control signals such as Synchronization Signal Blocks (SSBs), crucial for tasks like initial access and/or channel estimation. However, this procedure consumes energy, which is particularly challenging to handle for battery-constrained nodes such as RedCap devices. Specifically, in this work we study a mid-market Internet of Things (IoT) Smart Agriculture (SmA) deployment where an Unmanned Autonomous Vehicle (UAV) acts as a base station \"from the sky\" (UAV-gNB) to monitor and control ground User Equipments (UEs) in the field. Then, we formalize a multi-variate optimization problem to determine the optimal beam management design for RedCap SmA devices in order to reduce the energy consumption at the UAV-gNB. Specifically, we jointly optimize the transmission power and the beamwidth at the UAV-gNB. Based on the analysis, we derive the so-called \"regions of feasibility,\" i.e., the upper limit(s) of the beam management parameters for which RedCap Quality of Service (QoS) and energy constraints are met. We study the impact of factors like the total transmission power at the gNB, the Signal-to-Noise Ratio (SNR) threshold for successful packet decoding, the number of UEs in the region, and the misdetection probability. Simulation results demonstrate that there exists an optimal configuration for beam management to promote energy efficiency, which depends on the speed of the UEs, the beamwidth, and other network parameters.","sentences":["Beam management in 5G NR involves the transmission and reception of control signals such as Synchronization Signal Blocks (SSBs), crucial for tasks like initial access and/or channel estimation.","However, this procedure consumes energy, which is particularly challenging to handle for battery-constrained nodes such as RedCap devices.","Specifically, in this work we study a mid-market Internet of Things (IoT) Smart Agriculture (SmA) deployment where an Unmanned Autonomous Vehicle (UAV) acts as a base station \"from the sky\" (UAV-gNB) to monitor and control ground User Equipments (UEs) in the field.","Then, we formalize a multi-variate optimization problem to determine the optimal beam management design for RedCap SmA devices in order to reduce the energy consumption at the UAV-gNB.","Specifically, we jointly optimize the transmission power and the beamwidth at the UAV-gNB.","Based on the analysis, we derive the so-called \"regions of feasibility,\" i.e., the upper limit(s) of the beam management parameters for which RedCap Quality of Service (QoS) and energy constraints are met.","We study the impact of factors like the total transmission power at the gNB, the Signal-to-Noise Ratio (SNR) threshold for successful packet decoding, the number of UEs in the region, and the misdetection probability.","Simulation results demonstrate that there exists an optimal configuration for beam management to promote energy efficiency, which depends on the speed of the UEs, the beamwidth, and other network parameters."],"url":"http://arxiv.org/abs/2404.15857v1","category":"cs.NI"}
{"created":"2024-04-24 12:52:43","title":"3D Freehand Ultrasound using Visual Inertial and Deep Inertial Odometry for Measuring Patellar Tracking","abstract":"Patellofemoral joint (PFJ) issues affect one in four people, with 20% experiencing chronic knee pain despite treatment. Poor outcomes and pain after knee replacement surgery are often linked to patellar mal-tracking. Traditional imaging methods like CT and MRI face challenges, including cost and metal artefacts, and there's currently no ideal way to observe joint motion without issues such as soft tissue artefacts or radiation exposure. A new system to monitor joint motion could significantly improve understanding of PFJ dynamics, aiding in better patient care and outcomes. Combining 2D ultrasound with motion tracking for 3D reconstruction of the joint using semantic segmentation and position registration can be a solution. However, the need for expensive external infrastructure to estimate the trajectories of the scanner remains the main limitation to implementing 3D bone reconstruction from handheld ultrasound scanning clinically. We proposed the Visual-Inertial Odometry (VIO) and the deep learning-based inertial-only odometry methods as alternatives to motion capture for tracking a handheld ultrasound scanner. The 3D reconstruction generated by these methods has demonstrated potential for assessing the PFJ and for further measurements from free-hand ultrasound scans. The results show that the VIO method performs as well as the motion capture method, with average reconstruction errors of 1.25 mm and 1.21 mm, respectively. The VIO method is the first infrastructure-free method for 3D reconstruction of bone from wireless handheld ultrasound scanning with an accuracy comparable to methods that require external infrastructure.","sentences":["Patellofemoral joint (PFJ) issues affect one in four people, with 20% experiencing chronic knee pain despite treatment.","Poor outcomes and pain after knee replacement surgery are often linked to patellar mal-tracking.","Traditional imaging methods like CT and MRI face challenges, including cost and metal artefacts, and there's currently no ideal way to observe joint motion without issues such as soft tissue artefacts or radiation exposure.","A new system to monitor joint motion could significantly improve understanding of PFJ dynamics, aiding in better patient care and outcomes.","Combining 2D ultrasound with motion tracking for 3D reconstruction of the joint using semantic segmentation and position registration can be a solution.","However, the need for expensive external infrastructure to estimate the trajectories of the scanner remains the main limitation to implementing 3D bone reconstruction from handheld ultrasound scanning clinically.","We proposed the Visual-Inertial Odometry (VIO) and the deep learning-based inertial-only odometry methods as alternatives to motion capture for tracking a handheld ultrasound scanner.","The 3D reconstruction generated by these methods has demonstrated potential for assessing the PFJ and for further measurements from free-hand ultrasound scans.","The results show that the VIO method performs as well as the motion capture method, with average reconstruction errors of 1.25 mm and 1.21 mm, respectively.","The VIO method is the first infrastructure-free method for 3D reconstruction of bone from wireless handheld ultrasound scanning with an accuracy comparable to methods that require external infrastructure."],"url":"http://arxiv.org/abs/2404.15847v1","category":"physics.med-ph"}
{"created":"2024-04-24 12:06:34","title":"SNR Maximization and Localization for UAV-IRS-Assisted Near-Field Systems","abstract":"This letter introduces a novel unmanned aerial vehicle (UAV)-intelligent reflecting surface (IRS) structure into near-field localization systems to enhance the design flexibility of IRS, thereby obtaining additional performance gains. Specifically, a UAV-IRS is utilized to improve the harsh wireless environment and provide localization possibilities. To improve the localization accuracy, a joint optimization problem considering UAV position and UAV-IRS passive beamforming is formulated to maximize the receiving signal-to-noise ratio (SNR). An alternative optimization algorithm is proposed to solve the complex non-convex problem leveraging the projected gradient ascent (PGA) algorithm and the principle of minimizing the phase difference of the receiving signals. Closed-form expressions for UAV-IRS phase shift are derived to reduce the algorithm complexity. In the simulations, the proposed algorithm is compared with three different schemes and outperforms the others in both receiving SNR and localization accuracy.","sentences":["This letter introduces a novel unmanned aerial vehicle (UAV)-intelligent reflecting surface (IRS) structure into near-field localization systems to enhance the design flexibility of IRS, thereby obtaining additional performance gains.","Specifically, a UAV-IRS is utilized to improve the harsh wireless environment and provide localization possibilities.","To improve the localization accuracy, a joint optimization problem considering UAV position and UAV-IRS passive beamforming is formulated to maximize the receiving signal-to-noise ratio (SNR).","An alternative optimization algorithm is proposed to solve the complex non-convex problem leveraging the projected gradient ascent (PGA) algorithm and the principle of minimizing the phase difference of the receiving signals.","Closed-form expressions for UAV-IRS phase shift are derived to reduce the algorithm complexity.","In the simulations, the proposed algorithm is compared with three different schemes and outperforms the others in both receiving SNR and localization accuracy."],"url":"http://arxiv.org/abs/2404.15830v1","category":"eess.SP"}
{"created":"2024-04-24 12:05:10","title":"Robust Quantum Gate Complexity: Foundations","abstract":"Optimal control of closed quantum systems is a well studied geometrically elegant set of computational theory and techniques that have proven pivotal in the implementation and understanding of quantum computers. The design of a circuit itself corresponds to an optimal control problem of choosing the appropriate set of gates (which appear as control operands) in order to steer a qubit from an initial, easily prepared state, to one that is informative to the user in some sense, for e.g., an oracle whose evaluation is part of the circuit. However, contemporary devices are known to be noisy, and it is not certain that a circuit will behave as intended. Yet, although the computational tools exist in broader optimal control theory, robustness of adequate operation of a quantum control system with respect to uncertainty and errors has not yet been broadly studied in the literature. In this paper, we propose a new approach inspired by the closed quantum optimal control and its connection to geometric interpretations. To this end, we present the appropriate problem definitions of robustness in the context of quantum control, focusing on its broader implications for gate complexity.","sentences":["Optimal control of closed quantum systems is a well studied geometrically elegant set of computational theory and techniques that have proven pivotal in the implementation and understanding of quantum computers.","The design of a circuit itself corresponds to an optimal control problem of choosing the appropriate set of gates (which appear as control operands) in order to steer a qubit from an initial, easily prepared state, to one that is informative to the user in some sense, for e.g., an oracle whose evaluation is part of the circuit.","However, contemporary devices are known to be noisy, and it is not certain that a circuit will behave as intended.","Yet, although the computational tools exist in broader optimal control theory, robustness of adequate operation of a quantum control system with respect to uncertainty and errors has not yet been broadly studied in the literature.","In this paper, we propose a new approach inspired by the closed quantum optimal control and its connection to geometric interpretations.","To this end, we present the appropriate problem definitions of robustness in the context of quantum control, focusing on its broader implications for gate complexity."],"url":"http://arxiv.org/abs/2404.15828v1","category":"quant-ph"}
{"created":"2024-04-24 09:57:11","title":"BASS: Batched Attention-optimized Speculative Sampling","abstract":"Speculative decoding has emerged as a powerful method to improve latency and throughput in hosting large language models. However, most existing implementations focus on generating a single sequence. Real-world generative AI applications often require multiple responses and how to perform speculative decoding in a batched setting while preserving its latency benefits poses non-trivial challenges. This paper describes a system of batched speculative decoding that sets a new state of the art in multi-sequence generation latency and that demonstrates superior GPU utilization as well as quality of generations within a time budget. For example, for a 7.8B-size model on a single A100 GPU and with a batch size of 8, each sequence is generated at an average speed of 5.8ms per token, the overall throughput being 1.1K tokens per second. These results represent state-of-the-art latency and a 2.15X speed-up over optimized regular decoding. Within a time budget that regular decoding does not finish, our system is able to generate sequences with HumanEval Pass@First of 43% and Pass@All of 61%, far exceeding what's feasible with single-sequence speculative decoding. Our peak GPU utilization during decoding reaches as high as 15.8%, more than 3X the highest of that of regular decoding and around 10X of single-sequence speculative decoding.","sentences":["Speculative decoding has emerged as a powerful method to improve latency and throughput in hosting large language models.","However, most existing implementations focus on generating a single sequence.","Real-world generative AI applications often require multiple responses and how to perform speculative decoding in a batched setting while preserving its latency benefits poses non-trivial challenges.","This paper describes a system of batched speculative decoding that sets a new state of the art in multi-sequence generation latency and that demonstrates superior GPU utilization as well as quality of generations within a time budget.","For example, for a 7.8B-size model on a single A100 GPU and with a batch size of 8, each sequence is generated at an average speed of 5.8ms per token, the overall throughput being 1.1K tokens per second.","These results represent state-of-the-art latency and a 2.15X speed-up over optimized regular decoding.","Within a time budget that regular decoding does not finish, our system is able to generate sequences with HumanEval Pass@First of 43% and Pass@All of 61%, far exceeding what's feasible with single-sequence speculative decoding.","Our peak GPU utilization during decoding reaches as high as 15.8%, more than 3X the highest of that of regular decoding and around 10X of single-sequence speculative decoding."],"url":"http://arxiv.org/abs/2404.15778v1","category":"cs.LG"}
{"created":"2024-04-24 09:43:34","title":"Computational Design of Boron-Free Triangular Molecules with Inverted Singlet-Triplet Energy Gap","abstract":"A novel, computationally designed, class of triangular-shape organic molecules with an inverted singlet-triplet (IST) energy gap is investigated with the aid of ab initio methods of electronic structure theory. The considered molecular systems have a form of cyclic oligomers and their common feature is electronic conjugation localized along the molecular rim. Analysis of vertical transition energies from the electronic ground state, as well as from the lowest excited singlet and triplet states of selected molecules, is conducted. The results underscore the significance of optimizing excited-state geometries in theoretical models to accurately describe the optoelectronic properties of the IST molecules, particularly in relation to their applications in OLEDs.","sentences":["A novel, computationally designed, class of triangular-shape organic molecules with an inverted singlet-triplet (IST) energy gap is investigated with the aid of ab initio methods of electronic structure theory.","The considered molecular systems have a form of cyclic oligomers and their common feature is electronic conjugation localized along the molecular rim.","Analysis of vertical transition energies from the electronic ground state, as well as from the lowest excited singlet and triplet states of selected molecules, is conducted.","The results underscore the significance of optimizing excited-state geometries in theoretical models to accurately describe the optoelectronic properties of the IST molecules, particularly in relation to their applications in OLEDs."],"url":"http://arxiv.org/abs/2404.15768v1","category":"physics.chem-ph"}
{"created":"2024-04-24 09:37:22","title":"3D Face Morphing Attack Generation using Non-Rigid Registration","abstract":"Face Recognition Systems (FRS) are widely used in commercial environments, such as e-commerce and e-banking, owing to their high accuracy in real-world conditions. However, these systems are vulnerable to facial morphing attacks, which are generated by blending face color images of different subjects. This paper presents a new method for generating 3D face morphs from two bona fide point clouds. The proposed method first selects bona fide point clouds with neutral expressions. The two input point clouds were then registered using a Bayesian Coherent Point Drift (BCPD) without optimization, and the geometry and color of the registered point clouds were averaged to generate a face morphing point cloud. The proposed method generates 388 face-morphing point clouds from 200 bona fide subjects. The effectiveness of the method was demonstrated through extensive vulnerability experiments, achieving a Generalized Morphing Attack Potential (G-MAP) of 97.93%, which is superior to the existing state-of-the-art (SOTA) with a G-MAP of 81.61%.","sentences":["Face Recognition Systems (FRS) are widely used in commercial environments, such as e-commerce and e-banking, owing to their high accuracy in real-world conditions.","However, these systems are vulnerable to facial morphing attacks, which are generated by blending face color images of different subjects.","This paper presents a new method for generating 3D face morphs from two bona fide point clouds.","The proposed method first selects bona fide point clouds with neutral expressions.","The two input point clouds were then registered using a Bayesian Coherent Point Drift (BCPD) without optimization, and the geometry and color of the registered point clouds were averaged to generate a face morphing point cloud.","The proposed method generates 388 face-morphing point clouds from 200 bona fide subjects.","The effectiveness of the method was demonstrated through extensive vulnerability experiments, achieving a Generalized Morphing Attack Potential (G-MAP) of 97.93%, which is superior to the existing state-of-the-art (SOTA) with a G-MAP of 81.61%."],"url":"http://arxiv.org/abs/2404.15765v1","category":"cs.CV"}
{"created":"2024-04-24 09:10:51","title":"A Reconfigurable Subarray Architecture and Hybrid Beamforming for Millimeter-Wave Dual-Function-Radar-Communication Systems","abstract":"Dual-function-radar-communication (DFRC) is a promising candidate technology for next-generation networks. By integrating hybrid analog-digital (HAD) beamforming into a multi-user millimeter-wave (mmWave) DFRC system, we design a new reconfigurable subarray (RS) architecture and jointly optimize the HAD beamforming to maximize the communication sum-rate and ensure a prescribed signal-to-clutter-plus-noise ratio for radar sensing. Considering the non-convexity of this problem arising from multiplicative coupling of the analog and digital beamforming, we convert the sum-rate maximization into an equivalent weighted mean-square error minimization and apply penalty dual decomposition to decouple the analog and digital beamforming. Specifically, a second-order cone program is first constructed to optimize the fully digital counterpart of the HAD beamforming. Then, the sparsity of the RS architecture is exploited to obtain a low-complexity solution for the HAD beamforming. The convergence and complexity analyses of our algorithm are carried out under the RS architecture. Simulations corroborate that, with the RS architecture, DFRC offers effective communication and sensing and improves energy efficiency by 83.4% and 114.2% with a moderate number of radio frequency chains and phase shifters, compared to the persistently- and fullyconnected architectures, respectively.","sentences":["Dual-function-radar-communication (DFRC) is a promising candidate technology for next-generation networks.","By integrating hybrid analog-digital (HAD) beamforming into a multi-user millimeter-wave (mmWave) DFRC system, we design a new reconfigurable subarray (RS) architecture and jointly optimize the HAD beamforming to maximize the communication sum-rate and ensure a prescribed signal-to-clutter-plus-noise ratio for radar sensing.","Considering the non-convexity of this problem arising from multiplicative coupling of the analog and digital beamforming, we convert the sum-rate maximization into an equivalent weighted mean-square error minimization and apply penalty dual decomposition to decouple the analog and digital beamforming.","Specifically, a second-order cone program is first constructed to optimize the fully digital counterpart of the HAD beamforming.","Then, the sparsity of the RS architecture is exploited to obtain a low-complexity solution for the HAD beamforming.","The convergence and complexity analyses of our algorithm are carried out under the RS architecture.","Simulations corroborate that, with the RS architecture, DFRC offers effective communication and sensing and improves energy efficiency by 83.4% and 114.2% with a moderate number of radio frequency chains and phase shifters, compared to the persistently- and fullyconnected architectures, respectively."],"url":"http://arxiv.org/abs/2404.15750v1","category":"eess.SP"}
{"created":"2024-04-24 08:46:25","title":"Fine-grained Spatial-temporal MLP Architecture for Metro Origin-Destination Prediction","abstract":"Accurate prediction of metro traffic is crucial for optimizing metro scheduling and enhancing overall transport efficiency. Analyzing fine-grained and comprehensive relations among stations effectively is imperative for metro Origin-Destination (OD) prediction. However, existing metro OD models either mix information from multiple OD pairs from the station's perspective or exclusively focus on a subset of OD pairs. These approaches may overlook fine-grained relations among OD pairs, leading to difficulties in predicting potential anomalous conditions. To address these challenges, we analyze traffic variations from the perspective of all OD pairs and propose a fine-grained spatial-temporal MLP architecture for metro OD prediction, namely ODMixer. Specifically, our ODMixer has double-branch structure and involves the Channel Mixer, the Multi-view Mixer, and the Bidirectional Trend Learner. The Channel Mixer aims to capture short-term temporal relations among OD pairs, the Multi-view Mixer concentrates on capturing relations from both origin and destination perspectives. To model long-term temporal relations, we introduce the Bidirectional Trend Learner. Extensive experiments on two large-scale metro OD prediction datasets HZMOD and SHMO demonstrate the advantages of our ODMixer. The code will be available.","sentences":["Accurate prediction of metro traffic is crucial for optimizing metro scheduling and enhancing overall transport efficiency.","Analyzing fine-grained and comprehensive relations among stations effectively is imperative for metro Origin-Destination (OD) prediction.","However, existing metro OD models either mix information from multiple OD pairs from the station's perspective or exclusively focus on a subset of OD pairs.","These approaches may overlook fine-grained relations among OD pairs, leading to difficulties in predicting potential anomalous conditions.","To address these challenges, we analyze traffic variations from the perspective of all OD pairs and propose a fine-grained spatial-temporal MLP architecture for metro OD prediction, namely ODMixer.","Specifically, our ODMixer has double-branch structure and involves the Channel Mixer, the Multi-view Mixer, and the Bidirectional Trend Learner.","The Channel Mixer aims to capture short-term temporal relations among OD pairs, the Multi-view Mixer concentrates on capturing relations from both origin and destination perspectives.","To model long-term temporal relations, we introduce the Bidirectional Trend Learner.","Extensive experiments on two large-scale metro OD prediction datasets HZMOD and SHMO demonstrate the advantages of our ODMixer.","The code will be available."],"url":"http://arxiv.org/abs/2404.15734v1","category":"cs.CV"}
{"created":"2024-04-24 08:41:35","title":"BlissCam: Boosting Eye Tracking Efficiency with Learned In-Sensor Sparse Sampling","abstract":"Eye tracking is becoming an increasingly important task domain in emerging computing platforms such as Augmented/Virtual Reality (AR/VR). Today's eye tracking system suffers from long end-to-end tracking latency and can easily eat up half of the power budget of a mobile VR device. Most existing optimization efforts exclusively focus on the computation pipeline by optimizing the algorithm and/or designing dedicated accelerators while largely ignoring the front-end of any eye tracking pipeline: the image sensor. This paper makes a case for co-designing the imaging system with the computing system. In particular, we propose the notion of \"in-sensor sparse sampling\", whereby the pixels are drastically downsampled (by 20x) within the sensor. Such in-sensor sampling enhances the overall tracking efficiency by significantly reducing 1) the power consumption of the sensor readout chain and sensor-host communication interfaces, two major power contributors, and 2) the work done on the host, which receives and operates on far fewer pixels. With careful reuse of existing pixel circuitry, our proposed BLISSCAM requires little hardware augmentation to support the in-sensor operations. Our synthesis results show up to 8.2x energy reduction and 1.4x latency reduction over existing eye tracking pipelines.","sentences":["Eye tracking is becoming an increasingly important task domain in emerging computing platforms such as Augmented/Virtual Reality (AR/VR).","Today's eye tracking system suffers from long end-to-end tracking latency and can easily eat up half of the power budget of a mobile VR device.","Most existing optimization efforts exclusively focus on the computation pipeline by optimizing the algorithm and/or designing dedicated accelerators while largely ignoring the front-end of any eye tracking pipeline: the image sensor.","This paper makes a case for co-designing the imaging system with the computing system.","In particular, we propose the notion of \"in-sensor sparse sampling\", whereby the pixels are drastically downsampled (by 20x) within the sensor.","Such in-sensor sampling enhances the overall tracking efficiency by significantly reducing 1) the power consumption of the sensor readout chain and sensor-host communication interfaces, two major power contributors, and 2) the work done on the host, which receives and operates on far fewer pixels.","With careful reuse of existing pixel circuitry, our proposed BLISSCAM requires little hardware augmentation to support the in-sensor operations.","Our synthesis results show up to 8.2x energy reduction and 1.4x latency reduction over existing eye tracking pipelines."],"url":"http://arxiv.org/abs/2404.15733v1","category":"cs.AR"}
{"created":"2024-04-25 17:57:09","title":"Quantum effects on the evaporation of PBHs: contributions to dark matter","abstract":"We compute the relic abundance of dark matter in the presence of Primordial Black Holes (PBHs) beyond the semiclassical approximation. We take into account the quantum corrections due to the memory burden effect, which is assumed to suppress the black hole evaporation rate by the inverse power of its own entropy. Such quantum effect significantly enhances the lifetime, rendering the possibility of PBH mass $\\lesssim 10^{9}$ g being the sole dark matter (DM) candidate. However, Nature can not rule out the existence of fundamental particles such as DM. We, therefore, include the possibility of populating the dark sector by the decay of PBHs to those fundamental particles, adding the contribution to stable PBH whose lifetime is extended due to the quantum corrections. Depending on the strength of the burden effect, we show that a wide range of parameter space opens up in the initial PBH mass and fundamental dark matter mass plane that respects the correct relic abundance.","sentences":["We compute the relic abundance of dark matter in the presence of Primordial Black Holes (PBHs) beyond the semiclassical approximation.","We take into account the quantum corrections due to the memory burden effect, which is assumed to suppress the black hole evaporation rate by the inverse power of its own entropy.","Such quantum effect significantly enhances the lifetime, rendering the possibility of PBH mass $\\lesssim 10^{9}$ g being the sole dark matter (DM) candidate.","However, Nature can not rule out the existence of fundamental particles such as DM.","We, therefore, include the possibility of populating the dark sector by the decay of PBHs to those fundamental particles, adding the contribution to stable PBH whose lifetime is extended due to the quantum corrections.","Depending on the strength of the burden effect, we show that a wide range of parameter space opens up in the initial PBH mass and fundamental dark matter mass plane that respects the correct relic abundance."],"url":"http://arxiv.org/abs/2404.16815v1","category":"hep-ph"}
{"created":"2024-04-25 17:33:43","title":"A note on the order of the Tate--Shafarevich group modulo squares","abstract":"We investigate the order of the Tate--Shafarevich group of abelian varieties modulo rational squares. Our main result shows that every square-free natural number appears as the non square-free part of the Tate--Shafarevich group of some abelian variety, thereby validating a conjecture of W. Stein.","sentences":["We investigate the order of the Tate--Shafarevich group of abelian varieties modulo rational squares.","Our main result shows that every square-free natural number appears as the non square-free part of the Tate--Shafarevich group of some abelian variety, thereby validating a conjecture of W. Stein."],"url":"http://arxiv.org/abs/2404.16785v1","category":"math.NT"}
{"created":"2024-04-25 17:27:02","title":"Subset SSD for enhanced indexation with sector constraints","abstract":"In this paper we apply second order stochastic dominance (SSD) to the problem of enhanced indexation with asset subset (sector) constraints. The problem we consider is how to construct a portfolio that is designed to outperform a given market index whilst having regard to the proportion of the portfolio invested in distinct market sectors. In our approach, subset SSD, the portfolio associated with each sector is treated in a SSD manner. In other words in subset SSD we actively try to find sector portfolios that SSD dominate their respective sector indices. However the proportion of the overall portfolio invested in each sector is not pre-specified, rather it is decided via optimisation. Computational results are given for our approach as applied to the S\\&P~500 over the period $29^{\\text{th}}$ August 2018 to $29^{\\text{th}}$ December 2023. This period, over 5 years, includes the Covid pandemic, which had a significant effect on stock prices. Our results indicate that the scaled version of our subset SSD approach significantly outperforms the S\\&P~500 over the period considered. Our approach also outperforms the standard SSD based approach to the problem.","sentences":["In this paper we apply second order stochastic dominance (SSD) to the problem of enhanced indexation with asset subset (sector) constraints.","The problem we consider is how to construct a portfolio that is designed to outperform a given market index whilst having regard to the proportion of the portfolio invested in distinct market sectors.","In our approach, subset SSD, the portfolio associated with each sector is treated in a SSD manner.","In other words in subset SSD we actively try to find sector portfolios that SSD dominate their respective sector indices.","However the proportion of the overall portfolio invested in each sector is not pre-specified, rather it is decided via optimisation.","Computational results are given for our approach as applied to the S\\&P~500 over the period $29^{\\text{th}}$ August 2018 to $29^{\\text{th}}$ December 2023.","This period, over 5 years, includes the Covid pandemic, which had a significant effect on stock prices.","Our results indicate that the scaled version of our subset SSD approach significantly outperforms the S\\&P~500 over the period considered.","Our approach also outperforms the standard SSD based approach to the problem."],"url":"http://arxiv.org/abs/2404.16777v1","category":"q-fin.CP"}
{"created":"2024-04-25 17:26:59","title":"Modeling Selective Feature Attention for Representation-based Siamese Text Matching","abstract":"Representation-based Siamese networks have risen to popularity in lightweight text matching due to their low deployment and inference costs. While word-level attention mechanisms have been implemented within Siamese networks to improve performance, we propose Feature Attention (FA), a novel downstream block designed to enrich the modeling of dependencies among embedding features. Employing \"squeeze-and-excitation\" techniques, the FA block dynamically adjusts the emphasis on individual features, enabling the network to concentrate more on features that significantly contribute to the final classification. Building upon FA, we introduce a dynamic \"selection\" mechanism called Selective Feature Attention (SFA), which leverages a stacked BiGRU Inception structure. The SFA block facilitates multi-scale semantic extraction by traversing different stacked BiGRU layers, encouraging the network to selectively concentrate on semantic information and embedding features across varying levels of abstraction. Both the FA and SFA blocks offer a seamless integration capability with various Siamese networks, showcasing a plug-and-play characteristic. Experimental evaluations conducted across diverse text matching baselines and benchmarks underscore the indispensability of modeling feature attention and the superiority of the \"selection\" mechanism.","sentences":["Representation-based Siamese networks have risen to popularity in lightweight text matching due to their low deployment and inference costs.","While word-level attention mechanisms have been implemented within Siamese networks to improve performance, we propose Feature Attention (FA), a novel downstream block designed to enrich the modeling of dependencies among embedding features.","Employing \"squeeze-and-excitation\" techniques, the FA block dynamically adjusts the emphasis on individual features, enabling the network to concentrate more on features that significantly contribute to the final classification.","Building upon FA, we introduce a dynamic \"selection\" mechanism called Selective Feature Attention (SFA), which leverages a stacked BiGRU Inception structure.","The SFA block facilitates multi-scale semantic extraction by traversing different stacked BiGRU layers, encouraging the network to selectively concentrate on semantic information and embedding features across varying levels of abstraction.","Both the FA and SFA blocks offer a seamless integration capability with various Siamese networks, showcasing a plug-and-play characteristic.","Experimental evaluations conducted across diverse text matching baselines and benchmarks underscore the indispensability of modeling feature attention and the superiority of the \"selection\" mechanism."],"url":"http://arxiv.org/abs/2404.16776v1","category":"cs.CL"}
{"created":"2024-04-25 17:24:12","title":"$R_{D^{(*)}}$ and survival of the fittest scalar leptoquark","abstract":"Motivated by the long-standing discrepancy in lepton flavor universality ratios $R_D$ and $R_{D^{\\ast}}$ we assess the status of scalar leptoquark states $R_2$, $\\widetilde R_2$ and $S_1$ which can in principle provide a desired enhancement of $\\mathcal{B}(B\\to D^{(\\ast )}\\tau \\nu)$ in a minimal setup with two Yukawa couplings only. We consider unavoidable low-energy constraints, $Z$-pole measurements as well as high-$p_T$ constraints. After setting mass of each leptoquark to $1.5$ TeV we find that of all considered states only $S_1$ leptoquark, coupled to both chiralities of leptons and quarks, is still a completely viable solution while the scenario with $R_2$ is in growing tension with $\\Gamma(Z \\to \\tau \\tau)$ and with the LHC constraints on the di-tau tails at high-$p_T$. We comment on the future experimental tests of $S_1$ scenario.","sentences":["Motivated by the long-standing discrepancy in lepton flavor universality ratios $R_D$ and $R_{D^{\\ast}}$ we assess the status of scalar leptoquark states $R_2$, $\\widetilde R_2$ and $S_1$ which can in principle provide a desired enhancement of $\\mathcal{B}(B\\to D^{(\\ast )}\\tau \\nu)$ in a minimal setup with two Yukawa couplings only.","We consider unavoidable low-energy constraints, $Z$-pole measurements as well as high-$p_T$ constraints.","After setting mass of each leptoquark to $1.5$ TeV we find that of all considered states only $S_1$ leptoquark, coupled to both chiralities of leptons and quarks, is still a completely viable solution while the scenario with $R_2$ is in growing tension with $\\Gamma(Z \\to \\tau \\tau)$ and with the LHC constraints on the di-tau tails at high-$p_T$. We comment on the future experimental tests of $S_1$ scenario."],"url":"http://arxiv.org/abs/2404.16772v1","category":"hep-ph"}
{"created":"2024-04-25 17:16:41","title":"Dichalcogenides and difulfides nanostructures for hydrogen storage","abstract":"Hydrogen energy is a high-efficiency and clean energy. Large-surface-area, two-dimensional (2D) layered materials are expected to have an advantage in hydrogen storage applications. Among a large number of 2D materials, monolayer dichacogenides have emerged as promising candidate for hydrogen clean energy. In the present work, first-principles calculations and molecular dynamics simulations are carried out to investigate the adsorption behaviors of hydrogen molecules on transition metal dichalcogenides and disulphides. Furthermore we propose novel structures which may suitable for hydrogen storage. As the concentration increaes, 2D shows thermodynamic stability even at room temperature, suggesting a possible aplicability for hydrogen storage.","sentences":["Hydrogen energy is a high-efficiency and clean energy.","Large-surface-area, two-dimensional (2D) layered materials are expected to have an advantage in hydrogen storage applications.","Among a large number of 2D materials, monolayer dichacogenides have emerged as promising candidate for hydrogen clean energy.","In the present work, first-principles calculations and molecular dynamics simulations are carried out to investigate the adsorption behaviors of hydrogen molecules on transition metal dichalcogenides and disulphides.","Furthermore we propose novel structures which may suitable for hydrogen storage.","As the concentration increaes, 2D shows thermodynamic stability even at room temperature, suggesting a possible aplicability for hydrogen storage."],"url":"http://arxiv.org/abs/2404.16761v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-25 17:11:52","title":"Links and the Diaconis-Graham Inequality","abstract":"In 1977 Diaconis and Graham proved two inequalities relating different measures of disarray in permutations, and asked for a characterization of those permutations for which equality holds in one of these inequalities. Such a characterization was first given in 2013. Recently, another characterization was given by Woo, using a topological link in $\\mathbb R^3$ that can be associated to the cycle diagram of a permutation. We show that Woo's characterization extends much further: for any permutation, the discrepancy in Diaconis and Graham's inequality is directly related to the Euler characteristic of the associated link. This connection provides a new proof of the original result of Diaconis and Graham. We also characterize permutations with a fixed discrepancy in terms of their associated links and find that the stabilized-interval-free permutations are precisely those whose associated links are nonsplit.","sentences":["In 1977 Diaconis and Graham proved two inequalities relating different measures of disarray in permutations, and asked for a characterization of those permutations for which equality holds in one of these inequalities.","Such a characterization was first given in 2013.","Recently, another characterization was given by Woo, using a topological link in $\\mathbb R^3$ that can be associated to the cycle diagram of a permutation.","We show that Woo's characterization extends much further: for any permutation, the discrepancy in Diaconis and Graham's inequality is directly related to the Euler characteristic of the associated link.","This connection provides a new proof of the original result of Diaconis and Graham.","We also characterize permutations with a fixed discrepancy in terms of their associated links and find that the stabilized-interval-free permutations are precisely those whose associated links are nonsplit."],"url":"http://arxiv.org/abs/2404.16755v1","category":"math.CO"}
{"created":"2024-04-25 16:50:44","title":"Calculable neutrino Dirac mass matrix and one-loop $\\bar \u03b8$ in the minimal left-right symmetric model","abstract":"We revisit the contribution to the strong CP parameter $\\bar \\theta$ from leptonic CP violation at one-loop level in the minimal left-right symmetric model in the case of parity as the left-right symmetry. The Hermitian neutrino Dirac mass matrix $M_D$ can be calculated using the light and heavy neutrino masses and mixings. We propose a parameterization of the right-handed neutrino mixing matrix $V_R$ and construct the heavy neutrino mass that maintains the Hermiticity of $M_D$. We further apply it to evaluate the one-loop $\\bar\\theta$, denoted as $\\bar \\theta_{loop}$, as a function of the sterile neutrino masses for explicit examples of $V_R$. By requiring the magnitude of $\\bar \\theta_{loop}\\lesssim 10^{-10}$, we derive the upper limits on the sterile neutrino masses, which are within reach of direct searches at the Large Hadron Collider and neutrinoless double beta decay experiments. Furthermore, our parameterization is applicable to other phenomenological studies.","sentences":["We revisit the contribution to the strong CP parameter $\\bar \\theta$ from leptonic CP violation at one-loop level in the minimal left-right symmetric model in the case of parity as the left-right symmetry.","The Hermitian neutrino Dirac mass matrix $M_D$ can be calculated using the light and heavy neutrino masses and mixings.","We propose a parameterization of the right-handed neutrino mixing matrix $V_R$ and construct the heavy neutrino mass that maintains the Hermiticity of $M_D$. We further apply it to evaluate the one-loop $\\bar\\theta$, denoted as $\\bar \\theta_{loop}$, as a function of the sterile neutrino masses for explicit examples of $V_R$. By requiring the magnitude of $\\bar \\theta_{loop}\\lesssim 10^{-10}$, we derive the upper limits on the sterile neutrino masses, which are within reach of direct searches at the Large Hadron Collider and neutrinoless double beta decay experiments.","Furthermore, our parameterization is applicable to other phenomenological studies."],"url":"http://arxiv.org/abs/2404.16740v1","category":"hep-ph"}
{"created":"2024-04-25 16:47:34","title":"Open Source Software (OSS) Transparency for DoD Acquisition","abstract":"Caveat emptor, or let the buyer beware, is commonly attributed to open source software (OSS)-the onus is on the OSS consumer to ensure that it is fit for use in the consumer's context. OSS has been compared to an open market bazaar where consumers are free to browse all the source code and take a copy. In this paper, we observe challenges for the OSS consumer to obtain information about the process(es), project(s) used to produce a product and the protection(s) employed by those projects. We discuss the need for more transparency by OSS projects, where possible and introduce a framework for reasoning about those OSS projects and their products for use by the OSS consumer.","sentences":["Caveat emptor, or let the buyer beware, is commonly attributed to open source software (OSS)-the onus is on the OSS consumer to ensure that it is fit for use in the consumer's context.","OSS has been compared to an open market bazaar where consumers are free to browse all the source code and take a copy.","In this paper, we observe challenges for the OSS consumer to obtain information about the process(es), project(s) used to produce a product and the protection(s) employed by those projects.","We discuss the need for more transparency by OSS projects, where possible and introduce a framework for reasoning about those OSS projects and their products for use by the OSS consumer."],"url":"http://arxiv.org/abs/2404.16737v1","category":"cs.SE"}
{"created":"2024-04-25 16:24:17","title":"Log-normal glide and the formation of misfit dislocation networks in heteroepitaxial ZnS on GaP","abstract":"Scanning electron microscopy (SEM) based electron channeling contrast imaging (ECCI) is used to observe and quantify misfit dislocation (MD) networks formed at the heteroepitaxial interface between ZnS and GaP grown by molecular beam epitaxy (MBE). Below a critical thickness of 15-20 nm, no MDs are observed. However, crystallographic features with strong dipole contrast, consistent with unexpanded dislocation half-loops, are observed prior to the formation of visible interfacial MD segments and any notable strain relaxation. At higher film thicknesses (20 to 50 nm), interfacial MD lengths increase anisotropically in the two orthogonal in-plane <110> line directions, threading dislocation (TD) density increases, and a roughening transition is observed from atomically smooth two-dimensional (2D) to a multi-stepped three-dimensional (3D) morphology, providing evidence for step edge pinning via surface terminating dislocations. The ZnS strain relaxation, calculated from the total MD content observed via ECCI, matches the average strain relaxation measured by high-resolution x-ray diffraction (HRXRD). The MD lengths are found to follow a log-normal distribution, indicating that the combined MD nucleation and TD glide processes must have a normal distribution of activation energies. The estimated TD glide velocity ($v_{g}$) along [$\\bar{1}$10] is almost twice that along [110], but in both directions shows a maximum as a function of film thickness, indicating an initial burst of plasticity followed by dislocation pinning.","sentences":["Scanning electron microscopy (SEM) based electron channeling contrast imaging (ECCI) is used to observe and quantify misfit dislocation (MD) networks formed at the heteroepitaxial interface between ZnS and GaP grown by molecular beam epitaxy (MBE).","Below a critical thickness of 15-20 nm, no MDs are observed.","However, crystallographic features with strong dipole contrast, consistent with unexpanded dislocation half-loops, are observed prior to the formation of visible interfacial MD segments and any notable strain relaxation.","At higher film thicknesses (20 to 50 nm), interfacial MD lengths increase anisotropically in the two orthogonal in-plane <110> line directions, threading dislocation (TD) density increases, and a roughening transition is observed from atomically smooth two-dimensional (2D) to a multi-stepped three-dimensional (3D) morphology, providing evidence for step edge pinning via surface terminating dislocations.","The ZnS strain relaxation, calculated from the total MD content observed via ECCI, matches the average strain relaxation measured by high-resolution x-ray diffraction (HRXRD).","The MD lengths are found to follow a log-normal distribution, indicating that the combined MD nucleation and TD glide processes must have a normal distribution of activation energies.","The estimated TD glide velocity ($v_{g}$) along [$\\bar{1}$10] is almost twice that along [110], but in both directions shows a maximum as a function of film thickness, indicating an initial burst of plasticity followed by dislocation pinning."],"url":"http://arxiv.org/abs/2404.16714v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-25 15:51:29","title":"Deep Hard X-ray Survey of the M81 Field Based on INTEGRAL Data","abstract":"We have carried out a deep survey of the M81 field in the 25-60 keV energy band based on long-term (2003-2023) INTEGRAL observations. A record sensitivity of 0.16 mCrab at a detection significance of 4 sigma has been achieved in the central part of the field owing to the long accumulated exposure (19.2 Ms). The total area of the survey is 1004 deg^2 at a sensitivity level better than 0.72 mCrab. We have produced a catalog of sources detected at a significance level higher than 4 sigma. It contains 51 objects most of which are active galactic nuclei (AGNs). The median redshift of the Seyfert galaxies in the catalog is z=0.0366. Six sources have not been detected previously in any of the X-ray surveys. According to the available indirect data, all of them and two more sources that have already been entered previously into the INTEGRAL survey catalogs can also be AGNs, including those with strong internal absorption.","sentences":["We have carried out a deep survey of the M81 field in the 25-60 keV energy band based on long-term (2003-2023) INTEGRAL observations.","A record sensitivity of 0.16 mCrab at a detection significance of 4 sigma has been achieved in the central part of the field owing to the long accumulated exposure (19.2 Ms).","The total area of the survey is 1004 deg^2 at a sensitivity level better than 0.72 mCrab.","We have produced a catalog of sources detected at a significance level higher than 4 sigma.","It contains 51 objects most of which are active galactic nuclei (AGNs).","The median redshift of the Seyfert galaxies in the catalog is z=0.0366.","Six sources have not been detected previously in any of the X-ray surveys.","According to the available indirect data, all of them and two more sources that have already been entered previously into the INTEGRAL survey catalogs can also be AGNs, including those with strong internal absorption."],"url":"http://arxiv.org/abs/2404.16691v1","category":"astro-ph.HE"}
{"created":"2024-04-25 15:33:39","title":"Calibrating non-parametric morphological indicators from {\\it JWST} images for galaxies over $0.5<z<3$","abstract":"The measurements of morphological indicators of galaxies are often influenced by a series of observational effects. In this study, we utilize a sample of over 800 TNG50 simulated galaxies with log($M_*$/M$_\\odot$)$>9$ at $0.5<z<3$ to investigate the differences in non-parametric morphological indicators ($C$, $S$, $Gini$, $M_{\\rm 20}$, $A_{\\rm O}$, and $D_{\\rm O}$) derived from noise-free and high-resolution TNG50 images and mock images simulated to have the same observational conditions as {\\it JWST}/NIRCam. We quantify the relationship between intrinsic and observed values of the morphological indicators and accordingly apply this calibration to over 4600 galaxies in the same stellar mass and redshift ranges observed in {\\it JWST} CEERS and JADES surveys. We find a significant evolution of morphological indicators with rest-frame wavelength ($\\lambda_{\\rm rf}$) at $\\lambda_{\\rm rf}<1$\\,$\\mu$m, while essentially no obvious variations occur at $\\lambda_{\\rm rf}>1$\\,$\\mu$m. The morphological indicators of star-forming galaxies (SFGs) and quiescent galaxies (QGs) are significantly different. The morphologies of QGs exhibit a higher sensitivity to rest-frame wavelength than SFGs. After analyzing the evolution of morphological indicators in the rest-frame V-band (0.5-0.7\\,$\\mu$m) and rest-frame J-band (1.1-1.4\\,$\\mu$m), we find that the morphologies of QGs evolve substantially with both redshift and stellar mass. For SFGs, the $C$, $Gini$ and $M_{\\rm 20}$ show a rapid evolution with stellar mass at log($M_*$/M$_\\odot$)$\\geq10.5$, while the $A_{\\rm O}$, $D_{\\rm O}$ and $A$ evolve with both redshift and stellar mass. Our comparison shows that TNG50 simulations effectively reproduce the morphological indicators we measured from {\\it JWST} observations when the impact of dust attenuation is considered.","sentences":["The measurements of morphological indicators of galaxies are often influenced by a series of observational effects.","In this study, we utilize a sample of over 800 TNG50 simulated galaxies with log($M_*$/M$_\\odot$)$>9$ at $0.5<z<3$ to investigate the differences in non-parametric morphological indicators ($C$, $S$, $Gini$, $M_{\\rm 20}$, $A_{\\rm O}$, and $D_{\\rm O}$) derived from noise-free and high-resolution TNG50 images and mock images simulated to have the same observational conditions as {\\it JWST}/NIRCam.","We quantify the relationship between intrinsic and observed values of the morphological indicators and accordingly apply this calibration to over 4600 galaxies in the same stellar mass and redshift ranges observed in {\\it JWST} CEERS and JADES surveys.","We find a significant evolution of morphological indicators with rest-frame wavelength ($\\lambda_{\\rm rf}$) at $\\lambda_{\\rm rf}<1$\\,$\\mu$m, while essentially no obvious variations occur at $\\lambda_{\\rm rf}>1$\\,$\\mu$m.","The morphological indicators of star-forming galaxies (SFGs) and quiescent galaxies (QGs) are significantly different.","The morphologies of QGs exhibit a higher sensitivity to rest-frame wavelength than SFGs.","After analyzing the evolution of morphological indicators in the rest-frame V-band (0.5-0.7\\,$\\mu$m) and rest-frame J-band (1.1-1.4\\,$\\mu$m), we find that the morphologies of QGs evolve substantially with both redshift and stellar mass.","For SFGs, the $C$, $Gini$ and $M_{\\rm 20}$ show a rapid evolution with stellar mass at log($M_*$/M$_\\odot$)$\\geq10.5$, while the $A_{\\rm O}$, $D_{\\rm O}$ and $A$ evolve with both redshift and stellar mass.","Our comparison shows that TNG50 simulations effectively reproduce the morphological indicators we measured from {\\it JWST} observations when the impact of dust attenuation is considered."],"url":"http://arxiv.org/abs/2404.16686v1","category":"astro-ph.GA"}
{"created":"2024-04-25 15:11:31","title":"Simulation of depth-dose curves and water equivalent ratios of energetic proton beams in cortical bone","abstract":"We have determined the depth-dose curve, the penetration range, and the water equivalent ratio (WER), for proton beams of clinical energies in cortical bone, by means of a detailed and accurate simulation that combines molecular dynamics and Monte Carlo techniques. The fundamental input quantities (stopping power and energy loss straggling) for the simulation were obtained from a reliable electronic excitation spectrum of the condensed-phase target, which takes into account the organic and mineral phases that form it. Our simulations with these inputs, that are in excellent agreement with the scarce data available for a cortical bone target, deviate from simulations performed using other stopping quantities, such as those provided in the widely used ICRU Report 49. The results of this work emphasize the importance of an accurate determination of the stopping quantities of cortical bone in order to advance towards the millimetric precision for the proton penetration ranges and deposited dose needed in radiotherapy.","sentences":["We have determined the depth-dose curve, the penetration range, and the water equivalent ratio (WER), for proton beams of clinical energies in cortical bone, by means of a detailed and accurate simulation that combines molecular dynamics and Monte Carlo techniques.","The fundamental input quantities (stopping power and energy loss straggling) for the simulation were obtained from a reliable electronic excitation spectrum of the condensed-phase target, which takes into account the organic and mineral phases that form it.","Our simulations with these inputs, that are in excellent agreement with the scarce data available for a cortical bone target, deviate from simulations performed using other stopping quantities, such as those provided in the widely used ICRU Report 49.","The results of this work emphasize the importance of an accurate determination of the stopping quantities of cortical bone in order to advance towards the millimetric precision for the proton penetration ranges and deposited dose needed in radiotherapy."],"url":"http://arxiv.org/abs/2404.16667v1","category":"physics.med-ph"}
{"created":"2024-04-25 14:58:48","title":"Time-domain analysis of multi-waveband flares from AD Leonis","abstract":"Radio bursts of magnetically active stars reveal the intensity and activity of the stellar magnetic field. They may also be related to the planets around the stars. We monitored a radio-active star, AD Leonis, 3000 seconds per day for 17 days in November 2020, and 5000 seconds per day for 5 days in July 2023 with the Five-hundred-meter Aperture Spherical radio Telescope (FAST). Based on the simultaneous flux increases in Stokes I and Stokes V, one left-hand circular polarized radio burst is identified. The $\\sim50\\%$ degree of circular polarization indicates the burst being originated from non-thermal radiation related to the stellar magnetic field. Combining the newly discovered burst with previous observations of radio and X-ray bursts from AD Leonis, we did a periodicity analysis for the 49 bursts in total. No periodicity with confidence level $>3\\sigma$ is found, while a candidate period of 3.04 days at $\\approx 2\\sigma$ confidence level is presented and discussed. Results of recent FAST observations and the periodicity analysis suggest a more compact campaign of observation toward this source, from which a more optimistic result of period search could be achieved.","sentences":["Radio bursts of magnetically active stars reveal the intensity and activity of the stellar magnetic field.","They may also be related to the planets around the stars.","We monitored a radio-active star, AD Leonis, 3000 seconds per day for 17 days in November 2020, and 5000 seconds per day for 5 days in July 2023 with the Five-hundred-meter Aperture Spherical radio Telescope (FAST).","Based on the simultaneous flux increases in Stokes I and Stokes V, one left-hand circular polarized radio burst is identified.","The $\\sim50\\%$ degree of circular polarization indicates the burst being originated from non-thermal radiation related to the stellar magnetic field.","Combining the newly discovered burst with previous observations of radio and X-ray bursts from AD Leonis, we did a periodicity analysis for the 49 bursts in total.","No periodicity with confidence level $>3\\sigma$ is found, while a candidate period of 3.04 days at $\\approx 2\\sigma$ confidence level is presented and discussed.","Results of recent FAST observations and the periodicity analysis suggest a more compact campaign of observation toward this source, from which a more optimistic result of period search could be achieved."],"url":"http://arxiv.org/abs/2404.16661v1","category":"astro-ph.SR"}
{"created":"2024-04-25 14:47:46","title":"Rational Designing of Anthocyanidins-Directed Near-Infrared Two-Photon Fluorescence Probes","abstract":"Recently, two-photon fluorescent probes based on anthocyanidins molecules have attracted extensive attention due to their outstanding photophysical properties. However, there are only a few two-photon excited fluorescent probes that really meet the requirements of relatively long emission wavelengths (>600 nm), large two-photon absorption (TPA) cross sections (300 GM), significant Stokes shift (>80 nm), and high fluorescence intensity. Herein, the photophysical properties of a series of anthocyanidins with the same substituents but different fluorophore skeletons were investigated in detail. Compared with b-series molecules, a-series molecules with a six-membered ring in the backbone have a slightly higher reorganization energy. This results in more energy loss upon light excitation, enabling the reaction products to detect NTR through a larger Stokes shift. More importantly, there is very little decrease in fluorescence intensity as the Stokes shift increases. These features are extremely valuable for high-resolution NTR detection. In light of this, novel 2a-n (n=1-5) compounds are designed, which are accomplished by inhibiting the twisted intramolecular charge transfer (TICT) effect through alkyl cyclization, azetidine ring and extending {\\pi} conjugation. Among them, 2a-3 gains long emission spectrum ({\\lambda}em=691.42 nm), noticeable TPA cross section (957.36 GM), and large Stokes shift (110.88 nm), indicating that it serves as a promising candidate for two-photon fluorescent dyes. It is hoped that this work will offer some insightful theoretical direction for the development of novel high performance anthocyanin fluorescent materials.","sentences":["Recently, two-photon fluorescent probes based on anthocyanidins molecules have attracted extensive attention due to their outstanding photophysical properties.","However, there are only a few two-photon excited fluorescent probes that really meet the requirements of relatively long emission wavelengths (>600 nm), large two-photon absorption (TPA) cross sections (300 GM), significant Stokes shift (>80 nm), and high fluorescence intensity.","Herein, the photophysical properties of a series of anthocyanidins with the same substituents but different fluorophore skeletons were investigated in detail.","Compared with b-series molecules, a-series molecules with a six-membered ring in the backbone have a slightly higher reorganization energy.","This results in more energy loss upon light excitation, enabling the reaction products to detect NTR through a larger Stokes shift.","More importantly, there is very little decrease in fluorescence intensity as the Stokes shift increases.","These features are extremely valuable for high-resolution NTR detection.","In light of this, novel 2a-n (n=1-5) compounds are designed, which are accomplished by inhibiting the twisted intramolecular charge transfer (TICT) effect through alkyl cyclization, azetidine ring and extending {\\pi} conjugation.","Among them, 2a-3 gains long emission spectrum ({\\lambda}em=691.42 nm), noticeable TPA cross section (957.36 GM), and large Stokes shift (110.88 nm), indicating that it serves as a promising candidate for two-photon fluorescent dyes.","It is hoped that this work will offer some insightful theoretical direction for the development of novel high performance anthocyanin fluorescent materials."],"url":"http://arxiv.org/abs/2404.16655v1","category":"physics.chem-ph"}
{"created":"2024-04-25 14:31:02","title":"Gaussian free field and Liouville quantum gravity","abstract":"Over fourty years ago, the physicist Polyakov proposed a bold framework for string theory, in which the problem was reduced to the study of certain \"random surfaces\". He further made the tantalising suggestion that this theory could be explicitly solved. Recent breakthroughs from the last fifteen years have not only given a concrete mathematical basis for this theory but also verified some of its most striking predictions, as well as Polyakov's original vision. This theory, now known in the mathematics literature either as Liouville quantum gravity or Liouville conformal field theory, is based on a remarkable combination of ideas coming from different fields, above all probability and geometry. This book is intended to be an introduction to these developments assuming as few prerequisites as possible.","sentences":["Over fourty years ago, the physicist Polyakov proposed a bold framework for string theory, in which the problem was reduced to the study of certain \"random surfaces\".","He further made the tantalising suggestion that this theory could be explicitly solved.","Recent breakthroughs from the last fifteen years have not only given a concrete mathematical basis for this theory but also verified some of its most striking predictions, as well as Polyakov's original vision.","This theory, now known in the mathematics literature either as Liouville quantum gravity or Liouville conformal field theory, is based on a remarkable combination of ideas coming from different fields, above all probability and geometry.","This book is intended to be an introduction to these developments assuming as few prerequisites as possible."],"url":"http://arxiv.org/abs/2404.16642v1","category":"math.PR"}
{"created":"2024-04-25 14:30:38","title":"Extended high-ionization [MgIV] emission tracing widespread shocks in starbursts seen by JWST /NIRSpec","abstract":"We report the detection of extended (>0.5-1kpc) high-ionization [MgIV] 4.487 $\\mu$m (80 eV) emission in four local luminous infrared galaxies observed with JWST/NIRSpec. Excluding the nucleus and outflow of the Type 1 active galactic nucleus (AGN) in the sample, we find that the [MgIV] luminosity is well correlated with that of H recombination lines, which mainly trace star forming clumps in these objects, and that the [ArVI] 4.530 $\\mu$m (75 eV), usually seen in AGN, is undetected. On 100-400pc scales, the [MgIV] line profiles are broader (sigma([MgIV])=90 +- 25 km/s) and shifted (Delta_v up to +- 50 km/s) compared to those of the H recombination lines and lower ionization transitions (e.g., sigma(Hu-12)=57 +- 15 km/s). The [MgIV] kinematics follow the large scale rotating velocity field of these galaxies and the broad [MgIV] profiles are compatible with the broad wings detected in the H recombination lines. Based on these observational results, extended highly ionized gas more turbulent than the ambient interstellar medium, possibly as a result of ionizing shocks associated with star-formation, is the most likely origin of the [MgIV] emission. We also computed new grids of photoionization and shock models to investigate where the [MgIV] line originates. Shocks with velocities of 100-130 km/s reproduce the observed line ratios and the [MgIV] luminosity agrees with that expected from the mechanical energy released by supernove (SNe) in these regions. Therefore, these models support shocks induced by SNe as the origin of the [MgIV] line. Future studies on the stellar feedback from SNe will benefit from the [MgIV] line that is little affected by obscuration and, in absence of an AGN, can only be produced by shocks due to its high ionization potential.","sentences":["We report the detection of extended (>0.5-1kpc) high-ionization [MgIV] 4.487 $\\mu$m (80 eV) emission in four local luminous infrared galaxies observed with JWST/NIRSpec.","Excluding the nucleus and outflow of the Type 1 active galactic nucleus (AGN) in the sample, we find that the [MgIV] luminosity is well correlated with that of H recombination lines, which mainly trace star forming clumps in these objects, and that the [ArVI] 4.530 $\\mu$m (75 eV), usually seen in AGN, is undetected.","On 100-400pc scales, the [MgIV] line profiles are broader (sigma([MgIV])=90 +- 25 km/s) and shifted (Delta_v up to +- 50 km/s) compared to those of the H recombination lines and lower ionization transitions (e.g., sigma(Hu-12)=57 +- 15 km/s).","The [MgIV] kinematics follow the large scale rotating velocity field of these galaxies and the broad [MgIV] profiles are compatible with the broad wings detected in the H recombination lines.","Based on these observational results, extended highly ionized gas more turbulent than the ambient interstellar medium, possibly as a result of ionizing shocks associated with star-formation, is the most likely origin of the [MgIV] emission.","We also computed new grids of photoionization and shock models to investigate where the [MgIV] line originates.","Shocks with velocities of 100-130 km/s reproduce the observed line ratios and the [MgIV] luminosity agrees with that expected from the mechanical energy released by supernove (SNe) in these regions.","Therefore, these models support shocks induced by SNe as the origin of the [MgIV] line.","Future studies on the stellar feedback from SNe will benefit from the [MgIV] line that is little affected by obscuration and, in absence of an AGN, can only be produced by shocks due to its high ionization potential."],"url":"http://arxiv.org/abs/2404.16641v1","category":"astro-ph.GA"}
{"created":"2024-04-25 13:41:06","title":"Freezing density scaling of transport coefficients in the Weeks-Chandler-Andersen fluid","abstract":"It is shown that the transport coefficients (self-diffusion, shear viscosity, and thermal conductivity) of the Weeks-Chandler-Anderson (WCA) fluid along isotherms exhibit a freezing density scaling (FDS). The functional form of this FDS is essentially the same or closely related to those in the Lennard-Jones fluid, hard-sphere fluid, and some liquefied noble gases. This proves that this FDS represents a quasi-universal corresponding state principle for simple classical fluids with steep interactions. Some related aspects such as Stokes-Einstein relation without a hydrodynamic diameter and gas-to-liquid dynamical crossover are briefly discussed. Simple fitting formula for the transport coefficients of the dense WCA fluid are suggested.","sentences":["It is shown that the transport coefficients (self-diffusion, shear viscosity, and thermal conductivity) of the Weeks-Chandler-Anderson (WCA) fluid along isotherms exhibit a freezing density scaling (FDS).","The functional form of this FDS is essentially the same or closely related to those in the Lennard-Jones fluid, hard-sphere fluid, and some liquefied noble gases.","This proves that this FDS represents a quasi-universal corresponding state principle for simple classical fluids with steep interactions.","Some related aspects such as Stokes-Einstein relation without a hydrodynamic diameter and gas-to-liquid dynamical crossover are briefly discussed.","Simple fitting formula for the transport coefficients of the dense WCA fluid are suggested."],"url":"http://arxiv.org/abs/2404.16603v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-25 12:42:48","title":"Probing the pole origin of $X(3872)$ with the coupled-channel dynamics","abstract":"The $X(3872)$, as the first and the most crucial member in the exotic charmoniumlike $XYZ$ family, has been studied for a long time. However, its dynamical origin, whether stemming from a $D\\bar{D}^*$ hadronic molecule or the first excited $P$-wave charmonium $\\chi_{c1}(2P)$, remains controversial. In this Letter, we demonstrate that the $X(3872)$ definitely does not result from the mass shift of the higher bare $\\chi_{c1}(2P)$ resonance pole in the coupled-channel dynamics involving a short-distance $c\\bar{c}$ core and the long-distance $D\\bar{D}^*$ channels. Instead, it originates from either the $D\\bar{D}^*$ molecular pole or the shadow pole associated with the $P$-wave charmonium, which depends on the concrete coupling mode between the $c\\bar{c}$ and $D\\bar{D}^*$. In order to further exploit the nature of $X(3872)$, we carefully investigate potential mechanisms that contribute to its pole width, which suggests that the coupled-channel dynamics plays a critical role in causing a noticeable discrepancy between the pole widths of $X(3872)$ and $T_{cc}^+$. Interestingly, we bridge the quantitative connection among the dynamics origin of $X(3872)$, its pole width and the properties of the predicted new resonance. The precise measurement of the pole width of $X(3872)$ and the search for the new charmoniumlike resonance become highly significant and can be anticipated in future LHCb, BESIII and Belle II experiments.","sentences":["The $X(3872)$, as the first and the most crucial member in the exotic charmoniumlike $XYZ$ family, has been studied for a long time.","However, its dynamical origin, whether stemming from a $D\\bar{D}^*$ hadronic molecule or the first excited $P$-wave charmonium $\\chi_{c1}(2P)$, remains controversial.","In this Letter, we demonstrate that the $X(3872)$ definitely does not result from the mass shift of the higher bare $\\chi_{c1}(2P)$ resonance pole in the coupled-channel dynamics involving a short-distance $c\\bar{c}$ core and the long-distance $D\\bar{D}^*$ channels.","Instead, it originates from either the $D\\bar{D}^*$ molecular pole or the shadow pole associated with the $P$-wave charmonium, which depends on the concrete coupling mode between the $c\\bar{c}$ and $D\\bar{D}^*$. In order to further exploit the nature of $X(3872)$, we carefully investigate potential mechanisms that contribute to its pole width, which suggests that the coupled-channel dynamics plays a critical role in causing a noticeable discrepancy between the pole widths of $X(3872)$ and $T_{cc}^+$. Interestingly, we bridge the quantitative connection among the dynamics origin of $X(3872)$, its pole width and the properties of the predicted new resonance.","The precise measurement of the pole width of $X(3872)$ and the search for the new charmoniumlike resonance become highly significant and can be anticipated in future LHCb, BESIII and Belle II experiments."],"url":"http://arxiv.org/abs/2404.16575v1","category":"hep-ph"}
{"created":"2024-04-25 12:32:24","title":"STELLA lightcurves of energetic pair instability supernovae in the context of SN2018ibb","abstract":"SN2018ibb is a recently observed hydrogen poor super-luminous supernova which appears to be powered by the decay of $30\\;\\rm{M_\\odot}$ of radioactive nickel. This supernova has been suggested to show hybrid signatures of a pair instability supernova and an interacting supernova. In a previous paper, we found that rotating, metal enriched pair instability supernova progenitors appeared to check both of these boxes. In this paper, we model the lightcurves of the pair instability supernovae using STELLA. We find that the STELLA models can explain the overall shape of the bolometric lightcurve of SN2018ibb, though not specific morphological features such as the luminosity peak or the bump at roughly three hundred days after the peak. We also estimate the contribution from interaction, and find that with relatively low wind velocities, the circum-stellar medium originating from the stellar winds is consistent with the evidence for interaction in the spectra. The observed values of the photosphere velocity in the hundred days after peak luminosity are similar to the STELLA models, but the deceleration is lower. This leads to the biggest inconsistency which is the black body temperature of SN2018ibb being much hotter than any of the STELLA models. We note that this high temperature (and the flat velocity) may be difficult to reconcile with the long rise time of SN2018ibb, but nevertheless conclude that if it is accurate, this discrepancy represents a challenge for SN2018ibb being a robust PISN candidate. This result is noteworthy given the lack of other scenarios for this supernova.","sentences":["SN2018ibb is a recently observed hydrogen poor super-luminous supernova which appears to be powered by the decay of $30\\;\\rm{M_\\odot}$ of radioactive nickel.","This supernova has been suggested to show hybrid signatures of a pair instability supernova and an interacting supernova.","In a previous paper, we found that rotating, metal enriched pair instability supernova progenitors appeared to check both of these boxes.","In this paper, we model the lightcurves of the pair instability supernovae using STELLA.","We find that the STELLA models can explain the overall shape of the bolometric lightcurve of SN2018ibb, though not specific morphological features such as the luminosity peak or the bump at roughly three hundred days after the peak.","We also estimate the contribution from interaction, and find that with relatively low wind velocities, the circum-stellar medium originating from the stellar winds is consistent with the evidence for interaction in the spectra.","The observed values of the photosphere velocity in the hundred days after peak luminosity are similar to the STELLA models, but the deceleration is lower.","This leads to the biggest inconsistency which is the black body temperature of SN2018ibb being much hotter than any of the STELLA models.","We note that this high temperature (and the flat velocity) may be difficult to reconcile with the long rise time of SN2018ibb, but nevertheless conclude that if it is accurate, this discrepancy represents a challenge for SN2018ibb being a robust PISN candidate.","This result is noteworthy given the lack of other scenarios for this supernova."],"url":"http://arxiv.org/abs/2404.16570v1","category":"astro-ph.HE"}
{"created":"2024-04-25 12:29:10","title":"Simulating Ultrafast Transient Absorption Spectra from First Principles using a Time-Dependent Configuration Interaction Probe","abstract":"Transient absorption spectroscopy (TAS) is among the most common ultrafast photochemical experiments, but its interpretation remains challenging. In this work, we present an efficient and robust method for simulating TAS signals from first principles. Excited-state absorption and stimulated emission (SE) signals are computed using time-dependent complete active space configuration interaction (TD-CASCI) simulations, leveraging the robustness of time-domain simulation to minimize electronic structure failure. We demonstrate our approach by simulating the TAS signal of 1$^\\prime$-hydroxy-2$^\\prime$-acetonapthone (HAN) from ab initio multiple spawning nonadiabatic molecular dynamics simulations. Our results are compared to gas-phase TAS data recorded from both jet-cooled ($T\\sim 40$ K) and hot ($\\sim 403$ K) molecules via cavity-enhanced transient absorption spectroscopy (CE-TAS). Decomposition of the computed spectrum allows us to assign a rise in the SE signal to excited-state proton transfer and the ultimate decay of the signal to relaxation through a twisted conical intersection. The total cost of computing the observable signal ($\\sim$1700 graphics processing unit hours for $\\sim$4 ns of electron dynamics) was markedly less than that of the {\\em ab initio} multiple spawning calculations used to compute the underlying nonadiabatic dynamics.","sentences":["Transient absorption spectroscopy (TAS) is among the most common ultrafast photochemical experiments, but its interpretation remains challenging.","In this work, we present an efficient and robust method for simulating TAS signals from first principles.","Excited-state absorption and stimulated emission (SE) signals are computed using time-dependent complete active space configuration interaction (TD-CASCI) simulations, leveraging the robustness of time-domain simulation to minimize electronic structure failure.","We demonstrate our approach by simulating the TAS signal of 1$^\\prime$-hydroxy-2$^\\prime$-acetonapthone (HAN) from ab initio multiple spawning nonadiabatic molecular dynamics simulations.","Our results are compared to gas-phase TAS data recorded from both jet-cooled ($T\\sim 40$ K) and hot ($\\sim 403$ K) molecules via cavity-enhanced transient absorption spectroscopy (CE-TAS).","Decomposition of the computed spectrum allows us to assign a rise in the SE signal to excited-state proton transfer and the ultimate decay of the signal to relaxation through a twisted conical intersection.","The total cost of computing the observable signal ($\\sim$1700 graphics processing unit hours for $\\sim$4 ns of electron dynamics) was markedly less than that of the {\\em ab initio} multiple spawning calculations used to compute the underlying nonadiabatic dynamics."],"url":"http://arxiv.org/abs/2404.16568v1","category":"physics.chem-ph"}
{"created":"2024-04-25 12:24:37","title":"Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark","abstract":"Large Language Models (LLMs) offer the potential for automatic time series analysis and reporting, which is a critical task across many domains, spanning healthcare, finance, climate, energy, and many more. In this paper, we propose a framework for rigorously evaluating the capabilities of LLMs on time series understanding, encompassing both univariate and multivariate forms. We introduce a comprehensive taxonomy of time series features, a critical framework that delineates various characteristics inherent in time series data. Leveraging this taxonomy, we have systematically designed and synthesized a diverse dataset of time series, embodying the different outlined features. This dataset acts as a solid foundation for assessing the proficiency of LLMs in comprehending time series. Our experiments shed light on the strengths and limitations of state-of-the-art LLMs in time series understanding, revealing which features these models readily comprehend effectively and where they falter. In addition, we uncover the sensitivity of LLMs to factors including the formatting of the data, the position of points queried within a series and the overall time series length.","sentences":["Large Language Models (LLMs) offer the potential for automatic time series analysis and reporting, which is a critical task across many domains, spanning healthcare, finance, climate, energy, and many more.","In this paper, we propose a framework for rigorously evaluating the capabilities of LLMs on time series understanding, encompassing both univariate and multivariate forms.","We introduce a comprehensive taxonomy of time series features, a critical framework that delineates various characteristics inherent in time series data.","Leveraging this taxonomy, we have systematically designed and synthesized a diverse dataset of time series, embodying the different outlined features.","This dataset acts as a solid foundation for assessing the proficiency of LLMs in comprehending time series.","Our experiments shed light on the strengths and limitations of state-of-the-art LLMs in time series understanding, revealing which features these models readily comprehend effectively and where they falter.","In addition, we uncover the sensitivity of LLMs to factors including the formatting of the data, the position of points queried within a series and the overall time series length."],"url":"http://arxiv.org/abs/2404.16563v1","category":"cs.CL"}
{"created":"2024-04-25 09:34:21","title":"Potential energy surfaces from many-body functionals: analytical benchmarks and conserving many-body approximations","abstract":"We investigate analytically the performance of many-body energy functionals, derived respectively by Klein and Luttinger and Ward, at different levels of diagrammatic approximations, ranging from second Born, to GW, to the so-called T-matrix, for the calculation of total energies and potential energy surfaces. We benchmark our theoretical results on the extended two-site Hubbard model, which is analytically solvable and for which several exact properties can be calculated. Despite its simplicity, this model displays the physics of strongly correlated electrons: it is prototypical of the H$_2$ dissociation, a notoriously difficult problem to solve accurately for the majority of mean-field based approaches. We show that both functionals exhibit good to excellent variational properties, particularly in the case of the Luttinger-Ward one, which is in close agreement with fully self-consistent calculations, and elucidate the relation between the accuracy of the results and the different input one-body Green's functions. Provided that these are wisely chosen, we show how the Luttinger-Ward functional can be used as a computationally inexpensive alternative to fully self-consistent many-body calculations, without sacrificing the precision of the results obtained. Furthermore, in virtue of this accuracy, we argue that this functional can also be used to rank different many-body approximations at different regimes of electronic correlation, once again bypassing the need for self-consistency.","sentences":["We investigate analytically the performance of many-body energy functionals, derived respectively by Klein and Luttinger and Ward, at different levels of diagrammatic approximations, ranging from second Born, to GW, to the so-called T-matrix, for the calculation of total energies and potential energy surfaces.","We benchmark our theoretical results on the extended two-site Hubbard model, which is analytically solvable and for which several exact properties can be calculated.","Despite its simplicity, this model displays the physics of strongly correlated electrons: it is prototypical of the H$_2$ dissociation, a notoriously difficult problem to solve accurately for the majority of mean-field based approaches.","We show that both functionals exhibit good to excellent variational properties, particularly in the case of the Luttinger-Ward one, which is in close agreement with fully self-consistent calculations, and elucidate the relation between the accuracy of the results and the different input one-body Green's functions.","Provided that these are wisely chosen, we show how the Luttinger-Ward functional can be used as a computationally inexpensive alternative to fully self-consistent many-body calculations, without sacrificing the precision of the results obtained.","Furthermore, in virtue of this accuracy, we argue that this functional can also be used to rank different many-body approximations at different regimes of electronic correlation, once again bypassing the need for self-consistency."],"url":"http://arxiv.org/abs/2404.16453v1","category":"cond-mat.str-el"}
{"created":"2024-04-25 08:56:05","title":"Soft X-ray prompt emission from a high-redshift gamma-ray burst EP240315a","abstract":"Long gamma-ray bursts (GRBs) are believed to originate from core collapse of massive stars. High-redshift GRBs can probe the star formation and reionization history of the early universe, but their detection remains rare. Here we report the detection of a GRB triggered in the 0.5--4 keV band by the Wide-field X-ray Telescope (WXT) on board the Einstein Probe (EP) mission, designated as EP240315a, whose bright peak was also detected by the Swift Burst Alert Telescope and Konus-Wind through off-line analyses. At a redshift of $z=4.859$, EP240315a showed a much longer and more complicated light curve in the soft X-ray band than in gamma-rays. Benefiting from a large field-of-view ($\\sim$3600 deg$^2$) and a high sensitivity, EP-WXT captured the earlier engine activation and extended late engine activity through a continuous detection. With a peak X-ray flux at the faint end of previously known high-$z$ GRBs, the detection of EP240315a demonstrates the great potential for EP to study the early universe via GRBs.","sentences":["Long gamma-ray bursts (GRBs) are believed to originate from core collapse of massive stars.","High-redshift GRBs can probe the star formation and reionization history of the early universe, but their detection remains rare.","Here we report the detection of a GRB triggered in the 0.5--4 keV band by the Wide-field X-ray Telescope (WXT) on board the Einstein Probe (EP) mission, designated as EP240315a, whose bright peak was also detected by the Swift Burst Alert Telescope and Konus-Wind through off-line analyses.","At a redshift of $z=4.859$, EP240315a showed a much longer and more complicated light curve in the soft X-ray band than in gamma-rays.","Benefiting from a large field-of-view ($\\sim$3600 deg$^2$) and a high sensitivity, EP-WXT captured the earlier engine activation and extended late engine activity through a continuous detection.","With a peak X-ray flux at the faint end of previously known high-$z$ GRBs, the detection of EP240315a demonstrates the great potential for EP to study the early universe via GRBs."],"url":"http://arxiv.org/abs/2404.16425v1","category":"astro-ph.HE"}
{"created":"2024-04-25 08:23:48","title":"Antibacterial size effect of ZnO nanoparticles and their role as additives in emulsion waterborne paint","abstract":"Nosocomial infections (NIs) are prevalent in intensive care units due to antibiotic overuse. Metal oxide nanoparticles (NPs), like ZnO, offer potential solutions, yet understanding how NPs size impacts their antibacterial efficacy are lacking. This study focuses on the effect of nanoparticle size on kinetics of bacterial strains growth. NPs were synthesized using a sol-gel process with monoethanolamine (MEA) and water, characterized using X-ray diffraction (XRD), transmission electron microscopy (TEM), and Raman spectroscopy, confirming crystallization and size variations. ZnO NPs with mean size of 22, 35 and 66 nm were used against the most common nosocomial bacteria strains Escherichia coli (Gram-negative), Pseudomonas aeruginosa (Gram-negative), and Staphylococcus aureus (Gram-positive). The evaluation of NPs minimal inhibitory concentration (MIC) and bactericidal concentration (MBC) revealed superior antibacterial activity in smaller NPs. The bacterial population was monitored via optical absorbance, showing reduced specific growth rate, prolonged latency period, and increased inhibition percentage with smaller NPs, indicating a substantial deceleration in the growth of microorganisms. Pseudomonas aeruginosa exhibited the smallest sensitivity to ZnO NPs, attributed to its environmental stress resistance. Furthermore, the antibacterial efficacy of paint containing 1 wt% of 22 nm ZnO NPs was assessed and displayed activity against E. coli and S. aureus.","sentences":["Nosocomial infections (NIs) are prevalent in intensive care units due to antibiotic overuse.","Metal oxide nanoparticles (NPs), like ZnO, offer potential solutions, yet understanding how NPs size impacts their antibacterial efficacy are lacking.","This study focuses on the effect of nanoparticle size on kinetics of bacterial strains growth.","NPs were synthesized using a sol-gel process with monoethanolamine (MEA) and water, characterized using X-ray diffraction (XRD), transmission electron microscopy (TEM), and Raman spectroscopy, confirming crystallization and size variations.","ZnO NPs with mean size of 22, 35 and 66 nm were used against the most common nosocomial bacteria strains Escherichia coli (Gram-negative), Pseudomonas aeruginosa (Gram-negative), and Staphylococcus aureus (Gram-positive).","The evaluation of NPs minimal inhibitory concentration (MIC) and bactericidal concentration (MBC) revealed superior antibacterial activity in smaller NPs.","The bacterial population was monitored via optical absorbance, showing reduced specific growth rate, prolonged latency period, and increased inhibition percentage with smaller NPs, indicating a substantial deceleration in the growth of microorganisms.","Pseudomonas aeruginosa exhibited the smallest sensitivity to ZnO NPs, attributed to its environmental stress resistance.","Furthermore, the antibacterial efficacy of paint containing 1 wt% of 22 nm ZnO NPs was assessed and displayed activity against E. coli and S. aureus."],"url":"http://arxiv.org/abs/2404.16400v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-25 07:25:43","title":"Study of \u03b3-ray Emission from a Compact Radio Galaxy with the Fermi Large Area Telescope","abstract":"The radio galaxy PKS 1007+142 is classified as a compact steep-spectrum source (CSS) and belongs to the class of young Active Galactic Nuclei (AGNs). In this paper, we investigate the $\\gamma$-ray emission from this CSS by conducting a comprehensive analysis of the 15 yr Fermi Large Area Telescope (Fermi-LAT) observation data. The Fermi-LAT latest Source Catalog, 4FGL-DR4, includes an unassociated $\\gamma$-ray source, 4FGL J1010.0+1416, located at 0.24{\\deg} away from the radio position of PKS 1007+142. Using the 15 yr Fermi-LAT observation data, we re-estimate the best-fit position of the $\\gamma$-ray source and find that PKS 1007+142 is in close proximity to the $\\gamma$-ray source and falls within its 68% error circle. Therefore, we conclude that PKS 1007+142 is the most plausible counterpart to the unassociated LAT source with a detection test statistics (TS) $\\sim$43.4 ($\\sim 6.6\\sigma$). PKS 1007+142 exhibits a steep power-law spectrum in the 0.1--300 GeV band, with a photon spectral index ($\\Gamma_{\\gamma}$) of $2.86\\pm0.17$. The average flux in the considered time interval is $\\rm (2.14\\pm0.34)\\times10^{-12}\\ erg\\ cm^{-2}\\ s^{-1}$. Comparing PKS 1007+142 with other $\\gamma$-ray emitting AGNs in both the $L_{\\gamma}-\\Gamma_{\\gamma}$ and $L_\\gamma-L_{\\rm 1.4GHz}$ planes, it shows a softer $\\gamma$-ray spectrum and lower luminosity compared to other $\\gamma$-ray emitting CSSs. Furthermore, the possible origins of $\\gamma$-ray in PKS 1007+142 are also discussed.","sentences":["The radio galaxy PKS 1007+142 is classified as a compact steep-spectrum source (CSS) and belongs to the class of young Active Galactic Nuclei (AGNs).","In this paper, we investigate the $\\gamma$-ray emission from this CSS by conducting a comprehensive analysis of the 15 yr Fermi Large Area Telescope (Fermi-LAT) observation data.","The Fermi-LAT latest Source Catalog, 4FGL-DR4, includes an unassociated $\\gamma$-ray source, 4FGL J1010.0+1416, located at 0.24{\\deg} away from the radio position of PKS 1007+142.","Using the 15 yr Fermi-LAT observation data, we re-estimate the best-fit position of the $\\gamma$-ray source and find that PKS 1007+142 is in close proximity to the $\\gamma$-ray source and falls within its 68% error circle.","Therefore, we conclude that PKS 1007+142 is the most plausible counterpart to the unassociated LAT source with a detection test statistics (TS) $\\sim$43.4 ($\\sim 6.6\\sigma$).","PKS 1007+142 exhibits a steep power-law spectrum in the 0.1--300 GeV band, with a photon spectral index ($\\Gamma_{\\gamma}$) of $2.86\\pm0.17$. The average flux in the considered time interval is $\\rm (2.14\\pm0.34)\\times10^{-12}\\ erg\\ cm^{-2}\\","s^{-1}$.","Comparing PKS 1007+142 with other $\\gamma$-ray emitting AGNs in both the $L_{\\gamma}-\\Gamma_{\\gamma}$ and $L_\\gamma-L_{\\rm 1.4GHz}$ planes",", it shows a softer $\\gamma$-ray spectrum and lower luminosity compared to other $\\gamma$-ray emitting CSSs.","Furthermore, the possible origins of $\\gamma$-ray in PKS 1007+142 are also discussed."],"url":"http://arxiv.org/abs/2404.16373v1","category":"astro-ph.HE"}
{"created":"2024-04-25 07:23:58","title":"Conserved currents in five-dimensional proposals for lattice chiral gauge theories","abstract":"We apply the Grabowska-Kaplan framework, originally proposed for lattice chiral gauge theories, to QCD. We show that the resulting theory contains a conserved and gauge invariant singlet axial current, both on the lattice and in the continuum limit. This must give rise to a difference with QCD, with the simplest possibility being a superfluous Nambu-Goldstone boson in the physical spectrum not present in QCD. We find a similar unwanted conserved current in the recent \"disk\" formalism [Kaplan, Kaplan & Sen], this time limiting ourselves to the continuum formulation. A similar problem is expected when either of these formalisms is used for its original goal of constructing lattice chiral gauge theories. Finally we discuss a conjecture about the possible dynamics that might be associated with the unwanted conserved current, and the fate of 't Hooft vertices.","sentences":["We apply the Grabowska-Kaplan framework, originally proposed for lattice chiral gauge theories, to QCD.","We show that the resulting theory contains a conserved and gauge invariant singlet axial current, both on the lattice and in the continuum limit.","This must give rise to a difference with QCD, with the simplest possibility being a superfluous Nambu-Goldstone boson in the physical spectrum not present in QCD.","We find a similar unwanted conserved current in the recent \"disk\" formalism [Kaplan, Kaplan & Sen], this time limiting ourselves to the continuum formulation.","A similar problem is expected when either of these formalisms is used for its original goal of constructing lattice chiral gauge theories.","Finally we discuss a conjecture about the possible dynamics that might be associated with the unwanted conserved current, and the fate of 't Hooft vertices."],"url":"http://arxiv.org/abs/2404.16372v1","category":"hep-lat"}
{"created":"2024-04-25 06:54:35","title":"Byzantine Attacks Exploiting Penalties in Ethereum PoS","abstract":"In May 2023, the Ethereum blockchain experienced its first inactivity leak, a mechanism designed to reinstate chain finalization amid persistent network disruptions. This mechanism aims to reduce the voting power of validators who are unreachable within the network, reallocating this power to active validators. This paper investigates the implications of the inactivity leak on safety within the Ethereum blockchain. Our theoretical analysis reveals scenarios where actions by Byzantine validators expedite the finalization of two conflicting branches, and instances where Byzantine validators reach a voting power exceeding the critical safety threshold of one-third. Additionally, we revisit the probabilistic bouncing attack, illustrating how the inactivity leak can result in a probabilistic breach of safety, potentially allowing Byzantine validators to exceed the one-third safety threshold. Our findings uncover how penalizing inactive nodes can compromise blockchain properties, particularly in the presence of Byzantine validators capable of coordinating actions.","sentences":["In May 2023, the Ethereum blockchain experienced its first inactivity leak, a mechanism designed to reinstate chain finalization amid persistent network disruptions.","This mechanism aims to reduce the voting power of validators who are unreachable within the network, reallocating this power to active validators.","This paper investigates the implications of the inactivity leak on safety within the Ethereum blockchain.","Our theoretical analysis reveals scenarios where actions by Byzantine validators expedite the finalization of two conflicting branches, and instances where Byzantine validators reach a voting power exceeding the critical safety threshold of one-third.","Additionally, we revisit the probabilistic bouncing attack, illustrating how the inactivity leak can result in a probabilistic breach of safety, potentially allowing Byzantine validators to exceed the one-third safety threshold.","Our findings uncover how penalizing inactive nodes can compromise blockchain properties, particularly in the presence of Byzantine validators capable of coordinating actions."],"url":"http://arxiv.org/abs/2404.16363v1","category":"cs.CR"}
{"created":"2024-04-25 06:16:29","title":"Modular transformations of on-shell actions of (root-)$\\text{T}\\overline{\\text{T}}$ deformed holographic CFTs","abstract":"In this study, we examine the modular transformations of the (root-)$\\text{T}\\overline{\\text{T}}$ deformed torus partition function of a two-dimensional CFT (with a gravitational anomaly) from the holographic perspective by computing the on-shell actions of various saddle solutions of the dual gravity theories.","sentences":["In this study, we examine the modular transformations of the (root-)$\\text{T}\\overline{\\text{T}}$ deformed torus partition function of a two-dimensional CFT (with a gravitational anomaly) from the holographic perspective by computing the on-shell actions of various saddle solutions of the dual gravity theories."],"url":"http://arxiv.org/abs/2404.16354v1","category":"hep-th"}
{"created":"2024-04-25 06:01:12","title":"The fast X-ray transient EP240315a: a z ~ 5 gamma-ray burst in a Lyman continuum leaking galaxy","abstract":"The nature of the minute-to-hour long Fast X-ray Transients (FXTs) localised by telescopes such as Chandra, Swift, and XMM-Newton remains mysterious, with numerous models suggested for the events. Here, we report multi-wavelength observations of EP240315a, a 1600 s long transient detected by the Einstein Probe, showing it to have a redshift of z=4.859. We measure a low column density of neutral hydrogen, indicating that the event is embedded in a low-density environment, further supported by direct detection of leaking ionising Lyman-continuum. The observed properties are consistent with EP240315a being a long-duration gamma-ray burst, and these observations support an interpretation in which a significant fraction of the FXT population are lower-luminosity examples of similar events. Such transients are detectable at high redshifts by the Einstein Probe and, in the (near) future, out to even larger distances by SVOM, THESEUS, and Athena, providing samples of events into the epoch of reionisation.","sentences":["The nature of the minute-to-hour long Fast X-ray Transients (FXTs) localised by telescopes such as Chandra, Swift, and XMM-Newton remains mysterious, with numerous models suggested for the events.","Here, we report multi-wavelength observations of EP240315a, a 1600 s long transient detected by the Einstein Probe, showing it to have a redshift of z=4.859.","We measure a low column density of neutral hydrogen, indicating that the event is embedded in a low-density environment, further supported by direct detection of leaking ionising Lyman-continuum.","The observed properties are consistent with EP240315a being a long-duration gamma-ray burst, and these observations support an interpretation in which a significant fraction of the FXT population are lower-luminosity examples of similar events.","Such transients are detectable at high redshifts by the Einstein Probe and, in the (near) future, out to even larger distances by SVOM, THESEUS, and Athena, providing samples of events into the epoch of reionisation."],"url":"http://arxiv.org/abs/2404.16350v1","category":"astro-ph.HE"}
{"created":"2024-04-25 05:37:26","title":"Magnetically Driven Relativistic Jet in the High-Redshift Blazar OH~471","abstract":"Context : Understanding the mechanisms that launch and shape powerful relativistic jets from supermassive black holes (SMBHs) in high-redshift active galactic nuclei (AGN) is crucial for probing the co-evolution of SMBHs and galaxies over cosmic time.   Aims :We study the high-redshift ($z=3.396$) blazar OH~471 to explore the jet launching mechanism in the early Universe.   Methods : Using multi-frequency radio monitoring observations and high-resolution Very Long Baseline Interferometry imaging over three decades, we study the milliarcsecond structure and long-term variability of OH~471.   Results : Spectral modelling of the radio flux densities reveals a synchrotron self-absorbed spectrum indicating strong magnetic fields within the compact core. By applying the flux freezing approximation, we estimate the magnetic flux carried by the jet and find that it reaches or exceeds theoretical predictions for jets powered by black hole spin energy via the Blandford-Znajek mechanism. This implies that OH~471 was in a magnetically arrested disk (MAD) state where the magnetic flux accumulated near the horizon regulates the accretion flow, allowing efficient extraction of black hole rotational energy.   Conclusions : Our study demonstrates the dominance of MAD accretion in powering the prominent radio flares and relativistic jets observed in the radio-loud AGN OH~471 and statistical studies of large samples of high-redshift AGN will shed light on the role of MAD accretion in launching and accelerating the earliest relativistic jets.","sentences":["Context : Understanding the mechanisms that launch and shape powerful relativistic jets from supermassive black holes (SMBHs) in high-redshift active galactic nuclei (AGN) is crucial for probing the co-evolution of SMBHs and galaxies over cosmic time.   ","Aims :We study the high-redshift ($z=3.396$) blazar OH~471 to explore the jet launching mechanism in the early Universe.   Methods : Using multi-frequency radio monitoring observations and high-resolution Very Long Baseline Interferometry imaging over three decades, we study the milliarcsecond structure and long-term variability of OH~471.   ","Results : Spectral modelling of the radio flux densities reveals a synchrotron self-absorbed spectrum indicating strong magnetic fields within the compact core.","By applying the flux freezing approximation, we estimate the magnetic flux carried by the jet and find that it reaches or exceeds theoretical predictions for jets powered by black hole spin energy via the Blandford-Znajek mechanism.","This implies that OH~471 was in a magnetically arrested disk (MAD) state where the magnetic flux accumulated near the horizon regulates the accretion flow, allowing efficient extraction of black hole rotational energy.   ","Conclusions : Our study demonstrates the dominance of MAD accretion in powering the prominent radio flares and relativistic jets observed in the radio-loud AGN OH~471 and statistical studies of large samples of high-redshift AGN will shed light on the role of MAD accretion in launching and accelerating the earliest relativistic jets."],"url":"http://arxiv.org/abs/2404.16343v1","category":"astro-ph.GA"}
{"created":"2024-04-25 05:36:47","title":"Limitations in Fluorescence-Detected Entangled Two-Photon-Absorption Experiments: Exploring the Low- to High-Gain Squeezing Regimes","abstract":"We closely replicated and extended a recent experiment (\"Spatial properties of entangled two-photon absorption,\" Phys. Rev. Lett. 129, 183601, 2022) that reportedly observed enhancement of two-photon absorption rates in molecular samples by using time-frequency-entangled photon pairs, and we found that in the low-flux regime, where such enhancement is theoretically predicted in-principle, the two-photon fluorescence signal is below detection threshold using current state-of-the-art methods. The results are important in the context of efforts to enable quantum-enhanced molecular spectroscopy and imaging at ultra-low optical flux. Using an optical parametric down-conversion photon-pair source that can be varied from the low-gain spontaneous regime to the high-gain squeezing regime, we observed two-photon-induced fluorescence in the high-gain regime but in the low-gain regime any fluorescence was below detection threshold. We supplemented the molecular fluorescence experiments with a study of nonlinear-optical sum-frequency generation, for which we are able to observe the low-to-high-gain crossover, thereby verifying our theoretical models and experimental techniques. The observed rates (or lack thereof) in both experiments are consistent with theoretical predictions and with our previous experiments, and indicate that time-frequency photon entanglement does not provide a practical means to enhance in-solution molecular two-photon fluorescence spectroscopy or imaging with current techniques.","sentences":["We closely replicated and extended a recent experiment (\"Spatial properties of entangled two-photon absorption,\" Phys.","Rev. Lett.","129, 183601, 2022) that reportedly observed enhancement of two-photon absorption rates in molecular samples by using time-frequency-entangled photon pairs, and we found that in the low-flux regime, where such enhancement is theoretically predicted in-principle, the two-photon fluorescence signal is below detection threshold using current state-of-the-art methods.","The results are important in the context of efforts to enable quantum-enhanced molecular spectroscopy and imaging at ultra-low optical flux.","Using an optical parametric down-conversion photon-pair source that can be varied from the low-gain spontaneous regime to the high-gain squeezing regime, we observed two-photon-induced fluorescence in the high-gain regime but in the low-gain regime any fluorescence was below detection threshold.","We supplemented the molecular fluorescence experiments with a study of nonlinear-optical sum-frequency generation, for which we are able to observe the low-to-high-gain crossover, thereby verifying our theoretical models and experimental techniques.","The observed rates (or lack thereof) in both experiments are consistent with theoretical predictions and with our previous experiments, and indicate that time-frequency photon entanglement does not provide a practical means to enhance in-solution molecular two-photon fluorescence spectroscopy or imaging with current techniques."],"url":"http://arxiv.org/abs/2404.16342v1","category":"quant-ph"}
{"created":"2024-04-25 05:05:13","title":"Stability of the Standard Model vacuum with respect to vacuum tunneling to the Komatsu vacuum in the cMSSM","abstract":"We investigate the stability of the Standard Model vacuum with respect to vacuum tunneling to the Komatsu vacuum, which exists when $m_L^2 + m_{H_u}^2<0$, in the cMSSM. Employing the numerical tools \\verb|SARAH|, \\verb|SPheno| and \\verb|CosmoTransitions|, we scan and constrain the parameter space of the cMSSM up to 10 TeV. Regions excluded due to having a vacuum tunneling half-life less than the age of the observable universe are concentrated near the regions where the Standard Model vacuum is tachyonic and are more stringent at smaller $m_0$, larger and negative $A_0$, and larger $\\tan\\beta$. New excluded regions, which satisfy $m_h \\simeq 125 \\text{GeV}$, are found.","sentences":["We investigate the stability of the Standard Model vacuum with respect to vacuum tunneling to the Komatsu vacuum, which exists when $m_L^2 + m_{H_u}^2<0$, in the cMSSM.","Employing the numerical tools \\verb|SARAH|, \\verb|SPheno| and \\verb|CosmoTransitions|, we scan and constrain the parameter space of the cMSSM up to 10 TeV. Regions excluded due to having a vacuum tunneling half-life less than the age of the observable universe are concentrated near the regions where the Standard Model vacuum is tachyonic and are more stringent at smaller $m_0$, larger and negative $A_0$, and larger $\\tan\\beta$. New excluded regions, which satisfy $m_h","\\simeq 125 \\text{GeV}$, are found."],"url":"http://arxiv.org/abs/2404.16337v1","category":"hep-ph"}
{"created":"2024-04-25 04:41:29","title":"Isometric Spectral Subtriples","abstract":"We investigate the notion of subsystem in the framework of spectral triple as a generalized notion of noncommutative submanifold. In the case of manifolds, we consider several conditions on Dirac operators which turn embedded submanifolds into isometric submanifolds. We then suggest a definition of spectral subtriple based on the notion of submanifold algebra and the already existing notions of Riemannian, isometric, and totally geodesic morphisms. We have shown that our definitions work at least in some relevant almost commutative examples.","sentences":["We investigate the notion of subsystem in the framework of spectral triple as a generalized notion of noncommutative submanifold.","In the case of manifolds, we consider several conditions on Dirac operators which turn embedded submanifolds into isometric submanifolds.","We then suggest a definition of spectral subtriple based on the notion of submanifold algebra and the already existing notions of Riemannian, isometric, and totally geodesic morphisms.","We have shown that our definitions work at least in some relevant almost commutative examples."],"url":"http://arxiv.org/abs/2404.16332v1","category":"math-ph"}
{"created":"2024-04-25 04:36:36","title":"Identifying the ground state phases by spin-patterns in the Shastry-Sutherland model","abstract":"Exploring the influence of frustration on the phases and related phase transitions in condensed matter physics is of fundamental importance in uncovering the role played by frustration. In the two-dimensional square lattice, a minimal frustration has been formulated in 1981 as the Shastry-Sutherland (SS) model described by competitions between the nearest-neighbor bond ($J_1$) and the next-nearest-neighbor one ($J_2$). In the two limits of $\\alpha=J_2/J_1$, i.e. $\\alpha \\ll 1$ and $\\alpha \\gg 1$, the corresponding phases are the N{\\'e}el antiferromagnet (AFM) and the dimer-singlet(DS). Unfortunately, the intermediate regime remains controversial, and the nature of transition from the N{\\'e}el AFM to the intermediate state is also unclear. Here we provide a pattern language to explore the SS model and take the lattice size $L=4 \\times4$ with periodic boundary condition. We firstly diagonalize the Hamiltonian in an operator space to obtain all fundamental spin-patterns and then analyze their energy and occupancy evolutions with the frustration parameter $\\kappa=\\alpha / (1+\\alpha)$. Our results indicate that the intermediate regime is characterized by diagonal two-domain spin-pattern while the N{\\'e}el AFM state has a diagonal single-domain and the DS has mixings of diagonal single- and four-domain. While the transition from the DS to the intermediate phase occurred around $\\alpha_c = 1.5$ is the first-order in nature, consistent with that in literature, the one from the intermediate phase to the AFM is clearly seen around $\\alpha_c = 1.277$, where it has a reversal of the contributions from the single- and two-domain patterns to the ground state. The result indicates that the pattern language is powerful in identifying the possible phases in frustrated models.","sentences":["Exploring the influence of frustration on the phases and related phase transitions in condensed matter physics is of fundamental importance in uncovering the role played by frustration.","In the two-dimensional square lattice, a minimal frustration has been formulated in 1981 as the Shastry-Sutherland (SS) model described by competitions between the nearest-neighbor bond ($J_1$) and the next-nearest-neighbor one ($J_2$).","In the two limits of $\\alpha=J_2/J_1$, i.e. $\\alpha \\ll 1$ and $\\alpha \\gg 1$, the corresponding phases are the N{\\'e}el antiferromagnet (AFM) and the dimer-singlet(DS).","Unfortunately, the intermediate regime remains controversial, and the nature of transition from the N{\\'e}el AFM to the intermediate state is also unclear.","Here we provide a pattern language to explore the SS model and take the lattice size $L=4 \\times4$ with periodic boundary condition.","We firstly diagonalize the Hamiltonian in an operator space to obtain all fundamental spin-patterns and then analyze their energy and occupancy evolutions with the frustration parameter $\\kappa=\\alpha / (1+\\alpha)$. Our results indicate that the intermediate regime is characterized by diagonal two-domain spin-pattern while the N{\\'e}el AFM state has a diagonal single-domain and the DS has mixings of diagonal single-","and","four-domain.","While the transition from the DS to the intermediate phase occurred around $\\alpha_c = 1.5$ is the first-order in nature, consistent with that in literature, the one from the intermediate phase to the AFM is clearly seen around $\\alpha_c = 1.277$, where it has a reversal of the contributions from the single- and two-domain patterns to the ground state.","The result indicates that the pattern language is powerful in identifying the possible phases in frustrated models."],"url":"http://arxiv.org/abs/2404.16330v1","category":"cond-mat.str-el"}
{"created":"2024-04-25 03:56:09","title":"The Continuous-Time Weighted-Median Opinion Dynamics","abstract":"Opinion dynamics models are important in understanding and predicting opinion formation processes within social groups. Although the weighted-averaging opinion-update mechanism is widely adopted as the micro-foundation of opinion dynamics, it bears a non-negligibly unrealistic implication: opinion attractiveness increases with opinion distance. Recently, the weighted-median mechanism has been proposed as a new microscopic mechanism of opinion exchange. Numerous advancements have been achieved regarding this new micro-foundation, from theoretical analysis to empirical validation, in a discrete-time asynchronous setup. However, the original discrete-time weighted-median model does not allow for \"compromise behavior\" in opinion exchanges, i.e., no intermediate opinions are created between disagreeing agents. To resolve this problem, this paper propose a novel continuous-time weighted-median opinion dynamics model, in which agents' opinions move towards the weighted-medians of their out-neighbors' opinions. It turns out that the proof methods for the original discrete-time asynchronous model are no longer applicable to the analysis of the continuous-time model. In this paper, we first establish the existence and uniqueness of the solution to the continuous-time weighted-median opinion dynamics by showing that the weighted-median mapping is contractive on any graph. We also characterize the set of all the equilibria. Then, by leveraging a new LaSalle invariance principle argument, we prove the convergence of the continuous-time weighted-median model for any initial condition and derive a necessary and sufficient condition for the convergence to consensus.","sentences":["Opinion dynamics models are important in understanding and predicting opinion formation processes within social groups.","Although the weighted-averaging opinion-update mechanism is widely adopted as the micro-foundation of opinion dynamics, it bears a non-negligibly unrealistic implication: opinion attractiveness increases with opinion distance.","Recently, the weighted-median mechanism has been proposed as a new microscopic mechanism of opinion exchange.","Numerous advancements have been achieved regarding this new micro-foundation, from theoretical analysis to empirical validation, in a discrete-time asynchronous setup.","However, the original discrete-time weighted-median model does not allow for \"compromise behavior\" in opinion exchanges, i.e., no intermediate opinions are created between disagreeing agents.","To resolve this problem, this paper propose a novel continuous-time weighted-median opinion dynamics model, in which agents' opinions move towards the weighted-medians of their out-neighbors' opinions.","It turns out that the proof methods for the original discrete-time asynchronous model are no longer applicable to the analysis of the continuous-time model.","In this paper, we first establish the existence and uniqueness of the solution to the continuous-time weighted-median opinion dynamics by showing that the weighted-median mapping is contractive on any graph.","We also characterize the set of all the equilibria.","Then, by leveraging a new LaSalle invariance principle argument, we prove the convergence of the continuous-time weighted-median model for any initial condition and derive a necessary and sufficient condition for the convergence to consensus."],"url":"http://arxiv.org/abs/2404.16318v1","category":"eess.SY"}
{"created":"2024-04-25 03:34:33","title":"Measurement of Interstellar Magnetization by Synchrotron Polarization Variance","abstract":"Since synchrotron polarization fluctuations are related to the fundamental properties of the magnetic field, we propose the polarization intensity variance to measure the Galactic interstellar medium (ISM) magnetization. We confirm the method's applicability by comparing it with the polarization angle dispersion and its reliability by measuring the underlying Alfv\\'enic Mach number of MHD turbulence. With the finding of the power-law relation of $\\mathcal{A} \\propto M_{\\rm A}^{2}$ between polarization intensity variance $\\mathcal{A}$ and Alfv\\'enic Mach number $M_{\\rm A}$, we apply the new technique to the Canadian Galactic Plane Survey (CGPS) data, achieving Alfv\\'enic Mach number of the Galactic ISM. Our results show that the low-latitude Galactic ISM is dominated by sub-Alf\\'enic turbulence, with $M_{\\rm A}$ approximately between 0.5 and 1.0.","sentences":["Since synchrotron polarization fluctuations are related to the fundamental properties of the magnetic field, we propose the polarization intensity variance to measure the Galactic interstellar medium (ISM) magnetization.","We confirm the method's applicability by comparing it with the polarization angle dispersion and its reliability by measuring the underlying Alfv\\'enic Mach number of MHD turbulence.","With the finding of the power-law relation of $\\mathcal{A} \\propto M_{\\rm A}^{2}$ between polarization intensity variance $\\mathcal{A}$ and Alfv\\'enic Mach number $M_{\\rm A}$, we apply the new technique to the Canadian Galactic Plane Survey (CGPS) data, achieving Alfv\\'enic","Mach number of the Galactic ISM.","Our results show that the low-latitude Galactic ISM is dominated by sub-Alf\\'enic turbulence, with $M_{\\rm A}$ approximately between 0.5 and 1.0."],"url":"http://arxiv.org/abs/2404.16310v1","category":"astro-ph.GA"}
{"created":"2024-04-25 02:54:43","title":"Rapid-scanned and self-corrected repetition rates enabled in a bidirectional polarization-multiplexed fiber laser","abstract":"Repetition-rate-scanned lasers are practical in accordion frequency comb generation that serves as a variable gearbox connecting optical and radio wave domains. Rapid and wide-range scanned repetition rate can benefit versatile purposes, however scanning robustness remains unsecured that typically requires complicated feedback loops. Recently, multiplexed lasers have been demonstrated with the nature of common-noise rejection among simultaneously emitted combs. Here, we propose a bidirectional polarization-multiplexed fiber laser that delivers synchronized pulses with rapid-scanned and reference-free repetition rates. Benefiting from the all polarization-maintaining fiber configuration, the laser shows good robustness and inter-comb coherence. As rapid as 493.5 kHz/s scanning rate over 329-kHz scanning range of fundamental repetition rate is realized. The 1-hour and 1-day maximal variations of difference frequency are merely 0.52 Hz and 5.46 Hz. The capability to rebuilt steady state after mode hopping is also demonstrated. These results provide a promising solution for developing high-performance accordion-frequency laser sources.","sentences":["Repetition-rate-scanned lasers are practical in accordion frequency comb generation that serves as a variable gearbox connecting optical and radio wave domains.","Rapid and wide-range scanned repetition rate can benefit versatile purposes, however scanning robustness remains unsecured that typically requires complicated feedback loops.","Recently, multiplexed lasers have been demonstrated with the nature of common-noise rejection among simultaneously emitted combs.","Here, we propose a bidirectional polarization-multiplexed fiber laser that delivers synchronized pulses with rapid-scanned and reference-free repetition rates.","Benefiting from the all polarization-maintaining fiber configuration, the laser shows good robustness and inter-comb coherence.","As rapid as 493.5 kHz/s scanning rate over 329-kHz scanning range of fundamental repetition rate is realized.","The 1-hour and 1-day maximal variations of difference frequency are merely 0.52","Hz and 5.46 Hz.","The capability to rebuilt steady state after mode hopping is also demonstrated.","These results provide a promising solution for developing high-performance accordion-frequency laser sources."],"url":"http://arxiv.org/abs/2404.16303v1","category":"physics.optics"}
{"created":"2024-04-25 00:44:35","title":"Relativistic tidal separation of binary stars by supermassive black holes","abstract":"A binary stellar system that ventures too close to a supermassive black hole can become tidally separated. In this article, we investigate the role of relativistic effects in these encounters through 3-body simulations. We use the Hybrid Relativistic-Newtonian Approximation (HRNA), which combines the exact relativistic acceleration from a Schwarzschild black hole with a Newtonian description of the binary's self-gravity. This method is compared against Newtonian and Post-Newtonian (1PN) simulations. Our findings show good agreement between HRNA and 1PN results, both of which exhibit substantial differences from Newtonian simulations. This discrepancy is particularly pronounced in retrograde encounters, where relativistic simulations predict up to $30\\%$ more separation events and an earlier onset of binary separation ($\\beta=2$ compared to $2.5$ in Newtonian simulations, with $\\beta$ the impact parameter). Additionally, the HRNA model predicts about 15$\\%$ more potential extreme mass ratio inspirals and generate a higher number of hypervelocity star candidates, with velocities up to 2,000 km/s faster than those predicted from Newtonian simulations. Furthermore, compared to Newtonian cases, relativistic encounters are more likely to result in direct stellar collisions and binary mergers.","sentences":["A binary stellar system that ventures too close to a supermassive black hole can become tidally separated.","In this article, we investigate the role of relativistic effects in these encounters through 3-body simulations.","We use the Hybrid Relativistic-Newtonian Approximation (HRNA), which combines the exact relativistic acceleration from a Schwarzschild black hole with a Newtonian description of the binary's self-gravity.","This method is compared against Newtonian and Post-Newtonian (1PN) simulations.","Our findings show good agreement between HRNA and 1PN results, both of which exhibit substantial differences from Newtonian simulations.","This discrepancy is particularly pronounced in retrograde encounters, where relativistic simulations predict up to $30\\%$ more separation events and an earlier onset of binary separation ($\\beta=2$ compared to $2.5$ in Newtonian simulations, with $\\beta$ the impact parameter).","Additionally, the HRNA model predicts about 15$\\%$ more potential extreme mass ratio inspirals and generate a higher number of hypervelocity star candidates, with velocities up to 2,000 km/s faster than those predicted from Newtonian simulations.","Furthermore, compared to Newtonian cases, relativistic encounters are more likely to result in direct stellar collisions and binary mergers."],"url":"http://arxiv.org/abs/2404.16270v1","category":"astro-ph.HE"}
{"created":"2024-04-25 00:17:49","title":"New Timing Results of MSPs from NICER Observations","abstract":"Millisecond pulsars (MSPs) are known for their long-term stability. Using six years of observations from the Neutron Star Interior Composition Explorer (NICER), we have conducted an in-depth analysis of the X-ray timing results for six MSPs: PSRs B1937+21, B1821$-$24, J0437$-$4715, J0030+0451, J0218+4232, and J2124$-$3358. The timing stability parameter $\\sigma_z$ has been calculated, revealing remarkable timing precision on the order of $10^{-14}$ for PSRs B1937+21 and J0437$-$4715, and $10^{-13}$ for PSRs B1821$-$24, J0218+4232, and J0030+0451 over a timescale of 1000 days. These findings underscore the feasibility of autonomous in-orbit timekeeping using X-ray observations of MSPs. In addition, the consistency of long-term spin-down noise in the X-ray and radio bands has been investigated by comparison with IPTA radio data.","sentences":["Millisecond pulsars (MSPs) are known for their long-term stability.","Using six years of observations from the Neutron Star Interior Composition Explorer (NICER), we have conducted an in-depth analysis of the X-ray timing results for six MSPs: PSRs B1937+21, B1821$-$24, J0437$-$4715, J0030+0451, J0218+4232, and J2124$-$3358.","The timing stability parameter $\\sigma_z$ has been calculated, revealing remarkable timing precision on the order of $10^{-14}$ for PSRs B1937+21 and J0437$-$4715, and $10^{-13}$ for PSRs B1821$-$24, J0218+4232, and J0030","+0451 over a timescale of 1000 days.","These findings underscore the feasibility of autonomous in-orbit timekeeping using X-ray observations of MSPs.","In addition, the consistency of long-term spin-down noise in the X-ray and radio bands has been investigated by comparison with IPTA radio data."],"url":"http://arxiv.org/abs/2404.16263v1","category":"astro-ph.HE"}
{"created":"2024-04-25 00:11:27","title":"Reconstructing Cosmic History: JWST-Extended Mapping of the Hubble Flow from z$ \\sim $0 to z$ \\sim$7.5 with HII Galaxies","abstract":"Over twenty years ago, Type Ia Supernovae (SNIa) [arXiv:astro-ph/9805201, arXiv:astro-ph/9812133] observations revealed an accelerating Universe expansion, suggesting a significant dark energy presence, often modelled as a cosmological constant, $\\Lambda$. Despite its pivotal role in cosmology, the standard $\\Lambda$CDM model remains largely underexplored in the redshift range between distant SNIa and the Cosmic Microwave Background (CMB). This study harnesses the James Webb Space Telescope's advanced capabilities to extend the Hubble flow mapping across an unprecedented redshift range, from $z \\approx 0$ to $z \\approx 7.5$. Utilising a dataset of 231 HII galaxies and extragalactic HII regions, we employ the $\\text{L}-\\sigma$ relation, correlating the luminosity of Balmer lines with their velocity dispersion, to define a competitive technique for measuring cosmic distances. This approach maps the Universe's expansion over more than 12 billion years, covering 95\\% of its age. Our analysis, using Bayesian inference, constrains the parameter space $\\lbrace h, \\Omega_m, w_0\\rbrace = \\lbrace 0.731\\pm0.039, 0.302^{+0.12}_{-0.069}, -1.01^{+0.52}_{-0.29}\\rbrace $ (statistical) for a flat Universe. These results provide new insights into cosmic evolution and suggest uniformity in the photo-kinematical properties of young massive ionizing clusters in giant HII regions and HII galaxies across most of the Universe's history.","sentences":["Over twenty years ago, Type Ia Supernovae (SNIa) [arXiv:astro-ph/9805201, arXiv:astro-ph/9812133] observations revealed an accelerating Universe expansion, suggesting a significant dark energy presence, often modelled as a cosmological constant, $\\Lambda$. Despite its pivotal role in cosmology, the standard $\\Lambda$CDM model remains largely underexplored in the redshift range between distant SNIa and the Cosmic Microwave Background (CMB).","This study harnesses the James Webb Space Telescope's advanced capabilities to extend the Hubble flow mapping across an unprecedented redshift range, from $z \\approx 0$ to $z \\approx 7.5$. Utilising a dataset of 231 HII galaxies and extragalactic HII regions, we employ the $\\text{L}-\\sigma$ relation, correlating the luminosity of Balmer lines with their velocity dispersion, to define a competitive technique for measuring cosmic distances.","This approach maps the Universe's expansion over more than 12 billion years, covering 95\\% of its age.","Our analysis, using Bayesian inference, constrains the parameter space $\\lbrace h, \\Omega_m, w_0\\rbrace = \\lbrace 0.731\\pm0.039, 0.302^{+0.12}_{-0.069}, -1.01^{+0.52}_{-0.29}\\rbrace $ (statistical) for a flat Universe.","These results provide new insights into cosmic evolution and suggest uniformity in the photo-kinematical properties of young massive ionizing clusters in giant HII regions and HII galaxies across most of the Universe's history."],"url":"http://arxiv.org/abs/2404.16261v1","category":"astro-ph.CO"}
{"created":"2024-04-24 23:32:42","title":"Confronting the Diversity Problem: The Limits of Galaxy Rotation Curves as a tool to Understand Dark Matter Profiles","abstract":"While galaxy rotation curves provide one of the most powerful methods for measuring dark matter profiles in the inner regions of rotation-supported galaxies, at the dwarf scale there are factors that can complicate this analysis. Given the expectation of a universal profile in dark matter-only simulations, the diversity of observed rotation curves has become an often-discussed issue in Lambda Cold Dark Matter cosmology on galactic scales. We analyze a suite of Feedback in Realistic Environments (FIRE) simulations of $10^{10}-10^{12}$ $M_\\odot$ halos with standard cold dark matter, and compare the true circular velocity to rotation curve reconstructions. We find that, for galaxies with well-ordered gaseous disks, the measured rotation curve may deviate from true circular velocity by at most 10% within the radius of the disk. However, non-equilibrium behavior, non-circular motions, and non-thermal and non-kinetic stresses may cause much larger discrepancies of 50% or more. Most rotation curve reconstructions underestimate the true circular velocity, while some reconstructions transiently over-estimate it in the central few kiloparsecs due to dynamical phenomena. We further demonstrate that the features that contribute to these failures are not always visibly obvious in HI observations. If such dwarf galaxies are included in galaxy catalogs, they may give rise to the appearance of \"artificial\" rotation curve diversity that does not reflect the true variation in underlying dark matter profiles.","sentences":["While galaxy rotation curves provide one of the most powerful methods for measuring dark matter profiles in the inner regions of rotation-supported galaxies, at the dwarf scale there are factors that can complicate this analysis.","Given the expectation of a universal profile in dark matter-only simulations, the diversity of observed rotation curves has become an often-discussed issue in Lambda Cold Dark Matter cosmology on galactic scales.","We analyze a suite of Feedback in Realistic Environments (FIRE) simulations of $10^{10}-10^{12}$ $M_\\odot$ halos with standard cold dark matter, and compare the true circular velocity to rotation curve reconstructions.","We find that, for galaxies with well-ordered gaseous disks, the measured rotation curve may deviate from true circular velocity by at most 10% within the radius of the disk.","However, non-equilibrium behavior, non-circular motions, and non-thermal and non-kinetic stresses may cause much larger discrepancies of 50% or more.","Most rotation curve reconstructions underestimate the true circular velocity, while some reconstructions transiently over-estimate it in the central few kiloparsecs due to dynamical phenomena.","We further demonstrate that the features that contribute to these failures are not always visibly obvious in HI observations.","If such dwarf galaxies are included in galaxy catalogs, they may give rise to the appearance of \"artificial\" rotation curve diversity that does not reflect the true variation in underlying dark matter profiles."],"url":"http://arxiv.org/abs/2404.16247v1","category":"astro-ph.GA"}
{"created":"2024-04-24 22:59:30","title":"Young Stellar Objects in NGC 346: A JWST NIRCam/MIRI Imaging Survey","abstract":"We present a JWST imaging survey with NIRCam and MIRI of NGC 346, the brightest star-forming region in the Small Magellanic Cloud (SMC). By combining aperture and point spread function (PSF) photometry of eleven wavelength bands across these two instruments, we have detected more than 200,000 unique sources. Using near-infrared (IR) color analysis, we observe various evolved and young populations, including 196 young stellar objects (YSOs) and pre-main sequence stars suitable for forthcoming spectroscopic studies. We expand upon this work, creating mid-IR color-magnitude diagrams and determining color cuts to identify 833 reddened sources which are YSO candidates. We observe that these candidate sources are spatially associated with regions of dusty, filamentary nebulosity. Furthermore, we fit model YSO spectral energy distributions (SEDs) to a selection of sources with detections across all of our MIRI bands. We classify with a high degree of confidence 23 YSOs in this sample and estimate their radii, bolometric temperatures, luminosities, and masses. We detect YSOs approaching 1 solar mass, the lowest-mass extragalactic YSOs confirmed to date.","sentences":["We present a JWST imaging survey with NIRCam and MIRI of NGC 346, the brightest star-forming region in the Small Magellanic Cloud (SMC).","By combining aperture and point spread function (PSF) photometry of eleven wavelength bands across these two instruments, we have detected more than 200,000 unique sources.","Using near-infrared (IR) color analysis, we observe various evolved and young populations, including 196 young stellar objects (YSOs) and pre-main sequence stars suitable for forthcoming spectroscopic studies.","We expand upon this work, creating mid-IR color-magnitude diagrams and determining color cuts to identify 833 reddened sources which are YSO candidates.","We observe that these candidate sources are spatially associated with regions of dusty, filamentary nebulosity.","Furthermore, we fit model YSO spectral energy distributions (SEDs) to a selection of sources with detections across all of our MIRI bands.","We classify with a high degree of confidence 23 YSOs in this sample and estimate their radii, bolometric temperatures, luminosities, and masses.","We detect YSOs approaching 1 solar mass, the lowest-mass extragalactic YSOs confirmed to date."],"url":"http://arxiv.org/abs/2404.16242v1","category":"astro-ph.SR"}
{"created":"2024-04-24 22:33:09","title":"Dynamically induced multiferroic polarization","abstract":"We describe a mechanism by which both ferroelectric polarization and magnetization can be created in nonpolar, nonmagnetic materials. Using a combination of phenomenological modeling and first-principles calculations, we demonstrate that ferroelectric polarization, magnetization, or both simultaneously can be transiently induced by an ultrashort laser pulse upon linearly, circularly, or elliptically polarized excitation of phonon modes in $\\gamma$-LiBO$_2$. The direction and magnitude of the multiferroic polarization can be controlled by the chirality of the laser pulse and the phonon modes, offering a pathway for controlling multiferroicity and magnetoelectricity on ultrafast timescales.","sentences":["We describe a mechanism by which both ferroelectric polarization and magnetization can be created in nonpolar, nonmagnetic materials.","Using a combination of phenomenological modeling and first-principles calculations, we demonstrate that ferroelectric polarization, magnetization, or both simultaneously can be transiently induced by an ultrashort laser pulse upon linearly, circularly, or elliptically polarized excitation of phonon modes in $\\gamma$-LiBO$_2$. The direction and magnitude of the multiferroic polarization can be controlled by the chirality of the laser pulse and the phonon modes, offering a pathway for controlling multiferroicity and magnetoelectricity on ultrafast timescales."],"url":"http://arxiv.org/abs/2404.16234v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-24 22:05:02","title":"Pair creation in a space with spin noncommutativity of coordinates","abstract":"We study the Schwinger pair creation process for Dirac fermions in the presence of a Volkov plane wave and a constant external electric field by applying spin non-commutativity of coordinates. Using the Schwinger proper time method, the probability of pair creation from vacuum is calculated precisely and analytically. In the case of the Volkov plane wave, it is shown that the probability is zero. However, for a constant electric field, the spin noncommutativity of the coordinates contributes to the pair creation process and affects the calculated probability.","sentences":["We study the Schwinger pair creation process for Dirac fermions in the presence of a Volkov plane wave and a constant external electric field by applying spin non-commutativity of coordinates.","Using the Schwinger proper time method, the probability of pair creation from vacuum is calculated precisely and analytically.","In the case of the Volkov plane wave, it is shown that the probability is zero.","However, for a constant electric field, the spin noncommutativity of the coordinates contributes to the pair creation process and affects the calculated probability."],"url":"http://arxiv.org/abs/2404.16230v1","category":"hep-th"}
{"created":"2024-04-24 22:03:39","title":"Two qubit gate with macroscopic singlet-triplet qubits in synthetic spin-one chains in InAsP quantum dot nanowires","abstract":"We present a theory of a two qubit gate with macroscopic singlet-triplet (ST) qubits in synthetic spin-one chains in InAsP quantum dot nanowires. The macroscopic topologically protected singlet-triplet qubits are built with two spin-half Haldane quasiparticles. The Haldane quasiparticles are hosted by synthetic spin-one chain realized in chains of InAsP quantum dots embedded in an InP nanowire, with four electrons each. The quantum dot nanowire is described by a Hubbard-Kanamori (HK) Hamiltonian derived from an interacting atomistic model. Using exact diagonalization and Matrix Product States (MPS) tools, we demonstrate that the low-energy behavior of the HK Hamiltonian is effectively captured by an antiferromagnetic spin-one chain Hamiltonian. Next we consider two macroscopic qubits and present a method for creating a tunable coupling between the two macroscopic qubits by inserting an intermediate control dot between the two chains. Finally, we propose and demonstrate two approaches for generating highly accurate two-ST qubit gates : (1) by controlling the length of each qubit, and (2) by employing different background magnetic fields for the two qubits.","sentences":["We present a theory of a two qubit gate with macroscopic singlet-triplet (ST) qubits in synthetic spin-one chains in InAsP quantum dot nanowires.","The macroscopic topologically protected singlet-triplet qubits are built with two spin-half Haldane quasiparticles.","The Haldane quasiparticles are hosted by synthetic spin-one chain realized in chains of InAsP quantum dots embedded in an InP nanowire, with four electrons each.","The quantum dot nanowire is described by a Hubbard-Kanamori (HK) Hamiltonian derived from an interacting atomistic model.","Using exact diagonalization and Matrix Product States (MPS) tools, we demonstrate that the low-energy behavior of the HK Hamiltonian is effectively captured by an antiferromagnetic spin-one chain Hamiltonian.","Next we consider two macroscopic qubits and present a method for creating a tunable coupling between the two macroscopic qubits by inserting an intermediate control dot between the two chains.","Finally, we propose and demonstrate two approaches for generating highly accurate two-ST qubit gates : (1) by controlling the length of each qubit, and (2) by employing different background magnetic fields for the two qubits."],"url":"http://arxiv.org/abs/2404.16229v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-24 21:09:56","title":"Efficient Data Management for IPFS dApps","abstract":"Inefficient data management has been the Achilles heel of blockchain-based decentralized applications (dApps). An off-chain storage layer, which lies between the application and the blockchain layers, can improve space efficiency and data availability with erasure codes and decentralized maintenance. This paper presents two fundamental components of such storage layer designed and implemented for the IPFS network. The IPFS Community is a component built on top of the IPFS network that encodes and decodes data before uploading to the network. Since data is encoded with alpha entanglement codes, the solution requires less storage space than the native IPFS solution which replicates data by pinning content with the IPFS Cluster. To detect and repair failures in a timely manner, we introduce the monitoring and repair component. This novel component is activated by any node and distributes the load of repairs among various nodes. These two components are implemented as pluggable modules, and can, therefore, be easily migrated to other distributed file systems by adjusting the connector component.","sentences":["Inefficient data management has been the Achilles heel of blockchain-based decentralized applications (dApps).","An off-chain storage layer, which lies between the application and the blockchain layers, can improve space efficiency and data availability with erasure codes and decentralized maintenance.","This paper presents two fundamental components of such storage layer designed and implemented for the IPFS network.","The IPFS Community is a component built on top of the IPFS network that encodes and decodes data before uploading to the network.","Since data is encoded with alpha entanglement codes, the solution requires less storage space than the native IPFS solution which replicates data by pinning content with the IPFS Cluster.","To detect and repair failures in a timely manner, we introduce the monitoring and repair component.","This novel component is activated by any node and distributes the load of repairs among various nodes.","These two components are implemented as pluggable modules, and can, therefore, be easily migrated to other distributed file systems by adjusting the connector component."],"url":"http://arxiv.org/abs/2404.16210v1","category":"cs.NI"}
{"created":"2024-04-24 20:54:38","title":"Theoretical Analysis of the RX J0209.6-7427 X-ray Spectrum During Giant Outburst","abstract":"We model the spectral formation occurring in the binary X-ray pulsar RX~J0209.6-7427 during the 2019 super-Eddington outburst. Using a theoretical model previously developed by the authors, we are able to produce spectra that closely resemble the phase-averaged X-ray spectra observed using NuSTAR and Insight-HXMT during low and high luminosity states of the outburst, respectively. The theoretical model simulates the accretion of fully ionized gas in a dipole magnetic field, and includes a complete description of the radiation hydrodynamics, matter distribution, and spectral formation. Type II X-ray outbursts provide an opportunity to study accretion over a large range of luminosities for the same neutron star. The analysis performed here represents the first time both the outburst low and high states of an accretion-powered X-ray pulsar are modeled using a physics-based model rather than standard phenomenological fitting with arbitrary mathematical functions. We find the outer polar cap radius remains constant and the column is more fully-filled with increasing luminosity, Comptonized bremsstrahlung dominates the formation of the phase-averaged X-ray spectrum, and a negative correlation exists between cyclotron centroid energy and luminosity, as expected. The super-Eddington nature of the outburst is rendered possible due to the low scattering cross section for photons propagating parallel to the magnetic field. We also find emission through the column top dominates in both the low and high states, implying the pulse profiles should have a roughly sinusoidal shape, which agrees with observed properties of ultra-luminous X-ray pulsars.","sentences":["We model the spectral formation occurring in the binary X-ray pulsar RX~J0209.6-7427 during the 2019 super-Eddington outburst.","Using a theoretical model previously developed by the authors, we are able to produce spectra that closely resemble the phase-averaged X-ray spectra observed using NuSTAR and Insight-HXMT during low and high luminosity states of the outburst, respectively.","The theoretical model simulates the accretion of fully ionized gas in a dipole magnetic field, and includes a complete description of the radiation hydrodynamics, matter distribution, and spectral formation.","Type II X-ray outbursts provide an opportunity to study accretion over a large range of luminosities for the same neutron star.","The analysis performed here represents the first time both the outburst low and high states of an accretion-powered X-ray pulsar are modeled using a physics-based model rather than standard phenomenological fitting with arbitrary mathematical functions.","We find the outer polar cap radius remains constant and the column is more fully-filled with increasing luminosity, Comptonized bremsstrahlung dominates the formation of the phase-averaged X-ray spectrum, and a negative correlation exists between cyclotron centroid energy and luminosity, as expected.","The super-Eddington nature of the outburst is rendered possible due to the low scattering cross section for photons propagating parallel to the magnetic field.","We also find emission through the column top dominates in both the low and high states, implying the pulse profiles should have a roughly sinusoidal shape, which agrees with observed properties of ultra-luminous X-ray pulsars."],"url":"http://arxiv.org/abs/2404.16202v1","category":"astro-ph.HE"}
{"created":"2024-04-24 20:27:24","title":"Position-space renormalization schemes for four-quark operators in HQET","abstract":"X-space schemes are gauge-invariant, regulator-independent renormalization schemes that are defined by requiring position-space correlation functions of gauge invariant operators to be equal to their noninteracting values at particular kinematic points. These schemes can be used to nonperturbatively renormalize composite operators in Lattice Quantum Chromodynamics (LQCD), and by computing matching coefficients between the X-space scheme and MSbar in the dimensionally-regulated continuum, matrix elements calculated with LQCD can be converted to MSbar-renormalized matrix elements. Using X-space schemes for Heavy Quark Effective Theory (HQET) operators has the additional benefit that appropriate ratios of position-space correlation functions cancel the power divergent static-quark self-energy of Lattice HQET nonperturbatively. This work presents the O($\\alpha_S$) matching coefficients between X-space renormalized four-quark flavor-nonsinglet HQET operators relevant for the lifetimes of charm- and bottom-hadrons, and four-quark HQET operators relevant for mixing between neutral mesons containing a heavy quark, such as B-Bbar mixing.","sentences":["X-space schemes are gauge-invariant, regulator-independent renormalization schemes that are defined by requiring position-space correlation functions of gauge invariant operators to be equal to their noninteracting values at particular kinematic points.","These schemes can be used to nonperturbatively renormalize composite operators in Lattice Quantum Chromodynamics (LQCD), and by computing matching coefficients between the X-space scheme and MSbar in the dimensionally-regulated continuum, matrix elements calculated with LQCD can be converted to MSbar-renormalized matrix elements.","Using X-space schemes for Heavy Quark Effective Theory (HQET) operators has the additional benefit that appropriate ratios of position-space correlation functions cancel the power divergent static-quark self-energy of Lattice HQET nonperturbatively.","This work presents the O($\\alpha_S$) matching coefficients between X-space renormalized four-quark flavor-nonsinglet HQET operators relevant for the lifetimes of charm- and bottom-hadrons, and four-quark HQET operators relevant for mixing between neutral mesons containing a heavy quark, such as B-Bbar mixing."],"url":"http://arxiv.org/abs/2404.16191v1","category":"hep-lat"}
{"created":"2024-04-24 20:17:21","title":"Offset of M54 from the Sagittarius Dwarf Spheroidal Galaxy","abstract":"We present results from simultaneous modeling of 2D (projected along the line of sight) position, proper motion and line-of-sight velocity for \\textit{Gaia}- and APOGEE-observed stars near the centre of the Sagittarius (Sgr) dwarf spheroidal galaxy. We use a mixture model that allows for independent sub-populations contributed by the Sgr galaxy, its nuclear star cluster M54, and the Milky Way foreground. We find an offset of $0.295\\pm 0.029$ degrees between the inferred centroids of Sgr and M54, corresponding to a (projected) physical separation of $0.135\\pm 0.013$ kpc. The detected offset might plausibly be driven by unmodelled asymmetry in Sgr's stellar configuration; however, standard criteria for model selection favour our symmetric model over an alternative that allows for bilateral asymmetry. We infer an offset between the proper motion centres of Sgr and M54 of $[\\Delta\\mu_{\\alpha}\\cos\\delta,\\Delta\\mu_{\\delta}]=[4.9, -19.7] \\pm [6.8, 6.2]$ $\\mu$as yr$^{-1}$, with magnitude similar to the covariance expected due to spatially-correlated systematic error. We infer an offset of $4.1\\pm 1.2$ km s$^{-1}$ in line-of-sight velocity. Using inferred values for the systemic positions and motions of Sgr and M54 as initial conditions, we calculate the recent orbital history of a simplified Sgr/M54 system, which we demonstrate to be sensitive to any line-of-sight distance offset between M54 and Sgr, and to the distribution of dark matter within Sgr.","sentences":["We present results from simultaneous modeling of 2D (projected along the line of sight) position, proper motion and line-of-sight velocity for \\textit{Gaia}- and APOGEE-observed stars near the centre of the Sagittarius (Sgr) dwarf spheroidal galaxy.","We use a mixture model that allows for independent sub-populations contributed by the Sgr galaxy, its nuclear star cluster M54, and the Milky Way foreground.","We find an offset of $0.295\\pm 0.029$ degrees between the inferred centroids of Sgr and M54, corresponding to a (projected) physical separation of $0.135\\pm 0.013$ kpc.","The detected offset might plausibly be driven by unmodelled asymmetry in Sgr's stellar configuration; however, standard criteria for model selection favour our symmetric model over an alternative that allows for bilateral asymmetry.","We infer an offset between the proper motion centres of Sgr and M54 of $[\\Delta\\mu_{\\alpha}\\cos\\delta,\\Delta\\mu_{\\delta}]=[4.9, -19.7] \\pm [6.8, 6.2]$ $\\mu$as yr$^{-1}$, with magnitude similar to the covariance expected due to spatially-correlated systematic error.","We infer an offset of $4.1\\pm 1.2$ km s$^{-1}$ in line-of-sight velocity.","Using inferred values for the systemic positions and motions of Sgr and M54 as initial conditions, we calculate the recent orbital history of a simplified Sgr/M54 system, which we demonstrate to be sensitive to any line-of-sight distance offset between M54 and Sgr, and to the distribution of dark matter within Sgr."],"url":"http://arxiv.org/abs/2404.16184v1","category":"astro-ph.GA"}
{"created":"2024-04-24 20:05:01","title":"Position dependent radiation fields near accretion disks","abstract":"In disk wind models for active galactic nuclei (AGN) outflows, high-energy radiation poses a significant problem wherein the gas can become overionized, effectively disabling what is often inferred to be the largest force acting on the gas: the radiation force due to spectral line opacity. Calculations of this radiation force depend on the magnitude of ionizing radiation, which can strongly depend on the position above a disk where the radiation is anisotropic. As our first step to quantify the position and direction dependence of the radiation field, we assumed free streaming of photons and computed energy distributions of the mean intensity and components of flux as well as energy-integrated quantities such as mean photon energy. We find a significant dependence of radiation field properties on position, but this dependence is not necessarily the same for different field quantities. A key example is that the mean intensity is much softer than the radial flux at many points near the disk. Because the mean intensity largely controls ionization, this softening decreases the severity of the overionization problem. The position dependence of mean intensity implies the position dependence of gas opacity, which we illustrate by computing the radiation force a fluid element feels in an accelerating wind. We find that in a vertical accelerating flow, the force due to radiation is not parallel to the radiation flux. This misalignment is due to the force's geometric weighting by both the velocity field's directionality and the position dependence of the mean intensity.","sentences":["In disk wind models for active galactic nuclei (AGN) outflows, high-energy radiation poses a significant problem wherein the gas can become overionized, effectively disabling what is often inferred to be the largest force acting on the gas: the radiation force due to spectral line opacity.","Calculations of this radiation force depend on the magnitude of ionizing radiation, which can strongly depend on the position above a disk where the radiation is anisotropic.","As our first step to quantify the position and direction dependence of the radiation field, we assumed free streaming of photons and computed energy distributions of the mean intensity and components of flux as well as energy-integrated quantities such as mean photon energy.","We find a significant dependence of radiation field properties on position, but this dependence is not necessarily the same for different field quantities.","A key example is that the mean intensity is much softer than the radial flux at many points near the disk.","Because the mean intensity largely controls ionization, this softening decreases the severity of the overionization problem.","The position dependence of mean intensity implies the position dependence of gas opacity, which we illustrate by computing the radiation force a fluid element feels in an accelerating wind.","We find that in a vertical accelerating flow, the force due to radiation is not parallel to the radiation flux.","This misalignment is due to the force's geometric weighting by both the velocity field's directionality and the position dependence of the mean intensity."],"url":"http://arxiv.org/abs/2404.16175v1","category":"astro-ph.GA"}
{"created":"2024-04-24 20:03:31","title":"Mirror Construction for Nakajima Quiver Varieties","abstract":"In this paper, we construct the ADHM quiver representations and the corresponding sheaves as the mirror objects of formal deformations of the framed immersed Lagrangian sphere decorated with flat bundles. More generally, framed double quivers of Nakajima are constructed as localized mirrors of framed Lagrangian immersions in dimension two. This produces a localized mirror functor to the dg category of modules over the framed preprojective algebra.   For affine ADE quivers in specific multiplicities, the corresponding (unframed) Lagrangian immersions are homological tori, whose moduli of stable deformations are asymptotically locally Euclidean (ALE) spaces. We show that framed stable Lagrangian branes are transformed into monadic complexes of framed torsion-free sheaves over the ALE spaces.   A main ingredient is the notion of framed Lagrangian immersions. Moreover, it is important to note that the deformation space of a Lagrangian immersion with more than one component is stacky. Using the formalism of quiver algebroid stacks, we find isomorphisms between the moduli of stable Lagrangian immersions and that of special Lagrangian fibers of an SYZ fibration in the affine $A_n$ cases.","sentences":["In this paper, we construct the ADHM quiver representations and the corresponding sheaves as the mirror objects of formal deformations of the framed immersed Lagrangian sphere decorated with flat bundles.","More generally, framed double quivers of Nakajima are constructed as localized mirrors of framed Lagrangian immersions in dimension two.","This produces a localized mirror functor to the dg category of modules over the framed preprojective algebra.   ","For affine ADE quivers in specific multiplicities, the corresponding (unframed) Lagrangian immersions are homological tori, whose moduli of stable deformations are asymptotically locally Euclidean (ALE) spaces.","We show that framed stable Lagrangian branes are transformed into monadic complexes of framed torsion-free sheaves over the ALE spaces.   ","A main ingredient is the notion of framed Lagrangian immersions.","Moreover, it is important to note that the deformation space of a Lagrangian immersion with more than one component is stacky.","Using the formalism of quiver algebroid stacks, we find isomorphisms between the moduli of stable Lagrangian immersions and that of special Lagrangian fibers of an SYZ fibration in the affine $A_n$ cases."],"url":"http://arxiv.org/abs/2404.16172v1","category":"math.AG"}
