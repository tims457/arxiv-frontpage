{"created":"2024-02-12 18:59:39","title":"FAST: Factorizable Attention for Speeding up Transformers","abstract":"Motivated by the factorization inherent in the original fast multipole method and the improved fast Gauss transform we introduce a factorable form of attention that operates efficiently in high dimensions. This approach reduces the computational and memory complexity of the attention mechanism in transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our work presents a linearly scaled attention mechanism that maintains the full representation of the attention matrix without compromising on sparsification and incorporates the all-to-all relationship between tokens. We explore the properties of our new attention metric and conduct tests in various standard settings. Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where self-attention is used.","sentences":["Motivated by the factorization inherent in the original fast multipole method and the improved fast Gauss transform we introduce a factorable form of attention that operates efficiently in high dimensions.","This approach reduces the computational and memory complexity of the attention mechanism in transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our work presents a linearly scaled attention mechanism that maintains the full representation of the attention matrix without compromising on sparsification and incorporates the all-to-all relationship between tokens.","We explore the properties of our new attention metric and conduct tests in various standard settings.","Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where self-attention is used."],"url":"http://arxiv.org/abs/2402.07901v1","category":"cs.LG"}
{"created":"2024-02-12 18:58:58","title":"A systematic investigation of learnability from single child linguistic input","abstract":"Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematically train six different model architectures on five datasets (3 single-child and 2 baselines). We find that the models trained on single-child datasets showed consistent results that matched with previous work, underscoring the robustness of forming meaningful syntactic and semantic representations from a subset of a child's linguistic input.","sentences":["Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability.","However, a significant gap exists between the training data for these models and the linguistic input a child receives.","LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a).","Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input.","Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset.","Here, to examine the robustness of learnability from single-child input, we systematically train six different model architectures on five datasets (3 single-child and 2 baselines).","We find that the models trained on single-child datasets showed consistent results that matched with previous work, underscoring the robustness of forming meaningful syntactic and semantic representations from a subset of a child's linguistic input."],"url":"http://arxiv.org/abs/2402.07899v1","category":"cs.CL"}
{"created":"2024-02-12 18:57:06","title":"Detection of Spider Mites on Labrador Beans through Machine Learning Approaches Using Custom Datasets","abstract":"Amidst growing food production demands, early plant disease detection is essential to safeguard crops; this study proposes a visual machine learning approach for plant disease detection, harnessing RGB and NIR data collected in real-world conditions through a JAI FS-1600D-10GE camera to build an RGBN dataset. A two-stage early plant disease detection model with YOLOv8 and a sequential CNN was used to train on a dataset with partial labels, which showed a 3.6% increase in mAP compared to a single-stage end-to-end segmentation model. The sequential CNN model achieved 90.62% validation accuracy utilising RGBN data. An average of 6.25% validation accuracy increase is found using RGBN in classification compared to RGB using ResNet15 and the sequential CNN models. Further research and dataset improvements are needed to meet food production demands.","sentences":["Amidst growing food production demands, early plant disease detection is essential to safeguard crops; this study proposes a visual machine learning approach for plant disease detection, harnessing RGB and NIR data collected in real-world conditions through a JAI FS-1600D-10GE camera to build an RGBN dataset.","A two-stage early plant disease detection model with YOLOv8 and a sequential CNN was used to train on a dataset with partial labels, which showed a 3.6% increase in mAP compared to a single-stage end-to-end segmentation model.","The sequential CNN model achieved 90.62% validation accuracy utilising RGBN data.","An average of 6.25% validation accuracy increase is found using RGBN in classification compared to RGB using ResNet15 and the sequential CNN models.","Further research and dataset improvements are needed to meet food production demands."],"url":"http://arxiv.org/abs/2402.07895v1","category":"cs.CV"}
{"created":"2024-02-12 18:55:43","title":"Consistent eccentricities for gravitational wave astronomy: Resolving discrepancies between astrophysical simulations and waveform models","abstract":"Detecting imprints of orbital eccentricity in gravitational wave signals promises to shed light on the formation mechanisms of binary black holes. To constrain formation mechanisms, distributions of eccentricity derived from numerical simulations of astrophysical formation channels are compared to the estimates of eccentricity inferred from GW signals. We report that the definition of eccentricity typically used in astrophysical simulations is inconsistent with the one used while modeling GW signals, with the differences mainly arising due to the choice of reference frequency used in both cases. We also posit a prescription for calculating eccentricity from astrophysical simulations by evolving ordinary differential equations obtained from post-Newtonian theory, and using the dominant ($\\ell = m =2$) mode's frequency as the reference frequency; this ensures consistency in the definitions. On comparing the existing eccentricities of binaries present in the Cluster Monte Carlo catalog of globular cluster simulations with the eccentricities calculated using the prescription presented here, we find a significant discrepancy at $e \\gtrsim 0.2$; this discrepancy becomes worse with increasing eccentricity. We note the implications this discrepancy has for existing studies, and recommend that care be taken when comparing data-driven constraints on eccentricity to expectations from astrophysical formation channels.","sentences":["Detecting imprints of orbital eccentricity in gravitational wave signals promises to shed light on the formation mechanisms of binary black holes.","To constrain formation mechanisms, distributions of eccentricity derived from numerical simulations of astrophysical formation channels are compared to the estimates of eccentricity inferred from GW signals.","We report that the definition of eccentricity typically used in astrophysical simulations is inconsistent with the one used while modeling GW signals, with the differences mainly arising due to the choice of reference frequency used in both cases.","We also posit a prescription for calculating eccentricity from astrophysical simulations by evolving ordinary differential equations obtained from post-Newtonian theory, and using the dominant ($\\ell = m =2$) mode's frequency as the reference frequency; this ensures consistency in the definitions.","On comparing the existing eccentricities of binaries present in the Cluster Monte Carlo catalog of globular cluster simulations with the eccentricities calculated using the prescription presented here, we find a significant discrepancy at $e \\gtrsim 0.2$; this discrepancy becomes worse with increasing eccentricity.","We note the implications this discrepancy has for existing studies, and recommend that care be taken when comparing data-driven constraints on eccentricity to expectations from astrophysical formation channels."],"url":"http://arxiv.org/abs/2402.07892v1","category":"astro-ph.HE"}
{"created":"2024-02-12 18:54:02","title":"Label-Efficient Model Selection for Text Generation","abstract":"Model selection for a given target task can be costly, as it may entail extensive annotation of the quality of outputs of different models. We introduce DiffUse, an efficient method to make an informed decision between candidate text generation models. DiffUse reduces the required amount of preference annotations, thus saving valuable time and resources in performing evaluation. DiffUse intelligently selects instances by clustering embeddings that represent the semantic differences between model outputs. Thus, it is able to identify a subset of examples that are more informative for preference decisions. Our method is model-agnostic, and can be applied to any text generation model. Moreover, we propose a practical iterative approach for dynamically determining how many instances to annotate. In a series of experiments over hundreds of model pairs, we demonstrate that DiffUse can dramatically reduce the required number of annotations -- by up to 75% -- while maintaining high evaluation reliability.","sentences":["Model selection for a given target task can be costly, as it may entail extensive annotation of the quality of outputs of different models.","We introduce DiffUse, an efficient method to make an informed decision between candidate text generation models.","DiffUse reduces the required amount of preference annotations, thus saving valuable time and resources in performing evaluation.","DiffUse intelligently selects instances by clustering embeddings that represent the semantic differences between model outputs.","Thus, it is able to identify a subset of examples that are more informative for preference decisions.","Our method is model-agnostic, and can be applied to any text generation model.","Moreover, we propose a practical iterative approach for dynamically determining how many instances to annotate.","In a series of experiments over hundreds of model pairs, we demonstrate that DiffUse can dramatically reduce the required number of annotations -- by up to 75% -- while maintaining high evaluation reliability."],"url":"http://arxiv.org/abs/2402.07891v1","category":"cs.CL"}
{"created":"2024-02-12 18:53:20","title":"MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning","abstract":"Distributed decision-making in multi-agent systems presents difficult challenges for interactive behavior learning in both cooperative and competitive systems. To mitigate this complexity, MAIDRL presents a semi-centralized Dense Reinforcement Learning algorithm enhanced by agent influence maps (AIMs), for learning effective multi-agent control on StarCraft Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement Learning, MAIDCRL, by incorporating convolutional layers into the deep model architecture, and evaluate the performance on both homogeneous and heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL significantly improved the learning performance and achieved a faster learning rate compared to the existing MAIDRL, especially on more complicated heterogeneous SMAC scenarios. We further investigate the stability and robustness of our model. The statistics reflect that our model not only achieves higher winning rate in all the given scenarios but also boosts the agent's learning process in fine-grained decision-making.","sentences":["Distributed decision-making in multi-agent systems presents difficult challenges for interactive behavior learning in both cooperative and competitive systems.","To mitigate this complexity, MAIDRL presents a semi-centralized Dense Reinforcement Learning algorithm enhanced by agent influence maps (AIMs), for learning effective multi-agent control on StarCraft Multi-Agent Challenge (SMAC) scenarios.","In this paper, we extend the DenseNet in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement Learning, MAIDCRL, by incorporating convolutional layers into the deep model architecture, and evaluate the performance on both homogeneous and heterogeneous scenarios.","The results show that the CNN-enabled MAIDCRL significantly improved the learning performance and achieved a faster learning rate compared to the existing MAIDRL, especially on more complicated heterogeneous SMAC scenarios.","We further investigate the stability and robustness of our model.","The statistics reflect that our model not only achieves higher winning rate in all the given scenarios but also boosts the agent's learning process in fine-grained decision-making."],"url":"http://arxiv.org/abs/2402.07890v1","category":"cs.AI"}
{"created":"2024-02-12 18:52:39","title":"Toward an Android Static Analysis Approach for Data Protection","abstract":"Android applications collecting data from users must protect it according to the current legal frameworks. Such data protection has become even more important since the European Union rolled out the General Data Protection Regulation (GDPR). Since app developers are not legal experts, they find it difficult to write privacy-aware source code. Moreover, they have limited tool support to reason about data protection throughout their app development process.   This paper motivates the need for a static analysis approach to diagnose and explain data protection in Android apps. The analysis will recognize personal data sources in the source code, and aims to further examine the data flow originating from these sources. App developers can then address key questions about data manipulation, derived data, and the presence of technical measures. Despite challenges, we explore to what extent one can realize this analysis through static taint analysis, a common method for identifying security vulnerabilities. This is a first step towards designing a tool-based approach that aids app developers and assessors in ensuring data protection in Android apps, based on automated static program analysis.","sentences":["Android applications collecting data from users must protect it according to the current legal frameworks.","Such data protection has become even more important since the European Union rolled out the General Data Protection Regulation (GDPR).","Since app developers are not legal experts, they find it difficult to write privacy-aware source code.","Moreover, they have limited tool support to reason about data protection throughout their app development process.   ","This paper motivates the need for a static analysis approach to diagnose and explain data protection in Android apps.","The analysis will recognize personal data sources in the source code, and aims to further examine the data flow originating from these sources.","App developers can then address key questions about data manipulation, derived data, and the presence of technical measures.","Despite challenges, we explore to what extent one can realize this analysis through static taint analysis, a common method for identifying security vulnerabilities.","This is a first step towards designing a tool-based approach that aids app developers and assessors in ensuring data protection in Android apps, based on automated static program analysis."],"url":"http://arxiv.org/abs/2402.07889v1","category":"cs.SE"}
{"created":"2024-02-12 18:50:41","title":"Hybrid acousto-optical swing-up preparation of exciton and biexciton states in a quantum dot","abstract":"Recent years brought the idea of hybrid systems, in which quantum degrees of freedom, due to controlled couplings, allow the transfer of quantum information and may lead to the emergence of new generation devices. Due to the universal coupling with all solid-state systems and compatibility with miniaturization, acoustic fields will play an important role in interfacing such components. Optically active quantum dots (QDs) are at the forefront of systems for applications in quantum technologies and their multiple available interfaces make them a great component of hybrid systems. QDs generate polarization-entangled photon pairs, however deterministic and high-fidelity preparation of the state is needed. All resonant schemes need filtering to distinguish emitted photons from the excitation pulse, which limits the photon yield significantly. Thus, non-resonant excitation methods are needed like the recently proposed and successful swing-up scheme. Here, we propose a hybrid acousto-optical version of this non-resonant scheme to prepare exciton and biexciton states. We show that using acoustic modulation allows selectively exciting either exciton or biexciton states with just one mode of vibration and one optical pulse or vice versa: acoustic pulse during detuned optical driving. Thus, either of the fields can act as a trigger controlling the evolution. Further, we evaluate the impact of phonon decoherence at finite temperatures for two types of application-relevant QDs, InAs/GaAs and GaAs/AlGaAs, and find that for GaAs QDs exciton preparation can be almost decoherence-free even at liquid nitrogen temperatures already with currently available acoustic modulation frequencies. This approach may pave the way for generating entanglement between an emitter and a quantum acoustic mode when using the acoustic mode as a trigger for the transitions.","sentences":["Recent years brought the idea of hybrid systems, in which quantum degrees of freedom, due to controlled couplings, allow the transfer of quantum information and may lead to the emergence of new generation devices.","Due to the universal coupling with all solid-state systems and compatibility with miniaturization, acoustic fields will play an important role in interfacing such components.","Optically active quantum dots (QDs) are at the forefront of systems for applications in quantum technologies and their multiple available interfaces make them a great component of hybrid systems.","QDs generate polarization-entangled photon pairs, however deterministic and high-fidelity preparation of the state is needed.","All resonant schemes need filtering to distinguish emitted photons from the excitation pulse, which limits the photon yield significantly.","Thus, non-resonant excitation methods are needed like the recently proposed and successful swing-up scheme.","Here, we propose a hybrid acousto-optical version of this non-resonant scheme to prepare exciton and biexciton states.","We show that using acoustic modulation allows selectively exciting either exciton or biexciton states with just one mode of vibration and one optical pulse or vice versa: acoustic pulse during detuned optical driving.","Thus, either of the fields can act as a trigger controlling the evolution.","Further, we evaluate the impact of phonon decoherence at finite temperatures for two types of application-relevant QDs, InAs/GaAs and GaAs/AlGaAs, and find that for GaAs QDs exciton preparation can be almost decoherence-free even at liquid nitrogen temperatures already with currently available acoustic modulation frequencies.","This approach may pave the way for generating entanglement between an emitter and a quantum acoustic mode when using the acoustic mode as a trigger for the transitions."],"url":"http://arxiv.org/abs/2402.07887v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-12 18:50:07","title":"Distributed Anomaly Detection in Modern Power Systems: A Penalty-based Mitigation Approach","abstract":"The evolving landscape of electric power networks, influenced by the integration of distributed energy resources require the development of novel power system monitoring and control architectures. This paper develops algorithm to monitor and detect anomalies of different parts of a power system that cannot be measured directly, by applying neighboring measurements and a dynamic probing technique in a distributed fashion. Additionally, the proposed method accurately assesses the severity of the anomaly. A decision-making algorithm is introduced to effectively penalize anomalous agents, ensuring vigilant oversight of the entire power system's functioning. Simulation results show the efficacy of algorithms in distributed anomaly detection and mitigation.","sentences":["The evolving landscape of electric power networks, influenced by the integration of distributed energy resources require the development of novel power system monitoring and control architectures.","This paper develops algorithm to monitor and detect anomalies of different parts of a power system that cannot be measured directly, by applying neighboring measurements and a dynamic probing technique in a distributed fashion.","Additionally, the proposed method accurately assesses the severity of the anomaly.","A decision-making algorithm is introduced to effectively penalize anomalous agents, ensuring vigilant oversight of the entire power system's functioning.","Simulation results show the efficacy of algorithms in distributed anomaly detection and mitigation."],"url":"http://arxiv.org/abs/2402.07884v1","category":"eess.SY"}
{"created":"2024-02-12 18:47:57","title":"Affine vector space partitions and spreads of quadrics","abstract":"An affine spread is a set of subspaces of $\\mathrm{AG}(n, q)$ of the same dimension that partitions the points of $\\mathrm{AG}(n, q)$. Equivalently, an {\\em affine spread} is a set of projective subspaces of $\\mathrm{PG}(n, q)$ of the same dimension which partitions the points of $\\mathrm{PG}(n, q) \\setminus H_{\\infty}$; here $H_{\\infty}$ denotes the hyperplane at infinity of the projective closure of $\\mathrm{AG}(n, q)$. Let $\\mathcal{Q}$ be a non degenerate quadric of $H_\\infty$ and let $\\Pi$ be a generator of $\\mathcal{Q}$, where $\\Pi$ is a $t$-dimensional projective subspace. An affine spread $\\mathcal{P}$ consisting of $(t+1)$-dimensional projective subspaces of $\\mathrm{PG}(n, q)$ is called hyperbolic, parabolic or elliptic (according as $\\mathcal{Q}$ is hyperbolic, parabolic or elliptic) if the following hold: each member of $\\mathcal{P}$ meets $H_\\infty$ in a distinct generator of $\\mathcal{Q}$ disjoint from $\\Pi$; elements of $\\mathcal{P}$ have at most one point in common; if $S, T \\in \\mathcal{P}$, $|S \\cap T| = 1$, then $\\langle S, T \\rangle \\cap \\mathcal{Q}$ is a hyperbolic quadric of $\\mathcal{Q}$. In this note it is shown that a hyperbolic, parabolic or elliptic affine spread of $\\mathrm{PG}(n, q)$ is equivalent to a spread of $\\mathcal{Q}^+(n+1, q)$, $\\mathcal{Q}(n+1, q)$ or $\\mathcal{Q}^-(n+1, q)$, respectively.","sentences":["An affine spread is a set of subspaces of $\\mathrm{AG}(n, q)$ of the same dimension that partitions the points of $\\mathrm{AG}(n, q)$. Equivalently, an {\\em affine spread} is a set of projective subspaces of $\\mathrm{PG}(n, q)$ of the same dimension which partitions the points of $\\mathrm{PG}(n, q) \\setminus H_{\\infty}$; here $H_{\\infty}$ denotes the hyperplane at infinity of the projective closure of $\\mathrm{AG}(n, q)$. Let $\\mathcal{Q}$ be a non degenerate quadric of $H_\\infty$ and let $\\Pi$ be a generator of $\\mathcal{Q}$, where $\\Pi$ is a $t$-dimensional projective subspace.","An affine spread $\\mathcal{P}$ consisting of $(t+1)$-dimensional projective subspaces of $\\mathrm{PG}(n, q)$ is called hyperbolic, parabolic or elliptic (according as $\\mathcal{Q}$ is hyperbolic, parabolic or elliptic) if the following hold: each member of $\\mathcal{P}$ meets $H_\\infty$ in a distinct generator of $\\mathcal{Q}$ disjoint from $\\Pi$; elements of $\\mathcal{P}$ have at most one point in common; if $S, T \\in \\mathcal{P}$, $|S \\cap T| = 1$, then $\\langle S, T \\rangle \\cap \\mathcal{Q}$ is a hyperbolic quadric of $\\mathcal{Q}$. In this note it is shown that a hyperbolic, parabolic or elliptic affine spread of $\\mathrm{PG}(n, q)$ is equivalent to a spread of $\\mathcal{Q}^+(n+1, q)$, $\\mathcal{Q}(n+1, q)$ or $\\mathcal{Q}^-(n+1, q)$, respectively."],"url":"http://arxiv.org/abs/2402.07882v1","category":"math.CO"}
{"created":"2024-02-12 18:44:02","title":"Using Graph Theory for Improving Machine Learning-based Detection of Cyber Attacks","abstract":"Early detection of network intrusions and cyber threats is one of the main pillars of cybersecurity. One of the most effective approaches for this purpose is to analyze network traffic with the help of artificial intelligence algorithms, with the aim of detecting the possible presence of an attacker by distinguishing it from a legitimate user. This is commonly done by collecting the traffic exchanged between terminals in a network and analyzing it on a per-packet or per-connection basis. In this paper, we propose instead to perform pre-processing of network traffic under analysis with the aim of extracting some new metrics on which we can perform more efficient detection and overcome some limitations of classical approaches. These new metrics are based on graph theory, and consider the network as a whole, rather than focusing on individual packets or connections. Our approach is validated through experiments performed on publicly available data sets, from which it results that it can not only overcome some of the limitations of classical approaches, but also achieve a better detection capability of cyber threats.","sentences":["Early detection of network intrusions and cyber threats is one of the main pillars of cybersecurity.","One of the most effective approaches for this purpose is to analyze network traffic with the help of artificial intelligence algorithms, with the aim of detecting the possible presence of an attacker by distinguishing it from a legitimate user.","This is commonly done by collecting the traffic exchanged between terminals in a network and analyzing it on a per-packet or per-connection basis.","In this paper, we propose instead to perform pre-processing of network traffic under analysis with the aim of extracting some new metrics on which we can perform more efficient detection and overcome some limitations of classical approaches.","These new metrics are based on graph theory, and consider the network as a whole, rather than focusing on individual packets or connections.","Our approach is validated through experiments performed on publicly available data sets, from which it results that it can not only overcome some of the limitations of classical approaches, but also achieve a better detection capability of cyber threats."],"url":"http://arxiv.org/abs/2402.07878v1","category":"cs.CR"}
{"created":"2024-02-12 18:41:55","title":"WildfireGPT: Tailored Large Language Model for Wildfire Analysis","abstract":"The recent advancement of large language models (LLMs) represents a transformational capability at the frontier of artificial intelligence (AI) and machine learning (ML). However, LLMs are generalized models, trained on extensive text corpus, and often struggle to provide context-specific information, particularly in areas requiring specialized knowledge such as wildfire details within the broader context of climate change. For decision-makers and policymakers focused on wildfire resilience and adaptation, it is crucial to obtain responses that are not only precise but also domain-specific, rather than generic. To that end, we developed WildfireGPT, a prototype LLM agent designed to transform user queries into actionable insights on wildfire risks. We enrich WildfireGPT by providing additional context such as climate projections and scientific literature to ensure its information is current, relevant, and scientifically accurate. This enables WildfireGPT to be an effective tool for delivering detailed, user-specific insights on wildfire risks to support a diverse set of end users, including researchers, engineers, urban planners, emergency managers, and infrastructure operators.","sentences":["The recent advancement of large language models (LLMs) represents a transformational capability at the frontier of artificial intelligence (AI) and machine learning (ML).","However, LLMs are generalized models, trained on extensive text corpus, and often struggle to provide context-specific information, particularly in areas requiring specialized knowledge such as wildfire details within the broader context of climate change.","For decision-makers and policymakers focused on wildfire resilience and adaptation, it is crucial to obtain responses that are not only precise but also domain-specific, rather than generic.","To that end, we developed WildfireGPT, a prototype LLM agent designed to transform user queries into actionable insights on wildfire risks.","We enrich WildfireGPT by providing additional context such as climate projections and scientific literature to ensure its information is current, relevant, and scientifically accurate.","This enables WildfireGPT to be an effective tool for delivering detailed, user-specific insights on wildfire risks to support a diverse set of end users, including researchers, engineers, urban planners, emergency managers, and infrastructure operators."],"url":"http://arxiv.org/abs/2402.07877v1","category":"cs.AI"}
{"created":"2024-02-12 18:41:34","title":"Policy Improvement using Language Feedback Models","abstract":"We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.","sentences":["We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following.","To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions.","First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld).","Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens.","Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation.","Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning."],"url":"http://arxiv.org/abs/2402.07876v1","category":"cs.LG"}
{"created":"2024-02-12 18:41:31","title":"Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States","abstract":"In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not. Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data. This implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning). There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states. This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states. Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training. Experiments corroborate our theory, and demonstrate its conclusions on problems beyond LQR, where systems are non-linear and controllers are neural networks. We hypothesize that real-world optimal control may be greatly improved by developing methods for informed selection of initial states to train on.","sentences":["In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not.","Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data.","This implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning).","There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states.","This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states.","Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training.","Experiments corroborate our theory, and demonstrate its conclusions on problems beyond LQR, where systems are non-linear and controllers are neural networks.","We hypothesize that real-world optimal control may be greatly improved by developing methods for informed selection of initial states to train on."],"url":"http://arxiv.org/abs/2402.07875v1","category":"cs.LG"}
{"created":"2024-02-12 18:40:43","title":"Factorizating the Brauer monoid in polynomial time","abstract":"Finding a minimal factorization for a generic semigroup can be done by using the Froidure-Pin Algorithm, which is not feasible for semigroups of large sizes. On the other hand, if we restrict our attention to just a particular semigroup, we could leverage its structure to obtain a much faster algorithm. In particular, $\\mathcal{O}(N^2)$ algorithms are known for factorizing the Symmetric group $S_N$ and the Temperley-Lieb monoid $\\mathcal{T}\\mathcal{L}_N$, but none for their superset the Brauer monoid $\\mathcal{B}_{N}$. In this paper we hence propose a $\\mathcal{O}(N^4)$ factorization algorithm for $\\mathcal{B}_{N}$. At each iteration, the algorithm rewrites the input $X \\in \\mathcal{B}_{N}$ as $X = X' \\circ p_i$ such that $\\ell(X') = \\ell(X) - 1$, where $p_i$ is a factor for $X$ and $\\ell$ is a length function that returns the minimal number of factors needed to generate $X$.","sentences":["Finding a minimal factorization for a generic semigroup can be done by using the Froidure-Pin Algorithm, which is not feasible for semigroups of large sizes.","On the other hand, if we restrict our attention to just a particular semigroup, we could leverage its structure to obtain a much faster algorithm.","In particular, $\\mathcal{O}(N^2)$ algorithms are known for factorizing the Symmetric group $S_N$ and the Temperley-Lieb monoid $\\mathcal{T}\\mathcal{L}_N$, but none for their superset the Brauer monoid $\\mathcal{B}_{N}$. In this paper we hence propose a $\\mathcal{O}(N^4)$ factorization algorithm for $\\mathcal{B}_{N}$. At each iteration, the algorithm rewrites the input $X \\in \\mathcal{B}_{N}$ as $X = X' \\circ p_i$ such that $\\ell(X') = \\ell(X) - 1$, where $p_i$ is a factor for $X$ and $\\ell$ is a length function that returns the minimal number of factors needed to generate $X$."],"url":"http://arxiv.org/abs/2402.07874v1","category":"math.RA"}
{"created":"2024-02-12 18:33:47","title":"Scaling Laws for Fine-Grained Mixture of Experts","abstract":"Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.","sentences":["Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models.","In this work, we analyze their scaling properties, incorporating an expanded range of variables.","Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts.","Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity.","Leveraging these laws, we derive the optimal training configuration for a given computational budget.","Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget.","Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget."],"url":"http://arxiv.org/abs/2402.07871v1","category":"cs.LG"}
{"created":"2024-02-12 18:32:57","title":"Perfect stable regularity lemma and slice-wise stable hypergraphs","abstract":"We investigate various forms of (model-theoretic) stability for hypergraphs and their corresponding strengthenings of the hypergraph regularity lemma with respect to partitions of vertices. On the one hand, we provide a complete classification of the various possibilities in the ternary case. On the other hand, we provide an example of a family of slice-wise stable 3-hypergraphs so that for no partition of the vertices, any triple of parts has density close to 0 or 1. In particular, this addresses some questions and conjectures of Terry and Wolf. We work in the general measure theoretic context of graded probability spaces, so all our results apply both to measures in ultraproducts of finite graphs, leading to the aforementioned combinatorial applications, and to commuting definable Keisler measures, leading to applications in model theory.","sentences":["We investigate various forms of (model-theoretic) stability for hypergraphs and their corresponding strengthenings of the hypergraph regularity lemma with respect to partitions of vertices.","On the one hand, we provide a complete classification of the various possibilities in the ternary case.","On the other hand, we provide an example of a family of slice-wise stable 3-hypergraphs so that for no partition of the vertices, any triple of parts has density close to 0 or 1.","In particular, this addresses some questions and conjectures of Terry and Wolf.","We work in the general measure theoretic context of graded probability spaces, so all our results apply both to measures in ultraproducts of finite graphs, leading to the aforementioned combinatorial applications, and to commuting definable Keisler measures, leading to applications in model theory."],"url":"http://arxiv.org/abs/2402.07870v1","category":"math.CO"}
{"created":"2024-02-12 18:28:36","title":"PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models","abstract":"Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate those limitations. In particular, given a question, RAG retrieves relevant knowledge from a knowledge database to augment the input of the LLM. For instance, the retrieved knowledge could be a set of top-k texts that are most semantically similar to the given question when the knowledge database contains millions of texts collected from Wikipedia. As a result, the LLM could utilize the retrieved knowledge as the context to generate an answer for the given question. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. Particularly, we propose PoisonedRAG , a set of knowledge poisoning attacks to RAG, where an attacker could inject a few poisoned texts into the knowledge database such that the LLM generates an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge poisoning attacks as an optimization problem, whose solution is a set of poisoned texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on the RAG, we propose two solutions to solve the optimization problem, respectively. Our results on multiple benchmark datasets and LLMs show our attacks could achieve 90% attack success rates when injecting 5 poisoned texts for each target question into a database with millions of texts. We also evaluate recent defenses and our results show they are insufficient to defend against our attacks, highlighting the need for new defenses.","sentences":["Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities.","Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination.","Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate those limitations.","In particular, given a question, RAG retrieves relevant knowledge from a knowledge database to augment the input of the LLM.","For instance, the retrieved knowledge could be a set of top-k texts that are most semantically similar to the given question when the knowledge database contains millions of texts collected from Wikipedia.","As a result, the LLM could utilize the retrieved knowledge as the context to generate an answer for the given question.","Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored.","We aim to bridge the gap in this work.","Particularly, we propose PoisonedRAG , a set of knowledge poisoning attacks to RAG, where an attacker could inject a few poisoned texts into the knowledge database such that the LLM generates an attacker-chosen target answer for an attacker-chosen target question.","We formulate knowledge poisoning attacks as an optimization problem, whose solution is a set of poisoned texts.","Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on the RAG, we propose two solutions to solve the optimization problem, respectively.","Our results on multiple benchmark datasets and LLMs show our attacks could achieve 90% attack success rates when injecting 5 poisoned texts for each target question into a database with millions of texts.","We also evaluate recent defenses and our results show they are insufficient to defend against our attacks, highlighting the need for new defenses."],"url":"http://arxiv.org/abs/2402.07867v1","category":"cs.CR"}
{"created":"2024-02-12 18:21:14","title":"Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models","abstract":"Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using base vs. instruct-tuned language models, amongst others. We couple our analysis with three resource contributions: (1) a unified framework for evaluating VLMs, (2) optimized, flexible code for VLM training, and (3) checkpoints for all models, including a family of VLMs at the 7-13B scale that strictly outperform InstructBLIP and LLaVa v1.5, the state-of-the-art in open-source VLMs.","sentences":["Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3.","Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations.","To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities.","Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using base vs. instruct-tuned language models, amongst others.","We couple our analysis with three resource contributions: (1) a unified framework for evaluating VLMs, (2) optimized, flexible code for VLM training, and (3) checkpoints for all models, including a family of VLMs at the 7-13B scale that strictly outperform InstructBLIP and LLaVa v1.5, the state-of-the-art in open-source VLMs."],"url":"http://arxiv.org/abs/2402.07865v1","category":"cs.CV"}
{"created":"2024-02-12 18:14:43","title":"AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy","abstract":"Large language models (LLMs) show impressive capabilities, matching and sometimes exceeding human performance in many domains. This study explores the potential of LLMs to augment judgement in forecasting tasks. We evaluated the impact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to provide high-quality advice ('superforecasting'), and the other designed to be overconfident and base-rate-neglecting. Participants (N = 991) had the option to consult their assigned LLM assistant throughout the study, in contrast to a control group that used a less advanced model (DaVinci-003) without direct forecasting support. Our preregistered analyses reveal that LLM augmentation significantly enhances forecasting accuracy by 23% across both types of assistants, compared to the control group. This improvement occurs despite the superforecasting assistant's higher accuracy in predictions, indicating the augmentation's benefit is not solely due to model prediction accuracy. Exploratory analyses showed a pronounced effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by 43%, compared with 28% for the biased assistant. We further examine whether LLM augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-the-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty. Our findings do not consistently support these hypotheses. Our results suggest that access to an LLM assistant, even a biased one, can be a helpful decision aid in cognitively demanding tasks where the answer is not known at the time of interaction.","sentences":["Large language models (LLMs) show impressive capabilities, matching and sometimes exceeding human performance in many domains.","This study explores the potential of LLMs to augment judgement in forecasting tasks.","We evaluated the impact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to provide high-quality advice ('superforecasting'), and the other designed to be overconfident and base-rate-neglecting.","Participants (N = 991) had the option to consult their assigned LLM assistant throughout the study, in contrast to a control group that used a less advanced model (DaVinci-003) without direct forecasting support.","Our preregistered analyses reveal that LLM augmentation significantly enhances forecasting accuracy by 23% across both types of assistants, compared to the control group.","This improvement occurs despite the superforecasting assistant's higher accuracy in predictions, indicating the augmentation's benefit is not solely due to model prediction accuracy.","Exploratory analyses showed a pronounced effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by 43%, compared with 28% for the biased assistant.","We further examine whether LLM augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-the-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty.","Our findings do not consistently support these hypotheses.","Our results suggest that access to an LLM assistant, even a biased one, can be a helpful decision aid in cognitively demanding tasks where the answer is not known at the time of interaction."],"url":"http://arxiv.org/abs/2402.07862v1","category":"cs.CY"}
{"created":"2024-02-12 18:12:09","title":"On the Detection of Reviewer-Author Collusion Rings From Paper Bidding","abstract":"A major threat to the peer-review systems of computer science conferences is the existence of \"collusion rings\" between reviewers. In such collusion rings, reviewers who have also submitted their own papers to the conference work together to manipulate the conference's paper assignment, with the aim of being assigned to review each other's papers. The most straightforward way that colluding reviewers can manipulate the paper assignment is by indicating their interest in each other's papers through strategic paper bidding. One potential approach to solve this important problem would be to detect the colluding reviewers from their manipulated bids, after which the conference can take appropriate action. While prior work has has developed effective techniques to detect other kinds of fraud, no research has yet established that detecting collusion rings is even possible. In this work, we tackle the question of whether it is feasible to detect collusion rings from the paper bidding. To answer this question, we conduct empirical analysis of two realistic conference bidding datasets, including evaluations of existing algorithms for fraud detection in other applications. We find that collusion rings can achieve considerable success at manipulating the paper assignment while remaining hidden from detection: for example, in one dataset, undetected colluders are able to achieve assignment to up to 30% of the papers authored by other colluders. In addition, when 10 colluders bid on all of each other's papers, no detection algorithm outputs a group of reviewers with more than 31% overlap with the true colluders. These results suggest that collusion cannot be effectively detected from the bidding, demonstrating the need to develop more complex detection algorithms that leverage additional metadata.","sentences":["A major threat to the peer-review systems of computer science conferences is the existence of \"collusion rings\" between reviewers.","In such collusion rings, reviewers who have also submitted their own papers to the conference work together to manipulate the conference's paper assignment, with the aim of being assigned to review each other's papers.","The most straightforward way that colluding reviewers can manipulate the paper assignment is by indicating their interest in each other's papers through strategic paper bidding.","One potential approach to solve this important problem would be to detect the colluding reviewers from their manipulated bids, after which the conference can take appropriate action.","While prior work has has developed effective techniques to detect other kinds of fraud, no research has yet established that detecting collusion rings is even possible.","In this work, we tackle the question of whether it is feasible to detect collusion rings from the paper bidding.","To answer this question, we conduct empirical analysis of two realistic conference bidding datasets, including evaluations of existing algorithms for fraud detection in other applications.","We find that collusion rings can achieve considerable success at manipulating the paper assignment while remaining hidden from detection: for example, in one dataset, undetected colluders are able to achieve assignment to up to 30% of the papers authored by other colluders.","In addition, when 10 colluders bid on all of each other's papers, no detection algorithm outputs a group of reviewers with more than 31% overlap with the true colluders.","These results suggest that collusion cannot be effectively detected from the bidding, demonstrating the need to develop more complex detection algorithms that leverage additional metadata."],"url":"http://arxiv.org/abs/2402.07860v1","category":"cs.SI"}
{"created":"2024-02-12 18:10:17","title":"Lissard: Long and Simple Sequential Reasoning Datasets","abstract":"Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training. For example, state-of-the-art LLMs can find common items in two lists with up to 20 items but fail when lists have 80 items. In this paper, we introduce Lissard, a benchmark comprising seven tasks whose goal is to assess the ability of models to process and generate wide-range sequence lengths, requiring repetitive procedural execution. Our evaluation of open-source (Mistral-7B and Mixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent decline in performance across all models as the complexity of the sequence increases. The datasets and code are available at https://github.com/unicamp-dl/Lissard","sentences":["Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens.","However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training.","For example, state-of-the-art LLMs can find common items in two lists with up to 20 items but fail when lists have 80 items.","In this paper, we introduce Lissard, a benchmark comprising seven tasks whose goal is to assess the ability of models to process and generate wide-range sequence lengths, requiring repetitive procedural execution.","Our evaluation of open-source (Mistral-7B and Mixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent decline in performance across all models as the complexity of the sequence increases.","The datasets and code are available at https://github.com/unicamp-dl/Lissard"],"url":"http://arxiv.org/abs/2402.07859v1","category":"cs.CL"}
{"created":"2024-02-12 18:05:03","title":"Multiscale Neuroimaging Features for the Identification of Medication Class and Non-Responders in Mood Disorder Treatment","abstract":"In the clinical treatment of mood disorders, the complex behavioral symptoms presented by patients and variability of patient response to particular medication classes can create difficulties in providing fast and reliable treatment when standard diagnostic and prescription methods are used. Increasingly, the incorporation of physiological information such as neuroimaging scans and derivatives into the clinical process promises to alleviate some of the uncertainty surrounding this process. Particularly, if neural features can help to identify patients who may not respond to standard courses of anti-depressants or mood stabilizers, clinicians may elect to avoid lengthy and side-effect-laden treatments and seek out a different, more effective course that might otherwise not have been under consideration. Previously, approaches for the derivation of relevant neuroimaging features work at only one scale in the data, potentially limiting the depth of information available for clinical decision support. In this work, we show that the utilization of multi spatial scale neuroimaging features - particularly resting state functional networks and functional network connectivity measures - provide a rich and robust basis for the identification of relevant medication class and non-responders in the treatment of mood disorders. We demonstrate that the generated features, along with a novel approach for fast and automated feature selection, can support high accuracy rates in the identification of medication class and non-responders as well as the identification of novel, multi-scale biomarkers.","sentences":["In the clinical treatment of mood disorders, the complex behavioral symptoms presented by patients and variability of patient response to particular medication classes can create difficulties in providing fast and reliable treatment when standard diagnostic and prescription methods are used.","Increasingly, the incorporation of physiological information such as neuroimaging scans and derivatives into the clinical process promises to alleviate some of the uncertainty surrounding this process.","Particularly, if neural features can help to identify patients who may not respond to standard courses of anti-depressants or mood stabilizers, clinicians may elect to avoid lengthy and side-effect-laden treatments and seek out a different, more effective course that might otherwise not have been under consideration.","Previously, approaches for the derivation of relevant neuroimaging features work at only one scale in the data, potentially limiting the depth of information available for clinical decision support.","In this work, we show that the utilization of multi spatial scale neuroimaging features - particularly resting state functional networks and functional network connectivity measures - provide a rich and robust basis for the identification of relevant medication class and non-responders in the treatment of mood disorders.","We demonstrate that the generated features, along with a novel approach for fast and automated feature selection, can support high accuracy rates in the identification of medication class and non-responders as well as the identification of novel, multi-scale biomarkers."],"url":"http://arxiv.org/abs/2402.07858v1","category":"cs.LG"}
{"created":"2024-02-12 18:04:49","title":"Differentiation of simplicial manifolds I","abstract":"In this paper we expand a technique for differentiating higher Lie groupoids that was initially outlined in previous work and has been elaborated in current research. We extend this technique to the setting of general simplicial manifolds. The concept underlying this extension is that even though simplicial manifolds might not possess the additional criteria that characterize higher Lie groupoids, they do satisfy \"local Kan conditions\", which suffices to perform the constructions presented in the aforementioned study.","sentences":["In this paper we expand a technique for differentiating higher Lie groupoids that was initially outlined in previous work and has been elaborated in current research.","We extend this technique to the setting of general simplicial manifolds.","The concept underlying this extension is that even though simplicial manifolds might not possess the additional criteria that characterize higher Lie groupoids, they do satisfy \"local Kan conditions\", which suffices to perform the constructions presented in the aforementioned study."],"url":"http://arxiv.org/abs/2402.07857v1","category":"math.DG"}
{"created":"2024-02-12 17:59:26","title":"The Complexity of Algebraic Algorithms for LWE","abstract":"Arora & Ge introduced a noise-free polynomial system to compute the secret of a Learning With Errors (LWE) instance via linearization. Albrecht et al. later utilized the Arora-Ge polynomial model to study the complexity of Gr\\\"obner basis computations on LWE polynomial systems under the assumption of semi-regularity. In this paper we revisit the Arora-Ge polynomial and prove that it satisfies a genericity condition recently introduced by Caminata & Gorla, called being in generic coordinates. For polynomial systems in generic coordinates one can always estimate the complexity of DRL Gr\\\"obner basis computations in terms of the Castelnuovo-Mumford regularity and henceforth also via the Macaulay bound.   Moreover, we generalize the Gr\\\"obner basis algorithm of Semaev & Tenti to arbitrary polynomial systems with a finite degree of regularity. In particular, existence of this algorithm yields another approach to estimate the complexity of DRL Gr\\\"obner basis computations in terms of the degree of regularity. In practice, the degree of regularity of LWE polynomial systems is not known, though one can always estimate the lowest achievable degree of regularity. Consequently, from a designer's worst case perspective this approach yields sub-exponential complexity estimates for general, binary secret, and binary error LWE.   In recent works by Dachman-Soled et al. the hardness of LWE in the presence of side information was analyzed. Utilizing their framework we discuss how hints can be incorporated into LWE polynomial systems and how they affect the complexity of Gr\\\"obner basis computations.","sentences":["Arora & Ge introduced a noise-free polynomial system to compute the secret of a Learning With Errors (LWE) instance via linearization.","Albrecht et al. later utilized the Arora-Ge polynomial model to study the complexity of Gr\\\"obner basis computations on LWE polynomial systems under the assumption of semi-regularity.","In this paper we revisit the Arora-Ge polynomial and prove that it satisfies a genericity condition recently introduced by Caminata & Gorla, called being in generic coordinates.","For polynomial systems in generic coordinates one can always estimate the complexity of DRL Gr\\\"obner basis computations in terms of the Castelnuovo-Mumford regularity and henceforth also via the Macaulay bound.   ","Moreover, we generalize the Gr\\\"obner basis algorithm of Semaev & Tenti to arbitrary polynomial systems with a finite degree of regularity.","In particular, existence of this algorithm yields another approach to estimate the complexity of DRL Gr\\\"obner basis computations in terms of the degree of regularity.","In practice, the degree of regularity of LWE polynomial systems is not known, though one can always estimate the lowest achievable degree of regularity.","Consequently, from a designer's worst case perspective this approach yields sub-exponential complexity estimates for general, binary secret, and binary error LWE.   ","In recent works by Dachman-Soled et al.","the hardness of LWE in the presence of side information was analyzed.","Utilizing their framework we discuss how hints can be incorporated into LWE polynomial systems and how they affect the complexity of Gr\\\"obner basis computations."],"url":"http://arxiv.org/abs/2402.07852v1","category":"cs.CR"}
{"created":"2024-02-12 17:59:16","title":"High-order harmonic generation in 2D Transition Metal Disulphides","abstract":"In this paper we explore the capabilities of MoS2 and WS2 2D monolayers to produce radiation in the terahertz range by generation of high-order harmonics. This phenomenon, which is a result of the non-linear response of the electronic carrier population to the applied electric field, is studied by using a particle ensemble stochastic simulation approach based on the Monte Carlo method. The power of the produced harmonic signals is studied against the electric field amplitude, the external temperature, and the frequency of the excitation. Additionally, the stochastic nature of the simulation tool enables to discern the purely discrete harmonic signal from the background spectral noise that comes from the intrinsic carrier velocity fluctuations in the diffusive regime, permitting to set bandwidth thresholds for harmonic extraction. It was found that both TMDs showed similar thresholds bandwidths when compared to III-V semiconductor at low temperatures, while WS2 would be by far a better choice, over MoS2, for exploitation of the 7th and 9th harmonic generation.","sentences":["In this paper we explore the capabilities of MoS2 and WS2 2D monolayers to produce radiation in the terahertz range by generation of high-order harmonics.","This phenomenon, which is a result of the non-linear response of the electronic carrier population to the applied electric field, is studied by using a particle ensemble stochastic simulation approach based on the Monte Carlo method.","The power of the produced harmonic signals is studied against the electric field amplitude, the external temperature, and the frequency of the excitation.","Additionally, the stochastic nature of the simulation tool enables to discern the purely discrete harmonic signal from the background spectral noise that comes from the intrinsic carrier velocity fluctuations in the diffusive regime, permitting to set bandwidth thresholds for harmonic extraction.","It was found that both TMDs showed similar thresholds bandwidths when compared to III-V semiconductor at low temperatures, while WS2 would be by far a better choice, over MoS2, for exploitation of the 7th and 9th harmonic generation."],"url":"http://arxiv.org/abs/2402.07850v1","category":"physics.app-ph"}
{"created":"2024-02-12 17:56:52","title":"Generative Modeling of Discrete Joint Distributions by E-Geodesic Flow Matching on Assignment Manifolds","abstract":"This paper introduces a novel generative model for discrete distributions based on continuous normalizing flows on the submanifold of factorizing discrete measures. Integration of the flow gradually assigns categories and avoids issues of discretizing the latent continuous model like rounding, sample truncation etc. General non-factorizing discrete distributions capable of representing complex statistical dependencies of structured discrete data, can be approximated by embedding the submanifold into a the meta-simplex of all joint discrete distributions and data-driven averaging. Efficient training of the generative model is demonstrated by matching the flow of geodesics of factorizing discrete distributions. Various experiments underline the approach's broad applicability.","sentences":["This paper introduces a novel generative model for discrete distributions based on continuous normalizing flows on the submanifold of factorizing discrete measures.","Integration of the flow gradually assigns categories and avoids issues of discretizing the latent continuous model like rounding, sample truncation etc.","General non-factorizing discrete distributions capable of representing complex statistical dependencies of structured discrete data, can be approximated by embedding the submanifold into a the meta-simplex of all joint discrete distributions and data-driven averaging.","Efficient training of the generative model is demonstrated by matching the flow of geodesics of factorizing discrete distributions.","Various experiments underline the approach's broad applicability."],"url":"http://arxiv.org/abs/2402.07846v1","category":"cs.LG"}
{"created":"2024-02-12 17:53:43","title":"An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering","abstract":"Graph Neural Networks (GNNs) can be trained to detect communities within a graph by learning from the duality of feature and connectivity information. Currently, the common approach for optimisation of GNNs is to use comparisons to ground-truth for hyperparameter tuning and model selection. In this work, we show that nodes can be clustered into communities with GNNs by solely optimising for modularity, without any comparison to ground-truth. Although modularity is a graph partitioning quality metric, we show that this can be used to optimise GNNs that also encode features without a drop in performance. We take it a step further and also study whether the unsupervised metric performance can predict ground-truth performance. To investigate why modularity can be used to optimise GNNs, we design synthetic experiments that show the limitations of this approach. The synthetic graphs are created to highlight current capabilities in distinct, random and zero information space partitions in attributed graphs. We conclude that modularity can be used for hyperparameter optimisation and model selection on real-world datasets as well as being a suitable proxy for predicting ground-truth performance, however, GNNs fail to balance the information duality when the spaces contain conflicting signals.","sentences":["Graph Neural Networks (GNNs) can be trained to detect communities within a graph by learning from the duality of feature and connectivity information.","Currently, the common approach for optimisation of GNNs is to use comparisons to ground-truth for hyperparameter tuning and model selection.","In this work, we show that nodes can be clustered into communities with GNNs by solely optimising for modularity, without any comparison to ground-truth.","Although modularity is a graph partitioning quality metric, we show that this can be used to optimise GNNs that also encode features without a drop in performance.","We take it a step further and also study whether the unsupervised metric performance can predict ground-truth performance.","To investigate why modularity can be used to optimise GNNs, we design synthetic experiments that show the limitations of this approach.","The synthetic graphs are created to highlight current capabilities in distinct, random and zero information space partitions in attributed graphs.","We conclude that modularity can be used for hyperparameter optimisation and model selection on real-world datasets as well as being a suitable proxy for predicting ground-truth performance, however, GNNs fail to balance the information duality when the spaces contain conflicting signals."],"url":"http://arxiv.org/abs/2402.07845v1","category":"cs.LG"}
{"created":"2024-02-12 17:53:22","title":"Mercury: An Efficiency Benchmark for LLM Code Synthesis","abstract":"Despite advancements in evaluating Large Language Models (LLMs) for code synthesis, benchmarks have predominantly focused on functional correctness, overlooking the importance of code efficiency. We present Mercury, the first benchmark designated for assessing the code efficiency of LLM code synthesis tasks. Mercury consists of 1,889 programming tasks covering diverse difficulty levels alongside test case generators generating unlimited cases for comprehensive evaluation. Unlike existing benchmarks, Mercury integrates a novel metric Beyond@K to measure normalized code efficiency based on historical submissions, leading to a new evaluation indicator for code synthesis, which encourages generating functionally correct and computationally efficient code, mirroring the real-world software development standard. Our findings reveal that while LLMs demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, underscoring a new frontier for LLM research and development.","sentences":["Despite advancements in evaluating Large Language Models (LLMs) for code synthesis, benchmarks have predominantly focused on functional correctness, overlooking the importance of code efficiency.","We present Mercury, the first benchmark designated for assessing the code efficiency of LLM code synthesis tasks.","Mercury consists of 1,889 programming tasks covering diverse difficulty levels alongside test case generators generating unlimited cases for comprehensive evaluation.","Unlike existing benchmarks, Mercury integrates a novel metric Beyond@K to measure normalized code efficiency based on historical submissions, leading to a new evaluation indicator for code synthesis, which encourages generating functionally correct and computationally efficient code, mirroring the real-world software development standard.","Our findings reveal that while LLMs demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, underscoring a new frontier for LLM research and development."],"url":"http://arxiv.org/abs/2402.07844v1","category":"cs.SE"}
{"created":"2024-02-12 17:52:51","title":"Stabilizer entropy of quantum tetrahedra","abstract":"How complex is the structure of quantum geometry? In several approaches, the spacetime atoms are obtained by the SU(2) intertwiner called quantum tetrahedron. The complexity of this construction has a concrete consequence in recent efforts to simulate such models and toward experimental demonstrations of quantum gravity effects. There are, therefore, both a computational and an experimental complexity inherent to this class of models. In this paper, we study this complexity under the lens of stabilizer entropy (SE). We calculate the SE of the gauge-invariant basis states and its average in the SU(2) gauge invariant subspace. We find that the states of definite volume are singled out by the (near) maximal SE and give precise bounds to the verification protocols for experimental demonstrations on available quantum computers.","sentences":["How complex is the structure of quantum geometry?","In several approaches, the spacetime atoms are obtained by the SU(2) intertwiner called quantum tetrahedron.","The complexity of this construction has a concrete consequence in recent efforts to simulate such models and toward experimental demonstrations of quantum gravity effects.","There are, therefore, both a computational and an experimental complexity inherent to this class of models.","In this paper, we study this complexity under the lens of stabilizer entropy (SE).","We calculate the SE of the gauge-invariant basis states and its average in the SU(2) gauge invariant subspace.","We find that the states of definite volume are singled out by the (near) maximal SE and give precise bounds to the verification protocols for experimental demonstrations on available quantum computers."],"url":"http://arxiv.org/abs/2402.07843v1","category":"hep-th"}
{"created":"2024-02-12 17:46:58","title":"Quantile Least Squares: A Flexible Approach for Robust Estimation and Validation of Location-Scale Families","abstract":"In this paper, the problem of robust estimation and validation of location-scale families is revisited. The proposed methods exploit the joint asymptotic normality of sample quantiles (of i.i.d random variables) to construct the ordinary and generalized least squares estimators of location and scale parameters. These quantile least squares (QLS) estimators are easy to compute because they have explicit expressions, their robustness is achieved by excluding extreme quantiles from the least-squares estimation, and efficiency is boosted by using as many non-extreme quantiles as practically relevant. The influence functions of the QLS estimators are specified and plotted for several location-scale families. They closely resemble the shapes of some well-known influence functions yet those shapes emerge automatically (i.e., do not need to be specified). The joint asymptotic normality of the proposed estimators is established, and their finite-sample properties are explored using simulations. Also, computational costs of these estimators, as well as those of MLE, are evaluated for sample sizes n = 10^6, 10^7, 10^8, 10^9. For model validation, two goodness-of-fit tests are constructed and their performance is studied using simulations and real data. In particular, for the daily stock returns of Google over the last four years, both tests strongly support the logistic distribution assumption and reject other bell-shaped competitors.","sentences":["In this paper, the problem of robust estimation and validation of location-scale families is revisited.","The proposed methods exploit the joint asymptotic normality of sample quantiles (of i.i.d random variables) to construct the ordinary and generalized least squares estimators of location and scale parameters.","These quantile least squares (QLS) estimators are easy to compute because they have explicit expressions, their robustness is achieved by excluding extreme quantiles from the least-squares estimation, and efficiency is boosted by using as many non-extreme quantiles as practically relevant.","The influence functions of the QLS estimators are specified and plotted for several location-scale families.","They closely resemble the shapes of some well-known influence functions yet those shapes emerge automatically (i.e., do not need to be specified).","The joint asymptotic normality of the proposed estimators is established, and their finite-sample properties are explored using simulations.","Also, computational costs of these estimators, as well as those of MLE, are evaluated for sample sizes n = 10^6, 10^7, 10^8, 10^9.","For model validation, two goodness-of-fit tests are constructed and their performance is studied using simulations and real data.","In particular, for the daily stock returns of Google over the last four years, both tests strongly support the logistic distribution assumption and reject other bell-shaped competitors."],"url":"http://arxiv.org/abs/2402.07837v1","category":"stat.ME"}
{"created":"2024-02-12 17:45:40","title":"Generalizing across Temporal Domains with Koopman Operators","abstract":"In the field of domain generalization, the task of constructing a predictive model capable of generalizing to a target domain without access to target data remains challenging. This problem becomes further complicated when considering evolving dynamics between domains. While various approaches have been proposed to address this issue, a comprehensive understanding of the underlying generalization theory is still lacking. In this study, we contribute novel theoretic results that aligning conditional distribution leads to the reduction of generalization bounds. Our analysis serves as a key motivation for solving the Temporal Domain Generalization (TDG) problem through the application of Koopman Neural Operators, resulting in Temporal Koopman Networks (TKNets). By employing Koopman Operators, we effectively address the time-evolving distributions encountered in TDG using the principles of Koopman theory, where measurement functions are sought to establish linear transition relations between evolving domains. Through empirical evaluations conducted on synthetic and real-world datasets, we validate the effectiveness of our proposed approach.","sentences":["In the field of domain generalization, the task of constructing a predictive model capable of generalizing to a target domain without access to target data remains challenging.","This problem becomes further complicated when considering evolving dynamics between domains.","While various approaches have been proposed to address this issue, a comprehensive understanding of the underlying generalization theory is still lacking.","In this study, we contribute novel theoretic results that aligning conditional distribution leads to the reduction of generalization bounds.","Our analysis serves as a key motivation for solving the Temporal Domain Generalization (TDG) problem through the application of Koopman Neural Operators, resulting in Temporal Koopman Networks (TKNets).","By employing Koopman Operators, we effectively address the time-evolving distributions encountered in TDG using the principles of Koopman theory, where measurement functions are sought to establish linear transition relations between evolving domains.","Through empirical evaluations conducted on synthetic and real-world datasets, we validate the effectiveness of our proposed approach."],"url":"http://arxiv.org/abs/2402.07834v1","category":"cs.LG"}
{"created":"2024-02-12 17:34:17","title":"Surface gravity in spherically symmetric collapsing stars","abstract":"Here we consider the generalized Oppenheimer-Snyder collapse of a star into a four-dimensional Einstein-Gauss-Bonnet black hole as well as a class of regular black holes labeled by the polytropic index of the stellar matter. We then analyze the nature of the horizon and the corresponding surface gravity outside and inside the star. The Hayward and Nielsen-Visser dynamical surface gravity are in agreement with the one resulting from the Killing vector of the outer static metric. However, these two definitions inside the star do not coincide with the Killing surface gravity outside the star when the star crosses the event horizon. This motivates us to study the surface gravity using Fodor's approach to have a unique surface gravity at the mentioned moment. Then the extremality condition and the first law of thermodynamics are discussed at the trapping horizon of the star.","sentences":["Here we consider the generalized Oppenheimer-Snyder collapse of a star into a four-dimensional Einstein-Gauss-Bonnet black hole as well as a class of regular black holes labeled by the polytropic index of the stellar matter.","We then analyze the nature of the horizon and the corresponding surface gravity outside and inside the star.","The Hayward and Nielsen-Visser dynamical surface gravity are in agreement with the one resulting from the Killing vector of the outer static metric.","However, these two definitions inside the star do not coincide with the Killing surface gravity outside the star when the star crosses the event horizon.","This motivates us to study the surface gravity using Fodor's approach to have a unique surface gravity at the mentioned moment.","Then the extremality condition and the first law of thermodynamics are discussed at the trapping horizon of the star."],"url":"http://arxiv.org/abs/2402.07828v1","category":"gr-qc"}
{"created":"2024-02-12 17:34:13","title":"Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model","abstract":"Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at https://hf.co/CohereForAI/aya-101","sentences":["Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages.","What does it take to broaden access to breakthroughs beyond first-class citizen languages?","Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced.","Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages.","We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance.","Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models.","We open-source our instruction datasets and our model at https://hf.co/CohereForAI/aya-101"],"url":"http://arxiv.org/abs/2402.07827v1","category":"cs.CL"}
{"created":"2024-02-12 17:28:32","title":"Random optimization problems at fixed temperatures","abstract":"This article considers a class of disordered mean-field combinatorial optimization problems. We focus on the Gibbs measure, where the inverse temperature does not vary with the size of the graph and the edge weights are sampled from a general distribution under mild assumptions. Our results consist of the Law of Large Numbers and Central Limit Theorems for the log-partition function, the weight of a typical configuration, and the Gibbs average in both quenched and annealed forms. We also derive quenched Poisson convergence for the size of the intersection of two independent samples, yielding replica symmetry of the model. Applications cover popular models from the literature, such as the Minimal Matching Problem, Traveling Salesman Problem, and Minimal Spanning Tree Problem, on a sequence of deterministic and random dense block graphs of increasing size.","sentences":["This article considers a class of disordered mean-field combinatorial optimization problems.","We focus on the Gibbs measure, where the inverse temperature does not vary with the size of the graph and the edge weights are sampled from a general distribution under mild assumptions.","Our results consist of the Law of Large Numbers and Central Limit Theorems for the log-partition function, the weight of a typical configuration, and the Gibbs average in both quenched and annealed forms.","We also derive quenched Poisson convergence for the size of the intersection of two independent samples, yielding replica symmetry of the model.","Applications cover popular models from the literature, such as the Minimal Matching Problem, Traveling Salesman Problem, and Minimal Spanning Tree Problem, on a sequence of deterministic and random dense block graphs of increasing size."],"url":"http://arxiv.org/abs/2402.07825v1","category":"math.PR"}
{"created":"2024-02-12 17:27:39","title":"Uranus's influence on Neptune's exterior mean motion resonances","abstract":"Neptune's external mean motion resonances play an important role in sculpting the observed population of transneptunian objects (TNOs). The population of scattering TNOs are known to 'stick' to Neptune's resonances while evolving in semimajor axis ($a$), though simulations show that resonance sticking is less prevalent at $a\\gtrsim200-250$ au. Here we present an extensive numerical exploration of the strengths of Neptune's resonances for scattering TNOs with perihelion distances $q=33$ au. We show that the drop-off in resonance sticking for the large $a$ scattering TNOs is not a generic feature of scattering dynamics, but can instead be attributed to the specific configuration of Neptune and Uranus in our solar system. In simulations with just Uranus removed from the giant planet system, Neptune's resonances are strong in the scattering population out to at least $\\sim300$ au. Uranus and Neptune are near a 2:1 period ratio, and the variations in Neptune's orbit resulting from this near resonance are responsible for destabilizing Neptune's resonances for high-$e$ TNO orbits beyond the $\\sim20$:1 resonance at $a\\approx220$ au. Direct interactions between Uranus and the scattering population are responsible for slightly weakening Neptune's closer-in resonances. In simulations where Neptune and Uranus are placed in their mutual 2:1 resonance, we see almost no stable libration of scattering particles in Neptune's external resonances. Our results have important implications for how the strengths of Neptune's distant resonances varied during the epoch of planet migration when the Neptune-Uranus period ratio was evolving. These strength variations likely affected the distant resonant and detached TNO populations.","sentences":["Neptune's external mean motion resonances play an important role in sculpting the observed population of transneptunian objects (TNOs).","The population of scattering TNOs are known to 'stick' to Neptune's resonances while evolving in semimajor axis ($a$), though simulations show that resonance sticking is less prevalent at $a\\gtrsim200-250$ au.","Here we present an extensive numerical exploration of the strengths of Neptune's resonances for scattering TNOs with perihelion distances $q=33$ au.","We show that the drop-off in resonance sticking for the large $a$ scattering TNOs is not a generic feature of scattering dynamics, but can instead be attributed to the specific configuration of Neptune and Uranus in our solar system.","In simulations with just Uranus removed from the giant planet system, Neptune's resonances are strong in the scattering population out to at least $\\sim300$ au.","Uranus and Neptune are near a 2:1 period ratio, and the variations in Neptune's orbit resulting from this near resonance are responsible for destabilizing Neptune's resonances for high-$e$ TNO orbits beyond the $\\sim20$:1 resonance at $a\\approx220$ au.","Direct interactions between Uranus and the scattering population are responsible for slightly weakening Neptune's closer-in resonances.","In simulations where Neptune and Uranus are placed in their mutual 2:1 resonance, we see almost no stable libration of scattering particles in Neptune's external resonances.","Our results have important implications for how the strengths of Neptune's distant resonances varied during the epoch of planet migration when the Neptune-Uranus period ratio was evolving.","These strength variations likely affected the distant resonant and detached TNO populations."],"url":"http://arxiv.org/abs/2402.07824v1","category":"astro-ph.EP"}
{"created":"2024-02-12 17:26:35","title":"Understanding fitness landscapes in morpho-evolution via local optima networks","abstract":"Morpho-evolution (ME) refers to the simultaneous optimisation of a robot's design and controller to maximise performance given a task and environment. Many genetic encodings have been proposed which are capable of representing design and control. Previous research has provided empirical comparisons between encodings in terms of their performance with respect to an objective function and the diversity of designs that are evaluated, however there has been no attempt to explain the observed findings. We address this by applying Local Optima Network (LON) analysis to investigate the structure of the fitness landscapes induced by three different encodings when evolving a robot for a locomotion task, shedding new light on the ease by which different fitness landscapes can be traversed by a search process. This is the first time LON analysis has been applied in the field of ME despite its popularity in combinatorial optimisation domains; the findings will facilitate design of new algorithms or operators that are customised to ME landscapes in the future.","sentences":["Morpho-evolution (ME) refers to the simultaneous optimisation of a robot's design and controller to maximise performance given a task and environment.","Many genetic encodings have been proposed which are capable of representing design and control.","Previous research has provided empirical comparisons between encodings in terms of their performance with respect to an objective function and the diversity of designs that are evaluated, however there has been no attempt to explain the observed findings.","We address this by applying Local Optima Network (LON) analysis to investigate the structure of the fitness landscapes induced by three different encodings when evolving a robot for a locomotion task, shedding new light on the ease by which different fitness landscapes can be traversed by a search process.","This is the first time LON analysis has been applied in the field of ME despite its popularity in combinatorial optimisation domains; the findings will facilitate design of new algorithms or operators that are customised to ME landscapes in the future."],"url":"http://arxiv.org/abs/2402.07822v1","category":"cs.AI"}
{"created":"2024-02-12 17:24:15","title":"Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning","abstract":"Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replacement for SGD, this paper presents a comprehensive study both theoretically and empirically. First, we propose the stagewise DP zeroth-order method that dynamically schedules key hyperparameters. This design is grounded on the synergy between DP random perturbation and the gradient approximation error of the zeroth-order method, and its effect on finetuning trajectory. Second, we further enhance the scalability by reducing the trainable parameters that are identified by repurposing a data-free pruning technique requiring no additional data or extra privacy budget. We provide theoretical analysis for both proposed methods. We conduct extensive empirical analysis on both encoder-only masked language model and decoder-only autoregressive language model, achieving impressive results in terms of scalability and utility.","sentences":["Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks.","Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets.","Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability.","Most existing methods build upon the seminal work of DP-SGD.","Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD.","In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient.","Rather than treating the zeroth-order method as a drop-in replacement for SGD, this paper presents a comprehensive study both theoretically and empirically.","First, we propose the stagewise DP zeroth-order method that dynamically schedules key hyperparameters.","This design is grounded on the synergy between DP random perturbation and the gradient approximation error of the zeroth-order method, and its effect on finetuning trajectory.","Second, we further enhance the scalability by reducing the trainable parameters that are identified by repurposing a data-free pruning technique requiring no additional data or extra privacy budget.","We provide theoretical analysis for both proposed methods.","We conduct extensive empirical analysis on both encoder-only masked language model and decoder-only autoregressive language model, achieving impressive results in terms of scalability and utility."],"url":"http://arxiv.org/abs/2402.07818v1","category":"cs.LG"}
{"created":"2024-02-12 17:21:18","title":"An introduction to $V$-filtrations","abstract":"We give an introduction to the theory of $V$-filtrations of Malgrange and Kashiwara. After discussing the basic properties of this construction (in the case of a smooth hypersurface and, later, in the general case), we describe the connection with the theory of $b$-functions. As an example, we treat the case of weighted homogeneous isolated singularities. We discuss the compatibility of $V$-filtrations with proper push-forward and duality and the connection with nearby and vanishing cycles via the Riemann-Hilbert correspondence. We end by describing some invariants of singularities via the $V$-filtration.","sentences":["We give an introduction to the theory of $V$-filtrations of Malgrange and Kashiwara.","After discussing the basic properties of this construction (in the case of a smooth hypersurface and, later, in the general case), we describe the connection with the theory of $b$-functions.","As an example, we treat the case of weighted homogeneous isolated singularities.","We discuss the compatibility of $V$-filtrations with proper push-forward and duality and the connection with nearby and vanishing cycles via the Riemann-Hilbert correspondence.","We end by describing some invariants of singularities via the $V$-filtration."],"url":"http://arxiv.org/abs/2402.07816v1","category":"math.AG"}
{"created":"2024-02-12 17:19:02","title":"On super curves and supervolumes","abstract":"We study the geometry of super curves with a chosen supervolume form. We consider the algebra of divergence free vector fields $S(1|N)$ associated to such curves. When $N=2$ its derived algebra, called $S(2)$, defines a special family of curves, named $S(2)$-super curves. We exhibit an involution on the moduli space of such curves that generalizes Deligne's involution for $N=1$ super curves. The fixed point set of this involution consists on Manin's $SUSY_2$-super curves. We describe the moduli spaces of these curves.","sentences":["We study the geometry of super curves with a chosen supervolume form.","We consider the algebra of divergence free vector fields $S(1|N)$ associated to such curves.","When $N=2$ its derived algebra, called $S(2)$, defines a special family of curves, named $S(2)$-super curves.","We exhibit an involution on the moduli space of such curves that generalizes Deligne's involution for $N=1$ super curves.","The fixed point set of this involution consists on Manin's $SUSY_2$-super curves.","We describe the moduli spaces of these curves."],"url":"http://arxiv.org/abs/2402.07815v1","category":"math.RT"}
{"created":"2024-02-12 17:18:51","title":"PBADet: A One-Stage Anchor-Free Approach for Part-Body Association","abstract":"The detection of human parts (e.g., hands, face) and their correct association with individuals is an essential task, e.g., for ubiquitous human-machine interfaces and action recognition. Traditional methods often employ multi-stage processes, rely on cumbersome anchor-based systems, or do not scale well to larger part sets. This paper presents PBADet, a novel one-stage, anchor-free approach for part-body association detection. Building upon the anchor-free object representation across multi-scale feature maps, we introduce a singular part-to-body center offset that effectively encapsulates the relationship between parts and their parent bodies. Our design is inherently versatile and capable of managing multiple parts-to-body associations without compromising on detection accuracy or robustness. Comprehensive experiments on various datasets underscore the efficacy of our approach, which not only outperforms existing state-of-the-art techniques but also offers a more streamlined and efficient solution to the part-body association challenge.","sentences":["The detection of human parts (e.g., hands, face) and their correct association with individuals is an essential task, e.g., for ubiquitous human-machine interfaces and action recognition.","Traditional methods often employ multi-stage processes, rely on cumbersome anchor-based systems, or do not scale well to larger part sets.","This paper presents PBADet, a novel one-stage, anchor-free approach for part-body association detection.","Building upon the anchor-free object representation across multi-scale feature maps, we introduce a singular part-to-body center offset that effectively encapsulates the relationship between parts and their parent bodies.","Our design is inherently versatile and capable of managing multiple parts-to-body associations without compromising on detection accuracy or robustness.","Comprehensive experiments on various datasets underscore the efficacy of our approach, which not only outperforms existing state-of-the-art techniques but also offers a more streamlined and efficient solution to the part-body association challenge."],"url":"http://arxiv.org/abs/2402.07814v1","category":"cs.CV"}
{"created":"2024-02-12 17:17:50","title":"Retrieval-Augmented Thought Process as Sequential Decision Making","abstract":"Large Language Models (LLMs) have demonstrated their strong ability to assist people and show \"sparks of intelligence\". However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts. In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference. In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models.","sentences":["Large Language Models (LLMs) have demonstrated their strong ability to assist people and show \"sparks of intelligence\".","However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts.","In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP).","Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process.","To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference.","In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models."],"url":"http://arxiv.org/abs/2402.07812v1","category":"cs.CL"}
{"created":"2024-02-12 17:12:19","title":"Fixation for $\\mathcal{U}$-Ising and $\\mathcal{U}$-voter dynamics with frozen vertices","abstract":"The zero-temperature stochastic Ising model is a special case of the famous stochastic Ising model of statistical mechanics, and the voter model is another classical model in this field. In both models, each vertex of the graph $\\mathbb{Z}^d$ can have one of two states, and can change state to match the state of its neighbors. In 2017, Morris proposed generalizations of these models, the $\\mathcal{U}$-Ising and $\\mathcal{U}$-voter dynamics, in which a vertex can change state to match the state of certain subsets of vertices near it. These generalizations were inspired by similar generalizations in the related model of bootstrap percolation, where Balister, Bollob\\'as, Duminil-Copin, Morris, Przykucki, Smith and Uzzell were able to establish a very impressive universality classification of the generalized models. However, there have been very few results on the $\\mathcal{U}$-Ising and $\\mathcal{U}$-voter dynamics. The only one is due to Blanquicett in 2021, who obtained a few encouraging advances on the important question of fixation, which is only partially solved for the zero-temperature stochastic Ising model: will all vertices eventually settle on a given state or will them oscillate forever between the two states? In this work, we tackle a question which was solved for the zero-temperature stochastic Ising model by Damron, Eckner, Kogan, Newman and Sidoravicius in 2015: fixation when a fraction of the vertices of $\\mathbb{Z}^d$ are frozen in one of the states. For $d=1$ and $2$, in most cases we prove that if all frozen vertices are in the same state, all vertices eventually settle at this state. Moreover, if vertices can be frozen in both states but the proportion of vertices frozen in the second state is small enough, we were able to establish a universality classification identifying the models in which all vertices settle in a given state.","sentences":["The zero-temperature stochastic Ising model is a special case of the famous stochastic Ising model of statistical mechanics, and the voter model is another classical model in this field.","In both models, each vertex of the graph $\\mathbb{Z}^d$ can have one of two states, and can change state to match the state of its neighbors.","In 2017, Morris proposed generalizations of these models, the $\\mathcal{U}$-Ising and $\\mathcal{U}$-voter dynamics, in which a vertex can change state to match the state of certain subsets of vertices near it.","These generalizations were inspired by similar generalizations in the related model of bootstrap percolation, where Balister, Bollob\\'as, Duminil-Copin, Morris, Przykucki, Smith and Uzzell were able to establish a very impressive universality classification of the generalized models.","However, there have been very few results on the $\\mathcal{U}$-Ising and $\\mathcal{U}$-voter dynamics.","The only one is due to Blanquicett in 2021, who obtained a few encouraging advances on the important question of fixation, which is only partially solved for the zero-temperature stochastic Ising model: will all vertices eventually settle on a given state or will them oscillate forever between the two states?","In this work, we tackle a question which was solved for the zero-temperature stochastic Ising model by Damron, Eckner, Kogan, Newman and Sidoravicius in 2015: fixation when a fraction of the vertices of $\\mathbb{Z}^d$ are frozen in one of the states.","For $d=1$ and $2$, in most cases we prove that if all frozen vertices are in the same state, all vertices eventually settle at this state.","Moreover, if vertices can be frozen in both states but the proportion of vertices frozen in the second state is small enough, we were able to establish a universality classification identifying the models in which all vertices settle in a given state."],"url":"http://arxiv.org/abs/2402.07807v1","category":"math.PR"}
{"created":"2024-02-12 17:12:09","title":"A comparison of six linear and non-linear mixed models for longitudinal data: application to late-life cognitive trajectories","abstract":"Longitudinal characterization of cognitive change in late-life has received increasing attention to better understand age-related cognitive aging and cognitive changes reflecting pathology-related and mortality-related processes. Several mixed-effects models have been proposed to accommodate the non-linearity of cognitive decline and assess the putative influence of covariates on it. In this work, we examine the standard linear mixed model (LMM) with a linear function of time and five alternative models capturing non-linearity of change over time, including the LMM with a quadratic term, LMM with splines, the functional mixed model, the piecewise linear mixed model and the sigmoidal mixed model. We first theoretically describe the models. Next, using data from deceased participants from two prospective cohorts with annual cognitive testing, we compared the interpretation of the models by investigating the association of education on cognitive change before death. Finally, we performed a simulation study to empirically evaluate the models and provide practical recommendations. In particular, models were challenged by increasing follow-up spacing, increasing missing data, and decreasing sample size. With the exception of the LMM with a quadratic term, the fit of all models was generally adequate to capture non-linearity of cognitive change and models were relatively robust. Although spline-based models do not have interpretable nonlinearity parameters, their convergence was easier to achieve and they allow for graphical interpretation. In contrast the piecewise and the sigmoidal models, with interpretable non-linear parameters may require more data to achieve convergence.","sentences":["Longitudinal characterization of cognitive change in late-life has received increasing attention to better understand age-related cognitive aging and cognitive changes reflecting pathology-related and mortality-related processes.","Several mixed-effects models have been proposed to accommodate the non-linearity of cognitive decline and assess the putative influence of covariates on it.","In this work, we examine the standard linear mixed model (LMM) with a linear function of time and five alternative models capturing non-linearity of change over time, including the LMM with a quadratic term, LMM with splines, the functional mixed model, the piecewise linear mixed model and the sigmoidal mixed model.","We first theoretically describe the models.","Next, using data from deceased participants from two prospective cohorts with annual cognitive testing, we compared the interpretation of the models by investigating the association of education on cognitive change before death.","Finally, we performed a simulation study to empirically evaluate the models and provide practical recommendations.","In particular, models were challenged by increasing follow-up spacing, increasing missing data, and decreasing sample size.","With the exception of the LMM with a quadratic term, the fit of all models was generally adequate to capture non-linearity of cognitive change and models were relatively robust.","Although spline-based models do not have interpretable nonlinearity parameters, their convergence was easier to achieve and they allow for graphical interpretation.","In contrast the piecewise and the sigmoidal models, with interpretable non-linear parameters may require more data to achieve convergence."],"url":"http://arxiv.org/abs/2402.07806v1","category":"stat.AP"}
{"created":"2024-02-12 17:09:04","title":"Causal Discovery to Understand Hot Corrosion","abstract":"Gas turbine superalloys experience hot corrosion, driven by factors including corrosive deposit flux, temperature, gas composition, and component material. The full mechanism still needs clarification and research often focuses on laboratory work. As such, there is interest in causal discovery to confirm the significance of factors and identify potential missing causal relationships or co-dependencies between these factors. The causal discovery algorithm Fast Causal Inference (FCI) has been trialled on a small set of laboratory data, with the outputs evaluated for their significance to corrosion propagation, and compared to existing mechanistic understanding. FCI identified the salt deposition flux as the most influential corrosion variable for this limited dataset. However, HCl was the second most influential for pitting regions, compared to temperature for more uniformly corroding regions. Thus FCI generated causal links aligned with literature from a randomised corrosion dataset, while also identifying the presence of two different degradation modes in operation.","sentences":["Gas turbine superalloys experience hot corrosion, driven by factors including corrosive deposit flux, temperature, gas composition, and component material.","The full mechanism still needs clarification and research often focuses on laboratory work.","As such, there is interest in causal discovery to confirm the significance of factors and identify potential missing causal relationships or co-dependencies between these factors.","The causal discovery algorithm Fast Causal Inference (FCI) has been trialled on a small set of laboratory data, with the outputs evaluated for their significance to corrosion propagation, and compared to existing mechanistic understanding.","FCI identified the salt deposition flux as the most influential corrosion variable for this limited dataset.","However, HCl was the second most influential for pitting regions, compared to temperature for more uniformly corroding regions.","Thus FCI generated causal links aligned with literature from a randomised corrosion dataset, while also identifying the presence of two different degradation modes in operation."],"url":"http://arxiv.org/abs/2402.07804v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-12 17:07:02","title":"Towards a mathematical theory for consistency training in diffusion models","abstract":"Consistency models, which were proposed to mitigate the high computational overhead during the sampling phase of diffusion models, facilitate single-step sampling while attaining state-of-the-art empirical performance. When integrated into the training phase, consistency models attempt to train a sequence of consistency functions capable of mapping any point at any time step of the diffusion process to its starting point. Despite the empirical success, a comprehensive theoretical understanding of consistency training remains elusive. This paper takes a first step towards establishing theoretical underpinnings for consistency models. We demonstrate that, in order to generate samples within $\\varepsilon$ proximity to the target in distribution (measured by some Wasserstein metric), it suffices for the number of steps in consistency learning to exceed the order of $d^{5/2}/\\varepsilon$, with $d$ the data dimension. Our theory offers rigorous insights into the validity and efficacy of consistency models, illuminating their utility in downstream inference tasks.","sentences":["Consistency models, which were proposed to mitigate the high computational overhead during the sampling phase of diffusion models, facilitate single-step sampling while attaining state-of-the-art empirical performance.","When integrated into the training phase, consistency models attempt to train a sequence of consistency functions capable of mapping any point at any time step of the diffusion process to its starting point.","Despite the empirical success, a comprehensive theoretical understanding of consistency training remains elusive.","This paper takes a first step towards establishing theoretical underpinnings for consistency models.","We demonstrate that, in order to generate samples within $\\varepsilon$ proximity to the target in distribution (measured by some Wasserstein metric), it suffices for the number of steps in consistency learning to exceed the order of $d^{5/2}/\\varepsilon$, with $d$ the data dimension.","Our theory offers rigorous insights into the validity and efficacy of consistency models, illuminating their utility in downstream inference tasks."],"url":"http://arxiv.org/abs/2402.07802v1","category":"stat.ML"}
{"created":"2024-02-12 17:03:58","title":"Generalising Planning Environment Redesign","abstract":"In Environment Design, one interested party seeks to affect another agent's decisions by applying changes to the environment. Most research on planning environment (re)design assumes the interested party's objective is to facilitate the recognition of goals and plans, and search over the space of environment modifications to find the minimal set of changes that simplify those tasks and optimise a particular metric. This search space is usually intractable, so existing approaches devise metric-dependent pruning techniques for performing search more efficiently. This results in approaches that are not able to generalise across different objectives and/or metrics. In this paper, we argue that the interested party could have objectives and metrics that are not necessarily related to recognising agents' goals or plans. Thus, to generalise the task of Planning Environment Redesign, we develop a general environment redesign approach that is metric-agnostic and leverages recent research on top-quality planning to efficiently redesign planning environments according to any interested party's objective and metric. Experiments over a set of environment redesign benchmarks show that our general approach outperforms existing approaches when using well-known metrics, such as facilitating the recognition of goals, as well as its effectiveness when solving environment redesign tasks that optimise a novel set of different metrics.","sentences":["In Environment Design, one interested party seeks to affect another agent's decisions by applying changes to the environment.","Most research on planning environment (re)design assumes the interested party's objective is to facilitate the recognition of goals and plans, and search over the space of environment modifications to find the minimal set of changes that simplify those tasks and optimise a particular metric.","This search space is usually intractable, so existing approaches devise metric-dependent pruning techniques for performing search more efficiently.","This results in approaches that are not able to generalise across different objectives and/or metrics.","In this paper, we argue that the interested party could have objectives and metrics that are not necessarily related to recognising agents' goals or plans.","Thus, to generalise the task of Planning Environment Redesign, we develop a general environment redesign approach that is metric-agnostic and leverages recent research on top-quality planning to efficiently redesign planning environments according to any interested party's objective and metric.","Experiments over a set of environment redesign benchmarks show that our general approach outperforms existing approaches when using well-known metrics, such as facilitating the recognition of goals, as well as its effectiveness when solving environment redesign tasks that optimise a novel set of different metrics."],"url":"http://arxiv.org/abs/2402.07799v1","category":"cs.AI"}
{"created":"2024-02-12 16:59:31","title":"Tailoring spatiotemporal wavepackets via two-dimensional space-time duality","abstract":"Space-time (ST) beams, ultrafast optical wavepackets with customized spatial and temporal characteristics, present a significant contrast to conventional spatial-structured light and hold the potential to revolutionize our understanding and manipulation of light. However, the progress in ST beam research has been constrained by the absence of a universal framework for their analysis and generation. Here, we introduce the concept of \"two-dimensional ST duality\", establishing a foundational duality between spatial-structured light and ST beams. We show that breaking the exact balance between paraxial diffraction and narrow-band dispersion is crucial for guiding the dynamics of ST wavepackets. Leveraging this insight, we pioneer a versatile complex-amplitude modulation strategy, enabling the precise crafting of ST beams with an exceptional fidelity exceeding 97%. Furthermore, we uncover a new range of ST wavepackets by harnessing the exact one-to-one relationship between scalar spatial-structured light and ST beams. Our findings suggest a paradigm shift opportunity in ST beam research and may apply to a broader range of wave physics systems.","sentences":["Space-time (ST) beams, ultrafast optical wavepackets with customized spatial and temporal characteristics, present a significant contrast to conventional spatial-structured light and hold the potential to revolutionize our understanding and manipulation of light.","However, the progress in ST beam research has been constrained by the absence of a universal framework for their analysis and generation.","Here, we introduce the concept of \"two-dimensional ST duality\", establishing a foundational duality between spatial-structured light and ST beams.","We show that breaking the exact balance between paraxial diffraction and narrow-band dispersion is crucial for guiding the dynamics of ST wavepackets.","Leveraging this insight, we pioneer a versatile complex-amplitude modulation strategy, enabling the precise crafting of ST beams with an exceptional fidelity exceeding 97%.","Furthermore, we uncover a new range of ST wavepackets by harnessing the exact one-to-one relationship between scalar spatial-structured light and ST beams.","Our findings suggest a paradigm shift opportunity in ST beam research and may apply to a broader range of wave physics systems."],"url":"http://arxiv.org/abs/2402.07794v1","category":"physics.optics"}
{"created":"2024-02-12 16:59:05","title":"Empowering Federated Learning for Massive Models with NVIDIA FLARE","abstract":"In the ever-evolving landscape of artificial intelligence (AI) and large language models (LLMs), handling and leveraging data effectively has become a critical challenge. Most state-of-the-art machine learning algorithms are data-centric. However, as the lifeblood of model performance, necessary data cannot always be centralized due to various factors such as privacy, regulation, geopolitics, copyright issues, and the sheer effort required to move vast datasets. In this paper, we explore how federated learning enabled by NVIDIA FLARE can address these challenges with easy and scalable integration capabilities, enabling parameter-efficient and full supervised fine-tuning of LLMs for natural language processing and biopharmaceutical applications to enhance their accuracy and robustness.","sentences":["In the ever-evolving landscape of artificial intelligence (AI) and large language models (LLMs), handling and leveraging data effectively has become a critical challenge.","Most state-of-the-art machine learning algorithms are data-centric.","However, as the lifeblood of model performance, necessary data cannot always be centralized due to various factors such as privacy, regulation, geopolitics, copyright issues, and the sheer effort required to move vast datasets.","In this paper, we explore how federated learning enabled by NVIDIA FLARE can address these challenges with easy and scalable integration capabilities, enabling parameter-efficient and full supervised fine-tuning of LLMs for natural language processing and biopharmaceutical applications to enhance their accuracy and robustness."],"url":"http://arxiv.org/abs/2402.07792v1","category":"cs.LG"}
{"created":"2024-02-12 16:55:58","title":"Continuous Assurance of Autonomous Vehicle Behavior Through Machine Learned Correctness Properties","abstract":"Correctness properties are critical to conducting verification and validation on software systems, especially those cyberphysical systems whose functionality changes frequently due to software updates, changes in the operating environment, or newly learned behaviors. We detail a novel method to automatically construct expressive, executable correctness properties in the form of machine-learned correctness properties which can be used to ensure that a system's behavior is correct with respect to its design and operating requirements. We propose a method to bootstrap the creation of these correctness properties using a novel simulation-based generation of training and testing data using multiple extensions to the Cross Entropy algorithm for search-based optimization. Then, we apply this method to a software-in-the-loop evaluation of an autonomous vehicle to demonstrate that such models can assert about important properties of multi-agent cyberphysical systems. We demonstrate that this process brings the task of developing robust correctness properties from the realm of formal methods experts into the domain of system developers and engineers, and that machine-learned correctness properties are expressive enough to capture the correct behavior of cyberphysical systems in their complex environments. This advancement can provide evidence of dependability to system designers and users, enhancing trust in the deployment of autonomous vehicles and other intelligent transportation systems.","sentences":["Correctness properties are critical to conducting verification and validation on software systems, especially those cyberphysical systems whose functionality changes frequently due to software updates, changes in the operating environment, or newly learned behaviors.","We detail a novel method to automatically construct expressive, executable correctness properties in the form of machine-learned correctness properties which can be used to ensure that a system's behavior is correct with respect to its design and operating requirements.","We propose a method to bootstrap the creation of these correctness properties using a novel simulation-based generation of training and testing data using multiple extensions to the Cross Entropy algorithm for search-based optimization.","Then, we apply this method to a software-in-the-loop evaluation of an autonomous vehicle to demonstrate that such models can assert about important properties of multi-agent cyberphysical systems.","We demonstrate that this process brings the task of developing robust correctness properties from the realm of formal methods experts into the domain of system developers and engineers, and that machine-learned correctness properties are expressive enough to capture the correct behavior of cyberphysical systems in their complex environments.","This advancement can provide evidence of dependability to system designers and users, enhancing trust in the deployment of autonomous vehicles and other intelligent transportation systems."],"url":"http://arxiv.org/abs/2402.07791v1","category":"cs.SE"}
{"created":"2024-02-12 16:52:26","title":"Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis","abstract":"Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within a text to comprehend sentiment information. Previous studies integrated external knowledge, such as knowledge graphs, to enhance the semantic features in ABSA models. Recent research has examined the use of Graph Neural Networks (GNNs) on dependency and constituent trees for syntactic analysis. With the ongoing development of ABSA, more innovative linguistic and structural features are being incorporated (e.g. latent graph), but this also introduces complexity and confusion. As of now, a scalable framework for integrating diverse linguistic and structural features into ABSA does not exist. This paper presents the Extensible Multi-Granularity Fusion (EMGF) network, which integrates information from dependency and constituent syntactic, attention semantic , and external knowledge graphs. EMGF, equipped with multi-anchor triplet learning and orthogonal projection, efficiently harnesses the combined potential of each granularity feature and their synergistic interactions, resulting in a cumulative effect without additional computational expenses. Experimental findings on SemEval 2014 and Twitter datasets confirm EMGF's superiority over existing ABSA methods.","sentences":["Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within a text to comprehend sentiment information.","Previous studies integrated external knowledge, such as knowledge graphs, to enhance the semantic features in ABSA models.","Recent research has examined the use of Graph Neural Networks (GNNs) on dependency and constituent trees for syntactic analysis.","With the ongoing development of ABSA, more innovative linguistic and structural features are being incorporated (e.g. latent graph), but this also introduces complexity and confusion.","As of now, a scalable framework for integrating diverse linguistic and structural features into ABSA does not exist.","This paper presents the Extensible Multi-Granularity Fusion (EMGF) network, which integrates information from dependency and constituent syntactic, attention semantic , and external knowledge graphs.","EMGF, equipped with multi-anchor triplet learning and orthogonal projection, efficiently harnesses the combined potential of each granularity feature and their synergistic interactions, resulting in a cumulative effect without additional computational expenses.","Experimental findings on SemEval 2014 and Twitter datasets confirm EMGF's superiority over existing ABSA methods."],"url":"http://arxiv.org/abs/2402.07787v1","category":"cs.AI"}
{"created":"2024-02-12 16:50:35","title":"\"Layer-by-layer\" Unsupervised Clustering of Statistically Relevant Fluctuations in Noisy Time-series Data of Complex Dynamical Systems","abstract":"Complex systems are typically characterized by intricate internal dynamics that are often hard to elucidate. Ideally, this requires methods that allow to detect and classify in unsupervised way the microscopic dynamical events occurring in the system. However, decoupling statistically relevant fluctuations from the internal noise remains most often non-trivial. Here we describe \"Onion Clustering\": a simple, iterative unsupervised clustering method that efficiently detects and classifies statistically relevant fluctuations in noisy time-series data. We demonstrate its efficiency by analyzing simulation and experimental trajectories of various systems with complex internal dynamics, ranging from the atomic- to the microscopic-scale, in- and out-of-equilibrium. The method is based on an iterative detect-classify-archive approach. In similar way as peeling the external (evident) layer of an onion reveals the internal hidden ones, the method performs a first detection and classification of the most populated dynamical environment in the system and of its characteristic noise. The signal of such dynamical cluster is then removed from the time-series data and the remaining part, cleared-out from its noise, is analyzed again. At every iteration, the detection of hidden dynamical sub-domains is facilitated by an increasing (and adaptive) relevance-to-noise ratio. The process iterates until no new dynamical domains can be uncovered, revealing, as an output, the number of clusters that can be effectively distinguished/classified in statistically robust way as a function of the time-resolution of the analysis. Onion Clustering is general and benefits from clear-cut physical interpretability. We expect that it will help analyzing a variety of complex dynamical systems and time-series data.","sentences":["Complex systems are typically characterized by intricate internal dynamics that are often hard to elucidate.","Ideally, this requires methods that allow to detect and classify in unsupervised way the microscopic dynamical events occurring in the system.","However, decoupling statistically relevant fluctuations from the internal noise remains most often non-trivial.","Here we describe \"Onion Clustering\": a simple, iterative unsupervised clustering method that efficiently detects and classifies statistically relevant fluctuations in noisy time-series data.","We demonstrate its efficiency by analyzing simulation and experimental trajectories of various systems with complex internal dynamics, ranging from the atomic- to the microscopic-scale, in- and out-of-equilibrium.","The method is based on an iterative detect-classify-archive approach.","In similar way as peeling the external (evident) layer of an onion reveals the internal hidden ones, the method performs a first detection and classification of the most populated dynamical environment in the system and of its characteristic noise.","The signal of such dynamical cluster is then removed from the time-series data and the remaining part, cleared-out from its noise, is analyzed again.","At every iteration, the detection of hidden dynamical sub-domains is facilitated by an increasing (and adaptive) relevance-to-noise ratio.","The process iterates until no new dynamical domains can be uncovered, revealing, as an output, the number of clusters that can be effectively distinguished/classified in statistically robust way as a function of the time-resolution of the analysis.","Onion Clustering is general and benefits from clear-cut physical interpretability.","We expect that it will help analyzing a variety of complex dynamical systems and time-series data."],"url":"http://arxiv.org/abs/2402.07786v1","category":"physics.data-an"}
{"created":"2024-02-12 16:50:07","title":"HYPO: Hyperspherical Out-of-Distribution Generalization","abstract":"Out-of-distribution (OOD) generalization is critical for machine learning models deployed in the real world. However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments. In this paper, we propose a novel framework HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant representations in a hyperspherical space. In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles -- ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated. We further provide theoretical justifications on how our prototypical learning objective improves the OOD generalization bound. Through extensive experiments on challenging OOD benchmarks, we demonstrate that our approach outperforms competitive baselines and achieves superior performance. Code is available at https://github.com/deeplearning-wisc/hypo.","sentences":["Out-of-distribution (OOD) generalization is critical for machine learning models deployed in the real world.","However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments.","In this paper, we propose a novel framework HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant representations in a hyperspherical space.","In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles -- ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated.","We further provide theoretical justifications on how our prototypical learning objective improves the OOD generalization bound.","Through extensive experiments on challenging OOD benchmarks, we demonstrate that our approach outperforms competitive baselines and achieves superior performance.","Code is available at https://github.com/deeplearning-wisc/hypo."],"url":"http://arxiv.org/abs/2402.07785v1","category":"cs.LG"}
{"created":"2024-02-12 16:47:13","title":"Solving parameter-dependent semi-algebraic systems","abstract":"We consider systems of polynomial equations and inequalities in $\\mathbb{Q}[\\boldsymbol{y}][\\boldsymbol{x}]$ where $\\boldsymbol{x} = (x_1, \\ldots, x_n)$ and $\\boldsymbol{y} = (y_1, \\ldots,y_t)$. The $\\boldsymbol{y}$ indeterminates are considered as parameters and we assume that when specialising them generically, the set of common complex solutions, to the obtained equations, is finite. We consider the problem of real root classification for such parameter-dependent problems, i.e. identifying the possible number of real solutions depending on the values of the parameters and computing a description of the regions of the space of parameters over which the number of real roots remains invariant.   We design an algorithm for solving this problem. The formulas it outputs enjoy a determinantal structure. Under genericity assumptions, we show that its arithmetic complexity is polynomial in both the maximum degree $d$ and the number $s$ of the input inequalities and exponential in $nt+t^2$. The output formulas consist of polynomials of degree bounded by $(2s+n)d^{n+1}$. This is the first algorithm with such a singly exponential complexity. We report on practical experiments showing that a first implementation of this algorithm can tackle examples which were previously out of reach.","sentences":["We consider systems of polynomial equations and inequalities in $\\mathbb{Q}[\\boldsymbol{y}][\\boldsymbol{x}]$ where $\\boldsymbol{x} = (x_1, \\ldots, x_n)$ and $\\boldsymbol{y} = (y_1, \\ldots,y_t)$. The $\\boldsymbol{y}$ indeterminates are considered as parameters and we assume that when specialising them generically, the set of common complex solutions, to the obtained equations, is finite.","We consider the problem of real root classification for such parameter-dependent problems, i.e. identifying the possible number of real solutions depending on the values of the parameters and computing a description of the regions of the space of parameters over which the number of real roots remains invariant.   ","We design an algorithm for solving this problem.","The formulas it outputs enjoy a determinantal structure.","Under genericity assumptions, we show that its arithmetic complexity is polynomial in both the maximum degree $d$ and the number $s$ of the input inequalities and exponential in $nt+t^2$. The output formulas consist of polynomials of degree bounded by $(2s+n)d^{n+1}$. This is the first algorithm with such a singly exponential complexity.","We report on practical experiments showing that a first implementation of this algorithm can tackle examples which were previously out of reach."],"url":"http://arxiv.org/abs/2402.07782v1","category":"cs.SC"}
{"created":"2024-02-12 16:44:00","title":"Finding product sets in some classes of amenable groups","abstract":"In 2022, using methods from ergodic theory, Kra, Moreira, Richter, and Robertson resolved a longstanding conjecture of Erd\\H{o}s about sumsets in large subsets of the natural numbers. In this paper, we extend this result to several important classes of amenable groups, including all finitely generated virtually nilpotent groups, and all abelian groups $(G,+)$ with the property that the subgroup $2G := \\{g+g : g\\in G\\}$ has finite index. We prove that in any group $G$ from the above classes, any $A\\subset G$ with positive upper Banach density contains a shifted product set of the form $\\{tb_ib_j\\colon i<j\\}$, for some infinite sequence $(b_n)_{n\\in\\mathbb{N}}$ and some $t\\in G$. In fact, we show this result for all amenable groups that posses a property which we call square absolute continuity. Our results provide answers to several questions and conjectures posed in a recent survey of Kra, Moreira, Richter and Robertson.","sentences":["In 2022, using methods from ergodic theory, Kra, Moreira, Richter, and Robertson resolved a longstanding conjecture of Erd\\H{o}s about sumsets in large subsets of the natural numbers.","In this paper, we extend this result to several important classes of amenable groups, including all finitely generated virtually nilpotent groups, and all abelian groups $(G,+)$ with the property that the subgroup $2G := \\{g+g :","g\\in G\\}$ has finite index.","We prove that in any group $G$ from the above classes, any $A\\subset G$ with positive upper Banach density contains a shifted product set of the form $\\{tb_ib_j\\colon i<j\\}$, for some infinite sequence $(b_n)_{n\\in\\mathbb{N}}$ and some $t\\in G$. In fact, we show this result for all amenable groups that posses a property which we call square absolute continuity.","Our results provide answers to several questions and conjectures posed in a recent survey of Kra, Moreira, Richter and Robertson."],"url":"http://arxiv.org/abs/2402.07779v1","category":"math.DS"}
{"created":"2024-02-12 16:41:54","title":"TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection","abstract":"The proliferation of fake news has emerged as a severe societal problem, raising significant interest from industry and academia. While existing deep-learning based methods have made progress in detecting fake news accurately, their reliability may be compromised caused by the non-transparent reasoning processes, poor generalization abilities and inherent risks of integration with large language models (LLMs). To address this challenge, we propose {\\methodname}, a novel framework for trustworthy fake news detection that prioritizes explainability, generalizability and controllability of models. This is achieved via a dual-system framework that integrates cognition and decision systems, adhering to the principles above. The cognition system harnesses human expertise to generate logical predicates, which guide LLMs in generating human-readable logic atoms. Meanwhile, the decision system deduces generalizable logic rules to aggregate these atoms, enabling the identification of the truthfulness of the input news across diverse domains and enhancing transparency in the decision-making process. Finally, we present comprehensive evaluation results on four datasets, demonstrating the feasibility and trustworthiness of our proposed framework. Our implementation is available at \\url{https://github.com/less-and-less-bugs/Trust_TELLER}.","sentences":["The proliferation of fake news has emerged as a severe societal problem, raising significant interest from industry and academia.","While existing deep-learning based methods have made progress in detecting fake news accurately, their reliability may be compromised caused by the non-transparent reasoning processes, poor generalization abilities and inherent risks of integration with large language models (LLMs).","To address this challenge, we propose {\\methodname}, a novel framework for trustworthy fake news detection that prioritizes explainability, generalizability and controllability of models.","This is achieved via a dual-system framework that integrates cognition and decision systems, adhering to the principles above.","The cognition system harnesses human expertise to generate logical predicates, which guide LLMs in generating human-readable logic atoms.","Meanwhile, the decision system deduces generalizable logic rules to aggregate these atoms, enabling the identification of the truthfulness of the input news across diverse domains and enhancing transparency in the decision-making process.","Finally, we present comprehensive evaluation results on four datasets, demonstrating the feasibility and trustworthiness of our proposed framework.","Our implementation is available at \\url{https://github.com/less-and-less-bugs/Trust_TELLER}."],"url":"http://arxiv.org/abs/2402.07776v1","category":"cs.CL"}
{"created":"2024-02-12 16:41:41","title":"Growth Rate of the Number of Empty Triangles in the Plane","abstract":"Given a set $P$ of $n$ points in the plane, in general position, denote by $N_\\Delta(P)$ the number of empty triangles with vertices in $P$. In this paper we investigate by how much $N_\\Delta(P)$ changes if a point $x$ is removed from $P$. By constructing a graph $G_P(x)$ based on the arrangement of the empty triangles incident on $x$, we transform this geometric problem to the problem of counting triangles in the graph $G_P(x)$. We study properties of the graph $G_P(x)$ and, in particular, show that it is kite-free. This relates the growth rate of the number of empty triangles to the famous Ruzsa-Szemer\\'edi problem.","sentences":["Given a set $P$ of $n$ points in the plane, in general position, denote by $N_\\Delta(P)$ the number of empty triangles with vertices in $P$. In this paper we investigate by how much $N_\\Delta(P)$ changes if a point $x$ is removed from $P$. By constructing a graph $G_P(x)$ based on the arrangement of the empty triangles incident on $x$, we transform this geometric problem to the problem of counting triangles in the graph $G_P(x)$. We study properties of the graph $G_P(x)$ and, in particular, show that it is kite-free.","This relates the growth rate of the number of empty triangles to the famous Ruzsa-Szemer\\'edi problem."],"url":"http://arxiv.org/abs/2402.07775v1","category":"cs.DM"}
{"created":"2024-02-12 16:33:35","title":"End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty","abstract":"Many decision processes in artificial intelligence and operations research are modeled by parametric optimization problems whose defining parameters are unknown and must be inferred from observable data. The Predict-Then-Optimize (PtO) paradigm in machine learning aims to maximize downstream decision quality by training the parametric inference model end-to-end with the subsequent constrained optimization. This requires backpropagation through the optimization problem using approximation techniques specific to the problem's form, especially for nondifferentiable linear and mixed-integer programs. This paper extends the PtO methodology to optimization problems with nondifferentiable Ordered Weighted Averaging (OWA) objectives, known for their ability to ensure properties of fairness and robustness in decision models. Through a collection of training techniques and proposed application settings, it shows how optimization of OWA functions can be effectively integrated with parametric prediction for fair and robust optimization under uncertainty.","sentences":["Many decision processes in artificial intelligence and operations research are modeled by parametric optimization problems whose defining parameters are unknown and must be inferred from observable data.","The Predict-Then-Optimize (PtO) paradigm in machine learning aims to maximize downstream decision quality by training the parametric inference model end-to-end with the subsequent constrained optimization.","This requires backpropagation through the optimization problem using approximation techniques specific to the problem's form, especially for nondifferentiable linear and mixed-integer programs.","This paper extends the PtO methodology to optimization problems with nondifferentiable Ordered Weighted Averaging (OWA) objectives, known for their ability to ensure properties of fairness and robustness in decision models.","Through a collection of training techniques and proposed application settings, it shows how optimization of OWA functions can be effectively integrated with parametric prediction for fair and robust optimization under uncertainty."],"url":"http://arxiv.org/abs/2402.07772v1","category":"cs.AI"}
{"created":"2024-02-12 16:32:37","title":"Quantitative knowledge retrieval from large language models","abstract":"Large language models (LLMs) have been extensively studied for their abilities to generate convincing natural language sequences, however their utility for quantitative information retrieval is less well understood. In this paper we explore the feasibility of LLMs as a mechanism for quantitative knowledge retrieval to aid data analysis tasks such as elicitation of prior distributions for Bayesian models and imputation of missing data. We present a prompt engineering framework, treating an LLM as an interface to a latent space of scientific literature, comparing responses in different contexts and domains against more established approaches. Implications and challenges of using LLMs as 'experts' are discussed.","sentences":["Large language models (LLMs) have been extensively studied for their abilities to generate convincing natural language sequences, however their utility for quantitative information retrieval is less well understood.","In this paper we explore the feasibility of LLMs as a mechanism for quantitative knowledge retrieval to aid data analysis tasks such as elicitation of prior distributions for Bayesian models and imputation of missing data.","We present a prompt engineering framework, treating an LLM as an interface to a latent space of scientific literature, comparing responses in different contexts and domains against more established approaches.","Implications and challenges of using LLMs as 'experts' are discussed."],"url":"http://arxiv.org/abs/2402.07770v1","category":"cs.IR"}
{"created":"2024-02-12 16:32:14","title":"Observations of the new meteor shower from comet 46P/Wirtanen","abstract":"A new meteor shower $\\lambda$-Sculptorids produced by the comet 46P/Wirtanen was forecast for December 12, 2023. The predicted activity was highly uncertain, but generally considered to be low. Observations in Australia, New Zealand, and Oceania were solicited to help constrain the size distribution of meteoroids in the shower. This work aims to characterize the new meteor shower, by comparing the observed and predicted radiants and orbits, and to provide a calibration for future predictions. Global Meteor Network video cameras were used to observe the meteor shower. Multi-station observations were used to compute trajectories and orbits, while single-station observations were used to measure the flux profile. A total of 23 $\\lambda$-Sculptorid orbits have been measured. The shower peaked at a zenithal hourly rate (ZHR) of $0.65^{+0.24}_{-0.20}$ meteors per hour at $\\lambda_{\\odot} = 259.988^{\\circ} \\pm 0.042^{\\circ}$. Due to the low in-atmosphere speed of 15~km s$^{-1}$, the mean mass of observed meteoroids was 0.5~g ($\\sim10$~mm diameter), an order of magnitude higher than predicted. The dynamical simulations of the meteoroid stream can only produce such large meteoroids arriving at Earth in 2023 with correct radiants when a very low meteoroid density of $\\sim 100$~kg~m$^{-3}$ is assumed. However, this assumption cannot reproduce the activity profile. It may be reproduced by considering higher density meteoroids in a larger ecliptic plane-crossing time window ($\\Delta T$ = 20 days) and trails ejected prior to 1908, but then the observed radiant structure is not reproduced.","sentences":["A new meteor shower $\\lambda$-Sculptorids produced by the comet 46P/Wirtanen was forecast for December 12, 2023.","The predicted activity was highly uncertain, but generally considered to be low.","Observations in Australia, New Zealand, and Oceania were solicited to help constrain the size distribution of meteoroids in the shower.","This work aims to characterize the new meteor shower, by comparing the observed and predicted radiants and orbits, and to provide a calibration for future predictions.","Global Meteor Network video cameras were used to observe the meteor shower.","Multi-station observations were used to compute trajectories and orbits, while single-station observations were used to measure the flux profile.","A total of 23 $\\lambda$-Sculptorid orbits have been measured.","The shower peaked at a zenithal hourly rate (ZHR) of $0.65^{+0.24}_{-0.20}$ meteors per hour at $\\lambda_{\\odot} = 259.988^{\\circ} \\pm 0.042^{\\circ}$. Due to the low in-atmosphere speed of 15~km s$^{-1}$, the mean mass of observed meteoroids was 0.5~g ($\\sim10$~mm diameter), an order of magnitude higher than predicted.","The dynamical simulations of the meteoroid stream can only produce such large meteoroids arriving at Earth in 2023 with correct radiants when a very low meteoroid density of $\\sim 100$~kg~m$^{-3}$ is assumed.","However, this assumption cannot reproduce the activity profile.","It may be reproduced by considering higher density meteoroids in a larger ecliptic plane-crossing time window ($\\Delta T$ = 20 days) and trails ejected prior to 1908, but then the observed radiant structure is not reproduced."],"url":"http://arxiv.org/abs/2402.07769v1","category":"astro-ph.EP"}
{"created":"2024-02-12 16:25:47","title":"Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model","abstract":"Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive. To address this, we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model's output; and (iv) compositional generalization and a primacy bias with in-context exemplars. Overall, our work introduces a grounded, synthetic framework for studying stepwise inference and offers mechanistic hypotheses that can lay the foundation for a deeper understanding of this phenomenon.","sentences":["Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems.","Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive.","To address this, we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful.","Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph.","Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model's output; and (iv) compositional generalization and a primacy bias with in-context exemplars.","Overall, our work introduces a grounded, synthetic framework for studying stepwise inference and offers mechanistic hypotheses that can lay the foundation for a deeper understanding of this phenomenon."],"url":"http://arxiv.org/abs/2402.07757v1","category":"cs.LG"}
{"created":"2024-02-12 16:23:28","title":"Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models","abstract":"Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capabilities in diffusion language models.","sentences":["Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models.","This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models.","We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process.","In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance.","Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems.","Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding.","Our findings contribute to the understanding and development of reasoning capabilities in diffusion language models."],"url":"http://arxiv.org/abs/2402.07754v1","category":"cs.CL"}
{"created":"2024-02-12 16:20:06","title":"A Statistical and Multiwavelength Photometric Analysis of a Young Embedded Open Star Cluster: IC 1590","abstract":"We present a statistical and multiwavelength photometric studies of young open cluster IC 1590. We identified 91 cluster members using $Gaia$ DR3 astrometry data using ensemble-based unsupervised machine learning techniques. From $Gaia$ EDR3 data, we estimate the best-fitted parameters for IC 1590 using the Automated Stellar Cluster Analysis package (ASteCA) yielding the distance $d$ $\\sim$ 2.87 $\\pm$ 0.02 Kpc, age $\\sim$ 3.54 $\\pm$ 0.05 Myr, metallicity $z$ $\\sim$ 0.0212 $\\pm$ 0.003, binarity value of $\\sim$ 0.558, and extinction $A_v$ $\\sim$ 1.252 $\\pm$ 0.4 mag for an $R_v$ value of $\\sim$ 3.322 $\\pm$ 0.23. We estimate the initial mass function slope of the cluster to be $\\alpha$ = 1.081 $\\pm$ 0.112 for single stars and $\\alpha$ = 1.490 $\\pm$ 0.051 for a binary fraction of $\\sim$ 0.558 in the mass range 1 M$_{\\odot}$ $\\leq$ m(M$_{\\odot}$) $\\leq$ 100 M$_{\\odot}$. The $G$-band luminosity function slope is estimated to be $\\sim$ 0.33 $\\pm$ 0.09. We use $(J-H)$ versus $(H-K_s)$ color-color diagram to identify young stellar objects (YSOs). We found that all the identified YSOs have ages $\\leq$ 2 Myr and masses $\\sim$ 0.35 - 5.5 M$_{\\odot}$. We also fit the radial surface density profile. Using the galpy we performed orbit analysis of the cluster. The extinction map for the cluster region has been generated using the PNICER technique and it is almost similar to the dust structure obtained the dust structure from the 500 $\\mu$$m$ dust continuum emissions map of $Herschel$ SPIRE. We finally at the end discussed the star formation scenario in the cluster region.","sentences":["We present a statistical and multiwavelength photometric studies of young open cluster IC 1590.","We identified 91 cluster members using $Gaia$ DR3 astrometry data using ensemble-based unsupervised machine learning techniques.","From $Gaia$ EDR3 data, we estimate the best-fitted parameters for IC 1590 using the Automated Stellar Cluster Analysis package (ASteCA) yielding the distance $d$ $\\sim$ 2.87 $\\pm$ 0.02 Kpc, age $\\sim$ 3.54 $\\pm$ 0.05 Myr, metallicity $z$ $\\sim$ 0.0212 $\\pm$ 0.003, binarity value of $\\sim$ 0.558, and extinction $A_v$ $\\sim$ 1.252 $\\pm$ 0.4 mag for an $R_v$ value of $\\sim$ 3.322 $\\pm$ 0.23.","We estimate the initial mass function slope of the cluster to be $\\alpha$ = 1.081 $\\pm$ 0.112 for single stars and $\\alpha$ = 1.490 $\\pm$ 0.051 for a binary fraction of $\\sim$ 0.558 in the mass range 1 M$_{\\odot}$ $\\leq$ m(M$_{\\odot}$) $\\leq$ 100 M$_{\\odot}$. The $G$-band luminosity function slope is estimated to be $\\sim$ 0.33 $\\pm$ 0.09.","We use $(J-H)$ versus $(H-K_s)$ color-color diagram to identify young stellar objects (YSOs).","We found that all the identified YSOs have ages $\\leq$ 2 Myr and masses $\\sim$ 0.35","- 5.5 M$_{\\odot}$. We also fit the radial surface density profile.","Using the galpy we performed orbit analysis of the cluster.","The extinction map for the cluster region has been generated using the PNICER technique and it is almost similar to the dust structure obtained the dust structure from the 500 $\\mu$$m$ dust continuum emissions map of $Herschel$ SPIRE.","We finally at the end discussed the star formation scenario in the cluster region."],"url":"http://arxiv.org/abs/2402.07750v1","category":"astro-ph.GA"}
{"created":"2024-02-12 16:17:40","title":"Optimal score estimation via empirical Bayes smoothing","abstract":"We study the problem of estimating the score function of an unknown probability distribution $\\rho^*$ from $n$ independent and identically distributed observations in $d$ dimensions. Assuming that $\\rho^*$ is subgaussian and has a Lipschitz-continuous score function $s^*$, we establish the optimal rate of $\\tilde \\Theta(n^{-\\frac{2}{d+4}})$ for this estimation problem under the loss function $\\|\\hat s - s^*\\|^2_{L^2(\\rho^*)}$ that is commonly used in the score matching literature, highlighting the curse of dimensionality where sample complexity for accurate score estimation grows exponentially with the dimension $d$. Leveraging key insights in empirical Bayes theory as well as a new convergence rate of smoothed empirical distribution in Hellinger distance, we show that a regularized score estimator based on a Gaussian kernel attains this rate, shown optimal by a matching minimax lower bound. We also discuss the implication of our theory on the sample complexity of score-based generative models.","sentences":["We study the problem of estimating the score function of an unknown probability distribution $\\rho^*$ from $n$ independent and identically distributed observations in $d$ dimensions.","Assuming that $\\rho^*$ is subgaussian and has a Lipschitz-continuous score function $s^*$, we establish the optimal rate of $\\tilde \\Theta(n^{-\\frac{2}{d+4}})$ for this estimation problem under the loss function $\\|\\hat s - s^*\\|^2_{L^2(\\rho^*)}$ that is commonly used in the score matching literature, highlighting the curse of dimensionality where sample complexity for accurate score estimation grows exponentially with the dimension $d$. Leveraging key insights in empirical Bayes theory as well as a new convergence rate of smoothed empirical distribution in Hellinger distance, we show that a regularized score estimator based on a Gaussian kernel attains this rate, shown optimal by a matching minimax lower bound.","We also discuss the implication of our theory on the sample complexity of score-based generative models."],"url":"http://arxiv.org/abs/2402.07747v1","category":"math.ST"}
{"created":"2024-02-12 16:15:28","title":"Minimally Interactive Segmentation of Soft-Tissue Tumors on CT and MRI using Deep Learning","abstract":"Segmentations are crucial in medical imaging to obtain morphological, volumetric, and radiomics biomarkers. Manual segmentation is accurate but not feasible in the radiologist's clinical workflow, while automatic segmentation generally obtains sub-par performance. We therefore developed a minimally interactive deep learning-based segmentation method for soft-tissue tumors (STTs) on CT and MRI. The method requires the user to click six points near the tumor's extreme boundaries. These six points are transformed into a distance map and serve, with the image, as input for a Convolutional Neural Network. For training and validation, a multicenter dataset containing 514 patients and nine STT types in seven anatomical locations was used, resulting in a Dice Similarity Coefficient (DSC) of 0.85$\\pm$0.11 (mean $\\pm$ standard deviation (SD)) for CT and 0.84$\\pm$0.12 for T1-weighted MRI, when compared to manual segmentations made by expert radiologists. Next, the method was externally validated on a dataset including five unseen STT phenotypes in extremities, achieving 0.81$\\pm$0.08 for CT, 0.84$\\pm$0.09 for T1-weighted MRI, and 0.88\\pm0.08 for previously unseen T2-weighted fat-saturated (FS) MRI. In conclusion, our minimally interactive segmentation method effectively segments different types of STTs on CT and MRI, with robust generalization to previously unseen phenotypes and imaging modalities.","sentences":["Segmentations are crucial in medical imaging to obtain morphological, volumetric, and radiomics biomarkers.","Manual segmentation is accurate but not feasible in the radiologist's clinical workflow, while automatic segmentation generally obtains sub-par performance.","We therefore developed a minimally interactive deep learning-based segmentation method for soft-tissue tumors (STTs) on CT and MRI.","The method requires the user to click six points near the tumor's extreme boundaries.","These six points are transformed into a distance map and serve, with the image, as input for a Convolutional Neural Network.","For training and validation, a multicenter dataset containing 514 patients and nine STT types in seven anatomical locations was used, resulting in a Dice Similarity Coefficient (DSC) of 0.85$\\pm$0.11 (mean $\\pm$ standard deviation (SD)) for CT and 0.84$\\pm$0.12 for T1-weighted MRI, when compared to manual segmentations made by expert radiologists.","Next, the method was externally validated on a dataset including five unseen STT phenotypes in extremities, achieving 0.81$\\pm$0.08 for CT, 0.84$\\pm$0.09 for T1-weighted MRI, and 0.88\\pm0.08 for previously unseen T2-weighted fat-saturated (FS) MRI.","In conclusion, our minimally interactive segmentation method effectively segments different types of STTs on CT and MRI, with robust generalization to previously unseen phenotypes and imaging modalities."],"url":"http://arxiv.org/abs/2402.07746v1","category":"eess.IV"}
{"created":"2024-02-12 16:14:22","title":"Towards Unified Alignment Between Agents, Humans, and Environment","abstract":"The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\\mathbf{U}$nified $\\mathbf{A}$lignment for $\\mathbf{A}$gents ($\\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost statistics to reflect self-constraints. We then follow the principles of $\\mathbf{UA}^2$ to propose an initial design of our agent, and benchmark its performance with several candidate baselines in the retrofitted WebShop. The extensive experimental results further prove the importance of the principles of $\\mathbf{UA}^2$. Our research sheds light on the next steps of autonomous agent research with improved general problem-solving abilities.","sentences":["The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction.","However, the efficacy of agents remains limited when operating in intricate, realistic environments.","In this work, we introduce the principles of $\\mathbf{U}$nified $\\mathbf{A}$lignment for $\\mathbf{A}$gents ($\\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets.","From the perspective of $\\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates.","We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost statistics to reflect self-constraints.","We then follow the principles of $\\mathbf{UA}^2$ to propose an initial design of our agent, and benchmark its performance with several candidate baselines in the retrofitted WebShop.","The extensive experimental results further prove the importance of the principles of $\\mathbf{UA}^2$. Our research sheds light on the next steps of autonomous agent research with improved general problem-solving abilities."],"url":"http://arxiv.org/abs/2402.07744v1","category":"cs.AI"}
{"created":"2024-02-12 16:05:22","title":"Beyond Sparsity: Local Projections Inference with High-Dimensional Covariates","abstract":"Impulse response analysis studies how the economy responds to shocks, such as changes in interest rates, and helps policymakers manage these effects. While Vector Autoregression Models (VARs) with structural assumptions have traditionally dominated the estimation of impulse responses, local projections, the projection of future responses on current shock, have recently gained attention for their robustness and interpretability. Including many lags as controls is proposed as a means of robustness, and including a richer set of controls helps in its interpretation as a causal parameter. In both cases, an extensive number of controls leads to the consideration of high-dimensional techniques. While methods like LASSO exist, they mostly rely on sparsity assumptions - most of the parameters are exactly zero, which has limitations in dense data generation processes. This paper proposes a novel approach that incorporates high-dimensional covariates in local projections without relying on sparsity constraints. Adopting the Orthogonal Greedy Algorithm with a high-dimensional AIC (OGA+HDAIC) model selection method, this approach offers advantages including robustness in both sparse and dense scenarios, improved interpretability by prioritizing cross-sectional explanatory power, and more reliable causal inference in local projections.","sentences":["Impulse response analysis studies how the economy responds to shocks, such as changes in interest rates, and helps policymakers manage these effects.","While Vector Autoregression Models (VARs) with structural assumptions have traditionally dominated the estimation of impulse responses, local projections, the projection of future responses on current shock, have recently gained attention for their robustness and interpretability.","Including many lags as controls is proposed as a means of robustness, and including a richer set of controls helps in its interpretation as a causal parameter.","In both cases, an extensive number of controls leads to the consideration of high-dimensional techniques.","While methods like LASSO exist, they mostly rely on sparsity assumptions - most of the parameters are exactly zero, which has limitations in dense data generation processes.","This paper proposes a novel approach that incorporates high-dimensional covariates in local projections without relying on sparsity constraints.","Adopting the Orthogonal Greedy Algorithm with a high-dimensional AIC (OGA+HDAIC) model selection method, this approach offers advantages including robustness in both sparse and dense scenarios, improved interpretability by prioritizing cross-sectional explanatory power, and more reliable causal inference in local projections."],"url":"http://arxiv.org/abs/2402.07743v1","category":"econ.EM"}
{"created":"2024-02-12 16:04:01","title":"Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search","abstract":"In mixed-initiative conversational search systems, clarifying questions are used to help users who struggle to express their intentions in a single query. These questions aim to uncover user's information needs and resolve query ambiguities. We hypothesize that in scenarios where multimodal information is pertinent, the clarification process can be improved by using non-textual information. Therefore, we propose to add images to clarifying questions and formulate the novel task of asking multimodal clarifying questions in open-domain, mixed-initiative conversational search systems. To facilitate research into this task, we collect a dataset named Melon that contains over 4k multimodal clarifying questions, enriched with over 14k images. We also propose a multimodal query clarification model named Marto and adopt a prompt-based, generative fine-tuning strategy to perform the training of different stages with different prompts. Several analyses are conducted to understand the importance of multimodal contents during the query clarification phase. Experimental results indicate that the addition of images leads to significant improvements of up to 90% in retrieval performance when selecting the relevant images. Extensive analyses are also performed to show the superiority of Marto compared with discriminative baselines in terms of effectiveness and efficiency.","sentences":["In mixed-initiative conversational search systems, clarifying questions are used to help users who struggle to express their intentions in a single query.","These questions aim to uncover user's information needs and resolve query ambiguities.","We hypothesize that in scenarios where multimodal information is pertinent, the clarification process can be improved by using non-textual information.","Therefore, we propose to add images to clarifying questions and formulate the novel task of asking multimodal clarifying questions in open-domain, mixed-initiative conversational search systems.","To facilitate research into this task, we collect a dataset named Melon that contains over 4k multimodal clarifying questions, enriched with over 14k images.","We also propose a multimodal query clarification model named Marto and adopt a prompt-based, generative fine-tuning strategy to perform the training of different stages with different prompts.","Several analyses are conducted to understand the importance of multimodal contents during the query clarification phase.","Experimental results indicate that the addition of images leads to significant improvements of up to 90% in retrieval performance when selecting the relevant images.","Extensive analyses are also performed to show the superiority of Marto compared with discriminative baselines in terms of effectiveness and efficiency."],"url":"http://arxiv.org/abs/2402.07742v1","category":"cs.CL"}
{"created":"2024-02-12 15:57:31","title":"Task-conditioned adaptation of visual features in multi-task policy learning","abstract":"Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on a wide variety of tasks of the CortexBench benchmark and show that, compared to existing work, it can be addressed with a single policy. In particular, we demonstrate that adapting visual features is a key design choice and that the method generalizes to unseen tasks given visual demonstrations.","sentences":["Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules.","An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task.","Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning.","We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks.","We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations.","To this end, we propose a new optimization-based estimator.","We evaluate the method on a wide variety of tasks of the CortexBench benchmark and show that, compared to existing work, it can be addressed with a single policy.","In particular, we demonstrate that adapting visual features is a key design choice and that the method generalizes to unseen tasks given visual demonstrations."],"url":"http://arxiv.org/abs/2402.07739v1","category":"cs.CV"}
{"created":"2024-02-12 15:52:09","title":"Liftable Point-Line Configurations: Defining Equations and Irreducibility of Associated Matroid and Circuit Varieties","abstract":"We study point-line configurations through the lens of projective geometry and matroid theory. Our focus is on their realisation spaces, where we introduce the concepts of liftable and quasi-liftable configurations, exploring cases in which an $n$-tuple of collinear points can be lifted to a non-degenerate realisation of a point-line configuration. We show that forest configurations are liftable and characterise the realisation space of liftable configurations as the solution set of certain linear systems of equations. Moreover, we study the Zariski closure of the realisation spaces of liftable and quasi-liftable configurations, known as matroid varieties, and establish their irreducibility. Additionally, we compute an irreducible decomposition for their corresponding circuit varieties. Applying these liftability properties, we present a procedure generate some of the defining equations of the associated matroid varieties. As corollaries, we provide a geometric representation for the defining equations of two specific examples: the quadrilateral set and the $3\\times4$ grid. While the polynomials for the latter were previously computed using specialised algorithms tailored for this configuration, the geometric interpretation of these generators was missing. We compute a minimal generating set for the corresponding ideals.","sentences":["We study point-line configurations through the lens of projective geometry and matroid theory.","Our focus is on their realisation spaces, where we introduce the concepts of liftable and quasi-liftable configurations, exploring cases in which an $n$-tuple of collinear points can be lifted to a non-degenerate realisation of a point-line configuration.","We show that forest configurations are liftable and characterise the realisation space of liftable configurations as the solution set of certain linear systems of equations.","Moreover, we study the Zariski closure of the realisation spaces of liftable and quasi-liftable configurations, known as matroid varieties, and establish their irreducibility.","Additionally, we compute an irreducible decomposition for their corresponding circuit varieties.","Applying these liftability properties, we present a procedure generate some of the defining equations of the associated matroid varieties.","As corollaries, we provide a geometric representation for the defining equations of two specific examples: the quadrilateral set and the $3\\times4$ grid.","While the polynomials for the latter were previously computed using specialised algorithms tailored for this configuration, the geometric interpretation of these generators was missing.","We compute a minimal generating set for the corresponding ideals."],"url":"http://arxiv.org/abs/2402.07737v1","category":"math.CO"}
{"created":"2024-02-12 15:48:58","title":"Graph Structure Inference with BAM: Introducing the Bilinear Attention Mechanism","abstract":"In statistics and machine learning, detecting dependencies in datasets is a central challenge. We propose a novel neural network model for supervised graph structure learning, i.e., the process of learning a mapping between observational data and their underlying dependence structure. The model is trained with variably shaped and coupled simulated input data and requires only a single forward pass through the trained network for inference. By leveraging structural equation models and employing randomly generated multivariate Chebyshev polynomials for the simulation of training data, our method demonstrates robust generalizability across both linear and various types of non-linear dependencies. We introduce a novel bilinear attention mechanism (BAM) for explicit processing of dependency information, which operates on the level of covariance matrices of transformed data and respects the geometry of the manifold of symmetric positive definite matrices. Empirical evaluation demonstrates the robustness of our method in detecting a wide range of dependencies, excelling in undirected graph estimation and proving competitive in completed partially directed acyclic graph estimation through a novel two-step approach.","sentences":["In statistics and machine learning, detecting dependencies in datasets is a central challenge.","We propose a novel neural network model for supervised graph structure learning, i.e., the process of learning a mapping between observational data and their underlying dependence structure.","The model is trained with variably shaped and coupled simulated input data and requires only a single forward pass through the trained network for inference.","By leveraging structural equation models and employing randomly generated multivariate Chebyshev polynomials for the simulation of training data, our method demonstrates robust generalizability across both linear and various types of non-linear dependencies.","We introduce a novel bilinear attention mechanism (BAM) for explicit processing of dependency information, which operates on the level of covariance matrices of transformed data and respects the geometry of the manifold of symmetric positive definite matrices.","Empirical evaluation demonstrates the robustness of our method in detecting a wide range of dependencies, excelling in undirected graph estimation and proving competitive in completed partially directed acyclic graph estimation through a novel two-step approach."],"url":"http://arxiv.org/abs/2402.07735v1","category":"stat.ML"}
{"created":"2024-02-12 15:41:22","title":"AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension","abstract":"Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as Automatic Speech Recognition (ASR), and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench (\\textbf{A}udio \\textbf{I}nst\\textbf{R}uction \\textbf{Bench}mark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: \\textit{foundation} and \\textit{chat} benchmarks. The former consists of 19 tasks with approximately 19k single-choice questions, intending to inspect the basic single-task ability of LALMs. The latter one contains 2k instances of open-ended question-and-answer data, directly assessing the comprehension of the model on complex audio and its capacity to follow instructions. Both benchmarks require the model to generate hypotheses directly. We design a unified framework that leverages advanced language models, such as GPT-4, to evaluate the scores of generated hypotheses given the meta-information of the audio. Experimental results demonstrate a high level of consistency between GPT-4-based evaluation and human evaluation. By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research.","sentences":["Recently, instruction-following audio-language models have received broad attention for human-audio interaction.","However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field.","Previous models primarily focus on assessing different fundamental tasks, such as Automatic Speech Recognition (ASR), and lack an assessment of the open-ended generative capabilities centered around audio.","Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement.","In this paper, we introduce AIR-Bench (\\textbf{A}udio \\textbf{I}nst\\textbf{R}uction \\textbf{Bench}mark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format.","AIR-Bench encompasses two dimensions: \\textit{foundation} and \\textit{chat} benchmarks.","The former consists of 19 tasks with approximately 19k single-choice questions, intending to inspect the basic single-task ability of LALMs.","The latter one contains 2k instances of open-ended question-and-answer data, directly assessing the comprehension of the model on complex audio and its capacity to follow instructions.","Both benchmarks require the model to generate hypotheses directly.","We design a unified framework that leverages advanced language models, such as GPT-4, to evaluate the scores of generated hypotheses given the meta-information of the audio.","Experimental results demonstrate a high level of consistency between GPT-4-based evaluation and human evaluation.","By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research."],"url":"http://arxiv.org/abs/2402.07729v1","category":"eess.AS"}
{"created":"2024-02-12 15:39:05","title":"Unsupervised Sign Language Translation and Generation","abstract":"Motivated by the success of unsupervised neural machine translation (UNMT), we introduce an unsupervised sign language translation and generation network (USLNet), which learns from abundant single-modality (text and video) data without parallel sign language data. USLNet comprises two main components: single-modality reconstruction modules (text and video) that rebuild the input from its noisy version in the same modality and cross-modality back-translation modules (text-video-text and video-text-video) that reconstruct the input from its noisy version in the different modality using back-translation procedure.Unlike the single-modality back-translation procedure in text-based UNMT, USLNet faces the cross-modality discrepancy in feature representation, in which the length and the feature dimension mismatch between text and video sequences. We propose a sliding window method to address the issues of aligning variable-length text with video sequences. To our knowledge, USLNet is the first unsupervised sign language translation and generation model capable of generating both natural language text and sign language video in a unified manner. Experimental results on the BBC-Oxford Sign Language dataset (BOBSL) and Open-Domain American Sign Language dataset (OpenASL) reveal that USLNet achieves competitive results compared to supervised baseline models, indicating its effectiveness in sign language translation and generation.","sentences":["Motivated by the success of unsupervised neural machine translation (UNMT), we introduce an unsupervised sign language translation and generation network (USLNet), which learns from abundant single-modality (text and video) data without parallel sign language data.","USLNet comprises two main components: single-modality reconstruction modules (text and video) that rebuild the input from its noisy version in the same modality and cross-modality back-translation modules (text-video-text and video-text-video) that reconstruct the input from its noisy version in the different modality using back-translation procedure.","Unlike the single-modality back-translation procedure in text-based UNMT, USLNet faces the cross-modality discrepancy in feature representation, in which the length and the feature dimension mismatch between text and video sequences.","We propose a sliding window method to address the issues of aligning variable-length text with video sequences.","To our knowledge, USLNet is the first unsupervised sign language translation and generation model capable of generating both natural language text and sign language video in a unified manner.","Experimental results on the BBC-Oxford Sign Language dataset (BOBSL) and Open-Domain American Sign Language dataset (OpenASL) reveal that USLNet achieves competitive results compared to supervised baseline models, indicating its effectiveness in sign language translation and generation."],"url":"http://arxiv.org/abs/2402.07726v1","category":"cs.CL"}
{"created":"2024-02-12 15:35:32","title":"Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation","abstract":"Understanding the generalization properties of heavy-tailed stochastic optimization algorithms has attracted increasing attention over the past years. While illuminating interesting aspects of stochastic optimizers by using heavy-tailed stochastic differential equations as proxies, prior works either provided expected generalization bounds, or introduced non-computable information theoretic terms. Addressing these drawbacks, in this work, we prove high-probability generalization bounds for heavy-tailed SDEs which do not contain any nontrivial information theoretic terms. To achieve this goal, we develop new proof techniques based on estimating the entropy flows associated with the so-called fractional Fokker-Planck equation (a partial differential equation that governs the evolution of the distribution of the corresponding heavy-tailed SDE). In addition to obtaining high-probability bounds, we show that our bounds have a better dependence on the dimension of parameters as compared to prior art. Our results further identify a phase transition phenomenon, which suggests that heavy tails can be either beneficial or harmful depending on the problem structure. We support our theory with experiments conducted in a variety of settings.","sentences":["Understanding the generalization properties of heavy-tailed stochastic optimization algorithms has attracted increasing attention over the past years.","While illuminating interesting aspects of stochastic optimizers by using heavy-tailed stochastic differential equations as proxies, prior works either provided expected generalization bounds, or introduced non-computable information theoretic terms.","Addressing these drawbacks, in this work, we prove high-probability generalization bounds for heavy-tailed SDEs which do not contain any nontrivial information theoretic terms.","To achieve this goal, we develop new proof techniques based on estimating the entropy flows associated with the so-called fractional Fokker-Planck equation (a partial differential equation that governs the evolution of the distribution of the corresponding heavy-tailed SDE).","In addition to obtaining high-probability bounds, we show that our bounds have a better dependence on the dimension of parameters as compared to prior art.","Our results further identify a phase transition phenomenon, which suggests that heavy tails can be either beneficial or harmful depending on the problem structure.","We support our theory with experiments conducted in a variety of settings."],"url":"http://arxiv.org/abs/2402.07723v1","category":"stat.ML"}
{"created":"2024-02-12 15:35:24","title":"Path Integral Monte Carlo Study of a Doubly-Dipolar Bose Gas","abstract":"By combining first-principles path integral Monte Carlo methods and mean-field techniques, we explore the properties of cylindrically trapped doubly-dipolar Bose gases. We first verify the emergence of a pancake quantum droplet at low temperatures, validating previously mean-field calculations. In a regime of small doubly-dipolar interactions, first-principles calculations agree with the generalized Gross-Pitaevskii equation. Such an accordance disappears in a large interaction limit. Here the path integral Monte Carlo estimates the strong doubly-dipolar regime with accuracy. On the contrary, the Gross-Pitaevskii equation does not seize quantum fluctuations in full. We also provide a complete description of the system's quantum behavior in a wide range of parameters. When the system forms a droplet, the superfluid fraction exhibits an anisotropic behavior if compared to the usual Bose gas regime. Interestingly, we observe that the transition temperature from thermal gas to droplet results higher than that of the thermal gas to a Bose-Einstein condensate, indicating the robustness of the droplet against thermal fluctuations. Further, we investigate the anisotropic behavior of the superfluid fraction during the structural transition from a pancake to a cigar-shaped droplet by varying the ratio between electric and magnetic dipole interaction strengths. Our findings furnish evidence that the stability of doubly-dipolar Bose-Einstein condensates can be detected in experiments by means of dysprosium atoms.","sentences":["By combining first-principles path integral Monte Carlo methods and mean-field techniques, we explore the properties of cylindrically trapped doubly-dipolar Bose gases.","We first verify the emergence of a pancake quantum droplet at low temperatures, validating previously mean-field calculations.","In a regime of small doubly-dipolar interactions, first-principles calculations agree with the generalized Gross-Pitaevskii equation.","Such an accordance disappears in a large interaction limit.","Here the path integral Monte Carlo estimates the strong doubly-dipolar regime with accuracy.","On the contrary, the Gross-Pitaevskii equation does not seize quantum fluctuations in full.","We also provide a complete description of the system's quantum behavior in a wide range of parameters.","When the system forms a droplet, the superfluid fraction exhibits an anisotropic behavior if compared to the usual Bose gas regime.","Interestingly, we observe that the transition temperature from thermal gas to droplet results higher than that of the thermal gas to a Bose-Einstein condensate, indicating the robustness of the droplet against thermal fluctuations.","Further, we investigate the anisotropic behavior of the superfluid fraction during the structural transition from a pancake to a cigar-shaped droplet by varying the ratio between electric and magnetic dipole interaction strengths.","Our findings furnish evidence that the stability of doubly-dipolar Bose-Einstein condensates can be detected in experiments by means of dysprosium atoms."],"url":"http://arxiv.org/abs/2402.07722v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-12 15:32:38","title":"Efficient reductions between some statistical models","abstract":"We study the problem of approximately transforming a sample from a source statistical model to a sample from a target statistical model without knowing the parameters of the source model, and construct several computationally efficient such reductions between statistical experiments. In particular, we provide computationally efficient procedures that approximately reduce uniform, Erlang, and Laplace location models to general target families. We illustrate our methodology by establishing nonasymptotic reductions between some canonical high-dimensional problems, spanning mixtures of experts, phase retrieval, and signal denoising. Notably, the reductions are structure preserving and can accommodate missing data. We also point to a possible application in transforming one differentially private mechanism to another.","sentences":["We study the problem of approximately transforming a sample from a source statistical model to a sample from a target statistical model without knowing the parameters of the source model, and construct several computationally efficient such reductions between statistical experiments.","In particular, we provide computationally efficient procedures that approximately reduce uniform, Erlang, and Laplace location models to general target families.","We illustrate our methodology by establishing nonasymptotic reductions between some canonical high-dimensional problems, spanning mixtures of experts, phase retrieval, and signal denoising.","Notably, the reductions are structure preserving and can accommodate missing data.","We also point to a possible application in transforming one differentially private mechanism to another."],"url":"http://arxiv.org/abs/2402.07717v1","category":"math.ST"}
{"created":"2024-02-12 15:29:10","title":"$\u039b_{\\rm s}$CDM cosmology from a type-II minimally modified gravity","abstract":"We have successfully integrated $\\Lambda_{\\rm s}$CDM, a promising model for alleviating cosmological tensions, into a theoretical framework by endowing it with a specific Lagrangian from the VCDM model, a type-II minimally modified gravity. In this theory, we demonstrate that an auxiliary scalar field with a linear potential induces an effective cosmological constant, enabling the realization of an abrupt mirror AdS-dS transition in the late universe through a piecewise linear potential. To eliminate the sudden singularity in this setup and ensure stable transitions, we smooth out this potential. Realized within the VCDM theory, the $\\Lambda_{\\rm s}$CDM model facilitates two types of rapid smooth mirror AdS-dS transitions: (i) the agitated transition, associated with a smooth jump in the potential, where $\\Lambda_{\\rm s}$, and consequently $H$, exhibits a bump around the transition's midpoint; and (ii) the quiescent transition, associated with a smooth change in the slope of the potential, where $\\Lambda_{\\rm s}$ transitions gracefully. These transitions are likely to leave distinct imprints on the background and perturbation dynamics, potentially allowing the observational data to distinguish between them. This novel theoretical framework propels $\\Lambda_{\\rm s}$CDM into a fully predictive model capable of exploring the evolution of the Universe including the late-time AdS-dS transition epoch, and extends the applicability of the model. We believe further research is crucial in establishing $\\Lambda_{\\rm s}$CDM as a leading candidate or guide for a new concordance cosmological model.","sentences":["We have successfully integrated $\\Lambda_{\\rm s}$CDM, a promising model for alleviating cosmological tensions, into a theoretical framework by endowing it with a specific Lagrangian from the VCDM model, a type-II minimally modified gravity.","In this theory, we demonstrate that an auxiliary scalar field with a linear potential induces an effective cosmological constant, enabling the realization of an abrupt mirror AdS-dS transition in the late universe through a piecewise linear potential.","To eliminate the sudden singularity in this setup and ensure stable transitions, we smooth out this potential.","Realized within the VCDM theory, the $\\Lambda_{\\rm s}$CDM model facilitates two types of rapid smooth mirror AdS-dS transitions: (i) the agitated transition, associated with a smooth jump in the potential, where $\\Lambda_{\\rm s}$, and consequently $H$, exhibits a bump around the transition's midpoint; and (ii) the quiescent transition, associated with a smooth change in the slope of the potential, where $\\Lambda_{\\rm s}$ transitions gracefully.","These transitions are likely to leave distinct imprints on the background and perturbation dynamics, potentially allowing the observational data to distinguish between them.","This novel theoretical framework propels $\\Lambda_{\\rm s}$CDM into a fully predictive model capable of exploring the evolution of the Universe including the late-time AdS-dS transition epoch, and extends the applicability of the model.","We believe further research is crucial in establishing $\\Lambda_{\\rm s}$CDM as a leading candidate or guide for a new concordance cosmological model."],"url":"http://arxiv.org/abs/2402.07716v1","category":"astro-ph.CO"}
{"created":"2024-02-12 15:26:37","title":"Adaptive Artificial Immune Networks for Mitigating DoS flooding Attacks","abstract":"Denial of service attacks pose a threat in constant growth. This is mainly due to their tendency to gain in sophistication, ease of implementation, obfuscation and the recent improvements in occultation of fingerprints. On the other hand, progress towards self-organizing networks, and the different techniques involved in their development, such as software-defined networking, network-function virtualization, artificial intelligence or cloud computing, facilitates the design of new defensive strategies, more complete, consistent and able to adapt the defensive deployment to the current status of the network. In order to contribute to their development, in this paper, the use of artificial immune systems to mitigate denial of service attacks is proposed. The approach is based on building networks of distributed sensors suited to the requirements of the monitored environment. These components are capable of identifying threats and reacting according to the behavior of the biological defense mechanisms in human beings. It is accomplished by emulating the different immune reactions, the establishment of quarantine areas and the construction of immune memory. For their assessment, experiments with public domain datasets (KDD'99, CAIDA'07 and CAIDA'08) and simulations on various network configurations based on traffic samples gathered by the University Complutense of Madrid and flooding attacks generated by the tool DDoSIM were performed.","sentences":["Denial of service attacks pose a threat in constant growth.","This is mainly due to their tendency to gain in sophistication, ease of implementation, obfuscation and the recent improvements in occultation of fingerprints.","On the other hand, progress towards self-organizing networks, and the different techniques involved in their development, such as software-defined networking, network-function virtualization, artificial intelligence or cloud computing, facilitates the design of new defensive strategies, more complete, consistent and able to adapt the defensive deployment to the current status of the network.","In order to contribute to their development, in this paper, the use of artificial immune systems to mitigate denial of service attacks is proposed.","The approach is based on building networks of distributed sensors suited to the requirements of the monitored environment.","These components are capable of identifying threats and reacting according to the behavior of the biological defense mechanisms in human beings.","It is accomplished by emulating the different immune reactions, the establishment of quarantine areas and the construction of immune memory.","For their assessment, experiments with public domain datasets (KDD'99, CAIDA'07 and CAIDA'08) and simulations on various network configurations based on traffic samples gathered by the University Complutense of Madrid and flooding attacks generated by the tool DDoSIM were performed."],"url":"http://arxiv.org/abs/2402.07714v1","category":"cs.CR"}
{"created":"2024-02-12 15:26:15","title":"Closed-form expressions for multiscatter Dark Matter capture rates","abstract":"Any astrophysical object can, in principle, serve as a probe of the interaction between Dark Matter and regular, baryonic matter. This method is based on the potential observable consequences annihilations of captured Dark Matter has on the surface temperature of the object itself. In a series of previous papers we developed and validated simple analytic approximations for the total capture rates of Dark Matter (DM) valid in four distinct regions of the DM-nucleon scattering cross section ($\\sigma$) vs. DM particle mass ($m_X$) parameter space. In this work we summarize those previous results and extend them significantly, by deriving a completely general, closed form solution for the total capture rate of Dark Matter in the multiscatter regime. Moreover, we demonstrate the existence of a region in the $\\sigma$ vs. $m_X$ parameter space where the constraining power of any astrophysical object heated by annihilations of captured DM is lost. This corresponds to a maximal temperature ($T_{crit}$) any astrophysical object can have, such that it can still serve as a DM probe. Any object with observed temperature $T_{obs}>T_{crit}$ loses its DM constraining power. We provide analytic formulae that can be used to estimate $T_{crit}$ for any object.","sentences":["Any astrophysical object can, in principle, serve as a probe of the interaction between Dark Matter and regular, baryonic matter.","This method is based on the potential observable consequences annihilations of captured Dark Matter has on the surface temperature of the object itself.","In a series of previous papers we developed and validated simple analytic approximations for the total capture rates of Dark Matter (DM) valid in four distinct regions of the DM-nucleon scattering cross section ($\\sigma$) vs. DM particle mass ($m_X$) parameter space.","In this work we summarize those previous results and extend them significantly, by deriving a completely general, closed form solution for the total capture rate of Dark Matter in the multiscatter regime.","Moreover, we demonstrate the existence of a region in the $\\sigma$ vs. $m_X$ parameter space where the constraining power of any astrophysical object heated by annihilations of captured DM is lost.","This corresponds to a maximal temperature ($T_{crit}$) any astrophysical object can have, such that it can still serve as a DM probe.","Any object with observed temperature $T_{obs}>T_{crit}$ loses its DM constraining power.","We provide analytic formulae that can be used to estimate $T_{crit}$ for any object."],"url":"http://arxiv.org/abs/2402.07713v1","category":"astro-ph.CO"}
{"created":"2024-02-12 15:26:01","title":"Model Collapse Demystified: The Case of Regression","abstract":"In the era of large language models like ChatGPT, the phenomenon of \"model collapse\" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses. Under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.","sentences":["In the era of large language models like ChatGPT, the phenomenon of \"model collapse\" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses.","In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses.","Under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates.","We also propose a simple strategy based on adaptive regularization to mitigate model collapse.","Our theoretical results are validated with experiments."],"url":"http://arxiv.org/abs/2402.07712v1","category":"cs.LG"}
{"created":"2024-02-12 15:24:44","title":"Near optimal constructions of frameproof codes","abstract":"Frameproof codes are a class of secure codes that were originally introduced in the pioneering work of Boneh and Shaw in the context of digital fingerprinting. They can be used to enhance the security and credibility of digital content. Let $M_{c,l}(q)$ denote the largest cardinality of a $q$-ary $c$-frameproof code with length $l$. Based on an intriguing observation that relates $M_{c,l}(q)$ to the renowned Erd\\H{o}s Matching Conjecture in extremal set theory, in 2003, Blackburn posed an open problem on the precise value of the limit $R_{c,l}=\\lim_{q\\rightarrow\\infty}\\frac{M_{c,l}(q)}{q^{\\lceil l/c \\rceil}}$. By combining several ideas from the probabilistic method, we present a lower bound for $M_{c,l}(q)$, which, together with an upper bound of Blackburn, completely determines $R_{c,l}$ for {\\it all} fixed $c,l$, and resolves the above open problem in the full generality. We also present an improved upper bound for $M_{c,l}(q)$.","sentences":["Frameproof codes are a class of secure codes that were originally introduced in the pioneering work of Boneh and Shaw in the context of digital fingerprinting.","They can be used to enhance the security and credibility of digital content.","Let $M_{c,l}(q)$ denote the largest cardinality of a $q$-ary $c$-frameproof code with length $l$. Based on an intriguing observation that relates $M_{c,l}(q)$ to the renowned Erd\\H{o}s","Matching Conjecture in extremal set theory, in 2003, Blackburn posed an open problem on the precise value of the limit $R_{c,l}=\\lim_{q\\rightarrow\\infty}\\frac{M_{c,l}(q)}{q^{\\lceil l/c \\rceil}}$. By combining several ideas from the probabilistic method, we present a lower bound for $M_{c,l}(q)$, which, together with an upper bound of Blackburn, completely determines $R_{c,l}$ for {\\it all} fixed $c,l$, and resolves the above open problem in the full generality.","We also present an improved upper bound for $M_{c,l}(q)$."],"url":"http://arxiv.org/abs/2402.07711v1","category":"cs.IT"}
{"created":"2024-02-12 15:23:19","title":"Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA","abstract":"In recent years, there has been a significant increase in the utilization of deep learning methods, particularly convolutional neural networks (CNNs), which have emerged as the dominant approach in various domains that involve structured grid data, such as picture analysis and processing. Nevertheless, the exponential growth in the utilization of LiDAR and 3D sensors across many domains has resulted in an increased need for the analysis of 3D point clouds. The utilization of 3D point clouds is crucial in various applications, including object recognition and segmentation, as they offer a spatial depiction of things within a three-dimensional environment. In contrast to photos, point clouds exhibit sparsity and lack a regular grid, hence posing distinct processing and computational issues.","sentences":["In recent years, there has been a significant increase in the utilization of deep learning methods, particularly convolutional neural networks (CNNs), which have emerged as the dominant approach in various domains that involve structured grid data, such as picture analysis and processing.","Nevertheless, the exponential growth in the utilization of LiDAR and 3D sensors across many domains has resulted in an increased need for the analysis of 3D point clouds.","The utilization of 3D point clouds is crucial in various applications, including object recognition and segmentation, as they offer a spatial depiction of things within a three-dimensional environment.","In contrast to photos, point clouds exhibit sparsity and lack a regular grid, hence posing distinct processing and computational issues."],"url":"http://arxiv.org/abs/2402.07710v1","category":"cs.AI"}
{"created":"2024-02-12 15:17:31","title":"Online Sequential Decision-Making with Unknown Delays","abstract":"In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay. Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback. Our proposed algorithms are versatile and applicable to universal norms. Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points. For each type of algorithm, we provide corresponding regret bounds under cases of general convexity and relative strong convexity, respectively. We also demonstrate the efficiency of each algorithm under different norms through concrete examples. Furthermore, our theoretical results are consistent with the current best bounds when degenerated to standard settings.","sentences":["In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay.","Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback.","Our proposed algorithms are versatile and applicable to universal norms.","Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points.","For each type of algorithm, we provide corresponding regret bounds under cases of general convexity and relative strong convexity, respectively.","We also demonstrate the efficiency of each algorithm under different norms through concrete examples.","Furthermore, our theoretical results are consistent with the current best bounds when degenerated to standard settings."],"url":"http://arxiv.org/abs/2402.07703v1","category":"cs.LG"}
{"created":"2024-02-12 15:16:20","title":"Stabilization of Mn4+ in synthetic slags and identification of important slag forming phases","abstract":"The expected shortage of Li due to the strong increase in electromobility is an important issue for the recovery of Li from spent Li-ion batteries. One approach is pyrometallurgical processing, during which ignoble elements such as Li, Al and Mn enter the slag system. The Engineered Artificial Minerals (EnAM) strategy aims to efficiently recover critical elements. This study focuses on stabilizing Li-manganates in a synthetic slag and investigates the relationship between Mn4+ and Mg and Al in relation to phase formation. Therefore, three synthetic slags (Li, Mg, Al, Si, Ca, Mn, O) were synthesized. In addition to LiMn3+O2, Li2Mn4+O3 was also stabilized. Both phases crystallized in a Ca-silicate-rich matrix. In the structure of Li2MnO3 and LiMnO2, Li and Mn can substitute each other in certain proportions. As long as a mix of Mn2+ and Mn3+ is present in the slag, spinels form through the addition of Mg and/or Al.","sentences":["The expected shortage of Li due to the strong increase in electromobility is an important issue for the recovery of Li from spent Li-ion batteries.","One approach is pyrometallurgical processing, during which ignoble elements such as Li, Al and Mn enter the slag system.","The Engineered Artificial Minerals (EnAM) strategy aims to efficiently recover critical elements.","This study focuses on stabilizing Li-manganates in a synthetic slag and investigates the relationship between Mn4+ and Mg and Al in relation to phase formation.","Therefore, three synthetic slags (Li, Mg, Al, Si, Ca, Mn, O) were synthesized.","In addition to LiMn3+O2, Li2Mn4+O3 was also stabilized.","Both phases crystallized in a Ca-silicate-rich matrix.","In the structure of Li2MnO3 and LiMnO2, Li and Mn can substitute each other in certain proportions.","As long as a mix of Mn2+ and Mn3+ is present in the slag, spinels form through the addition of Mg and/or Al."],"url":"http://arxiv.org/abs/2402.07702v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-12 15:08:33","title":"On the Landau-Streater channel for orthogonal and unitary groups","abstract":"In $d=2j+1$ dimensions, the Landau-Streater quantum channel is defined on the basis of spin $j$ representation of the $su(2)$ algebra. Only for $j=1$, this channel is equivalent to the Werner-Holevo channel and enjoys covariance properties with respect to the group $SU(3)$. We extend this class of channels to higher dimensions in a way which is based on the Lie algebra $so(d)$. As a result it retains its equivalence to the Werner-Holevo channel in arbitrary dimensions. The resulting channel is covariant with respect to the unitary group $U(d)$. We then modify this channel in a way which can act as a noisy channel on qudits. The resulting modified channel now interpolates between the identity channel and the Werner-Holevo channel and its covariance is reduced to the subgroup of orthogonal matrices $SO(d)$. We then investigate some of the propeties of the resulting one-parameter family of channels, including their spectrum, their regions of lack of indivisibility, their one-shot classical capacity, entanglement-assisted capacity and the closed form of their complement channel and a possible lower bound for their quantum capacity. We briefly mention how the channel can be generalized to unitary groups.","sentences":["In $d=2j+1$ dimensions, the Landau-Streater quantum channel is defined on the basis of spin $j$ representation of the $su(2)$ algebra.","Only for $j=1$, this channel is equivalent to the Werner-Holevo channel and enjoys covariance properties with respect to the group $SU(3)$. We extend this class of channels to higher dimensions in a way which is based on the Lie algebra $so(d)$. As a result it retains its equivalence to the Werner-Holevo channel in arbitrary dimensions.","The resulting channel is covariant with respect to the unitary group $U(d)$. We then modify this channel in a way which can act as a noisy channel on qudits.","The resulting modified channel now interpolates between the identity channel and the Werner-Holevo channel and its covariance is reduced to the subgroup of orthogonal matrices $SO(d)$. We then investigate some of the propeties of the resulting one-parameter family of channels, including their spectrum, their regions of lack of indivisibility, their one-shot classical capacity, entanglement-assisted capacity and the closed form of their complement channel and a possible lower bound for their quantum capacity.","We briefly mention how the channel can be generalized to unitary groups."],"url":"http://arxiv.org/abs/2402.07700v1","category":"quant-ph"}
{"created":"2024-02-12 15:02:00","title":"Metastability and time scales for parabolic equations with drift 2: the general time scale","abstract":"Consider the elliptic operator given by \\[ \\mathscr{L}_\\epsilon f=b\\cdot\\nabla f+\\epsilon\\Delta f \\] for some smooth vector field $b:\\mathbb{R}^d\\to\\mathbb{R}^d$ and $\\epsilon>0$, and the initial-valued problem on $\\mathbb{R}^d$ \\[ \\left\\{\\begin{aligned}&\\partial_t u_\\epsilon=\\mathscr{L}_\\epsilon u_\\epsilon,\\\\ &u_\\epsilon(0,\\,\\cdot)=u_0(\\cdot), \\end{aligned} \\right. \\] for some bounded continuous function $u_0$. Under the hypothesis that the diffusion on $\\mathbb{R}^d$ induced by $\\mathscr{L}_\\epsilon$ has a Gibbs invariant measure of the form $\\exp \\{-U(x)/\\epsilon\\}dx$ for some smooth Morse potential function $U$, we provide the complete characterization of the multi-scale behavior of the solution $u_\\epsilon$ in the regime $\\epsilon\\to0$. More precisely, we find the critical time scales $1\\ll \\theta_\\epsilon^{(1)}\\ll\\cdots\\ll \\theta_\\epsilon^{(q)}$ as $\\epsilon\\to0$, and the kernels $R_t^{(p)}:M_0\\times M_0\\to\\mathbb{R}_+$, where $M_0$ denotes the set of local minima of $U$, such that \\[ \\lim_{\\epsilon\\to0}u_\\epsilon(t\\theta_\\epsilon^{(p)},\\,x)=\\sum_{m'\\in M_0}R_t^{(p)}(m,\\,m')u_0(m'), \\] for all $t>0$ and $x$ in the domain of attraction of $m$ for the dynamical system $\\dot{x}(t)=b(x(t))$. We then complete the characterization of the solution $u_\\epsilon$ by computing the exact asymptotic limit of the solution between time scales   $\\theta_\\epsilon^{(p)}$ and $\\theta_\\epsilon^{(p+1)}$ for each $p$, where $\\theta_\\epsilon^{(0)}=1$ and $\\theta_\\epsilon^{(q+1)}=\\infty$. Our proof relies on the full tree-structure characterization of the metastable behavior in different time-scales of the diffusion induced by $\\mathscr{L}_\\epsilon$. This result can be regarded as the precise refinement of Freidlin-Wentzell theory which was not known for more than a half century.","sentences":["Consider the elliptic operator given by \\[ \\mathscr{L}_\\epsilon f=b\\cdot\\nabla f+\\epsilon\\Delta f \\] for some smooth vector field $b:\\mathbb{R}^d\\to\\mathbb{R}^d$ and $\\epsilon>0$, and the initial-valued problem on $\\mathbb{R}^d$ \\[ \\left\\{\\begin{aligned}&\\partial_t u_\\epsilon=\\mathscr{L}_\\epsilon u_\\epsilon,\\\\ &u_\\epsilon(0,\\,\\cdot)=u_0(\\cdot), \\end{aligned} \\right.","\\] for some bounded continuous function $u_0$. Under the hypothesis that the diffusion on $\\mathbb{R}^d$ induced by $\\mathscr{L}_\\epsilon$ has a Gibbs invariant measure of the form $\\exp \\{-U(x)/\\epsilon\\}dx$ for some smooth Morse potential function $U$, we provide the complete characterization of the multi-scale behavior of the solution $u_\\epsilon$ in the regime $\\epsilon\\to0$. More precisely, we find the critical time scales $1\\ll \\theta_\\epsilon^{(1)}\\ll\\cdots\\ll \\theta_\\epsilon^{(q)}$ as $\\epsilon\\to0$, and the kernels $R_t^{(p)}:M_0\\times M_0\\to\\mathbb{R}_+$, where $M_0$ denotes the set of local minima of $U$, such that \\[ \\lim_{\\epsilon\\to0}u_\\epsilon(t\\theta_\\epsilon^{(p)},\\,x)=\\sum_{m'\\in M_0}R_t^{(p)}(m,\\,m')u_0(m'), \\] for all $t>0$ and $x$ in the domain of attraction of $m$ for the dynamical system $\\dot{x}(t)=b(x(t))$. We then complete the characterization of the solution $u_\\epsilon$ by computing the exact asymptotic limit of the solution between time scales   $\\theta_\\epsilon^{(p)}$ and $\\theta_\\epsilon^{(p+1)}$ for each $p$, where $\\theta_\\epsilon^{(0)}=1$ and $\\theta_\\epsilon^{(q+1)}=\\infty$. Our proof relies on the full tree-structure characterization of the metastable behavior in different time-scales of the diffusion induced by $\\mathscr{L}_\\epsilon$. This result can be regarded as the precise refinement of Freidlin-Wentzell theory which was not known for more than a half century."],"url":"http://arxiv.org/abs/2402.07695v1","category":"math.PR"}
{"created":"2024-02-12 15:01:42","title":"Cosmology at the Field Level with Probabilistic Machine Learning","abstract":"The large-scale structure in cosmology is highly non-Gaussian at late times and small length scales, making it difficult to describe analytically. Parameter inference, data reconstruction, and data generation tasks in cosmology are greatly aided by various machine learning models. In order to retain as much information as possible while solving these problems, this work operates at the field level, rather than at the level of summary statistics. The probability density function of the large-scale structure is learned with normalizing flows, a class of probabilistic generative models. Normalizing flows learn the transformation from a simple base distribution to a more complicated distribution, much like the matter content evolved to its present day complexities from a Gaussian field at early times. While the normalizing flows have accurately modelled 2-dimensional projections of the matter content, we find that denoising diffusion models are well-suited for volumetric data. A super-resolution emulator is developed for cosmological simulation volumes, generating high-resolution baryonic simulation volumes conditional on low-resolution dark matter simulations. The super-resolution emulator is trained to perform outpainting, and can thus upgrade very large cosmological volumes from low-resolution to high-resolution using an iterative outpainting procedure.","sentences":["The large-scale structure in cosmology is highly non-Gaussian at late times and small length scales, making it difficult to describe analytically.","Parameter inference, data reconstruction, and data generation tasks in cosmology are greatly aided by various machine learning models.","In order to retain as much information as possible while solving these problems, this work operates at the field level, rather than at the level of summary statistics.","The probability density function of the large-scale structure is learned with normalizing flows, a class of probabilistic generative models.","Normalizing flows learn the transformation from a simple base distribution to a more complicated distribution, much like the matter content evolved to its present day complexities from a Gaussian field at early times.","While the normalizing flows have accurately modelled 2-dimensional projections of the matter content, we find that denoising diffusion models are well-suited for volumetric data.","A super-resolution emulator is developed for cosmological simulation volumes, generating high-resolution baryonic simulation volumes conditional on low-resolution dark matter simulations.","The super-resolution emulator is trained to perform outpainting, and can thus upgrade very large cosmological volumes from low-resolution to high-resolution using an iterative outpainting procedure."],"url":"http://arxiv.org/abs/2402.07694v1","category":"astro-ph.CO"}
{"created":"2024-02-12 14:53:37","title":"OrderBkd: Textual backdoor attack through repositioning","abstract":"The use of third-party datasets and pre-trained machine learning models poses a threat to NLP systems due to possibility of hidden backdoor attacks. Existing attacks involve poisoning the data samples such as insertion of tokens or sentence paraphrasing, which either alter the semantics of the original texts or can be detected. Our main difference from the previous work is that we use the reposition of a two words in a sentence as a trigger. By designing and applying specific part-of-speech (POS) based rules for selecting these tokens, we maintain high attack success rate on SST-2 and AG classification datasets while outperforming existing attacks in terms of perplexity and semantic similarity to the clean samples. In addition, we show the robustness of our attack to the ONION defense method. All the code and data for the paper can be obtained at https://github.com/alekseevskaia/OrderBkd.","sentences":["The use of third-party datasets and pre-trained machine learning models poses a threat to NLP systems due to possibility of hidden backdoor attacks.","Existing attacks involve poisoning the data samples such as insertion of tokens or sentence paraphrasing, which either alter the semantics of the original texts or can be detected.","Our main difference from the previous work is that we use the reposition of a two words in a sentence as a trigger.","By designing and applying specific part-of-speech (POS) based rules for selecting these tokens, we maintain high attack success rate on SST-2 and AG classification datasets while outperforming existing attacks in terms of perplexity and semantic similarity to the clean samples.","In addition, we show the robustness of our attack to the ONION defense method.","All the code and data for the paper can be obtained at https://github.com/alekseevskaia/OrderBkd."],"url":"http://arxiv.org/abs/2402.07689v1","category":"cs.CL"}
{"created":"2024-02-12 14:53:28","title":"CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity","abstract":"Large Language Models (LLMs) excel across various domains, from computer vision to medical diagnostics. However, understanding the diverse landscape of cybersecurity, encompassing cryptography, reverse engineering, and managerial facets like risk assessment, presents a challenge, even for human experts. In this paper, we introduce CyberMetric, a benchmark dataset comprising 10,000 questions sourced from standards, certifications, research papers, books, and other publications in the cybersecurity domain. The questions are created through a collaborative process, i.e., merging expert knowledge with LLMs, including GPT-3.5 and Falcon-180B. Human experts spent over 200 hours verifying their accuracy and relevance. Beyond assessing LLMs' knowledge, the dataset's main goal is to facilitate a fair comparison between humans and different LLMs in cybersecurity. To achieve this, we carefully selected 80 questions covering a wide range of topics within cybersecurity and involved 30 participants of diverse expertise levels, facilitating a comprehensive comparison between human and machine intelligence in this area. The findings revealed that LLMs outperformed humans in almost every aspect of cybersecurity.","sentences":["Large Language Models (LLMs) excel across various domains, from computer vision to medical diagnostics.","However, understanding the diverse landscape of cybersecurity, encompassing cryptography, reverse engineering, and managerial facets like risk assessment, presents a challenge, even for human experts.","In this paper, we introduce CyberMetric, a benchmark dataset comprising 10,000 questions sourced from standards, certifications, research papers, books, and other publications in the cybersecurity domain.","The questions are created through a collaborative process, i.e., merging expert knowledge with LLMs, including GPT-3.5 and Falcon-180B.","Human experts spent over 200 hours verifying their accuracy and relevance.","Beyond assessing LLMs' knowledge, the dataset's main goal is to facilitate a fair comparison between humans and different LLMs in cybersecurity.","To achieve this, we carefully selected 80 questions covering a wide range of topics within cybersecurity and involved 30 participants of diverse expertise levels, facilitating a comprehensive comparison between human and machine intelligence in this area.","The findings revealed that LLMs outperformed humans in almost every aspect of cybersecurity."],"url":"http://arxiv.org/abs/2402.07688v1","category":"cs.AI"}
{"created":"2024-02-12 14:43:40","title":"Two Choices are Enough for P-LCPs, USOs, and Colorful Tangents","abstract":"We provide polynomial-time reductions between three search problems from three distinct areas: the P-matrix linear complementarity problem (P-LCP), finding the sink of a unique sink orientation (USO), and a variant of the $\\alpha$-Ham Sandwich problem. For all three settings, we show that \"two choices are enough\", meaning that the general non-binary version of the problem can be reduced in polynomial time to the binary version. This specifically means that generalized P-LCPs are equivalent to P-LCPs, and grid USOs are equivalent to cube USOs. These results are obtained by showing that both the P-LCP and our $\\alpha$-Ham Sandwich variant are equivalent to a new problem we introduce, P-Lin-Bellman. This problem can be seen as a new tool for formulating problems as P-LCPs.","sentences":["We provide polynomial-time reductions between three search problems from three distinct areas: the P-matrix linear complementarity problem (P-LCP), finding the sink of a unique sink orientation (USO), and a variant of the $\\alpha$-Ham Sandwich problem.","For all three settings, we show that \"two choices are enough\", meaning that the general non-binary version of the problem can be reduced in polynomial time to the binary version.","This specifically means that generalized P-LCPs are equivalent to P-LCPs, and grid USOs are equivalent to cube USOs.","These results are obtained by showing that both the P-LCP and our $\\alpha$-Ham Sandwich variant are equivalent to a new problem we introduce, P-Lin-Bellman.","This problem can be seen as a new tool for formulating problems as P-LCPs."],"url":"http://arxiv.org/abs/2402.07683v1","category":"cs.CC"}
{"created":"2024-02-12 14:40:54","title":"Large Language Models \"Ad Referendum\": How Good Are They at Machine Translation in the Legal Domain?","abstract":"This study evaluates the machine translation (MT) quality of two state-of-the-art large language models (LLMs) against a tradition-al neural machine translation (NMT) system across four language pairs in the legal domain. It combines automatic evaluation met-rics (AEMs) and human evaluation (HE) by professional transla-tors to assess translation ranking, fluency and adequacy. The re-sults indicate that while Google Translate generally outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4, comparably or slightly better in terms of producing contextually adequate and fluent translations. This discrepancy suggests LLMs' potential in handling specialized legal terminology and context, highlighting the importance of human evaluation methods in assessing MT quality. The study underscores the evolving capabil-ities of LLMs in specialized domains and calls for reevaluation of traditional AEMs to better capture the nuances of LLM-generated translations.","sentences":["This study evaluates the machine translation (MT) quality of two state-of-the-art large language models (LLMs) against a tradition-al neural machine translation (NMT) system across four language pairs in the legal domain.","It combines automatic evaluation met-rics (AEMs) and human evaluation (HE) by professional transla-tors to assess translation ranking, fluency and adequacy.","The re-sults indicate that while Google Translate generally outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4, comparably or slightly better in terms of producing contextually adequate and fluent translations.","This discrepancy suggests LLMs' potential in handling specialized legal terminology and context, highlighting the importance of human evaluation methods in assessing MT quality.","The study underscores the evolving capabil-ities of LLMs in specialized domains and calls for reevaluation of traditional AEMs to better capture the nuances of LLM-generated translations."],"url":"http://arxiv.org/abs/2402.07681v1","category":"cs.CL"}
{"created":"2024-02-12 14:40:43","title":"AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual Vision Transformer","abstract":"Combining LiDAR and camera data has shown potential in enhancing short-distance object detection in autonomous driving systems. Yet, the fusion encounters difficulties with extended distance detection due to the contrast between LiDAR's sparse data and the dense resolution of cameras. Besides, discrepancies in the two data representations further complicate fusion methods. We introduce AYDIV, a novel framework integrating a tri-phase alignment process specifically designed to enhance long-distance detection even amidst data discrepancies. AYDIV consists of the Global Contextual Fusion Alignment Transformer (GCFAT), which improves the extraction of camera features and provides a deeper understanding of large-scale patterns; the Sparse Fused Feature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial data fusion. AYDIV's performance on the Waymo Open Dataset (WOD) with an improvement of 1.24% in mAPH value(L2 difficulty) and the Argoverse2 Dataset with a performance improvement of 7.40% in AP value demonstrates its efficacy in comparison to other existing fusion-based methods. Our code is publicly available at https://github.com/sanjay-810/AYDIV2","sentences":["Combining LiDAR and camera data has shown potential in enhancing short-distance object detection in autonomous driving systems.","Yet, the fusion encounters difficulties with extended distance detection due to the contrast between LiDAR's sparse data and the dense resolution of cameras.","Besides, discrepancies in the two data representations further complicate fusion methods.","We introduce AYDIV, a novel framework integrating a tri-phase alignment process specifically designed to enhance long-distance detection even amidst data discrepancies.","AYDIV consists of the Global Contextual Fusion Alignment Transformer (GCFAT), which improves the extraction of camera features and provides a deeper understanding of large-scale patterns; the Sparse Fused Feature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial data fusion.","AYDIV's performance on the Waymo Open Dataset (WOD) with an improvement of 1.24% in mAPH value(L2 difficulty) and the Argoverse2 Dataset with a performance improvement of 7.40% in AP value demonstrates its efficacy in comparison to other existing fusion-based methods.","Our code is publicly available at https://github.com/sanjay-810/AYDIV2"],"url":"http://arxiv.org/abs/2402.07680v1","category":"cs.CV"}
{"created":"2024-02-12 14:25:13","title":"On Iverson's law of similarity","abstract":"Iverson (2006) proposed the law of similarity \\[ \\xi_{s}(\\lambda x)= \\gamma(\\lambda, s)\\xi_{\\eta(\\lambda, s)}(x) \\] for the sensitivity functions $\\xi_{s}\\, (s\\in S)$. Compared to the former models, the generality of this one lies in that here $\\gamma$ and $\\eta$ can also depend on the variables $\\lambda$ and $s$. In the literature, this model (or its special cases) is usually considered together with a given psychophysical representation (e.g. Fechnerian, subtractive, or affine). Our goal, however, is to study at first Iverson's law of similarity on its own. At first we show that if certain mild assumptions are fulfilled, then $\\xi$ can be written in a rather simple form containing only one-variable functions. The obtained form proves to be very useful when we assume some kind of representation.   Motivated by Hsu and Iverson (2016), in the second part of the third section we study the above model assuming that the mapping $\\eta$ is multiplicatively translational. First, we show how these mappings can be characterized. Later on we turn to the examination of the so-called power law. According to our results, the corresponding function $\\xi$ then does not have a Fechnerian representation, but it do have a subtractive representation. As an application of the results of the subsection, we close the paper with the study of the shift invariance property.","sentences":["Iverson (2006) proposed the law of similarity \\[ \\xi_{s}(\\lambda x)= \\gamma(\\lambda, s)\\xi_{\\eta(\\lambda, s)}(x) \\] for the sensitivity functions $\\xi_{s}\\, (s\\in S)$. Compared to the former models, the generality of this one lies in that here $\\gamma$ and $\\eta$ can also depend on the variables $\\lambda$ and $s$. In the literature, this model (or its special cases) is usually considered together with a given psychophysical representation (e.g. Fechnerian, subtractive, or affine).","Our goal, however, is to study at first Iverson's law of similarity on its own.","At first we show that if certain mild assumptions are fulfilled, then $\\xi$ can be written in a rather simple form containing only one-variable functions.","The obtained form proves to be very useful when we assume some kind of representation.   ","Motivated by Hsu and Iverson (2016), in the second part of the third section we study the above model assuming that the mapping $\\eta$ is multiplicatively translational.","First, we show how these mappings can be characterized.","Later on we turn to the examination of the so-called power law.","According to our results, the corresponding function $\\xi$ then does not have a Fechnerian representation, but it do have a subtractive representation.","As an application of the results of the subsection, we close the paper with the study of the shift invariance property."],"url":"http://arxiv.org/abs/2402.07670v1","category":"math.CA"}
{"created":"2024-02-12 14:20:15","title":"Approximating the Maximum Independent Set of Convex Polygons with a Bounded Number of Directions","abstract":"In the maximum independent set of convex polygons problem, we are given a set of $n$ convex polygons in the plane with the objective of selecting a maximum cardinality subset of non-overlapping polygons. Here we study a special case of the problem where the edges of the polygons can take at most $d$ fixed directions. We present an $8d/3$-approximation algorithm for this problem running in time $O((nd)^{O(d4^d)})$. The previous-best polynomial-time approximation (for constant $d$) was a classical $n^\\varepsilon$ approximation by Fox and Pach [SODA'11] that has recently been improved to a $OPT^{\\varepsilon}$-approximation algorithm by Cslovjecsek, Pilipczuk and W\\k{e}grzycki [SODA '24], which also extends to an arbitrary set of convex polygons. Our result builds on, and generalizes the recent constant factor approximation algorithms for the maximum independent set of axis-parallel rectangles problem (which is a special case of our problem with $d=2$) by Mitchell [FOCS'21] and G\\'{a}lvez, Khan, Mari, M\\\"{o}mke, Reddy, and Wiese [SODA'22].","sentences":["In the maximum independent set of convex polygons problem, we are given a set of $n$ convex polygons in the plane with the objective of selecting a maximum cardinality subset of non-overlapping polygons.","Here we study a special case of the problem where the edges of the polygons can take at most $d$ fixed directions.","We present an $8d/3$-approximation algorithm for this problem running in time $O((nd)^{O(d4^d)})$. The previous-best polynomial-time approximation (for constant $d$) was a classical $n^\\varepsilon$ approximation by Fox and","Pach","[SODA'11] that has recently been improved to a $OPT^{\\varepsilon}$-approximation algorithm by Cslovjecsek, Pilipczuk and W\\k{e}grzycki","[SODA '24], which also extends to an arbitrary set of convex polygons.","Our result builds on, and generalizes the recent constant factor approximation algorithms for the maximum independent set of axis-parallel rectangles problem (which is a special case of our problem with $d=2$) by Mitchell [FOCS'21] and G\\'{a}lvez, Khan, Mari, M\\\"{o}mke, Reddy, and","Wiese [SODA'22]."],"url":"http://arxiv.org/abs/2402.07666v1","category":"cs.CG"}
{"created":"2024-02-12 14:07:16","title":"Combining Evolutionary Strategies and Novelty Detection to go Beyond the Alignment Limit of the $Z_3$ 3HDM","abstract":"We present a novel Artificial Intelligence approach for Beyond the Standard Model parameter space scans by augmenting an Evolutionary Strategy with Novelty Detection. Our approach leverages the power of Evolutionary Strategies, previously shown to quickly converge to the valid regions of the parameter space, with a \\emph{novelty reward} to continue exploration once converged. Taking the $Z_3$ 3HDM as our Physics case, we show how our methodology allows us to quickly explore highly constrained multidimensional parameter spaces, providing up to eight orders of magnitude higher sampling efficiency when compared with pure random sampling and up to four orders of magnitude when compared to random sampling around the alignment limit. In turn, this enables us to explore regions of the parameter space that have been hitherto overlooked, leading to the possibility of novel phenomenological realisations of the $Z_3$ 3HDM that had not been considered before.","sentences":["We present a novel Artificial Intelligence approach for Beyond the Standard Model parameter space scans by augmenting an Evolutionary Strategy with Novelty Detection.","Our approach leverages the power of Evolutionary Strategies, previously shown to quickly converge to the valid regions of the parameter space, with a \\emph{novelty reward} to continue exploration once converged.","Taking the $Z_3$ 3HDM as our Physics case, we show how our methodology allows us to quickly explore highly constrained multidimensional parameter spaces, providing up to eight orders of magnitude higher sampling efficiency when compared with pure random sampling and up to four orders of magnitude when compared to random sampling around the alignment limit.","In turn, this enables us to explore regions of the parameter space that have been hitherto overlooked, leading to the possibility of novel phenomenological realisations of the $Z_3$ 3HDM that had not been considered before."],"url":"http://arxiv.org/abs/2402.07661v1","category":"hep-ph"}
{"created":"2024-02-12 14:03:45","title":"Multi-Behavior Collaborative Filtering with Partial Order Graph Convolutional Networks","abstract":"Representing the information of multiple behaviors in the single graph collaborative filtering (CF) vector has been a long-standing challenge. This is because different behaviors naturally form separate behavior graphs and learn separate CF embeddings. Existing models merge the separate embeddings by appointing the CF embeddings for some behaviors as the primary embedding and utilizing other auxiliaries to enhance the primary embedding. However, this approach often results in the joint embedding performing well on the main tasks but poorly on the auxiliary ones. To address the problem arising from the separate behavior graphs, we propose the concept of Partial Order Graphs (POG). POG defines the partial order relation of multiple behaviors and models behavior combinations as weighted edges to merge separate behavior graphs into a joint POG. Theoretical proof verifies that POG can be generalized to any given set of multiple behaviors. Based on POG, we propose the tailored Partial Order Graph Convolutional Networks (POGCN) that convolute neighbors' information while considering the behavior relations between users and items. POGCN also introduces a partial-order BPR sampling strategy for efficient and effective multiple-behavior CF training. POGCN has been successfully deployed on the homepage of Alibaba for two months, providing recommendation services for over one billion users. Extensive offline experiments conducted on three public benchmark datasets demonstrate that POGCN outperforms state-of-the-art multi-behavior baselines across all types of behaviors. Furthermore, online A/B tests confirm the superiority of POGCN in billion-scale recommender systems.","sentences":["Representing the information of multiple behaviors in the single graph collaborative filtering (CF) vector has been a long-standing challenge.","This is because different behaviors naturally form separate behavior graphs and learn separate CF embeddings.","Existing models merge the separate embeddings by appointing the CF embeddings for some behaviors as the primary embedding and utilizing other auxiliaries to enhance the primary embedding.","However, this approach often results in the joint embedding performing well on the main tasks but poorly on the auxiliary ones.","To address the problem arising from the separate behavior graphs, we propose the concept of Partial Order Graphs (POG).","POG defines the partial order relation of multiple behaviors and models behavior combinations as weighted edges to merge separate behavior graphs into a joint POG.","Theoretical proof verifies that POG can be generalized to any given set of multiple behaviors.","Based on POG, we propose the tailored Partial Order Graph Convolutional Networks (POGCN) that convolute neighbors' information while considering the behavior relations between users and items.","POGCN also introduces a partial-order BPR sampling strategy for efficient and effective multiple-behavior CF training.","POGCN has been successfully deployed on the homepage of Alibaba for two months, providing recommendation services for over one billion users.","Extensive offline experiments conducted on three public benchmark datasets demonstrate that POGCN outperforms state-of-the-art multi-behavior baselines across all types of behaviors.","Furthermore, online A/B tests confirm the superiority of POGCN in billion-scale recommender systems."],"url":"http://arxiv.org/abs/2402.07659v1","category":"cs.IR"}
{"created":"2024-02-12 14:01:12","title":"The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models","abstract":"In the rapidly evolving landscape of medical documentation, transcribing clinical dialogues accurately is increasingly paramount. This study explores the potential of Large Language Models (LLMs) to enhance the accuracy of Automatic Speech Recognition (ASR) systems in medical transcription. Utilizing the PriMock57 dataset, which encompasses a diverse range of primary care consultations, we apply advanced LLMs to refine ASR-generated transcripts. Our research is multifaceted, focusing on improvements in general Word Error Rate (WER), Medical Concept WER (MC-WER) for the accurate transcription of essential medical terms, and speaker diarization accuracy. Additionally, we assess the role of LLM post-processing in improving semantic textual similarity, thereby preserving the contextual integrity of clinical dialogues. Through a series of experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT) prompting techniques in enhancing diarization and correction accuracy. Our findings demonstrate that LLMs, particularly through CoT prompting, not only improve the diarization accuracy of existing ASR systems but also achieve state-of-the-art performance in this domain. This improvement extends to more accurately capturing medical concepts and enhancing the overall semantic coherence of the transcribed dialogues. These findings illustrate the dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.","sentences":["In the rapidly evolving landscape of medical documentation, transcribing clinical dialogues accurately is increasingly paramount.","This study explores the potential of Large Language Models (LLMs) to enhance the accuracy of Automatic Speech Recognition (ASR) systems in medical transcription.","Utilizing the PriMock57 dataset, which encompasses a diverse range of primary care consultations, we apply advanced LLMs to refine ASR-generated transcripts.","Our research is multifaceted, focusing on improvements in general Word Error Rate (WER), Medical Concept WER (MC-WER) for the accurate transcription of essential medical terms, and speaker diarization accuracy.","Additionally, we assess the role of LLM post-processing in improving semantic textual similarity, thereby preserving the contextual integrity of clinical dialogues.","Through a series of experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT) prompting techniques in enhancing diarization and correction accuracy.","Our findings demonstrate that LLMs, particularly through CoT prompting, not only improve the diarization accuracy of existing ASR systems but also achieve state-of-the-art performance in this domain.","This improvement extends to more accurately capturing medical concepts and enhancing the overall semantic coherence of the transcribed dialogues.","These findings illustrate the dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings."],"url":"http://arxiv.org/abs/2402.07658v1","category":"cs.CL"}
{"created":"2024-02-12 13:59:19","title":"Evaluation of plasma on deflection angle and shadow by a black hole solution immersed in perfect fluid in Rastall theory","abstract":"The present study examines the gravitational deflection of particles in curved space-times immersed in perfect fluid in the context of Rastall theory, applying the Gibbons-Werner useful technique. In its application to integral areas inside a four-dimensional space-time, the Gauss-Bonnet theorem studies the computation expression of the deflection angle. The Gibbons-Werner technique has two limitations in Rastall theory for space-times immersed in perfect fluid: the integral region is generally infinite and the integral complicates calculation, particularly for complex space-times and extremely accurate solutions. An infinite region approach to Gibbons-Werner is proposed to avoid singularity. For demonstrating the Gibbons-Werner method, we use a complete Rastall theory framework. It studies black hole solutions like the dust field, radiation field, quintessence field, cosmological constant field and phantom field using the Gibbons-Werner approach. Additionally, we check the deflection angle from these space-times under the influence of plasma. Furthermore, we compute analytically the influence of a plasma on a black hole shadow by using a ray-tracing approach. In the Hamiltonian equation, our model describes the plasma, so the light ray motion equations are independent of the plasma's velocity. It is assumed that plasma is a dispersive medium, pressure-less and non-magnetized. We investigate the perfect fluid in Rastall theory in further depth, where the plasma particle density corresponds to particle accumulation. The supermassive black hole shadows are explored in the case when plasma falls radially from infinity onto a black hole.","sentences":["The present study examines the gravitational deflection of particles in curved space-times immersed in perfect fluid in the context of Rastall theory, applying the Gibbons-Werner useful technique.","In its application to integral areas inside a four-dimensional space-time, the Gauss-Bonnet theorem studies the computation expression of the deflection angle.","The Gibbons-Werner technique has two limitations in Rastall theory for space-times immersed in perfect fluid: the integral region is generally infinite and the integral complicates calculation, particularly for complex space-times and extremely accurate solutions.","An infinite region approach to Gibbons-Werner is proposed to avoid singularity.","For demonstrating the Gibbons-Werner method, we use a complete Rastall theory framework.","It studies black hole solutions like the dust field, radiation field, quintessence field, cosmological constant field and phantom field using the Gibbons-Werner approach.","Additionally, we check the deflection angle from these space-times under the influence of plasma.","Furthermore, we compute analytically the influence of a plasma on a black hole shadow by using a ray-tracing approach.","In the Hamiltonian equation, our model describes the plasma, so the light ray motion equations are independent of the plasma's velocity.","It is assumed that plasma is a dispersive medium, pressure-less and non-magnetized.","We investigate the perfect fluid in Rastall theory in further depth, where the plasma particle density corresponds to particle accumulation.","The supermassive black hole shadows are explored in the case when plasma falls radially from infinity onto a black hole."],"url":"http://arxiv.org/abs/2402.07657v1","category":"gr-qc"}
{"created":"2024-02-12 13:57:14","title":"Inhomogeneous spin momentum induced orbital motion of birefringent particles in tight focusing of vector beams in optical tweezers","abstract":"Spin orbit interaction (SOI) due to tight focusing of light in optical tweezers has led to exciting and exotic avenues towards inducing rotation in microscopic particles. However, instances where the back action of the particles influences and modifies SOI effects so as to induce rotational motion are rarely known. Here, we tightly focus a vector beam having radial/azimuthal polarization carrying no intrinsic angular momentum, into a refractive index stratified medium, and observe orbital rotation of birefringent particles around the beam propagation axis. In order to validate our experimental findings, we perform numerical simulations of the underlying equations. Our simulations reveal that the interaction of light with a birefringent particle gives rise to inhomogeneous spin currents near the focus, resulting in a finite spin momentum. This spin momentum combines with the canonical momentum to finally generate an origin-dependent orbital angular momentum which is manifested in the rotation of the birefringent particles around the beam axis. Our study describes a unique modulation of the SOI of light due to interaction with anisotropic particles that can be used to identify new avenues for exotic and complex particle manipulation in optical tweezers.","sentences":["Spin orbit interaction (SOI) due to tight focusing of light in optical tweezers has led to exciting and exotic avenues towards inducing rotation in microscopic particles.","However, instances where the back action of the particles influences and modifies SOI effects so as to induce rotational motion are rarely known.","Here, we tightly focus a vector beam having radial/azimuthal polarization carrying no intrinsic angular momentum, into a refractive index stratified medium, and observe orbital rotation of birefringent particles around the beam propagation axis.","In order to validate our experimental findings, we perform numerical simulations of the underlying equations.","Our simulations reveal that the interaction of light with a birefringent particle gives rise to inhomogeneous spin currents near the focus, resulting in a finite spin momentum.","This spin momentum combines with the canonical momentum to finally generate an origin-dependent orbital angular momentum which is manifested in the rotation of the birefringent particles around the beam axis.","Our study describes a unique modulation of the SOI of light due to interaction with anisotropic particles that can be used to identify new avenues for exotic and complex particle manipulation in optical tweezers."],"url":"http://arxiv.org/abs/2402.07655v1","category":"physics.optics"}
{"created":"2024-02-12 13:46:12","title":"Revisiting effective Einstein equations on a 3-brane in the presence of torsion","abstract":"The effective Einstein equations on a 3-brane embedded in a 5-dimensional Riemann-Cartan bulk spacetime are revisited. Addressing the shortcomings in the hitherto published junction conditions on the brane in the presence of torsion, we have elaborated on our general form of the junction conditions recently published. Applying our general junction conditions, we have formulated the effective Einstein equations on a Z2 symmetric brane in a standard form highlighting the difference to those published so far.","sentences":["The effective Einstein equations on a 3-brane embedded in a 5-dimensional Riemann-Cartan bulk spacetime are revisited.","Addressing the shortcomings in the hitherto published junction conditions on the brane in the presence of torsion, we have elaborated on our general form of the junction conditions recently published.","Applying our general junction conditions, we have formulated the effective Einstein equations on a Z2 symmetric brane in a standard form highlighting the difference to those published so far."],"url":"http://arxiv.org/abs/2402.07649v1","category":"gr-qc"}
{"created":"2024-02-12 13:42:53","title":"DeformNet: Latent Space Modeling and Dynamics Prediction for Deformable Object Manipulation","abstract":"Manipulating deformable objects is a ubiquitous task in household environments, demanding adequate representation and accurate dynamics prediction due to the objects' infinite degrees of freedom. This work proposes DeformNet, which utilizes latent space modeling with a learned 3D representation model to tackle these challenges effectively. The proposed representation model combines a PointNet encoder and a conditional neural radiance field (NeRF), facilitating a thorough acquisition of object deformations and variations in lighting conditions. To model the complex dynamics, we employ a recurrent state-space model (RSSM) that accurately predicts the transformation of the latent representation over time. Extensive simulation experiments with diverse objectives demonstrate the generalization capabilities of DeformNet for various deformable object manipulation tasks, even in the presence of previously unseen goals. Finally, we deploy DeformNet on an actual UR5 robotic arm to demonstrate its capability in real-world scenarios.","sentences":["Manipulating deformable objects is a ubiquitous task in household environments, demanding adequate representation and accurate dynamics prediction due to the objects' infinite degrees of freedom.","This work proposes DeformNet, which utilizes latent space modeling with a learned 3D representation model to tackle these challenges effectively.","The proposed representation model combines a PointNet encoder and a conditional neural radiance field (NeRF), facilitating a thorough acquisition of object deformations and variations in lighting conditions.","To model the complex dynamics, we employ a recurrent state-space model (RSSM) that accurately predicts the transformation of the latent representation over time.","Extensive simulation experiments with diverse objectives demonstrate the generalization capabilities of DeformNet for various deformable object manipulation tasks, even in the presence of previously unseen goals.","Finally, we deploy DeformNet on an actual UR5 robotic arm to demonstrate its capability in real-world scenarios."],"url":"http://arxiv.org/abs/2402.07648v1","category":"cs.RO"}
{"created":"2024-02-12 13:42:11","title":"GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants","abstract":"We tackle the challenge of building real-world multimodal assistants for complex real-world tasks. We describe the practicalities and challenges of developing and deploying GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency. OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner. For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns. For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84% effectiveness with 100x lower latency. Overall, we provide insights and discuss tradeoffs for deploying both traditional models and LLMs to users in complex real-world multimodal environments in the Alexa TaskBot challenge. These experiences will continue to evolve as LLMs become more capable and efficient -- fundamentally reshaping OAT and future assistant architectures.","sentences":["We tackle the challenge of building real-world multimodal assistants for complex real-world tasks.","We describe the practicalities and challenges of developing and deploying","GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge.","Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency.","OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner.","For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns.","For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84% effectiveness with 100x lower latency.","Overall, we provide insights and discuss tradeoffs for deploying both traditional models and LLMs to users in complex real-world multimodal environments in the Alexa TaskBot challenge.","These experiences will continue to evolve as LLMs become more capable and efficient -- fundamentally reshaping OAT and future assistant architectures."],"url":"http://arxiv.org/abs/2402.07647v1","category":"cs.IR"}
{"created":"2024-02-12 13:36:37","title":"Tunable on-chip electro-optic frequency-comb generation at 8 \u03bcm wavelength","abstract":"Dual-comb spectroscopy is a powerful technique to measure optical spectra in a wide spectral range with high-frequency resolution. The development of compact systems operating in the long-wave infrared wavelength range is of high interest for spectroscopic and sensing applications. Amongst the different techniques to obtain optical frequency-combs, electro-optic frequency-comb generation presents major advantages thanks to the tunable repetition rate only limited by the bandwidth of the used electro-optical modulator. However, the development of integrated and efficient electro-optical modulators operating in a wide long-wave infrared spectral band is still at its infancy, and electro-optical frequency-comb has not been demonstrated so far beyond the telecom band. In this work, a Schottky-based modulator embedded in a Ge-rich graded SiGe waveguide is used for electro-optic frequency-comb generation. Considering the limited efficiency of the modulator, harmonically-rich RF signals are used to enhance the generation of comb lines around the optical carrier. Interestingly, this allows us to demonstrate the generation of electro-optical combs spanning over 2.4 GHz around 8 {\\mu}m wavelength. This paves the way towards fully integrated and tunable mid-infrared electro-optic frequency-comb generation systems.","sentences":["Dual-comb spectroscopy is a powerful technique to measure optical spectra in a wide spectral range with high-frequency resolution.","The development of compact systems operating in the long-wave infrared wavelength range is of high interest for spectroscopic and sensing applications.","Amongst the different techniques to obtain optical frequency-combs, electro-optic frequency-comb generation presents major advantages thanks to the tunable repetition rate only limited by the bandwidth of the used electro-optical modulator.","However, the development of integrated and efficient electro-optical modulators operating in a wide long-wave infrared spectral band is still at its infancy, and electro-optical frequency-comb has not been demonstrated so far beyond the telecom band.","In this work, a Schottky-based modulator embedded in a Ge-rich graded SiGe waveguide is used for electro-optic frequency-comb generation.","Considering the limited efficiency of the modulator, harmonically-rich RF signals are used to enhance the generation of comb lines around the optical carrier.","Interestingly, this allows us to demonstrate the generation of electro-optical combs spanning over 2.4 GHz around 8 {\\mu}m wavelength.","This paves the way towards fully integrated and tunable mid-infrared electro-optic frequency-comb generation systems."],"url":"http://arxiv.org/abs/2402.07646v1","category":"physics.optics"}
{"created":"2024-02-12 13:34:33","title":"Detecting the Clinical Features of Difficult-to-Treat Depression using Synthetic Data from Large Language Models","abstract":"Difficult-to-treat depression (DTD) has been proposed as a broader and more clinically comprehensive perspective on a person's depressive disorder where despite treatment, they continue to experience significant burden. We sought to develop a Large Language Model (LLM)-based tool capable of interrogating routinely-collected, narrative (free-text) electronic health record (EHR) data to locate published prognostic factors that capture the clinical syndrome of DTD. In this work, we use LLM-generated synthetic data (GPT3.5) and a Non-Maximum Suppression (NMS) algorithm to train a BERT-based span extraction model. The resulting model is then able to extract and label spans related to a variety of relevant positive and negative factors in real clinical data (i.e. spans of text that increase or decrease the likelihood of a patient matching the DTD syndrome). We show it is possible to obtain good overall performance (0.70 F1 across polarity) on real clinical data on a set of as many as 20 different factors, and high performance (0.85 F1 with 0.95 precision) on a subset of important DTD factors such as history of abuse, family history of affective disorder, illness severity and suicidality by training the model exclusively on synthetic data. Our results show promise for future healthcare applications especially in applications where traditionally, highly confidential medical data and human-expert annotation would normally be required.","sentences":["Difficult-to-treat depression (DTD) has been proposed as a broader and more clinically comprehensive perspective on a person's depressive disorder where despite treatment, they continue to experience significant burden.","We sought to develop a Large Language Model (LLM)-based tool capable of interrogating routinely-collected, narrative (free-text) electronic health record (EHR) data to locate published prognostic factors that capture the clinical syndrome of DTD.","In this work, we use LLM-generated synthetic data (GPT3.5) and a Non-Maximum Suppression (NMS) algorithm to train a BERT-based span extraction model.","The resulting model is then able to extract and label spans related to a variety of relevant positive and negative factors in real clinical data (i.e. spans of text that increase or decrease the likelihood of a patient matching the DTD syndrome).","We show it is possible to obtain good overall performance (0.70 F1 across polarity) on real clinical data on a set of as many as 20 different factors, and high performance (0.85 F1 with 0.95 precision) on a subset of important DTD factors such as history of abuse, family history of affective disorder, illness severity and suicidality by training the model exclusively on synthetic data.","Our results show promise for future healthcare applications especially in applications where traditionally, highly confidential medical data and human-expert annotation would normally be required."],"url":"http://arxiv.org/abs/2402.07645v1","category":"cs.CL"}
{"created":"2024-02-12 13:29:41","title":"Particle spectra of general Ricci-type Palatini or metric-affine theories","abstract":"In the context of weak-field metric-affine (i.e. Palatini) gravity near Minkowski spacetime, we compute the particle spectra in the simultaneous presence of all independent contractions quadratic in Ricci-type tensors. Apart from the full metric-affine geometry, we study kinematic limits with vanishing torsion (i.e. a symmetric connection) and vanishing non-metricity (i.e. a metric connection, which is physically indistinguishable from Poincar\\'e gauge theory at the level of the particle spectrum). We present a detailed report on how spin-parity projection operators can be used to derive systematically and unambiguously the character of the propagating states. The unitarity constraints derived from the requirements of tachyon- and ghost-freedom are obtained. We show that, even in the presence of all Ricci-type operators, only a narrow selection of viable theories emerges by a tuning.","sentences":["In the context of weak-field metric-affine (i.e. Palatini) gravity near Minkowski spacetime, we compute the particle spectra in the simultaneous presence of all independent contractions quadratic in Ricci-type tensors.","Apart from the full metric-affine geometry, we study kinematic limits with vanishing torsion (i.e. a symmetric connection) and vanishing non-metricity (i.e. a metric connection, which is physically indistinguishable from Poincar\\'e gauge theory at the level of the particle spectrum).","We present a detailed report on how spin-parity projection operators can be used to derive systematically and unambiguously the character of the propagating states.","The unitarity constraints derived from the requirements of tachyon- and ghost-freedom are obtained.","We show that, even in the presence of all Ricci-type operators, only a narrow selection of viable theories emerges by a tuning."],"url":"http://arxiv.org/abs/2402.07641v1","category":"hep-th"}
{"created":"2024-02-12 13:27:22","title":"Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data","abstract":"The ability to generate sentiment-controlled feedback in response to multimodal inputs, comprising both text and images, addresses a critical gap in human-computer interaction by enabling systems to provide empathetic, accurate, and engaging responses. This capability has profound applications in healthcare, marketing, and education. To this end, we construct a large-scale Controllable Multimodal Feedback Synthesis (CMFeed) dataset and propose a controllable feedback synthesis system. The proposed system includes an encoder, decoder, and controllability block for textual and visual inputs. It extracts textual and visual features using a transformer and Faster R-CNN networks and combines them to generate feedback. The CMFeed dataset encompasses images, text, reactions to the post, human comments with relevance scores, and reactions to the comments. The reactions to the post and comments are utilized to train the proposed model to produce feedback with a particular (positive or negative) sentiment. A sentiment classification accuracy of 77.23% has been achieved, 18.82% higher than the accuracy without using the controllability. Moreover, the system incorporates a similarity module for assessing feedback relevance through rank-based metrics. It implements an interpretability technique to analyze the contribution of textual and visual features during the generation of uncontrolled and controlled feedback.","sentences":["The ability to generate sentiment-controlled feedback in response to multimodal inputs, comprising both text and images, addresses a critical gap in human-computer interaction by enabling systems to provide empathetic, accurate, and engaging responses.","This capability has profound applications in healthcare, marketing, and education.","To this end, we construct a large-scale Controllable Multimodal Feedback Synthesis (CMFeed) dataset and propose a controllable feedback synthesis system.","The proposed system includes an encoder, decoder, and controllability block for textual and visual inputs.","It extracts textual and visual features using a transformer and Faster R-CNN networks and combines them to generate feedback.","The CMFeed dataset encompasses images, text, reactions to the post, human comments with relevance scores, and reactions to the comments.","The reactions to the post and comments are utilized to train the proposed model to produce feedback with a particular (positive or negative) sentiment.","A sentiment classification accuracy of 77.23% has been achieved, 18.82% higher than the accuracy without using the controllability.","Moreover, the system incorporates a similarity module for assessing feedback relevance through rank-based metrics.","It implements an interpretability technique to analyze the contribution of textual and visual features during the generation of uncontrolled and controlled feedback."],"url":"http://arxiv.org/abs/2402.07640v1","category":"cs.MM"}
{"created":"2024-02-12 13:24:32","title":"Tighter Bounds on the Information Bottleneck with Application to Deep Learning","abstract":"Deep Neural Nets (DNNs) learn latent representations induced by their downstream task, objective function, and other parameters. The quality of the learned representations impacts the DNN's generalization ability and the coherence of the emerging latent space. The Information Bottleneck (IB) provides a hypothetically optimal framework for data modeling, yet it is often intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired variational methods to approximate bounds on mutual information, resulting in improved robustness to adversarial attacks. This work introduces a new and tighter variational bound for the IB, improving performance of previous IB-inspired DNNs. These advancements strengthen the case for the IB and its variational approximations as a data modeling framework, and provide a simple method to significantly enhance the adversarial robustness of classifier DNNs.","sentences":["Deep Neural Nets (DNNs) learn latent representations induced by their downstream task, objective function, and other parameters.","The quality of the learned representations impacts the DNN's generalization ability and the coherence of the emerging latent space.","The Information Bottleneck (IB) provides a hypothetically optimal framework for data modeling, yet it is often intractable.","Recent efforts combined DNNs with the IB by applying VAE-inspired variational methods to approximate bounds on mutual information, resulting in improved robustness to adversarial attacks.","This work introduces a new and tighter variational bound for the IB, improving performance of previous IB-inspired DNNs.","These advancements strengthen the case for the IB and its variational approximations as a data modeling framework, and provide a simple method to significantly enhance the adversarial robustness of classifier DNNs."],"url":"http://arxiv.org/abs/2402.07639v1","category":"cs.LG"}
{"created":"2024-02-12 13:20:13","title":"On solution manifolds of some differential equations with more general state-dependent delay","abstract":"Differential equations with state-dependent delays define a semiflow of continuously differentiable solution operators in general only on the associated {\\it solution manifold} in the Banach space $C^1_n=C^1([-h,0],\\mathbb{R}^n)$. For a prototypic example we develop a new proof that its solution manifold is diffeomorphic to an open subset of the subspace given by $\\phi'(0)=0$, without recourse to a restrictive hypothesis about the form of delays which is instrumental in earlier work on the nature of solution manifolds. The new proof uses the framework of algebraic-delay systems.","sentences":["Differential equations with state-dependent delays define a semiflow of continuously differentiable solution operators in general only on the associated {\\it solution manifold} in the Banach space $C^1_n=C^1([-h,0],\\mathbb{R}^n)$. For a prototypic example we develop a new proof that its solution manifold is diffeomorphic to an open subset of the subspace given by $\\phi'(0)=0$, without recourse to a restrictive hypothesis about the form of delays which is instrumental in earlier work on the nature of solution manifolds.","The new proof uses the framework of algebraic-delay systems."],"url":"http://arxiv.org/abs/2402.07636v1","category":"math.DS"}
{"created":"2024-02-12 13:16:47","title":"Complete Instances Mining for Weakly Supervised Instance Segmentation","abstract":"Weakly supervised instance segmentation (WSIS) using only image-level labels is a challenging task due to the difficulty of aligning coarse annotations with the finer task. However, with the advancement of deep neural networks (DNNs), WSIS has garnered significant attention. Following a proposal-based paradigm, we encounter a redundant segmentation problem resulting from a single instance being represented by multiple proposals. For example, we feed a picture of a dog and proposals into the network and expect to output only one proposal containing a dog, but the network outputs multiple proposals. To address this problem, we propose a novel approach for WSIS that focuses on the online refinement of complete instances through the use of MaskIoU heads to predict the integrity scores of proposals and a Complete Instances Mining (CIM) strategy to explicitly model the redundant segmentation problem and generate refined pseudo labels. Our approach allows the network to become aware of multiple instances and complete instances, and we further improve its robustness through the incorporation of an Anti-noise strategy. Empirical evaluations on the PASCAL VOC 2012 and MS COCO datasets demonstrate that our method achieves state-of-the-art performance with a notable margin. Our implementation will be made available at https://github.com/ZechengLi19/CIM.","sentences":["Weakly supervised instance segmentation (WSIS) using only image-level labels is a challenging task due to the difficulty of aligning coarse annotations with the finer task.","However, with the advancement of deep neural networks (DNNs), WSIS has garnered significant attention.","Following a proposal-based paradigm, we encounter a redundant segmentation problem resulting from a single instance being represented by multiple proposals.","For example, we feed a picture of a dog and proposals into the network and expect to output only one proposal containing a dog, but the network outputs multiple proposals.","To address this problem, we propose a novel approach for WSIS that focuses on the online refinement of complete instances through the use of MaskIoU heads to predict the integrity scores of proposals and a Complete Instances Mining (CIM) strategy to explicitly model the redundant segmentation problem and generate refined pseudo labels.","Our approach allows the network to become aware of multiple instances and complete instances, and we further improve its robustness through the incorporation of an Anti-noise strategy.","Empirical evaluations on the PASCAL VOC 2012 and MS COCO datasets demonstrate that our method achieves state-of-the-art performance with a notable margin.","Our implementation will be made available at https://github.com/ZechengLi19/CIM."],"url":"http://arxiv.org/abs/2402.07633v1","category":"cs.CV"}
{"created":"2024-02-12 13:16:30","title":"Overconfident and Unconfident AI Hinder Human-AI Collaboration","abstract":"As artificial intelligence (AI) advances, human-AI collaboration has become increasingly prevalent across both professional and everyday settings. In such collaboration, AI can express its confidence level about its performance, serving as a crucial indicator for humans to evaluate AI's suggestions. However, AI may exhibit overconfidence or underconfidence--its expressed confidence is higher or lower than its actual performance--which may lead humans to mistakenly evaluate AI advice. Our study investigates the influences of AI's overconfidence and underconfidence on human trust, their acceptance of AI suggestions, and collaboration outcomes. Our study reveal that disclosing AI confidence levels and performance feedback facilitates better recognition of AI confidence misalignments. However, participants tend to withhold their trust as perceiving such misalignments, leading to a rejection of AI suggestions and subsequently poorer performance in collaborative tasks. Conversely, without such information, participants struggle to identify misalignments, resulting in either the neglect of correct AI advice or the following of incorrect AI suggestions, adversely affecting collaboration. This study offers valuable insights for enhancing human-AI collaboration by underscoring the importance of aligning AI's expressed confidence with its actual performance and the necessity of calibrating human trust towards AI confidence.","sentences":["As artificial intelligence (AI) advances, human-AI collaboration has become increasingly prevalent across both professional and everyday settings.","In such collaboration, AI can express its confidence level about its performance, serving as a crucial indicator for humans to evaluate AI's suggestions.","However, AI may exhibit overconfidence or underconfidence--its expressed confidence is higher or lower than its actual performance--which may lead humans to mistakenly evaluate AI advice.","Our study investigates the influences of AI's overconfidence and underconfidence on human trust, their acceptance of AI suggestions, and collaboration outcomes.","Our study reveal that disclosing AI confidence levels and performance feedback facilitates better recognition of AI confidence misalignments.","However, participants tend to withhold their trust as perceiving such misalignments, leading to a rejection of AI suggestions and subsequently poorer performance in collaborative tasks.","Conversely, without such information, participants struggle to identify misalignments, resulting in either the neglect of correct AI advice or the following of incorrect AI suggestions, adversely affecting collaboration.","This study offers valuable insights for enhancing human-AI collaboration by underscoring the importance of aligning AI's expressed confidence with its actual performance and the necessity of calibrating human trust towards AI confidence."],"url":"http://arxiv.org/abs/2402.07632v1","category":"cs.AI"}
{"created":"2024-02-12 13:13:04","title":"G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering","abstract":"Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop our Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever approach, which integrates the strengths of GNNs, LLMs, and Retrieval-Augmented Generation (RAG), and can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and resists hallucination. (Our codes and datasets are available at: https://github.com/XiaoxinHe/G-Retriever.)","sentences":["Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface.","In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph.","While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs.","In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning.","Toward this goal, we first develop our Graph Question Answering (GraphQA) benchmark with data collected from different tasks.","Then, we propose our G-Retriever approach, which integrates the strengths of GNNs, LLMs, and Retrieval-Augmented Generation (RAG), and can be fine-tuned to enhance graph understanding via soft prompting.","To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem.","Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and resists hallucination.","(Our codes and datasets are available at: https://github.com/XiaoxinHe/G-Retriever.)"],"url":"http://arxiv.org/abs/2402.07630v1","category":"cs.LG"}
{"created":"2024-02-12 13:11:11","title":"Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution for Weak Features","abstract":"We investigate the test risk of continuous-time stochastic gradient flow dynamics in learning theory. Using a path integral formulation we provide, in the regime of a small learning rate, a general formula for computing the difference between test risk curves of pure gradient and stochastic gradient flows. We apply the general theory to a simple model of weak features, which displays the double descent phenomenon, and explicitly compute the corrections brought about by the added stochastic term in the dynamics, as a function of time and model parameters. The analytical results are compared to simulations of discrete-time stochastic gradient descent and show good agreement.","sentences":["We investigate the test risk of continuous-time stochastic gradient flow dynamics in learning theory.","Using a path integral formulation we provide, in the regime of a small learning rate, a general formula for computing the difference between test risk curves of pure gradient and stochastic gradient flows.","We apply the general theory to a simple model of weak features, which displays the double descent phenomenon, and explicitly compute the corrections brought about by the added stochastic term in the dynamics, as a function of time and model parameters.","The analytical results are compared to simulations of discrete-time stochastic gradient descent and show good agreement."],"url":"http://arxiv.org/abs/2402.07626v1","category":"stat.ML"}
{"created":"2024-02-12 13:09:21","title":"AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts","abstract":"To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing models' mathematical reasoning capabilities. The AutoMathText dataset is available at https://huggingface.co/datasets/math-ai/AutoMathText. The code is available at https://github.com/yifanzhang-pro/AutoMathText.","sentences":["To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection.","Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data.","To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works.","Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing models' mathematical reasoning capabilities.","The AutoMathText dataset is available at https://huggingface.co/datasets/math-ai/AutoMathText.","The code is available at https://github.com/yifanzhang-pro/AutoMathText."],"url":"http://arxiv.org/abs/2402.07625v1","category":"cs.CL"}
{"created":"2024-02-12 12:59:40","title":"Fluctuation-dissipation relation in cosmic microwave background","abstract":"We study the fluctuation-dissipation relation for sound waves in the cosmic microwave background (CMB), employing effective field theory (EFT) for fluctuating hydrodynamics. Treating sound waves as the linear response to thermal radiation, we establish the fluctuation-dissipation relation within a cosmological framework. While dissipation is elucidated in established linear cosmological perturbation theory, the standard Boltzmann theory overlooks the associated noise, possibly contributing to inconsistencies in Lambda Cold Dark Matter ($\\Lambda$CDM) cosmology. This paper employs EFT for fluctuating hydrodynamics in cosmological perturbation theory, deriving sound wave noise. Notably, the long-time limit of the noise spectrum is independent of viscosity details, resembling a Brownian motion bounded in a harmonic potential. The net energy transfer between the sound wave system and the radiation environment reaches a balance within Hubble time, suggesting the thermal equilibrium of the sound waves themselves. The induced density power spectrum is characterized as white noise dependent on the inverse of the entropy density, which is negligibly small on the CMB scale. The energy density of the entire sound wave system scales as $a^{-4}$, akin to radiation. While the numerical factor is not determined in the present calculation, the back reaction of the sound wave system to the background radiation may not be negligible, serving as a potential source for various fitting issues in $\\Lambda$CDM cosmology.","sentences":["We study the fluctuation-dissipation relation for sound waves in the cosmic microwave background (CMB), employing effective field theory (EFT) for fluctuating hydrodynamics.","Treating sound waves as the linear response to thermal radiation, we establish the fluctuation-dissipation relation within a cosmological framework.","While dissipation is elucidated in established linear cosmological perturbation theory, the standard Boltzmann theory overlooks the associated noise, possibly contributing to inconsistencies in Lambda Cold Dark Matter ($\\Lambda$CDM) cosmology.","This paper employs EFT for fluctuating hydrodynamics in cosmological perturbation theory, deriving sound wave noise.","Notably, the long-time limit of the noise spectrum is independent of viscosity details, resembling a Brownian motion bounded in a harmonic potential.","The net energy transfer between the sound wave system and the radiation environment reaches a balance within Hubble time, suggesting the thermal equilibrium of the sound waves themselves.","The induced density power spectrum is characterized as white noise dependent on the inverse of the entropy density, which is negligibly small on the CMB scale.","The energy density of the entire sound wave system scales as $a^{-4}$, akin to radiation.","While the numerical factor is not determined in the present calculation, the back reaction of the sound wave system to the background radiation may not be negligible, serving as a potential source for various fitting issues in $\\Lambda$CDM cosmology."],"url":"http://arxiv.org/abs/2402.07623v1","category":"hep-th"}
{"created":"2024-02-12 12:54:10","title":"Generating exact polytropes in non-conservative unimodular geometries","abstract":"The trace-free Einstein equations contain one equation less than the complete field equations. In a static and spherically symmetric spacetime, the number of field equations is thus reduced to two. The equation of pressure isotropy of general relativity, however, is preserved thus showing that any known perfect fluid spacetime is a suitable candidate for the trace-free scenario. The extra freedom in imposing two constraints may now be exploited to include polytopes, something that is difficult in general relativity. The point here is that using any known exact solution one can find a polytropic star for various values of the polytropic index. One arrives at Tolman-Oppenheimer-Volkoff type equations and can study their solutions explicitly. Two examples of well-known stellar distributions that generate polytropes with physically reasonable behaviour are discussed. These models are regular, exhibit a sound speed that is never superluminal and are adiabatically stable in the sense of Chandrasekhar. We investigate a compactness measure confirming that our results are consistent with some observational data.","sentences":["The trace-free Einstein equations contain one equation less than the complete field equations.","In a static and spherically symmetric spacetime, the number of field equations is thus reduced to two.","The equation of pressure isotropy of general relativity, however, is preserved thus showing that any known perfect fluid spacetime is a suitable candidate for the trace-free scenario.","The extra freedom in imposing two constraints may now be exploited to include polytopes, something that is difficult in general relativity.","The point here is that using any known exact solution one can find a polytropic star for various values of the polytropic index.","One arrives at Tolman-Oppenheimer-Volkoff type equations and can study their solutions explicitly.","Two examples of well-known stellar distributions that generate polytropes with physically reasonable behaviour are discussed.","These models are regular, exhibit a sound speed that is never superluminal and are adiabatically stable in the sense of Chandrasekhar.","We investigate a compactness measure confirming that our results are consistent with some observational data."],"url":"http://arxiv.org/abs/2402.07620v1","category":"gr-qc"}
{"created":"2024-02-12 12:52:47","title":"Developing a Multi-variate Prediction Model For COVID-19 From Crowd-sourced Respiratory Voice Data","abstract":"COVID-19 has affected more than 223 countries worldwide and in the Post-COVID Era, there is a pressing need for non-invasive, low-cost, and highly scalable solutions to detect COVID-19. We develop a deep learning model to identify COVID-19 from voice recording data. The novelty of this work is in the development of deep learning models for COVID-19 identification from only voice recordings. We use the Cambridge COVID-19 Sound database which contains 893 speech samples, crowd-sourced from 4352 participants via a COVID-19 Sounds app. Voice features including Mel-spectrograms and Mel-frequency cepstral coefficients (MFCC) and CNN Encoder features are extracted. Based on the voice data, we develop deep learning classification models to detect COVID-19 cases. These models include Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) and Hidden-Unit BERT (HuBERT). We compare their predictive power to baseline machine learning models. HuBERT achieves the highest accuracy of 86\\% and the highest AUC of 0.93. The results achieved with the proposed models suggest promising results in COVID-19 diagnosis from voice recordings when compared to the results obtained from the state-of-the-art.","sentences":["COVID-19 has affected more than 223 countries worldwide and in the Post-COVID Era, there is a pressing need for non-invasive, low-cost, and highly scalable solutions to detect COVID-19.","We develop a deep learning model to identify COVID-19 from voice recording data.","The novelty of this work is in the development of deep learning models for COVID-19 identification from only voice recordings.","We use the Cambridge COVID-19 Sound database which contains 893 speech samples, crowd-sourced from 4352 participants via a COVID-19 Sounds app.","Voice features including Mel-spectrograms and Mel-frequency cepstral coefficients (MFCC) and CNN Encoder features are extracted.","Based on the voice data, we develop deep learning classification models to detect COVID-19 cases.","These models include Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) and Hidden-Unit BERT (HuBERT).","We compare their predictive power to baseline machine learning models.","HuBERT achieves the highest accuracy of 86\\% and the highest AUC of 0.93.","The results achieved with the proposed models suggest promising results in COVID-19 diagnosis from voice recordings when compared to the results obtained from the state-of-the-art."],"url":"http://arxiv.org/abs/2402.07619v1","category":"cs.SD"}
{"created":"2024-02-12 12:48:03","title":"Optimized noise-assisted simulation of the Lindblad equation with time-dependent coefficients on a noisy quantum processor","abstract":"Noise in quantum devices is generally considered detrimental to computational accuracy. However, the recent proposal of noise-assisted simulation has demonstrated that noise can be an asset in digital quantum simulations of open systems on Noisy Intermediate-Scale Quantum (NISQ) devices. In this context, we introduce an optimized decoherence rate control scheme that can significantly reduce computational requirements by multiple orders of magnitude, in comparison to the original noise-assisted simulation. We further extend this approach to encompass Lindblad equations with time-dependent coefficients, using only quantum error characterization and mitigation techniques. This extension allows for the perturbative simulation of non-Markovian dynamics on NISQ devices, eliminating the need for ancilla qubits or mid-circuit measurements. Our contributions are validated through numerical experiments on an emulated IBMQ device. Overall, our work offers valuable optimizations that bring current quantum processors closer to effectively simulating realistic open systems.","sentences":["Noise in quantum devices is generally considered detrimental to computational accuracy.","However, the recent proposal of noise-assisted simulation has demonstrated that noise can be an asset in digital quantum simulations of open systems on Noisy Intermediate-Scale Quantum (NISQ) devices.","In this context, we introduce an optimized decoherence rate control scheme that can significantly reduce computational requirements by multiple orders of magnitude, in comparison to the original noise-assisted simulation.","We further extend this approach to encompass Lindblad equations with time-dependent coefficients, using only quantum error characterization and mitigation techniques.","This extension allows for the perturbative simulation of non-Markovian dynamics on NISQ devices, eliminating the need for ancilla qubits or mid-circuit measurements.","Our contributions are validated through numerical experiments on an emulated IBMQ device.","Overall, our work offers valuable optimizations that bring current quantum processors closer to effectively simulating realistic open systems."],"url":"http://arxiv.org/abs/2402.07617v1","category":"quant-ph"}
{"created":"2024-02-12 12:48:02","title":"Anchor-based Large Language Models","abstract":"Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the AnLLM significantly improves computational efficiency and resource utilization, demonstrating the potential of the anchor-based attention approach in the context of LLMs for real-time inference in practical applications.","sentences":["Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation.","However, the substantial size and parameter volume of these LLMs require massive GPU memory.","This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing.","This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy.","This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency.","Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference.","Despite a minor compromise in accuracy, the AnLLM significantly improves computational efficiency and resource utilization, demonstrating the potential of the anchor-based attention approach in the context of LLMs for real-time inference in practical applications."],"url":"http://arxiv.org/abs/2402.07616v1","category":"cs.CL"}
{"created":"2024-02-12 12:42:48","title":"Riemannian trust-region methods for strict saddle functions with complexity guarantees","abstract":"The difficulty of minimizing a nonconvex function is in part explained by the presence of saddle points. This slows down optimization algorithms and impacts worst-case complexity guarantees. However, many nonconvex problems of interest possess a favorable structure for optimization, in the sense that saddle points can be escaped efficiently by appropriate algorithms. This strict saddle property has been extensively used in data science to derive good properties for first-order algorithms, such as convergence to second-order critical points. However, the analysis and the design of second-order algorithms in the strict saddle setting have received significantly less attention. In this paper, we consider second-order trust-region methods for a class of strict saddle functions defined on Riemannian manifolds. These functions exhibit (geodesic) strong convexity around minimizers and negative curvature at saddle points. We show that the standard trust-region method with exact subproblem minimization finds an approximate local minimizer in a number of iterations that depends logarithmically on the accuracy parameter, which significantly improves known results for general nonconvex optimization. We also propose an inexact variant of the algorithm that explicitly leverages the strict saddle property to compute the most appropriate step at every iteration. Our bounds for the inexact variant also improve over the general nonconvex case, and illustrate the benefit of using strict saddle properties within optimization algorithms. Keywords: Riemannian optimization, strict saddle function, second-order method, complexity guarantees.","sentences":["The difficulty of minimizing a nonconvex function is in part explained by the presence of saddle points.","This slows down optimization algorithms and impacts worst-case complexity guarantees.","However, many nonconvex problems of interest possess a favorable structure for optimization, in the sense that saddle points can be escaped efficiently by appropriate algorithms.","This strict saddle property has been extensively used in data science to derive good properties for first-order algorithms, such as convergence to second-order critical points.","However, the analysis and the design of second-order algorithms in the strict saddle setting have received significantly less attention.","In this paper, we consider second-order trust-region methods for a class of strict saddle functions defined on Riemannian manifolds.","These functions exhibit (geodesic) strong convexity around minimizers and negative curvature at saddle points.","We show that the standard trust-region method with exact subproblem minimization finds an approximate local minimizer in a number of iterations that depends logarithmically on the accuracy parameter, which significantly improves known results for general nonconvex optimization.","We also propose an inexact variant of the algorithm that explicitly leverages the strict saddle property to compute the most appropriate step at every iteration.","Our bounds for the inexact variant also improve over the general nonconvex case, and illustrate the benefit of using strict saddle properties within optimization algorithms.","Keywords: Riemannian optimization, strict saddle function, second-order method, complexity guarantees."],"url":"http://arxiv.org/abs/2402.07614v1","category":"math.OC"}
{"created":"2024-02-12 12:30:42","title":"Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping","abstract":"Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models. Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning. To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced few-shot ability to boost zero or one-shot performance. Based on easy-to-hard training recipe, we propose SOFT+ which further boost self-alignment's performance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across various classification and generation tasks, highlighting the potential of bootstrapping self-alignment on continually enhancing model alignment performance.","sentences":["Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability.","However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models.","This gives rise to a key query: What if we do multi-time bootstrapping self-alignment?","Does this strategy enhance model performance or lead to rapid degradation?","In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models.","Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning.","To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model.","Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced few-shot ability to boost zero or one-shot performance.","Based on easy-to-hard training recipe, we propose SOFT+ which further boost self-alignment's performance.","Our experiments demonstrate the efficiency of SOFT (SOFT+) across various classification and generation tasks, highlighting the potential of bootstrapping self-alignment on continually enhancing model alignment performance."],"url":"http://arxiv.org/abs/2402.07610v1","category":"cs.CL"}
{"created":"2024-02-12 12:16:17","title":"Variational post-selection for ground states and thermal states simulation","abstract":"Variational quantum algorithms (VQAs), as one of the most promising routes in the noisy intermediate-scale quantum (NISQ) era, offer various potential applications while also confront severe challenges due to near-term quantum hardware restrictions. In this work, we propose a framework to enhance the expressiveness of variational quantum ansatz by incorporating variational post-selection techniques. These techniques apply variational modules and neural network post-processing on ancilla qubits, which are compatible with the current generation of quantum devices. Equipped with variational post-selection, we demonstrate that the accuracy of the variational ground state and thermal state preparation for both quantum spin and molecule systems is substantially improved. Notably, in the case of estimating the local properties of a thermalized quantum system, we present a scalable approach that outperforms previous methods through the combination of neural post-selection and a new optimization objective.","sentences":["Variational quantum algorithms (VQAs), as one of the most promising routes in the noisy intermediate-scale quantum (NISQ) era, offer various potential applications while also confront severe challenges due to near-term quantum hardware restrictions.","In this work, we propose a framework to enhance the expressiveness of variational quantum ansatz by incorporating variational post-selection techniques.","These techniques apply variational modules and neural network post-processing on ancilla qubits, which are compatible with the current generation of quantum devices.","Equipped with variational post-selection, we demonstrate that the accuracy of the variational ground state and thermal state preparation for both quantum spin and molecule systems is substantially improved.","Notably, in the case of estimating the local properties of a thermalized quantum system, we present a scalable approach that outperforms previous methods through the combination of neural post-selection and a new optimization objective."],"url":"http://arxiv.org/abs/2402.07605v1","category":"quant-ph"}
{"created":"2024-02-12 12:02:13","title":"Resistive switching acceleration induced by thermal confinement","abstract":"Enhancing the switching speed of oxide-based memristive devices at a low voltage level is crucial for their use as non-volatile memory and their integration into emerging computing paradigms such as neuromorphic computing. Efforts to accelerate the switching speed often result in an energy tradeoff, leading to an increase of the minimum working voltage. In our study, we present an innovative solution: the introduction of a low thermal conductivity layer placed within the active electrode, which impedes the dissipation of heat generated during the switching process. The result is a notable acceleration in the switching speed of the memristive model system SrTiO$_{3}$ by a remarkable factor of 10$^{3}$, while preserving the integrity of the switching layer and the interfaces with the electrodes, rendering it adaptable to various filamentary memristive systems. The incorporation of HfO$_{2}$ or TaO$_{x}$ as heat-blocking layers not only streamlines the fabrication process, but also ensures compatibility with complementary metal-oxide-semiconductor technology.","sentences":["Enhancing the switching speed of oxide-based memristive devices at a low voltage level is crucial for their use as non-volatile memory and their integration into emerging computing paradigms such as neuromorphic computing.","Efforts to accelerate the switching speed often result in an energy tradeoff, leading to an increase of the minimum working voltage.","In our study, we present an innovative solution: the introduction of a low thermal conductivity layer placed within the active electrode, which impedes the dissipation of heat generated during the switching process.","The result is a notable acceleration in the switching speed of the memristive model system SrTiO$_{3}$ by a remarkable factor of 10$^{3}$, while preserving the integrity of the switching layer and the interfaces with the electrodes, rendering it adaptable to various filamentary memristive systems.","The incorporation of HfO$_{2}$ or TaO$_{x}$ as heat-blocking layers not only streamlines the fabrication process, but also ensures compatibility with complementary metal-oxide-semiconductor technology."],"url":"http://arxiv.org/abs/2402.07603v1","category":"physics.app-ph"}
{"created":"2024-02-12 11:59:47","title":"Topic-aware Most Influential Community Search in Social Networks","abstract":"Community search is a problem aimed at searching for densely connected subgraphs within a network based on query conditions, which has recently attracted significant attention. However, most previous community search studies have overlooked the coexistence relationship among attributes. They typically assign a single attribute to each node or edge (e.g.,only considering influence scores or keywords), which is difficult for users to obtain a comprehensive and beneficial information. Additionally, most of them also ignored the uncertainty in the attribute graph. Therefore, in this paper, we introduce two novel community models, namely topic-based interaction graph and $(k,l,\\eta)$-influential community. The former is a directed ucertain graph generated by the query topic distribution provided by users, while the latter is used for solving the topic-aware most influential community search problem in social networks. Furthermore, we propose an online search algorithm which computes the influence value of each vertex by considering the topic-aware information diffusion process on interaction graphs. And then, we use a peeling-pruning strategy to iteratively find the topic-aware most $(k,l,\\eta)$-influential community. To further speed up the search performance, we devise two lightweight index structures which efficiently support the search for the topic-aware most influential community within an optimal time. We also propose three optimization methods to improve the space and time costs of the index-based approach.","sentences":["Community search is a problem aimed at searching for densely connected subgraphs within a network based on query conditions, which has recently attracted significant attention.","However, most previous community search studies have overlooked the coexistence relationship among attributes.","They typically assign a single attribute to each node or edge (e.g.,only considering influence scores or keywords), which is difficult for users to obtain a comprehensive and beneficial information.","Additionally, most of them also ignored the uncertainty in the attribute graph.","Therefore, in this paper, we introduce two novel community models, namely topic-based interaction graph and $(k,l,\\eta)$-influential community.","The former is a directed ucertain graph generated by the query topic distribution provided by users, while the latter is used for solving the topic-aware most influential community search problem in social networks.","Furthermore, we propose an online search algorithm which computes the influence value of each vertex by considering the topic-aware information diffusion process on interaction graphs.","And then, we use a peeling-pruning strategy to iteratively find the topic-aware most $(k,l,\\eta)$-influential community.","To further speed up the search performance, we devise two lightweight index structures which efficiently support the search for the topic-aware most influential community within an optimal time.","We also propose three optimization methods to improve the space and time costs of the index-based approach."],"url":"http://arxiv.org/abs/2402.07601v1","category":"cs.SI"}
{"created":"2024-02-12 11:59:05","title":"Optical Routing with Binary Optimisation and Quantum Annealing","abstract":"A challenge for scalability of demand-responsive, elastic optical Dense Wavelength Division Multiplexing (DWDM) and Flexgrid networks is the computational complexity of allocating many optical routes on large networks. We demonstrate that demand satisfaction problems in communication networks can be formulated as quadratic unconstrained binary optimisation (QUBO) problems, and solved using a hybrid quantum annealer. Efficient encodings are developed which solve both unicast and multicast multicommodity-flow problems, while also adhering to individual requirements for maximum latency and resilience for each route. We present several QUBO formulations and analyse the qubit scaling. We demonstrate solutions using a hybrid solver, D-Wave Quantum Advantage QPU. Progress in generating optimal solutions with efficient use of computational resources will be beneficial to telecoms operators, enabling them to run dynamic optical network infrastructures which use resources efficiently, are resilient to local faults and cyber-attacks, and can be elastically responsive to demands.","sentences":["A challenge for scalability of demand-responsive, elastic optical Dense Wavelength Division Multiplexing (DWDM) and Flexgrid networks is the computational complexity of allocating many optical routes on large networks.","We demonstrate that demand satisfaction problems in communication networks can be formulated as quadratic unconstrained binary optimisation (QUBO) problems, and solved using a hybrid quantum annealer.","Efficient encodings are developed which solve both unicast and multicast multicommodity-flow problems, while also adhering to individual requirements for maximum latency and resilience for each route.","We present several QUBO formulations and analyse the qubit scaling.","We demonstrate solutions using a hybrid solver, D-Wave Quantum Advantage QPU.","Progress in generating optimal solutions with efficient use of computational resources will be beneficial to telecoms operators, enabling them to run dynamic optical network infrastructures which use resources efficiently, are resilient to local faults and cyber-attacks, and can be elastically responsive to demands."],"url":"http://arxiv.org/abs/2402.07600v1","category":"cs.NI"}
{"created":"2024-02-12 11:58:18","title":"Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model","abstract":"We propose a new algorithm for model-based distributional reinforcement learning (RL), and prove that it is minimax-optimal for approximating return distributions with a generative model (up to logarithmic factors), resolving an open question of Zhang et al. (2023). Our analysis provides new theoretical results on categorical approaches to distributional RL, and also introduces a new distributional Bellman equation, the stochastic categorical CDF Bellman equation, which we expect to be of independent interest. We also provide an experimental study comparing several model-based distributional RL algorithms, with several takeaways for practitioners.","sentences":["We propose a new algorithm for model-based distributional reinforcement learning (RL), and prove that it is minimax-optimal for approximating return distributions with a generative model (up to logarithmic factors), resolving an open question of Zhang et al. (2023).","Our analysis provides new theoretical results on categorical approaches to distributional RL, and also introduces a new distributional Bellman equation, the stochastic categorical CDF Bellman equation, which we expect to be of independent interest.","We also provide an experimental study comparing several model-based distributional RL algorithms, with several takeaways for practitioners."],"url":"http://arxiv.org/abs/2402.07598v1","category":"cs.LG"}
{"created":"2024-02-12 11:55:02","title":"Trustworthy SR: Resolving Ambiguity in Image Super-resolution via Diffusion Models and Human Feedback","abstract":"Super-resolution (SR) is an ill-posed inverse problem with a large set of feasible solutions that are consistent with a given low-resolution image. Various deterministic algorithms aim to find a single solution that balances fidelity and perceptual quality; however, this trade-off often causes visual artifacts that bring ambiguity in information-centric applications. On the other hand, diffusion models (DMs) excel in generating a diverse set of feasible SR images that span the solution space. The challenge is then how to determine the most likely solution among this set in a trustworthy manner. We observe that quantitative measures, such as PSNR, LPIPS, DISTS, are not reliable indicators to resolve ambiguous cases. To this effect, we propose employing human feedback, where we ask human subjects to select a small number of likely samples and we ensemble the averages of selected samples. This strategy leverages the high-quality image generation capabilities of DMs, while recognizing the importance of obtaining a single trustworthy solution, especially in use cases, such as identification of specific digits or letters, where generating multiple feasible solutions may not lead to a reliable outcome. Experimental results demonstrate that our proposed strategy provides more trustworthy solutions when compared to state-of-the art SR methods.","sentences":["Super-resolution (SR) is an ill-posed inverse problem with a large set of feasible solutions that are consistent with a given low-resolution image.","Various deterministic algorithms aim to find a single solution that balances fidelity and perceptual quality; however, this trade-off often causes visual artifacts that bring ambiguity in information-centric applications.","On the other hand, diffusion models (DMs) excel in generating a diverse set of feasible SR images that span the solution space.","The challenge is then how to determine the most likely solution among this set in a trustworthy manner.","We observe that quantitative measures, such as PSNR, LPIPS, DISTS, are not reliable indicators to resolve ambiguous cases.","To this effect, we propose employing human feedback, where we ask human subjects to select a small number of likely samples and we ensemble the averages of selected samples.","This strategy leverages the high-quality image generation capabilities of DMs, while recognizing the importance of obtaining a single trustworthy solution, especially in use cases, such as identification of specific digits or letters, where generating multiple feasible solutions may not lead to a reliable outcome.","Experimental results demonstrate that our proposed strategy provides more trustworthy solutions when compared to state-of-the art SR methods."],"url":"http://arxiv.org/abs/2402.07597v1","category":"eess.IV"}
{"created":"2024-02-12 11:49:08","title":"Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and DINOv2 in Medical Imaging Classification","abstract":"Medical image analysis frequently encounters data scarcity challenges. Transfer learning has been effective in addressing this issue while conserving computational resources. The recent advent of foundational models like the DINOv2, which uses the vision transformer architecture, has opened new opportunities in the field and gathered significant interest. However, DINOv2's performance on clinical data still needs to be verified. In this paper, we performed a glioma grading task using three clinical modalities of brain MRI data. We compared the performance of various pre-trained deep learning models, including those based on ImageNet and DINOv2, in a transfer learning context. Our focus was on understanding the impact of the freezing mechanism on performance. We also validated our findings on three other types of public datasets: chest radiography, fundus radiography, and dermoscopy. Our findings indicate that in our clinical dataset, DINOv2's performance was not as strong as ImageNet-based pre-trained models, whereas in public datasets, DINOv2 generally outperformed other models, especially when using the frozen mechanism. Similar performance was observed with various sizes of DINOv2 models across different tasks. In summary, DINOv2 is viable for medical image classification tasks, particularly with data resembling natural images. However, its effectiveness may vary with data that significantly differs from natural images such as MRI. In addition, employing smaller versions of the model can be adequate for medical task, offering resource-saving benefits. Our codes are available at https://github.com/GuanghuiFU/medical_DINOv2_eval.","sentences":["Medical image analysis frequently encounters data scarcity challenges.","Transfer learning has been effective in addressing this issue while conserving computational resources.","The recent advent of foundational models like the DINOv2, which uses the vision transformer architecture, has opened new opportunities in the field and gathered significant interest.","However, DINOv2's performance on clinical data still needs to be verified.","In this paper, we performed a glioma grading task using three clinical modalities of brain MRI data.","We compared the performance of various pre-trained deep learning models, including those based on ImageNet and DINOv2, in a transfer learning context.","Our focus was on understanding the impact of the freezing mechanism on performance.","We also validated our findings on three other types of public datasets: chest radiography, fundus radiography, and dermoscopy.","Our findings indicate that in our clinical dataset, DINOv2's performance was not as strong as ImageNet-based pre-trained models, whereas in public datasets, DINOv2 generally outperformed other models, especially when using the frozen mechanism.","Similar performance was observed with various sizes of DINOv2 models across different tasks.","In summary, DINOv2 is viable for medical image classification tasks, particularly with data resembling natural images.","However, its effectiveness may vary with data that significantly differs from natural images such as MRI.","In addition, employing smaller versions of the model can be adequate for medical task, offering resource-saving benefits.","Our codes are available at https://github.com/GuanghuiFU/medical_DINOv2_eval."],"url":"http://arxiv.org/abs/2402.07595v1","category":"eess.IV"}
{"created":"2024-02-12 11:48:54","title":"Foundational Inference Models for Dynamical Systems","abstract":"Ordinary differential equations (ODEs) underlie dynamical systems which serve as models for a vast number of natural and social phenomena. Yet inferring the ODE that best describes a set of noisy observations on one such phenomenon can be remarkably challenging, and the models available to achieve it tend to be highly specialized and complex too. In this work we propose a novel supervised learning framework for zero-shot inference of ODEs from noisy data. We first generate large datasets of one-dimensional ODEs, by sampling distributions over the space of initial conditions, and the space of vector fields defining them. We then learn neural maps between noisy observations on the solutions of these equations, and their corresponding initial condition and vector fields. The resulting models, which we call foundational inference models (FIM), can be (i) copied and matched along the time dimension to increase their resolution; and (ii) copied and composed to build inference models of any dimensionality, without the need of any finetuning. We use FIM to model both ground-truth dynamical systems of different dimensionalities and empirical time series data in a zero-shot fashion, and outperform state-of-the-art models which are finetuned to these systems. Our (pretrained) FIMs are available online","sentences":["Ordinary differential equations (ODEs) underlie dynamical systems which serve as models for a vast number of natural and social phenomena.","Yet inferring the ODE that best describes a set of noisy observations on one such phenomenon can be remarkably challenging, and the models available to achieve it tend to be highly specialized and complex too.","In this work we propose a novel supervised learning framework for zero-shot inference of ODEs from noisy data.","We first generate large datasets of one-dimensional ODEs, by sampling distributions over the space of initial conditions, and the space of vector fields defining them.","We then learn neural maps between noisy observations on the solutions of these equations, and their corresponding initial condition and vector fields.","The resulting models, which we call foundational inference models (FIM), can be (i) copied and matched along the time dimension to increase their resolution; and (ii) copied and composed to build inference models of any dimensionality, without the need of any finetuning.","We use FIM to model both ground-truth dynamical systems of different dimensionalities and empirical time series data in a zero-shot fashion, and outperform state-of-the-art models which are finetuned to these systems.","Our (pretrained) FIMs are available online"],"url":"http://arxiv.org/abs/2402.07594v1","category":"cs.LG"}
{"created":"2024-02-12 11:42:15","title":"New construction of a charged dipole black ring by Harrison transformation","abstract":"We present an exact solution for a non-BPS charged rotating black ring endowed with a dipole charge in the bosonic sector of five-dimensional minimal supergravity. Utilizing the electric Harrison transformation, we derive this solution by converting a five-dimensional vacuum solution into a charged solution within the realm of five-dimensional minimal supergravity. As the seed solution for the Harrison transformation, we use a vacuum solution of a rotating black ring possessing a Dirac-Misner string singularity. The resulting solution exhibits regularity, indicating the absence of curvature singularities, conical singularities, orbifold singularities, Dirac-Misner string singularities, and closed timelike curves both on and outside the horizon. This obtained solution carries mass, two angular momenta, an electric charge, and a dipole charge, with only three of these quantities being independent, similar to the charged rotating dipole black ring found previously by Elvang, Emparan and Figueras. However, aside from the vacuum case, these two solutions do not coincide. We discuss the difference between them in the phase space.","sentences":["We present an exact solution for a non-BPS charged rotating black ring endowed with a dipole charge in the bosonic sector of five-dimensional minimal supergravity.","Utilizing the electric Harrison transformation, we derive this solution by converting a five-dimensional vacuum solution into a charged solution within the realm of five-dimensional minimal supergravity.","As the seed solution for the Harrison transformation, we use a vacuum solution of a rotating black ring possessing a Dirac-Misner string singularity.","The resulting solution exhibits regularity, indicating the absence of curvature singularities, conical singularities, orbifold singularities, Dirac-Misner string singularities, and closed timelike curves both on and outside the horizon.","This obtained solution carries mass, two angular momenta, an electric charge, and a dipole charge, with only three of these quantities being independent, similar to the charged rotating dipole black ring found previously by Elvang, Emparan and Figueras.","However, aside from the vacuum case, these two solutions do not coincide.","We discuss the difference between them in the phase space."],"url":"http://arxiv.org/abs/2402.07589v1","category":"hep-th"}
{"created":"2024-02-12 11:30:15","title":"Passive detection of a random signal common to multi-sensor reference and surveillance arrays","abstract":"This paper addresses the passive detection of a common rank-one subspace signal received in two multi-sensor arrays. We consider the case of a one-antenna transmitter sending a common Gaussian signal, independent Gaussian noises with arbitrary spatial covariance, and known channel subspaces. The detector derived in this paper is a generalized likelihood ratio (GLR) test. For all but one of the unknown parameters, it is possible to find closed-form maximum likelihood (ML) estimator functions. We can further compress the likelihood to only an unknown vector whose ML estimate requires maximizing a product of ratios in quadratic forms, which is carried out using a trust-region algorithm. We propose two approximations of the GLR that do not require any numerical optimization: one based on a sample-based estimator of the unknown parameter whose ML estimate cannot be obtained in closed-form, and one derived under low-SNR conditions. Notably, all the detectors are scale-invariant, and the approximations are functions of beamformed data. However, they are not GLRTs for data that has been pre-processed with a beamformer, a point that is elaborated in the paper. These detectors outperform previously published correlation detectors on simulated data, in many cases quite significantly. Moreover, performance results quantify the performance gains over detectors that assume only the dimension of the subspace to be known.","sentences":["This paper addresses the passive detection of a common rank-one subspace signal received in two multi-sensor arrays.","We consider the case of a one-antenna transmitter sending a common Gaussian signal, independent Gaussian noises with arbitrary spatial covariance, and known channel subspaces.","The detector derived in this paper is a generalized likelihood ratio (GLR) test.","For all but one of the unknown parameters, it is possible to find closed-form maximum likelihood (ML) estimator functions.","We can further compress the likelihood to only an unknown vector whose ML estimate requires maximizing a product of ratios in quadratic forms, which is carried out using a trust-region algorithm.","We propose two approximations of the GLR that do not require any numerical optimization: one based on a sample-based estimator of the unknown parameter whose ML estimate cannot be obtained in closed-form, and one derived under low-SNR conditions.","Notably, all the detectors are scale-invariant, and the approximations are functions of beamformed data.","However, they are not GLRTs for data that has been pre-processed with a beamformer, a point that is elaborated in the paper.","These detectors outperform previously published correlation detectors on simulated data, in many cases quite significantly.","Moreover, performance results quantify the performance gains over detectors that assume only the dimension of the subspace to be known."],"url":"http://arxiv.org/abs/2402.07583v1","category":"eess.SP"}
{"created":"2024-02-12 11:28:56","title":"Quantum speed limit for Kirkwood-Dirac quasiprobabilities","abstract":"What is the minimal time until a quantum system can exhibit genuine quantum features? To answer this question we derive quantum speed limits for two-time correlation functions arising from statistics of measurements. Generally, these two-time correlators are described by quasiprobabilities, if the initial quantum state of the system does not commute with the measurement observables. Our quantum speed limits are derived from the Heisenberg-Robertson uncertainty relation, and set the minimal time at which a quasiprobability can become non-positive, which is evidence for the onset of non-classical traits in the system dynamics. As an illustrative example, we apply these results to a conditional quantum gate, by determining the optimal condition giving rise to non-classicality at maximum speed. Our analysis also hints at boosted power extraction in genuinely non-classical dynamics.","sentences":["What is the minimal time until a quantum system can exhibit genuine quantum features?","To answer this question we derive quantum speed limits for two-time correlation functions arising from statistics of measurements.","Generally, these two-time correlators are described by quasiprobabilities, if the initial quantum state of the system does not commute with the measurement observables.","Our quantum speed limits are derived from the Heisenberg-Robertson uncertainty relation, and set the minimal time at which a quasiprobability can become non-positive, which is evidence for the onset of non-classical traits in the system dynamics.","As an illustrative example, we apply these results to a conditional quantum gate, by determining the optimal condition giving rise to non-classicality at maximum speed.","Our analysis also hints at boosted power extraction in genuinely non-classical dynamics."],"url":"http://arxiv.org/abs/2402.07582v1","category":"quant-ph"}
{"created":"2024-02-12 11:18:32","title":"Topic Modeling as Multi-Objective Contrastive Optimization","abstract":"Recent representation learning approaches enhance neural topic models by optimizing the weighted linear combination of the evidence lower bound (ELBO) of the log-likelihood and the contrastive learning objective that contrasts pairs of input documents. However, document-level contrastive learning might capture low-level mutual information, such as word ratio, which disturbs topic modeling. Moreover, there is a potential conflict between the ELBO loss that memorizes input details for better reconstruction quality, and the contrastive loss which attempts to learn topic representations that generalize among input documents. To address these issues, we first introduce a novel contrastive learning method oriented towards sets of topic vectors to capture useful semantics that are shared among a set of input documents. Secondly, we explicitly cast contrastive topic modeling as a gradient-based multi-objective optimization problem, with the goal of achieving a Pareto stationary solution that balances the trade-off between the ELBO and the contrastive objective. Extensive experiments demonstrate that our framework consistently produces higher-performing neural topic models in terms of topic coherence, topic diversity, and downstream performance.","sentences":["Recent representation learning approaches enhance neural topic models by optimizing the weighted linear combination of the evidence lower bound (ELBO) of the log-likelihood and the contrastive learning objective that contrasts pairs of input documents.","However, document-level contrastive learning might capture low-level mutual information, such as word ratio, which disturbs topic modeling.","Moreover, there is a potential conflict between the ELBO loss that memorizes input details for better reconstruction quality, and the contrastive loss which attempts to learn topic representations that generalize among input documents.","To address these issues, we first introduce a novel contrastive learning method oriented towards sets of topic vectors to capture useful semantics that are shared among a set of input documents.","Secondly, we explicitly cast contrastive topic modeling as a gradient-based multi-objective optimization problem, with the goal of achieving a Pareto stationary solution that balances the trade-off between the ELBO and the contrastive objective.","Extensive experiments demonstrate that our framework consistently produces higher-performing neural topic models in terms of topic coherence, topic diversity, and downstream performance."],"url":"http://arxiv.org/abs/2402.07577v1","category":"cs.CL"}
{"created":"2024-02-12 11:12:27","title":"Love numbers and Love symmetries for $p$-form and gravitational perturbations of higher-dimensional spherically symmetric black holes","abstract":"The static Love numbers of four-dimensional asymptotically flat, isolated, general-relativistic black holes are known to be identically vanishing. The Love symmetry proposal suggests that such vanishings are addressed by selection rules following from the emergence of an enhanced $\\text{SL}(2,\\mathbb{R})$ (``Love'') symmetry in the near-zone region; more specifically, it is the fact that the black hole perturbations belong to a highest-weight representation of this near-zone $\\text{SL}(2,\\mathbb{R})$ symmetry, rather than the existence of the Love symmetry itself, that outputs the vanishings of the corresponding Love numbers. In higher spacetime dimensions, some towers of magic zeroes with regards to the black hole response problem have also been reported for scalar, electromagnetic and gravitational perturbations of the Schwarzschild-Tangherlini black hole. Here, we extend these results by supplementing with $p$-form perturbations of the Schwarzschild-Tangherlini black hole. We furthermore analytically extract the static Love numbers and the leading order dissipation numbers associated with spin-$0$ scalar and spin-$2$ tensor-type tidal perturbations of the higher-dimensional Reissner-Nordstr\\\"om black hole. We find that Love symmetries exist and that the vanishings of the static Love numbers are captured by representation theory arguments even for these higher spin perturbations of the higher-dimensional spherically symmetric black holes of General Relativity. Interestingly, these near-zone $\\text{SL}(2,\\mathbb{R})$ structures acquire extensions to Witt algebras. Our setup allows to also study the $p$-form response problem of a static spherically symmetric black hole in a generic theory of gravity. We perform explicit computations for some black holes in the presence of string-theoretic corrections and investigate under what geometric conditions Love symmetries emerge in the near-zone.","sentences":["The static Love numbers of four-dimensional asymptotically flat, isolated, general-relativistic black holes are known to be identically vanishing.","The Love symmetry proposal suggests that such vanishings are addressed by selection rules following from the emergence of an enhanced $\\text{SL}(2,\\mathbb{R})$ (``Love'') symmetry in the near-zone region; more specifically, it is the fact that the black hole perturbations belong to a highest-weight representation of this near-zone $\\text{SL}(2,\\mathbb{R})$ symmetry, rather than the existence of the Love symmetry itself, that outputs the vanishings of the corresponding Love numbers.","In higher spacetime dimensions, some towers of magic zeroes with regards to the black hole response problem have also been reported for scalar, electromagnetic and gravitational perturbations of the Schwarzschild-Tangherlini black hole.","Here, we extend these results by supplementing with $p$-form perturbations of the Schwarzschild-Tangherlini black hole.","We furthermore analytically extract the static Love numbers and the leading order dissipation numbers associated with spin-$0$ scalar and spin-$2$ tensor-type tidal perturbations of the higher-dimensional Reissner-Nordstr\\\"om black hole.","We find that Love symmetries exist and that the vanishings of the static Love numbers are captured by representation theory arguments even for these higher spin perturbations of the higher-dimensional spherically symmetric black holes of General Relativity.","Interestingly, these near-zone $\\text{SL}(2,\\mathbb{R})$ structures acquire extensions to Witt algebras.","Our setup allows to also study the $p$-form response problem of a static spherically symmetric black hole in a generic theory of gravity.","We perform explicit computations for some black holes in the presence of string-theoretic corrections and investigate under what geometric conditions Love symmetries emerge in the near-zone."],"url":"http://arxiv.org/abs/2402.07574v1","category":"hep-th"}
{"created":"2024-02-12 11:10:12","title":"Goal-Oriented and Semantic Communication in 6G AI-Native Networks: The 6G-GOALS Approach","abstract":"Recent advances in AI technologies have notably expanded device intelligence, fostering federation and cooperation among distributed AI agents. These advancements impose new requirements on future 6G mobile network architectures. To meet these demands, it is essential to transcend classical boundaries and integrate communication, computation, control, and intelligence. This paper presents the 6G-GOALS approach to goal-oriented and semantic communications for AI-Native 6G Networks. The proposed approach incorporates semantic, pragmatic, and goal-oriented communication into AI-native technologies, aiming to facilitate information exchange between intelligent agents in a more relevant, effective, and timely manner, thereby optimizing bandwidth, latency, energy, and electromagnetic field (EMF) radiation. The focus is on distilling data to its most relevant form and terse representation, aligning with the source's intent or the destination's objectives and context, or serving a specific goal. 6G-GOALS builds on three fundamental pillars: i) AI-enhanced semantic data representation, sensing, compression, and communication, ii) foundational AI reasoning and causal semantic data representation, contextual relevance, and value for goal-oriented effectiveness, and iii) sustainability enabled by more efficient wireless services. Finally, we illustrate two proof-of-concepts implementing semantic, goal-oriented, and pragmatic communication principles in near-future use cases. Our study covers the project's vision, methodologies, and potential impact.","sentences":["Recent advances in AI technologies have notably expanded device intelligence, fostering federation and cooperation among distributed AI agents.","These advancements impose new requirements on future 6G mobile network architectures.","To meet these demands, it is essential to transcend classical boundaries and integrate communication, computation, control, and intelligence.","This paper presents the 6G-GOALS approach to goal-oriented and semantic communications for AI-Native 6G Networks.","The proposed approach incorporates semantic, pragmatic, and goal-oriented communication into AI-native technologies, aiming to facilitate information exchange between intelligent agents in a more relevant, effective, and timely manner, thereby optimizing bandwidth, latency, energy, and electromagnetic field (EMF) radiation.","The focus is on distilling data to its most relevant form and terse representation, aligning with the source's intent or the destination's objectives and context, or serving a specific goal.","6G-GOALS builds on three fundamental pillars: i) AI-enhanced semantic data representation, sensing, compression, and communication, ii) foundational AI reasoning and causal semantic data representation, contextual relevance, and value for goal-oriented effectiveness, and iii) sustainability enabled by more efficient wireless services.","Finally, we illustrate two proof-of-concepts implementing semantic, goal-oriented, and pragmatic communication principles in near-future use cases.","Our study covers the project's vision, methodologies, and potential impact."],"url":"http://arxiv.org/abs/2402.07573v1","category":"eess.SP"}
{"created":"2024-02-12 11:06:12","title":"LISA Definition Study Report","abstract":"The Laser Interferometer Space Antenna (LISA) is the first scientific endeavour to detect and study gravitational waves from space. LISA will survey the sky for Gravitational Waves in the 0.1 mHz to 1 Hz frequency band which will enable the study of a vast number of objects ranging from Galactic binaries and stellar mass black holes in the Milky Way, to distant massive black-hole mergers and the expansion of the Universe. This definition study report, or Red Book, presents a summary of the very large body of work that has been undertaken on the LISA mission over the LISA definition phase.","sentences":["The Laser Interferometer Space Antenna (LISA) is the first scientific endeavour to detect and study gravitational waves from space.","LISA will survey the sky for Gravitational Waves in the 0.1 mHz to 1 Hz frequency band which will enable the study of a vast number of objects ranging from Galactic binaries and stellar mass black holes in the Milky Way, to distant massive black-hole mergers and the expansion of the Universe.","This definition study report, or Red Book, presents a summary of the very large body of work that has been undertaken on the LISA mission over the LISA definition phase."],"url":"http://arxiv.org/abs/2402.07571v1","category":"astro-ph.CO"}
{"created":"2024-02-12 11:04:14","title":"Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction","abstract":"We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains. In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude. GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner. Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines. Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate time series forecasting.","sentences":["We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting.","GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains.","In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude.","GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner.","Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines.","Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate time series forecasting."],"url":"http://arxiv.org/abs/2402.07570v1","category":"cs.LG"}
{"created":"2024-02-12 11:03:52","title":"Weisfeiler-Leman at the margin: When more expressivity matters","abstract":"The Weisfeiler-Leman algorithm ($1$-WL) is a well-studied heuristic for the graph isomorphism problem. Recently, the algorithm has played a prominent role in understanding the expressive power of message-passing graph neural networks (MPNNs) and being effective as a graph kernel. Despite its success, $1$-WL faces challenges in distinguishing non-isomorphic graphs, leading to the development of more expressive MPNN and kernel architectures. However, the relationship between enhanced expressivity and improved generalization performance remains unclear. Here, we show that an architecture's expressivity offers limited insights into its generalization performance when viewed through graph isomorphism. Moreover, we focus on augmenting $1$-WL and MPNNs with subgraph information and employ classical margin theory to investigate the conditions under which an architecture's increased expressivity aligns with improved generalization performance. In addition, we show that gradient flow pushes the MPNN's weights toward the maximum margin solution. Further, we introduce variations of expressive $1$-WL-based kernel and MPNN architectures with provable generalization properties. Our empirical study confirms the validity of our theoretical findings.","sentences":["The Weisfeiler-Leman algorithm ($1$-WL) is a well-studied heuristic for the graph isomorphism problem.","Recently, the algorithm has played a prominent role in understanding the expressive power of message-passing graph neural networks (MPNNs) and being effective as a graph kernel.","Despite its success, $1$-WL faces challenges in distinguishing non-isomorphic graphs, leading to the development of more expressive MPNN and kernel architectures.","However, the relationship between enhanced expressivity and improved generalization performance remains unclear.","Here, we show that an architecture's expressivity offers limited insights into its generalization performance when viewed through graph isomorphism.","Moreover, we focus on augmenting $1$-WL and MPNNs with subgraph information and employ classical margin theory to investigate the conditions under which an architecture's increased expressivity aligns with improved generalization performance.","In addition, we show that gradient flow pushes the MPNN's weights toward the maximum margin solution.","Further, we introduce variations of expressive $1$-WL-based kernel and MPNN architectures with provable generalization properties.","Our empirical study confirms the validity of our theoretical findings."],"url":"http://arxiv.org/abs/2402.07568v1","category":"cs.LG"}
{"created":"2024-02-12 10:58:24","title":"Uniaxial Compression based Recovery Analyses to Describe Polymer-Specific and Universal Nanoindentation Deformation Phenomena","abstract":"Sharp tip nanoindentation of glassy polymers is a constrained, localized viscoelastoplastic deformation. We interpret this complexity, in terms of the well-understood uniaxial deformation. From the uniaxial compression data in the literature, for PMMA, PC and crosslinked SU-8, we obtain their universal, yield-normalized recovery curves, with eps*=eps/eps_y, being one measure of the corresponding strain states (CSS). Nanoindentation recovery is determined from the 2sec constant rate unloading h-P data, modeled by a generalized power-law (variable power exponent). Comparing these data-sets, yields the correlation coefficient between the notional nanoindentation strain rate epsdot_N and strain and true strain rate and strain, c=epsdot_N/epsdot_t=eps_N/eps_t. The equivalent strain, eps, and the c value for any polymer, are within a narrow range, from the onset of indentation. Combining residual profiles via scanning probe microscopy with mathematical modeling of the indenter tip, provides the strain distribution beneath the tip. CSS measures examined here, indicate polymer-specific regions to regions common to glassy polymers, which are reached very early in the nanoindentation.","sentences":["Sharp tip nanoindentation of glassy polymers is a constrained, localized viscoelastoplastic deformation.","We interpret this complexity, in terms of the well-understood uniaxial deformation.","From the uniaxial compression data in the literature, for PMMA, PC and crosslinked SU-8, we obtain their universal, yield-normalized recovery curves, with eps*=eps/eps_y, being one measure of the corresponding strain states (CSS).","Nanoindentation recovery is determined from the 2sec constant rate unloading h-P data, modeled by a generalized power-law (variable power exponent).","Comparing these data-sets, yields the correlation coefficient between the notional nanoindentation strain rate epsdot_N and strain and true strain rate and strain, c=epsdot_N/epsdot_t=eps_N/eps_t.","The equivalent strain, eps, and the c value for any polymer, are within a narrow range, from the onset of indentation.","Combining residual profiles via scanning probe microscopy with mathematical modeling of the indenter tip, provides the strain distribution beneath the tip.","CSS measures examined here, indicate polymer-specific regions to regions common to glassy polymers, which are reached very early in the nanoindentation."],"url":"http://arxiv.org/abs/2402.07565v1","category":"cond-mat.soft"}
{"created":"2024-02-12 10:56:09","title":"Discovering Universal Semantic Triggers for Text-to-Image Synthesis","abstract":"Recently text-to-image models have gained widespread attention in the community due to their controllable and high-quality generation ability. However, the robustness of such models and their potential ethical issues have not been fully explored. In this paper, we introduce Universal Semantic Trigger, a meaningless token sequence that can be added at any location within the input text yet can induce generated images towards a preset semantic target.To thoroughly investigate it, we propose Semantic Gradient-based Search (SGS) framework. SGS automatically discovers the potential universal semantic triggers based on the given semantic targets. Furthermore, we design evaluation metrics to comprehensively evaluate semantic shift of images caused by these triggers. And our empirical analyses reveal that the mainstream open-source text-to-image models are vulnerable to our triggers, which could pose significant ethical threats. Our work contributes to a further understanding of text-to-image synthesis and helps users to automatically auditing their models before deployment.","sentences":["Recently text-to-image models have gained widespread attention in the community due to their controllable and high-quality generation ability.","However, the robustness of such models and their potential ethical issues have not been fully explored.","In this paper, we introduce Universal Semantic Trigger, a meaningless token sequence that can be added at any location within the input text yet can induce generated images towards a preset semantic target.","To thoroughly investigate it, we propose Semantic Gradient-based Search (SGS) framework.","SGS automatically discovers the potential universal semantic triggers based on the given semantic targets.","Furthermore, we design evaluation metrics to comprehensively evaluate semantic shift of images caused by these triggers.","And our empirical analyses reveal that the mainstream open-source text-to-image models are vulnerable to our triggers, which could pose significant ethical threats.","Our work contributes to a further understanding of text-to-image synthesis and helps users to automatically auditing their models before deployment."],"url":"http://arxiv.org/abs/2402.07562v1","category":"cs.CR"}
{"created":"2024-02-12 10:54:05","title":"Stabilization of control systems associated with a strongly continuous group","abstract":"This paper is devoted to the stabilization of a linear control system $y' = A y + B u$ and its suitable non-linear variants where $(A, \\cD(A))$ is an infinitesimal generator of a strongly continuous {\\it group} in a Hilbert space $\\mH$, and $B$ defined in a Hilbert space $\\mU$ is an admissible control operator with respect to the semigroup generated by $A$. Let $\\lambda \\in \\mR$ and assume that, for some {\\it positive} symmetric, invertible $Q = Q(\\lambda) \\in \\cL(\\mH)$, for some {\\it non-negative}, symmetric $R = R(\\lambda) \\in \\cL(\\mH)$, and for some {\\it non-negative}, symmetric $W = W(\\lambda) \\in \\cL(\\mU)$, it holds $$ A Q + Q A^* - B W B^* + Q R Q + 2 \\lambda Q = 0 $$ in the sense that $$ \\langle Qx, A^*y \\rangle_{\\mH} + \\langle A^*x, Q y \\rangle_{\\mH} - \\langle W B^*x, B^*y \\rangle_{\\mU} + \\langle R Qx, Q y \\rangle_{\\mH} + 2 \\lambda \\langle Q x, y \\rangle_{\\mH}= 0 \\quad \\forall \\, x, y \\in \\cD(A^*), $$ where $A^*$ is the adjoint of $A$ and $\\cD(A^*)$ is its domain. We present a new method to study the stabilization of such a system and its suitable nonlinear variants. Both the stabilization using dynamic feedback controls and the stabilization using static feedback controls in a weak sense are investigated. To our knowledge, the stabilization by dynamic feedback controls is new even in the linear setting. The nonlinear case is out of reach previously when $B$ is unbounded for both types of stabilization. Consequently, we derive that if the control system is exactly controllable in some positive time, then it is rapidly stabilizable.","sentences":["This paper is devoted to the stabilization of a linear control system $y' =","A y + B u$ and its suitable non-linear variants where $(A, \\cD(A))$ is an infinitesimal generator of a strongly continuous {\\it group} in a Hilbert space $\\mH$, and $B$ defined in a Hilbert space $\\mU$ is an admissible control operator with respect to the semigroup generated by $A$.","Let $\\lambda \\in \\mR$ and assume that, for some {\\it positive} symmetric, invertible $Q = Q(\\lambda) \\in \\cL(\\mH)$, for some {\\it non-negative}, symmetric $R = R(\\lambda) \\in \\cL(\\mH)$, and for some {\\it non-negative}, symmetric $W = W(\\lambda) \\in \\cL(\\mU)$, it holds $$ A Q + Q A^* - B W B^* + Q R Q + 2 \\lambda Q = 0 $$ in the sense that $$ \\langle Qx, A^*y \\rangle_{\\mH} + \\langle A^*x, Q y \\rangle_{\\mH} - \\langle W B^*x, B^*y \\rangle_{\\mU} +","\\langle R Qx, Q y \\rangle_{\\mH} + 2 \\lambda \\langle Q x, y \\rangle_{\\mH}= 0","\\quad \\forall \\, x, y \\in \\cD(A^*), $$ where $A^*$ is the adjoint of $A$ and $\\cD(A^*)$ is its domain.","We present a new method to study the stabilization of such a system and its suitable nonlinear variants.","Both the stabilization using dynamic feedback controls and the stabilization using static feedback controls in a weak sense are investigated.","To our knowledge, the stabilization by dynamic feedback controls is new even in the linear setting.","The nonlinear case is out of reach previously when $B$ is unbounded for both types of stabilization.","Consequently, we derive that if the control system is exactly controllable in some positive time, then it is rapidly stabilizable."],"url":"http://arxiv.org/abs/2402.07560v1","category":"math.OC"}
{"created":"2024-02-12 10:50:29","title":"Lecture notes on Heegaard Floer homology","abstract":"These are the notes for a lecture series on Heegaard Floer homology, given by the first author at the R\\'enyi Institute in January 2023, as part of a special semester titled ``Singularities and Low Dimensional Topology''. Familiarity with Heegaard diagrams and Morse theory is assumed. We first illustrate the relevant algebraic structures via grid homology, and then highlight the geometric rather than combinatorial nature of the general theory. We then define Heegaard Floer homology in the context of Lagrangian Floer homology, and describe some key properties.","sentences":["These are the notes for a lecture series on Heegaard Floer homology, given by the first author at the R\\'enyi Institute in January 2023, as part of a special semester titled ``Singularities and Low Dimensional Topology''.","Familiarity with Heegaard diagrams and Morse theory is assumed.","We first illustrate the relevant algebraic structures via grid homology, and then highlight the geometric rather than combinatorial nature of the general theory.","We then define Heegaard Floer homology in the context of Lagrangian Floer homology, and describe some key properties."],"url":"http://arxiv.org/abs/2402.07558v1","category":"math.GT"}
{"created":"2024-02-12 10:36:04","title":"Nucleon electric and magnetic polarizabilities in Holographic QCD","abstract":"Novel experimental results for the proton generalized electric polarizability, suggest an unexpected deviation from current theoretical predictions at low momentum transfer squared $Q^2$. Motivated by this puzzle, we analyze the resonance contributions to the sum of the generalized electric and magnetic nucleon polarizabilities $\\alpha_E(Q^2)$ and $\\beta_M(Q^2)$, within the Holographic QCD model by Witten, Sakai, and Sugimoto (WSS). In particular, we account for the contributions from the first low-lying nucleon resonances with spin 1/2 and 3/2 and both parities. After having extrapolated the WSS model parameters to fit experimental data on baryonic observables, our findings suggest that the resonance contributions alone do not solve the above-mentioned puzzle. Moreover, at least for the proton case, where data are available, our results are in qualitative agreement with resonance contributions extracted from experimental nucleon-resonance helicity amplitudes.","sentences":["Novel experimental results for the proton generalized electric polarizability, suggest an unexpected deviation from current theoretical predictions at low momentum transfer squared $Q^2$. Motivated by this puzzle, we analyze the resonance contributions to the sum of the generalized electric and magnetic nucleon polarizabilities $\\alpha_E(Q^2)$ and $\\beta_M(Q^2)$, within the Holographic QCD model by Witten, Sakai, and Sugimoto (WSS).","In particular, we account for the contributions from the first low-lying nucleon resonances with spin 1/2 and 3/2 and both parities.","After having extrapolated the WSS model parameters to fit experimental data on baryonic observables, our findings suggest that the resonance contributions alone do not solve the above-mentioned puzzle.","Moreover, at least for the proton case, where data are available, our results are in qualitative agreement with resonance contributions extracted from experimental nucleon-resonance helicity amplitudes."],"url":"http://arxiv.org/abs/2402.07553v1","category":"hep-ph"}
{"created":"2024-02-12 10:35:16","title":"Highly efficient channeling of single photons into guided modes of optical nanocapillary fibers","abstract":"We report numerically the efficient channeling of single photons from a single quantum emitter into guided modes of optical nanocapillary fibers (NCFs). The NCF is formed of a liquid core optical nanofiber with inner and outer diameters. We optimize the inner and outer diameters of the NCF filled with water medium by placing a single dipole source (SDS) inside. The maximum channeling efficiency of 52% is found when the radially polarized SDS is placed at the center of the NCF filled with the water medium. The optimum inner and outer diameters of the NCF are 100 nm and 360 nm for the emission wavelength of 620 nm, respectively. Additionally, we investigate the SDS position dependence inside the NCF considering experimental ambiguity in placing a single quantum emitter inside the NCF. We found that the channeling efficiency remains almost constant for the water medium at the optimum condition. The present platform may open a novel route for generating single photons in quantum technologies and detecting single cells in bio-sensing.","sentences":["We report numerically the efficient channeling of single photons from a single quantum emitter into guided modes of optical nanocapillary fibers (NCFs).","The NCF is formed of a liquid core optical nanofiber with inner and outer diameters.","We optimize the inner and outer diameters of the NCF filled with water medium by placing a single dipole source (SDS) inside.","The maximum channeling efficiency of 52% is found when the radially polarized SDS is placed at the center of the NCF filled with the water medium.","The optimum inner and outer diameters of the NCF are 100 nm and 360 nm for the emission wavelength of 620 nm, respectively.","Additionally, we investigate the SDS position dependence inside the NCF considering experimental ambiguity in placing a single quantum emitter inside the NCF.","We found that the channeling efficiency remains almost constant for the water medium at the optimum condition.","The present platform may open a novel route for generating single photons in quantum technologies and detecting single cells in bio-sensing."],"url":"http://arxiv.org/abs/2402.07552v1","category":"quant-ph"}
{"created":"2024-02-12 10:33:08","title":"De Casteljau's Algorithm in Geometric Data Analysis: Theory and Application","abstract":"For decades, de Casteljau's algorithm has been used as a fundamental building block in curve and surface design and has found a wide range of applications in fields such as scientific computing, and discrete geometry to name but a few. With increasing interest in nonlinear data science, its constructive approach has been shown to provide a principled way to generalize parametric smooth curves to manifolds. These curves have found remarkable new applications in the analysis of parameter-dependent, geometric data. This article provides a survey of the recent theoretical developments in this exciting area as well as its applications in fields such as geometric morphometrics and longitudinal data analysis in medicine, archaeology, and meteorology.","sentences":["For decades, de Casteljau's algorithm has been used as a fundamental building block in curve and surface design and has found a wide range of applications in fields such as scientific computing, and discrete geometry to name but a few.","With increasing interest in nonlinear data science, its constructive approach has been shown to provide a principled way to generalize parametric smooth curves to manifolds.","These curves have found remarkable new applications in the analysis of parameter-dependent, geometric data.","This article provides a survey of the recent theoretical developments in this exciting area as well as its applications in fields such as geometric morphometrics and longitudinal data analysis in medicine, archaeology, and meteorology."],"url":"http://arxiv.org/abs/2402.07550v1","category":"math.DG"}
{"created":"2024-02-12 10:19:17","title":"Ensuring trustworthy and ethical behaviour in intelligent logical agents","abstract":"Autonomous Intelligent Agents are employed in many applications upon which the life and welfare of living beings and vital social functions may depend. Therefore, agents should be trustworthy. A priori certification techniques (i.e., techniques applied prior to system's deployment) can be useful, but are not sufficient for agents that evolve, and thus modify their epistemic and belief state, and for open Multi-Agent Systems, where heterogeneous agents can join or leave the system at any stage of its operation. In this paper, we propose/refine/extend dynamic (runtime) logic-based self-checking techniques, devised in order to be able to ensure agents' trustworthy and ethical behaviour.","sentences":["Autonomous Intelligent Agents are employed in many applications upon which the life and welfare of living beings and vital social functions may depend.","Therefore, agents should be trustworthy.","A priori certification techniques (i.e., techniques applied prior to system's deployment) can be useful, but are not sufficient for agents that evolve, and thus modify their epistemic and belief state, and for open Multi-Agent Systems, where heterogeneous agents can join or leave the system at any stage of its operation.","In this paper, we propose/refine/extend dynamic (runtime) logic-based self-checking techniques, devised in order to be able to ensure agents' trustworthy and ethical behaviour."],"url":"http://arxiv.org/abs/2402.07547v1","category":"cs.MA"}
{"created":"2024-02-12 10:16:05","title":"TransAxx: Efficient Transformers with Approximate Computing","abstract":"Vision Transformer (ViT) models which were recently introduced by the transformer architecture have shown to be very competitive and often become a popular alternative to Convolutional Neural Networks (CNNs). However, the high computational requirements of these models limit their practical applicability especially on low-power devices. Current state-of-the-art employs approximate multipliers to address the highly increased compute demands of DNN accelerators but no prior research has explored their use on ViT models. In this work we propose TransAxx, a framework based on the popular PyTorch library that enables fast inherent support for approximate arithmetic to seamlessly evaluate the impact of approximate computing on DNNs such as ViT models. Using TransAxx we analyze the sensitivity of transformer models on the ImageNet dataset to approximate multiplications and perform approximate-aware finetuning to regain accuracy. Furthermore, we propose a methodology to generate approximate accelerators for ViT models. Our approach uses a Monte Carlo Tree Search (MCTS) algorithm to efficiently search the space of possible configurations using a hardware-driven hand-crafted policy. Our evaluation demonstrates the efficacy of our methodology in achieving significant trade-offs between accuracy and power, resulting in substantial gains without compromising on performance.","sentences":["Vision Transformer (ViT) models which were recently introduced by the transformer architecture have shown to be very competitive and often become a popular alternative to Convolutional Neural Networks (CNNs).","However, the high computational requirements of these models limit their practical applicability especially on low-power devices.","Current state-of-the-art employs approximate multipliers to address the highly increased compute demands of DNN accelerators but no prior research has explored their use on ViT models.","In this work we propose TransAxx, a framework based on the popular PyTorch library that enables fast inherent support for approximate arithmetic to seamlessly evaluate the impact of approximate computing on DNNs such as ViT models.","Using TransAxx we analyze the sensitivity of transformer models on the ImageNet dataset to approximate multiplications and perform approximate-aware finetuning to regain accuracy.","Furthermore, we propose a methodology to generate approximate accelerators for ViT models.","Our approach uses a Monte Carlo Tree Search (MCTS) algorithm to efficiently search the space of possible configurations using a hardware-driven hand-crafted policy.","Our evaluation demonstrates the efficacy of our methodology in achieving significant trade-offs between accuracy and power, resulting in substantial gains without compromising on performance."],"url":"http://arxiv.org/abs/2402.07545v1","category":"cs.LG"}
{"created":"2024-02-12 10:11:50","title":"Show Me How It's Done: The Role of Explanations in Fine-Tuning Language Models","abstract":"Our research demonstrates the significant benefits of using fine-tuning with explanations to enhance the performance of language models. Unlike prompting, which maintains the model's parameters, fine-tuning allows the model to learn and update its parameters during a training phase. In this study, we applied fine-tuning to various sized language models using data that contained explanations of the output rather than merely presenting the answers. We found that even smaller language models with as few as 60 million parameters benefited substantially from this approach. Interestingly, our results indicated that the detailed explanations were more beneficial to smaller models than larger ones, with the latter gaining nearly the same advantage from any form of explanation, irrespective of its length. Additionally, we demonstrate that the inclusion of explanations enables the models to solve tasks that they were not able to solve without explanations. Lastly, we argue that despite the challenging nature of adding explanations, samples that contain explanations not only reduce the volume of data required for training but also promote a more effective generalization by the model. In essence, our findings suggest that fine-tuning with explanations significantly bolsters the performance of large language models.","sentences":["Our research demonstrates the significant benefits of using fine-tuning with explanations to enhance the performance of language models.","Unlike prompting, which maintains the model's parameters, fine-tuning allows the model to learn and update its parameters during a training phase.","In this study, we applied fine-tuning to various sized language models using data that contained explanations of the output rather than merely presenting the answers.","We found that even smaller language models with as few as 60 million parameters benefited substantially from this approach.","Interestingly, our results indicated that the detailed explanations were more beneficial to smaller models than larger ones, with the latter gaining nearly the same advantage from any form of explanation, irrespective of its length.","Additionally, we demonstrate that the inclusion of explanations enables the models to solve tasks that they were not able to solve without explanations.","Lastly, we argue that despite the challenging nature of adding explanations, samples that contain explanations not only reduce the volume of data required for training but also promote a more effective generalization by the model.","In essence, our findings suggest that fine-tuning with explanations significantly bolsters the performance of large language models."],"url":"http://arxiv.org/abs/2402.07543v1","category":"cs.CL"}
{"created":"2024-02-12 10:09:16","title":"PKG API: A Tool for Personal Knowledge Graph Management","abstract":"Personal knowledge graphs (PKGs) offer individuals a way to store and consolidate their fragmented personal data in a central place, improving service personalization while maintaining full user control. Despite their potential, practical PKG implementations with user-friendly interfaces remain scarce. This work addresses this gap by proposing a complete solution to represent, manage, and interface with PKGs. Our approach includes (1) a user-facing PKG Client, enabling end-users to administer their personal data easily via natural language statements, and (2) a service-oriented PKG API. To tackle the complexity of representing these statements within a PKG, we present an RDF-based PKG vocabulary that supports this, along with properties for access rights and provenance.","sentences":["Personal knowledge graphs (PKGs) offer individuals a way to store and consolidate their fragmented personal data in a central place, improving service personalization while maintaining full user control.","Despite their potential, practical PKG implementations with user-friendly interfaces remain scarce.","This work addresses this gap by proposing a complete solution to represent, manage, and interface with PKGs.","Our approach includes (1) a user-facing PKG Client, enabling end-users to administer their personal data easily via natural language statements, and (2) a service-oriented PKG API.","To tackle the complexity of representing these statements within a PKG, we present an RDF-based PKG vocabulary that supports this, along with properties for access rights and provenance."],"url":"http://arxiv.org/abs/2402.07540v1","category":"cs.HC"}
{"created":"2024-02-12 10:06:03","title":"50 Years of Horndeski Gravity: Past, Present and Future","abstract":"An essay on Horndeski gravity, how it was formulated in the early 1970s and how it was 're-discovered' and widely adopted by Cosmologists more than thirty years later.","sentences":["An essay on Horndeski gravity, how it was formulated in the early 1970s and how it was 're-discovered' and widely adopted by Cosmologists more than thirty years later."],"url":"http://arxiv.org/abs/2402.07538v1","category":"gr-qc"}
{"created":"2024-02-12 10:05:49","title":"UAV-assisted Visual SLAM Generating Reconstructed 3D Scene Graphs in GPS-denied Environments","abstract":"Aerial robots play a vital role in various applications where the situational awareness of the robots concerning the environment is a fundamental demand. As one such use case, drones in GPS-denied environments require equipping with different sensors (e.g., vision sensors) that provide reliable sensing results while performing pose estimation and localization. In this paper, reconstructing the maps of indoor environments alongside generating 3D scene graphs for a high-level representation using a camera mounted on a drone is targeted. Accordingly, an aerial robot equipped with a companion computer and an RGB-D camera was built and employed to be appropriately integrated with a Visual Simultaneous Localization and Mapping (VSLAM) framework proposed by the authors. To enhance the situational awareness of the robot while reconstructing maps, various structural elements, including doors and walls, were labeled with printed fiducial markers, and a dictionary of the topological relations among them was fed to the system. The VSLAM system detects markers and reconstructs the map of the indoor areas enriched with higher-level semantic entities, including corridors and rooms. Another achievement is generating multi-layered vision-based situational graphs containing enhanced hierarchical representations of the indoor environment. In this regard, integrating VSLAM into the employed drone is the primary target of this paper to provide an end-to-end robot application for GPS-denied environments. To show the practicality of the system, various real-world condition experiments have been conducted in indoor scenarios with dissimilar structural layouts. Evaluations show the proposed drone application can perform adequately w.r.t. the ground-truth data and its baseline.","sentences":["Aerial robots play a vital role in various applications where the situational awareness of the robots concerning the environment is a fundamental demand.","As one such use case, drones in GPS-denied environments require equipping with different sensors (e.g., vision sensors) that provide reliable sensing results while performing pose estimation and localization.","In this paper, reconstructing the maps of indoor environments alongside generating 3D scene graphs for a high-level representation using a camera mounted on a drone is targeted.","Accordingly, an aerial robot equipped with a companion computer and an RGB-D camera was built and employed to be appropriately integrated with a Visual Simultaneous Localization and Mapping (VSLAM) framework proposed by the authors.","To enhance the situational awareness of the robot while reconstructing maps, various structural elements, including doors and walls, were labeled with printed fiducial markers, and a dictionary of the topological relations among them was fed to the system.","The VSLAM system detects markers and reconstructs the map of the indoor areas enriched with higher-level semantic entities, including corridors and rooms.","Another achievement is generating multi-layered vision-based situational graphs containing enhanced hierarchical representations of the indoor environment.","In this regard, integrating VSLAM into the employed drone is the primary target of this paper to provide an end-to-end robot application for GPS-denied environments.","To show the practicality of the system, various real-world condition experiments have been conducted in indoor scenarios with dissimilar structural layouts.","Evaluations show the proposed drone application can perform adequately w.r.t.","the ground-truth data and its baseline."],"url":"http://arxiv.org/abs/2402.07537v1","category":"cs.RO"}
{"created":"2024-02-12 10:04:07","title":"BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection","abstract":"Trading range breakout (TRB) is a key method in the technical analysis of financial trading, widely employed by traders in financial markets such as stocks, futures, and foreign exchange. However, distinguishing between true and false breakout and providing the correct rationale cause significant challenges to investors. Recently, large language models have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar. The reason is that the unique data and specific knowledge are required in breakout detection. To address these issues, we introduce BreakGPT, the first large language model for financial breakout detection. Furthermore, we have developed a novel framework for large language models, namely multi-stage structure, effectively reducing mistakes in downstream applications. Experimental results indicate that compared to GPT-3.5, BreakGPT improves the accuracy of answers and rational by 44%, with the multi-stage structure contributing 17.6% to the improvement. Additionally, it outperforms ChatGPT-4 by 42.07%. Our Code is publicly available: https://github.com/Neviim96/BreakGPT","sentences":["Trading range breakout (TRB) is a key method in the technical analysis of financial trading, widely employed by traders in financial markets such as stocks, futures, and foreign exchange.","However, distinguishing between true and false breakout and providing the correct rationale cause significant challenges to investors.","Recently, large language models have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar.","The reason is that the unique data and specific knowledge are required in breakout detection.","To address these issues, we introduce BreakGPT, the first large language model for financial breakout detection.","Furthermore, we have developed a novel framework for large language models, namely multi-stage structure, effectively reducing mistakes in downstream applications.","Experimental results indicate that compared to GPT-3.5, BreakGPT improves the accuracy of answers and rational by 44%, with the multi-stage structure contributing 17.6% to the improvement.","Additionally, it outperforms ChatGPT-4 by 42.07%.","Our Code is publicly available: https://github.com/Neviim96/BreakGPT"],"url":"http://arxiv.org/abs/2402.07536v1","category":"cs.AI"}
{"created":"2024-02-12 10:01:53","title":"Forecasting with Pairwise Gaussian Markov Models","abstract":"Pairwise Markov Models (PMMs) extend the wellknown Hidden Markov Models (HMMs). Being significantly more general, PMMs enable several types of processing, like Bayesian filtering or smoothing, similar to those used in HMMs. In this paper, we deal with Bayesian forecasting. The aim is to show analytically in the simple stationary Gaussian case that the extent results obtained with HMM can be improved. We complete contributions with a theoretical error study and two real examples we deal with. Experiments show that PMMs-based forecasting can significantly improve HMMs-based ones.","sentences":["Pairwise Markov Models (PMMs) extend the wellknown Hidden Markov Models (HMMs).","Being significantly more general, PMMs enable several types of processing, like Bayesian filtering or smoothing, similar to those used in HMMs.","In this paper, we deal with Bayesian forecasting.","The aim is to show analytically in the simple stationary Gaussian case that the extent results obtained with HMM can be improved.","We complete contributions with a theoretical error study and two real examples we deal with.","Experiments show that PMMs-based forecasting can significantly improve HMMs-based ones."],"url":"http://arxiv.org/abs/2402.07532v1","category":"math.DS"}
{"created":"2024-02-12 09:52:04","title":"Operating conditions and thermodynamic bounds of dual radiative heat engines","abstract":"A dual radiative heat engine is a device made of two facing optoelectronic components (diodes) and capable of generating electrical power from heat. It can operate in three different regimes depending on the component biases, namely in thermoradiative-negative electroluminescent (TRNEL), thermoradiative-photovoltaic (TRPV) or thermophotonic (TPX) regimes. The use of dual engines gives access to operating conditions which are unachievable by single radiative engines such as thermophotovoltaic systems: at the radiative limit, TRNEL devices systematically reach the Carnot efficiency, while TPX devices can achieve large power outputs by means of electroluminescent enhancement. Upper bounds of the maximum power output and related efficiency achieved by dual engines are derived analytically, and compared to usual bounds. Spectral filtering and nonradiative recombinations are also briefly considered. This work provides common framework and guidelines for the study of radiative engines, which represent a promising solution for reliable and scalable energy conversion.","sentences":["A dual radiative heat engine is a device made of two facing optoelectronic components (diodes) and capable of generating electrical power from heat.","It can operate in three different regimes depending on the component biases, namely in thermoradiative-negative electroluminescent (TRNEL), thermoradiative-photovoltaic (TRPV) or thermophotonic (TPX) regimes.","The use of dual engines gives access to operating conditions which are unachievable by single radiative engines such as thermophotovoltaic systems: at the radiative limit, TRNEL devices systematically reach the Carnot efficiency, while TPX devices can achieve large power outputs by means of electroluminescent enhancement.","Upper bounds of the maximum power output and related efficiency achieved by dual engines are derived analytically, and compared to usual bounds.","Spectral filtering and nonradiative recombinations are also briefly considered.","This work provides common framework and guidelines for the study of radiative engines, which represent a promising solution for reliable and scalable energy conversion."],"url":"http://arxiv.org/abs/2402.07527v1","category":"physics.app-ph"}
{"created":"2024-02-12 09:44:59","title":"Using Ensemble Inference to Improve Recall of Clone Detection","abstract":"Large-scale source-code clone detection is a challenging task. In our previous work, we proposed an approach (SSCD) that leverages artificial neural networks and approximates nearest neighbour search to effectively and efficiently locate clones in large-scale bodies of code, in a time-efficient manner. However, our literature review suggests that the relative efficacy of differing neural network models has not been assessed in the context of large-scale clone detection approaches. In this work, we aim to assess several such models individually, in terms of their potential to maximize recall, while preserving a high level of precision during clone detection. We investigate if ensemble inference (in this case, using the results of more than one of these neural network models in combination) can further assist in this task.   To assess this, we employed four state-of-the-art neural network models and evaluated them individually/in combination. The results, on an illustrative dataset of approximately 500K lines of C/C++ code, suggest that ensemble inference outperforms individual models in all trialled cases, when recall is concerned. Of individual models, the ADA model (belonging to the ChatGPT family of models) has the best performance. However commercial companies may not be prepared to hand their proprietary source code over to the cloud, as required by that approach. Consequently, they may be more interested in an ensemble-combination of CodeBERT-based and CodeT5 models, resulting in similar (if slightly lesser) recall and precision results.","sentences":["Large-scale source-code clone detection is a challenging task.","In our previous work, we proposed an approach (SSCD) that leverages artificial neural networks and approximates nearest neighbour search to effectively and efficiently locate clones in large-scale bodies of code, in a time-efficient manner.","However, our literature review suggests that the relative efficacy of differing neural network models has not been assessed in the context of large-scale clone detection approaches.","In this work, we aim to assess several such models individually, in terms of their potential to maximize recall, while preserving a high level of precision during clone detection.","We investigate if ensemble inference (in this case, using the results of more than one of these neural network models in combination) can further assist in this task.   ","To assess this, we employed four state-of-the-art neural network models and evaluated them individually/in combination.","The results, on an illustrative dataset of approximately 500K lines of C/C++ code, suggest that ensemble inference outperforms individual models in all trialled cases, when recall is concerned.","Of individual models, the ADA model (belonging to the ChatGPT family of models) has the best performance.","However commercial companies may not be prepared to hand their proprietary source code over to the cloud, as required by that approach.","Consequently, they may be more interested in an ensemble-combination of CodeBERT-based and CodeT5 models, resulting in similar (if slightly lesser) recall and precision results."],"url":"http://arxiv.org/abs/2402.07523v1","category":"cs.SE"}
{"created":"2024-02-12 09:41:00","title":"MAFIA: Multi-Adapter Fused Inclusive LanguAge Models","abstract":"Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model to maintain the performance on the downstream task. In this work, we aim to modularly debias a pretrained language model across multiple dimensions. Previous works extensively explored debiasing PLMs using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks and languages demonstrates the efficacy of our approach.","sentences":["Pretrained Language Models (PLMs) are widely used in NLP for various tasks.","Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases.","However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion.","Moreover, the methods typically involve finetuning the full model to maintain the performance on the downstream task.","In this work, we aim to modularly debias a pretrained language model across multiple dimensions.","Previous works extensively explored debiasing PLMs using limited US-centric counterfactual data augmentation (CDA).","We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way.","We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously.","An extensive evaluation on multiple tasks and languages demonstrates the efficacy of our approach."],"url":"http://arxiv.org/abs/2402.07519v1","category":"cs.CL"}
{"created":"2024-02-12 09:40:18","title":"Resilient Watermarking for LLM-Generated Codes","abstract":"With the development of large language models, multiple AIs are now made available for code generation (such as ChatGPT and StarCoder) and are adopted widely. It is often desirable to know whether a piece of code is generated by AI, and furthermore, which AI is the author. For instance, if a certain version of AI is known to generate vulnerable code, it is particularly important to know the creator. Existing approaches are not satisfactory as watermarking codes are challenging compared with watermarking text data, as codes can be altered with relative ease via widely-used code refactoring methods. In this work, we propose ACW (AI Code Watermarking), a novel method for watermarking AI-generated codes. ACW is efficient as it requires no training or fine-tuning and works in a black-box manner. It is resilient as the watermark cannot be easily removed or tampered through common code refactoring methods. The key idea of ACW is to selectively apply a set of carefully-designed semantic-preserving, idempotent code transformations, whose presence (or absence) allows us to determine the existence of the watermark. Our experimental results show that ACW is effective (i.e., achieving high accuracy, true positive rates and false positive rates), resilient and efficient, significantly outperforming existing approaches.","sentences":["With the development of large language models, multiple AIs are now made available for code generation (such as ChatGPT and StarCoder) and are adopted widely.","It is often desirable to know whether a piece of code is generated by AI, and furthermore, which AI is the author.","For instance, if a certain version of AI is known to generate vulnerable code, it is particularly important to know the creator.","Existing approaches are not satisfactory as watermarking codes are challenging compared with watermarking text data, as codes can be altered with relative ease via widely-used code refactoring methods.","In this work, we propose ACW (AI Code Watermarking), a novel method for watermarking AI-generated codes.","ACW is efficient as it requires no training or fine-tuning and works in a black-box manner.","It is resilient as the watermark cannot be easily removed or tampered through common code refactoring methods.","The key idea of ACW is to selectively apply a set of carefully-designed semantic-preserving, idempotent code transformations, whose presence (or absence) allows us to determine the existence of the watermark.","Our experimental results show that ACW is effective (i.e., achieving high accuracy, true positive rates and false positive rates), resilient and efficient, significantly outperforming existing approaches."],"url":"http://arxiv.org/abs/2402.07518v1","category":"cs.CR"}
{"created":"2024-02-12 09:40:17","title":"The spectroastrometric detectability of nearby Solar System-like exomoons","abstract":"Though efforts to detect them have been made with a variety of methods, no technique can claim a successful, confirmed detection of a moon outside the Solar System yet. Moon detection methods are restricted in capability to detecting moons of masses beyond what formation models would suggest, or they require surface temperatures exceeding what tidal heating simulations allow. We expand upon spectroastrometry, a method that makes use of the variation of the centre of light with wavelength as the result of an unresolved companion, which has previously been shown to be capable of detecting Earth-analogue moons around nearby exo-Jupiters, with the aim to place bounds on the types of moons detectable using this method. We derived a general, analytic expression for the spectroastrometric signal of a moon in any closed Keplerian orbit, as well as a new set of estimates on the noise due to photon noise, pointing inaccuracies, background and instrument noise, and a pixelated detector. This framework was consequently used to derive bounds on the temperature required for Solar System-like moons to be observable around super-Jupiters in nearby systems, with $\\epsilon$ Indi Ab as an archetype. We show that such a detection is possible with the ELT for Solar System-like moons of moderate temperatures (150-300 K) in line with existing literature on tidal heating, and that the detection of large (Mars-sized or greater) icy moons of temperatures such as those observed in our Solar System in the very nearest systems may be feasible.","sentences":["Though efforts to detect them have been made with a variety of methods, no technique can claim a successful, confirmed detection of a moon outside the Solar System yet.","Moon detection methods are restricted in capability to detecting moons of masses beyond what formation models would suggest, or they require surface temperatures exceeding what tidal heating simulations allow.","We expand upon spectroastrometry, a method that makes use of the variation of the centre of light with wavelength as the result of an unresolved companion, which has previously been shown to be capable of detecting Earth-analogue moons around nearby exo-Jupiters, with the aim to place bounds on the types of moons detectable using this method.","We derived a general, analytic expression for the spectroastrometric signal of a moon in any closed Keplerian orbit, as well as a new set of estimates on the noise due to photon noise, pointing inaccuracies, background and instrument noise, and a pixelated detector.","This framework was consequently used to derive bounds on the temperature required for Solar System-like moons to be observable around super-Jupiters in nearby systems, with $\\epsilon$ Indi Ab as an archetype.","We show that such a detection is possible with the ELT for Solar System-like moons of moderate temperatures (150-300 K) in line with existing literature on tidal heating, and that the detection of large (Mars-sized or greater) icy moons of temperatures such as those observed in our Solar System in the very nearest systems may be feasible."],"url":"http://arxiv.org/abs/2402.07517v1","category":"astro-ph.EP"}
{"created":"2024-02-12 09:40:08","title":"Gravitational Lensing and Clues of $r_{d}$ and $H_{0}$ Tension: Viscous Modified Chaplygin Gas and Variable Modified Chaplygin Gas in Loop Quantum Cosmology","abstract":"This paper explores the accelerated cosmic expansion in the late Universe through the investigation of two dark energy models: viscous modified Chaplygin gas (VsMCG) and variable modified Chaplygin gas (VMCG), alongside the $\\Lambda$CDM model. Our goal is to constrain fundamental cosmic parameters in these dark energy models with the $\\Lambda$CDM model using 30 of the latest $H(z)$ measurements from the cosmic chronometers method (CC), including Type Ia Supernovae, Gamma-Ray Bursts (GRB), Quasars, and 24 uncorrelated baryon acoustic oscillations (BAO) measurements from recent galaxy surveys across a redshift range from (0.106 < z < 2.33). Additionally, we include the most recent Hubble constant measurement from Riess in 2022. In the $\\Lambda$CDM, VsMCG, and VMCG frameworks, we determine best-fit parameters for the Hubble parameter ($H_{0}$) and sound horizon ($r_{d}$). The results reveal significant disparities between ($H_{0}$) and ($r_{d}$) values using late-time observational measurements, reflecting the widely recognized ($H_{0}$) and ($r_{d}$) tensions. We also study the gravitational lensing optical depth of the two dark energy models and find that the probability of finding gravitational lenses increases with source redshift $z_s$. Joint analysis of VsMCG and VMCG with the $\\Lambda$CDM model shows divergence in the early Universe but indistinguishability at low redshifts. Finally, using the Akaike information criteria approach for model comparison, our analysis indicates that none of the two dark energy models can be dismissed based on the latest observational measurements.","sentences":["This paper explores the accelerated cosmic expansion in the late Universe through the investigation of two dark energy models: viscous modified Chaplygin gas (VsMCG) and variable modified Chaplygin gas (VMCG), alongside the $\\Lambda$CDM model.","Our goal is to constrain fundamental cosmic parameters in these dark energy models with the $\\Lambda$CDM model using 30 of the latest $H(z)$ measurements from the cosmic chronometers method (CC), including Type Ia Supernovae, Gamma-Ray Bursts (GRB), Quasars, and 24 uncorrelated baryon acoustic oscillations (BAO) measurements from recent galaxy surveys across a redshift range from (0.106 < z < 2.33).","Additionally, we include the most recent Hubble constant measurement from Riess in 2022.","In the $\\Lambda$CDM, VsMCG, and VMCG frameworks, we determine best-fit parameters for the Hubble parameter ($H_{0}$) and sound horizon ($r_{d}$).","The results reveal significant disparities between ($H_{0}$) and ($r_{d}$) values using late-time observational measurements, reflecting the widely recognized ($H_{0}$) and ($r_{d}$) tensions.","We also study the gravitational lensing optical depth of the two dark energy models and find that the probability of finding gravitational lenses increases with source redshift $z_s$. Joint analysis of VsMCG and VMCG with the $\\Lambda$CDM model shows divergence in the early Universe but indistinguishability at low redshifts.","Finally, using the Akaike information criteria approach for model comparison, our analysis indicates that none of the two dark energy models can be dismissed based on the latest observational measurements."],"url":"http://arxiv.org/abs/2402.07515v1","category":"gr-qc"}
{"created":"2024-02-12 09:38:42","title":"Physics-informed machine learning as a kernel method","abstract":"Physics-informed machine learning combines the expressiveness of data-based approaches with the interpretability of physical models. In this context, we consider a general regression problem where the empirical risk is regularized by a partial differential equation that quantifies the physical inconsistency. We prove that for linear differential priors, the problem can be formulated as a kernel regression task. Taking advantage of kernel theory, we derive convergence rates for the minimizer of the regularized risk and show that it converges at least at the Sobolev minimax rate. However, faster rates can be achieved, depending on the physical error. This principle is illustrated with a one-dimensional example, supporting the claim that regularizing the empirical risk with physical information can be beneficial to the statistical performance of estimators.","sentences":["Physics-informed machine learning combines the expressiveness of data-based approaches with the interpretability of physical models.","In this context, we consider a general regression problem where the empirical risk is regularized by a partial differential equation that quantifies the physical inconsistency.","We prove that for linear differential priors, the problem can be formulated as a kernel regression task.","Taking advantage of kernel theory, we derive convergence rates for the minimizer of the regularized risk and show that it converges at least at the Sobolev minimax rate.","However, faster rates can be achieved, depending on the physical error.","This principle is illustrated with a one-dimensional example, supporting the claim that regularizing the empirical risk with physical information can be beneficial to the statistical performance of estimators."],"url":"http://arxiv.org/abs/2402.07514v1","category":"cs.AI"}
{"created":"2024-02-12 09:35:13","title":"The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese","abstract":"In the field of spoken language understanding, systems like Whisper and Multilingual Massive Speech (MMS) have shown state-of-the-art performances. This study is dedicated to a comprehensive exploration of the Whisper and MMS systems, with a focus on assessing biases in automatic speech recognition (ASR) inherent to casual conversation speech specific to the Portuguese language. Our investigation encompasses various categories, including gender, age, skin tone color, and geo-location. Alongside traditional ASR evaluation metrics such as Word Error Rate (WER), we have incorporated p-value statistical significance for gender bias analysis. Furthermore, we extensively examine the impact of data distribution and empirically show that oversampling techniques alleviate such stereotypical biases. This research represents a pioneering effort in quantifying biases in the Portuguese language context through the application of MMS and Whisper, contributing to a better understanding of ASR systems' performance in multilingual settings.","sentences":["In the field of spoken language understanding, systems like Whisper and Multilingual Massive Speech (MMS) have shown state-of-the-art performances.","This study is dedicated to a comprehensive exploration of the Whisper and MMS systems, with a focus on assessing biases in automatic speech recognition (ASR) inherent to casual conversation speech specific to the Portuguese language.","Our investigation encompasses various categories, including gender, age, skin tone color, and geo-location.","Alongside traditional ASR evaluation metrics such as Word Error Rate (WER), we have incorporated p-value statistical significance for gender bias analysis.","Furthermore, we extensively examine the impact of data distribution and empirically show that oversampling techniques alleviate such stereotypical biases.","This research represents a pioneering effort in quantifying biases in the Portuguese language context through the application of MMS and Whisper, contributing to a better understanding of ASR systems' performance in multilingual settings."],"url":"http://arxiv.org/abs/2402.07513v1","category":"cs.CL"}
{"created":"2024-02-12 09:33:55","title":"Naked singularities and black hole Killing horizons","abstract":"In this chapter, we study special photon orbits defined by means of Killing vectors and present a framework based on the properties of such null orbits. For concreteness, we restrict ourselves to the case of axially symmetric spacetimes describing either black holes with Killing horizons or naked singularities. The null-orbits framework is then applied to analyze properties of naked singularities and concepts of black hole thermodynamics.","sentences":["In this chapter, we study special photon orbits defined by means of Killing vectors and present a framework based on the properties of such null orbits.","For concreteness, we restrict ourselves to the case of axially symmetric spacetimes describing either black holes with Killing horizons or naked singularities.","The null-orbits framework is then applied to analyze properties of naked singularities and concepts of black hole thermodynamics."],"url":"http://arxiv.org/abs/2402.07512v1","category":"gr-qc"}
{"created":"2024-02-12 09:31:21","title":"Secret Collusion Among Generative AI Agents","abstract":"Recent capability increases in large language models (LLMs) open up applications in which teams of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both the AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities. We conclude by laying out a comprehensive research program to mitigate future risks of collusion between generative AI models.","sentences":["Recent capability increases in large language models (LLMs) open up applications in which teams of communicating generative AI agents solve joint tasks.","This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination.","Modern steganographic techniques could render such dynamics hard to detect.","In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both the AI and security literature.","We study incentives for the use of steganography, and propose a variety of mitigation measures.","Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion.","We provide extensive empirical results across a range of contemporary LLMs.","While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities.","We conclude by laying out a comprehensive research program to mitigate future risks of collusion between generative AI models."],"url":"http://arxiv.org/abs/2402.07510v1","category":"cs.AI"}
{"created":"2024-02-12 09:28:16","title":"Clustering Dynamics for Improved Speed Prediction Deriving from Topographical GPS Registrations","abstract":"A persistent challenge in the field of Intelligent Transportation Systems is to extract accurate traffic insights from geographic regions with scarce or no data coverage. To this end, we propose solutions for speed prediction using sparse GPS data points and their associated topographical and road design features. Our goal is to investigate whether we can use similarities in the terrain and infrastructure to train a machine learning model that can predict speed in regions where we lack transportation data. For this we create a Temporally Orientated Speed Dictionary Centered on Topographically Clustered Roads, which helps us to provide speed correlations to selected feature configurations. Our results show qualitative and quantitative improvement over new and standard regression methods. The presented framework provides a fresh perspective on devising strategies for missing data traffic analysis.","sentences":["A persistent challenge in the field of Intelligent Transportation Systems is to extract accurate traffic insights from geographic regions with scarce or no data coverage.","To this end, we propose solutions for speed prediction using sparse GPS data points and their associated topographical and road design features.","Our goal is to investigate whether we can use similarities in the terrain and infrastructure to train a machine learning model that can predict speed in regions where we lack transportation data.","For this we create a Temporally Orientated Speed Dictionary Centered on Topographically Clustered Roads, which helps us to provide speed correlations to selected feature configurations.","Our results show qualitative and quantitative improvement over new and standard regression methods.","The presented framework provides a fresh perspective on devising strategies for missing data traffic analysis."],"url":"http://arxiv.org/abs/2402.07507v1","category":"cs.AI"}
{"created":"2024-02-12 09:24:34","title":"NeuralSentinel: Safeguarding Neural Network Reliability and Trustworthiness","abstract":"The usage of Artificial Intelligence (AI) systems has increased exponentially, thanks to their ability to reduce the amount of data to be analyzed, the user efforts and preserving a high rate of accuracy. However, introducing this new element in the loop has converted them into attacked points that can compromise the reliability of the systems. This new scenario has raised crucial challenges regarding the reliability and trustworthiness of the AI models, as well as about the uncertainties in their response decisions, becoming even more crucial when applied in critical domains such as healthcare, chemical, electrical plants, etc. To contain these issues, in this paper, we present NeuralSentinel (NS), a tool able to validate the reliability and trustworthiness of AI models. This tool combines attack and defence strategies and explainability concepts to stress an AI model and help non-expert staff increase their confidence in this new system by understanding the model decisions. NS provide a simple and easy-to-use interface for helping humans in the loop dealing with all the needed information. This tool was deployed and used in a Hackathon event to evaluate the reliability of a skin cancer image detector. During the event, experts and non-experts attacked and defended the detector, learning which factors were the most important for model misclassification and which techniques were the most efficient. The event was also used to detect NS's limitations and gather feedback for further improvements.","sentences":["The usage of Artificial Intelligence (AI) systems has increased exponentially, thanks to their ability to reduce the amount of data to be analyzed, the user efforts and preserving a high rate of accuracy.","However, introducing this new element in the loop has converted them into attacked points that can compromise the reliability of the systems.","This new scenario has raised crucial challenges regarding the reliability and trustworthiness of the AI models, as well as about the uncertainties in their response decisions, becoming even more crucial when applied in critical domains such as healthcare, chemical, electrical plants, etc.","To contain these issues, in this paper, we present NeuralSentinel (NS), a tool able to validate the reliability and trustworthiness of AI models.","This tool combines attack and defence strategies and explainability concepts to stress an AI model and help non-expert staff increase their confidence in this new system by understanding the model decisions.","NS provide a simple and easy-to-use interface for helping humans in the loop dealing with all the needed information.","This tool was deployed and used in a Hackathon event to evaluate the reliability of a skin cancer image detector.","During the event, experts and non-experts attacked and defended the detector, learning which factors were the most important for model misclassification and which techniques were the most efficient.","The event was also used to detect NS's limitations and gather feedback for further improvements."],"url":"http://arxiv.org/abs/2402.07506v1","category":"cs.LG"}
{"created":"2024-02-12 09:19:38","title":"Affine term structure models driven by independent L\u00e9vy processes","abstract":"We characterize affine term structure models of non-negative short rate $R$ which may be obtained as solutions of autonomous SDEs driven by independent, one-dimensional L\\'evy martingales, that is equations of the form $$ dR(r)=F(R(t))dt+\\sum_{i=1}^{d}G_i(R(t-))dZ_i(t), \\quad R(0)=r_0\\geq 0, \\quad t>0, \\quad (1)$$ with deterministic real functions $F,G_1,...,G_d$ and independent one-dimensional L\\'evy martingales $Z_1,...,Z_d$. Using a general result on the form of the generators of affine term structure models due to Filipovi\\'c, it is shown, under the assumption that the Laplace transforms of the driving noises are regularly varying, that all possible solutions $R$ of (1) may be obtained also as solutions of autonomous SDEs driven by independent stable processes with stability indices in the range $(1,2]$. The obtained models include in particular the $\\alpha$-CIR model, introduced by Jiao et al., which proved to be still simple yet more reliable than the classical CIR model. Results on heavy tails of $R$ and its limit distribution in terms of the stability indices are proven. Finally, results of numerical calibration of the obtained models to the market term structure of interest rates are presented and compared with the CIR and $\\alpha$-CIR models.","sentences":["We characterize affine term structure models of non-negative short rate $R$ which may be obtained as solutions of autonomous SDEs driven by independent, one-dimensional L\\'evy martingales, that is equations of the form $$ dR(r)=F(R(t))dt+\\sum_{i=1}^{d}G_i(R(t-))dZ_i(t), \\quad R(0)=r_0\\geq 0, \\quad t>0, \\quad (1)$$ with deterministic real functions $F,G_1,...,G_d$ and independent one-dimensional L\\'evy martingales $Z_1,...,Z_d$.","Using a general result on the form of the generators of affine term structure models due to Filipovi\\'c, it is shown, under the assumption that the Laplace transforms of the driving noises are regularly varying, that all possible solutions $R$ of (1) may be obtained also as solutions of autonomous SDEs driven by independent stable processes with stability indices in the range $(1,2]$. The obtained models include in particular the $\\alpha$-CIR model, introduced by Jiao et al., which proved to be still simple yet more reliable than the classical CIR model.","Results on heavy tails of $R$ and its limit distribution in terms of the stability indices are proven.","Finally, results of numerical calibration of the obtained models to the market term structure of interest rates are presented and compared with the CIR and $\\alpha$-CIR models."],"url":"http://arxiv.org/abs/2402.07503v1","category":"math.PR"}
{"created":"2024-02-12 09:10:09","title":"One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning","abstract":"As network security receives widespread attention, encrypted traffic classification has become the current research focus. However, existing methods conduct traffic classification without sufficiently considering the common characteristics between data samples, leading to suboptimal performance. Moreover, they train the packet-level and flow-level classification tasks independently, which is redundant because the packet representations learned in the packet-level task can be exploited by the flow-level task. Therefore, in this paper, we propose an effective model named a Contrastive Learning Enhanced Temporal Fusion Encoder (CLE-TFE). In particular, we utilize supervised contrastive learning to enhance the packet-level and flow-level representations and perform graph data augmentation on the byte-level traffic graph so that the fine-grained semantic-invariant characteristics between bytes can be captured through contrastive learning. We also propose cross-level multi-task learning, which simultaneously accomplishes the packet-level and flow-level classification tasks in the same model with one training. Further experiments show that CLE-TFE achieves the best overall performance on the two tasks, while its computational overhead (i.e., floating point operations, FLOPs) is only about 1/14 of the pre-trained model (e.g., ET-BERT). We release the code at https://github.com/ViktorAxelsen/CLE-TFE","sentences":["As network security receives widespread attention, encrypted traffic classification has become the current research focus.","However, existing methods conduct traffic classification without sufficiently considering the common characteristics between data samples, leading to suboptimal performance.","Moreover, they train the packet-level and flow-level classification tasks independently, which is redundant because the packet representations learned in the packet-level task can be exploited by the flow-level task.","Therefore, in this paper, we propose an effective model named a Contrastive Learning Enhanced Temporal Fusion Encoder (CLE-TFE).","In particular, we utilize supervised contrastive learning to enhance the packet-level and flow-level representations and perform graph data augmentation on the byte-level traffic graph so that the fine-grained semantic-invariant characteristics between bytes can be captured through contrastive learning.","We also propose cross-level multi-task learning, which simultaneously accomplishes the packet-level and flow-level classification tasks in the same model with one training.","Further experiments show that CLE-TFE achieves the best overall performance on the two tasks, while its computational overhead (i.e., floating point operations, FLOPs) is only about 1/14 of the pre-trained model (e.g., ET-BERT).","We release the code at https://github.com/ViktorAxelsen/CLE-TFE"],"url":"http://arxiv.org/abs/2402.07501v1","category":"cs.LG"}
{"created":"2024-02-12 09:02:40","title":"Quaternionic lattices and poly-context-free word problem","abstract":"A finitely generated group $G$ is called poly-context-free if its word problem $\\mathrm{WP}(G)$ is an intersection of finitely many context-free languages. We consider the quaternionic lattices $\\Gamma_\\tau$ over the field $\\mathbb{F}_{q}(t)$ constructed by Stix-Vdovina (2017), and prove that they are not poly-context-free. As a corollary, since all the groups $\\Gamma_{\\tau}$ are quasi-isometric to $F_2\\times F_2$, the class of groups with poly-context-free word problem is not closed under quasi-isometries. The result follows from the description of the language $\\mathrm{WP}(\\Gamma_\\tau)\\cap a^*b^*c^*d^*$, which relies on the existence of anti-tori and certain power-type endomorphisms of the groups $\\Gamma_\\tau$.","sentences":["A finitely generated group $G$ is called poly-context-free if its word problem $\\mathrm{WP}(G)$ is an intersection of finitely many context-free languages.","We consider the quaternionic lattices $\\Gamma_\\tau$ over the field $\\mathbb{F}_{q}(t)$ constructed by Stix-Vdovina (2017), and prove that they are not poly-context-free.","As a corollary, since all the groups $\\Gamma_{\\tau}$ are quasi-isometric to $F_2\\times F_2$, the class of groups with poly-context-free word problem is not closed under quasi-isometries.","The result follows from the description of the language $\\mathrm{WP}(\\Gamma_\\tau)\\cap a^*b^*c^*d^*$, which relies on the existence of anti-tori and certain power-type endomorphisms of the groups $\\Gamma_\\tau$."],"url":"http://arxiv.org/abs/2402.07494v1","category":"math.GR"}
{"created":"2024-02-12 09:02:10","title":"Representations of the $su(1,1)$ current algebra and probabilistic perspectives","abstract":"We construct three representations of the $su(1,1)$ current algebra: in extended Fock space, with Gamma random measures, and with negative binomial (Pascal) point processes. For the second and third representations, the lowering and neutral operators are generators of measure-valued branching processes (Dawson-Watanabe superprocesses) and spatial birth-death processes. The vacuum is the constant function $1$ and iterated application of raising operators yields Laguerre and Meixner polynomials. In addition, we prove a Baker-Campbell-Hausdorff formula and give an explicit formula for the action of unitaries $\\exp( k^+(\\xi) - k^-(\\xi))\\exp(2 \\mathrm i k^0(\\theta))$ on exponential vectors. We explain how the representations fit in with a general scheme proposed by Araki and with representations of the $SL(2,\\mathbb{R})$ current group with Vershik, Gelfand and Graev's multiplicative measure.","sentences":["We construct three representations of the $su(1,1)$ current algebra: in extended Fock space, with Gamma random measures, and with negative binomial (Pascal) point processes.","For the second and third representations, the lowering and neutral operators are generators of measure-valued branching processes (Dawson-Watanabe superprocesses) and spatial birth-death processes.","The vacuum is the constant function $1$ and iterated application of raising operators yields Laguerre and Meixner polynomials.","In addition, we prove a Baker-Campbell-Hausdorff formula and give an explicit formula for the action of unitaries $\\exp( k^+(\\xi) - k^-(\\xi))\\exp(2 \\mathrm i k^0(\\theta))$ on exponential vectors.","We explain how the representations fit in with a general scheme proposed by Araki and with representations of the $SL(2,\\mathbb{R})$ current group with Vershik, Gelfand and Graev's multiplicative measure."],"url":"http://arxiv.org/abs/2402.07493v1","category":"math.PR"}
{"created":"2024-02-12 09:00:27","title":"Convolutional Neural Networks for signal detection in real LIGO data","abstract":"Searching the data of gravitational-wave detectors for signals from compact binary mergers is a computationally demanding task. Recently, machine learning algorithms have been proposed to address current and future challenges. However, the results of these publications often differ greatly due to differing choices in the evaluation procedure. The Machine Learning Gravitational-Wave Search Challenge was organized to resolve these issues and produce a unified framework for machine-learning search evaluation. Six teams submitted contributions, four of which are based on machine learning methods and two are state-of-the-art production analyses. This paper describes the submission from the team TPI FSU Jena and its updated variant. We also apply our algorithm to real O3b data and recover the relevant events of the GWTC-3 catalog.","sentences":["Searching the data of gravitational-wave detectors for signals from compact binary mergers is a computationally demanding task.","Recently, machine learning algorithms have been proposed to address current and future challenges.","However, the results of these publications often differ greatly due to differing choices in the evaluation procedure.","The Machine Learning Gravitational-Wave Search Challenge was organized to resolve these issues and produce a unified framework for machine-learning search evaluation.","Six teams submitted contributions, four of which are based on machine learning methods and two are state-of-the-art production analyses.","This paper describes the submission from the team TPI FSU Jena and its updated variant.","We also apply our algorithm to real O3b data and recover the relevant events of the GWTC-3 catalog."],"url":"http://arxiv.org/abs/2402.07492v1","category":"astro-ph.IM"}
{"created":"2024-02-12 08:58:29","title":"Network mechanism for generating genuinely correlative Gaussian states","abstract":"Generating a long-distance quantum state with genuine quantum correlation (GQC) is one of the most essential functions of quantum networks to support quantum communication. Here, we provide a deterministic scheme for generating multimode Gaussian states with certain GQC (including genuine entanglement). Efficient algorithms of generating multimode states are also proposed. Our scheme is useful for resolving the bottleneck in generating some multimode Gaussian states and may pave the way towards real world applications of preparing multipartite quantum states in current quantum technologies.","sentences":["Generating a long-distance quantum state with genuine quantum correlation (GQC) is one of the most essential functions of quantum networks to support quantum communication.","Here, we provide a deterministic scheme for generating multimode Gaussian states with certain GQC (including genuine entanglement).","Efficient algorithms of generating multimode states are also proposed.","Our scheme is useful for resolving the bottleneck in generating some multimode Gaussian states and may pave the way towards real world applications of preparing multipartite quantum states in current quantum technologies."],"url":"http://arxiv.org/abs/2402.07489v1","category":"quant-ph"}
{"created":"2024-02-12 08:55:55","title":"Abstract null geometry, energy-momentum map and applications to the constraint tensor","abstract":"We introduce and study the notion of null manifold. This is a smooth manifold ${\\mathcal N}$ endowed with a degenerate metric $\\gamma$ with one-dimensional radical at every point. We also define the notion of ruled null manifold, which is a special case of null manifolds. We prove that ruled null manifolds are in one-to-one correspondence with equivalence classes of null metric hypersurface data. This correspondence is used to endow any null manifold $({\\mathcal N},\\gamma)$ with a family of torsion-free connections related to each other by a well-defined gauge group. The whole construction allows one to define and use geometric notions on arbitrary null manifolds. The paper has a second part where we introduce a canonical map on any null metric hypersurface data and use its algebraic properties to define a canonical decomposition of any symmetric (0,2)-covariant tensor. This decomposition, together with two new differential operators compatible with this splitting, are used to decompose the constraint tensor in full generality and at the purely abstract level. This leads to a hierarchical structure of the (detached) Einstein vacuum null constraint equations without the need of introducing special coordinates or special foliations. The results are applied to study null shells arising from the matching of two spacetimes across null boundaries. The equations governing such objects are obtained in hierarchical form without imposing any topological, gauge or coordinate conditions on the shell.","sentences":["We introduce and study the notion of null manifold.","This is a smooth manifold ${\\mathcal N}$ endowed with a degenerate metric $\\gamma$ with one-dimensional radical at every point.","We also define the notion of ruled null manifold, which is a special case of null manifolds.","We prove that ruled null manifolds are in one-to-one correspondence with equivalence classes of null metric hypersurface data.","This correspondence is used to endow any null manifold $({\\mathcal N},\\gamma)$ with a family of torsion-free connections related to each other by a well-defined gauge group.","The whole construction allows one to define and use geometric notions on arbitrary null manifolds.","The paper has a second part where we introduce a canonical map on any null metric hypersurface data and use its algebraic properties to define a canonical decomposition of any symmetric (0,2)-covariant tensor.","This decomposition, together with two new differential operators compatible with this splitting, are used to decompose the constraint tensor in full generality and at the purely abstract level.","This leads to a hierarchical structure of the (detached) Einstein vacuum null constraint equations without the need of introducing special coordinates or special foliations.","The results are applied to study null shells arising from the matching of two spacetimes across null boundaries.","The equations governing such objects are obtained in hierarchical form without imposing any topological, gauge or coordinate conditions on the shell."],"url":"http://arxiv.org/abs/2402.07488v1","category":"gr-qc"}
{"created":"2024-02-12 08:51:06","title":"SLIT: Boosting Audio-Text Pre-Training via Multi-Stage Learning and Instruction Tuning","abstract":"Audio-text pre-training (ATP) has witnessed remarkable strides across a variety of downstream tasks. Yet, most existing pretrained audio models only specialize in either discriminative tasks or generative tasks. In this study, we develop SLIT, a novel ATP framework which transfers flexibly to both audio-text understanding and generation tasks, bootstrapping audio-text pre-training from frozen pretrained audio encoders and large language models. To bridge the modality gap during pre-training, we leverage Q-Former, which undergoes a multi-stage pre-training process. The first stage enhances audio-text representation learning from a frozen audio encoder, while the second stage boosts audio-to-text generative learning with a frozen language model. Furthermore, we introduce an ATP instruction tuning strategy, which enables flexible and informative feature extraction tailered to the given instructions for different tasks. Experiments show that SLIT achieves superior performances on a variety of audio-text understanding and generation tasks, and even demonstrates strong generalization capabilities when directly applied to zero-shot scenarios.","sentences":["Audio-text pre-training (ATP) has witnessed remarkable strides across a variety of downstream tasks.","Yet, most existing pretrained audio models only specialize in either discriminative tasks or generative tasks.","In this study, we develop SLIT, a novel ATP framework which transfers flexibly to both audio-text understanding and generation tasks, bootstrapping audio-text pre-training from frozen pretrained audio encoders and large language models.","To bridge the modality gap during pre-training, we leverage Q-Former, which undergoes a multi-stage pre-training process.","The first stage enhances audio-text representation learning from a frozen audio encoder, while the second stage boosts audio-to-text generative learning with a frozen language model.","Furthermore, we introduce an ATP instruction tuning strategy, which enables flexible and informative feature extraction tailered to the given instructions for different tasks.","Experiments show that SLIT achieves superior performances on a variety of audio-text understanding and generation tasks, and even demonstrates strong generalization capabilities when directly applied to zero-shot scenarios."],"url":"http://arxiv.org/abs/2402.07485v1","category":"cs.SD"}
{"created":"2024-02-12 08:45:08","title":"T-RAG: Lessons from the LLM Trenches","abstract":"Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure to represent entity hierarchies within the organization. This is used to generate a textual description to augment the context when responding to user queries pertaining to entities within the organization's hierarchy. Our evaluations show that this combination performs better than a simple RAG or finetuning implementation. Finally, we share some lessons learned based on our experiences building an LLM application for real-world use.","sentences":["Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains.","An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries.","Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications.","While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain.","We share our experiences building and deploying an LLM application for question answering over private organizational documents.","Our application combines the use of RAG with a finetuned open-source LLM.","Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure to represent entity hierarchies within the organization.","This is used to generate a textual description to augment the context when responding to user queries pertaining to entities within the organization's hierarchy.","Our evaluations show that this combination performs better than a simple RAG or finetuning implementation.","Finally, we share some lessons learned based on our experiences building an LLM application for real-world use."],"url":"http://arxiv.org/abs/2402.07483v1","category":"cs.AI"}
{"created":"2024-02-12 08:42:57","title":"Hypersurface data: General properties and Birkhoff theorem in spherical symmetry","abstract":"The notions of (metric) hypersurface data were introduced in [Mars,2013] as a tool to analyze, from an abstract viewpoint, hypersurfaces of arbitrary signature in pseudo-riemannian manifolds. In this paper, general geometric properties of these notions are studied. In particular, the properties of the gauge group inherent to the geometric construction are analyzed and the metric hypersurface connection and its corresponding curvature tensor are studied. The results set up the stage for various potential applications. The particular but relevant case of spherical symmetry is considered in detail. In particular, a collection of gauge invariant quantities and a radial covariant derivative is introduced, such that the constraint equations of the Einstein field equations with matter can be written in a very compact form. The general solution of these equations in the vacuum case and Lorentzian ambient signature is obtained, and a generalization of the Birkhoff theorem to this abstract hypersurface setting is derived.","sentences":["The notions of (metric) hypersurface data were introduced in [Mars,2013] as a tool to analyze, from an abstract viewpoint, hypersurfaces of arbitrary signature in pseudo-riemannian manifolds.","In this paper, general geometric properties of these notions are studied.","In particular, the properties of the gauge group inherent to the geometric construction are analyzed and the metric hypersurface connection and its corresponding curvature tensor are studied.","The results set up the stage for various potential applications.","The particular but relevant case of spherical symmetry is considered in detail.","In particular, a collection of gauge invariant quantities and a radial covariant derivative is introduced, such that the constraint equations of the Einstein field equations with matter can be written in a very compact form.","The general solution of these equations in the vacuum case and Lorentzian ambient signature is obtained, and a generalization of the Birkhoff theorem to this abstract hypersurface setting is derived."],"url":"http://arxiv.org/abs/2402.07482v1","category":"gr-qc"}
{"created":"2024-02-12 08:42:16","title":"On Salem numbers which are exceptional units II","abstract":"We show that for any natural number $n$ satisfying $n\\equiv 4 \\mod 8$ and $n\\not\\equiv 0 \\mod 5$, and for any odd integer $t\\geq \\frac{n+6}{2}$ there are infinitely many Salem numbers ${\\alpha}$ of degree $2t$ such that ${\\alpha}^n-1$ is a unit. This result, obtained using a generalization of a construction due to Gross and McMullen [5], partially completes the main result of [7].","sentences":["We show that for any natural number $n$ satisfying $n\\equiv 4 \\mod 8$ and $n\\not\\equiv 0 \\mod 5$, and for any odd integer $t\\geq \\frac{n+6}{2}$ there are infinitely many Salem numbers ${\\alpha}$ of degree $2t$ such that ${\\alpha}^n-1$ is a unit.","This result, obtained using a generalization of a construction due to Gross and McMullen","[5], partially completes the main result of [7]."],"url":"http://arxiv.org/abs/2402.07481v1","category":"math.NT"}
{"created":"2024-02-12 08:39:40","title":"Topological Safeguard for Evasion Attack based on the Interpretability of Artificial Neural Network Behavior","abstract":"In the last years, Deep Learning technology has been proposed in different fields, bringing many advances in each of them, but identifying new threats in these solutions regarding cybersecurity. Those implemented models have brought several vulnerabilities associated with Deep Learning technology. Moreover, those allow taking advantage of the implemented model, obtaining private information, and even modifying the model's decision-making. Therefore, interest in studying those vulnerabilities/attacks and designing defenses to avoid or fight them is gaining prominence among researchers. In particular, the widely known evasion attack is being analyzed by researchers; thus, several defenses to avoid such a threat can be found in the literature. Since the presentation of the L-BFG algorithm, this threat concerns the research community. However, it continues developing new and ingenious countermeasures since there is no perfect defense for all the known evasion algorithms. In this work, a novel detector of evasion attacks is developed. It focuses on the information of the activations of the neurons given by the model when an input sample is injected. Moreover, it puts attention to the topology of the targeted deep learning model to analyze the activations according to which neurons are connecting. This approach has been decided because the literature shows that the targeted model's topology contains essential information about if the evasion attack occurs. For this purpose, a huge data preprocessing is required to introduce all this information in the detector, which uses the Graph Convolutional Neural Network (GCN) technology. Thus, it understands the topology of the target model, obtaining promising results and improving the outcomes presented in the literature related to similar defenses.","sentences":["In the last years, Deep Learning technology has been proposed in different fields, bringing many advances in each of them, but identifying new threats in these solutions regarding cybersecurity.","Those implemented models have brought several vulnerabilities associated with Deep Learning technology.","Moreover, those allow taking advantage of the implemented model, obtaining private information, and even modifying the model's decision-making.","Therefore, interest in studying those vulnerabilities/attacks and designing defenses to avoid or fight them is gaining prominence among researchers.","In particular, the widely known evasion attack is being analyzed by researchers; thus, several defenses to avoid such a threat can be found in the literature.","Since the presentation of the L-BFG algorithm, this threat concerns the research community.","However, it continues developing new and ingenious countermeasures since there is no perfect defense for all the known evasion algorithms.","In this work, a novel detector of evasion attacks is developed.","It focuses on the information of the activations of the neurons given by the model when an input sample is injected.","Moreover, it puts attention to the topology of the targeted deep learning model to analyze the activations according to which neurons are connecting.","This approach has been decided because the literature shows that the targeted model's topology contains essential information about if the evasion attack occurs.","For this purpose, a huge data preprocessing is required to introduce all this information in the detector, which uses the Graph Convolutional Neural Network (GCN) technology.","Thus, it understands the topology of the target model, obtaining promising results and improving the outcomes presented in the literature related to similar defenses."],"url":"http://arxiv.org/abs/2402.07480v1","category":"cs.LG"}
{"created":"2024-02-12 08:32:29","title":"Food Recommendation as Language Processing (F-RLP): A Personalized and Contextual Paradigm","abstract":"State-of-the-art rule-based and classification-based food recommendation systems face significant challenges in becoming practical and useful. This difficulty arises primarily because most machine learning models struggle with problems characterized by an almost infinite number of classes and a limited number of samples within an unbalanced dataset. Conversely, the emergence of Large Language Models (LLMs) as recommendation engines offers a promising avenue. However, a general-purpose Recommendation as Language Processing (RLP) approach lacks the critical components necessary for effective food recommendations. To address this gap, we introduce Food Recommendation as Language Processing (F-RLP), a novel framework that offers a food-specific, tailored infrastructure. F-RLP leverages the capabilities of LLMs to maximize their potential, thereby paving the way for more accurate, personalized food recommendations.","sentences":["State-of-the-art rule-based and classification-based food recommendation systems face significant challenges in becoming practical and useful.","This difficulty arises primarily because most machine learning models struggle with problems characterized by an almost infinite number of classes and a limited number of samples within an unbalanced dataset.","Conversely, the emergence of Large Language Models (LLMs) as recommendation engines offers a promising avenue.","However, a general-purpose Recommendation as Language Processing (RLP) approach lacks the critical components necessary for effective food recommendations.","To address this gap, we introduce Food Recommendation as Language Processing (F-RLP), a novel framework that offers a food-specific, tailored infrastructure.","F-RLP leverages the capabilities of LLMs to maximize their potential, thereby paving the way for more accurate, personalized food recommendations."],"url":"http://arxiv.org/abs/2402.07477v1","category":"cs.AI"}
{"created":"2024-02-12 08:32:13","title":"Expansion of higher-dimensional cubical complexes with application to quantum locally testable codes","abstract":"We introduce a higher-dimensional \"cubical\" chain complex and apply it to the design of quantum locally testable codes. Our cubical chain complex can be constructed for any dimension $t$, and in a precise sense generalizes the Sipser-Spielman construction of expander codes (case $t=1$) and the constructions by Dinur et. al and Panteleev and Kalachev of a square complex (case $t$=2), which have been applied to the design of classical locally testable and quantum low-density parity check codes respectively. For $t=4$ our construction gives a family of quantum locally testable codes conditional on a conjecture about robustness of four-tuples of random linear maps. These codes have linear dimension, inverse poly-logarithmic relative distance and soundness, and polylogarithmic-size parity checks.   Our complex can be built in a modular way from two ingredients. Firstly, the geometry (edges, faces, cubes, etc.) is provided by a set $G$ of size $N$, together with pairwise commuting sets of actions $A_1,\\ldots,A_t$ on it. Secondly, the chain complex itself is obtained by associating local coefficient spaces based on codes, with each geometric object, and introducing local maps on those coefficient spaces.   We bound the cycle and co-cycle expansion of the chain complex. The assumptions we need are two-fold: firstly, each Cayley graph $Cay(G,A_j)$ needs to be a good (spectral) expander, and secondly, the families of codes and their duals both need to satisfy a form of robustness (that generalizes the condition of agreement testability for pairs of codes). While the first assumption is easy to satisfy, it is currently not known if the second can be achieved.","sentences":["We introduce a higher-dimensional \"cubical\" chain complex and apply it to the design of quantum locally testable codes.","Our cubical chain complex can be constructed for any dimension $t$, and in a precise sense generalizes the Sipser-Spielman construction of expander codes (case $t=1$) and the constructions by Dinur et.","al and Panteleev and Kalachev of a square complex (case $t$=2), which have been applied to the design of classical locally testable and quantum low-density parity check codes respectively.","For $t=4$ our construction gives a family of quantum locally testable codes conditional on a conjecture about robustness of four-tuples of random linear maps.","These codes have linear dimension, inverse poly-logarithmic relative distance and soundness, and polylogarithmic-size parity checks.   ","Our complex can be built in a modular way from two ingredients.","Firstly, the geometry (edges, faces, cubes, etc.) is provided by a set $G$ of size $N$, together with pairwise commuting sets of actions $A_1,\\ldots,A_t$ on it.","Secondly, the chain complex itself is obtained by associating local coefficient spaces based on codes, with each geometric object, and introducing local maps on those coefficient spaces.   ","We bound the cycle and co-cycle expansion of the chain complex.","The assumptions we need are two-fold: firstly, each Cayley graph $Cay(G,A_j)$ needs to be a good (spectral) expander, and secondly, the families of codes and their duals both need to satisfy a form of robustness (that generalizes the condition of agreement testability for pairs of codes).","While the first assumption is easy to satisfy, it is currently not known if the second can be achieved."],"url":"http://arxiv.org/abs/2402.07476v1","category":"quant-ph"}
{"created":"2024-02-12 08:19:10","title":"Moments of real Dirichlet $L$-functions and multiple Dirichlet series","abstract":"We consider the multiple Dirichlet series associated to the $k$th moment of real Dirichlet $L$-functions, and prove that it has a meromorphic continuation to a specific region in $\\mathbb{C}^{k+1}$, which is conditional under the generalized Lindel\\\"of hypothesis for $k\\geq 5$.   As a corollary, we obtain asymptotic formulas for the first three moments with a power-saving error term, and detect the 0- and 1-swap terms in related problems for any $k$ (conditionally under the Generalized Lindel\\\"of Hypothesis), recovering the recent results of Conrey and Rodgers on long Dirichlet polynomials.   The advantage of our method is its simplicity, since we don't need to modify the multiple Dirichlet series to obtain its meromorphic continuation. As a result, we obtain the asymptotic formulas directly in the form as they appear in the recipe predictions of Conrey, Farmer, Keating, Rubinstein and Snaith.","sentences":["We consider the multiple Dirichlet series associated to the $k$th moment of real Dirichlet $L$-functions, and prove that it has a meromorphic continuation to a specific region in $\\mathbb{C}^{k+1}$, which is conditional under the generalized Lindel\\\"of hypothesis for $k\\geq 5$.   As a corollary, we obtain asymptotic formulas for the first three moments with a power-saving error term, and detect the 0- and 1-swap terms in related problems for any $k$ (conditionally under the Generalized Lindel\\\"of Hypothesis), recovering the recent results of Conrey and Rodgers on long Dirichlet polynomials.   ","The advantage of our method is its simplicity, since we don't need to modify the multiple Dirichlet series to obtain its meromorphic continuation.","As a result, we obtain the asymptotic formulas directly in the form as they appear in the recipe predictions of Conrey, Farmer, Keating, Rubinstein and Snaith."],"url":"http://arxiv.org/abs/2402.07473v1","category":"math.NT"}
{"created":"2024-02-12 08:03:30","title":"You can monitor your hydration level using your smartphone camera","abstract":"This work proposes for the first time to utilize the regular smartphone -- a popular assistive gadget -- to design a novel, non-invasive method for self-monitoring of one's hydration level on a scale of 1 to 4. The proposed method involves recording a small video of a fingertip using the smartphone camera. Subsequently, a photoplethysmography (PPG) signal is extracted from the video data, capturing the fluctuations in peripheral blood volume as a reflection of a person's hydration level changes over time. To train and evaluate the artificial intelligence models, a custom multi-session labeled dataset was constructed by collecting video-PPG data from 25 fasting subjects during the month of Ramadan in 2023. With this, we solve two distinct problems: 1) binary classification (whether a person is hydrated or not), 2) four-class classification (whether a person is fully hydrated, mildly dehydrated, moderately dehydrated, or extremely dehydrated). For both classification problems, we feed the pre-processed and augmented PPG data to a number of machine learning, deep learning and transformer models which models provide a very high accuracy, i.e., in the range of 95% to 99%. We also propose an alternate method where we feed high-dimensional PPG time-series data to a DL model for feature extraction, followed by t-SNE method for feature selection and dimensionality reduction, followed by a number of ML classifiers that do dehydration level classification. Finally, we interpret the decisions by the developed deep learning model under the SHAP-based explainable artificial intelligence framework. The proposed method allows rapid, do-it-yourself, at-home testing of one's hydration level, is cost-effective and thus inline with the sustainable development goals 3 & 10 of the United Nations, and a step-forward to patient-centric healthcare systems, smart homes, and smart cities of future.","sentences":["This work proposes for the first time to utilize the regular smartphone -- a popular assistive gadget -- to design a novel, non-invasive method for self-monitoring of one's hydration level on a scale of 1 to 4.","The proposed method involves recording a small video of a fingertip using the smartphone camera.","Subsequently, a photoplethysmography (PPG) signal is extracted from the video data, capturing the fluctuations in peripheral blood volume as a reflection of a person's hydration level changes over time.","To train and evaluate the artificial intelligence models, a custom multi-session labeled dataset was constructed by collecting video-PPG data from 25 fasting subjects during the month of Ramadan in 2023.","With this, we solve two distinct problems: 1) binary classification (whether a person is hydrated or not), 2) four-class classification (whether a person is fully hydrated, mildly dehydrated, moderately dehydrated, or extremely dehydrated).","For both classification problems, we feed the pre-processed and augmented PPG data to a number of machine learning, deep learning and transformer models which models provide a very high accuracy, i.e., in the range of 95% to 99%.","We also propose an alternate method where we feed high-dimensional PPG time-series data to a DL model for feature extraction, followed by t-SNE method for feature selection and dimensionality reduction, followed by a number of ML classifiers that do dehydration level classification.","Finally, we interpret the decisions by the developed deep learning model under the SHAP-based explainable artificial intelligence framework.","The proposed method allows rapid, do-it-yourself, at-home testing of one's hydration level, is cost-effective and thus inline with the sustainable development goals 3 & 10 of the United Nations, and a step-forward to patient-centric healthcare systems, smart homes, and smart cities of future."],"url":"http://arxiv.org/abs/2402.07467v1","category":"eess.SP"}
{"created":"2024-02-12 07:59:25","title":"Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck Equations","abstract":"The Fokker-Planck (FP) equation is a foundational PDE in stochastic processes. However, curse of dimensionality (CoD) poses challenge when dealing with high-dimensional FP PDEs. Although Monte Carlo and vanilla Physics-Informed Neural Networks (PINNs) have shown the potential to tackle CoD, both methods exhibit numerical errors in high dimensions when dealing with the probability density function (PDF) associated with Brownian motion. The point-wise PDF values tend to decrease exponentially as dimension increases, surpassing the precision of numerical simulations and resulting in substantial errors. Moreover, due to its massive sampling, Monte Carlo fails to offer fast sampling. Modeling the logarithm likelihood (LL) via vanilla PINNs transforms the FP equation into a difficult HJB equation, whose error grows rapidly with dimension. To this end, we propose a novel approach utilizing a score-based solver to fit the score function in SDEs. The score function, defined as the gradient of the LL, plays a fundamental role in inferring LL and PDF and enables fast SDE sampling. Three fitting methods, Score Matching (SM), Sliced SM (SSM), and Score-PINN, are introduced. The proposed score-based SDE solver operates in two stages: first, employing SM, SSM, or Score-PINN to acquire the score; and second, solving the LL via an ODE using the obtained score. Comparative evaluations across these methods showcase varying trade-offs. The proposed method is evaluated across diverse SDEs, including anisotropic OU processes, geometric Brownian, and Brownian with varying eigenspace. We also test various distributions, including Gaussian, Log-normal, Laplace, and Cauchy. The numerical results demonstrate the score-based SDE solver's stability, speed, and performance across different settings, solidifying its potential as a solution to CoD for high-dimensional FP equations.","sentences":["The Fokker-Planck (FP) equation is a foundational PDE in stochastic processes.","However, curse of dimensionality (CoD) poses challenge when dealing with high-dimensional FP PDEs.","Although Monte Carlo and vanilla Physics-Informed Neural Networks (PINNs) have shown the potential to tackle CoD, both methods exhibit numerical errors in high dimensions when dealing with the probability density function (PDF) associated with Brownian motion.","The point-wise PDF values tend to decrease exponentially as dimension increases, surpassing the precision of numerical simulations and resulting in substantial errors.","Moreover, due to its massive sampling, Monte Carlo fails to offer fast sampling.","Modeling the logarithm likelihood (LL) via vanilla PINNs transforms the FP equation into a difficult HJB equation, whose error grows rapidly with dimension.","To this end, we propose a novel approach utilizing a score-based solver to fit the score function in SDEs.","The score function, defined as the gradient of the LL, plays a fundamental role in inferring LL and PDF and enables fast SDE sampling.","Three fitting methods, Score Matching (SM), Sliced SM (SSM), and Score-PINN, are introduced.","The proposed score-based SDE solver operates in two stages: first, employing SM, SSM, or Score-PINN to acquire the score; and second, solving the LL via an ODE using the obtained score.","Comparative evaluations across these methods showcase varying trade-offs.","The proposed method is evaluated across diverse SDEs, including anisotropic OU processes, geometric Brownian, and Brownian with varying eigenspace.","We also test various distributions, including Gaussian, Log-normal, Laplace, and Cauchy.","The numerical results demonstrate the score-based SDE solver's stability, speed, and performance across different settings, solidifying its potential as a solution to CoD for high-dimensional FP equations."],"url":"http://arxiv.org/abs/2402.07465v1","category":"cs.LG"}
{"created":"2024-02-12 07:49:48","title":"A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?","abstract":"The value-loading problem is a significant challenge for researchers aiming to create artificial intelligence (AI) systems that align with human values and preferences. This problem requires a method to define and regulate safe and optimal limits of AI behaviors. In this work, we propose HALO (Hormetic ALignment via Opponent processes), a regulatory paradigm that uses hormetic analysis to regulate the behavioral patterns of AI. Behavioral hormesis is a phenomenon where low frequencies of a behavior have beneficial effects, while high frequencies are harmful. By modeling behaviors as allostatic opponent processes, we can use either Behavioral Frequency Response Analysis (BFRA) or Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of repeatable behaviors. We demonstrate how HALO can solve the 'paperclip maximizer' scenario, a thought experiment where an unregulated AI tasked with making paperclips could end up converting all matter in the universe into paperclips. Our approach may be used to help create an evolving database of 'values' based on the hedonic calculus of repeatable behaviors with decreasing marginal utility. This positions HALO as a promising solution for the value-loading problem, which involves embedding human-aligned values into an AI system, and the weak-to-strong generalization problem, which explores whether weak models can supervise stronger models as they become more intelligent. Hence, HALO opens several research avenues that may lead to the development of a computational value system that allows an AI algorithm to learn whether the decisions it makes are right or wrong.","sentences":["The value-loading problem is a significant challenge for researchers aiming to create artificial intelligence (AI) systems that align with human values and preferences.","This problem requires a method to define and regulate safe and optimal limits of AI behaviors.","In this work, we propose HALO (Hormetic ALignment via Opponent processes), a regulatory paradigm that uses hormetic analysis to regulate the behavioral patterns of AI.","Behavioral hormesis is a phenomenon where low frequencies of a behavior have beneficial effects, while high frequencies are harmful.","By modeling behaviors as allostatic opponent processes, we can use either Behavioral Frequency Response Analysis (BFRA) or Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of repeatable behaviors.","We demonstrate how HALO can solve the 'paperclip maximizer' scenario, a thought experiment where an unregulated AI tasked with making paperclips could end up converting all matter in the universe into paperclips.","Our approach may be used to help create an evolving database of 'values' based on the hedonic calculus of repeatable behaviors with decreasing marginal utility.","This positions HALO as a promising solution for the value-loading problem, which involves embedding human-aligned values into an AI system, and the weak-to-strong generalization problem, which explores whether weak models can supervise stronger models as they become more intelligent.","Hence, HALO opens several research avenues that may lead to the development of a computational value system that allows an AI algorithm to learn whether the decisions it makes are right or wrong."],"url":"http://arxiv.org/abs/2402.07462v1","category":"cs.AI"}
{"created":"2024-02-12 07:30:23","title":"A Note on Kernel Functions of Dirichlet Spaces","abstract":"For a planar domain $\\Omega$, we consider the Dirichlet spaces with respect to a base point $\\zeta\\in\\Omega$ and the corresponding kernel functions. It is not known how these kernel functions behave as we vary the base point. In this note, we prove that these kernel functions vary smoothly. As an application of the smoothness result, we prove a Ramadanov-type theorem for these kernel functions on $\\Omega\\times\\Omega$. This extends the previously known convergence results of these kernel functions. In fact, we have made these observations in a more general setting, that is, for weighted kernel functions and their higher-order counterparts.","sentences":["For a planar domain $\\Omega$, we consider the Dirichlet spaces with respect to a base point $\\zeta\\in\\Omega$ and the corresponding kernel functions.","It is not known how these kernel functions behave as we vary the base point.","In this note, we prove that these kernel functions vary smoothly.","As an application of the smoothness result, we prove a Ramadanov-type theorem for these kernel functions on $\\Omega\\times\\Omega$. This extends the previously known convergence results of these kernel functions.","In fact, we have made these observations in a more general setting, that is, for weighted kernel functions and their higher-order counterparts."],"url":"http://arxiv.org/abs/2402.07457v1","category":"math.CV"}
{"created":"2024-02-12 07:29:22","title":"OS-Copilot: Towards Generalist Computer Agents with Self-Improvement","abstract":"Autonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of large language models (LLMs) has markedly accelerated progress in building digital agents. However, most of these agents are designed to interact with a narrow domain, such as a specific software or website. This narrow focus constrains their applicability for general computer tasks. To this end, we introduce OS-Copilot, a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications. We use OS-Copilot to create FRIDAY, a self-improving embodied agent for automating general computer tasks. On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods by 35%, showcasing strong generalization to unseen applications via accumulated skills from previous tasks. We also present numerical and quantitative evidence that FRIDAY learns to control and self-improve on Excel and Powerpoint with minimal supervision. Our OS-Copilot framework and empirical findings provide infrastructure and insights for future research toward more capable and general-purpose computer agents.","sentences":["Autonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of large language models (LLMs) has markedly accelerated progress in building digital agents.","However, most of these agents are designed to interact with a narrow domain, such as a specific software or website.","This narrow focus constrains their applicability for general computer tasks.","To this end, we introduce OS-Copilot, a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications.","We use OS-Copilot to create FRIDAY, a self-improving embodied agent for automating general computer tasks.","On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods by 35%, showcasing strong generalization to unseen applications via accumulated skills from previous tasks.","We also present numerical and quantitative evidence that FRIDAY learns to control and self-improve on Excel and Powerpoint with minimal supervision.","Our OS-Copilot framework and empirical findings provide infrastructure and insights for future research toward more capable and general-purpose computer agents."],"url":"http://arxiv.org/abs/2402.07456v1","category":"cs.AI"}
{"created":"2024-02-12 07:19:00","title":"TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound","abstract":"Different diseases, such as histological subtypes of breast lesions, have severely varying incidence rates. Even trained with substantial amount of in-distribution (ID) data, models often encounter out-of-distribution (OOD) samples belonging to unseen classes in clinical reality. To address this, we propose a novel framework built upon a long-tailed OOD detection task for breast ultrasound images. It is equipped with a triplet state augmentation (TriAug) which improves ID classification accuracy while maintaining a promising OOD detection performance. Meanwhile, we designed a balanced sphere loss to handle the class imbalanced problem.","sentences":["Different diseases, such as histological subtypes of breast lesions, have severely varying incidence rates.","Even trained with substantial amount of in-distribution (ID) data, models often encounter out-of-distribution (OOD) samples belonging to unseen classes in clinical reality.","To address this, we propose a novel framework built upon a long-tailed OOD detection task for breast ultrasound images.","It is equipped with a triplet state augmentation (TriAug) which improves ID classification accuracy while maintaining a promising OOD detection performance.","Meanwhile, we designed a balanced sphere loss to handle the class imbalanced problem."],"url":"http://arxiv.org/abs/2402.07452v1","category":"cs.CV"}
{"created":"2024-02-12 07:11:13","title":"AraSpider: Democratizing Arabic-to-SQL","abstract":"This study presents AraSpider, the first Arabic version of the Spider dataset, aimed at improving natural language processing (NLP) in the Arabic-speaking community. Four multilingual translation models were tested for their effectiveness in translating English to Arabic. Additionally, two models were assessed for their ability to generate SQL queries from Arabic text. The results showed that using back translation significantly improved the performance of both ChatGPT 3.5 and SQLCoder models, which are considered top performers on the Spider dataset. Notably, ChatGPT 3.5 demonstrated high-quality translation, while SQLCoder excelled in text-to-SQL tasks. The study underscores the importance of incorporating contextual schema and employing back translation strategies to enhance model performance in Arabic NLP tasks. Moreover, the provision of detailed methodologies for reproducibility and translation of the dataset into other languages highlights the research's commitment to promoting transparency and collaborative knowledge sharing in the field. Overall, these contributions advance NLP research, empower Arabic-speaking researchers, and enrich the global discourse on language comprehension and database interrogation.","sentences":["This study presents AraSpider, the first Arabic version of the Spider dataset, aimed at improving natural language processing (NLP) in the Arabic-speaking community.","Four multilingual translation models were tested for their effectiveness in translating English to Arabic.","Additionally, two models were assessed for their ability to generate SQL queries from Arabic text.","The results showed that using back translation significantly improved the performance of both ChatGPT 3.5 and SQLCoder models, which are considered top performers on the Spider dataset.","Notably, ChatGPT 3.5 demonstrated high-quality translation, while SQLCoder excelled in text-to-SQL tasks.","The study underscores the importance of incorporating contextual schema and employing back translation strategies to enhance model performance in Arabic NLP tasks.","Moreover, the provision of detailed methodologies for reproducibility and translation of the dataset into other languages highlights the research's commitment to promoting transparency and collaborative knowledge sharing in the field.","Overall, these contributions advance NLP research, empower Arabic-speaking researchers, and enrich the global discourse on language comprehension and database interrogation."],"url":"http://arxiv.org/abs/2402.07448v1","category":"cs.CL"}
{"created":"2024-02-12 07:04:07","title":"Spectral asymptotics for linear elasticity with mixed boundary conditions","abstract":"In this note, we shall show that the two-term spectral asymptotics for the operator of linear elasticity with mixed boundary conditions which were given by Capoferri and Mann in \\cite{CaMa-24} essentially are old well-known results due to T. Branson, P. Gilkey, B. {\\O}rsted and A. Pierzchalski in \\cite{BGOP}. In addition, we further point out that the so-called ``general formulae'' in \\cite{SaVa-97, CaFrLeVa-23} and the calculations for several examples in \\cite{CaMa-24, CaFrLeVa-23} are all wrong.","sentences":["In this note, we shall show that the two-term spectral asymptotics for the operator of linear elasticity with mixed boundary conditions which were given by Capoferri and Mann in \\cite{CaMa-24} essentially are old well-known results due to T. Branson, P. Gilkey, B. {\\O}rsted and A. Pierzchalski in \\cite{BGOP}.","In addition, we further point out that the so-called ``general formulae'' in \\cite{SaVa-97, CaFrLeVa-23} and the calculations for several examples in \\cite{CaMa-24, CaFrLeVa-23} are all wrong."],"url":"http://arxiv.org/abs/2402.07447v1","category":"math.AP"}
{"created":"2024-02-12 06:57:34","title":"Top-$K$ ranking with a monotone adversary","abstract":"In this paper, we address the top-$K$ ranking problem with a monotone adversary. We consider the scenario where a comparison graph is randomly generated and the adversary is allowed to add arbitrary edges. The statistician's goal is then to accurately identify the top-$K$ preferred items based on pairwise comparisons derived from this semi-random comparison graph. The main contribution of this paper is to develop a weighted maximum likelihood estimator (MLE) that achieves near-optimal sample complexity, up to a $\\log^2(n)$ factor, where n denotes the number of items under comparison. This is made possible through a combination of analytical and algorithmic innovations. On the analytical front, we provide a refined $\\ell_\\infty$ error analysis of the weighted MLE that is more explicit and tighter than existing analyses. It relates the $\\ell_\\infty$ error with the spectral properties of the weighted comparison graph. Motivated by this, our algorithmic innovation involves the development of an SDP-based approach to reweight the semi-random graph and meet specified spectral properties. Additionally, we propose a first-order method based on the Matrix Multiplicative Weight Update (MMWU) framework. This method efficiently solves the resulting SDP in nearly-linear time relative to the size of the semi-random comparison graph.","sentences":["In this paper, we address the top-$K$ ranking problem with a monotone adversary.","We consider the scenario where a comparison graph is randomly generated and the adversary is allowed to add arbitrary edges.","The statistician's goal is then to accurately identify the top-$K$ preferred items based on pairwise comparisons derived from this semi-random comparison graph.","The main contribution of this paper is to develop a weighted maximum likelihood estimator (MLE) that achieves near-optimal sample complexity, up to a $\\log^2(n)$ factor, where n denotes the number of items under comparison.","This is made possible through a combination of analytical and algorithmic innovations.","On the analytical front, we provide a refined $\\ell_\\infty$ error analysis of the weighted MLE that is more explicit and tighter than existing analyses.","It relates the $\\ell_\\infty$ error with the spectral properties of the weighted comparison graph.","Motivated by this, our algorithmic innovation involves the development of an SDP-based approach to reweight the semi-random graph and meet specified spectral properties.","Additionally, we propose a first-order method based on the Matrix Multiplicative Weight Update (MMWU) framework.","This method efficiently solves the resulting SDP in nearly-linear time relative to the size of the semi-random comparison graph."],"url":"http://arxiv.org/abs/2402.07445v1","category":"stat.ML"}
{"created":"2024-02-12 06:50:45","title":"The I/O Complexity of Attention, or How Optimal is Flash Attention?","abstract":"Self-attention is at the heart of the popular Transformer architecture, yet suffers from quadratic time and memory complexity. The breakthrough FlashAttention algorithm revealed I/O complexity as the true bottleneck in scaling Transformers. Given two levels of memory hierarchy, a fast cache (e.g. GPU on-chip SRAM) and a slow memory (e.g. GPU high-bandwidth memory), the I/O complexity measures the number of accesses to memory. FlashAttention computes attention using $\\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of the attention matrix, $d$ the head-dimension and $M$ the cache size. However, is this I/O complexity optimal? The known lower bound only rules out an I/O complexity of $o(Nd)$ when $M=\\Theta(Nd)$, since the output that needs to be written to slow memory is $\\Omega(Nd)$. This leads to the main question of our work: Is FlashAttention I/O optimal for all values of $M$?   We resolve the above question in its full generality by showing an I/O complexity lower bound that matches the upper bound provided by FlashAttention for any values of $M \\geq d^2$ within any constant factors. Further, we give a better algorithm with lower I/O complexity for $M < d^2$, and show that it is optimal as well. Moreover, our lower bounds do not rely on using combinatorial matrix multiplication for computing the attention matrix. We show even if one uses fast matrix multiplication, the above I/O complexity bounds cannot be improved. We do so by introducing a new communication complexity protocol for matrix compression, and connecting communication complexity to I/O complexity. To the best of our knowledge, this is the first work to establish a connection between communication complexity and I/O complexity, and we believe this connection could be of independent interest and will find many more applications in proving I/O complexity lower bounds in the future.","sentences":["Self-attention is at the heart of the popular Transformer architecture, yet suffers from quadratic time and memory complexity.","The breakthrough FlashAttention algorithm revealed I","/O complexity as the true bottleneck in scaling Transformers.","Given two levels of memory hierarchy, a fast cache (e.g. GPU on-chip SRAM) and a slow memory (e.g. GPU high-bandwidth memory), the I/O complexity measures the number of accesses to memory.","FlashAttention computes attention using $\\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of the attention matrix, $d$ the head-dimension and $M$ the cache size.","However, is this I/O complexity optimal?","The known lower bound only rules out an I/O complexity of $o(Nd)$ when $M=\\Theta(Nd)$, since the output that needs to be written to slow memory is $\\Omega(Nd)$. This leads to the main question of our work: Is FlashAttention I/O optimal for all values of $M$?   ","We resolve the above question in its full generality by showing an I/O complexity lower bound that matches the upper bound provided by FlashAttention for any values of $M \\geq d^2$ within any constant factors.","Further, we give a better algorithm with lower I/O complexity for $M < d^2$, and show that it is optimal as well.","Moreover, our lower bounds do not rely on using combinatorial matrix multiplication for computing the attention matrix.","We show even if one uses fast matrix multiplication, the above I/O complexity bounds cannot be improved.","We do so by introducing a new communication complexity protocol for matrix compression, and connecting communication complexity to I/O complexity.","To the best of our knowledge, this is the first work to establish a connection between communication complexity and I/O complexity, and we believe this connection could be of independent interest and will find many more applications in proving I/O complexity lower bounds in the future."],"url":"http://arxiv.org/abs/2402.07443v1","category":"cs.LG"}
{"created":"2024-02-12 06:49:48","title":"Game Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch","abstract":"Several attempts have been made to implement text command control for game agents. However, current technologies are limited to processing predefined format commands. This paper proposes a pioneering text command control system for a game agent that can understand natural language commands expressed in free-form. The proposed system uses a large language model (LLM) for code generation to interpret and transform natural language commands into behavior branch, a proposed knowledge expression based on behavior trees, which facilitates execution by the game agent. This study conducted empirical validation within a game environment that simulates a Pok\\'emon game and involved multiple participants. The results confirmed the system's ability to understand and carry out natural language commands, representing a noteworthy in the realm of real-time language interactive game agents.   Notice for the use of this material. The copyright of this material is retained by the Japanese Society for Artificial Intelligence (JSAI). This material is published here with the agreement of JSAI. Please be complied with Copyright Law of Japan if any users wish to reproduce, make derivative work, distribute or make available to the public any part or whole thereof. All Rights Reserved, Copyright (C) The Japanese Society for Artificial Intelligence.","sentences":["Several attempts have been made to implement text command control for game agents.","However, current technologies are limited to processing predefined format commands.","This paper proposes a pioneering text command control system for a game agent that can understand natural language commands expressed in free-form.","The proposed system uses a large language model (LLM) for code generation to interpret and transform natural language commands into behavior branch, a proposed knowledge expression based on behavior trees, which facilitates execution by the game agent.","This study conducted empirical validation within a game environment that simulates a Pok\\'emon game and involved multiple participants.","The results confirmed the system's ability to understand and carry out natural language commands, representing a noteworthy in the realm of real-time language interactive game agents.   ","Notice for the use of this material.","The copyright of this material is retained by the Japanese Society for Artificial Intelligence (JSAI).","This material is published here with the agreement of JSAI.","Please be complied with Copyright Law of Japan if any users wish to reproduce, make derivative work, distribute or make available to the public any part or whole thereof.","All Rights Reserved, Copyright (C)","The Japanese Society for Artificial Intelligence."],"url":"http://arxiv.org/abs/2402.07442v1","category":"cs.AI"}
{"created":"2024-02-12 06:48:57","title":"Fully Dynamic Geometric Vertex Cover and Matching","abstract":"In this work, we study two fundamental graph optimization problems, minimum vertex cover (MVC) and maximum-cardinality matching (MCM), for intersection graphs of geometric objects, e.g., disks, rectangles, hypercubes, etc., in $d$-dimensional Euclidean space. We consider the problems in fully dynamic settings, allowing insertions and deletions of objects.   We develop a general framework for dynamic MVC in intersection graphs, achieving sublinear amortized update time for most natural families of geometric objects. In particular, we show that -   \\begin{itemize}   \\item For a dynamic collection of disks in $\\mathbb{R}^2$ or hypercubes in $\\mathbb{R}^d$ (for constant $d$), it is possible to maintain a $(1+\\varepsilon)$-approximate vertex cover in $\\polylog$ amortized update time. These results also hold in the bipartite case.   \\item For a dynamic collection of rectangles in $\\mathbb{R}^2$, it is possible to maintain a $(\\frac{3}{2}+\\varepsilon)$-approximate vertex cover in $\\polylog$ amortized update time. \\end{itemize}   Along the way, we obtain the first near-linear time static algorithms for MVC in the above two cases with the same approximation factors.   Next, we turn our attention to the MCM problem. Although our MVC algorithms automatically allow us to approximate the size of the MCM in bipartite geometric intersection graphs, they do not produce a matching. We give another general framework to maintain an approximate maximum matching, and further extend the approach to handle non-bipartite intersection graphs. In particular, we show that -   \\begin{itemize} \\item For a dynamic collection of (bichromatic or monochromatic) disks in $\\mathbb{R}^2$ or hypercubes in $\\mathbb{R}^d$ (for constant $d$), it is possible to maintain a $(1+\\varepsilon)$-approximate matching in $\\polylog$ amortized update time. \\end{itemize}","sentences":["In this work, we study two fundamental graph optimization problems, minimum vertex cover (MVC) and maximum-cardinality matching (MCM), for intersection graphs of geometric objects, e.g., disks, rectangles, hypercubes, etc., in $d$-dimensional Euclidean space.","We consider the problems in fully dynamic settings, allowing insertions and deletions of objects.   ","We develop a general framework for dynamic MVC in intersection graphs, achieving sublinear amortized update time for most natural families of geometric objects.","In particular, we show that -   \\begin{itemize}   \\item For a dynamic collection of disks in $\\mathbb{R}^2$ or hypercubes in $\\mathbb{R}^d$ (for constant $d$), it is possible to maintain a $(1+\\varepsilon)$-approximate vertex cover in $\\polylog$ amortized update time.","These results also hold in the bipartite case.   ","\\item For a dynamic collection of rectangles in $\\mathbb{R}^2$, it is possible to maintain a $(\\frac{3}{2}+\\varepsilon)$-approximate vertex cover in $\\polylog$","amortized update time.","\\end{itemize}   Along the way, we obtain the first near-linear time static algorithms for MVC in the above two cases with the same approximation factors.   ","Next, we turn our attention to the MCM problem.","Although our MVC algorithms automatically allow us to approximate the size of the MCM in bipartite geometric intersection graphs, they do not produce a matching.","We give another general framework to maintain an approximate maximum matching, and further extend the approach to handle non-bipartite intersection graphs.","In particular, we show that -   \\begin{itemize} \\item For a dynamic collection of (bichromatic or monochromatic) disks in $\\mathbb{R}^2$ or hypercubes in $\\mathbb{R}^d$ (for constant $d$), it is possible to maintain a $(1+\\varepsilon)$-approximate matching in $\\polylog$ amortized update time.","\\end{itemize}"],"url":"http://arxiv.org/abs/2402.07441v1","category":"cs.CG"}
{"created":"2024-02-12 06:38:44","title":"Joint estimation of the predictive ability of experts using a multi-output Gaussian process","abstract":"A multi-output Gaussian process (GP) is introduced as a model for the joint posterior distribution of the local predictive ability of set of models and/or experts, conditional on a vector of covariates, from historical predictions in the form of log predictive scores. Following a power transformation of the log scores, a GP with Gaussian noise can be used, which allows faster computation by first using Hamiltonian Monte Carlo to sample the hyper-parameters of the GP from a model where the latent GP surface has been marginalized out, and then using these draws to generate draws of joint predictive ability conditional on a new vector of covariates. Linear pools based on learned joint local predictive ability are applied to predict daily bike usage in Washington DC.","sentences":["A multi-output Gaussian process (GP) is introduced as a model for the joint posterior distribution of the local predictive ability of set of models and/or experts, conditional on a vector of covariates, from historical predictions in the form of log predictive scores.","Following a power transformation of the log scores, a GP with Gaussian noise can be used, which allows faster computation by first using Hamiltonian Monte Carlo to sample the hyper-parameters of the GP from a model where the latent GP surface has been marginalized out, and then using these draws to generate draws of joint predictive ability conditional on a new vector of covariates.","Linear pools based on learned joint local predictive ability are applied to predict daily bike usage in Washington DC."],"url":"http://arxiv.org/abs/2402.07439v1","category":"stat.ME"}
{"created":"2024-02-12 06:38:36","title":"The Powell Conjecture for the genus-three Heegaard splitting of the $3$-sphere","abstract":"The Powell Conjecture states that four specific elements suffice to generate the Goeritz group of the Heegaard splitting of the $3$-sphere. We present an alternative proof of the Powell Conjecture when the genus of the splitting is $3$, and suggest a strategy for the case of higher genera.","sentences":["The Powell Conjecture states that four specific elements suffice to generate the Goeritz group of the Heegaard splitting of the $3$-sphere.","We present an alternative proof of the Powell Conjecture when the genus of the splitting is $3$, and suggest a strategy for the case of higher genera."],"url":"http://arxiv.org/abs/2402.07438v1","category":"math.GT"}
{"created":"2024-02-12 06:32:53","title":"Learning Optimal Tax Design in Nonatomic Congestion Games","abstract":"We study how to learn the optimal tax design to maximize the efficiency in nonatomic congestion games. It is known that self-interested behavior among the players can damage the system's efficiency. Tax mechanisms is a common method to alleviate this issue and induce socially optimal behavior. In this work, we take the initial step for learning the optimal tax that can minimize the social cost with \\emph{equilibrium feedback}, i.e., the tax designer can only observe the equilibrium state under the enforced tax. Existing algorithms are not applicable due to the exponentially large tax function space, nonexistence of the gradient, and nonconvexity of the objective. To tackle these challenges, our algorithm leverages several novel components: (1) piece-wise linear tax to approximate the optimal tax; (2) an extra linear term to guarantee a strongly convex potential function; (3) efficient subroutine to find the ``boundary'' tax. The algorithm can find an $\\epsilon$-optimal tax with $O(\\beta F^2/\\epsilon)$ sample complexity, where $\\beta$ is the smoothness of the cost function and $F$ is the number of facilities.","sentences":["We study how to learn the optimal tax design to maximize the efficiency in nonatomic congestion games.","It is known that self-interested behavior among the players can damage the system's efficiency.","Tax mechanisms is a common method to alleviate this issue and induce socially optimal behavior.","In this work, we take the initial step for learning the optimal tax that can minimize the social cost with \\emph{equilibrium feedback}, i.e., the tax designer can only observe the equilibrium state under the enforced tax.","Existing algorithms are not applicable due to the exponentially large tax function space, nonexistence of the gradient, and nonconvexity of the objective.","To tackle these challenges, our algorithm leverages several novel components: (1) piece-wise linear tax to approximate the optimal tax; (2) an extra linear term to guarantee a strongly convex potential function; (3) efficient subroutine to find the ``boundary'' tax.","The algorithm can find an $\\epsilon$-optimal tax with $O(\\beta F^2/\\epsilon)$ sample complexity, where $\\beta$ is the smoothness of the cost function and $F$ is the number of facilities."],"url":"http://arxiv.org/abs/2402.07437v1","category":"cs.GT"}
{"created":"2024-02-12 06:32:00","title":"Novel definition and quantitative analysis of branch structure with topological data analysis","abstract":"While branching network structures abound in nature, their objective analysis is more difficult than expected because existing quantitative methods often rely on the subjective judgment of branch structures. This problem is particularly pronounced when dealing with images comprising discrete particles. Here we propose an objective framework for quantitative analysis of branching networks by introducing the mathematical definitions for internal and external structures based on topological data analysis, specifically, persistent homology. We compare persistence diagrams constructed from images with and without plots on the convex hull. The unchanged points in the two diagrams are the internal structures and the difference between the two diagrams is the external structures. We construct a mathematical theory for our method and show that the internal structures have a monotonicity relationship with respect to the plots on the convex hull, while the external structures do not. This is the phenomenon related to the resolution of the image. Our method can be applied to a wide range of branch structures in biology, enabling objective analysis of numbers, spatial distributions, sizes, and more. Additionally, our method has the potential to be combined with other tools in topological data analysis, such as the generalized persistence landscape.","sentences":["While branching network structures abound in nature, their objective analysis is more difficult than expected because existing quantitative methods often rely on the subjective judgment of branch structures.","This problem is particularly pronounced when dealing with images comprising discrete particles.","Here we propose an objective framework for quantitative analysis of branching networks by introducing the mathematical definitions for internal and external structures based on topological data analysis, specifically, persistent homology.","We compare persistence diagrams constructed from images with and without plots on the convex hull.","The unchanged points in the two diagrams are the internal structures and the difference between the two diagrams is the external structures.","We construct a mathematical theory for our method and show that the internal structures have a monotonicity relationship with respect to the plots on the convex hull, while the external structures do not.","This is the phenomenon related to the resolution of the image.","Our method can be applied to a wide range of branch structures in biology, enabling objective analysis of numbers, spatial distributions, sizes, and more.","Additionally, our method has the potential to be combined with other tools in topological data analysis, such as the generalized persistence landscape."],"url":"http://arxiv.org/abs/2402.07436v1","category":"math.AT"}
{"created":"2024-02-12 06:29:57","title":"Analyzing Currency Fluctuations: A Comparative Study of GARCH, EWMA, and IV Models for GBP/USD and EUR/GBP Pairs","abstract":"In this study, we examine the fluctuation in the value of the Great Britain Pound (GBP). We focus particularly on its relationship with the United States Dollar (USD) and the Euro (EUR) currency pairs. Utilizing data from June 15, 2018, to June 15, 2023, we apply various mathematical models to assess their effectiveness in predicting the 20-day variation in the pairs' daily returns. Our analysis involves the implementation of Exponentially Weighted Moving Average (EWMA), Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models, and Implied Volatility (IV) models. To evaluate their performance, we compare the accuracy of their predictions using Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) metrics. We delve into the intricacies of GARCH models, examining their statistical characteristics when applied to the provided dataset. Our findings suggest the existence of asymmetric returns in the EUR/GBP pair, while such evidence is inconclusive for the GBP/USD pair. Additionally, we observe that GARCH-type models better fit the data when assuming residuals follow a standard t-distribution rather than a standard normal distribution. Furthermore, we investigate the efficacy of different forecasting techniques within GARCH-type models. Comparing rolling window forecasts to expanding window forecasts, we find no definitive superiority in either approach across the tested scenarios. Our experiments reveal that for the GBP/USD pair, the most accurate volatility forecasts stem from the utilization of GARCH models employing a rolling window methodology. Conversely, for the EUR/GBP pair, optimal forecasts are derived from GARCH models and Ordinary Least Squares (OLS) models incorporating the annualized implied volatility of the exchange rate as an independent variable.","sentences":["In this study, we examine the fluctuation in the value of the Great Britain Pound (GBP).","We focus particularly on its relationship with the United States Dollar (USD) and the Euro (EUR) currency pairs.","Utilizing data from June 15, 2018, to June 15, 2023, we apply various mathematical models to assess their effectiveness in predicting the 20-day variation in the pairs' daily returns.","Our analysis involves the implementation of Exponentially Weighted Moving Average (EWMA), Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models, and Implied Volatility (IV) models.","To evaluate their performance, we compare the accuracy of their predictions using Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) metrics.","We delve into the intricacies of GARCH models, examining their statistical characteristics when applied to the provided dataset.","Our findings suggest the existence of asymmetric returns in the EUR/GBP pair, while such evidence is inconclusive for the GBP/USD pair.","Additionally, we observe that GARCH-type models better fit the data when assuming residuals follow a standard t-distribution rather than a standard normal distribution.","Furthermore, we investigate the efficacy of different forecasting techniques within GARCH-type models.","Comparing rolling window forecasts to expanding window forecasts, we find no definitive superiority in either approach across the tested scenarios.","Our experiments reveal that for the GBP/USD pair, the most accurate volatility forecasts stem from the utilization of GARCH models employing a rolling window methodology.","Conversely, for the EUR/GBP pair, optimal forecasts are derived from GARCH models and Ordinary Least Squares (OLS) models incorporating the annualized implied volatility of the exchange rate as an independent variable."],"url":"http://arxiv.org/abs/2402.07435v1","category":"q-fin.ST"}
{"created":"2024-02-12 06:21:35","title":"Intrinsic Task-based Evaluation for Referring Expression Generation","abstract":"Recently, a human evaluation study of Referring Expression Generation (REG) models had an unexpected conclusion: on \\textsc{webnlg}, Referring Expressions (REs) generated by the state-of-the-art neural models were not only indistinguishable from the REs in \\textsc{webnlg} but also from the REs generated by a simple rule-based system. Here, we argue that this limitation could stem from the use of a purely ratings-based human evaluation (which is a common practice in Natural Language Generation). To investigate these issues, we propose an intrinsic task-based evaluation for REG models, in which, in addition to rating the quality of REs, participants were asked to accomplish two meta-level tasks. One of these tasks concerns the referential success of each RE; the other task asks participants to suggest a better alternative for each RE. The outcomes suggest that, in comparison to previous evaluations, the new evaluation protocol assesses the performance of each REG model more comprehensively and makes the participants' ratings more reliable and discriminable.","sentences":["Recently, a human evaluation study of Referring Expression Generation (REG) models had an unexpected conclusion: on \\textsc{webnlg}, Referring Expressions (REs) generated by the state-of-the-art neural models were not only indistinguishable from the REs in \\textsc{webnlg} but also from the REs generated by a simple rule-based system.","Here, we argue that this limitation could stem from the use of a purely ratings-based human evaluation (which is a common practice in Natural Language Generation).","To investigate these issues, we propose an intrinsic task-based evaluation for REG models, in which, in addition to rating the quality of REs, participants were asked to accomplish two meta-level tasks.","One of these tasks concerns the referential success of each RE; the other task asks participants to suggest a better alternative for each RE.","The outcomes suggest that, in comparison to previous evaluations, the new evaluation protocol assesses the performance of each REG model more comprehensively and makes the participants' ratings more reliable and discriminable."],"url":"http://arxiv.org/abs/2402.07432v1","category":"cs.CL"}
{"created":"2024-02-12 06:15:24","title":"SALAD: Smart AI Language Assistant Daily","abstract":"SALAD is an AI-driven language-learning application designed to help foreigners learn Japanese. It offers translations in Kanji-Kana-Romaji, speech recognition, translated audio, vocabulary tracking, grammar explanations, and songs generated from newly learned words. The app targets beginners and intermediate learners, aiming to make language acquisition more accessible and enjoyable. SALAD uses daily translations to enhance fluency and comfort in communication with native speakers. The primary objectives include effective Japanese language learning, user engagement, and progress tracking. A survey by us found that 39% of foreigners in Japan face discomfort in conversations with Japanese speakers. Over 60% of foreigners expressed confidence in SALAD's ability to enhance their Japanese language skills. The app uses large language models, speech recognition, and diffusion models to bridge the language gap and foster a more inclusive community in Japan.","sentences":["SALAD is an AI-driven language-learning application designed to help foreigners learn Japanese.","It offers translations in Kanji-Kana-Romaji, speech recognition, translated audio, vocabulary tracking, grammar explanations, and songs generated from newly learned words.","The app targets beginners and intermediate learners, aiming to make language acquisition more accessible and enjoyable.","SALAD uses daily translations to enhance fluency and comfort in communication with native speakers.","The primary objectives include effective Japanese language learning, user engagement, and progress tracking.","A survey by us found that 39% of foreigners in Japan face discomfort in conversations with Japanese speakers.","Over 60% of foreigners expressed confidence in SALAD's ability to enhance their Japanese language skills.","The app uses large language models, speech recognition, and diffusion models to bridge the language gap and foster a more inclusive community in Japan."],"url":"http://arxiv.org/abs/2402.07431v1","category":"cs.CL"}
{"created":"2024-02-12 06:06:09","title":"Particle Filter SLAM for Vehicle Localization","abstract":"Simultaneous Localization and Mapping (SLAM) presents a formidable challenge in robotics, involving the dynamic construction of a map while concurrently determining the precise location of the robotic agent within an unfamiliar environment. This intricate task is further compounded by the inherent \"chicken-and-egg\" dilemma, where accurate mapping relies on a dependable estimation of the robot's location, and vice versa. Moreover, the computational intensity of SLAM adds an additional layer of complexity, making it a crucial yet demanding topic in the field. In our research, we address the challenges of SLAM by adopting the Particle Filter SLAM method. Our approach leverages encoded data and fiber optic gyro (FOG) information to enable precise estimation of vehicle motion, while lidar technology contributes to environmental perception by providing detailed insights into surrounding obstacles. The integration of these data streams culminates in the establishment of a Particle Filter SLAM framework, representing a key endeavor in this paper to effectively navigate and overcome the complexities associated with simultaneous localization and mapping in robotic systems.","sentences":["Simultaneous Localization and Mapping (SLAM) presents a formidable challenge in robotics, involving the dynamic construction of a map while concurrently determining the precise location of the robotic agent within an unfamiliar environment.","This intricate task is further compounded by the inherent \"chicken-and-egg\" dilemma, where accurate mapping relies on a dependable estimation of the robot's location, and vice versa.","Moreover, the computational intensity of SLAM adds an additional layer of complexity, making it a crucial yet demanding topic in the field.","In our research, we address the challenges of SLAM by adopting the Particle Filter SLAM method.","Our approach leverages encoded data and fiber optic gyro (FOG) information to enable precise estimation of vehicle motion, while lidar technology contributes to environmental perception by providing detailed insights into surrounding obstacles.","The integration of these data streams culminates in the establishment of a Particle Filter SLAM framework, representing a key endeavor in this paper to effectively navigate and overcome the complexities associated with simultaneous localization and mapping in robotic systems."],"url":"http://arxiv.org/abs/2402.07429v1","category":"cs.AI"}
{"created":"2024-02-12 06:05:13","title":"Insights into Spatio-temporal dynamics during shock -- droplet flame interaction","abstract":"The study comprehensively investigates the response of a combusting droplet during its interaction with a high-speed transient flow that is imposed by a coaxially propagating blast wave. The blast wave is generated using a specially designed unique miniature shock generation apparatus that generates blast waves using the wire-explosion technique which facilitates a wide range of shock Mach number (1.03 < Ms < 1.8). The experiments are performed in two configurations: Open field blast wave and focused blast wave. The charging voltage and the configuration determine the shock Mach number (Ms) and flow characteristics. The flame is found to exhibit two major response patterns: partial extinction followed by re-ignition and full extinction. Simultaneously, the droplet also interacts with the flow imposed by the blast wave exhibiting different modes of response ranging from pure deformation, Rayleigh-Taylor piercing bag breakup, and shear-induced stripping. The KH instability is exhibited along the windward side interface of the droplet during the interaction with the blast wave decay profile which gets aggravated when the induced flow interaction ensues. Increasing the Mach number (Ms > 1.1) makes the droplet flame more vulnerable to extinction. However, the flame exhibits stretching and shedding, followed by re-ignition at lower Mach numbers (Ms < 1.06). In all cases, the flame base lifts off in response to the imposed flow, and the advection of the flame base interacting with the flame tip results in flame extinction. The entire interaction occurs in two stages: 1) interaction with the blast wave and the decaying velocity profile associated with it, and 2) interaction with the induced flow behind the blast wave as a result of the entrainment (delayed response). The criteria for partial and complete extinction of flame have been postulated which is in good agreement with the experiments.","sentences":["The study comprehensively investigates the response of a combusting droplet during its interaction with a high-speed transient flow that is imposed by a coaxially propagating blast wave.","The blast wave is generated using a specially designed unique miniature shock generation apparatus that generates blast waves using the wire-explosion technique which facilitates a wide range of shock Mach number (1.03 <","Ms < 1.8).","The experiments are performed in two configurations: Open field blast wave and focused blast wave.","The charging voltage and the configuration determine the shock Mach number (Ms) and flow characteristics.","The flame is found to exhibit two major response patterns: partial extinction followed by re-ignition and full extinction.","Simultaneously, the droplet also interacts with the flow imposed by the blast wave exhibiting different modes of response ranging from pure deformation, Rayleigh-Taylor piercing bag breakup, and shear-induced stripping.","The KH instability is exhibited along the windward side interface of the droplet during the interaction with the blast wave decay profile which gets aggravated when the induced flow interaction ensues.","Increasing the Mach number (Ms > 1.1) makes the droplet flame more vulnerable to extinction.","However, the flame exhibits stretching and shedding, followed by re-ignition at lower Mach numbers (Ms < 1.06).","In all cases, the flame base lifts off in response to the imposed flow, and the advection of the flame base interacting with the flame tip results in flame extinction.","The entire interaction occurs in two stages: 1) interaction with the blast wave and the decaying velocity profile associated with it, and 2) interaction with the induced flow behind the blast wave as a result of the entrainment (delayed response).","The criteria for partial and complete extinction of flame have been postulated which is in good agreement with the experiments."],"url":"http://arxiv.org/abs/2402.07427v1","category":"physics.flu-dyn"}
{"created":"2024-02-12 18:58:47","title":"CHIME/FRB Outriggers: KKO Station System and Commissioning Results","abstract":"Localizing fast radio bursts (FRBs) to their host galaxies is an essential step to better understanding their origins and using them as cosmic probes. The CHIME/FRB Outrigger program aims to add VLBI-localization capabilities to CHIME, such that FRBs may be localized to tens of milliarcsecond precision at the time of their discovery, more than sufficient for host galaxy identification. The first-built outrigger telescope is KKO, located 66 kilometers west of CHIME. Cross-correlating KKO with CHIME can achieve arcsecond-scale localization in right ascension while avoiding the worst effects of the ionosphere. This paper presents measurements of KKO's performance throughout its commissioning phase, as well as a summary of its design and function. We demonstrate KKO's capabilities as a standalone instrument by producing full-sky images, mapping the angular and frequency structure of the primary beam, and measuring feed positions. To demonstrate the localization capabilities of the CHIME -- KKO baseline, we collected five separate observations each for a set of twenty bright pulsars, and aimed to measure their positions to within 5~arcseconds. All of these pulses were successfully localized to within this specification. The next two outriggers are expected to be commissioned in 2024, and will enable subarcsecond localizations for approximately hundreds of FRBs each year.","sentences":["Localizing fast radio bursts (FRBs) to their host galaxies is an essential step to better understanding their origins and using them as cosmic probes.","The CHIME/FRB Outrigger program aims to add VLBI-localization capabilities to CHIME, such that FRBs may be localized to tens of milliarcsecond precision at the time of their discovery, more than sufficient for host galaxy identification.","The first-built outrigger telescope is KKO, located 66 kilometers west of CHIME.","Cross-correlating KKO with CHIME can achieve arcsecond-scale localization in right ascension while avoiding the worst effects of the ionosphere.","This paper presents measurements of KKO's performance throughout its commissioning phase, as well as a summary of its design and function.","We demonstrate KKO's capabilities as a standalone instrument by producing full-sky images, mapping the angular and frequency structure of the primary beam, and measuring feed positions.","To demonstrate the localization capabilities of the CHIME -- KKO baseline, we collected five separate observations each for a set of twenty bright pulsars, and aimed to measure their positions to within 5~arcseconds.","All of these pulses were successfully localized to within this specification.","The next two outriggers are expected to be commissioned in 2024, and will enable subarcsecond localizations for approximately hundreds of FRBs each year."],"url":"http://arxiv.org/abs/2402.07898v1","category":"astro-ph.IM"}
{"created":"2024-02-12 17:51:22","title":"Creating pair plasmas with observable collective effects","abstract":"Although existing technology cannot yet directly produce fields at the Schwinger level, experimental facilities can already explore strong-field QED phenomena by taking advantage of the Lorentz boost of energetic electron beams. Recent studies show that QED cascades can create electron-positron pairs at sufficiently high density to exhibit collective plasma effects. Signatures of the collective pair plasma effects can appear in exquisite detail through plasma-induced frequency upshifts and chirps in the laser spectrum. Maximizing the magnitude of the QED plasma signature demands high pair density and low pair energy, which suits the configuration of colliding an over $10^{18}{Jm^{-3}}$ energy-density electron beam with a $10^{22}\\mathrm{-}10^{23}{Wcm^{-2}}$ intensity laser pulse. The collision creates pairs that have a large plasma frequency, made even larger as they slow down or reverse direction due to both the radiation reaction and laser pressure. This paper explains at a tutorial level the key properties of the QED cascades and laser frequency upshift, and at the same time finds the minimum parameters that can be used to produce observable QED plasma.","sentences":["Although existing technology cannot yet directly produce fields at the Schwinger level, experimental facilities can already explore strong-field QED phenomena by taking advantage of the Lorentz boost of energetic electron beams.","Recent studies show that QED cascades can create electron-positron pairs at sufficiently high density to exhibit collective plasma effects.","Signatures of the collective pair plasma effects can appear in exquisite detail through plasma-induced frequency upshifts and chirps in the laser spectrum.","Maximizing the magnitude of the QED plasma signature demands high pair density and low pair energy, which suits the configuration of colliding an over $10^{18}{Jm^{-3}}$ energy-density electron beam with a $10^{22}\\mathrm{-}10^{23}{Wcm^{-2}}$ intensity laser pulse.","The collision creates pairs that have a large plasma frequency, made even larger as they slow down or reverse direction due to both the radiation reaction and laser pressure.","This paper explains at a tutorial level the key properties of the QED cascades and laser frequency upshift, and at the same time finds the minimum parameters that can be used to produce observable QED plasma."],"url":"http://arxiv.org/abs/2402.07840v1","category":"physics.plasm-ph"}
{"created":"2024-02-12 17:47:15","title":"2D MoS2 under switching field conditions: study of high-frequency noise from velocity fluctuations","abstract":"The transient high-frequency noise response of two-dimensional MoS2 under abrupt large signal switching field conditions is studied by means of an ensemble Monte Carlo simulator. Low-to-high and high-to-low transitions are analyzed at low (77 K) and room temperature, considering several underlying substrates. The incorporation of stochastic individual scattering events allows capturing the transient collective phonon-electron coupling, which is shown to be responsible for the appearance of an oscillatory behaviour in the average velocity and energy at low temperature in the case of MoS2 on SiO2, hBN and Al2O3. Activation and deactivation of surface polar phonon emissions in the low-to-high field switching process yield to the appearance of a relevant peak in the power spectral density of velocity fluctuations in the THz range. The results show the important influence of the substrate type in the noise behaviour of MoS2 at very high frequencies, which is critical for the design of future FET devices based on 2D TMD technology.","sentences":["The transient high-frequency noise response of two-dimensional MoS2 under abrupt large signal switching field conditions is studied by means of an ensemble Monte Carlo simulator.","Low-to-high and high-to-low transitions are analyzed at low (77 K) and room temperature, considering several underlying substrates.","The incorporation of stochastic individual scattering events allows capturing the transient collective phonon-electron coupling, which is shown to be responsible for the appearance of an oscillatory behaviour in the average velocity and energy at low temperature in the case of MoS2 on SiO2, hBN and Al2O3.","Activation and deactivation of surface polar phonon emissions in the low-to-high field switching process yield to the appearance of a relevant peak in the power spectral density of velocity fluctuations in the THz range.","The results show the important influence of the substrate type in the noise behaviour of MoS2 at very high frequencies, which is critical for the design of future FET devices based on 2D TMD technology."],"url":"http://arxiv.org/abs/2402.07838v1","category":"physics.app-ph"}
{"created":"2024-02-12 17:24:35","title":"A Benchmark Grocery Dataset of Realworld Point Clouds From Single View","abstract":"Fine-grained grocery object recognition is an important computer vision problem with broad applications in automatic checkout, in-store robotic navigation, and assistive technologies for the visually impaired. Existing datasets on groceries are mainly 2D images. Models trained on these datasets are limited to learning features from the regular 2D grids. While portable 3D sensors such as Kinect were commonly available for mobile phones, sensors such as LiDAR and TrueDepth, have recently been integrated into mobile phones. Despite the availability of mobile 3D sensors, there are currently no dedicated real-world large-scale benchmark 3D datasets for grocery. In addition, existing 3D datasets lack fine-grained grocery categories and have limited training samples. Furthermore, collecting data by going around the object versus the traditional photo capture makes data collection cumbersome. Thus, we introduce a large-scale grocery dataset called 3DGrocery100. It constitutes 100 classes, with a total of 87,898 3D point clouds created from 10,755 RGB-D single-view images. We benchmark our dataset on six recent state-of-the-art 3D point cloud classification models. Additionally, we also benchmark the dataset on few-shot and continual learning point cloud classification tasks. Project Page: https://bigdatavision.org/3DGrocery100/.","sentences":["Fine-grained grocery object recognition is an important computer vision problem with broad applications in automatic checkout, in-store robotic navigation, and assistive technologies for the visually impaired.","Existing datasets on groceries are mainly 2D images.","Models trained on these datasets are limited to learning features from the regular 2D grids.","While portable 3D sensors such as Kinect were commonly available for mobile phones, sensors such as LiDAR and TrueDepth, have recently been integrated into mobile phones.","Despite the availability of mobile 3D sensors, there are currently no dedicated real-world large-scale benchmark 3D datasets for grocery.","In addition, existing 3D datasets lack fine-grained grocery categories and have limited training samples.","Furthermore, collecting data by going around the object versus the traditional photo capture makes data collection cumbersome.","Thus, we introduce a large-scale grocery dataset called 3DGrocery100.","It constitutes 100 classes, with a total of 87,898 3D point clouds created from 10,755 RGB-D single-view images.","We benchmark our dataset on six recent state-of-the-art 3D point cloud classification models.","Additionally, we also benchmark the dataset on few-shot and continual learning point cloud classification tasks.","Project Page: https://bigdatavision.org/3DGrocery100/."],"url":"http://arxiv.org/abs/2402.07819v1","category":"cs.CV"}
{"created":"2024-02-12 16:49:37","title":"Cygnus OB2 as a test case for particle acceleration in young massive star clusters","abstract":"In this paper, we focus on the scientific case of Cygnus OB2, a northern sky young massive stellar cluster (YMSC) located towards the Cygnus X star-forming complex. We consider a model that assumes cosmic ray acceleration occurring only at the termination shock of the collective wind of the YMSC and address the question of whether, and under what hypotheses, hadronic emission by the accelerated particles can account for the observations of Cygnus OB2 obtained by Fermi-LAT, HAWC and LHAASO. In order to do so, we carefully review the available information on this source, also confronting different estimates of the relevant parameters with ad hoc developed simulations. Once other model parameters are fixed, the spectral and spatial properties of the emission are found to be very sensitive to the unknown properties of the turbulent magnetic field. Comparison with the data shows that our suggested scenario is incompatible with Kolmogorov turbulence. Assuming Kraichnan or Bohm type turbulence spectra, the model accounts well for the Very High Energy (VHE) data, but fails to reproduce the centrally peaked morphology observed by Fermi-LAT, suggesting that additional effects might be important for lower energy $\\gamma$-ray emission. We discuss how additional progress can be made with a more detailed and extended knowledge of the spectral and morphological properties of the emission.","sentences":["In this paper, we focus on the scientific case of Cygnus OB2, a northern sky young massive stellar cluster (YMSC) located towards the Cygnus X star-forming complex.","We consider a model that assumes cosmic ray acceleration occurring only at the termination shock of the collective wind of the YMSC and address the question of whether, and under what hypotheses, hadronic emission by the accelerated particles can account for the observations of Cygnus OB2 obtained by Fermi-LAT, HAWC and LHAASO.","In order to do so, we carefully review the available information on this source, also confronting different estimates of the relevant parameters with ad hoc developed simulations.","Once other model parameters are fixed, the spectral and spatial properties of the emission are found to be very sensitive to the unknown properties of the turbulent magnetic field.","Comparison with the data shows that our suggested scenario is incompatible with Kolmogorov turbulence.","Assuming Kraichnan or Bohm type turbulence spectra, the model accounts well for the Very High Energy (VHE) data, but fails to reproduce the centrally peaked morphology observed by Fermi-LAT, suggesting that additional effects might be important for lower energy $\\gamma$-ray emission.","We discuss how additional progress can be made with a more detailed and extended knowledge of the spectral and morphological properties of the emission."],"url":"http://arxiv.org/abs/2402.07784v1","category":"astro-ph.HE"}
{"created":"2024-02-12 16:43:55","title":"Algorithmic Fairness and Color-blind Racism: Navigating the Intersection","abstract":"Our focus lies at the intersection between two broader research perspectives: (1) the scientific study of algorithms and (2) the scholarship on race and racism. Many streams of research related to algorithmic fairness have been born out of interest at this intersection. We think about this intersection as the product of work derived from both sides. From (1) algorithms to (2) racism, the starting place might be an algorithmic question or method connected to a conceptualization of racism. On the other hand, from (2) racism to (1) algorithms, the starting place could be recognizing a setting where a legacy of racism is known to persist and drawing connections between that legacy and the introduction of algorithms into this setting. In either direction, meaningful disconnection can occur when conducting research at the intersection of racism and algorithms. The present paper urges collective reflection on research directions at this intersection. Despite being primarily motivated by instances of racial bias, research in algorithmic fairness remains mostly disconnected from scholarship on racism. In particular, there has not been an examination connecting algorithmic fairness discussions directly to the ideology of color-blind racism; we aim to fill this gap. We begin with a review of an essential account of color-blind racism then we review racial discourse within algorithmic fairness research and underline significant patterns, shifts and disconnects. Ultimately, we argue that researchers can improve the navigation of the landscape at the intersection by recognizing ideological shifts as such and iteratively re-orienting towards maintaining meaningful connections across interdisciplinary lines.","sentences":["Our focus lies at the intersection between two broader research perspectives: (1) the scientific study of algorithms and (2) the scholarship on race and racism.","Many streams of research related to algorithmic fairness have been born out of interest at this intersection.","We think about this intersection as the product of work derived from both sides.","From (1) algorithms to (2) racism, the starting place might be an algorithmic question or method connected to a conceptualization of racism.","On the other hand, from (2) racism to (1) algorithms, the starting place could be recognizing a setting where a legacy of racism is known to persist and drawing connections between that legacy and the introduction of algorithms into this setting.","In either direction, meaningful disconnection can occur when conducting research at the intersection of racism and algorithms.","The present paper urges collective reflection on research directions at this intersection.","Despite being primarily motivated by instances of racial bias, research in algorithmic fairness remains mostly disconnected from scholarship on racism.","In particular, there has not been an examination connecting algorithmic fairness discussions directly to the ideology of color-blind racism; we aim to fill this gap.","We begin with a review of an essential account of color-blind racism then we review racial discourse within algorithmic fairness research and underline significant patterns, shifts and disconnects.","Ultimately, we argue that researchers can improve the navigation of the landscape at the intersection by recognizing ideological shifts as such and iteratively re-orienting towards maintaining meaningful connections across interdisciplinary lines."],"url":"http://arxiv.org/abs/2402.07778v1","category":"cs.CY"}
{"created":"2024-02-12 16:35:02","title":"The Goodwillie calculus of polyhedral products","abstract":"We describe the Goodwillie calculus of polyhedral products in the case that the fat wedge filtration on the associated real moment-angle complex is trivial. We do this by analysing the behaviour on calculus of the Denham-Suciu fibre sequence, the Iriye-Kishimoto decomposition of the polyhedral product constructed from a collection of pairs of cones and their bases, and the Hilton-Milnor decomposition. As a corollary we show that the Goodwillie calculus of these polyhedral products converges integrally and diverges in $v_h$-periodic homotopy unless the simplicial complex is a full simplex.","sentences":["We describe the Goodwillie calculus of polyhedral products in the case that the fat wedge filtration on the associated real moment-angle complex is trivial.","We do this by analysing the behaviour on calculus of the Denham-Suciu fibre sequence, the Iriye-Kishimoto decomposition of the polyhedral product constructed from a collection of pairs of cones and their bases, and the Hilton-Milnor decomposition.","As a corollary we show that the Goodwillie calculus of these polyhedral products converges integrally and diverges in $v_h$-periodic homotopy unless the simplicial complex is a full simplex."],"url":"http://arxiv.org/abs/2402.07774v1","category":"math.AT"}
{"created":"2024-02-12 16:28:42","title":"Decay rate measurements $^{137}$Cs at J\u00e1nossy Underground Research Laboratory","abstract":"The question whether an annual modulation is observable during nuclear decay rate measurements has long been the subject of research. One of the possible explanations for the annual variations would be the effect of solar neutrinos, the flux of which changes in correlation with the Earth-Sun distance. A decay rate measurement with a $^{137}$Cs source and a HPGe detector is currently being conducted 30 meters below the ground at J\\'anossy Underground Research Laboratory (Csilleb\\'erc, Hungary). The laboratory is part of the Vesztergombi High Energy Laboratory (VLAB), one of the TOP 50 research infrastructures in Hungary. From October 2022 to March 2023, data of six months' worth has been collected, and hence this is a new opportunity to check whether the annual variation in decay rate can be observed. The laboratory, the experiment, the data processing method, and the first results are presented in this study.","sentences":["The question whether an annual modulation is observable during nuclear decay rate measurements has long been the subject of research.","One of the possible explanations for the annual variations would be the effect of solar neutrinos, the flux of which changes in correlation with the Earth-Sun distance.","A decay rate measurement with a $^{137}$Cs source and a HPGe detector is currently being conducted 30 meters below the ground at J\\'anossy Underground Research Laboratory (Csilleb\\'erc, Hungary).","The laboratory is part of the Vesztergombi High Energy Laboratory (VLAB), one of the TOP 50 research infrastructures in Hungary.","From October 2022 to March 2023, data of six months' worth has been collected, and hence this is a new opportunity to check whether the annual variation in decay rate can be observed.","The laboratory, the experiment, the data processing method, and the first results are presented in this study."],"url":"http://arxiv.org/abs/2402.07761v1","category":"nucl-ex"}
{"created":"2024-02-12 10:00:10","title":"Semantic Data for Humanities and Social Sciences (SDHSS): an Ecosystem of CIDOC CRM Extensions for Research Data Production and Reuse","abstract":"Given the challenge of giant knowledge graphs created by major eco-nomic actors, which could virtually replace research in the Humani-ties and Social Sciences (HSS) in responding to public concerns, thequestion arises of how to increase the value of research data throughtheir publication and networking, applying the FAIR principles. Bothan epistemological and a semantic analysis show that the most rel-evant part of research data is factual information, understood as arepresentation of the objects observed by the scientific disciplines,their properties and their relationships.This rich universe of information will be made understandable andtherefore reusable through the application of foundational ontologiesand a methodology based on the distinction between different levelsof abstraction, allowing the collective development of one or moreshared and reusable domain ontologies. This vision is being carriedout around the CIDOC CRM, as core ontology, and Semantic Datafor Humanities and Social Sciences (SDHSS), as a high-level exten-sion of it, as well as an ecosystem of sub-domain extensions that canbe easily managed through the ontome.net application. This willresult in an interoperability that is semantically richer than the sim-ple alignment of ontologies and less costly in terms of resources, andabove all adapted to the scientific and humanistic project of the HSS.","sentences":["Given the challenge of giant knowledge graphs created by major eco-nomic actors, which could virtually replace research in the Humani-ties and Social Sciences (HSS) in responding to public concerns, thequestion arises of how to increase the value of research data throughtheir publication and networking, applying the FAIR principles.","Bothan epistemological and a semantic analysis show that the most rel-evant part of research data is factual information, understood as arepresentation of the objects observed by the scientific disciplines,their properties and their relationships.","This rich universe of information will be made understandable andtherefore reusable through the application of foundational ontologiesand a methodology based on the distinction between different levelsof abstraction, allowing the collective development of one or moreshared and reusable domain ontologies.","This vision is being carriedout around the CIDOC CRM, as core ontology, and Semantic Datafor Humanities and Social Sciences (SDHSS), as a high-level exten-sion of it, as well as an ecosystem of sub-domain extensions that canbe easily managed through the ontome.net application.","This willresult in an interoperability that is semantically richer than the sim-ple alignment of ontologies and less costly in terms of resources, andabove all adapted to the scientific and humanistic project of the HSS."],"url":"http://arxiv.org/abs/2402.07531v1","category":"cs.IT"}
{"created":"2024-02-12 09:43:17","title":"A step towards the integration of machine learning and small area estimation","abstract":"The use of machine-learning techniques has grown in numerous research areas. Currently, it is also widely used in statistics, including the official statistics for data collection (e.g. satellite imagery, web scraping and text mining, data cleaning, integration and imputation) but also for data analysis. However, the usage of these methods in survey sampling including small area estimation is still very limited. Therefore, we propose a predictor supported by these algorithms which can be used to predict any population or subpopulation characteristics based on cross-sectional and longitudinal data. Machine learning methods have already been shown to be very powerful in identifying and modelling complex and nonlinear relationships between the variables, which means that they have very good properties in case of strong departures from the classic assumptions. Therefore, we analyse the performance of our proposal under a different set-up, in our opinion of greater importance in real-life surveys. We study only small departures from the assumed model, to show that our proposal is a good alternative in this case as well, even in comparison with optimal methods under the model. What is more, we propose the method of the accuracy estimation of machine learning predictors, giving the possibility of the accuracy comparison with classic methods, where the accuracy is measured as in survey sampling practice. The solution of this problem is indicated in the literature as one of the key issues in integration of these approaches. The simulation studies are based on a real, longitudinal dataset, freely available from the Polish Local Data Bank, where the prediction problem of subpopulation characteristics in the last period, with \"borrowing strength\" from other subpopulations and time periods, is considered.","sentences":["The use of machine-learning techniques has grown in numerous research areas.","Currently, it is also widely used in statistics, including the official statistics for data collection (e.g. satellite imagery, web scraping and text mining, data cleaning, integration and imputation) but also for data analysis.","However, the usage of these methods in survey sampling including small area estimation is still very limited.","Therefore, we propose a predictor supported by these algorithms which can be used to predict any population or subpopulation characteristics based on cross-sectional and longitudinal data.","Machine learning methods have already been shown to be very powerful in identifying and modelling complex and nonlinear relationships between the variables, which means that they have very good properties in case of strong departures from the classic assumptions.","Therefore, we analyse the performance of our proposal under a different set-up, in our opinion of greater importance in real-life surveys.","We study only small departures from the assumed model, to show that our proposal is a good alternative in this case as well, even in comparison with optimal methods under the model.","What is more, we propose the method of the accuracy estimation of machine learning predictors, giving the possibility of the accuracy comparison with classic methods, where the accuracy is measured as in survey sampling practice.","The solution of this problem is indicated in the literature as one of the key issues in integration of these approaches.","The simulation studies are based on a real, longitudinal dataset, freely available from the Polish Local Data Bank, where the prediction problem of subpopulation characteristics in the last period, with \"borrowing strength\" from other subpopulations and time periods, is considered."],"url":"http://arxiv.org/abs/2402.07521v1","category":"stat.ME"}
{"created":"2024-02-12 07:59:28","title":"VCR: Video representation for Contextual Retrieval","abstract":"Streamlining content discovery within media archives requires integrating advanced data representations and effective visualization techniques for clear communication of video topics to users. The proposed system addresses the challenge of efficiently navigating large video collections by exploiting a fusion of visual, audio, and textual features to accurately index and categorize video content through a text-based method. Additionally, semantic embeddings are employed to provide contextually relevant information and recommendations to users, resulting in an intuitive and engaging exploratory experience over our topics ontology map using OpenAI GPT-4.","sentences":["Streamlining content discovery within media archives requires integrating advanced data representations and effective visualization techniques for clear communication of video topics to users.","The proposed system addresses the challenge of efficiently navigating large video collections by exploiting a fusion of visual, audio, and textual features to accurately index and categorize video content through a text-based method.","Additionally, semantic embeddings are employed to provide contextually relevant information and recommendations to users, resulting in an intuitive and engaging exploratory experience over our topics ontology map using OpenAI GPT-4."],"url":"http://arxiv.org/abs/2402.07466v1","category":"cs.IR"}
{"created":"2024-02-12 07:42:11","title":"Anonymizing Test Data in Android: Does It Hurt?","abstract":"Failure data collected from the field (e.g., failure traces, bug reports, and memory dumps) represent an invaluable source of information for developers who need to reproduce and analyze failures. Unfortunately, field data may include sensitive information and thus cannot be collected indiscriminately. Privacy-preserving techniques can address this problem anonymizing data and reducing the risk of disclosing personal information. However, collecting anonymized information may harm reproducibility, that is, the anonymized data may not allow the reproduction of a failure observed in the field. In this paper, we present an empirical investigation about the impact of privacy-preserving techniques on the reproducibility of failures. In particular, we study how five privacy-preserving techniques may impact reproducibilty for 19 bugs in 17 Android applications. Results provide insights on how to select and configure privacy-preserving techniques.","sentences":["Failure data collected from the field (e.g., failure traces, bug reports, and memory dumps) represent an invaluable source of information for developers who need to reproduce and analyze failures.","Unfortunately, field data may include sensitive information and thus cannot be collected indiscriminately.","Privacy-preserving techniques can address this problem anonymizing data and reducing the risk of disclosing personal information.","However, collecting anonymized information may harm reproducibility, that is, the anonymized data may not allow the reproduction of a failure observed in the field.","In this paper, we present an empirical investigation about the impact of privacy-preserving techniques on the reproducibility of failures.","In particular, we study how five privacy-preserving techniques may impact reproducibilty for 19 bugs in 17 Android applications.","Results provide insights on how to select and configure privacy-preserving techniques."],"url":"http://arxiv.org/abs/2402.07460v1","category":"cs.SE"}
{"created":"2024-02-12 05:56:12","title":"News Recommendation with Attention Mechanism","abstract":"This paper explores the area of news recommendation, a key component of online information sharing. Initially, we provide a clear introduction to news recommendation, defining the core problem and summarizing current methods and notable recent algorithms. We then present our work on implementing the NRAM (News Recommendation with Attention Mechanism), an attention-based approach for news recommendation, and assess its effectiveness. Our evaluation shows that NRAM has the potential to significantly improve how news content is personalized for users on digital news platforms.","sentences":["This paper explores the area of news recommendation, a key component of online information sharing.","Initially, we provide a clear introduction to news recommendation, defining the core problem and summarizing current methods and notable recent algorithms.","We then present our work on implementing the NRAM (News Recommendation with Attention Mechanism), an attention-based approach for news recommendation, and assess its effectiveness.","Our evaluation shows that NRAM has the potential to significantly improve how news content is personalized for users on digital news platforms."],"url":"http://arxiv.org/abs/2402.07422v1","category":"cs.AI"}
{"created":"2024-02-12 05:48:52","title":"On the Transit Obfuscation Problem","abstract":"Concealing an intermediate point on a route or visible from a route is an important goal in some transportation and surveillance scenarios. This paper studies the Transit Obfuscation Problem, the problem of traveling from some start location to an end location while \"covering\" a specific transit point that needs to be concealed from adversaries. We propose the notion of transit anonymity, a quantitative guarantee of the anonymity of a specific transit point, even with a powerful adversary with full knowledge of the path planning algorithm. We propose and evaluate planning/search algorithms that satisfy this anonymity criterion.","sentences":["Concealing an intermediate point on a route or visible from a route is an important goal in some transportation and surveillance scenarios.","This paper studies the Transit Obfuscation Problem, the problem of traveling from some start location to an end location while \"covering\" a specific transit point that needs to be concealed from adversaries.","We propose the notion of transit anonymity, a quantitative guarantee of the anonymity of a specific transit point, even with a powerful adversary with full knowledge of the path planning algorithm.","We propose and evaluate planning/search algorithms that satisfy this anonymity criterion."],"url":"http://arxiv.org/abs/2402.07420v1","category":"cs.AI"}
{"created":"2024-02-12 05:48:31","title":"Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand","abstract":"Causal inference from observational data has recently found many applications in machine learning. While sound and complete algorithms exist to compute causal effects, many of these algorithms require explicit access to conditional likelihoods over the observational distribution, which is difficult to estimate in the high-dimensional regime, such as with images. To alleviate this issue, researchers have approached the problem by simulating causal relations with neural models and obtained impressive results. However, none of these existing approaches can be applied to generic scenarios such as causal graphs on image data with latent confounders, or obtain conditional interventional samples. In this paper, we show that any identifiable causal effect given an arbitrary causal graph can be computed through push-forward computations of conditional generative models. Based on this result, we devise a diffusion-based approach to sample from any (conditional) interventional distribution on image data. To showcase our algorithm's performance, we conduct experiments on a Colored MNIST dataset having both the treatment ($X$) and the target variables ($Y$) as images and obtain interventional samples from $P(y|do(x))$. As an application of our algorithm, we evaluate two large conditional generative models that are pre-trained on the CelebA dataset by analyzing the strength of spurious correlations and the level of disentanglement they achieve.","sentences":["Causal inference from observational data has recently found many applications in machine learning.","While sound and complete algorithms exist to compute causal effects, many of these algorithms require explicit access to conditional likelihoods over the observational distribution, which is difficult to estimate in the high-dimensional regime, such as with images.","To alleviate this issue, researchers have approached the problem by simulating causal relations with neural models and obtained impressive results.","However, none of these existing approaches can be applied to generic scenarios such as causal graphs on image data with latent confounders, or obtain conditional interventional samples.","In this paper, we show that any identifiable causal effect given an arbitrary causal graph can be computed through push-forward computations of conditional generative models.","Based on this result, we devise a diffusion-based approach to sample from any (conditional) interventional distribution on image data.","To showcase our algorithm's performance, we conduct experiments on a Colored MNIST dataset having both the treatment ($X$) and the target variables ($Y$) as images and obtain interventional samples from $P(y|do(x))$. As an application of our algorithm, we evaluate two large conditional generative models that are pre-trained on the CelebA dataset by analyzing the strength of spurious correlations and the level of disentanglement they achieve."],"url":"http://arxiv.org/abs/2402.07419v1","category":"cs.LG"}
{"created":"2024-02-12 05:46:10","title":"SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy Adaptation","abstract":"This work explores the zero-shot adaptation capability of semantic skills, semantically interpretable experts' behavior patterns, in cross-domain settings, where a user input in interleaved multi-modal snippets can prompt a new long-horizon task for different domains. In these cross-domain settings, we present a semantic skill translator framework SemTra which utilizes a set of multi-modal models to extract skills from the snippets, and leverages the reasoning capabilities of a pretrained language model to adapt these extracted skills to the target domain. The framework employs a two-level hierarchy for adaptation: task adaptation and skill adaptation. During task adaptation, seq-to-seq translation by the language model transforms the extracted skills into a semantic skill sequence, which is tailored to fit the cross-domain contexts. Skill adaptation focuses on optimizing each semantic skill for the target domain context, through parametric instantiations that are facilitated by language prompting and contrastive learning-based context inferences. This hierarchical adaptation empowers the framework to not only infer a complex task specification in one-shot from the interleaved multi-modal snippets, but also adapt it to new domains with zero-shot learning abilities. We evaluate our framework with Meta-World, Franka Kitchen, RLBench, and CARLA environments. The results clarify the framework's superiority in performing long-horizon tasks and adapting to different domains, showing its broad applicability in practical use cases, such as cognitive robots interpreting abstract instructions and autonomous vehicles operating under varied configurations.","sentences":["This work explores the zero-shot adaptation capability of semantic skills, semantically interpretable experts' behavior patterns, in cross-domain settings, where a user input in interleaved multi-modal snippets can prompt a new long-horizon task for different domains.","In these cross-domain settings, we present a semantic skill translator framework SemTra which utilizes a set of multi-modal models to extract skills from the snippets, and leverages the reasoning capabilities of a pretrained language model to adapt these extracted skills to the target domain.","The framework employs a two-level hierarchy for adaptation: task adaptation and skill adaptation.","During task adaptation, seq-to-seq translation by the language model transforms the extracted skills into a semantic skill sequence, which is tailored to fit the cross-domain contexts.","Skill adaptation focuses on optimizing each semantic skill for the target domain context, through parametric instantiations that are facilitated by language prompting and contrastive learning-based context inferences.","This hierarchical adaptation empowers the framework to not only infer a complex task specification in one-shot from the interleaved multi-modal snippets, but also adapt it to new domains with zero-shot learning abilities.","We evaluate our framework with Meta-World, Franka Kitchen, RLBench, and CARLA environments.","The results clarify the framework's superiority in performing long-horizon tasks and adapting to different domains, showing its broad applicability in practical use cases, such as cognitive robots interpreting abstract instructions and autonomous vehicles operating under varied configurations."],"url":"http://arxiv.org/abs/2402.07418v1","category":"cs.AI"}
{"created":"2024-02-12 05:13:44","title":"Auxiliary Reward Generation with Transition Distance Representation Learning","abstract":"Reinforcement learning (RL) has shown its strength in challenging sequential decision-making problems. The reward function in RL is crucial to the learning performance, as it serves as a measure of the task completion degree. In real-world problems, the rewards are predominantly human-designed, which requires laborious tuning, and is easily affected by human cognitive biases. To achieve automatic auxiliary reward generation, we propose a novel representation learning approach that can measure the ``transition distance'' between states. Building upon these representations, we introduce an auxiliary reward generation technique for both single-task and skill-chaining scenarios without the need for human knowledge. The proposed approach is evaluated in a wide range of manipulation tasks. The experiment results demonstrate the effectiveness of measuring the transition distance between states and the induced improvement by auxiliary rewards, which not only promotes better learning efficiency but also increases convergent stability.","sentences":["Reinforcement learning (RL) has shown its strength in challenging sequential decision-making problems.","The reward function in RL is crucial to the learning performance, as it serves as a measure of the task completion degree.","In real-world problems, the rewards are predominantly human-designed, which requires laborious tuning, and is easily affected by human cognitive biases.","To achieve automatic auxiliary reward generation, we propose a novel representation learning approach that can measure the ``transition distance'' between states.","Building upon these representations, we introduce an auxiliary reward generation technique for both single-task and skill-chaining scenarios without the need for human knowledge.","The proposed approach is evaluated in a wide range of manipulation tasks.","The experiment results demonstrate the effectiveness of measuring the transition distance between states and the induced improvement by auxiliary rewards, which not only promotes better learning efficiency but also increases convergent stability."],"url":"http://arxiv.org/abs/2402.07412v1","category":"cs.LG"}
{"created":"2024-02-12 04:59:58","title":"Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples","abstract":"The frequent occurrence of cyber-attacks has made webshell attacks and defense gradually become a research hotspot in the field of network security. However, the lack of publicly available benchmark datasets and the over-reliance on manually defined rules for webshell escape sample generation have slowed down the progress of research related to webshell escape sample generation strategies and artificial intelligence-based webshell detection algorithms. To address the drawbacks of weak webshell sample escape capabilities, the lack of webshell datasets with complex malicious features, and to promote the development of webshell detection technology, we propose the Hybrid Prompt algorithm for webshell escape sample generation with the help of large language models. As a prompt algorithm specifically developed for webshell sample generation, the Hybrid Prompt algorithm not only combines various prompt ideas including Chain of Thought, Tree of Thought, but also incorporates various components such as webshell hierarchical module and few-shot example to facilitate the LLM in learning and reasoning webshell escape strategies. Experimental results show that the Hybrid Prompt algorithm can work with multiple LLMs with excellent code reasoning ability to generate high-quality webshell samples with high Escape Rate (88.61% with GPT-4 model on VIRUSTOTAL detection engine) and Survival Rate (54.98% with GPT-4 model).","sentences":["The frequent occurrence of cyber-attacks has made webshell attacks and defense gradually become a research hotspot in the field of network security.","However, the lack of publicly available benchmark datasets and the over-reliance on manually defined rules for webshell escape sample generation have slowed down the progress of research related to webshell escape sample generation strategies and artificial intelligence-based webshell detection algorithms.","To address the drawbacks of weak webshell sample escape capabilities, the lack of webshell datasets with complex malicious features, and to promote the development of webshell detection technology, we propose the Hybrid Prompt algorithm for webshell escape sample generation with the help of large language models.","As a prompt algorithm specifically developed for webshell sample generation, the Hybrid Prompt algorithm not only combines various prompt ideas including Chain of Thought, Tree of Thought, but also incorporates various components such as webshell hierarchical module and few-shot example to facilitate the LLM in learning and reasoning webshell escape strategies.","Experimental results show that the Hybrid Prompt algorithm can work with multiple LLMs with excellent code reasoning ability to generate high-quality webshell samples with high Escape Rate (88.61% with GPT-4 model on VIRUSTOTAL detection engine) and Survival Rate (54.98% with GPT-4 model)."],"url":"http://arxiv.org/abs/2402.07408v1","category":"cs.CR"}
{"created":"2024-02-12 04:47:38","title":"Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic Hierarchy Process and GPT-4 for Automated Decision Support","abstract":"Our study presents a new framework that incorporates the Analytic Hierarchy Process (AHP) and Generative Pre-trained Transformer 4 (GPT-4) large language model (LLM), bringing novel approaches to cybersecurity Multiple-criteria Decision Making (MCDA). By utilizing the capabilities of GPT-4 autonomous agents as virtual experts, we automate the decision-making process, enhancing both efficiency and reliability. This new approach focuses on leveraging LLMs for sophisticated decision analysis, highlighting the synergy between traditional decision-making models and cutting-edge AI technologies. Our innovative methodology demonstrates significant advancements in using AI-driven agents for complex decision-making scenarios, highlighting the importance of AI in strategic cybersecurity applications. The findings reveal the transformative potential of combining AHP and LLMs, establishing a new paradigm for intelligent decision support systems in cybersecurity and beyond.","sentences":["Our study presents a new framework that incorporates the Analytic Hierarchy Process (AHP) and Generative Pre-trained Transformer 4 (GPT-4) large language model (LLM), bringing novel approaches to cybersecurity Multiple-criteria Decision Making (MCDA).","By utilizing the capabilities of GPT-4 autonomous agents as virtual experts, we automate the decision-making process, enhancing both efficiency and reliability.","This new approach focuses on leveraging LLMs for sophisticated decision analysis, highlighting the synergy between traditional decision-making models and cutting-edge AI technologies.","Our innovative methodology demonstrates significant advancements in using AI-driven agents for complex decision-making scenarios, highlighting the importance of AI in strategic cybersecurity applications.","The findings reveal the transformative potential of combining AHP and LLMs, establishing a new paradigm for intelligent decision support systems in cybersecurity and beyond."],"url":"http://arxiv.org/abs/2402.07404v1","category":"cs.AI"}
{"created":"2024-02-12 04:34:19","title":"BDIQA: A New Dataset for Video Question Answering to Explore Cognitive Reasoning through Theory of Mind","abstract":"As a foundational component of cognitive intelligence, theory of mind (ToM) can make AI more closely resemble human thought processes, thereby enhancing their interaction and collaboration with human. In particular, it can significantly improve a model's comprehension of videos in complex scenes. However, current video question answer (VideoQA) datasets focus on studying causal reasoning within events few of them genuinely incorporating human ToM. Consequently, there is a lack of development in ToM reasoning tasks within the area of VideoQA. This paper presents BDIQA, the first benchmark to explore the cognitive reasoning capabilities of VideoQA models in the context of ToM. BDIQA is inspired by the cognitive development of children's ToM and addresses the current deficiencies in machine ToM within datasets and tasks. Specifically, it offers tasks at two difficulty levels, assessing Belief, Desire and Intention (BDI) reasoning in both simple and complex scenarios. We conduct evaluations on several mainstream methods of VideoQA and diagnose their capabilities with zero shot, few shot and supervised learning. We find that the performance of pre-trained models on cognitive reasoning tasks remains unsatisfactory. To counter this challenge, we undertake thorough analysis and experimentation, ultimately presenting two guidelines to enhance cognitive reasoning derived from ablation analysis.","sentences":["As a foundational component of cognitive intelligence, theory of mind (ToM) can make AI more closely resemble human thought processes, thereby enhancing their interaction and collaboration with human.","In particular, it can significantly improve a model's comprehension of videos in complex scenes.","However, current video question answer (VideoQA) datasets focus on studying causal reasoning within events few of them genuinely incorporating human ToM.","Consequently, there is a lack of development in ToM reasoning tasks within the area of VideoQA.","This paper presents BDIQA, the first benchmark to explore the cognitive reasoning capabilities of VideoQA models in the context of ToM. BDIQA is inspired by the cognitive development of children's ToM and addresses the current deficiencies in machine ToM within datasets and tasks.","Specifically, it offers tasks at two difficulty levels, assessing Belief, Desire and Intention (BDI) reasoning in both simple and complex scenarios.","We conduct evaluations on several mainstream methods of VideoQA and diagnose their capabilities with zero shot, few shot and supervised learning.","We find that the performance of pre-trained models on cognitive reasoning tasks remains unsatisfactory.","To counter this challenge, we undertake thorough analysis and experimentation, ultimately presenting two guidelines to enhance cognitive reasoning derived from ablation analysis."],"url":"http://arxiv.org/abs/2402.07402v1","category":"cs.MM"}
{"created":"2024-02-12 04:27:43","title":"The ALMaQUEST Survey XIII: Understanding radial trends in star formation quenching via the relative roles of gas availability and star formation efficiency","abstract":"Star formation quenching is one of the key processes that shape the evolution of galaxies. In this study, we investigate the changes in molecular gas and star formation properties as galaxies transit from the star-forming main sequence to the passive regime. Our analysis reveals that as galaxies move away from the main sequence towards the green valley the radial profile of specific star formation rate surface density ($\\Sigma_\\mathrm{sSFR}$) is suppressed compared with main sequence galaxies out to a galactocentric radius of 1.5 $R_{e}$ ($\\sim$ 7 kpc for our sample). By combining radial profiles of gas fraction ($f_\\mathrm{gas}$) and star formation efficiency (SFE), we can discern the underlying mechanism that determines $\\Sigma_\\mathrm{sSFR}$ at different galactocentric radii. Analysis of relative contributions of $f_\\mathrm{gas}$ and SFE to $\\Sigma_\\mathrm{sSFR}$ uncovers a diverse range of quenching modes. Star formation in approximately half of our quenching galaxies is primarily driven by a single mode (i.e. either $f_\\mathrm{gas}$ or SFE), or a combination of both. A collective analysis of all galaxies reveals that the reduction in star formation within the central regions ($R$ $<$ 0.5 $R_{e}$) is primarily attributable to a decrease in SFE. Conversely, in the disk regions ($R$ $>$ 0.5 $R_{e}$), both $f_\\mathrm{gas}$ and SFE contribute to the suppression of star formation. Our findings suggest that multiple quenching mechanisms may be at play in our sample galaxies, and even within a single galaxy. We also compare our observational outcomes with those from galaxy simulations and discuss the implications of our data.","sentences":["Star formation quenching is one of the key processes that shape the evolution of galaxies.","In this study, we investigate the changes in molecular gas and star formation properties as galaxies transit from the star-forming main sequence to the passive regime.","Our analysis reveals that as galaxies move away from the main sequence towards the green valley the radial profile of specific star formation rate surface density ($\\Sigma_\\mathrm{sSFR}$) is suppressed compared with main sequence galaxies out to a galactocentric radius of 1.5 $R_{e}$ ($\\sim$ 7 kpc for our sample).","By combining radial profiles of gas fraction ($f_\\mathrm{gas}$) and star formation efficiency (SFE), we can discern the underlying mechanism that determines $\\Sigma_\\mathrm{sSFR}$ at different galactocentric radii.","Analysis of relative contributions of $f_\\mathrm{gas}$ and SFE to $\\Sigma_\\mathrm{sSFR}$ uncovers a diverse range of quenching modes.","Star formation in approximately half of our quenching galaxies is primarily driven by a single mode (i.e. either $f_\\mathrm{gas}$ or SFE), or a combination of both.","A collective analysis of all galaxies reveals that the reduction in star formation within the central regions ($R$ $<$ 0.5 $R_{e}$) is primarily attributable to a decrease in SFE.","Conversely, in the disk regions ($R$ $>$ 0.5 $R_{e}$), both $f_\\mathrm{gas}$ and SFE contribute to the suppression of star formation.","Our findings suggest that multiple quenching mechanisms may be at play in our sample galaxies, and even within a single galaxy.","We also compare our observational outcomes with those from galaxy simulations and discuss the implications of our data."],"url":"http://arxiv.org/abs/2402.07400v1","category":"astro-ph.GA"}
{"created":"2024-02-12 04:13:16","title":"VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization","abstract":"This paper presents VisLingInstruct, a novel approach to advancing Multi-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show impressive zero-shot abilities in multi-modal tasks, but their performance depends heavily on the quality of instructions. VisLingInstruct tackles this by autonomously evaluating and optimizing instructional texts through In-Context Learning, improving the synergy between visual perception and linguistic expression in MMLMs. Alongside this instructional advancement, we have also optimized the visual feature extraction modules in MMLMs, further augmenting their responsiveness to textual cues. Our comprehensive experiments on MMLMs, based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves zero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1% and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and HatefulMemes datasets.","sentences":["This paper presents VisLingInstruct, a novel approach to advancing Multi-Modal Language Models (MMLMs) in zero-shot learning.","Current MMLMs show impressive zero-shot abilities in multi-modal tasks, but their performance depends heavily on the quality of instructions.","VisLingInstruct tackles this by autonomously evaluating and optimizing instructional texts through In-Context Learning, improving the synergy between visual perception and linguistic expression in MMLMs.","Alongside this instructional advancement, we have also optimized the visual feature extraction modules in MMLMs, further augmenting their responsiveness to textual cues.","Our comprehensive experiments on MMLMs, based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves zero-shot performance in visual multi-modal tasks.","Notably, it achieves a 13.1% and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and HatefulMemes datasets."],"url":"http://arxiv.org/abs/2402.07398v1","category":"cs.AI"}
{"created":"2024-02-12 04:10:09","title":"Leveraging AI to Advance Science and Computing Education across Africa: Progress, Challenges, and Opportunities","abstract":"Across the African continent, students grapple with various educational challenges, including limited access to essential resources such as computers, internet connectivity, reliable electricity, and a shortage of qualified teachers. Despite these challenges, recent advances in AI such as BERT, and GPT-4 have demonstrated their potential for advancing education. Yet, these AI tools tend to be deployed and evaluated predominantly within the context of Western educational settings, with limited attention directed towards the unique needs and challenges faced by students in Africa. In this book chapter, we describe our works developing and deploying AI in Education tools in Africa: (1) SuaCode, an AI-powered app that enables Africans to learn to code using their smartphones, (2) AutoGrad, an automated grading, and feedback tool for graphical and interactive coding assignments, (3) a tool for code plagiarism detection that shows visual evidence of plagiarism, (4) Kwame, a bilingual AI teaching assistant for coding courses, (5) Kwame for Science, a web-based AI teaching assistant that provides instant answers to students' science questions and (6) Brilla AI, an AI contestant for the National Science and Maths Quiz competition. We discuss challenges and potential opportunities to use AI to advance science and computing education across Africa.","sentences":["Across the African continent, students grapple with various educational challenges, including limited access to essential resources such as computers, internet connectivity, reliable electricity, and a shortage of qualified teachers.","Despite these challenges, recent advances in AI such as BERT, and GPT-4 have demonstrated their potential for advancing education.","Yet, these AI tools tend to be deployed and evaluated predominantly within the context of Western educational settings, with limited attention directed towards the unique needs and challenges faced by students in Africa.","In this book chapter, we describe our works developing and deploying AI in Education tools in Africa: (1) SuaCode, an AI-powered app that enables Africans to learn to code using their smartphones, (2) AutoGrad, an automated grading, and feedback tool for graphical and interactive coding assignments, (3) a tool for code plagiarism detection that shows visual evidence of plagiarism, (4) Kwame, a bilingual AI teaching assistant for coding courses, (5) Kwame for Science, a web-based AI teaching assistant that provides instant answers to students' science questions and (6) Brilla AI, an AI contestant for the National Science and Maths Quiz competition.","We discuss challenges and potential opportunities to use AI to advance science and computing education across Africa."],"url":"http://arxiv.org/abs/2402.07397v1","category":"cs.CY"}
{"created":"2024-02-12 03:56:47","title":"Comparing the willingness to share for human-generated vs. AI-generated fake news","abstract":"Generative artificial intelligence (AI) presents large risks for society when it is used to create fake news. A crucial factor for fake news to go viral on social media is that users share such content. Here, we aim to shed light on the sharing behavior of users across human-generated vs. AI-generated fake news. Specifically, we study: (1) What is the perceived veracity of human-generated fake news vs. AI-generated fake news? (2) What is the user's willingness to share human-generated fake news vs. AI-generated fake news on social media? (3) What socio-economic characteristics let users fall for AI-generated fake news? To this end, we conducted a pre-registered, online experiment with $N=$ 988 subjects and 20 fake news from the COVID-19 pandemic generated by GPT-4 vs. humans. Our findings show that AI-generated fake news is perceived as less accurate than human-generated fake news, but both tend to be shared equally. Further, several socio-economic factors explain who falls for AI-generated fake news.","sentences":["Generative artificial intelligence (AI) presents large risks for society when it is used to create fake news.","A crucial factor for fake news to go viral on social media is that users share such content.","Here, we aim to shed light on the sharing behavior of users across human-generated vs. AI-generated fake news.","Specifically, we study: (1) What is the perceived veracity of human-generated fake news vs. AI-generated fake news?","(2) What is the user's willingness to share human-generated fake news vs. AI-generated fake news on social media?","(3) What socio-economic characteristics let users fall for AI-generated fake news?","To this end, we conducted a pre-registered, online experiment with $N=$ 988 subjects and 20 fake news from the COVID-19 pandemic generated by GPT-4 vs. humans.","Our findings show that AI-generated fake news is perceived as less accurate than human-generated fake news, but both tend to be shared equally.","Further, several socio-economic factors explain who falls for AI-generated fake news."],"url":"http://arxiv.org/abs/2402.07395v1","category":"cs.SI"}
{"created":"2024-02-12 03:40:32","title":"TeMPO: Efficient Time-Multiplexed Dynamic Photonic Tensor Core for Edge AI with Compact Slow-Light Electro-Optic Modulator","abstract":"Electronic-photonic computing systems offer immense potential in energy-efficient artificial intelligence (AI) acceleration tasks due to the superior computing speed and efficiency of optics, especially for real-time, low-energy deep neural network (DNN) inference tasks on resource-restricted edge platforms. However, current optical neural accelerators based on foundry-available devices and conventional system architecture still encounter a performance gap compared to highly customized electronic counterparts. To bridge the performance gap due to lack of domain specialization, we present a time-multiplexed dynamic photonic tensor accelerator, dubbed TeMPO, with cross-layer device/circuit/architecture customization. At the device level, we present foundry-compatible, customized photonic devices, including a slow-light electro-optic modulator with experimental demonstration, optical splitters, and phase shifters that significantly reduce the footprint and power in input encoding and dot-product calculation. At the circuit level, partial products are hierarchically accumulated via parallel photocurrent aggregation, lightweight capacitive temporal integration, and sequential digital summation, considerably relieving the analog-to-digital conversion bottleneck. We also employ a multi-tile, multi-core architecture to maximize hardware sharing for higher efficiency. Across diverse edge AI workloads, TeMPO delivers digital-comparable task accuracy with superior quantization/noise tolerance. We achieve a 368.6 TOPS peak performance, 22.3 TOPS/W energy efficiency, and 1.2 TOPS/mm$^2$ compute density, pushing the Pareto frontier in edge AI hardware. This work signifies the power of cross-layer co-design and domain-specific customization, paving the way for future electronic-photonic accelerators with even greater performance and efficiency.","sentences":["Electronic-photonic computing systems offer immense potential in energy-efficient artificial intelligence (AI) acceleration tasks due to the superior computing speed and efficiency of optics, especially for real-time, low-energy deep neural network (DNN) inference tasks on resource-restricted edge platforms.","However, current optical neural accelerators based on foundry-available devices and conventional system architecture still encounter a performance gap compared to highly customized electronic counterparts.","To bridge the performance gap due to lack of domain specialization, we present a time-multiplexed dynamic photonic tensor accelerator, dubbed TeMPO, with cross-layer device/circuit/architecture customization.","At the device level, we present foundry-compatible, customized photonic devices, including a slow-light electro-optic modulator with experimental demonstration, optical splitters, and phase shifters that significantly reduce the footprint and power in input encoding and dot-product calculation.","At the circuit level, partial products are hierarchically accumulated via parallel photocurrent aggregation, lightweight capacitive temporal integration, and sequential digital summation, considerably relieving the analog-to-digital conversion bottleneck.","We also employ a multi-tile, multi-core architecture to maximize hardware sharing for higher efficiency.","Across diverse edge AI workloads, TeMPO delivers digital-comparable task accuracy with superior quantization/noise tolerance.","We achieve a 368.6 TOPS peak performance, 22.3 TOPS/W energy efficiency, and 1.2 TOPS/mm$^2$ compute density, pushing the Pareto frontier in edge AI hardware.","This work signifies the power of cross-layer co-design and domain-specific customization, paving the way for future electronic-photonic accelerators with even greater performance and efficiency."],"url":"http://arxiv.org/abs/2402.07393v1","category":"cs.ET"}
{"created":"2024-02-12 03:04:42","title":"Exploring Perceptual Limitation of Multimodal Large Language Models","abstract":"Multimodal Large Language Models (MLLMs) have recently shown remarkable perceptual capability in answering visual questions, however, little is known about the limits of their perception. In particular, while prior works have provided anecdotal evidence of MLLMs' sensitivity to object size, this phenomenon and its underlying causes have not been explored comprehensively. In this work, we quantitatively study the perception of small visual objects in several state-of-the-art MLLMs and reveal a pervasive limitation in answering questions about small objects in images. Next, we identify four independent factors that can contribute to this limitation -- object quality, size, distractors, and location -- and conduct controlled intervention studies to measure the effect of each factor on MLLMs' perception. In particular, we find that lower object quality and smaller object size can both independently reduce MLLMs' ability to answer visual questions. More surprisingly, we find that the location of the object in the image and the presence of visual distractors can also significantly reduce MLLMs' question answering accuracy. Our study provides a better understanding of the perceptual limitation of MLLMs and contributes new evaluation protocols for analyzing the perception of future MLLMs. To facilitate further investigations, we release our code and data.","sentences":["Multimodal Large Language Models (MLLMs) have recently shown remarkable perceptual capability in answering visual questions, however, little is known about the limits of their perception.","In particular, while prior works have provided anecdotal evidence of MLLMs' sensitivity to object size, this phenomenon and its underlying causes have not been explored comprehensively.","In this work, we quantitatively study the perception of small visual objects in several state-of-the-art MLLMs and reveal a pervasive limitation in answering questions about small objects in images.","Next, we identify four independent factors that can contribute to this limitation -- object quality, size, distractors, and location -- and conduct controlled intervention studies to measure the effect of each factor on MLLMs' perception.","In particular, we find that lower object quality and smaller object size can both independently reduce MLLMs' ability to answer visual questions.","More surprisingly, we find that the location of the object in the image and the presence of visual distractors can also significantly reduce MLLMs' question answering accuracy.","Our study provides a better understanding of the perceptual limitation of MLLMs and contributes new evaluation protocols for analyzing the perception of future MLLMs.","To facilitate further investigations, we release our code and data."],"url":"http://arxiv.org/abs/2402.07384v1","category":"cs.CV"}
{"created":"2024-02-12 02:47:23","title":"RIS-Empowered LEO Satellite Networks for 6G: Promising Usage Scenarios and Future Directions","abstract":"Low-Earth orbit (LEO) satellite systems have been deemed a promising key enabler for current 5G and the forthcoming 6G wireless networks. Such LEO satellite constellations can provide worldwide three-dimensional coverage, high data rate, and scalability, thus enabling truly ubiquitous connectivity. On the other hand, another promising technology, reconfigurable intelligent surfaces (RISs), has emerged with favorable features, such as flexible deployment, cost & power efficiency, less transmission delay, noise-free nature, and in-band full-duplex structure. LEO satellite networks have many practical imperfections and limitations; however, exploiting RISs has been shown to be a potential solution to overcome these challenges. Particularly, RISs can enhance link quality, reduce the Doppler shift effect, and mitigate inter-/intra beam interference. In this article, we delve into exploiting RISs in LEO satellite networks. First, we present a holistic overview of LEO satellite communication and RIS technology, highlighting potential benefits and challenges. Second, we describe promising usage scenarios and applications in detail. Finally, we discuss potential future directions and challenges on RIS-empowered LEO networks, offering futuristic visions of the upcoming 6G era.","sentences":["Low-Earth orbit (LEO) satellite systems have been deemed a promising key enabler for current 5G and the forthcoming 6G wireless networks.","Such LEO satellite constellations can provide worldwide three-dimensional coverage, high data rate, and scalability, thus enabling truly ubiquitous connectivity.","On the other hand, another promising technology, reconfigurable intelligent surfaces (RISs), has emerged with favorable features, such as flexible deployment, cost & power efficiency, less transmission delay, noise-free nature, and in-band full-duplex structure.","LEO satellite networks have many practical imperfections and limitations; however, exploiting RISs has been shown to be a potential solution to overcome these challenges.","Particularly, RISs can enhance link quality, reduce the Doppler shift effect, and mitigate inter-/intra beam interference.","In this article, we delve into exploiting RISs in LEO satellite networks.","First, we present a holistic overview of LEO satellite communication and RIS technology, highlighting potential benefits and challenges.","Second, we describe promising usage scenarios and applications in detail.","Finally, we discuss potential future directions and challenges on RIS-empowered LEO networks, offering futuristic visions of the upcoming 6G era."],"url":"http://arxiv.org/abs/2402.07381v1","category":"cs.IT"}
{"created":"2024-02-12 02:19:45","title":"Young double-slit interference with single hard x-ray photons","abstract":"Young double-slit experiments using monochromatic hard X-rays with the energy of 25 keV are presented. The experiments were performed at a synchrotron source with a distance of 110 m between the interferometer and the detector to produce an interference pattern with a sufficiently broad period that could be adequately sampled by a photon-counting detector with 75 micrometre pixels. In the single-particle version of the experiment, over one million image frames with a single registered photon in each one were collected. The sum of these frames showed a clear presence of the interference pattern with the expected period. Subsequent analysis provided an objective estimation of the minimal number of detected photons required to determine, in accordance with the Rose criterion, the presence of the photon interference. Apart from a general theoretical interest, these investigations were aimed at exploring the possibility of medical X-ray phase-contrast imaging in photon-counting mode at minimal radiation doses.","sentences":["Young double-slit experiments using monochromatic hard X-rays with the energy of 25 keV are presented.","The experiments were performed at a synchrotron source with a distance of 110 m between the interferometer and the detector to produce an interference pattern with a sufficiently broad period that could be adequately sampled by a photon-counting detector with 75 micrometre pixels.","In the single-particle version of the experiment, over one million image frames with a single registered photon in each one were collected.","The sum of these frames showed a clear presence of the interference pattern with the expected period.","Subsequent analysis provided an objective estimation of the minimal number of detected photons required to determine, in accordance with the Rose criterion, the presence of the photon interference.","Apart from a general theoretical interest, these investigations were aimed at exploring the possibility of medical X-ray phase-contrast imaging in photon-counting mode at minimal radiation doses."],"url":"http://arxiv.org/abs/2402.07377v1","category":"physics.optics"}
{"created":"2024-02-12 02:16:59","title":"Unsupervised Discovery of Object-Centric Neural Fields","abstract":"We study inferring 3D object-centric scene representations from a single image. While recent methods have shown potential in unsupervised 3D object discovery from simple synthetic images, they fail to generalize to real-world scenes with visually rich and diverse objects. This limitation stems from their object representations, which entangle objects' intrinsic attributes like shape and appearance with extrinsic, viewer-centric properties such as their 3D location. To address this bottleneck, we propose Unsupervised discovery of Object-Centric neural Fields (uOCF). uOCF focuses on learning the intrinsics of objects and models the extrinsics separately. Our approach significantly improves systematic generalization, thus enabling unsupervised learning of high-fidelity object-centric scene representations from sparse real-world images. To evaluate our approach, we collect three new datasets, including two real kitchen environments. Extensive experiments show that uOCF enables unsupervised discovery of visually rich objects from a single real image, allowing applications such as 3D object segmentation and scene manipulation. Notably, uOCF demonstrates zero-shot generalization to unseen objects from a single real image. Project page: https://red-fairy.github.io/uOCF/","sentences":["We study inferring 3D object-centric scene representations from a single image.","While recent methods have shown potential in unsupervised 3D object discovery from simple synthetic images, they fail to generalize to real-world scenes with visually rich and diverse objects.","This limitation stems from their object representations, which entangle objects' intrinsic attributes like shape and appearance with extrinsic, viewer-centric properties such as their 3D location.","To address this bottleneck, we propose Unsupervised discovery of Object-Centric neural Fields (uOCF).","uOCF focuses on learning the intrinsics of objects and models the extrinsics separately.","Our approach significantly improves systematic generalization, thus enabling unsupervised learning of high-fidelity object-centric scene representations from sparse real-world images.","To evaluate our approach, we collect three new datasets, including two real kitchen environments.","Extensive experiments show that uOCF enables unsupervised discovery of visually rich objects from a single real image, allowing applications such as 3D object segmentation and scene manipulation.","Notably, uOCF demonstrates zero-shot generalization to unseen objects from a single real image.","Project page: https://red-fairy.github.io/uOCF/"],"url":"http://arxiv.org/abs/2402.07376v1","category":"cs.CV"}
{"created":"2024-02-12 02:01:53","title":"SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder","abstract":"Face swapping has gained significant attention for its varied applications. The majority of previous face swapping approaches have relied on the seesaw game training scheme, which often leads to the instability of the model training and results in undesired samples with blended identities due to the target identity leakage problem. This paper introduces the Shape Agnostic Masked AutoEncoder (SAMAE) training scheme, a novel self-supervised approach designed to enhance face swapping model training. Our training scheme addresses the limitations of traditional training methods by circumventing the conventional seesaw game and introducing clear ground truth through its self-reconstruction training regime. It effectively mitigates identity leakage by masking facial regions of the input images and utilizing learned disentangled identity and non-identity features. Additionally, we tackle the shape misalignment problem with new techniques including perforation confusion and random mesh scaling, and establishes a new state-of-the-art, surpassing other baseline methods, preserving both identity and non-identity attributes, without sacrificing on either aspect.","sentences":["Face swapping has gained significant attention for its varied applications.","The majority of previous face swapping approaches have relied on the seesaw game training scheme, which often leads to the instability of the model training and results in undesired samples with blended identities due to the target identity leakage problem.","This paper introduces the Shape Agnostic Masked AutoEncoder (SAMAE) training scheme, a novel self-supervised approach designed to enhance face swapping model training.","Our training scheme addresses the limitations of traditional training methods by circumventing the conventional seesaw game and introducing clear ground truth through its self-reconstruction training regime.","It effectively mitigates identity leakage by masking facial regions of the input images and utilizing learned disentangled identity and non-identity features.","Additionally, we tackle the shape misalignment problem with new techniques including perforation confusion and random mesh scaling, and establishes a new state-of-the-art, surpassing other baseline methods, preserving both identity and non-identity attributes, without sacrificing on either aspect."],"url":"http://arxiv.org/abs/2402.07370v1","category":"cs.CV"}
{"created":"2024-02-12 01:55:51","title":"Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning","abstract":"This study evaluates the ability of Large Language Model (LLM)-based Subpopulation Representative Models (SRMs) to generalize from empirical data, utilizing in-context learning with data from the 2016 and 2020 American National Election Studies. We explore generalization across response variables and demographic subgroups. While conditioning with empirical data improves performance on the whole, the benefit of in-context learning varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others. The inequitable benefits of in-context learning for SRM present a challenge for practitioners implementing SRMs, and for decision-makers who might come to rely on them. Our work highlights a need for fine-grained benchmarks captured from diverse subpopulations that test not only fidelity but generalization.","sentences":["This study evaluates the ability of Large Language Model (LLM)-based Subpopulation Representative Models (SRMs) to generalize from empirical data, utilizing in-context learning with data from the 2016 and 2020 American National Election Studies.","We explore generalization across response variables and demographic subgroups.","While conditioning with empirical data improves performance on the whole, the benefit of in-context learning varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others.","The inequitable benefits of in-context learning for SRM present a challenge for practitioners implementing SRMs, and for decision-makers who might come to rely on them.","Our work highlights a need for fine-grained benchmarks captured from diverse subpopulations that test not only fidelity but generalization."],"url":"http://arxiv.org/abs/2402.07368v1","category":"cs.LG"}
{"created":"2024-02-12 01:47:06","title":"Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing","abstract":"Federated learning (FL) is a machine learning paradigm where the clients possess decentralized training data and the central server handles aggregation and scheduling. Typically, FL algorithms involve clients training their local models using stochastic gradient descent (SGD), which carries drawbacks such as slow convergence and being prone to getting stuck in suboptimal solutions. In this work, we propose a message passing based Bayesian federated learning (BFL) framework to avoid these drawbacks.Specifically, we formulate the problem of deep neural network (DNN) learning and compression and as a sparse Bayesian inference problem, in which group sparse prior is employed to achieve structured model compression. Then, we propose an efficient BFL algorithm called EMTDAMP, where expectation maximization (EM) and turbo deep approximate message passing (TDAMP) are combined to achieve distributed learning and compression. The central server aggregates local posterior distributions to update global posterior distributions and update hyperparameters based on EM to accelerate convergence. The clients perform TDAMP to achieve efficient approximate message passing over DNN with joint prior distribution. We detail the application of EMTDAMP to Boston housing price prediction and handwriting recognition, and present extensive numerical results to demonstrate the advantages of EMTDAMP.","sentences":["Federated learning (FL) is a machine learning paradigm where the clients possess decentralized training data and the central server handles aggregation and scheduling.","Typically, FL algorithms involve clients training their local models using stochastic gradient descent (SGD), which carries drawbacks such as slow convergence and being prone to getting stuck in suboptimal solutions.","In this work, we propose a message passing based Bayesian federated learning (BFL) framework to avoid these drawbacks.","Specifically, we formulate the problem of deep neural network (DNN) learning and compression and as a sparse Bayesian inference problem, in which group sparse prior is employed to achieve structured model compression.","Then, we propose an efficient BFL algorithm called EMTDAMP, where expectation maximization (EM) and turbo deep approximate message passing (TDAMP) are combined to achieve distributed learning and compression.","The central server aggregates local posterior distributions to update global posterior distributions and update hyperparameters based on EM to accelerate convergence.","The clients perform TDAMP to achieve efficient approximate message passing over DNN with joint prior distribution.","We detail the application of EMTDAMP to Boston housing price prediction and handwriting recognition, and present extensive numerical results to demonstrate the advantages of EMTDAMP."],"url":"http://arxiv.org/abs/2402.07366v1","category":"cs.LG"}
{"created":"2024-02-12 01:26:17","title":"Structured Satellite-UAV-Terrestrial Networks for 6G Internet of Things","abstract":"The upcoming sixth generation (6G) wireless communication network is envisioned to cover space, air, and maritime areas, in addition to urban-centered terrestrial coverage by the fifth generation (5G) network, to support intelligent Internet of Things (IoT). Towards this end, we investigate structured integration of satellites, unmanned aerial vehicles (UAVs), and terrestrial networks, aiming to serve future universal IoT possibly with a massive number of devices in the coverage holes of current 5G. The hybrid satellite-UAV-terrestrial network usually leads to high system complexity, due to the heterogeneity and dynamics of space/air/ground links. With a systematic thinking, we propose to create and exploit hierarchies for the integrated network. Four basic structures are discussed by learning from the synergies in our human body. To orchestrate multiple heterogeneous basic structures, we further propose a process-oriented on-demand coverage method, which characterizes the system behavior as a series of events over time and is able to tackle the system complexity elaborately. We also outline open issues for promoting the agility and intelligence of structured satellite-UAV-terrestrial networks in the making.","sentences":["The upcoming sixth generation (6G) wireless communication network is envisioned to cover space, air, and maritime areas, in addition to urban-centered terrestrial coverage by the fifth generation (5G) network, to support intelligent Internet of Things (IoT).","Towards this end, we investigate structured integration of satellites, unmanned aerial vehicles (UAVs), and terrestrial networks, aiming to serve future universal IoT possibly with a massive number of devices in the coverage holes of current 5G.","The hybrid satellite-UAV-terrestrial network usually leads to high system complexity, due to the heterogeneity and dynamics of space/air/ground links.","With a systematic thinking, we propose to create and exploit hierarchies for the integrated network.","Four basic structures are discussed by learning from the synergies in our human body.","To orchestrate multiple heterogeneous basic structures, we further propose a process-oriented on-demand coverage method, which characterizes the system behavior as a series of events over time and is able to tackle the system complexity elaborately.","We also outline open issues for promoting the agility and intelligence of structured satellite-UAV-terrestrial networks in the making."],"url":"http://arxiv.org/abs/2402.07359v1","category":"cs.IT"}
{"created":"2024-02-12 00:44:37","title":"Antagonistic AI","abstract":"The vast majority of discourse around AI development assumes that subservient, \"moral\" models aligned with \"human values\" are universally beneficial -- in short, that good AI is sycophantic AI. We explore the shadow of the sycophantic paradigm, a design space we term antagonistic AI: AI systems that are disagreeable, rude, interrupting, confrontational, challenging, etc. -- embedding opposite behaviors or values. Far from being \"bad\" or \"immoral,\" we consider whether antagonistic AI systems may sometimes have benefits to users, such as forcing users to confront their assumptions, build resilience, or develop healthier relational boundaries. Drawing from formative explorations and a speculative design workshop where participants designed fictional AI technologies that employ antagonism, we lay out a design space for antagonistic AI, articulating potential benefits, design techniques, and methods of embedding antagonistic elements into user experience. Finally, we discuss the many ethical challenges of this space and identify three dimensions for the responsible design of antagonistic AI -- consent, context, and framing.","sentences":["The vast majority of discourse around AI development assumes that subservient, \"moral\" models aligned with \"human values\" are universally beneficial -- in short, that good AI is sycophantic AI.","We explore the shadow of the sycophantic paradigm, a design space we term antagonistic AI: AI systems that are disagreeable, rude, interrupting, confrontational, challenging, etc. -- embedding opposite behaviors or values.","Far from being \"bad\" or \"immoral,\" we consider whether antagonistic AI systems may sometimes have benefits to users, such as forcing users to confront their assumptions, build resilience, or develop healthier relational boundaries.","Drawing from formative explorations and a speculative design workshop where participants designed fictional AI technologies that employ antagonism, we lay out a design space for antagonistic AI, articulating potential benefits, design techniques, and methods of embedding antagonistic elements into user experience.","Finally, we discuss the many ethical challenges of this space and identify three dimensions for the responsible design of antagonistic AI -- consent, context, and framing."],"url":"http://arxiv.org/abs/2402.07350v1","category":"cs.AI"}
{"created":"2024-02-12 00:36:34","title":"Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble","abstract":"Recent work has shown the defense of 01 loss sign activation neural networks against image classification adversarial attacks. A public challenge to attack the models on CIFAR10 dataset remains undefeated. We ask the following question in this study: are 01 loss sign activation neural networks hard to deceive with a popular black box text adversarial attack program called TextFooler? We study this question on four popular text classification datasets: IMDB reviews, Yelp reviews, MR sentiment classification, and AG news classification. We find that our 01 loss sign activation network is much harder to attack with TextFooler compared to sigmoid activation cross entropy and binary neural networks. We also study a 01 loss sign activation convolutional neural network with a novel global pooling step specific to sign activation networks. With this new variation we see a significant gain in adversarial accuracy rendering TextFooler practically useless against it. We make our code freely available at \\url{https://github.com/zero-one-loss/wordcnn01} and \\url{https://github.com/xyzacademic/mlp01example}. Our work here suggests that 01 loss sign activation networks could be further developed to create fool proof models against text adversarial attacks.","sentences":["Recent work has shown the defense of 01 loss sign activation neural networks against image classification adversarial attacks.","A public challenge to attack the models on CIFAR10 dataset remains undefeated.","We ask the following question in this study: are 01 loss sign activation neural networks hard to deceive with a popular black box text adversarial attack program called TextFooler?","We study this question on four popular text classification datasets: IMDB reviews, Yelp reviews, MR sentiment classification, and AG news classification.","We find that our 01 loss sign activation network is much harder to attack with TextFooler compared to sigmoid activation cross entropy and binary neural networks.","We also study a 01 loss sign activation convolutional neural network with a novel global pooling step specific to sign activation networks.","With this new variation we see a significant gain in adversarial accuracy rendering TextFooler practically useless against it.","We make our code freely available at \\url{https://github.com/zero-one-loss/wordcnn01} and \\url{https://github.com/xyzacademic/mlp01example}.","Our work here suggests that 01 loss sign activation networks could be further developed to create fool proof models against text adversarial attacks."],"url":"http://arxiv.org/abs/2402.07347v1","category":"cs.LG"}
{"created":"2024-02-12 00:22:47","title":"Measurement Scheduling for ICU Patients with Offline Reinforcement Learning","abstract":"Scheduling laboratory tests for ICU patients presents a significant challenge. Studies show that 20-40% of lab tests ordered in the ICU are redundant and could be eliminated without compromising patient safety. Prior work has leveraged offline reinforcement learning (Offline-RL) to find optimal policies for ordering lab tests based on patient information. However, new ICU patient datasets have since been released, and various advancements have been made in Offline-RL methods. In this study, we first introduce a preprocessing pipeline for the newly-released MIMIC-IV dataset geared toward time-series tasks. We then explore the efficacy of state-of-the-art Offline-RL methods in identifying better policies for ICU patient lab test scheduling. Besides assessing methodological performance, we also discuss the overall suitability and practicality of using Offline-RL frameworks for scheduling laboratory tests in ICU settings.","sentences":["Scheduling laboratory tests for ICU patients presents a significant challenge.","Studies show that 20-40% of lab tests ordered in the ICU are redundant and could be eliminated without compromising patient safety.","Prior work has leveraged offline reinforcement learning (Offline-RL) to find optimal policies for ordering lab tests based on patient information.","However, new ICU patient datasets have since been released, and various advancements have been made in Offline-RL methods.","In this study, we first introduce a preprocessing pipeline for the newly-released MIMIC-IV dataset geared toward time-series tasks.","We then explore the efficacy of state-of-the-art Offline-RL methods in identifying better policies for ICU patient lab test scheduling.","Besides assessing methodological performance, we also discuss the overall suitability and practicality of using Offline-RL frameworks for scheduling laboratory tests in ICU settings."],"url":"http://arxiv.org/abs/2402.07344v1","category":"cs.LG"}
{"created":"2024-02-12 00:20:43","title":"Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation","abstract":"We ideate a future design workflow that involves AI technology. Drawing from activity and communication theory, we attempt to isolate the new value large AI models can provide design compared to past technologies. We arrive at three affordances -- dynamic grounding, constructive negotiation, and sustainable motivation -- that summarize latent qualities of natural language-enabled foundation models that, if explicitly designed for, can support the process of design. Through design fiction, we then imagine a future interface as a diegetic prototype, the story of Squirrel Game, that demonstrates each of our three affordances in a realistic usage scenario. Our design process, terminology, and diagrams aim to contribute to future discussions about the relative affordances of AI technology with regard to collaborating with human designers.","sentences":["We ideate a future design workflow that involves AI technology.","Drawing from activity and communication theory, we attempt to isolate the new value large AI models can provide design compared to past technologies.","We arrive at three affordances -- dynamic grounding, constructive negotiation, and sustainable motivation -- that summarize latent qualities of natural language-enabled foundation models that, if explicitly designed for, can support the process of design.","Through design fiction, we then imagine a future interface as a diegetic prototype, the story of Squirrel Game, that demonstrates each of our three affordances in a realistic usage scenario.","Our design process, terminology, and diagrams aim to contribute to future discussions about the relative affordances of AI technology with regard to collaborating with human designers."],"url":"http://arxiv.org/abs/2402.07342v1","category":"cs.HC"}
{"created":"2024-02-11 23:57:09","title":"Differentially Private Training of Mixture of Experts Models","abstract":"This position paper investigates the integration of Differential Privacy (DP) in the training of Mixture of Experts (MoE) models within the field of natural language processing. As Large Language Models (LLMs) scale to billions of parameters, leveraging expansive datasets, they exhibit enhanced linguistic capabilities and emergent abilities. However, this growth raises significant computational and privacy concerns. Our study addresses these issues by exploring the potential of MoE models, known for their computational efficiency, and the application of DP, a standard for privacy preservation. We present the first known attempt to train MoE models under the constraints of DP, addressing the unique challenges posed by their architecture and the complexities of DP integration. Our initial experimental studies demonstrate that MoE models can be effectively trained with DP, achieving performance that is competitive with their non-private counterparts. This initial study aims to provide valuable insights and ignite further research in the domain of privacy-preserving MoE models, softly laying the groundwork for prospective developments in this evolving field.","sentences":["This position paper investigates the integration of Differential Privacy (DP) in the training of Mixture of Experts (MoE) models within the field of natural language processing.","As Large Language Models (LLMs) scale to billions of parameters, leveraging expansive datasets, they exhibit enhanced linguistic capabilities and emergent abilities.","However, this growth raises significant computational and privacy concerns.","Our study addresses these issues by exploring the potential of MoE models, known for their computational efficiency, and the application of DP, a standard for privacy preservation.","We present the first known attempt to train MoE models under the constraints of DP, addressing the unique challenges posed by their architecture and the complexities of DP integration.","Our initial experimental studies demonstrate that MoE models can be effectively trained with DP, achieving performance that is competitive with their non-private counterparts.","This initial study aims to provide valuable insights and ignite further research in the domain of privacy-preserving MoE models, softly laying the groundwork for prospective developments in this evolving field."],"url":"http://arxiv.org/abs/2402.07334v1","category":"cs.CR"}
{"created":"2024-02-11 23:52:25","title":"Planar near-field antenna measurements with a uniform step larger than half-wavelength","abstract":"In this paper, a new sampling scheme of the near field radiated by a planar source is proposed and assessed. More in detail, the paper shows a uniform sampling criterion that allows representing the near field over a plane with a number of measurements lower than the classical half-wavelength sampling. At first, a discretization strategy of the near field based on the warping method is recalled from the literature. The latter requires to collect a non-redundant number of field measurements that are non-uniformly arranged over the observation domain. Despite this, the warping sampling scheme works well only if the measurement plane does not overcome the source. When the observation domain is larger, it does not predict the exact positions of the field samples at the edges of the measurement plane; accordingly, in these regions it is not possible to recover the near field behavior by the collected samples. To overcome this drawback, a spatially varying oversampling is exploited. The latter is chosen in such a way that the resulting sampling becomes uniform. Such choice also ensures a growth of the sampling rate only at the edges of the observation domain permitting the retrieval of the near field by its samples. Finally, numerical simulations based on experimental data corroborate the effectiveness of the approach in recovering both the near and the far field.","sentences":["In this paper, a new sampling scheme of the near field radiated by a planar source is proposed and assessed.","More in detail, the paper shows a uniform sampling criterion that allows representing the near field over a plane with a number of measurements lower than the classical half-wavelength sampling.","At first, a discretization strategy of the near field based on the warping method is recalled from the literature.","The latter requires to collect a non-redundant number of field measurements that are non-uniformly arranged over the observation domain.","Despite this, the warping sampling scheme works well only if the measurement plane does not overcome the source.","When the observation domain is larger, it does not predict the exact positions of the field samples at the edges of the measurement plane; accordingly, in these regions it is not possible to recover the near field behavior by the collected samples.","To overcome this drawback, a spatially varying oversampling is exploited.","The latter is chosen in such a way that the resulting sampling becomes uniform.","Such choice also ensures a growth of the sampling rate only at the edges of the observation domain permitting the retrieval of the near field by its samples.","Finally, numerical simulations based on experimental data corroborate the effectiveness of the approach in recovering both the near and the far field."],"url":"http://arxiv.org/abs/2402.07333v1","category":"eess.SP"}
{"created":"2024-02-11 23:50:12","title":"Intent-Based Access Control: Using LLMs to Intelligently Manage Access Control","abstract":"In every enterprise database, administrators must define an access control policy that specifies which users have access to which assets. Access control straddles two worlds: policy (organization-level principles that define who should have access) and process (database-level primitives that actually implement the policy). Assessing and enforcing process compliance with a policy is a manual and ad-hoc task. This paper introduces a new paradigm for access control called Intent-Based Access Control for Databases (IBAC-DB). In IBAC-DB, access control policies are expressed more precisely using a novel format, the natural language access control matrix (NLACM). Database access control primitives are synthesized automatically from these NLACMs. These primitives can be used to generate new DB configurations and/or evaluate existing ones. This paper presents a reference architecture for an IBAC-DB interface, an initial implementation for PostgreSQL (which we call LLM4AC), and initial benchmarks that evaluate the accuracy and scope of such a system. We find that our chosen implementation, LLM4AC, vastly outperforms other baselines, achieving near-perfect F1 scores on our initial benchmarks.","sentences":["In every enterprise database, administrators must define an access control policy that specifies which users have access to which assets.","Access control straddles two worlds: policy (organization-level principles that define who should have access) and process (database-level primitives that actually implement the policy).","Assessing and enforcing process compliance with a policy is a manual and ad-hoc task.","This paper introduces a new paradigm for access control called Intent-Based Access Control for Databases (IBAC-DB).","In IBAC-DB, access control policies are expressed more precisely using a novel format, the natural language access control matrix (NLACM).","Database access control primitives are synthesized automatically from these NLACMs.","These primitives can be used to generate new DB configurations and/or evaluate existing ones.","This paper presents a reference architecture for an IBAC-DB interface, an initial implementation for PostgreSQL (which we call LLM4AC), and initial benchmarks that evaluate the accuracy and scope of such a system.","We find that our chosen implementation, LLM4AC, vastly outperforms other baselines, achieving near-perfect F1 scores on our initial benchmarks."],"url":"http://arxiv.org/abs/2402.07332v1","category":"cs.DB"}
{"created":"2024-02-11 23:27:24","title":"Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers","abstract":"Due to the complex nature of human emotions and the diversity of emotion representation methods in humans, emotion recognition is a challenging field. In this research, three input modalities, namely text, audio (speech), and video, are employed to generate multimodal feature vectors. For generating features for each of these modalities, pre-trained Transformer models with fine-tuning are utilized. In each modality, a Transformer model is used with transfer learning to extract feature and emotional structure. These features are then fused together, and emotion recognition is performed using a classifier. To select an appropriate fusion method and classifier, various feature-level and decision-level fusion techniques have been experimented with, and ultimately, the best model, which combines feature-level fusion by concatenating feature vectors and classification using a Support Vector Machine on the IEMOCAP multimodal dataset, achieves an accuracy of 75.42%. Keywords: Multimodal Emotion Recognition, IEMOCAP, Self-Supervised Learning, Transfer Learning, Transformer.","sentences":["Due to the complex nature of human emotions and the diversity of emotion representation methods in humans, emotion recognition is a challenging field.","In this research, three input modalities, namely text, audio (speech), and video, are employed to generate multimodal feature vectors.","For generating features for each of these modalities, pre-trained Transformer models with fine-tuning are utilized.","In each modality, a Transformer model is used with transfer learning to extract feature and emotional structure.","These features are then fused together, and emotion recognition is performed using a classifier.","To select an appropriate fusion method and classifier, various feature-level and decision-level fusion techniques have been experimented with, and ultimately, the best model, which combines feature-level fusion by concatenating feature vectors and classification using a Support Vector Machine on the IEMOCAP multimodal dataset, achieves an accuracy of 75.42%.","Keywords: Multimodal Emotion Recognition, IEMOCAP, Self-Supervised Learning, Transfer Learning, Transformer."],"url":"http://arxiv.org/abs/2402.07327v1","category":"cs.AI"}
{"created":"2024-02-11 23:23:31","title":"Persian Speech Emotion Recognition by Fine-Tuning Transformers","abstract":"Given the significance of speech emotion recognition, numerous methods have been developed in recent years to create effective and efficient systems in this domain. One of these methods involves the use of pretrained transformers, fine-tuned to address this specific problem, resulting in high accuracy. Despite extensive discussions and global-scale efforts to enhance these systems, the application of this innovative and effective approach has received less attention in the context of Persian speech emotion recognition. In this article, we review the field of speech emotion recognition and its background, with an emphasis on the importance of employing transformers in this context. We present two models, one based on spectrograms and the other on the audio itself, fine-tuned using the shEMO dataset. These models significantly enhance the accuracy of previous systems, increasing it from approximately 65% to 80% on the mentioned dataset. Subsequently, to investigate the effect of multilinguality on the fine-tuning process, these same models are fine-tuned twice. First, they are fine-tuned using the English IEMOCAP dataset, and then they are fine-tuned with the Persian shEMO dataset. This results in an improved accuracy of 82% for the Persian emotion recognition system. Keywords: Persian Speech Emotion Recognition, shEMO, Self-Supervised Learning","sentences":["Given the significance of speech emotion recognition, numerous methods have been developed in recent years to create effective and efficient systems in this domain.","One of these methods involves the use of pretrained transformers, fine-tuned to address this specific problem, resulting in high accuracy.","Despite extensive discussions and global-scale efforts to enhance these systems, the application of this innovative and effective approach has received less attention in the context of Persian speech emotion recognition.","In this article, we review the field of speech emotion recognition and its background, with an emphasis on the importance of employing transformers in this context.","We present two models, one based on spectrograms and the other on the audio itself, fine-tuned using the shEMO dataset.","These models significantly enhance the accuracy of previous systems, increasing it from approximately 65% to 80% on the mentioned dataset.","Subsequently, to investigate the effect of multilinguality on the fine-tuning process, these same models are fine-tuned twice.","First, they are fine-tuned using the English IEMOCAP dataset, and then they are fine-tuned with the Persian shEMO dataset.","This results in an improved accuracy of 82% for the Persian emotion recognition system.","Keywords: Persian Speech Emotion Recognition, shEMO, Self-Supervised Learning"],"url":"http://arxiv.org/abs/2402.07326v1","category":"cs.AI"}
{"created":"2024-02-11 23:14:56","title":"Adaptive Voronoi-based Column Selection Methods for Interpretable Dimensionality Reduction","abstract":"In data analysis, there continues to be a need for interpretable dimensionality reduction methods whereby instrinic meaning associated with the data is retained in the reduced space. Standard approaches such as Principal Component Analysis (PCA) and the Singular Value Decomposition (SVD) fail at this task. A popular alternative is the CUR decomposition. In an SVD-like manner, the CUR decomposition approximates a matrix $A \\in \\mathbb{R}^{m \\times n}$ as $A \\approx CUR$, where $C$ and $R$ are matrices whose columns and rows are selected from the original matrix \\cite{goreinov1997theory}, \\cite{mahoney2009cur}. The difficulty in constructing a CUR decomposition is in determining which columns and rows to select when forming $C$ and $R$. Current column/row selection algorithms, particularly those that rely on an SVD, become infeasible as the size of the data becomes large \\cite{dong2021simpler}. We address this problem by reducing the column/row selection problem to a collection of smaller sub-problems. The basic idea is to first partition the rows/columns of a matrix, and then apply an existing selection algorithm on each piece; for illustration purposes we use the Discrete Empirical Interpolation Method (\\textsf{DEIM}) \\cite{sorensen2016deim}. For the first task, we consider two existing algorithms that construct a Voronoi Tessellation (VT) of the rows and columns of a given matrix. We then extend these methods to automatically adapt to the data. The result is four data-driven row/column selection methods that are well-suited for parallelization, and compatible with nearly any existing column/row selection strategy. Theory and numerical examples show the design to be competitive with the original \\textsf{DEIM} routine.","sentences":["In data analysis, there continues to be a need for interpretable dimensionality reduction methods whereby instrinic meaning associated with the data is retained in the reduced space.","Standard approaches such as Principal Component Analysis (PCA) and the Singular Value Decomposition (SVD) fail at this task.","A popular alternative is the CUR decomposition.","In an SVD-like manner, the CUR decomposition approximates a matrix $A \\in \\mathbb{R}^{m \\times n}$ as $A \\approx CUR$, where $C$ and $R$ are matrices whose columns and rows are selected from the original matrix \\cite{goreinov1997theory}, \\cite{mahoney2009cur}.","The difficulty in constructing a CUR decomposition is in determining which columns and rows to select when forming $C$ and $R$. Current column/row selection algorithms, particularly those that rely on an SVD, become infeasible as the size of the data becomes large \\cite{dong2021simpler}.","We address this problem by reducing the column/row selection problem to a collection of smaller sub-problems.","The basic idea is to first partition the rows/columns of a matrix, and then apply an existing selection algorithm on each piece; for illustration purposes we use the Discrete Empirical Interpolation Method (\\textsf{DEIM}) \\cite{sorensen2016deim}.","For the first task, we consider two existing algorithms that construct a Voronoi Tessellation (VT) of the rows and columns of a given matrix.","We then extend these methods to automatically adapt to the data.","The result is four data-driven row/column selection methods that are well-suited for parallelization, and compatible with nearly any existing column/row selection strategy.","Theory and numerical examples show the design to be competitive with the original \\textsf{DEIM} routine."],"url":"http://arxiv.org/abs/2402.07325v1","category":"math.NA"}
{"created":"2024-02-11 22:59:19","title":"Lessons Learned from Mining the Hugging Face Repository","abstract":"The rapidly evolving fields of Machine Learning (ML) and Artificial Intelligence have witnessed the emergence of platforms like Hugging Face (HF) as central hubs for model development and sharing. This experience report synthesizes insights from two comprehensive studies conducted on HF, focusing on carbon emissions and the evolutionary and maintenance aspects of ML models. Our objective is to provide a practical guide for future researchers embarking on mining software repository studies within the HF ecosystem to enhance the quality of these studies. We delve into the intricacies of the replication package used in our studies, highlighting the pivotal tools and methodologies that facilitated our analysis. Furthermore, we propose a nuanced stratified sampling strategy tailored for the diverse HF Hub dataset, ensuring a representative and comprehensive analytical approach. The report also introduces preliminary guidelines, transitioning from repository mining to cohort studies, to establish causality in repository mining studies, particularly within the ML model of HF context. This transition is inspired by existing frameworks and is adapted to suit the unique characteristics of the HF model ecosystem. Our report serves as a guiding framework for researchers, contributing to the responsible and sustainable advancement of ML, and fostering a deeper understanding of the broader implications of ML models.","sentences":["The rapidly evolving fields of Machine Learning (ML) and Artificial Intelligence have witnessed the emergence of platforms like Hugging Face (HF) as central hubs for model development and sharing.","This experience report synthesizes insights from two comprehensive studies conducted on HF, focusing on carbon emissions and the evolutionary and maintenance aspects of ML models.","Our objective is to provide a practical guide for future researchers embarking on mining software repository studies within the HF ecosystem to enhance the quality of these studies.","We delve into the intricacies of the replication package used in our studies, highlighting the pivotal tools and methodologies that facilitated our analysis.","Furthermore, we propose a nuanced stratified sampling strategy tailored for the diverse HF Hub dataset, ensuring a representative and comprehensive analytical approach.","The report also introduces preliminary guidelines, transitioning from repository mining to cohort studies, to establish causality in repository mining studies, particularly within the ML model of HF context.","This transition is inspired by existing frameworks and is adapted to suit the unique characteristics of the HF model ecosystem.","Our report serves as a guiding framework for researchers, contributing to the responsible and sustainable advancement of ML, and fostering a deeper understanding of the broader implications of ML models."],"url":"http://arxiv.org/abs/2402.07323v1","category":"cs.SE"}
{"created":"2024-02-11 22:53:21","title":"Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets","abstract":"This research explores the integration of language embeddings for active learning in autonomous driving datasets, with a focus on novelty detection. Novelty arises from unexpected scenarios that autonomous vehicles struggle to navigate, necessitating higher-level reasoning abilities. Our proposed method employs language-based representations to identify novel scenes, emphasizing the dual purpose of safety takeover responses and active learning. The research presents a clustering experiment using Contrastive Language-Image Pretrained (CLIP) embeddings to organize datasets and detect novelties. We find that the proposed algorithm effectively isolates novel scenes from a collection of subsets derived from two real-world driving datasets, one vehicle-mounted and one infrastructure-mounted. From the generated clusters, we further present methods for generating textual explanations of elements which differentiate scenes classified as novel from other scenes in the data pool, presenting qualitative examples from the clustered results. Our results demonstrate the effectiveness of language-driven embeddings in identifying novel elements and generating explanations of data, and we further discuss potential applications in safe takeovers, data curation, and multi-task active learning.","sentences":["This research explores the integration of language embeddings for active learning in autonomous driving datasets, with a focus on novelty detection.","Novelty arises from unexpected scenarios that autonomous vehicles struggle to navigate, necessitating higher-level reasoning abilities.","Our proposed method employs language-based representations to identify novel scenes, emphasizing the dual purpose of safety takeover responses and active learning.","The research presents a clustering experiment using Contrastive Language-Image Pretrained (CLIP) embeddings to organize datasets and detect novelties.","We find that the proposed algorithm effectively isolates novel scenes from a collection of subsets derived from two real-world driving datasets, one vehicle-mounted and one infrastructure-mounted.","From the generated clusters, we further present methods for generating textual explanations of elements which differentiate scenes classified as novel from other scenes in the data pool, presenting qualitative examples from the clustered results.","Our results demonstrate the effectiveness of language-driven embeddings in identifying novel elements and generating explanations of data, and we further discuss potential applications in safe takeovers, data curation, and multi-task active learning."],"url":"http://arxiv.org/abs/2402.07320v1","category":"cs.CV"}
{"created":"2024-02-11 22:40:12","title":"ODIN: Disentangled Reward Mitigates Hacking in RLHF","abstract":"In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators to achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads on shared feature representations to predict the rewards, one trained to correlate with length, and the other trained to decorrelate with length and therefore focus more on the actual content. We then discard the length head in RL to prevent reward hacking on length. Experiments demonstrate that our approach almost eliminates the reward correlation with length, and improves the obtained policy by a significant margin.","sentences":["In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs.","A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators to achieve high scores.","The same issue also holds for some reward models in RL.","To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters.","Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias.","We further propose to improve the reward model by jointly training two linear heads on shared feature representations to predict the rewards, one trained to correlate with length, and the other trained to decorrelate with length and therefore focus more on the actual content.","We then discard the length head in RL to prevent reward hacking on length.","Experiments demonstrate that our approach almost eliminates the reward correlation with length, and improves the obtained policy by a significant margin."],"url":"http://arxiv.org/abs/2402.07319v1","category":"cs.LG"}
{"created":"2024-02-11 21:44:21","title":"A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference","abstract":"Reinforcement Learning from Human Feedback (RLHF) learns from the preference signal provided by a probabilistic preference model, which takes a prompt and two responses as input, and produces a score indicating the preference of one response against another. So far, the most popular RLHF paradigm is reward-based, which starts with an initial step of reward modeling, and the constructed reward is then used to provide a reward signal for the subsequent reward optimization stage. However, the existence of a reward function is a strong assumption and the reward-based RLHF is limited in expressivity and cannot capture the real-world complicated human preference.   In this work, we provide theoretical insights for a recently proposed learning paradigm, Nash learning from human feedback (NLHF), which considered a general preference model and formulated the alignment process as a game between two competitive LLMs. The learning objective is to find a policy that consistently generates responses preferred over any competing policy while staying close to the initial model. The objective is defined as the Nash equilibrium (NE) of the KL-regularized preference model. We aim to make the first attempt to study the theoretical learnability of the KL-regularized NLHF by considering both offline and online settings. For the offline learning from a pre-collected dataset, we propose algorithms that are efficient under suitable coverage conditions of the dataset. For batch online learning from iterative interactions with a preference oracle, our proposed algorithm enjoys a finite sample guarantee under the structural condition of the underlying preference model. Our results connect the new NLHF paradigm with traditional RL theory, and validate the potential of reward-model-free learning under general preference.","sentences":["Reinforcement Learning from Human Feedback (RLHF) learns from the preference signal provided by a probabilistic preference model, which takes a prompt and two responses as input, and produces a score indicating the preference of one response against another.","So far, the most popular RLHF paradigm is reward-based, which starts with an initial step of reward modeling, and the constructed reward is then used to provide a reward signal for the subsequent reward optimization stage.","However, the existence of a reward function is a strong assumption and the reward-based RLHF is limited in expressivity and cannot capture the real-world complicated human preference.   ","In this work, we provide theoretical insights for a recently proposed learning paradigm, Nash learning from human feedback (NLHF), which considered a general preference model and formulated the alignment process as a game between two competitive LLMs.","The learning objective is to find a policy that consistently generates responses preferred over any competing policy while staying close to the initial model.","The objective is defined as the Nash equilibrium (NE) of the KL-regularized preference model.","We aim to make the first attempt to study the theoretical learnability of the KL-regularized NLHF by considering both offline and online settings.","For the offline learning from a pre-collected dataset, we propose algorithms that are efficient under suitable coverage conditions of the dataset.","For batch online learning from iterative interactions with a preference oracle, our proposed algorithm enjoys a finite sample guarantee under the structural condition of the underlying preference model.","Our results connect the new NLHF paradigm with traditional RL theory, and validate the potential of reward-model-free learning under general preference."],"url":"http://arxiv.org/abs/2402.07314v1","category":"cs.LG"}
{"created":"2024-02-11 20:15:52","title":"Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning","abstract":"Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized. Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders. However, existing serverless FL systems implicitly assume a uniform global model architecture across all participating clients during training. This assumption fails to address fundamental challenges in practical FL due to the resource and statistical data heterogeneity among FL clients. To address these challenges and enable heterogeneous client models in serverless FL, we utilize Knowledge Distillation (KD) in this paper. Towards this, we propose novel optimized serverless workflows for two popular conventional federated KD techniques, i.e., FedMD and FedDF. We implement these workflows by introducing several extensions to an open-source serverless FL system called FedLess. Moreover, we comprehensively evaluate the two strategies on multiple datasets across varying levels of client data heterogeneity using heterogeneous client models with respect to accuracy, fine-grained training times, and costs. Results from our experiments demonstrate that serverless FedDF is more robust to extreme non-IID data distributions, is faster, and leads to lower costs than serverless FedMD. In addition, compared to the original implementation, our optimizations for particular steps in FedMD and FedDF lead to an average speedup of 3.5x and 1.76x across all datasets.","sentences":["Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized.","Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders.","However, existing serverless FL systems implicitly assume a uniform global model architecture across all participating clients during training.","This assumption fails to address fundamental challenges in practical FL due to the resource and statistical data heterogeneity among FL clients.","To address these challenges and enable heterogeneous client models in serverless FL, we utilize Knowledge Distillation (KD) in this paper.","Towards this, we propose novel optimized serverless workflows for two popular conventional federated KD techniques, i.e., FedMD and FedDF.","We implement these workflows by introducing several extensions to an open-source serverless FL system called FedLess.","Moreover, we comprehensively evaluate the two strategies on multiple datasets across varying levels of client data heterogeneity using heterogeneous client models with respect to accuracy, fine-grained training times, and costs.","Results from our experiments demonstrate that serverless FedDF is more robust to extreme non-IID data distributions, is faster, and leads to lower costs than serverless FedMD.","In addition, compared to the original implementation, our optimizations for particular steps in FedMD and FedDF lead to an average speedup of 3.5x and 1.76x across all datasets."],"url":"http://arxiv.org/abs/2402.07295v1","category":"cs.LG"}
{"created":"2024-02-11 20:15:44","title":"On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study","abstract":"Static call graph (CG) construction often over-approximates call relations, leading to sound, but imprecise results. Recent research has explored machine learning (ML)-based CG pruning as a means to enhance precision by eliminating false edges. However, current methods suffer from a limited evaluation dataset, imbalanced training data, and reduced recall, which affects practical downstream analyses. Prior results were also not compared with advanced static CG construction techniques yet. This study tackles these issues. We introduce the NYXCorpus, a dataset of real-world Java programs with high test coverage and we collect traces from test executions and build a ground truth of dynamic CGs. We leverage these CGs to explore conservative pruning strategies during the training and inference of ML-based CG pruners. We conduct a comparative analysis of static CGs generated using zero control flow analysis (0-CFA) and those produced by a context-sensitive 1-CFA algorithm, evaluating both with and without pruning. We find that CG pruning is a difficult task for real-world Java projects and substantial improvements in the CG precision (+25%) meet reduced recall (-9%). However, our experiments show promising results: even when we favor recall over precision by using an F2 metric in our experiments, we can show that pruned CGs have comparable quality to a context-sensitive 1-CFA analysis while being computationally less demanding. Resulting CGs are much smaller (69%), and substantially faster (3.5x speed-up), with virtually unchanged results in our downstream analysis.","sentences":["Static call graph (CG) construction often over-approximates call relations, leading to sound, but imprecise results.","Recent research has explored machine learning (ML)-based CG pruning as a means to enhance precision by eliminating false edges.","However, current methods suffer from a limited evaluation dataset, imbalanced training data, and reduced recall, which affects practical downstream analyses.","Prior results were also not compared with advanced static CG construction techniques yet.","This study tackles these issues.","We introduce the NYXCorpus, a dataset of real-world Java programs with high test coverage and we collect traces from test executions and build a ground truth of dynamic CGs.","We leverage these CGs to explore conservative pruning strategies during the training and inference of ML-based CG pruners.","We conduct a comparative analysis of static CGs generated using zero control flow analysis (0-CFA) and those produced by a context-sensitive 1-CFA algorithm, evaluating both with and without pruning.","We find that CG pruning is a difficult task for real-world Java projects and substantial improvements in the CG precision (+25%) meet reduced recall (-9%).","However, our experiments show promising results: even when we favor recall over precision by using an F2 metric in our experiments, we can show that pruned CGs have comparable quality to a context-sensitive 1-CFA analysis while being computationally less demanding.","Resulting CGs are much smaller (69%), and substantially faster (3.5x speed-up), with virtually unchanged results in our downstream analysis."],"url":"http://arxiv.org/abs/2402.07294v1","category":"cs.SE"}
{"created":"2024-02-11 19:13:26","title":"How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?","abstract":"In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.","sentences":["In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener.","How do large language models (LLMs) handle such nuanced trade-offs?","To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs.","We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs.","We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty.","Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context.","Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting."],"url":"http://arxiv.org/abs/2402.07282v1","category":"cs.CL"}
{"created":"2024-02-11 18:23:54","title":"Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer","abstract":"Biomarker identification is critical for precise disease diagnosis and understanding disease pathogenesis in omics data analysis, like using fold change and regression analysis. Graph neural networks (GNNs) have been the dominant deep learning model for analyzing graph-structured data. However, we found two major limitations of existing GNNs in omics data analysis, i.e., limited-prediction (diagnosis) accuracy and limited-reproducible biomarker identification capacity across multiple datasets. The root of the challenges is the unique graph structure of biological signaling pathways, which consists of a large number of targets and intensive and complex signaling interactions among these targets. To resolve these two challenges, in this study, we presented a novel GNN model architecture, named PathFormer, which systematically integrate signaling network, priori knowledge and omics data to rank biomarkers and predict disease diagnosis. In the comparison results, PathFormer outperformed existing GNN models significantly in terms of highly accurate prediction capability ( 30% accuracy improvement in disease diagnosis compared with existing GNN models) and high reproducibility of biomarker ranking across different datasets. The improvement was confirmed using two independent Alzheimer's Disease (AD) and cancer transcriptomic datasets. The PathFormer model can be directly applied to other omics data analysis studies.","sentences":["Biomarker identification is critical for precise disease diagnosis and understanding disease pathogenesis in omics data analysis, like using fold change and regression analysis.","Graph neural networks (GNNs) have been the dominant deep learning model for analyzing graph-structured data.","However, we found two major limitations of existing GNNs in omics data analysis, i.e., limited-prediction (diagnosis) accuracy and limited-reproducible biomarker identification capacity across multiple datasets.","The root of the challenges is the unique graph structure of biological signaling pathways, which consists of a large number of targets and intensive and complex signaling interactions among these targets.","To resolve these two challenges, in this study, we presented a novel GNN model architecture, named PathFormer, which systematically integrate signaling network, priori knowledge and omics data to rank biomarkers and predict disease diagnosis.","In the comparison results, PathFormer outperformed existing GNN models significantly in terms of highly accurate prediction capability ( 30% accuracy improvement in disease diagnosis compared with existing GNN models) and high reproducibility of biomarker ranking across different datasets.","The improvement was confirmed using two independent Alzheimer's Disease (AD) and cancer transcriptomic datasets.","The PathFormer model can be directly applied to other omics data analysis studies."],"url":"http://arxiv.org/abs/2402.07268v1","category":"q-bio.GN"}
{"created":"2024-02-11 18:11:12","title":"Trade-off Between Spatial and Angular Resolution in Facial Recognition","abstract":"Ensuring robustness in face recognition systems across various challenging conditions is crucial for their versatility. State-of-the-art methods often incorporate additional information, such as depth, thermal, or angular data, to enhance performance. However, light field-based face recognition approaches that leverage angular information face computational limitations. This paper investigates the fundamental trade-off between spatio-angular resolution in light field representation to achieve improved face recognition performance. By utilizing macro-pixels with varying angular resolutions while maintaining the overall image size, we aim to quantify the impact of angular information at the expense of spatial resolution, while considering computational constraints. Our experimental results demonstrate a notable performance improvement in face recognition systems by increasing the angular resolution, up to a certain extent, at the cost of spatial resolution.","sentences":["Ensuring robustness in face recognition systems across various challenging conditions is crucial for their versatility.","State-of-the-art methods often incorporate additional information, such as depth, thermal, or angular data, to enhance performance.","However, light field-based face recognition approaches that leverage angular information face computational limitations.","This paper investigates the fundamental trade-off between spatio-angular resolution in light field representation to achieve improved face recognition performance.","By utilizing macro-pixels with varying angular resolutions while maintaining the overall image size, we aim to quantify the impact of angular information at the expense of spatial resolution, while considering computational constraints.","Our experimental results demonstrate a notable performance improvement in face recognition systems by increasing the angular resolution, up to a certain extent, at the cost of spatial resolution."],"url":"http://arxiv.org/abs/2402.07263v1","category":"cs.CV"}
{"created":"2024-02-11 18:09:15","title":"A Novel Technique to Parameterize Congestion Control in 6TiSCH IIoT Networks","abstract":"The Industrial Internet of Things (IIoT) refers to the use of interconnected smart devices, sensors, and other technologies to create a network of intelligent systems that can monitor and manage industrial processes. 6TiSCH (IPv6 over the Time Slotted Channel Hopping mode of IEEE 802.15.4e) as an enabling technology facilitates low-power and low-latency communication between IoT devices in industrial environments. The Routing Protocol for Low power and lossy networks (RPL), which is used as the de-facto routing protocol for 6TiSCH networks is observed to suffer from several limitations, especially during congestion in the network. Therefore, there is an immediate need for some modifications to the RPL to deal with this problem. Under traffic load which keeps on changing continuously at different instants of time, the proposed mechanism aims at finding the appropriate parent for a node that can forward the packet to the destination through the least congested path with minimal packet loss. This facilitates congestion management under dynamic traffic loads. For this, a new metric for routing using the concept of exponential weighting has been proposed, which takes the number of packets present in the queue of the node into account when choosing the parent at a particular instance of time. Additionally, the paper proposes a parent selection and swapping mechanism for congested networks. Performance evaluations are carried out in order to validate the proposed work. The results show an improvement in the performance of RPL under heavy and dynamic traffic loads.","sentences":["The Industrial Internet of Things (IIoT) refers to the use of interconnected smart devices, sensors, and other technologies to create a network of intelligent systems that can monitor and manage industrial processes.","6TiSCH (IPv6 over the Time Slotted Channel Hopping mode of IEEE 802.15.4e) as an enabling technology facilitates low-power and low-latency communication between IoT devices in industrial environments.","The Routing Protocol for Low power and lossy networks (RPL), which is used as the de-facto routing protocol for 6TiSCH networks is observed to suffer from several limitations, especially during congestion in the network.","Therefore, there is an immediate need for some modifications to the RPL to deal with this problem.","Under traffic load which keeps on changing continuously at different instants of time, the proposed mechanism aims at finding the appropriate parent for a node that can forward the packet to the destination through the least congested path with minimal packet loss.","This facilitates congestion management under dynamic traffic loads.","For this, a new metric for routing using the concept of exponential weighting has been proposed, which takes the number of packets present in the queue of the node into account when choosing the parent at a particular instance of time.","Additionally, the paper proposes a parent selection and swapping mechanism for congested networks.","Performance evaluations are carried out in order to validate the proposed work.","The results show an improvement in the performance of RPL under heavy and dynamic traffic loads."],"url":"http://arxiv.org/abs/2402.07261v1","category":"cs.NI"}
{"created":"2024-02-11 18:04:06","title":"RIS-Augmented Millimeter-Wave MIMO Systems for Passive Drone Detection","abstract":"In the past decade, the number of amateur drones is increasing, and this trend is expected to continue in the future. The security issues brought by abuse and misconduct of drones become more and more severe and may incur a negative impact to the society. In this paper, we leverage existing cellular multiple-input multiple-output (MIMO) base station (BS) infrastructure, operating at millimeter wave (mmWave) frequency bands, for drone detection in a device-free manner with the aid of one reconfigurable intelligent surface (RIS), deployed in the proximity of the BS. We theoretically examine the feasibility of drone detection with the aid of the generalized likelihood ratio test (GLRT) and validate via simulations that, the optimized deployment of an RIS can bring added benefits compared to RIS-free systems. In addition, the effect of RIS training beams, training overhead, and radar cross section, is investigated in order to offer theoretical design guidance for the proposed cellular RIS-based passive drone detection system.","sentences":["In the past decade, the number of amateur drones is increasing, and this trend is expected to continue in the future.","The security issues brought by abuse and misconduct of drones become more and more severe and may incur a negative impact to the society.","In this paper, we leverage existing cellular multiple-input multiple-output (MIMO) base station (BS) infrastructure, operating at millimeter wave (mmWave) frequency bands, for drone detection in a device-free manner with the aid of one reconfigurable intelligent surface (RIS), deployed in the proximity of the BS.","We theoretically examine the feasibility of drone detection with the aid of the generalized likelihood ratio test (GLRT) and validate via simulations that, the optimized deployment of an RIS can bring added benefits compared to RIS-free systems.","In addition, the effect of RIS training beams, training overhead, and radar cross section, is investigated in order to offer theoretical design guidance for the proposed cellular RIS-based passive drone detection system."],"url":"http://arxiv.org/abs/2402.07259v1","category":"eess.SP"}
{"created":"2024-02-11 18:01:52","title":"Data Quality Aware Approaches for Addressing Model Drift of Semantic Segmentation Models","abstract":"In the midst of the rapid integration of artificial intelligence (AI) into real world applications, one pressing challenge we confront is the phenomenon of model drift, wherein the performance of AI models gradually degrades over time, compromising their effectiveness in real-world, dynamic environments. Once identified, we need techniques for handling this drift to preserve the model performance and prevent further degradation. This study investigates two prominent quality aware strategies to combat model drift: data quality assessment and data conditioning based on prior model knowledge. The former leverages image quality assessment metrics to meticulously select high-quality training data, improving the model robustness, while the latter makes use of learned feature vectors from existing models to guide the selection of future data, aligning it with the model's prior knowledge. Through comprehensive experimentation, this research aims to shed light on the efficacy of these approaches in enhancing the performance and reliability of semantic segmentation models, thereby contributing to the advancement of computer vision capabilities in real-world scenarios.","sentences":["In the midst of the rapid integration of artificial intelligence (AI) into real world applications, one pressing challenge we confront is the phenomenon of model drift, wherein the performance of AI models gradually degrades over time, compromising their effectiveness in real-world, dynamic environments.","Once identified, we need techniques for handling this drift to preserve the model performance and prevent further degradation.","This study investigates two prominent quality aware strategies to combat model drift: data quality assessment and data conditioning based on prior model knowledge.","The former leverages image quality assessment metrics to meticulously select high-quality training data, improving the model robustness, while the latter makes use of learned feature vectors from existing models to guide the selection of future data, aligning it with the model's prior knowledge.","Through comprehensive experimentation, this research aims to shed light on the efficacy of these approaches in enhancing the performance and reliability of semantic segmentation models, thereby contributing to the advancement of computer vision capabilities in real-world scenarios."],"url":"http://arxiv.org/abs/2402.07258v1","category":"cs.CV"}
{"created":"2024-02-11 17:32:23","title":"DIMON: Learning Solution Operators of Partial Differential Equations on a Diffeomorphic Family of Domains","abstract":"The solution of a PDE over varying initial/boundary conditions on multiple domains is needed in a wide variety of applications, but it is computationally expensive if the solution is computed de novo whenever the initial/boundary conditions of the domain change. We introduce a general operator learning framework, called DIffeomorphic Mapping Operator learNing (DIMON) to learn approximate PDE solutions over a family of domains $\\{\\Omega_{\\theta}}_\\theta$, that learns the map from initial/boundary conditions and domain $\\Omega_\\theta$ to the solution of the PDE, or to specified functionals thereof. DIMON is based on transporting a given problem (initial/boundary conditions and domain $\\Omega_{\\theta}$) to a problem on a reference domain $\\Omega_{0}$, where training data from multiple problems is used to learn the map to the solution on $\\Omega_{0}$, which is then re-mapped to the original domain $\\Omega_{\\theta}$. We consider several problems to demonstrate the performance of the framework in learning both static and time-dependent PDEs on non-rigid geometries; these include solving the Laplace equation, reaction-diffusion equations, and a multiscale PDE that characterizes the electrical propagation on the left ventricle. This work paves the way toward the fast prediction of PDE solutions on a family of domains and the application of neural operators in engineering and precision medicine.","sentences":["The solution of a PDE over varying initial/boundary conditions on multiple domains is needed in a wide variety of applications, but it is computationally expensive if the solution is computed de novo whenever the initial/boundary conditions of the domain change.","We introduce a general operator learning framework, called DIffeomorphic Mapping Operator learNing (DIMON) to learn approximate PDE solutions over a family of domains $\\{\\Omega_{\\theta}}_\\theta$, that learns the map from initial/boundary conditions and domain $\\Omega_\\theta$ to the solution of the PDE, or to specified functionals thereof.","DIMON is based on transporting a given problem (initial/boundary conditions and domain $\\Omega_{\\theta}$) to a problem on a reference domain $\\Omega_{0}$, where training data from multiple problems is used to learn the map to the solution on $\\Omega_{0}$, which is then re-mapped to the original domain $\\Omega_{\\theta}$. We consider several problems to demonstrate the performance of the framework in learning both static and time-dependent PDEs on non-rigid geometries; these include solving the Laplace equation, reaction-diffusion equations, and a multiscale PDE that characterizes the electrical propagation on the left ventricle.","This work paves the way toward the fast prediction of PDE solutions on a family of domains and the application of neural operators in engineering and precision medicine."],"url":"http://arxiv.org/abs/2402.07250v1","category":"cs.LG"}
{"created":"2024-02-11 17:29:58","title":"The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey","abstract":"The precise prediction of molecular properties is essential for advancements in drug development, particularly in virtual screening and compound optimization. The recent introduction of numerous deep learning-based methods has shown remarkable potential in enhancing molecular property prediction (MPP), especially improving accuracy and insights into molecular structures. Yet, two critical questions arise: does the integration of domain knowledge augment the accuracy of molecular property prediction and does employing multi-modal data fusion yield more precise results than unique data source methods? To explore these matters, we comprehensively review and quantitatively analyze recent deep learning methods based on various benchmarks. We discover that integrating molecular information will improve both MPP regression and classification tasks by upto 3.98% and 1.72%, respectively. We also discover that the utilizing 3-dimensional information with 1-dimensional and 2-dimensional information simultaneously can substantially enhance MPP upto 4.2%. The two consolidated insights offer crucial guidance for future advancements in drug discovery.","sentences":["The precise prediction of molecular properties is essential for advancements in drug development, particularly in virtual screening and compound optimization.","The recent introduction of numerous deep learning-based methods has shown remarkable potential in enhancing molecular property prediction (MPP), especially improving accuracy and insights into molecular structures.","Yet, two critical questions arise: does the integration of domain knowledge augment the accuracy of molecular property prediction and does employing multi-modal data fusion yield more precise results than unique data source methods?","To explore these matters, we comprehensively review and quantitatively analyze recent deep learning methods based on various benchmarks.","We discover that integrating molecular information will improve both MPP regression and classification tasks by upto 3.98% and 1.72%, respectively.","We also discover that the utilizing 3-dimensional information with 1-dimensional and 2-dimensional information simultaneously can substantially enhance MPP upto 4.2%.","The two consolidated insights offer crucial guidance for future advancements in drug discovery."],"url":"http://arxiv.org/abs/2402.07249v1","category":"cs.LG"}
{"created":"2024-02-11 16:58:59","title":"SAIS: A Novel Bio-Inspired Artificial Immune System Based on Symbiotic Paradigm","abstract":"We propose a novel type of Artificial Immune System (AIS): Symbiotic Artificial Immune Systems (SAIS), drawing inspiration from symbiotic relationships in biology. SAIS parallels the three key stages (i.e., mutualism, commensalism and parasitism) of population updating from the Symbiotic Organisms Search (SOS) algorithm. This parallel approach effectively addresses the challenges of large population size and enhances population diversity in AIS, which traditional AIS and SOS struggle to resolve efficiently. We conducted a series of experiments, which demonstrated that our SAIS achieved comparable performance to the state-of-the-art approach SOS and outperformed other popular AIS approaches and evolutionary algorithms across 26 benchmark problems. Furthermore, we investigated the problem of parameter selection and found that SAIS performs better in handling larger population sizes while requiring fewer generations. Finally, we believe SAIS, as a novel bio-inspired and immune-inspired algorithm, paves the way for innovation in bio-inspired computing with the symbiotic paradigm.","sentences":["We propose a novel type of Artificial Immune System (AIS): Symbiotic Artificial Immune Systems (SAIS), drawing inspiration from symbiotic relationships in biology.","SAIS parallels the three key stages (i.e., mutualism, commensalism and parasitism) of population updating from the Symbiotic Organisms Search (SOS) algorithm.","This parallel approach effectively addresses the challenges of large population size and enhances population diversity in AIS, which traditional AIS and SOS struggle to resolve efficiently.","We conducted a series of experiments, which demonstrated that our SAIS achieved comparable performance to the state-of-the-art approach SOS and outperformed other popular AIS approaches and evolutionary algorithms across 26 benchmark problems.","Furthermore, we investigated the problem of parameter selection and found that SAIS performs better in handling larger population sizes while requiring fewer generations.","Finally, we believe SAIS, as a novel bio-inspired and immune-inspired algorithm, paves the way for innovation in bio-inspired computing with the symbiotic paradigm."],"url":"http://arxiv.org/abs/2402.07244v1","category":"cs.NE"}
{"created":"2024-02-11 16:49:12","title":"Optimizing Genetically-Driven Synaptogenesis","abstract":"In this paper we introduce SynaptoGen, a novel framework that aims to bridge the gap between genetic manipulations and neuronal network behavior by simulating synaptogenesis and guiding the development of neuronal networks capable of solving predetermined computational tasks. Drawing inspiration from recent advancements in the field, we propose SynaptoGen as a bio-plausible approach to modeling synaptogenesis through differentiable functions. To validate SynaptoGen, we conduct a preliminary experiment using reinforcement learning as a benchmark learning framework, demonstrating its effectiveness in generating neuronal networks capable of solving the OpenAI Gym's Cart Pole task, compared to carefully designed baselines. The results highlight the potential of SynaptoGen to inspire further advancements in neuroscience and computational modeling, while also acknowledging the need for incorporating more realistic genetic rules and synaptic conductances in future research. Overall, SynaptoGen represents a promising avenue for exploring the intersection of genetics, neuroscience, and artificial intelligence.","sentences":["In this paper we introduce SynaptoGen, a novel framework that aims to bridge the gap between genetic manipulations and neuronal network behavior by simulating synaptogenesis and guiding the development of neuronal networks capable of solving predetermined computational tasks.","Drawing inspiration from recent advancements in the field, we propose SynaptoGen as a bio-plausible approach to modeling synaptogenesis through differentiable functions.","To validate SynaptoGen, we conduct a preliminary experiment using reinforcement learning as a benchmark learning framework, demonstrating its effectiveness in generating neuronal networks capable of solving the OpenAI Gym's Cart Pole task, compared to carefully designed baselines.","The results highlight the potential of SynaptoGen to inspire further advancements in neuroscience and computational modeling, while also acknowledging the need for incorporating more realistic genetic rules and synaptic conductances in future research.","Overall, SynaptoGen represents a promising avenue for exploring the intersection of genetics, neuroscience, and artificial intelligence."],"url":"http://arxiv.org/abs/2402.07242v1","category":"cs.NE"}
{"created":"2024-02-11 15:56:03","title":"CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain","abstract":"Large Language Models (LLMs) have demonstrated significant potential and effectiveness across multiple application domains. To assess the performance of mainstream LLMs in public security tasks, this study aims to construct a specialized evaluation benchmark tailored to the Chinese public security domain--CPSDbench. CPSDbench integrates datasets related to public security collected from real-world scenarios, supporting a comprehensive assessment of LLMs across four key dimensions: text classification, information extraction, question answering, and text generation. Furthermore, this study introduces a set of innovative evaluation metrics designed to more precisely quantify the efficacy of LLMs in executing tasks related to public security. Through the in-depth analysis and evaluation conducted in this research, we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the future development of more accurate and customized LLM models targeted at applications in this field.","sentences":["Large Language Models (LLMs) have demonstrated significant potential and effectiveness across multiple application domains.","To assess the performance of mainstream LLMs in public security tasks, this study aims to construct a specialized evaluation benchmark tailored to the Chinese public security domain--CPSDbench.","CPSDbench integrates datasets related to public security collected from real-world scenarios, supporting a comprehensive assessment of LLMs across four key dimensions: text classification, information extraction, question answering, and text generation.","Furthermore, this study introduces a set of innovative evaluation metrics designed to more precisely quantify the efficacy of LLMs in executing tasks related to public security.","Through the in-depth analysis and evaluation conducted in this research, we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the future development of more accurate and customized LLM models targeted at applications in this field."],"url":"http://arxiv.org/abs/2402.07234v1","category":"cs.AI"}
{"created":"2024-02-11 15:50:35","title":"TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation","abstract":"Natural language processing (NLP) is a key component of intelligent transportation systems (ITS), but it faces many challenges in the transportation domain, such as domain-specific knowledge and data, and multi-modal inputs and outputs. This paper presents TransGPT, a novel (multi-modal) large language model for the transportation domain, which consists of two independent variants: TransGPT-SM for single-modal data and TransGPT-MM for multi-modal data. TransGPT-SM is finetuned on a single-modal Transportation dataset (STD) that contains textual data from various sources in the transportation domain. TransGPT-MM is finetuned on a multi-modal Transportation dataset (MTD) that we manually collected from three areas of the transportation domain: driving tests, traffic signs, and landmarks. We evaluate TransGPT on several benchmark datasets for different tasks in the transportation domain, and show that it outperforms baseline models on most tasks. We also showcase the potential applications of TransGPT for traffic analysis and modeling, such as generating synthetic traffic scenarios, explaining traffic phenomena, answering traffic-related questions, providing traffic recommendations, and generating traffic reports. This work advances the state-of-the-art of NLP in the transportation domain and provides a useful tool for ITS researchers and practitioners.","sentences":["Natural language processing (NLP) is a key component of intelligent transportation systems (ITS), but it faces many challenges in the transportation domain, such as domain-specific knowledge and data, and multi-modal inputs and outputs.","This paper presents TransGPT, a novel (multi-modal) large language model for the transportation domain, which consists of two independent variants: TransGPT-SM for single-modal data and TransGPT-MM for multi-modal data.","TransGPT-SM is finetuned on a single-modal Transportation dataset (STD) that contains textual data from various sources in the transportation domain.","TransGPT-MM is finetuned on a multi-modal Transportation dataset (MTD) that we manually collected from three areas of the transportation domain: driving tests, traffic signs, and landmarks.","We evaluate TransGPT on several benchmark datasets for different tasks in the transportation domain, and show that it outperforms baseline models on most tasks.","We also showcase the potential applications of TransGPT for traffic analysis and modeling, such as generating synthetic traffic scenarios, explaining traffic phenomena, answering traffic-related questions, providing traffic recommendations, and generating traffic reports.","This work advances the state-of-the-art of NLP in the transportation domain and provides a useful tool for ITS researchers and practitioners."],"url":"http://arxiv.org/abs/2402.07233v1","category":"cs.CL"}
{"created":"2024-02-11 15:36:33","title":"Successive Refinement in Large-Scale Computation: Advancing Model Inference Applications","abstract":"Modern computationally-intensive applications often operate under time constraints, necessitating acceleration methods and distribution of computational workloads across multiple entities. However, the outcome is either achieved within the desired timeline or not, and in the latter case, valuable resources are wasted. In this paper, we introduce solutions for layered-resolution computation. These solutions allow lower-resolution results to be obtained at an earlier stage than the final result. This innovation notably enhances the deadline-based systems, as if a computational job is terminated due to time constraints, an approximate version of the final result can still be generated. Moreover, in certain operational regimes, a high-resolution result might be unnecessary, because the low-resolution result may already deviate significantly from the decision threshold, for example in AI-based decision-making systems. Therefore, operators can decide whether higher resolution is needed or not based on intermediate results, enabling computations with adaptive resolution. We present our framework for two critical and computationally demanding jobs: distributed matrix multiplication (linear) and model inference in machine learning (nonlinear). Our theoretical and empirical results demonstrate that the execution delay for the first resolution is significantly shorter than that for the final resolution, while maintaining overall complexity comparable to the conventional one-shot approach. Our experiments further illustrate how the layering feature increases the likelihood of meeting deadlines and enables adaptability and transparency in massive, large-scale computations.","sentences":["Modern computationally-intensive applications often operate under time constraints, necessitating acceleration methods and distribution of computational workloads across multiple entities.","However, the outcome is either achieved within the desired timeline or not, and in the latter case, valuable resources are wasted.","In this paper, we introduce solutions for layered-resolution computation.","These solutions allow lower-resolution results to be obtained at an earlier stage than the final result.","This innovation notably enhances the deadline-based systems, as if a computational job is terminated due to time constraints, an approximate version of the final result can still be generated.","Moreover, in certain operational regimes, a high-resolution result might be unnecessary, because the low-resolution result may already deviate significantly from the decision threshold, for example in AI-based decision-making systems.","Therefore, operators can decide whether higher resolution is needed or not based on intermediate results, enabling computations with adaptive resolution.","We present our framework for two critical and computationally demanding jobs: distributed matrix multiplication (linear) and model inference in machine learning (nonlinear).","Our theoretical and empirical results demonstrate that the execution delay for the first resolution is significantly shorter than that for the final resolution, while maintaining overall complexity comparable to the conventional one-shot approach.","Our experiments further illustrate how the layering feature increases the likelihood of meeting deadlines and enables adaptability and transparency in massive, large-scale computations."],"url":"http://arxiv.org/abs/2402.07229v1","category":"cs.IT"}
{"created":"2024-02-11 15:23:13","title":"Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL","abstract":"Offline Goal-Conditioned Reinforcement Learning (Offline GCRL) is an important problem in RL that focuses on acquiring diverse goal-oriented skills solely from pre-collected behavior datasets. In this setting, the reward feedback is typically absent except when the goal is achieved, which makes it difficult to learn policies especially from a finite dataset of suboptimal behaviors. In addition, realistic scenarios involve long-horizon planning, which necessitates the extraction of useful skills within sub-trajectories. Recently, the conditional diffusion model has been shown to be a promising approach to generate high-quality long-horizon plans for RL. However, their practicality for the goal-conditioned setting is still limited due to a number of technical assumptions made by the methods. In this paper, we propose SSD (Sub-trajectory Stitching with Diffusion), a model-based offline GCRL method that leverages the conditional diffusion model to address these limitations. In summary, we use the diffusion model that generates future plans conditioned on the target goal and value, with the target value estimated from the goal-relabeled offline dataset. We report state-of-the-art performance in the standard benchmark set of GCRL tasks, and demonstrate the capability to successfully stitch the segments of suboptimal trajectories in the offline data to generate high-quality plans.","sentences":["Offline Goal-Conditioned Reinforcement Learning (Offline GCRL) is an important problem in RL that focuses on acquiring diverse goal-oriented skills solely from pre-collected behavior datasets.","In this setting, the reward feedback is typically absent except when the goal is achieved, which makes it difficult to learn policies especially from a finite dataset of suboptimal behaviors.","In addition, realistic scenarios involve long-horizon planning, which necessitates the extraction of useful skills within sub-trajectories.","Recently, the conditional diffusion model has been shown to be a promising approach to generate high-quality long-horizon plans for RL.","However, their practicality for the goal-conditioned setting is still limited due to a number of technical assumptions made by the methods.","In this paper, we propose SSD (Sub-trajectory Stitching with Diffusion), a model-based offline GCRL method that leverages the conditional diffusion model to address these limitations.","In summary, we use the diffusion model that generates future plans conditioned on the target goal and value, with the target value estimated from the goal-relabeled offline dataset.","We report state-of-the-art performance in the standard benchmark set of GCRL tasks, and demonstrate the capability to successfully stitch the segments of suboptimal trajectories in the offline data to generate high-quality plans."],"url":"http://arxiv.org/abs/2402.07226v1","category":"cs.AI"}
{"created":"2024-02-11 14:39:40","title":"The Reasons that Agents Act: Intention and Instrumental Goals","abstract":"Intention is an important and challenging concept in AI. It is important because it underlies many other concepts we care about, such as agency, manipulation, legal responsibility, and blame. However, ascribing intent to AI systems is contentious, and there is no universally accepted theory of intention applicable to AI agents. We operationalise the intention with which an agent acts, relating to the reasons it chooses its decision. We introduce a formal definition of intention in structural causal influence models, grounded in the philosophy literature on intent and applicable to real-world machine learning systems. Through a number of examples and results, we show that our definition captures the intuitive notion of intent and satisfies desiderata set-out by past work. In addition, we show how our definition relates to past concepts, including actual causality, and the notion of instrumental goals, which is a core idea in the literature on safe AI agents. Finally, we demonstrate how our definition can be used to infer the intentions of reinforcement learning agents and language models from their behaviour.","sentences":["Intention is an important and challenging concept in AI.","It is important because it underlies many other concepts we care about, such as agency, manipulation, legal responsibility, and blame.","However, ascribing intent to AI systems is contentious, and there is no universally accepted theory of intention applicable to AI agents.","We operationalise the intention with which an agent acts, relating to the reasons it chooses its decision.","We introduce a formal definition of intention in structural causal influence models, grounded in the philosophy literature on intent and applicable to real-world machine learning systems.","Through a number of examples and results, we show that our definition captures the intuitive notion of intent and satisfies desiderata set-out by past work.","In addition, we show how our definition relates to past concepts, including actual causality, and the notion of instrumental goals, which is a core idea in the literature on safe AI agents.","Finally, we demonstrate how our definition can be used to infer the intentions of reinforcement learning agents and language models from their behaviour."],"url":"http://arxiv.org/abs/2402.07221v1","category":"cs.AI"}
{"created":"2024-02-11 14:21:37","title":"Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification","abstract":"In legal decisions, split votes (SV) occur when judges cannot reach a unanimous decision, posing a difficulty for lawyers who must navigate diverse legal arguments and opinions. In high-stakes domains, understanding the alignment of perceived difficulty between humans and AI systems is crucial to build trust. However, existing NLP calibration methods focus on a classifier's awareness of predictive performance, measured against the human majority class, overlooking inherent human label variation (HLV). This paper explores split votes as naturally observable human disagreement and value pluralism. We collect judges' vote distributions from the European Court of Human Rights (ECHR), and present SV-ECHR, a case outcome classification (COC) dataset with SV information. We build a taxonomy of disagreement with SV-specific subcategories. We further assess the alignment of perceived difficulty between models and humans, as well as confidence- and human-calibration of COC models. We observe limited alignment with the judge vote distribution. To our knowledge, this is the first systematic exploration of calibration to human judgements in legal NLP. Our study underscores the necessity for further research on measuring and enhancing model calibration considering HLV in legal decision tasks.","sentences":["In legal decisions, split votes (SV) occur when judges cannot reach a unanimous decision, posing a difficulty for lawyers who must navigate diverse legal arguments and opinions.","In high-stakes domains, understanding the alignment of perceived difficulty between humans and AI systems is crucial to build trust.","However, existing NLP calibration methods focus on a classifier's awareness of predictive performance, measured against the human majority class, overlooking inherent human label variation (HLV).","This paper explores split votes as naturally observable human disagreement and value pluralism.","We collect judges' vote distributions from the European Court of Human Rights (ECHR), and present SV-ECHR, a case outcome classification (COC) dataset with SV information.","We build a taxonomy of disagreement with SV-specific subcategories.","We further assess the alignment of perceived difficulty between models and humans, as well as confidence- and human-calibration of COC models.","We observe limited alignment with the judge vote distribution.","To our knowledge, this is the first systematic exploration of calibration to human judgements in legal NLP.","Our study underscores the necessity for further research on measuring and enhancing model calibration considering HLV in legal decision tasks."],"url":"http://arxiv.org/abs/2402.07214v1","category":"cs.CL"}
{"created":"2024-02-11 13:41:28","title":"ML Framework for Wireless MAC Protocol Design","abstract":"Adaptivity, reconfigurability and intelligence are key features of the next-generation wireless networks to meet the increasingly diverse quality of service (QoS) requirements of the future applications. Conventional protocol designs, however, struggle to provide flexibility and agility to changing radio environments, traffic types and different user service requirements. In this paper, we explore the potential of deep reinforcement learning (DRL), in particular Proximal Policy Optimization (PPO), to design and configure intelligent and application-specific medium access control (MAC) protocols. We propose a framework that enables the addition, removal, or modification of protocol features to meet individual application needs. The DRL channel access policy design empowers the protocol to adapt and optimize in accordance with the network and radio environment. Through extensive simulations, we demonstrate the superior performance of the learned protocols over legacy IEEE 802.11ac in terms of throughput and latency.","sentences":["Adaptivity, reconfigurability and intelligence are key features of the next-generation wireless networks to meet the increasingly diverse quality of service (QoS) requirements of the future applications.","Conventional protocol designs, however, struggle to provide flexibility and agility to changing radio environments, traffic types and different user service requirements.","In this paper, we explore the potential of deep reinforcement learning (DRL), in particular Proximal Policy Optimization (PPO), to design and configure intelligent and application-specific medium access control (MAC) protocols.","We propose a framework that enables the addition, removal, or modification of protocol features to meet individual application needs.","The DRL channel access policy design empowers the protocol to adapt and optimize in accordance with the network and radio environment.","Through extensive simulations, we demonstrate the superior performance of the learned protocols over legacy IEEE 802.11ac in terms of throughput and latency."],"url":"http://arxiv.org/abs/2402.07208v1","category":"cs.NI"}
{"created":"2024-02-11 13:30:53","title":"Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning","abstract":"In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in cooperation with an embedding-based module for retrieving candidate POIs from the user's POI database. Then, a spatial optimization module is used to order these POIs, followed by LLM crafting a personalized, spatially coherent itinerary. To the best of our knowledge, this study marks the first integration of LLMs to innovate itinerary planning solutions. Extensive experiments on offline datasets and online subjective evaluation have demonstrated the capacities of our system to deliver more responsive and spatially coherent itineraries than current LLM-based solutions. Our system has been deployed in production at the TuTu online travel service and has attracted thousands of users for their urban travel planning.","sentences":["In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language.","OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization.","Recently, large language models (LLMs) have shown potential in handling diverse tasks.","However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP.","Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs.","Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database.","For each user request, we leverage LLM in cooperation with an embedding-based module for retrieving candidate POIs from the user's POI database.","Then, a spatial optimization module is used to order these POIs, followed by LLM crafting a personalized, spatially coherent itinerary.","To the best of our knowledge, this study marks the first integration of LLMs to innovate itinerary planning solutions.","Extensive experiments on offline datasets and online subjective evaluation have demonstrated the capacities of our system to deliver more responsive and spatially coherent itineraries than current LLM-based solutions.","Our system has been deployed in production at the TuTu online travel service and has attracted thousands of users for their urban travel planning."],"url":"http://arxiv.org/abs/2402.07204v1","category":"cs.AI"}
{"created":"2024-02-11 13:26:06","title":"Link-aware link prediction over temporal graph by pattern recognition","abstract":"A temporal graph can be considered as a stream of links, each of which represents an interaction between two nodes at a certain time. On temporal graphs, link prediction is a common task, which aims to answer whether the query link is true or not. To do this task, previous methods usually focus on the learning of representations of the two nodes in the query link. We point out that the learned representation by their models may encode too much information with side effects for link prediction because they have not utilized the information of the query link, i.e., they are link-unaware. Based on this observation, we propose a link-aware model: historical links and the query link are input together into the following model layers to distinguish whether this input implies a reasonable pattern that ends with the query link. During this process, we focus on the modeling of link evolution patterns rather than node representations. Experiments on six datasets show that our model achieves strong performances compared with state-of-the-art baselines, and the results of link prediction are interpretable. The code and datasets are available on the project website: https://github.com/lbq8942/TGACN.","sentences":["A temporal graph can be considered as a stream of links, each of which represents an interaction between two nodes at a certain time.","On temporal graphs, link prediction is a common task, which aims to answer whether the query link is true or not.","To do this task, previous methods usually focus on the learning of representations of the two nodes in the query link.","We point out that the learned representation by their models may encode too much information with side effects for link prediction because they have not utilized the information of the query link, i.e., they are link-unaware.","Based on this observation, we propose a link-aware model: historical links and the query link are input together into the following model layers to distinguish whether this input implies a reasonable pattern that ends with the query link.","During this process, we focus on the modeling of link evolution patterns rather than node representations.","Experiments on six datasets show that our model achieves strong performances compared with state-of-the-art baselines, and the results of link prediction are interpretable.","The code and datasets are available on the project website: https://github.com/lbq8942/TGACN."],"url":"http://arxiv.org/abs/2402.07199v1","category":"cs.AI"}
{"created":"2024-02-11 13:24:13","title":"GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks","abstract":"Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse research fields of artificial intelligence, especially for open-ended tasks. While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor. To break this dilemma, we propose to bridge the pretrained GM and LLM by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of LLMs to offer various open-ended tasks for GM. To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node information, neighbor information and model information. By treating the node representation as a type of language, the proposed GraphTranslator empowers an LLM to make predictions based on node representation and language instructions, providing a unified perspective for both pre-defined and open-ended tasks. Extensive results show that the proposed GraphTranslator effectively improves the results of zero-shot node classification. The graph question answering experiments reveal our GraphTranslator potential across a broad spectrum of open-ended applications through language instructions.","sentences":["Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse research fields of artificial intelligence, especially for open-ended tasks.","While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form.","Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor.","To break this dilemma, we propose to bridge the pretrained GM and LLM by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of LLMs to offer various open-ended tasks for GM.","To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node information, neighbor information and model information.","By treating the node representation as a type of language, the proposed GraphTranslator empowers an LLM to make predictions based on node representation and language instructions, providing a unified perspective for both pre-defined and open-ended tasks.","Extensive results show that the proposed GraphTranslator effectively improves the results of zero-shot node classification.","The graph question answering experiments reveal our GraphTranslator potential across a broad spectrum of open-ended applications through language instructions."],"url":"http://arxiv.org/abs/2402.07197v1","category":"cs.AI"}
{"created":"2024-02-11 12:57:16","title":"GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention","abstract":"Graph invariant learning (GIL) has been an effective approach to discovering the invariant relationships between graph data and its labels for different graph learning tasks under various distribution shifts. Many recent endeavors of GIL focus on extracting the invariant subgraph from the input graph for prediction as a regularization strategy to improve the generalization performance of graph learning. Despite their success, such methods also have various limitations in obtaining their invariant subgraphs. In this paper, we provide in-depth analyses of the drawbacks of existing works and propose corresponding principles of our invariant subgraph extraction: 1) the sparsity, to filter out the variant features, 2) the softness, for a broader solution space, and 3) the differentiability, for a soundly end-to-end optimization. To meet these principles in one shot, we leverage the Optimal Transport (OT) theory and propose a novel graph attention mechanism called Graph Sinkhorn Attention (GSINA). This novel approach serves as a powerful regularization method for GIL tasks. By GSINA, we are able to obtain meaningful, differentiable invariant subgraphs with controllable sparsity and softness. Moreover, GSINA is a general graph learning framework that could handle GIL tasks of multiple data grain levels. Extensive experiments on both synthetic and real-world datasets validate the superiority of our GSINA, which outperforms the state-of-the-art GIL methods by large margins on both graph-level tasks and node-level tasks. Our code is publicly available at \\url{https://github.com/dingfangyu/GSINA}.","sentences":["Graph invariant learning (GIL) has been an effective approach to discovering the invariant relationships between graph data and its labels for different graph learning tasks under various distribution shifts.","Many recent endeavors of GIL focus on extracting the invariant subgraph from the input graph for prediction as a regularization strategy to improve the generalization performance of graph learning.","Despite their success, such methods also have various limitations in obtaining their invariant subgraphs.","In this paper, we provide in-depth analyses of the drawbacks of existing works and propose corresponding principles of our invariant subgraph extraction: 1) the sparsity, to filter out the variant features, 2) the softness, for a broader solution space, and 3) the differentiability, for a soundly end-to-end optimization.","To meet these principles in one shot, we leverage the Optimal Transport (OT) theory and propose a novel graph attention mechanism called Graph Sinkhorn Attention (GSINA).","This novel approach serves as a powerful regularization method for GIL tasks.","By GSINA, we are able to obtain meaningful, differentiable invariant subgraphs with controllable sparsity and softness.","Moreover, GSINA is a general graph learning framework that could handle GIL tasks of multiple data grain levels.","Extensive experiments on both synthetic and real-world datasets validate the superiority of our GSINA, which outperforms the state-of-the-art GIL methods by large margins on both graph-level tasks and node-level tasks.","Our code is publicly available at \\url{https://github.com/dingfangyu/GSINA}."],"url":"http://arxiv.org/abs/2402.07191v1","category":"cs.LG"}
{"created":"2024-02-11 12:35:28","title":"A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense","abstract":"Deep neural networks (DNNs) are well known to be vulnerable to adversarial examples (AEs). In previous studies, the use of models encrypted with a secret key was demonstrated to be robust against white-box attacks, but not against black-box ones. In this paper, we propose a novel method using the vision transformer (ViT) that is a random ensemble of encrypted models for enhancing robustness against both white-box and black-box attacks. In addition, a benchmark attack method, called AutoAttack, is applied to models to test adversarial robustness objectively. In experiments, the method was demonstrated to be robust against not only white-box attacks but also black-box ones in an image classification task on the CIFAR-10 and ImageNet datasets. The method was also compared with the state-of-the-art in a standardized benchmark for adversarial robustness, RobustBench, and it was verified to outperform conventional defenses in terms of clean accuracy and robust accuracy.","sentences":["Deep neural networks (DNNs) are well known to be vulnerable to adversarial examples (AEs).","In previous studies, the use of models encrypted with a secret key was demonstrated to be robust against white-box attacks, but not against black-box ones.","In this paper, we propose a novel method using the vision transformer (ViT) that is a random ensemble of encrypted models for enhancing robustness against both white-box and black-box attacks.","In addition, a benchmark attack method, called AutoAttack, is applied to models to test adversarial robustness objectively.","In experiments, the method was demonstrated to be robust against not only white-box attacks but also black-box ones in an image classification task on the CIFAR-10 and ImageNet datasets.","The method was also compared with the state-of-the-art in a standardized benchmark for adversarial robustness, RobustBench, and it was verified to outperform conventional defenses in terms of clean accuracy and robust accuracy."],"url":"http://arxiv.org/abs/2402.07183v1","category":"cs.AI"}
{"created":"2024-02-11 12:29:16","title":"MAGNETO: Edge AI for Human Activity Recognition -- Privacy and Personalization","abstract":"Human activity recognition (HAR) is a well-established field, significantly advanced by modern machine learning (ML) techniques. While companies have successfully integrated HAR into consumer products, they typically rely on a predefined activity set, which limits personalizations at the user level (edge devices). Despite advancements in Incremental Learning for updating models with new data, this often occurs on the Cloud, necessitating regular data transfers between cloud and edge devices, thus leading to data privacy issues. In this paper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the Cloud to the Edge. MAGNETO allows incremental human activity learning directly on the Edge devices, without any data exchange with the Cloud. This enables strong privacy guarantees, low processing latency, and a high degree of personalization for users. In particular, we demonstrate MAGNETO in an Android device, validating the whole pipeline from data collection to result visualization.","sentences":["Human activity recognition (HAR) is a well-established field, significantly advanced by modern machine learning (ML) techniques.","While companies have successfully integrated HAR into consumer products, they typically rely on a predefined activity set, which limits personalizations at the user level (edge devices).","Despite advancements in Incremental Learning for updating models with new data, this often occurs on the Cloud, necessitating regular data transfers between cloud and edge devices, thus leading to data privacy issues.","In this paper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the Cloud to the Edge.","MAGNETO allows incremental human activity learning directly on the Edge devices, without any data exchange with the Cloud.","This enables strong privacy guarantees, low processing latency, and a high degree of personalization for users.","In particular, we demonstrate MAGNETO in an Android device, validating the whole pipeline from data collection to result visualization."],"url":"http://arxiv.org/abs/2402.07180v1","category":"cs.LG"}
{"created":"2024-02-11 12:03:01","title":"EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches","abstract":"Voice messages, by nature, prevent users from gauging the emotional tone without fully diving into the audio content. This hinders the shared emotional experience at the pre-retrieval stage. Research scarcely explored \"Emotional Teasers\"-pre-retrieval cues offering a glimpse into an awaiting message's emotional tone without disclosing its content. We introduce EmoWear, a smartwatch voice messaging system enabling users to apply 30 animation teasers on message bubbles to reflect emotions. EmoWear eases senders' choice by prioritizing emotions based on semantic and acoustic processing. EmoWear was evaluated in comparison with a mirroring system using color-coded message bubbles as emotional cues (N=24). Results showed EmoWear significantly enhanced emotional communication experience in both receiving and sending messages. The animated teasers were considered intuitive and valued for diverse expressions. Desirable interaction qualities and practical implications are distilled for future design. We thereby contribute both a novel system and empirical knowledge concerning emotional teasers for voice messaging.","sentences":["Voice messages, by nature, prevent users from gauging the emotional tone without fully diving into the audio content.","This hinders the shared emotional experience at the pre-retrieval stage.","Research scarcely explored \"Emotional Teasers\"-pre-retrieval cues offering a glimpse into an awaiting message's emotional tone without disclosing its content.","We introduce EmoWear, a smartwatch voice messaging system enabling users to apply 30 animation teasers on message bubbles to reflect emotions.","EmoWear eases senders' choice by prioritizing emotions based on semantic and acoustic processing.","EmoWear was evaluated in comparison with a mirroring system using color-coded message bubbles as emotional cues (N=24).","Results showed EmoWear significantly enhanced emotional communication experience in both receiving and sending messages.","The animated teasers were considered intuitive and valued for diverse expressions.","Desirable interaction qualities and practical implications are distilled for future design.","We thereby contribute both a novel system and empirical knowledge concerning emotional teasers for voice messaging."],"url":"http://arxiv.org/abs/2402.07174v1","category":"cs.HC"}
{"created":"2024-02-11 11:41:35","title":"Research on the multi-stage impact of digital economy on rural revitalization in Hainan Province based on GPM model","abstract":"The rapid development of the digital economy has had a profound impact on the implementation of the rural revitalization strategy. Based on this, this study takes Hainan Province as the research object to deeply explore the impact of digital economic development on rural revitalization. The study collected panel data from 2003 to 2022 to construct an evaluation index system for the digital economy and rural revitalization and used panel regression analysis and other methods to explore the promotion effect of the digital economy on rural revitalization. Research results show that the digital economy has a significant positive impact on rural revitalization, and this impact increases as the level of fiscal expenditure increases. The issuance of digital RMB has further exerted a regulatory effect and promoted the development of the digital economy and the process of rural revitalization. At the same time, the establishment of the Hainan Free Trade Port has also played a positive role in promoting the development of the digital economy and rural revitalization. In the prediction of the optimal strategy for rural revitalization based on the development levels of the primary, secondary, and tertiary industries (Rate1, Rate2, and Rate3), it was found that rate1 can encourage Hainan Province to implement digital economic innovation, encourage rate3 to implement promotion behaviors, and increase rate2 can At the level of sustainable development when rate3 promotes rate2's digital economic innovation behavior, it can standardize rate2's production behavior to the greatest extent, accelerate the faster application of the digital economy to the rural revitalization industry, and promote the technological advancement of enterprises.","sentences":["The rapid development of the digital economy has had a profound impact on the implementation of the rural revitalization strategy.","Based on this, this study takes Hainan Province as the research object to deeply explore the impact of digital economic development on rural revitalization.","The study collected panel data from 2003 to 2022 to construct an evaluation index system for the digital economy and rural revitalization and used panel regression analysis and other methods to explore the promotion effect of the digital economy on rural revitalization.","Research results show that the digital economy has a significant positive impact on rural revitalization, and this impact increases as the level of fiscal expenditure increases.","The issuance of digital RMB has further exerted a regulatory effect and promoted the development of the digital economy and the process of rural revitalization.","At the same time, the establishment of the Hainan Free Trade Port has also played a positive role in promoting the development of the digital economy and rural revitalization.","In the prediction of the optimal strategy for rural revitalization based on the development levels of the primary, secondary, and tertiary industries (Rate1, Rate2, and Rate3), it was found that rate1 can encourage Hainan Province to implement digital economic innovation, encourage rate3 to implement promotion behaviors, and increase rate2 can At the level of sustainable development when rate3 promotes rate2's digital economic innovation behavior, it can standardize rate2's production behavior to the greatest extent, accelerate the faster application of the digital economy to the rural revitalization industry, and promote the technological advancement of enterprises."],"url":"http://arxiv.org/abs/2402.07170v1","category":"econ.GN"}
{"created":"2024-02-11 11:24:09","title":"Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy","abstract":"Treatment planning is currently a patient specific, time-consuming, and resource demanding task in radiotherapy. Dose-volume histogram (DVH) prediction plays a critical role in automating this process. The geometric relationship between DVHs in radiotherapy plans and organs-at-risk (OAR) and planning target volume (PTV) has been well established. This study explores the potential of deep learning models for predicting DVHs using images and subsequent human intervention facilitated by a large-language model (LLM) to enhance the planning quality. We propose a pipeline to convert unstructured images to a structured graph consisting of image-patch nodes and dose nodes. A novel Dose Graph Neural Network (DoseGNN) model is developed for predicting DVHs from the structured graph. The proposed DoseGNN is enhanced with the LLM to encode massive knowledge from prescriptions and interactive instructions from clinicians. In this study, we introduced an online human-AI collaboration (OHAC) system as a practical implementation of the concept proposed for the automation of intensity-modulated radiotherapy (IMRT) planning. In comparison to the widely-employed DL models used in radiotherapy, DoseGNN achieved mean square errors that were 80$\\%$, 76$\\%$ and 41.0$\\%$ of those predicted by Swin U-Net Transformer, 3D U-Net CNN and vanilla MLP, respectively. Moreover, the LLM-empowered DoseGNN model facilitates seamless adjustment to treatment plans through interaction with clinicians using natural language.","sentences":["Treatment planning is currently a patient specific, time-consuming, and resource demanding task in radiotherapy.","Dose-volume histogram (DVH) prediction plays a critical role in automating this process.","The geometric relationship between DVHs in radiotherapy plans and organs-at-risk (OAR) and planning target volume (PTV) has been well established.","This study explores the potential of deep learning models for predicting DVHs using images and subsequent human intervention facilitated by a large-language model (LLM) to enhance the planning quality.","We propose a pipeline to convert unstructured images to a structured graph consisting of image-patch nodes and dose nodes.","A novel Dose Graph Neural Network (DoseGNN) model is developed for predicting DVHs from the structured graph.","The proposed DoseGNN is enhanced with the LLM to encode massive knowledge from prescriptions and interactive instructions from clinicians.","In this study, we introduced an online human-AI collaboration (OHAC) system as a practical implementation of the concept proposed for the automation of intensity-modulated radiotherapy (IMRT) planning.","In comparison to the widely-employed DL models used in radiotherapy, DoseGNN achieved mean square errors that were 80$\\%$, 76$\\%$ and 41.0$\\%$ of those predicted by Swin U-Net Transformer, 3D U-Net CNN and vanilla MLP, respectively.","Moreover, the LLM-empowered DoseGNN model facilitates seamless adjustment to treatment plans through interaction with clinicians using natural language."],"url":"http://arxiv.org/abs/2402.07167v1","category":"cs.AI"}
{"created":"2024-02-11 11:23:28","title":"Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias","abstract":"We provide a birds eye view of the rapid developments in AI and Deep Learning that has led to the path-breaking emergence of AI in Large Language Models. The aim of this study is to place all these developments in a pragmatic broader historical social perspective without any exaggerations while at the same time without any pessimism that created the AI winter in the 1970s to 1990s. We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic. We note here that just as this emergence of AI seems to occur at a threshold point in the number of neural connections or weights, it has also been observed that human brain and especially the cortex region is nothing special or extraordinary but simply a case of scaled-up version of the primate brain and that even the human intelligence seems like an emergent phenomena of scale.","sentences":["We provide a birds eye view of the rapid developments in AI and Deep Learning that has led to the path-breaking emergence of AI in Large Language Models.","The aim of this study is to place all these developments in a pragmatic broader historical social perspective without any exaggerations while at the same time without any pessimism that created the AI winter in the 1970s to 1990s.","We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic.","We note here that just as this emergence of AI seems to occur at a threshold point in the number of neural connections or weights, it has also been observed that human brain and especially the cortex region is nothing special or extraordinary but simply a case of scaled-up version of the primate brain and that even the human intelligence seems like an emergent phenomena of scale."],"url":"http://arxiv.org/abs/2402.07166v1","category":"cs.AI"}
{"created":"2024-02-11 11:03:08","title":"Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces","abstract":"The advancement of Large Language Models (LLM) has also resulted in an equivalent proliferation in its applications. Software design, being one, has gained tremendous benefits in using LLMs as an interface component that extends fixed user stories. However, inclusion of LLM-based AI agents in software design often poses unexpected challenges, especially in the estimation of development efforts. Through the example of UI-based user stories, we provide a comparison against traditional methods and propose a new way to enhance specifications of natural language-based questions that allows for the estimation of development effort by taking into account data sources, interfaces and algorithms.","sentences":["The advancement of Large Language Models (LLM) has also resulted in an equivalent proliferation in its applications.","Software design, being one, has gained tremendous benefits in using LLMs as an interface component that extends fixed user stories.","However, inclusion of LLM-based AI agents in software design often poses unexpected challenges, especially in the estimation of development efforts.","Through the example of UI-based user stories, we provide a comparison against traditional methods and propose a new way to enhance specifications of natural language-based questions that allows for the estimation of development effort by taking into account data sources, interfaces and algorithms."],"url":"http://arxiv.org/abs/2402.07158v1","category":"cs.SE"}
{"created":"2024-02-11 11:03:04","title":"Natural Language Reinforcement Learning","abstract":"Reinforcement Learning (RL) has shown remarkable abilities in learning policies for decision-making tasks. However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals. To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language Reinforcement Learning (NLRL), which innovatively combines RL principles with natural language representation. Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space. We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework.","sentences":["Reinforcement Learning (RL) has shown remarkable abilities in learning policies for decision-making tasks.","However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals.","To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language Reinforcement Learning (NLRL), which innovatively combines RL principles with natural language representation.","Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space.","We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4.","Initial experiments over tabular MDPs demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework."],"url":"http://arxiv.org/abs/2402.07157v1","category":"cs.CL"}
{"created":"2024-02-11 10:50:20","title":"Error Estimation for Physics-informed Neural Networks Approximating Semilinear Wave Equations","abstract":"This paper provides rigorous error bounds for physics-informed neural networks approximating the semilinear wave equation. We provide bounds for the generalization and training error in terms of the width of the network's layers and the number of training points for a tanh neural network with two hidden layers. Our main result is a bound of the total error in the $H^1([0,T];L^2(\\Omega))$-norm in terms of the training error and the number of training points, which can be made arbitrarily small under some assumptions. We illustrate our theoretical bounds with numerical experiments.","sentences":["This paper provides rigorous error bounds for physics-informed neural networks approximating the semilinear wave equation.","We provide bounds for the generalization and training error in terms of the width of the network's layers and the number of training points for a tanh neural network with two hidden layers.","Our main result is a bound of the total error in the $H^1([0,T];L^2(\\Omega))$-norm in terms of the training error and the number of training points, which can be made arbitrarily small under some assumptions.","We illustrate our theoretical bounds with numerical experiments."],"url":"http://arxiv.org/abs/2402.07153v1","category":"math.NA"}
{"created":"2024-02-11 10:44:41","title":"Explainable Global Wildfire Prediction Models using Graph Neural Networks","abstract":"Wildfire prediction has become increasingly crucial due to the escalating impacts of climate change. Traditional CNN-based wildfire prediction models struggle with handling missing oceanic data and addressing the long-range dependencies across distant regions in meteorological data. In this paper, we introduce an innovative Graph Neural Network (GNN)-based model for global wildfire prediction. We propose a hybrid model that combines the spatial prowess of Graph Convolutional Networks (GCNs) with the temporal depth of Long Short-Term Memory (LSTM) networks. Our approach uniquely transforms global climate and wildfire data into a graph representation, addressing challenges such as null oceanic data locations and long-range dependencies inherent in traditional models. Benchmarking against established architectures using an unseen ensemble of JULES-INFERNO simulations, our model demonstrates superior predictive accuracy. Furthermore, we emphasise the model's explainability, unveiling potential wildfire correlation clusters through community detection and elucidating feature importance via Integrated Gradient analysis. Our findings not only advance the methodological domain of wildfire prediction but also underscore the importance of model transparency, offering valuable insights for stakeholders in wildfire management.","sentences":["Wildfire prediction has become increasingly crucial due to the escalating impacts of climate change.","Traditional CNN-based wildfire prediction models struggle with handling missing oceanic data and addressing the long-range dependencies across distant regions in meteorological data.","In this paper, we introduce an innovative Graph Neural Network (GNN)-based model for global wildfire prediction.","We propose a hybrid model that combines the spatial prowess of Graph Convolutional Networks (GCNs) with the temporal depth of Long Short-Term Memory (LSTM) networks.","Our approach uniquely transforms global climate and wildfire data into a graph representation, addressing challenges such as null oceanic data locations and long-range dependencies inherent in traditional models.","Benchmarking against established architectures using an unseen ensemble of JULES-INFERNO simulations, our model demonstrates superior predictive accuracy.","Furthermore, we emphasise the model's explainability, unveiling potential wildfire correlation clusters through community detection and elucidating feature importance via Integrated Gradient analysis.","Our findings not only advance the methodological domain of wildfire prediction but also underscore the importance of model transparency, offering valuable insights for stakeholders in wildfire management."],"url":"http://arxiv.org/abs/2402.07152v1","category":"cs.LG"}
{"created":"2024-02-11 10:33:33","title":"Perfectly Spherical Bloch Hyper-spheres from Quantum Matrix Geometry","abstract":"Leveraging analogies between precessing quantum spin systems and charge-monopole systems, we construct Bloch hyper-spheres with $\\it{exact}$ spherical symmetries in arbitrary dimensions. Such a Bloch hyper-sphere is realized as a collection of the orbits of precessing quantum spins, and its geometry mathematically aligns with the quantum Nambu geometry of a higher dimensional fuzzy sphere. Stabilizer group symmetry of the Bloch hyper-sphere necessarily introduces degenerate spin-coherent states and gives rise to Wilczek-Zee geometric phases of non-Abelian monopoles associated with the hyper-sphere holonomies. The degenerate spin-coherent states naturally induce matrix-valued quantum geometric tensors also. While the physical properties of Bloch hyper-spheres with minimal spin in even and odd dimensions are quite similar, their large spin counterparts differ qualitatively depending on the parity of dimensions. Exact correspondences between spin-coherent states and monopole harmonics in higher dimensions are established. We also investigate density matrices described by Bloch hyper-balls and elucidate their corresponding statistical and geometric properties such as von Neumann entropies and Bures quantum metrics.","sentences":["Leveraging analogies between precessing quantum spin systems and charge-monopole systems, we construct Bloch hyper-spheres with $\\it{exact}$ spherical symmetries in arbitrary dimensions.","Such a Bloch hyper-sphere is realized as a collection of the orbits of precessing quantum spins, and its geometry mathematically aligns with the quantum Nambu geometry of a higher dimensional fuzzy sphere.","Stabilizer group symmetry of the Bloch hyper-sphere necessarily introduces degenerate spin-coherent states and gives rise to Wilczek-Zee geometric phases of non-Abelian monopoles associated with the hyper-sphere holonomies.","The degenerate spin-coherent states naturally induce matrix-valued quantum geometric tensors also.","While the physical properties of Bloch hyper-spheres with minimal spin in even and odd dimensions are quite similar, their large spin counterparts differ qualitatively depending on the parity of dimensions.","Exact correspondences between spin-coherent states and monopole harmonics in higher dimensions are established.","We also investigate density matrices described by Bloch hyper-balls and elucidate their corresponding statistical and geometric properties such as von Neumann entropies and Bures quantum metrics."],"url":"http://arxiv.org/abs/2402.07149v1","category":"quant-ph"}
{"created":"2024-02-11 10:23:34","title":"X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design","abstract":"We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose a gating strategy that uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations of adaptations are established to solve specific tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis, protein mechanics and design. The impact of this work include access to readily expandable, adaptable and changeable models with strong domain knowledge and the capability to integrate across areas of knowledge. With the X-LoRA model featuring experts in biology, mathematics, reasoning, bio-inspired materials, mechanics and materials, chemistry, and protein mechanics we conduct a series of physics-focused case studies. We examine knowledge recall, protein mechanics forward/inverse tasks, protein design, and adversarial agentic modeling including ontological knowledge graphs. The model is capable not only of making quantitative predictions of nanomechanical properties of proteins, but also reasons over the results and correctly predicts likely mechanisms that explain distinct molecular behaviors.","sentences":["We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA).","Starting with a set of pre-trained LoRA adapters, we propose a gating strategy that uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations of adaptations are established to solve specific tasks.","The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations.","Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure.","We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis, protein mechanics and design.","The impact of this work include access to readily expandable, adaptable and changeable models with strong domain knowledge and the capability to integrate across areas of knowledge.","With the X-LoRA model featuring experts in biology, mathematics, reasoning, bio-inspired materials, mechanics and materials, chemistry, and protein mechanics we conduct a series of physics-focused case studies.","We examine knowledge recall, protein mechanics forward/inverse tasks, protein design, and adversarial agentic modeling including ontological knowledge graphs.","The model is capable not only of making quantitative predictions of nanomechanical properties of proteins, but also reasons over the results and correctly predicts likely mechanisms that explain distinct molecular behaviors."],"url":"http://arxiv.org/abs/2402.07148v1","category":"cond-mat.soft"}
{"created":"2024-02-11 09:46:24","title":"Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models","abstract":"In recent years, Large Language Models have reached state-of-the-art performance across multiple domains. However, the progress in the field of graph reasoning remains limited. Our work delves into this gap by thoroughly investigating graph reasoning with LLM. In this work, we reveal the impact of text sequence on LLM spatial understanding, finding that graph-descriptive text sequences significantly affect LLM reasoning performance on graphs. By altering the graph-descriptive text sequences, we enhance the performance of LLM from 42.22\\% to 70\\%. Furthermore, we evaluate the relationship between LLM performance and graph size, discovering that the reasoning performance of LLM does not monotonically decrease with the increase in graph size. Conclusively, we introduce the Scaled Graph Reasoning benchmark for assessing LLM performance across varied graph sizes.","sentences":["In recent years, Large Language Models have reached state-of-the-art performance across multiple domains.","However, the progress in the field of graph reasoning remains limited.","Our work delves into this gap by thoroughly investigating graph reasoning with LLM.","In this work, we reveal the impact of text sequence on LLM spatial understanding, finding that graph-descriptive text sequences significantly affect LLM reasoning performance on graphs.","By altering the graph-descriptive text sequences, we enhance the performance of LLM from 42.22\\% to 70\\%.","Furthermore, we evaluate the relationship between LLM performance and graph size, discovering that the reasoning performance of LLM does not monotonically decrease with the increase in graph size.","Conclusively, we introduce the Scaled Graph Reasoning benchmark for assessing LLM performance across varied graph sizes."],"url":"http://arxiv.org/abs/2402.07140v1","category":"cs.AI"}
{"created":"2024-02-11 08:54:37","title":"An attempt to generate new bridge types from latent space of denoising diffusion Implicit model","abstract":"Use denoising diffusion implicit model for bridge-type innovation. The process of adding noise and denoising to an image can be likened to the process of a corpse rotting and a detective restoring the scene of a victim being killed, to help beginners understand. Through an easy-to-understand algebraic method, derive the function formulas for adding noise and denoising, making it easier for beginners to master the mathematical principles of the model. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , denoising diffusion implicit model is constructed and trained. From the latent space sampling, new bridge types with asymmetric structures can be generated. Denoising diffusion implicit model can organically combine different structural components on the basis of human original bridge types, and create new bridge types.","sentences":["Use denoising diffusion implicit model for bridge-type innovation.","The process of adding noise and denoising to an image can be likened to the process of a corpse rotting and a detective restoring the scene of a victim being killed, to help beginners understand.","Through an easy-to-understand algebraic method, derive the function formulas for adding noise and denoising, making it easier for beginners to master the mathematical principles of the model.","Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , denoising diffusion implicit model is constructed and trained.","From the latent space sampling, new bridge types with asymmetric structures can be generated.","Denoising diffusion implicit model can organically combine different structural components on the basis of human original bridge types, and create new bridge types."],"url":"http://arxiv.org/abs/2402.07129v1","category":"cs.LG"}
{"created":"2024-02-11 08:41:42","title":"Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation","abstract":"Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets. While curated datasets can help, challenges remain in generalizability and real-world transfer. Meanwhile, large-scale \"in-the-wild\" video datasets have driven progress in computer vision through self-supervised techniques. Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online. Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias. This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations. We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for robotic manipulation. The survey summarizes video-based learning approaches, analyses their benefits over standard datasets, survey metrics, and benchmarks, and discusses open challenges and future directions in this nascent domain at the intersection of computer vision, natural language processing, and robot learning.","sentences":["Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets.","While curated datasets can help, challenges remain in generalizability and real-world transfer.","Meanwhile, large-scale \"in-the-wild\" video datasets have driven progress in computer vision through self-supervised techniques.","Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online.","Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias.","This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations.","We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for robotic manipulation.","The survey summarizes video-based learning approaches, analyses their benefits over standard datasets, survey metrics, and benchmarks, and discusses open challenges and future directions in this nascent domain at the intersection of computer vision, natural language processing, and robot learning."],"url":"http://arxiv.org/abs/2402.07127v1","category":"cs.RO"}
{"created":"2024-02-11 08:31:41","title":"Econometric analysis to estimate the impact of holidays on airfares","abstract":"The number of air transportation passengers during the holidays in Brazil has grown notably since the late nineties. One of the reasons is greater competition in airfares made possible by economic liberalization. This paper presents an econometric model of airline pricing aiming at estimating the impacts of holiday periods on fares, with special emphasis on three-day holiday events. It makes use of a database with daily collected data from the internet between 2008 and 2010 for the major Brazilian city, Sao Paulo. The econometric panel data model employs a two-way error components \"within\" estimator, controlling for airline/airport-pair fixed effect along with quotation and departure months effects. The decomposition of time effects between quotation and departure month effects is the main methodological contribution of the paper. Results allow for a comparative analysis of the performance of Sao Paulo's downtown and international airports - respectively, Congonhas (CGH), and Guarulhos (GRU) airports. As a result, the price of tickets bought 60 days in advance for flights with two stops leaving from the downtown airport fell by most.","sentences":["The number of air transportation passengers during the holidays in Brazil has grown notably since the late nineties.","One of the reasons is greater competition in airfares made possible by economic liberalization.","This paper presents an econometric model of airline pricing aiming at estimating the impacts of holiday periods on fares, with special emphasis on three-day holiday events.","It makes use of a database with daily collected data from the internet between 2008 and 2010 for the major Brazilian city, Sao Paulo.","The econometric panel data model employs a two-way error components \"within\" estimator, controlling for airline/airport-pair fixed effect along with quotation and departure months effects.","The decomposition of time effects between quotation and departure month effects is the main methodological contribution of the paper.","Results allow for a comparative analysis of the performance of Sao Paulo's downtown and international airports - respectively, Congonhas (CGH), and Guarulhos (GRU) airports.","As a result, the price of tickets bought 60 days in advance for flights with two stops leaving from the downtown airport fell by most."],"url":"http://arxiv.org/abs/2402.07124v1","category":"econ.GN"}
{"created":"2024-02-11 07:27:01","title":"Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding Remote Smartphone-based Consultation","abstract":"Blindness and other eye diseases are a global health concern, particularly in low- and middle-income countries like India. In this regard, during the COVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi attachment for smartphone-based eye imaging gained in use. However, quality of user-captured image often remained inadequate, requiring clinician vetting and delays. In this backdrop, we propose an AI-based quality assessment system with instant feedback mimicking clinicians' judgments and tested on patient-captured images. Dividing the complex problem hierarchically, here we tackle a nontrivial part, and demonstrate a proof of the concept.","sentences":["Blindness and other eye diseases are a global health concern, particularly in low- and middle-income countries like India.","In this regard, during the COVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi attachment for smartphone-based eye imaging gained in use.","However, quality of user-captured image often remained inadequate, requiring clinician vetting and delays.","In this backdrop, we propose an AI-based quality assessment system with instant feedback mimicking clinicians' judgments and tested on patient-captured images.","Dividing the complex problem hierarchically, here we tackle a nontrivial part, and demonstrate a proof of the concept."],"url":"http://arxiv.org/abs/2402.07118v1","category":"cs.HC"}
{"created":"2024-02-11 05:17:56","title":"Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning","abstract":"We present a novel statistical approach to incorporating uncertainty awareness in model-free distributional reinforcement learning involving quantile regression-based deep Q networks. The proposed algorithm, $\\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$, aims to address key challenges associated with separately estimating aleatoric and epistemic uncertainty in stochastic environments. It combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of $\\textit{global}$ uncertainty as opposed to $\\textit{local}$ estimates based on simple variance, overcoming limitations of traditional methods in computational and statistical efficiency and handling of out-of-distribution (OOD) observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar), CEQR-DQN is shown to surpass similar existing frameworks in scores and learning speed. Its ability to rigorously evaluate uncertainty improves exploration strategies and can serve as a blueprint for other algorithms requiring uncertainty awareness.","sentences":["We present a novel statistical approach to incorporating uncertainty awareness in model-free distributional reinforcement learning involving quantile regression-based deep Q networks.","The proposed algorithm, $\\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$, aims to address key challenges associated with separately estimating aleatoric and epistemic uncertainty in stochastic environments.","It combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of $\\textit{global}$ uncertainty as opposed to $\\textit{local}$ estimates based on simple variance, overcoming limitations of traditional methods in computational and statistical efficiency and handling of out-of-distribution (OOD) observations.","Tested on a suite of miniaturized Atari games (i.e., MinAtar), CEQR-DQN is shown to surpass similar existing frameworks in scores and learning speed.","Its ability to rigorously evaluate uncertainty improves exploration strategies and can serve as a blueprint for other algorithms requiring uncertainty awareness."],"url":"http://arxiv.org/abs/2402.07107v1","category":"cs.LG"}
{"created":"2024-02-11 04:53:40","title":"Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments","abstract":"Learning a good history representation is one of the core challenges of reinforcement learning (RL) in partially observable environments. Recent works have shown the advantages of various auxiliary tasks for facilitating representation learning. However, the effectiveness of such auxiliary tasks has not been fully convincing, especially in partially observable environments that require long-term memorization and inference. In this empirical study, we investigate the effectiveness of future prediction for learning the representations of histories, possibly of extensive length, in partially observable environments. We first introduce an approach that decouples the task of learning history representations from policy optimization via future prediction. Then, our main contributions are two-fold: (a) we demonstrate that the performance of reinforcement learning is strongly correlated with the prediction accuracy of future observations in partially observable environments, and (b) our approach can significantly improve the overall end-to-end approach by preventing high-variance noisy signals from reinforcement learning objectives to influence the representation learning. We illustrate our claims on three types of benchmarks that necessitate the ability to process long histories for high returns.","sentences":["Learning a good history representation is one of the core challenges of reinforcement learning (RL) in partially observable environments.","Recent works have shown the advantages of various auxiliary tasks for facilitating representation learning.","However, the effectiveness of such auxiliary tasks has not been fully convincing, especially in partially observable environments that require long-term memorization and inference.","In this empirical study, we investigate the effectiveness of future prediction for learning the representations of histories, possibly of extensive length, in partially observable environments.","We first introduce an approach that decouples the task of learning history representations from policy optimization via future prediction.","Then, our main contributions are two-fold: (a) we demonstrate that the performance of reinforcement learning is strongly correlated with the prediction accuracy of future observations in partially observable environments, and (b) our approach can significantly improve the overall end-to-end approach by preventing high-variance noisy signals from reinforcement learning objectives to influence the representation learning.","We illustrate our claims on three types of benchmarks that necessitate the ability to process long histories for high returns."],"url":"http://arxiv.org/abs/2402.07102v1","category":"cs.LG"}
{"created":"2024-02-11 03:54:44","title":"Improving Pallet Detection Using Synthetic Data","abstract":"The use of synthetic data in machine learning saves a significant amount of time when implementing an effective object detector. However, there is limited research in this domain. This study aims to improve upon previously applied implementations in the task of instance segmentation of pallets in a warehouse environment. This study proposes using synthetically generated domain-randomised data as well as data generated through Unity to achieve this. This study achieved performance improvements on the stacked and racked pallet categories by 69% and 50% mAP50, respectively when being evaluated on real data. Additionally, it was found that there was a considerable impact on the performance of a model when it was evaluated against images in a darker environment, dropping as low as 3% mAP50 when being evaluated on images with an 80% brightness reduction. This study also created a two-stage detector that used YOLOv8 and SAM, but this proved to have unstable performance. The use of domain-randomised data proved to have negligible performance improvements when compared to the Unity-generated data.","sentences":["The use of synthetic data in machine learning saves a significant amount of time when implementing an effective object detector.","However, there is limited research in this domain.","This study aims to improve upon previously applied implementations in the task of instance segmentation of pallets in a warehouse environment.","This study proposes using synthetically generated domain-randomised data as well as data generated through Unity to achieve this.","This study achieved performance improvements on the stacked and racked pallet categories by 69% and 50% mAP50, respectively when being evaluated on real data.","Additionally, it was found that there was a considerable impact on the performance of a model when it was evaluated against images in a darker environment, dropping as low as 3% mAP50 when being evaluated on images with an 80% brightness reduction.","This study also created a two-stage detector that used YOLOv8 and SAM, but this proved to have unstable performance.","The use of domain-randomised data proved to have negligible performance improvements when compared to the Unity-generated data."],"url":"http://arxiv.org/abs/2402.07098v1","category":"cs.RO"}
{"created":"2024-02-11 02:34:42","title":"Self-Correcting Self-Consuming Loops for Generative Model Training","abstract":"As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates \"self-consuming loops\" which may lead to training instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of self-correcting self-consuming loops on the challenging human motion synthesis task, and observe that it successfully avoids model collapse, even when the ratio of synthetic data to real data is as high as 100%.","sentences":["As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data.","Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates \"self-consuming loops\" which may lead to training instability or even collapse, unless certain conditions are met.","Our paper aims to stabilize self-consuming generative model training.","Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable.","We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale.","We empirically validate the effectiveness of self-correcting self-consuming loops on the challenging human motion synthesis task, and observe that it successfully avoids model collapse, even when the ratio of synthetic data to real data is as high as 100%."],"url":"http://arxiv.org/abs/2402.07087v1","category":"cs.LG"}
{"created":"2024-02-11 02:25:19","title":"CodPy: a Python library for numerics, machine learning, and statistics","abstract":"This monograph offers an introduction to a collection of numerical algorithms implemented in the library CodPy (an acronym that stands for the Curse Of Dimensionality in PYthon), which has found widespread applications across various areas, including machine learning, statistics, and computational physics. We develop here a strategy based on the theory of reproducing kernel Hilbert spaces (RKHS) and the theory of optimal transport. Initially designed for mathematical finance, this library has since been enhanced and broadened to be applicable to problems arising in engineering and industry. In order to present the general principles and techniques employed in CodPy and its applications, we have structured this monograph into two main parts. First of all, we focus on the fundamental principles of kernel-based representations of data and solutions, also that the presentation therein is supplemented with illustrative examples only. Next, we discuss the application of these principles to many classes of concrete problems, spanning from the numerical approximation of partial differential equations to (supervised, unsupervised) machine learning, extending to generative methods with a focus on stochastic aspects.","sentences":["This monograph offers an introduction to a collection of numerical algorithms implemented in the library CodPy (an acronym that stands for the Curse Of Dimensionality in PYthon), which has found widespread applications across various areas, including machine learning, statistics, and computational physics.","We develop here a strategy based on the theory of reproducing kernel Hilbert spaces (RKHS) and the theory of optimal transport.","Initially designed for mathematical finance, this library has since been enhanced and broadened to be applicable to problems arising in engineering and industry.","In order to present the general principles and techniques employed in CodPy and its applications, we have structured this monograph into two main parts.","First of all, we focus on the fundamental principles of kernel-based representations of data and solutions, also that the presentation therein is supplemented with illustrative examples only.","Next, we discuss the application of these principles to many classes of concrete problems, spanning from the numerical approximation of partial differential equations to (supervised, unsupervised) machine learning, extending to generative methods with a focus on stochastic aspects."],"url":"http://arxiv.org/abs/2402.07084v1","category":"math.NA"}
{"created":"2024-02-11 01:35:15","title":"RiskMiner: Discovering Formulaic Alphas via Risk Seeking Monte Carlo Tree Search","abstract":"The formulaic alphas are mathematical formulas that transform raw stock data into indicated signals. In the industry, a collection of formulaic alphas is combined to enhance modeling accuracy. Existing alpha mining only employs the neural network agent, unable to utilize the structural information of the solution space. Moreover, they didn't consider the correlation between alphas in the collection, which limits the synergistic performance. To address these problems, we propose a novel alpha mining framework, which formulates the alpha mining problems as a reward-dense Markov Decision Process (MDP) and solves the MDP by the risk-seeking Monte Carlo Tree Search (MCTS). The MCTS-based agent fully exploits the structural information of discrete solution space and the risk-seeking policy explicitly optimizes the best-case performance rather than average outcomes. Comprehensive experiments are conducted to demonstrate the efficiency of our framework. Our method outperforms all state-of-the-art benchmarks on two real-world stock sets under various metrics. Backtest experiments show that our alphas achieve the most profitable results under a realistic trading setting.","sentences":["The formulaic alphas are mathematical formulas that transform raw stock data into indicated signals.","In the industry, a collection of formulaic alphas is combined to enhance modeling accuracy.","Existing alpha mining only employs the neural network agent, unable to utilize the structural information of the solution space.","Moreover, they didn't consider the correlation between alphas in the collection, which limits the synergistic performance.","To address these problems, we propose a novel alpha mining framework, which formulates the alpha mining problems as a reward-dense Markov Decision Process (MDP) and solves the MDP by the risk-seeking Monte Carlo Tree Search (MCTS).","The MCTS-based agent fully exploits the structural information of discrete solution space and the risk-seeking policy explicitly optimizes the best-case performance rather than average outcomes.","Comprehensive experiments are conducted to demonstrate the efficiency of our framework.","Our method outperforms all state-of-the-art benchmarks on two real-world stock sets under various metrics.","Backtest experiments show that our alphas achieve the most profitable results under a realistic trading setting."],"url":"http://arxiv.org/abs/2402.07080v1","category":"q-fin.CP"}
{"created":"2024-02-11 01:03:41","title":"Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training","abstract":"Cloud solutions have gained significant popularity in the technology industry as they offer a combination of services and tools to tackle specific problems. However, despite their widespread use, the task of identifying appropriate company customers for a specific target solution to the sales team of a solution provider remains a complex business problem that existing matching systems have yet to adequately address. In this work, we study the B2B solution matching problem and identify two main challenges of this scenario: (1) the modeling of complex multi-field features and (2) the limited, incomplete, and sparse transaction data. To tackle these challenges, we propose a framework CAMA, which is built with a hierarchical multi-field matching structure as its backbone and supplemented by three data augmentation strategies and a contrastive pre-training objective to compensate for the imperfections in the available data. Through extensive experiments on a real-world dataset, we demonstrate that CAMA outperforms several strong baseline matching models significantly. Furthermore, we have deployed our matching framework on a system of Huawei Cloud. Our observations indicate an improvement of about 30% compared to the previous online model in terms of Conversion Rate (CVR), which demonstrates its great business value.","sentences":["Cloud solutions have gained significant popularity in the technology industry as they offer a combination of services and tools to tackle specific problems.","However, despite their widespread use, the task of identifying appropriate company customers for a specific target solution to the sales team of a solution provider remains a complex business problem that existing matching systems have yet to adequately address.","In this work, we study the B2B solution matching problem and identify two main challenges of this scenario: (1) the modeling of complex multi-field features and (2) the limited, incomplete, and sparse transaction data.","To tackle these challenges, we propose a framework CAMA, which is built with a hierarchical multi-field matching structure as its backbone and supplemented by three data augmentation strategies and a contrastive pre-training objective to compensate for the imperfections in the available data.","Through extensive experiments on a real-world dataset, we demonstrate that CAMA outperforms several strong baseline matching models significantly.","Furthermore, we have deployed our matching framework on a system of Huawei Cloud.","Our observations indicate an improvement of about 30% compared to the previous online model in terms of Conversion Rate (CVR), which demonstrates its great business value."],"url":"http://arxiv.org/abs/2402.07076v1","category":"cs.IR"}
{"created":"2024-02-11 00:00:05","title":"Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine","abstract":"We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm in order to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. We also show the theoretical guarantee of our algorithm to converge to an optimal policy. We demonstrate that LARL-RM speeds up the convergence by 30% by implementing our method in two case studies.","sentences":["We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm in order to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning.","Our method uses Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton.","We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches.","Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand.","We also show the theoretical guarantee of our algorithm to converge to an optimal policy.","We demonstrate that LARL-RM speeds up the convergence by 30% by implementing our method in two case studies."],"url":"http://arxiv.org/abs/2402.07069v1","category":"cs.LG"}
{"created":"2024-02-10 22:20:37","title":"Domain Adaptable Fine-Tune Distillation Framework For Advancing Farm Surveillance","abstract":"In this study, we propose an automated framework for camel farm monitoring, introducing two key contributions: the Unified Auto-Annotation framework and the Fine-Tune Distillation framework. The Unified Auto-Annotation approach combines two models, GroundingDINO (GD), and Segment-Anything-Model (SAM), to automatically annotate raw datasets extracted from surveillance videos. Building upon this foundation, the Fine-Tune Distillation framework conducts fine-tuning of student models using the auto-annotated dataset. This process involves transferring knowledge from a large teacher model to a student model, resembling a variant of Knowledge Distillation. The Fine-Tune Distillation framework aims to be adaptable to specific use cases, enabling the transfer of knowledge from the large models to the small models, making it suitable for domain-specific applications. By leveraging our raw dataset collected from Al-Marmoom Camel Farm in Dubai, UAE, and a pre-trained teacher model, GroundingDINO, the Fine-Tune Distillation framework produces a lightweight deployable model, YOLOv8. This framework demonstrates high performance and computational efficiency, facilitating efficient real-time object detection. Our code is available at \\href{https://github.com/Razaimam45/Fine-Tune-Distillation}{https://github.com/Razaimam45/Fine-Tune-Distillation}","sentences":["In this study, we propose an automated framework for camel farm monitoring, introducing two key contributions: the Unified Auto-Annotation framework and the Fine-Tune Distillation framework.","The Unified Auto-Annotation approach combines two models, GroundingDINO (GD), and Segment-Anything-Model (SAM), to automatically annotate raw datasets extracted from surveillance videos.","Building upon this foundation, the Fine-Tune Distillation framework conducts fine-tuning of student models using the auto-annotated dataset.","This process involves transferring knowledge from a large teacher model to a student model, resembling a variant of Knowledge Distillation.","The Fine-Tune Distillation framework aims to be adaptable to specific use cases, enabling the transfer of knowledge from the large models to the small models, making it suitable for domain-specific applications.","By leveraging our raw dataset collected from Al-Marmoom Camel Farm in Dubai, UAE, and a pre-trained teacher model, GroundingDINO, the Fine-Tune Distillation framework produces a lightweight deployable model, YOLOv8.","This framework demonstrates high performance and computational efficiency, facilitating efficient real-time object detection.","Our code is available at \\href{https://github.com/Razaimam45/Fine-Tune-Distillation}{https://github.com/Razaimam45/Fine-Tune-Distillation}"],"url":"http://arxiv.org/abs/2402.07059v1","category":"cs.CV"}
{"created":"2024-02-10 21:46:34","title":"$L^*LM$: Learning Automata from Examples using Natural Language Oracles","abstract":"Expert demonstrations have proven an easy way to indirectly specify complex tasks. Recent algorithms even support extracting unambiguous formal specifications, e.g. deterministic finite automata (DFA), from demonstrations. Unfortunately, these techniques are generally not sample efficient. In this work, we introduce $L^*LM$, an algorithm for learning DFAs from both demonstrations and natural language. Due to the expressivity of natural language, we observe a significant improvement in the data efficiency of learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large language models to answer membership queries about the underlying task. This is then combined with recent techniques for transforming learning from demonstrations into a sequence of labeled example learning problems. In our experiments, we observe the two modalities complement each other, yielding a powerful few-shot learner.","sentences":["Expert demonstrations have proven an easy way to indirectly specify complex tasks.","Recent algorithms even support extracting unambiguous formal specifications, e.g. deterministic finite automata (DFA), from demonstrations.","Unfortunately, these techniques are generally not sample efficient.","In this work, we introduce $L^*LM$, an algorithm for learning DFAs from both demonstrations and natural language.","Due to the expressivity of natural language, we observe a significant improvement in the data efficiency of learning DFAs from expert demonstrations.","Technically, $L^*LM$ leverages large language models to answer membership queries about the underlying task.","This is then combined with recent techniques for transforming learning from demonstrations into a sequence of labeled example learning problems.","In our experiments, we observe the two modalities complement each other, yielding a powerful few-shot learner."],"url":"http://arxiv.org/abs/2402.07051v1","category":"cs.LG"}
{"created":"2024-02-10 21:44:28","title":"A Factor Graph Model of Trust for a Collaborative Multi-Agent System","abstract":"In the field of Multi-Agent Systems (MAS), known for their openness, dynamism, and cooperative nature, the ability to trust the resources and services of other agents is crucial. Trust, in this setting, is the reliance and confidence an agent has in the information, behaviors, intentions, truthfulness, and capabilities of others within the system. Our paper introduces a new graphical approach that utilizes factor graphs to represent the interdependent behaviors and trustworthiness among agents. This includes modeling the behavior of robots as a trajectory of actions using a Gaussian process factor graph, which accounts for smoothness, obstacle avoidance, and trust-related factors. Our method for evaluating trust is decentralized and considers key interdependent sub-factors such as proximity safety, consistency, and cooperation. The overall system comprises a network of factor graphs that interact through trust-related factors and employs a Bayesian inference method to dynamically assess trust-based decisions with informed consent. The effectiveness of this method is validated via simulations and empirical tests with autonomous robots navigating unsignalized intersections.","sentences":["In the field of Multi-Agent Systems (MAS), known for their openness, dynamism, and cooperative nature, the ability to trust the resources and services of other agents is crucial.","Trust, in this setting, is the reliance and confidence an agent has in the information, behaviors, intentions, truthfulness, and capabilities of others within the system.","Our paper introduces a new graphical approach that utilizes factor graphs to represent the interdependent behaviors and trustworthiness among agents.","This includes modeling the behavior of robots as a trajectory of actions using a Gaussian process factor graph, which accounts for smoothness, obstacle avoidance, and trust-related factors.","Our method for evaluating trust is decentralized and considers key interdependent sub-factors such as proximity safety, consistency, and cooperation.","The overall system comprises a network of factor graphs that interact through trust-related factors and employs a Bayesian inference method to dynamically assess trust-based decisions with informed consent.","The effectiveness of this method is validated via simulations and empirical tests with autonomous robots navigating unsignalized intersections."],"url":"http://arxiv.org/abs/2402.07049v1","category":"cs.AI"}
{"created":"2024-02-10 21:06:34","title":"A Tale of Tails: Model Collapse as a Change of Scaling Laws","abstract":"As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning\" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation using the large language model Llama2.","sentences":["As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data.","Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data.","In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus?","Will future models, still improve, or be doomed to degenerate up to total (model) collapse?","We develop a theoretical framework of model collapse through the lens of scaling laws.","We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning\" of skills, and grokking when mixing human and synthesized data.","Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation using the large language model Llama2."],"url":"http://arxiv.org/abs/2402.07043v1","category":"cs.LG"}
{"created":"2024-02-10 20:39:04","title":"Coordinated Disclosure for AI: Beyond Security Vulnerabilities","abstract":"Harm reporting in the field of Artificial Intelligence (AI) currently operates on an ad hoc basis, lacking a structured process for disclosing or addressing algorithmic flaws. In contrast, the Coordinated Vulnerability Disclosure (CVD) ethos and ecosystem play a pivotal role in software security and transparency. Within the U.S. context, there has been a protracted legal and policy struggle to establish a safe harbor from the Computer Fraud and Abuse Act, aiming to foster institutional support for security researchers acting in good faith. Notably, algorithmic flaws in Machine Learning (ML) models present distinct challenges compared to traditional software vulnerabilities, warranting a specialized approach. To address this gap, we propose the implementation of a dedicated Coordinated Flaw Disclosure (CFD) framework tailored to the intricacies of machine learning and artificial intelligence issues. This paper delves into the historical landscape of disclosures in ML, encompassing the ad hoc reporting of harms and the emergence of participatory auditing. By juxtaposing these practices with the well-established disclosure norms in cybersecurity, we argue that the broader adoption of CFD has the potential to enhance public trust through transparent processes that carefully balance the interests of both organizations and the community.","sentences":["Harm reporting in the field of Artificial Intelligence (AI) currently operates on an ad hoc basis, lacking a structured process for disclosing or addressing algorithmic flaws.","In contrast, the Coordinated Vulnerability Disclosure (CVD) ethos and ecosystem play a pivotal role in software security and transparency.","Within the U.S. context, there has been a protracted legal and policy struggle to establish a safe harbor from the Computer Fraud and Abuse Act, aiming to foster institutional support for security researchers acting in good faith.","Notably, algorithmic flaws in Machine Learning (ML) models present distinct challenges compared to traditional software vulnerabilities, warranting a specialized approach.","To address this gap, we propose the implementation of a dedicated Coordinated Flaw Disclosure (CFD) framework tailored to the intricacies of machine learning and artificial intelligence issues.","This paper delves into the historical landscape of disclosures in ML, encompassing the ad hoc reporting of harms and the emergence of participatory auditing.","By juxtaposing these practices with the well-established disclosure norms in cybersecurity, we argue that the broader adoption of CFD has the potential to enhance public trust through transparent processes that carefully balance the interests of both organizations and the community."],"url":"http://arxiv.org/abs/2402.07039v1","category":"cs.AI"}
{"created":"2024-02-10 20:38:50","title":"Nonlinear Modes as a Tool for Comparing the Mathematical Structure of Dynamic Models of Soft Robots","abstract":"Continuum soft robots are nonlinear mechanical systems with theoretically infinite degrees of freedom (DoFs) that exhibit complex behaviors. Achieving motor intelligence under dynamic conditions necessitates the development of control-oriented reduced-order models (ROMs), which employ as few DoFs as possible while still accurately capturing the core characteristics of the theoretically infinite-dimensional dynamics. However, there is no quantitative way to measure if the ROM of a soft robot has succeeded in this task. In other fields, like structural dynamics or flexible link robotics, linear normal modes are routinely used to this end. Yet, this theory is not applicable to soft robots due to their nonlinearities. In this work, we propose to use the recent nonlinear extension in modal theory -- called eigenmanifolds -- as a means to evaluate control-oriented models for soft robots and compare them. To achieve this, we propose three similarity metrics relying on the projection of the nonlinear modes of the system into a task space of interest. We use this approach to compare quantitatively, for the first time, ROMs of increasing order generated under the piecewise constant curvature (PCC) hypothesis with a high-dimensional finite element (FE)-like model of a soft arm. Results show that by increasing the order of the discretization, the eigenmanifolds of the PCC model converge to those of the FE model.","sentences":["Continuum soft robots are nonlinear mechanical systems with theoretically infinite degrees of freedom (DoFs) that exhibit complex behaviors.","Achieving motor intelligence under dynamic conditions necessitates the development of control-oriented reduced-order models (ROMs), which employ as few DoFs as possible while still accurately capturing the core characteristics of the theoretically infinite-dimensional dynamics.","However, there is no quantitative way to measure if the ROM of a soft robot has succeeded in this task.","In other fields, like structural dynamics or flexible link robotics, linear normal modes are routinely used to this end.","Yet, this theory is not applicable to soft robots due to their nonlinearities.","In this work, we propose to use the recent nonlinear extension in modal theory -- called eigenmanifolds -- as a means to evaluate control-oriented models for soft robots and compare them.","To achieve this, we propose three similarity metrics relying on the projection of the nonlinear modes of the system into a task space of interest.","We use this approach to compare quantitatively, for the first time, ROMs of increasing order generated under the piecewise constant curvature (PCC) hypothesis with a high-dimensional finite element (FE)-like model of a soft arm.","Results show that by increasing the order of the discretization, the eigenmanifolds of the PCC model converge to those of the FE model."],"url":"http://arxiv.org/abs/2402.07038v1","category":"cs.RO"}
{"created":"2024-02-10 20:06:26","title":"Distilling Symbolic Priors for Concept Learning into Neural Networks","abstract":"Humans can learn new concepts from a small number of examples by drawing on their inductive biases. These inductive biases have previously been captured by using Bayesian models defined over symbolic hypothesis spaces. Is it possible to create a neural network that displays the same inductive biases? We show that inductive biases that enable rapid concept learning can be instantiated in artificial neural networks by distilling a prior distribution from a symbolic Bayesian model via meta-learning, an approach for extracting the common structure from a set of tasks. By generating the set of tasks used in meta-learning from the prior distribution of a Bayesian model, we are able to transfer that prior into a neural network. We use this approach to create a neural network with an inductive bias towards concepts expressed as short logical formulas. Analyzing results from previous behavioral experiments in which people learned logical concepts from a few examples, we find that our meta-trained models are highly aligned with human performance.","sentences":["Humans can learn new concepts from a small number of examples by drawing on their inductive biases.","These inductive biases have previously been captured by using Bayesian models defined over symbolic hypothesis spaces.","Is it possible to create a neural network that displays the same inductive biases?","We show that inductive biases that enable rapid concept learning can be instantiated in artificial neural networks by distilling a prior distribution from a symbolic Bayesian model via meta-learning, an approach for extracting the common structure from a set of tasks.","By generating the set of tasks used in meta-learning from the prior distribution of a Bayesian model, we are able to transfer that prior into a neural network.","We use this approach to create a neural network with an inductive bias towards concepts expressed as short logical formulas.","Analyzing results from previous behavioral experiments in which people learned logical concepts from a few examples, we find that our meta-trained models are highly aligned with human performance."],"url":"http://arxiv.org/abs/2402.07035v1","category":"cs.LG"}
{"created":"2024-02-10 19:54:08","title":"Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models","abstract":"Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over $3$ tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods. The code of Fiddler is publicly available at \\url{https://github.com/efeslab/fiddler}","sentences":["Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks.","However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes.","Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU.","In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models.","The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU.","Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over $3$ tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods.","The code of Fiddler is publicly available at \\url{https://github.com/efeslab/fiddler}"],"url":"http://arxiv.org/abs/2402.07033v1","category":"cs.LG"}
{"created":"2024-02-10 19:45:40","title":"Instance-Level Safety-Aware Fidelity of Synthetic Data and Its Calibration","abstract":"Modeling and calibrating the fidelity of synthetic data is paramount in shaping the future of safe and reliable self-driving technology by offering a cost-effective and scalable alternative to real-world data collection. We focus on its role in safety-critical applications, introducing four types of instance-level fidelity that go beyond mere visual input characteristics. The aim is to align synthetic data with real-world safety issues. We suggest an optimization method to refine the synthetic data generator, reducing fidelity gaps identified by the DNN-based component. Our findings show this tuning enhances the correlation between safety-critical errors in synthetic and real images.","sentences":["Modeling and calibrating the fidelity of synthetic data is paramount in shaping the future of safe and reliable self-driving technology by offering a cost-effective and scalable alternative to real-world data collection.","We focus on its role in safety-critical applications, introducing four types of instance-level fidelity that go beyond mere visual input characteristics.","The aim is to align synthetic data with real-world safety issues.","We suggest an optimization method to refine the synthetic data generator, reducing fidelity gaps identified by the DNN-based component.","Our findings show this tuning enhances the correlation between safety-critical errors in synthetic and real images."],"url":"http://arxiv.org/abs/2402.07031v1","category":"cs.SE"}
{"created":"2024-02-10 19:09:17","title":"Finding safe 3D robot grasps through efficient haptic exploration with unscented Bayesian optimization and collision penalty","abstract":"Robust grasping is a major, and still unsolved, problem in robotics. Information about the 3D shape of an object can be obtained either from prior knowledge (e.g., accurate models of known objects or approximate models of familiar objects) or real-time sensing (e.g., partial point clouds of unknown objects) and can be used to identify good potential grasps. However, due to modeling and sensing inaccuracies, local exploration is often needed to refine such grasps and successfully apply them in the real world. The recently proposed unscented Bayesian optimization technique can make such exploration safer by selecting grasps that are robust to uncertainty in the input space (e.g., inaccuracies in the grasp execution). Extending our previous work on 2D optimization, in this paper we propose a 3D haptic exploration strategy that combines unscented Bayesian optimization with a novel collision penalty heuristic to find safe grasps in a very efficient way: while by augmenting the search-space to 3D we are able to find better grasps, the collision penalty heuristic allows us to do so without increasing the number of exploration steps.","sentences":["Robust grasping is a major, and still unsolved, problem in robotics.","Information about the 3D shape of an object can be obtained either from prior knowledge (e.g., accurate models of known objects or approximate models of familiar objects) or real-time sensing (e.g., partial point clouds of unknown objects) and can be used to identify good potential grasps.","However, due to modeling and sensing inaccuracies, local exploration is often needed to refine such grasps and successfully apply them in the real world.","The recently proposed unscented Bayesian optimization technique can make such exploration safer by selecting grasps that are robust to uncertainty in the input space (e.g., inaccuracies in the grasp execution).","Extending our previous work on 2D optimization, in this paper we propose a 3D haptic exploration strategy that combines unscented Bayesian optimization with a novel collision penalty heuristic to find safe grasps in a very efficient way: while by augmenting the search-space to 3D we are able to find better grasps, the collision penalty heuristic allows us to do so without increasing the number of exploration steps."],"url":"http://arxiv.org/abs/2402.07024v1","category":"cs.RO"}
{"created":"2024-02-10 19:08:28","title":"Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations","abstract":"Large language models have the potential to be valuable in the healthcare industry, but it's crucial to verify their safety and effectiveness through rigorous evaluation. For this purpose, we comprehensively evaluated both open-source LLMs and Google's new multimodal LLM called Gemini across Medical reasoning, hallucination detection, and Medical Visual Question Answering tasks. While Gemini showed competence, it lagged behind state-of-the-art models like MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved an accuracy of 61.45\\% on the medical VQA dataset, significantly lower than GPT-4V's score of 88\\%. Our analysis revealed that Gemini is highly susceptible to hallucinations, overconfidence, and knowledge gaps, which indicate risks if deployed uncritically. We also performed a detailed analysis by medical subject and test type, providing actionable feedback for developers and clinicians. To mitigate risks, we applied prompting strategies that improved performance. Additionally, we facilitated future research and development by releasing a Python module for medical LLM evaluation and establishing a dedicated leaderboard on Hugging Face for medical domain LLMs. Python module can be found at https://github.com/promptslab/RosettaEval","sentences":["Large language models have the potential to be valuable in the healthcare industry, but it's crucial to verify their safety and effectiveness through rigorous evaluation.","For this purpose, we comprehensively evaluated both open-source LLMs and Google's new multimodal LLM called Gemini across Medical reasoning, hallucination detection, and Medical Visual Question Answering tasks.","While Gemini showed competence, it lagged behind state-of-the-art models like MedPaLM 2 and GPT-4 in diagnostic accuracy.","Additionally, Gemini achieved an accuracy of 61.45\\% on the medical VQA dataset, significantly lower than GPT-4V's score of 88\\%.","Our analysis revealed that Gemini is highly susceptible to hallucinations, overconfidence, and knowledge gaps, which indicate risks if deployed uncritically.","We also performed a detailed analysis by medical subject and test type, providing actionable feedback for developers and clinicians.","To mitigate risks, we applied prompting strategies that improved performance.","Additionally, we facilitated future research and development by releasing a Python module for medical LLM evaluation and establishing a dedicated leaderboard on Hugging Face for medical domain LLMs.","Python module can be found at https://github.com/promptslab/RosettaEval"],"url":"http://arxiv.org/abs/2402.07023v1","category":"cs.CL"}
{"created":"2024-02-10 18:27:28","title":"REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models","abstract":"The integration of multimodal Electronic Health Records (EHR) data has significantly improved clinical predictive capabilities. Leveraging clinical notes and multivariate time-series EHR, existing models often lack the medical context relevent to clinical tasks, prompting the incorporation of external knowledge, particularly from the knowledge graph (KG). Previous approaches with KG knowledge have primarily focused on structured knowledge extraction, neglecting unstructured data modalities and semantic high dimensional medical knowledge. In response, we propose REALM, a Retrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR representations that address these limitations. Firstly, we apply Large Language Model (LLM) to encode long context clinical notes and GRU model to encode time-series EHR data. Secondly, we prompt LLM to extract task-relevant medical entities and match entities in professionally labeled external knowledge graph (PrimeKG) with corresponding medical knowledge. By matching and aligning with clinical standards, our framework eliminates hallucinations and ensures consistency. Lastly, we propose an adaptive multimodal fusion network to integrate extracted knowledge with multimodal EHR data. Our extensive experiments on MIMIC-III mortality and readmission tasks showcase the superior performance of our REALM framework over baselines, emphasizing the effectiveness of each module. REALM framework contributes to refining the use of multimodal EHR data in healthcare and bridging the gap with nuanced medical context essential for informed clinical predictions.","sentences":["The integration of multimodal Electronic Health Records (EHR) data has significantly improved clinical predictive capabilities.","Leveraging clinical notes and multivariate time-series EHR, existing models often lack the medical context relevent to clinical tasks, prompting the incorporation of external knowledge, particularly from the knowledge graph (KG).","Previous approaches with KG knowledge have primarily focused on structured knowledge extraction, neglecting unstructured data modalities and semantic high dimensional medical knowledge.","In response, we propose REALM, a Retrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR representations that address these limitations.","Firstly, we apply Large Language Model (LLM) to encode long context clinical notes and GRU model to encode time-series EHR data.","Secondly, we prompt LLM to extract task-relevant medical entities and match entities in professionally labeled external knowledge graph (PrimeKG) with corresponding medical knowledge.","By matching and aligning with clinical standards, our framework eliminates hallucinations and ensures consistency.","Lastly, we propose an adaptive multimodal fusion network to integrate extracted knowledge with multimodal EHR data.","Our extensive experiments on MIMIC-III mortality and readmission tasks showcase the superior performance of our REALM framework over baselines, emphasizing the effectiveness of each module.","REALM framework contributes to refining the use of multimodal EHR data in healthcare and bridging the gap with nuanced medical context essential for informed clinical predictions."],"url":"http://arxiv.org/abs/2402.07016v1","category":"cs.AI"}
{"created":"2024-02-10 18:14:57","title":"FedImpro: Measuring and Improving Client Update in Federated Learning","abstract":"Federated Learning (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients. To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models. In this paper, we present an alternative perspective on client drift and aim to mitigate it by generating improved local models. First, we analyze the generalization contribution of local training and conclude that this generalization contribution is bounded by the conditional Wasserstein distance between the data distribution of different clients. Then, we propose FedImpro, to construct similar conditional distributions for local training. Specifically, FedImpro decouples the model into high-level and low-level components, and trains the high-level portion on reconstructed feature distributions. This approach enhances the generalization contribution and reduces the dissimilarity of gradients in FL. Experimental results show that FedImpro can help FL defend against data heterogeneity and enhance the generalization performance of the model.","sentences":["Federated Learning (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients.","To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models.","In this paper, we present an alternative perspective on client drift and aim to mitigate it by generating improved local models.","First, we analyze the generalization contribution of local training and conclude that this generalization contribution is bounded by the conditional Wasserstein distance between the data distribution of different clients.","Then, we propose FedImpro, to construct similar conditional distributions for local training.","Specifically, FedImpro decouples the model into high-level and low-level components, and trains the high-level portion on reconstructed feature distributions.","This approach enhances the generalization contribution and reduces the dissimilarity of gradients in FL.","Experimental results show that FedImpro can help FL defend against data heterogeneity and enhance the generalization performance of the model."],"url":"http://arxiv.org/abs/2402.07011v1","category":"cs.LG"}
{"created":"2024-02-10 18:14:19","title":"Impact of Voice Fidelity on Decision Making: A Potential Dark Pattern?","abstract":"Manipulative design in user interfaces (conceptualized as dark patterns) has emerged as a significant impediment to the ethical design of technology and a threat to user agency and freedom of choice. While previous research focused on exploring these patterns in the context of graphical user interfaces, the impact of speech has largely been overlooked. We conducted a listening test (N = 50) to elicit participants' preferences regarding different synthetic voices that varied in terms of synthesis method (concatenative vs. neural) and prosodic qualities (speech pace and pitch variance), and then evaluated their impact in an online decision-making study (N = 101). Our results indicate a significant effect of voice qualities on the participant's choices, independently from the content of the available options. Our results also indicate that the voice's perceived engagement, ease of understanding, and domain fit directly translate to its impact on participants' behaviour in decision-making tasks.","sentences":["Manipulative design in user interfaces (conceptualized as dark patterns) has emerged as a significant impediment to the ethical design of technology and a threat to user agency and freedom of choice.","While previous research focused on exploring these patterns in the context of graphical user interfaces, the impact of speech has largely been overlooked.","We conducted a listening test (N = 50) to elicit participants' preferences regarding different synthetic voices that varied in terms of synthesis method (concatenative vs. neural) and prosodic qualities (speech pace and pitch variance), and then evaluated their impact in an online decision-making study (N = 101).","Our results indicate a significant effect of voice qualities on the participant's choices, independently from the content of the available options.","Our results also indicate that the voice's perceived engagement, ease of understanding, and domain fit directly translate to its impact on participants' behaviour in decision-making tasks."],"url":"http://arxiv.org/abs/2402.07010v1","category":"cs.HC"}
{"created":"2024-02-10 17:54:01","title":"Min-Max transformation for the measurement of sports performance through PIR","abstract":"The objective of this work is to analyze the usefulness to transform the information to measure sports performance. This analysis is carried out within the field of basketball due to the existing tradition in this sport in data collection, although it is easily adaptable to any other sport. As a result, a modification of the Performance Index Rating (PIR) is proposed to measure sports performance. The results obtained are illustrated from the statistics of Larry Bird, Earvin Johnson, Michael Jordan and Kobe Bryant throughout their careers and can serve to optimize the process of player renewal/firing/hiring in the design of a roster or to aid decision-making in awarding individual awards as best player of a season.","sentences":["The objective of this work is to analyze the usefulness to transform the information to measure sports performance.","This analysis is carried out within the field of basketball due to the existing tradition in this sport in data collection, although it is easily adaptable to any other sport.","As a result, a modification of the Performance Index Rating (PIR) is proposed to measure sports performance.","The results obtained are illustrated from the statistics of Larry Bird, Earvin Johnson, Michael Jordan and Kobe Bryant throughout their careers and can serve to optimize the process of player renewal/firing/hiring in the design of a roster or to aid decision-making in awarding individual awards as best player of a season."],"url":"http://arxiv.org/abs/2402.07004v1","category":"stat.AP"}
{"created":"2024-02-10 17:39:34","title":"Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off","abstract":"To defend against privacy leakage of user data, differential privacy is widely used in federated learning, but it is not free. The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds. In this paper, we introduce a novel federated learning framework with rigorous privacy guarantees, named FedCEO, designed to strike a trade-off between model utility and user privacy by letting clients ''Collaborate with Each Other''. Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to flexibly truncate high-frequency components in spectral space. This implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes. Moreover, we improve the SOTA utility-privacy trade-off bound by an order of $\\sqrt{d}$, where $d$ is the input dimension. We illustrate our theoretical results with experiments on representative image datasets. It observes significant performance improvements and strict privacy guarantees under different privacy settings.","sentences":["To defend against privacy leakage of user data, differential privacy is widely used in federated learning, but it is not free.","The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds.","In this paper, we introduce a novel federated learning framework with rigorous privacy guarantees, named FedCEO, designed to strike a trade-off between model utility and user privacy by letting clients ''Collaborate with Each Other''.","Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to flexibly truncate high-frequency components in spectral space.","This implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes.","Moreover, we improve the SOTA utility-privacy trade-off bound by an order of $\\sqrt{d}$, where $d$ is the input dimension.","We illustrate our theoretical results with experiments on representative image datasets.","It observes significant performance improvements and strict privacy guarantees under different privacy settings."],"url":"http://arxiv.org/abs/2402.07002v1","category":"cs.LG"}
{"created":"2024-02-10 17:35:59","title":"Artificial Intelligence-Enabled Optimization of Battery-Grade Lithium Carbonate Production","abstract":"By 2035, the need for battery-grade lithium is expected to quadruple. About half of this lithium is currently sourced from brines and must be converted from a chloride into lithium carbonate (Li2CO3) through a process called softening. Conventional softening methods using sodium or potassium salts contribute to carbon emissions during reagent mining and battery manufacturing, exacerbating global warming. This study introduces an alternative approach using carbon dioxide (CO2(g)) as the carbonating reagent in the lithium softening process, offering a carbon capture solution. We employed an active learning-driven high-throughput method to rapidly capture CO2(g) and convert it to lithium carbonate. The model was simplified by focusing on the elemental concentrations of C, Li, and N for practical measurement and tracking, avoiding the complexities of ion speciation equilibria. This approach led to an optimized lithium carbonate process that capitalizes on CO2(g) capture and improves the battery metal supply chain's carbon efficiency.","sentences":["By 2035, the need for battery-grade lithium is expected to quadruple.","About half of this lithium is currently sourced from brines and must be converted from a chloride into lithium carbonate (Li2CO3) through a process called softening.","Conventional softening methods using sodium or potassium salts contribute to carbon emissions during reagent mining and battery manufacturing, exacerbating global warming.","This study introduces an alternative approach using carbon dioxide (CO2(g)) as the carbonating reagent in the lithium softening process, offering a carbon capture solution.","We employed an active learning-driven high-throughput method to rapidly capture CO2(g) and convert it to lithium carbonate.","The model was simplified by focusing on the elemental concentrations of C, Li, and N for practical measurement and tracking, avoiding the complexities of ion speciation equilibria.","This approach led to an optimized lithium carbonate process that capitalizes on CO2(g) capture and improves the battery metal supply chain's carbon efficiency."],"url":"http://arxiv.org/abs/2402.07000v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-10 16:54:28","title":"A Rational Analysis of the Speech-to-Song Illusion","abstract":"The speech-to-song illusion is a robust psychological phenomenon whereby a spoken sentence sounds increasingly more musical as it is repeated. Despite decades of research, a complete formal account of this transformation is still lacking, and some of its nuanced characteristics, namely, that certain phrases appear to transform while others do not, is not well understood. Here we provide a formal account of this phenomenon, by recasting it as a statistical inference whereby a rational agent attempts to decide whether a sequence of utterances is more likely to have been produced in a song or speech. Using this approach and analyzing song and speech corpora, we further introduce a novel prose-to-lyrics illusion that is purely text-based. In this illusion, simply duplicating written sentences makes them appear more like song lyrics. We provide robust evidence for this new illusion in both human participants and large language models.","sentences":["The speech-to-song illusion is a robust psychological phenomenon whereby a spoken sentence sounds increasingly more musical as it is repeated.","Despite decades of research, a complete formal account of this transformation is still lacking, and some of its nuanced characteristics, namely, that certain phrases appear to transform while others do not, is not well understood.","Here we provide a formal account of this phenomenon, by recasting it as a statistical inference whereby a rational agent attempts to decide whether a sequence of utterances is more likely to have been produced in a song or speech.","Using this approach and analyzing song and speech corpora, we further introduce a novel prose-to-lyrics illusion that is purely text-based.","In this illusion, simply duplicating written sentences makes them appear more like song lyrics.","We provide robust evidence for this new illusion in both human participants and large language models."],"url":"http://arxiv.org/abs/2402.06992v1","category":"q-bio.NC"}
{"created":"2024-02-10 16:42:18","title":"Designing for Work with Intelligent Entities: A Review of Perspectives","abstract":"As the power of Artificial Intelligence (AI) continues to advance, there is increased interest in how best to combine AI-based agents with humans to achieve mission effectiveness. Three perspectives have emerged. The first stems from more conventional human factors traditions and views these entities as highly capable tools that humans can use to accomplish increasingly sophisticated tasks. The second \"camp\" believes that as the sophistication of these entities increases, it becomes increasingly appropriate to talk about them as \"teammates\" and use the research on human teams as a foundation for further exploration. The third perspective is emerging and finds both the \"tools\" and \"teammate\" metaphors flawed and limiting. This perspective emphasizes \"joint activity,\" \"joint cognitive activity,\" or something similar. In this article, we briefly review these three perspectives.","sentences":["As the power of Artificial Intelligence (AI) continues to advance, there is increased interest in how best to combine AI-based agents with humans to achieve mission effectiveness.","Three perspectives have emerged.","The first stems from more conventional human factors traditions and views these entities as highly capable tools that humans can use to accomplish increasingly sophisticated tasks.","The second \"camp\" believes that as the sophistication of these entities increases, it becomes increasingly appropriate to talk about them as \"teammates\" and use the research on human teams as a foundation for further exploration.","The third perspective is emerging and finds both the \"tools\" and \"teammate\" metaphors flawed and limiting.","This perspective emphasizes \"joint activity,\" \"joint cognitive activity,\" or something similar.","In this article, we briefly review these three perspectives."],"url":"http://arxiv.org/abs/2402.06989v1","category":"cs.HC"}
{"created":"2024-02-10 16:23:12","title":"OSSAR: Towards Open-Set Surgical Activity Recognition in Robot-assisted Surgery","abstract":"In the realm of automated robotic surgery and computer-assisted interventions, understanding robotic surgical activities stands paramount. Existing algorithms dedicated to surgical activity recognition predominantly cater to pre-defined closed-set paradigms, ignoring the challenges of real-world open-set scenarios. Such algorithms often falter in the presence of test samples originating from classes unseen during training phases. To tackle this problem, we introduce an innovative Open-Set Surgical Activity Recognition (OSSAR) framework. Our solution leverages the hyperspherical reciprocal point strategy to enhance the distinction between known and unknown classes in the feature space. Additionally, we address the issue of over-confidence in the closed set by refining model calibration, avoiding misclassification of unknown classes as known ones. To support our assertions, we establish an open-set surgical activity benchmark utilizing the public JIGSAWS dataset. Besides, we also collect a novel dataset on endoscopic submucosal dissection for surgical activity tasks. Extensive comparisons and ablation experiments on these datasets demonstrate the significant outperformance of our method over existing state-of-the-art approaches. Our proposed solution can effectively address the challenges of real-world surgical scenarios. Our code is publicly accessible at https://github.com/longbai1006/OSSAR.","sentences":["In the realm of automated robotic surgery and computer-assisted interventions, understanding robotic surgical activities stands paramount.","Existing algorithms dedicated to surgical activity recognition predominantly cater to pre-defined closed-set paradigms, ignoring the challenges of real-world open-set scenarios.","Such algorithms often falter in the presence of test samples originating from classes unseen during training phases.","To tackle this problem, we introduce an innovative Open-Set Surgical Activity Recognition (OSSAR) framework.","Our solution leverages the hyperspherical reciprocal point strategy to enhance the distinction between known and unknown classes in the feature space.","Additionally, we address the issue of over-confidence in the closed set by refining model calibration, avoiding misclassification of unknown classes as known ones.","To support our assertions, we establish an open-set surgical activity benchmark utilizing the public JIGSAWS dataset.","Besides, we also collect a novel dataset on endoscopic submucosal dissection for surgical activity tasks.","Extensive comparisons and ablation experiments on these datasets demonstrate the significant outperformance of our method over existing state-of-the-art approaches.","Our proposed solution can effectively address the challenges of real-world surgical scenarios.","Our code is publicly accessible at https://github.com/longbai1006/OSSAR."],"url":"http://arxiv.org/abs/2402.06985v1","category":"cs.CV"}
{"created":"2024-02-10 16:16:24","title":"Speech motion anomaly detection via cross-modal translation of 4D motion fields from tagged MRI","abstract":"Understanding the relationship between tongue motion patterns during speech and their resulting speech acoustic outcomes -- i.e., articulatory-acoustic relation -- is of great importance in assessing speech quality and developing innovative treatment and rehabilitative strategies. This is especially important when evaluating and detecting abnormal articulatory features in patients with speech-related disorders. In this work, we aim to develop a framework for detecting speech motion anomalies in conjunction with their corresponding speech acoustics. This is achieved through the use of a deep cross-modal translator trained on data from healthy individuals only, which bridges the gap between 4D motion fields obtained from tagged MRI and 2D spectrograms derived from speech acoustic data. The trained translator is used as an anomaly detector, by measuring the spectrogram reconstruction quality on healthy individuals or patients. In particular, the cross-modal translator is likely to yield limited generalization capabilities on patient data, which includes unseen out-of-distribution patterns and demonstrates subpar performance, when compared with healthy individuals.~A one-class SVM is then used to distinguish the spectrograms of healthy individuals from those of patients. To validate our framework, we collected a total of 39 paired tagged MRI and speech waveforms, consisting of data from 36 healthy individuals and 3 tongue cancer patients. We used both 3D convolutional and transformer-based deep translation models, training them on the healthy training set and then applying them to both the healthy and patient testing sets. Our framework demonstrates a capability to detect abnormal patient data, thereby illustrating its potential in enhancing the understanding of the articulatory-acoustic relation for both healthy individuals and patients.","sentences":["Understanding the relationship between tongue motion patterns during speech and their resulting speech acoustic outcomes -- i.e., articulatory-acoustic relation -- is of great importance in assessing speech quality and developing innovative treatment and rehabilitative strategies.","This is especially important when evaluating and detecting abnormal articulatory features in patients with speech-related disorders.","In this work, we aim to develop a framework for detecting speech motion anomalies in conjunction with their corresponding speech acoustics.","This is achieved through the use of a deep cross-modal translator trained on data from healthy individuals only, which bridges the gap between 4D motion fields obtained from tagged MRI and 2D spectrograms derived from speech acoustic data.","The trained translator is used as an anomaly detector, by measuring the spectrogram reconstruction quality on healthy individuals or patients.","In particular, the cross-modal translator is likely to yield limited generalization capabilities on patient data, which includes unseen out-of-distribution patterns and demonstrates subpar performance, when compared with healthy individuals.~A one-class SVM is then used to distinguish the spectrograms of healthy individuals from those of patients.","To validate our framework, we collected a total of 39 paired tagged MRI and speech waveforms, consisting of data from 36 healthy individuals and 3 tongue cancer patients.","We used both 3D convolutional and transformer-based deep translation models, training them on the healthy training set and then applying them to both the healthy and patient testing sets.","Our framework demonstrates a capability to detect abnormal patient data, thereby illustrating its potential in enhancing the understanding of the articulatory-acoustic relation for both healthy individuals and patients."],"url":"http://arxiv.org/abs/2402.06984v1","category":"cs.SD"}
{"created":"2024-02-10 16:15:42","title":"Cylindrical compression of thin wires by irradiation with a Joule-class short pulse laser","abstract":"Equation of state measurements at Jovian or stellar conditions are currently conducted by dynamic shock compression driven by multi-kilojoule multi-beam nanosecond-duration lasers. These experiments require precise design of the target and specific tailoring of the spatial and temporal laser profiles to reach the highest pressures. At the same time, the studies are limited by the low repetition rate of the lasers. Here, we show that by the irradiation of a thin wire with single beam Joule-class short-pulse laser, a converging cylindrical shock is generated compressing the wire material to conditions relevant for the above applications. The shockwave was observed using Phase Contrast Imaging employing a hard X-ray Free Electron Laser with unprecedented temporal and spatial sensitivity. The data collected for Cu wires is in agreement with hydrodynamic simulations of an ablative shock launched by a highly-impulsive and transient resistive heating of the wire surface. The subsequent cylindrical shockwave travels towards the wire axis and is predicted to reach a compression factor of 9 and pressures above 800 Mbar. Simulations for astrophysical relevant materials underline the potential of this compression technique as a new tool for high energy density studies at high repetition rates.","sentences":["Equation of state measurements at Jovian or stellar conditions are currently conducted by dynamic shock compression driven by multi-kilojoule multi-beam nanosecond-duration lasers.","These experiments require precise design of the target and specific tailoring of the spatial and temporal laser profiles to reach the highest pressures.","At the same time, the studies are limited by the low repetition rate of the lasers.","Here, we show that by the irradiation of a thin wire with single beam Joule-class short-pulse laser, a converging cylindrical shock is generated compressing the wire material to conditions relevant for the above applications.","The shockwave was observed using Phase Contrast Imaging employing a hard X-ray Free Electron Laser with unprecedented temporal and spatial sensitivity.","The data collected for Cu wires is in agreement with hydrodynamic simulations of an ablative shock launched by a highly-impulsive and transient resistive heating of the wire surface.","The subsequent cylindrical shockwave travels towards the wire axis and is predicted to reach a compression factor of 9 and pressures above 800 Mbar.","Simulations for astrophysical relevant materials underline the potential of this compression technique as a new tool for high energy density studies at high repetition rates."],"url":"http://arxiv.org/abs/2402.06983v1","category":"physics.plasm-ph"}
{"created":"2024-02-10 16:13:09","title":"Treatment-wise Glioblastoma Survival Inference with Multi-parametric Preoperative MRI","abstract":"In this work, we aim to predict the survival time (ST) of glioblastoma (GBM) patients undergoing different treatments based on preoperative magnetic resonance (MR) scans. The personalized and precise treatment planning can be achieved by comparing the ST of different treatments. It is well established that both the current status of the patient (as represented by the MR scans) and the choice of treatment are the cause of ST. While previous related MR-based glioblastoma ST studies have focused only on the direct mapping of MR scans to ST, they have not included the underlying causal relationship between treatments and ST. To address this limitation, we propose a treatment-conditioned regression model for glioblastoma ST that incorporates treatment information in addition to MR scans. Our approach allows us to effectively utilize the data from all of the treatments in a unified manner, rather than having to train separate models for each of the treatments. Furthermore, treatment can be effectively injected into each convolutional layer through the adaptive instance normalization we employ. We evaluate our framework on the BraTS20 ST prediction task. Three treatment options are considered: Gross Total Resection (GTR), Subtotal Resection (STR), and no resection. The evaluation results demonstrate the effectiveness of injecting the treatment for estimating GBM survival.","sentences":["In this work, we aim to predict the survival time (ST) of glioblastoma (GBM) patients undergoing different treatments based on preoperative magnetic resonance (MR) scans.","The personalized and precise treatment planning can be achieved by comparing the ST of different treatments.","It is well established that both the current status of the patient (as represented by the MR scans) and the choice of treatment are the cause of ST.","While previous related MR-based glioblastoma ST studies have focused only on the direct mapping of MR scans to ST, they have not included the underlying causal relationship between treatments and ST.","To address this limitation, we propose a treatment-conditioned regression model for glioblastoma ST that incorporates treatment information in addition to MR scans.","Our approach allows us to effectively utilize the data from all of the treatments in a unified manner, rather than having to train separate models for each of the treatments.","Furthermore, treatment can be effectively injected into each convolutional layer through the adaptive instance normalization we employ.","We evaluate our framework on the BraTS20 ST prediction task.","Three treatment options are considered: Gross Total Resection (GTR), Subtotal Resection (STR), and no resection.","The evaluation results demonstrate the effectiveness of injecting the treatment for estimating GBM survival."],"url":"http://arxiv.org/abs/2402.06982v1","category":"cs.CV"}
{"created":"2024-02-10 15:32:53","title":"Event-Keyed Summarization","abstract":"We introduce event-keyed summarization (EKS), a novel task that marries traditional summarization and document-level event extraction, with the goal of generating a contextualized summary for a specific event, given a document and an extracted event structure. We introduce a dataset for this task, MUCSUM, consisting of summaries of all events in the classic MUC-4 dataset, along with a set of baselines that comprises both pretrained LM standards in the summarization literature, as well as larger frontier models. We show that ablations that reduce EKS to traditional summarization or structure-to-text yield inferior summaries of target events and that MUCSUM is a robust benchmark for this task. Lastly, we conduct a human evaluation of both reference and model summaries, and provide some detailed analysis of the results.","sentences":["We introduce event-keyed summarization (EKS), a novel task that marries traditional summarization and document-level event extraction, with the goal of generating a contextualized summary for a specific event, given a document and an extracted event structure.","We introduce a dataset for this task, MUCSUM, consisting of summaries of all events in the classic MUC-4 dataset, along with a set of baselines that comprises both pretrained LM standards in the summarization literature, as well as larger frontier models.","We show that ablations that reduce EKS to traditional summarization or structure-to-text yield inferior summaries of target events and that MUCSUM is a robust benchmark for this task.","Lastly, we conduct a human evaluation of both reference and model summaries, and provide some detailed analysis of the results."],"url":"http://arxiv.org/abs/2402.06973v1","category":"cs.CL"}
{"created":"2024-02-10 14:52:52","title":"Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue","abstract":"Tuning pretrained language models for dialogue generation has been a prevalent paradigm for building capable dialogue agents. Yet, traditional tuning narrowly views dialogue generation as resembling other language generation tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such a manner leads to unsatisfactory chat consistency of the built agent. In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently. We propose an efficient Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the agent and user individually with two adapters built upon large language models, where they utilize utterances round by round in alternating order and are tuned via a round-level memory caching mechanism. Extensive experiments demonstrate that, our framework performs superior to traditional fine-tuning and harbors the tremendous potential for improving dialogue consistency.","sentences":["Tuning pretrained language models for dialogue generation has been a prevalent paradigm for building capable dialogue agents.","Yet, traditional tuning narrowly views dialogue generation as resembling other language generation tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be.","Such a manner leads to unsatisfactory chat consistency of the built agent.","In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently.","We propose an efficient Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework.","It models the agent and user individually with two adapters built upon large language models, where they utilize utterances round by round in alternating order and are tuned via a round-level memory caching mechanism.","Extensive experiments demonstrate that, our framework performs superior to traditional fine-tuning and harbors the tremendous potential for improving dialogue consistency."],"url":"http://arxiv.org/abs/2402.06967v1","category":"cs.CL"}
{"created":"2024-02-10 14:36:31","title":"Tree Ensembles for Contextual Bandits","abstract":"We propose a novel framework for contextual multi-armed bandits based on tree ensembles. Our framework integrates two widely used bandit methods, Upper Confidence Bound and Thompson Sampling, for both standard and combinatorial settings. We demonstrate the effectiveness of our framework via several experimental studies, employing XGBoost, a popular tree ensemble method. Compared to state-of-the-art methods based on neural networks, our methods exhibit superior performance in terms of both regret minimization and computational runtime, when applied to benchmark datasets and the real-world application of navigation over road networks.","sentences":["We propose a novel framework for contextual multi-armed bandits based on tree ensembles.","Our framework integrates two widely used bandit methods, Upper Confidence Bound and Thompson Sampling, for both standard and combinatorial settings.","We demonstrate the effectiveness of our framework via several experimental studies, employing XGBoost, a popular tree ensemble method.","Compared to state-of-the-art methods based on neural networks, our methods exhibit superior performance in terms of both regret minimization and computational runtime, when applied to benchmark datasets and the real-world application of navigation over road networks."],"url":"http://arxiv.org/abs/2402.06963v1","category":"cs.LG"}
{"created":"2024-02-10 13:57:51","title":"Architectural Neural Backdoors from First Principles","abstract":"While previous research backdoored neural networks by changing their parameters, recent work uncovered a more insidious threat: backdoors embedded within the definition of the network's architecture. This involves injecting common architectural components, such as activation functions and pooling layers, to subtly introduce a backdoor behavior that persists even after (full re-)training. However, the full scope and implications of architectural backdoors have remained largely unexplored. Bober-Irizar et al. [2023] introduced the first architectural backdoor; they showed how to create a backdoor for a checkerboard pattern, but never explained how to target an arbitrary trigger pattern of choice. In this work we construct an arbitrary trigger detector which can be used to backdoor an architecture with no human supervision. This leads us to revisit the concept of architecture backdoors and taxonomise them, describing 12 distinct types. To gauge the difficulty of detecting such backdoors, we conducted a user study, revealing that ML developers can only identify suspicious components in common model definitions as backdoors in 37% of cases, while they surprisingly preferred backdoored models in 33% of cases. To contextualize these results, we find that language models outperform humans at the detection of backdoors. Finally, we discuss defenses against architectural backdoors, emphasizing the need for robust and comprehensive strategies to safeguard the integrity of ML systems.","sentences":["While previous research backdoored neural networks by changing their parameters, recent work uncovered a more insidious threat: backdoors embedded within the definition of the network's architecture.","This involves injecting common architectural components, such as activation functions and pooling layers, to subtly introduce a backdoor behavior that persists even after (full re-)training.","However, the full scope and implications of architectural backdoors have remained largely unexplored.","Bober-Irizar et al.","[2023] introduced the first architectural backdoor; they showed how to create a backdoor for a checkerboard pattern, but never explained how to target an arbitrary trigger pattern of choice.","In this work we construct an arbitrary trigger detector which can be used to backdoor an architecture with no human supervision.","This leads us to revisit the concept of architecture backdoors and taxonomise them, describing 12 distinct types.","To gauge the difficulty of detecting such backdoors, we conducted a user study, revealing that ML developers can only identify suspicious components in common model definitions as backdoors in 37% of cases, while they surprisingly preferred backdoored models in 33% of cases.","To contextualize these results, we find that language models outperform humans at the detection of backdoors.","Finally, we discuss defenses against architectural backdoors, emphasizing the need for robust and comprehensive strategies to safeguard the integrity of ML systems."],"url":"http://arxiv.org/abs/2402.06957v1","category":"cs.CR"}
{"created":"2024-02-10 13:51:09","title":"Training dynamics in Physics-Informed Neural Networks with feature mapping","abstract":"Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine learning approach for solving Partial Differential Equations (PDEs). Although its variants have achieved significant progress, the empirical success of utilising feature mapping from the wider Implicit Neural Representations studies has been substantially neglected. We investigate the training dynamics of PINNs with a feature mapping layer via the limiting Conjugate Kernel and Neural Tangent Kernel, which sheds light on the convergence and generalisation of the model. We also show the inadequacy of commonly used Fourier-based feature mapping in some scenarios and propose the conditional positive definite Radial Basis Function as a better alternative. The empirical results reveal the efficacy of our method in diverse forward and inverse problem sets. This simple technique can be easily implemented in coordinate input networks and benefits the broad PINNs research.","sentences":["Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine learning approach for solving Partial Differential Equations (PDEs).","Although its variants have achieved significant progress, the empirical success of utilising feature mapping from the wider Implicit Neural Representations studies has been substantially neglected.","We investigate the training dynamics of PINNs with a feature mapping layer via the limiting Conjugate Kernel and Neural Tangent Kernel, which sheds light on the convergence and generalisation of the model.","We also show the inadequacy of commonly used Fourier-based feature mapping in some scenarios and propose the conditional positive definite Radial Basis Function as a better alternative.","The empirical results reveal the efficacy of our method in diverse forward and inverse problem sets.","This simple technique can be easily implemented in coordinate input networks and benefits the broad PINNs research."],"url":"http://arxiv.org/abs/2402.06955v1","category":"cs.LG"}
{"created":"2024-02-10 13:42:14","title":"Estimating the Effect of Crosstalk Error on Circuit Fidelity Using Noisy Intermediate-Scale Quantum Devices","abstract":"Current advancements in technology have focused the attention of the quantum computing community toward exploring the potential of near-term devices whose computing power surpasses that of classical computers in practical applications. An unresolved central question revolves around whether the inherent noise in these devices can be overcome or whether any potential quantum advantage would be limited. There is no doubt that crosstalk is one of the main sources of noise in noisy intermediate-scale quantum (NISQ) systems, and it poses a fundamental challenge to hardware designs. Crosstalk between parallel instructions can corrupt quantum states and cause incorrect program execution. In this study, we present a comprehensive analysis of the crosstalk error effect on NISQ computers. Our approach is extremely straightforward and practical for characterizing the crosstalk error of various multi-qubit devices. In particular, we combine the randomized benchmarking (RB) and simultaneous randomized benchmarking (SRB) protocol to characterize the crosstalk error from the correlation controlled-NOT (CNOT) gate. We demonstrate this protocol experimentally on 5- \\& 7-qubit devices. Our results demonstrate the crosstalk error model of two different IBM quantum devices over the experimental week and compare the error variation against the machine, number of qubits, quantum volume, processor, and topology of the IBM quantum devices. We then confirm the improvement in the circuit fidelity on different benchmarks by up to 3.06x via inserting an instruction barrier, as compared with an IBM quantum noisy device which offers near-optimal crosstalk mitigation in practice. Most importantly, we provide insight to ensure that the quantum operation can perform its quantum magic undisturbed.","sentences":["Current advancements in technology have focused the attention of the quantum computing community toward exploring the potential of near-term devices whose computing power surpasses that of classical computers in practical applications.","An unresolved central question revolves around whether the inherent noise in these devices can be overcome or whether any potential quantum advantage would be limited.","There is no doubt that crosstalk is one of the main sources of noise in noisy intermediate-scale quantum (NISQ) systems, and it poses a fundamental challenge to hardware designs.","Crosstalk between parallel instructions can corrupt quantum states and cause incorrect program execution.","In this study, we present a comprehensive analysis of the crosstalk error effect on NISQ computers.","Our approach is extremely straightforward and practical for characterizing the crosstalk error of various multi-qubit devices.","In particular, we combine the randomized benchmarking (RB) and simultaneous randomized benchmarking (SRB) protocol to characterize the crosstalk error from the correlation controlled-NOT (CNOT) gate.","We demonstrate this protocol experimentally on 5- \\& 7-qubit devices.","Our results demonstrate the crosstalk error model of two different IBM quantum devices over the experimental week and compare the error variation against the machine, number of qubits, quantum volume, processor, and topology of the IBM quantum devices.","We then confirm the improvement in the circuit fidelity on different benchmarks by up to 3.06x via inserting an instruction barrier, as compared with an IBM quantum noisy device which offers near-optimal crosstalk mitigation in practice.","Most importantly, we provide insight to ensure that the quantum operation can perform its quantum magic undisturbed."],"url":"http://arxiv.org/abs/2402.06952v1","category":"quant-ph"}
{"created":"2024-02-10 13:18:10","title":"Evaluation Metrics for Automated Typographic Poster Generation","abstract":"Computational Design approaches facilitate the generation of typographic design, but evaluating these designs remains a challenging task. In this paper, we propose a set of heuristic metrics for typographic design evaluation, focusing on their legibility, which assesses the text visibility, aesthetics, which evaluates the visual quality of the design, and semantic features, which estimate how effectively the design conveys the content semantics. We experiment with a constrained evolutionary approach for generating typographic posters, incorporating the proposed evaluation metrics with varied setups, and treating the legibility metrics as constraints. We also integrate emotion recognition to identify text semantics automatically and analyse the performance of the approach and the visual characteristics outputs.","sentences":["Computational Design approaches facilitate the generation of typographic design, but evaluating these designs remains a challenging task.","In this paper, we propose a set of heuristic metrics for typographic design evaluation, focusing on their legibility, which assesses the text visibility, aesthetics, which evaluates the visual quality of the design, and semantic features, which estimate how effectively the design conveys the content semantics.","We experiment with a constrained evolutionary approach for generating typographic posters, incorporating the proposed evaluation metrics with varied setups, and treating the legibility metrics as constraints.","We also integrate emotion recognition to identify text semantics automatically and analyse the performance of the approach and the visual characteristics outputs."],"url":"http://arxiv.org/abs/2402.06945v1","category":"cs.MM"}
{"created":"2024-02-10 13:00:37","title":"Toward Scalable Generative AI via Mixture of Experts in Mobile Edge Networks","abstract":"The advancement of generative artificial intelligence (GAI) has driven revolutionary applications like ChatGPT. The widespread of these applications relies on the mixture of experts (MoE), which contains multiple experts and selectively engages them for each task to lower operation costs while maintaining performance. Despite MoE, GAI faces challenges in resource consumption when deployed on user devices. This paper proposes mobile edge networks supported MoE-based GAI. We first review the MoE from traditional AI and GAI perspectives, including structure, principles, and applications. We then propose a framework that transfers subtasks to devices in mobile edge networks, aiding GAI model operation on user devices. We discuss challenges in this process and introduce a deep reinforcement learning based algorithm to select edge devices for subtask execution. Experimental results will show that our framework not only facilitates GAI's deployment on resource-limited devices but also generates higher-quality content compared to methods without edge network support.","sentences":["The advancement of generative artificial intelligence (GAI) has driven revolutionary applications like ChatGPT.","The widespread of these applications relies on the mixture of experts (MoE), which contains multiple experts and selectively engages them for each task to lower operation costs while maintaining performance.","Despite MoE, GAI faces challenges in resource consumption when deployed on user devices.","This paper proposes mobile edge networks supported MoE-based GAI.","We first review the MoE from traditional AI and GAI perspectives, including structure, principles, and applications.","We then propose a framework that transfers subtasks to devices in mobile edge networks, aiding GAI model operation on user devices.","We discuss challenges in this process and introduce a deep reinforcement learning based algorithm to select edge devices for subtask execution.","Experimental results will show that our framework not only facilitates GAI's deployment on resource-limited devices but also generates higher-quality content compared to methods without edge network support."],"url":"http://arxiv.org/abs/2402.06942v1","category":"cs.NI"}
{"created":"2024-02-12 18:59:14","title":"Wavefront Randomization Improves Deconvolution","abstract":"The performance of an imaging system is limited by optical aberrations, which cause blurriness in the resulting image. Digital correction techniques, such as deconvolution, have limited ability to correct the blur, since some spatial frequencies in the scene are not measured adequately due to the aberrations ('zeros' of the system transfer function). We prove that the addition of a random mask to an imaging system removes its dependence on aberrations, reducing the likelihood of zeros in the transfer function and consequently reducing the sensitivity to noise during deconvolution. and consequently result in lower sensitivity to noise during deconvolution. In simulation, we show that this strategy improves image quality over a range of aberration types, aberration strengths, and signal-to-noise ratios.","sentences":["The performance of an imaging system is limited by optical aberrations, which cause blurriness in the resulting image.","Digital correction techniques, such as deconvolution, have limited ability to correct the blur, since some spatial frequencies in the scene are not measured adequately due to the aberrations ('zeros' of the system transfer function).","We prove that the addition of a random mask to an imaging system removes its dependence on aberrations, reducing the likelihood of zeros in the transfer function and consequently reducing the sensitivity to noise during deconvolution.","and consequently result in lower sensitivity to noise during deconvolution.","In simulation, we show that this strategy improves image quality over a range of aberration types, aberration strengths, and signal-to-noise ratios."],"url":"http://arxiv.org/abs/2402.07900v1","category":"cs.CV"}
{"created":"2024-02-12 18:58:01","title":"A holographic mobile-based application for practicing pronunciation of basic English vocabulary for Spanish speaking children","abstract":"This paper describes a holographic mobile-based application designed to help Spanish-speaking children to practice the pronunciation of basic English vocabulary words. The mastery of vocabulary is a fundamental step when learning a language but is often perceived as boring. Producing the correct pronunciation is frequently regarded as the most difficult and complex skill for new learners of English. In order to address these problems this research takes advantage of the power of multi-channel stimuli (sound, image and interaction) in a mobilebased hologram application in order to motivate students and improve their experience of practicing. We adapted the prize-winning HolograFX game and developed a new mobile application to help practice English pronunciation. A 3D holographic robot that acts as a virtual teacher interacts via voice with the children. To test the tool we carried out an experiment with 70 Spanish pre-school children divided into three classes, the control group using traditional methods such as images in books and on the blackboard, and two experimental groups using our drills and practice software. One experimental group used the mobile application without the holographic game and the other experimental group used the application with the holographic game. We performed pre-test and post-test performance assessments, a satisfaction survey and emotion analysis. The results are very promising. They show that the use of the holographic mobile-based application had a significant impact on the children's motivation. It also improved their performance compared to traditional methods used in the classroom.","sentences":["This paper describes a holographic mobile-based application designed to help Spanish-speaking children to practice the pronunciation of basic English vocabulary words.","The mastery of vocabulary is a fundamental step when learning a language but is often perceived as boring.","Producing the correct pronunciation is frequently regarded as the most difficult and complex skill for new learners of English.","In order to address these problems this research takes advantage of the power of multi-channel stimuli (sound, image and interaction) in a mobilebased hologram application in order to motivate students and improve their experience of practicing.","We adapted the prize-winning HolograFX game and developed a new mobile application to help practice English pronunciation.","A 3D holographic robot that acts as a virtual teacher interacts via voice with the children.","To test the tool we carried out an experiment with 70 Spanish pre-school children divided into three classes, the control group using traditional methods such as images in books and on the blackboard, and two experimental groups using our drills and practice software.","One experimental group used the mobile application without the holographic game and the other experimental group used the application with the holographic game.","We performed pre-test and post-test performance assessments, a satisfaction survey and emotion analysis.","The results are very promising.","They show that the use of the holographic mobile-based application had a significant impact on the children's motivation.","It also improved their performance compared to traditional methods used in the classroom."],"url":"http://arxiv.org/abs/2402.07897v1","category":"cs.HC"}
{"created":"2024-02-12 18:56:53","title":"MODIPHY: Multimodal Obscured Detection for IoT using PHantom Convolution-Enabled Faster YOLO","abstract":"Low-light conditions and occluded scenarios impede object detection in real-world Internet of Things (IoT) applications like autonomous vehicles and security systems. While advanced machine learning models strive for accuracy, their computational demands clash with the limitations of resource-constrained devices, hampering real-time performance. In our current research, we tackle this challenge, by introducing \"YOLO Phantom\", one of the smallest YOLO models ever conceived. YOLO Phantom utilizes the novel Phantom Convolution block, achieving comparable accuracy to the latest YOLOv8n model while simultaneously reducing both parameters and model size by 43%, resulting in a significant 19% reduction in Giga Floating Point Operations (GFLOPs). YOLO Phantom leverages transfer learning on our multimodal RGB-infrared dataset to address low-light and occlusion issues, equipping it with robust vision under adverse conditions. Its real-world efficacy is demonstrated on an IoT platform with advanced low-light and RGB cameras, seamlessly connecting to an AWS-based notification endpoint for efficient real-time object detection. Benchmarks reveal a substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB detection, respectively, compared to the baseline YOLOv8n model. For community contribution, both the code and the multimodal dataset are available on GitHub.","sentences":["Low-light conditions","and","occluded scenarios impede object detection in real-world Internet of Things (IoT) applications like autonomous vehicles and security systems.","While advanced machine learning models strive for accuracy, their computational demands clash with the limitations of resource-constrained devices, hampering real-time performance.","In our current research, we tackle this challenge, by introducing \"YOLO Phantom\", one of the smallest YOLO models ever conceived.","YOLO Phantom utilizes the novel Phantom Convolution block, achieving comparable accuracy to the latest YOLOv8n model while simultaneously reducing both parameters and model size by 43%, resulting in a significant 19% reduction in Giga Floating Point Operations (GFLOPs).","YOLO Phantom leverages transfer learning on our multimodal RGB-infrared dataset to address low-light and occlusion issues, equipping it with robust vision under adverse conditions.","Its real-world efficacy is demonstrated on an IoT platform with advanced low-light and RGB cameras, seamlessly connecting to an AWS-based notification endpoint for efficient real-time object detection.","Benchmarks reveal a substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB detection, respectively, compared to the baseline YOLOv8n model.","For community contribution, both the code and the multimodal dataset are available on GitHub."],"url":"http://arxiv.org/abs/2402.07894v1","category":"cs.CV"}
{"created":"2024-02-12 18:56:13","title":"The TESS-Keck Survey XXI: 13 New Planets and Homogeneous Properties for 21 Subgiant Systems","abstract":"We present a dedicated transit and radial velocity survey of planets orbiting subgiant stars observed by the TESS Mission. Using $\\sim$$16$ nights on Keck/HIRES, we confirm and characterize $12$ new transiting planets -- $\\rm TOI-329\\,b$, $\\rm HD\\,39688\\,b$ ($\\rm TOI-480$), $\\rm TOI-603\\,b$, $\\rm TOI-1199\\,b$, $\\rm TOI-1294\\,b$, $\\rm TOI-1439\\,b$, $\\rm TOI-1605\\,b$, $\\rm TOI-1828\\,b$, $\\rm HD\\,148193\\,b$ ($\\rm TOI-1836$), $\\rm TOI-1885\\,b$, $\\rm HD\\,83342\\,b$ ($\\rm TOI-1898$), $\\rm TOI-2019\\,b$ -- and provide updated properties for 9 previously confirmed TESS subgiant systems ($\\rm TOI-197$, $\\rm TOI-954$, $\\rm TOI-1181$, $\\rm TOI-1296$, $\\rm TOI-1298$, $\\rm TOI-1601$, $\\rm TOI-1736$, $\\rm TOI-1842$, $\\rm TOI-2145$). We also report the discovery of an outer, non-transiting planet, $\\rm TOI-1294\\,c$ ($P=160.1\\pm2.5$ days, $M_{\\mathrm{p}}=148.3^{+18.2}_{-16.4} \\,M_{\\oplus}$), and three additional stars with long-term RV trends. We find that at least $19\\pm8\\%$ of subgiants in our sample of $21$ stars have outer companions, comparable to main-sequence stars. We perform a homogeneous analysis of the stars and planets in the sample, with median uncertainties of $3\\%$, $8\\%$ and $15\\%$ for planet radii, masses and ages, doubling the number of known planets orbiting subgiant stars with bulk densities measured to better than $10\\%$. We observe a dearth of giant planets around evolved stars with short orbital periods, consistent with tidal dissipation theories that predict the rapid inspiral of planets as their host stars leave the main sequence. We note the possible evidence for two distinct classes of hot Jupiter populations, indicating multiple formation channels to explain the observed distributions around evolved stars. Finally, continued RV monitoring of planets in this sample will provide a more comprehensive understanding of demographics for evolved planetary systems.","sentences":["We present a dedicated transit and radial velocity survey of planets orbiting subgiant stars observed by the TESS Mission.","Using $\\sim$$16$ nights on Keck/HIRES, we confirm and characterize $12$ new transiting planets -- $\\rm TOI-329\\,b$, $\\rm HD\\,39688\\,b$ ($\\rm TOI-480$), $\\rm TOI-603\\,b$, $\\rm TOI-1199\\,b$, $\\rm TOI-1294\\,b$, $\\rm TOI-1439\\,b$, $\\rm TOI-1605\\,b$, $\\rm TOI-1828\\,b$, $\\rm HD\\,148193\\,b$ ($\\rm TOI-1836$), $\\rm TOI-1885\\,b$, $\\rm HD\\,83342\\,b$ ($\\rm TOI-1898$), $\\rm TOI-2019\\,b$ -- and provide updated properties for 9 previously confirmed TESS subgiant systems ($\\rm TOI-197$, $\\rm TOI-954$, $\\rm TOI-1181$, $\\rm TOI-1296$, $\\rm TOI-1298$, $\\rm TOI-1601$, $\\rm TOI-1736$, $\\rm TOI-1842$, $\\rm TOI-2145$).","We also report the discovery of an outer, non-transiting planet, $\\rm TOI-1294\\,c$ ($P=160.1\\pm2.5$ days, $M_{\\mathrm{p}}=148.3^{+18.2}_{-16.4} \\,M_{\\oplus}$), and three additional stars with long-term RV trends.","We find that at least $19\\pm8\\%$ of subgiants in our sample of $21$ stars have outer companions, comparable to main-sequence stars.","We perform a homogeneous analysis of the stars and planets in the sample, with median uncertainties of $3\\%$, $8\\%$ and $15\\%$ for planet radii, masses and ages, doubling the number of known planets orbiting subgiant stars with bulk densities measured to better than $10\\%$. We observe a dearth of giant planets around evolved stars with short orbital periods, consistent with tidal dissipation theories that predict the rapid inspiral of planets as their host stars leave the main sequence.","We note the possible evidence for two distinct classes of hot Jupiter populations, indicating multiple formation channels to explain the observed distributions around evolved stars.","Finally, continued RV monitoring of planets in this sample will provide a more comprehensive understanding of demographics for evolved planetary systems."],"url":"http://arxiv.org/abs/2402.07893v1","category":"astro-ph.EP"}
{"created":"2024-02-12 18:50:10","title":"Stellar flares","abstract":"Magnetic storms on stars manifest as remarkable, randomly occurring changes of the luminosity over durations that are tiny in comparison to the normal evolution of stars. These stellar flares are bursts of electromagnetic radiation from X-ray to radio wavelengths, and they occur on most stars with outer convection zones. They are analogous to the events on the Sun known as solar flares, which impact our everyday life and modern technological society. Stellar flares, however, can attain much greater energies than those on the Sun. Despite this, we think that these phenomena are rather similar in origin to solar flares, which result from a catastrophic conversion of latent magnetic field energy into atmospheric heating within a region that is relatively small in comparison to normal stellar sizes. We review the last several decades of stellar flare research. We summarize multi-wavelength observational results and the associated thermal and nonthermal processes in flaring stellar atmospheres. Static and hydrodynamic models are reviewed with an emphasis on recent progress in radiation-hydrodynamics and the physical diagnostics in flare spectra. Thanks to their effects on the space weather of exoplanetary systems (and thus in our search for life elsewhere in the universe) and their preponderance in \\emph{Kepler} mission data, white-light stellar flares have re-emerged in the last decade as a widely-impactful area of study within astrophysics. Yet, there is still much we do not understand, both empirically and theoretically, about the spectrum of flare radiation, its origin, and its time evolution. We conclude with several big-picture questions that are fundamental in our pursuit toward a greater understanding of these enigmatic stellar phonemena and, by extension, those on the Sun.","sentences":["Magnetic storms on stars manifest as remarkable, randomly occurring changes of the luminosity over durations that are tiny in comparison to the normal evolution of stars.","These stellar flares are bursts of electromagnetic radiation from X-ray to radio wavelengths, and they occur on most stars with outer convection zones.","They are analogous to the events on the Sun known as solar flares, which impact our everyday life and modern technological society.","Stellar flares, however, can attain much greater energies than those on the Sun.","Despite this, we think that these phenomena are rather similar in origin to solar flares, which result from a catastrophic conversion of latent magnetic field energy into atmospheric heating within a region that is relatively small in comparison to normal stellar sizes.","We review the last several decades of stellar flare research.","We summarize multi-wavelength observational results and the associated thermal and nonthermal processes in flaring stellar atmospheres.","Static and hydrodynamic models are reviewed with an emphasis on recent progress in radiation-hydrodynamics and the physical diagnostics in flare spectra.","Thanks to their effects on the space weather of exoplanetary systems (and thus in our search for life elsewhere in the universe) and their preponderance in \\emph{Kepler} mission data, white-light stellar flares have re-emerged in the last decade as a widely-impactful area of study within astrophysics.","Yet, there is still much we do not understand, both empirically and theoretically, about the spectrum of flare radiation, its origin, and its time evolution.","We conclude with several big-picture questions that are fundamental in our pursuit toward a greater understanding of these enigmatic stellar phonemena and, by extension, those on the Sun."],"url":"http://arxiv.org/abs/2402.07885v1","category":"astro-ph.SR"}
{"created":"2024-02-12 18:48:50","title":"Equivalence of cost concentration and gradient vanishing for quantum circuits: an elementary proof in the Riemannian formulation","abstract":"The optimization of quantum circuits can be hampered by a decay of average gradient amplitudes with the system size. When the decay is exponential, this is called the barren plateau problem. Considering explicit circuit parametrizations (in terms of rotation angles), it has been shown in Arrasmith et al., Quantum Sci. Technol. 7, 045015 (2022) that barren plateaus are equivalent to an exponential decay of the cost-function variance. We show that the issue becomes particularly simple in the (parametrization-free) Riemannian formulation of such optimization problems. An elementary derivation shows that the single-gate variance of the cost function is strictly equal to half the variance of the Riemannian single-gate gradient, where we sample variable gates according to the uniform Haar measure. The total variances of the cost function and its gradient are both bounded from above by the sum of single-gate variances and, conversely, bound single-gate variances from above. So, decays of gradients and cost-function variations go hand in hand, and barren plateau problems cannot be resolved by avoiding gradient-based in favor of gradient-free optimization methods.","sentences":["The optimization of quantum circuits can be hampered by a decay of average gradient amplitudes with the system size.","When the decay is exponential, this is called the barren plateau problem.","Considering explicit circuit parametrizations (in terms of rotation angles), it has been shown in Arrasmith et al., Quantum Sci. Technol. 7, 045015 (2022) that barren plateaus are equivalent to an exponential decay of the cost-function variance.","We show that the issue becomes particularly simple in the (parametrization-free) Riemannian formulation of such optimization problems.","An elementary derivation shows that the single-gate variance of the cost function is strictly equal to half the variance of the Riemannian single-gate gradient, where we sample variable gates according to the uniform Haar measure.","The total variances of the cost function and its gradient are both bounded from above by the sum of single-gate variances and, conversely, bound single-gate variances from above.","So, decays of gradients and cost-function variations go hand in hand, and barren plateau problems cannot be resolved by avoiding gradient-based in favor of gradient-free optimization methods."],"url":"http://arxiv.org/abs/2402.07883v1","category":"quant-ph"}
{"created":"2024-02-12 18:33:47","title":"PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs","abstract":"Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data?   In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.","sentences":["Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding.","This opens the door to richer interaction with the world, for example robotic control.","However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories.","How can we enable VLMs to handle such settings without fine-tuning on task-specific data?   ","In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering.","In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories).","The VLM then selects the best ones for the task.","These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer.","We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization.","We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities.","Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains.","Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo."],"url":"http://arxiv.org/abs/2402.07872v1","category":"cs.RO"}
{"created":"2024-02-12 18:31:34","title":"Jet Suppression and Azimuthal Anisotropy from RHIC to LHC","abstract":"Azimuthal anisotropies of high-$p_T$ particles produced in heavy-ion collisions are understood as an effect of a geometrical selection bias. Particles oriented in the direction in which the QCD medium formed in these collisions is shorter, suffer less energy loss, and thus, are over-represented in the final ensemble compared to those oriented in the direction in which the medium is longer. In this work we present the first semi-analytical predictions, including propagation through a realistic, hydrodynamical background, of the azimuthal anisotropies for jets, obtaining a quantitative agreement with available experimental data as function of the jet $p_T$, its cone size $R$ and the collisions centrality. Jets are multi-partonic, extended objects and their energy loss is sensitive to substructure fluctuations. This is determined by the physics of color coherence that relates to the ability of the medium to resolve those partonic fluctuations. Namely, color dipoles whose angle is smaller than a critical angle, $\\theta_c$, are not resolved by the medium and they effectively act as a coherent source of energy loss. We find that jet azimuthal anisotropies have a specially strong dependence on coherence physics due to the marked length-dependence of $\\theta_c$. By combining our predictions for the collision systems and center of mass energies studied at RHIC and the LHC, covering a wide range of typical values of $\\theta_c$, we show that the relative size of jet azimuthal anisotropies for jets with different cone-sizes $R$ follow a universal trend that indicates a transition from a coherent regime of jet quenching to a decoherent regime. These results suggest a way forward to reveal the role played by the physics of jet color decoherence in probing deconfined QCD matter.","sentences":["Azimuthal anisotropies of high-$p_T$ particles produced in heavy-ion collisions are understood as an effect of a geometrical selection bias.","Particles oriented in the direction in which the QCD medium formed in these collisions is shorter, suffer less energy loss, and thus, are over-represented in the final ensemble compared to those oriented in the direction in which the medium is longer.","In this work we present the first semi-analytical predictions, including propagation through a realistic, hydrodynamical background, of the azimuthal anisotropies for jets, obtaining a quantitative agreement with available experimental data as function of the jet $p_T$, its cone size $R$ and the collisions centrality.","Jets are multi-partonic, extended objects and their energy loss is sensitive to substructure fluctuations.","This is determined by the physics of color coherence that relates to the ability of the medium to resolve those partonic fluctuations.","Namely, color dipoles whose angle is smaller than a critical angle, $\\theta_c$, are not resolved by the medium and they effectively act as a coherent source of energy loss.","We find that jet azimuthal anisotropies have a specially strong dependence on coherence physics due to the marked length-dependence of $\\theta_c$. By combining our predictions for the collision systems and center of mass energies studied at RHIC and the LHC, covering a wide range of typical values of $\\theta_c$, we show that the relative size of jet azimuthal anisotropies for jets with different cone-sizes $R$ follow a universal trend that indicates a transition from a coherent regime of jet quenching to a decoherent regime.","These results suggest a way forward to reveal the role played by the physics of jet color decoherence in probing deconfined QCD matter."],"url":"http://arxiv.org/abs/2402.07869v1","category":"hep-ph"}
{"created":"2024-02-12 18:29:17","title":"Nesting Particle Filters for Experimental Design in Dynamical Systems","abstract":"In this paper, we propose a novel approach to Bayesian Experimental Design (BED) for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC^2 algorithm that uses a nested sequential Monte Carlo (SMC) estimator of the expected information gain and embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform gradient-based policy optimization. This is in contrast to recent approaches that rely on biased estimators of the expected information gain (EIG) to amortize the cost of experiments by learning a design policy in advance. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies.","sentences":["In this paper, we propose a novel approach to Bayesian Experimental Design (BED) for non-exchangeable data that formulates it as risk-sensitive policy optimization.","We develop the Inside-Out SMC^2 algorithm that uses a nested sequential Monte Carlo (SMC) estimator of the expected information gain and embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform gradient-based policy optimization.","This is in contrast to recent approaches that rely on biased estimators of the expected information gain (EIG) to amortize the cost of experiments by learning a design policy in advance.","Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies."],"url":"http://arxiv.org/abs/2402.07868v1","category":"cs.LG"}
{"created":"2024-02-12 18:23:11","title":"Virtual Channel Purification","abstract":"Quantum error mitigation is a key approach for extracting target state properties on state-of-the-art noisy machines and early fault-tolerant devices. Using the ideas from flag fault tolerance and virtual state purification, we develop the virtual channel purification (VCP) protocol, which consumes similar qubit and gate resources as virtual state purification but offers up to exponentially stronger error suppression with increased system size and more noisy operation copies. Furthermore, VCP removes most of the assumptions required in virtual state purification. Essentially, VCP is the first quantum error mitigation protocol that does not require specific knowledge about the noise models, the target quantum state, and the target problem while still offering rigorous performance guarantees for practical noise regimes. Further connections are made between VCP and quantum error correction to produce one of the first protocols that combine quantum error correction and quantum error mitigation beyond concatenation. We can remove all noise in the channel while paying only the same sampling cost as low-order purification, reaching beyond the standard bias-variance trade-off in quantum error mitigation. Our protocol can also be adapted to key tasks in quantum networks like channel capacity activation and entanglement distribution.","sentences":["Quantum error mitigation is a key approach for extracting target state properties on state-of-the-art noisy machines and early fault-tolerant devices.","Using the ideas from flag fault tolerance and virtual state purification, we develop the virtual channel purification (VCP) protocol, which consumes similar qubit and gate resources as virtual state purification but offers up to exponentially stronger error suppression with increased system size and more noisy operation copies.","Furthermore, VCP removes most of the assumptions required in virtual state purification.","Essentially, VCP is the first quantum error mitigation protocol that does not require specific knowledge about the noise models, the target quantum state, and the target problem while still offering rigorous performance guarantees for practical noise regimes.","Further connections are made between VCP and quantum error correction to produce one of the first protocols that combine quantum error correction and quantum error mitigation beyond concatenation.","We can remove all noise in the channel while paying only the same sampling cost as low-order purification, reaching beyond the standard bias-variance trade-off in quantum error mitigation.","Our protocol can also be adapted to key tasks in quantum networks like channel capacity activation and entanglement distribution."],"url":"http://arxiv.org/abs/2402.07866v1","category":"quant-ph"}
{"created":"2024-02-12 17:57:13","title":"Quarkonia dissociation at finite magnetic field in the presence of momentum anisotropy","abstract":"In this study, we investigate the potential of heavy quarkonia within a magnetized hot QGP medium having finite momentum anisotropy. The phenomenon of inverse magnetic catalysis is introduced into the system, influencing the magnetic field-modified Debye mass and thereby altering the effective quark masses. Concurrently, the impact of momentum anisotropy in the medium is considered that influence the particle distribution in the medium. The thermal decay width and dissociation temperature of quarkonium states, specifically the 1S and 2S states of charmonium and bottomonium, are computed. Our results reveal that both momentum anisotropy and the inverse magnetic catalysis effects play a significant role in modifying the thermal decay width and dissociation temperature of these heavy quarkonia states.","sentences":["In this study, we investigate the potential of heavy quarkonia within a magnetized hot QGP medium having finite momentum anisotropy.","The phenomenon of inverse magnetic catalysis is introduced into the system, influencing the magnetic field-modified Debye mass and thereby altering the effective quark masses.","Concurrently, the impact of momentum anisotropy in the medium is considered that influence the particle distribution in the medium.","The thermal decay width and dissociation temperature of quarkonium states, specifically the 1S and 2S states of charmonium and bottomonium, are computed.","Our results reveal that both momentum anisotropy and the inverse magnetic catalysis effects play a significant role in modifying the thermal decay width and dissociation temperature of these heavy quarkonia states."],"url":"http://arxiv.org/abs/2402.07848v1","category":"hep-ph"}
{"created":"2024-02-12 17:52:47","title":"Exact lower bound of the uncertainty principle product for the harmonic oscillator with position-momentum coupling","abstract":"We show that the uncertainty principle product for the position and momentum operators for a system described by the Hamiltonian $ \\hat H= \\frac{\\hat{p}^2}{2m} +\\frac{1}{2} m \\omega^2 \\hat{x}^2+\\frac{\\mu}{2}(\\hat x \\hat p+ \\hat p \\hat x)$ where $\\mu<\\omega$ reads $\\Delta x \\Delta p\\ge\\frac{\\hbar \\omega}{2\\sqrt{\\omega^2-\\mu^2}}$. All the values bellow this lower bound are thus quantum-mechanically forbidden. We construct the annihilation and creation operators for this system and we calculate the expectation values of the operators $\\hat p$ and $\\hat x$ with respect to the corresponding coherent states.","sentences":["We show that the uncertainty principle product for the position and momentum operators for a system described by the Hamiltonian $ \\hat H= \\frac{\\hat{p}^2}{2m} +\\frac{1}{2} m \\omega^2 \\hat{x}^2+\\frac{\\mu}{2}(\\hat x \\hat p+ \\hat p \\hat x)$ where $\\mu<\\omega$ reads $\\Delta x \\Delta p\\ge\\frac{\\hbar \\omega}{2\\sqrt{\\omega^2-\\mu^2}}$. All the values bellow this lower bound are thus quantum-mechanically forbidden.","We construct the annihilation and creation operators for this system and we calculate the expectation values of the operators $\\hat p$ and $\\hat x$ with respect to the corresponding coherent states."],"url":"http://arxiv.org/abs/2402.07842v1","category":"quant-ph"}
{"created":"2024-02-12 17:43:02","title":"Best Practices for Facing the Security Challenges of Internet of Things Devices Focusing on Software Development Life Cycle","abstract":"In the past few years, the number of IoT devices has grown substantially, and this trend is likely to continue. An increasing amount of effort is being put into developing software for the ever-increasing IoT devices. Every IoT system at its core has software that enables the devices to function efficiently. But security has always been a concern in this age of information and technology. Security for IoT devices is now a top priority due to the growing number of threats. This study introduces best practices for ensuring security in the IoT, with an emphasis on guidelines to be utilized in software development for IoT devices. The objective of the study is to raise awareness of potential threats, emphasizing the secure software development lifecycle. The study will also serve as a point of reference for future developments and provide a solid foundation for securing IoT software and dealing with vulnerabilities.","sentences":["In the past few years, the number of IoT devices has grown substantially, and this trend is likely to continue.","An increasing amount of effort is being put into developing software for the ever-increasing IoT devices.","Every IoT system at its core has software that enables the devices to function efficiently.","But security has always been a concern in this age of information and technology.","Security for IoT devices is now a top priority due to the growing number of threats.","This study introduces best practices for ensuring security in the IoT, with an emphasis on guidelines to be utilized in software development for IoT devices.","The objective of the study is to raise awareness of potential threats, emphasizing the secure software development lifecycle.","The study will also serve as a point of reference for future developments and provide a solid foundation for securing IoT software and dealing with vulnerabilities."],"url":"http://arxiv.org/abs/2402.07832v1","category":"cs.SE"}
{"created":"2024-02-12 17:41:08","title":"Self-heating effects and switching dynamics in graphene multiterminal Josephson junctions","abstract":"We experimentally investigate the electronic transport properties of a three-terminal graphene Josephson junction. We find that self-heating effects strongly influence the behaviour of this multiterminal Josephson junction (MTJJ) system. We show that existing simulation methods based on resistively and capacitively shunted Josephson junction networks can be significantly improved by taking into account these heating effects. We also investigate the phase dynamics in our MTJJ by measuring its switching current distribution and find correlated switching events in different junctions. We show that the switching dynamics is governed by phase diffusion at low temperatures. Furthermore, we find that self-heating introduces additional damping which results in overdamped I-V characteristics when normal and supercurrents coexist in the device.","sentences":["We experimentally investigate the electronic transport properties of a three-terminal graphene Josephson junction.","We find that self-heating effects strongly influence the behaviour of this multiterminal Josephson junction (MTJJ) system.","We show that existing simulation methods based on resistively and capacitively shunted Josephson junction networks can be significantly improved by taking into account these heating effects.","We also investigate the phase dynamics in our MTJJ by measuring its switching current distribution and find correlated switching events in different junctions.","We show that the switching dynamics is governed by phase diffusion at low temperatures.","Furthermore, we find that self-heating introduces additional damping which results in overdamped I-V characteristics when normal and supercurrents coexist in the device."],"url":"http://arxiv.org/abs/2402.07831v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-12 17:25:23","title":"On Computationally Efficient Multi-Class Calibration","abstract":"Consider a multi-class labelling problem, where the labels can take values in $[k]$, and a predictor predicts a distribution over the labels. In this work, we study the following foundational question: Are there notions of multi-class calibration that give strong guarantees of meaningful predictions and can be achieved in time and sample complexities polynomial in $k$? Prior notions of calibration exhibit a tradeoff between computational efficiency and expressivity: they either suffer from having sample complexity exponential in $k$, or needing to solve computationally intractable problems, or give rather weak guarantees.   Our main contribution is a notion of calibration that achieves all these desiderata: we formulate a robust notion of projected smooth calibration for multi-class predictions, and give new recalibration algorithms for efficiently calibrating predictors under this definition with complexity polynomial in $k$. Projected smooth calibration gives strong guarantees for all downstream decision makers who want to use the predictor for binary classification problems of the form: does the label belong to a subset $T \\subseteq [k]$: e.g. is this an image of an animal? It ensures that the probabilities predicted by summing the probabilities assigned to labels in $T$ are close to some perfectly calibrated binary predictor for that task. We also show that natural strengthenings of our definition are computationally hard to achieve: they run into information theoretic barriers or computational intractability. Underlying both our upper and lower bounds is a tight connection that we prove between multi-class calibration and the well-studied problem of agnostic learning in the (standard) binary prediction setting.","sentences":["Consider a multi-class labelling problem, where the labels can take values in $[k]$, and a predictor predicts a distribution over the labels.","In this work, we study the following foundational question: Are there notions of multi-class calibration that give strong guarantees of meaningful predictions and can be achieved in time and sample complexities polynomial in $k$?","Prior notions of calibration exhibit a tradeoff between computational efficiency and expressivity: they either suffer from having sample complexity exponential in $k$, or needing to solve computationally intractable problems, or give rather weak guarantees.   ","Our main contribution is a notion of calibration that achieves all these desiderata: we formulate a robust notion of projected smooth calibration for multi-class predictions, and give new recalibration algorithms for efficiently calibrating predictors under this definition with complexity polynomial in $k$. Projected smooth calibration gives strong guarantees for all downstream decision makers who want to use the predictor for binary classification problems of the form: does the label belong to a subset $T \\subseteq","[k]$: e.g. is this an image of an animal?","It ensures that the probabilities predicted by summing the probabilities assigned to labels in $T$ are close to some perfectly calibrated binary predictor for that task.","We also show that natural strengthenings of our definition are computationally hard to achieve: they run into information theoretic barriers or computational intractability.","Underlying both our upper and lower bounds is a tight connection that we prove between multi-class calibration and the well-studied problem of agnostic learning in the (standard) binary prediction setting."],"url":"http://arxiv.org/abs/2402.07821v1","category":"cs.LG"}
{"created":"2024-02-12 17:24:44","title":"First-Order Phase Transition in Perovskites Pr$_{0.67}$Sr$_{0.33}$MnO$_{3}$ - Magneto-Caloric Properties -- Effect of Multi-Spin Interaction","abstract":"We show by extensive Monte Carlo simulations that we need a multi-spin interaction in addition to pairwise interactions in order to reproduce the temperature dependence of the experimental magnetization observed in the perovskite compound Pr$_{0.67}$Sr$_{0.33}$MnO$_{3}$. The multi-spin interaction is introduced in the Hamiltonian as follows: each spin interacts simultaneously with its four nearest-neighbors. It does not have the reversal invariance as in a pairwise interaction where reversing the directions of two spins leaves the interaction energy invariant. As a consequence, it competes with the pairwise interactions between magnetic ions. The multi-spin interaction allows the sample magnetization $M$ to increase, to decrease or to have a plateau with increasing $T$. In this paper we show that $M$ increases with increasing $T$ before making a vertical fall at the transition temperature $T_C$, in contrast to the usual decrease of $M$ with increasing $T$ in most of magnetic systems. This result is in an excellent agreement with the experimental data observed in Pr$_{0.67}$Sr$_{0.33}$MnO$_{3}$. Furthermore, we show by the energy histogram taken at $T_C$ that the transition is clearly of first order. We also calculate the magnetic entropy change $|\\Delta S_m|$ and the Relative Cooling Power (RCP) by using the set of curves of $M$ obtained under an applied magnetic field $H$ varying from 0 to 5 Tesla across the transition temperature region. We obtain a good agreement with experiments on $|\\Delta S_m|$ and the values of RCP. This perovskite compound has a good potential in refrigeration application due to its high RCP.","sentences":["We show by extensive Monte Carlo simulations that we need a multi-spin interaction in addition to pairwise interactions in order to reproduce the temperature dependence of the experimental magnetization observed in the perovskite compound Pr$_{0.67}$Sr$_{0.33}$MnO$_{3}$.","The multi-spin interaction is introduced in the Hamiltonian as follows: each spin interacts simultaneously with its four nearest-neighbors.","It does not have the reversal invariance as in a pairwise interaction where reversing the directions of two spins leaves the interaction energy invariant.","As a consequence, it competes with the pairwise interactions between magnetic ions.","The multi-spin interaction allows the sample magnetization $M$ to increase, to decrease or to have a plateau with increasing $T$. In this paper we show that $M$ increases with increasing $T$ before making a vertical fall at the transition temperature $T_C$, in contrast to the usual decrease of $M$ with increasing $T$ in most of magnetic systems.","This result is in an excellent agreement with the experimental data observed in Pr$_{0.67}$Sr$_{0.33}$MnO$_{3}$.","Furthermore, we show by the energy histogram taken at $T_C$ that the transition is clearly of first order.","We also calculate the magnetic entropy change $|\\Delta S_m|$ and the Relative Cooling Power (RCP) by using the set of curves of $M$ obtained under an applied magnetic field $H$ varying from 0 to 5 Tesla across the transition temperature region.","We obtain a good agreement with experiments on $|\\Delta S_m|$ and the values of RCP.","This perovskite compound has a good potential in refrigeration application due to its high RCP."],"url":"http://arxiv.org/abs/2402.07820v1","category":"cond-mat.str-el"}
{"created":"2024-02-12 17:18:01","title":"Anomalous scaling in shell model turbulence","abstract":"Shell model turbulence is a simplified mathematical framework that captures essential features of incompressible fluid turbulence such as the energy cascade, intermittency and anomalous scaling of the fluid observables. We perform a precision analysis of shell model of a complex velocity field in the steady state turbulent regime, including a calculation of the leading hundred anomalous scaling exponents, the probability distribution function of the magnitude and phase of the velocity and the correlations among them at different shells. We analyze the tail of velocity distribution function and find that the high moments exhibit a linear scaling that differs from Kolomogorov's. We explain the origin of this asymptotic scaling that offers a new insight to the structure of fluid turbulence.","sentences":["Shell model turbulence is a simplified mathematical framework that captures essential features of incompressible fluid turbulence such as the energy cascade, intermittency and anomalous scaling of the fluid observables.","We perform a precision analysis of shell model of a complex velocity field in the steady state turbulent regime, including a calculation of the leading hundred anomalous scaling exponents, the probability distribution function of the magnitude and phase of the velocity and the correlations among them at different shells.","We analyze the tail of velocity distribution function and find that the high moments exhibit a linear scaling that differs from Kolomogorov's.","We explain the origin of this asymptotic scaling that offers a new insight to the structure of fluid turbulence."],"url":"http://arxiv.org/abs/2402.07813v1","category":"physics.flu-dyn"}
{"created":"2024-02-12 17:06:01","title":"Flux qubit-based detector of microwave photons","abstract":"A theory of detection of microwave photons with a flux qubit-based detector is presented. We consider semiclassical approximation with the electromagnetic field being in a coherent state. Flux qubit is considered as a multilevel quantum system (qudit). By solving the Lindblad equation, we describe the time evolution of occupations of the qudit's levels for readout and reset stages of detection. When considering the reset stage, the time evolution is described by multiple avoided-level crossings, thus providing a multilevel Landau-Zener-Stuckelberg-Majorana (LZSM) problem. In addition to numerical calculations, we present an approximate analytical solution for the description of the reset stage dynamics based on the adiabatic-impulse approximation and rate equation approach. Our theory may be useful for the theoretical description of driven-dissipative dynamics of qudits, including applications such as single-photon detection.","sentences":["A theory of detection of microwave photons with a flux qubit-based detector is presented.","We consider semiclassical approximation with the electromagnetic field being in a coherent state.","Flux qubit is considered as a multilevel quantum system (qudit).","By solving the Lindblad equation, we describe the time evolution of occupations of the qudit's levels for readout and reset stages of detection.","When considering the reset stage, the time evolution is described by multiple avoided-level crossings, thus providing a multilevel Landau-Zener-Stuckelberg-Majorana (LZSM) problem.","In addition to numerical calculations, we present an approximate analytical solution for the description of the reset stage dynamics based on the adiabatic-impulse approximation and rate equation approach.","Our theory may be useful for the theoretical description of driven-dissipative dynamics of qudits, including applications such as single-photon detection."],"url":"http://arxiv.org/abs/2402.07801v1","category":"quant-ph"}
{"created":"2024-02-12 17:04:37","title":"TIaRA TESS 1: Estimating exoplanet yields from Year 1 and Year 3 SPOC lightcurves","abstract":"We present a study of the detection efficiency for the TESS mission, focusing on the yield of longer-period transiting exoplanets ($P > 25$ days). We created the Transit Investigation and Recoverability Application (TIaRA) pipeline to use real TESS data with injected transits to create sensitivity maps which we combine with occurrence rates derived from Kepler. This allows us to predict longer-period exoplanet yields, which will help design follow-up photometric and spectroscopic programs, such as the NGTS Monotransit Program. For the TESS Year 1 and Year 3 SPOC FFI lightucurves, we find $2271^{+241}_{-138}$ exoplanets should be detectable around AFGKM dwarf host stars. We find $215^{+37}_{-23}$ exoplanets should be detected from single-transit events or \"monotransits\". An additional $113^{+22}_{-13}$ detections should result from \"biennial duotransit\" events with one transit in Year 1 and a second in Year 3. We also find that K dwarf stars yield the most detections by TESS per star observed. When comparing our results to the TOI catalogue we find our predictions agree within $1\\sigma$ of the number of discovered systems with periods between 0.78 and 6.25\\,days and agree to $2\\sigma$ for periods between 6.25 and 25\\,days. Beyond periods of 25 days we predict $403^{+64}_{-38}$ detections, which is 3 times as many detections as there are in the TOI catalogue with $>3\\sigma$ confidence. This indicates a significant number of long-period planets yet to be discovered from \\tess\\ data as monotransits or biennial duotransits.","sentences":["We present a study of the detection efficiency for the TESS mission, focusing on the yield of longer-period transiting exoplanets ($P > 25$ days).","We created the Transit Investigation and Recoverability Application (TIaRA) pipeline to use real TESS data with injected transits to create sensitivity maps which we combine with occurrence rates derived from Kepler.","This allows us to predict longer-period exoplanet yields, which will help design follow-up photometric and spectroscopic programs, such as the NGTS Monotransit Program.","For the TESS Year 1 and Year 3 SPOC FFI lightucurves, we find $2271^{+241}_{-138}$ exoplanets should be detectable around AFGKM dwarf host stars.","We find $215^{+37}_{-23}$ exoplanets should be detected from single-transit events or \"monotransits\".","An additional $113^{+22}_{-13}$ detections should result from \"biennial duotransit\" events with one transit in Year 1 and a second in Year 3.","We also find that K dwarf stars yield the most detections by TESS per star observed.","When comparing our results to the TOI catalogue we find our predictions agree within $1\\sigma$ of the number of discovered systems with periods between 0.78 and 6.25\\,days and agree to $2\\sigma$ for periods between 6.25 and 25\\,days.","Beyond periods of 25 days we predict $403^{+64}_{-38}$ detections, which is 3 times as many detections as there are in the TOI catalogue with $>3\\sigma$ confidence.","This indicates a significant number of long-period planets yet to be discovered from \\tess\\ data as monotransits or biennial duotransits."],"url":"http://arxiv.org/abs/2402.07800v1","category":"astro-ph.EP"}
{"created":"2024-02-12 16:54:55","title":"Instability of periodic waves for the Korteweg-de Vries-Burgers equation with monostable source","abstract":"In this paper, it is proved that the KdV-Burgers equation with a monostable source term of Fisher-KPP type has small-amplitude periodic traveling wave solutions with finite fundamental period. These solutions emerge from a supercritical local Hopf bifurcation around a critical value of the wave speed. Moreover, it is shown that these periodic waves are spectrally unstable as solutions to the PDE, that is, the Floquet (continuous) spectrum of the linearization around each periodic wave intersects the unstable half plane of complex values with positive real part. To that end, classical perturbation theory for linear operators is applied in order to prove that the spectrum of the linearized operator around the wave can be approximated by that of a constant coefficient operator around the zero solution, which intersects the unstable complex half plane.","sentences":["In this paper, it is proved that the KdV-Burgers equation with a monostable source term of Fisher-KPP type has small-amplitude periodic traveling wave solutions with finite fundamental period.","These solutions emerge from a supercritical local Hopf bifurcation around a critical value of the wave speed.","Moreover, it is shown that these periodic waves are spectrally unstable as solutions to the PDE, that is, the Floquet (continuous) spectrum of the linearization around each periodic wave intersects the unstable half plane of complex values with positive real part.","To that end, classical perturbation theory for linear operators is applied in order to prove that the spectrum of the linearized operator around the wave can be approximated by that of a constant coefficient operator around the zero solution, which intersects the unstable complex half plane."],"url":"http://arxiv.org/abs/2402.07789v1","category":"math.AP"}
{"created":"2024-02-12 16:54:22","title":"Multi-Intent Attribute-Aware Text Matching in Searching","abstract":"Text matching systems have become a fundamental service in most searching platforms. For instance, they are responsible for matching user queries to relevant candidate items, or rewriting the user-input query to a pre-selected high-performing one for a better search experience. In practice, both the queries and items often contain multiple attributes, such as the category of the item and the location mentioned in the query, which represent condensed key information that is helpful for matching. However, most of the existing works downplay the effectiveness of attributes by integrating them into text representations as supplementary information. Hence, in this work, we focus on exploring the relationship between the attributes from two sides. Since attributes from two ends are often not aligned in terms of number and type, we propose to exploit the benefit of attributes by multiple-intent modeling. The intents extracted from attributes summarize the diverse needs of queries and provide rich content of items, which are more refined and abstract, and can be aligned for paired inputs. Concretely, we propose a multi-intent attribute-aware matching model (MIM), which consists of three main components: attribute-aware encoder, multi-intent modeling, and intent-aware matching. In the attribute-aware encoder, the text and attributes are weighted and processed through a scaled attention mechanism with regard to the attributes' importance. Afterward, the multi-intent modeling extracts intents from two ends and aligns them. Herein, we come up with a distribution loss to ensure the learned intents are diverse but concentrated, and a kullback-leibler divergence loss that aligns the learned intents. Finally, in the intent-aware matching, the intents are evaluated by a self-supervised masking task, and then incorporated to output the final matching result.","sentences":["Text matching systems have become a fundamental service in most searching platforms.","For instance, they are responsible for matching user queries to relevant candidate items, or rewriting the user-input query to a pre-selected high-performing one for a better search experience.","In practice, both the queries and items often contain multiple attributes, such as the category of the item and the location mentioned in the query, which represent condensed key information that is helpful for matching.","However, most of the existing works downplay the effectiveness of attributes by integrating them into text representations as supplementary information.","Hence, in this work, we focus on exploring the relationship between the attributes from two sides.","Since attributes from two ends are often not aligned in terms of number and type, we propose to exploit the benefit of attributes by multiple-intent modeling.","The intents extracted from attributes summarize the diverse needs of queries and provide rich content of items, which are more refined and abstract, and can be aligned for paired inputs.","Concretely, we propose a multi-intent attribute-aware matching model (MIM), which consists of three main components: attribute-aware encoder, multi-intent modeling, and intent-aware matching.","In the attribute-aware encoder, the text and attributes are weighted and processed through a scaled attention mechanism with regard to the attributes' importance.","Afterward, the multi-intent modeling extracts intents from two ends and aligns them.","Herein, we come up with a distribution loss to ensure the learned intents are diverse but concentrated, and a kullback-leibler divergence loss that aligns the learned intents.","Finally, in the intent-aware matching, the intents are evaluated by a self-supervised masking task, and then incorporated to output the final matching result."],"url":"http://arxiv.org/abs/2402.07788v1","category":"cs.CL"}
{"created":"2024-02-12 16:49:32","title":"Comparison of stable spin textures in in-plane vs. out-of-plane magnetized exchange-biased multilayers","abstract":"This paper delves into the origins and specificity of the unique stable spin textures (360{\\deg} closed loop domain walls and skyrmions) observed in exchange-biased systems, with either in-plane or out-of-plane magnetic anisotropy. In the case of skyrmions, which are nanometer-sized bubbles consisting of closed-loop 180{\\deg} walls in perpendicularly-magnetized media, the stability of these spin textures arises from the existence of Dzyaloshinskii-Moriya Interactions (DMI). These interactions induce chirality of the domain walls, yielding to some extent a so-called topological protection. More complex structures such as skyrmoniums have been observed, consisting of closed loop 360{\\deg} walls. Recently, skyrmions formed in the absence of an applied external magnetic field have been stabilized in exchange biased out-of-plane magnetized systems. About two decades ago, another type of stable spin-textures were observed in exchange biased systems, with in-plane magnetization, in particular in the pinned reference layer of spin-valves. These textures consist of 360{\\deg}-domain-wall rings, the stability of which arises from the easy-plane shape anisotropy of these layers. In this paper, we compare these spin-textures and highlight the similarities and differences in their formation, structure and origin of their stability.","sentences":["This paper delves into the origins and specificity of the unique stable spin textures (360{\\deg} closed loop domain walls and skyrmions) observed in exchange-biased systems, with either in-plane or out-of-plane magnetic anisotropy.","In the case of skyrmions, which are nanometer-sized bubbles consisting of closed-loop 180{\\deg} walls in perpendicularly-magnetized media, the stability of these spin textures arises from the existence of Dzyaloshinskii-Moriya Interactions (DMI).","These interactions induce chirality of the domain walls, yielding to some extent a so-called topological protection.","More complex structures such as skyrmoniums have been observed, consisting of closed loop 360{\\deg} walls.","Recently, skyrmions formed in the absence of an applied external magnetic field have been stabilized in exchange biased out-of-plane magnetized systems.","About two decades ago, another type of stable spin-textures were observed in exchange biased systems, with in-plane magnetization, in particular in the pinned reference layer of spin-valves.","These textures consist of 360{\\deg}-domain-wall rings, the stability of which arises from the easy-plane shape anisotropy of these layers.","In this paper, we compare these spin-textures and highlight the similarities and differences in their formation, structure and origin of their stability."],"url":"http://arxiv.org/abs/2402.07783v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-12 16:44:09","title":"On the Asymptotic Behavior in Time of the Kinetic Energy in a Rigid Body-Liquid Problem","abstract":"We give sufficient conditions on the initial data for the decay in time of the kinetic energy, $E$, of solutions to the system of equations describing the motion of a rigid body in a Navier-Stokes liquid. More precisely, assuming the initial data ``small\" in appropriate norm, we show that if, in addition, the initial velocity field of the liquid, $v_0$, is in $L^q$, $q\\in(1,2)$, then $E(t)$ vanishes as $t\\to\\infty$ with a specific order of decay. The order remains, however, unspecified if $v_0\\in L^2$.","sentences":["We give sufficient conditions on the initial data for the decay in time of the kinetic energy, $E$, of solutions to the system of equations describing the motion of a rigid body in a Navier-Stokes liquid.","More precisely, assuming the initial data ``small\" in appropriate norm, we show that if, in addition, the initial velocity field of the liquid, $v_0$, is in $L^q$, $q\\in(1,2)$, then $E(t)$ vanishes as $t\\to\\infty$ with a specific order of decay.","The order remains, however, unspecified if $v_0\\in L^2$."],"url":"http://arxiv.org/abs/2402.07780v1","category":"math.AP"}
{"created":"2024-02-12 16:43:02","title":"Novel Low-Complexity Model Development for Li-ion Cells Using Online Impedance Measurement","abstract":"Modeling of Li-ion cells is used in battery management systems (BMS) to determine key states such as state-of-charge (SoC), state-of-health (SoH), etc. Accurate models are also useful in developing a cell-level digital-twin that can be used for protection and diagnostics in the BMS. In this paper, a low-complexity model development is proposed based on the equivalent circuit model (ECM) of the Li-ion cells. The proposed approach uses online impedance measurement at discrete frequencies to derive the ECM that matches closely with the results from the electro-impedance spectroscopy (EIS). The proposed method is suitable to be implemented in a microcontroller with low-computational power, typically used in BMS. Practical design guidelines are proposed to ensure fast and accurate model development. Using the proposed method to enhance the functions of a typical automotive BMS is described. Experimental validation is performed using large prismatic cells and small-capacity cylindrical cells. Root-mean-square error (RMSE) of less than 3\\% is observed for a wide variation of operating conditions.","sentences":["Modeling of Li-ion cells is used in battery management systems (BMS) to determine key states such as state-of-charge (SoC), state-of-health (SoH), etc.","Accurate models are also useful in developing a cell-level digital-twin that can be used for protection and diagnostics in the BMS.","In this paper, a low-complexity model development is proposed based on the equivalent circuit model (ECM) of the Li-ion cells.","The proposed approach uses online impedance measurement at discrete frequencies to derive the ECM that matches closely with the results from the electro-impedance spectroscopy (EIS).","The proposed method is suitable to be implemented in a microcontroller with low-computational power, typically used in BMS.","Practical design guidelines are proposed to ensure fast and accurate model development.","Using the proposed method to enhance the functions of a typical automotive BMS is described.","Experimental validation is performed using large prismatic cells and small-capacity cylindrical cells.","Root-mean-square error (RMSE) of less than 3\\% is observed for a wide variation of operating conditions."],"url":"http://arxiv.org/abs/2402.07777v1","category":"eess.SY"}
{"created":"2024-02-12 16:31:05","title":"Measuring friction from simulations of folded graphene sheets","abstract":"We run molecular dynamics simulations of folded graphene sheets and present a procedure to measure the sliding friction in these systems based on the rate of decay of a damped-harmonic oscillator. This procedure allowed us to study the affect the size, geometry and the temperature of the graphene sheet had on the ability to propagate the initial fold and the rate at which it settles to a final 'fully-folded' equilibrium state. We offer simple rationalisations for the relationships between the initial geometries of our simulations and the friction values that emerge.","sentences":["We run molecular dynamics simulations of folded graphene sheets and present a procedure to measure the sliding friction in these systems based on the rate of decay of a damped-harmonic oscillator.","This procedure allowed us to study the affect the size, geometry and the temperature of the graphene sheet had on the ability to propagate the initial fold and the rate at which it settles to a final 'fully-folded' equilibrium state.","We offer simple rationalisations for the relationships between the initial geometries of our simulations and the friction values that emerge."],"url":"http://arxiv.org/abs/2402.07768v1","category":"physics.chem-ph"}
{"created":"2024-02-12 16:29:48","title":"Designing for effective heat transfer in a solid thermal energy storage system","abstract":"Thermal energy storage using sensible heating of a solid storage medium is a potential low-cost technology for long-duration energy storage. To effectively get heat in and out of the solid material, channels of heat transfer fluid can be embedded within the storage material. Here we present design principles to improve performance of channel-embedded thermal energy storage systems, and we apply these principles to a high-temperature system using graphite as the storage material and liquid tin as the heat transfer fluid. We first analyze the impact of geometry and material properties on the performance of the system, determining the ideal channel spacing and length to achieve high (dis)charge temperature uniformity. We then analyze how controlling the fluid flowrate, heating infrastructure, and heat engine can increase discharge power uniformity and accelerate charging. Finally, we model 100 high-temperature graphite storage blocks using a porous media approximation and implement the developed design principles to demonstrate significant improvement in performance for both discharging (constant discharge power for >90% of rated duration) and charging (>90% charged within 4 hours). Overall, the hierarchical design procedure presented here enables the design of cheap yet high-performing solid thermal energy storage systems.","sentences":["Thermal energy storage using sensible heating of a solid storage medium is a potential low-cost technology for long-duration energy storage.","To effectively get heat in and out of the solid material, channels of heat transfer fluid can be embedded within the storage material.","Here we present design principles to improve performance of channel-embedded thermal energy storage systems, and we apply these principles to a high-temperature system using graphite as the storage material and liquid tin as the heat transfer fluid.","We first analyze the impact of geometry and material properties on the performance of the system, determining the ideal channel spacing and length to achieve high (dis)charge temperature uniformity.","We then analyze how controlling the fluid flowrate, heating infrastructure, and heat engine can increase discharge power uniformity and accelerate charging.","Finally, we model 100 high-temperature graphite storage blocks using a porous media approximation and implement the developed design principles to demonstrate significant improvement in performance for both discharging (constant discharge power for >90% of rated duration) and charging (>90% charged within 4 hours).","Overall, the hierarchical design procedure presented here enables the design of cheap yet high-performing solid thermal energy storage systems."],"url":"http://arxiv.org/abs/2402.07764v1","category":"physics.app-ph"}
{"created":"2024-02-12 16:28:52","title":"Scalable Structure Learning for Sparse Context-Specific Causal Systems","abstract":"Several approaches to graphically representing context-specific relations among jointly distributed categorical variables have been proposed, along with structure learning algorithms. While existing optimization-based methods have limited scalability due to the large number of context-specific models, the constraint-based methods are more prone to error than even constraint-based DAG learning algorithms since more relations must be tested. We present a hybrid algorithm for learning context-specific models that scales to hundreds of variables while testing no more constraints than standard DAG learning algorithms. Scalable learning is achieved through a combination of an order-based MCMC algorithm and sparsity assumptions analogous to those typically invoked for DAG models. To implement the method, we solve a special case of an open problem recently posed by Alon and Balogh. The method is shown to perform well on synthetic data and real world examples, in terms of both accuracy and scalability.","sentences":["Several approaches to graphically representing context-specific relations among jointly distributed categorical variables have been proposed, along with structure learning algorithms.","While existing optimization-based methods have limited scalability due to the large number of context-specific models, the constraint-based methods are more prone to error than even constraint-based DAG learning algorithms since more relations must be tested.","We present a hybrid algorithm for learning context-specific models that scales to hundreds of variables while testing no more constraints than standard DAG learning algorithms.","Scalable learning is achieved through a combination of an order-based MCMC algorithm and sparsity assumptions analogous to those typically invoked for DAG models.","To implement the method, we solve a special case of an open problem recently posed by Alon and Balogh.","The method is shown to perform well on synthetic data and real world examples, in terms of both accuracy and scalability."],"url":"http://arxiv.org/abs/2402.07762v1","category":"stat.ML"}
{"created":"2024-02-12 16:28:38","title":"The Strength and Shapes of Contact Binary Objectcts","abstract":"While contact binary objects are common in the solar system, their formation mechanism is unclear. In this work we examine several contact binaries and calculate the necessary strength parameters that allow the two lobes to merge without the smaller of the two being gravitationally destroyed by the larger. We find a small but non-zero amount of cohesion or a large friction angle is required for the smaller lobe to survive the merging process, consistent with observations. This means it is possible for two previously separated rubble piles to experience a collapse of their mutual orbit and form a contact binary. The necessary strength required to survive this merger depends on the relative size, shape, and density of the body, with prolate shapes requiring more cohesion than oblate shapes.","sentences":["While contact binary objects are common in the solar system, their formation mechanism is unclear.","In this work we examine several contact binaries and calculate the necessary strength parameters that allow the two lobes to merge without the smaller of the two being gravitationally destroyed by the larger.","We find a small but non-zero amount of cohesion or a large friction angle is required for the smaller lobe to survive the merging process, consistent with observations.","This means it is possible for two previously separated rubble piles to experience a collapse of their mutual orbit and form a contact binary.","The necessary strength required to survive this merger depends on the relative size, shape, and density of the body, with prolate shapes requiring more cohesion than oblate shapes."],"url":"http://arxiv.org/abs/2402.07760v1","category":"astro-ph.EP"}
{"created":"2024-02-12 16:28:36","title":"Robust and accurate simulations of flows over orography using non-conforming meshes","abstract":"We systematically validate the static local mesh refinement capabilities of a recently proposed IMEX-DG scheme implemented in the framework of the deal.II library. Non-conforming meshes are employed in atmospheric flow simulations to increase the resolution around complex orography. A number of numerical experiments based on classical benchmarks with idealized as well as real orography profiles demonstrate that simulations with the refined mesh are stable for long lead times and no spurious effects arise at the interfaces of mesh regions with different resolutions. Moreover, correct values of the momentum flux are retrieved and the correct large-scale orographic response is established. Hence, large-scale orography-driven flow features can be simulated without loss of accuracy using a much lower total amount of degrees of freedom. In a context of spatial resolutions approaching the hectometric scale in numerical weather prediction models, these results support the use of locally refined, non-conforming meshes as a reliable and effective tool to greatly reduce the dependence of atmospheric models on orographic wave drag parametrizations.","sentences":["We systematically validate the static local mesh refinement capabilities of a recently proposed IMEX-DG scheme implemented in the framework of the deal.","II library.","Non-conforming meshes are employed in atmospheric flow simulations to increase the resolution around complex orography.","A number of numerical experiments based on classical benchmarks with idealized as well as real orography profiles demonstrate that simulations with the refined mesh are stable for long lead times and no spurious effects arise at the interfaces of mesh regions with different resolutions.","Moreover, correct values of the momentum flux are retrieved and the correct large-scale orographic response is established.","Hence, large-scale orography-driven flow features can be simulated without loss of accuracy using a much lower total amount of degrees of freedom.","In a context of spatial resolutions approaching the hectometric scale in numerical weather prediction models, these results support the use of locally refined, non-conforming meshes as a reliable and effective tool to greatly reduce the dependence of atmospheric models on orographic wave drag parametrizations."],"url":"http://arxiv.org/abs/2402.07759v1","category":"physics.ao-ph"}
{"created":"2024-02-12 16:27:24","title":"Semistability conditions defined by ample classes","abstract":"We study a class of semistability conditions defined by a system of ample classes for coherent sheaves over a smooth projective variety. Under some necessary boundedness assumptions, we show the existence of a well-behaved chamber structure for the variation of moduli spaces of sheaves with respect to the change of semistability.","sentences":["We study a class of semistability conditions defined by a system of ample classes for coherent sheaves over a smooth projective variety.","Under some necessary boundedness assumptions, we show the existence of a well-behaved chamber structure for the variation of moduli spaces of sheaves with respect to the change of semistability."],"url":"http://arxiv.org/abs/2402.07758v1","category":"math.AG"}
{"created":"2024-02-12 16:21:50","title":"Mixed Q-Functionals: Advancing Value-Based Methods in Cooperative MARL with Continuous Action Domains","abstract":"Tackling multi-agent learning problems efficiently is a challenging task in continuous action domains. While value-based algorithms excel in sample efficiency when applied to discrete action domains, they are usually inefficient when dealing with continuous actions. Policy-based algorithms, on the other hand, attempt to address this challenge by leveraging critic networks for guiding the learning process and stabilizing the gradient estimation. The limitations in the estimation of true return and falling into local optima in these methods result in inefficient and often sub-optimal policies. In this paper, we diverge from the trend of further enhancing critic networks, and focus on improving the effectiveness of value-based methods in multi-agent continuous domains by concurrently evaluating numerous actions. We propose a novel multi-agent value-based algorithm, Mixed Q-Functionals (MQF), inspired from the idea of Q-Functionals, that enables agents to transform their states into basis functions. Our algorithm fosters collaboration among agents by mixing their action-values. We evaluate the efficacy of our algorithm in six cooperative multi-agent scenarios. Our empirical findings reveal that MQF outperforms four variants of Deep Deterministic Policy Gradient through rapid action evaluation and increased sample efficiency.","sentences":["Tackling multi-agent learning problems efficiently is a challenging task in continuous action domains.","While value-based algorithms excel in sample efficiency when applied to discrete action domains, they are usually inefficient when dealing with continuous actions.","Policy-based algorithms, on the other hand, attempt to address this challenge by leveraging critic networks for guiding the learning process and stabilizing the gradient estimation.","The limitations in the estimation of true return and falling into local optima in these methods result in inefficient and often sub-optimal policies.","In this paper, we diverge from the trend of further enhancing critic networks, and focus on improving the effectiveness of value-based methods in multi-agent continuous domains by concurrently evaluating numerous actions.","We propose a novel multi-agent value-based algorithm, Mixed Q-Functionals (MQF), inspired from the idea of Q-Functionals, that enables agents to transform their states into basis functions.","Our algorithm fosters collaboration among agents by mixing their action-values.","We evaluate the efficacy of our algorithm in six cooperative multi-agent scenarios.","Our empirical findings reveal that MQF outperforms four variants of Deep Deterministic Policy Gradient through rapid action evaluation and increased sample efficiency."],"url":"http://arxiv.org/abs/2402.07752v1","category":"cs.MA"}
{"created":"2024-02-12 16:17:52","title":"The GALAH survey: Elemental abundances in open clusters using joint effective temperature and surface gravity photometric priors","abstract":"The ability to measure precise and accurate stellar effective temperatures ($T_{\\rm{eff}}$) and surface gravities ($\\log(g)$) is essential in determining accurate and precise abundances of chemical elements in stars. Measuring $\\log(g)$ from isochrones fitted to colour-magnitude diagrams of open clusters is significantly more accurate and precise compared to spectroscopic $\\log(g)$. By determining the ranges of ages, metallicity, and extinction of isochrones that fit the colour-magnitude diagram, we constructed a joint probability distribution of $T_{\\rm{eff}}$ and $\\log(g)$. The joint photometric probability shows the complex correlations between $T_{\\rm{eff}}$ and $\\log(g)$, which depend on the evolutionary stage of the star. We show that by using this photometric prior while fitting spectra, we can acquire more precise spectroscopic stellar parameters and abundances of chemical elements. This reveals higher-order abundance trends in open clusters like traces of atomic diffusion. We used photometry and astrometry provided by the \\textit{Gaia} DR3 catalogue, Padova isochrones, and Galactic Archaeology with HERMES (GALAH) DR4 spectra. We analysed the spectra of 1979 stars in nine open clusters, using MCMC to fit the spectroscopic abundances of 26 elements, $T_{\\rm{eff}}$, $\\log(g)$, $v_{\\rm{mic}}$, and $v_{\\rm{broad}}$. We found that using photometric priors improves the accuracy of abundances and $\\log(g)$, which enables us to view higher-order trends of abundances caused by atomic diffusion in M67 and Ruprecht 147.","sentences":["The ability to measure precise and accurate stellar effective temperatures ($T_{\\rm{eff}}$) and surface gravities ($\\log(g)$) is essential in determining accurate and precise abundances of chemical elements in stars.","Measuring $\\log(g)$ from isochrones fitted to colour-magnitude diagrams of open clusters is significantly more accurate and precise compared to spectroscopic $\\log(g)$. By determining the ranges of ages, metallicity, and extinction of isochrones that fit the colour-magnitude diagram, we constructed a joint probability distribution of $T_{\\rm{eff}}$ and $\\log(g)$. The joint photometric probability shows the complex correlations between $T_{\\rm{eff}}$ and $\\log(g)$, which depend on the evolutionary stage of the star.","We show that by using this photometric prior while fitting spectra, we can acquire more precise spectroscopic stellar parameters and abundances of chemical elements.","This reveals higher-order abundance trends in open clusters like traces of atomic diffusion.","We used photometry and astrometry provided by the \\textit{Gaia} DR3 catalogue, Padova isochrones, and Galactic Archaeology with HERMES (GALAH) DR4 spectra.","We analysed the spectra of 1979 stars in nine open clusters, using MCMC to fit the spectroscopic abundances of 26 elements, $T_{\\rm{eff}}$, $\\log(g)$, $v_{\\rm{mic}}$, and $v_{\\rm{broad}}$. We found that using photometric priors improves the accuracy of abundances and $\\log(g)$, which enables us to view higher-order trends of abundances caused by atomic diffusion in M67 and Ruprecht 147."],"url":"http://arxiv.org/abs/2402.07748v1","category":"astro-ph.GA"}
{"created":"2024-02-12 15:46:17","title":"Approximate Analytical Solutions for the Circular Restricted Three-Body Problem Including Non-Hamiltonian Solar Radiation Pressure","abstract":"The circular restricted three-body problem (CR3BP) with solar radiation pressure (SRP) has often been analyzed with assumptions made on a spacecraft's attitude, such that the problem remains Hamiltonian. These assumptions are unsatisfactorily limiting for a starshade mission since the starshade's attitude will inherently vary from the configuration that corresponds to Hamiltonian dynamics. This paper presents the derivation of the equations of motion for CR3BP with SRP that permit the application of the Lindstedt-Poincare method, such that approximate solutions are produced, which may serve as invaluable trajectory design tools. Examples of periodic orbits and manifolds corresponding to three sets of attitude angles are shown and the accuracy of their seventh-order approximations is considered.","sentences":["The circular restricted three-body problem (CR3BP) with solar radiation pressure (SRP) has often been analyzed with assumptions made on a spacecraft's attitude, such that the problem remains Hamiltonian.","These assumptions are unsatisfactorily limiting for a starshade mission since the starshade's attitude will inherently vary from the configuration that corresponds to Hamiltonian dynamics.","This paper presents the derivation of the equations of motion for CR3BP with SRP that permit the application of the Lindstedt-Poincare method, such that approximate solutions are produced, which may serve as invaluable trajectory design tools.","Examples of periodic orbits and manifolds corresponding to three sets of attitude angles are shown and the accuracy of their seventh-order approximations is considered."],"url":"http://arxiv.org/abs/2402.07734v1","category":"math.DS"}
{"created":"2024-02-12 15:42:28","title":"Photoinduced Topological Phase Transitions in a Kitaev kagome magnet","abstract":"Based on the magnetic Floquet-Bloch theory, we investigate topological natures of Floquet magnons in a laser-irradiated kagome magnet exhibiting Heisenberg, Kitaev and Dzyaloshinskii Moriya interactions in the presence of an external magnetic field applied along directions of [111]. In a strong magnetic field perpendicular to the lattice plane [111] direction, the system is in the fully polarized paramagnetic phase and the magnon band carries an asymmetric Chern number throughout the region of the phase diagram. Our results show that periodically driven intrinsic topological magnetic materials can be manipulated into different topological phases with different thermal conductivities and Berry curvature signatures by adjusting the light intensity throughout the phase diagram region.","sentences":["Based on the magnetic Floquet-Bloch theory, we investigate topological natures of Floquet magnons in a laser-irradiated kagome magnet exhibiting Heisenberg, Kitaev and Dzyaloshinskii Moriya interactions in the presence of an external magnetic field applied along directions of [111].","In a strong magnetic field perpendicular to the lattice plane [111] direction, the system is in the fully polarized paramagnetic phase and the magnon band carries an asymmetric Chern number throughout the region of the phase diagram.","Our results show that periodically driven intrinsic topological magnetic materials can be manipulated into different topological phases with different thermal conductivities and Berry curvature signatures by adjusting the light intensity throughout the phase diagram region."],"url":"http://arxiv.org/abs/2402.07731v1","category":"cond-mat.str-el"}
{"created":"2024-02-12 15:39:21","title":"Distributed Observer Design over Directed Switching Topologies","abstract":"The distributed observer design problem holds significant importance in cases in which the output information of a system is decentralized across different subsystems. Each subsystem has a local observer and access to one part of the measurement outputs and information exchanged through communication networks. This paper focuses on the design of distributed observer with jointly connected directed switching networks. The problem presents challenges due to passive switching modes and the open-loop unboundedness that results from local observability. To overcome these challenges, we develop a network transformation mapping method whereby each local observer can classify itself into an independent subgraph based on independent judgment. Next, an observable decomposition and reorganization method is developed for the digraph case to ensure that each subgraph possesses independent dynamic properties. Asymptotic omniscience is then proven using a developed recursive proof method. This paper includes many previous results as special cases, because most are only suitable for undirected switching topologies or fast-switching cases. An adaptive coupling gain design is proposed to simplify the calculation and verification of conditions that guarantee asymptotic omniscience. Finally, simulation results with the power system show the validity of the developed theory.","sentences":["The distributed observer design problem holds significant importance in cases in which the output information of a system is decentralized across different subsystems.","Each subsystem has a local observer and access to one part of the measurement outputs and information exchanged through communication networks.","This paper focuses on the design of distributed observer with jointly connected directed switching networks.","The problem presents challenges due to passive switching modes and the open-loop unboundedness that results from local observability.","To overcome these challenges, we develop a network transformation mapping method whereby each local observer can classify itself into an independent subgraph based on independent judgment.","Next, an observable decomposition and reorganization method is developed for the digraph case to ensure that each subgraph possesses independent dynamic properties.","Asymptotic omniscience is then proven using a developed recursive proof method.","This paper includes many previous results as special cases, because most are only suitable for undirected switching topologies or fast-switching cases.","An adaptive coupling gain design is proposed to simplify the calculation and verification of conditions that guarantee asymptotic omniscience.","Finally, simulation results with the power system show the validity of the developed theory."],"url":"http://arxiv.org/abs/2402.07727v1","category":"math.DS"}
{"created":"2024-02-12 15:37:25","title":"A mathematical model for fibrous dysplasia: The role of the flow of mutant cells","abstract":"Fibrous dysplasia (FD) is a mosaic non-inheritable genetic disorder of the skeleton in which normal bone is replaced by structurally unsound fibro-osseous tissue. There is no curative treatment for FD, partly because its pathophysiology is not yet fully known. We present a simple mathematical model of the disease incorporating its basic known biology, to gain insight on the dynamics of the involved bone-cell populations, and shed light on its pathophysiology. Our mathematical models account for the dynamic evolution over time of several interacting populations of bone cells averaged over a volume of bone of sufficient size in order to obtain consistent results. We develop an analytical study of the model and study its basic properties. The existence and stability of steady states are studied, an analysis of sensitivity on the model parameters is done, and different numerical simulations provide findings in agreement with the analytical results. We discuss the model dynamics match with known facts on the disease, and how some open questions could be addressed using the model.","sentences":["Fibrous dysplasia (FD) is a mosaic non-inheritable genetic disorder of the skeleton in which normal bone is replaced by structurally unsound fibro-osseous tissue.","There is no curative treatment for FD, partly because its pathophysiology is not yet fully known.","We present a simple mathematical model of the disease incorporating its basic known biology, to gain insight on the dynamics of the involved bone-cell populations, and shed light on its pathophysiology.","Our mathematical models account for the dynamic evolution over time of several interacting populations of bone cells averaged over a volume of bone of sufficient size in order to obtain consistent results.","We develop an analytical study of the model and study its basic properties.","The existence and stability of steady states are studied, an analysis of sensitivity on the model parameters is done, and different numerical simulations provide findings in agreement with the analytical results.","We discuss the model dynamics match with known facts on the disease, and how some open questions could be addressed using the model."],"url":"http://arxiv.org/abs/2402.07724v1","category":"q-bio.TO"}
{"created":"2024-02-12 15:21:14","title":"Topological Edge States in Reconfigurable Multi-stable Mechanical Metamaterials","abstract":"Multi-stable mechanical structures find cutting-edge applications across various domains due to their reconfigurability, which offers innovative possibilities for engineering and technology advancements. This study explores the emergence of topological states in a one-dimensional chain-like multi-stable mechanical metamaterial composed of bistable units through a combination of mechanical and optical experiments. Drawing inspiration from the SSH (Su-Schrieffer-Heeger) model in condensed matter physics, we leverage the unique mechanical properties of the reconfigurable ligament-oscillator metamaterial to engineer a system with coexisting topological phases. Based on the one-dimensional periodic bistable chain, there is an exponential decay diffusion of elastic energy from both end boundaries towards the interior of the body. Experimental characterizations demonstrate the existence of stable topological phases within the reconfigurable multi-stable mechanical metamaterial. The findings underscore the potential of reconfigurable mechanical metamaterials as versatile platforms for flexibly exploring and manipulating topological phenomena, with applications ranging from impact resistance to energy harvesting and information processing.","sentences":["Multi-stable mechanical structures find cutting-edge applications across various domains due to their reconfigurability, which offers innovative possibilities for engineering and technology advancements.","This study explores the emergence of topological states in a one-dimensional chain-like multi-stable mechanical metamaterial composed of bistable units through a combination of mechanical and optical experiments.","Drawing inspiration from the SSH (Su-Schrieffer-Heeger) model in condensed matter physics, we leverage the unique mechanical properties of the reconfigurable ligament-oscillator metamaterial to engineer a system with coexisting topological phases.","Based on the one-dimensional periodic bistable chain, there is an exponential decay diffusion of elastic energy from both end boundaries towards the interior of the body.","Experimental characterizations demonstrate the existence of stable topological phases within the reconfigurable multi-stable mechanical metamaterial.","The findings underscore the potential of reconfigurable mechanical metamaterials as versatile platforms for flexibly exploring and manipulating topological phenomena, with applications ranging from impact resistance to energy harvesting and information processing."],"url":"http://arxiv.org/abs/2402.07707v1","category":"physics.app-ph"}
{"created":"2024-02-12 15:21:07","title":"Genuine and faux single G centers in carbon-implanted silicon","abstract":"Among the wide variety of single fluorescent defects investigated in silicon, numerous studies have focused on color centers with a zero-phonon line around $1.28 \\mu$m and identified to a common carbon-complex in silicon, namely the G center. However, inconsistent estimates regarding their quantum efficiency cast doubt on the correct identification of these individual emitters. Through a comparative analysis of their single-photon emission properties, we demonstrate that these single color centers are split in two distinct families of point defects. A first family consists of the genuine single G centers with a well-identified microscopic structure and whose photoluminescence has been investigated on ensemble measurements since the 60's. The remaining defects belong to a new color center, which we will refer to as G$^{\\star}$ center, whose atomic configuration has yet to be determined. These results provide a safeguard against future defect misidentifications, which is crucial for further development of quantum technologies relying on G or G$^{\\star}$ center quantum properties.","sentences":["Among the wide variety of single fluorescent defects investigated in silicon, numerous studies have focused on color centers with a zero-phonon line around $1.28 \\mu$m and identified to a common carbon-complex in silicon, namely the G center.","However, inconsistent estimates regarding their quantum efficiency cast doubt on the correct identification of these individual emitters.","Through a comparative analysis of their single-photon emission properties, we demonstrate that these single color centers are split in two distinct families of point defects.","A first family consists of the genuine single G centers with a well-identified microscopic structure and whose photoluminescence has been investigated on ensemble measurements since the 60's.","The remaining defects belong to a new color center, which we will refer to as G$^{\\star}$ center, whose atomic configuration has yet to be determined.","These results provide a safeguard against future defect misidentifications, which is crucial for further development of quantum technologies relying on G or G$^{\\star}$ center quantum properties."],"url":"http://arxiv.org/abs/2402.07705v1","category":"quant-ph"}
{"created":"2024-02-12 15:08:49","title":"Combination Therapy for Chronic Hepatitis B Using Capsid Recycling Inhibitor","abstract":"In this paper, we investigate the dynamics of hepatitis B virus infection taking into account the implementation of combination therapy through mathematical modeling. This model is established considering the interplay between uninfected cells, infected cells, capsids, and viruses. Three drugs are considered for specific roles (i) pegylated interferon (PEG IFN) for immune modulation, (ii) lamivudine (LMV) as a reverse-transcriptase inhibitor, and (iii) entecavir (ETV) to block capsid recycling. Using these drugs, three combination therapies are introduced, specifically CT PEG IFN plus LMV, CT PEG IFN plus ETV, and CT PEG IFN plus LMV plus ETV. As a result, when LMV is used in combination therapy with PEG IFN and ETV, the impacts of ETV become insignificant. In conclusion, if the appropriate drug effectively inhibits reverse transcription, there is no need for an additional inhibitor to block capsid recycling.","sentences":["In this paper, we investigate the dynamics of hepatitis B virus infection taking into account the implementation of combination therapy through mathematical modeling.","This model is established considering the interplay between uninfected cells, infected cells, capsids, and viruses.","Three drugs are considered for specific roles (i) pegylated interferon (PEG IFN) for immune modulation, (ii) lamivudine (LMV) as a reverse-transcriptase inhibitor, and (iii) entecavir (ETV) to block capsid recycling.","Using these drugs, three combination therapies are introduced, specifically CT PEG IFN plus LMV, CT PEG IFN plus ETV, and CT PEG IFN plus LMV plus ETV.","As a result, when LMV is used in combination therapy with PEG IFN and ETV, the impacts of ETV become insignificant.","In conclusion, if the appropriate drug effectively inhibits reverse transcription, there is no need for an additional inhibitor to block capsid recycling."],"url":"http://arxiv.org/abs/2402.07701v1","category":"q-bio.CB"}
{"created":"2024-02-12 15:00:51","title":"LFOC+: A Fair OS-level Cache-Clustering Policy for Commodity Multicore Systems","abstract":"Commodity multicore systems are increasingly adopting hardware support that enables the system software to partition the last-level cache (LLC). This support makes it possible for the operating system (OS) or the Virtual Machine Monitor (VMM) to mitigate shared-resource contention effects on multicores by assigning different co-running applications to various cache partitions. Recently cache-clustering (or partition-sharing) strategies have emerged as a way to improve system throughput and fairness on new platforms with cache-partitioning support. As opposed to strict cache-partitioning, which allocates separate cache partitions to each application, cache-clustering allows partitions to be shared by a group of applications.   In this article we propose LFOC+, a fairness-aware OS-level cache-clustering policy for commodity multicore systems. LFOC+ tries to mimic the behavior of the optimal cache-clustering solution for fairness, which we could obtain for different workload scenarios by using a simulation tool. Our dynamic cache-clustering strategy continuously gathers data from performance monitoring counters to classify applications at runtime based on the degree of cache sensitivity and contentiousness, and effectively separates cache-sensitive applications from aggressor programs to improve fairness, while providing acceptable system throughput.   We implemented LFOC+ in the Linux kernel and evaluated it on a real system featuring an Intel Skylake processor, where we compare its effectiveness to that of four previously proposed cache-clustering policies. Our experimental analysis reveals that LFOC+ constitutes a lightweight OS-level policy and improves fairness relative to two other state-of-the-art fairness-aware strategies --Dunn and LFOC--, by up to 22\\% and up to 20.6\\%, respectively, and by 9\\% and 4.9\\% on average.","sentences":["Commodity multicore systems are increasingly adopting hardware support that enables the system software to partition the last-level cache (LLC).","This support makes it possible for the operating system (OS) or the Virtual Machine Monitor (VMM) to mitigate shared-resource contention effects on multicores by assigning different co-running applications to various cache partitions.","Recently cache-clustering (or partition-sharing) strategies have emerged as a way to improve system throughput and fairness on new platforms with cache-partitioning support.","As opposed to strict cache-partitioning, which allocates separate cache partitions to each application, cache-clustering allows partitions to be shared by a group of applications.   ","In this article we propose LFOC+, a fairness-aware OS-level cache-clustering policy for commodity multicore systems.","LFOC+ tries to mimic the behavior of the optimal cache-clustering solution for fairness, which we could obtain for different workload scenarios by using a simulation tool.","Our dynamic cache-clustering strategy continuously gathers data from performance monitoring counters to classify applications at runtime based on the degree of cache sensitivity and contentiousness, and effectively separates cache-sensitive applications from aggressor programs to improve fairness, while providing acceptable system throughput.   ","We implemented LFOC+ in the Linux kernel and evaluated it on a real system featuring an Intel Skylake processor, where we compare its effectiveness to that of four previously proposed cache-clustering policies.","Our experimental analysis reveals that LFOC+ constitutes a lightweight OS-level policy and improves fairness relative to two other state-of-the-art fairness-aware strategies --Dunn and LFOC--, by up to 22\\% and up to 20.6\\%, respectively, and by 9\\% and 4.9\\% on average."],"url":"http://arxiv.org/abs/2402.07693v1","category":"cs.AR"}
{"created":"2024-02-12 14:59:40","title":"Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints","abstract":"Bayesian optimization has been successfully applied to optimize black-box functions where the number of evaluations is severely limited. However, in many real-world applications, it is hard or impossible to know in advance which designs are feasible due to some physical or system limitations. These issues lead to an even more challenging problem of optimizing an unknown function with unknown constraints. In this paper, we observe that in such scenarios optimal solution typically lies on the boundary between feasible and infeasible regions of the design space, making it considerably more difficult than that with interior optima. Inspired by this observation, we propose BE-CBO, a new Bayesian optimization method that efficiently explores the boundary between feasible and infeasible designs. To identify the boundary, we learn the constraints with an ensemble of neural networks that outperform the standard Gaussian Processes for capturing complex boundaries. Our method demonstrates superior performance against state-of-the-art methods through comprehensive experiments on synthetic and real-world benchmarks.","sentences":["Bayesian optimization has been successfully applied to optimize black-box functions where the number of evaluations is severely limited.","However, in many real-world applications, it is hard or impossible to know in advance which designs are feasible due to some physical or system limitations.","These issues lead to an even more challenging problem of optimizing an unknown function with unknown constraints.","In this paper, we observe that in such scenarios optimal solution typically lies on the boundary between feasible and infeasible regions of the design space, making it considerably more difficult than that with interior optima.","Inspired by this observation, we propose BE-CBO, a new Bayesian optimization method that efficiently explores the boundary between feasible and infeasible designs.","To identify the boundary, we learn the constraints with an ensemble of neural networks that outperform the standard Gaussian Processes for capturing complex boundaries.","Our method demonstrates superior performance against state-of-the-art methods through comprehensive experiments on synthetic and real-world benchmarks."],"url":"http://arxiv.org/abs/2402.07692v1","category":"cs.LG"}
{"created":"2024-02-12 14:57:51","title":"Evaluation of a Smart Mobile Robotic System for Industrial Plant Inspection and Supervision","abstract":"Automated and autonomous industrial inspection is a longstanding research field, driven by the necessity to enhance safety and efficiency within industrial settings. In addressing this need, we introduce an autonomously navigating robotic system designed for comprehensive plant inspection. This innovative system comprises a robotic platform equipped with a diverse array of sensors integrated to facilitate the detection of various process and infrastructure parameters. These sensors encompass optical (LiDAR, Stereo, UV/IR/RGB cameras), olfactory (electronic nose), and acoustic (microphone array) capabilities, enabling the identification of factors such as methane leaks, flow rates, and infrastructural anomalies. The proposed system underwent individual evaluation at a wastewater treatment site within a chemical plant, providing a practical and challenging environment for testing. The evaluation process encompassed key aspects such as object detection, 3D localization, and path planning. Furthermore, specific evaluations were conducted for optical methane leak detection and localization, as well as acoustic assessments focusing on pump equipment and gas leak localization.","sentences":["Automated and autonomous industrial inspection is a longstanding research field, driven by the necessity to enhance safety and efficiency within industrial settings.","In addressing this need, we introduce an autonomously navigating robotic system designed for comprehensive plant inspection.","This innovative system comprises a robotic platform equipped with a diverse array of sensors integrated to facilitate the detection of various process and infrastructure parameters.","These sensors encompass optical (LiDAR, Stereo, UV/IR/RGB cameras), olfactory (electronic nose), and acoustic (microphone array) capabilities, enabling the identification of factors such as methane leaks, flow rates, and infrastructural anomalies.","The proposed system underwent individual evaluation at a wastewater treatment site within a chemical plant, providing a practical and challenging environment for testing.","The evaluation process encompassed key aspects such as object detection, 3D localization, and path planning.","Furthermore, specific evaluations were conducted for optical methane leak detection and localization, as well as acoustic assessments focusing on pump equipment and gas leak localization."],"url":"http://arxiv.org/abs/2402.07691v1","category":"cs.RO"}
{"created":"2024-02-12 14:55:48","title":"Interplay of pseudo-Hermitian symmetries and degenerate manifolds in the eigenspectrum of non-Hermitian systems","abstract":"In this letter, we study how the spectrum of pseudo-Hermitian systems is influenced by the ambiguity in the choice of the pseudo-metric operator. In particular, we analyze the case when different parameter-independent choices of pseudo-metric are possible and how it can lead to the appearance of robust degenerate manifolds in the parameter space of the system.","sentences":["In this letter, we study how the spectrum of pseudo-Hermitian systems is influenced by the ambiguity in the choice of the pseudo-metric operator.","In particular, we analyze the case when different parameter-independent choices of pseudo-metric are possible and how it can lead to the appearance of robust degenerate manifolds in the parameter space of the system."],"url":"http://arxiv.org/abs/2402.07690v1","category":"quant-ph"}
{"created":"2024-02-12 14:53:12","title":"Privacy-Preserving Gaze Data Streaming in Immersive Interactive Virtual Reality: Robustness and User Experience","abstract":"Eye tracking is routinely being incorporated into virtual reality (VR) systems. Prior research has shown that eye tracking data can be used for re-identification attacks. The state of our knowledge about currently existing privacy mechanisms is limited to privacy-utility trade-off curves based on data-centric metrics of utility, such as prediction error, and black-box threat models. We propose that for interactive VR applications, it is essential to consider user-centric notions of utility and a variety of threat models. We develop a methodology to evaluate real-time privacy mechanisms for interactive VR applications that incorporate subjective user experience and task performance metrics. We evaluate selected privacy mechanisms using this methodology and find that re-identification accuracy can be decreased to as low as 14% while maintaining a high usability score and reasonable task performance. Finally, we elucidate three threat scenarios (black-box, black-box with exemplars, and white-box) and assess how well the different privacy mechanisms hold up to these adversarial scenarios. This work advances the state of the art in VR privacy by providing a methodology for end-to-end assessment of the risk of re-identification attacks and potential mitigating solutions.","sentences":["Eye tracking is routinely being incorporated into virtual reality (VR) systems.","Prior research has shown that eye tracking data can be used for re-identification attacks.","The state of our knowledge about currently existing privacy mechanisms is limited to privacy-utility trade-off curves based on data-centric metrics of utility, such as prediction error, and black-box threat models.","We propose that for interactive VR applications, it is essential to consider user-centric notions of utility and a variety of threat models.","We develop a methodology to evaluate real-time privacy mechanisms for interactive VR applications that incorporate subjective user experience and task performance metrics.","We evaluate selected privacy mechanisms using this methodology and find that re-identification accuracy can be decreased to as low as 14% while maintaining a high usability score and reasonable task performance.","Finally, we elucidate three threat scenarios (black-box, black-box with exemplars, and white-box) and assess how well the different privacy mechanisms hold up to these adversarial scenarios.","This work advances the state of the art in VR privacy by providing a methodology for end-to-end assessment of the risk of re-identification attacks and potential mitigating solutions."],"url":"http://arxiv.org/abs/2402.07687v1","category":"cs.HC"}
{"created":"2024-02-12 14:52:30","title":"Global well-posedness and asymptotic behavior for the Euler-alignment system with pressure","abstract":"We study the Cauchy problem of the compressible Euler system with strongly singular velocity alignment. We establish a global well-posedness theory for the system with small smooth initial data. Additionally, we derive asymptotic emergent behaviors for the system, providing time decay estimates with optimal decay rates. Notably, the optimal decay rate we obtain does not align with the corresponding fractional heat equation within our considered range, where the parameter $\\alpha\\in(0,1)$. This highlights the distinct feature of the alignment operator.","sentences":["We study the Cauchy problem of the compressible Euler system with strongly singular velocity alignment.","We establish a global well-posedness theory for the system with small smooth initial data.","Additionally, we derive asymptotic emergent behaviors for the system, providing time decay estimates with optimal decay rates.","Notably, the optimal decay rate we obtain does not align with the corresponding fractional heat equation within our considered range, where the parameter $\\alpha\\in(0,1)$. This highlights the distinct feature of the alignment operator."],"url":"http://arxiv.org/abs/2402.07686v1","category":"math.AP"}
{"created":"2024-02-12 14:42:33","title":"Auxiliary Tasks to Boost Biaffine Semantic Dependency Parsing","abstract":"The biaffine parser of Dozat and Manning (2017) was successfully extended to semantic dependency parsing (SDP) (Dozat and Manning, 2018). Its performance on graphs is surprisingly high given that, without the constraint of producing a tree, all arcs for a given sentence are predicted independently from each other (modulo a shared representation of tokens). To circumvent such an independence of decision, while retaining the O(n^2) complexity and highly parallelizable architecture, we propose to use simple auxiliary tasks that introduce some form of interdependence between arcs. Experiments on the three English acyclic datasets of SemEval 2015 task 18 (Oepen et al., 2015), and on French deep syntactic cyclic graphs (Ribeyre et al., 2014) show modest but systematic performance gains on a near state-of-the-art baseline using transformer-based contextualized representations. This provides a simple and robust method to boost SDP performance.","sentences":["The biaffine parser of Dozat and Manning (2017) was successfully extended to semantic dependency parsing (SDP) (Dozat and Manning, 2018).","Its performance on graphs is surprisingly high given that, without the constraint of producing a tree, all arcs for a given sentence are predicted independently from each other (modulo a shared representation of tokens).","To circumvent such an independence of decision, while retaining the O(n^2) complexity and highly parallelizable architecture, we propose to use simple auxiliary tasks that introduce some form of interdependence between arcs.","Experiments on the three English acyclic datasets of SemEval 2015 task 18 (Oepen et al., 2015), and on French deep syntactic cyclic graphs (Ribeyre et al., 2014) show modest but systematic performance gains on a near state-of-the-art baseline using transformer-based contextualized representations.","This provides a simple and robust method to boost SDP performance."],"url":"http://arxiv.org/abs/2402.07682v1","category":"cs.CL"}
{"created":"2024-02-12 14:38:46","title":"GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted Assembly Guidance","abstract":"Guidance for assemblable parts is a promising field for augmented reality. Augmented reality assembly guidance requires 6D object poses of target objects in real time. Especially in time-critical medical or industrial settings, continuous and markerless tracking of individual parts is essential to visualize instructions superimposed on or next to the target object parts. In this regard, occlusions by the user's hand or other objects and the complexity of different assembly states complicate robust and real-time markerless multi-object tracking. To address this problem, we present Graph-based Object Tracking (GBOT), a novel graph-based single-view RGB-D tracking approach. The real-time markerless multi-object tracking is initialized via 6D pose estimation and updates the graph-based assembly poses. The tracking through various assembly states is achieved by our novel multi-state assembly graph. We update the multi-state assembly graph by utilizing the relative poses of the individual assembly parts. Linking the individual objects in this graph enables more robust object tracking during the assembly process. For evaluation, we introduce a synthetic dataset of publicly available and 3D printable assembly assets as a benchmark for future work. Quantitative experiments in synthetic data and further qualitative study in real test data show that GBOT can outperform existing work towards enabling context-aware augmented reality assembly guidance. Dataset and code will be made publically available.","sentences":["Guidance for assemblable parts is a promising field for augmented reality.","Augmented reality assembly guidance requires 6D object poses of target objects in real time.","Especially in time-critical medical or industrial settings, continuous and markerless tracking of individual parts is essential to visualize instructions superimposed on or next to the target object parts.","In this regard, occlusions by the user's hand or other objects and the complexity of different assembly states complicate robust and real-time markerless multi-object tracking.","To address this problem, we present Graph-based Object Tracking (GBOT), a novel graph-based single-view RGB-D tracking approach.","The real-time markerless multi-object tracking is initialized via 6D pose estimation and updates the graph-based assembly poses.","The tracking through various assembly states is achieved by our novel multi-state assembly graph.","We update the multi-state assembly graph by utilizing the relative poses of the individual assembly parts.","Linking the individual objects in this graph enables more robust object tracking during the assembly process.","For evaluation, we introduce a synthetic dataset of publicly available and 3D printable assembly assets as a benchmark for future work.","Quantitative experiments in synthetic data and further qualitative study in real test data show that GBOT can outperform existing work towards enabling context-aware augmented reality assembly guidance.","Dataset and code will be made publically available."],"url":"http://arxiv.org/abs/2402.07677v1","category":"cs.CV"}
{"created":"2024-02-12 14:35:11","title":"Statistical modelling and Bayesian inversion for a Compton imaging system: application to radioactive source localisation","abstract":"This paper presents a statistical forward model for a Compton imaging system, called Compton imager. This system, under development at the University of Illinois Urbana Champaign, is a variant of Compton cameras with a single type of sensors which can simultaneously act as scatterers and absorbers. This imager is convenient for imaging situations requiring a wide field of view. The proposed statistical forward model is then used to solve the inverse problem of estimating the location and energy of point-like sources from observed data. This inverse problem is formulated and solved in a Bayesian framework by using a Metropolis within Gibbs algorithm for the estimation of the location, and an expectation-maximization algorithm for the estimation of the energy. This approach leads to more accurate estimation when compared with the deterministic standard back-projection approach, with the additional benefit of uncertainty quantification in the low photon imaging setting.","sentences":["This paper presents a statistical forward model for a Compton imaging system, called Compton imager.","This system, under development at the University of Illinois Urbana Champaign, is a variant of Compton cameras with a single type of sensors which can simultaneously act as scatterers and absorbers.","This imager is convenient for imaging situations requiring a wide field of view.","The proposed statistical forward model is then used to solve the inverse problem of estimating the location and energy of point-like sources from observed data.","This inverse problem is formulated and solved in a Bayesian framework by using a Metropolis within Gibbs algorithm for the estimation of the location, and an expectation-maximization algorithm for the estimation of the energy.","This approach leads to more accurate estimation when compared with the deterministic standard back-projection approach, with the additional benefit of uncertainty quantification in the low photon imaging setting."],"url":"http://arxiv.org/abs/2402.07676v1","category":"stat.AP"}
{"created":"2024-02-12 14:31:20","title":"A Multi-Tenant System for 5/6G Testbed as-a-Service","abstract":"In order to fulfill the stringent requirements and fast advancements of 5G and beyond applications, it is inevitable to develop research/industrial testbeds to examine the different proposed innovative features of 5G and beyond. In this paper, we propose a testbed including 5G and beyond technologies by combining open-source solutions and our developed Network Services/Functions to enhance the ETSI-NFV MANO framework. Our testbed contains an automation framework to reduce both run time and setup complexity. It provides various services: Metal as a Service (MaaS), Infrastructure as a Service (IaaS), Platform as a Service (PaaS), different Network Functions (NFs), and Network Services (NSs), under the context of full automation of End-to-End (E2E) NSs on top of the services provided by ETSI.","sentences":["In order to fulfill the stringent requirements and fast advancements of 5G and beyond applications, it is inevitable to develop research/industrial testbeds to examine the different proposed innovative features of 5G and beyond.","In this paper, we propose a testbed including 5G and beyond technologies by combining open-source solutions and our developed Network Services/Functions to enhance the ETSI-NFV MANO framework.","Our testbed contains an automation framework to reduce both run time and setup complexity.","It provides various services: Metal as a Service (MaaS), Infrastructure as a Service (IaaS), Platform as a Service (PaaS), different Network Functions (NFs), and Network Services (NSs), under the context of full automation of End-to-End (E2E) NSs on top of the services provided by ETSI."],"url":"http://arxiv.org/abs/2402.07674v1","category":"cs.NI"}
{"created":"2024-02-12 14:27:32","title":"Photonic cellular automaton simulation of relativistic quantum fields: observation of Zitterbewegung","abstract":"Quantum Cellular Automaton (QCA) is a model for universal quantum computation and a natural candidate for digital quantum simulation of relativistic quantum fields. Here we introduce the first photonic platform for implementing QCA-simulation of a free relativistic Dirac quantum field in 1+1 dimension, through a Dirac Quantum Cellular Automaton (DQCA). Encoding the field position degree of freedom in the Orbital Angular Momentum (OAM) of single photons, our state-of-the-art setup experimentally realizes 8 steps of a DQCA, with the possibility of having complete control over the input OAM state preparation and the output measurement making use of two spatial light modulators. Therefore, studying the distribution in the OAM space at each step, we were able to reproduce the time evolution of the free Dirac field observing, the Zitterbewegung, an oscillatory movement extremely difficult to see in real case experimental scenario that is a signature of the interference of particle and antiparticle states. The accordance between the expected and measured Zitterbewegung oscillations certifies the simulator performances, paving the way towards the application of photonic platforms to the simulation of more complex relativistic effects.","sentences":["Quantum Cellular Automaton (QCA) is a model for universal quantum computation and a natural candidate for digital quantum simulation of relativistic quantum fields.","Here we introduce the first photonic platform for implementing QCA-simulation of a free relativistic Dirac quantum field in 1+1 dimension, through a Dirac Quantum Cellular Automaton (DQCA).","Encoding the field position degree of freedom in the Orbital Angular Momentum (OAM) of single photons, our state-of-the-art setup experimentally realizes 8 steps of a DQCA, with the possibility of having complete control over the input OAM state preparation and the output measurement making use of two spatial light modulators.","Therefore, studying the distribution in the OAM space at each step, we were able to reproduce the time evolution of the free Dirac field observing, the Zitterbewegung, an oscillatory movement extremely difficult to see in real case experimental scenario that is a signature of the interference of particle and antiparticle states.","The accordance between the expected and measured Zitterbewegung oscillations certifies the simulator performances, paving the way towards the application of photonic platforms to the simulation of more complex relativistic effects."],"url":"http://arxiv.org/abs/2402.07672v1","category":"quant-ph"}
{"created":"2024-02-12 14:20:33","title":"Tutorial: Shaping the Spatial Correlations of Entangled Photon Pairs","abstract":"Quantum imaging enhances imaging systems performance, potentially surpassing fundamental limits such as noise and resolution. However, these schemes have limitations and are still a long way from replacing classical techniques. Therefore, there is a strong focus on improving the practicality of quantum imaging methods, with the goal of finding real-world applications. With this in mind, in this tutorial we describe how the concepts of classical light shaping can be applied to imaging schemes based on entangled photon pairs. We detail two basic experimental configurations in which a spatial light modulator is used to shape the spatial correlations of a photon pair state and highlight the key differences between this and classical shaping. We then showcase two recent examples that expand on these concepts to perform aberration and scattering correction with photon pairs. We include specific details on the key steps of these experiments, with the goal that this can be used as a guide for building photon-pair-based imaging and shaping experiments.","sentences":["Quantum imaging enhances imaging systems performance, potentially surpassing fundamental limits such as noise and resolution.","However, these schemes have limitations and are still a long way from replacing classical techniques.","Therefore, there is a strong focus on improving the practicality of quantum imaging methods, with the goal of finding real-world applications.","With this in mind, in this tutorial we describe how the concepts of classical light shaping can be applied to imaging schemes based on entangled photon pairs.","We detail two basic experimental configurations in which a spatial light modulator is used to shape the spatial correlations of a photon pair state and highlight the key differences between this and classical shaping.","We then showcase two recent examples that expand on these concepts to perform aberration and scattering correction with photon pairs.","We include specific details on the key steps of these experiments, with the goal that this can be used as a guide for building photon-pair-based imaging and shaping experiments."],"url":"http://arxiv.org/abs/2402.07667v1","category":"quant-ph"}
{"created":"2024-02-12 14:16:37","title":"Enabling performance portability of data-parallel OpenMP applications on asymmetric multicore processors","abstract":"Asymmetric multicore processors (AMPs) couple high-performance big cores and low-power small cores with the same instruction-set architecture but different features, such as clock frequency or microarchitecture. Previous work has shown that asymmetric designs may deliver higher energy efficiency than symmetric multicores for diverse workloads. Despite their benefits, AMPs pose significant challenges to runtime systems of parallel programming models. While previous work has mainly explored how to efficiently execute task-based parallel applications on AMPs, via enhancements in the runtime system, improving the performance of unmodified data-parallel applications on these architectures is still a big challenge. In this work we analyze the particular case of loop-based OpenMP applications, which are widely used today in scientific and engineering domains, and constitute the dominant application type in many parallel benchmark suites used for performance evaluation on multicore systems. We observed that conventional loop-scheduling OpenMP approaches are unable to efficiently cope with the load imbalance that naturally stems from the different performance delivered by big and small cores.   To address this shortcoming, we propose \\textit{Asymmetric Iteration Distribution} (AID), a set of novel loop-scheduling methods for AMPs that distribute iterations unevenly across worker threads to efficiently deal with performance asymmetry. We implemented AID in \\textit{libgomp} --the GNU OpenMP runtime system--, and evaluated it on two different asymmetric multicore platforms. Our analysis reveals that the AID methods constitute effective replacements of the \\texttt{static} and \\texttt{dynamic} methods on AMPs, and are capable of improving performance over these conventional strategies by up to 56\\% and 16.8\\%, respectively.","sentences":["Asymmetric multicore processors (AMPs) couple high-performance big cores and low-power small cores with the same instruction-set architecture but different features, such as clock frequency or microarchitecture.","Previous work has shown that asymmetric designs may deliver higher energy efficiency than symmetric multicores for diverse workloads.","Despite their benefits, AMPs pose significant challenges to runtime systems of parallel programming models.","While previous work has mainly explored how to efficiently execute task-based parallel applications on AMPs, via enhancements in the runtime system, improving the performance of unmodified data-parallel applications on these architectures is still a big challenge.","In this work we analyze the particular case of loop-based OpenMP applications, which are widely used today in scientific and engineering domains, and constitute the dominant application type in many parallel benchmark suites used for performance evaluation on multicore systems.","We observed that conventional loop-scheduling OpenMP approaches are unable to efficiently cope with the load imbalance that naturally stems from the different performance delivered by big and small cores.   ","To address this shortcoming, we propose \\textit{Asymmetric Iteration Distribution} (AID), a set of novel loop-scheduling methods for AMPs that distribute iterations unevenly across worker threads to efficiently deal with performance asymmetry.","We implemented AID in \\textit{libgomp} --the GNU OpenMP runtime system--, and evaluated it on two different asymmetric multicore platforms.","Our analysis reveals that the AID methods constitute effective replacements of the \\texttt{static} and \\texttt{dynamic} methods on AMPs, and are capable of improving performance over these conventional strategies by up to 56\\% and 16.8\\%, respectively."],"url":"http://arxiv.org/abs/2402.07664v1","category":"cs.DC"}
{"created":"2024-02-12 14:10:38","title":"A hybrid memetic-ANS optimization algorithm for the home health care and home care routing and re","abstract":"This paper addresses a realistic home health care and home care (HHC\\&HC) problem which has become increasingly complex in the face of demographic aging and post-COVID-19 disruptions. The HHC\\&HC sector, as the essential component of modern health care systems, faces unique challenges in efficiently scheduling and routing caregivers to meet the rising demand for home-based care services. Traditional approaches often fall short in addressing the dynamic nature of care requests, especially in accommodating new, same-day service requests without compromising scheduled visits. To tackle these issues, We define the problem as an HHC\\&HC routing and rescheduling problem with rejection of new customers (HHC\\&HCRRP-RNC), focusing on rescheduling for a single HHC\\&HC caregiver in response to new customer requests within a single period. This problem is a variant of both the single-machine reschedule problem and the orienteering problem with mandatory visits (OPMV), where certain nodes must be visited while others are optional. A mixed integer linear programming (MILP) model is developed to cater to two groups of customers: pre-scheduled existing customers and same-day service new customers. The model emphasized maintaining minimal disruptions to the original schedule for existing customers as a constraint, highlighting the balance between adhering to scheduled visits and accommodating new customers. A hybrid memetic-Adaptive Neighborhood Search (ANS) optimization algorithm is proposed to tackle the model. This approach aims to minimize operational costs and opportunity costs while enhancing service quality and patient satisfaction. Through computational experiments, our proposed algorithm demonstrates notable performance, offering significant improvements in both efficiency and robustness within the problem domain.","sentences":["This paper addresses a realistic home health care and home care (HHC\\&HC) problem which has become increasingly complex in the face of demographic aging and post-COVID-19 disruptions.","The HHC\\&HC sector, as the essential component of modern health care systems, faces unique challenges in efficiently scheduling and routing caregivers to meet the rising demand for home-based care services.","Traditional approaches often fall short in addressing the dynamic nature of care requests, especially in accommodating new, same-day service requests without compromising scheduled visits.","To tackle these issues, We define the problem as an HHC\\&HC routing and rescheduling problem with rejection of new customers (HHC\\&HCRRP-RNC), focusing on rescheduling for a single HHC\\&HC caregiver in response to new customer requests within a single period.","This problem is a variant of both the single-machine reschedule problem and the orienteering problem with mandatory visits (OPMV), where certain nodes must be visited while others are optional.","A mixed integer linear programming (MILP) model is developed to cater to two groups of customers: pre-scheduled existing customers and same-day service new customers.","The model emphasized maintaining minimal disruptions to the original schedule for existing customers as a constraint, highlighting the balance between adhering to scheduled visits and accommodating new customers.","A hybrid memetic-Adaptive Neighborhood Search (ANS) optimization algorithm is proposed to tackle the model.","This approach aims to minimize operational costs and opportunity costs while enhancing service quality and patient satisfaction.","Through computational experiments, our proposed algorithm demonstrates notable performance, offering significant improvements in both efficiency and robustness within the problem domain."],"url":"http://arxiv.org/abs/2402.07662v1","category":"math.OC"}
{"created":"2024-02-12 13:57:30","title":"Low Cost Carriers induce specific and identifiable delay propagation patterns: an analysis of the EU and US systems","abstract":"The impact of air transport delays and their propagation has long been studied, mainly from environmental and mobility viewpoints, using a wide range of data analysis tools and simulations. Less attention has nevertheless been devoted to how delays create meso-scale structures around each airport. In this work we tackle this issue by reconstructing functional networks of delay propagation centred at each airport, and studying their identifiability (i.e. how unique they are) using Deep Learning models. We find that such delay propagation neighbourhoods are highly unique when they correspond to airports with a high share of Low Cost Carriers operations; and demonstrate the robustness of these findings for the EU and US systems, and to different methodological choices. We further discuss some operational implications of this uniqueness.","sentences":["The impact of air transport delays and their propagation has long been studied, mainly from environmental and mobility viewpoints, using a wide range of data analysis tools and simulations.","Less attention has nevertheless been devoted to how delays create meso-scale structures around each airport.","In this work we tackle this issue by reconstructing functional networks of delay propagation centred at each airport, and studying their identifiability (i.e. how unique they are) using Deep Learning models.","We find that such delay propagation neighbourhoods are highly unique when they correspond to airports with a high share of Low Cost Carriers operations; and demonstrate the robustness of these findings for the EU and US systems, and to different methodological choices.","We further discuss some operational implications of this uniqueness."],"url":"http://arxiv.org/abs/2402.07656v1","category":"physics.soc-ph"}
{"created":"2024-02-12 13:46:30","title":"Spin orbit resonance cascade via core shell model. Application to Mercury and Ganymede","abstract":"We discuss a model describing the spin orbit resonance cascade. We assume that the primary has a two-layer (core-shell) structure: it is composed by a thin solid crust and an inner and heavier solid core that are interacting due to the presence of a fluid interface. We assume two sources of dissipation: a viscous one, depending on the relative angular velocity between core and crust and a tidal one, smaller than the first, due to the viscoelastic structure of the core. We show how these two sources of dissipation are needful for the capture in spin-orbit resonance. The crust and the core fall in resonance with different time scales if the viscous coupling between them is big enough. Finally, the tidal dissipation of the viscoelastic core, decreasing the eccentricity, brings the system out of the resonance in a third very long time scale. This mechanism of entry and exit from resonance ends in the $1:1$ stable state.","sentences":["We discuss a model describing the spin orbit resonance cascade.","We assume that the primary has a two-layer (core-shell) structure: it is composed by a thin solid crust and an inner and heavier solid core that are interacting due to the presence of a fluid interface.","We assume two sources of dissipation: a viscous one, depending on the relative angular velocity between core and crust and a tidal one, smaller than the first, due to the viscoelastic structure of the core.","We show how these two sources of dissipation are needful for the capture in spin-orbit resonance.","The crust and the core fall in resonance with different time scales if the viscous coupling between them is big enough.","Finally, the tidal dissipation of the viscoelastic core, decreasing the eccentricity, brings the system out of the resonance in a third very long time scale.","This mechanism of entry and exit from resonance ends in the $1:1$ stable state."],"url":"http://arxiv.org/abs/2402.07650v1","category":"math-ph"}
{"created":"2024-02-12 13:32:42","title":"Localization in Massive MIMO Networks: From Near-Field to Far-Field","abstract":"Source localization is the process of estimating the location of signal sources based on the signals received at different antennas of an antenna array. It has diverse applications, ranging from radar systems and underwater acoustics to wireless communication networks. Subspace-based approaches are among the most effective techniques for source localization due to their high accuracy, with Multiple SIgnal Classification (MUSIC) and Estimation of Signal Parameters by Rotational Invariance Techniques (ESPRIT) being two prominent methods in this category. These techniques leverage the fact that the space spanned by the eigenvectors of the covariance matrix of the received signals can be divided into signal and noise subspaces, which are mutually orthogonal. Originally designed for far-field source localization, these methods have undergone several modifications to accommodate near-field scenarios as well. This chapter aims to present the foundations of MUSIC and ESPRIT algorithms and introduce some of their variations for both far-field and near-field localization by a single array of antennas. We further provide numerical examples to demonstrate the performance of the presented methods.","sentences":["Source localization is the process of estimating the location of signal sources based on the signals received at different antennas of an antenna array.","It has diverse applications, ranging from radar systems and underwater acoustics to wireless communication networks.","Subspace-based approaches are among the most effective techniques for source localization due to their high accuracy, with Multiple SIgnal Classification (MUSIC) and Estimation of Signal Parameters by Rotational Invariance Techniques (ESPRIT) being two prominent methods in this category.","These techniques leverage the fact that the space spanned by the eigenvectors of the covariance matrix of the received signals can be divided into signal and noise subspaces, which are mutually orthogonal.","Originally designed for far-field source localization, these methods have undergone several modifications to accommodate near-field scenarios as well.","This chapter aims to present the foundations of MUSIC and ESPRIT algorithms and introduce some of their variations for both far-field and near-field localization by a single array of antennas.","We further provide numerical examples to demonstrate the performance of the presented methods."],"url":"http://arxiv.org/abs/2402.07644v1","category":"eess.SP"}
{"created":"2024-02-12 13:32:04","title":"A Lattice-Reduction Aided Vector Perturbation Precoder Relying on Quantum Annealing","abstract":"Quantum annealing (QA) is proposed for vector perturbation precoding (VPP) in multiple input multiple output (MIMO) communications systems. The mathematical framework of VPP is presented, outlining the problem formulation and the benefits of lattice reduction algorithms. Lattice reduction aided quantum vector perturbation (LRAQVP) is designed by harnessing physical quantum hardware, and the optimization of hardware parameters is discussed. We observe a 5dB gain over lattice reduction zero forcing precoding (LRZFP), which behaves similarly to a quantum annealing algorithm operating without a lattice reduction stage. The proposed algorithm is also shown to approach the performance of a sphere encoder, which exhibits an exponentially escalating complexity.","sentences":["Quantum annealing (QA) is proposed for vector perturbation precoding (VPP) in multiple input multiple output (MIMO) communications systems.","The mathematical framework of VPP is presented, outlining the problem formulation and the benefits of lattice reduction algorithms.","Lattice reduction aided quantum vector perturbation (LRAQVP) is designed by harnessing physical quantum hardware, and the optimization of hardware parameters is discussed.","We observe a 5dB gain over lattice reduction zero forcing precoding (LRZFP), which behaves similarly to a quantum annealing algorithm operating without a lattice reduction stage.","The proposed algorithm is also shown to approach the performance of a sphere encoder, which exhibits an exponentially escalating complexity."],"url":"http://arxiv.org/abs/2402.07643v1","category":"cs.IT"}
{"created":"2024-02-12 13:21:02","title":"Observation of Larmor-like precession of a single birefringent particle due to spin-dependent forces in tilted optical tweezers","abstract":"We observe clear precessional motion of highly birefringent liquid crystal (LC) particles trapped in a spherically aberrated optical trap which is built around a tilted refractive index stratified medium. For input circularly polarized light, the breaking of azimuthal symmetry induced by the tilt leads to an asymmetric intensity distribution in the radial direction near the trap focal plane, which - in combination with the spin-orbit conversion effects for input circularly polarized light - results in non-uniform canonical and spin momentum densities in those regions. In addition, while the canonical momentum remains always oriented towards the axial direction, the spin momentum reverses direction along spatial loops in the radial direction. As a consequence, the total momentum precesses around the canonical momentum vector along elliptical spatial loops - akin to a Larmor-like precession of magnetic moment (total momentum in our case) around a magnetic field (canonical momentum). We probe this precession experimentally using the single trapped LC particles - with the direction of precession determined by the helicity of the input light and the precession frequency varying linearly with the laser power. Our experimental results are validated by numerical simulations of the system where we employ the Debye-Wolf theory for tight focusing in the presence of a tilted stratified media.","sentences":["We observe clear precessional motion of highly birefringent liquid crystal (LC) particles trapped in a spherically aberrated optical trap which is built around a tilted refractive index stratified medium.","For input circularly polarized light, the breaking of azimuthal symmetry induced by the tilt leads to an asymmetric intensity distribution in the radial direction near the trap focal plane, which - in combination with the spin-orbit conversion effects for input circularly polarized light - results in non-uniform canonical and spin momentum densities in those regions.","In addition, while the canonical momentum remains always oriented towards the axial direction, the spin momentum reverses direction along spatial loops in the radial direction.","As a consequence, the total momentum precesses around the canonical momentum vector along elliptical spatial loops - akin to a Larmor-like precession of magnetic moment (total momentum in our case) around a magnetic field (canonical momentum).","We probe this precession experimentally using the single trapped LC particles - with the direction of precession determined by the helicity of the input light and the precession frequency varying linearly with the laser power.","Our experimental results are validated by numerical simulations of the system where we employ the Debye-Wolf theory for tight focusing in the presence of a tilted stratified media."],"url":"http://arxiv.org/abs/2402.07638v1","category":"physics.optics"}
{"created":"2024-02-12 13:15:08","title":"Higher-order Connection Laplacians for Directed Simplicial Complexes","abstract":"Higher-order networks encode the many-body interactions existing in complex systems, such as the brain, protein complexes, and social interactions. Simplicial complexes are higher-order networks that allow a comprehensive investigation of the interplay between topology and dynamics. However, simplicial complexes have the limitation that they only capture undirected higher-order interactions while in real-world scenarios, often there is a need to introduce the direction of simplices, extending the popular notion of direction of edges. On graphs and networks the Magnetic Laplacian, a special case of Connection Laplacian, is becoming a popular operator to treat edge directionality. Here we tackle the challenge of treating directional simplicial complexes by formulating Higher-order Connection Laplacians taking into account the configurations induced by the simplices' directions. Specifically, we define all the Connection Laplacians of directed simplicial complexes of dimension two and we discuss the induced higher-order diffusion dynamics by considering instructive synthetic examples of simplicial complexes. The proposed higher-order diffusion processes can be adopted in real scenarios when we want to consider higher-order diffusion displaying non-trivial frustration effects due to conflicting directionalities of the incident simplices.","sentences":["Higher-order networks encode the many-body interactions existing in complex systems, such as the brain, protein complexes, and social interactions.","Simplicial complexes are higher-order networks that allow a comprehensive investigation of the interplay between topology and dynamics.","However, simplicial complexes have the limitation that they only capture undirected higher-order interactions while in real-world scenarios, often there is a need to introduce the direction of simplices, extending the popular notion of direction of edges.","On graphs and networks the Magnetic Laplacian, a special case of Connection Laplacian, is becoming a popular operator to treat edge directionality.","Here we tackle the challenge of treating directional simplicial complexes by formulating Higher-order Connection Laplacians taking into account the configurations induced by the simplices' directions.","Specifically, we define all the Connection Laplacians of directed simplicial complexes of dimension two","and we discuss the induced higher-order diffusion dynamics by considering instructive synthetic examples of simplicial complexes.","The proposed higher-order diffusion processes can be adopted in real scenarios when we want to consider higher-order diffusion displaying non-trivial frustration effects due to conflicting directionalities of the incident simplices."],"url":"http://arxiv.org/abs/2402.07631v1","category":"cs.SI"}
{"created":"2024-02-12 13:11:40","title":"On implicit and explicit representations for 1D distributed port-Hamiltonian systems","abstract":"First, two examples of 1D distributed port-Hamiltonian systems with dissipation, given in explicit (descriptor) form, are considered: the Dzekster model for the seepage of underground water and a nanorod model with non-local viscous damping. Implicit representations in Stokes-Lagrange subspaces are formulated. These formulations lead to modified Hamiltonian functions with spatial differential operators. The associated power balance equations are derived, together with the new boundary ports. Second, the port-Hamiltonian formulations for the Timoshenko and the Euler-Bernoulli beams are recalled, the latter being a flow-constrained version of the former. Implicit representations of these models in Stokes-Lagrange subspaces and corresponding power balance equations are derived. Bijective transformations are proposed between the explicit and implicit representations. It is proven these transformations commute with the flow-constraint projection operator.","sentences":["First, two examples of 1D distributed port-Hamiltonian systems with dissipation, given in explicit (descriptor) form, are considered: the Dzekster model for the seepage of underground water and a nanorod model with non-local viscous damping.","Implicit representations in Stokes-Lagrange subspaces are formulated.","These formulations lead to modified Hamiltonian functions with spatial differential operators.","The associated power balance equations are derived, together with the new boundary ports.","Second, the port-Hamiltonian formulations for the Timoshenko and the Euler-Bernoulli beams are recalled, the latter being a flow-constrained version of the former.","Implicit representations of these models in Stokes-Lagrange subspaces and corresponding power balance equations are derived.","Bijective transformations are proposed between the explicit and implicit representations.","It is proven these transformations commute with the flow-constraint projection operator."],"url":"http://arxiv.org/abs/2402.07628v1","category":"math.DS"}
{"created":"2024-02-12 13:11:29","title":"Unveiling the GeI2-Assisted Oriented Growth of Perovskite Crystallite for High-Performance Flexible Sn Perovskite Solar Cells","abstract":"Tin perovskites are emerging as promising alternatives to their lead-based counterparts for high-performance and flexible perovskite solar cells (PSCs). However, their rapid crystallization often leads to inadequate film quality and poor device performance. In this study, the role of GeI2 as an additive is investigated for controlling the nucleation and crystallization processes of formamidium tin triiodide (FASnI3). The findings reveal the preferential formation of a Ge-rich layer at the bottom of the perovskite film upon the introduction of GeI2. It is proposed that the initial formation of the Ge-complex acts as a crystallization regulator, promoting oriented growth of subsequent FASnI3 crystals and enhancing overall crystallinity. Through the incorporation of an optimal amount of GeI2, flexible Sn PSCs with an efficiency of 10.8% were achieved. Furthermore, it was observed that the GeI2 additive ensures a remarkable shelf-life for the devices, with the rigid cells retaining 91% of their initial performance after more than 13,800 hours of storage in an N2 gas environment. This study elucidates the mechanistic role of GeI2 in regulating the nucleation and crystallization process of tin perovskites, providing valuable insights into the significance of additive engineering for the development of high-performance flexible tin PSCs.","sentences":["Tin perovskites are emerging as promising alternatives to their lead-based counterparts for high-performance and flexible perovskite solar cells (PSCs).","However, their rapid crystallization often leads to inadequate film quality and poor device performance.","In this study, the role of GeI2 as an additive is investigated for controlling the nucleation and crystallization processes of formamidium tin triiodide (FASnI3).","The findings reveal the preferential formation of a Ge-rich layer at the bottom of the perovskite film upon the introduction of GeI2.","It is proposed that the initial formation of the Ge-complex acts as a crystallization regulator, promoting oriented growth of subsequent FASnI3 crystals and enhancing overall crystallinity.","Through the incorporation of an optimal amount of GeI2, flexible Sn PSCs with an efficiency of 10.8% were achieved.","Furthermore, it was observed that the GeI2 additive ensures a remarkable shelf-life for the devices, with the rigid cells retaining 91% of their initial performance after more than 13,800 hours of storage in an N2 gas environment.","This study elucidates the mechanistic role of GeI2 in regulating the nucleation and crystallization process of tin perovskites, providing valuable insights into the significance of additive engineering for the development of high-performance flexible tin PSCs."],"url":"http://arxiv.org/abs/2402.07627v1","category":"physics.app-ph"}
{"created":"2024-02-12 12:55:35","title":"Correctness Verification of Neural Networks Approximating Differential Equations","abstract":"Verification of Neural Networks (NNs) that approximate the solution of Partial Differential Equations (PDEs) is a major milestone towards enhancing their trustworthiness and accelerating their deployment, especially for safety-critical systems. If successful, such NNs can become integral parts of simulation software tools which can accelerate the simulation of complex dynamic systems more than 100 times. However, the verification of these functions poses major challenges; it is not straightforward how to efficiently bound them or how to represent the derivative of the NN. This work addresses both these problems. First, we define the NN derivative as a finite difference approximation. Then, we formulate the PDE residual bounding problem alongside the Initial Value Problem's error propagation. Finally, for the first time, we tackle the problem of bounding an NN function without a priori knowledge of the output domain. For this, we build a parallel branching algorithm that combines the incomplete CROWN solver and Gradient Attack for termination and domain rejection conditions. We demonstrate the strengths and weaknesses of the proposed framework, and we suggest further work to enhance its efficiency.","sentences":["Verification of Neural Networks (NNs) that approximate the solution of Partial Differential Equations (PDEs) is a major milestone towards enhancing their trustworthiness and accelerating their deployment, especially for safety-critical systems.","If successful, such NNs can become integral parts of simulation software tools which can accelerate the simulation of complex dynamic systems more than 100 times.","However, the verification of these functions poses major challenges; it is not straightforward how to efficiently bound them or how to represent the derivative of the NN.","This work addresses both these problems.","First, we define the NN derivative as a finite difference approximation.","Then, we formulate the PDE residual bounding problem alongside the Initial Value Problem's error propagation.","Finally, for the first time, we tackle the problem of bounding an NN function without a priori knowledge of the output domain.","For this, we build a parallel branching algorithm that combines the incomplete CROWN solver and Gradient Attack for termination and domain rejection conditions.","We demonstrate the strengths and weaknesses of the proposed framework, and we suggest further work to enhance its efficiency."],"url":"http://arxiv.org/abs/2402.07621v1","category":"eess.SY"}
{"created":"2024-02-12 12:51:13","title":"Reconciling differential radii in the silver chain through improved measurement and {\\it ab initio} calculations","abstract":"Nuclear charge radius differences in the silver isotopic chain have been reported through different combinations of experiment and theory, and clear disagreement is observed beyond the assigned uncertainties. This study investigates this issue by combining high-accuracy calculations for six low-lying states of atomic silver with an improved measurement of the $5s ^2S_{1/2} - 5p ^2P_{3/2}$ transition optical isotope shift. Our calculations predict measured electronic transition energies in Ag I at the 0.3\\% level, the highest accuracy achieved in this system so far. We calculate electronic isotope shift factors by employing analytical response relativistic coupled-cluster theory, and find that a consistent charge radius difference between 107,109Ag is returned when combining our calculations with the available optical isotope shift measurements. We therefore recommend an improved value for the mean-squared charge radius difference between $^{107}$Ag and $^{109}$Ag as $0.207(3)[4]$ fm$^2$, in marginal agreement with the value derived from muonic Ag experiments, and an updated set of charge radii differences across the isotopic chain.","sentences":["Nuclear charge radius differences in the silver isotopic chain have been reported through different combinations of experiment and theory, and clear disagreement is observed beyond the assigned uncertainties.","This study investigates this issue by combining high-accuracy calculations for six low-lying states of atomic silver with an improved measurement of the $5s ^2S_{1/2} - 5p ^2P_{3/2}$ transition optical isotope shift.","Our calculations predict measured electronic transition energies in Ag I at the 0.3\\% level, the highest accuracy achieved in this system so far.","We calculate electronic isotope shift factors by employing analytical response relativistic coupled-cluster theory, and find that a consistent charge radius difference between 107,109Ag is returned when combining our calculations with the available optical isotope shift measurements.","We therefore recommend an improved value for the mean-squared charge radius difference between $^{107}$Ag and $^{109}$Ag as $0.207(3)[4]$ fm$^2$, in marginal agreement with the value derived from muonic Ag experiments, and an updated set of charge radii differences across the isotopic chain."],"url":"http://arxiv.org/abs/2402.07618v1","category":"physics.atom-ph"}
{"created":"2024-02-12 12:35:05","title":"Local geometry of Equilibria and a Poincar\u00e9-Bendixson-type Theorem for Holomorphic Flows","abstract":"In this paper, we explore the local geometry of dynamical systems $\\dot{x}=F(x)$ with real time parameterization, where $F$ is holomorphic on connected open subsets of $\\mathbb{C}\\stackrel{\\sim}{=}\\mathbb{R}^2$. We describe the geometry of first-order equilibria. For equilibria of higher orders, we establish an equivalent condition for \"definite directions\", allowing us to reverse the implication in Theorem 2 of Chapter 2.10 in [Differential equations and dynamical systems, Lawrence Perko (1990)] under the additional condition of holomorphy. This enables the geometric construction of a finite elliptic decomposition. We derive a holomorphic Poincar\\'e-Bendixson-type theorem, leading to the conclusion that bounded non-periodic orbits are always homoclinic or heteroclinic.","sentences":["In this paper, we explore the local geometry of dynamical systems $\\dot{x}=F(x)$ with real time parameterization, where $F$ is holomorphic on connected open subsets of $\\mathbb{C}\\stackrel{\\sim}{=}\\mathbb{R}^2$. We describe the geometry of first-order equilibria.","For equilibria of higher orders, we establish an equivalent condition for \"definite directions\", allowing us to reverse the implication in Theorem 2 of Chapter 2.10 in [Differential equations and dynamical systems, Lawrence Perko (1990)]","under the additional condition of holomorphy.","This enables the geometric construction of a finite elliptic decomposition.","We derive a holomorphic Poincar\\'e-Bendixson-type theorem, leading to the conclusion that bounded non-periodic orbits are always homoclinic or heteroclinic."],"url":"http://arxiv.org/abs/2402.07612v1","category":"math.DS"}
{"created":"2024-02-12 12:26:42","title":"Arboreal Obstructed Atomic Insulating and Metallic Phases of Fermions","abstract":"We explore phases of free fermions on arenas that do not tessellate a manifold. Specializing to arboreal arenas described by tree graphs which possess a notion of translation symmetry, we study possible fermionic phases in the BDI symmetry class on the $p$-coordinated Bethe lattice. We find that there are $p$ distinct obstructed atomic insulating phases that are characterized by distinct edge states, pattern of entanglement, and a winding characteristic that we define here. These distinct insulting phases are always separated by a metallic region in the parameter space rather than isolated quantum critical points. The metallic region itself comprises several distinct metallic phases that are distinguished by the winding characteristic and correlation functions. The correlation functions of distinct subsystems display non-analytic behavior at distinct points in the metallic region, signaling a cascade of subsystem transitions. An intriguing feature of these arboreal metals is the presence of truncated subsystems with zero energy boundary modes despite being gapless. This work suggests new opportunities for synthetic quantum systems to realize these novel phases.","sentences":["We explore phases of free fermions on arenas that do not tessellate a manifold.","Specializing to arboreal arenas described by tree graphs which possess a notion of translation symmetry, we study possible fermionic phases in the BDI symmetry class on the $p$-coordinated Bethe lattice.","We find that there are $p$ distinct obstructed atomic insulating phases that are characterized by distinct edge states, pattern of entanglement, and a winding characteristic that we define here.","These distinct insulting phases are always separated by a metallic region in the parameter space rather than isolated quantum critical points.","The metallic region itself comprises several distinct metallic phases that are distinguished by the winding characteristic and correlation functions.","The correlation functions of distinct subsystems display non-analytic behavior at distinct points in the metallic region, signaling a cascade of subsystem transitions.","An intriguing feature of these arboreal metals is the presence of truncated subsystems with zero energy boundary modes despite being gapless.","This work suggests new opportunities for synthetic quantum systems to realize these novel phases."],"url":"http://arxiv.org/abs/2402.07608v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-12 12:24:26","title":"Is Every Product System Concrete?","abstract":"We present a reasonably good class of subsemigroups $P$ of locally compact groups such that every product system over $P$ is isomorphic to the product of an $E_0$-semigroup over $P$. Our class includes all solid Borel subsemigroups of locally compact abelian groups.","sentences":["We present a reasonably good class of subsemigroups $P$ of locally compact groups such that every product system over $P$ is isomorphic to the product of an $E_0$-semigroup over $P$. Our class includes all solid Borel subsemigroups of locally compact abelian groups."],"url":"http://arxiv.org/abs/2402.07607v1","category":"math.OA"}
{"created":"2024-02-12 12:00:40","title":"DART: A Compact Platform For Autonomous Driving Research","abstract":"This paper presents the design of a research platform for autonomous driving applications, the Delft's Autonomous-driving Robotic Testbed (DART). Our goal was to design a small-scale car-like robot equipped with all the hardware needed for on-board navigation and control while keeping it cost-effective and easy to replicate. To develop DART, we built on an existing off-the-shelf model and augmented its sensor suite to improve its capabilities for control and motion planning tasks. We detail the hardware setup and the system identification challenges to derive the vehicle's models. Furthermore, we present some use cases where we used DART to test different motion planning applications to show the versatility of the platform. Finally, we provide a git repository with all the details to replicate DART, complete with a simulation environment and the data used for system identification.","sentences":["This paper presents the design of a research platform for autonomous driving applications, the Delft's Autonomous-driving Robotic Testbed (DART).","Our goal was to design a small-scale car-like robot equipped with all the hardware needed for on-board navigation and control while keeping it cost-effective and easy to replicate.","To develop DART, we built on an existing off-the-shelf model and augmented its sensor suite to improve its capabilities for control and motion planning tasks.","We detail the hardware setup and the system identification challenges to derive the vehicle's models.","Furthermore, we present some use cases where we used DART to test different motion planning applications to show the versatility of the platform.","Finally, we provide a git repository with all the details to replicate DART, complete with a simulation environment and the data used for system identification."],"url":"http://arxiv.org/abs/2402.07602v1","category":"cs.RO"}
{"created":"2024-02-12 11:52:21","title":"Sheet Music Transformer: End-To-End Optical Music Recognition Beyond Monophonic Transcription","abstract":"State-of-the-art end-to-end Optical Music Recognition (OMR) has, to date, primarily been carried out using monophonic transcription techniques to handle complex score layouts, such as polyphony, often by resorting to simplifications or specific adaptations. Despite their efficacy, these approaches imply challenges related to scalability and limitations. This paper presents the Sheet Music Transformer, the first end-to-end OMR model designed to transcribe complex musical scores without relying solely on monophonic strategies. Our model employs a Transformer-based image-to-sequence framework that predicts score transcriptions in a standard digital music encoding format from input images. Our model has been tested on two polyphonic music datasets and has proven capable of handling these intricate music structures effectively. The experimental outcomes not only indicate the competence of the model, but also show that it is better than the state-of-the-art methods, thus contributing to advancements in end-to-end OMR transcription.","sentences":["State-of-the-art end-to-end Optical Music Recognition (OMR) has, to date, primarily been carried out using monophonic transcription techniques to handle complex score layouts, such as polyphony, often by resorting to simplifications or specific adaptations.","Despite their efficacy, these approaches imply challenges related to scalability and limitations.","This paper presents the Sheet Music Transformer, the first end-to-end OMR model designed to transcribe complex musical scores without relying solely on monophonic strategies.","Our model employs a Transformer-based image-to-sequence framework that predicts score transcriptions in a standard digital music encoding format from input images.","Our model has been tested on two polyphonic music datasets and has proven capable of handling these intricate music structures effectively.","The experimental outcomes not only indicate the competence of the model, but also show that it is better than the state-of-the-art methods, thus contributing to advancements in end-to-end OMR transcription."],"url":"http://arxiv.org/abs/2402.07596v1","category":"cs.CV"}
{"created":"2024-02-12 11:48:00","title":"Inverse source problems for coupled parabolic systems from measurements of one internal component","abstract":"This paper is devoted to the study of inverse source problems for coupled systems of heat equations with constant or spatial--dependent coupling terms and whose internal measurements involve a reduced number of observed states. The analysis is developed for two kind of systems: the first one consists of parabolic equations with zero order coupling terms (or the so-called non-self-adjoint matrix potential) and whose possibly space-dependent coefficients. The second one consists of parabolic equations with coupling in the diffusion matrix. In all configurations the source is decomposed in separate variables, where the temporal part is known and scalar, whereas the spatial dependence is an unknown vector field. This work builds on previous methodologies for the recovery of source in scalar equations and Stokes fluids, thus expanding the field to include coupled systems of second order parabolic equations. Numerical algorithms through the finite element method in 1D and 2D are performed. Several examples showing that the algorithms make possible to recover space-dependent sources.","sentences":["This paper is devoted to the study of inverse source problems for coupled systems of heat equations with constant or spatial--dependent coupling terms and whose internal measurements involve a reduced number of observed states.","The analysis is developed for two kind of systems: the first one consists of parabolic equations with zero order coupling terms (or the so-called non-self-adjoint matrix potential) and whose possibly space-dependent coefficients.","The second one consists of parabolic equations with coupling in the diffusion matrix.","In all configurations the source is decomposed in separate variables, where the temporal part is known and scalar, whereas the spatial dependence is an unknown vector field.","This work builds on previous methodologies for the recovery of source in scalar equations and Stokes fluids, thus expanding the field to include coupled systems of second order parabolic equations.","Numerical algorithms through the finite element method in 1D and 2D are performed.","Several examples showing that the algorithms make possible to recover space-dependent sources."],"url":"http://arxiv.org/abs/2402.07593v1","category":"math.OC"}
{"created":"2024-02-12 11:43:56","title":"A Big Ring on the Sky","abstract":"We present the discovery of `A Big Ring on the Sky' (BR), the second ultra-large-scale structure (uLSS) found in MgII-absorber catalogues, following the previously reported Giant Arc (GA). In cosmological terms the BR is close to the GA - at the same redshift $z \\sim 0.8$ and with a separation on the sky of only $\\sim 12^\\circ$. Two extraordinary uLSSs in such close configuration raises the possibility that together they form an even more extraordinary cosmological system. The BR is a striking circular, annulus-like, structure of diameter $\\sim 400$ Mpc (proper size, present epoch). The method of discovery is as described in the GA paper, but here using the new MgII-absorber catalogues restricted to DR16Q quasars. Using the Convex Hull of Member Spheres (CHMS) algorithm, we estimate that the annulus and inner absorbers of the BR have departures from random expectations, at the density of the control field, of up to $5.2\\sigma$. We present the discovery of the BR, assess its significance using the CHMS, Minimal Spanning Tree (MST), FilFinder and Cuzick & Edwards (CE) methods, show it in the context of the GA+BR system, and suggest some implications for the origins of uLSS and for our understanding of cosmology. For example, it may be that unusual geometric patterns, such as these uLSSs, have an origin in cosmic strings.","sentences":["We present the discovery of `A Big Ring on the Sky' (BR), the second ultra-large-scale structure (uLSS) found in MgII-absorber catalogues, following the previously reported Giant Arc (GA).","In cosmological terms the BR is close to the GA - at the same redshift $z \\sim 0.8$ and with a separation on the sky of only $\\sim 12^\\circ$. Two extraordinary uLSSs in such close configuration raises the possibility that together they form an even more extraordinary cosmological system.","The BR is a striking circular, annulus-like, structure of diameter $\\sim 400$ Mpc (proper size, present epoch).","The method of discovery is as described in the GA paper, but here using the new MgII-absorber catalogues restricted to DR16Q quasars.","Using the Convex Hull of Member Spheres (CHMS) algorithm, we estimate that the annulus and inner absorbers of the BR have departures from random expectations, at the density of the control field, of up to $5.2\\sigma$. We present the discovery of the BR, assess its significance using the CHMS, Minimal Spanning Tree (MST), FilFinder and Cuzick & Edwards (CE) methods, show it in the context of the GA+BR system, and suggest some implications for the origins of uLSS and for our understanding of cosmology.","For example, it may be that unusual geometric patterns, such as these uLSSs, have an origin in cosmic strings."],"url":"http://arxiv.org/abs/2402.07591v1","category":"astro-ph.CO"}
{"created":"2024-02-12 11:39:37","title":"Magnetized strangelets with anomalous magnetic moment and Coulomb interactions","abstract":"We study the magnetized strangelets in the baryon density-dependent quark mass model, including the effects of both confinement and lead-order perturbation interactions. The properties of magnetized strangelets are investigated under the the field strength 2*10^17 G, where the anisotropy caused by the strong magnetic field is insignificant can be treated approximately as an isotropic system. The consideration of anomalous magnetic moments in the energy spectrum naturally solves the difficulty of infrared divergence encountered in integrating the density of states. The Coulomb interaction is accounted for a self-consistent treatment. The energy per baryon, mechanically stable radius, strangeness and electric charge of magnetized strangelets are presented, where their dependence on the field strength and parameter of confinement and perturbation are investigated.","sentences":["We study the magnetized strangelets in the baryon density-dependent quark mass model, including the effects of both confinement and lead-order perturbation interactions.","The properties of magnetized strangelets are investigated under the the field strength 2*10^17 G, where the anisotropy caused by the strong magnetic field is insignificant can be treated approximately as an isotropic system.","The consideration of anomalous magnetic moments in the energy spectrum naturally solves the difficulty of infrared divergence encountered in integrating the density of states.","The Coulomb interaction is accounted for a self-consistent treatment.","The energy per baryon, mechanically stable radius, strangeness and electric charge of magnetized strangelets are presented, where their dependence on the field strength and parameter of confinement and perturbation are investigated."],"url":"http://arxiv.org/abs/2402.07587v1","category":"hep-ph"}
{"created":"2024-02-12 11:35:25","title":"Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning","abstract":"In the evolving field of machine learning, ensuring fairness has become a critical concern, prompting the development of algorithms designed to mitigate discriminatory outcomes in decision-making processes. However, achieving fairness in the presence of group-specific concept drift remains an unexplored frontier, and our research represents pioneering efforts in this regard. Group-specific concept drift refers to situations where one group experiences concept drift over time while another does not, leading to a decrease in fairness even if accuracy remains fairly stable. Within the framework of federated learning, where clients collaboratively train models, its distributed nature further amplifies these challenges since each client can experience group-specific concept drift independently while still sharing the same underlying concept, creating a complex and dynamic environment for maintaining fairness. One of the significant contributions of our research is the formalization and introduction of the problem of group-specific concept drift and its distributed counterpart, shedding light on its critical importance in the realm of fairness. In addition, leveraging insights from prior research, we adapt an existing distributed concept drift adaptation algorithm to tackle group-specific distributed concept drift which utilizes a multi-model approach, a local group-specific drift detection mechanism, and continuous clustering of models over time. The findings from our experiments highlight the importance of addressing group-specific concept drift and its distributed counterpart to advance fairness in machine learning.","sentences":["In the evolving field of machine learning, ensuring fairness has become a critical concern, prompting the development of algorithms designed to mitigate discriminatory outcomes in decision-making processes.","However, achieving fairness in the presence of group-specific concept drift remains an unexplored frontier, and our research represents pioneering efforts in this regard.","Group-specific concept drift refers to situations where one group experiences concept drift over time while another does not, leading to a decrease in fairness even if accuracy remains fairly stable.","Within the framework of federated learning, where clients collaboratively train models, its distributed nature further amplifies these challenges since each client can experience group-specific concept drift independently while still sharing the same underlying concept, creating a complex and dynamic environment for maintaining fairness.","One of the significant contributions of our research is the formalization and introduction of the problem of group-specific concept drift and its distributed counterpart, shedding light on its critical importance in the realm of fairness.","In addition, leveraging insights from prior research, we adapt an existing distributed concept drift adaptation algorithm to tackle group-specific distributed concept drift which utilizes a multi-model approach, a local group-specific drift detection mechanism, and continuous clustering of models over time.","The findings from our experiments highlight the importance of addressing group-specific concept drift and its distributed counterpart to advance fairness in machine learning."],"url":"http://arxiv.org/abs/2402.07586v1","category":"cs.LG"}
{"created":"2024-02-12 11:35:04","title":"Identifying architectural design decisions for achieving green ML serving","abstract":"The growing use of large machine learning models highlights concerns about their increasing computational demands. While the energy consumption of their training phase has received attention, fewer works have considered the inference phase. For ML inference, the binding of ML models to the ML system for user access, known as ML serving, is a critical yet understudied step for achieving efficiency in ML applications.   We examine the literature in ML architectural design decisions and Green AI, with a special focus on ML serving. The aim is to analyze ML serving architectural design decisions for the purpose of understanding and identifying them with respect to quality characteristics from the point of view of researchers and practitioners in the context of ML serving literature.   Our results (i) identify ML serving architectural design decisions along with their corresponding components and associated technological stack, and (ii) provide an overview of the quality characteristics studied in the literature, including energy efficiency.   This preliminary study is the first step in our goal to achieve green ML serving. Our analysis may aid ML researchers and practitioners in making green-aware architecture design decisions when serving their models.","sentences":["The growing use of large machine learning models highlights concerns about their increasing computational demands.","While the energy consumption of their training phase has received attention, fewer works have considered the inference phase.","For ML inference, the binding of ML models to the ML system for user access, known as ML serving, is a critical yet understudied step for achieving efficiency in ML applications.   ","We examine the literature in ML architectural design decisions and Green AI, with a special focus on ML serving.","The aim is to analyze ML serving architectural design decisions for the purpose of understanding and identifying them with respect to quality characteristics from the point of view of researchers and practitioners in the context of ML serving literature.   ","Our results (i) identify ML serving architectural design decisions along with their corresponding components and associated technological stack, and (ii) provide an overview of the quality characteristics studied in the literature, including energy efficiency.   ","This preliminary study is the first step in our goal to achieve green ML serving.","Our analysis may aid ML researchers and practitioners in making green-aware architecture design decisions when serving their models."],"url":"http://arxiv.org/abs/2402.07585v1","category":"cs.LG"}
{"created":"2024-02-12 11:34:42","title":"Privacy-Optimized Randomized Response for Sharing Multi-Attribute Data","abstract":"With the increasing amount of data in society, privacy concerns in data sharing have become widely recognized. Particularly, protecting personal attribute information is essential for a wide range of aims from crowdsourcing to realizing personalized medicine. Although various differentially private methods based on randomized response have been proposed for single attribute information or specific analysis purposes such as frequency estimation, there is a lack of studies on the mechanism for sharing individuals' multiple categorical information itself. The existing randomized response for sharing multi-attribute data uses the Kronecker product to perturb each attribute information in turn according to the respective privacy level but achieves only a weak privacy level for the entire dataset. Therefore, in this study, we propose a privacy-optimized randomized response that guarantees the strongest privacy in sharing multi-attribute data. Furthermore, we present an efficient heuristic algorithm for constructing a near-optimal mechanism. The time complexity of our algorithm is O(k^2), where k is the number of attributes, and it can be performed in about 1 second even for large datasets with k = 1,000. The experimental results demonstrate that both of our methods provide significantly stronger privacy guarantees for the entire dataset than the existing method. In addition, we show an analysis example using genome statistics to confirm that our methods can achieve less than half the output error compared with that of the existing method. Overall, this study is an important step toward trustworthy sharing and analysis of multi-attribute data. The Python implementation of our experiments and supplemental results are available at https://github.com/ay0408/Optimized-RR.","sentences":["With the increasing amount of data in society, privacy concerns in data sharing have become widely recognized.","Particularly, protecting personal attribute information is essential for a wide range of aims from crowdsourcing to realizing personalized medicine.","Although various differentially private methods based on randomized response have been proposed for single attribute information or specific analysis purposes such as frequency estimation, there is a lack of studies on the mechanism for sharing individuals' multiple categorical information itself.","The existing randomized response for sharing multi-attribute data uses the Kronecker product to perturb each attribute information in turn according to the respective privacy level but achieves only a weak privacy level for the entire dataset.","Therefore, in this study, we propose a privacy-optimized randomized response that guarantees the strongest privacy in sharing multi-attribute data.","Furthermore, we present an efficient heuristic algorithm for constructing a near-optimal mechanism.","The time complexity of our algorithm is O(k^2), where k is the number of attributes, and it can be performed in about 1 second even for large datasets with k = 1,000.","The experimental results demonstrate that both of our methods provide significantly stronger privacy guarantees for the entire dataset than the existing method.","In addition, we show an analysis example using genome statistics to confirm that our methods can achieve less than half the output error compared with that of the existing method.","Overall, this study is an important step toward trustworthy sharing and analysis of multi-attribute data.","The Python implementation of our experiments and supplemental results are available at https://github.com/ay0408/Optimized-RR."],"url":"http://arxiv.org/abs/2402.07584v1","category":"cs.CR"}
{"created":"2024-02-12 11:21:25","title":"Superconductivity in new family of Rhenium-based binary alloys: Re$_{7}$X$_{3}$ (X = Nb, Ta, Ti, Zr, Hf)","abstract":"Rhenium-based superconductors have recently attracted significant interest due to their unconventional superconducting properties. In this work, we report the synthesis and properties of new superconducting Re$_{7}$X$_{3}$ (X = Nb, Ta, Ti, Zr, Hf) binary alloys which maintain a fixed composition of rhenium while crystallizing in centrosymmetric to non-centrosymmetric crystal structures, depending on the elements of the X site. Comprehensive structural and superconducting properties were investigated using powder x-ray diffraction, AC transport, magnetization, and specific heat measurements, and on the basis of these measurements, the superconducting phase diagram was constructed. The results suggest a complex interplay of crystal structure and the Re/X ratio, which governs the strength of spin-orbital coupling and controls the unconventional superconducting behavior in Re-based superconductors.","sentences":["Rhenium-based superconductors have recently attracted significant interest due to their unconventional superconducting properties.","In this work, we report the synthesis and properties of new superconducting Re$_{7}$X$_{3}$ (X = Nb, Ta, Ti, Zr, Hf) binary alloys which maintain a fixed composition of rhenium while crystallizing in centrosymmetric to non-centrosymmetric crystal structures, depending on the elements of the X site.","Comprehensive structural and superconducting properties were investigated using powder x-ray diffraction, AC transport, magnetization, and specific heat measurements, and on the basis of these measurements, the superconducting phase diagram was constructed.","The results suggest a complex interplay of crystal structure and the Re/X ratio, which governs the strength of spin-orbital coupling and controls the unconventional superconducting behavior in Re-based superconductors."],"url":"http://arxiv.org/abs/2402.07580v1","category":"cond-mat.supr-con"}
{"created":"2024-02-12 11:20:22","title":"LFOC: A Lightweight Fairness-Oriented Cache Clustering Policy for Commodity Multicores","abstract":"Multicore processors constitute the main architecture choice for modern computing systems in different market segments. Despite their benefits, the contention that naturally appears when multiple applications compete for the use of shared resources among cores, such as the last-level cache (LLC), may lead to substantial performance degradation. This may have a negative impact on key system aspects such as throughput and fairness. Assigning the various applications in the workload to separate LLC partitions with possibly different sizes, has been proven effective to mitigate shared-resource contention effects.   In this article we propose LFOC, a clustering-based cache partitioning scheme that strives to deliver fairness while providing acceptable system throughput. LFOC leverages the Intel Cache Allocation Technology (CAT), which enables the system software to divide the LLC into different partitions. To accomplish its goals, LFOC tries to mimic the behavior of the optimal cache-clustering solution, which we could approximate by means of a simulator in different scenarios. To this end, LFOC effectively identifies streaming aggressor programs and cache sensitive applications, which are then assigned to separate cache partitions.   We implemented LFOC in the Linux kernel and evaluated it on a real system featuring an Intel Skylake processor, where we compare its effectiveness to that of two state-of-the-art policies that optimize fairness and throughput, respectively. Our experimental analysis reveals that LFOC is able to bring a higher reduction in unfairness by leveraging a lightweight algorithm suitable for adoption in a real OS.","sentences":["Multicore processors constitute the main architecture choice for modern computing systems in different market segments.","Despite their benefits, the contention that naturally appears when multiple applications compete for the use of shared resources among cores, such as the last-level cache (LLC), may lead to substantial performance degradation.","This may have a negative impact on key system aspects such as throughput and fairness.","Assigning the various applications in the workload to separate LLC partitions with possibly different sizes, has been proven effective to mitigate shared-resource contention effects.   ","In this article we propose LFOC, a clustering-based cache partitioning scheme that strives to deliver fairness while providing acceptable system throughput.","LFOC leverages the Intel Cache Allocation Technology (CAT), which enables the system software to divide the LLC into different partitions.","To accomplish its goals, LFOC tries to mimic the behavior of the optimal cache-clustering solution, which we could approximate by means of a simulator in different scenarios.","To this end, LFOC effectively identifies streaming aggressor programs and cache sensitive applications, which are then assigned to separate cache partitions.   ","We implemented LFOC in the Linux kernel and evaluated it on a real system featuring an Intel Skylake processor, where we compare its effectiveness to that of two state-of-the-art policies that optimize fairness and throughput, respectively.","Our experimental analysis reveals that LFOC is able to bring a higher reduction in unfairness by leveraging a lightweight algorithm suitable for adoption in a real OS."],"url":"http://arxiv.org/abs/2402.07578v1","category":"cs.DC"}
{"created":"2024-02-12 11:14:47","title":"Flux-tunable Kitaev chain in a quantum dot array","abstract":"Connecting quantum dots through Andreev bound states in a semiconductor-superconductor hybrid provides a platform to create a Kitaev chain. Interestingly, in a double quantum dot, a pair of poor man's Majorana zero modes can emerge when the system is fine-tuned to a sweet spot, where superconducting and normal couplings are equal in magnitude. Control of the Andreev bound states is crucial for achieving this, usually implemented by varying its chemical potential. In this work, we propose using Andreev bound states in a narrow Josephson junction to mediate both types of couplings, with the ratio tunable by the phase difference across the junction. Now a minimal Kitaev chain can be easily tuned into the strong coupling regime by varying the phase and junction asymmetry, even without changing the dot-hybrid coupling strength. Furthermore, we identify an optimal sweet spot at $\\pi$ phase, enhancing the excitation gap and robustness against phase fluctuations. Our proposal introduces a new device platform and a new tuning method for realizing quantum-dot-based Kitaev chains.","sentences":["Connecting quantum dots through Andreev bound states in a semiconductor-superconductor hybrid provides a platform to create a Kitaev chain.","Interestingly, in a double quantum dot, a pair of poor man's Majorana zero modes can emerge when the system is fine-tuned to a sweet spot, where superconducting and normal couplings are equal in magnitude.","Control of the Andreev bound states is crucial for achieving this, usually implemented by varying its chemical potential.","In this work, we propose using Andreev bound states in a narrow Josephson junction to mediate both types of couplings, with the ratio tunable by the phase difference across the junction.","Now a minimal Kitaev chain can be easily tuned into the strong coupling regime by varying the phase and junction asymmetry, even without changing the dot-hybrid coupling strength.","Furthermore, we identify an optimal sweet spot at $\\pi$ phase, enhancing the excitation gap and robustness against phase fluctuations.","Our proposal introduces a new device platform and a new tuning method for realizing quantum-dot-based Kitaev chains."],"url":"http://arxiv.org/abs/2402.07575v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-12 11:06:29","title":"Room-temperature optically detected coherent control of molecular spins","abstract":"Benefiting from both molecular tunability and versatile methods for deployment, optically interfaced molecular spins are a promising platform for quantum technologies such as sensing and imaging. Room-temperature optically detected coherent spin control is a key enabler for many applications, combining sensitive readout, versatile spin manipulation, and ambient operation. Here we demonstrate such functionality in a molecular spin system. Using the photoexcited triplet state of organic chromophores (pentacene doped in a para-terphenyl host), we optically detect coherent spin manipulation with photoluminescence contrasts exceeding 10% and microsecond coherence times at room temperature. We further demonstrate how coherent control of multiple triplet sublevels can significantly enhance optical spin contrast, and extend optically detected coherent control to a thermally evaporated thin film, retaining high photoluminescence contrast and coherence times of order one microsecond. These results open opportunities for room-temperature quantum technologies that can be systematically tailored through synthetic chemistry.","sentences":["Benefiting from both molecular tunability and versatile methods for deployment, optically interfaced molecular spins are a promising platform for quantum technologies such as sensing and imaging.","Room-temperature optically detected coherent spin control is a key enabler for many applications, combining sensitive readout, versatile spin manipulation, and ambient operation.","Here we demonstrate such functionality in a molecular spin system.","Using the photoexcited triplet state of organic chromophores (pentacene doped in a para-terphenyl host), we optically detect coherent spin manipulation with photoluminescence contrasts exceeding 10% and microsecond coherence times at room temperature.","We further demonstrate how coherent control of multiple triplet sublevels can significantly enhance optical spin contrast, and extend optically detected coherent control to a thermally evaporated thin film, retaining high photoluminescence contrast and coherence times of order one microsecond.","These results open opportunities for room-temperature quantum technologies that can be systematically tailored through synthetic chemistry."],"url":"http://arxiv.org/abs/2402.07572v1","category":"quant-ph"}
{"created":"2024-02-12 11:01:37","title":"Stability of traveling waves in a nonlinear hyperbolic system approximating a dimer array of oscillators","abstract":"We study a semilinear hyperbolic system of PDEs which arises as a continuum approximation of the discrete nonlinear dimer array model introduced by Hadad, Vitelli and Alu (HVA) in \\cite{HVA17}. We classify the system's traveling waves, and study their stability properties. We focus on traveling pulse solutions (``solitons'') on a nontrivial background and moving domain wall solutions (kinks); both arise as heteroclinic connections between spatially uniform equilibrium of a reduced dynamical system. We present analytical results on: nonlinear stability and spectral stability of supersonic pulses, and spectral stability of moving domain walls. Our stability results are in terms of weighted $H^1$ norms of the perturbation, which capture the phenomenon of {\\it convective stabilization}; as time advances, the traveling wave ``outruns'' the \\underline{growing} disturbance excited by an initial perturbation; the non-trivial spatially uniform equilibria are linearly exponentially unstable. We use our analytical results to interpret phenomena observed in numerical simulations.","sentences":["We study a semilinear hyperbolic system of PDEs which arises as a continuum approximation of the discrete nonlinear dimer array model introduced by Hadad, Vitelli and Alu (HVA) in \\cite{HVA17}.","We classify the system's traveling waves, and study their stability properties.","We focus on traveling pulse solutions (``solitons'') on a nontrivial background and moving domain wall solutions (kinks); both arise as heteroclinic connections between spatially uniform equilibrium of a reduced dynamical system.","We present analytical results on: nonlinear stability and spectral stability of supersonic pulses, and spectral stability of moving domain walls.","Our stability results are in terms of weighted $H^1$ norms of the perturbation, which capture the phenomenon of {\\it convective stabilization}; as time advances, the traveling wave ``outruns'' the \\underline{growing} disturbance excited by an initial perturbation; the non-trivial spatially uniform equilibria are linearly exponentially unstable.","We use our analytical results to interpret phenomena observed in numerical simulations."],"url":"http://arxiv.org/abs/2402.07567v1","category":"nlin.PS"}
{"created":"2024-02-12 11:01:07","title":"The DarkSide-20k experiment","abstract":"The DarkSide-20k experiment represents the present goal of the Global Argon Dark Matter Collaboration program. Bringing together the experience from previous argon-based detectors, as well as the knowledge gained on large volume membrane cryostats developed within the DUNE program, the community is now building a dual-phase LAr-TPC equipped with SiPM arrays for light readout. The main goal of the experiment is to discover or to extend the current sensitivity limits on the search for dark matter WIMP-like particles. Currently, the experiment has entered the construction phase and the external cryostat is being put in place at Laboratori Nazionali del Gran Sasso (LNGS), Italy. Detector construction will follow, and data taking is expected to start in late 2026. This contribution will introduce the DarkSide detector and goals, and it will report on the ongoing construction of the underground infrastructure at LNGS. Finally, it will concentrate on the current activities on large arrays of silicon light detectors, that are at the base of the construction of the detector light readout system.","sentences":["The DarkSide-20k experiment represents the present goal of the Global Argon Dark Matter Collaboration program.","Bringing together the experience from previous argon-based detectors, as well as the knowledge gained on large volume membrane cryostats developed within the DUNE program, the community is now building a dual-phase LAr-TPC equipped with SiPM arrays for light readout.","The main goal of the experiment is to discover or to extend the current sensitivity limits on the search for dark matter WIMP-like particles.","Currently, the experiment has entered the construction phase and the external cryostat is being put in place at Laboratori Nazionali del Gran Sasso (LNGS), Italy.","Detector construction will follow, and data taking is expected to start in late 2026.","This contribution will introduce the DarkSide detector and goals, and it will report on the ongoing construction of the underground infrastructure at LNGS.","Finally, it will concentrate on the current activities on large arrays of silicon light detectors, that are at the base of the construction of the detector light readout system."],"url":"http://arxiv.org/abs/2402.07566v1","category":"physics.ins-det"}
{"created":"2024-02-12 10:56:47","title":"Joint User and Beam Selection in Millimeter Wave Networks","abstract":"We study the problem of selecting a user equipment (UE) and a beam for each access point (AP) for concurrent transmissions in a millimeter wave (mmWave) network, such that the sum of weighted rates of UEs is maximized. We prove that this problem is NP-complete. We propose two algorithms -- Markov Chain Monte Carlo (MCMC) based and local interaction game (LIG) based UE and beam selection -- and prove that both of them asymptotically achieve the optimal solution. Also, we propose two fast greedy algorithms -- NGUB1 and NGUB2 -- for UE and beam selection. Through extensive simulations, we show that our proposed greedy algorithms outperform the most relevant algorithms proposed in prior work and perform close to the asymptotically optimal algorithms.","sentences":["We study the problem of selecting a user equipment (UE) and a beam for each access point (AP) for concurrent transmissions in a millimeter wave (mmWave) network, such that the sum of weighted rates of UEs is maximized.","We prove that this problem is NP-complete.","We propose two algorithms -- Markov Chain Monte Carlo (MCMC) based and local interaction game (LIG) based UE and beam selection -- and prove that both of them asymptotically achieve the optimal solution.","Also, we propose two fast greedy algorithms -- NGUB1 and NGUB2 -- for UE and beam selection.","Through extensive simulations, we show that our proposed greedy algorithms outperform the most relevant algorithms proposed in prior work and perform close to the asymptotically optimal algorithms."],"url":"http://arxiv.org/abs/2402.07563v1","category":"eess.SY"}
{"created":"2024-02-12 10:54:57","title":"A Reinforcement Learning Approach to the Design of Quantum Chains for Optimal Energy Transfer","abstract":"We propose a bottom-up approach, based on Reinforcement Learning, to the design of a chain achieving efficient excitation-transfer performances. We assume distance-dependent interactions among particles arranged in a chain under tight-binding conditions. Starting from two particles and a localised excitation, we gradually increase the number of constitutents of the system so as to improve the transfer probability. We formulate the problem of finding the optimal locations and numbers of particles as a Markov Decision Process: we use Proximal Policy Optimization to find the optimal chain-building policies and the optimal chain configurations under different scenarios. We consider both the case in which the target is a sink connected to the end of the chain and the case in which the target is the right-most particle in the chain. We address the problem of disorder in the chain induced by particle positioning errors. We are able to achieve extremely high excitation transfer in all cases, with different chain configurations and properties depending on the specific conditions.","sentences":["We propose a bottom-up approach, based on Reinforcement Learning, to the design of a chain achieving efficient excitation-transfer performances.","We assume distance-dependent interactions among particles arranged in a chain under tight-binding conditions.","Starting from two particles and a localised excitation, we gradually increase the number of constitutents of the system so as to improve the transfer probability.","We formulate the problem of finding the optimal locations and numbers of particles as a Markov Decision Process: we use Proximal Policy Optimization to find the optimal chain-building policies and the optimal chain configurations under different scenarios.","We consider both the case in which the target is a sink connected to the end of the chain and the case in which the target is the right-most particle in the chain.","We address the problem of disorder in the chain induced by particle positioning errors.","We are able to achieve extremely high excitation transfer in all cases, with different chain configurations and properties depending on the specific conditions."],"url":"http://arxiv.org/abs/2402.07561v1","category":"quant-ph"}
{"created":"2024-02-12 10:39:17","title":"Digital Twins Below the Surface: Enhancing Underwater Teleoperation","abstract":"Subsea exploration, inspection, and intervention operations heavily rely on remotely operated vehicles (ROVs). However, the inherent complexity of the underwater environment presents significant challenges to the operators of these vehicles. This paper delves into the challenges associated with navigation and maneuvering tasks in the teleoperation of ROVs, such as reduced situational awareness and heightened teleoperator workload. To address these challenges, we introduce an underwater Digital Twin (DT) system designed to enhance underwater teleoperation, enable autonomous navigation, support system monitoring, and facilitate system testing through simulation. Our approach involves a dynamic representation of the underwater robot and its environment using desktop virtual reality, as well as the integration of mapping, localization, path planning and simulation capabilities within the DT system. Our research demonstrates the system's adaptability, versatility and feasibility, highlighting significant challenges and, in turn, improving the teleoperators' situational awareness and reducing their workload.","sentences":["Subsea exploration, inspection, and intervention operations heavily rely on remotely operated vehicles (ROVs).","However, the inherent complexity of the underwater environment presents significant challenges to the operators of these vehicles.","This paper delves into the challenges associated with navigation and maneuvering tasks in the teleoperation of ROVs, such as reduced situational awareness and heightened teleoperator workload.","To address these challenges, we introduce an underwater Digital Twin (DT) system designed to enhance underwater teleoperation, enable autonomous navigation, support system monitoring, and facilitate system testing through simulation.","Our approach involves a dynamic representation of the underwater robot and its environment using desktop virtual reality, as well as the integration of mapping, localization, path planning and simulation capabilities within the DT system.","Our research demonstrates the system's adaptability, versatility and feasibility, highlighting significant challenges and, in turn, improving the teleoperators' situational awareness and reducing their workload."],"url":"http://arxiv.org/abs/2402.07556v1","category":"cs.RO"}
{"created":"2024-02-12 10:30:45","title":"A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit for Analog In-Memory Computing","abstract":"Analog In-Memory Computing (AIMC) is an emerging technology for fast and energy-efficient Deep Learning (DL) inference. However, a certain amount of digital post-processing is required to deal with circuit mismatches and non-idealities associated with the memory devices. Efficient near-memory digital logic is critical to retain the high area/energy efficiency and low latency of AIMC. Existing systems adopt Floating Point 16 (FP16) arithmetic with limited parallelization capability and high latency. To overcome these limitations, we propose a Near-Memory digital Processing Unit (NMPU) based on fixed-point arithmetic. It achieves competitive accuracy and higher computing throughput than previous approaches while minimizing the area overhead. Moreover, the NMPU supports standard DL activation steps, such as ReLU and Batch Normalization. We perform a physical implementation of the NMPU design in a 14 nm CMOS technology and provide detailed performance, power, and area assessments. We validate the efficacy of the NMPU by using data from an AIMC chip and demonstrate that a simulated AIMC system with the proposed NMPU outperforms existing FP16-based implementations, providing 139$\\times$ speed-up, 7.8$\\times$ smaller area, and a competitive power consumption. Additionally, our approach achieves an inference accuracy of 86.65 %/65.06 %, with an accuracy drop of just 0.12 %/0.4 % compared to the FP16 baseline when benchmarked with ResNet9/ResNet32 networks trained on the CIFAR10/CIFAR100 datasets, respectively.","sentences":["Analog In-Memory Computing (AIMC) is an emerging technology for fast and energy-efficient Deep Learning (DL) inference.","However, a certain amount of digital post-processing is required to deal with circuit mismatches and non-idealities associated with the memory devices.","Efficient near-memory digital logic is critical to retain the high area/energy efficiency and low latency of AIMC.","Existing systems adopt Floating Point 16 (FP16) arithmetic with limited parallelization capability and high latency.","To overcome these limitations, we propose a Near-Memory digital Processing Unit (NMPU) based on fixed-point arithmetic.","It achieves competitive accuracy and higher computing throughput than previous approaches while minimizing the area overhead.","Moreover, the NMPU supports standard DL activation steps, such as ReLU and Batch Normalization.","We perform a physical implementation of the NMPU design in a 14 nm CMOS technology and provide detailed performance, power, and area assessments.","We validate the efficacy of the NMPU by using data from an AIMC chip and demonstrate that a simulated AIMC system with the proposed NMPU outperforms existing FP16-based implementations, providing 139$\\times$ speed-up, 7.8$\\times$ smaller area, and a competitive power consumption.","Additionally, our approach achieves an inference accuracy of 86.65 %/65.06","%, with an accuracy drop of just 0.12 %/0.4","% compared to the FP16 baseline when benchmarked with ResNet9/ResNet32 networks trained on the CIFAR10/CIFAR100 datasets, respectively."],"url":"http://arxiv.org/abs/2402.07549v1","category":"cs.AR"}
{"created":"2024-02-12 10:28:59","title":"NOMAD CAMELS: Configurable Application for Measurements, Experiments and Laboratory Systems","abstract":"NOMAD CAMELS (short: CAMELS) is a configurable, open-source measurement software that records fully self-describing experimental data. It has its origins in the field of experimental physics where a wide variety of measurement instruments are used in frequently changing experimental setups and measurement protocols. CAMELS provides a graphical user interface (GUI) which allows the user to configure experiments without the need of programming skills or deep understanding of instrument communication. CAMELS translates user-defined measurement protocols into stand-alone executable Python code for full transparency of the actual measurement sequences. Existing large-scale, distributed control systems using e.g. EPICS can be natively implemented. CAMELS is designed with focus on full recording of data and metadata. When shared with others, data produced with CAMELS allow full understanding of the measurement and the resulting data in accordance with the FAIR (Findable, Accessible, Interoperable and Re-usable) principles.","sentences":["NOMAD CAMELS (short: CAMELS) is a configurable, open-source measurement software that records fully self-describing experimental data.","It has its origins in the field of experimental physics where a wide variety of measurement instruments are used in frequently changing experimental setups and measurement protocols.","CAMELS provides a graphical user interface (GUI) which allows the user to configure experiments without the need of programming skills or deep understanding of instrument communication.","CAMELS translates user-defined measurement protocols into stand-alone executable Python code for full transparency of the actual measurement sequences.","Existing large-scale, distributed control systems using e.g. EPICS can be natively implemented.","CAMELS is designed with focus on full recording of data and metadata.","When shared with others, data produced with CAMELS allow full understanding of the measurement and the resulting data in accordance with the FAIR (Findable, Accessible, Interoperable and Re-usable) principles."],"url":"http://arxiv.org/abs/2402.07548v1","category":"physics.ins-det"}
{"created":"2024-02-12 10:16:55","title":"Agro-ecological control of a pest-host system: optimizing the harvest","abstract":"We delve into the interactions between a prey-predator and a vector-borne epidemic system, driven by agro-ecological motivations. This system involves an ODE, two reaction--diffusion PDEs and one reaction--diffusion--advection PDE. It has no complete variational or monotonic structure and features spatially heterogeneous coefficients. Our initial focus is to examine the continuity of a quantity known as ''harvest'', which depends on the time-integral of infected vectors. We analyze its asymptotic behaviour as the domain becomes homogeneous. Then we tackle a non-standard optimal control problem related to the linearized harvest and conduct an analysis to establish the existence, uniqueness, and properties of optimizers. Finally, we refine the location of optimizers under specific initial conditions.","sentences":["We delve into the interactions between a prey-predator and a vector-borne epidemic system, driven by agro-ecological motivations.","This system involves an ODE, two reaction--diffusion PDEs and one reaction--diffusion--advection PDE.","It has no complete variational or monotonic structure and features spatially heterogeneous coefficients.","Our initial focus is to examine the continuity of a quantity known as ''harvest'', which depends on the time-integral of infected vectors.","We analyze its asymptotic behaviour as the domain becomes homogeneous.","Then we tackle a non-standard optimal control problem related to the linearized harvest and conduct an analysis to establish the existence, uniqueness, and properties of optimizers.","Finally, we refine the location of optimizers under specific initial conditions."],"url":"http://arxiv.org/abs/2402.07546v1","category":"math.AP"}
{"created":"2024-02-12 10:15:32","title":"Line-of-sight Cox percolation on Poisson-Delaunay triangulation","abstract":"In this work, percolation properties of device-to-device (D2D) networks in urban environments are investigated. The street system is modeled by a Poisson-Delaunay triangulation (PDT). Users are of two types: given either by a Cox process supported by the edges of the PDT or by a Bernoulli process on the vertices of the PDT (i.e. on streets and at crossroads). Percolation of the resulting connectivity graph G p,$\\lambda$,r is interpreted as long-range connection in the D2D network. According to the parameters p, $\\lambda$, r of the model, we state several percolation regimes in Theorem 1 (see also Fig. 3). This work completes and specifies results of Le Gall et al [23]. To do it, we take advantage of a percolation tool, inspired by enhancement techniques, used to our knowledge for the first time in the context of communication networks.","sentences":["In this work, percolation properties of device-to-device (D2D) networks in urban environments are investigated.","The street system is modeled by a Poisson-Delaunay triangulation (PDT).","Users are of two types: given either by a Cox process supported by the edges of the PDT or by a Bernoulli process on the vertices of the PDT (i.e. on streets and at crossroads).","Percolation of the resulting connectivity graph G p,$\\lambda$,r is interpreted as long-range connection in the D2D network.","According to the parameters p, $\\lambda$, r of the model, we state several percolation regimes in Theorem 1 (see also Fig. 3).","This work completes and specifies results of Le Gall","et al","[23].","To do it, we take advantage of a percolation tool, inspired by enhancement techniques, used to our knowledge for the first time in the context of communication networks."],"url":"http://arxiv.org/abs/2402.07544v1","category":"math.PR"}
{"created":"2024-02-12 09:57:47","title":"Accelerating Distributed Deep Learning using Lossless Homomorphic Compression","abstract":"As deep neural networks (DNNs) grow in complexity and size, the resultant increase in communication overhead during distributed training has become a significant bottleneck, challenging the scalability of distributed training systems. Existing solutions, while aiming to mitigate this bottleneck through worker-level compression and in-network aggregation, fall short due to their inability to efficiently reconcile the trade-offs between compression effectiveness and computational overhead, hindering overall performance and scalability. In this paper, we introduce a novel compression algorithm that effectively merges worker-level compression with in-network aggregation. Our solution is both homomorphic, allowing for efficient in-network aggregation without CPU/GPU processing, and lossless, ensuring no compromise on training accuracy. Theoretically optimal in compression and computational efficiency, our approach is empirically validated across diverse DNN models such as NCF, LSTM, VGG19, and BERT-base, showing up to a 6.33$\\times$ improvement in aggregation throughput and a 3.74$\\times$ increase in per-iteration training speed.","sentences":["As deep neural networks (DNNs) grow in complexity and size, the resultant increase in communication overhead during distributed training has become a significant bottleneck, challenging the scalability of distributed training systems.","Existing solutions, while aiming to mitigate this bottleneck through worker-level compression and in-network aggregation, fall short due to their inability to efficiently reconcile the trade-offs between compression effectiveness and computational overhead, hindering overall performance and scalability.","In this paper, we introduce a novel compression algorithm that effectively merges worker-level compression with in-network aggregation.","Our solution is both homomorphic, allowing for efficient in-network aggregation without CPU/GPU processing, and lossless, ensuring no compromise on training accuracy.","Theoretically optimal in compression and computational efficiency, our approach is empirically validated across diverse DNN models such as NCF, LSTM, VGG19, and BERT-base, showing up to a 6.33$\\times$ improvement in aggregation throughput and a 3.74$\\times$ increase in per-iteration training speed."],"url":"http://arxiv.org/abs/2402.07529v1","category":"cs.DC"}
{"created":"2024-02-12 09:49:56","title":"Morse sequences","abstract":"We introduce the notion of a Morse sequence, which provides a simple and effective approach to discrete Morse theory. A Morse sequence is a sequence composed solely of two elementary operations, that is, expansions (the inverse of a collapse), and fillings (the inverse of a perforation). We show that a Morse sequence may be seen as an alternative way to represent the gradient vector field of an arbitrary discrete Morse function. We also show that it is possible, in a straightforward manner, to make a link between Morse sequences and different kinds of Morse functions. At last, we introduce maximal Morse sequences, which formalize two basic schemes for building a Morse sequence from an arbitrary simplicial complex.","sentences":["We introduce the notion of a Morse sequence, which provides a simple and effective approach to discrete Morse theory.","A Morse sequence is a sequence composed solely of two elementary operations, that is, expansions (the inverse of a collapse), and fillings (the inverse of a perforation).","We show that a Morse sequence may be seen as an alternative way to represent the gradient vector field of an arbitrary discrete Morse function.","We also show that it is possible, in a straightforward manner, to make a link between Morse sequences and different kinds of Morse functions.","At last, we introduce maximal Morse sequences, which formalize two basic schemes for building a Morse sequence from an arbitrary simplicial complex."],"url":"http://arxiv.org/abs/2402.07526v1","category":"cs.CV"}
{"created":"2024-02-12 09:46:31","title":"Semialgebraicity of the convergence domain of an algebraic power series","abstract":"Given a power series in finitely many variables that is algebraic over the corresponding polynomial ring over a subfield of the reals, we show that its convergence domain is semialgebraic over the real closure of the subfield. This gives in particular that the convergence radius of a univariate Puiseux series that is algebraic in the above sense belongs to the real closure or is infinity.","sentences":["Given a power series in finitely many variables that is algebraic over the corresponding polynomial ring over a subfield of the reals, we show that its convergence domain is semialgebraic over the real closure of the subfield.","This gives in particular that the convergence radius of a univariate Puiseux series that is algebraic in the above sense belongs to the real closure or is infinity."],"url":"http://arxiv.org/abs/2402.07524v1","category":"math.CV"}
{"created":"2024-02-12 09:22:12","title":"Dynamical phase transitions in $XY$ model: a Monte Carlo and mean-field theory study","abstract":"We investigate the dynamical phases and phase transitions arising in a classical two-dimensional anisotropic $XY$ model under the influence of a periodically driven temporal external magnetic field. We use a combination of finite temperature classical Monte Carlo simulation, implemented within a CPU + GPU paradigm, utilizing local dynamics provided by the Glauber algorithm and a phenomenological mean-field equation-of-motion approach to study the model. We investigate several parameter regimes of the variables (magnetic field, anisotropy, and the external drive frequency) that influence the anisotropic $XY$ system. We identify four possible dynamical phases -- Ising-SBO, Ising-SRO, $XY$-SBO and $XY$-SRO. Both techniques indicate that only three of them (Ising-SRO, Ising-SBO, and $XY$-SRO) are stable dynamical phases in the thermodynamic sense. Within the Monte Carlo framework, a finite size scaling analysis shows that $XY$-SBO does not survive in the thermodynamic limit giving way to either an Ising-SBO or a $XY$-SRO regime. The finite size scaling analysis further shows that the transitions between the three remaining dynamical phases either belong to the two-dimensional Ising universality class or are first-order in nature. From the perspective of the mean-field calculations, $XY$-SBO represents a transient dynamical feature that is eventually lost to either Ising-SBO or $XY$-SRO.","sentences":["We investigate the dynamical phases and phase transitions arising in a classical two-dimensional anisotropic $XY$ model under the influence of a periodically driven temporal external magnetic field.","We use a combination of finite temperature classical Monte Carlo simulation, implemented within a CPU + GPU paradigm, utilizing local dynamics provided by the Glauber algorithm and a phenomenological mean-field equation-of-motion approach to study the model.","We investigate several parameter regimes of the variables (magnetic field, anisotropy, and the external drive frequency) that influence the anisotropic $XY$ system.","We identify four possible dynamical phases -- Ising-SBO, Ising-SRO, $XY$-SBO and $XY$-SRO.","Both techniques indicate that only three of them (Ising-SRO, Ising-SBO, and $XY$-SRO) are stable dynamical phases in the thermodynamic sense.","Within the Monte Carlo framework, a finite size scaling analysis shows that $XY$-SBO does not survive in the thermodynamic limit giving way to either an Ising-SBO or a $XY$-SRO regime.","The finite size scaling analysis further shows that the transitions between the three remaining dynamical phases either belong to the two-dimensional Ising universality class or are first-order in nature.","From the perspective of the mean-field calculations, $XY$-SBO represents a transient dynamical feature that is eventually lost to either Ising-SBO or $XY$-SRO."],"url":"http://arxiv.org/abs/2402.07505v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-12 09:08:12","title":"Efficient reduction of Feynman integrals on supercomputers","abstract":"Feynman integral reduction by means of integration-by-parts identities is a major power gadget in a theorist toolbox indispensable for calculation of multiloop quantum effects relevant for particle phenomenology and formal theory alike. An algorithmic approach consists of solving a large sparse non-square system of homogeneous linear equations with polynomial coefficients. While an analytical way of doing this is legitimate and was pursued for decades, it undoubtedly has its limitations when applied in complicated circumstances. Thus, a complementary framework based on modular arithmetic becomes critical on the way to conquer the current `what is possible' frontier. This calls for use of supercomputers to address the reduction problem. In order to properly utilize these computational resources, one has to efficiently optimize the technique for this purpose. Presently, we discuss and implement various methods which allow us to significantly improve performance of Feynman integral reduction within the FIRE environment.","sentences":["Feynman integral reduction by means of integration-by-parts identities is a major power gadget in a theorist toolbox indispensable for calculation of multiloop quantum effects relevant for particle phenomenology and formal theory alike.","An algorithmic approach consists of solving a large sparse non-square system of homogeneous linear equations with polynomial coefficients.","While an analytical way of doing this is legitimate and was pursued for decades, it undoubtedly has its limitations when applied in complicated circumstances.","Thus, a complementary framework based on modular arithmetic becomes critical on the way to conquer the current `what is possible' frontier.","This calls for use of supercomputers to address the reduction problem.","In order to properly utilize these computational resources, one has to efficiently optimize the technique for this purpose.","Presently, we discuss and implement various methods which allow us to significantly improve performance of Feynman integral reduction within the FIRE environment."],"url":"http://arxiv.org/abs/2402.07499v1","category":"hep-ph"}
{"created":"2024-02-12 09:03:26","title":"Modeling blazar broadband emission with convolutional neural networks -- II. External Compton model","abstract":"In the context of modeling spectral energy distributions (SEDs) for blazars, we extend the method that uses a convolutional neural network (CNN) to include external inverse Compton processes. The model assumes that relativistic electrons within the emitting region can interact and up-scatter external photon originating from the accretion disk, the broad-line region, and the torus, to produce the observed high-energy emission. We trained the CNN on a numerical model that accounts for the injection of electrons, their self-consistent cooling, and pair creation-annihilation processes, considering both internal and all external photon fields. Despite the larger number of parameters compared to the synchrotron self-Compton model and the greater diversity in spectral shapes, the CNN enables an accurate computation of the SED for a specified set of parameters. The performance of the CNN is demonstrated by fitting the SED of two flat-spectrum radio quasars, namely 3C 454.3 and CTA 102, and obtaining their parameter posterior distributions. For the first source, the available data in the low-energy band allowed us to constrain the minimum Lorentz factor of the electrons, $\\gamma_{\\rm min}$, while for the second source, due to the lack of these data, $\\gamma_{\\rm min} = 10^2$ was set. We used the obtained parameters to investigate the energetics of the system. The model developed here, along with one from B\\'egu\\'e et al. (2023), enables self-consistent, in-depth modeling of blazar broadband emissions within leptonic scenario.","sentences":["In the context of modeling spectral energy distributions (SEDs) for blazars, we extend the method that uses a convolutional neural network (CNN) to include external inverse Compton processes.","The model assumes that relativistic electrons within the emitting region can interact and up-scatter external photon originating from the accretion disk, the broad-line region, and the torus, to produce the observed high-energy emission.","We trained the CNN on a numerical model that accounts for the injection of electrons, their self-consistent cooling, and pair creation-annihilation processes, considering both internal and all external photon fields.","Despite the larger number of parameters compared to the synchrotron self-Compton model and the greater diversity in spectral shapes, the CNN enables an accurate computation of the SED for a specified set of parameters.","The performance of the CNN is demonstrated by fitting the SED of two flat-spectrum radio quasars, namely 3C 454.3 and CTA 102, and obtaining their parameter posterior distributions.","For the first source, the available data in the low-energy band allowed us to constrain the minimum Lorentz factor of the electrons, $\\gamma_{\\rm min}$, while for the second source, due to the lack of these data, $\\gamma_{\\rm min} = 10^2$ was set.","We used the obtained parameters to investigate the energetics of the system.","The model developed here, along with one from B\\'egu\\'e et al. (2023), enables self-consistent, in-depth modeling of blazar broadband emissions within leptonic scenario."],"url":"http://arxiv.org/abs/2402.07495v1","category":"astro-ph.HE"}
{"created":"2024-02-12 08:26:25","title":"High-resolution Cryogenic Spectroscopy of Single Molecules in Nanoprinted Crystals","abstract":"We perform laser spectroscopy at liquid helium temperatures (T=2 K) to investigate single dibenzoterrylene (DBT) molecules doped in anthracene crystals of nanoscopic height fabricated by electrohydrodynamic dripping. Using high-resolution fluorescence excitation spectroscopy, we show that zero-phonon lines of single molecules in printed nanocrystals are nearly as narrow as the Fourier-limited transitions observed for the same guest-host system in the bulk. Moreover, the spectral instabilities are comparable to or less than one linewidth. By recording super-resolution images of DBT molecules and varying the polarization of the excitation beam, we determine the dimensions of the printed crystals and the orientation of the crystals' axes. Electrohydrodynamic printing of organic nano and microcrystals paves the way for a series of applications, where controlled positioning of quantum emitters with narrow optical transitions is desirable.","sentences":["We perform laser spectroscopy at liquid helium temperatures (T=2 K) to investigate single dibenzoterrylene (DBT) molecules doped in anthracene crystals of nanoscopic height fabricated by electrohydrodynamic dripping.","Using high-resolution fluorescence excitation spectroscopy, we show that zero-phonon lines of single molecules in printed nanocrystals are nearly as narrow as the Fourier-limited transitions observed for the same guest-host system in the bulk.","Moreover, the spectral instabilities are comparable to or less than one linewidth.","By recording super-resolution images of DBT molecules and varying the polarization of the excitation beam, we determine the dimensions of the printed crystals and the orientation of the crystals' axes.","Electrohydrodynamic printing of organic nano and microcrystals paves the way for a series of applications, where controlled positioning of quantum emitters with narrow optical transitions is desirable."],"url":"http://arxiv.org/abs/2402.07474v1","category":"quant-ph"}
{"created":"2024-02-12 08:17:23","title":"Cartesian atomic cluster expansion for machine learning interatomic potentials","abstract":"Machine learning interatomic potentials are revolutionizing large-scale, accurate atomistic modelling in material science and chemistry. These potentials often use atomic cluster expansion or equivariant message passing with spherical harmonics as basis functions. However, the dependence on Clebsch-Gordan coefficients for maintaining rotational symmetry leads to computational inefficiencies and redundancies. We propose an alternative: a Cartesian-coordinates-based atomic density expansion. This approach provides a complete description of atomic environments while maintaining interaction body orders. Additionally, we integrate low-dimensional embeddings of various chemical elements and inter-atomic message passing. The resulting potential, named Cartesian Atomic Cluster Expansion (CACE), exhibits good accuracy, stability, and generalizability. We validate its performance in diverse systems, including bulk water, small molecules, and 25-element high-entropy alloys.","sentences":["Machine learning interatomic potentials are revolutionizing large-scale, accurate atomistic modelling in material science and chemistry.","These potentials often use atomic cluster expansion or equivariant message passing with spherical harmonics as basis functions.","However, the dependence on Clebsch-Gordan coefficients for maintaining rotational symmetry leads to computational inefficiencies and redundancies.","We propose an alternative: a Cartesian-coordinates-based atomic density expansion.","This approach provides a complete description of atomic environments while maintaining interaction body orders.","Additionally, we integrate low-dimensional embeddings of various chemical elements and inter-atomic message passing.","The resulting potential, named Cartesian Atomic Cluster Expansion (CACE), exhibits good accuracy, stability, and generalizability.","We validate its performance in diverse systems, including bulk water, small molecules, and 25-element high-entropy alloys."],"url":"http://arxiv.org/abs/2402.07472v1","category":"physics.comp-ph"}
{"created":"2024-02-12 08:12:59","title":"Transient growth of a wake vortex and its initiation via inertial particles","abstract":"The transient dynamics of a wake vortex, modelled by a strong swirling $q$-vortex, are examined with an emphasis on exploring optimal transient growth constructed by continuous eigenmodes associated with continuous spectra. The pivotal contribution of the viscous critical-layer eigenmodes (Lee & Marcus, J. Fluid Mech., vol. 967) amongst the entire eigenmode families to optimal perturbations is numerically confirmed, based on a spectral collocation method for a radially unbounded domain that ensures correct analyticity and far-field behaviour. The consistence of the numerical method against numerical sensitivity provides reliability of results as well as flexibility for tuning. Both axisymmetric and helical perturbation cases with axial wavenumbers of order unity or less are considered in the study through both linearised theory and non-linear simulations, yielding results that align with literature on both energy growth curves and optimal perturbation structures. Additionally, the initiation process of transient growth is discussed to reveal its practicability. Inspired by ice crystals in contrails, the role of backward influences of inertial particles on the carrier vortex flow, especially via particle drag, is underscored. In the pursuit of optimal transient growth, the particles are initially distributed at the periphery of the vortex core to disturb the vortex. Two-way coupled vortex-particle simulations conclusively demonstrate clear indications of optimal transient growth during continual vortex-particle interactions, reinforcing the robustness and significance of the transient growth process in the original non-linear vortex system over finite time periods.","sentences":["The transient dynamics of a wake vortex, modelled by a strong swirling $q$-vortex, are examined with an emphasis on exploring optimal transient growth constructed by continuous eigenmodes associated with continuous spectra.","The pivotal contribution of the viscous critical-layer eigenmodes (Lee & Marcus, J. Fluid Mech., vol. 967) amongst the entire eigenmode families to optimal perturbations is numerically confirmed, based on a spectral collocation method for a radially unbounded domain that ensures correct analyticity and far-field behaviour.","The consistence of the numerical method against numerical sensitivity provides reliability of results as well as flexibility for tuning.","Both axisymmetric and helical perturbation cases with axial wavenumbers of order unity or less are considered in the study through both linearised theory and non-linear simulations, yielding results that align with literature on both energy growth curves and optimal perturbation structures.","Additionally, the initiation process of transient growth is discussed to reveal its practicability.","Inspired by ice crystals in contrails, the role of backward influences of inertial particles on the carrier vortex flow, especially via particle drag, is underscored.","In the pursuit of optimal transient growth, the particles are initially distributed at the periphery of the vortex core to disturb the vortex.","Two-way coupled vortex-particle simulations conclusively demonstrate clear indications of optimal transient growth during continual vortex-particle interactions, reinforcing the robustness and significance of the transient growth process in the original non-linear vortex system over finite time periods."],"url":"http://arxiv.org/abs/2402.07469v1","category":"physics.flu-dyn"}
{"created":"2024-02-12 08:07:31","title":"Dynamics of Saturn's Polar Regions","abstract":"We analyze data retrieved by the Imaging Science System onboard the Cassini spacecraft to study the horizontal velocity and vorticity fields of Saturn's Polar Regions (latitudes 60-90$^\\circ$N in June-December 2013 and 60-90$^\\circ$S in October 2006 and July-December 2008), including the Northern region where the hexagonal wave is prominent. With the aid of an automated two dimensional correlation algorithm we determine two-dimensional maps of zonal and meridional winds, and deduce vorticity maps. We extract zonal averages of zonal winds, providing wind profiles that reach latitudes as high 89.5$^\\circ$ in the south and 89.9$^\\circ$ in the north. Wind measurements cover the intense polar cyclonic vortices that reach similar peak velocities of 150 ms-1 at 88.5$^\\circ$. The hexagonal wave lies in the core of an intense eastward jet at planetocentric latitude 75.8$^\\circ$N with motions that become non-zonal at the hexagonal feature. In the south hemisphere the peak of the eastward jet is located at planetocentric latitude 70.4$^\\circ$S. A large anticyclone (the South Polar Spot, SPS), similar to the North Polar Spot (NPS) observed at the Voyager times (1980-81), has been observed in images from April 2008 to January 2009 in the South Polar Region at latitude -66.1$^\\circ$ close to the eastward jet. The SPS does not apparently excite a wave on the jet. We analyze the stability of the zonal jets, finding potential instabilities at the flanks of the eastward jets around 70$^\\circ$ and we measure the eddy wind components, suggesting momentum transfer from eddy motion to the westward jets closer to the poles.","sentences":["We analyze data retrieved by the Imaging Science System onboard the Cassini spacecraft to study the horizontal velocity and vorticity fields of Saturn's Polar Regions (latitudes 60-90$^\\circ$N in June-December 2013 and 60-90$^\\circ$S in October 2006 and July-December 2008), including the Northern region where the hexagonal wave is prominent.","With the aid of an automated two dimensional correlation algorithm we determine two-dimensional maps of zonal and meridional winds, and deduce vorticity maps.","We extract zonal averages of zonal winds, providing wind profiles that reach latitudes as high 89.5$^\\circ$ in the south and 89.9$^\\circ$ in the north.","Wind measurements cover the intense polar cyclonic vortices that reach similar peak velocities of 150 ms-1 at 88.5$^\\circ$.","The hexagonal wave lies in the core of an intense eastward jet at planetocentric latitude 75.8$^\\circ$N with motions that become non-zonal at the hexagonal feature.","In the south hemisphere the peak of the eastward jet is located at planetocentric latitude 70.4$^\\circ$S.","A large anticyclone (the South Polar Spot, SPS), similar to the North Polar Spot (NPS) observed at the Voyager times (1980-81), has been observed in images from April 2008 to January 2009 in the South Polar Region at latitude -66.1$^\\circ$ close to the eastward jet.","The SPS does not apparently excite a wave on the jet.","We analyze the stability of the zonal jets, finding potential instabilities at the flanks of the eastward jets around 70$^\\circ$ and we measure the eddy wind components, suggesting momentum transfer from eddy motion to the westward jets closer to the poles."],"url":"http://arxiv.org/abs/2402.07468v1","category":"astro-ph.EP"}
{"created":"2024-02-12 07:52:55","title":"PyDMD: A Python package for robust dynamic mode decomposition","abstract":"The dynamic mode decomposition (DMD) is a simple and powerful data-driven modeling technique that is capable of revealing coherent spatiotemporal patterns from data. The method's linear algebra-based formulation additionally allows for a variety of optimizations and extensions that make the algorithm practical and viable for real-world data analysis. As a result, DMD has grown to become a leading method for dynamical system analysis across multiple scientific disciplines. PyDMD is a Python package that implements DMD and several of its major variants. In this work, we expand the PyDMD package to include a number of cutting-edge DMD methods and tools specifically designed to handle dynamics that are noisy, multiscale, parameterized, prohibitively high-dimensional, or even strongly nonlinear. We provide a complete overview of the features available in PyDMD as of version 1.0, along with a brief overview of the theory behind the DMD algorithm, information for developers, tips regarding practical DMD usage, and introductory coding examples. All code is available at https://github.com/PyDMD/PyDMD .","sentences":["The dynamic mode decomposition (DMD) is a simple and powerful data-driven modeling technique that is capable of revealing coherent spatiotemporal patterns from data.","The method's linear algebra-based formulation additionally allows for a variety of optimizations and extensions that make the algorithm practical and viable for real-world data analysis.","As a result, DMD has grown to become a leading method for dynamical system analysis across multiple scientific disciplines.","PyDMD is a Python package that implements DMD and several of its major variants.","In this work, we expand the PyDMD package to include a number of cutting-edge DMD methods and tools specifically designed to handle dynamics that are noisy, multiscale, parameterized, prohibitively high-dimensional, or even strongly nonlinear.","We provide a complete overview of the features available in PyDMD as of version 1.0, along with a brief overview of the theory behind the DMD algorithm, information for developers, tips regarding practical DMD usage, and introductory coding examples.","All code is available at https://github.com/PyDMD/PyDMD ."],"url":"http://arxiv.org/abs/2402.07463v1","category":"stat.CO"}
{"created":"2024-02-12 07:29:17","title":"Bifurcations of an elastic disc coated with an elastic inextensible rod","abstract":"An analytical solution is derived for the bifurcations of an elastic disc that is constrained on the boundary with an isoperimetric Cosserat coating. The latter is treated as an elastic circular rod, either perfectly or partially bonded (with a slip interface in the latter case) and is subjected to three different types of uniformly distributed radial loads (including hydrostatic pressure). The proposed solution technique employs complex potentials to treat the disc's interior and incremental Lagrangian equations to describe the prestressed elastic rod modelling the coating. The bifurcations of the disc occur with modes characterized by different circumferential wavenumbers, ranging between ovalization and high-order waviness, as a function of the ratio between the elastic stiffness of the disc and the bending stiffness of its coating. The presented results find applications in various fields, such as coated fibres, mechanical rollers, and the growth and morphogenesis of plants and fruits.","sentences":["An analytical solution is derived for the bifurcations of an elastic disc that is constrained on the boundary with an isoperimetric Cosserat coating.","The latter is treated as an elastic circular rod, either perfectly or partially bonded (with a slip interface in the latter case) and is subjected to three different types of uniformly distributed radial loads (including hydrostatic pressure).","The proposed solution technique employs complex potentials to treat the disc's interior and incremental Lagrangian equations to describe the prestressed elastic rod modelling the coating.","The bifurcations of the disc occur with modes characterized by different circumferential wavenumbers, ranging between ovalization and high-order waviness, as a function of the ratio between the elastic stiffness of the disc and the bending stiffness of its coating.","The presented results find applications in various fields, such as coated fibres, mechanical rollers, and the growth and morphogenesis of plants and fruits."],"url":"http://arxiv.org/abs/2402.07455v1","category":"physics.class-ph"}
{"created":"2024-02-12 07:15:23","title":"The TESS-Keck Survey. XII. A Dense 1.8 R$_\\oplus$ Ultra-Short-Period Planet Possibly Clinging to a High-Mean-Molecular-Weight Atmosphere After the First Gyr","abstract":"The extreme environments of ultra-short-period planets (USPs) make excellent laboratories to study how exoplanets obtain, lose, retain, and/or regain gaseous atmospheres. We present the confirmation and characterization of the USP TOI-1347 b, a $1.8 \\pm 0.1$ R$_\\oplus$ planet on a 0.85 day orbit that was detected with photometry from the TESS mission. We measured radial velocities of the TOI-1347 system using Keck/HIRES and HARPS-N and found the USP to be unusually massive at $11.1 \\pm 1.2$ M$_\\oplus$. The measured mass and radius of TOI-1347 b imply an Earth-like bulk composition. A thin H/He envelope (>0.01% by mass) can be ruled out at high confidence. The system is between 1 and 1.8 Gyr old; therefore, intensive photoevaporation should have concluded. We detected a tentative phase curve variation (3$\\sigma$) and a secondary eclipse (2$\\sigma$) in TESS photometry, which if confirmed could indicate the presence of a high-mean-molecular-weight atmosphere. We recommend additional optical and infrared observations to confirm the presence of an atmosphere and investigate its composition.","sentences":["The extreme environments of ultra-short-period planets (USPs) make excellent laboratories to study how exoplanets obtain, lose, retain, and/or regain gaseous atmospheres.","We present the confirmation and characterization of the USP TOI-1347 b, a $1.8 \\pm 0.1$ R$_\\oplus$ planet on a 0.85 day orbit that was detected with photometry from the TESS mission.","We measured radial velocities of the TOI-1347 system using Keck/HIRES and HARPS-N and found the USP to be unusually massive at $11.1 \\pm 1.2$ M$_\\oplus$. The measured mass and radius of TOI-1347 b imply an Earth-like bulk composition.","A thin H/He envelope (>0.01% by mass) can be ruled out at high confidence.","The system is between 1 and 1.8 Gyr old; therefore, intensive photoevaporation should have concluded.","We detected a tentative phase curve variation (3$\\sigma$) and a secondary eclipse (2$\\sigma$) in TESS photometry, which if confirmed could indicate the presence of a high-mean-molecular-weight atmosphere.","We recommend additional optical and infrared observations to confirm the presence of an atmosphere and investigate its composition."],"url":"http://arxiv.org/abs/2402.07451v1","category":"astro-ph.EP"}
{"created":"2024-02-12 07:11:17","title":"Magnetism and superconductivity in the $t-J$ model of $La_3Ni_2O_7$ under multiband Gutzwiller approximation","abstract":"The recent discovery of possible high temperature superconductivity in single crystals of $La_3Ni_2O_7$ under pressure renews the interest in research on nickelates. The DFT calculations reveal that both $d_{z^2}$ and $d_{x^2-y^2}$ orbitals are active, which suggests a minimal two-orbital model to capture the low-energy physics of this system. In this work, we study a bilayer two-orbital $t-J$ model within multiband Gutzwiller approximation, and discuss the magnetism as well as the superconductivity over a wide range of the hole doping. Owing to the inter-orbital super-exchange process between $d_{z^2}$ and $d_{x^2-y^2}$ orbitals, the induced ferromagnetic coupling within layers competes with the conventional antiferromagnetic coupling, and leads to complicated hole doping dependence for the magnetic properties in the system. With increasing hole doping, the system transfers to A-AFM state from the starting G-AFM state. We also find the inter-layer superconducting pairing of $d_{x^2-y^2}$ orbitals dominates due to the large hopping parameter of $d_{z^2}$ along the vertical inter-layer bonds and significant Hund's coupling between $d_{z^2}$ and $d_{x^2-y^2}$ orbitals. Meanwhile, the G-AFM state and superconductivity state can coexist in the low hole doping regime. To take account of the pressure, we also analyze the impacts of inter-layer hopping amplitude on the system properties.","sentences":["The recent discovery of possible high temperature superconductivity in single crystals of $La_3Ni_2O_7$ under pressure renews the interest in research on nickelates.","The DFT calculations reveal that both $d_{z^2}$ and $d_{x^2-y^2}$ orbitals are active, which suggests a minimal two-orbital model to capture the low-energy physics of this system.","In this work, we study a bilayer two-orbital $t-J$ model within multiband Gutzwiller approximation, and discuss the magnetism as well as the superconductivity over a wide range of the hole doping.","Owing to the inter-orbital super-exchange process between $d_{z^2}$ and $d_{x^2-y^2}$ orbitals, the induced ferromagnetic coupling within layers competes with the conventional antiferromagnetic coupling, and leads to complicated hole doping dependence for the magnetic properties in the system.","With increasing hole doping, the system transfers to A-AFM state from the starting G-AFM state.","We also find the inter-layer superconducting pairing of $d_{x^2-y^2}$ orbitals dominates due to the large hopping parameter of $d_{z^2}$ along the vertical inter-layer bonds and significant Hund's coupling between $d_{z^2}$ and $d_{x^2-y^2}$ orbitals.","Meanwhile, the G-AFM state and superconductivity state can coexist in the low hole doping regime.","To take account of the pressure, we also analyze the impacts of inter-layer hopping amplitude on the system properties."],"url":"http://arxiv.org/abs/2402.07449v1","category":"cond-mat.supr-con"}
{"created":"2024-02-12 06:43:52","title":"Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT","abstract":"Retrieval pipelines-an integral component of many machine learning systems-perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text. Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to fine-tune this model for retrieval under the batch size limitations imposed by GPU memory constraints. To address these challenges, we first introduce LoCoV1, a novel 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective. We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scaling to documents up to 32K tokens long. We describe a pretraining data mixture which allows this encoder to process both short and long context sequences, and a finetuning approach that adapts this base model to retrieval with only single-sample batches. Finally, we validate the M2-BERT retrieval encoder on LoCoV1, finding that it outperforms competitive baselines by up to 23.3 points, despite containing 5-90x fewer parameters.","sentences":["Retrieval pipelines-an integral component of many machine learning systems-perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text.","Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to fine-tune this model for retrieval under the batch size limitations imposed by GPU memory constraints.","To address these challenges, we first introduce LoCoV1, a novel 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective.","We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scaling to documents up to 32K tokens long.","We describe a pretraining data mixture which allows this encoder to process both short and long context sequences, and a finetuning approach that adapts this base model to retrieval with only single-sample batches.","Finally, we validate the M2-BERT retrieval encoder on LoCoV1, finding that it outperforms competitive baselines by up to 23.3 points, despite containing 5-90x fewer parameters."],"url":"http://arxiv.org/abs/2402.07440v1","category":"cs.IR"}
{"created":"2024-02-12 06:27:53","title":"Parameterizations for Gradient-based Markov Chain Monte Carlo on the Stiefel Manifold: A Comparative Study","abstract":"Orthogonal matrices play an important role in probability and statistics, particularly in high-dimensional statistical models. Parameterizing these models using orthogonal matrices facilitates dimension reduction and parameter identification. However, establishing the theoretical validity of statistical inference in these models from a frequentist perspective is challenging, leading to a preference for Bayesian approaches because of their ability to offer consistent uncertainty quantification. Markov chain Monte Carlo methods are commonly used for numerical approximation of posterior distributions, and sampling on the Stiefel manifold, which comprises orthogonal matrices, poses significant difficulties. While various strategies have been proposed for this purpose, gradient-based Markov chain Monte Carlo with parameterizations is the most efficient. However, a comprehensive comparison of these parameterizations is lacking in the existing literature. This study aims to address this gap by evaluating numerical efficiency of the four alternative parameterizations of orthogonal matrices under equivalent conditions. The evaluation was conducted for four problems. The results suggest that polar expansion parameterization is the most efficient, particularly for the high-dimensional and complex problems. However, all parameterizations exhibit limitations in significantly high-dimensional or difficult tasks, emphasizing the need for further advancements in sampling methods for orthogonal matrices.","sentences":["Orthogonal matrices play an important role in probability and statistics, particularly in high-dimensional statistical models.","Parameterizing these models using orthogonal matrices facilitates dimension reduction and parameter identification.","However, establishing the theoretical validity of statistical inference in these models from a frequentist perspective is challenging, leading to a preference for Bayesian approaches because of their ability to offer consistent uncertainty quantification.","Markov chain Monte Carlo methods are commonly used for numerical approximation of posterior distributions, and sampling on the Stiefel manifold, which comprises orthogonal matrices, poses significant difficulties.","While various strategies have been proposed for this purpose, gradient-based Markov chain Monte Carlo with parameterizations is the most efficient.","However, a comprehensive comparison of these parameterizations is lacking in the existing literature.","This study aims to address this gap by evaluating numerical efficiency of the four alternative parameterizations of orthogonal matrices under equivalent conditions.","The evaluation was conducted for four problems.","The results suggest that polar expansion parameterization is the most efficient, particularly for the high-dimensional and complex problems.","However, all parameterizations exhibit limitations in significantly high-dimensional or difficult tasks, emphasizing the need for further advancements in sampling methods for orthogonal matrices."],"url":"http://arxiv.org/abs/2402.07434v1","category":"stat.CO"}
{"created":"2024-02-12 06:25:13","title":"Logical Synchrony Networks: A formal model for deterministic distribution","abstract":"Kahn Process Networks (KPNs) are a deterministic Model of Computation (MoC) for distributed systems. KPNs supports non-blocking writes and blocking reads, with the consequent assumption of unbounded buffers between processes. Variants such as Finite FIFO Platforms (FFP) have been developed, which enforce boundedness. One issue with existing models is that they mix process synchronisation with process execution. In this paper we address how these two facets may be decoupled.   This paper explores a recent alternative called bittide, which decouples the execution of a process from the control needed for process synchronisation, and thus preserves determinism and boundedness while ensuring pipelined execution for better throughput. Our intuition is that such an approach could leverage not only determinism and buffer boundedness but may potentially offer better overall throughput.   To understand the behavior of these systems we define a formal model -- a deterministic MoC called Logical Synchrony Networks (LSNs). LSNs describes a network of processes modelled as a graph, with edges representing invariant logical delays between a producer process and the corresponding consumer process. We show that this abstraction is satisfied by KPNs. Subsequently, we show that both FFPs and bittide faithfully implement this abstraction. Thus, we show for the first time that FFPs and bittide offer two alternative ways of implementing deterministic distributed systems with the latter being more performant.","sentences":["Kahn Process Networks (KPNs) are a deterministic Model of Computation (MoC) for distributed systems.","KPNs supports non-blocking writes and blocking reads, with the consequent assumption of unbounded buffers between processes.","Variants such as Finite FIFO Platforms (FFP) have been developed, which enforce boundedness.","One issue with existing models is that they mix process synchronisation with process execution.","In this paper we address how these two facets may be decoupled.   ","This paper explores a recent alternative called bittide, which decouples the execution of a process from the control needed for process synchronisation, and thus preserves determinism and boundedness while ensuring pipelined execution for better throughput.","Our intuition is that such an approach could leverage not only determinism and buffer boundedness but may potentially offer better overall throughput.   ","To understand the behavior of these systems we define a formal model -- a deterministic MoC called Logical Synchrony Networks (LSNs).","LSNs describes a network of processes modelled as a graph, with edges representing invariant logical delays between a producer process and the corresponding consumer process.","We show that this abstraction is satisfied by KPNs.","Subsequently, we show that both FFPs and bittide faithfully implement this abstraction.","Thus, we show for the first time that FFPs and bittide offer two alternative ways of implementing deterministic distributed systems with the latter being more performant."],"url":"http://arxiv.org/abs/2402.07433v1","category":"cs.DC"}
{"created":"2024-02-12 06:05:56","title":"Netload Range Cost Curves for a Transmission-Aware Distribution System Planning under DER Growth Uncertainty","abstract":"In the face of a substantial and uncertain growth of behind-the-meter Distributed Energy Resources (DERs), utilities and regulators are currently in the search for new network planning strategies for facilitating an efficient Transmission & Distribution (T&D) coordination. In this context, here we propose a novel distribution system planning methodology to facilitate coordinated planning exercises with transmission system planners through the management of long-term DER growth uncertainty and its impact on the substation netload. The proposed approach is based on the design of a transmission-aware distribution planning model embedding DER growth uncertainty, which is used to determine a \"menu\" of secure distribution network upgrade options with different associated costs and peak netload guarantees observed from the transmission-side, referred here as Netload Range Cost Curves (NRCCs). NRCCs can provide a practical approach for coordinating T&D planning exercises, as these curves can be integrated into existing transmission planning workflows, and specify a direct incentive for distribution planners to evaluate peak netload reduction alternatives in their planning process. We perform computational experiments based on a realistic distribution network that demonstrate the benefits and applicability of our proposed planning approach.","sentences":["In the face of a substantial and uncertain growth of behind-the-meter Distributed Energy Resources (DERs), utilities and regulators are currently in the search for new network planning strategies for facilitating an efficient Transmission & Distribution (T&D) coordination.","In this context, here we propose a novel distribution system planning methodology to facilitate coordinated planning exercises with transmission system planners through the management of long-term DER growth uncertainty and its impact on the substation netload.","The proposed approach is based on the design of a transmission-aware distribution planning model embedding DER growth uncertainty, which is used to determine a \"menu\" of secure distribution network upgrade options with different associated costs and peak netload guarantees observed from the transmission-side, referred here as Netload Range Cost Curves (NRCCs).","NRCCs can provide a practical approach for coordinating T&D planning exercises, as these curves can be integrated into existing transmission planning workflows, and specify a direct incentive for distribution planners to evaluate peak netload reduction alternatives in their planning process.","We perform computational experiments based on a realistic distribution network that demonstrate the benefits and applicability of our proposed planning approach."],"url":"http://arxiv.org/abs/2402.07428v1","category":"math.OC"}
{"created":"2024-02-12 05:38:11","title":"Context-aware Multi-Model Object Detection for Diversely Heterogeneous Compute Systems","abstract":"In recent years, deep neural networks (DNNs) have gained widespread adoption for continuous mobile object detection (OD) tasks, particularly in autonomous systems. However, a prevalent issue in their deployment is the one-size-fits-all approach, where a single DNN is used, resulting in inefficient utilization of computational resources. This inefficiency is particularly detrimental in energy-constrained systems, as it degrades overall system efficiency. We identify that, the contextual information embedded in the input data stream (e.g. the frames in the camera feed that the OD models are run on) could be exploited to allow a more efficient multi-model-based OD process. In this paper, we propose SHIFT which continuously selects from a variety of DNN-based OD models depending on the dynamically changing contextual information and computational constraints. During this selection, SHIFT uniquely considers multi-accelerator execution to better optimize the energy-efficiency while satisfying the latency constraints. Our proposed methodology results in improvements of up to 7.5x in energy usage and 2.8x in latency compared to state-of-the-art GPU-based single model OD approaches.","sentences":["In recent years, deep neural networks (DNNs) have gained widespread adoption for continuous mobile object detection (OD) tasks, particularly in autonomous systems.","However, a prevalent issue in their deployment is the one-size-fits-all approach, where a single DNN is used, resulting in inefficient utilization of computational resources.","This inefficiency is particularly detrimental in energy-constrained systems, as it degrades overall system efficiency.","We identify that, the contextual information embedded in the input data stream (e.g. the frames in the camera feed that the OD models are run on) could be exploited to allow a more efficient multi-model-based OD process.","In this paper, we propose SHIFT which continuously selects from a variety of DNN-based OD models depending on the dynamically changing contextual information and computational constraints.","During this selection, SHIFT uniquely considers multi-accelerator execution to better optimize the energy-efficiency while satisfying the latency constraints.","Our proposed methodology results in improvements of up to 7.5x in energy usage and 2.8x in latency compared to state-of-the-art GPU-based single model OD approaches."],"url":"http://arxiv.org/abs/2402.07415v1","category":"cs.LG"}
{"created":"2024-02-12 05:12:09","title":"Potential-Based Reward Shaping For Intrinsic Motivation","abstract":"Recently there has been a proliferation of intrinsic motivation (IM) reward-shaping methods to learn in complex and sparse-reward environments. These methods can often inadvertently change the set of optimal policies in an environment, leading to suboptimal behavior. Previous work on mitigating the risks of reward shaping, particularly through potential-based reward shaping (PBRS), has not been applicable to many IM methods, as they are often complex, trainable functions themselves, and therefore dependent on a wider set of variables than the traditional reward functions that PBRS was developed for. We present an extension to PBRS that we prove preserves the set of optimal policies under a more general set of functions than has been previously proven. We also present {\\em Potential-Based Intrinsic Motivation} (PBIM), a method for converting IM rewards into a potential-based form that is useable without altering the set of optimal policies. Testing in the MiniGrid DoorKey and Cliff Walking environments, we demonstrate that PBIM successfully prevents the agent from converging to a suboptimal policy and can speed up training.","sentences":["Recently there has been a proliferation of intrinsic motivation (IM) reward-shaping methods to learn in complex and sparse-reward environments.","These methods can often inadvertently change the set of optimal policies in an environment, leading to suboptimal behavior.","Previous work on mitigating the risks of reward shaping, particularly through potential-based reward shaping (PBRS), has not been applicable to many IM methods, as they are often complex, trainable functions themselves, and therefore dependent on a wider set of variables than the traditional reward functions that PBRS was developed for.","We present an extension to PBRS that we prove preserves the set of optimal policies under a more general set of functions than has been previously proven.","We also present {\\em Potential-Based Intrinsic Motivation} (PBIM), a method for converting IM rewards into a potential-based form that is useable without altering the set of optimal policies.","Testing in the MiniGrid DoorKey and Cliff Walking environments, we demonstrate that PBIM successfully prevents the agent from converging to a suboptimal policy and can speed up training."],"url":"http://arxiv.org/abs/2402.07411v1","category":"cs.LG"}
{"created":"2024-02-12 05:05:55","title":"A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)","abstract":"Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable generalization capabilities across multiple challenging distribution shifts. However, there is still much to be explored in terms of their robustness to the variations of specific visual factors. In real-world applications, reliable and safe systems must consider other safety objectives beyond classification accuracy, such as predictive uncertainty. Yet, the effectiveness of CLIP models on such safety-related features is less-explored. Driven by the above, this work comprehensively investigates the safety objectives of CLIP models, specifically focusing on three key properties: resilience to visual factor variations, calibrated uncertainty estimations, and the ability to detect anomalous inputs. To this end, we study 83 CLIP models and 127 ImageNet classifiers. They are diverse in architecture, (pre)training distribution and training strategies. We consider 10 visual factors (e.g., shape and pattern), 5 types of out-of-distribution data, and 8 natural and challenging test conditions with different shift types, such as texture, style, and perturbation shifts. Our study has unveiled several previously unknown insights into CLIP models. For instance, they are not consistently more calibrated than other ImageNet models, which contradicts existing findings. Additionally, our analysis underscores the significance of training source design by showcasing its profound influence on the three safety-related properties. We believe our comprehensive study can shed light on and help guide the development of more robust and reliable CLIP models.","sentences":["Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable generalization capabilities across multiple challenging distribution shifts.","However, there is still much to be explored in terms of their robustness to the variations of specific visual factors.","In real-world applications, reliable and safe systems must consider other safety objectives beyond classification accuracy, such as predictive uncertainty.","Yet, the effectiveness of CLIP models on such safety-related features is less-explored.","Driven by the above, this work comprehensively investigates the safety objectives of CLIP models, specifically focusing on three key properties: resilience to visual factor variations, calibrated uncertainty estimations, and the ability to detect anomalous inputs.","To this end, we study 83 CLIP models and 127 ImageNet classifiers.","They are diverse in architecture, (pre)training distribution and training strategies.","We consider 10 visual factors (e.g., shape and pattern), 5 types of out-of-distribution data, and 8 natural and challenging test conditions with different shift types, such as texture, style, and perturbation shifts.","Our study has unveiled several previously unknown insights into CLIP models.","For instance, they are not consistently more calibrated than other ImageNet models, which contradicts existing findings.","Additionally, our analysis underscores the significance of training source design by showcasing its profound influence on the three safety-related properties.","We believe our comprehensive study can shed light on and help guide the development of more robust and reliable CLIP models."],"url":"http://arxiv.org/abs/2402.07410v1","category":"cs.CV"}
{"created":"2024-02-12 04:59:34","title":"Conformal Predictive Programming for Chance Constrained Optimization","abstract":"Motivated by the advances in conformal prediction (CP), we propose conformal predictive programming (CPP), an approach to solve chance constrained optimization (CCO) problems, i.e., optimization problems with nonlinear constraint functions affected by arbitrary random parameters. CPP utilizes samples from these random parameters along with the quantile lemma -- which is central to CP -- to transform the CCO problem into a deterministic optimization problem. We then present two tractable reformulations of CPP by: (1) writing the quantile as a linear program along with its KKT conditions (CPP-KKT), and (2) using mixed integer programming (CPP-MIP). CPP comes with marginal probabilistic feasibility guarantees for the CCO problem that are conceptually different from existing approaches, e.g., the sample approximation and the scenario approach. While we explore algorithmic similarities with the sample approximation approach, we emphasize that the strength of CPP is that it can easily be extended to incorporate different variants of CP. To illustrate this, we present robust conformal predictive programming to deal with distribution shifts in the uncertain parameters of the CCO problem.","sentences":["Motivated by the advances in conformal prediction (CP), we propose conformal predictive programming (CPP), an approach to solve chance constrained optimization (CCO) problems, i.e., optimization problems with nonlinear constraint functions affected by arbitrary random parameters.","CPP utilizes samples from these random parameters along with the quantile lemma -- which is central to CP -- to transform the CCO problem into a deterministic optimization problem.","We then present two tractable reformulations of CPP by: (1) writing the quantile as a linear program along with its KKT conditions (CPP-KKT), and (2) using mixed integer programming (CPP-MIP).","CPP comes with marginal probabilistic feasibility guarantees for the CCO problem that are conceptually different from existing approaches, e.g., the sample approximation and the scenario approach.","While we explore algorithmic similarities with the sample approximation approach, we emphasize that the strength of CPP is that it can easily be extended to incorporate different variants of CP.","To illustrate this, we present robust conformal predictive programming to deal with distribution shifts in the uncertain parameters of the CCO problem."],"url":"http://arxiv.org/abs/2402.07407v1","category":"eess.SY"}
{"created":"2024-02-12 04:05:54","title":"Robust Quantum Control via a Model Predictive Control Strategy","abstract":"This article presents a robust control strategy using Time-Optimal Model Predictive Control (TOMPC) for a two-level quantum system subject to bounded uncertainties. In this method, the control field is optimized over a finite horizon using a nominal quantum system as the reference and then the optimal control for the first time interval is applied and a projective measurement is implemented on the uncertain system. The new control field for the next time interval will be iteratively optimized based on the measurement result. We present theoretical results to guarantee the stability of the TOMPC algorithm. We also characterize the robustness and the convergence rate of the TOMPC strategy for the control of two-level systems. Numerical simulations further demonstrate that, in the presence of uncertainties, our quantum TOMPC algorithm enhances robustness and steers the state to the desired state with high fidelity. This work contributes to the progress of Model Predictive Control in quantum control and explores its potential in practical applications of quantum technology.","sentences":["This article presents a robust control strategy using Time-Optimal Model Predictive Control (TOMPC) for a two-level quantum system subject to bounded uncertainties.","In this method, the control field is optimized over a finite horizon using a nominal quantum system as the reference and then the optimal control for the first time interval is applied and a projective measurement is implemented on the uncertain system.","The new control field for the next time interval will be iteratively optimized based on the measurement result.","We present theoretical results to guarantee the stability of the TOMPC algorithm.","We also characterize the robustness and the convergence rate of the TOMPC strategy for the control of two-level systems.","Numerical simulations further demonstrate that, in the presence of uncertainties, our quantum TOMPC algorithm enhances robustness and steers the state to the desired state with high fidelity.","This work contributes to the progress of Model Predictive Control in quantum control and explores its potential in practical applications of quantum technology."],"url":"http://arxiv.org/abs/2402.07396v1","category":"quant-ph"}
{"created":"2024-02-12 03:54:44","title":"Cloud-cloud collision and cluster formation in the W5-NW complex","abstract":"We present a detailed structural and gas kinematic study of the star-forming complex W5-NW. A cloud-cloud collision scenario unravels with evidences of collision induced star and cluster formation. Various signatures of cloud-cloud collision such as \"complementary distribution\" and \"bridging-features\" are explored. At the colliding region, the two clouds have complementary morphologies, where W5-NWb has a filamentary key-like shape which fits into the U-shaped cavity in W5-NWa that behaves like a keyhole. The interaction region between the two clouds is characterised by bridging features with intermediate velocities connecting the two clouds. A skewed V-shaped bridging feature is also detected at the site of collision. A robust picture of the molecular gas distribution highlighting the bridges is seen in the position-position-velocity diagram obtained using the SCOUSEPY algorithm. Star cluster formation with an over-density of Class I and Class II young stellar objects is also seen towards this cloud complex, likely triggered by the cloud collision event.","sentences":["We present a detailed structural and gas kinematic study of the star-forming complex W5-NW.","A cloud-cloud collision scenario unravels with evidences of collision induced star and cluster formation.","Various signatures of cloud-cloud collision such as \"complementary distribution\" and \"bridging-features\" are explored.","At the colliding region, the two clouds have complementary morphologies, where W5-NWb has a filamentary key-like shape which fits into the U-shaped cavity in W5-NWa that behaves like a keyhole.","The interaction region between the two clouds is characterised by bridging features with intermediate velocities connecting the two clouds.","A skewed V-shaped bridging feature is also detected at the site of collision.","A robust picture of the molecular gas distribution highlighting the bridges is seen in the position-position-velocity diagram obtained using the SCOUSEPY algorithm.","Star cluster formation with an over-density of Class I and Class II young stellar objects is also seen towards this cloud complex, likely triggered by the cloud collision event."],"url":"http://arxiv.org/abs/2402.07394v1","category":"astro-ph.GA"}
{"created":"2024-02-12 18:17:01","title":"An approximation algorithm for Maximum DiCut vs. Cut","abstract":"Goemans and Williamson designed a 0.878-approximation algorithm for Max-Cut in undirected graphs [JACM'95]. Khot, Kindler, Mosel, and O'Donnel showed that the approximation ratio of the Goemans-Williamson algorithm is optimal assuming Khot's Unique Games Conjecture [SICOMP'07]. In the problem of maximum cuts in directed graphs (Max-DiCut), in which we seek as many edges going from one particular side of the cut to the other, the situation is more complicated but the recent work of Brakensiek, Huang, Potechin, and Zwick showed that their 0.874-approximation algorithm is tight under the Unique Games Conjecture (up to a small delta)[FOCS'23].   We consider a promise version of the problem and design an SDP-based algorithm which, if given a directed graph G that has a directed cut of value rho, finds an undirected cut in G (ignoring edge directions) with value at least \\rho.","sentences":["Goemans and Williamson designed a 0.878-approximation algorithm for Max-Cut in undirected graphs [JACM'95].","Khot, Kindler, Mosel, and O'Donnel showed that the approximation ratio of the Goemans-Williamson algorithm is optimal assuming","Khot's Unique Games Conjecture","[SICOMP'07].","In the problem of maximum cuts in directed graphs (Max-DiCut), in which we seek as many edges going from one particular side of the cut to the other, the situation is more complicated but the recent work of Brakensiek, Huang, Potechin, and Zwick showed that their 0.874-approximation algorithm is tight under the Unique Games Conjecture (up to a small delta)[FOCS'23].   ","We consider a promise version of the problem and design an SDP-based algorithm which, if given a directed graph G that has a directed cut of value rho, finds an undirected cut in G (ignoring edge directions) with value at least \\rho."],"url":"http://arxiv.org/abs/2402.07863v1","category":"cs.DS"}
{"created":"2024-02-12 18:12:52","title":"TOI-1199\\:b and TOI-1273\\:b: Two new transiting hot Saturns detected and characterized with SOPHIE and TESS","abstract":"We report the characterization of two planet candidates detected by the Transiting Exoplanet Survey Satellite (TESS), TOI-1199\\:b and TOI-1273\\:b, with periods of 3.7 and 4.6\\,days, respectively. Follow-up observations for both targets, which include several ground-based light curves, confirmed the transit events. High-precision radial velocities from the SOPHIE spectrograph revealed signals at the expected frequencies and phases of the transiting candidates and allowed mass determinations with a precision of 8.4\\% and 6.7\\% for TOI-1199\\:b and TOI-1273\\:b, respectively. The planetary and orbital parameters were derived from a joint analysis of the radial velocities and photometric data. We find that the planets have masses of 0.239$\\,\\pm\\,$0.020\\,M$_{\\mathrm{J}}$ \\ and 0.222$\\,\\pm\\,$0.015\\,M$_{\\mathrm{J}}$ \\ and radii of 0.938$\\,\\pm\\,$0.025\\,R$_{\\mathrm{J}}$ \\ and 0.99$\\,\\pm\\,$0.22\\,R$_{\\mathrm{J}}$,\\ respectively. The grazing transit of TOI-1273\\:b translates to a larger uncertainty in its radius, and hence also in its bulk density, compared to TOI-1199\\:b. The inferred bulk densities of 0.358$\\,\\pm\\,$0.041\\,g\\,cm$^{-3}$ \\ and 0.28$\\,\\pm\\,$0.11\\,g\\,cm$^{-3}$ \\ are among the lowest known for exoplanets in this mass range, which, considering the brightness of the host stars ($V$$\\approx$11\\,mag), render them particularly amenable to atmospheric characterization via the transit spectroscopy technique. The better constraints on the parameters of TOI-1199\\:b provide a transmission spectroscopy metric of 134\\,$\\pm$\\,17, making it the better suited of the two planets for atmospheric studies.","sentences":["We report the characterization of two planet candidates detected by the Transiting Exoplanet Survey Satellite (TESS), TOI-1199\\:b and TOI-1273\\:b, with periods of 3.7 and 4.6\\,days, respectively.","Follow-up observations for both targets, which include several ground-based light curves, confirmed the transit events.","High-precision radial velocities from the SOPHIE spectrograph revealed signals at the expected frequencies and phases of the transiting candidates and allowed mass determinations with a precision of 8.4\\% and 6.7\\% for TOI-1199\\:b and TOI-1273\\:b, respectively.","The planetary and orbital parameters were derived from a joint analysis of the radial velocities and photometric data.","We find that the planets have masses of 0.239$\\,\\pm\\,$0.020\\,M$_{\\mathrm{J}}$ \\ and 0.222$\\,\\pm\\,$0.015\\,M$_{\\mathrm{J}}$ \\ and radii of 0.938$\\,\\pm\\,$0.025\\,R$_{\\mathrm{J}}$ \\ and 0.99$\\,\\pm\\,$0.22\\,R$_{\\mathrm{J}}$,\\ respectively.","The grazing transit of TOI-1273\\:b translates to a larger uncertainty in its radius, and hence also in its bulk density, compared to TOI-1199\\:b.","The inferred bulk densities of 0.358$\\,\\pm\\,$0.041\\,g\\,cm$^{-3}$ \\ and 0.28$\\,\\pm\\,$0.11\\,g\\,cm$^{-3}$ \\ are among the lowest known for exoplanets in this mass range, which, considering the brightness of the host stars ($V$$\\approx$11\\,mag), render them particularly amenable to atmospheric characterization via the transit spectroscopy technique.","The better constraints on the parameters of TOI-1199\\:b provide a transmission spectroscopy metric of 134\\,$\\pm$\\,17, making it the better suited of the two planets for atmospheric studies."],"url":"http://arxiv.org/abs/2402.07861v1","category":"astro-ph.EP"}
{"created":"2024-02-12 18:00:37","title":"The Physical Properties of Low Redshift FeLoBAL Quasars. IV. Optical-Near IR Spectral Energy Distributions and Near-IR Variability Properties","abstract":"We present the optical-near infrared spectral energy distributions (SED) and near infrared variability properties of 30 low-redshift iron low-ionization Broad Absorption Line quasars (FeLoBALQs) and matched samples of LoBALQs and unabsorbed quasars. Significant correlations between the SED properties and accretion rate indicators found among the unabsorbed comparison sample objects suggest an intrinsic origin for SED differences. A range of reddening likely mutes these correlations among the FeLoBAL quasars. The restframe optical-band reddening is correlated with the location of the outflow, suggesting a link between the outflows and the presence of dust. We analyzed WISE variability and provide a correction for photometry uncertainties in an appendix. We found an anticorrelation between the variability amplitude and inferred continuum emission region size, and suggest that as the origin of the anticorrelation between variability amplitude and luminosity typically observed in quasars. We found that the LoBALQ optical emission line and other parameters are more similar to those of the unabsorbed continuum sample objects than the FeLoBALQs. Thus, FeLoBAL quasars are a special population of objects. We interpret the results using an accretion-rate scenario for FeLoBAL quasars. The high accretion rate FeLoBAL quasars are radiating powerfully enough to drive a thick, high-velocity outflow. Quasars with intermediate accretion rates may have an outflow, but it is not sufficiently thick to include FeII absorption. Low accretion rate FeLoBAL outflows originate in absorption in a failing torus, no longer optically thick enough to reprocess radiation into the near-IR.","sentences":["We present the optical-near infrared spectral energy distributions (SED) and near infrared variability properties of 30 low-redshift iron low-ionization Broad Absorption Line quasars (FeLoBALQs) and matched samples of LoBALQs and unabsorbed quasars.","Significant correlations between the SED properties and accretion rate indicators found among the unabsorbed comparison sample objects suggest an intrinsic origin for SED differences.","A range of reddening likely mutes these correlations among the FeLoBAL quasars.","The restframe optical-band reddening is correlated with the location of the outflow, suggesting a link between the outflows and the presence of dust.","We analyzed WISE variability and provide a correction for photometry uncertainties in an appendix.","We found an anticorrelation between the variability amplitude and inferred continuum emission region size, and suggest that as the origin of the anticorrelation between variability amplitude and luminosity typically observed in quasars.","We found that the LoBALQ optical emission line and other parameters are more similar to those of the unabsorbed continuum sample objects than the FeLoBALQs.","Thus, FeLoBAL quasars are a special population of objects.","We interpret the results using an accretion-rate scenario for FeLoBAL quasars.","The high accretion rate FeLoBAL quasars are radiating powerfully enough to drive a thick, high-velocity outflow.","Quasars with intermediate accretion rates may have an outflow, but it is not sufficiently thick to include FeII absorption.","Low accretion rate FeLoBAL outflows originate in absorption in a failing torus, no longer optically thick enough to reprocess radiation into the near-IR."],"url":"http://arxiv.org/abs/2402.07855v1","category":"astro-ph.GA"}
{"created":"2024-02-12 17:57:54","title":"Triply Periodic Helical Weaves","abstract":"Weaving typically involves forming a sufrace by interlacing fibers into a mechanically stable arrangement, effectively making a two-dimensional object out of one-dimensional objects. Moorish Fretwork involves interweaving solid helical elements into mechanically stable two-dimensional arrangements by exploiting the helices' screw symmetry. A three-dimensional extension of this idea was demonstrated by Alexandru Usineviciu at Bridges in 2015. Here we expand the idea further by considering cases informed by invariant cylindrical rod packing, and cases related to geodesics of the gyroid. Simulations and/or physical models of nineteen triply periodic arrangements of interwoven helices are shown, with physical models demonstrated for eight.","sentences":["Weaving typically involves forming a sufrace by interlacing fibers into a mechanically stable arrangement, effectively making a two-dimensional object out of one-dimensional objects.","Moorish Fretwork involves interweaving solid helical elements into mechanically stable two-dimensional arrangements by exploiting the helices' screw symmetry.","A three-dimensional extension of this idea was demonstrated by Alexandru Usineviciu at Bridges in 2015.","Here we expand the idea further by considering cases informed by invariant cylindrical rod packing, and cases related to geodesics of the gyroid.","Simulations and/or physical models of nineteen triply periodic arrangements of interwoven helices are shown, with physical models demonstrated for eight."],"url":"http://arxiv.org/abs/2402.07849v1","category":"math.GT"}
{"created":"2024-02-12 17:50:56","title":"Towards Meta-Pruning via Optimal Transport","abstract":"Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts. This paper introduces a novel approach named Intra-Fusion, challenging this prevailing pruning paradigm. Unlike existing methods that focus on designing meaningful neuron importance metrics, Intra-Fusion redefines the overlying pruning procedure. Through utilizing the concepts of model fusion and Optimal Transport, we leverage an agnostically given importance metric to arrive at a more effective sparse model representation. Notably, our approach achieves substantial accuracy recovery without the need for resource-intensive fine-tuning, making it an efficient and promising tool for neural network compression.   Additionally, we explore how fusion can be added to the pruning process to significantly decrease the training time while maintaining competitive performance. We benchmark our results for various networks on commonly used datasets such as CIFAR-10, CIFAR-100, and ImageNet. More broadly, we hope that the proposed Intra-Fusion approach invigorates exploration into a fresh alternative to the predominant compression approaches. Our code is available here: https://github.com/alexandertheus/Intra-Fusion.","sentences":["Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts.","This paper introduces a novel approach named Intra-Fusion, challenging this prevailing pruning paradigm.","Unlike existing methods that focus on designing meaningful neuron importance metrics, Intra-Fusion redefines the overlying pruning procedure.","Through utilizing the concepts of model fusion and Optimal Transport, we leverage an agnostically given importance metric to arrive at a more effective sparse model representation.","Notably, our approach achieves substantial accuracy recovery without the need for resource-intensive fine-tuning, making it an efficient and promising tool for neural network compression.   ","Additionally, we explore how fusion can be added to the pruning process to significantly decrease the training time while maintaining competitive performance.","We benchmark our results for various networks on commonly used datasets such as CIFAR-10, CIFAR-100, and ImageNet.","More broadly, we hope that the proposed Intra-Fusion approach invigorates exploration into a fresh alternative to the predominant compression approaches.","Our code is available here: https://github.com/alexandertheus/Intra-Fusion."],"url":"http://arxiv.org/abs/2402.07839v1","category":"cs.CV"}
{"created":"2024-02-12 17:27:32","title":"XZ-type Tanner-graph-recursive-expansion code","abstract":"Quantum stabilizer codes face the problem of low coding rate. In this letter, we propose a new class of quantum stabilizer codes called XZ-type Tanner-graph-recursive-expansion code. Though this code still have zero asymptotic coding rate, its coding rate tends to zero extremely slowly with the growth of code length. Under the same code length, its coding rate is much higher than that of surface code. We prove that the code distance of XZ-type Tanner-graph-recursive-expansion code is $O(log(N))$. Moreover, the code capacity noise threshold is around 0.078, which is obtained by fully decoupled belief propagation decoder. This letter shows that the idea of recursively expanding Tanner graph might have potential to construct quantum codes with better performance.","sentences":["Quantum stabilizer codes face the problem of low coding rate.","In this letter, we propose a new class of quantum stabilizer codes called XZ-type Tanner-graph-recursive-expansion code.","Though this code still have zero asymptotic coding rate, its coding rate tends to zero extremely slowly with the growth of code length.","Under the same code length, its coding rate is much higher than that of surface code.","We prove that the code distance of XZ-type Tanner-graph-recursive-expansion code is $O(log(N))$. Moreover, the code capacity noise threshold is around 0.078, which is obtained by fully decoupled belief propagation decoder.","This letter shows that the idea of recursively expanding Tanner graph might have potential to construct quantum codes with better performance."],"url":"http://arxiv.org/abs/2402.07823v1","category":"quant-ph"}
{"created":"2024-02-12 17:13:02","title":"Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation","abstract":"Scientific modeling applications often require estimating a distribution of parameters consistent with a dataset of observations - an inference task also known as source distribution estimation. This problem can be ill-posed, however, since many different source distributions might produce the same distribution of data-consistent simulations. To make a principled choice among many equally valid sources, we propose an approach which targets the maximum entropy distribution, i.e., prioritizes retaining as much uncertainty as possible. Our method is purely sample-based - leveraging the Sliced-Wasserstein distance to measure the discrepancy between the dataset and simulations - and thus suitable for simulators with intractable likelihoods. We benchmark our method on several tasks, and show that it can recover source distributions with substantially higher entropy without sacrificing the fidelity of the simulations. Finally, to demonstrate the utility of our approach, we infer source distributions for parameters of the Hodgkin-Huxley neuron model from experimental datasets with thousands of measurements. In summary, we propose a principled framework for inferring unique source distributions of scientific simulator parameters while retaining as much uncertainty as possible.","sentences":["Scientific modeling applications often require estimating a distribution of parameters consistent with a dataset of observations - an inference task also known as source distribution estimation.","This problem can be ill-posed, however, since many different source distributions might produce the same distribution of data-consistent simulations.","To make a principled choice among many equally valid sources, we propose an approach which targets the maximum entropy distribution, i.e., prioritizes retaining as much uncertainty as possible.","Our method is purely sample-based - leveraging the Sliced-Wasserstein distance to measure the discrepancy between the dataset and simulations - and thus suitable for simulators with intractable likelihoods.","We benchmark our method on several tasks, and show that it can recover source distributions with substantially higher entropy without sacrificing the fidelity of the simulations.","Finally, to demonstrate the utility of our approach, we infer source distributions for parameters of the Hodgkin-Huxley neuron model from experimental datasets with thousands of measurements.","In summary, we propose a principled framework for inferring unique source distributions of scientific simulator parameters while retaining as much uncertainty as possible."],"url":"http://arxiv.org/abs/2402.07808v1","category":"cs.LG"}
{"created":"2024-02-12 16:59:06","title":"Tuning-Free Stochastic Optimization","abstract":"Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of \"tuning-free\" algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD). When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible. We discuss conditions under which tuning-free optimization is possible even over unbounded domains. In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is sufficiently well-behaved. For the task of finding a stationary point of a smooth and potentially nonconvex function, we give a variant of SGD that matches the best-known high-probability convergence rate for tuned SGD at only an additional polylogarithmic cost. However, we also give an impossibility result that shows no algorithm can hope to match the optimal expected convergence rate for tuned SGD with high probability.","sentences":["Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive.","This creates a need for algorithms that can tune themselves on-the-fly.","We formalize the notion of \"tuning-free\" algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters.","We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD).","When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms.","We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible.","We discuss conditions under which tuning-free optimization is possible even over unbounded domains.","In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is sufficiently well-behaved.","For the task of finding a stationary point of a smooth and potentially nonconvex function, we give a variant of SGD that matches the best-known high-probability convergence rate for tuned SGD at only an additional polylogarithmic cost.","However, we also give an impossibility result that shows no algorithm can hope to match the optimal expected convergence rate for tuned SGD with high probability."],"url":"http://arxiv.org/abs/2402.07793v1","category":"math.OC"}
{"created":"2024-02-12 16:55:19","title":"From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration","abstract":"The assessment of binary classifier performance traditionally centers on discriminative ability using metrics, such as accuracy. However, these metrics often disregard the model's inherent uncertainty, especially when dealing with sensitive decision-making domains, such as finance or healthcare. Given that model-predicted scores are commonly seen as event probabilities, calibration is crucial for accurate interpretation. In our study, we analyze the sensitivity of various calibration measures to score distortions and introduce a refined metric, the Local Calibration Score. Comparing recalibration methods, we advocate for local regressions, emphasizing their dual role as effective recalibration tools and facilitators of smoother visualizations. We apply these findings in a real-world scenario using Random Forest classifier and regressor to predict credit default while simultaneously measuring calibration during performance optimization.","sentences":["The assessment of binary classifier performance traditionally centers on discriminative ability using metrics, such as accuracy.","However, these metrics often disregard the model's inherent uncertainty, especially when dealing with sensitive decision-making domains, such as finance or healthcare.","Given that model-predicted scores are commonly seen as event probabilities, calibration is crucial for accurate interpretation.","In our study, we analyze the sensitivity of various calibration measures to score distortions and introduce a refined metric, the Local Calibration Score.","Comparing recalibration methods, we advocate for local regressions, emphasizing their dual role as effective recalibration tools and facilitators of smoother visualizations.","We apply these findings in a real-world scenario using Random Forest classifier and regressor to predict credit default while simultaneously measuring calibration during performance optimization."],"url":"http://arxiv.org/abs/2402.07790v1","category":"cs.LG"}
{"created":"2024-02-12 16:47:08","title":"IR-Aware ECO Timing Optimization Using Reinforcement Learning","abstract":"Engineering change orders (ECOs) in late stages make minimal design fixes to recover from timing shifts due to excessive IR drops. This paper integrates IR-drop-aware timing analysis and ECO timing optimization using reinforcement learning (RL). The method operates after physical design and power grid synthesis, and rectifies IR-drop-induced timing degradation through gate sizing. It incorporates the Lagrangian relaxation (LR) technique into a novel RL framework, which trains a relational graph convolutional network (R-GCN) agent to sequentially size gates to fix timing violations. The R-GCN agent outperforms a classical LR-only algorithm: in an open 45nm technology, it (a) moves the Pareto front of the delay-area tradeoff curve to the left and (b) saves runtime over the classical method by running fast inference using trained models at iso-quality. The RL model is transferable across timing specifications, and transferable to unseen designs with zero-shot learning or fine tuning.","sentences":["Engineering change orders (ECOs) in late stages make minimal design fixes to recover from timing shifts due to excessive IR drops.","This paper integrates IR-drop-aware timing analysis and ECO timing optimization using reinforcement learning (RL).","The method operates after physical design and power grid synthesis, and rectifies IR-drop-induced timing degradation through gate sizing.","It incorporates the Lagrangian relaxation (LR) technique into a novel RL framework, which trains a relational graph convolutional network (R-GCN) agent to sequentially size gates to fix timing violations.","The R-GCN agent outperforms a classical LR-only algorithm: in an open 45nm technology, it (a) moves the Pareto front of the delay-area tradeoff curve to the left and (b) saves runtime over the classical method by running fast inference using trained models at iso-quality.","The RL model is transferable across timing specifications, and transferable to unseen designs with zero-shot learning or fine tuning."],"url":"http://arxiv.org/abs/2402.07781v1","category":"cs.AR"}
{"created":"2024-02-12 16:33:56","title":"Relativistic corrections to prompt double charmonium hadroproduction near threshold","abstract":"We calculate the relativistic corrections to prompt $J/\\psi$ pair and $J/\\psi+\\psi(2S)$ hadroproduction through the color-singlet channel within the framework of nonrelativistic QCD (NRQCD) factorization. The short-distance coefficients are obtained by matching full-QCD and NRQCD calculations at the partonic level, in which both squared amplitude and phase space are expanded in $v^2$. We find that such an expansion of the phase space spoils the convergence of NRQCD factorization near the production threshold. To fix this problem, we propose to modify the matching between full QCD and NRQCD by adopting the physical phase space. In this modified approach, the theoretical uncertainties due to the choice of charm quark mass $m_c$ are largely reduced and the overall agreement of our predictions with LHC data is significantly improved, both as for total and differential cross sections.","sentences":["We calculate the relativistic corrections to prompt $J/\\psi$ pair and $J/\\psi+\\psi(2S)$ hadroproduction through the color-singlet channel within the framework of nonrelativistic QCD (NRQCD) factorization.","The short-distance coefficients are obtained by matching full-QCD and NRQCD calculations at the partonic level, in which both squared amplitude and phase space are expanded in $v^2$. We find that such an expansion of the phase space spoils the convergence of NRQCD factorization near the production threshold.","To fix this problem, we propose to modify the matching between full QCD and NRQCD by adopting the physical phase space.","In this modified approach, the theoretical uncertainties due to the choice of charm quark mass $m_c$ are largely reduced and the overall agreement of our predictions with LHC data is significantly improved, both as for total and differential cross sections."],"url":"http://arxiv.org/abs/2402.07773v1","category":"hep-ph"}
{"created":"2024-02-12 16:30:03","title":"Multi-Facility Location Models Incorporating Multipurpose Shopping Trips","abstract":"This paper continues to develop and explore the impact of multipurpose trips on retail location. We develop the model of locating multiple competing facilities of a chain where several competing facilities exist in the area. There may be some existing facilities of the same chain as well. The addition of multiple new outlets can cause cannibalization of existing sales, but this effect is mitigated by selecting good locations, and the total market share captured by the chain increases. The introduction of multipurpose trips enhances the total market share of the location decision maker. Since in reality many customers combine a visit to more than one facility in one shopping trip, the model predicts the expected market share captured more accurately. Therefore, the selected locations for new facilities are more accurate as well.","sentences":["This paper continues to develop and explore the impact of multipurpose trips on retail location.","We develop the model of locating multiple competing facilities of a chain where several competing facilities exist in the area.","There may be some existing facilities of the same chain as well.","The addition of multiple new outlets can cause cannibalization of existing sales, but this effect is mitigated by selecting good locations, and the total market share captured by the chain increases.","The introduction of multipurpose trips enhances the total market share of the location decision maker.","Since in reality many customers combine a visit to more than one facility in one shopping trip, the model predicts the expected market share captured more accurately.","Therefore, the selected locations for new facilities are more accurate as well."],"url":"http://arxiv.org/abs/2402.07765v1","category":"math.OC"}
{"created":"2024-02-12 16:15:25","title":"Predictive Churn with the Set of Good Models","abstract":"Machine learning models in modern mass-market applications are often updated over time. One of the foremost challenges faced is that, despite increasing overall performance, these updates may flip specific model predictions in unpredictable ways. In practice, researchers quantify the number of unstable predictions between models pre and post update -- i.e., predictive churn. In this paper, we study this effect through the lens of predictive multiplicity -- i.e., the prevalence of conflicting predictions over the set of near-optimal models (the Rashomon set). We show how traditional measures of predictive multiplicity can be used to examine expected churn over this set of prospective models -- i.e., the set of models that may be used to replace a baseline model in deployment. We present theoretical results on the expected churn between models within the Rashomon set from different perspectives. And we characterize expected churn over model updates via the Rashomon set, pairing our analysis with empirical results on real-world datasets -- showing how our approach can be used to better anticipate, reduce, and avoid churn in consumer-facing applications. Further, we show that our approach is useful even for models enhanced with uncertainty awareness.","sentences":["Machine learning models in modern mass-market applications are often updated over time.","One of the foremost challenges faced is that, despite increasing overall performance, these updates may flip specific model predictions in unpredictable ways.","In practice, researchers quantify the number of unstable predictions between models pre and post update -- i.e., predictive churn.","In this paper, we study this effect through the lens of predictive multiplicity -- i.e., the prevalence of conflicting predictions over the set of near-optimal models (the Rashomon set).","We show how traditional measures of predictive multiplicity can be used to examine expected churn over this set of prospective models -- i.e., the set of models that may be used to replace a baseline model in deployment.","We present theoretical results on the expected churn between models within the Rashomon set from different perspectives.","And we characterize expected churn over model updates via the Rashomon set, pairing our analysis with empirical results on real-world datasets -- showing how our approach can be used to better anticipate, reduce, and avoid churn in consumer-facing applications.","Further, we show that our approach is useful even for models enhanced with uncertainty awareness."],"url":"http://arxiv.org/abs/2402.07745v1","category":"cs.LG"}
{"created":"2024-02-12 15:44:43","title":"Tuning Structural and Electronic Properties of Metal-Organic Framework 5 by Metal Substitution and Linker Functionalization","abstract":"The chemical flexibility of metal-organic frameworks (MOFs) offers an ideal platform to tune structure and composition for specific applications, from gas sensing to catalysis and from photoelectric conversion to energy storage. This variability gives rise to a large configurational space that can be efficiently explored using high-throughput computational methods. In this work, we investigate from first principles the structural and electronic properties of MOF-5 variants obtained by replacing Zn with Be, Mg, Cd, Ca, Sr, and Ba, and by functionalizing the originally H-passivated linkers with CH$_3$, NO$_2$, Cl, Br, NH$_2$, OH, and COOH groups. To build and analyze the resulting 56 structures, we employ density-functional theory calculations embedded in an in-house developed library for automatized calculations. Our findings reveal that structural properties are mainly defined by metal atoms and large functional groups which distort the lattice and modify coordination. Stability is largely influenced by functionalization and enhanced by COOH and OH groups which promote the formation of hydrogen bonds. The charge distribution within the linker is especially influenced by functional groups with electron-withdrawing character while the metal nodes play a minor role. Likewise, the band-gap size is crucially determined by ligand functionalization. The smallest gaps are found with NH$_2$ and OH groups which introduce localized orbitals at the top of the valence band. This characteristic makes these functionalizations particularly promising for the design of MOF-5 variants with enhanced gas uptake and sensing properties.","sentences":["The chemical flexibility of metal-organic frameworks (MOFs) offers an ideal platform to tune structure and composition for specific applications, from gas sensing to catalysis and from photoelectric conversion to energy storage.","This variability gives rise to a large configurational space that can be efficiently explored using high-throughput computational methods.","In this work, we investigate from first principles the structural and electronic properties of MOF-5 variants obtained by replacing Zn with Be, Mg, Cd, Ca, Sr, and Ba, and by functionalizing the originally H-passivated linkers with CH$_3$, NO$_2$, Cl, Br, NH$_2$, OH, and COOH groups.","To build and analyze the resulting 56 structures, we employ density-functional theory calculations embedded in an in-house developed library for automatized calculations.","Our findings reveal that structural properties are mainly defined by metal atoms and large functional groups which distort the lattice and modify coordination.","Stability is largely influenced by functionalization and enhanced by COOH and OH groups which promote the formation of hydrogen bonds.","The charge distribution within the linker is especially influenced by functional groups with electron-withdrawing character while the metal nodes play a minor role.","Likewise, the band-gap size is crucially determined by ligand functionalization.","The smallest gaps are found with NH$_2$ and OH groups which introduce localized orbitals at the top of the valence band.","This characteristic makes these functionalizations particularly promising for the design of MOF-5 variants with enhanced gas uptake and sensing properties."],"url":"http://arxiv.org/abs/2402.07733v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-12 15:34:56","title":"LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation","abstract":"Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources. But it still faces challenges of resource consumption when scaling up to larger models. Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem. However, these efforts only analyzed parameter features to evaluate their importance. Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model. To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output. We retain LoRA for important layers and the LoRA of the other layers share the same parameters. Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop.","sentences":["Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources.","But it still faces challenges of resource consumption when scaling up to larger models.","Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem.","However, these efforts only analyzed parameter features to evaluate their importance.","Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model.","To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output.","We retain LoRA for important layers and the LoRA of the other layers share the same parameters.","Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop."],"url":"http://arxiv.org/abs/2402.07721v1","category":"cs.LG"}
{"created":"2024-02-12 15:26:46","title":"Assembly bias in eBOSS","abstract":"Analytical models of galaxy-halo connection such as the Halo Occupation Distribution (HOD) model have been widely used over the past decades as a means to intensively test perturbative models on quasi-linear scales. However, these models fail to reproduce the galaxy-galaxy lensing signal on non-linear scales, over-predicting the observed signal up to 40%. With ongoing Stage-IV galaxy surveys such as DESI and EUCLID, it is now crucial to accurately model the galaxy-halo connection up to intra-halo scales to accurately estimate theoretical uncertainties of perturbative models. This paper compares the standard HOD model to an extended HOD framework that incorporates as additional features galaxy assembly bias and local environmental dependencies on halo occupation. These models have been calibrated against the observed clustering and galaxy-galaxy lensing signal of eBOSS Luminous Red Galaxies (LRG) and Emission Lines Galaxies (ELG) in the range 0.6 < z < 1.1. A combined clustering-lensing cosmological analysis is then performed on the simulated galaxy samples of both standard and extended HOD frameworks to quantify the systematic budget of perturbative models. The extended HOD model offers a more comprehensive understanding of the connection between galaxies and their surroundings. In particular, we found that the LRGs preferentially occupy denser and more anisotropic environments. Our results highlight the importance of considering environmental factors in galaxy formation models, with an extended HOD framework that reproduces the observed signal within 20% on scales below 10 Mpc/h. Our cosmological analysis reveals that our perturbative model yields similar constraints regardless of the galaxy population, with a better goodness of fit for the extended HOD. These results suggest that the extended HOD framework should be used to quantify modeling systematics.","sentences":["Analytical models of galaxy-halo connection such as the Halo Occupation Distribution (HOD) model have been widely used over the past decades as a means to intensively test perturbative models on quasi-linear scales.","However, these models fail to reproduce the galaxy-galaxy lensing signal on non-linear scales, over-predicting the observed signal up to 40%.","With ongoing Stage-IV galaxy surveys such as DESI and EUCLID, it is now crucial to accurately model the galaxy-halo connection up to intra-halo scales to accurately estimate theoretical uncertainties of perturbative models.","This paper compares the standard HOD model to an extended HOD framework that incorporates as additional features galaxy assembly bias and local environmental dependencies on halo occupation.","These models have been calibrated against the observed clustering and galaxy-galaxy lensing signal of eBOSS Luminous Red Galaxies (LRG) and Emission Lines Galaxies (ELG) in the range 0.6 <","z < 1.1.","A combined clustering-lensing cosmological analysis is then performed on the simulated galaxy samples of both standard and extended HOD frameworks to quantify the systematic budget of perturbative models.","The extended HOD model offers a more comprehensive understanding of the connection between galaxies and their surroundings.","In particular, we found that the LRGs preferentially occupy denser and more anisotropic environments.","Our results highlight the importance of considering environmental factors in galaxy formation models, with an extended HOD framework that reproduces the observed signal within 20% on scales below 10 Mpc/h. Our cosmological analysis reveals that our perturbative model yields similar constraints regardless of the galaxy population, with a better goodness of fit for the extended HOD.","These results suggest that the extended HOD framework should be used to quantify modeling systematics."],"url":"http://arxiv.org/abs/2402.07715v1","category":"astro-ph.CO"}
{"created":"2024-02-12 15:21:58","title":"Signed Distance Field based Segmentation and Statistical Shape Modelling of the Left Atrial Appendage","abstract":"Patients with atrial fibrillation have a 5-7 fold increased risk of having an ischemic stroke. In these cases, the most common site of thrombus localization is inside the left atrial appendage (LAA) and studies have shown a correlation between the LAA shape and the risk of ischemic stroke. These studies make use of manual measurement and qualitative assessment of shape and are therefore prone to large inter-observer discrepancies, which may explain the contradictions between the conclusions in different studies. We argue that quantitative shape descriptors are necessary to robustly characterize LAA morphology and relate to other functional parameters and stroke risk.   Deep Learning methods are becoming standardly available for segmenting cardiovascular structures from high resolution images such as computed tomography (CT), but only few have been tested for LAA segmentation. Furthermore, the majority of segmentation algorithms produces non-smooth 3D models that are not ideal for further processing, such as statistical shape analysis or computational fluid modelling. In this paper we present a fully automatic pipeline for image segmentation, mesh model creation and statistical shape modelling of the LAA. The LAA anatomy is implicitly represented as a signed distance field (SDF), which is directly regressed from the CT image using Deep Learning. The SDF is further used for registering the LAA shapes to a common template and build a statistical shape model (SSM). Based on 106 automatically segmented LAAs, the built SSM reveals that the LAA shape can be quantified using approximately 5 PCA modes and allows the identification of two distinct shape clusters corresponding to the so-called chicken-wing and non-chicken-wing morphologies.","sentences":["Patients with atrial fibrillation have a 5-7 fold increased risk of having an ischemic stroke.","In these cases, the most common site of thrombus localization is inside the left atrial appendage (LAA) and studies have shown a correlation between the LAA shape and the risk of ischemic stroke.","These studies make use of manual measurement and qualitative assessment of shape and are therefore prone to large inter-observer discrepancies, which may explain the contradictions between the conclusions in different studies.","We argue that quantitative shape descriptors are necessary to robustly characterize LAA morphology and relate to other functional parameters and stroke risk.   ","Deep Learning methods are becoming standardly available for segmenting cardiovascular structures from high resolution images such as computed tomography (CT), but only few have been tested for LAA segmentation.","Furthermore, the majority of segmentation algorithms produces non-smooth 3D models that are not ideal for further processing, such as statistical shape analysis or computational fluid modelling.","In this paper we present a fully automatic pipeline for image segmentation, mesh model creation and statistical shape modelling of the LAA.","The LAA anatomy is implicitly represented as a signed distance field (SDF), which is directly regressed from the CT image using Deep Learning.","The SDF is further used for registering the LAA shapes to a common template and build a statistical shape model (SSM).","Based on 106 automatically segmented LAAs, the built SSM reveals that the LAA shape can be quantified using approximately 5 PCA modes and allows the identification of two distinct shape clusters corresponding to the so-called chicken-wing and non-chicken-wing morphologies."],"url":"http://arxiv.org/abs/2402.07708v1","category":"cs.CV"}
{"created":"2024-02-12 15:06:56","title":"Optimal consumption and investment under relative performance criteria with Epstein-Zin utility","abstract":"We consider the strategic interaction of traders in a continuous-time financial market with Epstein-Zin-type recursive intertemporal preferences and performance concerns. We derive explicitly an equilibrium for the finite player and the mean-field version of the game, based on a study of geometric backward stochastic differential equations of Bernoulli type that describe the best replies of traders. Our results show that Epstein-Zin preferences can lead to substantially different equilibrium behavior.","sentences":["We consider the strategic interaction of traders in a continuous-time financial market with Epstein-Zin-type recursive intertemporal preferences and performance concerns.","We derive explicitly an equilibrium for the finite player and the mean-field version of the game, based on a study of geometric backward stochastic differential equations of Bernoulli type that describe the best replies of traders.","Our results show that Epstein-Zin preferences can lead to substantially different equilibrium behavior."],"url":"http://arxiv.org/abs/2402.07698v1","category":"math.OC"}
{"created":"2024-02-12 15:04:52","title":"Synthesizing Strongly Equivalent Logic Programs: Beth Definability for Answer Set Programs via Craig Interpolation in First-Order Logic","abstract":"We show a projective Beth definability theorem for logic programs under the stable model semantics: For given programs $P$ and $Q$ and vocabulary $V$ (set of predicates) the existence of a program $R$ in $V$ such that $P \\cup R$ and $P \\cup Q$ are strongly equivalent can be expressed as a first-order entailment. Moreover, our result is effective: A program $R$ can be constructed from a Craig interpolant for this entailment, using a known first-order encoding for testing strong equivalence, which we apply in reverse to extract programs from formulas. As a further perspective, this allows transforming logic programs via transforming their first-order encodings. In a prototypical implementation, the Craig interpolation is performed by first-order provers based on clausal tableaux or resolution calculi. Our work shows how definability and interpolation, which underlie modern logic-based approaches to advanced tasks in knowledge representation, transfer to answer set programming.","sentences":["We show a projective Beth definability theorem for logic programs under the stable model semantics: For given programs $P$ and $Q$ and vocabulary $V$ (set of predicates) the existence of a program $R$ in $V$ such that $P \\cup R$ and $P \\cup Q$ are strongly equivalent can be expressed as a first-order entailment.","Moreover, our result is effective: A program $R$ can be constructed from a Craig interpolant for this entailment, using a known first-order encoding for testing strong equivalence, which we apply in reverse to extract programs from formulas.","As a further perspective, this allows transforming logic programs via transforming their first-order encodings.","In a prototypical implementation, the Craig interpolation is performed by first-order provers based on clausal tableaux or resolution calculi.","Our work shows how definability and interpolation, which underlie modern logic-based approaches to advanced tasks in knowledge representation, transfer to answer set programming."],"url":"http://arxiv.org/abs/2402.07696v1","category":"cs.LO"}
{"created":"2024-02-12 12:56:40","title":"Propagation of logarithmic regularity and inviscid limit for the 2D Euler equations","abstract":"The aim of this note is to study the Cauchy problem for the 2D Euler equations under very low regularity assumptions on the initial datum. We prove propagation of regularity of logarithmic order in the class of weak solutions with $L^p$ initial vorticity, provided that $p\\geq 4$. We also study the inviscid limit from the 2D Navier-Stokes equations for vorticity with logarithmic regularity in the Yudovich class, showing a rate of convergence of order $|\\log\\nu|^{-\\alpha/2}$ with $\\alpha>0$.","sentences":["The aim of this note is to study the Cauchy problem for the 2D Euler equations under very low regularity assumptions on the initial datum.","We prove propagation of regularity of logarithmic order in the class of weak solutions with $L^p$ initial vorticity, provided that $p\\geq 4$. We also study the inviscid limit from the 2D Navier-Stokes equations for vorticity with logarithmic regularity in the Yudovich class, showing a rate of convergence of order $|\\log\\nu|^{-\\alpha/2}$ with $\\alpha>0$."],"url":"http://arxiv.org/abs/2402.07622v1","category":"math.AP"}
{"created":"2024-02-12 12:38:20","title":"Global optimality under amenable symmetry constraints","abstract":"We ask whether there exists a function or measure that (1) minimizes a given convex functional or risk and (2) satisfies a symmetry property specified by an amenable group of transformations. Examples of such symmetry properties are invariance, equivariance, or quasi-invariance. Our results draw on old ideas of Stein and Le Cam and on approximate group averages that appear in ergodic theorems for amenable groups. A class of convex sets known as orbitopes in convex analysis emerges as crucial, and we establish properties of such orbitopes in nonparametric settings. We also show how a simple device called a cocycle can be used to reduce different forms of symmetry to a single problem. As applications, we obtain results on invariant kernel mean embeddings and a Monge-Kantorovich theorem on optimality of transport plans under symmetry constraints. We also explain connections to the Hunt-Stein theorem on invariant tests.","sentences":["We ask whether there exists a function or measure that (1) minimizes a given convex functional or risk and (2) satisfies a symmetry property specified by an amenable group of transformations.","Examples of such symmetry properties are invariance, equivariance, or quasi-invariance.","Our results draw on old ideas of Stein and Le Cam and on approximate group averages that appear in ergodic theorems for amenable groups.","A class of convex sets known as orbitopes in convex analysis emerges as crucial, and we establish properties of such orbitopes in nonparametric settings.","We also show how a simple device called a cocycle can be used to reduce different forms of symmetry to a single problem.","As applications, we obtain results on invariant kernel mean embeddings and a Monge-Kantorovich theorem on optimality of transport plans under symmetry constraints.","We also explain connections to the Hunt-Stein theorem on invariant tests."],"url":"http://arxiv.org/abs/2402.07613v1","category":"math.ST"}
{"created":"2024-02-12 11:43:03","title":"Quantum phases of XY model with three-spin terms: interplay of topology and entanglement","abstract":"Magnetic and topological properties along with quantum correlations in terms of several entanglement measures have been investigated for an antiferromagnetic spin-1/2 XY model in the presence of transverse magnetic field and XZX$-$YZY type of three-spin interactions. Symmetries of the spin Hamiltonian have been identified. Under the Jordan-Wigner transformation, the spin Hamiltonian converted into spinless superconducting model with nearest neighbor hopping and Cooper pairing terms in addition to next nearest neighbor Cooper pairing potential. Long range antiferromagnetic order has been studied in terms of staggered spin-spin correlation functions, while the topological orders have been characterized by winding numbers. Magnetic and topological phase diagrams have been prepared. Faithful coexistence of magnetic and topological superconducting phases is found in the entire parameter regime. Boundaries of various quantum phases have been marked and positions of bicritical points have been identified.","sentences":["Magnetic and topological properties along with quantum correlations in terms of several entanglement measures have been investigated for an antiferromagnetic spin-1/2 XY model in the presence of transverse magnetic field and XZX$-$YZY type of three-spin interactions.","Symmetries of the spin Hamiltonian have been identified.","Under the Jordan-Wigner transformation, the spin Hamiltonian converted into spinless superconducting model with nearest neighbor hopping and Cooper pairing terms in addition to next nearest neighbor Cooper pairing potential.","Long range antiferromagnetic order has been studied in terms of staggered spin-spin correlation functions, while the topological orders have been characterized by winding numbers.","Magnetic and topological phase diagrams have been prepared.","Faithful coexistence of magnetic and topological superconducting phases is found in the entire parameter regime.","Boundaries of various quantum phases have been marked and positions of bicritical points have been identified."],"url":"http://arxiv.org/abs/2402.07590v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-12 10:42:17","title":"Total and Symmetry resolved Entanglement spectra in some Fermionic CFTs from the BCFT approach","abstract":"In this work, we study the universal total and symmetry-resolved entanglement spectra for a single interval of some $2$d Fermionic CFTs using the Boundary Conformal Field theory (BCFT) approach. In this approach, the partition of Hilbert space is achieved by cutting out discs around the entangling boundary points and imposing boundary conditions preserving the extended symmetry under scrutiny. The reduced density moments are then related to the BCFT partition functions and are also found to be diagonal in the symmetry charge sectors. In particular, we first study the entanglement spectra of massless Dirac fermion and modular invariant Dirac fermion by considering the boundary conditions preserving either the axial or the vector $U(1)$ symmetry. The total entanglement spectra of the modular invariant Dirac fermion are shown to match with the compact boson result at the duality radius, while for the massless Dirac fermion, it is found that the boundary entropy term doesn't match with the self-dual compact boson. The symmetry-resolved entanglement is found to be the same in all cases, except for the charge spectrum which is dependent on both the symmetry and the theory. We also study the entanglement spectra of $N$ massless Dirac fermions by considering boundary conditions preserving different chiral $U(1)^N$ symmetries. Entanglement spectra are studied for $U(1)^M$ subgroups, where $M\\leq N$, by imposing boundary conditions preserving different chiral symmetries. The total entanglement spectra are found to be sensitive to the representations of the $U(1)^M$ symmetry in the boundary theory among other behaviours at $O(1)$. Similar results are also found for the Symmetry resolved entanglement entropies. The characteristic $\\log\\log\\left(\\ell/\\epsilon\\right)$ term of the $U(1)$ symmetry is found to be proportional to $M$ in the symmetry-resolved entanglement spectra.","sentences":["In this work, we study the universal total and symmetry-resolved entanglement spectra for a single interval of some $2$d Fermionic CFTs using the Boundary Conformal Field theory (BCFT) approach.","In this approach, the partition of Hilbert space is achieved by cutting out discs around the entangling boundary points and imposing boundary conditions preserving the extended symmetry under scrutiny.","The reduced density moments are then related to the BCFT partition functions and are also found to be diagonal in the symmetry charge sectors.","In particular, we first study the entanglement spectra of massless Dirac fermion and modular invariant Dirac fermion by considering the boundary conditions preserving either the axial or the vector $U(1)$ symmetry.","The total entanglement spectra of the modular invariant Dirac fermion are shown to match with the compact boson result at the duality radius, while for the massless Dirac fermion, it is found that the boundary entropy term doesn't match with the self-dual compact boson.","The symmetry-resolved entanglement is found to be the same in all cases, except for the charge spectrum which is dependent on both the symmetry and the theory.","We also study the entanglement spectra of $N$ massless Dirac fermions by considering boundary conditions preserving different chiral $U(1)^N$ symmetries.","Entanglement spectra are studied for $U(1)^M$ subgroups, where $M\\leq N$, by imposing boundary conditions preserving different chiral symmetries.","The total entanglement spectra are found to be sensitive to the representations of the $U(1)^M$ symmetry in the boundary theory among other behaviours at $O(1)$. Similar results are also found for the Symmetry resolved entanglement entropies.","The characteristic $\\log\\log\\left(\\ell/\\epsilon\\right)$ term of the $U(1)$ symmetry is found to be proportional to $M$ in the symmetry-resolved entanglement spectra."],"url":"http://arxiv.org/abs/2402.07557v1","category":"hep-th"}
{"created":"2024-02-12 10:37:54","title":"Thermodynamically consistent modelling of viscoelastic solids under finite strain","abstract":"The present article is concerned with modelling the viscoelastic behavior of Polydimethylsiloxane (PDMS) in large-strain regime. Starting from the basic principles of thermodynamics, an incremental variational formulation is derived. Within this model, the free energy density and dissipation function determine elastic and viscous properties of the solid. The main contribution of this paper is the estimation of the parameters in the proposed phenomenological model from measurements conducted on PDMS samples. This non-linear material model simplifies to a Prony-series representation in frequency domain in case of small deformations. The coefficients of this Prony-series are detected from dynamical temperature mechanical analysis measurements. Time-temperature superposition allows to combine measurements at different temperatures, such that a sufficiently large frequency range is available for subsequent fitting of Prony-parameters. A set of material parameters is thus provided. The incremental variational formulation directly lends itself to finite element discretization, where an efficient and stable choice of elements is proposed for radially symmetric problems. This formulation allows to verify the proposed model against experimental data gained in ball-drop experiments.","sentences":["The present article is concerned with modelling the viscoelastic behavior of Polydimethylsiloxane (PDMS) in large-strain regime.","Starting from the basic principles of thermodynamics, an incremental variational formulation is derived.","Within this model, the free energy density and dissipation function determine elastic and viscous properties of the solid.","The main contribution of this paper is the estimation of the parameters in the proposed phenomenological model from measurements conducted on PDMS samples.","This non-linear material model simplifies to a Prony-series representation in frequency domain in case of small deformations.","The coefficients of this Prony-series are detected from dynamical temperature mechanical analysis measurements.","Time-temperature superposition allows to combine measurements at different temperatures, such that a sufficiently large frequency range is available for subsequent fitting of Prony-parameters.","A set of material parameters is thus provided.","The incremental variational formulation directly lends itself to finite element discretization, where an efficient and stable choice of elements is proposed for radially symmetric problems.","This formulation allows to verify the proposed model against experimental data gained in ball-drop experiments."],"url":"http://arxiv.org/abs/2402.07555v1","category":"physics.app-ph"}
{"created":"2024-02-12 09:47:50","title":"Reinforcement learning based demand charge minimization using energy storage","abstract":"Utilities have introduced demand charges to encourage customers to reduce their demand peaks, since a high peak may cause very high costs for both the utility and the consumer. We herein study the bill minimization problem for customers equipped with an energy storage device and a self-owned renewable energy production. A model-free reinforcement learning algorithm is carefully designed to reduce both the energy charge and the demand charge of the consumer. The proposed algorithm does not need forecasting models for the energy demand and the renewable energy production. The resulting controller can be used online, and progressively improved with newly gathered data. The algorithm is validated on real data from an office building of IFPEN Solaize site. Numerical results show that our algorithm can reduce electricity bills with both daily and monthly demand charges.","sentences":["Utilities have introduced demand charges to encourage customers to reduce their demand peaks, since a high peak may cause very high costs for both the utility and the consumer.","We herein study the bill minimization problem for customers equipped with an energy storage device and a self-owned renewable energy production.","A model-free reinforcement learning algorithm is carefully designed to reduce both the energy charge and the demand charge of the consumer.","The proposed algorithm does not need forecasting models for the energy demand and the renewable energy production.","The resulting controller can be used online, and progressively improved with newly gathered data.","The algorithm is validated on real data from an office building of IFPEN Solaize site.","Numerical results show that our algorithm can reduce electricity bills with both daily and monthly demand charges."],"url":"http://arxiv.org/abs/2402.07525v1","category":"math.OC"}
{"created":"2024-02-12 09:42:57","title":"Fortran... ok, and what's next?","abstract":"Modern Fortran is a standardized language that includes object-oriented and parallel programming paradigms. The Fortran-lang community, created at the end of 2019, is actively working to modernize its ecosystem. New compilers are under development. And the fourth Fortran standard of the 21st century is due to be published in autumn 2023.","sentences":["Modern Fortran is a standardized language that includes object-oriented and parallel programming paradigms.","The Fortran-lang community, created at the end of 2019, is actively working to modernize its ecosystem.","New compilers are under development.","And the fourth Fortran standard of the 21st century is due to be published in autumn 2023."],"url":"http://arxiv.org/abs/2402.07520v1","category":"cs.DC"}
{"created":"2024-02-12 09:29:11","title":"Remarks on variable Lebesgue spaces and fractional Navier-Stokes equations","abstract":"In this work we study the 3D Navier-Stokes equations, under the action of an external force and with the fractional Laplacian operator $(-\\Delta)^{\\alpha}$ in the diffusion term, from the point of view of variable Lebesgue spaces. Based on decay estimates of the fractional heat kernel we prove the existence and uniqueness of mild solutions on this functional setting. Thus, in a first theorem we obtain an unique local-in-time solution in the space $L^{p(\\cdot)} \\left( [0,T], L^{q} (\\mathbb{R}^3) \\right)$. As a bi-product, in a second theorem we prove the existence of an unique global-in-time solution in the mixed-space $\\mathcal{L}^{p(\\cdot)}_{\\frac{3}{2\\alpha -1}}(\\mathbb{R}^3, L^\\infty([0,T[))$.","sentences":["In this work we study the 3D Navier-Stokes equations, under the action of an external force and with the fractional Laplacian operator $(-\\Delta)^{\\alpha}$ in the diffusion term, from the point of view of variable Lebesgue spaces.","Based on decay estimates of the fractional heat kernel we prove the existence and uniqueness of mild solutions on this functional setting.","Thus, in a first theorem we obtain an unique local-in-time solution in the space $L^{p(\\cdot)} \\left( [0,T], L^{q} (\\mathbb{R}^3) \\right)$.","As a bi-product, in a second theorem we prove the existence of an unique global-in-time solution in the mixed-space $\\mathcal{L}^{p(\\cdot)}_{\\frac{3}{2\\alpha -1}}(\\mathbb{R}^3, L^\\infty([0,T[))$."],"url":"http://arxiv.org/abs/2402.07508v1","category":"math.AP"}
{"created":"2024-02-12 08:14:03","title":"Pushing The Limit of LLM Capacity for Text Classification","abstract":"The value of text classification's future research has encountered challenges and uncertainties, due to the extraordinary efficacy demonstrated by large language models (LLMs) across numerous downstream NLP tasks. In this era of open-ended language modeling, where task boundaries are gradually fading, an urgent question emerges: have we made significant advances in text classification under the full benefit of LLMs? To answer this question, we propose RGPT, an adaptive boosting framework tailored to produce a specialized text classification LLM by recurrently ensembling a pool of strong base learners. The base learners are constructed by adaptively adjusting the distribution of training samples and iteratively fine-tuning LLMs with them. Such base learners are then ensembled to be a specialized text classification LLM, by recurrently incorporating the historical predictions from the previous learners. Through a comprehensive empirical comparison, we show that RGPT significantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by 1.36% on average. Further evaluation experiments show a clear surpassing of RGPT over human classification.","sentences":["The value of text classification's future research has encountered challenges and uncertainties, due to the extraordinary efficacy demonstrated by large language models (LLMs) across numerous downstream NLP tasks.","In this era of open-ended language modeling, where task boundaries are gradually fading, an urgent question emerges: have we made significant advances in text classification under the full benefit of LLMs?","To answer this question, we propose RGPT, an adaptive boosting framework tailored to produce a specialized text classification LLM by recurrently ensembling a pool of strong base learners.","The base learners are constructed by adaptively adjusting the distribution of training samples and iteratively fine-tuning LLMs with them.","Such base learners are then ensembled to be a specialized text classification LLM, by recurrently incorporating the historical predictions from the previous learners.","Through a comprehensive empirical comparison, we show that RGPT significantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by 1.36% on average.","Further evaluation experiments show a clear surpassing of RGPT over human classification."],"url":"http://arxiv.org/abs/2402.07470v1","category":"cs.CL"}
{"created":"2024-02-12 07:20:26","title":"Quantum transport enabled by non-adiabatic transitions","abstract":"Quantum transport of charge or energy in networks with discrete sites is central to diverse quantum technologies, from molecular electronics to light harvesting and quantum opto-mechanical metamaterials. A one dimensional network can be viewed as waveguide. We show that if such waveguide is hybridised with a control unit that contains a few sites, then transmission through the waveguide depends sensitively on the motion of the sites in the control unit. Together, the hybrid waveguide and its control-unit form a Fano-Anderson chain whose Born-Oppenheimer surfaces inherit characteristics from both components: A bandstructure from the waveguide and potential energy steps as a function of site coordinates from the control-unit. Using time-dependent quantum wave packets, we reveal conditions under which the hybrid structure becomes transmissive only if the control unit contains mobile sites that induce non-adiabatic transitions between the surfaces. Hence, our approach provides functional synthetic Born-Oppenheimer surfaces for hybrid quantum technologies combining mechanic and excitonic elements, and has possible applications such as switching and temperature sensing.","sentences":["Quantum transport of charge or energy in networks with discrete sites is central to diverse quantum technologies, from molecular electronics to light harvesting and quantum opto-mechanical metamaterials.","A one dimensional network can be viewed as waveguide.","We show that if such waveguide is hybridised with a control unit that contains a few sites, then transmission through the waveguide depends sensitively on the motion of the sites in the control unit.","Together, the hybrid waveguide and its control-unit form a Fano-Anderson chain whose Born-Oppenheimer surfaces inherit characteristics from both components: A bandstructure from the waveguide and potential energy steps as a function of site coordinates from the control-unit.","Using time-dependent quantum wave packets, we reveal conditions under which the hybrid structure becomes transmissive only if the control unit contains mobile sites that induce non-adiabatic transitions between the surfaces.","Hence, our approach provides functional synthetic Born-Oppenheimer surfaces for hybrid quantum technologies combining mechanic and excitonic elements, and has possible applications such as switching and temperature sensing."],"url":"http://arxiv.org/abs/2402.07454v1","category":"quant-ph"}
{"created":"2024-02-12 07:20:05","title":"Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs","abstract":"Consider the domain of multiclass classification within the adversarial online setting. What is the price of relying on bandit feedback as opposed to full information? To what extent can an adaptive adversary amplify the loss compared to an oblivious one? To what extent can a randomized learner reduce the loss compared to a deterministic one? We study these questions in the mistake bound model and provide nearly tight answers.   We demonstrate that the optimal mistake bound under bandit feedback is at most $O(k)$ times higher than the optimal mistake bound in the full information case, where $k$ represents the number of labels. This bound is tight and provides an answer to an open question previously posed and studied by Daniely and Helbertal ['13] and by Long ['17, '20], who focused on deterministic learners.   Moreover, we present nearly optimal bounds of $\\tilde{\\Theta}(k)$ on the gap between randomized and deterministic learners, as well as between adaptive and oblivious adversaries in the bandit feedback setting. This stands in contrast to the full information scenario, where adaptive and oblivious adversaries are equivalent, and the gap in mistake bounds between randomized and deterministic learners is a constant multiplicative factor of $2$.   In addition, our results imply that in some cases the optimal randomized mistake bound is approximately the square-root of its deterministic parallel. Previous results show that this is essentially the smallest it can get.","sentences":["Consider the domain of multiclass classification within the adversarial online setting.","What is the price of relying on bandit feedback as opposed to full information?","To what extent can an adaptive adversary amplify the loss compared to an oblivious one?","To what extent can a randomized learner reduce the loss compared to a deterministic one?","We study these questions in the mistake bound model and provide nearly tight answers.   ","We demonstrate that the optimal mistake bound under bandit feedback is at most $O(k)$ times higher than the optimal mistake bound in the full information case, where $k$ represents the number of labels.","This bound is tight and provides an answer to an open question previously posed and studied by Daniely and Helbertal ['13] and by Long ['17, '20], who focused on deterministic learners.   ","Moreover, we present nearly optimal bounds of $\\tilde{\\Theta}(k)$ on the gap between randomized and deterministic learners, as well as between adaptive and oblivious adversaries in the bandit feedback setting.","This stands in contrast to the full information scenario, where adaptive and oblivious adversaries are equivalent, and the gap in mistake bounds between randomized and deterministic learners is a constant multiplicative factor of $2$.   In addition, our results imply that in some cases the optimal randomized mistake bound is approximately the square-root of its deterministic parallel.","Previous results show that this is essentially the smallest it can get."],"url":"http://arxiv.org/abs/2402.07453v1","category":"cs.LG"}
{"created":"2024-02-12 07:14:25","title":"Machine learning mapping of lattice correlated data","abstract":"We discuss a novel approach based on machine learning (ML) regression models to reduce the computational cost of disconnected diagrams in lattice QCD calculations. This method creates a mapping between the results of fermionic loops computed at different quark masses and flow times. The ML model, trained with just a small fraction of the complete data set, provides similar predictions and uncertainties over the calculation done over the whole ensemble, resulting in a significant computational gain.","sentences":["We discuss a novel approach based on machine learning (ML) regression models to reduce the computational cost of disconnected diagrams in lattice QCD calculations.","This method creates a mapping between the results of fermionic loops computed at different quark masses and flow times.","The ML model, trained with just a small fraction of the complete data set, provides similar predictions and uncertainties over the calculation done over the whole ensemble, resulting in a significant computational gain."],"url":"http://arxiv.org/abs/2402.07450v1","category":"hep-lat"}
{"created":"2024-02-12 07:03:14","title":"Quality Does Matter: A Detailed Look at the Quality and Utility of Web-Mined Parallel Corpora","abstract":"We conducted a detailed analysis on the quality of web-mined corpora for two low-resource languages (making three language pairs, English-Sinhala, English-Tamil and Sinhala-Tamil). We ranked each corpus according to a similarity measure and carried out an intrinsic and extrinsic evaluation on different portions of this ranked corpus. We show that there are significant quality differences between different portions of web-mined corpora and that the quality varies across languages and datasets. We also show that, for some web-mined datasets, Neural Machine Translation (NMT) models trained with their highest-ranked 25k portion can be on par with human-curated datasets.","sentences":["We conducted a detailed analysis on the quality of web-mined corpora for two low-resource languages (making three language pairs, English-Sinhala, English-Tamil and Sinhala-Tamil).","We ranked each corpus according to a similarity measure and carried out an intrinsic and extrinsic evaluation on different portions of this ranked corpus.","We show that there are significant quality differences between different portions of web-mined corpora and that the quality varies across languages and datasets.","We also show that, for some web-mined datasets, Neural Machine Translation (NMT) models trained with their highest-ranked 25k portion can be on par with human-curated datasets."],"url":"http://arxiv.org/abs/2402.07446v1","category":"cs.CL"}
{"created":"2024-02-12 06:04:41","title":"Computational Aspects of Bayesian Persuasion under Approximate Best Response","abstract":"We study Bayesian persuasion under approximate best response, where the receiver may choose any action that is not too much suboptimal, given their posterior belief upon receiving the signal. We focus on the computational aspects of the problem, aiming to design algorithms that efficiently compute (almost) optimal strategies for the sender. Despite the absence of the revelation principle -- which has been one of the most powerful tools in Bayesian persuasion -- we design polynomial-time exact algorithms for the problem when either the state space or the action space is small, as well as a quasi-polynomial-time approximation scheme (QPTAS) for the general problem. On the negative side, we show there is no polynomial-time exact algorithm for the general problem unless $\\mathsf{P} = \\mathsf{NP}$. Our results build on several new algorithmic ideas, which might be useful in other principal-agent problems where robustness is desired.","sentences":["We study Bayesian persuasion under approximate best response, where the receiver may choose any action that is not too much suboptimal, given their posterior belief upon receiving the signal.","We focus on the computational aspects of the problem, aiming to design algorithms that efficiently compute (almost) optimal strategies for the sender.","Despite the absence of the revelation principle -- which has been one of the most powerful tools in Bayesian persuasion -- we design polynomial-time exact algorithms for the problem when either the state space or the action space is small, as well as a quasi-polynomial-time approximation scheme (QPTAS) for the general problem.","On the negative side, we show there is no polynomial-time exact algorithm for the general problem unless $\\mathsf{P} = \\mathsf{NP}$. Our results build on several new algorithmic ideas, which might be useful in other principal-agent problems where robustness is desired."],"url":"http://arxiv.org/abs/2402.07426v1","category":"cs.GT"}
{"created":"2024-02-12 05:44:10","title":"An Empirical Study Into What Matters for Calibrating Vision-Language Models","abstract":"Vision--Language Models (VLMs) have emerged as the dominant approach for zero-shot recognition, adept at handling diverse scenarios and significant distribution changes. However, their deployment in risk-sensitive areas requires a deeper understanding of their uncertainty estimation capabilities, a relatively uncharted area. In this study, we explore the calibration properties of VLMs across different architectures, datasets, and training strategies. In particular, we analyze the uncertainty estimation performance of VLMs when calibrated in one domain, label set or hierarchy level, and tested in a different one. Our findings reveal that while VLMs are not inherently calibrated for uncertainty, temperature scaling significantly and consistently improves calibration, even across shifts in distribution and changes in label set. Moreover, VLMs can be calibrated with a very small set of examples. Through detailed experimentation, we highlight the potential applications and importance of our insights, aiming for more reliable and effective use of VLMs in critical, real-world scenarios.","sentences":["Vision--Language Models (VLMs) have emerged as the dominant approach for zero-shot recognition, adept at handling diverse scenarios and significant distribution changes.","However, their deployment in risk-sensitive areas requires a deeper understanding of their uncertainty estimation capabilities, a relatively uncharted area.","In this study, we explore the calibration properties of VLMs across different architectures, datasets, and training strategies.","In particular, we analyze the uncertainty estimation performance of VLMs when calibrated in one domain, label set or hierarchy level, and tested in a different one.","Our findings reveal that while VLMs are not inherently calibrated for uncertainty, temperature scaling significantly and consistently improves calibration, even across shifts in distribution and changes in label set.","Moreover, VLMs can be calibrated with a very small set of examples.","Through detailed experimentation, we highlight the potential applications and importance of our insights, aiming for more reliable and effective use of VLMs in critical, real-world scenarios."],"url":"http://arxiv.org/abs/2402.07417v1","category":"cs.CV"}
{"created":"2024-02-12 04:40:19","title":"Make it more specific: A novel uncertainty based airway segmentation application on 3D U-Net and its variants","abstract":"Each medical segmentation task should be considered with a specific AI algorithm based on its scenario so that the most accurate prediction model can be obtained. The most popular algorithms in medical segmentation, 3D U-Net and its variants, can directly implement the task of lung trachea segmentation, but its failure to consider the special tree-like structure of the trachea suggests that there is much room for improvement in its segmentation accuracy. Therefore, a research gap exists because a great amount of state-of-the-art DL algorithms are vanilla 3D U-Net structures, which do not introduce the various performance-enhancing modules that come with special natural image modality in lung airway segmentation. In this paper, we proposed two different network structures Branch-Level U-Net (B-UNet) and Branch-Level CE-UNet (B-CE-UNet) which are based on U-Net structure and compared the prediction results with the same dataset. Specially, both of the two networks add branch loss and central line loss to learn the feature of fine branch endings of the airways. Uncertainty estimation algorithms are also included to attain confident predictions and thereby, increase the overall trustworthiness of our whole model. In addition, predictions of the lung trachea based on the maximum connectivity rate were calculated and extracted during post-processing for segmentation refinement and pruning.","sentences":["Each medical segmentation task should be considered with a specific AI algorithm based on its scenario so that the most accurate prediction model can be obtained.","The most popular algorithms in medical segmentation, 3D U-Net and its variants, can directly implement the task of lung trachea segmentation, but its failure to consider the special tree-like structure of the trachea suggests that there is much room for improvement in its segmentation accuracy.","Therefore, a research gap exists because a great amount of state-of-the-art DL algorithms are vanilla 3D U-Net structures, which do not introduce the various performance-enhancing modules that come with special natural image modality in lung airway segmentation.","In this paper, we proposed two different network structures Branch-Level U-Net (B-UNet) and Branch-Level CE-UNet (B-CE-UNet) which are based on U-Net structure and compared the prediction results with the same dataset.","Specially, both of the two networks add branch loss and central line loss to learn the feature of fine branch endings of the airways.","Uncertainty estimation algorithms are also included to attain confident predictions and thereby, increase the overall trustworthiness of our whole model.","In addition, predictions of the lung trachea based on the maximum connectivity rate were calculated and extracted during post-processing for segmentation refinement and pruning."],"url":"http://arxiv.org/abs/2402.07403v1","category":"cs.CV"}
{"created":"2024-02-12 03:31:34","title":"Replicability is Asymptotically Free in Multi-armed Bandits","abstract":"This work is motivated by the growing demand for reproducible machine learning. We study the stochastic multi-armed bandit problem. In particular, we consider a replicable algorithm that ensures, with high probability, that the algorithm's sequence of actions is not affected by the randomness inherent in the dataset. We observe that existing algorithms require $O(1/\\rho^2)$ times more regret than nonreplicable algorithms, where $\\rho$ is the level of nonreplication. However, we demonstrate that this additional cost is unnecessary when the time horizon $T$ is sufficiently large for a given $\\rho$, provided that the magnitude of the confidence bounds is chosen carefully. We introduce an explore-then-commit algorithm that draws arms uniformly before committing to a single arm. Additionally, we examine a successive elimination algorithm that eliminates suboptimal arms at the end of each phase. To ensure the replicability of these algorithms, we incorporate randomness into their decision-making processes. We extend the use of successive elimination to the linear bandit problem as well. For the analysis of these algorithms, we propose a principled approach to limiting the probability of nonreplication. This approach elucidates the steps that existing research has implicitly followed. Furthermore, we derive the first lower bound for the two-armed replicable bandit problem, which implies the optimality of the proposed algorithms up to a $\\log\\log T$ factor for the two-armed case.","sentences":["This work is motivated by the growing demand for reproducible machine learning.","We study the stochastic multi-armed bandit problem.","In particular, we consider a replicable algorithm that ensures, with high probability, that the algorithm's sequence of actions is not affected by the randomness inherent in the dataset.","We observe that existing algorithms require $O(1/\\rho^2)$ times more regret than nonreplicable algorithms, where $\\rho$ is the level of nonreplication.","However, we demonstrate that this additional cost is unnecessary when the time horizon $T$ is sufficiently large for a given $\\rho$, provided that the magnitude of the confidence bounds is chosen carefully.","We introduce an explore-then-commit algorithm that draws arms uniformly before committing to a single arm.","Additionally, we examine a successive elimination algorithm that eliminates suboptimal arms at the end of each phase.","To ensure the replicability of these algorithms, we incorporate randomness into their decision-making processes.","We extend the use of successive elimination to the linear bandit problem as well.","For the analysis of these algorithms, we propose a principled approach to limiting the probability of nonreplication.","This approach elucidates the steps that existing research has implicitly followed.","Furthermore, we derive the first lower bound for the two-armed replicable bandit problem, which implies the optimality of the proposed algorithms up to a $\\log\\log T$ factor for the two-armed case."],"url":"http://arxiv.org/abs/2402.07391v1","category":"stat.ML"}
{"created":"2024-02-12 03:24:54","title":"Chemical inhomogeneities in high-entropy alloys help mitigate the strength-ductility trade-off","abstract":"Metallurgists have long been accustomed to a trade-off between yield strength and tensile ductility. Extending previously known strain-hardening mechanisms, the emerging multi-principal-element alloys (MPEAs) offer additional help in promoting the strength-ductility synergy, towards gigapascal yield strength simultaneously with pure-metal-like tensile ductility. The highly concentrated chemical make-up in these 'high-entropy' alloys (HEAs) adds, at ultrafine spatial scale from sub-nanometer to tens of nanometers, inherent chemical inhomogeneities in local composition and local chemical order (LCO). These institute a 'nano-cocktail' environment that exerts extra dragging forces, rendering a much wavier motion of dislocation lines (in stick-slip mode) different from dilute solutions. The variable fault energy landscape also makes the dislocation movement sluggish, increasing their chances to hit one another and react to increase entanglement. The accumulation of dislocations (plus faults) dynamically stores obstacles against ensuing dislocation motion to sustain an adequate strain-hardening rate at high flow stresses, delaying plastic instability to enable large (uniform) elongation. The successes summarized advocate MPEAs as an effective recipe towards ultrahigh strength at little expense of tensile ductility. The insight gained also answers the question as to what new mechanical behavior the HEAs have to offer, beyond what has been well documented for traditional metals and solid solutions.","sentences":["Metallurgists have long been accustomed to a trade-off between yield strength and tensile ductility.","Extending previously known strain-hardening mechanisms, the emerging multi-principal-element alloys (MPEAs) offer additional help in promoting the strength-ductility synergy, towards gigapascal yield strength simultaneously with pure-metal-like tensile ductility.","The highly concentrated chemical make-up in these 'high-entropy' alloys (HEAs) adds, at ultrafine spatial scale from sub-nanometer to tens of nanometers, inherent chemical inhomogeneities in local composition and local chemical order (LCO).","These institute a 'nano-cocktail' environment that exerts extra dragging forces, rendering a much wavier motion of dislocation lines (in stick-slip mode) different from dilute solutions.","The variable fault energy landscape also makes the dislocation movement sluggish, increasing their chances to hit one another and react to increase entanglement.","The accumulation of dislocations (plus faults) dynamically stores obstacles against ensuing dislocation motion to sustain an adequate strain-hardening rate at high flow stresses, delaying plastic instability to enable large (uniform) elongation.","The successes summarized advocate MPEAs as an effective recipe towards ultrahigh strength at little expense of tensile ductility.","The insight gained also answers the question as to what new mechanical behavior the HEAs have to offer, beyond what has been well documented for traditional metals and solid solutions."],"url":"http://arxiv.org/abs/2402.07389v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-12 03:19:30","title":"The Limits of Assumption-free Tests for Algorithm Performance","abstract":"Algorithm evaluation and comparison are fundamental questions in machine learning and statistics -- how well does an algorithm perform at a given modeling task, and which algorithm performs best? Many methods have been developed to assess algorithm performance, often based around cross-validation type strategies, retraining the algorithm of interest on different subsets of the data and assessing its performance on the held-out data points. Despite the broad use of such procedures, the theoretical properties of these methods are not yet fully understood. In this work, we explore some fundamental limits for answering these questions with limited amounts of data. In particular, we make a distinction between two questions: how good is an algorithm $A$ at the problem of learning from a training set of size $n$, versus, how good is a particular fitted model produced by running $A$ on a particular training data set of size $n$?   Our main results prove that, for any test that treats the algorithm $A$ as a ``black box'' (i.e., we can only study the behavior of $A$ empirically), there is a fundamental limit on our ability to carry out inference on the performance of $A$, unless the number of available data points $N$ is many times larger than the sample size $n$ of interest. (On the other hand, evaluating the performance of a particular fitted model is easy as long as a holdout data set is available -- that is, as long as $N-n$ is not too small.) We also ask whether an assumption of algorithmic stability might be sufficient to circumvent this hardness result. Surprisingly, we find that this is not the case: the same hardness result still holds for the problem of evaluating the performance of $A$, aside from a high-stability regime where fitted models are essentially nonrandom. Finally, we also establish similar hardness results for the problem of comparing multiple algorithms.","sentences":["Algorithm evaluation and comparison are fundamental questions in machine learning and statistics -- how well does an algorithm perform at a given modeling task, and which algorithm performs best?","Many methods have been developed to assess algorithm performance, often based around cross-validation type strategies, retraining the algorithm of interest on different subsets of the data and assessing its performance on the held-out data points.","Despite the broad use of such procedures, the theoretical properties of these methods are not yet fully understood.","In this work, we explore some fundamental limits for answering these questions with limited amounts of data.","In particular, we make a distinction between two questions: how good is an algorithm $A$ at the problem of learning from a training set of size $n$, versus, how good is a particular fitted model produced by running $A$ on a particular training data set of size $n$?   ","Our main results prove that, for any test that treats the algorithm $A$ as a ``black box'' (i.e., we can only study the behavior of $A$ empirically), there is a fundamental limit on our ability to carry out inference on the performance of $A$, unless the number of available data points $N$ is many times larger than the sample size $n$ of interest.","(On the other hand, evaluating the performance of a particular fitted model is easy as long as a holdout data set is available -- that is, as long as $N-n$ is not too small.)","We also ask whether an assumption of algorithmic stability might be sufficient to circumvent this hardness result.","Surprisingly, we find that this is not the case: the same hardness result still holds for the problem of evaluating the performance of $A$, aside from a high-stability regime where fitted models are essentially nonrandom.","Finally, we also establish similar hardness results for the problem of comparing multiple algorithms."],"url":"http://arxiv.org/abs/2402.07388v1","category":"math.ST"}
{"created":"2024-02-12 03:05:54","title":"Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy Induction from Limited Examples","abstract":"Automatic taxonomy induction is crucial for web search, recommendation systems, and question answering. Manual curation of taxonomies is expensive in terms of human effort, making automatic taxonomy construction highly desirable. In this work, we introduce Chain-of-Layer which is an in-context learning framework designed to induct taxonomies from a given set of entities. Chain-of-Layer breaks down the task into selecting relevant candidate entities in each layer and gradually building the taxonomy from top to bottom. To minimize errors, we introduce the Ensemble-based Ranking Filter to reduce the hallucinated content generated at each iteration. Through extensive experiments, we demonstrate that Chain-of-Layer achieves state-of-the-art performance on four real-world benchmarks.","sentences":["Automatic taxonomy induction is crucial for web search, recommendation systems, and question answering.","Manual curation of taxonomies is expensive in terms of human effort, making automatic taxonomy construction highly desirable.","In this work, we introduce Chain-of-Layer which is an in-context learning framework designed to induct taxonomies from a given set of entities.","Chain-of-Layer breaks down the task into selecting relevant candidate entities in each layer and gradually building the taxonomy from top to bottom.","To minimize errors, we introduce the Ensemble-based Ranking Filter to reduce the hallucinated content generated at each iteration.","Through extensive experiments, we demonstrate that Chain-of-Layer achieves state-of-the-art performance on four real-world benchmarks."],"url":"http://arxiv.org/abs/2402.07386v1","category":"cs.CL"}
{"created":"2024-02-12 03:05:36","title":"An Interpretable Low-complexity Model for Wireless Channel Estimation","abstract":"With the advent of machine learning, there has been renewed interest in the problem of wireless channel estimation. This paper presents a novel low-complexity wireless channel estimation scheme based on a tapped delay line (TDL) model of wireless signal propagation, where a data-driven machine learning approach is used to estimate the path delays and gains. Advantages of this approach include low computation time and training data requirements, as well as interpretability since the estimated model parameters and their variance provide comprehensive representation of the dynamic wireless multipath environment. We evaluate this model's performance using Matlab's ray-tracing tool under static and dynamic conditions for increased realism instead of the standard evaluation approaches using statistical channel models. Our results show that our TDL-based model can accurately estimate the path delays and associated gains for a broad-range of locations and operating conditions. Root-mean-square estimation error remained less than $10^{-4}$, or $-40$dB, for SNR $\\geq 30$dB in all of our experiments.   The key motivation for the novel channel estimation model is to gain environment awareness, i.e., detecting changes in path delays and gains related to interesting objects and events in the field. The channel state with multipath delays and gains is a detailed measure to sense the field than the single-tap channel state indicator calculated in current OFDM systems.","sentences":["With the advent of machine learning, there has been renewed interest in the problem of wireless channel estimation.","This paper presents a novel low-complexity wireless channel estimation scheme based on a tapped delay line (TDL) model of wireless signal propagation, where a data-driven machine learning approach is used to estimate the path delays and gains.","Advantages of this approach include low computation time and training data requirements, as well as interpretability since the estimated model parameters and their variance provide comprehensive representation of the dynamic wireless multipath environment.","We evaluate this model's performance using Matlab's ray-tracing tool under static and dynamic conditions for increased realism instead of the standard evaluation approaches using statistical channel models.","Our results show that our TDL-based model can accurately estimate the path delays and associated gains for a broad-range of locations and operating conditions.","Root-mean-square estimation error remained less than $10^{-4}$, or $-40$dB, for SNR $\\geq 30$dB in all of our experiments.   ","The key motivation for the novel channel estimation model is to gain environment awareness, i.e., detecting changes in path delays and gains related to interesting objects and events in the field.","The channel state with multipath delays and gains is a detailed measure to sense the field than the single-tap channel state indicator calculated in current OFDM systems."],"url":"http://arxiv.org/abs/2402.07385v1","category":"cs.NI"}
{"created":"2024-02-12 02:58:10","title":"Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like","abstract":"Laughter is one of the most expressive and natural aspects of human speech, conveying emotions, social cues, and humor. However, most text-to-speech (TTS) systems lack the ability to produce realistic and appropriate laughter sounds, limiting their applications and user experience. While there have been prior works to generate natural laughter, they fell short in terms of controlling the timing and variety of the laughter to be generated. In this work, we propose ELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker based on a short audio prompt with precise control of laughter timing and expression. Specifically, ELaTE works on the audio prompt to mimic the voice characteristic, the text prompt to indicate the contents of the generated speech, and the input to control the laughter expression, which can be either the start and end times of laughter, or the additional audio prompt that contains laughter to be mimicked. We develop our model based on the foundation of conditional flow-matching-based zero-shot TTS, and fine-tune it with frame-level representation from a laughter detector as additional conditioning. With a simple scheme to mix small-scale laughter-conditioned data with large-scale pre-training data, we demonstrate that a pre-trained zero-shot TTS model can be readily fine-tuned to generate natural laughter with precise controllability, without losing any quality of the pre-trained zero-shot TTS model. Through the evaluations, we show that ELaTE can generate laughing speech with significantly higher quality and controllability compared to conventional models. See https://aka.ms/elate/ for demo samples.","sentences":["Laughter is one of the most expressive and natural aspects of human speech, conveying emotions, social cues, and humor.","However, most text-to-speech (TTS) systems lack the ability to produce realistic and appropriate laughter sounds, limiting their applications and user experience.","While there have been prior works to generate natural laughter, they fell short in terms of controlling the timing and variety of the laughter to be generated.","In this work, we propose ELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker based on a short audio prompt with precise control of laughter timing and expression.","Specifically, ELaTE works on the audio prompt to mimic the voice characteristic, the text prompt to indicate the contents of the generated speech, and the input to control the laughter expression, which can be either the start and end times of laughter, or the additional audio prompt that contains laughter to be mimicked.","We develop our model based on the foundation of conditional flow-matching-based zero-shot TTS, and fine-tune it with frame-level representation from a laughter detector as additional conditioning.","With a simple scheme to mix small-scale laughter-conditioned data with large-scale pre-training data, we demonstrate that a pre-trained zero-shot TTS model can be readily fine-tuned to generate natural laughter with precise controllability, without losing any quality of the pre-trained zero-shot TTS model.","Through the evaluations, we show that ELaTE can generate laughing speech with significantly higher quality and controllability compared to conventional models.","See https://aka.ms/elate/ for demo samples."],"url":"http://arxiv.org/abs/2402.07383v1","category":"eess.AS"}
{"created":"2024-02-12 02:09:08","title":"Real-World Atmospheric Turbulence Correction via Domain Adaptation","abstract":"Atmospheric turbulence, a common phenomenon in daily life, is primarily caused by the uneven heating of the Earth's surface. This phenomenon results in distorted and blurred acquired images or videos and can significantly impact downstream vision tasks, particularly those that rely on capturing clear, stable images or videos from outdoor environments, such as accurately detecting or recognizing objects. Therefore, people have proposed ways to simulate atmospheric turbulence and designed effective deep learning-based methods to remove the atmospheric turbulence effect. However, these synthesized turbulent images can not cover all the range of real-world turbulence effects. Though the models have achieved great performance for synthetic scenarios, there always exists a performance drop when applied to real-world cases. Moreover, reducing real-world turbulence is a more challenging task as there are no clean ground truth counterparts provided to the models during training. In this paper, we propose a real-world atmospheric turbulence mitigation model under a domain adaptation framework, which links the supervised simulated atmospheric turbulence correction with the unsupervised real-world atmospheric turbulence correction. We will show our proposed method enhances performance in real-world atmospheric turbulence scenarios, improving both image quality and downstream vision tasks.","sentences":["Atmospheric turbulence, a common phenomenon in daily life, is primarily caused by the uneven heating of the Earth's surface.","This phenomenon results in distorted and blurred acquired images or videos and can significantly impact downstream vision tasks, particularly those that rely on capturing clear, stable images or videos from outdoor environments, such as accurately detecting or recognizing objects.","Therefore, people have proposed ways to simulate atmospheric turbulence and designed effective deep learning-based methods to remove the atmospheric turbulence effect.","However, these synthesized turbulent images can not cover all the range of real-world turbulence effects.","Though the models have achieved great performance for synthetic scenarios, there always exists a performance drop when applied to real-world cases.","Moreover, reducing real-world turbulence is a more challenging task as there are no clean ground truth counterparts provided to the models during training.","In this paper, we propose a real-world atmospheric turbulence mitigation model under a domain adaptation framework, which links the supervised simulated atmospheric turbulence correction with the unsupervised real-world atmospheric turbulence correction.","We will show our proposed method enhances performance in real-world atmospheric turbulence scenarios, improving both image quality and downstream vision tasks."],"url":"http://arxiv.org/abs/2402.07371v1","category":"cs.CV"}
{"created":"2024-02-12 01:55:40","title":"Utilizing Large LanguageModels to Detect Privacy Leaks in Mini-App Code","abstract":"Mini-applications, commonly referred to as mini-apps, are compact software programs embedded within larger applications or platforms, offering targeted functionality without the need for separate installations. Typically web-based or cloud-hosted, these mini-apps streamline user experiences by providing focused services accessible through web browsers or mobile apps. Their simplicity, speed, and integration capabilities make them valuable additions to messaging platforms, social media networks, e-commerce sites, and various digital environments. WeChat Mini Programs, a prominent feature of China's leading messaging app, exemplify this trend, offering users a seamless array of services without additional downloads. Leveraging WeChat's extensive user base and payment infrastructure, Mini Programs facilitate efficient transactions and bridge online and offline experiences, shaping China's digital landscape significantly. This paper investigates the potential of employing Large Language Models (LLMs) to detect privacy breaches within WeChat Mini Programs. Given the widespread use of Mini Programs and growing concerns about data privacy, this research seeks to determine if LLMs can effectively identify instances of privacy leakage within this ecosystem. Through meticulous analysis and experimentation, we aim to highlight the efficacy of LLMs in safeguarding user privacy and security within the WeChat Mini Program environment, thereby contributing to a more secure digital landscape.","sentences":["Mini-applications, commonly referred to as mini-apps, are compact software programs embedded within larger applications or platforms, offering targeted functionality without the need for separate installations.","Typically web-based or cloud-hosted, these mini-apps streamline user experiences by providing focused services accessible through web browsers or mobile apps.","Their simplicity, speed, and integration capabilities make them valuable additions to messaging platforms, social media networks, e-commerce sites, and various digital environments.","WeChat Mini Programs, a prominent feature of China's leading messaging app, exemplify this trend, offering users a seamless array of services without additional downloads.","Leveraging WeChat's extensive user base and payment infrastructure, Mini Programs facilitate efficient transactions and bridge online and offline experiences, shaping China's digital landscape significantly.","This paper investigates the potential of employing Large Language Models (LLMs) to detect privacy breaches within WeChat Mini Programs.","Given the widespread use of Mini Programs and growing concerns about data privacy, this research seeks to determine if LLMs can effectively identify instances of privacy leakage within this ecosystem.","Through meticulous analysis and experimentation, we aim to highlight the efficacy of LLMs in safeguarding user privacy and security within the WeChat Mini Program environment, thereby contributing to a more secure digital landscape."],"url":"http://arxiv.org/abs/2402.07367v1","category":"cs.CR"}
{"created":"2024-02-12 01:40:31","title":"A Deep Learning Method for Optimal Investment Under Relative Performance Criteria Among Heterogeneous Agents","abstract":"Graphon games have been introduced to study games with many players who interact through a weighted graph of interaction. By passing to the limit, a game with a continuum of players is obtained, in which the interactions are through a graphon. In this paper, we focus on a graphon game for optimal investment under relative performance criteria, and we propose a deep learning method. The method builds upon two key ingredients: first, a characterization of Nash equilibria by forward-backward stochastic differential equations and, second, recent advances of machine learning algorithms for stochastic differential games. We provide numerical experiments on two different financial models. In each model, we compare the effect of several graphons, which correspond to different structures of interactions.","sentences":["Graphon games have been introduced to study games with many players who interact through a weighted graph of interaction.","By passing to the limit, a game with a continuum of players is obtained, in which the interactions are through a graphon.","In this paper, we focus on a graphon game for optimal investment under relative performance criteria, and we propose a deep learning method.","The method builds upon two key ingredients: first, a characterization of Nash equilibria by forward-backward stochastic differential equations and, second, recent advances of machine learning algorithms for stochastic differential games.","We provide numerical experiments on two different financial models.","In each model, we compare the effect of several graphons, which correspond to different structures of interactions."],"url":"http://arxiv.org/abs/2402.07365v1","category":"math.OC"}
{"created":"2024-02-12 01:33:33","title":"Strategically-Robust Learning Algorithms for Bidding in First-Price Auctions","abstract":"Learning to bid in repeated first-price auctions is a fundamental problem at the interface of game theory and machine learning, which has seen a recent surge in interest due to the transition of display advertising to first-price auctions. In this work, we propose a novel concave formulation for pure-strategy bidding in first-price auctions, and use it to analyze natural Gradient-Ascent-based algorithms for this problem. Importantly, our analysis goes beyond regret, which was the typical focus of past work, and also accounts for the strategic backdrop of online-advertising markets where bidding algorithms are deployed -- we prove that our algorithms cannot be exploited by a strategic seller and that they incentivize truth-telling for the buyer.   Concretely, we show that our algorithms achieve $O(\\sqrt{T})$ regret when the highest competing bids are generated adversarially, and show that no online algorithm can do better. We further prove that the regret improves to $O(\\log T)$ when the competition is stationary and stochastic. Moving beyond regret, we show that a strategic seller cannot exploit our algorithms to extract more revenue on average than is possible under the optimal mechanism, i.e., the seller cannot do much better than posting the monopoly reserve price in each auction. Finally, we prove that our algorithm is also incentive compatible -- it is a (nearly) dominant strategy for the buyer to report her values truthfully to the algorithm as a whole.","sentences":["Learning to bid in repeated first-price auctions is a fundamental problem at the interface of game theory and machine learning, which has seen a recent surge in interest due to the transition of display advertising to first-price auctions.","In this work, we propose a novel concave formulation for pure-strategy bidding in first-price auctions, and use it to analyze natural Gradient-Ascent-based algorithms for this problem.","Importantly, our analysis goes beyond regret, which was the typical focus of past work, and also accounts for the strategic backdrop of online-advertising markets where bidding algorithms are deployed -- we prove that our algorithms cannot be exploited by a strategic seller and that they incentivize truth-telling for the buyer.   ","Concretely, we show that our algorithms achieve $O(\\sqrt{T})$ regret when the highest competing bids are generated adversarially, and show that no online algorithm can do better.","We further prove that the regret improves to $O(\\log T)$ when the competition is stationary and stochastic.","Moving beyond regret, we show that a strategic seller cannot exploit our algorithms to extract more revenue on average than is possible under the optimal mechanism, i.e., the seller cannot do much better than posting the monopoly reserve price in each auction.","Finally, we prove that our algorithm is also incentive compatible -- it is a (nearly) dominant strategy for the buyer to report her values truthfully to the algorithm as a whole."],"url":"http://arxiv.org/abs/2402.07363v1","category":"cs.GT"}
{"created":"2024-02-12 01:17:09","title":"Regression Trees for Fast and Adaptive Prediction Intervals","abstract":"Predictive models make mistakes. Hence, there is a need to quantify the uncertainty associated with their predictions. Conformal inference has emerged as a powerful tool to create statistically valid prediction regions around point predictions, but its naive application to regression problems yields non-adaptive regions. New conformal scores, often relying upon quantile regressors or conditional density estimators, aim to address this limitation. Although they are useful for creating prediction bands, these scores are detached from the original goal of quantifying the uncertainty around an arbitrary predictive model. This paper presents a new, model-agnostic family of methods to calibrate prediction intervals for regression problems with local coverage guarantees. Our approach is based on pursuing the coarsest partition of the feature space that approximates conditional coverage. We create this partition by training regression trees and Random Forests on conformity scores. Our proposal is versatile, as it applies to various conformity scores and prediction settings and demonstrates superior scalability and performance compared to established baselines in simulated and real-world datasets. We provide a Python package locart that implements our methods using the standard scikit-learn interface.","sentences":["Predictive models make mistakes.","Hence, there is a need to quantify the uncertainty associated with their predictions.","Conformal inference has emerged as a powerful tool to create statistically valid prediction regions around point predictions, but its naive application to regression problems yields non-adaptive regions.","New conformal scores, often relying upon quantile regressors or conditional density estimators, aim to address this limitation.","Although they are useful for creating prediction bands, these scores are detached from the original goal of quantifying the uncertainty around an arbitrary predictive model.","This paper presents a new, model-agnostic family of methods to calibrate prediction intervals for regression problems with local coverage guarantees.","Our approach is based on pursuing the coarsest partition of the feature space that approximates conditional coverage.","We create this partition by training regression trees and Random Forests on conformity scores.","Our proposal is versatile, as it applies to various conformity scores and prediction settings and demonstrates superior scalability and performance compared to established baselines in simulated and real-world datasets.","We provide a Python package locart that implements our methods using the standard scikit-learn interface."],"url":"http://arxiv.org/abs/2402.07357v1","category":"stat.ML"}
{"created":"2024-02-12 01:03:39","title":"Re-DiffiNet: Modeling discrepancies in tumor segmentation using diffusion","abstract":"Identification of tumor margins is essential for surgical decision-making for glioblastoma patients and provides reliable assistance for neurosurgeons. Despite improvements in deep learning architectures for tumor segmentation over the years, creating a fully autonomous system suitable for clinical floors remains a formidable challenge because the model predictions have not yet reached the desired level of accuracy and generalizability for clinical applications. Generative modeling techniques have seen significant improvements in recent times. Specifically, Generative Adversarial Networks (GANs) and Denoising-diffusion-based models (DDPMs) have been used to generate higher-quality images with fewer artifacts and finer attributes. In this work, we introduce a framework called Re-Diffinet for modeling the discrepancy between the outputs of a segmentation model like U-Net and the ground truth, using DDPMs. By explicitly modeling the discrepancy, the results show an average improvement of 0.55\\% in the Dice score and 16.28\\% in HD95 from cross-validation over 5-folds, compared to the state-of-the-art U-Net segmentation model.","sentences":["Identification of tumor margins is essential for surgical decision-making for glioblastoma patients and provides reliable assistance for neurosurgeons.","Despite improvements in deep learning architectures for tumor segmentation over the years, creating a fully autonomous system suitable for clinical floors remains a formidable challenge because the model predictions have not yet reached the desired level of accuracy and generalizability for clinical applications.","Generative modeling techniques have seen significant improvements in recent times.","Specifically, Generative Adversarial Networks (GANs) and Denoising-diffusion-based models (DDPMs) have been used to generate higher-quality images with fewer artifacts and finer attributes.","In this work, we introduce a framework called Re-Diffinet for modeling the discrepancy between the outputs of a segmentation model like U-Net and the ground truth, using DDPMs.","By explicitly modeling the discrepancy, the results show an average improvement of 0.55\\% in the Dice score and 16.28\\% in HD95 from cross-validation over 5-folds, compared to the state-of-the-art U-Net segmentation model."],"url":"http://arxiv.org/abs/2402.07354v1","category":"eess.IV"}
{"created":"2024-02-12 00:44:58","title":"Ontology Engineering to Model the European Cultural Heritage: The Case of Cultural Gems","abstract":"Cultural gems is a web application conceived by the European Commission's Joint Research Centre (DG JRC), which aims at engaging people and organisations across Europe to create a unique repository of cultural and creative places. The main goal is to provide a vision of European culture in order to strengthen a sense of identity within a single European cultural realm. Cultural gems maps more than 130,000 physical places in over 300 European cities and towns, and since 2020 it also lists online cultural initiatives. The new release aims, among other, to increase the interoperability of the application. At this purpose, we provide an overview on the current development of an ontology for Cultural gems used to map cultural heritage in European cities by using Linked Open Data (LOD) standards, and making the data FAIR, that is Findable, Accessible, Interoperable, and Reusable. We provide an overview of the methodology, presenting the structure of the ontology, and the services and tools we are currently building on top.","sentences":["Cultural gems is a web application conceived by the European Commission's Joint Research Centre (DG JRC), which aims at engaging people and organisations across Europe to create a unique repository of cultural and creative places.","The main goal is to provide a vision of European culture in order to strengthen a sense of identity within a single European cultural realm.","Cultural gems maps more than 130,000 physical places in over 300 European cities and towns, and since 2020 it also lists online cultural initiatives.","The new release aims, among other, to increase the interoperability of the application.","At this purpose, we provide an overview on the current development of an ontology for Cultural gems used to map cultural heritage in European cities by using Linked Open Data (LOD) standards, and making the data FAIR, that is Findable, Accessible, Interoperable, and Reusable.","We provide an overview of the methodology, presenting the structure of the ontology, and the services and tools we are currently building on top."],"url":"http://arxiv.org/abs/2402.07351v1","category":"cs.CY"}
{"created":"2024-02-12 00:19:09","title":"Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization","abstract":"Adapting to a priori unknown noise level is a very important but challenging problem in sequential decision-making as efficient exploration typically requires knowledge of the noise level, which is often loosely specified. We report significant progress in addressing this issue in linear bandits in two respects. First, we propose a novel confidence set that is `semi-adaptive' to the unknown sub-Gaussian parameter $\\sigma_*^2$ in the sense that the (normalized) confidence width scales with $\\sqrt{d\\sigma_*^2 + \\sigma_0^2}$ where $d$ is the dimension and $\\sigma_0^2$ is the specified sub-Gaussian parameter (known) that can be much larger than $\\sigma_*^2$. This is a significant improvement over $\\sqrt{d\\sigma_0^2}$ of the standard confidence set of Abbasi-Yadkori et al. (2011), especially when $d$ is large. We show that this leads to an improved regret bound in linear bandits. Second, for bounded rewards, we propose a novel variance-adaptive confidence set that has a much improved numerical performance upon prior art. We then apply this confidence set to develop, as we claim, the first practical variance-adaptive linear bandit algorithm via an optimistic approach, which is enabled by our novel regret analysis technique. Both of our confidence sets rely critically on `regret equality' from online learning. Our empirical evaluation in Bayesian optimization tasks shows that our algorithms demonstrate better or comparable performance compared to existing methods.","sentences":["Adapting to a priori unknown noise level is a very important but challenging problem in sequential decision-making as efficient exploration typically requires knowledge of the noise level, which is often loosely specified.","We report significant progress in addressing this issue in linear bandits in two respects.","First, we propose a novel confidence set that is `semi-adaptive' to the unknown sub-Gaussian parameter $\\sigma_*^2$ in the sense that the (normalized) confidence width scales with $\\sqrt{d\\sigma_*^2 + \\sigma_0^2}$ where $d$ is the dimension and $\\sigma_0^2$ is the specified sub-Gaussian parameter (known) that can be much larger than $\\sigma_*^2$. This is a significant improvement over $\\sqrt{d\\sigma_0^2}$ of the standard confidence set of Abbasi-Yadkori et al. (2011), especially when $d$ is large.","We show that this leads to an improved regret bound in linear bandits.","Second, for bounded rewards, we propose a novel variance-adaptive confidence set that has a much improved numerical performance upon prior art.","We then apply this confidence set to develop, as we claim, the first practical variance-adaptive linear bandit algorithm via an optimistic approach, which is enabled by our novel regret analysis technique.","Both of our confidence sets rely critically on `regret equality' from online learning.","Our empirical evaluation in Bayesian optimization tasks shows that our algorithms demonstrate better or comparable performance compared to existing methods."],"url":"http://arxiv.org/abs/2402.07341v1","category":"stat.ML"}
{"created":"2024-02-12 00:18:25","title":"Random Geometric Graph Alignment with Graph Neural Networks","abstract":"We characterize the performance of graph neural networks for graph alignment problems in the presence of vertex feature information. More specifically, given two graphs that are independent perturbations of a single random geometric graph with noisy sparse features, the task is to recover an unknown one-to-one mapping between the vertices of the two graphs. We show under certain conditions on the sparsity and noise level of the feature vectors, a carefully designed one-layer graph neural network can with high probability recover the correct alignment between the vertices with the help of the graph structure. We also prove that our conditions on the noise level are tight up to logarithmic factors. Finally we compare the performance of the graph neural network to directly solving an assignment problem on the noisy vertex features. We demonstrate that when the noise level is at least constant this direct matching fails to have perfect recovery while the graph neural network can tolerate noise level growing as fast as a power of the size of the graph.","sentences":["We characterize the performance of graph neural networks for graph alignment problems in the presence of vertex feature information.","More specifically, given two graphs that are independent perturbations of a single random geometric graph with noisy sparse features, the task is to recover an unknown one-to-one mapping between the vertices of the two graphs.","We show under certain conditions on the sparsity and noise level of the feature vectors, a carefully designed one-layer graph neural network can with high probability recover the correct alignment between the vertices with the help of the graph structure.","We also prove that our conditions on the noise level are tight up to logarithmic factors.","Finally we compare the performance of the graph neural network to directly solving an assignment problem on the noisy vertex features.","We demonstrate that when the noise level is at least constant this direct matching fails to have perfect recovery while the graph neural network can tolerate noise level growing as fast as a power of the size of the graph."],"url":"http://arxiv.org/abs/2402.07340v1","category":"cs.LG"}
{"created":"2024-02-11 23:28:31","title":"Computing discrete residues of rational functions","abstract":"In 2012 Chen and Singer introduced the notion of discrete residues for rational functions as a complete obstruction to rational summability. More explicitly, for a given rational function f(x), there exists a rational function g(x) such that f(x) = g(x+1) - g(x) if and only if every discrete residue of f(x) is zero. Discrete residues have many important further applications beyond summability: to creative telescoping problems, thence to the determination of (differential-)algebraic relations among hypergeometric sequences, and subsequently to the computation of (differential) Galois groups of difference equations. However, the discrete residues of a rational function are defined in terms of its complete partial fraction decomposition, which makes their direct computation impractical due to the high complexity of completely factoring arbitrary denominator polynomials into linear factors. We develop a factorization-free algorithm to compute discrete residues of rational functions, relying only on gcd computations and linear algebra.","sentences":["In 2012 Chen and Singer introduced the notion of discrete residues for rational functions as a complete obstruction to rational summability.","More explicitly, for a given rational function f(x), there exists a rational function g(x) such that f(x) = g(x+1) - g(x)","if and only if every discrete residue of f(x) is zero.","Discrete residues have many important further applications beyond summability:","to creative telescoping problems, thence to the determination of (differential-)algebraic relations among hypergeometric sequences, and subsequently to the computation of (differential) Galois groups of difference equations.","However, the discrete residues of a rational function are defined in terms of its complete partial fraction decomposition, which makes their direct computation impractical due to the high complexity of completely factoring arbitrary denominator polynomials into linear factors.","We develop a factorization-free algorithm to compute discrete residues of rational functions, relying only on gcd computations and linear algebra."],"url":"http://arxiv.org/abs/2402.07328v1","category":"cs.SC"}
{"created":"2024-02-11 22:58:52","title":"Interference Among First-Price Pacing Equilibria: A Bias and Variance Analysis","abstract":"Online A/B testing is widely used in the internet industry to inform decisions on new feature roll-outs. For online marketplaces (such as advertising markets), standard approaches to A/B testing may lead to biased results when buyers operate under a budget constraint, as budget consumption in one arm of the experiment impacts performance of the other arm. To counteract this interference, one can use a budget-split design where the budget constraint operates on a per-arm basis and each arm receives an equal fraction of the budget, leading to ``budget-controlled A/B testing.'' Despite clear advantages of budget-controlled A/B testing, performance degrades when budget are split too small, limiting the overall throughput of such systems. In this paper, we propose a parallel budget-controlled A/B testing design where we use market segmentation to identify submarkets in the larger market, and we run parallel experiments on each submarket.   Our contributions are as follows: First, we introduce and demonstrate the effectiveness of the parallel budget-controlled A/B test design with submarkets in a large online marketplace environment. Second, we formally define market interference in first-price auction markets using the first price pacing equilibrium (FPPE) framework. Third, we propose a debiased surrogate that eliminates the first-order bias of FPPE, drawing upon the principles of sensitivity analysis in mathematical programs. Fourth, we derive a plug-in estimator for the surrogate and establish its asymptotic normality. Fifth, we provide an estimation procedure for submarket parallel budget-controlled A/B tests. Finally, we present numerical examples on semi-synthetic data, confirming that the debiasing technique achieves the desired coverage properties.","sentences":["Online A/B testing is widely used in the internet industry to inform decisions on new feature roll-outs.","For online marketplaces (such as advertising markets), standard approaches to A/B testing may lead to biased results when buyers operate under a budget constraint, as budget consumption in one arm of the experiment impacts performance of the other arm.","To counteract this interference, one can use a budget-split design where the budget constraint operates on a per-arm basis and each arm receives an equal fraction of the budget, leading to ``budget-controlled A/B testing.''","Despite clear advantages of budget-controlled A/B testing, performance degrades when budget are split too small, limiting the overall throughput of such systems.","In this paper, we propose a parallel budget-controlled A/B testing design where we use market segmentation to identify submarkets in the larger market, and we run parallel experiments on each submarket.   ","Our contributions are as follows:","First, we introduce and demonstrate the effectiveness of the parallel budget-controlled A/B test design with submarkets in a large online marketplace environment.","Second, we formally define market interference in first-price auction markets using the first price pacing equilibrium (FPPE) framework.","Third, we propose a debiased surrogate that eliminates the first-order bias of FPPE, drawing upon the principles of sensitivity analysis in mathematical programs.","Fourth, we derive a plug-in estimator for the surrogate and establish its asymptotic normality.","Fifth, we provide an estimation procedure for submarket parallel budget-controlled A/B tests.","Finally, we present numerical examples on semi-synthetic data, confirming that the debiasing technique achieves the desired coverage properties."],"url":"http://arxiv.org/abs/2402.07322v1","category":"math.ST"}
{"created":"2024-02-11 22:08:46","title":"Congruences of modular forms and modularity of Tate-Shafarevich classes","abstract":"We prove, under suitable assumptions, that $p$-torsion Tate-Shafarevich classes for elliptic curves over the rationals are visible in quotients of Jacobians of modular curves, as predicted by a conjecture of Jetchev-Stein. The key ingredient is the non-triviality of the Bertolini-Darmon bipartite Kolyvagin system, which implies that suitable cohomology classes of the system form a basis of the Selmer group modulo $p$.","sentences":["We prove, under suitable assumptions, that $p$-torsion Tate-Shafarevich classes for elliptic curves over the rationals are visible in quotients of Jacobians of modular curves, as predicted by a conjecture of Jetchev-Stein.","The key ingredient is the non-triviality of the Bertolini-Darmon bipartite Kolyvagin system, which implies that suitable cohomology classes of the system form a basis of the Selmer group modulo $p$."],"url":"http://arxiv.org/abs/2402.07317v1","category":"math.NT"}
{"created":"2024-02-11 22:06:57","title":"Bounds on Growth and Impossibility of Collapse for Point Vortex Systems","abstract":"We consider 2D point vortex systems and, under certain conditions on the masses of the point vortices, prove that collapse is impossible and provide bounds on the growth of the system. The bounds are typically of the form $O(t^a)$ for some $a<1/2$, but we obtain various results for various assumptions on the masses.","sentences":["We consider 2D point vortex systems and, under certain conditions on the masses of the point vortices, prove that collapse is impossible and provide bounds on the growth of the system.","The bounds are typically of the form $O(t^a)$ for some $a<1/2$, but we obtain various results for various assumptions on the masses."],"url":"http://arxiv.org/abs/2402.07316v1","category":"math.CA"}
{"created":"2024-02-11 21:36:49","title":"A Qubit with Simultaneously Maximized Speed and Coherence","abstract":"To overcome the threshold for fault-tolerant quantum computation, qubits have to be protected from their noisy environment to attain the necessary high fidelities. Recent experiments discovered sweet spots with strongly enhanced coherence. However, decoupling a qubit from its surroundings also limits the control over the qubit's state, typically leading to either coherent but slow or fast but short-lived qubits. This trade-off appears to be a severe fundamental limitation hampering the performance of qubits. Here, we show how this can be circumvented by demonstrating a simultaneously fast and coherent tunable regime in a hole spin qubit. In this regime, we can triple the operation speed, while simultaneously quadrupling the coherence time when tuning a local electric field, demonstrating that the qubit speed and coherence scale together without compromise. This relies on strong, quasi 1D confinement providing a local maximum in drive strength, where charge fluctuations are decoupled and thus the coherence is enhanced, yet the drive speed is maximal. A Ge/Si core/shell nanowire, operated at 1.5 K, provides the strong confinement. The driving mechanism here is the strong and tunable direct Rashba spin-orbit interaction, achieving a maximal strength at finite electrical field due to gate-dependent heavy-hole light-hole mixing. Breaking the speed-coherence trade-off makes it possible to boost fidelity and speed of one- and two-qubit gates. This concept can be expanded to planar arrays of hole or electron spin qubits as well. In this regime, the coupling to a microwave resonator is also predicted to be both strong and coherent. Altogether, this is opening a new path towards fault-tolerant quantum computation.","sentences":["To overcome the threshold for fault-tolerant quantum computation, qubits have to be protected from their noisy environment to attain the necessary high fidelities.","Recent experiments discovered sweet spots with strongly enhanced coherence.","However, decoupling a qubit from its surroundings also limits the control over the qubit's state, typically leading to either coherent but slow or fast but short-lived qubits.","This trade-off appears to be a severe fundamental limitation hampering the performance of qubits.","Here, we show how this can be circumvented by demonstrating a simultaneously fast and coherent tunable regime in a hole spin qubit.","In this regime, we can triple the operation speed, while simultaneously quadrupling the coherence time when tuning a local electric field, demonstrating that the qubit speed and coherence scale together without compromise.","This relies on strong, quasi 1D confinement providing a local maximum in drive strength, where charge fluctuations are decoupled and thus the coherence is enhanced, yet the drive speed is maximal.","A Ge/Si core/shell nanowire, operated at 1.5 K, provides the strong confinement.","The driving mechanism here is the strong and tunable direct Rashba spin-orbit interaction, achieving a maximal strength at finite electrical field due to gate-dependent heavy-hole light-hole mixing.","Breaking the speed-coherence trade-off makes it possible to boost fidelity and speed of one- and two-qubit gates.","This concept can be expanded to planar arrays of hole or electron spin qubits as well.","In this regime, the coupling to a microwave resonator is also predicted to be both strong and coherent.","Altogether, this is opening a new path towards fault-tolerant quantum computation."],"url":"http://arxiv.org/abs/2402.07313v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-11 21:12:21","title":"Self-Consistent Conformal Prediction","abstract":"In decision-making guided by machine learning, decision-makers often take identical actions in contexts with identical predicted outcomes. Conformal prediction helps decision-makers quantify outcome uncertainty for actions, allowing for better risk management. Inspired by this perspective, we introduce self-consistent conformal prediction, which yields both Venn-Abers calibrated predictions and conformal prediction intervals that are valid conditional on actions prompted by model predictions. Our procedure can be applied post-hoc to any black-box predictor to provide rigorous, action-specific decision-making guarantees. Numerical experiments show our approach strikes a balance between interval efficiency and conditional validity.","sentences":["In decision-making guided by machine learning, decision-makers often take identical actions in contexts with identical predicted outcomes.","Conformal prediction helps decision-makers quantify outcome uncertainty for actions, allowing for better risk management.","Inspired by this perspective, we introduce self-consistent conformal prediction, which yields both Venn-Abers calibrated predictions and conformal prediction intervals that are valid conditional on actions prompted by model predictions.","Our procedure can be applied post-hoc to any black-box predictor to provide rigorous, action-specific decision-making guarantees.","Numerical experiments show our approach strikes a balance between interval efficiency and conditional validity."],"url":"http://arxiv.org/abs/2402.07307v1","category":"stat.ML"}
{"created":"2024-02-11 20:17:10","title":"Estimating the Mixing Coefficients of Geometrically Ergodic Markov Processes","abstract":"We propose methods to estimate the individual $\\beta$-mixing coefficients of a real-valued geometrically ergodic Markov process from a single sample-path $X_0,X_1, \\dots,X_n$. Under standard smoothness conditions on the densities, namely, that the joint density of the pair $(X_0,X_m)$ for each $m$ lies in a Besov space $B^s_{1,\\infty}(\\mathbb R^2)$ for some known $s>0$, we obtain a rate of convergence of order $\\mathcal{O}(\\log(n) n^{-[s]/(2[s]+2)})$ for the expected error of our estimator in this case\\footnote{We use $[s]$ to denote the integer part of the decomposition $s=[s]+\\{s\\}$ of $s \\in (0,\\infty)$ into an integer term and a {\\em strictly positive} remainder term $\\{s\\} \\in (0,1]$.}. We complement this result with a high-probability bound on the estimation error, and further obtain analogues of these bounds in the case where the state-space is finite. Naturally no density assumptions are required in this setting; the expected error rate is shown to be of order $\\mathcal O(\\log(n) n^{-1/2})$.","sentences":["We propose methods to estimate the individual $\\beta$-mixing coefficients of a real-valued geometrically ergodic Markov process from a single sample-path $X_0,X_1, \\dots,X_n$. Under standard smoothness conditions on the densities, namely, that the joint density of the pair $(X_0,X_m)$ for each $m$ lies in a Besov space $B^s_{1,\\infty}(\\mathbb R^2)$ for some known $s>0$, we obtain a rate of convergence of order $\\mathcal{O}(\\log(n) n^{-[s]/(2[s]+2)})$ for the expected error of our estimator in this case\\footnote{We use $[s]$ to denote the integer part of the decomposition $s=[s]+\\{s\\}$ of $s \\in (0,\\infty)$ into an integer term and a {\\em strictly positive} remainder term $\\{s\\} \\in (0,1]$.}.","We complement this result with a high-probability bound on the estimation error, and further obtain analogues of these bounds in the case where the state-space is finite.","Naturally no density assumptions are required in this setting; the expected error rate is shown to be of order $\\mathcal O(\\log(n) n^{-1/2})$."],"url":"http://arxiv.org/abs/2402.07296v1","category":"math.ST"}
{"created":"2024-02-11 20:04:36","title":"Influence of high pressure on Ce3+ luminescence in LuAlO3 and YAlO3 single crystals and single crystalline layers","abstract":"Results of spectroscopic studies at ambient and high pressures of a LuAlO3:Ce3+ (LuAP:Ce) single crystalline film (SCF) as well as LuAP:Ce and YAlO3:Ce (YAP:Ce) single crystals are reported. Room temperature absorption measurements of the single crystals in the vacuum UV region allowed establishing the bandgap energies of 7.63 eV for YAP and 7.86 eV for LuAP, with an assumption of the direct band-gaps. Luminescence of Ce3+ in LuAP and YAP bulk crystals was measured as a function of temperature from 6 K up to 873 K. Temperature quenching of the Ce3+ luminescence in YAP:Ce was observed above 650 K, which is related to the location of the lowest Ce3+ 5d level at 1.27 eV below the conduction band minimum. No temperature quenching occurred in LuAP:Ce up to 873 K, mostly due to the lower energy of the 4f levels with respect to the valence band maximum. The barycenter energies and splittings of Ce3+ 5d states in YAP and LuAP at room temperature were precisely established. Theoretical calculations of the Ce3+ 5d states energy structure under pressure revealed a discrepancy between the obtained experimental results and the prediction of Dorenbos' theoretical model. The discrepancy can be removed if instead of the 5d state of the free Ce3+ ion the bandgap of the compound is taken as reference energy for the red shift of the 5d level. This hypothesis also allows us to take into account the pressure-induced increase of the bandgap energy, expected for the studied compounds. Pressure dependences of LuAP:Ce luminescence spectra suggest that a certain type of phase transition occurs above 15 GPa.","sentences":["Results of spectroscopic studies at ambient and high pressures of a LuAlO3:Ce3+ (LuAP:Ce) single crystalline film (SCF) as well as LuAP:Ce and YAlO3:Ce (YAP:Ce) single crystals are reported.","Room temperature absorption measurements of the single crystals in the vacuum UV region allowed establishing the bandgap energies of 7.63 eV for YAP and 7.86 eV for LuAP, with an assumption of the direct band-gaps.","Luminescence of Ce3+ in LuAP and YAP bulk crystals was measured as a function of temperature from 6 K up to 873 K. Temperature quenching of the Ce3+ luminescence in YAP:","Ce was observed above 650 K, which is related to the location of the lowest Ce3+ 5d level at 1.27 eV below the conduction band minimum.","No temperature quenching occurred in LuAP:","Ce up to 873 K, mostly due to the lower energy of the 4f levels with respect to the valence band maximum.","The barycenter energies and splittings of Ce3+ 5d states in YAP and LuAP at room temperature were precisely established.","Theoretical calculations of the Ce3+ 5d states energy structure under pressure revealed a discrepancy between the obtained experimental results and the prediction of Dorenbos' theoretical model.","The discrepancy can be removed if instead of the 5d state of the free Ce3+ ion the bandgap of the compound is taken as reference energy for the red shift of the 5d level.","This hypothesis also allows us to take into account the pressure-induced increase of the bandgap energy, expected for the studied compounds.","Pressure dependences of LuAP:","Ce luminescence spectra suggest that a certain type of phase transition occurs above 15 GPa."],"url":"http://arxiv.org/abs/2402.07291v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-11 19:17:39","title":"The effects of interparticle cohesion on the collapse of granular columns","abstract":"The presence of interparticle cohesion can drastically change the behavior of granular materials. For instance, powders are challenging to handle, and one can make a sandcastle using wet grains. In this study, we report experimental results for columns of model cohesive grains collapsing under their own weight in air and spreading on a rough horizontal surface. The effects of two different sources of interparticle cohesion on two collapse geometries are compared and rationalized in a common framework. Grains are made cohesive by adding a small amount of water, such that they are in the pendular state, or by applying a polymer coating. The effects of cohesion are reported for a cylindrical column that spreads unconfined axisymmetrically and a confined rectangular column that flows in a single direction. A dimensionless number, comparing macroscopic cohesive strength to particle weight, is shown to capture the effects of cohesion on the final morphology. To this end, a characterization of the cohesive strength of the granular materials is obtained, independent of the physical source of cohesion at the particle scale. Such a framework allows for a common description of cohesive granular materials with different sources of cohesion.","sentences":["The presence of interparticle cohesion can drastically change the behavior of granular materials.","For instance, powders are challenging to handle, and one can make a sandcastle using wet grains.","In this study, we report experimental results for columns of model cohesive grains collapsing under their own weight in air and spreading on a rough horizontal surface.","The effects of two different sources of interparticle cohesion on two collapse geometries are compared and rationalized in a common framework.","Grains are made cohesive by adding a small amount of water, such that they are in the pendular state, or by applying a polymer coating.","The effects of cohesion are reported for a cylindrical column that spreads unconfined axisymmetrically and a confined rectangular column that flows in a single direction.","A dimensionless number, comparing macroscopic cohesive strength to particle weight, is shown to capture the effects of cohesion on the final morphology.","To this end, a characterization of the cohesive strength of the granular materials is obtained, independent of the physical source of cohesion at the particle scale.","Such a framework allows for a common description of cohesive granular materials with different sources of cohesion."],"url":"http://arxiv.org/abs/2402.07285v1","category":"cond-mat.soft"}
{"created":"2024-02-11 19:14:28","title":"Power Transformer Fault Prediction Based on Knowledge Graphs","abstract":"In this paper, we address the challenge of learning with limited fault data for power transformers. Traditional operation and maintenance tools lack effective predictive capabilities for potential faults. The scarcity of extensive fault data makes it difficult to apply machine learning techniques effectively. To solve this problem, we propose a novel approach that leverages the knowledge graph (KG) technology in combination with gradient boosting decision trees (GBDT). This method is designed to efficiently learn from a small set of high-dimensional data, integrating various factors influencing transformer faults and historical operational data. Our approach enables accurate safe state assessments and fault analyses of power transformers despite the limited fault characteristic data. Experimental results demonstrate that this method outperforms other learning approaches in prediction accuracy, such as artificial neural networks (ANN) and logistic regression (LR). Furthermore, it offers significant improvements in progressiveness, practicality, and potential for widespread application.","sentences":["In this paper, we address the challenge of learning with limited fault data for power transformers.","Traditional operation and maintenance tools lack effective predictive capabilities for potential faults.","The scarcity of extensive fault data makes it difficult to apply machine learning techniques effectively.","To solve this problem, we propose a novel approach that leverages the knowledge graph (KG) technology in combination with gradient boosting decision trees (GBDT).","This method is designed to efficiently learn from a small set of high-dimensional data, integrating various factors influencing transformer faults and historical operational data.","Our approach enables accurate safe state assessments and fault analyses of power transformers despite the limited fault characteristic data.","Experimental results demonstrate that this method outperforms other learning approaches in prediction accuracy, such as artificial neural networks (ANN) and logistic regression (LR).","Furthermore, it offers significant improvements in progressiveness, practicality, and potential for widespread application."],"url":"http://arxiv.org/abs/2402.07283v1","category":"cs.LG"}
{"created":"2024-02-11 19:12:51","title":"Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study","abstract":"Detection of anomalous situations for complex mission-critical systems holds paramount importance when their service continuity needs to be ensured. A major challenge in detecting anomalies from the operational data arises due to the imbalanced class distribution problem since the anomalies are supposed to be rare events. This paper evaluates a diverse array of machine learning-based anomaly detection algorithms through a comprehensive benchmark study. The paper contributes significantly by conducting an unbiased comparison of various anomaly detection algorithms, spanning classical machine learning including various tree-based approaches to deep learning and outlier detection methods. The inclusion of 104 publicly available and a few proprietary industrial systems datasets enhances the diversity of the study, allowing for a more realistic evaluation of algorithm performance and emphasizing the importance of adaptability to real-world scenarios. The paper dispels the deep learning myth, demonstrating that though powerful, deep learning is not a universal solution in this case. We observed that recently proposed tree-based evolutionary algorithms outperform in many scenarios. We noticed that tree-based approaches catch a singleton anomaly in a dataset where deep learning methods fail. On the other hand, classical SVM performs the best on datasets with more than 10% anomalies, implying that such scenarios can be best modeled as a classification problem rather than anomaly detection. To our knowledge, such a study on a large number of state-of-the-art algorithms using diverse data sets, with the objective of guiding researchers and practitioners in making informed algorithmic choices, has not been attempted earlier.","sentences":["Detection of anomalous situations for complex mission-critical systems holds paramount importance when their service continuity needs to be ensured.","A major challenge in detecting anomalies from the operational data arises due to the imbalanced class distribution problem since the anomalies are supposed to be rare events.","This paper evaluates a diverse array of machine learning-based anomaly detection algorithms through a comprehensive benchmark study.","The paper contributes significantly by conducting an unbiased comparison of various anomaly detection algorithms, spanning classical machine learning including various tree-based approaches to deep learning and outlier detection methods.","The inclusion of 104 publicly available and a few proprietary industrial systems datasets enhances the diversity of the study, allowing for a more realistic evaluation of algorithm performance and emphasizing the importance of adaptability to real-world scenarios.","The paper dispels the deep learning myth, demonstrating that though powerful, deep learning is not a universal solution in this case.","We observed that recently proposed tree-based evolutionary algorithms outperform in many scenarios.","We noticed that tree-based approaches catch a singleton anomaly in a dataset where deep learning methods fail.","On the other hand, classical SVM performs the best on datasets with more than 10% anomalies, implying that such scenarios can be best modeled as a classification problem rather than anomaly detection.","To our knowledge, such a study on a large number of state-of-the-art algorithms using diverse data sets, with the objective of guiding researchers and practitioners in making informed algorithmic choices, has not been attempted earlier."],"url":"http://arxiv.org/abs/2402.07281v1","category":"cs.LG"}
{"created":"2024-02-11 19:01:35","title":"Bundling Demand in K-12 Broadband Procurement","abstract":"We evaluate the effects of bundling demand for broadband internet by K-12 schools. In 2014, New Jersey switched from decentralized procurements to a new procurement system that bundled schools into four regional groups. Using an event study approach, we find that, on average, prices for participants decreased by one-third, and broadband speed purchased increased sixfold. We bound the change in school expenditures due to the program and find that participants saved at least as much as their total \"E-rate\" subsidy from the federal government. Under weak assumptions on demand, we show that participating schools experienced large welfare gains.","sentences":["We evaluate the effects of bundling demand for broadband internet by K-12 schools.","In 2014, New Jersey switched from decentralized procurements to a new procurement system that bundled schools into four regional groups.","Using an event study approach, we find that, on average, prices for participants decreased by one-third, and broadband speed purchased increased sixfold.","We bound the change in school expenditures due to the program and find that participants saved at least as much as their total \"E-rate\" subsidy from the federal government.","Under weak assumptions on demand, we show that participating schools experienced large welfare gains."],"url":"http://arxiv.org/abs/2402.07277v1","category":"econ.GN"}
{"created":"2024-02-11 18:26:18","title":"Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy","abstract":"The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models' capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our benchmark to a suite of vision-language models and show a detailed comparison of their abilities on object, action, and attribute classification. Our contributions aim to lay the foundation for more precise and meaningful assessments, facilitating targeted progress in the exciting field of vision-language modeling.","sentences":["The evaluation of text-generative vision-language models is a challenging yet crucial endeavor.","By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models' capabilities.","We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models.","To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category.","Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers.","We perform a human evaluation study upon which we base our decision on the final metric.","We apply our benchmark to a suite of vision-language models and show a detailed comparison of their abilities on object, action, and attribute classification.","Our contributions aim to lay the foundation for more precise and meaningful assessments, facilitating targeted progress in the exciting field of vision-language modeling."],"url":"http://arxiv.org/abs/2402.07270v1","category":"cs.CV"}
{"created":"2024-02-11 18:17:10","title":"Spillover Effects of US Monetary Policy on Emerging Markets Amidst Uncertainty","abstract":"This paper examines the impact of US monetary policy tightening on emerging markets, distinguishing between direct and indirect spillover effects using the global vector autoregression with stochastic volatility covering 32 countries. The paper demonstrates that an increase in the US interest rate significantly reduces output for emerging markets, leading to larger, more prolonged, and persistent declines. Such an impact is further intensified by global trade integration, causing a sharper yet slightly quicker rebounding output drop. The spillover effects are significantly amplified when US monetary policy tightening is accompanied by an increase in monetary policy uncertainty. Finally, emerging markets exhibit considerable heterogeneity in their responses to US monetary policy shocks.","sentences":["This paper examines the impact of US monetary policy tightening on emerging markets, distinguishing between direct and indirect spillover effects using the global vector autoregression with stochastic volatility covering 32 countries.","The paper demonstrates that an increase in the US interest rate significantly reduces output for emerging markets, leading to larger, more prolonged, and persistent declines.","Such an impact is further intensified by global trade integration, causing a sharper yet slightly quicker rebounding output drop.","The spillover effects are significantly amplified when US monetary policy tightening is accompanied by an increase in monetary policy uncertainty.","Finally, emerging markets exhibit considerable heterogeneity in their responses to US monetary policy shocks."],"url":"http://arxiv.org/abs/2402.07266v1","category":"econ.GN"}
{"created":"2024-02-11 17:26:16","title":"The Pairwise Matching Design is Optimal under Extreme Noise and Assignments","abstract":"We consider the general performance of the difference-in-means estimator in an equally-allocated two-arm randomized experiment under common experimental endpoints such as continuous (regression), incidence, proportion, count and uncensored survival. We consider two sources of randomness: the subject-specific assignments and the contribution of unobserved subject-specific measurements. We then examine mean squared error (MSE) performance under a new, more realistic \"simultaneous tail criterion\". We prove that the pairwise matching design of Greevy et al. (2004) performs best asymptotically under this criterion when compared to other blocking designs. We also prove that the optimal design must be less random than complete randomization and more random than any deterministic, optimized allocation. Theoretical results are supported by simulations in all five response types.","sentences":["We consider the general performance of the difference-in-means estimator in an equally-allocated two-arm randomized experiment under common experimental endpoints such as continuous (regression), incidence, proportion, count and uncensored survival.","We consider two sources of randomness: the subject-specific assignments and the contribution of unobserved subject-specific measurements.","We then examine mean squared error (MSE) performance under a new, more realistic \"simultaneous tail criterion\".","We prove that the pairwise matching design of Greevy et al.","(2004) performs best asymptotically under this criterion when compared to other blocking designs.","We also prove that the optimal design must be less random than complete randomization and more random than any deterministic, optimized allocation.","Theoretical results are supported by simulations in all five response types."],"url":"http://arxiv.org/abs/2402.07247v1","category":"stat.ME"}
{"created":"2024-02-11 17:10:31","title":"Towards Generalized Inverse Reinforcement Learning","abstract":"This paper studies generalized inverse reinforcement learning (GIRL) in Markov decision processes (MDPs), that is, the problem of learning the basic components of an MDP given observed behavior (policy) that might not be optimal. These components include not only the reward function and transition probability matrices, but also the action space and state space that are not exactly known but are known to belong to given uncertainty sets. We address two key challenges in GIRL: first, the need to quantify the discrepancy between the observed policy and the underlying optimal policy; second, the difficulty of mathematically characterizing the underlying optimal policy when the basic components of an MDP are unobservable or partially observable. Then, we propose the mathematical formulation for GIRL and develop a fast heuristic algorithm. Numerical results on both finite and infinite state problems show the merit of our formulation and algorithm.","sentences":["This paper studies generalized inverse reinforcement learning (GIRL) in Markov decision processes (MDPs), that is, the problem of learning the basic components of an MDP given observed behavior (policy) that might not be optimal.","These components include not only the reward function and transition probability matrices, but also the action space and state space that are not exactly known but are known to belong to given uncertainty sets.","We address two key challenges in GIRL:","first, the need to quantify the discrepancy between the observed policy and the underlying optimal policy; second, the difficulty of mathematically characterizing the underlying optimal policy when the basic components of an MDP are unobservable or partially observable.","Then, we propose the mathematical formulation for GIRL and develop a fast heuristic algorithm.","Numerical results on both finite and infinite state problems show the merit of our formulation and algorithm."],"url":"http://arxiv.org/abs/2402.07246v1","category":"cs.LG"}
{"created":"2024-02-11 16:57:08","title":"PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point Cloud Compression","abstract":"The universality of the point cloud format enables many 3D applications, making the compression of point clouds a critical phase in practice. Sampled as discrete 3D points, a point cloud approximates 2D surface(s) embedded in 3D with a finite bit-depth. However, the point distribution of a practical point cloud changes drastically as its bit-depth increases, requiring different methodologies for effective consumption/analysis. In this regard, a heterogeneous point cloud compression (PCC) framework is proposed. We unify typical point cloud representations -- point-based, voxel-based, and tree-based representations -- and their associated backbones under a learning-based framework to compress an input point cloud at different bit-depth levels. Having recognized the importance of voxel-domain processing, we augment the framework with a proposed context-aware upsampling for decoding and an enhanced voxel transformer for feature aggregation. Extensive experimentation demonstrates the state-of-the-art performance of our proposal on a wide range of point clouds.","sentences":["The universality of the point cloud format enables many 3D applications, making the compression of point clouds a critical phase in practice.","Sampled as discrete 3D points, a point cloud approximates 2D surface(s) embedded in 3D with a finite bit-depth.","However, the point distribution of a practical point cloud changes drastically as its bit-depth increases, requiring different methodologies for effective consumption/analysis.","In this regard, a heterogeneous point cloud compression (PCC) framework is proposed.","We unify typical point cloud representations -- point-based, voxel-based, and tree-based representations -- and their associated backbones under a learning-based framework to compress an input point cloud at different bit-depth levels.","Having recognized the importance of voxel-domain processing, we augment the framework with a proposed context-aware upsampling for decoding and an enhanced voxel transformer for feature aggregation.","Extensive experimentation demonstrates the state-of-the-art performance of our proposal on a wide range of point clouds."],"url":"http://arxiv.org/abs/2402.07243v1","category":"cs.CV"}
{"created":"2024-02-11 16:40:33","title":"Proof of Diligence: Cryptoeconomic Security for Rollups","abstract":"Layer 1 (L1) blockchains such as Ethereum are secured under an \"honest supermajority of stake\" assumption for a large pool of validators who verify each and every transaction on it. This high security comes at a scalability cost which not only effects the throughput of the blockchain but also results in high gas fees for executing transactions on chain. The most successful solution for this problem is provided by optimistic rollups, Layer 2 (L2) blockchains that execute transactions outside L1 but post the transaction data on L1. The security for such L2 chains is argued, informally, under the assumption that a set of nodes will check the transaction data posted on L1 and raise an alarm (a fraud proof) if faulty transactions are detected. However, all current deployments lack a proper incentive mechanism for ensuring that these nodes will do their job ``diligently'', and simply rely on a cursory incentive alignment argument for security. We solve this problem by introducing an incentivized watchtower network designed to serve as the first line of defense for rollups. Our main contribution is a ``Proof of Diligence'' protocol that requires watchtowers to continuously provide a proof that they have verified L2 assertions and get rewarded for the same. Proof of Diligence protocol includes a carefully-designed incentive mechanism that is provably secure when watchtowers are rational actors, under a mild rational independence assumption.   Our proposed system is now live on Ethereum testnet. We deployed a watchtower network and implemented Proof of Diligence for multiple optimistic rollups. We extract execution as well as inclusion proofs for transactions as a part of the bounty. Each watchtower has minimal additional computational overhead beyond access to standard L1 and L2 RPC nodes.","sentences":["Layer 1 (L1) blockchains such as Ethereum are secured under an \"honest supermajority of stake\" assumption for a large pool of validators who verify each and every transaction on it.","This high security comes at a scalability cost which not only effects the throughput of the blockchain but also results in high gas fees for executing transactions on chain.","The most successful solution for this problem is provided by optimistic rollups, Layer 2 (L2) blockchains that execute transactions outside L1 but post the transaction data on L1.","The security for such L2 chains is argued, informally, under the assumption that a set of nodes will check the transaction data posted on L1 and raise an alarm (a fraud proof) if faulty transactions are detected.","However, all current deployments lack a proper incentive mechanism for ensuring that these nodes will do their job ``diligently'', and simply rely on a cursory incentive alignment argument for security.","We solve this problem by introducing an incentivized watchtower network designed to serve as the first line of defense for rollups.","Our main contribution is a ``Proof of Diligence'' protocol that requires watchtowers to continuously provide a proof that they have verified L2 assertions and get rewarded for the same.","Proof of Diligence protocol includes a carefully-designed incentive mechanism that is provably secure when watchtowers are rational actors, under a mild rational independence assumption.   ","Our proposed system is now live on Ethereum testnet.","We deployed a watchtower network and implemented Proof of Diligence for multiple optimistic rollups.","We extract execution as well as inclusion proofs for transactions as a part of the bounty.","Each watchtower has minimal additional computational overhead beyond access to standard L1 and L2 RPC nodes."],"url":"http://arxiv.org/abs/2402.07241v1","category":"cs.CR"}
{"created":"2024-02-11 16:31:15","title":"Weakly interacting one-dimensional topological insulators: a bosonization approach","abstract":"We investigate the topological properties of one-dimensional weakly interacting topological insulators using bosonization. To do that we study the topological edge states that emerge at the edges of a model realized by a strong impurity or at the boundary between topologically distinct phases. In the bosonic model, the edge states are manifested as degenerate bosonic kinks at the boundaries. We first illustrate this idea on the example of the interacting Su-Schrieffer-Heeger (SSH) chain. We compute the localization length of the edge states as the width of an edge soliton that occurs in the SSH model in the presence of a strong impurity. Next, we examine models of two capacitively coupled SSH chains that can be either identical or in distinct topological phases. We find that weak Hubbard interaction reduces the ground state degeneracy in the topological phase of identical chains. We then prove that similarly to the non-interacting model, the degeneracy of the edge states in the interacting case is protected by chiral symmetry. We then study topological insulators built from two SSH chains with inter-chain hopping, that represent models of different chiral symmetric universality classes. We demonstrate in bosonic language that the topological index of a weakly coupled model is determined by the type of inter-chain coupling, invariant under one of two possible chiral symmetry operators. Finally, we show that a general one-dimensional model in a phase with topological index $\\nu$ is equivalent at low energies to a theory of at least $\\nu$ SSH chains. We illustrate this idea on the example of an SSH model with longer-range hopping.","sentences":["We investigate the topological properties of one-dimensional weakly interacting topological insulators using bosonization.","To do that we study the topological edge states that emerge at the edges of a model realized by a strong impurity or at the boundary between topologically distinct phases.","In the bosonic model, the edge states are manifested as degenerate bosonic kinks at the boundaries.","We first illustrate this idea on the example of the interacting Su-Schrieffer-Heeger (SSH) chain.","We compute the localization length of the edge states as the width of an edge soliton that occurs in the SSH model in the presence of a strong impurity.","Next, we examine models of two capacitively coupled SSH chains that can be either identical or in distinct topological phases.","We find that weak Hubbard interaction reduces the ground state degeneracy in the topological phase of identical chains.","We then prove that similarly to the non-interacting model, the degeneracy of the edge states in the interacting case is protected by chiral symmetry.","We then study topological insulators built from two SSH chains with inter-chain hopping, that represent models of different chiral symmetric universality classes.","We demonstrate in bosonic language that the topological index of a weakly coupled model is determined by the type of inter-chain coupling, invariant under one of two possible chiral symmetry operators.","Finally, we show that a general one-dimensional model in a phase with topological index $\\nu$ is equivalent at low energies to a theory of at least $\\nu$ SSH chains.","We illustrate this idea on the example of an SSH model with longer-range hopping."],"url":"http://arxiv.org/abs/2402.07239v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-11 16:26:12","title":"Invariant $\u03bb$-translators in Lorentz-Minkowski space","abstract":"Given $\\lambda\\in\\mathbb{R}$ and $\\textbf{v}\\in\\mathbb{L}^3$, a $\\lambda$-translator with velocity $\\textbf{v}$ is an immersed surface in $\\mathbb{L}^3$ whose mean curvature satisfies $H=\\langle N,\\textbf{v}\\rangle+\\lambda$, where $N$ is a unit normal vector field. When $\\lambda=0$, we fall into the class of translating solitons of the mean curvature flow. In this paper we study $\\lambda$-translators in $\\mathbb{L}^3$ that are invariant under a 1-parameter group of translations and rotations. The former are cylindrical surfaces and explicit parametrizations are found, distinguishing on the causality of both the ruling direction and the $\\lambda$-translators. In the case of rotational $\\lambda$-translators we distinguish between spacelike and timelike rotations and exhibit the qualitative properties of rotational $\\lambda$-translators by analyzing the non-linear autonomous system fulfilled by the coordinate functions of the generating curves.","sentences":["Given $\\lambda\\in\\mathbb{R}$ and $\\textbf{v}\\in\\mathbb{L}^3$, a $\\lambda$-translator with velocity $\\textbf{v}$ is an immersed surface in $\\mathbb{L}^3$ whose mean curvature satisfies $H=\\langle N,\\textbf{v}\\rangle+\\lambda$, where $N$ is a unit normal vector field.","When $\\lambda=0$, we fall into the class of translating solitons of the mean curvature flow.","In this paper we study $\\lambda$-translators in $\\mathbb{L}^3$ that are invariant under a 1-parameter group of translations and rotations.","The former are cylindrical surfaces and explicit parametrizations are found, distinguishing on the causality of both the ruling direction and the $\\lambda$-translators.","In the case of rotational $\\lambda$-translators we distinguish between spacelike and timelike rotations and exhibit the qualitative properties of rotational $\\lambda$-translators by analyzing the non-linear autonomous system fulfilled by the coordinate functions of the generating curves."],"url":"http://arxiv.org/abs/2402.07237v1","category":"math.DG"}
{"created":"2024-02-11 16:19:34","title":"Extending Inferences from Randomized Clinical Trials to Target Populations Using Observational Data: A Scoping Review of Transportability Methods","abstract":"Objective: Randomized clinical trial (RCT) results often define clinical decision making, but the highly curated populations of trails, and the rigorous methodology utilized, are often not reflective of real-world practice. The objective of this scoping review is to identify the ability of practical methods to transport findings from randomized clinical trials (RCTs) to target populations. Study design: A scoping review was conducted on the literature regarding transportability from RCTs to observational data cohorts. Each study was assessed for the transportability methodology used, and to what degree the RCT treatment effect (estimated target parameter) was seen in the target population (intended patients) in observational data. Results: A total of 15 published papers were included. The research topics include cardiovascular diseases, infectious diseases, psychiatry, oncology, orthopedics, anesthesiology, and hematology. These studies show that the outcomes from RCTs could be extended to real-world settings, with varying degrees of effect size and precision. However, in some cases, the estimated target parameters for the target population were statistically significantly different from those in clinical trials. Conclusion: Despite variations in the magnitude of effects between RCTs and real-world studies, transportability methods can play an important role in effectively bridging the gap between clinical research and practical healthcare applications, offering valuable insights for evidence-based medicine.","sentences":["Objective: Randomized clinical trial (RCT) results often define clinical decision making, but the highly curated populations of trails, and the rigorous methodology utilized, are often not reflective of real-world practice.","The objective of this scoping review is to identify the ability of practical methods to transport findings from randomized clinical trials (RCTs) to target populations.","Study design: A scoping review was conducted on the literature regarding transportability from RCTs to observational data cohorts.","Each study was assessed for the transportability methodology used, and to what degree the RCT treatment effect (estimated target parameter) was seen in the target population (intended patients) in observational data.","Results:","A total of 15 published papers were included.","The research topics include cardiovascular diseases, infectious diseases, psychiatry, oncology, orthopedics, anesthesiology, and hematology.","These studies show that the outcomes from RCTs could be extended to real-world settings, with varying degrees of effect size and precision.","However, in some cases, the estimated target parameters for the target population were statistically significantly different from those in clinical trials.","Conclusion: Despite variations in the magnitude of effects between RCTs and real-world studies, transportability methods can play an important role in effectively bridging the gap between clinical research and practical healthcare applications, offering valuable insights for evidence-based medicine."],"url":"http://arxiv.org/abs/2402.07236v1","category":"stat.AP"}
{"created":"2024-02-11 15:35:40","title":"Recovering pulsar periodicity from time of arrival data by finding the shortest vector in a lattice","abstract":"The strict periodicity of pulsars is the primary source of information we have to learn about their nature and environment, it allows us to challenge general relativity and measure gravitational waves. Identifying such a periodicity from a discrete set of arrival times is a difficult algorithmic problem, particularly when the pulsar is in a binary system. This challenge is especially acute in $\\gamma$-ray pulsar astronomy as there are hundreds of unassociated Fermi-LAT sources awaiting a timing solution that will reveal their nature, and may allow adding them to pulsar timing arrays. The same issue arises when attempting to recover a strict periodicity for repeating fast radio bursts (FRBs). Such a detection would be a major breakthrough, providing us with the FRB source's age, magnetic field, and binary orbit.   The problem of recovering a timing solution from sparse time-of-arrival (TOA) data is currently unsolvable for pulsars in binary systems and incredibly hard even for single pulsars. In a series of papers, we will develop an algorithmic set of tools that will allow us to solve the timing recovery problem under different regimes. In this paper, we frame the timing recovery problem as the problem of finding a short vector in a lattice and obtain the solution using off-the-shelf lattice reduction and sieving techniques. As a proof of concept, we solve PSR J0318+0253, a millisecond $\\gamma$-ray pulsar discovered by FAST in a $\\gamma$-ray directed search, in a few CPU-minutes. We discuss the assumptions of the standard lattice techniques and quantify their performance and limitations.","sentences":["The strict periodicity of pulsars is the primary source of information we have to learn about their nature and environment, it allows us to challenge general relativity and measure gravitational waves.","Identifying such a periodicity from a discrete set of arrival times is a difficult algorithmic problem, particularly when the pulsar is in a binary system.","This challenge is especially acute in $\\gamma$-ray pulsar astronomy as there are hundreds of unassociated Fermi-LAT sources awaiting a timing solution that will reveal their nature, and may allow adding them to pulsar timing arrays.","The same issue arises when attempting to recover a strict periodicity for repeating fast radio bursts (FRBs).","Such a detection would be a major breakthrough, providing us with the FRB source's age, magnetic field, and binary orbit.   ","The problem of recovering a timing solution from sparse time-of-arrival (TOA) data is currently unsolvable for pulsars in binary systems and incredibly hard even for single pulsars.","In a series of papers, we will develop an algorithmic set of tools that will allow us to solve the timing recovery problem under different regimes.","In this paper, we frame the timing recovery problem as the problem of finding a short vector in a lattice and obtain the solution using off-the-shelf lattice reduction and sieving techniques.","As a proof of concept, we solve PSR J0318+0253, a millisecond $\\gamma$-ray pulsar discovered by FAST in a $\\gamma$-ray directed search, in a few CPU-minutes.","We discuss the assumptions of the standard lattice techniques and quantify their performance and limitations."],"url":"http://arxiv.org/abs/2402.07228v1","category":"astro-ph.HE"}
{"created":"2024-02-11 15:30:08","title":"Time-Delayed Game Strategy Analysis Among Japan, Other Nations, and the International Atomic Energy Agency in the Context of Fukushima Nuclear Wastewater Discharge Decision","abstract":"This academic paper examines the strategic interactions between Japan, other nations, and the International Atomic Energy Agency (IAEA) regarding Japan's decision to release treated nuclear wastewater from the Fukushima Daiichi Nuclear Power Plant into the sea. It introduces a payoff matrix and time-delay elements in replicator dynamic equations to mirror real-world decision-making delays. The paper analyzes the stability of strategies and conditions for different stable states using characteristic roots of a linearized system and numerical simulations. It concludes that time delays significantly affect decision-making stability and evolution trajectories in nuclear wastewater disposal strategies. The study highlights the importance of efficient wastewater treatment technology, the impact of export tax revenue losses on Japan's strategies, and the role of international cooperation. The novelty of the research lies in integrating time-delay elements from ocean dynamics and governmental decision-making into the game-theoretical model.","sentences":["This academic paper examines the strategic interactions between Japan, other nations, and the International Atomic Energy Agency (IAEA) regarding Japan's decision to release treated nuclear wastewater from the Fukushima Daiichi Nuclear Power Plant into the sea.","It introduces a payoff matrix and time-delay elements in replicator dynamic equations to mirror real-world decision-making delays.","The paper analyzes the stability of strategies and conditions for different stable states using characteristic roots of a linearized system and numerical simulations.","It concludes that time delays significantly affect decision-making stability and evolution trajectories in nuclear wastewater disposal strategies.","The study highlights the importance of efficient wastewater treatment technology, the impact of export tax revenue losses on Japan's strategies, and the role of international cooperation.","The novelty of the research lies in integrating time-delay elements from ocean dynamics and governmental decision-making into the game-theoretical model."],"url":"http://arxiv.org/abs/2402.07227v1","category":"math.DS"}
{"created":"2024-02-11 15:14:59","title":"Arbitrarily configurable nonlinear topological modes","abstract":"Topological modes (TMs) are typically localized at boundaries, interfaces and dislocations, and exponentially decay into the bulk of a large enough lattice. Recently, the non-Hermitian skin effect has been leveraged to delocalize the wavefunctions of TMs from the boundary and thus to increase the capacity of TMs dramatically. Here, we explore the capability of nonlinearity in designing and reconfiguring the wavefunctions of TMs. With growing intensity, wavefunctions of these in-gap nonlinear TMs undergo an initial deviation from exponential decay, gradually merge into arbitrarily designable plateaus, then encompass the entire nonlinear domain, and eventually concentrate at the nonlinear boundary. Intriguingly, such extended nonlinear TMs are still robust against defects and disorders, and stable in dynamics under external excitation. Advancing the conceptual understanding of the nonlinear TMs, our results open new avenues for increasing the capacity of TMs and developing compact and reconfigurable topological devices.","sentences":["Topological modes (TMs) are typically localized at boundaries, interfaces and dislocations, and exponentially decay into the bulk of a large enough lattice.","Recently, the non-Hermitian skin effect has been leveraged to delocalize the wavefunctions of TMs from the boundary and thus to increase the capacity of TMs dramatically.","Here, we explore the capability of nonlinearity in designing and reconfiguring the wavefunctions of TMs.","With growing intensity, wavefunctions of these in-gap nonlinear TMs undergo an initial deviation from exponential decay, gradually merge into arbitrarily designable plateaus, then encompass the entire nonlinear domain, and eventually concentrate at the nonlinear boundary.","Intriguingly, such extended nonlinear TMs are still robust against defects and disorders, and stable in dynamics under external excitation.","Advancing the conceptual understanding of the nonlinear TMs, our results open new avenues for increasing the capacity of TMs and developing compact and reconfigurable topological devices."],"url":"http://arxiv.org/abs/2402.07224v1","category":"quant-ph"}
{"created":"2024-02-11 14:50:01","title":"On (Mis)perceptions of testing effectiveness: an empirical study","abstract":"A recurring problem in software development is incorrect decision making on the techniques, methods and tools to be used. Mostly, these decisions are based on developers' perceptions about them. A factor influencing people's perceptions is past experience, but it is not the only one. In this research, we aim to discover how well the perceptions of the defect detection effectiveness of different techniques match their real effectiveness in the absence of prior experience. To do this, we conduct an empirical study plus a replication. During the original study, we conduct a controlled experiment with students applying two testing techniques and a code review technique. At the end of the experiment, they take a survey to find out which technique they perceive to be most effective. The results show that participants' perceptions are wrong and that this mismatch is costly in terms of quality. In order to gain further insight into the results, we replicate the controlled experiment and extend the survey to include questions about participants' opinions on the techniques and programs. The results of the replicated study confirm the findings of the original study and suggest that participants' perceptions might be based not on their opinions about complexity or preferences for techniques but on how well they think that they have applied the techniques.","sentences":["A recurring problem in software development is incorrect decision making on the techniques, methods and tools to be used.","Mostly, these decisions are based on developers' perceptions about them.","A factor influencing people's perceptions is past experience, but it is not the only one.","In this research, we aim to discover how well the perceptions of the defect detection effectiveness of different techniques match their real effectiveness in the absence of prior experience.","To do this, we conduct an empirical study plus a replication.","During the original study, we conduct a controlled experiment with students applying two testing techniques and a code review technique.","At the end of the experiment, they take a survey to find out which technique they perceive to be most effective.","The results show that participants' perceptions are wrong and that this mismatch is costly in terms of quality.","In order to gain further insight into the results, we replicate the controlled experiment and extend the survey to include questions about participants' opinions on the techniques and programs.","The results of the replicated study confirm the findings of the original study and suggest that participants' perceptions might be based not on their opinions about complexity or preferences for techniques but on how well they think that they have applied the techniques."],"url":"http://arxiv.org/abs/2402.07222v1","category":"cs.SE"}
{"created":"2024-02-11 14:01:30","title":"Fukushima Nuclear Wastewater Discharge: An Evolutionary Game Theory Approach to International and Domestic Interaction and Strategic Decision-Making","abstract":"On August 24, 2023, Japan controversially decided to discharge nuclear wastewater from the Fukushima Daiichi Nuclear Power Plant into the ocean, sparking intense domestic and global debates. This study uses evolutionary game theory to analyze the strategic dynamics between Japan, other countries, and the Japan Fisheries Association. By incorporating economic, legal, international aid, and environmental factors, the research identifies three evolutionarily stable strategies, analyzing them via numerical simulations. The focus is on Japan's shift from wastewater release to its cessation, exploring the myriad factors influencing this transition and their effects on stakeholders' decisions. Key insights highlight the need for international cooperation, rigorous scientific research, public education, and effective wastewater treatment methods. Offering both a fresh theoretical perspective and practical guidance, this study aims to foster global consensus on nuclear wastewater management, crucial for marine conservation and sustainable development.","sentences":["On August 24, 2023, Japan controversially decided to discharge nuclear wastewater from the Fukushima Daiichi Nuclear Power Plant into the ocean, sparking intense domestic and global debates.","This study uses evolutionary game theory to analyze the strategic dynamics between Japan, other countries, and the Japan Fisheries Association.","By incorporating economic, legal, international aid, and environmental factors, the research identifies three evolutionarily stable strategies, analyzing them via numerical simulations.","The focus is on Japan's shift from wastewater release to its cessation, exploring the myriad factors influencing this transition and their effects on stakeholders' decisions.","Key insights highlight the need for international cooperation, rigorous scientific research, public education, and effective wastewater treatment methods.","Offering both a fresh theoretical perspective and practical guidance, this study aims to foster global consensus on nuclear wastewater management, crucial for marine conservation and sustainable development."],"url":"http://arxiv.org/abs/2402.07210v1","category":"math.DS"}
{"created":"2024-02-11 13:26:40","title":"Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks","abstract":"Lightweight design of Convolutional Neural Networks (CNNs) requires co-design efforts in the model architectures and compression techniques. As a novel design paradigm that separates training and inference, a structural re-parameterized (SR) network such as the representative RepVGG revitalizes the simple VGG-like network with a high accuracy comparable to advanced and often more complicated networks. However, the merging process in SR networks introduces outliers into weights, making their distribution distinct from conventional networks and thus heightening difficulties in quantization. To address this, we propose an operator-level improvement for training called Outlier Aware Batch Normalization (OABN). Additionally, to meet the demands of limited bitwidths while upkeeping the inference accuracy, we develop a clustering-based non-uniform quantization framework for Quantization-Aware Training (QAT) named ClusterQAT. Integrating OABN with ClusterQAT, the quantized performance of RepVGG is largely enhanced, particularly when the bitwidth falls below 8.","sentences":["Lightweight design of Convolutional Neural Networks (CNNs) requires co-design efforts in the model architectures and compression techniques.","As a novel design paradigm that separates training and inference, a structural re-parameterized (SR) network such as the representative RepVGG revitalizes the simple VGG-like network with a high accuracy comparable to advanced and often more complicated networks.","However, the merging process in SR networks introduces outliers into weights, making their distribution distinct from conventional networks and thus heightening difficulties in quantization.","To address this, we propose an operator-level improvement for training called Outlier Aware Batch Normalization (OABN).","Additionally, to meet the demands of limited bitwidths while upkeeping the inference accuracy, we develop a clustering-based non-uniform quantization framework for Quantization-Aware Training (QAT) named ClusterQAT.","Integrating OABN with ClusterQAT, the quantized performance of RepVGG is largely enhanced, particularly when the bitwidth falls below 8."],"url":"http://arxiv.org/abs/2402.07200v1","category":"cs.CV"}
{"created":"2024-02-11 13:25:53","title":"More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning","abstract":"In this paper, we prove that Distributional Reinforcement Learning (DistRL), which learns the return distribution, can obtain second-order bounds in both online and offline RL in general settings with function approximation. Second-order bounds are instance-dependent bounds that scale with the variance of return, which we prove are tighter than the previously known small-loss bounds of distributional RL. To the best of our knowledge, our results are the first second-order bounds for low-rank MDPs and for offline RL. When specializing to contextual bandits (one-step RL problem), we show that a distributional learning based optimism algorithm achieves a second-order worst-case regret bound, and a second-order gap dependent bound, simultaneously. We also empirically demonstrate the benefit of DistRL in contextual bandits on real-world datasets. We highlight that our analysis with DistRL is relatively simple, follows the general framework of optimism in the face of uncertainty and does not require weighted regression. Our results suggest that DistRL is a promising framework for obtaining second-order bounds in general RL settings, thus further reinforcing the benefits of DistRL.","sentences":["In this paper, we prove that Distributional Reinforcement Learning (DistRL), which learns the return distribution, can obtain second-order bounds in both online and offline RL in general settings with function approximation.","Second-order bounds are instance-dependent bounds that scale with the variance of return, which we prove are tighter than the previously known small-loss bounds of distributional RL.","To the best of our knowledge, our results are the first second-order bounds for low-rank MDPs and for offline RL.","When specializing to contextual bandits (one-step RL problem), we show that a distributional learning based optimism algorithm achieves a second-order worst-case regret bound, and a second-order gap dependent bound, simultaneously.","We also empirically demonstrate the benefit of DistRL in contextual bandits on real-world datasets.","We highlight that our analysis with DistRL is relatively simple, follows the general framework of optimism in the face of uncertainty and does not require weighted regression.","Our results suggest that DistRL is a promising framework for obtaining second-order bounds in general RL settings, thus further reinforcing the benefits of DistRL."],"url":"http://arxiv.org/abs/2402.07198v1","category":"cs.LG"}
{"created":"2024-02-11 13:17:55","title":"Trilayer multi-orbital models of $\\mathrm{La_{4}Ni_{3}O_{10}}$","abstract":"Recently, the discovery of superconductivity in Ruddlesden-Popper (RP) $\\mathrm{La_4Ni_3O_{10}}$ under pressure has further expanded the realm of nickelate-based superconductor family. In this paper, we performed a first-principle study of $\\mathrm{La_4Ni_3O_{10}}$ for both $P2_1/a$ phase at ambient pressure and $I4/mmm$ phase at high pressure, with $U$=0, 3.5 eV. Our results confirmed the characteristic upward shift of Ni-$d_{z^2}$ bonding band under pressure. Moreover, our analysis of electronic spectrum and orbital occupancy unveil the dynamical mechanism of electronic reconstructions under pressure, embedded in a critical dual effect. Based on our DFT results, we further proposed a trilayer two-orbital model by performing Wannier downfolding on Ni-$e_g$ orbitals. Our model reveals four Fermi surface sheets with $\\alpha,\\beta,\\beta^\\prime,\\gamma$ pockets, bearing resemblance to that of bilayer $\\mathrm{La_3Ni_2O_7}$. According to the model, our calculated spin susceptibility under random phase approximation predicts an analogous magnetic signal at ${\\rm q}=(\\frac{\\pi}{2},\\frac{\\pi}{2})$, which is more associated with nesting within $\\beta,\\beta^\\prime$ pockets. Finally, a high energy sixteen-orbital model with direct $dp,pp$ hoppings is proposed, which implies that $\\mathrm{La_4Ni_3O_{10}}$ also lies in charge-transfer picture within Zaanen-Sawatzky-Allen scheme. Our exposition of electronic reconstructions and multi-orbital models shed light on theoretical electronic correlation study and experimental exploration for lower pressure in RP series.","sentences":["Recently, the discovery of superconductivity in Ruddlesden-Popper (RP) $\\mathrm{La_4Ni_3O_{10}}$ under pressure has further expanded the realm of nickelate-based superconductor family.","In this paper, we performed a first-principle study of $\\mathrm{La_4Ni_3O_{10}}$ for both $P2_1/a$ phase at ambient pressure and $I4/mmm$ phase at high pressure, with $U$=0, 3.5 eV. Our results confirmed the characteristic upward shift of Ni-$d_{z^2}$ bonding band under pressure.","Moreover, our analysis of electronic spectrum and orbital occupancy unveil the dynamical mechanism of electronic reconstructions under pressure, embedded in a critical dual effect.","Based on our DFT results, we further proposed a trilayer two-orbital model by performing Wannier downfolding on Ni-$e_g$ orbitals.","Our model reveals four Fermi surface sheets with $\\alpha,\\beta,\\beta^\\prime,\\gamma$ pockets, bearing resemblance to that of bilayer $\\mathrm{La_3Ni_2O_7}$.","According to the model, our calculated spin susceptibility under random phase approximation predicts an analogous magnetic signal at ${\\rm q}=(\\frac{\\pi}{2},\\frac{\\pi}{2})$, which is more associated with nesting within $\\beta,\\beta^\\prime$ pockets.","Finally, a high energy sixteen-orbital model with direct $dp,pp$ hoppings is proposed, which implies that $\\mathrm{La_4Ni_3O_{10}}$ also lies in charge-transfer picture within Zaanen-Sawatzky-Allen scheme.","Our exposition of electronic reconstructions and multi-orbital models shed light on theoretical electronic correlation study and experimental exploration for lower pressure in RP series."],"url":"http://arxiv.org/abs/2402.07196v1","category":"cond-mat.supr-con"}
{"created":"2024-02-11 12:58:42","title":"Spatio-spectral classification of hyperspectral images for brain cancer detection during surgical operations","abstract":"Surgery for brain cancer is a major problem in neurosurgery. The diffuse infiltration into the surrounding normal brain by these tumors makes their accurate identification by the naked eye difficult. Since surgery is the common treatment for brain cancer, an accurate radical resection of the tumor leads to improved survival rates for patients. However, the identification of the tumor boundaries during surgery is challenging. Hyperspectral imaging is a noncontact, non-ionizing and non-invasive technique suitable for medical diagnosis. This study presents the development of a novel classification method taking into account the spatial and spectral characteristics of the hyperspectral images to help neurosurgeons to accurately determine the tumor boundaries in surgical-time during the resection, avoiding excessive excision of normal tissue or unintentionally leaving residual tumor. The algorithm proposed in this study to approach an efficient solution consists of a hybrid framework that combines both supervised and unsupervised machine learning methods. To evaluate the proposed approach, five hyperspectral images of surface of the brain affected by glioblastoma tumor in vivo from five different patients have been used. The final classification maps obtained have been analyzed and validated by specialists. These preliminary results are promising, obtaining an accurate delineation of the tumor area.","sentences":["Surgery for brain cancer is a major problem in neurosurgery.","The diffuse infiltration into the surrounding normal brain by these tumors makes their accurate identification by the naked eye difficult.","Since surgery is the common treatment for brain cancer, an accurate radical resection of the tumor leads to improved survival rates for patients.","However, the identification of the tumor boundaries during surgery is challenging.","Hyperspectral imaging is a noncontact, non-ionizing and non-invasive technique suitable for medical diagnosis.","This study presents the development of a novel classification method taking into account the spatial and spectral characteristics of the hyperspectral images to help neurosurgeons to accurately determine the tumor boundaries in surgical-time during the resection, avoiding excessive excision of normal tissue or unintentionally leaving residual tumor.","The algorithm proposed in this study to approach an efficient solution consists of a hybrid framework that combines both supervised and unsupervised machine learning methods.","To evaluate the proposed approach, five hyperspectral images of surface of the brain affected by glioblastoma tumor in vivo from five different patients have been used.","The final classification maps obtained have been analyzed and validated by specialists.","These preliminary results are promising, obtaining an accurate delineation of the tumor area."],"url":"http://arxiv.org/abs/2402.07192v1","category":"eess.IV"}
{"created":"2024-02-11 12:51:38","title":"Improved Hierarchical Coded Caching Schemes from $t$-Designs","abstract":"Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN) achieves an optimal transmission rate $R$ under uncoded placement but requires a subpacketization level $F$ which increases exponentially with the number of users $K$ where the number of files $N \\geq K$. Placement delivery array (PDA) was proposed as a tool to design coded caching schemes with reduced subpacketization level by Yan \\textit{et al.} in \\cite{YCT}. This paper proposes two novel classes of PDA constructions from combinatorial $t$-designs that achieve an improved transmission rate for a given low subpacketization level, cache size and number of users compared to existing coded caching schemes from $t$-designs. A $(K, F, Z, S)$ PDA composed of a specific symbol $\\star$ and $S$ non-negative integers corresponds to a coded caching scheme with subpacketization level $F$, $K$ users each caching $Z$ packets and the demands of all the users are met with a rate $R=\\frac{S}{F}$. For a given $K$, $F$ and $Z$, a lower bound on $S$ such that a $(K, F, Z, S)$ PDA exists is given by Cheng \\textit{et al.} in \\cite{MJXQ}. Our first class of proposed PDA achieves this lower bound on $S$. The second class of PDA also achieves this lower bound in some cases. From these two classes of PDAs, we then construct hierarchical placement delivery arrays (HPDA), proposed by Kong \\textit{et al.} in \\cite{KYWM}, which characterizes a hierarchical two-layer coded caching system. These constructions give very low subpacketization level schemes with optimal rate with respect to the lower bound on $S$ in first layer transmission from server to attached mirrors.","sentences":["Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN) achieves an optimal transmission rate $R$ under uncoded placement but requires a subpacketization level $F$ which increases exponentially with the number of users $K$ where the number of files $N \\geq K$. Placement delivery array (PDA) was proposed as a tool to design coded caching schemes with reduced subpacketization level by Yan \\textit{et al.} in \\cite{YCT}.","This paper proposes two novel classes of PDA constructions from combinatorial $t$-designs that achieve an improved transmission rate for a given low subpacketization level, cache size and number of users compared to existing coded caching schemes from $t$-designs.","A $(K, F, Z, S)$ PDA composed of a specific symbol $\\star$ and $S$ non-negative integers corresponds to a coded caching scheme with subpacketization level $F$, $K$ users each caching $Z$ packets and the demands of all the users are met with a rate $R=\\frac{S}{F}$. For a given $K$, $F$ and $Z$, a lower bound on $S$ such that a $(K, F, Z, S)$ PDA exists is given by Cheng \\textit{et","al.} in \\cite{MJXQ}.","Our first class of proposed PDA achieves this lower bound on $S$. The second class of PDA also achieves this lower bound in some cases.","From these two classes of PDAs, we then construct hierarchical placement delivery arrays (HPDA), proposed by Kong \\textit{et al.} in \\cite{KYWM}, which characterizes a hierarchical two-layer coded caching system.","These constructions give very low subpacketization level schemes with optimal rate with respect to the lower bound on $S$ in first layer transmission from server to attached mirrors."],"url":"http://arxiv.org/abs/2402.07188v1","category":"cs.IT"}
{"created":"2024-02-11 12:35:13","title":"Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning","abstract":"A significant challenge in multi-objective reinforcement learning is obtaining a Pareto front of policies that attain optimal performance under different preferences. We introduce Iterated Pareto Referent Optimisation (IPRO), a principled algorithm that decomposes the task of finding the Pareto front into a sequence of single-objective problems for which various solution methods exist. This enables us to establish convergence guarantees while providing an upper bound on the distance to undiscovered Pareto optimal solutions at each step. Empirical evaluations demonstrate that IPRO matches or outperforms methods that require additional domain knowledge. By leveraging problem-specific single-objective solvers, our approach also holds promise for applications beyond multi-objective reinforcement learning, such as in pathfinding and optimisation.","sentences":["A significant challenge in multi-objective reinforcement learning is obtaining a Pareto front of policies that attain optimal performance under different preferences.","We introduce Iterated Pareto Referent Optimisation (IPRO), a principled algorithm that decomposes the task of finding the Pareto front into a sequence of single-objective problems for which various solution methods exist.","This enables us to establish convergence guarantees while providing an upper bound on the distance to undiscovered Pareto optimal solutions at each step.","Empirical evaluations demonstrate that IPRO matches or outperforms methods that require additional domain knowledge.","By leveraging problem-specific single-objective solvers, our approach also holds promise for applications beyond multi-objective reinforcement learning, such as in pathfinding and optimisation."],"url":"http://arxiv.org/abs/2402.07182v1","category":"cs.LG"}
{"created":"2024-02-11 12:00:23","title":"Quantum geometric bound for saturated ferromagnetism","abstract":"Despite its abundance in nature, predicting the occurrence of ferromagnetism in the ground state is possible only under very limited conditions such as in a flat band system with repulsive interaction or in a band with a single hole under infinitely large Coulomb repulsion, etc. Here, we propose a general condition to achieve saturated ferromagnetism based on the quantum geometry of electronic wave functions in itinerant electron systems. By analyzing multi-band repulsive Hubbard models with an integer band filling, relevant to either ferromagnetic insulators or semimetals, we propose a rigorous quantum geometric upper bound on the spin stiffness. By employing this geometric bound, we establish that saturated ferromagnetism is prohibited in the absence of interband coupling, even when the local Hubbard repulsion is infinitely large. As a corollary, this shows that saturated ferromagnetism is forbidden in any half-filled Hubbard model. We also derive the condition that the upper bound of the spin stiffness can be completely characterized by the Abelian quantum metric. We believe that our findings reveal a profound connection between quantum geometry and ferromagnetism, which can be extended to various symmetry-broken ground states in itinerant electronic systems.","sentences":["Despite its abundance in nature, predicting the occurrence of ferromagnetism in the ground state is possible only under very limited conditions such as in a flat band system with repulsive interaction or in a band with a single hole under infinitely large Coulomb repulsion, etc.","Here, we propose a general condition to achieve saturated ferromagnetism based on the quantum geometry of electronic wave functions in itinerant electron systems.","By analyzing multi-band repulsive Hubbard models with an integer band filling, relevant to either ferromagnetic insulators or semimetals, we propose a rigorous quantum geometric upper bound on the spin stiffness.","By employing this geometric bound, we establish that saturated ferromagnetism is prohibited in the absence of interband coupling, even when the local Hubbard repulsion is infinitely large.","As a corollary, this shows that saturated ferromagnetism is forbidden in any half-filled Hubbard model.","We also derive the condition that the upper bound of the spin stiffness can be completely characterized by the Abelian quantum metric.","We believe that our findings reveal a profound connection between quantum geometry and ferromagnetism, which can be extended to various symmetry-broken ground states in itinerant electronic systems."],"url":"http://arxiv.org/abs/2402.07171v1","category":"cond-mat.str-el"}
{"created":"2024-02-11 11:17:31","title":"\"To renormalize or not to renormalize ?'' in the proton-deuteron scattering calculations","abstract":"We discuss two approaches which, by applying the screening method, permit one to include the long range proton-proton (pp) Coulomb force in proton-deuteron (pd) momentum-space scattering calculations. In the first one, based on Alt-Grassberger-Sandhas (AGS) equation, presented in Phys. Rev. C{\\bf{71}}, 054005 (2005) and {\\bf{73}}, 057001 (2006), one needs to renormalize elastic scattering amplitude before calculating observables. In the second treatment, proposed by us in Eur. Phys. Journal A {\\bf{41}}, 369 (2009), {\\bf{41}}, 385 (2009), and arXiv:2310.03433 [nucl.th], this renormalization is avoided. For the proton induced deuteron breakup reaction both approaches require renormalization of the corresponding transition amplitudes. We derive the basic equations underlying both methods under the assumption that all contributing partial wave states are included and explain why in our approach renormalization of the elastic scattering amplitude is superfluous. We show that in order to take into account in the screening limit all partial waves it is required that four additional terms, based on the 3-dimensional and partial-wave projected pp Coulomb t-matrices, identical for both approaches, must appear in transition amplitudes. We investigate importance of these terms for elastic pd scattering below the breakup threshold.","sentences":["We discuss two approaches which, by applying the screening method, permit one to include the long range proton-proton (pp) Coulomb force in proton-deuteron (pd) momentum-space scattering calculations.","In the first one, based on Alt-Grassberger-Sandhas (AGS) equation, presented in Phys.","Rev. C{\\bf{71}}, 054005 (2005) and {\\bf{73}}, 057001 (2006), one needs to renormalize elastic scattering amplitude before calculating observables.","In the second treatment, proposed by us in Eur. Phys.","Journal A {\\bf{41}}, 369 (2009), {\\bf{41}}, 385 (2009), and arXiv:2310.03433","[nucl.th], this renormalization is avoided.","For the proton induced deuteron breakup reaction both approaches require renormalization of the corresponding transition amplitudes.","We derive the basic equations underlying both methods under the assumption that all contributing partial wave states are included and explain why in our approach renormalization of the elastic scattering amplitude is superfluous.","We show that in order to take into account in the screening limit all partial waves it is required that four additional terms, based on the 3-dimensional and partial-wave projected pp Coulomb t-matrices, identical for both approaches, must appear in transition amplitudes.","We investigate importance of these terms for elastic pd scattering below the breakup threshold."],"url":"http://arxiv.org/abs/2402.07163v1","category":"nucl-th"}
{"created":"2024-02-11 11:15:47","title":"Extended $N$-centered ensemble density functional theory of double electronic excitations","abstract":"A recent work [arXiv:2401.04685] has merged $N$-centered ensembles of neutral and charged electronic ground states with ensembles of neutral ground and excited states, thus providing a general and in-principle exact (so-called extended $N$-centered) ensemble density functional theory of neutral and charged electronic excitations. This formalism made it possible to revisit the concept of density-functional derivative discontinuity, in the particular case of single excitations from the highest occupied Kohn-Sham (KS) molecular orbital, without invoking the usual \"asymptotic behavior of the density\" argument. In this work, we address a broader class of excitations, with a particular focus on double excitations. An exact implementation of the theory is presented for the two-electron Hubbard dimer model. A thorough comparison of the true physical ground- and excited-state electronic structures with that of the fictitious ensemble density-functional KS system is also presented. Depending on the choice of the density-functional ensemble as well as the asymmetry of the dimer and the correlation strength, an inversion of states can be observed. In some other cases, the strong mixture of KS states within the true physical system makes the assignment \"single excitation\" or \"double excitation\" irrelevant.","sentences":["A recent work [arXiv:2401.04685] has merged $N$-centered ensembles of neutral and charged electronic ground states with ensembles of neutral ground and excited states, thus providing a general and in-principle exact (so-called extended $N$-centered) ensemble density functional theory of neutral and charged electronic excitations.","This formalism made it possible to revisit the concept of density-functional derivative discontinuity, in the particular case of single excitations from the highest occupied Kohn-Sham (KS) molecular orbital, without invoking the usual \"asymptotic behavior of the density\" argument.","In this work, we address a broader class of excitations, with a particular focus on double excitations.","An exact implementation of the theory is presented for the two-electron Hubbard dimer model.","A thorough comparison of the true physical ground- and excited-state electronic structures with that of the fictitious ensemble density-functional KS system is also presented.","Depending on the choice of the density-functional ensemble as well as the asymmetry of the dimer and the correlation strength, an inversion of states can be observed.","In some other cases, the strong mixture of KS states within the true physical system makes the assignment \"single excitation\" or \"double excitation\" irrelevant."],"url":"http://arxiv.org/abs/2402.07161v1","category":"cond-mat.str-el"}
{"created":"2024-02-11 11:02:25","title":"A hybrid iterative method based on MIONet for PDEs: Theory and numerical examples","abstract":"We propose a hybrid iterative method based on MIONet for PDEs, which combines the traditional numerical iterative solver and the recent powerful machine learning method of neural operator, and further systematically analyze its theoretical properties, including the convergence condition, the spectral behavior, as well as the convergence rate, in terms of the errors of the discretization and the model inference. We show the theoretical results for the frequently-used smoothers, i.e. Richardson (damped Jacobi) and Gauss-Seidel. We give an upper bound of the convergence rate of the hybrid method w.r.t. the model correction period, which indicates a minimum point to make the hybrid iteration converge fastest. Several numerical examples including the hybrid Richardson (Gauss-Seidel) iteration for the 1-d (2-d) Poisson equation are presented to verify our theoretical results, and also reflect an excellent acceleration effect. As a meshless acceleration method, it is provided with enormous potentials for practice applications.","sentences":["We propose a hybrid iterative method based on MIONet for PDEs, which combines the traditional numerical iterative solver and the recent powerful machine learning method of neural operator, and further systematically analyze its theoretical properties, including the convergence condition, the spectral behavior, as well as the convergence rate, in terms of the errors of the discretization and the model inference.","We show the theoretical results for the frequently-used smoothers, i.e. Richardson (damped Jacobi) and Gauss-Seidel.","We give an upper bound of the convergence rate of the hybrid method w.r.t.","the model correction period, which indicates a minimum point to make the hybrid iteration converge fastest.","Several numerical examples including the hybrid Richardson (Gauss-Seidel) iteration for the 1-d (2-d) Poisson equation are presented to verify our theoretical results, and also reflect an excellent acceleration effect.","As a meshless acceleration method, it is provided with enormous potentials for practice applications."],"url":"http://arxiv.org/abs/2402.07156v1","category":"math.NA"}
{"created":"2024-02-11 10:57:21","title":"Grain boundary strain localization in CdTe solar cell revealed by Scanning 3D X-ray diffraction microscopy","abstract":"Cadmium Telluride (CdTe) solar cell technology is a promising candidate to help boost green energy production. However, impurities and structural defects are major barriers to improving the solar power conversion efficiency. Grain boundaries often act as aggregation sites for impurities, resulting in strain localization in areas of high diffusion. In this study, we demonstrate the use of scanning 3D X-ray diffraction microscopy to non-destructively make 3D maps of the grains, their phase, orientation, and local strain within a CdTe solar cell absorber layer with a resolution of 100 nm. We quantify twin boundaries and suggest how they affect grain size and orientation distribution. Local strain analysis reveals that strain is primarily associated with high misorientation grain boundaries, whereas twin boundaries do not show high strain values. We also observe that high-strain grain boundaries form a continuous pathway connected to the CdS layer. Hence, this high-strain region is believed to be associated with the diffusion of sulfur from the CdS layer along grain boundaries. This hypothesis is supported by SEM EDS and X-ray fluorescence experiments. The method and analysis demonstrated in this work can be applied to different polycrystalline materials where the characterization of grain boundary properties is essential to understand the microstructural phenomena.","sentences":["Cadmium Telluride (CdTe) solar cell technology is a promising candidate to help boost green energy production.","However, impurities and structural defects are major barriers to improving the solar power conversion efficiency.","Grain boundaries often act as aggregation sites for impurities, resulting in strain localization in areas of high diffusion.","In this study, we demonstrate the use of scanning 3D X-ray diffraction microscopy to non-destructively make 3D maps of the grains, their phase, orientation, and local strain within a CdTe solar cell absorber layer with a resolution of 100 nm.","We quantify twin boundaries and suggest how they affect grain size and orientation distribution.","Local strain analysis reveals that strain is primarily associated with high misorientation grain boundaries, whereas twin boundaries do not show high strain values.","We also observe that high-strain grain boundaries form a continuous pathway connected to the CdS layer.","Hence, this high-strain region is believed to be associated with the diffusion of sulfur from the CdS layer along grain boundaries.","This hypothesis is supported by SEM EDS and X-ray fluorescence experiments.","The method and analysis demonstrated in this work can be applied to different polycrystalline materials where the characterization of grain boundary properties is essential to understand the microstructural phenomena."],"url":"http://arxiv.org/abs/2402.07155v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-11 10:38:32","title":"Reducing the metal-graphene contact resistance through laser-induced defects","abstract":"Graphene has been extensively studied for a variety of electronic and optoelectronic applications. The reported contact resistance between metal and graphene, or rather its specific contact resistance (R{_C}), ranges from a few tens of {\\Omega} {\\mu}m up to a few k{\\Omega} {\\mu}m. Manufacturable solutions for defining ohmic contacts to graphene remain a subject of research. Here, we report a scalable method based on laser irradiation of graphene to reduce the R{_C} in nickel-contacted devices. A laser with a wavelength of {\\lambda} = 532 nm is used to induce defects at the contact regions, which are monitored \\textit{in-situ} using micro-Raman spectroscopy. Physical damage is observed using \\textit{ex-situ} atomic force and scanning electron microscopy. The transfer line method (TLM) is used to extract R{_C} from back-gated graphene devices with and without laser treatment under ambient and vacuum conditions. A significant reduction in R{_C} is observed in devices where the contacts are laser irradiated, which scales with the laser power. The lowest R{_C} of about 250 {\\Omega} {\\mu}m is obtained for the devices irradiated with a laser power of 20 mW, compared to 900 {\\Omega} {\\mu}m for the untreated devices. The reduction is attributed to an increase in defect density, which leads to the formation of crystallite edges and in-plane dangling bonds that enhance the injection of charge carriers from the metal into the graphene. Our work suggests laser irradiation as a scalable technology for R{_C} reduction in graphene and potentially other two-dimensional materials.","sentences":["Graphene has been extensively studied for a variety of electronic and optoelectronic applications.","The reported contact resistance between metal and graphene, or rather its specific contact resistance (R{_C}), ranges from a few tens of {\\Omega} {\\mu}m up to a few k{\\Omega} {\\mu}m.","Manufacturable solutions for defining ohmic contacts to graphene remain a subject of research.","Here, we report a scalable method based on laser irradiation of graphene to reduce the R{_C} in nickel-contacted devices.","A laser with a wavelength of {\\lambda} = 532 nm is used to induce defects at the contact regions, which are monitored \\textit{in-situ} using micro-Raman spectroscopy.","Physical damage is observed using \\textit{ex-situ} atomic force and scanning electron microscopy.","The transfer line method (TLM) is used to extract R{_C} from back-gated graphene devices with and without laser treatment under ambient and vacuum conditions.","A significant reduction in R{_C} is observed in devices where the contacts are laser irradiated, which scales with the laser power.","The lowest R{_C} of about 250 {\\Omega} {\\mu}m is obtained for the devices irradiated with a laser power of 20 mW, compared to 900 {\\Omega} {\\mu}m for the untreated devices.","The reduction is attributed to an increase in defect density, which leads to the formation of crystallite edges and in-plane dangling bonds that enhance the injection of charge carriers from the metal into the graphene.","Our work suggests laser irradiation as a scalable technology for R{_C} reduction in graphene and potentially other two-dimensional materials."],"url":"http://arxiv.org/abs/2402.07151v1","category":"physics.app-ph"}
{"created":"2024-02-11 10:08:41","title":"A Fundamental Analysis of the Impact on Traffic Assignment by Toll System of Electric Road System","abstract":"Electric road system (ERS) is expected to make electric vehicles (EVs) more popular as EVs with Dynamic Wireless Power Transfer (DWPT) system can be charged while driving on ERS. Although some studies dealt with ERS implementation, its toll system has not been explored yet. This paper aims at a fundamental analysis on impact of ERS toll system on a traffic assignment. We conduct assignments on a simple network where two vehicle types (EVs with DWPT and others) are co-existing. The results under two toll systems showed some undesirable situations, such as total travel time was not minimised, total charged volume was not optimised, and ERS was not utilised. The occurrence of them depended on the ratio of EVs, battery level, value of electricity, and toll price. The difficulty to control such situations by toll price was discussed as the battery level and value of electricity may vary over time.","sentences":["Electric road system (ERS) is expected to make electric vehicles (EVs) more popular as EVs with Dynamic Wireless Power Transfer (DWPT) system can be charged while driving on ERS.","Although some studies dealt with ERS implementation, its toll system has not been explored yet.","This paper aims at a fundamental analysis on impact of ERS toll system on a traffic assignment.","We conduct assignments on a simple network where two vehicle types (EVs with DWPT and others) are co-existing.","The results under two toll systems showed some undesirable situations, such as total travel time was not minimised, total charged volume was not optimised, and ERS was not utilised.","The occurrence of them depended on the ratio of EVs, battery level, value of electricity, and toll price.","The difficulty to control such situations by toll price was discussed as the battery level and value of electricity may vary over time."],"url":"http://arxiv.org/abs/2402.07144v1","category":"eess.SY"}
{"created":"2024-02-11 10:06:52","title":"Electronic structure of the alternating monolayer-trilayer phase of La3Ni2O7","abstract":"Signatures of superconductivity near 80 K were recently discovered in crystals of La-Ni-O with the stoichiometry of 3:2:7 under high pressures. Structural studies have shown conflicting results highlighting both the expected bilayer (2222) structure and an unexpected alternating monolayer-trilayer (1313) structure, both apparently showing superconductivity. Here, we perform angle-resolved photoemission spectroscopy on crystals with the 1313 structure to elucidate the distinct electronic features compared to the previously studied 2222 structure. Our measurements suggest that a flat band of high electron density is much closer to the Fermi energy in the 1313 structure compared to the 2222 structure. Furthermore, we observe an additional electron pocket at the Brillouin zone center with Ni-$d_{z^2}$ orbital character, highlighting the importance of the out-of-plane orbital component. In comparison to LDA+U band structure calculations, we find strong renormalization of both the Ni-$d_{z^2}$ and Ni-$d_{x^2-y^2}$ derived bands, suggesting strong correlation effects beyond that of moderate Hubbard physics. These results reveal important differences in the electronic structure brought about by the distinct structural motifs with the same stoichiometry. Such differences may contain the key to understanding superconductivity in the layered perovskite nickelates.","sentences":["Signatures of superconductivity near 80 K were recently discovered in crystals of La-Ni-O with the stoichiometry of 3:2:7 under high pressures.","Structural studies have shown conflicting results highlighting both the expected bilayer (2222) structure and an unexpected alternating monolayer-trilayer (1313) structure, both apparently showing superconductivity.","Here, we perform angle-resolved photoemission spectroscopy on crystals with the 1313 structure to elucidate the distinct electronic features compared to the previously studied 2222 structure.","Our measurements suggest that a flat band of high electron density is much closer to the Fermi energy in the 1313 structure compared to the 2222 structure.","Furthermore, we observe an additional electron pocket at the Brillouin zone center with Ni-$d_{z^2}$ orbital character, highlighting the importance of the out-of-plane orbital component.","In comparison to LDA+U band structure calculations, we find strong renormalization of both the Ni-$d_{z^2}$ and Ni-$d_{x^2-y^2}$ derived bands, suggesting strong correlation effects beyond that of moderate Hubbard physics.","These results reveal important differences in the electronic structure brought about by the distinct structural motifs with the same stoichiometry.","Such differences may contain the key to understanding superconductivity in the layered perovskite nickelates."],"url":"http://arxiv.org/abs/2402.07143v1","category":"cond-mat.supr-con"}
{"created":"2024-02-11 09:46:15","title":"Towards Robust Car Following Dynamics Modeling via Blackbox Models: Methodology, Analysis, and Recommendations","abstract":"The selection of the target variable is important while learning parameters of the classical car following models like GIPPS, IDM, etc. There is a vast body of literature on which target variable is optimal for classical car following models, but there is no study that empirically evaluates the selection of optimal target variables for black-box models, such as LSTM, etc. The black-box models, like LSTM and Gaussian Process (GP) are increasingly being used to model car following behavior without wise selection of target variables. The current work tests different target variables, like acceleration, velocity, and headway, for three black-box models, i.e., GP, LSTM, and Kernel Ridge Regression. These models have different objective functions and work in different vector spaces, e.g., GP works in function space, and LSTM works in parameter space. The experiments show that the optimal target variable recommendations for black-box models differ from classical car following models depending on the objective function and the vector space. It is worth mentioning that models and datasets used during evaluation are diverse in nature: the datasets contained both automated and human-driven vehicle trajectories; the black-box models belong to both parametric and non-parametric classes of models. This diversity is important during the analysis of variance, wherein we try to find the interaction between datasets, models, and target variables. It is shown that the models and target variables interact and recommended target variables don't depend on the dataset under consideration.","sentences":["The selection of the target variable is important while learning parameters of the classical car following models like GIPPS, IDM, etc.","There is a vast body of literature on which target variable is optimal for classical car following models, but there is no study that empirically evaluates the selection of optimal target variables for black-box models, such as LSTM, etc.","The black-box models, like LSTM and Gaussian Process (GP) are increasingly being used to model car following behavior without wise selection of target variables.","The current work tests different target variables, like acceleration, velocity, and headway, for three black-box models, i.e., GP, LSTM, and Kernel Ridge Regression.","These models have different objective functions and work in different vector spaces, e.g., GP works in function space, and LSTM works in parameter space.","The experiments show that the optimal target variable recommendations for black-box models differ from classical car following models depending on the objective function and the vector space.","It is worth mentioning that models and datasets used during evaluation are diverse in nature: the datasets contained both automated and human-driven vehicle trajectories; the black-box models belong to both parametric and non-parametric classes of models.","This diversity is important during the analysis of variance, wherein we try to find the interaction between datasets, models, and target variables.","It is shown that the models and target variables interact and recommended target variables don't depend on the dataset under consideration."],"url":"http://arxiv.org/abs/2402.07139v1","category":"cs.LG"}
{"created":"2024-02-11 08:49:46","title":"Intrinsic constraint on $T_c$ for unconventional superconductivity","abstract":"Can room temperature superconductivity be achieved in correlated materials under ambient pressure? Our answer to this billion-dollar question is probably no, at least for realistic models within the current theoretical framework. This is shown by our systematic simulations on the pairing instability of some effective models for two-dimensional superconductivity. For a square lattice model with nearest-neighbour pairing, we find a plaquette state formed of weakly-connected $2\\times2$ blocks for sufficiently large pairing interaction. The superconductivity is suppressed on both sides away from its melting quantum critical point. Thus, the plaquette state constrains the magnitude of $T_c$ for large pairing interactions and may be viewed as a strong-coupling parent state of $d$-wave superconductivity, in resemblance of other competing orders. We then extend our simulations to a variety of effective models covering nearest-neighbour or onsite pairing, single layer or two-layer, intralayer or interlayer pairing, and find an intrinsic maximum of the ratio $T_c/J\\approx 0.04-0.07$, where $J$ is the onsite or nearest-neighbour pairing interaction. Comparison with existing experiments supports this constraint in cuprate, iron-based, nickelate, and heavy fermion superconductors, despite these compounds are so complicated well beyond our simplified models. As a result, the known families of unconventional superconductivity seem to almost exhaust their potentials in reaching the maximal $T_c$ allowed by their spin exchange interaction, while achieving room temperature superconductor would require a much larger $J$ beyond 400-700 meV, which seems unrealistic and hence demands novel pairing mechanisms.","sentences":["Can room temperature superconductivity be achieved in correlated materials under ambient pressure?","Our answer to this billion-dollar question is probably no, at least for realistic models within the current theoretical framework.","This is shown by our systematic simulations on the pairing instability of some effective models for two-dimensional superconductivity.","For a square lattice model with nearest-neighbour pairing, we find a plaquette state formed of weakly-connected $2\\times2$ blocks for sufficiently large pairing interaction.","The superconductivity is suppressed on both sides away from its melting quantum critical point.","Thus, the plaquette state constrains the magnitude of $T_c$ for large pairing interactions and may be viewed as a strong-coupling parent state of $d$-wave superconductivity, in resemblance of other competing orders.","We then extend our simulations to a variety of effective models covering nearest-neighbour or onsite pairing, single layer or two-layer, intralayer or interlayer pairing, and find an intrinsic maximum of the ratio $T_c/J\\approx 0.04-0.07$, where $J$ is the onsite or nearest-neighbour pairing interaction.","Comparison with existing experiments supports this constraint in cuprate, iron-based, nickelate, and heavy fermion superconductors, despite these compounds are so complicated well beyond our simplified models.","As a result, the known families of unconventional superconductivity seem to almost exhaust their potentials in reaching the maximal $T_c$ allowed by their spin exchange interaction, while achieving room temperature superconductor would require a much larger $J$ beyond 400-700 meV, which seems unrealistic and hence demands novel pairing mechanisms."],"url":"http://arxiv.org/abs/2402.07128v1","category":"cond-mat.supr-con"}
{"created":"2024-02-11 08:35:55","title":"Intermodal competition in the Brazilian interstate travel market","abstract":"This paper presents a test of intermodal interaction between coaches and airlines in Brazil in order to check for the efficacy of recent liberalization measures designed to promote competition in both industries. Interstate travel service in the country is heavily provided by coaches, and the system is fully operated by the private sector under public delegation through permits and authorizations. Agency-based regulation was introduced in 2002 along with a price cap regime aimed at enhancing the flexibility to change fares in response to demand and cost conditions. By making use of a reaction function-based model of coach operators' pricing decisions in the interstate travel market, we then estimate the sensitivity of the changes in coach fares to the changes in airline fares in a simultaneous-equation framework. Intermodal interaction among coach operators and airlines is found to be highly significant and probably due to the competition for a small but increasing set of premium, quality-sensitive, coach passengers.","sentences":["This paper presents a test of intermodal interaction between coaches and airlines in Brazil in order to check for the efficacy of recent liberalization measures designed to promote competition in both industries.","Interstate travel service in the country is heavily provided by coaches, and the system is fully operated by the private sector under public delegation through permits and authorizations.","Agency-based regulation was introduced in 2002 along with a price cap regime aimed at enhancing the flexibility to change fares in response to demand and cost conditions.","By making use of a reaction function-based model of coach operators' pricing decisions in the interstate travel market, we then estimate the sensitivity of the changes in coach fares to the changes in airline fares in a simultaneous-equation framework.","Intermodal interaction among coach operators and airlines is found to be highly significant and probably due to the competition for a small but increasing set of premium, quality-sensitive, coach passengers."],"url":"http://arxiv.org/abs/2402.07125v1","category":"econ.GN"}
{"created":"2024-02-12 17:22:42","title":"Injecting Wiktionary to improve token-level contextual representations using contrastive learning","abstract":"While static word embeddings are blind to context, for lexical semantics tasks context is rather too present in contextual word embeddings, vectors of same-meaning occurrences being too different (Ethayarajh, 2019). Fine-tuning pre-trained language models (PLMs) using contrastive learning was proposed, leveraging automatically self-augmented examples (Liu et al., 2021b). In this paper, we investigate how to inject a lexicon as an alternative source of supervision, using the English Wiktionary. We also test how dimensionality reduction impacts the resulting contextual word embeddings. We evaluate our approach on the Word-In-Context (WiC) task, in the unsupervised setting (not using the training set). We achieve new SoTA result on the original WiC test set. We also propose two new WiC test sets for which we show that our fine-tuning method achieves substantial improvements. We also observe improvements, although modest, for the semantic frame induction task. Although we experimented on English to allow comparison with related work, our method is adaptable to the many languages for which large Wiktionaries exist.","sentences":["While static word embeddings are blind to context, for lexical semantics tasks context is rather too present in contextual word embeddings, vectors of same-meaning occurrences being too different (Ethayarajh, 2019).","Fine-tuning pre-trained language models (PLMs) using contrastive learning was proposed, leveraging automatically self-augmented examples (Liu et al., 2021b).","In this paper, we investigate how to inject a lexicon as an alternative source of supervision, using the English Wiktionary.","We also test how dimensionality reduction impacts the resulting contextual word embeddings.","We evaluate our approach on the Word-In-Context (WiC) task, in the unsupervised setting (not using the training set).","We achieve new SoTA result on the original WiC test set.","We also propose two new WiC test sets for which we show that our fine-tuning method achieves substantial improvements.","We also observe improvements, although modest, for the semantic frame induction task.","Although we experimented on English to allow comparison with related work, our method is adaptable to the many languages for which large Wiktionaries exist."],"url":"http://arxiv.org/abs/2402.07817v1","category":"cs.CL"}
{"created":"2024-02-12 15:52:27","title":"Universal link predictor by In-context Learning","abstract":"Link prediction is a crucial task in graph machine learning, where the goal is to infer missing or future links within a graph. Traditional approaches leverage heuristic methods based on widely observed connectivity patterns, offering broad applicability and generalizability without the need for model training. Despite their utility, these methods are limited by their reliance on human-derived heuristics and lack the adaptability of data-driven approaches. Conversely, parametric link predictors excel in automatically learning the connectivity patterns from data and achieving state-of-the-art but fail short to directly transfer across different graphs. Instead, it requires the cost of extensive training and hyperparameter optimization to adapt to the target graph. In this work, we introduce the Universal Link Predictor (UniLP), a novel model that combines the generalizability of heuristic approaches with the pattern learning capabilities of parametric models. UniLP is designed to autonomously identify connectivity patterns across diverse graphs, ready for immediate application to any unseen graph dataset without targeted training. We address the challenge of conflicting connectivity patterns-arising from the unique distributions of different graphs-through the implementation of In-context Learning (ICL). This approach allows UniLP to dynamically adjust to various target graphs based on contextual demonstrations, thereby avoiding negative transfer. Through rigorous experimentation, we demonstrate UniLP's effectiveness in adapting to new, unseen graphs at test time, showcasing its ability to perform comparably or even outperform parametric models that have been finetuned for specific datasets. Our findings highlight UniLP's potential to set a new standard in link prediction, combining the strengths of heuristic and parametric methods in a single, versatile framework.","sentences":["Link prediction is a crucial task in graph machine learning, where the goal is to infer missing or future links within a graph.","Traditional approaches leverage heuristic methods based on widely observed connectivity patterns, offering broad applicability and generalizability without the need for model training.","Despite their utility, these methods are limited by their reliance on human-derived heuristics and lack the adaptability of data-driven approaches.","Conversely, parametric link predictors excel in automatically learning the connectivity patterns from data and achieving state-of-the-art but fail short to directly transfer across different graphs.","Instead, it requires the cost of extensive training and hyperparameter optimization to adapt to the target graph.","In this work, we introduce the Universal Link Predictor (UniLP), a novel model that combines the generalizability of heuristic approaches with the pattern learning capabilities of parametric models.","UniLP is designed to autonomously identify connectivity patterns across diverse graphs, ready for immediate application to any unseen graph dataset without targeted training.","We address the challenge of conflicting connectivity patterns-arising from the unique distributions of different graphs-through the implementation of In-context Learning (ICL).","This approach allows UniLP to dynamically adjust to various target graphs based on contextual demonstrations, thereby avoiding negative transfer.","Through rigorous experimentation, we demonstrate UniLP's effectiveness in adapting to new, unseen graphs at test time, showcasing its ability to perform comparably or even outperform parametric models that have been finetuned for specific datasets.","Our findings highlight UniLP's potential to set a new standard in link prediction, combining the strengths of heuristic and parametric methods in a single, versatile framework."],"url":"http://arxiv.org/abs/2402.07738v1","category":"cs.LG"}
{"created":"2024-02-12 13:50:07","title":"Well-posedness for the NLS hierarchy","abstract":"We prove well-posedness for higher-order equations in the so-called NLS hierarchy (also known as part of the AKNS hierarchy) in almost critical Fourier-Lebesgue spaces and in modulation spaces. We show the $j$th equation in the hierarchy is locally well-posed for initial data in $\\hat H^s_r(\\mathbb{R})$ for $s \\ge \\frac{j-1}{r'}$ and $1 < r \\le 2$ and also in $M^s_{2, p}(\\mathbb{R})$ for $s = \\frac{j-1}{2}$ and $2 \\le p < \\infty$. Supplementing our results with corresponding ill-posedness results in Fourier-Lebesgue spaces shows optimality. Using the conserved quantities derived in Koch-Tataru (2018) we argue that the hierarchy equations are globally well-posed for data in $H^s(\\mathbb{R})$ for $s \\ge \\frac{j-1}{2}$.   Our arguments are based on the Fourier restriction norm method in Bourgain spaces adapted to our data spaces and bi- & trilinear refinements of Strichartz estimates.","sentences":["We prove well-posedness for higher-order equations in the so-called NLS hierarchy (also known as part of the AKNS hierarchy) in almost critical Fourier-Lebesgue spaces and in modulation spaces.","We show the $j$th equation in the hierarchy is locally well-posed for initial data in $\\hat H^s_r(\\mathbb{R})$ for $s \\ge \\frac{j-1}{r'}$ and $1 < r \\le 2$ and also in $M^s_{2, p}(\\mathbb{R})$ for $s = \\frac{j-1}{2}$ and $2 \\le p < \\infty$. Supplementing our results with corresponding ill-posedness results in Fourier-Lebesgue spaces shows optimality.","Using the conserved quantities derived in Koch-Tataru (2018)","we argue that the hierarchy equations are globally well-posed for data in $H^s(\\mathbb{R})$ for $s \\ge \\frac{j-1}{2}$.   ","Our arguments are based on the Fourier restriction norm method in Bourgain spaces adapted to our data spaces and bi- & trilinear refinements of Strichartz estimates."],"url":"http://arxiv.org/abs/2402.07652v1","category":"math.AP"}
{"created":"2024-02-12 11:58:41","title":"Interactive singing melody extraction based on active adaptation","abstract":"Extraction of predominant pitch from polyphonic audio is one of the fundamental tasks in the field of music information retrieval and computational musicology. To accomplish this task using machine learning, a large amount of labeled audio data is required to train the model. However, a classical model pre-trained on data from one domain (source), e.g., songs of a particular singer or genre, may not perform comparatively well in extracting melody from other domains (target). The performance of such models can be boosted by adapting the model using very little annotated data from the target domain. In this work, we propose an efficient interactive melody adaptation method. Our method selects the regions in the target audio that require human annotation using a confidence criterion based on normalized true class probability. The annotations are used by the model to adapt itself to the target domain using meta-learning. Our method also provides a novel meta-learning approach that handles class imbalance, i.e., a few representative samples from a few classes are available for adaptation in the target domain. Experimental results show that the proposed method outperforms other adaptive melody extraction baselines. The proposed method is model-agnostic and hence can be applied to other non-adaptive melody extraction models to boost their performance. Also, we released a Hindustani Alankaar and Raga (HAR) dataset containing 523 audio files of about 6.86 hours of duration intended for singing melody extraction tasks.","sentences":["Extraction of predominant pitch from polyphonic audio is one of the fundamental tasks in the field of music information retrieval and computational musicology.","To accomplish this task using machine learning, a large amount of labeled audio data is required to train the model.","However, a classical model pre-trained on data from one domain (source), e.g., songs of a particular singer or genre, may not perform comparatively well in extracting melody from other domains (target).","The performance of such models can be boosted by adapting the model using very little annotated data from the target domain.","In this work, we propose an efficient interactive melody adaptation method.","Our method selects the regions in the target audio that require human annotation using a confidence criterion based on normalized true class probability.","The annotations are used by the model to adapt itself to the target domain using meta-learning.","Our method also provides a novel meta-learning approach that handles class imbalance, i.e., a few representative samples from a few classes are available for adaptation in the target domain.","Experimental results show that the proposed method outperforms other adaptive melody extraction baselines.","The proposed method is model-agnostic and hence can be applied to other non-adaptive melody extraction models to boost their performance.","Also, we released a Hindustani Alankaar and Raga (HAR) dataset containing 523 audio files of about 6.86 hours of duration intended for singing melody extraction tasks."],"url":"http://arxiv.org/abs/2402.07599v1","category":"eess.AS"}
{"created":"2024-02-12 09:31:43","title":"Global subelliptic estimates for geometric Kramers-Fokker-Planck operators on closed manifolds","abstract":"In this article we reconsider the proof of subelliptic estimates for Geometric Kramers-Fokker-Planck operators, a class which includes Bismut's hypoelliptic Laplacian, when the base manifold is closed (no boundary). The method is significantly different from the ones proposed by Bismut-Lebeau in [BiLe] and Lebeau in [Leb1] and [Leb2]. As a new result we are able to prove maximal subelliptic estimates with some control of the constants in the two asymptotic regimes of high (b $\\rightarrow$ 0) and low (b $\\rightarrow$ +$\\infty$) friction. After a dyadic partition in the momentum variable, the analysis is essentially local in the position variable, contrary to the microlocal reduction techniques of the previous works. In particular this method will be easier to adapt on manifolds with boundaries. A byproduct of our analysis is the introduction of a very convenient double exponent Sobolev scale associated with globally defined differential operators. Applications of this convenient parameter dependent functional analysis to accurate spectral problems, in particular for Bismut's hypoelliptic Laplacian with all its specific geometry, is deferred to subsequent works.","sentences":["In this article we reconsider the proof of subelliptic estimates for Geometric Kramers-Fokker-Planck operators, a class which includes Bismut's hypoelliptic Laplacian, when the base manifold is closed (no boundary).","The method is significantly different from the ones proposed by Bismut-Lebeau in [BiLe] and Lebeau in [Leb1] and [Leb2].","As a new result we are able to prove maximal subelliptic estimates with some control of the constants in the two asymptotic regimes of high (b $\\rightarrow$ 0) and low (b $\\rightarrow$ +$\\infty$) friction.","After a dyadic partition in the momentum variable, the analysis is essentially local in the position variable, contrary to the microlocal reduction techniques of the previous works.","In particular this method will be easier to adapt on manifolds with boundaries.","A byproduct of our analysis is the introduction of a very convenient double exponent Sobolev scale associated with globally defined differential operators.","Applications of this convenient parameter dependent functional analysis to accurate spectral problems, in particular for Bismut's hypoelliptic Laplacian with all its specific geometry, is deferred to subsequent works."],"url":"http://arxiv.org/abs/2402.07511v1","category":"math.AP"}
{"created":"2024-02-12 08:47:53","title":"An elementary approach to mixing and dissipation enhancement by transport noise","abstract":"We investigate the mixing properties of solutions to the stochastic transport equation $d u= \\circ d W \\cdot\\nabla u$, where the driving noise $W(t,x)$ is white in time, colored and divergence-free in space. Furthermore, we prove the dissipation enhancement in the presence of a small viscous term. Applying our results, we also derive the mixing properties for a regularized stochastic 2D Euler equation.","sentences":["We investigate the mixing properties of solutions to the stochastic transport equation $d u= \\circ d","W \\cdot\\nabla u$, where the driving noise $W(t,x)$ is white in time, colored and divergence-free in space.","Furthermore, we prove the dissipation enhancement in the presence of a small viscous term.","Applying our results, we also derive the mixing properties for a regularized stochastic 2D Euler equation."],"url":"http://arxiv.org/abs/2402.07484v1","category":"math.PR"}
{"created":"2024-02-12 06:02:24","title":"Debiasing Recommendation with Personal Popularity","abstract":"Global popularity (GP) bias is the phenomenon that popular items are recommended much more frequently than they should be, which goes against the goal of providing personalized recommendations and harms user experience and recommendation accuracy. Many methods have been proposed to reduce GP bias but they fail to notice the fundamental problem of GP, i.e., it considers popularity from a \\textit{global} perspective of \\textit{all users} and uses a single set of popular items, and thus cannot capture the interests of individual users. As such, we propose a user-aware version of item popularity named \\textit{personal popularity} (PP), which identifies different popular items for each user by considering the users that share similar interests. As PP models the preferences of individual users, it naturally helps to produce personalized recommendations and mitigate GP bias. To integrate PP into recommendation, we design a general \\textit{personal popularity aware counterfactual} (PPAC) framework, which adapts easily to existing recommendation models. In particular, PPAC recognizes that PP and GP have both direct and indirect effects on recommendations and controls direct effects with counterfactual inference techniques for unbiased recommendations. All codes and datasets are available at \\url{https://github.com/Stevenn9981/PPAC}.","sentences":["Global popularity (GP) bias is the phenomenon that popular items are recommended much more frequently than they should be, which goes against the goal of providing personalized recommendations and harms user experience and recommendation accuracy.","Many methods have been proposed to reduce GP bias but they fail to notice the fundamental problem of GP, i.e., it considers popularity from a \\textit{global} perspective of \\textit{all users} and uses a single set of popular items, and thus cannot capture the interests of individual users.","As such, we propose a user-aware version of item popularity named \\textit{personal popularity} (PP), which identifies different popular items for each user by considering the users that share similar interests.","As PP models the preferences of individual users, it naturally helps to produce personalized recommendations and mitigate GP bias.","To integrate PP into recommendation, we design a general \\textit{personal popularity aware counterfactual} (PPAC) framework, which adapts easily to existing recommendation models.","In particular, PPAC recognizes that PP and GP have both direct and indirect effects on recommendations and controls direct effects with counterfactual inference techniques for unbiased recommendations.","All codes and datasets are available at \\url{https://github.com/Stevenn9981/PPAC}."],"url":"http://arxiv.org/abs/2402.07425v1","category":"cs.IR"}
{"created":"2024-02-11 23:39:42","title":"Deep Learning for Medical Image Segmentation with Imprecise Annotation","abstract":"Medical image segmentation (MIS) plays an instrumental role in medical image analysis, where considerable efforts have been devoted to automating the process. Currently, mainstream MIS approaches are based on deep neural networks (DNNs) which are typically trained on a dataset that contains annotation masks produced by doctors. However, in the medical domain, the annotation masks generated by different doctors can inherently vary because a doctor may unnecessarily produce precise and unique annotations to meet the goal of diagnosis. Therefore, the DNN model trained on the data annotated by certain doctors, often just a single doctor, could undesirably favour those doctors who annotate the training data, leading to the unsatisfaction of a new doctor who will use the trained model. To address this issue, this work investigates the utilization of multi-expert annotation to enhance the adaptability of the model to a new doctor and we conduct a pilot study on the MRI brain segmentation task. Experimental results demonstrate that the model trained on a dataset with multi-expert annotation can efficiently cater for a new doctor, after lightweight fine-tuning on just a few annotations from the new doctor.","sentences":["Medical image segmentation (MIS) plays an instrumental role in medical image analysis, where considerable efforts have been devoted to automating the process.","Currently, mainstream MIS approaches are based on deep neural networks (DNNs) which are typically trained on a dataset that contains annotation masks produced by doctors.","However, in the medical domain, the annotation masks generated by different doctors can inherently vary because a doctor may unnecessarily produce precise and unique annotations to meet the goal of diagnosis.","Therefore, the DNN model trained on the data annotated by certain doctors, often just a single doctor, could undesirably favour those doctors who annotate the training data, leading to the unsatisfaction of a new doctor who will use the trained model.","To address this issue, this work investigates the utilization of multi-expert annotation to enhance the adaptability of the model to a new doctor and we conduct a pilot study on the MRI brain segmentation task.","Experimental results demonstrate that the model trained on a dataset with multi-expert annotation can efficiently cater for a new doctor, after lightweight fine-tuning on just a few annotations from the new doctor."],"url":"http://arxiv.org/abs/2402.07330v1","category":"cs.CV"}
{"created":"2024-02-11 18:32:54","title":"Adaptive Finite Element Methods","abstract":"This is a survey on the theory of adaptive finite element methods (AFEMs), which are fundamental in modern computational science and engineering but whose mathematical assessment is a formidable challenge. We present a self-contained and up-to-date discussion of AFEMs for linear second order elliptic PDEs and dimension d>1, with emphasis on foundational issues. After a brief review of functional analysis and basic finite element theory, including piecewise polynomial approximation in graded meshes, we present the core material for coercive problems. We start with a novel a posteriori error analysis applicable to rough data, which delivers estimators fully equivalent to the solution error. They are used in the design and study of three AFEMs depending on the structure of data. We prove linear convergence of these algorithms and rate-optimality provided the solution and data belong to suitable approximation classes. We also address the relation between approximation and regularity classes. We finally extend this theory to discontinuous Galerkin methods as prototypes of non-conforming AFEMs and beyond coercive problems to inf-sup stable AFEMs.","sentences":["This is a survey on the theory of adaptive finite element methods (AFEMs), which are fundamental in modern computational science and engineering but whose mathematical assessment is a formidable challenge.","We present a self-contained and up-to-date discussion of AFEMs for linear second order elliptic PDEs and dimension d>1, with emphasis on foundational issues.","After a brief review of functional analysis and basic finite element theory, including piecewise polynomial approximation in graded meshes, we present the core material for coercive problems.","We start with a novel a posteriori error analysis applicable to rough data, which delivers estimators fully equivalent to the solution error.","They are used in the design and study of three AFEMs depending on the structure of data.","We prove linear convergence of these algorithms and rate-optimality provided the solution and data belong to suitable approximation classes.","We also address the relation between approximation and regularity classes.","We finally extend this theory to discontinuous Galerkin methods as prototypes of non-conforming AFEMs and beyond coercive problems to inf-sup stable AFEMs."],"url":"http://arxiv.org/abs/2402.07273v1","category":"math.NA"}
{"created":"2024-02-11 18:05:04","title":"Optimization of laser pumping for the generation of multi-pulse structures in fiber lasers","abstract":"We address the challenge of configuring a fiber laser cavity to enable efficient access to multi-pulse structures such as dissipative soliton molecules. We theoretically compare multi-pulsing routes in the parameter space of the laser. By using a two-dimensional parameter space, we experimentally demonstrate an important reduction in the laser pumping power required to form soliton molecules.","sentences":["We address the challenge of configuring a fiber laser cavity to enable efficient access to multi-pulse structures such as dissipative soliton molecules.","We theoretically compare multi-pulsing routes in the parameter space of the laser.","By using a two-dimensional parameter space, we experimentally demonstrate an important reduction in the laser pumping power required to form soliton molecules."],"url":"http://arxiv.org/abs/2402.07260v1","category":"physics.optics"}
{"created":"2024-02-11 17:40:49","title":"Eco-evolutionary dynamics of adapting pathogens and host immunity","abstract":"As pathogens spread in a population of hosts, immunity is built up and the pool of susceptible individuals is depleted. This generates selective pressure, to which many human RNA viruses, such as influenza virus or SARS-CoV-2, respond with rapid antigenic evolution and frequent emergence of immune evasive variants. However, the host's immune systems adapt and older immune responses wane, such that escape variants only enjoy a growth advantage for a limited time. If variant growth dynamics and reshaping of host-immunity operate on comparable time scales, viral adaptation is determined by eco-evolutionary interactions that are not captured by models of rapid evolution in a fixed environment. Here, we use a Susceptible/Infected model to describe the interaction between an evolving viral population in a dynamic but immunologically diverse host population. We show that depending on strain cross-immunity, heterogeneity of the host population, and durability of immune responses, escape variants initially grow exponentially, but lose their growth advantage before reaching high frequencies. Their subsequent dynamics follows an anomalous random walk determined by future escape variants and results in variant trajectories that are unpredictable. This model can explain the apparent contradiction between the clearly adaptive nature of antigenic evolution and the quasi-neutral dynamics of high frequency variants observed for influenza viruses.","sentences":["As pathogens spread in a population of hosts, immunity is built up and the pool of susceptible individuals is depleted.","This generates selective pressure, to which many human RNA viruses, such as influenza virus or SARS-CoV-2, respond with rapid antigenic evolution and frequent emergence of immune evasive variants.","However, the host's immune systems adapt and older immune responses wane, such that escape variants only enjoy a growth advantage for a limited time.","If variant growth dynamics and reshaping of host-immunity operate on comparable time scales, viral adaptation is determined by eco-evolutionary interactions that are not captured by models of rapid evolution in a fixed environment.","Here, we use a Susceptible/Infected model to describe the interaction between an evolving viral population in a dynamic but immunologically diverse host population.","We show that depending on strain cross-immunity, heterogeneity of the host population, and durability of immune responses, escape variants initially grow exponentially, but lose their growth advantage before reaching high frequencies.","Their subsequent dynamics follows an anomalous random walk determined by future escape variants and results in variant trajectories that are unpredictable.","This model can explain the apparent contradiction between the clearly adaptive nature of antigenic evolution and the quasi-neutral dynamics of high frequency variants observed for influenza viruses."],"url":"http://arxiv.org/abs/2402.07252v1","category":"q-bio.PE"}
{"created":"2024-02-11 15:49:50","title":"GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains","abstract":"Trajectories are sequences of timestamped location samples. In sparse trajectories, the locations are sampled infrequently; and while such trajectories are prevalent in real-world settings, they are challenging to use to enable high-quality transportation-related applications. Current methodologies either assume densely sampled and accurately map-matched trajectories, or they rely on two-stage schemes, yielding sub-optimal applications.   To extend the utility of sparse trajectories, we propose a novel sparse trajectory learning framework, GenSTL. The framework is pre-trained to form connections between sparse trajectories and dense counterparts using auto-regressive generation of feature domains. GenSTL can subsequently be applied directly in downstream tasks, or it can be fine-tuned first. This way, GenSTL eliminates the reliance on the availability of large-scale dense and map-matched trajectory data. The inclusion of a well-crafted feature domain encoding layer and a hierarchical masked trajectory encoder enhances GenSTL's learning capabilities and adaptability. Experiments on two real-world trajectory datasets offer insight into the framework's ability to contend with sparse trajectories with different sampling intervals and its versatility across different downstream tasks, thus offering evidence of its practicality in real-world applications.","sentences":["Trajectories are sequences of timestamped location samples.","In sparse trajectories, the locations are sampled infrequently; and while such trajectories are prevalent in real-world settings, they are challenging to use to enable high-quality transportation-related applications.","Current methodologies either assume densely sampled and accurately map-matched trajectories, or they rely on two-stage schemes, yielding sub-optimal applications.   ","To extend the utility of sparse trajectories, we propose a novel sparse trajectory learning framework, GenSTL.","The framework is pre-trained to form connections between sparse trajectories and dense counterparts using auto-regressive generation of feature domains.","GenSTL can subsequently be applied directly in downstream tasks, or it can be fine-tuned first.","This way, GenSTL eliminates the reliance on the availability of large-scale dense and map-matched trajectory data.","The inclusion of a well-crafted feature domain encoding layer and a hierarchical masked trajectory encoder enhances GenSTL's learning capabilities and adaptability.","Experiments on two real-world trajectory datasets offer insight into the framework's ability to contend with sparse trajectories with different sampling intervals and its versatility across different downstream tasks, thus offering evidence of its practicality in real-world applications."],"url":"http://arxiv.org/abs/2402.07232v1","category":"cs.LG"}
{"created":"2024-02-11 13:40:08","title":"GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting","abstract":"We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation. We first utilize large language models (LLMs) to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints. We then propose an object-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the LLMs to align with the generated scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene. Source codes and models will be available at https://gala3d.github.io/.","sentences":["We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation.","We first utilize large language models (LLMs) to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints.","We then propose an object-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the LLMs to align with the generated scene.","Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene.","Source codes and models will be available at https://gala3d.github.io/."],"url":"http://arxiv.org/abs/2402.07207v1","category":"cs.CV"}
{"created":"2024-02-11 13:29:51","title":"On the Inhibition of Rayleigh Taylor Instability by Capillarity in the Navier Stokes Korteweg Model","abstract":"Bresch--Desjardins--Gisclon--Sart had derived that the capillarity slows down the growth rate of Rayleigh--Taylor (RT) instability in an inhomogeneous incompressible fluid endowed with internal capillarity based on a linearized incompressible Navier--Stokes--Korteweg (NSK) equations in 2008. Later Li--Zhang further obtained another result that the capillarity inhibits RT instability also based on the linearized equations in (SIAM J. Math. Anal. 3287--3315, 2023), if the capillarity coefficient is bigger than some threshold. In this paper, we further rigorously prove such phenomenon of capillarity inhibiting the RT instability in the \\emph{nonlinear} incompressible NSK equations in a horizontally periodic slab domain with Navier (slip) boundary conditions. The key idea in the proof is to capture the dissipative estimates of the tangential derivatives of density. Such dissipative estimates result in the decay-in-time of both the velocity and the perturbation density which is very useful to overcome the difficulties arising from the nonlinear terms.","sentences":["Bresch--Desjardins--Gisclon--Sart had derived that the capillarity slows down the growth rate of Rayleigh--Taylor (RT) instability in an inhomogeneous incompressible fluid endowed with internal capillarity based on a linearized incompressible Navier--Stokes--Korteweg (NSK) equations in 2008.","Later Li--Zhang further obtained another result that the capillarity inhibits RT instability also based on the linearized equations in (SIAM J. Math.","Anal. 3287--3315, 2023), if the capillarity coefficient is bigger than some threshold.","In this paper, we further rigorously prove such phenomenon of capillarity inhibiting the RT instability in the \\emph{nonlinear} incompressible NSK equations in a horizontally periodic slab domain with Navier (slip) boundary conditions.","The key idea in the proof is to capture the dissipative estimates of the tangential derivatives of density.","Such dissipative estimates result in the decay-in-time of both the velocity and the perturbation density which is very useful to overcome the difficulties arising from the nonlinear terms."],"url":"http://arxiv.org/abs/2402.07201v1","category":"math.AP"}
{"created":"2024-02-11 12:24:27","title":"A finite geometry, inertia assisted coarsening-to-complexity transition in homogeneous frictional systems","abstract":"The emergence of statistical complexity in frictional systems (where nonlinearity and dissipation are confined to an interface), manifested in broad distributions of various observables, is not yet understood. We study this problem in velocity-driven, homogeneous (no quenched disorder) unstable frictional systems of height $H$. For large $H$, such frictional systems were recently shown to undergo continuous coarsening until settling into a spatially periodic traveling solution. We show that when the system's height-to-length ratio becomes small -- characteristic of various engineering and geophysical systems -- , coarsening is less effective and the periodic solution is dynamically avoided. Instead, and consistently with previous reports, the system settles into a stochastic, statistically stationary state. The latter features slip bursts, whose slip rate is larger than the driving velocity, which are non-trivially distributed. The slip bursts are classified into two types: predominantly non-propagating, accompanied by small total slip and propagating, accompanied by large total slip. The statistical distributions emerge from dynamically self-generated heterogeneity, where both the non-equilibrium history of the interface and wave reflections from finite boundaries, mediated by material inertia, play central roles. Specifically, the dynamics and statistics of large bursts reveal a timescale $\\sim\\!H/c_{\\rm s}$, where $c_{\\rm s}$ is the shear wave-speed.","sentences":["The emergence of statistical complexity in frictional systems (where nonlinearity and dissipation are confined to an interface), manifested in broad distributions of various observables, is not yet understood.","We study this problem in velocity-driven, homogeneous (no quenched disorder) unstable frictional systems of height $H$. For large $H$, such frictional systems were recently shown to undergo continuous coarsening until settling into a spatially periodic traveling solution.","We show that when the system's height-to-length ratio becomes small -- characteristic of various engineering and geophysical systems -- , coarsening is less effective and the periodic solution is dynamically avoided.","Instead, and consistently with previous reports, the system settles into a stochastic, statistically stationary state.","The latter features slip bursts, whose slip rate is larger than the driving velocity, which are non-trivially distributed.","The slip bursts are classified into two types: predominantly non-propagating, accompanied by small total slip and propagating, accompanied by large total slip.","The statistical distributions emerge from dynamically self-generated heterogeneity, where both the non-equilibrium history of the interface and wave reflections from finite boundaries, mediated by material inertia, play central roles.","Specifically, the dynamics and statistics of large bursts reveal a timescale $\\sim\\!H/c_{\\rm s}$, where $c_{\\rm s}$ is the shear wave-speed."],"url":"http://arxiv.org/abs/2402.07178v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-11 11:11:39","title":"PASOA- PArticle baSed Bayesian Optimal Adaptive design","abstract":"We propose a new procedure named PASOA, for Bayesian experimental design, that performs sequential design optimization by simultaneously providing accurate estimates of successive posterior distributions for parameter inference. The sequential design process is carried out via a contrastive estimation principle, using stochastic optimization and Sequential Monte Carlo (SMC) samplers to maximise the Expected Information Gain (EIG). As larger information gains are obtained for larger distances between successive posterior distributions, this EIG objective may worsen classical SMC performance. To handle this issue, tempering is proposed to have both a large information gain and an accurate SMC sampling, that we show is crucial for performance. This novel combination of stochastic optimization and tempered SMC allows to jointly handle design optimization and parameter inference. We provide a proof that the obtained optimal design estimators benefit from some consistency property. Numerical experiments confirm the potential of the approach, which outperforms other recent existing procedures.","sentences":["We propose a new procedure named PASOA, for Bayesian experimental design, that performs sequential design optimization by simultaneously providing accurate estimates of successive posterior distributions for parameter inference.","The sequential design process is carried out via a contrastive estimation principle, using stochastic optimization and Sequential Monte Carlo (SMC) samplers to maximise the Expected Information Gain (EIG).","As larger information gains are obtained for larger distances between successive posterior distributions, this EIG objective may worsen classical SMC performance.","To handle this issue, tempering is proposed to have both a large information gain and an accurate SMC sampling, that we show is crucial for performance.","This novel combination of stochastic optimization and tempered SMC allows to jointly handle design optimization and parameter inference.","We provide a proof that the obtained optimal design estimators benefit from some consistency property.","Numerical experiments confirm the potential of the approach, which outperforms other recent existing procedures."],"url":"http://arxiv.org/abs/2402.07160v1","category":"stat.ML"}
{"created":"2024-02-11 03:27:22","title":"Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation","abstract":"Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem -- that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug). ConvAug first generates multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby giving the model a larger learning space. A contrastive learning objective is then employed to train a better conversational context encoder. Extensive experiments conducted on four public datasets, under both normal and zero-shot settings, demonstrate the effectiveness, generalizability, and applicability of ConvAug.","sentences":["Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages.","Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem -- that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded.","Consequently, they often struggle to generalize to diverse conversations in real-world scenarios.","In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug).","ConvAug first generates multi-level augmented conversations to capture the diverse nature of conversational contexts.","Inspired by human cognition, we devise a cognition-aware process to mitigate the generation of false positives, false negatives, and hallucinations.","Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby giving the model a larger learning space.","A contrastive learning objective is then employed to train a better conversational context encoder.","Extensive experiments conducted on four public datasets, under both normal and zero-shot settings, demonstrate the effectiveness, generalizability, and applicability of ConvAug."],"url":"http://arxiv.org/abs/2402.07092v1","category":"cs.CL"}
{"created":"2024-02-11 02:52:39","title":"Quantum multiparameter estimation enhanced by a topological phase transition","abstract":"In quantum multiparameter estimation, multiple to-be-estimated parameters are encoded in a quantum dynamics system by a unitary evolution. As the parameters vary, the system may undergo a topological phase transition (TPT). In this paper, we investigate two SU(2) TPT models and propose the singular behavior of the quantum metric tensor around the TPT point as a tool for the simultaneous optimal estimation of multiple parameters. We find that the proposed TPT sensing protocol can achieve the same metrology performance as the quantum-control-enhanced one. Moreover, the probe state of the TPT sensing protocol is only the ground state of the Hamiltonian rather than the entangled state required in the control-enhanced one. In addition, an adaptive multiparameter estimation strategy is developed for updating the estimated values until the desired quantum Cram\\'er-Rao bound is approached. Our work reinforces the connection between quantum multiparameter estimation and topology physics, with potential inspiration for quantum critical metrology.","sentences":["In quantum multiparameter estimation, multiple to-be-estimated parameters are encoded in a quantum dynamics system by a unitary evolution.","As the parameters vary, the system may undergo a topological phase transition (TPT).","In this paper, we investigate two SU(2) TPT models and propose the singular behavior of the quantum metric tensor around the TPT point as a tool for the simultaneous optimal estimation of multiple parameters.","We find that the proposed TPT sensing protocol can achieve the same metrology performance as the quantum-control-enhanced one.","Moreover, the probe state of the TPT sensing protocol is only the ground state of the Hamiltonian rather than the entangled state required in the control-enhanced one.","In addition, an adaptive multiparameter estimation strategy is developed for updating the estimated values until the desired quantum Cram\\'er-Rao bound is approached.","Our work reinforces the connection between quantum multiparameter estimation and topology physics, with potential inspiration for quantum critical metrology."],"url":"http://arxiv.org/abs/2402.07089v1","category":"quant-ph"}
{"created":"2024-02-10 22:10:58","title":"Rate-Quality or Energy-Quality Pareto Fronts for Adaptive Video Streaming?","abstract":"Adaptive video streaming is a key enabler for optimising the delivery of offline encoded video content. The research focus to date has been on optimisation, based solely on rate-quality curves. This paper adds an additional dimension, the energy expenditure, and explores construction of bitrate ladders based on decoding energy-quality curves rather than the conventional rate-quality curves. Pareto fronts are extracted from the rate-quality and energy-quality spaces to select optimal points. Bitrate ladders are constructed from these points using conventional rate-based rules together with a novel quality-based approach. Evaluation on a subset of YouTube-UGC videos encoded with x.265 shows that the energy-quality ladders reduce energy requirements by 28-31% on average at the cost of slightly higher bitrates. The results indicate that optimising based on energy-quality curves rather than rate-quality curves and using quality levels to create the rungs could potentially improve energy efficiency for a comparable quality of experience.","sentences":["Adaptive video streaming is a key enabler for optimising the delivery of offline encoded video content.","The research focus to date has been on optimisation, based solely on rate-quality curves.","This paper adds an additional dimension, the energy expenditure, and explores construction of bitrate ladders based on decoding energy-quality curves rather than the conventional rate-quality curves.","Pareto fronts are extracted from the rate-quality and energy-quality spaces to select optimal points.","Bitrate ladders are constructed from these points using conventional rate-based rules together with a novel quality-based approach.","Evaluation on a subset of YouTube-UGC videos encoded with x.265 shows that the energy-quality ladders reduce energy requirements by 28-31% on average at the cost of slightly higher bitrates.","The results indicate that optimising based on energy-quality curves rather than rate-quality curves and using quality levels to create the rungs could potentially improve energy efficiency for a comparable quality of experience."],"url":"http://arxiv.org/abs/2402.07057v1","category":"eess.IV"}
{"created":"2024-02-10 20:50:56","title":"Optimal convergence rates of an adaptive hybrid FEM-BEM method for full-space linear transmission problems","abstract":"We consider a hybrid FEM-BEM method to compute approximations of full-space linear elliptic transmission problems. First, we derive a priori and a posteriori error estimates. Then, building on the latter, we present an adaptive algorithm and prove that it converges at optimal rates with respect to the number of mesh elements. Finally, we provide numerical experiments, demonstrating the practical performance of the adaptive algorithm.","sentences":["We consider a hybrid FEM-BEM method to compute approximations of full-space linear elliptic transmission problems.","First, we derive a priori and a posteriori error estimates.","Then, building on the latter, we present an adaptive algorithm and prove that it converges at optimal rates with respect to the number of mesh elements.","Finally, we provide numerical experiments, demonstrating the practical performance of the adaptive algorithm."],"url":"http://arxiv.org/abs/2402.07040v1","category":"math.NA"}
{"created":"2024-02-10 18:49:52","title":"Bayesian Optimization with Adaptive Kernels for Robot Control","abstract":"Active policy search combines the trial-and-error methodology from policy search with Bayesian optimization to actively find the optimal policy. First, policy search is a type of reinforcement learning which has become very popular for robot control, for its ability to deal with complex continuous state and action spaces. Second, Bayesian optimization is a sample efficient global optimization method that uses a surrogate model, like a Gaussian process, and optimal decision making to carefully select each sample during the optimization process. Sample efficiency is of paramount importance when each trial involves the real robot, expensive Monte Carlo runs, or a complex simulator. Black-box Bayesian optimization generally assumes a cost function from a stationary process, because nonstationary modeling is usually based on prior knowledge. However, many control problems are inherently nonstationary due to their failure conditions, terminal states and other abrupt effects. In this paper, we present a kernel function specially designed for Bayesian optimization, that allows nonstationary modeling without prior knowledge, using an adaptive local region. The new kernel results in an improved local search (exploitation), without penalizing the global search (exploration), as shown experimentally in well-known optimization benchmarks and robot control scenarios. We finally show its potential for the design of the wing shape of a UAV.","sentences":["Active policy search combines the trial-and-error methodology from policy search with Bayesian optimization to actively find the optimal policy.","First, policy search is a type of reinforcement learning which has become very popular for robot control, for its ability to deal with complex continuous state and action spaces.","Second, Bayesian optimization is a sample efficient global optimization method that uses a surrogate model, like a Gaussian process, and optimal decision making to carefully select each sample during the optimization process.","Sample efficiency is of paramount importance when each trial involves the real robot, expensive Monte Carlo runs, or a complex simulator.","Black-box Bayesian optimization generally assumes a cost function from a stationary process, because nonstationary modeling is usually based on prior knowledge.","However, many control problems are inherently nonstationary due to their failure conditions, terminal states and other abrupt effects.","In this paper, we present a kernel function specially designed for Bayesian optimization, that allows nonstationary modeling without prior knowledge, using an adaptive local region.","The new kernel results in an improved local search (exploitation), without penalizing the global search (exploration), as shown experimentally in well-known optimization benchmarks and robot control scenarios.","We finally show its potential for the design of the wing shape of a UAV."],"url":"http://arxiv.org/abs/2402.07021v1","category":"cs.RO"}
{"created":"2024-02-10 18:36:42","title":"Informativeness of Reward Functions in Reinforcement Learning","abstract":"Reward functions are central in specifying the task we want a reinforcement learning agent to perform. Given a task and desired optimal behavior, we study the problem of designing informative reward functions so that the designed rewards speed up the agent's convergence. In particular, we consider expert-driven reward design settings where an expert or teacher seeks to provide informative and interpretable rewards to a learning agent. Existing works have considered several different reward design formulations; however, the key challenge is formulating a reward informativeness criterion that adapts w.r.t. the agent's current policy and can be optimized under specified structural constraints to obtain interpretable rewards. In this paper, we propose a novel reward informativeness criterion, a quantitative measure that captures how the agent's current policy will improve if it receives rewards from a specific reward function. We theoretically showcase the utility of the proposed informativeness criterion for adaptively designing rewards for an agent. Experimental results on two navigation tasks demonstrate the effectiveness of our adaptive reward informativeness criterion.","sentences":["Reward functions are central in specifying the task we want a reinforcement learning agent to perform.","Given a task and desired optimal behavior, we study the problem of designing informative reward functions so that the designed rewards speed up the agent's convergence.","In particular, we consider expert-driven reward design settings where an expert or teacher seeks to provide informative and interpretable rewards to a learning agent.","Existing works have considered several different reward design formulations; however, the key challenge is formulating a reward informativeness criterion that adapts w.r.t.","the agent's current policy and can be optimized under specified structural constraints to obtain interpretable rewards.","In this paper, we propose a novel reward informativeness criterion, a quantitative measure that captures how the agent's current policy will improve if it receives rewards from a specific reward function.","We theoretically showcase the utility of the proposed informativeness criterion for adaptively designing rewards for an agent.","Experimental results on two navigation tasks demonstrate the effectiveness of our adaptive reward informativeness criterion."],"url":"http://arxiv.org/abs/2402.07019v1","category":"cs.LG"}
{"created":"2024-02-10 13:26:14","title":"Should I try multiple optimizers when fine-tuning pre-trained Transformers for NLP tasks? Should I tune their hyperparameters?","abstract":"NLP research has explored different neural model architectures and sizes, datasets, training objectives, and transfer learning techniques. However, the choice of optimizer during training has not been explored as extensively. Typically, some variant of Stochastic Gradient Descent (SGD) is employed, selected among numerous variants, using unclear criteria, often with minimal or no tuning of the optimizer's hyperparameters. Experimenting with five GLUE datasets, two models (DistilBERT and DistilRoBERTa), and seven popular optimizers (SGD, SGD with Momentum, Adam, AdaMax, Nadam, AdamW, and AdaBound), we find that when the hyperparameters of the optimizers are tuned, there is no substantial difference in test performance across the five more elaborate (adaptive) optimizers, despite differences in training loss. Furthermore, tuning just the learning rate is in most cases as good as tuning all the hyperparameters. Hence, we recommend picking any of the best-behaved adaptive optimizers (e.g., Adam) and tuning only its learning rate. When no hyperparameter can be tuned, SGD with Momentum is the best choice.","sentences":["NLP research has explored different neural model architectures and sizes, datasets, training objectives, and transfer learning techniques.","However, the choice of optimizer during training has not been explored as extensively.","Typically, some variant of Stochastic Gradient Descent (SGD) is employed, selected among numerous variants, using unclear criteria, often with minimal or no tuning of the optimizer's hyperparameters.","Experimenting with five GLUE datasets, two models (DistilBERT and DistilRoBERTa), and seven popular optimizers (SGD, SGD with Momentum, Adam, AdaMax, Nadam, AdamW, and AdaBound), we find that when the hyperparameters of the optimizers are tuned, there is no substantial difference in test performance across the five more elaborate (adaptive) optimizers, despite differences in training loss.","Furthermore, tuning just the learning rate is in most cases as good as tuning all the hyperparameters.","Hence, we recommend picking any of the best-behaved adaptive optimizers (e.g., Adam) and tuning only its learning rate.","When no hyperparameter can be tuned, SGD with Momentum is the best choice."],"url":"http://arxiv.org/abs/2402.06948v1","category":"cs.CL"}
{"created":"2024-02-10 11:53:48","title":"LiFi: Lightweight Controlled Text Generation with Fine-Grained Control Codes","abstract":"In the rapidly evolving field of text generation, the demand for more precise control mechanisms has become increasingly apparent. To address this need, we present a novel methodology, LIFI, which offers a lightweight approach with fine-grained control for controlled text generation. Unlike previous studies that train pre-trained language models to follow discrete, categorical, and exclusive control codes, LIFI learns controlled text generation under the guidance of continuous, relative, and nonexclusive control codes. These fine-grained codes are automatically derived from an attribute classifier, initially trained with a small amount of labeled data and subsequently employed to label abundant unlabeled data, thus garnering more extensive supervision signals. Moreover, to achieve efficient control, we incorporate the fine-grained control codes with adapters, a parameter- and compute-efficient way to steer a pre-trained language model. We evaluate LIFI on two conventional tasks -- sentiment control and topic control -- and one newly proposed task -- stylistic novel writing. Comprehensive experimental results validate the effectiveness of our proposed methods, demonstrating substantial performance improvements over existing baselines.","sentences":["In the rapidly evolving field of text generation, the demand for more precise control mechanisms has become increasingly apparent.","To address this need, we present a novel methodology, LIFI, which offers a lightweight approach with fine-grained control for controlled text generation.","Unlike previous studies that train pre-trained language models to follow discrete, categorical, and exclusive control codes, LIFI learns controlled text generation under the guidance of continuous, relative, and nonexclusive control codes.","These fine-grained codes are automatically derived from an attribute classifier, initially trained with a small amount of labeled data and subsequently employed to label abundant unlabeled data, thus garnering more extensive supervision signals.","Moreover, to achieve efficient control, we incorporate the fine-grained control codes with adapters, a parameter- and compute-efficient way to steer a pre-trained language model.","We evaluate LIFI on two conventional tasks -- sentiment control and topic control -- and one newly proposed task -- stylistic novel writing.","Comprehensive experimental results validate the effectiveness of our proposed methods, demonstrating substantial performance improvements over existing baselines."],"url":"http://arxiv.org/abs/2402.06930v1","category":"cs.CL"}
{"created":"2024-02-10 11:33:53","title":"Data assimilation for the stochastic Camassa-Holm equation using particle filtering: a numerical investigation","abstract":"In this study, we explore data assimilation for the Stochastic Camassa-Holm equation through the application of the particle filtering framework. Specifically, our approach integrates adaptive tempering, jittering, and nudging techniques to construct an advanced particle filtering system. All filtering processes are executed utilizing ensemble parallelism. We conduct extensive numerical experiments across various scenarios of the Stochastic Camassa-Holm model with transport noise and viscosity to examine the impact of different filtering procedures on the performance of the data assimilation process. Our analysis focuses on how observational data and the data assimilation step influence the accuracy and uncertainty of the obtained results.","sentences":["In this study, we explore data assimilation for the Stochastic Camassa-Holm equation through the application of the particle filtering framework.","Specifically, our approach integrates adaptive tempering, jittering, and nudging techniques to construct an advanced particle filtering system.","All filtering processes are executed utilizing ensemble parallelism.","We conduct extensive numerical experiments across various scenarios of the Stochastic Camassa-Holm model with transport noise and viscosity to examine the impact of different filtering procedures on the performance of the data assimilation process.","Our analysis focuses on how observational data and the data assimilation step influence the accuracy and uncertainty of the obtained results."],"url":"http://arxiv.org/abs/2402.06927v1","category":"math.NA"}
{"created":"2024-02-10 08:59:59","title":"Opinion models, data, and politics","abstract":"We investigate the connection between Potts (Curie-Weiss) models and stochastic opinion models in the view of the Boltzmann distribution and stochastic Glauber dynamics. We particularly find that the q-voter model can be considered as a natural extension of the Zealot model which is adapted by Lagrangian parameters. We also discuss weak and strong effects continuum limits for the models. We then fit four models (Curie-Weiss, strong and weak effects limit for the q-voter model, and the reinforcement model) to election data from United States, United Kingdom, France and Germany. We find that particularly the weak effects models are able to fit the data (Kolmogorov-Smirnov test), where the weak effects reinforcement model performs best (AIC). The resulting estimates are interpreted in the view of political sciences, and also the importance of this kind of model-based approaches to election data for the political sciences is discussed.","sentences":["We investigate the connection between Potts (Curie-Weiss) models and stochastic opinion models in the view of the Boltzmann distribution and stochastic Glauber dynamics.","We particularly find that the q-voter model can be considered as a natural extension of the Zealot model which is adapted by Lagrangian parameters.","We also discuss weak and strong effects continuum limits for the models.","We then fit four models (Curie-Weiss, strong and weak effects limit for the q-voter model, and the reinforcement model) to election data from United States, United Kingdom, France and Germany.","We find that particularly the weak effects models are able to fit the data (Kolmogorov-Smirnov test), where the weak effects reinforcement model performs best (AIC).","The resulting estimates are interpreted in the view of political sciences, and also the importance of this kind of model-based approaches to election data for the political sciences is discussed."],"url":"http://arxiv.org/abs/2402.06910v1","category":"physics.soc-ph"}
{"created":"2024-02-10 08:23:13","title":"ROSE: Rotation-based Squeezing Robotic Gripper toward Universal Handling of Objects","abstract":"Robotics hand/grippers nowadays are not limited to manufacturing lines; instead, they are widely utilized in cluttered environments, such as restaurants, farms, and warehouses. In such scenarios, they need to deal with high uncertainty of the grasped objects' shapes, postures, surfaces, and material properties, which requires complex integration of sensing and decision-making process. On the other hand, integrating soft materials into the gripper's design may tolerate the above uncertainties and reduce complexity in control. In this paper, we introduce ROSE, a novel soft gripper that can embrace the object and squeeze it by buckling a funnel-liked thin-walled soft membrane around the object by simple rotation of the base. Thanks to this design, ROSE hand can adapt to a wide range of objects that can fit in the funnel and handle with gentle gripping force. Regardless of this, ROSE can generate a high lift force (up to 33kgf) while significantly reducing the normal pressure on the gripped objects. In our experiment, a 198g ROSE can be integrated into a robot arm with a single actuation and successfully lift various types of objects, even after 400,000 trials. The embracing mechanism helps reduce the dependence of friction between the object and the membrane, as ROSE could pick up a chicken egg submerged inside an olive oil tank. We also report a feasible design for equipping the ROSE hand with tactile sensing while appealing to the scalability of the design to fit a wide range of objects. Video: https://youtu.be/E1wAI09LaoY","sentences":["Robotics hand/grippers nowadays are not limited to manufacturing lines; instead, they are widely utilized in cluttered environments, such as restaurants, farms, and warehouses.","In such scenarios, they need to deal with high uncertainty of the grasped objects' shapes, postures, surfaces, and material properties, which requires complex integration of sensing and decision-making process.","On the other hand, integrating soft materials into the gripper's design may tolerate the above uncertainties and reduce complexity in control.","In this paper, we introduce ROSE, a novel soft gripper that can embrace the object and squeeze it by buckling a funnel-liked thin-walled soft membrane around the object by simple rotation of the base.","Thanks to this design, ROSE hand can adapt to a wide range of objects that can fit in the funnel and handle with gentle gripping force.","Regardless of this, ROSE can generate a high lift force (up to 33kgf) while significantly reducing the normal pressure on the gripped objects.","In our experiment, a 198g ROSE can be integrated into a robot arm with a single actuation and successfully lift various types of objects, even after 400,000 trials.","The embracing mechanism helps reduce the dependence of friction between the object and the membrane, as ROSE could pick up a chicken egg submerged inside an olive oil tank.","We also report a feasible design for equipping the ROSE hand with tactile sensing while appealing to the scalability of the design to fit a wide range of objects.","Video: https://youtu.be/E1wAI09LaoY"],"url":"http://arxiv.org/abs/2402.06906v1","category":"cs.RO"}
{"created":"2024-02-10 08:20:23","title":"A novel coarse space applying to the weighted Schwarz method for Helmholtz equations","abstract":"In this paper we are concerned with restricted additive Schwarz with local impedance transformation conditions for a family of Helmholtz problems in two dimensions. These problems are discretized by the finite element method with conforming nodal finite elements. We design and analyze a new adaptive coarse space for this kind of restricted additive Schwarz method. This coarse space is spanned by some eigenvalue functions of local generalized eigenvalue problems, which are defined by weighted positive semi-definite bilinear forms on subspaces consisting of local discrete Helmholtz-harmonic functions from impedance boundary data. We proved that a two-level hybrid Schwarz preconditioner with the proposed coarse space possesses uniformly convergence independent of the mesh size, the subdomain size and the wave numbers under suitable assumptions. We also introduce an economic coarse space to avoid solving generalized eigenvalue problems. Numerical experiments confirm the theoretical results.","sentences":["In this paper we are concerned with restricted additive Schwarz with local impedance transformation conditions for a family of Helmholtz problems in two dimensions.","These problems are discretized by the finite element method with conforming nodal finite elements.","We design and analyze a new adaptive coarse space for this kind of restricted additive Schwarz method.","This coarse space is spanned by some eigenvalue functions of local generalized eigenvalue problems, which are defined by weighted positive semi-definite bilinear forms on subspaces consisting of local discrete Helmholtz-harmonic functions from impedance boundary data.","We proved that a two-level hybrid Schwarz preconditioner with the proposed coarse space possesses uniformly convergence independent of the mesh size, the subdomain size and the wave numbers under suitable assumptions.","We also introduce an economic coarse space to avoid solving generalized eigenvalue problems.","Numerical experiments confirm the theoretical results."],"url":"http://arxiv.org/abs/2402.06905v1","category":"math.NA"}
{"created":"2024-02-12 18:36:10","title":"Tanaka rigidity of graph Lie algebras","abstract":"We give sufficient conditions on a labeled direct graph to determine whether the Tanaka prolongation of its associated Lie algebra is infinite-dimensional. In the case that all directed edges are labeled differently, the corresponding graph Lie algebra is of infinite type if and only if the graph has a vertex of degree one.","sentences":["We give sufficient conditions on a labeled direct graph to determine whether the Tanaka prolongation of its associated Lie algebra is infinite-dimensional.","In the case that all directed edges are labeled differently, the corresponding graph Lie algebra is of infinite type if and only if the graph has a vertex of degree one."],"url":"http://arxiv.org/abs/2402.07873v1","category":"math.DG"}
{"created":"2024-02-12 18:04:45","title":"The Phase Transition of $4D$ Yang-Mills Charged GB AdS Black Hole with Cloud of Strings","abstract":"In this paper, we present an exact spherically symmetric and Yang-Mills charged AdS black hole solution in the context of $4D$ Einstein-Gauss-Bonnet (EGB) gravity in the presence of a cloud of strings. Thermodynamics of this solution is studied. The critical behavior, the types of phase transitions in canonical ensemble, the Joule-Thomson expansion, the Clapeyron equation and the critical exponents shall be investigated.","sentences":["In this paper, we present an exact spherically symmetric and Yang-Mills charged AdS black hole solution in the context of $4D$ Einstein-Gauss-Bonnet (EGB) gravity in the presence of a cloud of strings.","Thermodynamics of this solution is studied.","The critical behavior, the types of phase transitions in canonical ensemble, the Joule-Thomson expansion, the Clapeyron equation and the critical exponents shall be investigated."],"url":"http://arxiv.org/abs/2402.07856v1","category":"hep-th"}
{"created":"2024-02-12 17:59:20","title":"Comparing skill of historical rainfall data based monsoon rainfall prediction in India with NCEP-NWP forecasts","abstract":"In this draft we consider the problem of forecasting rainfall across India during the four monsoon months, one day as well as three days in advance. We train neural networks using historical daily gridded precipitation data for India obtained from IMD for the time period $1901- 2022$, at a spatial resolution of $1^{\\circ} \\times 1^{\\circ}$. This is compared with the numerical weather prediction (NWP) forecasts obtained from NCEP (National Centre for Environmental Prediction) available for the period 2011-2022. We conduct a detailed country wide analysis and separately analyze some of the most populated cities in India. Our conclusion is that forecasts obtained by applying deep learning to historical rainfall data are more accurate compared to NWP forecasts as well as predictions based on persistence. On average, compared to our predictions, forecasts from NCEP-NWP model have about 34% higher error for a single day prediction, and over 68% higher error for a three day prediction. Similarly, persistence estimates report a 29% higher error in a single day forecast, and over 54% error in a three day forecast. We further observe that data up to 20 days in the past is useful in reducing errors of one and three day forecasts, when a transformer based learning architecture, and to a lesser extent when an LSTM is used. A key conclusion suggested by our preliminary analysis is that NWP forecasts can be substantially improved upon through more and diverse data relevant to monsoon prediction combined with carefully selected neural network architecture.","sentences":["In this draft we consider the problem of forecasting rainfall across India during the four monsoon months, one day as well as three days in advance.","We train neural networks using historical daily gridded precipitation data for India obtained from IMD for the time period $1901- 2022$, at a spatial resolution of $1^{\\circ} \\times 1^{\\circ}$.","This is compared with the numerical weather prediction (NWP) forecasts obtained from NCEP (National Centre for Environmental Prediction) available for the period 2011-2022.","We conduct a detailed country wide analysis and separately analyze some of the most populated cities in India.","Our conclusion is that forecasts obtained by applying deep learning to historical rainfall data are more accurate compared to NWP forecasts as well as predictions based on persistence.","On average, compared to our predictions, forecasts from NCEP-NWP model have about 34% higher error for a single day prediction, and over 68% higher error for a three day prediction.","Similarly, persistence estimates report a 29% higher error in a single day forecast, and over 54% error in a three day forecast.","We further observe that data up to 20 days in the past is useful in reducing errors of one and three day forecasts, when a transformer based learning architecture, and to a lesser extent when an LSTM is used.","A key conclusion suggested by our preliminary analysis is that NWP forecasts can be substantially improved upon through more and diverse data relevant to monsoon prediction combined with carefully selected neural network architecture."],"url":"http://arxiv.org/abs/2402.07851v1","category":"cs.LG"}
{"created":"2024-02-12 17:56:55","title":"Canonical Lifts in Multisymplectic De Donder-Weyl Hamiltonian Field Theories","abstract":"In this paper, we define canonical lifts of vector fields to the multisymplectic multimomentum bundles of De Donder-Weyl Hamiltonian first-order field theories and to the appropriate premultisymplectic embedded constraint submanifolds on which singular field theories are studied. These new canonical lifts are used to study the so-called natural Noether symmetries present in both regular and singular Hamiltonian field theories along with their associated conserved quantities obtained from Noether's theorem. The Klein-Gordon field, Einstein-Cartan gravity in $3+1$ dimensions, and the Polyakov bosonic string are analyzed in depth as applications of these concepts; as a peripheral result obtained in the analysis of the bosonic string, we provide a new geometrical interpretation of the well-known Virasoro constraint.","sentences":["In this paper, we define canonical lifts of vector fields to the multisymplectic multimomentum bundles of De Donder-Weyl Hamiltonian first-order field theories and to the appropriate premultisymplectic embedded constraint submanifolds on which singular field theories are studied.","These new canonical lifts are used to study the so-called natural Noether symmetries present in both regular and singular Hamiltonian field theories along with their associated conserved quantities obtained from Noether's theorem.","The Klein-Gordon field, Einstein-Cartan gravity in $3+1$ dimensions, and the Polyakov bosonic string are analyzed in depth as applications of these concepts; as a peripheral result obtained in the analysis of the bosonic string, we provide a new geometrical interpretation of the well-known Virasoro constraint."],"url":"http://arxiv.org/abs/2402.07847v1","category":"math-ph"}
{"created":"2024-02-12 17:32:06","title":"Schr\u00f6dinger type equations with singular coefficients and lower order terms","abstract":"In this paper we investigate the well-posedness of the Cauchy problem for a Schr\\\"odinger operator with singular lower order terms. We allow distributional coefficients and we approach this problem via the regularising methods at the core of the theory of very weak solutions. We prove that a very weak solution exists and it is unique modulo negligible perturbations. Very weak solutions converge to classical solutions when the equation coefficients are regular enough.","sentences":["In this paper we investigate the well-posedness of the Cauchy problem for a Schr\\\"odinger operator with singular lower order terms.","We allow distributional coefficients and we approach this problem via the regularising methods at the core of the theory of very weak solutions.","We prove that a very weak solution exists and it is unique modulo negligible perturbations.","Very weak solutions converge to classical solutions when the equation coefficients are regular enough."],"url":"http://arxiv.org/abs/2402.07826v1","category":"math.AP"}
{"created":"2024-02-12 17:15:58","title":"Small separators, upper bounds for $l^\\infty$-widths, and systolic geometry","abstract":"We investigate the dependence on the dimension in the inequalities that relate the volume of a closed submanifold $M^n\\subset \\mathbb{R}^N$ with its $l^\\infty$-width $W^{l^\\infty}_{n-1}(M^n)$ defined as the infimum over all continuous maps $\\phi:M^n\\longrightarrow K^{n-1}\\subset\\mathbb{R}^N$ of $sup_{x\\in M^n}\\Vert \\phi(x)-x\\Vert_{l^\\infty}$. We prove that $W^{l^\\infty}_{n-1}(M^n)\\leq const\\ \\sqrt{n}\\ vol(M^n)^{\\frac{1}{n}}$, and if the codimension $N-n$ is equal to $1$, then $W^{l^\\infty}_{n-1}(M^n)\\leq 3\\ vol(M^n)^{\\frac{1}{n}}$.   As a corollary, we prove that if $M^n\\subset \\mathbb{R}^N$ is essential, then there exists a non-contractible closed curve on $M^n$ contained in a cube in $\\mathbb{R}^N$ with side length $const\\ \\sqrt{n}\\ vol^{\\frac{1}{n}}(M^n)$ with sides parallel to the coordinate axes. If the codimension is $1$, then the side length of the cube is $12\\cdot vol^{\\frac{1}{n}}(M^n)$.","sentences":["We investigate the dependence on the dimension in the inequalities that relate the volume of a closed submanifold $M^n\\subset \\mathbb{R}^N$ with its $l^\\infty$-width $W^{l^\\infty}_{n-1}(M^n)$ defined as the infimum over all continuous maps $\\phi:M^n\\longrightarrow K^{n-1}\\subset\\mathbb{R}^N$ of $sup_{x\\in M^n}\\Vert \\phi(x)-x\\Vert_{l^\\infty}$.","We prove that $W^{l^\\infty}_{n-1}(M^n)\\leq const\\ \\sqrt{n}\\ vol(M^n)^{\\frac{1}{n}}$, and if the codimension $N-n$ is equal to $1$, then $W^{l^\\infty}_{n-1}(M^n)\\leq 3\\ vol(M^n)^{\\frac{1}{n}}$.   As a corollary, we prove that if $M^n\\subset \\mathbb{R}^N$ is essential, then there exists a non-contractible closed curve on $M^n$ contained in a cube in $\\mathbb{R}^N$ with side length $const\\ \\sqrt{n}\\ vol^{\\frac{1}{n}}(M^n)$ with sides parallel to the coordinate axes.","If the codimension is $1$, then the side length of the cube is $12\\cdot vol^{\\frac{1}{n}}(M^n)$."],"url":"http://arxiv.org/abs/2402.07810v1","category":"math.DG"}
{"created":"2024-02-12 17:15:19","title":"Quantum walks, the discrete wave equation and Chebyshev polynomials","abstract":"A quantum walk is the quantum analogue of a random walk. While it is relatively well understood how quantum walks can speed up random walk hitting times, it is a long-standing open question to what extent quantum walks can speed up the spreading or mixing rate of random walks on graphs. In this expository paper, inspired by a blog post by Terence Tao, we describe a particular perspective on this question that derives quantum walks from the discrete wave equation on graphs. This yields a description of the quantum walk dynamics as simply applying a Chebyshev polynomial to the random walk transition matrix. This perspective decouples the problem from its quantum origin, and highlights connections to earlier (non-quantum) work and the use of Chebyshev polynomials in random walk theory as in the Varopoulos-Carne bound. We illustrate the approach by proving a weak limit of the quantum walk dynamics on the lattice. This gives a different proof of the quadratically improved spreading behavior of quantum walks on lattices.","sentences":["A quantum walk is the quantum analogue of a random walk.","While it is relatively well understood how quantum walks can speed up random walk hitting times, it is a long-standing open question to what extent quantum walks can speed up the spreading or mixing rate of random walks on graphs.","In this expository paper, inspired by a blog post by Terence Tao, we describe a particular perspective on this question that derives quantum walks from the discrete wave equation on graphs.","This yields a description of the quantum walk dynamics as simply applying a Chebyshev polynomial to the random walk transition matrix.","This perspective decouples the problem from its quantum origin, and highlights connections to earlier (non-quantum) work and the use of Chebyshev polynomials in random walk theory as in the Varopoulos-Carne bound.","We illustrate the approach by proving a weak limit of the quantum walk dynamics on the lattice.","This gives a different proof of the quadratically improved spreading behavior of quantum walks on lattices."],"url":"http://arxiv.org/abs/2402.07809v1","category":"quant-ph"}
{"created":"2024-02-12 17:01:12","title":"Estimation of non-uniform blur using a patch-based regression convolutional neural network (CNN)","abstract":"The non-uniform blur of atmospheric turbulence can be modeled as a superposition of linear motion blur kernels at a patch level. We propose a regression convolutional neural network (CNN) to predict angle and length of a linear motion blur kernel for varying sized patches. We analyze the robustness of the network for different patch sizes and the performance of the network in regions where the characteristics of the blur are transitioning. Alternating patch sizes per epoch in training, we find coefficient of determination scores across a range of patch sizes of $R^2>0.78$ for length and $R^2>0.94$ for angle prediction. We find that blur predictions in regions overlapping two blur characteristics transition between the two characteristics as overlap changes. These results validate the use of such a network for prediction of non-uniform blur characteristics at a patch level.","sentences":["The non-uniform blur of atmospheric turbulence can be modeled as a superposition of linear motion blur kernels at a patch level.","We propose a regression convolutional neural network (CNN) to predict angle and length of a linear motion blur kernel for varying sized patches.","We analyze the robustness of the network for different patch sizes and the performance of the network in regions where the characteristics of the blur are transitioning.","Alternating patch sizes per epoch in training, we find coefficient of determination scores across a range of patch sizes of $R^2>0.78$ for length and $R^2>0.94$ for angle prediction.","We find that blur predictions in regions overlapping two blur characteristics transition between the two characteristics as overlap changes.","These results validate the use of such a network for prediction of non-uniform blur characteristics at a patch level."],"url":"http://arxiv.org/abs/2402.07796v1","category":"eess.IV"}
{"created":"2024-02-12 16:59:39","title":"Finite and infinite order differential properties of the reduced Mittag--Leffler polynomials","abstract":"This paper deals with the Mittag-Leffler polynomials (MLP) by extracting their essence which consists of real polynomials with fine properties. They are orthogonal on the real line instead of the imaginary axes for MLP. Beside recurrence relations and zeros, we will point to the closed form of its Fourier transform. The most important contribution consists of the new differential properties, especially the finite and infinite differential equation.","sentences":["This paper deals with the Mittag-Leffler polynomials (MLP) by extracting their essence which consists of real polynomials with fine properties.","They are orthogonal on the real line instead of the imaginary axes for MLP.","Beside recurrence relations and zeros, we will point to the closed form of its Fourier transform.","The most important contribution consists of the new differential properties, especially the finite and infinite differential equation."],"url":"http://arxiv.org/abs/2402.07795v1","category":"math.CA"}
{"created":"2024-02-12 16:28:57","title":"Multi-level Optimal Control with Neural Surrogate Models","abstract":"Optimal actuator and control design is studied as a multi-level optimisation problem, where the actuator design is evaluated based on the performance of the associated optimal closed loop. The evaluation of the optimal closed loop for a given actuator realisation is a computationally demanding task, for which the use of a neural network surrogate is proposed. The use of neural network surrogates to replace the lower level of the optimisation hierarchy enables the use of fast gradient-based and gradient-free consensus-based optimisation methods to determine the optimal actuator design. The effectiveness of the proposed surrogate models and optimisation methods is assessed in a test related to optimal actuator location for heat control.","sentences":["Optimal actuator and control design is studied as a multi-level optimisation problem, where the actuator design is evaluated based on the performance of the associated optimal closed loop.","The evaluation of the optimal closed loop for a given actuator realisation is a computationally demanding task, for which the use of a neural network surrogate is proposed.","The use of neural network surrogates to replace the lower level of the optimisation hierarchy enables the use of fast gradient-based and gradient-free consensus-based optimisation methods to determine the optimal actuator design.","The effectiveness of the proposed surrogate models and optimisation methods is assessed in a test related to optimal actuator location for heat control."],"url":"http://arxiv.org/abs/2402.07763v1","category":"math.OC"}
{"created":"2024-02-12 16:24:41","title":"Global existence for the Willmore flow with boundary via Simon's Li-Yau inequality","abstract":"It is well-known that the Willmore flow of closed spherical immersions exists globally in time and converges if the initial datum has Willmore energy below $8\\pi$ - exactly the Li-Yau energy threshold below which all closed immersions are embedded. Extending the Li-Yau inequality for closed surfaces via Simon's monotonicity formula also for surfaces with boundary, given Dirichlet boundary conditions, one obtains an energy threshold $C_{\\mathrm{LY}}$ below which surfaces with this boundary are embedded.   With a new argument, using the Li-Yau inequality and tools from geometric measure theory, we show that the Willmore flow with Dirichlet boundary data starting in cylindrical surfaces of revolution exists globally in time if the energy of the initial datum is below $C_{\\mathrm{LY}}$. Moreover, given Dirichlet boundary data, we also obtain the existence of a Willmore minimizer in the class of cylindrical surfaces of revolution if the corresponding infimum lies below $C_{\\mathrm{LY}}$ which improves previous results for the stationary problem.","sentences":["It is well-known that the Willmore flow of closed spherical immersions exists globally in time and converges if the initial datum has Willmore energy below $8\\pi$ - exactly the Li-Yau energy threshold below which all closed immersions are embedded.","Extending the Li-Yau inequality for closed surfaces via Simon's monotonicity formula also for surfaces with boundary, given Dirichlet boundary conditions, one obtains an energy threshold $C_{\\mathrm{LY}}$ below which surfaces with this boundary are embedded.   ","With a new argument, using the Li-Yau inequality and tools from geometric measure theory, we show that the Willmore flow with Dirichlet boundary data starting in cylindrical surfaces of revolution exists globally in time if the energy of the initial datum is below $C_{\\mathrm{LY}}$. Moreover, given Dirichlet boundary data, we also obtain the existence of a Willmore minimizer in the class of cylindrical surfaces of revolution if the corresponding infimum lies below $C_{\\mathrm{LY}}$ which improves previous results for the stationary problem."],"url":"http://arxiv.org/abs/2402.07755v1","category":"math.AP"}
{"created":"2024-02-12 15:49:19","title":"Multimodal Learned Sparse Retrieval for Image Suggestion","abstract":"Learned Sparse Retrieval (LSR) is a group of neural methods designed to encode queries and documents into sparse lexical vectors. These vectors can be efficiently indexed and retrieved using an inverted index. While LSR has shown promise in text retrieval, its potential in multi-modal retrieval remains largely unexplored. Motivated by this, in this work, we explore the application of LSR in the multi-modal domain, i.e., we focus on Multi-Modal Learned Sparse Retrieval (MLSR). We conduct experiments using several MLSR model configurations and evaluate the performance on the image suggestion task. We find that solving the task solely based on the image content is challenging. Enriching the image content with its caption improves the model performance significantly, implying the importance of image captions to provide fine-grained concepts and context information of images. Our approach presents a practical and effective solution for training LSR retrieval models in multi-modal settings.","sentences":["Learned Sparse Retrieval (LSR) is a group of neural methods designed to encode queries and documents into sparse lexical vectors.","These vectors can be efficiently indexed and retrieved using an inverted index.","While LSR has shown promise in text retrieval, its potential in multi-modal retrieval remains largely unexplored.","Motivated by this, in this work, we explore the application of LSR in the multi-modal domain, i.e., we focus on Multi-Modal Learned Sparse Retrieval (MLSR).","We conduct experiments using several MLSR model configurations and evaluate the performance on the image suggestion task.","We find that solving the task solely based on the image content is challenging.","Enriching the image content with its caption improves the model performance significantly, implying the importance of image captions to provide fine-grained concepts and context information of images.","Our approach presents a practical and effective solution for training LSR retrieval models in multi-modal settings."],"url":"http://arxiv.org/abs/2402.07736v1","category":"cs.IR"}
{"created":"2024-02-12 14:46:31","title":"Towards a Foundation Model for Brain Age Prediction using coVariance Neural Networks","abstract":"Brain age is the estimate of biological age derived from neuroimaging datasets using machine learning algorithms. Increasing brain age with respect to chronological age can reflect increased vulnerability to neurodegeneration and cognitive decline. In this paper, we study NeuroVNN, based on coVariance neural networks, as a paradigm for foundation model for the brain age prediction application. NeuroVNN is pre-trained as a regression model on healthy population to predict chronological age using cortical thickness features and fine-tuned to estimate brain age in different neurological contexts. Importantly, NeuroVNN adds anatomical interpretability to brain age and has a `scale-free' characteristic that allows its transference to datasets curated according to any arbitrary brain atlas. Our results demonstrate that NeuroVNN can extract biologically plausible brain age estimates in different populations, as well as transfer successfully to datasets of dimensionalities distinct from that for the dataset used to train NeuroVNN.","sentences":["Brain age is the estimate of biological age derived from neuroimaging datasets using machine learning algorithms.","Increasing brain age with respect to chronological age can reflect increased vulnerability to neurodegeneration and cognitive decline.","In this paper, we study NeuroVNN, based on coVariance neural networks, as a paradigm for foundation model for the brain age prediction application.","NeuroVNN is pre-trained as a regression model on healthy population to predict chronological age using cortical thickness features and fine-tuned to estimate brain age in different neurological contexts.","Importantly, NeuroVNN adds anatomical interpretability to brain age and has a `scale-free' characteristic that allows its transference to datasets curated according to any arbitrary brain atlas.","Our results demonstrate that NeuroVNN can extract biologically plausible brain age estimates in different populations, as well as transfer successfully to datasets of dimensionalities distinct from that for the dataset used to train NeuroVNN."],"url":"http://arxiv.org/abs/2402.07684v1","category":"q-bio.QM"}
{"created":"2024-02-12 14:33:26","title":"The Massless Dirac Equation in Three Dimensions: Dispersive estimates and zero energy obstructions","abstract":"We investigate dispersive estimates for the massless three dimensional Dirac equation with a potential. In particular, we show that the Dirac evolution satisfies a $\\langle t\\rangle^{-1}$ decay rate as an operator from $L^1$ to $L^\\infty$ regardless of the existence of zero energy eigenfunctions. We also show this decay rate may be improved to $\\langle t\\rangle ^{-1-\\gamma}$ for any $0\\leq \\gamma < 1/2$ at the cost of spatial weights. This estimate, along with the $L^2$ conservation law allows one to deduce a family of Strichartz estimates in the case of a threshold eigenvalue. We classify the structure of threshold obstructions as being composed of zero energy eigenfunctions. Finally, we show the Dirac evolution is bounded for all time with minimal requirements on the decay of the potential and smoothness of initial data.","sentences":["We investigate dispersive estimates for the massless three dimensional Dirac equation with a potential.","In particular, we show that the Dirac evolution satisfies a $\\langle t\\rangle^{-1}$ decay rate as an operator from $L^1$ to $L^\\infty$ regardless of the existence of zero energy eigenfunctions.","We also show this decay rate may be improved to $\\langle t\\rangle ^{-1-\\gamma}$ for any $0\\leq \\gamma < 1/2$ at the cost of spatial weights.","This estimate, along with the $L^2$ conservation law allows one to deduce a family of Strichartz estimates in the case of a threshold eigenvalue.","We classify the structure of threshold obstructions as being composed of zero energy eigenfunctions.","Finally, we show the Dirac evolution is bounded for all time with minimal requirements on the decay of the potential and smoothness of initial data."],"url":"http://arxiv.org/abs/2402.07675v1","category":"math.AP"}
{"created":"2024-02-12 14:29:27","title":"A Computational Model of the Electrically or Acoustically Evoked Compound Action Potential in Cochlear Implant Users with Residual Hearing","abstract":"Objective: In cochlear implant (CI) users with residual acoustic hearing, compound action potentials (CAPs) can be evoked by acoustic or electric stimulation and recorded through the electrodes of the CI. We propose a novel computational model to simulate electrically and acoustically evoked CAPs in humans, taking into account the interaction between combined electric-acoustic stimulation that occurs at the level of the auditory nerve. Methods: The model consists of three components: a 3D finite element method model of an implanted cochlea, a phenomenological single-neuron spiking model for electric-acoustic stimulation, and a physiological multi-compartment neuron model to simulate the individual nerve fiber contributions to the CAP. Results: The CAP morphologies predicted for electric pulses and for acoustic clicks, chirps, and tone bursts closely resembled those known from humans. The spread of excitation derived from electrically evoked CAPs by varying the recording electrode along the CI electrode array was consistent with published human data. The predicted CAP amplitude growth functions for both electric and acoustic stimulation largely resembled human data, with deviations in absolute CAP amplitudes for acoustic stimulation. The model reproduced the suppression of electrically evoked CAPs by simultaneously presented acoustic tone bursts for different masker frequencies and probe stimulation electrodes. Conclusion: The proposed model can simulate CAP responses to electric, acoustic, or combined electric-acoustic stimulation. It takes into account the dependence on stimulation and recording sites in the cochlea, as well as the interaction between electric and acoustic stimulation. Significance: The model can be used in the future to investigate objective methods, such as hearing threshold assessment or estimation of neural health through electrically or acoustically evoked CAPs.","sentences":["Objective: In cochlear implant (CI) users with residual acoustic hearing, compound action potentials (CAPs) can be evoked by acoustic or electric stimulation and recorded through the electrodes of the CI.","We propose a novel computational model to simulate electrically and acoustically evoked CAPs in humans, taking into account the interaction between combined electric-acoustic stimulation that occurs at the level of the auditory nerve.","Methods: The model consists of three components: a 3D finite element method model of an implanted cochlea, a phenomenological single-neuron spiking model for electric-acoustic stimulation, and a physiological multi-compartment neuron model to simulate the individual nerve fiber contributions to the CAP.","Results:","The CAP morphologies predicted for electric pulses and for acoustic clicks, chirps, and tone bursts closely resembled those known from humans.","The spread of excitation derived from electrically evoked CAPs by varying the recording electrode along the CI electrode array was consistent with published human data.","The predicted CAP amplitude growth functions for both electric and acoustic stimulation largely resembled human data, with deviations in absolute CAP amplitudes for acoustic stimulation.","The model reproduced the suppression of electrically evoked CAPs by simultaneously presented acoustic tone bursts for different masker frequencies and probe stimulation electrodes.","Conclusion: The proposed model can simulate CAP responses to electric, acoustic, or combined electric-acoustic stimulation.","It takes into account the dependence on stimulation and recording sites in the cochlea, as well as the interaction between electric and acoustic stimulation.","Significance: The model can be used in the future to investigate objective methods, such as hearing threshold assessment or estimation of neural health through electrically or acoustically evoked CAPs."],"url":"http://arxiv.org/abs/2402.07673v1","category":"physics.med-ph"}
{"created":"2024-02-12 14:26:33","title":"Piecewise Polynomial Tensor Network Quantum Feature Encoding","abstract":"This work introduces a novel method for embedding continuous variables into quantum circuits via piecewise polynomial features, utilizing low-rank tensor networks. Our approach, termed Piecewise Polynomial Tensor Network Quantum Feature Encoding (PPTNQFE), aims to broaden the applicability of quantum algorithms by incorporating spatially localized representations suited for numerical applications like solving partial differential equations and function regression. We demonstrate the potential of PPTNQFE through efficient point evaluations of solutions of discretized differential equations and in modeling functions with localized features such as jump discontinuities. Although promising, challenges such as unexplored noise impact and design of trainable circuits remain. This study opens new avenues for enhancing quantum models with novel feature embeddings and leveraging TN representations for a wider array of function types in quantum machine learning.","sentences":["This work introduces a novel method for embedding continuous variables into quantum circuits via piecewise polynomial features, utilizing low-rank tensor networks.","Our approach, termed Piecewise Polynomial Tensor Network Quantum Feature Encoding (PPTNQFE), aims to broaden the applicability of quantum algorithms by incorporating spatially localized representations suited for numerical applications like solving partial differential equations and function regression.","We demonstrate the potential of PPTNQFE through efficient point evaluations of solutions of discretized differential equations and in modeling functions with localized features such as jump discontinuities.","Although promising, challenges such as unexplored noise impact and design of trainable circuits remain.","This study opens new avenues for enhancing quantum models with novel feature embeddings and leveraging TN representations for a wider array of function types in quantum machine learning."],"url":"http://arxiv.org/abs/2402.07671v1","category":"quant-ph"}
{"created":"2024-02-12 14:20:12","title":"Uniqueness of weak solutions for Hamilton-Jacobi equations","abstract":"It is well known that when the nonlinearity is convex, the Hamilton-Jacobi PDE admits a unique semi-convex weak solution, which is the viscosity solution. In this paper, motivated by problems arising from spin glasses, we show that if the Hamilton-Jacobi PDE with strictly convex nonlinearity and regular enough initial condition admits a semi-concave weak solution, then this solution is the viscosity solution.","sentences":["It is well known that when the nonlinearity is convex, the Hamilton-Jacobi PDE admits a unique semi-convex weak solution, which is the viscosity solution.","In this paper, motivated by problems arising from spin glasses, we show that if the Hamilton-Jacobi PDE with strictly convex nonlinearity and regular enough initial condition admits a semi-concave weak solution, then this solution is the viscosity solution."],"url":"http://arxiv.org/abs/2402.07665v1","category":"math.AP"}
{"created":"2024-02-12 13:52:34","title":"Impact of spatial transformations on landscape features of CEC2022 basic benchmark problems","abstract":"When benchmarking optimization heuristics, we need to take care to avoid an algorithm exploiting biases in the construction of the used problems. One way in which this might be done is by providing different versions of each problem but with transformations applied to ensure the algorithms are equipped with mechanisms for successfully tackling a range of problems. In this paper, we investigate several of these problem transformations and show how they influence the low-level landscape features of a set of 5 problems from the CEC2022 benchmark suite. Our results highlight that even relatively small transformations can significantly alter the measured landscape features. This poses a wider question of what properties we want to preserve when creating problem transformations, and how to fairly measure them.","sentences":["When benchmarking optimization heuristics, we need to take care to avoid an algorithm exploiting biases in the construction of the used problems.","One way in which this might be done is by providing different versions of each problem but with transformations applied to ensure the algorithms are equipped with mechanisms for successfully tackling a range of problems.","In this paper, we investigate several of these problem transformations and show how they influence the low-level landscape features of a set of 5 problems from the CEC2022 benchmark suite.","Our results highlight that even relatively small transformations can significantly alter the measured landscape features.","This poses a wider question of what properties we want to preserve when creating problem transformations, and how to fairly measure them."],"url":"http://arxiv.org/abs/2402.07654v1","category":"cs.NE"}
{"created":"2024-02-12 10:34:32","title":"Pearcey integrals, Stokes lines and exact baryonic layers in the low energy limit of QCD","abstract":"The first analytic solutions representing baryonic layers living at finite baryon density within a constant magnetic field in the gauged Skyrme model are constructed. A remarkable feature of these configurations is that, if the Skyrme term is neglected, then these baryonic layers in the constant magnetic background cannot be found analytically and their energies grow very fast with the magnetic field. On the other hand, if the Skyrme term is taken into account, the field equations can be solved analytically and the corresponding solutions have a smooth limit for large magnetic fields. Thus, the Skyrme term discloses the universal character of these configurations living at finite Baryon density in a constant magnetic field. The classical gran-canonical partition function of these configurations can be expressed explicitly in terms of the Pearcey integral. This fact allows us to determine analytically the Stokes lines of the partition function and the corresponding dependence on the baryonic chemical potential as well as on the external magnetic field. In this way, we can determine various critical curves in the ($\\mu_B-B_{ext}$) plane which separates different physical behaviors. These families of inhomogeneous baryonic condensates can be also dressed with chiral conformal excitations of the solutions representing modulations of the layers themselves. Some physical consequences are analyzed.","sentences":["The first analytic solutions representing baryonic layers living at finite baryon density within a constant magnetic field in the gauged Skyrme model are constructed.","A remarkable feature of these configurations is that, if the Skyrme term is neglected, then these baryonic layers in the constant magnetic background cannot be found analytically and their energies grow very fast with the magnetic field.","On the other hand, if the Skyrme term is taken into account, the field equations can be solved analytically and the corresponding solutions have a smooth limit for large magnetic fields.","Thus, the Skyrme term discloses the universal character of these configurations living at finite Baryon density in a constant magnetic field.","The classical gran-canonical partition function of these configurations can be expressed explicitly in terms of the Pearcey integral.","This fact allows us to determine analytically the Stokes lines of the partition function and the corresponding dependence on the baryonic chemical potential as well as on the external magnetic field.","In this way, we can determine various critical curves in the ($\\mu_B-B_{ext}$) plane which separates different physical behaviors.","These families of inhomogeneous baryonic condensates can be also dressed with chiral conformal excitations of the solutions representing modulations of the layers themselves.","Some physical consequences are analyzed."],"url":"http://arxiv.org/abs/2402.07551v1","category":"hep-th"}
{"created":"2024-02-12 10:02:44","title":"Highly singular (frequentially sparse) steady solutions for the 2D Navier-Stokes equations on the torus","abstract":"We construct non-trivial steady solutions in $H^{-1}$ for the 2D Navier-Stokes equations on the torus. In particular, the solutions are not square integrable, so that we have to redefine the notion of solutions.","sentences":["We construct non-trivial steady solutions in $H^{-1}$ for the 2D Navier-Stokes equations on the torus.","In particular, the solutions are not square integrable, so that we have to redefine the notion of solutions."],"url":"http://arxiv.org/abs/2402.07534v1","category":"math.AP"}
{"created":"2024-02-12 09:07:54","title":"Accelerated Smoothing: A Scalable Approach to Randomized Smoothing","abstract":"Randomized smoothing has emerged as a potent certifiable defense against adversarial attacks by employing smoothing noises from specific distributions to ensure the robustness of a smoothed classifier. However, the utilization of Monte Carlo sampling in this process introduces a compute-intensive element, which constrains the practicality of randomized smoothing on a larger scale. To address this limitation, we propose a novel approach that replaces Monte Carlo sampling with the training of a surrogate neural network. Through extensive experimentation in various settings, we demonstrate the efficacy of our approach in approximating the smoothed classifier with remarkable precision. Furthermore, we demonstrate that our approach significantly accelerates the robust radius certification process, providing nearly $600$X improvement in computation time, overcoming the computational bottlenecks associated with traditional randomized smoothing.","sentences":["Randomized smoothing has emerged as a potent certifiable defense against adversarial attacks by employing smoothing noises from specific distributions to ensure the robustness of a smoothed classifier.","However, the utilization of Monte Carlo sampling in this process introduces a compute-intensive element, which constrains the practicality of randomized smoothing on a larger scale.","To address this limitation, we propose a novel approach that replaces Monte Carlo sampling with the training of a surrogate neural network.","Through extensive experimentation in various settings, we demonstrate the efficacy of our approach in approximating the smoothed classifier with remarkable precision.","Furthermore, we demonstrate that our approach significantly accelerates the robust radius certification process, providing nearly $600$X improvement in computation time, overcoming the computational bottlenecks associated with traditional randomized smoothing."],"url":"http://arxiv.org/abs/2402.07498v1","category":"cs.LG"}
{"created":"2024-02-12 09:05:01","title":"Understanding Deep Learning defenses Against Adversarial Examples Through Visualizations for Dynamic Risk Assessment","abstract":"In recent years, Deep Neural Network models have been developed in different fields, where they have brought many advances. However, they have also started to be used in tasks where risk is critical. A misdiagnosis of these models can lead to serious accidents or even death. This concern has led to an interest among researchers to study possible attacks on these models, discovering a long list of vulnerabilities, from which every model should be defended. The adversarial example attack is a widely known attack among researchers, who have developed several defenses to avoid such a threat. However, these defenses are as opaque as a deep neural network model, how they work is still unknown. This is why visualizing how they change the behavior of the target model is interesting in order to understand more precisely how the performance of the defended model is being modified. For this work, some defenses, against adversarial example attack, have been selected in order to visualize the behavior modification of each of them in the defended model. Adversarial training, dimensionality reduction and prediction similarity were the selected defenses, which have been developed using a model composed by convolution neural network layers and dense neural network layers. In each defense, the behavior of the original model has been compared with the behavior of the defended model, representing the target model by a graph in a visualization.","sentences":["In recent years, Deep Neural Network models have been developed in different fields, where they have brought many advances.","However, they have also started to be used in tasks where risk is critical.","A misdiagnosis of these models can lead to serious accidents or even death.","This concern has led to an interest among researchers to study possible attacks on these models, discovering a long list of vulnerabilities, from which every model should be defended.","The adversarial example attack is a widely known attack among researchers, who have developed several defenses to avoid such a threat.","However, these defenses are as opaque as a deep neural network model, how they work is still unknown.","This is why visualizing how they change the behavior of the target model is interesting in order to understand more precisely how the performance of the defended model is being modified.","For this work, some defenses, against adversarial example attack, have been selected in order to visualize the behavior modification of each of them in the defended model.","Adversarial training, dimensionality reduction and prediction similarity were the selected defenses, which have been developed using a model composed by convolution neural network layers and dense neural network layers.","In each defense, the behavior of the original model has been compared with the behavior of the defended model, representing the target model by a graph in a visualization."],"url":"http://arxiv.org/abs/2402.07496v1","category":"cs.LG"}
{"created":"2024-02-12 08:52:35","title":"Score-based Diffusion Models via Stochastic Differential Equations -- a Technical Tutorial","abstract":"This is an expository article on the score-based diffusion models, with a particular focus on the formulation via stochastic differential equations (SDE). After a gentle introduction, we discuss the two pillars in the diffusion modeling -- sampling and score matching, which encompass the SDE/ODE sampling, score matching efficiency, the consistency model, and reinforcement learning. Short proofs are given to illustrate the main idea of the stated results. The article is primarily for introducing the beginners to the field, and practitioners may also find some analysis useful in designing new models or algorithms.","sentences":["This is an expository article on the score-based diffusion models, with a particular focus on the formulation via stochastic differential equations (SDE).","After a gentle introduction, we discuss the two pillars in the diffusion modeling -- sampling and score matching, which encompass the SDE/ODE sampling, score matching efficiency, the consistency model, and reinforcement learning.","Short proofs are given to illustrate the main idea of the stated results.","The article is primarily for introducing the beginners to the field, and practitioners may also find some analysis useful in designing new models or algorithms."],"url":"http://arxiv.org/abs/2402.07487v1","category":"cs.LG"}
{"created":"2024-02-12 08:36:39","title":"Fourth-order modon in a rotating self-gravitating fluid","abstract":"We present a two-dimensional nonlinear equation to govern the dynamics of disturbances in a rotating self-gravitating fluid. The nonlinear term of the equation has the form of a Poisson bracket (Jacobian), and the linear part contains, along with the Laplacian, a biharmonic operator. A solution was found in the form of a dipole vortex (modon). The solution and all its derivatives up to the fourth order are continuous on the separatrix.","sentences":["We present a two-dimensional nonlinear equation to govern the dynamics of disturbances in a rotating self-gravitating fluid.","The nonlinear term of the equation has the form of a Poisson bracket (Jacobian), and the linear part contains, along with the Laplacian, a biharmonic operator.","A solution was found in the form of a dipole vortex (modon).","The solution and all its derivatives up to the fourth order are continuous on the separatrix."],"url":"http://arxiv.org/abs/2402.07479v1","category":"nlin.PS"}
{"created":"2024-02-12 08:28:19","title":"A version of Bakry-\u00c9mery Ricci flow on a finite graph","abstract":"In this paper, we study the Bakry-\\'Emery Ricci flow on finite graphs. Our main result is the local existence and uniqueness of solutions to the Ricci flow. We prove the long-time convergence or finite-time blow up for the Bakry-\\'Emery Ricci flow on finite trees and circles.","sentences":["In this paper, we study the Bakry-\\'Emery Ricci flow on finite graphs.","Our main result is the local existence and uniqueness of solutions to the Ricci flow.","We prove the long-time convergence or finite-time blow up for the Bakry-\\'Emery Ricci flow on finite trees and circles."],"url":"http://arxiv.org/abs/2402.07475v1","category":"math.DG"}
{"created":"2024-02-12 08:16:58","title":"Differentially Private Decentralized Learning with Random Walks","abstract":"The popularity of federated learning comes from the possibility of better scalability and the ability for participants to keep control of their data, improving data security and sovereignty. Unfortunately, sharing model updates also creates a new privacy attack surface. In this work, we characterize the privacy guarantees of decentralized learning with random walk algorithms, where a model is updated by traveling from one node to another along the edges of a communication graph. Using a recent variant of differential privacy tailored to the study of decentralized algorithms, namely Pairwise Network Differential Privacy, we derive closed-form expressions for the privacy loss between each pair of nodes where the impact of the communication topology is captured by graph theoretic quantities. Our results further reveal that random walk algorithms tends to yield better privacy guarantees than gossip algorithms for nodes close from each other. We supplement our theoretical results with empirical evaluation on synthetic and real-world graphs and datasets.","sentences":["The popularity of federated learning comes from the possibility of better scalability and the ability for participants to keep control of their data, improving data security and sovereignty.","Unfortunately, sharing model updates also creates a new privacy attack surface.","In this work, we characterize the privacy guarantees of decentralized learning with random walk algorithms, where a model is updated by traveling from one node to another along the edges of a communication graph.","Using a recent variant of differential privacy tailored to the study of decentralized algorithms, namely Pairwise Network Differential Privacy, we derive closed-form expressions for the privacy loss between each pair of nodes where the impact of the communication topology is captured by graph theoretic quantities.","Our results further reveal that random walk algorithms tends to yield better privacy guarantees than gossip algorithms for nodes close from each other.","We supplement our theoretical results with empirical evaluation on synthetic and real-world graphs and datasets."],"url":"http://arxiv.org/abs/2402.07471v1","category":"cs.LG"}
{"created":"2024-02-12 04:21:30","title":"On beat-driven and spontaneous excitations of zonal flows by drift waves","abstract":"Using the slab plasma as a paradigm model, we have derived analytically equations for the nonlinear generation of zero-frequency zonal flows by electron drift waves including, on the same footing, both the beat-driven and spontaneous excitations. It is found that the beat-driven zonal flow tends to reduce the frequency mismatch between the electron drift waves and, thereby, contributes to a significant O(1) enhancement of the modulational instability drive and lowering its threshold. Implications to tokamaks plasmas as well as drift-wave soliton formation are also discussed.","sentences":["Using the slab plasma as a paradigm model, we have derived analytically equations for the nonlinear generation of zero-frequency zonal flows by electron drift waves including, on the same footing, both the beat-driven and spontaneous excitations.","It is found that the beat-driven zonal flow tends to reduce the frequency mismatch between the electron drift waves and, thereby, contributes to a significant O(1) enhancement of the modulational instability drive and lowering its threshold.","Implications to tokamaks plasmas as well as drift-wave soliton formation are also discussed."],"url":"http://arxiv.org/abs/2402.07399v1","category":"physics.plasm-ph"}
{"created":"2024-02-12 03:30:06","title":"Drift wave soliton formation via forced-driven zonal flow and implication on plasma confinement","abstract":"In this work, gyrokinetic theory of drift waves (DWs) self-regulation via the forced driven zonal flow (ZF) is presented, and finite diamagnetic drift frequency due to plasma nonuniformity is shown to play dominant role in ZF forced generation. The obtained nonlinear DW equation is a nonlinear Schr\\\"odinger equation, in which the linear dispersiveness, linear growth, nonuniformity of diamagnetic drift frequency, and cubic nonlinearity induced by feedback of forced-driven ZF to DWs are self-consistently included. The nonlinear DW equation is solved numerically in both uniform and nonuniform plasmas. It is shown that DWenvelope soliton may form due to the balance of linear dispersiveness and nonlinearity, and lead to turbulence spreading to linearly stable region. It is further found that though the threshold on DW amplitude for soliton formation is well within the relevant parameter regimes of realistic tokamak experiments, solitons can not extend beyond the range bounded by the turning points of the wave packet when plasma nonuniformity is self-consistently accounted for.","sentences":["In this work, gyrokinetic theory of drift waves (DWs) self-regulation via the forced driven zonal flow (ZF) is presented, and finite diamagnetic drift frequency due to plasma nonuniformity is shown to play dominant role in ZF forced generation.","The obtained nonlinear DW equation is a nonlinear Schr\\\"odinger equation, in which the linear dispersiveness, linear growth, nonuniformity of diamagnetic drift frequency, and cubic nonlinearity induced by feedback of forced-driven ZF to DWs are self-consistently included.","The nonlinear DW equation is solved numerically in both uniform and nonuniform plasmas.","It is shown that DWenvelope soliton may form due to the balance of linear dispersiveness and nonlinearity, and lead to turbulence spreading to linearly stable region.","It is further found that though the threshold on DW amplitude for soliton formation is well within the relevant parameter regimes of realistic tokamak experiments, solitons can not extend beyond the range bounded by the turning points of the wave packet when plasma nonuniformity is self-consistently accounted for."],"url":"http://arxiv.org/abs/2402.07390v1","category":"physics.plasm-ph"}
{"created":"2024-02-12 02:52:56","title":"Constructive Coordinatization of Desarguesian Planes","abstract":"A classical theory of Desarguesian geometry, originating with D. Hilbert in his 1899 treatise, Grundlagen der Geometrie, leads from axioms to the construction of a division ring from which coordinates may be assigned to points, and equations to lines; this theory is highly nonconstructive. The present paper develops this coordinatization theory constructively, in accordance with the principles introduced by Errett Bishop in his 1967 book, Foundations of Constructive Analysis. The traditional geometric axioms are adopted, together with two supplementary axioms which are constructively stronger versions of portions of the usual axioms. Stronger definitions, with enhanced constructive meaning, are also selected; these are based on a single primitive notion, and are classically equivalent to the traditional definitions. Brouwerian counterexamples are included; these point out specific nonconstructivities in the classical theory, and the consequent need for strengthened definitions and results in a constructive theory. All the major results of the classical theory are established, in their original form, revealing their hidden constructive content.","sentences":["A classical theory of Desarguesian geometry, originating with D. Hilbert in his 1899 treatise, Grundlagen der Geometrie, leads from axioms to the construction of a division ring from which coordinates may be assigned to points, and equations to lines; this theory is highly nonconstructive.","The present paper develops this coordinatization theory constructively, in accordance with the principles introduced by Errett Bishop in his 1967 book, Foundations of Constructive Analysis.","The traditional geometric axioms are adopted, together with two supplementary axioms which are constructively stronger versions of portions of the usual axioms.","Stronger definitions, with enhanced constructive meaning, are also selected; these are based on a single primitive notion, and are classically equivalent to the traditional definitions.","Brouwerian counterexamples are included; these point out specific nonconstructivities in the classical theory, and the consequent need for strengthened definitions and results in a constructive theory.","All the major results of the classical theory are established, in their original form, revealing their hidden constructive content."],"url":"http://arxiv.org/abs/2402.07382v1","category":"math.MG"}
{"created":"2024-02-12 01:04:39","title":"Sampling from the Mean-Field Stationary Distribution","abstract":"We study the complexity of sampling from the stationary distribution of a mean-field SDE, or equivalently, the complexity of minimizing a functional over the space of probability measures which includes an interaction term.   Our main insight is to decouple the two key aspects of this problem: (1) approximation of the mean-field SDE via a finite-particle system, via uniform-in-time propagation of chaos, and (2) sampling from the finite-particle stationary distribution, via standard log-concave samplers. Our approach is conceptually simpler and its flexibility allows for incorporating the state-of-the-art for both algorithms and theory. This leads to improved guarantees in numerous settings, including better guarantees for optimizing certain two-layer neural networks in the mean-field regime.","sentences":["We study the complexity of sampling from the stationary distribution of a mean-field SDE, or equivalently, the complexity of minimizing a functional over the space of probability measures which includes an interaction term.   ","Our main insight is to decouple the two key aspects of this problem: (1) approximation of the mean-field SDE via a finite-particle system, via uniform-in-time propagation of chaos, and (2) sampling from the finite-particle stationary distribution, via standard log-concave samplers.","Our approach is conceptually simpler and its flexibility allows for incorporating the state-of-the-art for both algorithms and theory.","This leads to improved guarantees in numerous settings, including better guarantees for optimizing certain two-layer neural networks in the mean-field regime."],"url":"http://arxiv.org/abs/2402.07355v1","category":"math.ST"}
{"created":"2024-02-12 01:02:22","title":"Data Distribution-based Curriculum Learning","abstract":"The order of training samples can have a significant impact on the performance of a classifier. Curriculum learning is a method of ordering training samples from easy to hard. This paper proposes the novel idea of a curriculum learning approach called Data Distribution-based Curriculum Learning (DDCL). DDCL uses the data distribution of a dataset to build a curriculum based on the order of samples. Two types of scoring methods known as DDCL (Density) and DDCL (Point) are used to score training samples thus determining their training order. DDCL (Density) uses the sample density to assign scores while DDCL (Point) utilises the Euclidean distance for scoring. We evaluate the proposed DDCL approach by conducting experiments on multiple datasets using a neural network, support vector machine and random forest classifier. Evaluation results show that the application of DDCL improves the average classification accuracy for all datasets compared to standard evaluation without any curriculum. Moreover, analysis of the error losses for a single training epoch reveals that convergence is faster when using DDCL over the no curriculum method.","sentences":["The order of training samples can have a significant impact on the performance of a classifier.","Curriculum learning is a method of ordering training samples from easy to hard.","This paper proposes the novel idea of a curriculum learning approach called Data Distribution-based Curriculum Learning (DDCL).","DDCL uses the data distribution of a dataset to build a curriculum based on the order of samples.","Two types of scoring methods known as DDCL (Density) and DDCL (Point) are used to score training samples thus determining their training order.","DDCL (Density) uses the sample density to assign scores while DDCL (Point) utilises the Euclidean distance for scoring.","We evaluate the proposed DDCL approach by conducting experiments on multiple datasets using a neural network, support vector machine and random forest classifier.","Evaluation results show that the application of DDCL improves the average classification accuracy for all datasets compared to standard evaluation without any curriculum.","Moreover, analysis of the error losses for a single training epoch reveals that convergence is faster when using DDCL over the no curriculum method."],"url":"http://arxiv.org/abs/2402.07352v1","category":"cs.LG"}
{"created":"2024-02-11 21:21:20","title":"Two-loop non-planar four-point topology with massive internal loop","abstract":"We study a set of two-loop non-planar master integrals needed for the NNLO QCD corrections to diphoton and dijet production at hadron colliders. The top-sector topology contains an internal massive fermion loop and is known to contain elliptic curves. Leveraging the method of differential equations, we provide a comprehensive discussion for deriving an $\\epsilon$-factorized differential equation related to the most intricate sector within the Feynman integral family. Despite the dependence on multiple scales and the presence of two elliptic sectors, we demonstrate how to leverage the properties of their maximal cuts and the factorization of the Picard-Fuchs operator to deal with the complexity of the analytic computation. In particular, we construct a transformation matrix that brings the differential equations into a format enabling the convenient expression of analytic results in terms of Chen's iterated integrals.","sentences":["We study a set of two-loop non-planar master integrals needed for the NNLO QCD corrections to diphoton and dijet production at hadron colliders.","The top-sector topology contains an internal massive fermion loop and is known to contain elliptic curves.","Leveraging the method of differential equations, we provide a comprehensive discussion for deriving an $\\epsilon$-factorized differential equation related to the most intricate sector within the Feynman integral family.","Despite the dependence on multiple scales and the presence of two elliptic sectors, we demonstrate how to leverage the properties of their maximal cuts and the factorization of the Picard-Fuchs operator to deal with the complexity of the analytic computation.","In particular, we construct a transformation matrix that brings the differential equations into a format enabling the convenient expression of analytic results in terms of Chen's iterated integrals."],"url":"http://arxiv.org/abs/2402.07311v1","category":"hep-th"}
{"created":"2024-02-11 21:16:42","title":"BioNeRF: Biologically Plausible Neural Radiance Fields for View Synthesis","abstract":"This paper presents BioNeRF, a biologically plausible architecture that models scenes in a 3D representation and synthesizes new views through radiance fields. Since NeRF relies on the network weights to store the scene's 3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism that fuses inputs from multiple sources into a memory-like structure, improving the storing capacity and extracting more intrinsic and correlated information. BioNeRF also mimics a behavior observed in pyramidal cells concerning contextual information, in which the memory is provided as the context and combined with the inputs of two subsequent neural models, one responsible for producing the volumetric densities and the other the colors used to render the scene. Experimental results show that BioNeRF outperforms state-of-the-art results concerning a quality measure that encodes human perception in two datasets: real-world images and synthetic data.","sentences":["This paper presents BioNeRF, a biologically plausible architecture that models scenes in a 3D representation and synthesizes new views through radiance fields.","Since NeRF relies on the network weights to store the scene's 3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism that fuses inputs from multiple sources into a memory-like structure, improving the storing capacity and extracting more intrinsic and correlated information.","BioNeRF also mimics a behavior observed in pyramidal cells concerning contextual information, in which the memory is provided as the context and combined with the inputs of two subsequent neural models, one responsible for producing the volumetric densities and the other the colors used to render the scene.","Experimental results show that BioNeRF outperforms state-of-the-art results concerning a quality measure that encodes human perception in two datasets: real-world images and synthetic data."],"url":"http://arxiv.org/abs/2402.07310v1","category":"cs.CV"}
{"created":"2024-02-11 21:13:11","title":"The Aubin-Lions-Dubinskii theorems on compactness in Bochner spaces","abstract":"A fundamental issue in the theory of time-dependent differential equations is to characterize precompact sets in Bochner spaces.   We here survey the theory, starting with the classical Aubin-Lions inequality and its important extension by Dubinskii. In particular, we give a simple and self-contained proof of the compactness result due to Chen, Jungel, and Liu.","sentences":["A fundamental issue in the theory of time-dependent differential equations is to characterize precompact sets in Bochner spaces.   ","We here survey the theory, starting with the classical Aubin-Lions inequality and its important extension by Dubinskii.","In particular, we give a simple and self-contained proof of the compactness result due to Chen, Jungel, and Liu."],"url":"http://arxiv.org/abs/2402.07308v1","category":"math.AP"}
{"created":"2024-02-11 20:42:49","title":"LISR: Learning Linear 3D Implicit Surface Representation Using Compactly Supported Radial Basis Functions","abstract":"Implicit 3D surface reconstruction of an object from its partial and noisy 3D point cloud scan is the classical geometry processing and 3D computer vision problem. In the literature, various 3D shape representations have been developed, differing in memory efficiency and shape retrieval effectiveness, such as volumetric, parametric, and implicit surfaces. Radial basis functions provide memory-efficient parameterization of the implicit surface. However, we show that training a neural network using the mean squared error between the ground-truth implicit surface and the linear basis-based implicit surfaces does not converge to the global solution. In this work, we propose locally supported compact radial basis functions for a linear representation of the implicit surface. This representation enables us to generate 3D shapes with arbitrary topologies at any resolution due to their continuous nature. We then propose a neural network architecture for learning the linear implicit shape representation of the 3D surface of an object. We learn linear implicit shapes within a supervised learning framework using ground truth Signed-Distance Field (SDF) data for guidance. The classical strategies face difficulties in finding linear implicit shapes from a given 3D point cloud due to numerical issues (requires solving inverse of a large matrix) in basis and query point selection. The proposed approach achieves better Chamfer distance and comparable F-score than the state-of-the-art approach on the benchmark dataset. We also show the effectiveness of the proposed approach by using it for the 3D shape completion task.","sentences":["Implicit 3D surface reconstruction of an object from its partial and noisy 3D point cloud scan is the classical geometry processing and 3D computer vision problem.","In the literature, various 3D shape representations have been developed, differing in memory efficiency and shape retrieval effectiveness, such as volumetric, parametric, and implicit surfaces.","Radial basis functions provide memory-efficient parameterization of the implicit surface.","However, we show that training a neural network using the mean squared error between the ground-truth implicit surface and the linear basis-based implicit surfaces does not converge to the global solution.","In this work, we propose locally supported compact radial basis functions for a linear representation of the implicit surface.","This representation enables us to generate 3D shapes with arbitrary topologies at any resolution due to their continuous nature.","We then propose a neural network architecture for learning the linear implicit shape representation of the 3D surface of an object.","We learn linear implicit shapes within a supervised learning framework using ground truth Signed-Distance Field (SDF) data for guidance.","The classical strategies face difficulties in finding linear implicit shapes from a given 3D point cloud due to numerical issues (requires solving inverse of a large matrix) in basis and query point selection.","The proposed approach achieves better Chamfer distance and comparable F-score than the state-of-the-art approach on the benchmark dataset.","We also show the effectiveness of the proposed approach by using it for the 3D shape completion task."],"url":"http://arxiv.org/abs/2402.07301v1","category":"cs.CV"}
{"created":"2024-02-11 20:27:31","title":"Supervised Reconstruction for Silhouette Tomography","abstract":"In this paper, we introduce silhouette tomography, a novel formulation of X-ray computed tomography that relies only on the geometry of the imaging system. We formulate silhouette tomography mathematically and provide a simple method for obtaining a particular solution to the problem, assuming that any solution exists. We then propose a supervised reconstruction approach that uses a deep neural network to solve the silhouette tomography problem. We present experimental results on a synthetic dataset that demonstrate the effectiveness of the proposed method.","sentences":["In this paper, we introduce silhouette tomography, a novel formulation of X-ray computed tomography that relies only on the geometry of the imaging system.","We formulate silhouette tomography mathematically and provide a simple method for obtaining a particular solution to the problem, assuming that any solution exists.","We then propose a supervised reconstruction approach that uses a deep neural network to solve the silhouette tomography problem.","We present experimental results on a synthetic dataset that demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2402.07298v1","category":"eess.IV"}
{"created":"2024-02-11 19:30:54","title":"Solutions of generalized supergravity equations with the BTZ black hole metric","abstract":"We proceed to investigate the solutions of generalized supergravity equations (GSE) in three dimensions. Our candidate is the metric of BTZ black hole. It is shown that only the cases with $J=M=0$ and $J=0,~ M\\neq 0$ of the BTZ metric satisfy the GSE. In the former, we find a family of solutions including the field strength $H_{_{r \\varphi t}}=2r/l$, the cosmological constant $\\Lambda=-1/l^2$, one-form $Z_{\\mu}$ and a vector field which is obtained to be a linear combination of the directions of the time translation and rotational symmetries. In the latter, the solutions possess the same field strength as before, while the cosmological constant $\\Lambda$, one-form $Z_{\\mu}$ and vector field $I$ will be different from the previous case. Finally, we show that the charged black string solution found by Horne and Horowitz, which is Abelian T-dual to the the BTZ black hole solution, can be considered as a solution for the GSE.","sentences":["We proceed to investigate the solutions of generalized supergravity equations (GSE) in three dimensions.","Our candidate is the metric of BTZ black hole.","It is shown that only the cases with $J=M=0$ and $J=0,~ M\\neq 0$ of the BTZ metric satisfy the GSE.","In the former, we find a family of solutions including the field strength $H_{_{r \\varphi t}}=2r/l$, the cosmological constant $\\Lambda=-1/l^2$, one-form $Z_{\\mu}$ and a vector field which is obtained to be a linear combination of the directions of the time translation and rotational symmetries.","In the latter, the solutions possess the same field strength as before, while the cosmological constant $\\Lambda$, one-form $Z_{\\mu}$ and vector field $I$ will be different from the previous case.","Finally, we show that the charged black string solution found by Horne and Horowitz, which is Abelian T-dual to the the BTZ black hole solution, can be considered as a solution for the GSE."],"url":"http://arxiv.org/abs/2402.07287v1","category":"hep-th"}
{"created":"2024-02-11 19:00:11","title":"Asymptotics of Weil-Petersson volumes and two-dimensional quantum gravities","abstract":"We propose a refined expression for the large genus asymptotics of the Weil-Petersson volumes of the moduli space of super-Riemann surfaces with an arbitrary number of boundaries. Our formula leverages the connection between JT supergravity and its matrix model definition, utilizing some basic tools of resurgence theory. The final result holds for arbitrary boundary lengths and preserves the polynomial structure of the super-volumes. As a byproduct we also obtain a prediction for the large genus asymptotics of generalized $\\Theta$-class intersection numbers. We extend our proposal to the case of the quantum volumes relevant for the Virasoro minimal string/Liouville gravity. Performing the classical limit on the quantum volumes, we recover a formula for the ordinary Weil-Petersson building blocks of JT gravity.","sentences":["We propose a refined expression for the large genus asymptotics of the Weil-Petersson volumes of the moduli space of super-Riemann surfaces with an arbitrary number of boundaries.","Our formula leverages the connection between JT supergravity and its matrix model definition, utilizing some basic tools of resurgence theory.","The final result holds for arbitrary boundary lengths and preserves the polynomial structure of the super-volumes.","As a byproduct we also obtain a prediction for the large genus asymptotics of generalized $\\Theta$-class intersection numbers.","We extend our proposal to the case of the quantum volumes relevant for the Virasoro minimal string/Liouville gravity.","Performing the classical limit on the quantum volumes, we recover a formula for the ordinary Weil-Petersson building blocks of JT gravity."],"url":"http://arxiv.org/abs/2402.07276v1","category":"hep-th"}
{"created":"2024-02-11 18:33:39","title":"Constant mean curvature graphs with prescribed asymptotic values in $\\mathbb{E}(-1,\u03c4)$","abstract":"In the homogeneous manifold $\\mathbb{E}(-1,\\tau),$ for $0<H<\\tfrac{1}{2},$ we prove the existence of entire $H$-graphs which are asymptotic to a rectifiable curve of the asymptotic boundary. We also find necessary and sufficient conditions for the existence of $H$-graphs over unbounded domains having prescribed, possibly infinite boundary data.","sentences":["In the homogeneous manifold $\\mathbb{E}(-1,\\tau),$ for $0<H<\\tfrac{1}{2},$ we prove the existence of entire $H$-graphs which are asymptotic to a rectifiable curve of the asymptotic boundary.","We also find necessary and sufficient conditions for the existence of $H$-graphs over unbounded domains having prescribed, possibly infinite boundary data."],"url":"http://arxiv.org/abs/2402.07274v1","category":"math.DG"}
{"created":"2024-02-11 18:24:25","title":"The Boundary Condition for Some Isomonodromy Equations","abstract":"In this article, we study a special class of Jimbo-Miwa-Mori-Sato isomonodromy equations, which can be seen as a higher-dimensional generalization of Painlev\\'e VI. We first construct its convergent $n\\times n$ matrix series solutions satisfying certain boundary condition. We then use the Riemann-Hilbert approach to prove that the resulting solutions are almost all the solutions. Along the way, we find a shrinking phenomenon of the eigenvalues of the submatrices of the generic matrix solutions in the long time behaviour.","sentences":["In this article, we study a special class of Jimbo-Miwa-Mori-Sato isomonodromy equations, which can be seen as a higher-dimensional generalization of Painlev\\'e VI.","We first construct its convergent $n\\times n$ matrix series solutions satisfying certain boundary condition.","We then use the Riemann-Hilbert approach to prove that the resulting solutions are almost all the solutions.","Along the way, we find a shrinking phenomenon of the eigenvalues of the submatrices of the generic matrix solutions in the long time behaviour."],"url":"http://arxiv.org/abs/2402.07269v1","category":"math.CA"}
{"created":"2024-02-11 18:16:18","title":"Geometric and topological properties of manifolds in robot motion planning","abstract":"Manifolds occur naturally as configuration spaces of robotic systems. They provide global descriptions of local coordinate systems that are common tools in expressing positions of robots. The purpose of this survey is threefold. Firstly, we present an overview over various results on topological complexities of manifolds and related topics. Several constructions for manifolds, e.g. symplectic structures and connected sums, can be used to compute or estimate topological complexities. Secondly, we take a look at geodesic motion planning in Riemannian manifolds. In this setting, results from Riemannian geometry are employed to estimate the complexity of motion planning along shortest paths in manifolds. Thirdly, we will discuss results on connections between critical point theory and the topology of manifolds that are related to motion planning problems. Here, we consider the role of navigation functions for topological complexity and outline their relations to newer numerical homotopy invariants, namely spherical complexities.","sentences":["Manifolds occur naturally as configuration spaces of robotic systems.","They provide global descriptions of local coordinate systems that are common tools in expressing positions of robots.","The purpose of this survey is threefold.","Firstly, we present an overview over various results on topological complexities of manifolds and related topics.","Several constructions for manifolds, e.g. symplectic structures and connected sums, can be used to compute or estimate topological complexities.","Secondly, we take a look at geodesic motion planning in Riemannian manifolds.","In this setting, results from Riemannian geometry are employed to estimate the complexity of motion planning along shortest paths in manifolds.","Thirdly, we will discuss results on connections between critical point theory and the topology of manifolds that are related to motion planning problems.","Here, we consider the role of navigation functions for topological complexity and outline their relations to newer numerical homotopy invariants, namely spherical complexities."],"url":"http://arxiv.org/abs/2402.07265v1","category":"math.GT"}
{"created":"2024-02-11 18:09:50","title":"Low-Resource Counterspeech Generation for Indic Languages: The Case of Bengali and Hindi","abstract":"With the rise of online abuse, the NLP community has begun investigating the use of neural architectures to generate counterspeech that can \"counter\" the vicious tone of such abusive speech and dilute/ameliorate their rippling effect over the social network. However, most of the efforts so far have been primarily focused on English. To bridge the gap for low-resource languages such as Bengali and Hindi, we create a benchmark dataset of 5,062 abusive speech/counterspeech pairs, of which 2,460 pairs are in Bengali and 2,602 pairs are in Hindi. We implement several baseline models considering various interlingual transfer mechanisms with different configurations to generate suitable counterspeech to set up an effective benchmark. We observe that the monolingual setup yields the best performance. Further, using synthetic transfer, language models can generate counterspeech to some extent; specifically, we notice that transferability is better when languages belong to the same language family.","sentences":["With the rise of online abuse, the NLP community has begun investigating the use of neural architectures to generate counterspeech that can \"counter\" the vicious tone of such abusive speech and dilute/ameliorate their rippling effect over the social network.","However, most of the efforts so far have been primarily focused on English.","To bridge the gap for low-resource languages such as Bengali and Hindi, we create a benchmark dataset of 5,062 abusive speech/counterspeech pairs, of which 2,460 pairs are in Bengali and 2,602 pairs are in Hindi.","We implement several baseline models considering various interlingual transfer mechanisms with different configurations to generate suitable counterspeech to set up an effective benchmark.","We observe that the monolingual setup yields the best performance.","Further, using synthetic transfer, language models can generate counterspeech to some extent; specifically, we notice that transferability is better when languages belong to the same language family."],"url":"http://arxiv.org/abs/2402.07262v1","category":"cs.CL"}
{"created":"2024-02-11 17:40:26","title":"Physics-Informed Neural Networks with Hard Linear Equality Constraints","abstract":"Surrogate modeling is used to replace computationally expensive simulations. Neural networks have been widely applied as surrogate models that enable efficient evaluations over complex physical systems. Despite this, neural networks are data-driven models and devoid of any physics. The incorporation of physics into neural networks can improve generalization and data efficiency. The physics-informed neural network (PINN) is an approach to leverage known physical constraints present in the data, but it cannot strictly satisfy them in the predictions. This work proposes a novel physics-informed neural network, KKT-hPINN, which rigorously guarantees hard linear equality constraints through projection layers derived from KKT conditions. Numerical experiments on Aspen models of a continuous stirred-tank reactor (CSTR) unit, an extractive distillation subsystem, and a chemical plant demonstrate that this model can further enhance the prediction accuracy.","sentences":["Surrogate modeling is used to replace computationally expensive simulations.","Neural networks have been widely applied as surrogate models that enable efficient evaluations over complex physical systems.","Despite this, neural networks are data-driven models and devoid of any physics.","The incorporation of physics into neural networks can improve generalization and data efficiency.","The physics-informed neural network (PINN) is an approach to leverage known physical constraints present in the data, but it cannot strictly satisfy them in the predictions.","This work proposes a novel physics-informed neural network, KKT-hPINN, which rigorously guarantees hard linear equality constraints through projection layers derived from KKT conditions.","Numerical experiments on Aspen models of a continuous stirred-tank reactor (CSTR) unit, an extractive distillation subsystem, and a chemical plant demonstrate that this model can further enhance the prediction accuracy."],"url":"http://arxiv.org/abs/2402.07251v1","category":"cs.LG"}
{"created":"2024-02-11 17:27:26","title":"Depth Separations in Neural Networks: Separating the Dimension from the Accuracy","abstract":"We prove an exponential separation between depth 2 and depth 3 neural networks, when approximating an $\\mathcal{O}(1)$-Lipschitz target function to constant accuracy, with respect to a distribution with support in $[0,1]^{d}$, assuming exponentially bounded weights. This addresses an open problem posed in \\citet{safran2019depth}, and proves that the curse of dimensionality manifests in depth 2 approximation, even in cases where the target function can be represented efficiently using depth 3. Previously, lower bounds that were used to separate depth 2 from depth 3 required that at least one of the Lipschitz parameter, target accuracy or (some measure of) the size of the domain of approximation scale polynomially with the input dimension, whereas we fix the former two and restrict our domain to the unit hypercube. Our lower bound holds for a wide variety of activation functions, and is based on a novel application of an average- to worst-case random self-reducibility argument, to reduce the problem to threshold circuits lower bounds.","sentences":["We prove an exponential separation between depth 2 and depth 3 neural networks, when approximating an $\\mathcal{O}(1)$-Lipschitz target function to constant accuracy, with respect to a distribution with support in $[0,1]^{d}$, assuming exponentially bounded weights.","This addresses an open problem posed in \\citet{safran2019depth}, and proves that the curse of dimensionality manifests in depth 2 approximation, even in cases where the target function can be represented efficiently using depth 3.","Previously, lower bounds that were used to separate depth 2 from depth 3 required that at least one of the Lipschitz parameter, target accuracy or (some measure of) the size of the domain of approximation scale polynomially with the input dimension, whereas we fix the former two and restrict our domain to the unit hypercube.","Our lower bound holds for a wide variety of activation functions, and is based on a novel application of an average- to worst-case random self-reducibility argument, to reduce the problem to threshold circuits lower bounds."],"url":"http://arxiv.org/abs/2402.07248v1","category":"cs.LG"}
{"created":"2024-02-11 17:09:21","title":"Semi-Mamba-UNet: Pixel-Level Contrastive Cross-Supervised Visual Mamba-based UNet for Semi-Supervised Medical Image Segmentation","abstract":"Medical image segmentation is essential in diagnostics, treatment planning, and healthcare, with deep learning offering promising advancements. Notably, Convolutional Neural Network (CNN) excel in capturing local image features, whereas Vision Transformer (ViT) adeptly model long-range dependencies through multi-head self-attention mechanisms. Despite their strengths, both CNN and ViT face challenges in efficiently processing long-range dependencies within medical images, often requiring substantial computational resources. This issue, combined with the high cost and limited availability of expert annotations, poses significant obstacles to achieving precise segmentation. To address these challenges, this paper introduces the Semi-Mamba-UNet, which integrates a visual mamba-based UNet architecture with a conventional UNet into a semi-supervised learning (SSL) framework. This innovative SSL approach leverages dual networks to jointly generate pseudo labels and cross supervise each other, drawing inspiration from consistency regularization techniques. Furthermore, we introduce a self-supervised pixel-level contrastive learning strategy, employing a projector pair to further enhance feature learning capabilities. Our comprehensive evaluation on a publicly available MRI cardiac segmentation dataset, comparing against various SSL frameworks with different UNet-based segmentation networks, highlights the superior performance of Semi-Mamba-UNet. The source code has been made publicly accessible.","sentences":["Medical image segmentation is essential in diagnostics, treatment planning, and healthcare, with deep learning offering promising advancements.","Notably, Convolutional Neural Network (CNN) excel in capturing local image features, whereas Vision Transformer (ViT) adeptly model long-range dependencies through multi-head self-attention mechanisms.","Despite their strengths, both CNN and ViT face challenges in efficiently processing long-range dependencies within medical images, often requiring substantial computational resources.","This issue, combined with the high cost and limited availability of expert annotations, poses significant obstacles to achieving precise segmentation.","To address these challenges, this paper introduces the Semi-Mamba-UNet, which integrates a visual mamba-based UNet architecture with a conventional UNet into a semi-supervised learning (SSL) framework.","This innovative SSL approach leverages dual networks to jointly generate pseudo labels and cross supervise each other, drawing inspiration from consistency regularization techniques.","Furthermore, we introduce a self-supervised pixel-level contrastive learning strategy, employing a projector pair to further enhance feature learning capabilities.","Our comprehensive evaluation on a publicly available MRI cardiac segmentation dataset, comparing against various SSL frameworks with different UNet-based segmentation networks, highlights the superior performance of Semi-Mamba-UNet.","The source code has been made publicly accessible."],"url":"http://arxiv.org/abs/2402.07245v1","category":"eess.IV"}
{"created":"2024-02-11 16:28:16","title":"Spherically symmetric teleparallel geometries","abstract":"We are interested in the development of spherically symmetric geometries in $F(T)$ teleparallel gravity which are of physical importance. We first express the general forms for the spherically symmetric frame and the zero curvature, metric compatible, spin connection. We then analyse the antisymmetric field equations (the solutions of which split into two cases, which we subsequently consider separately), and derive and analyse the resulting symmetric field equations. In order to further study the applications of spherically symmetric teleparallel models, we study $3$ subcases in which there is an additional affine symmetry so that the resulting field equations reduce to a system of ordinary differential equations. First, we study static spherical symmetric geometries and solve the antisymmetric field equations and subsequently derive the full set of symmetric field equations. In particular, we investigate vacuum spacetimes and obtain a number of new solutions. Second, we consider an additional affine frame symmetry in order to expand the affine frame symmetry group to that of a spatially homogeneous Kantowski-Sachs geometry. Third, we study the special case of spherical symmetry with an additional fourth similarity affine vector.","sentences":["We are interested in the development of spherically symmetric geometries in $F(T)$ teleparallel gravity which are of physical importance.","We first express the general forms for the spherically symmetric frame and the zero curvature, metric compatible, spin connection.","We then analyse the antisymmetric field equations (the solutions of which split into two cases, which we subsequently consider separately), and derive and analyse the resulting symmetric field equations.","In order to further study the applications of spherically symmetric teleparallel models, we study $3$ subcases in which there is an additional affine symmetry so that the resulting field equations reduce to a system of ordinary differential equations.","First, we study static spherical symmetric geometries and solve the antisymmetric field equations and subsequently derive the full set of symmetric field equations.","In particular, we investigate vacuum spacetimes and obtain a number of new solutions.","Second, we consider an additional affine frame symmetry in order to expand the affine frame symmetry group to that of a spatially homogeneous Kantowski-Sachs geometry.","Third, we study the special case of spherical symmetry with an additional fourth similarity affine vector."],"url":"http://arxiv.org/abs/2402.07238v1","category":"gr-qc"}
{"created":"2024-02-11 15:46:31","title":"Ab initio simulations of the thermodynamic properties and phase transition of Fermi systems based on fictitious identical particles and physics-informed neural networks","abstract":"Fictitious identical particle thermodynamics has emerged as a powerful tool to overcome the fermion sign problem, enabling highly accurate simulations of one thousand fermions in warm dense matter (T. Dornheim et al., J. Phys. Chem. Lett. 15, 1305 (2024)). However, inferring the thermodynamic properties of Fermi systems from a large number of exact numerical simulations of the bosonic sector still poses subtle challenges, especially in the regime of high quantum degeneracy and in the presence of phase transitions. In this work, we demonstrate that physics-informed neural networks (PINNs), trained on data from extensive and sign-problem-free numerical simulations of the bosonic sector, offer a valuable means to infer the thermodynamic properties of Fermi systems. PINNs can play a particularly crucial role in capturing phase transitions. To illustrate the methodology of fictitious identical particles combined with PINNs for simulating the thermodynamics of Fermi systems, we explore its application in realistic scenarios, including ultracold Fermi gases in periodic potentials, and phase transitions of pair condensation formed in the unitary limit in a three-dimensional harmonic trap. For the spatially continuous Fermi-Hubbard model, we efficiently and reliably simulated hundreds of fermions here. For the Fermi gas in the unitary limit, based on the fictitious identical particle combined with PINNs, our approach confirms the universal result of the critical temperature with the increasing of the number of fermions, and is consistent with the experimental observations.","sentences":["Fictitious identical particle thermodynamics has emerged as a powerful tool to overcome the fermion sign problem, enabling highly accurate simulations of one thousand fermions in warm dense matter (T. Dornheim et al., J. Phys.","Chem.","Lett.","15, 1305 (2024)).","However, inferring the thermodynamic properties of Fermi systems from a large number of exact numerical simulations of the bosonic sector still poses subtle challenges, especially in the regime of high quantum degeneracy and in the presence of phase transitions.","In this work, we demonstrate that physics-informed neural networks (PINNs), trained on data from extensive and sign-problem-free numerical simulations of the bosonic sector, offer a valuable means to infer the thermodynamic properties of Fermi systems.","PINNs can play a particularly crucial role in capturing phase transitions.","To illustrate the methodology of fictitious identical particles combined with PINNs for simulating the thermodynamics of Fermi systems, we explore its application in realistic scenarios, including ultracold Fermi gases in periodic potentials, and phase transitions of pair condensation formed in the unitary limit in a three-dimensional harmonic trap.","For the spatially continuous Fermi-Hubbard model, we efficiently and reliably simulated hundreds of fermions here.","For the Fermi gas in the unitary limit, based on the fictitious identical particle combined with PINNs, our approach confirms the universal result of the critical temperature with the increasing of the number of fermions, and is consistent with the experimental observations."],"url":"http://arxiv.org/abs/2402.07231v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-11 14:53:47","title":"Radiant fluence from ray tracing in optical multipass systems","abstract":"In applications of optical multipass cells in photochemical reactors and laser excitation of weak transitions, estimation of the radiation dose in a volume of interest allows to assess the performance and optimize the design of the cell. We adopt radiant fluence as the figure of merit and employ the radiative transfer equation to derive analytical expressions for average radiant fluence in a given volume of interest. These are used to establish practical approximations and a Monte Carlo ray tracing model of the spatial distribution of fluence. Ray tracing is performed with Zemax OpticsStudio 18.9.","sentences":["In applications of optical multipass cells in photochemical reactors and laser excitation of weak transitions, estimation of the radiation dose in a volume of interest allows to assess the performance and optimize the design of the cell.","We adopt radiant fluence as the figure of merit and employ the radiative transfer equation to derive analytical expressions for average radiant fluence in a given volume of interest.","These are used to establish practical approximations and a Monte Carlo ray tracing model of the spatial distribution of fluence.","Ray tracing is performed with Zemax OpticsStudio 18.9."],"url":"http://arxiv.org/abs/2402.07223v1","category":"physics.optics"}
{"created":"2024-02-11 13:35:49","title":"On the solution of constrained Sylvester-observer equation","abstract":"In this paper explicit necessary and sufficient conditions for the constrained Sylvester-observer equation are established, in order to have a solution over the field of real numbers. Furthermore, a procedure is given for the computation of the solution. Our approach is based on properties of real and polynomial matrices. Applications of the main results of this paper to linear control theory are discussed","sentences":["In this paper explicit necessary and sufficient conditions for the constrained Sylvester-observer equation are established, in order to have a solution over the field of real numbers.","Furthermore, a procedure is given for the computation of the solution.","Our approach is based on properties of real and polynomial matrices.","Applications of the main results of this paper to linear control theory are discussed"],"url":"http://arxiv.org/abs/2402.07206v1","category":"math.OC"}
{"created":"2024-02-11 12:33:08","title":"3D Gaussian as a New Vision Era: A Survey","abstract":"3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the field of Computer Graphics, offering explicit scene representation and novel view synthesis without the reliance on neural networks, such as Neural Radiance Fields (NeRF). This technique has found diverse applications in areas such as robotics, urban mapping, autonomous navigation, and virtual reality/augmented reality, just name a few. Given the growing popularity and expanding research in 3D Gaussian Splatting, this paper presents a comprehensive survey of relevant papers from the past year. We organize the survey into taxonomies based on characteristics and applications, providing an introduction to the theoretical underpinnings of 3D Gaussian Splatting. Our goal through this survey is to acquaint new researchers with 3D Gaussian Splatting, serve as a valuable reference for seminal works in the field, and inspire future research directions, as discussed in our concluding section.","sentences":["3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the field of Computer Graphics, offering explicit scene representation and novel view synthesis without the reliance on neural networks, such as Neural Radiance Fields (NeRF).","This technique has found diverse applications in areas such as robotics, urban mapping, autonomous navigation, and virtual reality/augmented reality, just name a few.","Given the growing popularity and expanding research in 3D Gaussian Splatting, this paper presents a comprehensive survey of relevant papers from the past year.","We organize the survey into taxonomies based on characteristics and applications, providing an introduction to the theoretical underpinnings of 3D Gaussian Splatting.","Our goal through this survey is to acquaint new researchers with 3D Gaussian Splatting, serve as a valuable reference for seminal works in the field, and inspire future research directions, as discussed in our concluding section."],"url":"http://arxiv.org/abs/2402.07181v1","category":"cs.CV"}
{"created":"2024-02-11 12:01:24","title":"Review of some modified generalized Korteweg - de Vries - Kuramoto-Sivashinsky equations (mgKdV-KS)","abstract":"This paper reviews the results of existence and uniqueness of the solutions of these equations: the Korteweg-de Vries equation, the Kuramoto-Sivashinsky equation, the generalized Korteweg-de Vries-Kuramoto-Sivashinski equation and the non homogeneous boundary value problem for KdV-KS equation in quarter plane.","sentences":["This paper reviews the results of existence and uniqueness of the solutions of these equations: the Korteweg-de Vries equation, the Kuramoto-Sivashinsky equation, the generalized Korteweg-de Vries-Kuramoto-Sivashinski equation and the non homogeneous boundary value problem for KdV-KS equation in quarter plane."],"url":"http://arxiv.org/abs/2402.07172v1","category":"math.AP"}
{"created":"2024-02-11 11:40:20","title":"Differential algebra of polytopes and inversion formulas","abstract":"We use the differential algebra of simple polytopes to explain the remarkable relation of the combinatorics of the associahedra and permutohedra with the compositional and multiplicative inversion of formal power series. This approach allows to single out the associahedra and permutohedra among all graph-associahedra and emphasizes the significance of the differential equations for polytopes derived earlier by one of the authors. We discuss also the link with the geometry of Deligne-Mumford moduli spaces $\\bar M_{0,n}$ and provide a new interpretation of the combinatorics of cyclohedra in relation with the classical Fa\\`a di Bruno's formula.","sentences":["We use the differential algebra of simple polytopes to explain the remarkable relation of the combinatorics of the associahedra and permutohedra with the compositional and multiplicative inversion of formal power series.","This approach allows to single out the associahedra and permutohedra among all graph-associahedra and emphasizes the significance of the differential equations for polytopes derived earlier by one of the authors.","We discuss also the link with the geometry of Deligne-Mumford moduli spaces $\\bar M_{0,n}$ and provide a new interpretation of the combinatorics of cyclohedra in relation with the classical Fa\\`a di Bruno's formula."],"url":"http://arxiv.org/abs/2402.07168v1","category":"math.CO"}
{"created":"2024-02-11 11:17:00","title":"Joint Source-Channel Coding for Wireless Image Transmission: A Deep Compressed-Sensing Based Method","abstract":"Nowadays, the demand for image transmission over wireless networks has surged significantly. To meet the need for swift delivery of high-quality images through time-varying channels with limited bandwidth, the development of efficient transmission strategies and techniques for preserving image quality is of importance. This paper introduces an innovative approach to Joint Source-Channel Coding (JSCC) tailored for wireless image transmission. It capitalizes on the power of Compressed Sensing (CS) to achieve superior compression and resilience to channel noise. In this method, the process begins with the compression of images using a block-based CS technique implemented through a Convolutional Neural Network (CNN) structure. Subsequently, the images are encoded by directly mapping image blocks to complex-valued channel input symbols. Upon reception, the data is decoded to recover the channel-encoded information, effectively removing the noise introduced during transmission. To finalize the process, a novel CNN-based reconstruction network is employed to restore the original image from the channel-decoded data. The performance of the proposed method is assessed using the CIFAR-10 and Kodak datasets. The results illustrate a substantial improvement over existing JSCC frameworks when assessed in terms of metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) across various channel Signal-to-Noise Ratios (SNRs) and channel bandwidth values. These findings underscore the potential of harnessing CNN-based CS for the development of deep JSCC algorithms tailored for wireless image transmission.","sentences":["Nowadays, the demand for image transmission over wireless networks has surged significantly.","To meet the need for swift delivery of high-quality images through time-varying channels with limited bandwidth, the development of efficient transmission strategies and techniques for preserving image quality is of importance.","This paper introduces an innovative approach to Joint Source-Channel Coding (JSCC) tailored for wireless image transmission.","It capitalizes on the power of Compressed Sensing (CS) to achieve superior compression and resilience to channel noise.","In this method, the process begins with the compression of images using a block-based CS technique implemented through a Convolutional Neural Network (CNN) structure.","Subsequently, the images are encoded by directly mapping image blocks to complex-valued channel input symbols.","Upon reception, the data is decoded to recover the channel-encoded information, effectively removing the noise introduced during transmission.","To finalize the process, a novel CNN-based reconstruction network is employed to restore the original image from the channel-decoded data.","The performance of the proposed method is assessed using the CIFAR-10 and Kodak datasets.","The results illustrate a substantial improvement over existing JSCC frameworks when assessed in terms of metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) across various channel Signal-to-Noise Ratios (SNRs) and channel bandwidth values.","These findings underscore the potential of harnessing CNN-based CS for the development of deep JSCC algorithms tailored for wireless image transmission."],"url":"http://arxiv.org/abs/2402.07162v1","category":"cs.CE"}
{"created":"2024-02-11 09:21:18","title":"On the eigenvalues of the spheroidal wave equation","abstract":"This paper presents some new results on the eigenvalues of the spheroidal wave equation. We study the angular and Coulomb spheroidal wave equation as a special case of a more general linear Hamiltonian system depending on three parameters. We prove that the eigenvalues of this system satisfy a first-order quasilinear partial differential equation with respect to the parameters. This relation offers a new insight on how the eigenvalues of the spheroidal wave equation depend on the spheroidal parameter. Apart from analytical considerations, the PDE we obtain can also be used for a numerical computation of spheroidal eigenvalues.","sentences":["This paper presents some new results on the eigenvalues of the spheroidal wave equation.","We study the angular and Coulomb spheroidal wave equation as a special case of a more general linear Hamiltonian system depending on three parameters.","We prove that the eigenvalues of this system satisfy a first-order quasilinear partial differential equation with respect to the parameters.","This relation offers a new insight on how the eigenvalues of the spheroidal wave equation depend on the spheroidal parameter.","Apart from analytical considerations, the PDE we obtain can also be used for a numerical computation of spheroidal eigenvalues."],"url":"http://arxiv.org/abs/2402.07133v1","category":"math.AP"}
{"created":"2024-02-11 08:59:02","title":"Resampling methods for Private Statistical Inference","abstract":"We consider the task of constructing confidence intervals with differential privacy. We propose two private variants of the non-parametric bootstrap, which privately compute the median of the results of multiple ``little'' bootstraps run on partitions of the data and give asymptotic bounds on the coverage error of the resulting confidence intervals. For a fixed differential privacy parameter $\\epsilon$, our methods enjoy the same error rates as that of the non-private bootstrap to within logarithmic factors in the sample size $n$. We empirically validate the performance of our methods for mean estimation, median estimation, and logistic regression with both real and synthetic data. Our methods achieve similar coverage accuracy to existing methods (and non-private baselines) while providing notably shorter ($\\gtrsim 10$ times) confidence intervals than previous approaches.","sentences":["We consider the task of constructing confidence intervals with differential privacy.","We propose two private variants of the non-parametric bootstrap, which privately compute the median of the results of multiple ``little'' bootstraps run on partitions of the data and give asymptotic bounds on the coverage error of the resulting confidence intervals.","For a fixed differential privacy parameter $\\epsilon$, our methods enjoy the same error rates as that of the non-private bootstrap to within logarithmic factors in the sample size $n$. We empirically validate the performance of our methods for mean estimation, median estimation, and logistic regression with both real and synthetic data.","Our methods achieve similar coverage accuracy to existing methods (and non-private baselines) while providing notably shorter ($\\gtrsim 10$ times) confidence intervals than previous approaches."],"url":"http://arxiv.org/abs/2402.07131v1","category":"stat.ML"}
{"created":"2024-02-11 08:10:01","title":"Quasidiscrete spectrum Cherenkov radiation by a charge moving inside a dielectric waveguide","abstract":"We investigate the Cherenkov radiation by a charge uniformly moving inside a dielectric cylindrical channel in a homogeneous medium. The expressions for the Fourier components of the electric and magnetic fields are derived by using the electromagnetic field Green tensor. The spectral distribution of the Cherenkov radiation intensity in the exterior medium is studied for the general case of frequency dispersion of the interior and exterior dielectric functions. It is shown that, under certain conditions on the dielectric permittivities, strong narrow peaks appear in the spectral distribution. The spectral locations of those peaks are specified and their heights and widths are estimated analytically on the base of the dispersion equation for the electromagnetic eigenmodes of the cylinder.","sentences":["We investigate the Cherenkov radiation by a charge uniformly moving inside a dielectric cylindrical channel in a homogeneous medium.","The expressions for the Fourier components of the electric and magnetic fields are derived by using the electromagnetic field Green tensor.","The spectral distribution of the Cherenkov radiation intensity in the exterior medium is studied for the general case of frequency dispersion of the interior and exterior dielectric functions.","It is shown that, under certain conditions on the dielectric permittivities, strong narrow peaks appear in the spectral distribution.","The spectral locations of those peaks are specified and their heights and widths are estimated analytically on the base of the dispersion equation for the electromagnetic eigenmodes of the cylinder."],"url":"http://arxiv.org/abs/2402.07121v1","category":"physics.optics"}
{"created":"2024-02-11 07:49:35","title":"Two-Stage Multi-task Self-Supervised Learning for Medical Image Segmentation","abstract":"Medical image segmentation has been significantly advanced by deep learning (DL) techniques, though the data scarcity inherent in medical applications poses a great challenge to DL-based segmentation methods. Self-supervised learning offers a solution by creating auxiliary learning tasks from the available dataset and then leveraging the knowledge acquired from solving auxiliary tasks to help better solve the target segmentation task. Different auxiliary tasks may have different properties and thus can help the target task to different extents. It is desired to leverage their complementary advantages to enhance the overall assistance to the target task. To achieve this, existing methods often adopt a joint training paradigm, which co-solves segmentation and auxiliary tasks by integrating their losses or intermediate gradients. However, direct coupling of losses or intermediate gradients risks undesirable interference because the knowledge acquired from solving each auxiliary task at every training step may not always benefit the target task. To address this issue, we propose a two-stage training approach. In the first stage, the target segmentation task will be independently co-solved with each auxiliary task in both joint training and pre-training modes, with the better model selected via validation performance. In the second stage, the models obtained with respect to each auxiliary task are converted into a single model using an ensemble knowledge distillation method. Our approach allows for making best use of each auxiliary task to create multiple elite segmentation models and then combine them into an even more powerful model. We employed five auxiliary tasks of different proprieties in our approach and applied it to train the U-Net model on an X-ray pneumothorax segmentation dataset. Experimental results demonstrate the superiority of our approach over several existing methods.","sentences":["Medical image segmentation has been significantly advanced by deep learning (DL) techniques, though the data scarcity inherent in medical applications poses a great challenge to DL-based segmentation methods.","Self-supervised learning offers a solution by creating auxiliary learning tasks from the available dataset and then leveraging the knowledge acquired from solving auxiliary tasks to help better solve the target segmentation task.","Different auxiliary tasks may have different properties and thus can help the target task to different extents.","It is desired to leverage their complementary advantages to enhance the overall assistance to the target task.","To achieve this, existing methods often adopt a joint training paradigm, which co-solves segmentation and auxiliary tasks by integrating their losses or intermediate gradients.","However, direct coupling of losses or intermediate gradients risks undesirable interference because the knowledge acquired from solving each auxiliary task at every training step may not always benefit the target task.","To address this issue, we propose a two-stage training approach.","In the first stage, the target segmentation task will be independently co-solved with each auxiliary task in both joint training and pre-training modes, with the better model selected via validation performance.","In the second stage, the models obtained with respect to each auxiliary task are converted into a single model using an ensemble knowledge distillation method.","Our approach allows for making best use of each auxiliary task to create multiple elite segmentation models and then combine them into an even more powerful model.","We employed five auxiliary tasks of different proprieties in our approach and applied it to train the U-Net model on an X-ray pneumothorax segmentation dataset.","Experimental results demonstrate the superiority of our approach over several existing methods."],"url":"http://arxiv.org/abs/2402.07119v1","category":"cs.CV"}
{"created":"2024-02-11 06:04:39","title":"Statistical properties of probabilistic context-sensitive grammars","abstract":"Probabilistic context-free grammars, commonly used to randomly generate trees, have been well analyzed theoretically and have found applications in various domains. Despite their utility, the distributions that the grammar can express are limited to those in which the distribution of a subtree depends only on its root and not on its context. This limitation becomes a challenge for modeling various real-world phenomena such as natural language. To overcome this limitation, a probabilistic context-sensitive grammar is introduced, where a subtree's distribution depends on its context, and its statistical properties are explored. Numerical analysis reveals that the distribution of a symbol does not exhibit a qualitative difference from that in the context-free case, but mutual information does. Furthermore, a metric is introduced to directly quantify the breaking of this limitation. This metric is zero in the context-free case, and is applicable to an arbitrary distribution of a tree. Measuring this metric enables the detection of a distinct difference between probabilistic context-free and context-sensitive grammars.","sentences":["Probabilistic context-free grammars, commonly used to randomly generate trees, have been well analyzed theoretically and have found applications in various domains.","Despite their utility, the distributions that the grammar can express are limited to those in which the distribution of a subtree depends only on its root and not on its context.","This limitation becomes a challenge for modeling various real-world phenomena such as natural language.","To overcome this limitation, a probabilistic context-sensitive grammar is introduced, where a subtree's distribution depends on its context, and its statistical properties are explored.","Numerical analysis reveals that the distribution of a symbol does not exhibit a qualitative difference from that in the context-free case, but mutual information does.","Furthermore, a metric is introduced to directly quantify the breaking of this limitation.","This metric is zero in the context-free case, and is applicable to an arbitrary distribution of a tree.","Measuring this metric enables the detection of a distinct difference between probabilistic context-free and context-sensitive grammars."],"url":"http://arxiv.org/abs/2402.07113v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-11 04:09:50","title":"Rethinking the Capacity of Graph Neural Networks for Branching Strategy","abstract":"Graph neural networks (GNNs) have been widely used to predict properties and heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP solvers. This paper investigates the capacity of GNNs to represent strong branching (SB) scores that provide an efficient strategy in the branch-and-bound algorithm.   Although message-passing GNN (MP-GNN), as the simplest GNN structure, is frequently employed in the existing literature to learn SB scores, we prove a fundamental limitation in its expressive power -- there exist two MILP instances with different SB scores that cannot be distinguished by any MP-GNN, regardless of the number of parameters. In addition, we establish a universal approximation theorem for another GNN structure called the second-order folklore GNN (2-FGNN). We show that for any data distribution over MILPs, there always exists a 2-FGNN that can approximate the SB score with arbitrarily high accuracy and arbitrarily high probability. A small-scale numerical experiment is conducted to directly validate our theoretical findings.","sentences":["Graph neural networks (GNNs) have been widely used to predict properties and heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP solvers.","This paper investigates the capacity of GNNs to represent strong branching (SB) scores that provide an efficient strategy in the branch-and-bound algorithm.   ","Although message-passing GNN (MP-GNN), as the simplest GNN structure, is frequently employed in the existing literature to learn SB scores, we prove a fundamental limitation in its expressive power -- there exist two MILP instances with different SB scores that cannot be distinguished by any MP-GNN, regardless of the number of parameters.","In addition, we establish a universal approximation theorem for another GNN structure called the second-order folklore GNN (2-FGNN).","We show that for any data distribution over MILPs, there always exists a 2-FGNN that can approximate the SB score with arbitrarily high accuracy and arbitrarily high probability.","A small-scale numerical experiment is conducted to directly validate our theoretical findings."],"url":"http://arxiv.org/abs/2402.07099v1","category":"cs.LG"}
{"created":"2024-02-11 00:02:36","title":"Efficient Algorithms for Sum-of-Minimum Optimization","abstract":"In this work, we propose a novel optimization model termed ``sum-of-minimum\" optimization. This model computes the sum (or average) of $N$ values where each is the minimum of the $k$ results of applying a distinct objective function to the same set of $k$ variables, and the model seeks to minimize this sum (or average) over those $k$ variables. When the $N$ functions are distance measures, we recover the $k$-means clustering problem by treating the $k$ variables as $k$ cluster centroids and the sum as the total distance of the input points to their nearest centroids. Therefore, the sum-of-minimum model embodies a clustering functionality with more general measurements instead of just distances.   We develop efficient algorithms for sum-of-minimum optimization by generalizing a randomized initialization algorithm for classic $k$-means (Arthur & Vassilvitskii, 2007) and Lloyd's algorithm (Lloyd, 1982). We establish a new tight bound for the generalized initialization algorithm and prove a gradient-descent-like convergence rate for the generalized Lloyd's algorithm.   The efficiency of our algorithms is numerically examined on multiple tasks including generalized principal component analysis, mixed linear regression, and small-scale neural network training. Our approach compares favorably to previous ones that are based on simpler but less precise optimization reformulations.","sentences":["In this work, we propose a novel optimization model termed ``sum-of-minimum\" optimization.","This model computes the sum (or average) of $N$ values where each is the minimum of the $k$ results of applying a distinct objective function to the same set of $k$ variables, and the model seeks to minimize this sum (or average) over those $k$ variables.","When the $N$ functions are distance measures, we recover the $k$-means clustering problem by treating the $k$ variables as $k$ cluster centroids and the sum as the total distance of the input points to their nearest centroids.","Therefore, the sum-of-minimum model embodies a clustering functionality with more general measurements instead of just distances.   ","We develop efficient algorithms for sum-of-minimum optimization by generalizing a randomized initialization algorithm for classic $k$-means (Arthur & Vassilvitskii, 2007) and Lloyd's algorithm (Lloyd, 1982).","We establish a new tight bound for the generalized initialization algorithm and prove a gradient-descent-like convergence rate for the generalized Lloyd's algorithm.   ","The efficiency of our algorithms is numerically examined on multiple tasks including generalized principal component analysis, mixed linear regression, and small-scale neural network training.","Our approach compares favorably to previous ones that are based on simpler but less precise optimization reformulations."],"url":"http://arxiv.org/abs/2402.07070v1","category":"math.OC"}
{"created":"2024-02-10 23:42:05","title":"Differentially Private Range Queries with Correlated Input Perturbation","abstract":"This work proposes a class of locally differentially private mechanisms for linear queries, in particular range queries, that leverages correlated input perturbation to simultaneously achieve unbiasedness, consistency, statistical transparency, and control over utility requirements in terms of accuracy targets expressed either in certain query margins or as implied by the hierarchical database structure. The proposed Cascade Sampling algorithm instantiates the mechanism exactly and efficiently. Our bounds show that we obtain near-optimal utility while being empirically competitive against output perturbation methods.","sentences":["This work proposes a class of locally differentially private mechanisms for linear queries, in particular range queries, that leverages correlated input perturbation to simultaneously achieve unbiasedness, consistency, statistical transparency, and control over utility requirements in terms of accuracy targets expressed either in certain query margins or as implied by the hierarchical database structure.","The proposed Cascade Sampling algorithm instantiates the mechanism exactly and efficiently.","Our bounds show that we obtain near-optimal utility while being empirically competitive against output perturbation methods."],"url":"http://arxiv.org/abs/2402.07066v1","category":"cs.CR"}
{"created":"2024-02-10 22:25:02","title":"Spectral convergence of a semi-discretized numerical system for the Boltzmann equation with uncertainties","abstract":"In this paper, we study the Boltzmann equation with uncertainties and prove that the spectral convergence of the semi-discretized numerical system holds in a combined velocity and random space, where the Fourier-spectral method is applied for approximation in the velocity space whereas the generalized polynomial chaos (gPC)-based stochastic Galerkin (SG) method is employed to discretize the random variable. Our proof is based on a delicate energy estimate for showing the well-posedness of the numerical solution as well as a rigorous control of its negative part in our well-designed functional space that involves high-order derivatives of both the velocity and random variables. This paper rigorously justifies the statement proposed in [Remark 4.4, J. Hu and S. Jin, J. Comput. Phys., 315 (2016), pp. 150-168].","sentences":["In this paper, we study the Boltzmann equation with uncertainties and prove that the spectral convergence of the semi-discretized numerical system holds in a combined velocity and random space, where the Fourier-spectral method is applied for approximation in the velocity space whereas the generalized polynomial chaos (gPC)-based stochastic Galerkin (SG) method is employed to discretize the random variable.","Our proof is based on a delicate energy estimate for showing the well-posedness of the numerical solution as well as a rigorous control of its negative part in our well-designed functional space that involves high-order derivatives of both the velocity and random variables.","This paper rigorously justifies the statement proposed in [Remark 4.4, J. Hu and S. Jin, J. Comput.","Phys., 315 (2016), pp.","150-168]."],"url":"http://arxiv.org/abs/2402.07060v1","category":"math.NA"}
{"created":"2024-02-10 21:58:42","title":"Optimization of Super-Directive Linear Arrays with Differential Evolution for High Realized Gain","abstract":"Due to the low impedance and high feeding currents, it is naturally challenging to design super-directive antenna arrays that perfectly match the feed line, and this becomes almost impossible as the number of elements increases. In this paper, we assert that it is crucial to consider the trade-off between directivity and overall efficiency (to achieve high realized gain) before employing super-directive arrays in real-world applications. Given this trade-off (high directivity and low mismatch for high realized gain), a 4-element dipole array (unit array) is optimized using the differential evolution (DE) algorithm. Then, the performance of the unit array in subarray configuration scenarios is analyzed. Finally, the obtained parameters are verified using the CST full-wave simulation software. The results clearly indicate that the proposed unit array is a strong candidate for dense array applications, particularly in the context of massive multiple-input multiple-output (MIMO), thanks to its notable high gain and efficiency.","sentences":["Due to the low impedance and high feeding currents, it is naturally challenging to design super-directive antenna arrays that perfectly match the feed line, and this becomes almost impossible as the number of elements increases.","In this paper, we assert that it is crucial to consider the trade-off between directivity and overall efficiency (to achieve high realized gain) before employing super-directive arrays in real-world applications.","Given this trade-off (high directivity and low mismatch for high realized gain), a 4-element dipole array (unit array) is optimized using the differential evolution (DE) algorithm.","Then, the performance of the unit array in subarray configuration scenarios is analyzed.","Finally, the obtained parameters are verified using the CST full-wave simulation software.","The results clearly indicate that the proposed unit array is a strong candidate for dense array applications, particularly in the context of massive multiple-input multiple-output (MIMO), thanks to its notable high gain and efficiency."],"url":"http://arxiv.org/abs/2402.07055v1","category":"eess.SP"}
{"created":"2024-02-10 21:53:06","title":"Certified homotopy tracking using the Krawczyk method","abstract":"We revisit the problem of certifying the correctness of approximate solution paths computed by numerical homotopy continuation methods. We propose a conceptually simple approach based on a parametric variant of the Krawczyk method from interval arithmetic. Unlike most previous methods for certified path-tracking, our approach is applicable in the general setting of parameter homotopies commonly used to solve polynomial systems of equations. We also describe a novel preconditioning strategy and give theoretical correctness and termination results. Experiments using a preliminary implementation of the method indicate that our approach is competitive with specialized methods appearing previously in the literature, in spite of our more general setting.","sentences":["We revisit the problem of certifying the correctness of approximate solution paths computed by numerical homotopy continuation methods.","We propose a conceptually simple approach based on a parametric variant of the Krawczyk method from interval arithmetic.","Unlike most previous methods for certified path-tracking, our approach is applicable in the general setting of parameter homotopies commonly used to solve polynomial systems of equations.","We also describe a novel preconditioning strategy and give theoretical correctness and termination results.","Experiments using a preliminary implementation of the method indicate that our approach is competitive with specialized methods appearing previously in the literature, in spite of our more general setting."],"url":"http://arxiv.org/abs/2402.07053v1","category":"math.NA"}
{"created":"2024-02-10 21:24:32","title":"Effects of Strong Capacitive Coupling Between Meta-Atoms in rf SQUID Metamaterials","abstract":"We consider, for the first time, the effects of strong capacitive and inductive coupling between radio frequency Superconducting Quantum Interference Devices (rf SQUIDs) in an overlapping metamaterial geometry when driven by rf flux at and near their self-resonant frequencies. The equations of motion containing the gauge-invariant phases on the Josephson junctions in each SQUID are set up and solved which include the high-frequency displacement currents through capacitive overlap between the wiring of SQUID loops. We begin by modeling two overlapping SQUIDs and studying the response in both the linear and nonlinear high-frequency driving limits. By exploring a sequence of more and more complicated arrays, the formalism is eventually extended to the $N\\times N \\times 2$ overlapping metamaterial array, where we develop an understanding of the many ($8N^2-8N+3$) resulting resonant modes in terms of three classes of resonances. The capacitive coupling gives rise to qualitatively new self-resonant response of rf SQUID metamaterials, and is demonstrated through analytical theory, numerical modeling, and experiment in the 10-30 GHz range on capacitively and inductively coupled rf SQUID metamaterials.","sentences":["We consider, for the first time, the effects of strong capacitive and inductive coupling between radio frequency Superconducting Quantum Interference Devices (rf SQUIDs) in an overlapping metamaterial geometry when driven by rf flux at and near their self-resonant frequencies.","The equations of motion containing the gauge-invariant phases on the Josephson junctions in each SQUID are set up and solved which include the high-frequency displacement currents through capacitive overlap between the wiring of SQUID loops.","We begin by modeling two overlapping SQUIDs and studying the response in both the linear and nonlinear high-frequency driving limits.","By exploring a sequence of more and more complicated arrays, the formalism is eventually extended to the $N\\times N \\times 2$ overlapping metamaterial array, where we develop an understanding of the many ($8N^2-8N+3$) resulting resonant modes in terms of three classes of resonances.","The capacitive coupling gives rise to qualitatively new self-resonant response of rf SQUID metamaterials, and is demonstrated through analytical theory, numerical modeling, and experiment in the 10-30 GHz range on capacitively and inductively coupled rf SQUID metamaterials."],"url":"http://arxiv.org/abs/2402.07044v1","category":"cond-mat.supr-con"}
{"created":"2024-02-10 20:25:43","title":"Unified Inverse Dynamics of Modular Serial Mechanical Systems with Application to Soft Robotics","abstract":"The robotic field has been witnessing a progressive departure from classic robotic systems composed of serial/stiff links interconnected by simple rigid joints. Novel robotic concepts, e.g., soft robots, often maintain a series-like structure, but their mechanical modules exhibit complex and unconventional articulation patterns. Research in efficient recursive formulations of the dynamic models for subclasses of these systems has been extremely active in the past decade. Yet, as of today, no single recursive inverse dynamics algorithm can describe the behavior of all these systems. This paper addresses this challenge by proposing a new iterative formulation based on Kane equations. Its computational complexity is optimal, i.e., linear with the number of modules. While the proposed formulation is not claimed to be necessarily more efficient than state-of-the-art techniques for specific subclasses of robots, we illustrate its usefulness in the modeling of different complex systems. We propose two new models of soft robots: (i) a class of pneumatically actuated soft arms that deform along their cross-sectional area, and (ii) a piecewise strain model with Gaussian functions.","sentences":["The robotic field has been witnessing a progressive departure from classic robotic systems composed of serial/stiff links interconnected by simple rigid joints.","Novel robotic concepts, e.g., soft robots, often maintain a series-like structure, but their mechanical modules exhibit complex and unconventional articulation patterns.","Research in efficient recursive formulations of the dynamic models for subclasses of these systems has been extremely active in the past decade.","Yet, as of today, no single recursive inverse dynamics algorithm can describe the behavior of all these systems.","This paper addresses this challenge by proposing a new iterative formulation based on Kane equations.","Its computational complexity is optimal, i.e., linear with the number of modules.","While the proposed formulation is not claimed to be necessarily more efficient than state-of-the-art techniques for specific subclasses of robots, we illustrate its usefulness in the modeling of different complex systems.","We propose two new models of soft robots: (i) a class of pneumatically actuated soft arms that deform along their cross-sectional area, and (ii) a piecewise strain model with Gaussian functions."],"url":"http://arxiv.org/abs/2402.07037v1","category":"cs.RO"}
{"created":"2024-02-10 20:21:35","title":"Sachs equations and plane waves, I: Rosen universes","abstract":"This article, the first in a series, analyzes the general theory of plane wave spacetimes. Following Dmitri Aleekseevsky, these are defined as spacetimes admitting a group of dilations leaving invariant a smooth curve. If this curve is specified as part of the structure, the spacetime is termed a Penrose limit, whose theory was developed first by Roger Penrose. The main result is that every plane wave is a Rosen universe, a generalization of the smooth metrics of Albert Einstein and Nathan Rosen, allowing for certain isolated co-ordinate singularities; the latter are characterized. We conclude with an extended example, using the techniques developed in the article to associate a vacuum plane wave in four dimensions to any hyperbolic billiard trajectory.","sentences":["This article, the first in a series, analyzes the general theory of plane wave spacetimes.","Following Dmitri Aleekseevsky, these are defined as spacetimes admitting a group of dilations leaving invariant a smooth curve.","If this curve is specified as part of the structure, the spacetime is termed a Penrose limit, whose theory was developed first by Roger Penrose.","The main result is that every plane wave is a Rosen universe, a generalization of the smooth metrics of Albert Einstein and Nathan Rosen, allowing for certain isolated co-ordinate singularities; the latter are characterized.","We conclude with an extended example, using the techniques developed in the article to associate a vacuum plane wave in four dimensions to any hyperbolic billiard trajectory."],"url":"http://arxiv.org/abs/2402.07036v1","category":"gr-qc"}
{"created":"2024-02-10 19:12:31","title":"Generalization Error of Graph Neural Networks in the Mean-field Regime","abstract":"This work provides a theoretical framework for assessing the generalization error of graph classification tasks via graph neural networks in the over-parameterized regime, where the number of parameters surpasses the quantity of data points. We explore two widely utilized types of graph neural networks: graph convolutional neural networks and message passing graph neural networks. Prior to this study, existing bounds on the generalization error in the over-parametrized regime were uninformative, limiting our understanding of over-parameterized network performance. Our novel approach involves deriving upper bounds within the mean-field regime for evaluating the generalization error of these graph neural networks. We establish upper bounds with a convergence rate of $O(1/n)$, where $n$ is the number of graph samples. These upper bounds offer a theoretical assurance of the networks' performance on unseen data in the challenging over-parameterized regime and overall contribute to our understanding of their performance.","sentences":["This work provides a theoretical framework for assessing the generalization error of graph classification tasks via graph neural networks in the over-parameterized regime, where the number of parameters surpasses the quantity of data points.","We explore two widely utilized types of graph neural networks: graph convolutional neural networks and message passing graph neural networks.","Prior to this study, existing bounds on the generalization error in the over-parametrized regime were uninformative, limiting our understanding of over-parameterized network performance.","Our novel approach involves deriving upper bounds within the mean-field regime for evaluating the generalization error of these graph neural networks.","We establish upper bounds with a convergence rate of $O(1/n)$, where $n$ is the number of graph samples.","These upper bounds offer a theoretical assurance of the networks' performance on unseen data in the challenging over-parameterized regime and overall contribute to our understanding of their performance."],"url":"http://arxiv.org/abs/2402.07025v1","category":"stat.ML"}
{"created":"2024-02-10 18:28:52","title":"Space-time shape optimization of rotating electric machines","abstract":"This article is devoted to the shape optimization of the internal structure of an electric motor, and more precisely of the arrangement of air and ferromagnetic material inside the rotor part with the aim to increase the torque of the machine. The governing physical problem is the time-dependent, non linear magneto-quasi-static version of Maxwell's equations. This multiphase problem can be reformulated on a 2d section of the real cylindrical 3d configuration; however, due to the rotation of the machine, the geometry of the various material phases at play (the ferromagnetic material, the permanent magnets, air, etc.) undergoes a prescribed motion over the considered time period. This original setting raises a number of issues. From the theoretical viewpoint, we prove the well-posedness of this unusual non linear evolution problem featuring a moving geometry. We then calculate the shape derivative of a performance criterion depending on the shape of the ferromagnetic phase via the corresponding magneto-quasi-static potential. Our numerical framework to address this problem is based on a shape gradient algorithm. The non linear time periodic evolution problems for the magneto-quasi-static potential is solved in the time domain, with a Newton-Raphson method. The discretization features a space-time finite element method, applied on a precise, meshed representation of the space-time region of interest, which encloses a body-fitted representation of the various material phases of the motor at all the considered stages of the time period. After appraising the efficiency of our numerical framework on an academic problem, we present a quite realistic example of optimal design of the ferromagnetic phase of the rotor of an electric machine.","sentences":["This article is devoted to the shape optimization of the internal structure of an electric motor, and more precisely of the arrangement of air and ferromagnetic material inside the rotor part with the aim to increase the torque of the machine.","The governing physical problem is the time-dependent, non linear magneto-quasi-static version of Maxwell's equations.","This multiphase problem can be reformulated on a 2d section of the real cylindrical 3d configuration; however, due to the rotation of the machine, the geometry of the various material phases at play (the ferromagnetic material, the permanent magnets, air, etc.) undergoes a prescribed motion over the considered time period.","This original setting raises a number of issues.","From the theoretical viewpoint, we prove the well-posedness of this unusual non linear evolution problem featuring a moving geometry.","We then calculate the shape derivative of a performance criterion depending on the shape of the ferromagnetic phase via the corresponding magneto-quasi-static potential.","Our numerical framework to address this problem is based on a shape gradient algorithm.","The non linear time periodic evolution problems for the magneto-quasi-static potential is solved in the time domain, with a Newton-Raphson method.","The discretization features a space-time finite element method, applied on a precise, meshed representation of the space-time region of interest, which encloses a body-fitted representation of the various material phases of the motor at all the considered stages of the time period.","After appraising the efficiency of our numerical framework on an academic problem, we present a quite realistic example of optimal design of the ferromagnetic phase of the rotor of an electric machine."],"url":"http://arxiv.org/abs/2402.07017v1","category":"math.OC"}
{"created":"2024-02-10 18:20:51","title":"Boosting energy levels in graphene magnetic quantum dots through magnetic flux and inhomogeneous gap","abstract":"We investigate the effects of a magnetic flux and an inhomogeneous gap on the energy spectrum of a graphene magnetic quantum dot (GMQD). We employ the Dirac equation within the infinite mass framework to obtain analytical eigenspinor expressions. Applying boundary conditions yields an energy spectrum equation in terms of system parameters like radius, field, energy, flux, and gap. In the infinite limit, we recover Landau levels for graphene in a magnetic field. We demonstrate that the energy spectrum significantly increases in the presence of flux and a gap inside the GMQD, which lengthens the lifetime of trapped electron states. We show that higher flux also produces new Landau levels of negative angular momentum. Meanwhile, we find that the gap widens the separation between the electron and hole energy bands. Therefore, we conclude that the flux and gap regulate electron mobility via influencing confinement and extending quasi-bound state duration, as verified through studying radial probability.","sentences":["We investigate the effects of a magnetic flux and an inhomogeneous gap on the energy spectrum of a graphene magnetic quantum dot (GMQD).","We employ the Dirac equation within the infinite mass framework to obtain analytical eigenspinor expressions.","Applying boundary conditions yields an energy spectrum equation in terms of system parameters like radius, field, energy, flux, and gap.","In the infinite limit, we recover Landau levels for graphene in a magnetic field.","We demonstrate that the energy spectrum significantly increases in the presence of flux and a gap inside the GMQD, which lengthens the lifetime of trapped electron states.","We show that higher flux also produces new Landau levels of negative angular momentum.","Meanwhile, we find that the gap widens the separation between the electron and hole energy bands.","Therefore, we conclude that the flux and gap regulate electron mobility via influencing confinement and extending quasi-bound state duration, as verified through studying radial probability."],"url":"http://arxiv.org/abs/2402.07014v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-10 18:00:21","title":"Nonlinear electro-elastic finite element analysis with neural network constitutive models","abstract":"In the present work, the applicability of physics-augmented neural network (PANN) constitutive models for complex electro-elastic finite element analysis is demonstrated. For the investigations, PANN models for electro-elastic material behavior at finite deformations are calibrated to different synthetically generated datasets, including an analytical isotropic potential, a homogenised rank-one laminate, and a homogenised metamaterial with a spherical inclusion. Subsequently, boundary value problems inspired by engineering applications of composite electro-elastic materials are considered. Scenarios with large electrically induced deformations and instabilities are particularly challenging and thus necessitate extensive investigations of the PANN constitutive models in the context of finite element analyses. First of all, an excellent prediction quality of the model is required for very general load cases occurring in the simulation. Furthermore, simulation of large deformations and instabilities poses challenges on the stability of the numerical solver, which is closely related to the constitutive model. In all cases studied, the PANN models yield excellent prediction qualities and a stable numerical behavior even in highly nonlinear scenarios. This can be traced back to the PANN models excellent performance in learning both the first and second derivatives of the ground truth electro-elastic potentials, even though it is only calibrated on the first derivatives. Overall, this work demonstrates the applicability of PANN constitutive models for the efficient and robust simulation of engineering applications of composite electro-elastic materials.","sentences":["In the present work, the applicability of physics-augmented neural network (PANN) constitutive models for complex electro-elastic finite element analysis is demonstrated.","For the investigations, PANN models for electro-elastic material behavior at finite deformations are calibrated to different synthetically generated datasets, including an analytical isotropic potential, a homogenised rank-one laminate, and a homogenised metamaterial with a spherical inclusion.","Subsequently, boundary value problems inspired by engineering applications of composite electro-elastic materials are considered.","Scenarios with large electrically induced deformations and instabilities are particularly challenging and thus necessitate extensive investigations of the PANN constitutive models in the context of finite element analyses.","First of all, an excellent prediction quality of the model is required for very general load cases occurring in the simulation.","Furthermore, simulation of large deformations and instabilities poses challenges on the stability of the numerical solver, which is closely related to the constitutive model.","In all cases studied, the PANN models yield excellent prediction qualities and a stable numerical behavior even in highly nonlinear scenarios.","This can be traced back to the PANN models excellent performance in learning both the first and second derivatives of the ground truth electro-elastic potentials, even though it is only calibrated on the first derivatives.","Overall, this work demonstrates the applicability of PANN constitutive models for the efficient and robust simulation of engineering applications of composite electro-elastic materials."],"url":"http://arxiv.org/abs/2402.07007v1","category":"cs.CE"}
{"created":"2024-02-10 17:33:47","title":"Double-Mode RR Lyrae Stars Observed by K2: Analysis of High-Precision Kepler Photometry","abstract":"The results of a Fourier analysis of high-precision Kepler photometry of 75 double-mode RR~Lyrae (RRd) stars observed during NASA's K2 Mission (2014-18) are presented. Seventy-two of the stars are `classical' RRd (cRRd) stars lying along a well-defined curve in the Petersen diagram and showing no evidence of Blazhko modulations. The remaining three stars are `anomalous' RRd (aRRd) stars that lie well below the cRRd curve in the Petersen diagram. These stars have larger fundamental-mode amplitudes than first-overtone amplitudes and exhibit Blazhko variations. Period-amplitude relations for the individual pulsation components of the cRRd stars are examined, as well as correlations involving Fourier phase-difference and amplitude-ratio parameters that characterize the light curves for the two radial modes. A simple statistical model relating the fundamental (P0) and first-overtone (P1) periods to [Fe/H] provides insight into the functional form of the Petersen diagram. A calibration equation for estimating [Fe/H]phot abundances of `classical' RRd stars is derived by inverting the model and using 211 field and 57 globular cluster cRRd stars with spectroscopic metallicities to estimate the model coefficients. The equation is used to obtain [Fe/H]phot for the full sample of 72 K2 cRRd stars and for 2130 cRRd stars observed by the ESA Gaia Mission. Of the 49 K2 cRRd stars that are in the Gaia DR3 catalogue only five were found to be correctly classified, the remainder having been misclassified `RRc' or `RRab'.","sentences":["The results of a Fourier analysis of high-precision Kepler photometry of 75 double-mode RR~Lyrae (RRd) stars observed during NASA's K2 Mission (2014-18) are presented.","Seventy-two of the stars are `classical' RRd (cRRd) stars lying along a well-defined curve in the Petersen diagram and showing no evidence of Blazhko modulations.","The remaining three stars are `anomalous' RRd (aRRd) stars that lie well below the cRRd curve in the Petersen diagram.","These stars have larger fundamental-mode amplitudes than first-overtone amplitudes and exhibit Blazhko variations.","Period-amplitude relations for the individual pulsation components of the cRRd stars are examined, as well as correlations involving Fourier phase-difference and amplitude-ratio parameters that characterize the light curves for the two radial modes.","A simple statistical model relating the fundamental (P0) and first-overtone (P1) periods to [Fe/H] provides insight into the functional form of the Petersen diagram.","A calibration equation for estimating [Fe/H]phot abundances of `classical' RRd stars is derived by inverting the model and using 211 field and 57 globular cluster cRRd stars with spectroscopic metallicities to estimate the model coefficients.","The equation is used to obtain [Fe/H]phot for the full sample of 72 K2 cRRd stars and for 2130 cRRd stars observed by the ESA Gaia Mission.","Of the 49 K2 cRRd stars that are in the Gaia DR3 catalogue only five were found to be correctly classified, the remainder having been misclassified `RRc' or `RRab'."],"url":"http://arxiv.org/abs/2402.06997v1","category":"astro-ph.SR"}
{"created":"2024-02-10 17:17:12","title":"Self-induced transparency of long water waves over bathymetry: the dispersive shock mechanism","abstract":"Dispersive shock waves (DSW) are a salient feature of long water waves often observed in tidal bores and tsunami/meteotsunami contexts. Their interaction with bathymetry is poorly understood. The shoreline hazard from tsunamis and meteotsunamis critically depends on the fraction of incoming energy flux transmitted across the shallow nearshore shelf. Here, by considering nonlinear dynamics of waves over variable depth within the framework of the Boussinesq equations we show that the transmitted energy flux fraction can strongly depend on the initial amplitude of the incoming wave and the distance it travels. The phenomenon is similar to self-induced transparency in nonlinear optics: small amplitude waves are reflected by bathymetry inhomogeneity, while a larger amplitude ones pass through. The mechanism of self-induced transparency of long water waves can be explained as follows. In linear setting a bathymetry inhomogeneity, of length comparable to that of the incident wavelength, by transmitting high wavenumber components acts as a high-pass filter. The DSW evolution efficiently transfers wave energy into high wavenumber band, where reflection is negligible. By examining an idealized model of bathymetry we show that this is an order one effect and explore its dependence on parameters in the range relevant for meteotsunamis. The role of wave energy transfer into high wavenumber band owing to the growth of bound harmonics unrelated to the DSW was found to be small.","sentences":["Dispersive shock waves (DSW) are a salient feature of long water waves often observed in tidal bores and tsunami/meteotsunami contexts.","Their interaction with bathymetry is poorly understood.","The shoreline hazard from tsunamis and meteotsunamis critically depends on the fraction of incoming energy flux transmitted across the shallow nearshore shelf.","Here, by considering nonlinear dynamics of waves over variable depth within the framework of the Boussinesq equations we show that the transmitted energy flux fraction can strongly depend on the initial amplitude of the incoming wave and the distance it travels.","The phenomenon is similar to self-induced transparency in nonlinear optics: small amplitude waves are reflected by bathymetry inhomogeneity, while a larger amplitude ones pass through.","The mechanism of self-induced transparency of long water waves can be explained as follows.","In linear setting a bathymetry inhomogeneity, of length comparable to that of the incident wavelength, by transmitting high wavenumber components acts as a high-pass filter.","The DSW evolution efficiently transfers wave energy into high wavenumber band, where reflection is negligible.","By examining an idealized model of bathymetry we show that this is an order one effect and explore its dependence on parameters in the range relevant for meteotsunamis.","The role of wave energy transfer into high wavenumber band owing to the growth of bound harmonics unrelated to the DSW was found to be small."],"url":"http://arxiv.org/abs/2402.06995v1","category":"physics.flu-dyn"}
{"created":"2024-02-10 16:47:53","title":"Guided Sketch-Based Program Induction by Search Gradients","abstract":"Many tasks can be easily solved using machine learning techniques. However, some tasks cannot readily be solved using statistical models, requiring a symbolic approach instead. Program induction is one of the ways that such tasks can be solved by means of capturing an interpretable and generalizable algorithm through training. However, contemporary approaches to program induction are not sophisticated enough to readily be applied to various types of tasks as they tend to be formulated as a single, all-encompassing model, usually parameterized by neural networks. In an attempt to make program induction a viable solution for many scenarios, we propose a framework for learning parameterized programs via search gradients using evolution strategies. This formulation departs from traditional program induction as it allows for the programmer to impart task-specific code to the program 'sketch', while also enjoying the benefits of accelerated learning through end-to-end gradient-based optimization.","sentences":["Many tasks can be easily solved using machine learning techniques.","However, some tasks cannot readily be solved using statistical models, requiring a symbolic approach instead.","Program induction is one of the ways that such tasks can be solved by means of capturing an interpretable and generalizable algorithm through training.","However, contemporary approaches to program induction are not sophisticated enough to readily be applied to various types of tasks as they tend to be formulated as a single, all-encompassing model, usually parameterized by neural networks.","In an attempt to make program induction a viable solution for many scenarios, we propose a framework for learning parameterized programs via search gradients using evolution strategies.","This formulation departs from traditional program induction as it allows for the programmer to impart task-specific code to the program 'sketch', while also enjoying the benefits of accelerated learning through end-to-end gradient-based optimization."],"url":"http://arxiv.org/abs/2402.06990v1","category":"cs.LG"}
{"created":"2024-02-10 15:50:57","title":"Neural Rearrangement Planning for Object Retrieval from Confined Spaces Perceivable by Robot's In-hand RGB-D Sensor","abstract":"Rearrangement planning for object retrieval tasks from confined spaces is a challenging problem, primarily due to the lack of open space for robot motion and limited perception. Several traditional methods exist to solve object retrieval tasks, but they require overhead cameras for perception and a time-consuming exhaustive search to find a solution and often make unrealistic assumptions, such as having identical, simple geometry objects in the environment. This paper presents a neural object retrieval framework that efficiently performs rearrangement planning of unknown, arbitrary objects in confined spaces to retrieve the desired object using a given robot grasp. Our method actively senses the environment with the robot's in-hand camera. It then selects and relocates the non-target objects such that they do not block the robot path homotopy to the target object, thus also aiding an underlying path planner in quickly finding robot motion sequences. Furthermore, we demonstrate our framework in challenging scenarios, including real-world cabinet-like environments with arbitrary household objects. The results show that our framework achieves the best performance among all presented methods and is, on average, two orders of magnitude computationally faster than the best-performing baselines.","sentences":["Rearrangement planning for object retrieval tasks from confined spaces is a challenging problem, primarily due to the lack of open space for robot motion and limited perception.","Several traditional methods exist to solve object retrieval tasks, but they require overhead cameras for perception and a time-consuming exhaustive search to find a solution and often make unrealistic assumptions, such as having identical, simple geometry objects in the environment.","This paper presents a neural object retrieval framework that efficiently performs rearrangement planning of unknown, arbitrary objects in confined spaces to retrieve the desired object using a given robot grasp.","Our method actively senses the environment with the robot's in-hand camera.","It then selects and relocates the non-target objects such that they do not block the robot path homotopy to the target object, thus also aiding an underlying path planner in quickly finding robot motion sequences.","Furthermore, we demonstrate our framework in challenging scenarios, including real-world cabinet-like environments with arbitrary household objects.","The results show that our framework achieves the best performance among all presented methods and is, on average, two orders of magnitude computationally faster than the best-performing baselines."],"url":"http://arxiv.org/abs/2402.06976v1","category":"cs.RO"}
{"created":"2024-02-10 15:15:44","title":"Index theory for H-elliptic and transversally H-elliptic operators from KK theoretic viewpoint","abstract":"This study provides a comprehensive definition for transversally H-elliptic operators and explores the index theorem of H-elliptic and transversally H-elliptic operators through the lens of KK-theory, employing Kasparov's approach. Furthermore, our analysis incorporates a detailed examination of conditions, specifically delving into the Fourier transform of the nilpotent group C^{\\ast}-algebra.","sentences":["This study provides a comprehensive definition for transversally H-elliptic operators and explores the index theorem of H-elliptic and transversally H-elliptic operators through the lens of KK-theory, employing Kasparov's approach.","Furthermore, our analysis incorporates a detailed examination of conditions, specifically delving into the Fourier transform of the nilpotent group C^{\\ast}-algebra."],"url":"http://arxiv.org/abs/2402.06970v1","category":"math.KT"}
{"created":"2024-02-10 14:45:23","title":"DeepCover: Advancing RNN Test Coverage and Online Error Prediction using State Machine Extraction","abstract":"Recurrent neural networks (RNNs) have emerged as powerful tools for processing sequential data in various fields, including natural language processing and speech recognition. However, the lack of explainability in RNN models has limited their interpretability, posing challenges in understanding their internal workings. To address this issue, this paper proposes a methodology for extracting a state machine (SM) from an RNN-based model to provide insights into its internal function. The proposed SM extraction algorithm was assessed using four newly proposed metrics: Purity, Richness, Goodness, and Scale. The proposed methodology along with its assessment metrics contribute to increasing explainability in RNN models by providing a clear representation of their internal decision making process through the extracted SM. In addition to improving the explainability of RNNs, the extracted SM can be used to advance testing and and monitoring of the primary RNN-based model. To enhance RNN testing, we introduce six model coverage criteria based on the extracted SM, serving as metrics for evaluating the effectiveness of test suites designed to analyze the primary model. We also propose a tree-based model to predict the error probability of the primary model for each input based on the extracted SM. We evaluated our proposed online error prediction approach using the MNIST dataset and Mini Speech Commands dataset, achieving an area under the curve (AUC) exceeding 80\\% for the receiver operating characteristic (ROC) chart.","sentences":["Recurrent neural networks (RNNs) have emerged as powerful tools for processing sequential data in various fields, including natural language processing and speech recognition.","However, the lack of explainability in RNN models has limited their interpretability, posing challenges in understanding their internal workings.","To address this issue, this paper proposes a methodology for extracting a state machine (SM) from an RNN-based model to provide insights into its internal function.","The proposed SM extraction algorithm was assessed using four newly proposed metrics: Purity, Richness, Goodness, and Scale.","The proposed methodology along with its assessment metrics contribute to increasing explainability in RNN models by providing a clear representation of their internal decision making process through the extracted SM.","In addition to improving the explainability of RNNs, the extracted SM can be used to advance testing and and monitoring of the primary RNN-based model.","To enhance RNN testing, we introduce six model coverage criteria based on the extracted SM, serving as metrics for evaluating the effectiveness of test suites designed to analyze the primary model.","We also propose a tree-based model to predict the error probability of the primary model for each input based on the extracted SM.","We evaluated our proposed online error prediction approach using the MNIST dataset and Mini Speech Commands dataset, achieving an area under the curve (AUC) exceeding 80\\% for the receiver operating characteristic (ROC) chart."],"url":"http://arxiv.org/abs/2402.06966v1","category":"cs.LG"}
{"created":"2024-02-10 13:15:50","title":"Numerical Solution of Nonclassical Boundary Value Problems","abstract":"We provide a new approach to obtain solutions of certain evolution equations set in a Banach space and equipped with nonlocal boundary conditions. From this approach we derive a family of numerical schemes for the approximation of the solutions. We show by numerical tests that these schemes are numerically robust and computationally efficient.","sentences":["We provide a new approach to obtain solutions of certain evolution equations set in a Banach space and equipped with nonlocal boundary conditions.","From this approach we derive a family of numerical schemes for the approximation of the solutions.","We show by numerical tests that these schemes are numerically robust and computationally efficient."],"url":"http://arxiv.org/abs/2402.06943v1","category":"math.NA"}
{"created":"2024-02-12 17:52:05","title":"Do Membership Inference Attacks Work on Large Language Models?","abstract":"Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model's training data. Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of large language models (LLMs). We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members. We identify specific settings where LLMs have been shown to be vulnerable to membership inference and show that the apparent success in such settings can be attributed to a distribution shift, such as when members and non-members are drawn from the seemingly identical domain but with different temporal ranges. We release our code and data as a unified benchmark package that includes all existing MIAs, supporting future work.","sentences":["Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model's training data.","Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of large language models (LLMs).","We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters.","We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains.","Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members.","We identify specific settings where LLMs have been shown to be vulnerable to membership inference and show that the apparent success in such settings can be attributed to a distribution shift, such as when members and non-members are drawn from the seemingly identical domain but with different temporal ranges.","We release our code and data as a unified benchmark package that includes all existing MIAs, supporting future work."],"url":"http://arxiv.org/abs/2402.07841v1","category":"cs.CL"}
{"created":"2024-02-12 17:07:40","title":"Protoplanet collisions: new scaling laws from SPH simulations","abstract":"One common approach for solving collisions between protoplanets in simulations of planet formation is to employ analytical scaling laws. The most widely used one was developed by Leinhardt & Stewart (2012) from a catalog of ~ 180 N-body simulations of rubble-pile collisions. In this work, we use a new catalogue of more than 20,000 SPH simulations to test the validity and the prediction capability of Leinhardt & Stewart (2012) scaling laws. We find that these laws overestimate the fragmentation efficiency in the merging regime and they are not able to properly reproduce the collision outcomes in the super-catastrophic regime. In the merging regime, we also notice a significant dependence between the collision outcome, in terms of the largest remnant mass, and the relative mass of the colliding protoplanets. Here, we present a new set of scaling laws that are able to better predict the collision outcome in all regimes and it is also able to reproduce the observed dependence on the mass ratio. We compare our new scaling laws against a machine learning approach and obtain similar prediction efficiency.","sentences":["One common approach for solving collisions between protoplanets in simulations of planet formation is to employ analytical scaling laws.","The most widely used one was developed by Leinhardt & Stewart (2012) from a catalog of ~ 180 N-body simulations of rubble-pile collisions.","In this work, we use a new catalogue of more than 20,000 SPH simulations to test the validity and the prediction capability of Leinhardt & Stewart (2012) scaling laws.","We find that these laws overestimate the fragmentation efficiency in the merging regime and they are not able to properly reproduce the collision outcomes in the super-catastrophic regime.","In the merging regime, we also notice a significant dependence between the collision outcome, in terms of the largest remnant mass, and the relative mass of the colliding protoplanets.","Here, we present a new set of scaling laws that are able to better predict the collision outcome in all regimes and it is also able to reproduce the observed dependence on the mass ratio.","We compare our new scaling laws against a machine learning approach and obtain similar prediction efficiency."],"url":"http://arxiv.org/abs/2402.07803v1","category":"astro-ph.EP"}
{"created":"2024-02-12 16:30:41","title":"Text Detoxification as Style Transfer in English and Hindi","abstract":"This paper focuses on text detoxification, i.e., automatically converting toxic text into non-toxic text. This task contributes to safer and more respectful online communication and can be considered a Text Style Transfer (TST) task, where the text style changes while its content is preserved. We present three approaches: knowledge transfer from a similar task, multi-task learning approach, combining sequence-to-sequence modeling with various toxicity classification tasks, and, delete and reconstruct approach. To support our research, we utilize a dataset provided by Dementieva et al.(2021), which contains multiple versions of detoxified texts corresponding to toxic texts. In our experiments, we selected the best variants through expert human annotators, creating a dataset where each toxic sentence is paired with a single, appropriate detoxified version. Additionally, we introduced a small Hindi parallel dataset, aligning with a part of the English dataset, suitable for evaluation purposes. Our results demonstrate that our approach effectively balances text detoxication while preserving the actual content and maintaining fluency.","sentences":["This paper focuses on text detoxification, i.e., automatically converting toxic text into non-toxic text.","This task contributes to safer and more respectful online communication and can be considered a Text Style Transfer (TST) task, where the text style changes while its content is preserved.","We present three approaches: knowledge transfer from a similar task, multi-task learning approach, combining sequence-to-sequence modeling with various toxicity classification tasks, and, delete and reconstruct approach.","To support our research, we utilize a dataset provided by Dementieva et al.(2021), which contains multiple versions of detoxified texts corresponding to toxic texts.","In our experiments, we selected the best variants through expert human annotators, creating a dataset where each toxic sentence is paired with a single, appropriate detoxified version.","Additionally, we introduced a small Hindi parallel dataset, aligning with a part of the English dataset, suitable for evaluation purposes.","Our results demonstrate that our approach effectively balances text detoxication while preserving the actual content and maintaining fluency."],"url":"http://arxiv.org/abs/2402.07767v1","category":"cs.CL"}
{"created":"2024-02-12 14:48:31","title":"Contrastive Multiple Instance Learning for Weakly Supervised Person ReID","abstract":"The acquisition of large-scale, precisely labeled datasets for person re-identification (ReID) poses a significant challenge. Weakly supervised ReID has begun to address this issue, although its performance lags behind fully supervised methods. In response, we introduce Contrastive Multiple Instance Learning (CMIL), a novel framework tailored for more effective weakly supervised ReID. CMIL distinguishes itself by requiring only a single model and no pseudo labels while leveraging contrastive losses -- a technique that has significantly enhanced traditional ReID performance yet is absent in all prior MIL-based approaches. Through extensive experiments and analysis across three datasets, CMIL not only matches state-of-the-art performance on the large-scale SYSU-30k dataset with fewer assumptions but also consistently outperforms all baselines on the WL-market1501 and Weakly Labeled MUddy racer re-iDentification dataset (WL-MUDD) datasets. We introduce and release the WL-MUDD dataset, an extension of the MUDD dataset featuring naturally occurring weak labels from the real-world application at PerformancePhoto.co. All our code and data are accessible at https://drive.google.com/file/d/1rjMbWB6m-apHF3Wg_cfqc8QqKgQ21AsT/view?usp=drive_link.","sentences":["The acquisition of large-scale, precisely labeled datasets for person re-identification (ReID) poses a significant challenge.","Weakly supervised ReID has begun to address this issue, although its performance lags behind fully supervised methods.","In response, we introduce Contrastive Multiple Instance Learning (CMIL), a novel framework tailored for more effective weakly supervised ReID.","CMIL distinguishes itself by requiring only a single model and no pseudo labels while leveraging contrastive losses -- a technique that has significantly enhanced traditional ReID performance yet is absent in all prior MIL-based approaches.","Through extensive experiments and analysis across three datasets, CMIL not only matches state-of-the-art performance on the large-scale SYSU-30k dataset with fewer assumptions but also consistently outperforms all baselines on the WL-market1501 and Weakly Labeled MUddy racer re-iDentification dataset (WL-MUDD) datasets.","We introduce and release the WL-MUDD dataset, an extension of the MUDD dataset featuring naturally occurring weak labels from the real-world application at PerformancePhoto.co.","All our code and data are accessible at https://drive.google.com/file/d/1rjMbWB6m-apHF3Wg_cfqc8QqKgQ21AsT/view?usp=drive_link."],"url":"http://arxiv.org/abs/2402.07685v1","category":"cs.CV"}
{"created":"2024-02-12 13:30:34","title":"A Flow-based Credibility Metric for Safety-critical Pedestrian Detection","abstract":"Safety is of utmost importance for perception in automated driving (AD). However, a prime safety concern in state-of-the art object detection is that standard evaluation schemes utilize safety-agnostic metrics to argue sufficient detection performance. Hence, it is imperative to leverage supplementary domain knowledge to accentuate safety-critical misdetections during evaluation tasks. To tackle the underspecification, this paper introduces a novel credibility metric, called c-flow, for pedestrian bounding boxes. To this end, c-flow relies on a complementary optical flow signal from image sequences and enhances the analyses of safety-critical misdetections without requiring additional labels. We implement and evaluate c-flow with a state-of-the-art pedestrian detector on a large AD dataset. Our analysis demonstrates that c-flow allows developers to identify safety-critical misdetections.","sentences":["Safety is of utmost importance for perception in automated driving (AD).","However, a prime safety concern in state-of-the art object detection is that standard evaluation schemes utilize safety-agnostic metrics to argue sufficient detection performance.","Hence, it is imperative to leverage supplementary domain knowledge to accentuate safety-critical misdetections during evaluation tasks.","To tackle the underspecification, this paper introduces a novel credibility metric, called c-flow, for pedestrian bounding boxes.","To this end, c-flow relies on a complementary optical flow signal from image sequences and enhances the analyses of safety-critical misdetections without requiring additional labels.","We implement and evaluate c-flow with a state-of-the-art pedestrian detector on a large AD dataset.","Our analysis demonstrates that c-flow allows developers to identify safety-critical misdetections."],"url":"http://arxiv.org/abs/2402.07642v1","category":"cs.CV"}
{"created":"2024-02-12 11:41:42","title":"Rethinking Scaling Laws for Learning in Strategic Environments","abstract":"The deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model$\\unicode{x2013}$and the more data one has access to$\\unicode{x2013}$the more one can improve performance. As models get deployed in a variety of real world scenarios, they inevitably face strategic environments. In this work, we consider the natural question of how the interplay of models and strategic interactions affects scaling laws. We find that strategic interactions can break the conventional view of scaling laws$\\unicode{x2013}$meaning that performance does not necessarily monotonically improve as models get larger and/ or more expressive (even with infinite data). We show the implications of this phenomenon in several contexts including strategic regression, strategic classification, and multi-agent reinforcement learning through examples of strategic environments in which$\\unicode{x2013}$by simply restricting the expressivity of one's model or policy class$\\unicode{x2013}$one can achieve strictly better equilibrium outcomes. Motivated by these examples, we then propose a new paradigm for model-selection in games wherein an agent seeks to choose amongst different model classes to use as their action set in a game.","sentences":["The deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model$\\unicode{x2013}$and the more data one has access to$\\unicode{x2013}$the more one can improve performance.","As models get deployed in a variety of real world scenarios, they inevitably face strategic environments.","In this work, we consider the natural question of how the interplay of models and strategic interactions affects scaling laws.","We find that strategic interactions can break the conventional view of scaling laws$\\unicode{x2013}$meaning that performance does not necessarily monotonically improve as models get larger and/ or more expressive (even with infinite data).","We show the implications of this phenomenon in several contexts including strategic regression, strategic classification, and multi-agent reinforcement learning through examples of strategic environments in which$\\unicode{x2013}$by simply restricting the expressivity of one's model or policy class$\\unicode{x2013}$one can achieve strictly better equilibrium outcomes.","Motivated by these examples, we then propose a new paradigm for model-selection in games wherein an agent seeks to choose amongst different model classes to use as their action set in a game."],"url":"http://arxiv.org/abs/2402.07588v1","category":"cs.GT"}
{"created":"2024-02-12 10:10:12","title":"ASAP-Repair: API-Specific Automated Program Repair Based on API Usage Graphs","abstract":"Modern software development relies on the reuse of code via Application Programming Interfaces (APIs). Such reuse relieves developers from learning and developing established algorithms and data structures anew, enabling them to focus on their problem at hand. However, there is also the risk of misusing an API due to a lack of understanding or proper documentation. While many techniques target API misuse detection, only limited efforts have been put into automatically repairing API misuses. In this paper, we present our advances on our technique API-Specific Automated Program Repair (ASAP-Repair). ASAP-Repair is intended to fix API misuses based on API Usage Graphs (AUGs) by leveraging API usage templates of state-of-the-art API misuse detectors. We demonstrate that ASAP-Repair is in principle applicable on an established API misuse dataset. Moreover, we discuss next steps and challenges to evolve ASAP-Repair towards a full-fledged Automatic Program Repair (APR) technique.","sentences":["Modern software development relies on the reuse of code via Application Programming Interfaces (APIs).","Such reuse relieves developers from learning and developing established algorithms and data structures anew, enabling them to focus on their problem at hand.","However, there is also the risk of misusing an API due to a lack of understanding or proper documentation.","While many techniques target API misuse detection, only limited efforts have been put into automatically repairing API misuses.","In this paper, we present our advances on our technique API-Specific Automated Program Repair (ASAP-Repair).","ASAP-Repair is intended to fix API misuses based on API Usage Graphs (AUGs) by leveraging API usage templates of state-of-the-art API misuse detectors.","We demonstrate that ASAP-Repair is in principle applicable on an established API misuse dataset.","Moreover, we discuss next steps and challenges to evolve ASAP-Repair towards a full-fledged Automatic Program Repair (APR) technique."],"url":"http://arxiv.org/abs/2402.07542v1","category":"cs.SE"}
{"created":"2024-02-12 09:10:24","title":"ClusterTabNet: Supervised clustering method for table detection and table structure recognition","abstract":"We present a novel deep-learning-based method to cluster words in documents which we apply to detect and recognize tables given the OCR output. We interpret table structure bottom-up as a graph of relations between pairs of words (belonging to the same row, column, header, as well as to the same table) and use a transformer encoder model to predict its adjacency matrix. We demonstrate the performance of our method on the PubTables-1M dataset as well as PubTabNet and FinTabNet datasets. Compared to the current state-of-the-art detection methods such as DETR and Faster R-CNN, our method achieves similar or better accuracy, while requiring a significantly smaller model.","sentences":["We present a novel deep-learning-based method to cluster words in documents which we apply to detect and recognize tables given the OCR output.","We interpret table structure bottom-up as a graph of relations between pairs of words (belonging to the same row, column, header, as well as to the same table) and use a transformer encoder model to predict its adjacency matrix.","We demonstrate the performance of our method on the PubTables-1M dataset as well as PubTabNet and FinTabNet datasets.","Compared to the current state-of-the-art detection methods such as DETR and Faster R-CNN, our method achieves similar or better accuracy, while requiring a significantly smaller model."],"url":"http://arxiv.org/abs/2402.07502v1","category":"cs.LG"}
{"created":"2024-02-12 07:37:19","title":"On the Distance from Calibration in Sequential Prediction","abstract":"We study a sequential binary prediction setting where the forecaster is evaluated in terms of the calibration distance, which is defined as the $L_1$ distance between the predicted values and the set of predictions that are perfectly calibrated in hindsight. This is analogous to a calibration measure recently proposed by B{\\l}asiok, Gopalan, Hu and Nakkiran (STOC 2023) for the offline setting. The calibration distance is a natural and intuitive measure of deviation from perfect calibration, and satisfies a Lipschitz continuity property which does not hold for many popular calibration measures, such as the $L_1$ calibration error and its variants.   We prove that there is a forecasting algorithm that achieves an $O(\\sqrt{T})$ calibration distance in expectation on an adversarially chosen sequence of $T$ binary outcomes. At the core of this upper bound is a structural result showing that the calibration distance is accurately approximated by the lower calibration distance, which is a continuous relaxation of the former. We then show that an $O(\\sqrt{T})$ lower calibration distance can be achieved via a simple minimax argument and a reduction to online learning on a Lipschitz class.   On the lower bound side, an $\\Omega(T^{1/3})$ calibration distance is shown to be unavoidable, even when the adversary outputs a sequence of independent random bits, and has an additional ability to early stop (i.e., to stop producing random bits and output the same bit in the remaining steps). Interestingly, without this early stopping, the forecaster can achieve a much smaller calibration distance of $\\mathrm{polylog}(T)$.","sentences":["We study a sequential binary prediction setting where the forecaster is evaluated in terms of the calibration distance, which is defined as the $L_1$ distance between the predicted values and the set of predictions that are perfectly calibrated in hindsight.","This is analogous to a calibration measure recently proposed by B{\\l}asiok, Gopalan, Hu and Nakkiran (STOC 2023) for the offline setting.","The calibration distance is a natural and intuitive measure of deviation from perfect calibration, and satisfies a Lipschitz continuity property which does not hold for many popular calibration measures, such as the $L_1$ calibration error and its variants.   ","We prove that there is a forecasting algorithm that achieves an $O(\\sqrt{T})$ calibration distance in expectation on an adversarially chosen sequence of $T$ binary outcomes.","At the core of this upper bound is a structural result showing that the calibration distance is accurately approximated by the lower calibration distance, which is a continuous relaxation of the former.","We then show that an $O(\\sqrt{T})$ lower calibration distance can be achieved via a simple minimax argument and a reduction to online learning on a Lipschitz class.   ","On the lower bound side, an $\\Omega(T^{1/3})$ calibration distance is shown to be unavoidable, even when the adversary outputs a sequence of independent random bits, and has an additional ability to early stop (i.e., to stop producing random bits and output the same bit in the remaining steps).","Interestingly, without this early stopping, the forecaster can achieve a much smaller calibration distance of $\\mathrm{polylog}(T)$."],"url":"http://arxiv.org/abs/2402.07458v1","category":"cs.LG"}
{"created":"2024-02-12 01:59:51","title":"Diff-RNTraj: A Structure-aware Diffusion Model for Road Network-constrained Trajectory Generation","abstract":"Trajectory data is essential for various applications as it records the movement of vehicles. However, publicly available trajectory datasets remain limited in scale due to privacy concerns, which hinders the development of trajectory data mining and trajectory-based applications. To address this issue, some methods for generating synthetic trajectories have been proposed to expand the scale of the dataset. However, all existing methods generate trajectories in the geographical coordinate system, which poses two limitations for their utilization in practical applications: 1) the inability to ensure that the generated trajectories are constrained on the road. 2) the lack of road-related information. In this paper, we propose a new problem to meet the practical application need, \\emph{i.e.}, road network-constrained trajectory (RNTraj) generation, which can directly generate trajectories on the road network with road-related information. RNTraj is a hybrid type of data, in which each point is represented by a discrete road segment and a continuous moving rate. To generate RNTraj, we design a diffusion model called Diff-RNTraj. This model can effectively handle the hybrid RNTraj using a continuous diffusion framework by incorporating a pre-training strategy to embed hybrid RNTraj into continuous representations. During the sampling stage, a RNTraj decoder is designed to map the continuous representation generated by the diffusion model back to the hybrid RNTraj format. Furthermore, Diff-RNTraj introduces a novel loss function to enhance the spatial validity of the generated trajectories. Extensive experiments conducted on two real-world trajectory datasets demonstrate the effectiveness of the proposed model.","sentences":["Trajectory data is essential for various applications as it records the movement of vehicles.","However, publicly available trajectory datasets remain limited in scale due to privacy concerns, which hinders the development of trajectory data mining and trajectory-based applications.","To address this issue, some methods for generating synthetic trajectories have been proposed to expand the scale of the dataset.","However, all existing methods generate trajectories in the geographical coordinate system, which poses two limitations for their utilization in practical applications: 1) the inability to ensure that the generated trajectories are constrained on the road.","2) the lack of road-related information.","In this paper, we propose a new problem to meet the practical application need, \\emph{i.e.}, road network-constrained trajectory (RNTraj) generation, which can directly generate trajectories on the road network with road-related information.","RNTraj is a hybrid type of data, in which each point is represented by a discrete road segment and a continuous moving rate.","To generate RNTraj, we design a diffusion model called Diff-RNTraj.","This model can effectively handle the hybrid RNTraj using a continuous diffusion framework by incorporating a pre-training strategy to embed hybrid RNTraj into continuous representations.","During the sampling stage, a RNTraj decoder is designed to map the continuous representation generated by the diffusion model back to the hybrid RNTraj format.","Furthermore, Diff-RNTraj introduces a novel loss function to enhance the spatial validity of the generated trajectories.","Extensive experiments conducted on two real-world trajectory datasets demonstrate the effectiveness of the proposed model."],"url":"http://arxiv.org/abs/2402.07369v1","category":"cs.LG"}
{"created":"2024-02-12 01:11:49","title":"A Novel Gaussian Min-Max Theorem and its Applications","abstract":"A celebrated result by Gordon allows one to compare the min-max behavior of two Gaussian processes if certain inequality conditions are met. The consequences of this result include the Gaussian min-max (GMT) and convex Gaussian min-max (CGMT) theorems which have had far-reaching implications in high-dimensional statistics, machine learning, non-smooth optimization, and signal processing. Both theorems rely on a pair of Gaussian processes, first identified by Slepian, that satisfy Gordon's comparison inequalities. To date, no other pair of Gaussian processes satisfying these inequalities has been discovered. In this paper, we identify such a new pair. The resulting theorems extend the classical GMT and CGMT Theorems from the case where the underlying Gaussian matrix in the primary process has iid rows to where it has independent but non-identically-distributed ones. The new CGMT is applied to the problems of multi-source Gaussian regression, as well as to binary classification of general Gaussian mixture models.","sentences":["A celebrated result by Gordon allows one to compare the min-max behavior of two Gaussian processes if certain inequality conditions are met.","The consequences of this result include the Gaussian min-max (GMT) and convex Gaussian min-max (CGMT) theorems which have had far-reaching implications in high-dimensional statistics, machine learning, non-smooth optimization, and signal processing.","Both theorems rely on a pair of Gaussian processes, first identified by Slepian, that satisfy Gordon's comparison inequalities.","To date, no other pair of Gaussian processes satisfying these inequalities has been discovered.","In this paper, we identify such a new pair.","The resulting theorems extend the classical GMT and CGMT Theorems from the case where the underlying Gaussian matrix in the primary process has iid rows to where it has independent but non-identically-distributed ones.","The new CGMT is applied to the problems of multi-source Gaussian regression, as well as to binary classification of general Gaussian mixture models."],"url":"http://arxiv.org/abs/2402.07356v1","category":"cs.LG"}
{"created":"2024-02-12 00:01:52","title":"Cultural gems linked open data: Mapping culture and intangible heritage in European cities","abstract":"The recovery and resilience of the cultural and creative sectors after the COVID-19 pandemic is a current topic with priority for the European Commission. Cultural gems is a crowdsourced web platform managed by the Joint Research Centre of the European Commission aimed at creating community-led maps as well as a common repository for cultural and creative places across European cities and towns. More than 130,000 physical locations and online cultural activities in more than 300 European cities and towns are currently tracked by the application. The main objective of Cultural gems consists in raising a holistic vision of European culture, reinforcing a sense of belonging to a common European cultural space. This data article describes the ontology developed for Cultural gems, adopted to represent the domain of knowledge of the application by means of FAIR (Findable, Accessible, Interoperable, Reusable) principles and following the paradigms of Linked Open Data (LOD). We provide an overview of this dataset, and describe the ontology model, along with the services used to access and consume the data.","sentences":["The recovery and resilience of the cultural and creative sectors after the COVID-19 pandemic is a current topic with priority for the European Commission.","Cultural gems is a crowdsourced web platform managed by the Joint Research Centre of the European Commission aimed at creating community-led maps as well as a common repository for cultural and creative places across European cities and towns.","More than 130,000 physical locations and online cultural activities in more than 300 European cities and towns are currently tracked by the application.","The main objective of Cultural gems consists in raising a holistic vision of European culture, reinforcing a sense of belonging to a common European cultural space.","This data article describes the ontology developed for Cultural gems, adopted to represent the domain of knowledge of the application by means of FAIR (Findable, Accessible, Interoperable, Reusable) principles and following the paradigms of Linked Open Data (LOD).","We provide an overview of this dataset, and describe the ontology model, along with the services used to access and consume the data."],"url":"http://arxiv.org/abs/2402.07335v1","category":"cs.DL"}
{"created":"2024-02-11 22:58:49","title":"Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs","abstract":"How do transformer-based large language models (LLMs) store and retrieve knowledge? We focus on the most basic form of this task -- factual recall, where the model is tasked with explicitly surfacing stored facts in prompts of form `Fact: The Colosseum is in the country of'. We find that the mechanistic story behind factual recall is more complex than previously thought. It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct attribute. We term this generic phenomena the additive motif: models compute through summing up multiple independent contributions. Each mechanism's contribution may be insufficient alone, but summing results in constructive interfere on the correct answer. In addition, we extend the method of direct logit attribution to attribute an attention head's output to individual source tokens. We use this technique to unpack what we call `mixed heads' -- which are themselves a pair of two separate additive updates from different source tokens.","sentences":["How do transformer-based large language models (LLMs) store and retrieve knowledge?","We focus on the most basic form of this task -- factual recall, where the model is tasked with explicitly surfacing stored facts in prompts of form `Fact: The Colosseum is in the country of'.","We find that the mechanistic story behind factual recall is more complex than previously thought.","It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct attribute.","We term this generic phenomena the additive motif: models compute through summing up multiple independent contributions.","Each mechanism's contribution may be insufficient alone, but summing results in constructive interfere on the correct answer.","In addition, we extend the method of direct logit attribution to attribute an attention head's output to individual source tokens.","We use this technique to unpack what we call `mixed heads' -- which are themselves a pair of two separate additive updates from different source tokens."],"url":"http://arxiv.org/abs/2402.07321v1","category":"cs.LG"}
{"created":"2024-02-11 22:13:24","title":"Optimization in SciML -- A Function Space Perspective","abstract":"We provide an infinite-dimensional view on optimization problems encountered in scientific machine learning (SciML) and advocate for the paradigm first optimize, then discretize for their solution. This amounts to first choosing an appropriate infinite-dimensional algorithm which is then discretized in a second step. To illustrate this point, we discuss recently proposed state-of-the-art algorithms for SciML applications and see that they can be derived within this framework. Hence, this perspective allows for a principled guide for the design of optimization algorithms for SciML. As the infinite-dimensional viewpoint is presently underdeveloped we formalize it here to foster the development of novel optimization algorithms.","sentences":["We provide an infinite-dimensional view on optimization problems encountered in scientific machine learning (SciML) and advocate for the paradigm first optimize, then discretize for their solution.","This amounts to first choosing an appropriate infinite-dimensional algorithm which is then discretized in a second step.","To illustrate this point, we discuss recently proposed state-of-the-art algorithms for SciML applications and see that they can be derived within this framework.","Hence, this perspective allows for a principled guide for the design of optimization algorithms for SciML.","As the infinite-dimensional viewpoint is presently underdeveloped we formalize it here to foster the development of novel optimization algorithms."],"url":"http://arxiv.org/abs/2402.07318v1","category":"math.OC"}
{"created":"2024-02-11 22:05:45","title":"On-Premises Superconducting Quantum Computer for Education and Research","abstract":"With a growing interest in quantum technology globally, there is an increasing need for accessing relevant physical systems for education and research. In this paper we introduce a commercially available on-site quantum computer utilizing superconducting technology, offering insights into its fundamental hardware and software components. We show how this system can be used in education to teach quantum concepts and deepen understanding of quantum theory and quantum computing. It offers learning opportunities for future talent and contributes to technological progress. Additionally, we demonstrate its use in research by replicating some notable recent achievements.","sentences":["With a growing interest in quantum technology globally, there is an increasing need for accessing relevant physical systems for education and research.","In this paper we introduce a commercially available on-site quantum computer utilizing superconducting technology, offering insights into its fundamental hardware and software components.","We show how this system can be used in education to teach quantum concepts and deepen understanding of quantum theory and quantum computing.","It offers learning opportunities for future talent and contributes to technological progress.","Additionally, we demonstrate its use in research by replicating some notable recent achievements."],"url":"http://arxiv.org/abs/2402.07315v1","category":"quant-ph"}
{"created":"2024-02-11 21:16:26","title":"HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs","abstract":"Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we propose a new architecture, HyperBERT, a mixed text-hypergraph model which simultaneously models hypergraph relational structure while maintaining the high-quality text encoding capabilities of a pre-trained BERT. Notably, HyperBERT presents results that achieve a new state-of-the-art on 5 challenging text-attributed hypergraph node classification benchmarks.","sentences":["Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges.","Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention.","However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability.","To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification.","Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text.","In this paper, we propose a new architecture, HyperBERT, a mixed text-hypergraph model which simultaneously models hypergraph relational structure while maintaining the high-quality text encoding capabilities of a pre-trained BERT.","Notably, HyperBERT presents results that achieve a new state-of-the-art on 5 challenging text-attributed hypergraph node classification benchmarks."],"url":"http://arxiv.org/abs/2402.07309v1","category":"cs.LG"}
{"created":"2024-02-11 20:52:29","title":"Insights into Natural Language Database Query Errors: From Attention Misalignment to User Handling Strategies","abstract":"Querying structured databases with natural language (NL2SQL) has remained a difficult problem for years. Recently, the advancement of machine learning (ML), natural language processing (NLP), and large language models (LLM) have led to significant improvements in performance, with the best model achieving ~85% percent accuracy on the benchmark Spider dataset. However, there is a lack of a systematic understanding of the types, causes, and effectiveness of error-handling mechanisms of errors for erroneous queries nowadays. To bridge the gap, a taxonomy of errors made by four representative NL2SQL models was built in this work, along with an in-depth analysis of the errors. Second, the causes of model errors were explored by analyzing the model-human attention alignment to the natural language query. Last, a within-subjects user study with 26 participants was conducted to investigate the effectiveness of three interactive error-handling mechanisms in NL2SQL. Findings from this paper shed light on the design of model structure and error discovery and repair strategies for natural language data query interfaces in the future.","sentences":["Querying structured databases with natural language (NL2SQL) has remained a difficult problem for years.","Recently, the advancement of machine learning (ML), natural language processing (NLP), and large language models (LLM) have led to significant improvements in performance, with the best model achieving ~85% percent accuracy on the benchmark Spider dataset.","However, there is a lack of a systematic understanding of the types, causes, and effectiveness of error-handling mechanisms of errors for erroneous queries nowadays.","To bridge the gap, a taxonomy of errors made by four representative NL2SQL models was built in this work, along with an in-depth analysis of the errors.","Second, the causes of model errors were explored by analyzing the model-human attention alignment to the natural language query.","Last, a within-subjects user study with 26 participants was conducted to investigate the effectiveness of three interactive error-handling mechanisms in NL2SQL.","Findings from this paper shed light on the design of model structure and error discovery and repair strategies for natural language data query interfaces in the future."],"url":"http://arxiv.org/abs/2402.07304v1","category":"cs.HC"}
{"created":"2024-02-11 20:42:01","title":"SPICA: Interactive Video Content Exploration through Augmented Audio Descriptions for Blind or Low-Vision Viewers","abstract":"Blind or Low-Vision (BLV) users often rely on audio descriptions (AD) to access video content. However, conventional static ADs can leave out detailed information in videos, impose a high mental load, neglect the diverse needs and preferences of BLV users, and lack immersion. To tackle these challenges, we introduce SPICA, an AI-powered system that enables BLV users to interactively explore video content. Informed by prior empirical studies on BLV video consumption, SPICA offers novel interactive mechanisms for supporting temporal navigation of frame captions and spatial exploration of objects within key frames. Leveraging an audio-visual machine learning pipeline, SPICA augments existing ADs by adding interactivity, spatial sound effects, and individual object descriptions without requiring additional human annotation. Through a user study with 14 BLV participants, we evaluated the usability and usefulness of SPICA and explored user behaviors, preferences, and mental models when interacting with augmented ADs.","sentences":["Blind or Low-Vision (BLV) users often rely on audio descriptions (AD) to access video content.","However, conventional static ADs can leave out detailed information in videos, impose a high mental load, neglect the diverse needs and preferences of BLV users, and lack immersion.","To tackle these challenges, we introduce SPICA, an AI-powered system that enables BLV users to interactively explore video content.","Informed by prior empirical studies on BLV video consumption, SPICA offers novel interactive mechanisms for supporting temporal navigation of frame captions and spatial exploration of objects within key frames.","Leveraging an audio-visual machine learning pipeline, SPICA augments existing ADs by adding interactivity, spatial sound effects, and individual object descriptions without requiring additional human annotation.","Through a user study with 14 BLV participants, we evaluated the usability and usefulness of SPICA and explored user behaviors, preferences, and mental models when interacting with augmented ADs."],"url":"http://arxiv.org/abs/2402.07300v1","category":"cs.HC"}
{"created":"2024-02-11 19:16:01","title":"CLIPPER: Robust Data Association without an Initial Guess","abstract":"Identifying correspondences in noisy data is a critically important step in estimation processes. When an informative initial estimation guess is available, the data association challenge is less acute; however, the existence of a high-quality initial guess is rare in most contexts. We explore graph-theoretic formulations for data association, which do not require an initial estimation guess. Existing graph-theoretic approaches optimize over unweighted graphs, discarding important consistency information encoded in weighted edges, and frequently attempt to solve NP-hard problems exactly. In contrast, we formulate a new optimization problem that fully leverages weighted graphs and seeks the densest edge-weighted clique. We introduce two relaxations to this problem: a convex semidefinite relaxation which we find to be empirically tight, and a fast first-order algorithm called CLIPPER which frequently arrives at nearly-optimal solutions in milliseconds. When evaluated on point cloud registration problems, our algorithms remain robust up to at least 95% outliers while existing algorithms begin breaking down at 80% outliers. Code is available at https://mit-acl.github.io/clipper.","sentences":["Identifying correspondences in noisy data is a critically important step in estimation processes.","When an informative initial estimation guess is available, the data association challenge is less acute; however, the existence of a high-quality initial guess is rare in most contexts.","We explore graph-theoretic formulations for data association, which do not require an initial estimation guess.","Existing graph-theoretic approaches optimize over unweighted graphs, discarding important consistency information encoded in weighted edges, and frequently attempt to solve NP-hard problems exactly.","In contrast, we formulate a new optimization problem that fully leverages weighted graphs and seeks the densest edge-weighted clique.","We introduce two relaxations to this problem: a convex semidefinite relaxation which we find to be empirically tight, and a fast first-order algorithm called CLIPPER which frequently arrives at nearly-optimal solutions in milliseconds.","When evaluated on point cloud registration problems, our algorithms remain robust up to at least 95% outliers while existing algorithms begin breaking down at 80% outliers.","Code is available at https://mit-acl.github.io/clipper."],"url":"http://arxiv.org/abs/2402.07284v1","category":"cs.RO"}
{"created":"2024-02-12 18:00:23","title":"Introducing Novel Planar Micromixers with Pillars and Gaps and Studying the Impact of Various Geometric Parameters on the Efficiency of Micromixers","abstract":"Chemical bioreactions play a significant role in many of the microfluidic devices, and their applications in biomedical science have seen substantial growth. Given that effective mixing is vital for initiating biochemical reactions in many applications, micromixers have become increasingly prevalent for high-throughput assays. In this study, numerical study is conducted to examine the fluid flow and mass transfer characteristics in novel micromixers featuring an array of pillars. The study explores the effects of pillar array design on mixing performance and pressure drop, drawing from principles such as contraction-expansion and split-recombine. Two configurations of pillar arrays are introduced, each undergoing investigation regarding parameters such as pillar diameter, gap size between pillar groups, distance between pillars, and vertical shift in pillar groups. Subsequently, optimal micromixers are identified, exhibiting mixing efficiency exceeding 99.7% at moderate Reynolds number (Re = 1), a level typically challenging for micromixers to attain high mixing efficiency. Notably, the pressure drop remains low at 1102 Pa. Furthermore, the variations in mixing index over time and across different positions along the channel are examined. Both configurations demonstrate short mixing lengths and times. The combination of rapid mixing, low pressure drop, and short mixing length positions the novel micromixers as highly promising for microfluidic applications.","sentences":["Chemical bioreactions play a significant role in many of the microfluidic devices, and their applications in biomedical science have seen substantial growth.","Given that effective mixing is vital for initiating biochemical reactions in many applications, micromixers have become increasingly prevalent for high-throughput assays.","In this study, numerical study is conducted to examine the fluid flow and mass transfer characteristics in novel micromixers featuring an array of pillars.","The study explores the effects of pillar array design on mixing performance and pressure drop, drawing from principles such as contraction-expansion and split-recombine.","Two configurations of pillar arrays are introduced, each undergoing investigation regarding parameters such as pillar diameter, gap size between pillar groups, distance between pillars, and vertical shift in pillar groups.","Subsequently, optimal micromixers are identified, exhibiting mixing efficiency exceeding 99.7% at moderate Reynolds number (Re = 1), a level typically challenging for micromixers to attain high mixing efficiency.","Notably, the pressure drop remains low at 1102 Pa.","Furthermore, the variations in mixing index over time and across different positions along the channel are examined.","Both configurations demonstrate short mixing lengths and times.","The combination of rapid mixing, low pressure drop, and short mixing length positions the novel micromixers as highly promising for microfluidic applications."],"url":"http://arxiv.org/abs/2402.07854v1","category":"physics.flu-dyn"}
{"created":"2024-02-12 16:33:23","title":"Insights into $(k,\u03c1)$-shortcutting algorithms","abstract":"A graph is called a $(k,\\rho)$-graph iff every node can reach $\\rho$ of its nearest neighbors in at most k hops. This property proved useful in the analysis and design of parallel shortest-path algorithms. Any graph can be transformed into a $(k,\\rho)$-graph by adding shortcuts. Formally, the $(k,\\rho)$-Minimum-Shortcut problem asks to find an appropriate shortcut set of minimal cardinality.   We show that the $(k,\\rho)$-Minimum-Shortcut problem is NP-complete in the practical regime of $k \\ge 3$ and $\\rho = \\Theta(n^\\epsilon)$ for $\\epsilon > 0$. With a related construction, we bound the approximation factor of known $(k,\\rho)$-Minimum-Shortcut problem heuristics from below and propose algorithmic countermeasures improving the approximation quality. Further, we describe an integer linear problem (ILP) solving the $(k,\\rho)$-Minimum-Shortcut problem optimally. Finally, we compare the practical performance and quality of all algorithms in an empirical campaign.","sentences":["A graph is called a $(k,\\rho)$-graph iff every node can reach $\\rho$ of its nearest neighbors in at most k hops.","This property proved useful in the analysis and design of parallel shortest-path algorithms.","Any graph can be transformed into a $(k,\\rho)$-graph by adding shortcuts.","Formally, the $(k,\\rho)$-Minimum-Shortcut problem asks to find an appropriate shortcut set of minimal cardinality.   ","We show that the $(k,\\rho)$-Minimum-Shortcut problem is NP-complete in the practical regime of $k \\ge 3$ and $\\rho = \\Theta(n^\\epsilon)$ for $\\epsilon > 0$. With a related construction, we bound the approximation factor of known $(k,\\rho)$-Minimum-Shortcut problem heuristics from below and propose algorithmic countermeasures improving the approximation quality.","Further, we describe an integer linear problem (ILP) solving the $(k,\\rho)$-Minimum-Shortcut problem optimally.","Finally, we compare the practical performance and quality of all algorithms in an empirical campaign."],"url":"http://arxiv.org/abs/2402.07771v1","category":"cs.DS"}
{"created":"2024-02-12 15:38:31","title":"$\u03c3(500)$ resonance pole positions as function of $m_\u03c0$: analysis with a unitary coupled-channel model","abstract":"Resonance pole positions of the $f_0(500)$ alias $\\sigma(500)$ meson are computed and plotted as a continuous function of pion mass in the framework of a unitary and analytic coupled-channel model for scalar mesons as dynamical $q\\bar{q}$ states. The $\\sigma$ is described with a light and a strange $q\\bar{q}$ seed, mixing with each other mainly through the common $\\pi\\pi$, $K\\bar{K}$, and $\\eta\\eta$ meson-meson channels. The few model parameters are fitted to experimental $S$-wave $\\pi\\pi$ phase shifts up to 1 GeV, yielding, in the case of the physical pion mass, resonance poles at $(460-i222)$ MeV for the $\\sigma(500)$ and $(978-i37)$ MeV for the $f_0(980)$. Resonance, bound-state, and virtual-state pole trajectories are shown as a function of $m_\\pi$ running from 139.57 MeV to 1 GeV. These are compared to recent lattice QCD computations that use interpolating fields corresponding to the model's channels, i.e., for a few discrete $m_\\pi $values.","sentences":["Resonance pole positions of the $f_0(500)$ alias $\\sigma(500)$ meson are computed and plotted as a continuous function of pion mass in the framework of a unitary and analytic coupled-channel model for scalar mesons as dynamical $q\\bar{q}$ states.","The $\\sigma$ is described with a light and a strange $q\\bar{q}$ seed, mixing with each other mainly through the common $\\pi\\pi$, $K\\bar{K}$, and $\\eta\\eta$ meson-meson channels.","The few model parameters are fitted to experimental $S$-wave $\\pi\\pi$ phase shifts up to 1 GeV, yielding, in the case of the physical pion mass, resonance poles at $(460-i222)$ MeV for the $\\sigma(500)$ and $(978-i37)$ MeV for the $f_0(980)$. Resonance, bound-state, and virtual-state pole trajectories are shown as a function of $m_\\pi$ running from 139.57 MeV to 1 GeV.","These are compared to recent lattice QCD computations that use interpolating fields corresponding to the model's channels, i.e., for a few discrete $m_\\pi $values."],"url":"http://arxiv.org/abs/2402.07725v1","category":"hep-ph"}
{"created":"2024-02-12 15:34:04","title":"Interaction-Based Driving Scenario Classification and Labeling","abstract":"Scenario data play a vital role in autonomous driving related researches, and it is essential to obtain refined descriptions and labels to extract and index scenarios with different types of interactions. However, existing methods cannot cope well with the problem of scenario classification and comparison with vehicle interactions as the core. In this paper, we propose a framework for interaction-based refined scenario classification and labeling. Based on the summarized basic types of vehicle interactions, we slice scenario data stream into a series of scenario segments via spatiotemporal scenario evolution tree. The scenario segment statistics of many published scenario datasets are further analyzed. We also propose the scenario metric Graph-DTW based on Graph Computation Tree and Dynamic Time Warping to conduct refined scenario comparison and labeling. The extreme interactive scenarios and corner cases can be efficiently filtered and extracted. Moreover, testing examples on trajectory prediction model demonstrate the effectiveness and advantages of scenario labeling and the proposed metric. The overall framework can provide solid support for the usage and indexing of scenario data.","sentences":["Scenario data play a vital role in autonomous driving related researches, and it is essential to obtain refined descriptions and labels to extract and index scenarios with different types of interactions.","However, existing methods cannot cope well with the problem of scenario classification and comparison with vehicle interactions as the core.","In this paper, we propose a framework for interaction-based refined scenario classification and labeling.","Based on the summarized basic types of vehicle interactions, we slice scenario data stream into a series of scenario segments via spatiotemporal scenario evolution tree.","The scenario segment statistics of many published scenario datasets are further analyzed.","We also propose the scenario metric Graph-DTW based on Graph Computation Tree and Dynamic Time Warping to conduct refined scenario comparison and labeling.","The extreme interactive scenarios and corner cases can be efficiently filtered and extracted.","Moreover, testing examples on trajectory prediction model demonstrate the effectiveness and advantages of scenario labeling and the proposed metric.","The overall framework can provide solid support for the usage and indexing of scenario data."],"url":"http://arxiv.org/abs/2402.07720v1","category":"cs.SE"}
{"created":"2024-02-12 15:32:48","title":"Local Centrality Minimization with Quality Guarantees","abstract":"Centrality measures, quantifying the importance of vertices or edges, play a fundamental role in network analysis. To date, triggered by some positive approximability results, a large body of work has been devoted to studying centrality maximization, where the goal is to maximize the centrality score of a target vertex by manipulating the structure of a given network. On the other hand, due to the lack of such results, only very little attention has been paid to centrality minimization, despite its practical usefulness.   In this study, we introduce a novel optimization model for local centrality minimization, where the manipulation is allowed only around the target vertex. We prove the NP-hardness of our model and that the most intuitive greedy algorithm has a quite limited performance in terms of approximation ratio. Then we design two effective approximation algorithms: The first algorithm is a highly-scalable algorithm that has an approximation ratio unachievable by the greedy algorithm, while the second algorithm is a bicriteria approximation algorithm that solves a continuous relaxation based on the Lov\\'asz extension, using a projected subgradient method. To the best of our knowledge, ours are the first polynomial-time algorithms with provable approximation guarantees for centrality minimization. Experiments using a variety of real-world networks demonstrate the effectiveness of our proposed algorithms: Our first algorithm is applicable to million-scale graphs and obtains much better solutions than those of scalable baselines, while our second algorithm is rather strong against adversarial instances.","sentences":["Centrality measures, quantifying the importance of vertices or edges, play a fundamental role in network analysis.","To date, triggered by some positive approximability results, a large body of work has been devoted to studying centrality maximization, where the goal is to maximize the centrality score of a target vertex by manipulating the structure of a given network.","On the other hand, due to the lack of such results, only very little attention has been paid to centrality minimization, despite its practical usefulness.   ","In this study, we introduce a novel optimization model for local centrality minimization, where the manipulation is allowed only around the target vertex.","We prove the NP-hardness of our model and that the most intuitive greedy algorithm has a quite limited performance in terms of approximation ratio.","Then we design two effective approximation algorithms: The first algorithm is a highly-scalable algorithm that has an approximation ratio unachievable by the greedy algorithm, while the second algorithm is a bicriteria approximation algorithm that solves a continuous relaxation based on the Lov\\'asz extension, using a projected subgradient method.","To the best of our knowledge, ours are the first polynomial-time algorithms with provable approximation guarantees for centrality minimization.","Experiments using a variety of real-world networks demonstrate the effectiveness of our proposed algorithms: Our first algorithm is applicable to million-scale graphs and obtains much better solutions than those of scalable baselines, while our second algorithm is rather strong against adversarial instances."],"url":"http://arxiv.org/abs/2402.07718v1","category":"cs.SI"}
{"created":"2024-02-12 11:44:52","title":"Superconducting single-photon detector integrated in DBR with optical microconnector for MM or SM fiber","abstract":"This paper presents the development of a superconducting nanowire single-photon detector (SNSPD) integrated into a distributed Bragg reflector (DBR) with a design center wavelength of 830 nm and a width of 200 nm. This SNSPD is made of a superconducting niobium nitride (NbN) thin film that is produced using plasma-enhanced atomic layer deposition (PEALD). The DBR is made of 15 alternating layers of silicon nitride and silicon oxide that are produced through plasma-enhanced chemical vapor deposition (PECVD). The reflection efficiency of the mirror is 90% at a wavelength of 830 nm. For sufficient optical coupling, an optical micro-connector optimized for multimode or single-mode optical fibers with a diameter of 128 {\\mu}m was formed using two-photon polymerization techniques. The niobium nitride film was deposited onto the DBR surface in-situ in two separate reactors connected by a vacuum transfer. The in-situ technique of deposition of a superconducting niobium nitride film and a distributed Bragg reflector has allowed achieving a detection efficiency of 90% at a wavelength of 830 nm and a dark count rate of 10 s-1 at a temperature of 2.5 K. Additionally, the detector jitter was 50 ps.","sentences":["This paper presents the development of a superconducting nanowire single-photon detector (SNSPD) integrated into a distributed Bragg reflector (DBR) with a design center wavelength of 830 nm and a width of 200 nm.","This SNSPD is made of a superconducting niobium nitride (NbN) thin film that is produced using plasma-enhanced atomic layer deposition (PEALD).","The DBR is made of 15 alternating layers of silicon nitride and silicon oxide that are produced through plasma-enhanced chemical vapor deposition (PECVD).","The reflection efficiency of the mirror is 90% at a wavelength of 830 nm.","For sufficient optical coupling, an optical micro-connector optimized for multimode or single-mode optical fibers with a diameter of 128 {\\mu}m was formed using two-photon polymerization techniques.","The niobium nitride film was deposited onto the DBR surface in-situ in two separate reactors connected by a vacuum transfer.","The in-situ technique of deposition of a superconducting niobium nitride film and a distributed Bragg reflector has allowed achieving a detection efficiency of 90% at a wavelength of 830 nm and a dark count rate of 10 s-1 at a temperature of 2.5 K. Additionally, the detector jitter was 50 ps."],"url":"http://arxiv.org/abs/2402.07592v1","category":"cond-mat.supr-con"}
{"created":"2024-02-12 09:00:06","title":"Ultra-fast Waveguide MUTC Photodiodes over 220 GHz","abstract":"We present InP-based evanescently-coupled waveguide modified uni-traveling carrier photodiodes (MUTC-PDs) exhibiting a breakthrough in bandwidth. The optimization of carrier transport and optical coupling is achieved through a detailed discussion on the design of the cliff layer and waveguide layer. Addressing the parasitic capacitance challenge, we introduce benzocyclobutene (BCB) beneath the PD electrodes, effectively overcoming the bandwidth bottleneck associated with the RC time constant. Devices with sizes of 2 * 7 um2 and 2 * 10 um2 achieve 3-dB bandwidths over 220 GHz, along with external responsivities of 0.161 A/W and 0.237 A/W, respectively. Notably, the RF output power reaches a peak of -1.69 dBm at 215 GHz for 2 * 15 um2 PDs.","sentences":["We present InP-based evanescently-coupled waveguide modified uni-traveling carrier photodiodes (MUTC-PDs) exhibiting a breakthrough in bandwidth.","The optimization of carrier transport and optical coupling is achieved through a detailed discussion on the design of the cliff layer and waveguide layer.","Addressing the parasitic capacitance challenge, we introduce benzocyclobutene (BCB) beneath the PD electrodes, effectively overcoming the bandwidth bottleneck associated with the RC time constant.","Devices with sizes of 2 * 7 um2 and 2 * 10 um2 achieve 3-dB bandwidths over 220 GHz, along with external responsivities of 0.161 A/W and 0.237 A/W, respectively.","Notably, the RF output power reaches a peak of -1.69 dBm at 215 GHz for 2 * 15 um2 PDs."],"url":"http://arxiv.org/abs/2402.07491v1","category":"physics.app-ph"}
{"created":"2024-02-12 05:37:49","title":"Epsilon near zero metal oxide based spectrally selective reflectors","abstract":"Epsilon near zero (ENZ) materials can contribute significantly to the advancement of spectrally selective coatings aimed at enhancing efficient use of solar radiation and thermal energy management. Here, we demonstrate a subwavelength thick, multilayer optical coating that imparts a spectrally \"step function\" like reflectivity onto diverse surfaces, from stainless steel to glass, employing indium tin oxide as the key ENZ material. The coating, harnessing the ENZ and plasmonic properties of nominally nanostructured ITO along with ultrathin layers of Cr and Cr2O3 show 15% reflectivity over the visible to near-infrared and 80% reflectivity (and low emissivity) beyond a cut-in wavelength around 1500 nm, which is tunable in the infrared. A combination of simulations and experimental results are used to optimize the coating architecture and gain insights into the relevance of the components. The straightforward design with high thermal stability will find applications requiring passive cooling.","sentences":["Epsilon near zero (ENZ) materials can contribute significantly to the advancement of spectrally selective coatings aimed at enhancing efficient use of solar radiation and thermal energy management.","Here, we demonstrate a subwavelength thick, multilayer optical coating that imparts a spectrally \"step function\" like reflectivity onto diverse surfaces, from stainless steel to glass, employing indium tin oxide as the key ENZ material.","The coating, harnessing the ENZ and plasmonic properties of nominally nanostructured ITO along with ultrathin layers of Cr and Cr2O3 show 15% reflectivity over the visible to near-infrared and 80% reflectivity (and low emissivity) beyond a cut-in wavelength around 1500 nm, which is tunable in the infrared.","A combination of simulations and experimental results are used to optimize the coating architecture and gain insights into the relevance of the components.","The straightforward design with high thermal stability will find applications requiring passive cooling."],"url":"http://arxiv.org/abs/2402.07414v1","category":"physics.optics"}
{"created":"2024-02-12 02:40:37","title":"Distribution Locational Marginal Emission for Carbon Alleviation in Distribution Networks: Formulation, Calculation, and Implication","abstract":"Regulating the proper carbon-aware intervention policy is one of the keys to emission alleviation in the distribution network, whose basis lies in effectively attributing the emission responsibility using emission factors. This paper establishes the distribution locational marginal emission (DLME) to calculate the marginal change of emission from the marginal change of both active and reactive load demand for incentivizing carbon alleviation. It first formulates the day-head distribution network scheduling model based on the second-order cone program (SOCP). The emission propagation and responsibility are analyzed from demand to supply to system emission. Considering the complex and implicit mapping of the SOCP-based scheduling model, the implicit theorem is leveraged to exploit the optimal condition of SOCP. The corresponding SOCP-based implicit derivation approach is proposed to calculate the DLMEs effectively in a model-based way. Comprehensive numerical studies are conducted to verify the superiority of the proposed method by comparing its calculation efficacy to the conventional marginal estimation approach, assessing its effectiveness in carbon alleviation with comparison to the average emission factors, and evaluating its carbon alleviation ability of reactive DLME.","sentences":["Regulating the proper carbon-aware intervention policy is one of the keys to emission alleviation in the distribution network, whose basis lies in effectively attributing the emission responsibility using emission factors.","This paper establishes the distribution locational marginal emission (DLME) to calculate the marginal change of emission from the marginal change of both active and reactive load demand for incentivizing carbon alleviation.","It first formulates the day-head distribution network scheduling model based on the second-order cone program (SOCP).","The emission propagation and responsibility are analyzed from demand to supply to system emission.","Considering the complex and implicit mapping of the SOCP-based scheduling model, the implicit theorem is leveraged to exploit the optimal condition of SOCP.","The corresponding SOCP-based implicit derivation approach is proposed to calculate the DLMEs effectively in a model-based way.","Comprehensive numerical studies are conducted to verify the superiority of the proposed method by comparing its calculation efficacy to the conventional marginal estimation approach, assessing its effectiveness in carbon alleviation with comparison to the average emission factors, and evaluating its carbon alleviation ability of reactive DLME."],"url":"http://arxiv.org/abs/2402.07379v1","category":"eess.SY"}
{"created":"2024-02-12 02:28:58","title":"Combination of crystal growth with optical floating zone and evaluation of Nd3+:LaAlO3 crystals with the dynamic nuclear polarization of 139La and 27Al","abstract":"Producing a polarized lanthanum (La) target with high polarization and long relaxation time is crucial for realizing time-reversal violation experiments using polarized neutron beams. We use a LaAlO3 crystal doped with a small amount of Nd3+ ions for the polarized lanthanum target. Optimizing the amount of Nd3+ ions is considerably important because the achievable polarization and relaxation time strongly depend on this amount. We established a fundamental method to grow single crystals of Nd3+:LaAlO3 using an optical floating zone method that employs halogen lamps and evaluated the crystals with the dynamic nuclear polarization (DNP) method for polarizing nuclear spins. Two crystal samples were grown by ourselves and evaluated with the DNP at 1.3 K and 2.3 T for the first time except for the target materials of protons. The enhancement of NMR signals for 139La and 27Al was successfully observed, and the enhancement factors were eventually 3.5+-0.3 and 13+-3 for the samples with Nd3+ ions of 0.05 and 0.01 mol%, respectively. The combination scheme of the crystal growth and evaluation of the crystals is found to be effectively applicable for optimizing the amount of Nd3+ ions for improving the performance of the polarized target.","sentences":["Producing a polarized lanthanum (La) target with high polarization and long relaxation time is crucial for realizing time-reversal violation experiments using polarized neutron beams.","We use a LaAlO3 crystal doped with a small amount of Nd3+ ions for the polarized lanthanum target.","Optimizing the amount of Nd3+ ions is considerably important because the achievable polarization and relaxation time strongly depend on this amount.","We established a fundamental method to grow single crystals of Nd3+:LaAlO3 using an optical floating zone method that employs halogen lamps and evaluated the crystals with the dynamic nuclear polarization (DNP) method for polarizing nuclear spins.","Two crystal samples were grown by ourselves and evaluated with the DNP at 1.3 K and 2.3 T for the first time except for the target materials of protons.","The enhancement of NMR signals for 139La and 27Al was successfully observed, and the enhancement factors were eventually 3.5+-0.3 and 13+-3 for the samples with Nd3","+ ions of 0.05 and 0.01 mol%, respectively.","The combination scheme of the crystal growth and evaluation of the crystals is found to be effectively applicable for optimizing the amount of Nd3+ ions for improving the performance of the polarized target."],"url":"http://arxiv.org/abs/2402.07378v1","category":"physics.ins-det"}
{"created":"2024-02-12 01:30:52","title":"Measurement-induced state transitions in dispersive qubit readout schemes","abstract":"The dispersive readout scheme enables quantum non-demolition measurement of superconducting qubits. An increased readout power can shorten the readout time and reduce the state discrimination error but can promote qubit transitions into higher noncomputational states. The ability to predict the onset of these measurement-induced state transitions can aid the optimization of qubit circuits and provide means for comparing the readout performance of different qubit types. Building upon the concept of dressed coherent states, we consider two straightforward metrics for determining the maximum number of photons that can be used for dispersive readout without causing state transitions. We focus on the fluxonium readout to demonstrate the independence of the metrics from any qubit-type-specific approximations. The dispersive readout of transmons and other superconducting qubits can be treated universally in the same fashion.","sentences":["The dispersive readout scheme enables quantum non-demolition measurement of superconducting qubits.","An increased readout power can shorten the readout time and reduce the state discrimination error but can promote qubit transitions into higher noncomputational states.","The ability to predict the onset of these measurement-induced state transitions can aid the optimization of qubit circuits and provide means for comparing the readout performance of different qubit types.","Building upon the concept of dressed coherent states, we consider two straightforward metrics for determining the maximum number of photons that can be used for dispersive readout without causing state transitions.","We focus on the fluxonium readout to demonstrate the independence of the metrics from any qubit-type-specific approximations.","The dispersive readout of transmons and other superconducting qubits can be treated universally in the same fashion."],"url":"http://arxiv.org/abs/2402.07360v1","category":"quant-ph"}
{"created":"2024-02-12 01:02:33","title":"Optimized Gr\u00f6bner basis algorithms for maximal determinantal ideals and critical point computations","abstract":"Given polynomials $g$ and $f_1,\\dots,f_p$, all in $\\Bbbk[x_1,\\dots,x_n]$ for some field $\\Bbbk$, we consider the problem of computing the critical points of the restriction of $g$ to the variety defined by $f_1=\\cdots=f_p=0$. These are defined by the simultaneous vanishing of the $f_i$'s and all maximal minors of the Jacobian matrix associated to $(g,f_1, \\ldots, f_p)$. We use the Eagon-Northcott complex associated to the ideal generated by these maximal minors to gain insight into the syzygy module of the system defining these critical points. We devise new $F_5$-type criteria to predict and avoid more reductions to zero when computing a Gr\\\"obner basis for the defining system of this critical locus. We give a bound for the arithmetic complexity of this enhanced $F_5$ algorithm and compare it to the best previously known bound for computing critical points using Gr\\\"obner bases.","sentences":["Given polynomials $g$ and $f_1,\\dots,f_p$, all in $\\Bbbk[x_1,\\dots,x_n]$ for some field $\\Bbbk$, we consider the problem of computing the critical points of the restriction of $g$ to the variety defined by $f_1=\\cdots=f_p=0$. These are defined by the simultaneous vanishing of the $f_i$'s and all maximal minors of the Jacobian matrix associated to $(g,f_1, \\ldots, f_p)$. We use the Eagon-Northcott complex associated to the ideal generated by these maximal minors to gain insight into the syzygy module of the system defining these critical points.","We devise new $F_5$-type criteria to predict and avoid more reductions to zero when computing a Gr\\\"obner basis for the defining system of this critical locus.","We give a bound for the arithmetic complexity of this enhanced $F_5$ algorithm and compare it to the best previously known bound for computing critical points using Gr\\\"obner bases."],"url":"http://arxiv.org/abs/2402.07353v1","category":"cs.SC"}
{"created":"2024-02-11 20:45:16","title":"A Review of the GIC Blocker Placement Problem","abstract":"Space weather poses a tremendous threat to power systems: geomagnetic disturbances could result in widespread disruptions and long-duration blackouts, including severe damage to system components. To mitigate their impacts, a handful of strategies exist, with the most promising being the deployment of transformer neutral blocking devices. The high cost of these devices, however, precludes their installation at all substations; this motivates the development of effective solutions for the cost-effective placement of such devices. While the current state-of-the-art in blocker placement methods is insufficient to be applied to real-sized power grids, ongoing research continues to increase the size of networks for which the placement problem remains tractable. Along these lines, the contributions of this paper are two fold: first, a comprehensive overview of the current state-of-the-art in blocker placement methods is provided; and second, a complete optimization formulation - implemented and benchmarked in an open-source software - for the blocker placement problem is presented.","sentences":["Space weather poses a tremendous threat to power systems: geomagnetic disturbances could result in widespread disruptions and long-duration blackouts, including severe damage to system components.","To mitigate their impacts, a handful of strategies exist, with the most promising being the deployment of transformer neutral blocking devices.","The high cost of these devices, however, precludes their installation at all substations; this motivates the development of effective solutions for the cost-effective placement of such devices.","While the current state-of-the-art in blocker placement methods is insufficient to be applied to real-sized power grids, ongoing research continues to increase the size of networks for which the placement problem remains tractable.","Along these lines, the contributions of this paper are two fold: first, a comprehensive overview of the current state-of-the-art in blocker placement methods is provided; and second, a complete optimization formulation - implemented and benchmarked in an open-source software - for the blocker placement problem is presented."],"url":"http://arxiv.org/abs/2402.07302v1","category":"eess.SY"}
{"created":"2024-02-11 20:33:20","title":"Quantum Quality with Classical Cost: Ab Initio Nonadiabatic Dynamics Simulations using the Mapping Approach to Surface Hopping","abstract":"Nonadiabatic dynamics methods are an essential tool for investigating photochemical processes. In the context of employing first principles electronic structure techniques, such simulations can be carried out in a practical manner using semiclassical trajectory-based methods or wave packet approaches. It is commonly thought that wave packet approaches offer inherent advantages over their semiclassical counterparts in terms of accuracy, and that this trait simply comes at a higher computational cost. Here we demonstrate that the mapping approach to surface hopping (MASH), a recently introduced trajectory-based nonadiabatic dynamics method, can be applied in tandem with ab initio electronic structure much more cheaply than wave packet techniques, without any loss in accuracy.","sentences":["Nonadiabatic dynamics methods are an essential tool for investigating photochemical processes.","In the context of employing first principles electronic structure techniques, such simulations can be carried out in a practical manner using semiclassical trajectory-based methods or wave packet approaches.","It is commonly thought that wave packet approaches offer inherent advantages over their semiclassical counterparts in terms of accuracy, and that this trait simply comes at a higher computational cost.","Here we demonstrate that the mapping approach to surface hopping (MASH), a recently introduced trajectory-based nonadiabatic dynamics method, can be applied in tandem with ab initio electronic structure much more cheaply than wave packet techniques, without any loss in accuracy."],"url":"http://arxiv.org/abs/2402.07299v1","category":"physics.chem-ph"}
{"created":"2024-02-11 17:46:33","title":"American Sign Language Video to Text Translation","abstract":"Sign language to text is a crucial technology that can break down communication barriers for individuals with hearing difficulties. We replicate and try to improve on a recently published study. We evaluate models using BLEU and rBLEU metrics to ensure translation quality. During our ablation study, we found that the model's performance is significantly influenced by optimizers, activation functions, and label smoothing. Further research aims to refine visual feature capturing, enhance decoder utilization, and integrate pre-trained decoders for better translation outcomes. Our source code is available to facilitate replication of our results and encourage future research.","sentences":["Sign language to text is a crucial technology that can break down communication barriers for individuals with hearing difficulties.","We replicate and try to improve on a recently published study.","We evaluate models using BLEU and rBLEU metrics to ensure translation quality.","During our ablation study, we found that the model's performance is significantly influenced by optimizers, activation functions, and label smoothing.","Further research aims to refine visual feature capturing, enhance decoder utilization, and integrate pre-trained decoders for better translation outcomes.","Our source code is available to facilitate replication of our results and encourage future research."],"url":"http://arxiv.org/abs/2402.07255v1","category":"cs.CL"}
{"created":"2024-02-11 16:36:48","title":"Thresholded Oja does Sparse PCA?","abstract":"We consider the problem of Sparse Principal Component Analysis (PCA) when the ratio $d/n \\rightarrow c > 0$. There has been a lot of work on optimal rates on sparse PCA in the offline setting, where all the data is available for multiple passes. In contrast, when the population eigenvector is $s$-sparse, streaming algorithms that have $O(d)$ storage and $O(nd)$ time complexity either typically require strong initialization conditions or have a suboptimal error. We show that a simple algorithm that thresholds and renormalizes the output of Oja's algorithm (the Oja vector) obtains an optimal error rate. This is very surprising because, without thresholding, the Oja vector has a large error. Our analysis centers around bounding the entries of the unnormalized Oja vector, which involves the projection of a product of independent random matrices on a random initial vector. This is nontrivial and novel since previous analyses of Oja's algorithm and matrix products have been done when the trace of the population covariance matrix is bounded while in our setting, this quantity can be as large as $n$.","sentences":["We consider the problem of Sparse Principal Component Analysis (PCA) when the ratio $d/n \\rightarrow c > 0$.","There has been a lot of work on optimal rates on sparse PCA in the offline setting, where all the data is available for multiple passes.","In contrast, when the population eigenvector is $s$-sparse, streaming algorithms that have $O(d)$ storage and $O(nd)$ time complexity either typically require strong initialization conditions or have a suboptimal error.","We show that a simple algorithm that thresholds and renormalizes the output of Oja's algorithm (the Oja vector) obtains an optimal error rate.","This is very surprising because, without thresholding, the Oja vector has a large error.","Our analysis centers around bounding the entries of the unnormalized Oja vector, which involves the projection of a product of independent random matrices on a random initial vector.","This is nontrivial and novel since previous analyses of Oja's algorithm and matrix products have been done when the trace of the population covariance matrix is bounded while in our setting, this quantity can be as large as $n$."],"url":"http://arxiv.org/abs/2402.07240v1","category":"math.ST"}
{"created":"2024-02-11 14:21:22","title":"Non-Radial Free Geodesics. II. In Spatially Curved FLRW Spacetime","abstract":"This paper presents an in-depth exploration of timelike free geodesics in spatially curved Friedmann-Lema\\^itre-Robertson-Walker (FLRW) spacetime. A unified approach for these geodesics encompassing both radial and non-radial trajectories across Euclidean, spherical, and hyperbolic geometries is employed. Using the symmetry properties of the system, two constants of motion related to this dynamical system are derived. This treatment facilitates the explicit computation of radial and angular peculiar velocities, along with the evolution of the comoving radial distance and angle over time. The study introduces three distinct methods for characterizing geodesic solutions, further delving into the flatness limit, the null geodesic limit, the radial geodesic limit, the comoving geodesic limit, and the circular orbits. Additionally, we analyze Killing vectors, reflecting the symmetries inherent in the dynamical system. This study provides a more profound understanding of the behavior of free particles as observed from a comoving reference frame.","sentences":["This paper presents an in-depth exploration of timelike free geodesics in spatially curved Friedmann-Lema\\^itre-Robertson-Walker (FLRW) spacetime.","A unified approach for these geodesics encompassing both radial and non-radial trajectories across Euclidean, spherical, and hyperbolic geometries is employed.","Using the symmetry properties of the system, two constants of motion related to this dynamical system are derived.","This treatment facilitates the explicit computation of radial and angular peculiar velocities, along with the evolution of the comoving radial distance and angle over time.","The study introduces three distinct methods for characterizing geodesic solutions, further delving into the flatness limit, the null geodesic limit, the radial geodesic limit, the comoving geodesic limit, and the circular orbits.","Additionally, we analyze Killing vectors, reflecting the symmetries inherent in the dynamical system.","This study provides a more profound understanding of the behavior of free particles as observed from a comoving reference frame."],"url":"http://arxiv.org/abs/2402.07213v1","category":"gr-qc"}
{"created":"2024-02-11 14:04:13","title":"Towards Fast Stochastic Sampling in Diffusion Generative Models","abstract":"Diffusion models suffer from slow sample generation at inference time. Despite recent efforts, improving the sampling efficiency of stochastic samplers for diffusion models remains a promising direction. We propose Splitting Integrators for fast stochastic sampling in pre-trained diffusion models in augmented spaces. Commonly used in molecular dynamics, splitting-based integrators attempt to improve sampling efficiency by cleverly alternating between numerical updates involving the data, auxiliary, or noise variables. However, we show that a naive application of splitting integrators is sub-optimal for fast sampling. Consequently, we propose several principled modifications to naive splitting samplers for improving sampling efficiency and denote the resulting samplers as Reduced Splitting Integrators. In the context of Phase Space Langevin Diffusion (PSLD) [Pandey \\& Mandt, 2023] on CIFAR-10, our stochastic sampler achieves an FID score of 2.36 in only 100 network function evaluations (NFE) as compared to 2.63 for the best baselines.","sentences":["Diffusion models suffer from slow sample generation at inference time.","Despite recent efforts, improving the sampling efficiency of stochastic samplers for diffusion models remains a promising direction.","We propose Splitting Integrators for fast stochastic sampling in pre-trained diffusion models in augmented spaces.","Commonly used in molecular dynamics, splitting-based integrators attempt to improve sampling efficiency by cleverly alternating between numerical updates involving the data, auxiliary, or noise variables.","However, we show that a naive application of splitting integrators is sub-optimal for fast sampling.","Consequently, we propose several principled modifications to naive splitting samplers for improving sampling efficiency and denote the resulting samplers as Reduced Splitting Integrators.","In the context of Phase Space Langevin Diffusion (PSLD)","[Pandey \\& Mandt, 2023] on CIFAR-10, our stochastic sampler achieves an FID score of 2.36 in only 100 network function evaluations (NFE) as compared to 2.63 for the best baselines."],"url":"http://arxiv.org/abs/2402.07211v1","category":"cs.LG"}
{"created":"2024-02-11 13:00:04","title":"The Implicit Bias of Gradient Noise: A Symmetry Perspective","abstract":"We characterize the learning dynamics of stochastic gradient descent (SGD) when continuous symmetry exists in the loss function, where the divergence between SGD and gradient descent is dramatic. We show that depending on how the symmetry affects the learning dynamics, we can divide a family of symmetry into two classes. For one class of symmetry, SGD naturally converges to solutions that have a balanced and aligned gradient noise. For the other class of symmetry, SGD will almost always diverge. Then, we show that our result remains applicable and can help us understand the training dynamics even when the symmetry is not present in the loss function. Our main result is universal in the sense that it only depends on the existence of the symmetry and is independent of the details of the loss function. We demonstrate that the proposed theory offers an explanation of progressive sharpening and flattening and can be applied to common practical problems such as representation normalization, matrix factorization, and the use of warmup.","sentences":["We characterize the learning dynamics of stochastic gradient descent (SGD) when continuous symmetry exists in the loss function, where the divergence between SGD and gradient descent is dramatic.","We show that depending on how the symmetry affects the learning dynamics, we can divide a family of symmetry into two classes.","For one class of symmetry, SGD naturally converges to solutions that have a balanced and aligned gradient noise.","For the other class of symmetry, SGD will almost always diverge.","Then, we show that our result remains applicable and can help us understand the training dynamics even when the symmetry is not present in the loss function.","Our main result is universal in the sense that it only depends on the existence of the symmetry and is independent of the details of the loss function.","We demonstrate that the proposed theory offers an explanation of progressive sharpening and flattening and can be applied to common practical problems such as representation normalization, matrix factorization, and the use of warmup."],"url":"http://arxiv.org/abs/2402.07193v1","category":"cs.LG"}
{"created":"2024-02-11 12:51:14","title":"Almost minimal models of log surfaces","abstract":"We generalize Miyanishi's theory of almost minimal models of log smooth surfaces with reduced boundary to the case of arbitrary log surfaces defined over an algebraically closed field. Given an MMP run of a log surface $(X,D)$ we define and construct its almost minimal model, whose underlying surface has singularities not worse than $X$ and which differs from a minimal model by a contraction of some curves supported in the boundary only. For boundaries of type $rD$, where $D$ is reduced and $r\\in [0,1]\\cap \\mathbb{Q}$, we show that if $X$ is smooth or $r\\in [0,\\frac{1}{2}]$ then the construction respects $(1-r)$-divisorial log terminality and $(1-r)$-log canonicity. We show that the assumptions are optimal, too.","sentences":["We generalize Miyanishi's theory of almost minimal models of log smooth surfaces with reduced boundary to the case of arbitrary log surfaces defined over an algebraically closed field.","Given an MMP run of a log surface $(X,D)$ we define and construct its almost minimal model, whose underlying surface has singularities not worse than $X$ and which differs from a minimal model by a contraction of some curves supported in the boundary only.","For boundaries of type $rD$, where $D$ is reduced and $r\\in [0,1]\\cap \\mathbb{Q}$, we show that if $X$ is smooth or $r\\in [0,\\frac{1}{2}]$ then the construction respects $(1-r)$-divisorial log terminality and $(1-r)$-log canonicity.","We show that the assumptions are optimal, too."],"url":"http://arxiv.org/abs/2402.07187v1","category":"math.AG"}
{"created":"2024-02-11 12:25:41","title":"Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models","abstract":"The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves the robustness of RAG-based LLMs through a highly effective detector trained on neuron activation triggered by GGPP generated prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of our methods.","sentences":["The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains.","Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs.","However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied.","In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers.","We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP).","GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers.","It can also cope with instructions in the prompts requesting to ignore irrelevant context.","We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves the robustness of RAG-based LLMs through a highly effective detector trained on neuron activation triggered by GGPP generated prompts.","Our evaluation on open-sourced LLMs demonstrates the effectiveness of our methods."],"url":"http://arxiv.org/abs/2402.07179v1","category":"cs.CL"}
{"created":"2024-02-11 09:01:42","title":"BAFLineDP: Code Bilinear Attention Fusion Framework for Line-Level Defect Prediction","abstract":"Software defect prediction aims to identify defect-prone code, aiding developers in optimizing testing resource allocation. Most defect prediction approaches primarily focus on coarse-grained, file-level defect prediction, which fails to provide developers with the precision required to locate defective code. Recently, some researchers have proposed fine-grained, line-level defect prediction methods. However, most of these approaches lack an in-depth consideration of the contextual semantics of code lines and neglect the local interaction information among code lines. To address the above issues, this paper presents a line-level defect prediction method grounded in a code bilinear attention fusion framework (BAFLineDP). This method discerns defective code files and lines by integrating source code line semantics, line-level context, and local interaction information between code lines and line-level context. Through an extensive analysis involving within- and cross-project defect prediction across 9 distinct projects encompassing 32 releases, our results demonstrate that BAFLineDP outperforms current advanced file-level and line-level defect prediction approaches.","sentences":["Software defect prediction aims to identify defect-prone code, aiding developers in optimizing testing resource allocation.","Most defect prediction approaches primarily focus on coarse-grained, file-level defect prediction, which fails to provide developers with the precision required to locate defective code.","Recently, some researchers have proposed fine-grained, line-level defect prediction methods.","However, most of these approaches lack an in-depth consideration of the contextual semantics of code lines and neglect the local interaction information among code lines.","To address the above issues, this paper presents a line-level defect prediction method grounded in a code bilinear attention fusion framework (BAFLineDP).","This method discerns defective code files and lines by integrating source code line semantics, line-level context, and local interaction information between code lines and line-level context.","Through an extensive analysis involving within- and cross-project defect prediction across 9 distinct projects encompassing 32 releases, our results demonstrate that BAFLineDP outperforms current advanced file-level and line-level defect prediction approaches."],"url":"http://arxiv.org/abs/2402.07132v1","category":"cs.SE"}
{"created":"2024-02-11 08:39:40","title":"Hermitian Rank and Rigidity of Holomorphic Mappings","abstract":"Huang's Lemma is an important tool in CR geometry to study rigidity problems. This paper introduces a generalization of Huang's Lemma based on the rigidity properties of holomorphic mappings preserving certain orthogonality on projective spaces, which is optimal for the case of partial linearity. By exploring the intricate relationship between rigidity and Huang's Lemma, we establish that the rigidity properties of Segre maps or proper holomorphic mappings between generalized balls with Levi-degenerate boundaries can be inferred from those between generalized balls with Levi-non-degenerate boundaries by in a coordinate-free and more geometric manner.","sentences":["Huang's Lemma is an important tool in CR geometry to study rigidity problems.","This paper introduces a generalization of Huang's Lemma based on the rigidity properties of holomorphic mappings preserving certain orthogonality on projective spaces, which is optimal for the case of partial linearity.","By exploring the intricate relationship between rigidity and Huang's Lemma, we establish that the rigidity properties of Segre maps or proper holomorphic mappings between generalized balls with Levi-degenerate boundaries can be inferred from those between generalized balls with Levi-non-degenerate boundaries by in a coordinate-free and more geometric manner."],"url":"http://arxiv.org/abs/2402.07126v1","category":"math.CV"}
{"created":"2024-02-11 08:20:26","title":"Empirical Analysis of Quantum Approximate Optimization Algorithm for Knapsack-based Financial Portfolio Optimization","abstract":"Portfolio optimization is a primary component of the decision-making process in finance, aiming to tactfully allocate assets to achieve optimal returns while considering various constraints. Herein, we proposed a method that uses the knapsack-based portfolio optimization problem and incorporates the quantum computing capabilities of the quantum walk mixer with the quantum approximate optimization algorithm (QAOA) to address the challenges presented by the NP-hard problem. Additionally, we present the sequential procedure of our suggested approach and demonstrate empirical proof to illustrate the effectiveness of the proposed method in finding the optimal asset allocations across various constraints and asset choices. Moreover, we discuss the effectiveness of the QAOA components in relation to our proposed method. Consequently, our study successfully achieves the approximate ratio of the portfolio optimization technique using a circuit layer of p >= 3, compared to the classical best-known solution of the knapsack problem. Our proposed methods potentially contribute to the growing field of quantum finance by offering insights into the potential benefits of employing quantum algorithms for complex optimization tasks in financial portfolio management.","sentences":["Portfolio optimization is a primary component of the decision-making process in finance, aiming to tactfully allocate assets to achieve optimal returns while considering various constraints.","Herein, we proposed a method that uses the knapsack-based portfolio optimization problem and incorporates the quantum computing capabilities of the quantum walk mixer with the quantum approximate optimization algorithm (QAOA) to address the challenges presented by the NP-hard problem.","Additionally, we present the sequential procedure of our suggested approach and demonstrate empirical proof to illustrate the effectiveness of the proposed method in finding the optimal asset allocations across various constraints and asset choices.","Moreover, we discuss the effectiveness of the QAOA components in relation to our proposed method.","Consequently, our study successfully achieves the approximate ratio of the portfolio optimization technique using a circuit layer of p >= 3, compared to the classical best-known solution of the knapsack problem.","Our proposed methods potentially contribute to the growing field of quantum finance by offering insights into the potential benefits of employing quantum algorithms for complex optimization tasks in financial portfolio management."],"url":"http://arxiv.org/abs/2402.07123v1","category":"quant-ph"}
{"created":"2024-02-11 07:06:42","title":"On Integer Programs with Irrational Data","abstract":"An integer program (IP) with a finite number of feasible solutions may have an unbounded linear programming relaxation if it contains irrational parameters, due to implicit constraints enforced by the irrational numbers. We show that those constraints can be obtained if the irrational parameters are polynomials of roots of integers over the field of rational numbers, leading to an equivalent rational formulation. We also establish a weaker result for IPs involving the general class of algebraic irrational parameters, which extends to IPs with a particular form of transcendental numbers.","sentences":["An integer program (IP) with a finite number of feasible solutions may have an unbounded linear programming relaxation if it contains irrational parameters, due to implicit constraints enforced by the irrational numbers.","We show that those constraints can be obtained if the irrational parameters are polynomials of roots of integers over the field of rational numbers, leading to an equivalent rational formulation.","We also establish a weaker result for IPs involving the general class of algebraic irrational parameters, which extends to IPs with a particular form of transcendental numbers."],"url":"http://arxiv.org/abs/2402.07117v1","category":"math.OC"}
{"created":"2024-02-11 06:21:18","title":"Towards Quantifying the Preconditioning Effect of Adam","abstract":"There is a notable dearth of results characterizing the preconditioning effect of Adam and showing how it may alleviate the curse of ill-conditioning -- an issue plaguing gradient descent (GD). In this work, we perform a detailed analysis of Adam's preconditioning effect for quadratic functions and quantify to what extent Adam can mitigate the dependence on the condition number of the Hessian. Our key finding is that Adam can suffer less from the condition number but at the expense of suffering a dimension-dependent quantity. Specifically, for a $d$-dimensional quadratic with a diagonal Hessian having condition number $\\kappa$, we show that the effective condition number-like quantity controlling the iteration complexity of Adam without momentum is $\\mathcal{O}(\\min(d, \\kappa))$. For a diagonally dominant Hessian, we obtain a bound of $\\mathcal{O}(\\min(d \\sqrt{d \\kappa}, \\kappa))$ for the corresponding quantity. Thus, when $d < \\mathcal{O}(\\kappa^p)$ where $p = 1$ for a diagonal Hessian and $p = 1/3$ for a diagonally dominant Hessian, Adam can outperform GD (which has an $\\mathcal{O}(\\kappa)$ dependence). On the negative side, our results suggest that Adam can be worse than GD for a sufficiently non-diagonal Hessian even if $d \\ll \\mathcal{O}(\\kappa^{1/3})$; we corroborate this with empirical evidence. Finally, we extend our analysis to functions satisfying per-coordinate Lipschitz smoothness and a modified version of the Polyak-\\L ojasiewicz condition.","sentences":["There is a notable dearth of results characterizing the preconditioning effect of Adam and showing how it may alleviate the curse of ill-conditioning -- an issue plaguing gradient descent (GD).","In this work, we perform a detailed analysis of Adam's preconditioning effect for quadratic functions and quantify to what extent Adam can mitigate the dependence on the condition number of the Hessian.","Our key finding is that Adam can suffer less from the condition number but at the expense of suffering a dimension-dependent quantity.","Specifically, for a $d$-dimensional quadratic with a diagonal Hessian having condition number $\\kappa$, we show that the effective condition number-like quantity controlling the iteration complexity of Adam without momentum is $\\mathcal{O}(\\min(d, \\kappa))$. For a diagonally dominant Hessian, we obtain a bound of $\\mathcal{O}(\\min(d \\sqrt{d \\kappa}, \\kappa))$ for the corresponding quantity.","Thus, when $d < \\mathcal{O}(\\kappa^p)$ where $p = 1$ for a diagonal Hessian and $p = 1/3$ for a diagonally dominant Hessian, Adam can outperform GD (which has an $\\mathcal{O}(\\kappa)$ dependence).","On the negative side, our results suggest that Adam can be worse than GD for a sufficiently non-diagonal Hessian even if $d \\ll \\mathcal{O}(\\kappa^{1/3})$; we corroborate this with empirical evidence.","Finally, we extend our analysis to functions satisfying per-coordinate Lipschitz smoothness and a modified version of the Polyak-\\L ojasiewicz condition."],"url":"http://arxiv.org/abs/2402.07114v1","category":"cs.LG"}
{"created":"2024-02-11 05:56:51","title":"Linear Stability Analysis of Oblique Couette-Poiseuille flows","abstract":"We perform a detailed numerical study of modal and non-modal stability in oblique Couette-Poiseuille profiles, which are among the simplest examples of three-dimensional boundary layers. Through a comparison with the Orr-Sommerfeld operator for the aligned case, we show how an effective wall speed succinctly characterizes modal stability. Large-scale parameter sweeps reveal that the misalignment between the pressure gradient and wall motion is, in general, destabilizing. For flows that are sufficiently oblique, the instability is found to depend exclusively on the direction of wall motion and not on its speed, a conclusion supported, in part, by the perturbation energy budget and the evolution of the critical layers. Closed forms for the critical parameters in this regime are derived using a simple analysis. Finally, a modified long-wavelength approximation is developed, and the resulting asymptotic eigenvalue problem is used to show that there is no cutoff wall speed for unconditional stability whenever the angle of wall motion is non-zero, in stark contrast to the aligned case. From a non-modal perspective, pseudo-resonance is examined through the resolvent and the $\\epsilon$-pseudospectra. An analysis of the unforced initial value problem shows that the maximum energy gain is highly dependent on both the magnitude and direction of the wall velocity. However, the strongest amplification is always achieved for configurations that are only weakly skewed. Finally, the optimal perturbations appear to develop via a lift-up effect induced by an Orr-like mechanism.","sentences":["We perform a detailed numerical study of modal and non-modal stability in oblique Couette-Poiseuille profiles, which are among the simplest examples of three-dimensional boundary layers.","Through a comparison with the Orr-Sommerfeld operator for the aligned case, we show how an effective wall speed succinctly characterizes modal stability.","Large-scale parameter sweeps reveal that the misalignment between the pressure gradient and wall motion is, in general, destabilizing.","For flows that are sufficiently oblique, the instability is found to depend exclusively on the direction of wall motion and not on its speed, a conclusion supported, in part, by the perturbation energy budget and the evolution of the critical layers.","Closed forms for the critical parameters in this regime are derived using a simple analysis.","Finally, a modified long-wavelength approximation is developed, and the resulting asymptotic eigenvalue problem is used to show that there is no cutoff wall speed for unconditional stability whenever the angle of wall motion is non-zero, in stark contrast to the aligned case.","From a non-modal perspective, pseudo-resonance is examined through the resolvent and the $\\epsilon$-pseudospectra.","An analysis of the unforced initial value problem shows that the maximum energy gain is highly dependent on both the magnitude and direction of the wall velocity.","However, the strongest amplification is always achieved for configurations that are only weakly skewed.","Finally, the optimal perturbations appear to develop via a lift-up effect induced by an Orr-like mechanism."],"url":"http://arxiv.org/abs/2402.07112v1","category":"physics.flu-dyn"}
{"created":"2024-02-11 05:35:50","title":"Decoupling Learning and Decision-Making: Breaking the $\\mathcal{O}(\\sqrt{T})$ Barrier in Online Resource Allocation with First-Order Methods","abstract":"Online linear programming plays an important role in both revenue management and resource allocation, and recent research has focused on developing efficient first-order online learning algorithms. Despite the empirical success of first-order methods, they typically achieve a regret no better than $\\mathcal{O}(\\sqrt{T})$, which is suboptimal compared to the $\\mathcal{O}(\\log T)$ bound guaranteed by the state-of-the-art linear programming (LP)-based online algorithms. This paper establishes several important facts about online linear programming, which unveils the challenge for first-order-method-based online algorithms to achieve beyond $\\mathcal{O}(\\sqrt{T})$ regret. To address the challenge, we introduce a new algorithmic framework that decouples learning from decision-making. More importantly, for the first time, we show that first-order methods can attain regret $\\mathcal{O}(T^{1/3})$ with this new framework. Lastly, we conduct numerical experiments to validate our theoretical findings.","sentences":["Online linear programming plays an important role in both revenue management and resource allocation, and recent research has focused on developing efficient first-order online learning algorithms.","Despite the empirical success of first-order methods, they typically achieve a regret no better than $\\mathcal{O}(\\sqrt{T})$, which is suboptimal compared to the $\\mathcal{O}(\\log T)$ bound guaranteed by the state-of-the-art linear programming (LP)-based online algorithms.","This paper establishes several important facts about online linear programming, which unveils the challenge for first-order-method-based online algorithms to achieve beyond $\\mathcal{O}(\\sqrt{T})$ regret.","To address the challenge, we introduce a new algorithmic framework that decouples learning from decision-making.","More importantly, for the first time, we show that first-order methods can attain regret $\\mathcal{O}(T^{1/3})$ with this new framework.","Lastly, we conduct numerical experiments to validate our theoretical findings."],"url":"http://arxiv.org/abs/2402.07108v1","category":"cs.LG"}
{"created":"2024-02-11 05:15:46","title":"Transport multi-paths with capacity constraints","abstract":"This article generalizes the study of branched/ramified optimal transportation to those with capacity constraints. Each admissible transport network studied here is represented by a transport multi-path between measures, with a capacity constraint on each of its components. The associated transport cost is given by the sum of the $\\textbf{M}_{\\alpha}$-cost of each component. Using this new formulation, we prove the existence of an optimal solution and provide an upper bound on the number of components for the solution. Additionally, we conduct analytical examinations of the properties (e.g. ``map-compatibility\", and ``simple common-source property\") of each solution component and explore the interplay among components, particularly in the discrete case.","sentences":["This article generalizes the study of branched/ramified optimal transportation to those with capacity constraints.","Each admissible transport network studied here is represented by a transport multi-path between measures, with a capacity constraint on each of its components.","The associated transport cost is given by the sum of the $\\textbf{M}_{\\alpha}$-cost of each component.","Using this new formulation, we prove the existence of an optimal solution and provide an upper bound on the number of components for the solution.","Additionally, we conduct analytical examinations of the properties (e.g. ``map-compatibility\", and ``simple common-source property\") of each solution component and explore the interplay among components, particularly in the discrete case."],"url":"http://arxiv.org/abs/2402.07106v1","category":"math.OC"}
{"created":"2024-02-11 04:59:52","title":"Learning protein-ligand unbinding pathways via single-parameter community detection","abstract":"Understanding the dynamics of biomolecular complexes, e.g., of protein-ligand (un)binding, requires the understanding of paths such systems take between metastable states. In MD simulation data, paths are usually not observable per se, but need to be inferred from simulation trajectories. Here we present a novel approach to cluster trajectories based on a community detection algorithm that requires the definition of only a single free parameter. Using the streptavidin-biotin complex as benchmark system and the A2a adenosine receptor in complex with the inhibitor ZM241385 as an elaborate application, we demonstrate how such clusters of trajectories correspond to pathways, and how the approach help in the identification of reaction coordinates for a considered (un)binding process.","sentences":["Understanding the dynamics of biomolecular complexes, e.g., of protein-ligand (un)binding, requires the understanding of paths such systems take between metastable states.","In MD simulation data, paths are usually not observable per se, but need to be inferred from simulation trajectories.","Here we present a novel approach to cluster trajectories based on a community detection algorithm that requires the definition of only a single free parameter.","Using the streptavidin-biotin complex as benchmark system and the A2a adenosine receptor in complex with the inhibitor ZM241385 as an elaborate application, we demonstrate how such clusters of trajectories correspond to pathways, and how the approach help in the identification of reaction coordinates for a considered (un)binding process."],"url":"http://arxiv.org/abs/2402.07103v1","category":"physics.comp-ph"}
{"created":"2024-02-11 04:26:35","title":"On the Complexity of First-Order Methods in Stochastic Bilevel Optimization","abstract":"We consider the problem of finding stationary points in Bilevel optimization when the lower-level problem is unconstrained and strongly convex. The problem has been extensively studied in recent years; the main technical challenge is to keep track of lower-level solutions $y^*(x)$ in response to the changes in the upper-level variables $x$. Subsequently, all existing approaches tie their analyses to a genie algorithm that knows lower-level solutions and, therefore, need not query any points far from them. We consider a dual question to such approaches: suppose we have an oracle, which we call $y^*$-aware, that returns an $O(\\epsilon)$-estimate of the lower-level solution, in addition to first-order gradient estimators {\\it locally unbiased} within the $\\Theta(\\epsilon)$-ball around $y^*(x)$. We study the complexity of finding stationary points with such an $y^*$-aware oracle: we propose a simple first-order method that converges to an $\\epsilon$ stationary point using $O(\\epsilon^{-6}), O(\\epsilon^{-4})$ access to first-order $y^*$-aware oracles. Our upper bounds also apply to standard unbiased first-order oracles, improving the best-known complexity of first-order methods by $O(\\epsilon)$ with minimal assumptions. We then provide the matching $\\Omega(\\epsilon^{-6})$, $\\Omega(\\epsilon^{-4})$ lower bounds without and with an additional smoothness assumption on $y^*$-aware oracles, respectively. Our results imply that any approach that simulates an algorithm with an $y^*$-aware oracle must suffer the same lower bounds.","sentences":["We consider the problem of finding stationary points in Bilevel optimization when the lower-level problem is unconstrained and strongly convex.","The problem has been extensively studied in recent years; the main technical challenge is to keep track of lower-level solutions $y^*(x)$ in response to the changes in the upper-level variables $x$.","Subsequently, all existing approaches tie their analyses to a genie algorithm that knows lower-level solutions and, therefore, need not query any points far from them.","We consider a dual question to such approaches: suppose we have an oracle, which we call $y^*$-aware, that returns an $O(\\epsilon)$-estimate of the lower-level solution, in addition to first-order gradient estimators {\\it locally unbiased} within the $\\Theta(\\epsilon)$-ball around $y^*(x)$. We study the complexity of finding stationary points with such an $y^*$-aware oracle: we propose a simple first-order method that converges to an $\\epsilon$ stationary point using $O(\\epsilon^{-6}), O(\\epsilon^{-4})$ access to first-order $y^*$-aware oracles.","Our upper bounds also apply to standard unbiased first-order oracles, improving the best-known complexity of first-order methods by $O(\\epsilon)$ with minimal assumptions.","We then provide the matching $\\Omega(\\epsilon^{-6})$, $\\Omega(\\epsilon^{-4})$ lower bounds without and with an additional smoothness assumption on $y^*$-aware oracles, respectively.","Our results imply that any approach that simulates an algorithm with an $y^*$-aware oracle must suffer the same lower bounds."],"url":"http://arxiv.org/abs/2402.07101v1","category":"math.OC"}
{"created":"2024-02-11 04:14:33","title":"Many-Body Eigenstates from Quantum Manifold Optimization","abstract":"Quantum computing offers several new pathways toward finding many-body eigenstates, with variational approaches being some of the most flexible and near-term oriented. These require particular parameterizations of the state, and for solving multiple eigenstates must incorporate orthogonality. In this work, we use techniques from manifold optimization to arrive at solutions of the many-body eigenstate problem via direct minimization over the Stiefel and Grassmannian manifolds, avoiding parameterizations of the states and allowing for multiple eigenstates to be simultaneously calculated. These Riemannian manifolds naturally encode orthogonality constraints and have efficient quantum representations of the states and tangent vectors. We provide example calculations for quantum many-body molecular systems and discuss different pathways for solving the multiple eigenstate problem.","sentences":["Quantum computing offers several new pathways toward finding many-body eigenstates, with variational approaches being some of the most flexible and near-term oriented.","These require particular parameterizations of the state, and for solving multiple eigenstates must incorporate orthogonality.","In this work, we use techniques from manifold optimization to arrive at solutions of the many-body eigenstate problem via direct minimization over the Stiefel and Grassmannian manifolds, avoiding parameterizations of the states and allowing for multiple eigenstates to be simultaneously calculated.","These Riemannian manifolds naturally encode orthogonality constraints and have efficient quantum representations of the states and tangent vectors.","We provide example calculations for quantum many-body molecular systems and discuss different pathways for solving the multiple eigenstate problem."],"url":"http://arxiv.org/abs/2402.07100v1","category":"quant-ph"}
{"created":"2024-02-11 01:51:15","title":"Refined Sample Complexity for Markov Games with Independent Linear Function Approximation","abstract":"Markov Games (MG) is an important model for Multi-Agent Reinforcement Learning (MARL). It was long believed that the \"curse of multi-agents\" (i.e., the algorithmic performance drops exponentially with the number of agents) is unavoidable until several recent works (Daskalakis et al., 2023; Cui et al., 2023; Wang et al., 2023. While these works did resolve the curse of multi-agents, when the state spaces are prohibitively large and (linear) function approximations are deployed, they either had a slower convergence rate of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions $A_{\\max}$ -- which is avoidable in single-agent cases even when the loss functions can arbitrarily vary with time (Dai et al., 2023). This paper first refines the `AVLPR` framework by Wang et al. (2023), with an insight of *data-dependent* (i.e., stochastic) pessimistic estimation of the sub-optimality gap, allowing a broader choice of plug-in algorithms. When specialized to MGs with independent linear function approximations, we propose novel *action-dependent bonuses* to cover occasionally extreme estimation errors. With the help of state-of-the-art techniques from the single-agent RL literature, we give the first algorithm that tackles the curse of multi-agents, attains the optimal $O(T^{-1/2})$ convergence rate, and avoids $\\text{poly}(A_{\\max})$ dependency simultaneously.","sentences":["Markov Games (MG) is an important model for Multi-Agent Reinforcement Learning (MARL).","It was long believed that the \"curse of multi-agents\" (i.e., the algorithmic performance drops exponentially with the number of agents) is unavoidable until several recent works (Daskalakis et al., 2023; Cui et al., 2023; Wang et al., 2023.","While these works did resolve the curse of multi-agents, when the state spaces are prohibitively large and (linear) function approximations are deployed, they either had a slower convergence rate of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions $A_{\\max}$ -- which is avoidable in single-agent cases even when the loss functions can arbitrarily vary with time (Dai et al., 2023).","This paper first refines the `AVLPR` framework by Wang et al. (2023), with an insight of *data-dependent* (i.e., stochastic) pessimistic estimation of the sub-optimality gap, allowing a broader choice of plug-in algorithms.","When specialized to MGs with independent linear function approximations, we propose novel *action-dependent bonuses* to cover occasionally extreme estimation errors.","With the help of state-of-the-art techniques from the single-agent RL literature, we give the first algorithm that tackles the curse of multi-agents, attains the optimal $O(T^{-1/2})$ convergence rate, and avoids $\\text{poly}(A_{\\max})$ dependency simultaneously."],"url":"http://arxiv.org/abs/2402.07082v1","category":"cs.LG"}
{"created":"2024-02-11 01:21:56","title":"The Relevance Feature and Vector Machine for health applications","abstract":"This paper presents the Relevance Feature and Vector Machine (RFVM), a novel model that addresses the challenges of the fat-data problem when dealing with clinical prospective studies. The fat-data problem refers to the limitations of Machine Learning (ML) algorithms when working with databases in which the number of features is much larger than the number of samples (a common scenario in certain medical fields). To overcome such limitations, the RFVM incorporates different characteristics: (1) A Bayesian formulation which enables the model to infer its parameters without overfitting thanks to the Bayesian model averaging. (2) A joint optimisation that overcomes the limitations arising from the fat-data characteristic by simultaneously including the variables that define the primal space (features) and those that define the dual space (observations). (3) An integrated prunning that removes the irrelevant features and samples during the training iterative optimization. Also, this last point turns out crucial when performing medical prospective studies, enabling researchers to exclude unnecessary medical tests, reducing costs and inconvenience for patients, and identifying the critical patients/subjects that characterize the disorder and, subsequently, optimize the patient recruitment process that leads to a balanced cohort. The model capabilities are tested against state-of-the-art models in several medical datasets with fat-data problems. These experimental works show that RFVM is capable of achieving competitive classification accuracies while providing the most compact subset of data (in both terms of features and samples). Moreover, the selected features (medical tests) seem to be aligned with the existing medical literature.","sentences":["This paper presents the Relevance Feature and Vector Machine (RFVM), a novel model that addresses the challenges of the fat-data problem when dealing with clinical prospective studies.","The fat-data problem refers to the limitations of Machine Learning (ML) algorithms when working with databases in which the number of features is much larger than the number of samples (a common scenario in certain medical fields).","To overcome such limitations, the RFVM incorporates different characteristics: (1) A Bayesian formulation which enables the model to infer its parameters without overfitting thanks to the Bayesian model averaging.","(2) A joint optimisation that overcomes the limitations arising from the fat-data characteristic by simultaneously including the variables that define the primal space (features) and those that define the dual space (observations).","(3) An integrated prunning that removes the irrelevant features and samples during the training iterative optimization.","Also, this last point turns out crucial when performing medical prospective studies, enabling researchers to exclude unnecessary medical tests, reducing costs and inconvenience for patients, and identifying the critical patients/subjects that characterize the disorder and, subsequently, optimize the patient recruitment process that leads to a balanced cohort.","The model capabilities are tested against state-of-the-art models in several medical datasets with fat-data problems.","These experimental works show that RFVM is capable of achieving competitive classification accuracies while providing the most compact subset of data (in both terms of features and samples).","Moreover, the selected features (medical tests) seem to be aligned with the existing medical literature."],"url":"http://arxiv.org/abs/2402.07079v1","category":"cs.LG"}
{"created":"2024-02-11 00:22:17","title":"Modeling of Key Quality Indicators for End-to-End Network Management: Preparing for 5G","abstract":"Thanks to evolving cellular telecommunication networks, providers can deploy a wide range of services. Soon, 5G mobile networks will be available to handle all types of services and applications for vast numbers of users through their mobile equipment. To effectively manage new 5G systems, end-to-end (E2E) performance analysis and optimization will be key features. However, estimating the end-user experience is not an easy task for network operators. The amount of end-user performance information operators can measure from the network is limited, complicating this approach. Here we explore the calculation of service metrics [known as key quality indicators (KQIs)] from classic low-layer measurements and parameters. We propose a complete machine-learning (ML) modeling framework. This system's low-layer metrics can be applied to measure service-layer performance. To assess the approach, we implemented and evaluated the proposed system on a real cellular network testbed.","sentences":["Thanks to evolving cellular telecommunication networks, providers can deploy a wide range of services.","Soon, 5G mobile networks will be available to handle all types of services and applications for vast numbers of users through their mobile equipment.","To effectively manage new 5G systems, end-to-end (E2E) performance analysis and optimization will be key features.","However, estimating the end-user experience is not an easy task for network operators.","The amount of end-user performance information operators can measure from the network is limited, complicating this approach.","Here we explore the calculation of service metrics [known as key quality indicators (KQIs)] from classic low-layer measurements and parameters.","We propose a complete machine-learning (ML) modeling framework.","This system's low-layer metrics can be applied to measure service-layer performance.","To assess the approach, we implemented and evaluated the proposed system on a real cellular network testbed."],"url":"http://arxiv.org/abs/2402.07071v1","category":"cs.NI"}
{"created":"2024-02-10 23:13:17","title":"Exact Semi-Definite Programs for Piecewise SOS-Convex Moment Optimization and Applications","abstract":"In this paper, we present exact Semi-Definite Program (SDP) reformulations for a broad class of infinite-dimensional moment optimization problems involving piecewise Sums-of-Squares (SOS) convex polynomials and projected spectrahedral support sets. These reformulations show that the optimal value of the original moment problem can be found by solving a single SDP. In particular, we show how an optimal probability measure is recovered for these problems by solving a single SDP. This is done by first establishing an SOS representation for the non-negativity of a piecewise SOS-convex polynomial on a projected spectrahedron. Finally, as an application and a proof-of-concept, we present numerical results for Newsvendor and revenue maximization problems with higher-order moments by solving their equivalent SDP reformulations. They promise a flexible and efficient approach to solving these models. The main novelty of the present work in relation to the recent research lies in finding the solution to a moment problem, for the first time, with the piecewise SOS-convex functions from its numerically tractable exact SDP reformulation.","sentences":["In this paper, we present exact Semi-Definite Program (SDP) reformulations for a broad class of infinite-dimensional moment optimization problems involving piecewise Sums-of-Squares (SOS) convex polynomials and projected spectrahedral support sets.","These reformulations show that the optimal value of the original moment problem can be found by solving a single SDP.","In particular, we show how an optimal probability measure is recovered for these problems by solving a single SDP.","This is done by first establishing an SOS representation for the non-negativity of a piecewise SOS-convex polynomial on a projected spectrahedron.","Finally, as an application and a proof-of-concept, we present numerical results for Newsvendor and revenue maximization problems with higher-order moments by solving their equivalent SDP reformulations.","They promise a flexible and efficient approach to solving these models.","The main novelty of the present work in relation to the recent research lies in finding the solution to a moment problem, for the first time, with the piecewise SOS-convex functions from its numerically tractable exact SDP reformulation."],"url":"http://arxiv.org/abs/2402.07064v1","category":"math.OC"}
{"created":"2024-02-10 23:09:40","title":"On the Convergence Rate of MCTS for the Optimal Value Estimation in Markov Decision Processes","abstract":"A recent theoretical analysis of a corrected-version of a Monte-Carlo tree search (MCTS) method, so-called UCT, established an unexpected result, due to a great deal of empirical successes reported from heuristic usage of UCT with relevant adjustments for the problem domains in the literature, that its convergence rate in estimating the expected error relative to the optimal value of a finite-horizon Markov decision process (MDP) at an initial state is polynomial. We strengthen this dispiriting slow-convergence result by arguing within a simpler framework in the perspective of MDP, apart from the usual MCTS description, that just simpler UCB1 applied with the policy set as the arm set is actually competitive with or asymptotically faster than the corrected-version of UCT because of its logarithmic convergence-rate. We also point out that MCTS in general has the worst-case time and space complexities that depend on the state-set size which contradicts the original spirit of MCTS. Unless heuristically used, UCT-based MCTS has yet to have theoretical supports for its applicabilities.","sentences":["A recent theoretical analysis of a corrected-version of a Monte-Carlo tree search (MCTS) method, so-called UCT, established an unexpected result, due to a great deal of empirical successes reported from heuristic usage of UCT with relevant adjustments for the problem domains in the literature, that its convergence rate in estimating the expected error relative to the optimal value of a finite-horizon Markov decision process (MDP) at an initial state is polynomial.","We strengthen this dispiriting slow-convergence result by arguing within a simpler framework in the perspective of MDP, apart from the usual MCTS description, that just simpler UCB1 applied with the policy set as the arm set is actually competitive with or asymptotically faster than the corrected-version of UCT because of its logarithmic convergence-rate.","We also point out that MCTS in general has the worst-case time and space complexities that depend on the state-set size which contradicts the original spirit of MCTS.","Unless heuristically used, UCT-based MCTS has yet to have theoretical supports for its applicabilities."],"url":"http://arxiv.org/abs/2402.07063v1","category":"math.OC"}
{"created":"2024-02-10 22:38:21","title":"Fast UCB-type algorithms for stochastic bandits with heavy and super heavy symmetric noise","abstract":"In this study, we propose a new method for constructing UCB-type algorithms for stochastic multi-armed bandits based on general convex optimization methods with an inexact oracle. We derive the regret bounds corresponding to the convergence rates of the optimization methods. We propose a new algorithm Clipped-SGD-UCB and show, both theoretically and empirically, that in the case of symmetric noise in the reward, we can achieve an $O(\\log T\\sqrt{KT\\log T})$ regret bound instead of $O\\left (T^{\\frac{1}{1+\\alpha}} K^{\\frac{\\alpha}{1+\\alpha}} \\right)$ for the case when the reward distribution satisfies $\\mathbb{E}_{X \\in D}[|X|^{1+\\alpha}] \\leq \\sigma^{1+\\alpha}$ ($\\alpha \\in (0, 1])$, i.e. perform better than it is assumed by the general lower bound for bandits with heavy-tails. Moreover, the same bound holds even when the reward distribution does not have the expectation, that is, when $\\alpha<0$.","sentences":["In this study, we propose a new method for constructing UCB-type algorithms for stochastic multi-armed bandits based on general convex optimization methods with an inexact oracle.","We derive the regret bounds corresponding to the convergence rates of the optimization methods.","We propose a new algorithm Clipped-SGD-UCB and show, both theoretically and empirically, that in the case of symmetric noise in the reward, we can achieve an $O(\\log T\\sqrt{KT\\log T})$ regret bound instead of $O\\left (T^{\\frac{1}{1+\\alpha}} K^{\\frac{\\alpha}{1+\\alpha}} \\right)$ for the case when the reward distribution satisfies $\\mathbb{E}_{X \\in D}[|X|^{1+\\alpha}]","\\leq \\sigma^{1+\\alpha}$ ($\\alpha \\in (0, 1])$, i.e. perform better than it is assumed by the general lower bound for bandits with heavy-tails.","Moreover, the same bound holds even when the reward distribution does not have the expectation, that is, when $\\alpha<0$."],"url":"http://arxiv.org/abs/2402.07062v1","category":"cs.LG"}
{"created":"2024-02-10 22:09:08","title":"Efficient ($\\sim$10$\\%$) Generation of Vacuum Ultraviolet Femtosecond Pulses via Four-Wave Mixing in Hollow-Core Fibers","abstract":"We report the generation of the 5th harmonic of Ti:sapphire, at 160 nm, with more than 4~$\\mu$J of pulse energy, and a pulse length of 37 fs with a 1 kHz repetition rate. The VUV pulses are produced using Four-Wave Difference Frequency Mixing (FWDFM) in a helium-filled stretched Hollow-Core Fiber (HCF), driven by a pump at 267 nm and seeded at 800 nm. Guided by simulations using Luna.jl, we are able to optimize the process carefully. The result is a conversion efficiency of $\\sim$10$\\%$ from the 267 nm pump beam, rivaling the efficient optical mixing schemes in nonlinear crystals.","sentences":["We report the generation of the 5th harmonic of Ti:sapphire, at 160 nm, with more than 4~$\\mu$J of pulse energy, and a pulse length of 37 fs with a 1 kHz repetition rate.","The VUV pulses are produced using Four-Wave Difference Frequency Mixing (FWDFM) in a helium-filled stretched Hollow-Core Fiber (HCF), driven by a pump at 267 nm and seeded at 800 nm.","Guided by simulations using Luna.jl, we are able to optimize the process carefully.","The result is a conversion efficiency of $\\sim$10$\\%$ from the 267 nm pump beam, rivaling the efficient optical mixing schemes in nonlinear crystals."],"url":"http://arxiv.org/abs/2402.07056v1","category":"physics.optics"}
{"created":"2024-02-10 21:51:59","title":"Understanding the Training Speedup from Sampling with Approximate Losses","abstract":"It is well known that selecting samples with large losses/gradients can significantly reduce the number of training steps. However, the selection overhead is often too high to yield any meaningful gains in terms of overall training time. In this work, we focus on the greedy approach of selecting samples with large \\textit{approximate losses} instead of exact losses in order to reduce the selection overhead. For smooth convex losses, we show that such a greedy strategy can converge to a constant factor of the minimum value of the average loss in fewer iterations than the standard approach of random selection. We also theoretically quantify the effect of the approximation level. We then develop SIFT which uses early exiting to obtain approximate losses with an intermediate layer's representations for sample selection. We evaluate SIFT on the task of training a 110M parameter 12-layer BERT base model and show significant gains (in terms of training hours and number of backpropagation steps) without any optimized implementation over vanilla training. For e.g., to reach 64% validation accuracy, SIFT with exit at the first layer takes ~43 hours compared to ~57 hours of vanilla training.","sentences":["It is well known that selecting samples with large losses/gradients can significantly reduce the number of training steps.","However, the selection overhead is often too high to yield any meaningful gains in terms of overall training time.","In this work, we focus on the greedy approach of selecting samples with large \\textit{approximate losses} instead of exact losses in order to reduce the selection overhead.","For smooth convex losses, we show that such a greedy strategy can converge to a constant factor of the minimum value of the average loss in fewer iterations than the standard approach of random selection.","We also theoretically quantify the effect of the approximation level.","We then develop SIFT which uses early exiting to obtain approximate losses with an intermediate layer's representations for sample selection.","We evaluate SIFT on the task of training a 110M parameter 12-layer BERT base model and show significant gains (in terms of training hours and number of backpropagation steps) without any optimized implementation over vanilla training.","For e.g., to reach 64% validation accuracy, SIFT with exit at the first layer takes ~43 hours compared to ~57 hours of vanilla training."],"url":"http://arxiv.org/abs/2402.07052v1","category":"cs.LG"}
{"created":"2024-02-10 19:21:29","title":"Quantum Speedup for Spectral Approximation of Kronecker Products","abstract":"Given its widespread application in machine learning and optimization, the Kronecker product emerges as a pivotal linear algebra operator. However, its computational demands render it an expensive operation, leading to heightened costs in spectral approximation of it through traditional computation algorithms. Existing classical methods for spectral approximation exhibit a linear dependency on the matrix dimension denoted by $n$, considering matrices of size $A_1 \\in \\mathbb{R}^{n \\times d}$ and $A_2 \\in \\mathbb{R}^{n \\times d}$. Our work introduces an innovative approach to efficiently address the spectral approximation of the Kronecker product $A_1 \\otimes A_2$ using quantum methods. By treating matrices as quantum states, our proposed method significantly reduces the time complexity of spectral approximation to $O_{d,\\epsilon}(\\sqrt{n})$.","sentences":["Given its widespread application in machine learning and optimization, the Kronecker product emerges as a pivotal linear algebra operator.","However, its computational demands render it an expensive operation, leading to heightened costs in spectral approximation of it through traditional computation algorithms.","Existing classical methods for spectral approximation exhibit a linear dependency on the matrix dimension denoted by $n$, considering matrices of size $A_1 \\in \\mathbb{R}^{n \\times d}$ and $A_2 \\in \\mathbb{R}^{n \\times d}$. Our work introduces an innovative approach to efficiently address the spectral approximation of the Kronecker product $A_1 \\otimes A_2$ using quantum methods.","By treating matrices as quantum states, our proposed method significantly reduces the time complexity of spectral approximation to $O_{d,\\epsilon}(\\sqrt{n})$."],"url":"http://arxiv.org/abs/2402.07027v1","category":"cs.DS"}
{"created":"2024-02-10 18:03:15","title":"An Optimization Framework for Processing and Transfer Learning for the Brain Tumor Segmentation","abstract":"Tumor segmentation from multi-modal brain MRI images is a challenging task due to the limited samples, high variance in shapes and uneven distribution of tumor morphology. The performance of automated medical image segmentation has been significant improvement by the recent advances in deep learning. However, the model predictions have not yet reached the desired level for clinical use in terms of accuracy and generalizability. In order to address the distinct problems presented in Challenges 1, 2, and 3 of BraTS 2023, we have constructed an optimization framework based on a 3D U-Net model for brain tumor segmentation. This framework incorporates a range of techniques, including various pre-processing and post-processing techniques, and transfer learning. On the validation datasets, this multi-modality brain tumor segmentation framework achieves an average lesion-wise Dice score of 0.79, 0.72, 0.74 on Challenges 1, 2, 3 respectively.","sentences":["Tumor segmentation from multi-modal brain MRI images is a challenging task due to the limited samples, high variance in shapes and uneven distribution of tumor morphology.","The performance of automated medical image segmentation has been significant improvement by the recent advances in deep learning.","However, the model predictions have not yet reached the desired level for clinical use in terms of accuracy and generalizability.","In order to address the distinct problems presented in Challenges 1, 2, and 3 of BraTS 2023, we have constructed an optimization framework based on a 3D U-Net model for brain tumor segmentation.","This framework incorporates a range of techniques, including various pre-processing and post-processing techniques, and transfer learning.","On the validation datasets, this multi-modality brain tumor segmentation framework achieves an average lesion-wise Dice score of 0.79, 0.72, 0.74 on Challenges 1, 2, 3 respectively."],"url":"http://arxiv.org/abs/2402.07008v1","category":"eess.IV"}
{"created":"2024-02-10 17:34:51","title":"Under Pressure: Comparative Statics for Optimal Stopping Problems in Nonstationary Environments","abstract":"We present a general optimal stopping problem accommodating a broad spectrum of nonstationary environments. These include scenarios where the decision maker's patience (i.e., discount rate), time pressure (i.e., arrival rate of a stochastic deadline), learning speed (i.e., volatility of the diffusion process) can change over time both gradually and abruptly. Our paper offers three main contributions. First, we prove this general problem has a well-defined solution under mild regularity conditions. Second, we develop comprehensive comparative statics results that are crucial to characterize the shape of the stopping region in the broad class of \\textit{monotone environments}. Finally, we leverage these comparative statics to examine the speed-accuracy tradeoffs in various information acquisition problems, revealing how decision accuracy varies over time in response to changes in the discount rate, learning speed, or deadline-induced time pressure. Notably, our main comparative static results hold locally and thus can also capture non-monotone relations.","sentences":["We present a general optimal stopping problem accommodating a broad spectrum of nonstationary environments.","These include scenarios where the decision maker's patience (i.e., discount rate), time pressure (i.e., arrival rate of a stochastic deadline), learning speed (i.e., volatility of the diffusion process) can change over time both gradually and abruptly.","Our paper offers three main contributions.","First, we prove this general problem has a well-defined solution under mild regularity conditions.","Second, we develop comprehensive comparative statics results that are crucial to characterize the shape of the stopping region in the broad class of \\textit{monotone environments}.","Finally, we leverage these comparative statics to examine the speed-accuracy tradeoffs in various information acquisition problems, revealing how decision accuracy varies over time in response to changes in the discount rate, learning speed, or deadline-induced time pressure.","Notably, our main comparative static results hold locally and thus can also capture non-monotone relations."],"url":"http://arxiv.org/abs/2402.06999v1","category":"econ.TH"}
{"created":"2024-02-10 16:52:22","title":"Reciprocal Visibility","abstract":"We propose a guidance strategy to optimize real-time synthetic aperture sampling for occlusion removal with drones by pre-scanned point-cloud data. Depth information can be used to compute visibility of points on the ground for individual drone positions in the air. Inspired by Helmholtz reciprocity, we introduce reciprocal visibility to determine the dual situation - the visibility of potential sampling position in the air from given points of interest on the ground. The resulting visibility map encodes which point on the ground is visible by which magnitude from any position in the air. Based on such a map, we demonstrate a first greedy sampling optimization.","sentences":["We propose a guidance strategy to optimize real-time synthetic aperture sampling for occlusion removal with drones by pre-scanned point-cloud data.","Depth information can be used to compute visibility of points on the ground for individual drone positions in the air.","Inspired by Helmholtz reciprocity, we introduce reciprocal visibility to determine the dual situation - the visibility of potential sampling position in the air from given points of interest on the ground.","The resulting visibility map encodes which point on the ground is visible by which magnitude from any position in the air.","Based on such a map, we demonstrate a first greedy sampling optimization."],"url":"http://arxiv.org/abs/2402.06991v1","category":"cs.CV"}
{"created":"2024-02-10 16:09:56","title":"Structures vibration control via tuned mass dampers using a co-evolution coral reefs optimization algorithm","abstract":"In this paper we tackle a problem of optimal design and location of Tuned Mass Dampers (TMDs) for structures subjected to earthquake ground motions, using a novel meta-heuristic algorithm. Specifically, the Coral Reefs Optimization (CRO) with Substrate Layer (CRO-SL) is proposed as a competitive co-evolution algorithm with different exploration procedures within a single population of solutions. The proposed approach is able to solve the TMD design and location problem, by exploiting the combination of different types of searching mechanisms. This promotes a powerful evolutionary-like algorithm for optimization problems, which is shown to be very effective in this particular problem of TMDs tuning. The proposed algorithm's performance has been evaluated and compared with several reference algorithms in two building models with two and four floors, respectively.","sentences":["In this paper we tackle a problem of optimal design and location of Tuned Mass Dampers (TMDs) for structures subjected to earthquake ground motions, using a novel meta-heuristic algorithm.","Specifically, the Coral Reefs Optimization (CRO) with Substrate Layer (CRO-SL) is proposed as a competitive co-evolution algorithm with different exploration procedures within a single population of solutions.","The proposed approach is able to solve the TMD design and location problem, by exploiting the combination of different types of searching mechanisms.","This promotes a powerful evolutionary-like algorithm for optimization problems, which is shown to be very effective in this particular problem of TMDs tuning.","The proposed algorithm's performance has been evaluated and compared with several reference algorithms in two building models with two and four floors, respectively."],"url":"http://arxiv.org/abs/2402.06981v1","category":"eess.SY"}
{"created":"2024-02-10 15:23:45","title":"In-Context Data Distillation with TabPFN","abstract":"Foundation models have revolutionized tasks in computer vision and natural language processing. However, in the realm of tabular data, tree-based models like XGBoost continue to dominate. TabPFN, a transformer model tailored for tabular data, mirrors recent foundation models in its exceptional in-context learning capability, being competitive with XGBoost's performance without the need for task-specific training or hyperparameter tuning. Despite its promise, TabPFN's applicability is hindered by its data size constraint, limiting its use in real-world scenarios. To address this, we present in-context data distillation (ICD), a novel methodology that effectively eliminates these constraints by optimizing TabPFN's context. ICD efficiently enables TabPFN to handle significantly larger datasets with a fixed memory budget, improving TabPFN's quadratic memory complexity but at the cost of a linear number of tuning steps. Notably, TabPFN, enhanced with ICD, demonstrates very strong performance against established tree-based models and modern deep learning methods on 48 large tabular datasets from OpenML.","sentences":["Foundation models have revolutionized tasks in computer vision and natural language processing.","However, in the realm of tabular data, tree-based models like XGBoost continue to dominate.","TabPFN, a transformer model tailored for tabular data, mirrors recent foundation models in its exceptional in-context learning capability, being competitive with XGBoost's performance without the need for task-specific training or hyperparameter tuning.","Despite its promise, TabPFN's applicability is hindered by its data size constraint, limiting its use in real-world scenarios.","To address this, we present in-context data distillation (ICD), a novel methodology that effectively eliminates these constraints by optimizing TabPFN's context.","ICD efficiently enables TabPFN to handle significantly larger datasets with a fixed memory budget, improving TabPFN's quadratic memory complexity but at the cost of a linear number of tuning steps.","Notably, TabPFN, enhanced with ICD, demonstrates very strong performance against established tree-based models and modern deep learning methods on 48 large tabular datasets from OpenML."],"url":"http://arxiv.org/abs/2402.06971v1","category":"cs.LG"}
{"created":"2024-02-10 14:56:36","title":"Contextual Stochastic Vehicle Routing with Time Windows","abstract":"We study the vehicle routing problem with time windows (VRPTW) and stochastic travel times, in which the decision-maker observes related contextual information, represented as feature variables, before making routing decisions. Despite the extensive literature on stochastic VRPs, the integration of feature variables has received limited attention in this context. We introduce the contextual stochastic VRPTW, which minimizes the total transportation cost and expected late arrival penalties conditioned on the observed features. Since the joint distribution of travel times and features is unknown, we present novel data-driven prescriptive models that use historical data to provide an approximate solution to the problem. We distinguish the prescriptive models between point-based approximation, sample average approximation, and penalty-based approximation, each taking a different perspective on dealing with stochastic travel times and features. We develop specialized branch-price-and-cut algorithms to solve these data-driven prescriptive models. In our computational experiments, we compare the out-of-sample cost performance of different methods on instances with up to one hundred customers. Our results show that, surprisingly, a feature-dependent sample average approximation outperforms existing and novel methods in most settings.","sentences":["We study the vehicle routing problem with time windows (VRPTW) and stochastic travel times, in which the decision-maker observes related contextual information, represented as feature variables, before making routing decisions.","Despite the extensive literature on stochastic VRPs, the integration of feature variables has received limited attention in this context.","We introduce the contextual stochastic VRPTW, which minimizes the total transportation cost and expected late arrival penalties conditioned on the observed features.","Since the joint distribution of travel times and features is unknown, we present novel data-driven prescriptive models that use historical data to provide an approximate solution to the problem.","We distinguish the prescriptive models between point-based approximation, sample average approximation, and penalty-based approximation, each taking a different perspective on dealing with stochastic travel times and features.","We develop specialized branch-price-and-cut algorithms to solve these data-driven prescriptive models.","In our computational experiments, we compare the out-of-sample cost performance of different methods on instances with up to one hundred customers.","Our results show that, surprisingly, a feature-dependent sample average approximation outperforms existing and novel methods in most settings."],"url":"http://arxiv.org/abs/2402.06968v1","category":"cs.LG"}
{"created":"2024-02-10 14:29:27","title":"Zepyros: A webserver to evaluate the shape complementarity of protein-protein interfaces","abstract":"Shape complementarity of molecular surfaces at the interfaces is a well-known characteristic of protein-protein binding regions, and it is critical in influencing the stability of the complex. Measuring such complementarity is at the basis of methods for both the prediction of possible interactions and for the design/optimization of speficic ones. However, only a limited number of tools are currently available to efficiently and rapidly assess it. Here, we introduce Zepyros, a webserver for fast measuring of the shape complementarity between two molecular interfaces of a given protein-protein complex using structural information. Zepyros is implemented as a publicly available tool with a user-friendly interface. Our server can be found at the following link (all major browser supported): https://zepyros.bio-groups.com","sentences":["Shape complementarity of molecular surfaces at the interfaces is a well-known characteristic of protein-protein binding regions, and it is critical in influencing the stability of the complex.","Measuring such complementarity is at the basis of methods for both the prediction of possible interactions and for the design/optimization of speficic ones.","However, only a limited number of tools are currently available to efficiently and rapidly assess it.","Here, we introduce Zepyros, a webserver for fast measuring of the shape complementarity between two molecular interfaces of a given protein-protein complex using structural information.","Zepyros is implemented as a publicly available tool with a user-friendly interface.","Our server can be found at the following link (all major browser supported): https://zepyros.bio-groups.com"],"url":"http://arxiv.org/abs/2402.06960v1","category":"q-bio.QM"}
{"created":"2024-02-10 13:39:44","title":"Semantic Object-level Modeling for Robust Visual Camera Relocalization","abstract":"Visual relocalization is crucial for autonomous visual localization and navigation of mobile robotics. Due to the improvement of CNN-based object detection algorithm, the robustness of visual relocalization is greatly enhanced especially in viewpoints where classical methods fail. However, ellipsoids (quadrics) generated by axis-aligned object detection may limit the accuracy of the object-level representation and degenerate the performance of visual relocalization system. In this paper, we propose a novel method of automatic object-level voxel modeling for accurate ellipsoidal representations of objects. As for visual relocalization, we design a better pose optimization strategy for camera pose recovery, to fully utilize the projection characteristics of 2D fitted ellipses and the 3D accurate ellipsoids. All of these modules are entirely intergrated into visual SLAM system. Experimental results show that our semantic object-level mapping and object-based visual relocalization methods significantly enhance the performance of visual relocalization in terms of robustness to new viewpoints.","sentences":["Visual relocalization is crucial for autonomous visual localization and navigation of mobile robotics.","Due to the improvement of CNN-based object detection algorithm, the robustness of visual relocalization is greatly enhanced especially in viewpoints where classical methods fail.","However, ellipsoids (quadrics) generated by axis-aligned object detection may limit the accuracy of the object-level representation and degenerate the performance of visual relocalization system.","In this paper, we propose a novel method of automatic object-level voxel modeling for accurate ellipsoidal representations of objects.","As for visual relocalization, we design a better pose optimization strategy for camera pose recovery, to fully utilize the projection characteristics of 2D fitted ellipses and the 3D accurate ellipsoids.","All of these modules are entirely intergrated into visual SLAM system.","Experimental results show that our semantic object-level mapping and object-based visual relocalization methods significantly enhance the performance of visual relocalization in terms of robustness to new viewpoints."],"url":"http://arxiv.org/abs/2402.06951v1","category":"cs.CV"}
