{"created":"2024-04-19 17:59:48","title":"MoVA: Adapting Mixture of Vision Experts to Multimodal Context","abstract":"As the key component in multimodal large language models (MLLMs), the ability of the visual encoder greatly affects MLLM's understanding on diverse image content. Although some large-scale pretrained vision encoders such as vision encoders in CLIP and DINOv2 have brought promising performance, we found that there is still no single vision encoder that can dominate various image content understanding, e.g., the CLIP vision encoder leads to outstanding results on general image understanding but poor performance on document or chart content. To alleviate the bias of CLIP vision encoder, we first delve into the inherent behavior of different pre-trained vision encoders and then propose the MoVA, a powerful and novel MLLM, adaptively routing and fusing task-specific vision experts with a coarse-to-fine mechanism. In the coarse-grained stage, we design a context-aware expert routing strategy to dynamically select the most suitable vision experts according to the user instruction, input image, and expertise of vision experts. This benefits from the powerful model function understanding ability of the large language model (LLM) equipped with expert-routing low-rank adaptation (LoRA). In the fine-grained stage, we elaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to extract and fuse task-specific knowledge from various experts. This coarse-to-fine paradigm effectively leverages representations from experts based on multimodal context and model expertise, further enhancing the generalization ability. We conduct extensive experiments to evaluate the effectiveness of the proposed approach. Without any bells and whistles, MoVA can achieve significant performance gains over current state-of-the-art methods in a wide range of challenging multimodal benchmarks. Codes and models will be available at https://github.com/TempleX98/MoVA.","sentences":["As the key component in multimodal large language models (MLLMs), the ability of the visual encoder greatly affects MLLM's understanding on diverse image content.","Although some large-scale pretrained vision encoders such as vision encoders in CLIP and DINOv2 have brought promising performance, we found that there is still no single vision encoder that can dominate various image content understanding, e.g., the CLIP vision encoder leads to outstanding results on general image understanding but poor performance on document or chart content.","To alleviate the bias of CLIP vision encoder, we first delve into the inherent behavior of different pre-trained vision encoders and then propose the MoVA, a powerful and novel MLLM, adaptively routing and fusing task-specific vision experts with a coarse-to-fine mechanism.","In the coarse-grained stage, we design a context-aware expert routing strategy to dynamically select the most suitable vision experts according to the user instruction, input image, and expertise of vision experts.","This benefits from the powerful model function understanding ability of the large language model (LLM) equipped with expert-routing low-rank adaptation (LoRA).","In the fine-grained stage, we elaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to extract and fuse task-specific knowledge from various experts.","This coarse-to-fine paradigm effectively leverages representations from experts based on multimodal context and model expertise, further enhancing the generalization ability.","We conduct extensive experiments to evaluate the effectiveness of the proposed approach.","Without any bells and whistles, MoVA can achieve significant performance gains over current state-of-the-art methods in a wide range of challenging multimodal benchmarks.","Codes and models will be available at https://github.com/TempleX98/MoVA."],"url":"http://arxiv.org/abs/2404.13046v1","category":"cs.CV"}
{"created":"2024-04-19 17:57:29","title":"Data Alignment for Zero-Shot Concept Generation in Dermatology AI","abstract":"AI in dermatology is evolving at a rapid pace but the major limitation to training trustworthy classifiers is the scarcity of data with ground-truth concept level labels, which are meta-labels semantically meaningful to humans. Foundation models like CLIP providing zero-shot capabilities can help alleviate this challenge by leveraging vast amounts of image-caption pairs available on the internet. CLIP can be fine-tuned using domain specific image-caption pairs to improve classification performance. However, CLIP's pre-training data is not well-aligned with the medical jargon that clinicians use to perform diagnoses. The development of large language models (LLMs) in recent years has led to the possibility of leveraging the expressive nature of these models to generate rich text. Our goal is to use these models to generate caption text that aligns well with both the clinical lexicon and with the natural human language used in CLIP's pre-training data. Starting with captions used for images in PubMed articles, we extend them by passing the raw captions through an LLM fine-tuned on the field's several textbooks. We find that using captions generated by an expressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot concept classification performance.","sentences":["AI in dermatology is evolving at a rapid pace but the major limitation to training trustworthy classifiers is the scarcity of data with ground-truth concept level labels, which are meta-labels semantically meaningful to humans.","Foundation models like CLIP providing zero-shot capabilities can help alleviate this challenge by leveraging vast amounts of image-caption pairs available on the internet.","CLIP can be fine-tuned using domain specific image-caption pairs to improve classification performance.","However, CLIP's pre-training data is not well-aligned with the medical jargon that clinicians use to perform diagnoses.","The development of large language models (LLMs) in recent years has led to the possibility of leveraging the expressive nature of these models to generate rich text.","Our goal is to use these models to generate caption text that aligns well with both the clinical lexicon and with the natural human language used in CLIP's pre-training data.","Starting with captions used for images in PubMed articles, we extend them by passing the raw captions through an LLM fine-tuned on the field's several textbooks.","We find that using captions generated by an expressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot concept classification performance."],"url":"http://arxiv.org/abs/2404.13043v1","category":"cs.CV"}
{"created":"2024-04-19 17:54:58","title":"Reduction systems and degree bounds for integration","abstract":"In symbolic integration, the Risch--Norman algorithm aims to find closed forms of elementary integrals over differential fields by an ansatz for the integral, which usually is based on heuristic degree bounds. Norman presented an approach that avoids degree bounds and only relies on the completion of reduction systems. We give a formalization of his approach and we develop a refined completion process, which terminates in more instances. In some situations when the algorithm does not terminate, one can detect patterns allowing to still describe infinite reduction systems that are complete. We present such infinite systems for the fields generated by Airy functions and complete elliptic integrals, respectively. Moreover, we show how complete reduction systems can be used to find rigorous degree bounds. In particular, we give a general formula for weighted degree bounds and we apply it to find tight bounds for above examples.","sentences":["In symbolic integration, the Risch--Norman algorithm aims to find closed forms of elementary integrals over differential fields by an ansatz for the integral, which usually is based on heuristic degree bounds.","Norman presented an approach that avoids degree bounds and only relies on the completion of reduction systems.","We give a formalization of his approach and we develop a refined completion process, which terminates in more instances.","In some situations when the algorithm does not terminate, one can detect patterns allowing to still describe infinite reduction systems that are complete.","We present such infinite systems for the fields generated by Airy functions and complete elliptic integrals, respectively.","Moreover, we show how complete reduction systems can be used to find rigorous degree bounds.","In particular, we give a general formula for weighted degree bounds and we apply it to find tight bounds for above examples."],"url":"http://arxiv.org/abs/2404.13042v1","category":"cs.SC"}
{"created":"2024-04-19 17:53:43","title":"Analysis of Classifier-Free Guidance Weight Schedulers","abstract":"Classifier-Free Guidance (CFG) enhances the quality and condition adherence of text-to-image diffusion models. It operates by combining the conditional and unconditional predictions using a fixed weight. However, recent works vary the weights throughout the diffusion process, reporting superior results but without providing any rationale or analysis. By conducting comprehensive experiments, this paper provides insights into CFG weight schedulers. Our findings suggest that simple, monotonically increasing weight schedulers consistently lead to improved performances, requiring merely a single line of code. In addition, more complex parametrized schedulers can be optimized for further improvement, but do not generalize across different models and tasks.","sentences":["Classifier-Free Guidance (CFG) enhances the quality and condition adherence of text-to-image diffusion models.","It operates by combining the conditional and unconditional predictions using a fixed weight.","However, recent works vary the weights throughout the diffusion process, reporting superior results but without providing any rationale or analysis.","By conducting comprehensive experiments, this paper provides insights into CFG weight schedulers.","Our findings suggest that simple, monotonically increasing weight schedulers consistently lead to improved performances, requiring merely a single line of code.","In addition, more complex parametrized schedulers can be optimized for further improvement, but do not generalize across different models and tasks."],"url":"http://arxiv.org/abs/2404.13040v1","category":"cs.CV"}
{"created":"2024-04-19 17:51:52","title":"LaPA: Latent Prompt Assist Model For Medical Visual Question Answering","abstract":"Medical visual question answering (Med-VQA) aims to automate the prediction of correct answers for medical images and questions, thereby assisting physicians in reducing repetitive tasks and alleviating their workload. Existing approaches primarily focus on pre-training models using additional and comprehensive datasets, followed by fine-tuning to enhance performance in downstream tasks. However, there is also significant value in exploring existing models to extract clinically relevant information. In this paper, we propose the Latent Prompt Assist model (LaPA) for medical visual question answering. Firstly, we design a latent prompt generation module to generate the latent prompt with the constraint of the target answer. Subsequently, we propose a multi-modal fusion block with latent prompt fusion module that utilizes the latent prompt to extract clinical-relevant information from uni-modal and multi-modal features. Additionally, we introduce a prior knowledge fusion module to integrate the relationship between diseases and organs with the clinical-relevant information. Finally, we combine the final integrated information with image-language cross-modal information to predict the final answers. Experimental results on three publicly available Med-VQA datasets demonstrate that LaPA outperforms the state-of-the-art model ARL, achieving improvements of 1.83%, 0.63%, and 1.80% on VQA-RAD, SLAKE, and VQA-2019, respectively. The code is publicly available at https://github.com/GaryGuTC/LaPA_model.","sentences":["Medical visual question answering (Med-VQA) aims to automate the prediction of correct answers for medical images and questions, thereby assisting physicians in reducing repetitive tasks and alleviating their workload.","Existing approaches primarily focus on pre-training models using additional and comprehensive datasets, followed by fine-tuning to enhance performance in downstream tasks.","However, there is also significant value in exploring existing models to extract clinically relevant information.","In this paper, we propose the Latent Prompt Assist model (LaPA) for medical visual question answering.","Firstly, we design a latent prompt generation module to generate the latent prompt with the constraint of the target answer.","Subsequently, we propose a multi-modal fusion block with latent prompt fusion module that utilizes the latent prompt to extract clinical-relevant information from uni-modal and multi-modal features.","Additionally, we introduce a prior knowledge fusion module to integrate the relationship between diseases and organs with the clinical-relevant information.","Finally, we combine the final integrated information with image-language cross-modal information to predict the final answers.","Experimental results on three publicly available Med-VQA datasets demonstrate that LaPA outperforms the state-of-the-art model ARL, achieving improvements of 1.83%, 0.63%, and 1.80% on VQA-RAD, SLAKE, and VQA-2019, respectively.","The code is publicly available at https://github.com/GaryGuTC/LaPA_model."],"url":"http://arxiv.org/abs/2404.13039v1","category":"cs.CV"}
{"created":"2024-04-19 17:49:56","title":"Mapping Social Choice Theory to RLHF","abstract":"Recent work on the limitations of using reinforcement learning from human feedback (RLHF) to incorporate human preferences into model behavior often raises social choice theory as a reference point. Social choice theory's analysis of settings such as voting mechanisms provides technical infrastructure that can inform how to aggregate human preferences amid disagreement. We analyze the problem settings of social choice and RLHF, identify key differences between them, and discuss how these differences may affect the RLHF interpretation of well-known technical results in social choice.","sentences":["Recent work on the limitations of using reinforcement learning from human feedback (RLHF) to incorporate human preferences into model behavior often raises social choice theory as a reference point.","Social choice theory's analysis of settings such as voting mechanisms provides technical infrastructure that can inform how to aggregate human preferences amid disagreement.","We analyze the problem settings of social choice and RLHF, identify key differences between them, and discuss how these differences may affect the RLHF interpretation of well-known technical results in social choice."],"url":"http://arxiv.org/abs/2404.13038v1","category":"cs.AI"}
{"created":"2024-04-19 17:49:33","title":"Comment on \"Anomalous contribution to galactic rotation curves due to stochastic spacetime\"","abstract":"We analyze the model put forth by Ref. \\cite{Oppenheim:2024rcp}, in which it is claimed that \"post-quantum classical gravity\" (PQCG), a stochastic theory of gravity, leads to modified Newtonian dynamics (MOND) behavior on galactic scales that reproduces galactic rotation curves. We show that this analysis has two basic problems: (i) the equations of PQCG do not lead to a new large scale force of the form claimed in the paper, and (ii) the form claimed is not of the MONDian form anyhow and so does not correspond to observed galactic dynamics. We also mention other potential problems that might arise from stochastic gravitational behavior.","sentences":["We analyze the model put forth by Ref.","\\cite{Oppenheim:2024rcp}, in which it is claimed that \"post-quantum classical gravity\" (PQCG), a stochastic theory of gravity, leads to modified Newtonian dynamics (MOND) behavior on galactic scales that reproduces galactic rotation curves.","We show that this analysis has two basic problems: (i) the equations of PQCG do not lead to a new large scale force of the form claimed in the paper, and (ii) the form claimed is not of the MONDian form anyhow and so does not correspond to observed galactic dynamics.","We also mention other potential problems that might arise from stochastic gravitational behavior."],"url":"http://arxiv.org/abs/2404.13037v1","category":"gr-qc"}
{"created":"2024-04-19 17:48:48","title":"Embedded Domain Walls and Electroweak Baryogenesis","abstract":"Embedded walls are domain wall solutions which are unstable in the vacuum but stabilized in a plasma of the early Universe. We show how embedded walls in which the electroweak symmetry is restored can lead to an efficient scenario of electroweak baryogenesis. We construct an extension of the Standard Model of particle physics in which embedded walls exist and are stabilized in an electromagnetic plasma.","sentences":["Embedded walls are domain wall solutions which are unstable in the vacuum but stabilized in a plasma of the early Universe.","We show how embedded walls in which the electroweak symmetry is restored can lead to an efficient scenario of electroweak baryogenesis.","We construct an extension of the Standard Model of particle physics in which embedded walls exist and are stabilized in an electromagnetic plasma."],"url":"http://arxiv.org/abs/2404.13035v1","category":"hep-ph"}
{"created":"2024-04-19 17:44:10","title":"Quantum Spacetimes from General Relativity?","abstract":"We introduce a non-commutative product for curved spacetimes, that can be regarded as a generalization of the Rieffel (or Moyal-Weyl) product. This product employs the exponential map and a Poisson tensor, and the deformed product maintains associativity under the condition that the Poisson tensor $\\Theta$ satisfies $\\Theta^{\\mu\\nu}\\nabla_{\\nu}\\Theta^{\\rho\\sigma}=0$, in relation to a Levi-Cevita connection. We proceed to solve the associativity condition for various physical spacetimes, uncovering non-commutative structures with compelling properties.","sentences":["We introduce a non-commutative product for curved spacetimes, that can be regarded as a generalization of the Rieffel (or Moyal-Weyl) product.","This product employs the exponential map and a Poisson tensor, and the deformed product maintains associativity under the condition that the Poisson tensor $\\Theta$ satisfies $\\Theta^{\\mu\\nu}\\nabla_{\\nu}\\Theta^{\\rho\\sigma}=0$, in relation to a Levi-Cevita connection.","We proceed to solve the associativity condition for various physical spacetimes, uncovering non-commutative structures with compelling properties."],"url":"http://arxiv.org/abs/2404.13029v1","category":"gr-qc"}
{"created":"2024-04-19 17:43:26","title":"When Life gives you LLMs, make LLM-ADE: Large Language Models with Adaptive Data Engineering","abstract":"This paper presents the LLM-ADE framework, a novel methodology for continued pre-training of large language models (LLMs) that addresses the challenges of catastrophic forgetting and double descent. LLM-ADE employs dynamic architectural adjustments, including selective block freezing and expansion, tailored to specific datasets. This strategy enhances model adaptability to new data while preserving previously acquired knowledge. We demonstrate LLM-ADE's effectiveness on the TinyLlama model across various general knowledge benchmarks, showing significant performance improvements without the drawbacks of traditional continuous training methods. This approach promises a more versatile and robust way to keep LLMs current and efficient in real-world applications.","sentences":["This paper presents the LLM-ADE framework, a novel methodology for continued pre-training of large language models (LLMs) that addresses the challenges of catastrophic forgetting and double descent.","LLM-ADE employs dynamic architectural adjustments, including selective block freezing and expansion, tailored to specific datasets.","This strategy enhances model adaptability to new data while preserving previously acquired knowledge.","We demonstrate LLM-ADE's effectiveness on the TinyLlama model across various general knowledge benchmarks, showing significant performance improvements without the drawbacks of traditional continuous training methods.","This approach promises a more versatile and robust way to keep LLMs current and efficient in real-world applications."],"url":"http://arxiv.org/abs/2404.13028v1","category":"cs.CE"}
{"created":"2024-04-19 17:41:05","title":"PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation","abstract":"Realistic object interactions are crucial for creating immersive virtual experiences, yet synthesizing realistic 3D object dynamics in response to novel interactions remains a significant challenge. Unlike unconditional or text-conditioned dynamics generation, action-conditioned dynamics requires perceiving the physical material properties of objects and grounding the 3D motion prediction on these properties, such as object stiffness. However, estimating physical material properties is an open problem due to the lack of material ground-truth data, as measuring these properties for real objects is highly difficult. We present PhysDreamer, a physics-based approach that endows static 3D objects with interactive dynamics by leveraging the object dynamics priors learned by video generation models. By distilling these priors, PhysDreamer enables the synthesis of realistic object responses to novel interactions, such as external forces or agent manipulations. We demonstrate our approach on diverse examples of elastic objects and evaluate the realism of the synthesized interactions through a user study. PhysDreamer takes a step towards more engaging and realistic virtual experiences by enabling static 3D objects to dynamically respond to interactive stimuli in a physically plausible manner. See our project page at https://physdreamer.github.io/.","sentences":["Realistic object interactions are crucial for creating immersive virtual experiences, yet synthesizing realistic 3D object dynamics in response to novel interactions remains a significant challenge.","Unlike unconditional or text-conditioned dynamics generation, action-conditioned dynamics requires perceiving the physical material properties of objects and grounding the 3D motion prediction on these properties, such as object stiffness.","However, estimating physical material properties is an open problem due to the lack of material ground-truth data, as measuring these properties for real objects is highly difficult.","We present PhysDreamer, a physics-based approach that endows static 3D objects with interactive dynamics by leveraging the object dynamics priors learned by video generation models.","By distilling these priors, PhysDreamer enables the synthesis of realistic object responses to novel interactions, such as external forces or agent manipulations.","We demonstrate our approach on diverse examples of elastic objects and evaluate the realism of the synthesized interactions through a user study.","PhysDreamer takes a step towards more engaging and realistic virtual experiences by enabling static 3D objects to dynamically respond to interactive stimuli in a physically plausible manner.","See our project page at https://physdreamer.github.io/."],"url":"http://arxiv.org/abs/2404.13026v1","category":"cs.CV"}
{"created":"2024-04-19 17:41:01","title":"Gravitational wave probes of Barrow cosmology with LISA standard sirens","abstract":"We study the Barrow cosmological model, which proposes that quantum gravity effects create a complex, fractal structure for the universe's apparent horizon. We leverage the thermodynamics - gravity conjecture. By applying the Clausius relation to the apparent horizon of the Friedmann - Lema\\^itre - Robertson - Walker universe within this framework, we derive modified field equations where the Barrow entropy is linked to the horizon. We assess the Barrow cosmology against current observations - cosmic microwave background , supernovae , and baryon acoustic oscillations data - and include projections for future Laser Interferometer Space Antenna (LISA) standard sirens (SS). Our numerical results suggest a modest improvement in the Hubble tension for Barrow cosmology with phantom dark energy behavior, compared to the standard cosmological model. Furthermore, incorporating simulated LISA SS data alongside existing observational constraints tightens the limitations on cosmological parameters, particularly the deformation exponent.","sentences":["We study the Barrow cosmological model, which proposes that quantum gravity effects create a complex, fractal structure for the universe's apparent horizon.","We leverage the thermodynamics - gravity conjecture.","By applying the Clausius relation to the apparent horizon of the Friedmann - Lema\\^itre - Robertson - Walker universe within this framework, we derive modified field equations where the Barrow entropy is linked to the horizon.","We assess the Barrow cosmology against current observations - cosmic microwave background , supernovae , and baryon acoustic oscillations data - and include projections for future Laser Interferometer Space Antenna (LISA) standard sirens (SS).","Our numerical results suggest a modest improvement in the Hubble tension for Barrow cosmology with phantom dark energy behavior, compared to the standard cosmological model.","Furthermore, incorporating simulated LISA SS data alongside existing observational constraints tightens the limitations on cosmological parameters, particularly the deformation exponent."],"url":"http://arxiv.org/abs/2404.13025v1","category":"gr-qc"}
{"created":"2024-04-19 17:31:39","title":"Single-loop Projection-free and Projected Gradient-based Algorithms for Nonconvex-concave Saddle Point Problems with Bilevel Structure","abstract":"In this paper, we explore a class of constrained saddle point problems with a bilevel structure, where the upper-level objective function is nonconvex-concave and smooth subject to a strongly convex lower-level objective function. This class of problems finds wide applicability in machine learning, including robust multi-task learning, adversarial learning, and robust meta-learning. While some studies have focused on simpler formulations, e.g., when the upper-level objective function is linear in the maximization component, there remains a significant gap in developing efficient projection-free and projection-based algorithms with theoretical guarantees for more general settings. To bridge this gap, we propose efficient single-loop one-sided projection-free, and fully projection-based primal-dual methods. By leveraging regularization and nested approximation techniques, we initially devise a bilevel primal-dual one-sided projection-free algorithm, requiring $\\mathcal{O}(\\epsilon^{-4})$ iterations to attain an $\\epsilon$-stationary point. Subsequently, we develop a bilevel primal-dual fully projected algorithm, capable of achieving an $\\epsilon$-stationary solution within $\\mathcal{O}(\\epsilon^{-5})$ iterations. To the best of our knowledge, our proposed algorithms represent among the first methods for solving general non-bilinear saddle point problems with a bilevel structure.","sentences":["In this paper, we explore a class of constrained saddle point problems with a bilevel structure, where the upper-level objective function is nonconvex-concave and smooth subject to a strongly convex lower-level objective function.","This class of problems finds wide applicability in machine learning, including robust multi-task learning, adversarial learning, and robust meta-learning.","While some studies have focused on simpler formulations, e.g., when the upper-level objective function is linear in the maximization component, there remains a significant gap in developing efficient projection-free and projection-based algorithms with theoretical guarantees for more general settings.","To bridge this gap, we propose efficient single-loop one-sided projection-free, and fully projection-based primal-dual methods.","By leveraging regularization and nested approximation techniques, we initially devise a bilevel primal-dual one-sided projection-free algorithm, requiring $\\mathcal{O}(\\epsilon^{-4})$ iterations to attain an $\\epsilon$-stationary point.","Subsequently, we develop a bilevel primal-dual fully projected algorithm, capable of achieving an $\\epsilon$-stationary solution within $\\mathcal{O}(\\epsilon^{-5})$ iterations.","To the best of our knowledge, our proposed algorithms represent among the first methods for solving general non-bilinear saddle point problems with a bilevel structure."],"url":"http://arxiv.org/abs/2404.13021v1","category":"math.OC"}
{"created":"2024-04-19 17:22:51","title":"Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models","abstract":"We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded and fine-grained visual perception ability. Beyond holistic image understanding, Groma is adept at region-level tasks such as region captioning and visual grounding. Such capabilities are built upon a localized visual tokenization mechanism, where an image input is decomposed into regions of interest and subsequently encoded into region tokens. By integrating region tokens into user instructions and model responses, we seamlessly enable Groma to understand user-specified region inputs and ground its textual output to images. Besides, to enhance the grounded chat ability of Groma, we curate a visually grounded instruction dataset by leveraging the powerful GPT-4V and visual prompting techniques. Compared with MLLMs that rely on the language model or external module for localization, Groma consistently demonstrates superior performances in standard referring and grounding benchmarks, highlighting the advantages of embedding localization into image tokenization. Project page: https://groma-mllm.github.io/.","sentences":["We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded and fine-grained visual perception ability.","Beyond holistic image understanding, Groma is adept at region-level tasks such as region captioning and visual grounding.","Such capabilities are built upon a localized visual tokenization mechanism, where an image input is decomposed into regions of interest and subsequently encoded into region tokens.","By integrating region tokens into user instructions and model responses, we seamlessly enable Groma to understand user-specified region inputs and ground its textual output to images.","Besides, to enhance the grounded chat ability of Groma, we curate a visually grounded instruction dataset by leveraging the powerful GPT-4V and visual prompting techniques.","Compared with MLLMs that rely on the language model or external module for localization, Groma consistently demonstrates superior performances in standard referring and grounding benchmarks, highlighting the advantages of embedding localization into image tokenization.","Project page: https://groma-mllm.github.io/."],"url":"http://arxiv.org/abs/2404.13013v1","category":"cs.CV"}
{"created":"2024-04-19 17:13:24","title":"Online Policy Optimization in Unknown Nonlinear Systems","abstract":"We study online policy optimization in nonlinear time-varying dynamical systems where the true dynamical models are unknown to the controller. This problem is challenging because, unlike in linear systems, the controller cannot obtain globally accurate estimations of the ground-truth dynamics using local exploration. We propose a meta-framework that combines a general online policy optimization algorithm ($\\texttt{ALG}$) with a general online estimator of the dynamical system's model parameters ($\\texttt{EST}$). We show that if the hypothetical joint dynamics induced by $\\texttt{ALG}$ with known parameters satisfies several desired properties, the joint dynamics under inexact parameters from $\\texttt{EST}$ will be robust to errors. Importantly, the final policy regret only depends on $\\texttt{EST}$'s predictions on the visited trajectory, which relaxes a bottleneck on identifying the true parameters globally. To demonstrate our framework, we develop a computationally efficient variant of Gradient-based Adaptive Policy Selection, called Memoryless GAPS (M-GAPS), and use it to instantiate $\\texttt{ALG}$. Combining M-GAPS with online gradient descent to instantiate $\\texttt{EST}$ yields (to our knowledge) the first local regret bound for online policy optimization in nonlinear time-varying systems with unknown dynamics.","sentences":["We study online policy optimization in nonlinear time-varying dynamical systems where the true dynamical models are unknown to the controller.","This problem is challenging because, unlike in linear systems, the controller cannot obtain globally accurate estimations of the ground-truth dynamics using local exploration.","We propose a meta-framework that combines a general online policy optimization algorithm ($\\texttt{ALG}$) with a general online estimator of the dynamical system's model parameters ($\\texttt{EST}$).","We show that if the hypothetical joint dynamics induced by $\\texttt{ALG}$ with known parameters satisfies several desired properties, the joint dynamics under inexact parameters from $\\texttt{EST}$ will be robust to errors.","Importantly, the final policy regret only depends on $\\texttt{EST}$'s predictions on the visited trajectory, which relaxes a bottleneck on identifying the true parameters globally.","To demonstrate our framework, we develop a computationally efficient variant of Gradient-based Adaptive Policy Selection, called Memoryless GAPS (M-GAPS), and use it to instantiate $\\texttt{ALG}$. Combining M-GAPS with online gradient descent to instantiate $\\texttt{EST}$ yields (to our knowledge) the first local regret bound for online policy optimization in nonlinear time-varying systems with unknown dynamics."],"url":"http://arxiv.org/abs/2404.13009v1","category":"math.OC"}
{"created":"2024-04-19 17:13:21","title":"Enhancing Generalization in Audio Deepfake Detection: A Neural Collapse based Sampling and Training Approach","abstract":"Generalization in audio deepfake detection presents a significant challenge, with models trained on specific datasets often struggling to detect deepfakes generated under varying conditions and unknown algorithms. While collectively training a model using diverse datasets can enhance its generalization ability, it comes with high computational costs. To address this, we propose a neural collapse-based sampling approach applied to pre-trained models trained on distinct datasets to create a new training database. Using ASVspoof 2019 dataset as a proof-of-concept, we implement pre-trained models with Resnet and ConvNext architectures. Our approach demonstrates comparable generalization on unseen data while being computationally efficient, requiring less training data. Evaluation is conducted using the In-the-wild dataset.","sentences":["Generalization in audio deepfake detection presents a significant challenge, with models trained on specific datasets often struggling to detect deepfakes generated under varying conditions and unknown algorithms.","While collectively training a model using diverse datasets can enhance its generalization ability, it comes with high computational costs.","To address this, we propose a neural collapse-based sampling approach applied to pre-trained models trained on distinct datasets to create a new training database.","Using ASVspoof 2019 dataset as a proof-of-concept, we implement pre-trained models with Resnet and ConvNext architectures.","Our approach demonstrates comparable generalization on unseen data while being computationally efficient, requiring less training data.","Evaluation is conducted using the In-the-wild dataset."],"url":"http://arxiv.org/abs/2404.13008v1","category":"cs.SD"}
{"created":"2024-04-19 17:06:03","title":"Algebraic Topology of Certain Sasaki Joins","abstract":"The join construction produces a third Sasaki manifold from two others, and we investigate the algebraic topology of the joins of circle bundles over surfaces of positive genus with weighted three-spheres. Topologically, such a join has the structure of a lens space bundle over a surface. We calculate invariants determined by the fundamental group, the homology, and the cohomology. We find that, in general, there is torsion in the integral homology of the join. The torsion gives rise to two linking forms, and we identify these linking forms.","sentences":["The join construction produces a third Sasaki manifold from two others, and we investigate the algebraic topology of the joins of circle bundles over surfaces of positive genus with weighted three-spheres.","Topologically, such a join has the structure of a lens space bundle over a surface.","We calculate invariants determined by the fundamental group, the homology, and the cohomology.","We find that, in general, there is torsion in the integral homology of the join.","The torsion gives rise to two linking forms, and we identify these linking forms."],"url":"http://arxiv.org/abs/2404.13005v1","category":"math.AT"}
{"created":"2024-04-19 17:01:46","title":"FinLangNet: A Novel Deep Learning Framework for Credit Risk Prediction Using Linguistic Analogy in Financial Data","abstract":"Recent industrial applications in risk prediction still heavily rely on extensively manually-tuned, statistical learning methods. Real-world financial data, characterized by its high-dimensionality, sparsity, high noise levels, and significant imbalance, poses unique challenges for the effective application of deep neural network models. In this work, we introduce a novel deep learning risk prediction framework, FinLangNet, which conceptualizes credit loan trajectories in a structure that mirrors linguistic constructs. This framework is tailored for credit risk prediction using real-world financial data, drawing on structural similarities to language by adapting natural language processing techniques. It focuses on analyzing the evolution and predictability of credit histories through detailed financial event sequences. Our research demonstrates that FinLangNet surpasses traditional statistical methods in predicting credit risk and that its integration with these methods enhances credit card fraud prediction models, achieving a significant improvement of over 1.5 points in the Kolmogorov-Smirnov metric.","sentences":["Recent industrial applications in risk prediction still heavily rely on extensively manually-tuned, statistical learning methods.","Real-world financial data, characterized by its high-dimensionality, sparsity, high noise levels, and significant imbalance, poses unique challenges for the effective application of deep neural network models.","In this work, we introduce a novel deep learning risk prediction framework, FinLangNet, which conceptualizes credit loan trajectories in a structure that mirrors linguistic constructs.","This framework is tailored for credit risk prediction using real-world financial data, drawing on structural similarities to language by adapting natural language processing techniques.","It focuses on analyzing the evolution and predictability of credit histories through detailed financial event sequences.","Our research demonstrates that FinLangNet surpasses traditional statistical methods in predicting credit risk and that its integration with these methods enhances credit card fraud prediction models, achieving a significant improvement of over 1.5 points in the Kolmogorov-Smirnov metric."],"url":"http://arxiv.org/abs/2404.13004v1","category":"cs.CE"}
{"created":"2024-04-19 16:59:04","title":"Towards Robust Ferrous Scrap Material Classification with Deep Learning and Conformal Prediction","abstract":"In the steel production domain, recycling ferrous scrap is essential for environmental and economic sustainability, as it reduces both energy consumption and greenhouse gas emissions. However, the classification of scrap materials poses a significant challenge, requiring advancements in automation technology. Additionally, building trust among human operators is a major obstacle. Traditional approaches often fail to quantify uncertainty and lack clarity in model decision-making, which complicates acceptance. In this article, we describe how conformal prediction can be employed to quantify uncertainty and add robustness in scrap classification. We have adapted the Split Conformal Prediction technique to seamlessly integrate with state-of-the-art computer vision models, such as the Vision Transformer (ViT), Swin Transformer, and ResNet-50, while also incorporating Explainable Artificial Intelligence (XAI) methods. We evaluate the approach using a comprehensive dataset of 8147 images spanning nine ferrous scrap classes. The application of the Split Conformal Prediction method allowed for the quantification of each model's uncertainties, which enhanced the understanding of predictions and increased the reliability of the results. Specifically, the Swin Transformer model demonstrated more reliable outcomes than the others, as evidenced by its smaller average size of prediction sets and achieving an average classification accuracy exceeding 95%. Furthermore, the Score-CAM method proved highly effective in clarifying visual features, significantly enhancing the explainability of the classification decisions.","sentences":["In the steel production domain, recycling ferrous scrap is essential for environmental and economic sustainability, as it reduces both energy consumption and greenhouse gas emissions.","However, the classification of scrap materials poses a significant challenge, requiring advancements in automation technology.","Additionally, building trust among human operators is a major obstacle.","Traditional approaches often fail to quantify uncertainty and lack clarity in model decision-making, which complicates acceptance.","In this article, we describe how conformal prediction can be employed to quantify uncertainty and add robustness in scrap classification.","We have adapted the Split Conformal Prediction technique to seamlessly integrate with state-of-the-art computer vision models, such as the Vision Transformer (ViT), Swin Transformer, and ResNet-50, while also incorporating Explainable Artificial Intelligence (XAI) methods.","We evaluate the approach using a comprehensive dataset of 8147 images spanning nine ferrous scrap classes.","The application of the Split Conformal Prediction method allowed for the quantification of each model's uncertainties, which enhanced the understanding of predictions and increased the reliability of the results.","Specifically, the Swin Transformer model demonstrated more reliable outcomes than the others, as evidenced by its smaller average size of prediction sets and achieving an average classification accuracy exceeding 95%.","Furthermore, the Score-CAM method proved highly effective in clarifying visual features, significantly enhancing the explainability of the classification decisions."],"url":"http://arxiv.org/abs/2404.13002v1","category":"cs.CV"}
{"created":"2024-04-19 16:55:12","title":"RadRotator: 3D Rotation of Radiographs with Diffusion Models","abstract":"Transforming two-dimensional (2D) images into three-dimensional (3D) volumes is a well-known yet challenging problem for the computer vision community. In the medical domain, a few previous studies attempted to convert two or more input radiographs into computed tomography (CT) volumes. Following their effort, we introduce a diffusion model-based technology that can rotate the anatomical content of any input radiograph in 3D space, potentially enabling the visualization of the entire anatomical content of the radiograph from any viewpoint in 3D. Similar to previous studies, we used CT volumes to create Digitally Reconstructed Radiographs (DRRs) as the training data for our model. However, we addressed two significant limitations encountered in previous studies: 1. We utilized conditional diffusion models with classifier-free guidance instead of Generative Adversarial Networks (GANs) to achieve higher mode coverage and improved output image quality, with the only trade-off being slower inference time, which is often less critical in medical applications; and 2. We demonstrated that the unreliable output of style transfer deep learning (DL) models, such as Cycle-GAN, to transfer the style of actual radiographs to DRRs could be replaced with a simple yet effective training transformation that randomly changes the pixel intensity histograms of the input and ground-truth imaging data during training. This transformation makes the diffusion model agnostic to any distribution variations of the input data pixel intensity, enabling the reliable training of a DL model on input DRRs and applying the exact same model to conventional radiographs (or DRRs) during inference.","sentences":["Transforming two-dimensional (2D) images into three-dimensional (3D) volumes is a well-known yet challenging problem for the computer vision community.","In the medical domain, a few previous studies attempted to convert two or more input radiographs into computed tomography (CT) volumes.","Following their effort, we introduce a diffusion model-based technology that can rotate the anatomical content of any input radiograph in 3D space, potentially enabling the visualization of the entire anatomical content of the radiograph from any viewpoint in 3D. Similar to previous studies, we used CT volumes to create Digitally Reconstructed Radiographs (DRRs) as the training data for our model.","However, we addressed two significant limitations encountered in previous studies:","1.","We utilized conditional diffusion models with classifier-free guidance instead of Generative Adversarial Networks (GANs) to achieve higher mode coverage and improved output image quality, with the only trade-off being slower inference time, which is often less critical in medical applications; and 2.","We demonstrated that the unreliable output of style transfer deep learning (DL) models, such as Cycle-GAN, to transfer the style of actual radiographs to DRRs could be replaced with a simple yet effective training transformation that randomly changes the pixel intensity histograms of the input and ground-truth imaging data during training.","This transformation makes the diffusion model agnostic to any distribution variations of the input data pixel intensity, enabling the reliable training of a DL model on input DRRs and applying the exact same model to conventional radiographs (or DRRs) during inference."],"url":"http://arxiv.org/abs/2404.13000v1","category":"eess.IV"}
{"created":"2024-04-19 16:54:55","title":"Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned Reinforcement Learning","abstract":"Exploration efficiency poses a significant challenge in goal-conditioned reinforcement learning (GCRL) tasks, particularly those with long horizons and sparse rewards. A primary limitation to exploration efficiency is the agent's inability to leverage environmental structural patterns. In this study, we introduce a novel framework, GEASD, designed to capture these patterns through an adaptive skill distribution during the learning process. This distribution optimizes the local entropy of achieved goals within a contextual horizon, enhancing goal-spreading behaviors and facilitating deep exploration in states containing familiar structural patterns. Our experiments reveal marked improvements in exploration efficiency using the adaptive skill distribution compared to a uniform skill distribution. Additionally, the learned skill distribution demonstrates robust generalization capabilities, achieving substantial exploration progress in unseen tasks containing similar local structures.","sentences":["Exploration efficiency poses a significant challenge in goal-conditioned reinforcement learning (GCRL) tasks, particularly those with long horizons and sparse rewards.","A primary limitation to exploration efficiency is the agent's inability to leverage environmental structural patterns.","In this study, we introduce a novel framework, GEASD, designed to capture these patterns through an adaptive skill distribution during the learning process.","This distribution optimizes the local entropy of achieved goals within a contextual horizon, enhancing goal-spreading behaviors and facilitating deep exploration in states containing familiar structural patterns.","Our experiments reveal marked improvements in exploration efficiency using the adaptive skill distribution compared to a uniform skill distribution.","Additionally, the learned skill distribution demonstrates robust generalization capabilities, achieving substantial exploration progress in unseen tasks containing similar local structures."],"url":"http://arxiv.org/abs/2404.12999v1","category":"cs.LG"}
{"created":"2024-04-19 16:41:16","title":"Veronese sections and interlacing matrices of polynomials and formal power series","abstract":"The concept of a fully interlacing matrix of formal power series with real coefficients is introduced. This concept extends and strengthens that of an interlacing sequence of real-rooted polynomials with nonnegative coefficients, in the special case of row and column matrices. The fully interlacing property is shown to be preserved under matrix products, flips across the reverse diagonal and Veronese sections of the power series involved. These results and their corollaries generalize, unify and simplify several results which have previously appeared in the literature. An application to the theory of uniform triangulations of simplicial complexes is included.","sentences":["The concept of a fully interlacing matrix of formal power series with real coefficients is introduced.","This concept extends and strengthens that of an interlacing sequence of real-rooted polynomials with nonnegative coefficients, in the special case of row and column matrices.","The fully interlacing property is shown to be preserved under matrix products, flips across the reverse diagonal and Veronese sections of the power series involved.","These results and their corollaries generalize, unify and simplify several results which have previously appeared in the literature.","An application to the theory of uniform triangulations of simplicial complexes is included."],"url":"http://arxiv.org/abs/2404.12989v1","category":"math.CO"}
{"created":"2024-04-19 16:36:46","title":"Understanding Intra-Household Educational Inequalities: Gender, Birth Order, and Ability Dynamics in Benin's Households","abstract":"This paper investigates the nuanced interplay of gender, birth order, and innate ability in shaping educational disparities among children within households, employing both a reduced-form analysis and a structural model of household resource allocation. By decomposing overall disparities, I identify the contributions of gender, birth order, and innate ability differences. In the context of Benin, for households with non-educated heads and mixed-gender children, total inequality comprises 50% gender disparity, 20% birth order effect, and 30% ability gap. Conversely, in households with only sons or only daughters and non-educated heads, total inequality is predominantly driven by ability disparities 70% for daughters and 83% for sons, with lesser contributions from birth order effects. Furthermore, my analysis reveals that in households with non-educated heads, firstborn daughters surpass their younger brothers in educational attainment on the intensive margin if their innate ability exceeds their brothers' by at least 13%, a figure reduced to 8% in households with college-educated parents. Additionally, the study unveils parental preferences favoring sons' education over daughters' among non-educated parents, with a perceived 22% higher average benefit. Targeted policies aimed at reducing composite education costs prove effective in mitigating gender and birth order gaps, albeit with a modest 5% reduction in total inequality. Overall, this research underscores the complex dynamics influencing intra-household educational inequalities and suggests policy avenues to address them.","sentences":["This paper investigates the nuanced interplay of gender, birth order, and innate ability in shaping educational disparities among children within households, employing both a reduced-form analysis and a structural model of household resource allocation.","By decomposing overall disparities, I identify the contributions of gender, birth order, and innate ability differences.","In the context of Benin, for households with non-educated heads and mixed-gender children, total inequality comprises 50% gender disparity, 20% birth order effect, and 30% ability gap.","Conversely, in households with only sons or only daughters and non-educated heads, total inequality is predominantly driven by ability disparities 70% for daughters and 83% for sons, with lesser contributions from birth order effects.","Furthermore, my analysis reveals that in households with non-educated heads, firstborn daughters surpass their younger brothers in educational attainment on the intensive margin if their innate ability exceeds their brothers' by at least 13%, a figure reduced to 8% in households with college-educated parents.","Additionally, the study unveils parental preferences favoring sons' education over daughters' among non-educated parents, with a perceived 22% higher average benefit.","Targeted policies aimed at reducing composite education costs prove effective in mitigating gender and birth order gaps, albeit with a modest 5% reduction in total inequality.","Overall, this research underscores the complex dynamics influencing intra-household educational inequalities and suggests policy avenues to address them."],"url":"http://arxiv.org/abs/2404.12988v1","category":"econ.GN"}
{"created":"2024-04-19 16:34:15","title":"Eye-tracking in Mixed Reality for Diagnosis of Neurodegenerative Diseases","abstract":"Parkinson's disease ranks as the second most prevalent neurodegenerative disorder globally. This research aims to develop a system leveraging Mixed Reality capabilities for tracking and assessing eye movements. In this paper, we present a medical scenario and outline the development of an application designed to capture eye-tracking signals through Mixed Reality technology for the evaluation of neurodegenerative diseases. Additionally, we introduce a pipeline for extracting clinically relevant features from eye-gaze analysis, describing the capabilities of the proposed system from a medical perspective. The study involved a cohort of healthy control individuals and patients suffering from Parkinson's disease, showcasing the feasibility and potential of the proposed technology for non-intrusive monitoring of eye movement patterns for the diagnosis of neurodegenerative diseases.   Clinical relevance - Developing a non-invasive biomarker for Parkinson's disease is urgently needed to accurately detect the disease's onset. This would allow for the timely introduction of neuroprotective treatment at the earliest stage and enable the continuous monitoring of intervention outcomes. The ability to detect subtle changes in eye movements allows for early diagnosis, offering a critical window for intervention before more pronounced symptoms emerge. Eye tracking provides objective and quantifiable biomarkers, ensuring reliable assessments of disease progression and cognitive function. The eye gaze analysis using Mixed Reality glasses is wireless, facilitating convenient assessments in both home and hospital settings. The approach offers the advantage of utilizing hardware that requires no additional specialized attachments, enabling examinations through personal eyewear.","sentences":["Parkinson's disease ranks as the second most prevalent neurodegenerative disorder globally.","This research aims to develop a system leveraging Mixed Reality capabilities for tracking and assessing eye movements.","In this paper, we present a medical scenario and outline the development of an application designed to capture eye-tracking signals through Mixed Reality technology for the evaluation of neurodegenerative diseases.","Additionally, we introduce a pipeline for extracting clinically relevant features from eye-gaze analysis, describing the capabilities of the proposed system from a medical perspective.","The study involved a cohort of healthy control individuals and patients suffering from Parkinson's disease, showcasing the feasibility and potential of the proposed technology for non-intrusive monitoring of eye movement patterns for the diagnosis of neurodegenerative diseases.   ","Clinical relevance - Developing a non-invasive biomarker for Parkinson's disease is urgently needed to accurately detect the disease's onset.","This would allow for the timely introduction of neuroprotective treatment at the earliest stage and enable the continuous monitoring of intervention outcomes.","The ability to detect subtle changes in eye movements allows for early diagnosis, offering a critical window for intervention before more pronounced symptoms emerge.","Eye tracking provides objective and quantifiable biomarkers, ensuring reliable assessments of disease progression and cognitive function.","The eye gaze analysis using Mixed Reality glasses is wireless, facilitating convenient assessments in both home and hospital settings.","The approach offers the advantage of utilizing hardware that requires no additional specialized attachments, enabling examinations through personal eyewear."],"url":"http://arxiv.org/abs/2404.12984v1","category":"cs.HC"}
{"created":"2024-04-19 16:25:44","title":"Non-vanishing of geodesic periods of automorphic forms","abstract":"We prove that one hundred percent of closed geodesic periods of a Maa{\\ss} form for the modular group are non-vanishing when ordered by length. We present applications to the non-vanishing of central values of Rankin--Selberg $L$-functions. Similar results for holomorphic forms for general Fuchsian groups of the first kind with a cusp are also obtained, as well as results towards normal distribution.","sentences":["We prove that one hundred percent of closed geodesic periods of a Maa{\\ss} form for the modular group are non-vanishing when ordered by length.","We present applications to the non-vanishing of central values of Rankin--Selberg $L$-functions.","Similar results for holomorphic forms for general Fuchsian groups of the first kind with a cusp are also obtained, as well as results towards normal distribution."],"url":"http://arxiv.org/abs/2404.12982v1","category":"math.NT"}
{"created":"2024-04-19 16:05:35","title":"Non-extensive statistics in Au-Au collisions","abstract":"Particle production yields measured in central Au-Au collision at RHIC are obtained with free Fermi and Bose gases and also with a replacement of these statistics by non-extensive statistics. For the latter calculation, a set of different parameters was used with values of the Tsallis parameter $q$ chosen between 1.01 and 1.25, with 1.16 generating the best agreement with experimental data, an indication that non-extensive statistics may be one of the underlying features in heavy ion-collisions.","sentences":["Particle production yields measured in central Au-Au collision at RHIC are obtained with free Fermi and Bose gases and also with a replacement of these statistics by non-extensive statistics.","For the latter calculation, a set of different parameters was used with values of the Tsallis parameter $q$ chosen between 1.01 and 1.25, with 1.16 generating the best agreement with experimental data, an indication that non-extensive statistics may be one of the underlying features in heavy ion-collisions."],"url":"http://arxiv.org/abs/2404.12977v1","category":"hep-ph"}
{"created":"2024-04-19 16:04:26","title":"FineRec:Exploring Fine-grained Sequential Recommendation","abstract":"Sequential recommendation is dedicated to offering items of interest for users based on their history behaviors. The attribute-opinion pairs, expressed by users in their reviews for items, provide the potentials to capture user preferences and item characteristics at a fine-grained level. To this end, we propose a novel framework FineRec that explores the attribute-opinion pairs of reviews to finely handle sequential recommendation. Specifically, we utilize a large language model to extract attribute-opinion pairs from reviews. For each attribute, a unique attribute-specific user-opinion-item graph is created, where corresponding opinions serve as the edges linking heterogeneous user and item nodes. To tackle the diversity of opinions, we devise a diversity-aware convolution operation to aggregate information within the graphs, enabling attribute-specific user and item representation learning. Ultimately, we present an interaction-driven fusion mechanism to integrate attribute-specific user/item representations across all attributes for generating recommendations. Extensive experiments conducted on several realworld datasets demonstrate the superiority of our FineRec over existing state-of-the-art methods. Further analysis also verifies the effectiveness of our fine-grained manner in handling the task.","sentences":["Sequential recommendation is dedicated to offering items of interest for users based on their history behaviors.","The attribute-opinion pairs, expressed by users in their reviews for items, provide the potentials to capture user preferences and item characteristics at a fine-grained level.","To this end, we propose a novel framework FineRec that explores the attribute-opinion pairs of reviews to finely handle sequential recommendation.","Specifically, we utilize a large language model to extract attribute-opinion pairs from reviews.","For each attribute, a unique attribute-specific user-opinion-item graph is created, where corresponding opinions serve as the edges linking heterogeneous user and item nodes.","To tackle the diversity of opinions, we devise a diversity-aware convolution operation to aggregate information within the graphs, enabling attribute-specific user and item representation learning.","Ultimately, we present an interaction-driven fusion mechanism to integrate attribute-specific user/item representations across all attributes for generating recommendations.","Extensive experiments conducted on several realworld datasets demonstrate the superiority of our FineRec over existing state-of-the-art methods.","Further analysis also verifies the effectiveness of our fine-grained manner in handling the task."],"url":"http://arxiv.org/abs/2404.12975v1","category":"cs.IR"}
{"created":"2024-04-19 16:02:15","title":"Quantifying seasonal hydrogen storage demands under cost and market uptake uncertainties in energy system transformation pathways","abstract":"Climate neutrality paradigms put electricity systems at the core of a clean energy supply. At the same time, indirect electrification, with a potential uptake of hydrogen or derived fuel economy, plays a crucial role in decarbonising the energy supply and industrial processes. Besides energy markets coordinating the transition, climate and energy policy targets require fundamental changes and expansions in the energy transmission, import, distribution, and storage infrastructures. While existing studies identify relevant demands for hydrogen, critical decisions involve imports versus domestic fuel production and investments in new or repurposing existing pipeline and storage infrastructure. Linking the pan-European energy system planning model SCOPE SD with the multiperiod European gas market model IMAGINE, the case study analysis and its transformation pathway results indicate extensive network development of hydrogen infrastructure, including expansion beyond refurbished methane infrastructure. However, the ranges of future hydrogen storage costs and market uptake restrictions expose and quantify the uncertainty of its role in Europes transformation. The study finds that rapidly planning the construction of hydrogen storage and pipeline infrastructure is crucial to achieving the required capacity by 2050.","sentences":["Climate neutrality paradigms put electricity systems at the core of a clean energy supply.","At the same time, indirect electrification, with a potential uptake of hydrogen or derived fuel economy, plays a crucial role in decarbonising the energy supply and industrial processes.","Besides energy markets coordinating the transition, climate and energy policy targets require fundamental changes and expansions in the energy transmission, import, distribution, and storage infrastructures.","While existing studies identify relevant demands for hydrogen, critical decisions involve imports versus domestic fuel production and investments in new or repurposing existing pipeline and storage infrastructure.","Linking the pan-European energy system planning model SCOPE SD with the multiperiod European gas market model IMAGINE, the case study analysis and its transformation pathway results indicate extensive network development of hydrogen infrastructure, including expansion beyond refurbished methane infrastructure.","However, the ranges of future hydrogen storage costs and market uptake restrictions expose and quantify the uncertainty of its role in Europes transformation.","The study finds that rapidly planning the construction of hydrogen storage and pipeline infrastructure is crucial to achieving the required capacity by 2050."],"url":"http://arxiv.org/abs/2404.12974v1","category":"econ.GN"}
{"created":"2024-04-19 15:54:46","title":"Disentangling ID and Modality Effects for Session-based Recommendation","abstract":"Session-based recommendation aims to predict intents of anonymous users based on their limited behaviors. Modeling user behaviors involves two distinct rationales: co-occurrence patterns reflected by item IDs, and fine-grained preferences represented by item modalities (e.g., text and images). However, existing methods typically entangle these causes, leading to their failure in achieving accurate and explainable recommendations. To this end, we propose a novel framework DIMO to disentangle the effects of ID and modality in the task. At the item level, we introduce a co-occurrence representation schema to explicitly incorporate cooccurrence patterns into ID representations. Simultaneously, DIMO aligns different modalities into a unified semantic space to represent them uniformly. At the session level, we present a multi-view self-supervised disentanglement, including proxy mechanism and counterfactual inference, to disentangle ID and modality effects without supervised signals. Leveraging these disentangled causes, DIMO provides recommendations via causal inference and further creates two templates for generating explanations. Extensive experiments on multiple real-world datasets demonstrate the consistent superiority of DIMO over existing methods. Further analysis also confirms DIMO's effectiveness in generating explanations.","sentences":["Session-based recommendation aims to predict intents of anonymous users based on their limited behaviors.","Modeling user behaviors involves two distinct rationales: co-occurrence patterns reflected by item IDs, and fine-grained preferences represented by item modalities (e.g., text and images).","However, existing methods typically entangle these causes, leading to their failure in achieving accurate and explainable recommendations.","To this end, we propose a novel framework DIMO to disentangle the effects of ID and modality in the task.","At the item level, we introduce a co-occurrence representation schema to explicitly incorporate cooccurrence patterns into ID representations.","Simultaneously, DIMO aligns different modalities into a unified semantic space to represent them uniformly.","At the session level, we present a multi-view self-supervised disentanglement, including proxy mechanism and counterfactual inference, to disentangle ID and modality effects without supervised signals.","Leveraging these disentangled causes, DIMO provides recommendations via causal inference and further creates two templates for generating explanations.","Extensive experiments on multiple real-world datasets demonstrate the consistent superiority of DIMO over existing methods.","Further analysis also confirms DIMO's effectiveness in generating explanations."],"url":"http://arxiv.org/abs/2404.12969v1","category":"cs.IR"}
{"created":"2024-04-19 15:54:11","title":"Bootstrap confidence intervals: A comparative simulation study","abstract":"Bootstrap is a widely used technique that allows estimating the properties of a given estimator, such as its bias and standard error. In this paper, we evaluate and compare five bootstrap-based methods for making confidence intervals: two of them (Normal and Studentized) based on the bootstrap estimate of the standard error; another two (Quantile and Better) based on the estimated distribution of the parameter estimator; and finally, considering an interval constructed based on Bayesian bootstrap, relying on the notion of credible interval. The methods are compared through Monte Carlo simulations in different scenarios, including samples with autocorrelation induced by a copula model. The results are also compared with respect to the coverage rate, the median interval length and a novel indicator, proposed in this paper, combining both of them. The results show that the Studentized method has the best coverage rate, although the smallest intervals are attained by the Bayesian method. In general, all methods are appropriate and demonstrated good performance even in the scenarios violating the independence assumption.","sentences":["Bootstrap is a widely used technique that allows estimating the properties of a given estimator, such as its bias and standard error.","In this paper, we evaluate and compare five bootstrap-based methods for making confidence intervals: two of them (Normal and Studentized) based on the bootstrap estimate of the standard error; another two (Quantile and Better) based on the estimated distribution of the parameter estimator; and finally, considering an interval constructed based on Bayesian bootstrap, relying on the notion of credible interval.","The methods are compared through Monte Carlo simulations in different scenarios, including samples with autocorrelation induced by a copula model.","The results are also compared with respect to the coverage rate, the median interval length and a novel indicator, proposed in this paper, combining both of them.","The results show that the Studentized method has the best coverage rate, although the smallest intervals are attained by the Bayesian method.","In general, all methods are appropriate and demonstrated good performance even in the scenarios violating the independence assumption."],"url":"http://arxiv.org/abs/2404.12967v1","category":"stat.CO"}
{"created":"2024-04-19 15:53:27","title":"Eyes Can Deceive: Benchmarking Counterfactual Reasoning Abilities of Multi-modal Large Language Models","abstract":"Counterfactual reasoning, as a crucial manifestation of human intelligence, refers to making presuppositions based on established facts and extrapolating potential outcomes. Existing multimodal large language models (MLLMs) have exhibited impressive cognitive and reasoning capabilities, which have been examined across a wide range of Visual Question Answering (VQA) benchmarks. Nevertheless, how will existing MLLMs perform when faced with counterfactual questions? To answer this question, we first curate a novel \\textbf{C}ounter\\textbf{F}actual \\textbf{M}ulti\\textbf{M}odal reasoning benchmark, abbreviated as \\textbf{CFMM}, to systematically assess the counterfactual reasoning capabilities of MLLMs. Our CFMM comprises six challenging tasks, each including hundreds of carefully human-labeled counterfactual questions, to evaluate MLLM's counterfactual reasoning capabilities across diverse aspects. Through experiments, interestingly, we find that existing MLLMs prefer to believe what they see, but ignore the counterfactual presuppositions presented in the question, thereby leading to inaccurate responses. Furthermore, we evaluate a wide range of prevalent MLLMs on our proposed CFMM. The significant gap between their performance on our CFMM and that on several VQA benchmarks indicates that there is still considerable room for improvement in existing MLLMs toward approaching human-level intelligence. On the other hand, through boosting MLLMs performances on our CFMM in the future, potential avenues toward developing MLLMs with advanced intelligence can be explored.","sentences":["Counterfactual reasoning, as a crucial manifestation of human intelligence, refers to making presuppositions based on established facts and extrapolating potential outcomes.","Existing multimodal large language models (MLLMs) have exhibited impressive cognitive and reasoning capabilities, which have been examined across a wide range of Visual Question Answering (VQA) benchmarks.","Nevertheless, how will existing MLLMs perform when faced with counterfactual questions?","To answer this question, we first curate a novel \\textbf{C}ounter\\textbf{F}actual \\textbf{M}ulti\\textbf{M}odal reasoning benchmark, abbreviated as \\textbf{CFMM}, to systematically assess the counterfactual reasoning capabilities of MLLMs.","Our CFMM comprises six challenging tasks, each including hundreds of carefully human-labeled counterfactual questions, to evaluate MLLM's counterfactual reasoning capabilities across diverse aspects.","Through experiments, interestingly, we find that existing MLLMs prefer to believe what they see, but ignore the counterfactual presuppositions presented in the question, thereby leading to inaccurate responses.","Furthermore, we evaluate a wide range of prevalent MLLMs on our proposed CFMM.","The significant gap between their performance on our CFMM and that on several VQA benchmarks indicates that there is still considerable room for improvement in existing MLLMs toward approaching human-level intelligence.","On the other hand, through boosting MLLMs performances on our CFMM in the future, potential avenues toward developing MLLMs with advanced intelligence can be explored."],"url":"http://arxiv.org/abs/2404.12966v1","category":"cs.CV"}
{"created":"2024-04-19 15:52:06","title":"A note on higher-order and nonlinear limiting approaches for continuously bounds-preserving discontinuous Galerkin methods","abstract":"In (Dzanic, J. Comp. Phys., 508:113010, 2024), a limiting approach for high-order discontinuous Galerkin schemes was introduced which allowed for imposing constraints on the solution continuously (i.e., everywhere within the element). While exact for linear constraint functionals, this approach only imposed a sufficient (but not the minimum necessary) amount of limiting for nonlinear constraint functionals. This short note shows how this limiting approach can be extended to allow exactness for general nonlinear quasiconcave constraint functionals through a nonlinear limiting procedure, reducing unnecessary numerical dissipation. Some examples are shown for nonlinear pressure and entropy constraints in the compressible gas dynamics equations, where both analytic and iterative approaches are used.","sentences":["In (Dzanic, J. Comp.","Phys., 508:113010, 2024), a limiting approach for high-order discontinuous Galerkin schemes was introduced which allowed for imposing constraints on the solution continuously (i.e., everywhere within the element).","While exact for linear constraint functionals, this approach only imposed a sufficient (but not the minimum necessary) amount of limiting for nonlinear constraint functionals.","This short note shows how this limiting approach can be extended to allow exactness for general nonlinear quasiconcave constraint functionals through a nonlinear limiting procedure, reducing unnecessary numerical dissipation.","Some examples are shown for nonlinear pressure and entropy constraints in the compressible gas dynamics equations, where both analytic and iterative approaches are used."],"url":"http://arxiv.org/abs/2404.12965v1","category":"math.NA"}
{"created":"2024-04-19 15:46:41","title":"On the McKean-Vlasov SDE with branching","abstract":"We study a nonlinear branching diffusion process in the sense of McKean, i.e., where particles are subjected to a mean-field interaction. We consider first a strong formulation of the problem and we provide an existence and uniqueness result by using contraction arguments. Then we consider the notion of weak solution and its equivalent martingale problem formulation. In this setting, we provide a general weak existence result, as well as a propagation of chaos property, i.e., the McKean-Vlasov branching diffusion is the limit of a large population branching process with mean-field interaction when the population size grows to infinity.","sentences":["We study a nonlinear branching diffusion process in the sense of McKean, i.e., where particles are subjected to a mean-field interaction.","We consider first a strong formulation of the problem and we provide an existence and uniqueness result by using contraction arguments.","Then we consider the notion of weak solution and its equivalent martingale problem formulation.","In this setting, we provide a general weak existence result, as well as a propagation of chaos property, i.e., the McKean-Vlasov branching diffusion is the limit of a large population branching process with mean-field interaction when the population size grows to infinity."],"url":"http://arxiv.org/abs/2404.12964v1","category":"math.PR"}
{"created":"2024-04-19 15:41:23","title":"Generally noise-resilient quantum gates for trapped-ions","abstract":"We present an entangling gate scheme for trapped-ion chains that achieves high-fidelity operations with excited motional states despite multiple error sources. Our approach incorporates all relevant motional modes and exhibits enhanced robustness against both motional heating effects and detuning errors, critical features for building robust and scalable trapped-ion quantum computers.","sentences":["We present an entangling gate scheme for trapped-ion chains that achieves high-fidelity operations with excited motional states despite multiple error sources.","Our approach incorporates all relevant motional modes and exhibits enhanced robustness against both motional heating effects and detuning errors, critical features for building robust and scalable trapped-ion quantum computers."],"url":"http://arxiv.org/abs/2404.12961v1","category":"quant-ph"}
{"created":"2024-04-19 15:40:47","title":"Improving Pediatric Pneumonia Diagnosis with Adult Chest X-ray Images Utilizing Contrastive Learning and Embedding Similarity","abstract":"Despite the advancement of deep learning-based computer-aided diagnosis (CAD) methods for pneumonia from adult chest x-ray (CXR) images, the performance of CAD methods applied to pediatric images remains suboptimal, mainly due to the lack of large-scale annotated pediatric imaging datasets. Establishing a proper framework to leverage existing adult large-scale CXR datasets can thus enhance pediatric pneumonia detection performance. In this paper, we propose a three-branch parallel path learning-based framework that utilizes both adult and pediatric datasets to improve the performance of deep learning models on pediatric test datasets. The paths are trained with pediatric only, adult only, and both types of CXRs, respectively. Our proposed framework utilizes the multi-positive contrastive loss to cluster the classwise embeddings and the embedding similarity loss among these three parallel paths to make the classwise embeddings as close as possible to reduce the effect of domain shift. Experimental evaluations on open-access adult and pediatric CXR datasets show that the proposed method achieves a superior AUROC score of 0.8464 compared to 0.8348 obtained using the conventional approach of join training on both datasets. The proposed approach thus paves the way for generalized CAD models that are effective for both adult and pediatric age groups.","sentences":["Despite the advancement of deep learning-based computer-aided diagnosis (CAD) methods for pneumonia from adult chest x-ray (CXR) images, the performance of CAD methods applied to pediatric images remains suboptimal, mainly due to the lack of large-scale annotated pediatric imaging datasets.","Establishing a proper framework to leverage existing adult large-scale CXR datasets can thus enhance pediatric pneumonia detection performance.","In this paper, we propose a three-branch parallel path learning-based framework that utilizes both adult and pediatric datasets to improve the performance of deep learning models on pediatric test datasets.","The paths are trained with pediatric only, adult only, and both types of CXRs, respectively.","Our proposed framework utilizes the multi-positive contrastive loss to cluster the classwise embeddings and the embedding similarity loss among these three parallel paths to make the classwise embeddings as close as possible to reduce the effect of domain shift.","Experimental evaluations on open-access adult and pediatric CXR datasets show that the proposed method achieves a superior AUROC score of 0.8464 compared to 0.8348 obtained using the conventional approach of join training on both datasets.","The proposed approach thus paves the way for generalized CAD models that are effective for both adult and pediatric age groups."],"url":"http://arxiv.org/abs/2404.12958v1","category":"eess.IV"}
{"created":"2024-04-19 15:39:21","title":"Anisotropic electron-phonon interactions in 2D lead-halide perovskites","abstract":"Two-dimensional hybrid organic-inorganic metal halide perovskites offer enhanced stability for perovskite-based applications. Their crystal structure's soft and ionic nature gives rise to strong interactions between charge carriers and ionic rearrangements. Here, we investigate the interaction of photo-generated electrons and ionic polarizations in single-crystal 2D perovskite butylammonium lead iodide, varying the inorganic lammelae thickness in the 2D single crystals. We determined the directionality of the transition dipole moments of the relevant phonon modes (in the 0.3-3 THz range) by angle-and-polarization dependent THz transmission measurements. We find a clear anisotropy of the in-plane photoconductivity, with a 10% reduction along the axis parallel with the transition dipole moment of the most strongly coupled phonon. Detailed calculations, based on Feynman polaron theory, indicate that the anisotropy originates from directional electron-phonon interactions.","sentences":["Two-dimensional hybrid organic-inorganic metal halide perovskites offer enhanced stability for perovskite-based applications.","Their crystal structure's soft and ionic nature gives rise to strong interactions between charge carriers and ionic rearrangements.","Here, we investigate the interaction of photo-generated electrons and ionic polarizations in single-crystal 2D perovskite butylammonium lead iodide, varying the inorganic lammelae thickness in the 2D single crystals.","We determined the directionality of the transition dipole moments of the relevant phonon modes (in the 0.3-3 THz range) by angle-and-polarization dependent THz transmission measurements.","We find a clear anisotropy of the in-plane photoconductivity, with a 10% reduction along the axis parallel with the transition dipole moment of the most strongly coupled phonon.","Detailed calculations, based on Feynman polaron theory, indicate that the anisotropy originates from directional electron-phonon interactions."],"url":"http://arxiv.org/abs/2404.12955v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-19 15:26:36","title":"Next Generation Loss Function for Image Classification","abstract":"Neural networks are trained by minimizing a loss function that defines the discrepancy between the predicted model output and the target value. The selection of the loss function is crucial to achieve task-specific behaviour and highly influences the capability of the model. A variety of loss functions have been proposed for a wide range of tasks affecting training and model performance. For classification tasks, the cross entropy is the de-facto standard and usually the first choice. Here, we try to experimentally challenge the well-known loss functions, including cross entropy (CE) loss, by utilizing the genetic programming (GP) approach, a population-based evolutionary algorithm. GP constructs loss functions from a set of operators and leaf nodes and these functions are repeatedly recombined and mutated to find an optimal structure. Experiments were carried out on different small-sized datasets CIFAR-10, CIFAR-100 and Fashion-MNIST using an Inception model. The 5 best functions found were evaluated for different model architectures on a set of standard datasets ranging from 2 to 102 classes and very different sizes. One function, denoted as Next Generation Loss (NGL), clearly stood out showing same or better performance for all tested datasets compared to CE. To evaluate the NGL function on a large-scale dataset, we tested its performance on the Imagenet-1k dataset where it showed improved top-1 accuracy compared to models trained with identical settings and other losses. Finally, the NGL was trained on a segmentation downstream task for Pascal VOC 2012 and COCO-Stuff164k datasets improving the underlying model performance.","sentences":["Neural networks are trained by minimizing a loss function that defines the discrepancy between the predicted model output and the target value.","The selection of the loss function is crucial to achieve task-specific behaviour and highly influences the capability of the model.","A variety of loss functions have been proposed for a wide range of tasks affecting training and model performance.","For classification tasks, the cross entropy is the de-facto standard and usually the first choice.","Here, we try to experimentally challenge the well-known loss functions, including cross entropy (CE) loss, by utilizing the genetic programming (GP) approach, a population-based evolutionary algorithm.","GP constructs loss functions from a set of operators and leaf nodes and these functions are repeatedly recombined and mutated to find an optimal structure.","Experiments were carried out on different small-sized datasets CIFAR-10, CIFAR-100 and Fashion-MNIST using an Inception model.","The 5 best functions found were evaluated for different model architectures on a set of standard datasets ranging from 2 to 102 classes and very different sizes.","One function, denoted as Next Generation Loss (NGL), clearly stood out showing same or better performance for all tested datasets compared to CE.","To evaluate the NGL function on a large-scale dataset, we tested its performance on the Imagenet-1k dataset where it showed improved top-1 accuracy compared to models trained with identical settings and other losses.","Finally, the NGL was trained on a segmentation downstream task for Pascal VOC 2012 and COCO-Stuff164k datasets improving the underlying model performance."],"url":"http://arxiv.org/abs/2404.12948v1","category":"cs.CV"}
{"created":"2024-04-19 15:22:34","title":"A Ritt-Kreiss condition: spectral localization and norm estimates","abstract":"A new condition is introduced by generalizing the Ritt and Kreiss operators named $(\\alpha, \\beta)$-RK condition. Geometrical properties of the spectrum for the case $\\beta < 1$ are studied, moreover it is shown that in that case if $\\alpha + \\beta = 1$ the operator is Ritt. Estimates for the power and power differences norms for this type of operators are also studied. Lastly we apply this theory to obtain and interpolation result over Ritt and Kreiss operator on $L^p$ spaces.","sentences":["A new condition is introduced by generalizing the Ritt and Kreiss operators named $(\\alpha, \\beta)$-RK condition.","Geometrical properties of the spectrum for the case $\\beta < 1$ are studied, moreover it is shown that in that case if $\\alpha + \\beta = 1$ the operator is Ritt.","Estimates for the power and power differences norms for this type of operators are also studied.","Lastly we apply this theory to obtain and interpolation result over Ritt and Kreiss operator on $L^p$ spaces."],"url":"http://arxiv.org/abs/2404.12946v1","category":"math.FA"}
{"created":"2024-04-19 15:22:27","title":"Modelling the magnetic properties of 1D arrays of FePc molecules","abstract":"We investigate the magnetic properties of Fe Phthalocyanines (FePc) that are experimentally arranged in quasi one-dimensional chains when they are grown in thin films or powders. By means of DFT calculations we reproduce the structural parameters found in experiments, and then we build a generalized Heisenberg magnetic model with single ion anisotropy, and calculate its parameters. The results show a anisotropic exchange interaction $J$ between FePc molecules, and an easy plane single ion anisotropy $D$. By means of Monte Carlo simulations, with this model, we found an explanation to the non-saturation of the magnetization found at high fields, which we interpret is due to the anisotropic exchange interaction $J$. Finally, we also investigate the presence of magnetic solitons versus temperature and magnetic field. This results provide additional evidence that FePc is a soliton bearing molecular compound, with solitons easily excited mainly in the molecular $xy$ plane.","sentences":["We investigate the magnetic properties of Fe Phthalocyanines (FePc) that are experimentally arranged in quasi one-dimensional chains when they are grown in thin films or powders.","By means of DFT calculations we reproduce the structural parameters found in experiments, and then we build a generalized Heisenberg magnetic model with single ion anisotropy, and calculate its parameters.","The results show a anisotropic exchange interaction $J$ between FePc molecules, and an easy plane single ion anisotropy $D$. By means of Monte Carlo simulations, with this model, we found an explanation to the non-saturation of the magnetization found at high fields, which we interpret is due to the anisotropic exchange interaction $J$. Finally, we also investigate the presence of magnetic solitons versus temperature and magnetic field.","This results provide additional evidence that FePc is a soliton bearing molecular compound, with solitons easily excited mainly in the molecular $xy$ plane."],"url":"http://arxiv.org/abs/2404.12945v1","category":"cond-mat.str-el"}
{"created":"2024-04-19 15:21:26","title":"Visualizing Intelligent Tutor Interactions for Responsive Pedagogy","abstract":"Intelligent tutoring systems leverage AI models of expert learning and student knowledge to deliver personalized tutoring to students. While these intelligent tutors have demonstrated improved student learning outcomes, it is still unclear how teachers might integrate them into curriculum and course planning to support responsive pedagogy. In this paper, we conducted a design study with five teachers who have deployed Apprentice Tutors, an intelligent tutoring platform, in their classes. We characterized their challenges around analyzing student interaction data from intelligent tutoring systems and built VisTA (Visualizations for Tutor Analytics), a visual analytics system that shows detailed provenance data across multiple coordinated views. We evaluated VisTA with the same five teachers, and found that the visualizations helped them better interpret intelligent tutor data, gain insights into student problem-solving provenance, and decide on necessary follow-up actions - such as providing students with further support or reviewing skills in the classroom. Finally, we discuss potential extensions of VisTA into sequence query and detection, as well as the potential for the visualizations to be useful for encouraging self-directed learning in students.","sentences":["Intelligent tutoring systems leverage AI models of expert learning and student knowledge to deliver personalized tutoring to students.","While these intelligent tutors have demonstrated improved student learning outcomes, it is still unclear how teachers might integrate them into curriculum and course planning to support responsive pedagogy.","In this paper, we conducted a design study with five teachers who have deployed Apprentice Tutors, an intelligent tutoring platform, in their classes.","We characterized their challenges around analyzing student interaction data from intelligent tutoring systems and built VisTA (Visualizations for Tutor Analytics), a visual analytics system that shows detailed provenance data across multiple coordinated views.","We evaluated VisTA with the same five teachers, and found that the visualizations helped them better interpret intelligent tutor data, gain insights into student problem-solving provenance, and decide on necessary follow-up actions - such as providing students with further support or reviewing skills in the classroom.","Finally, we discuss potential extensions of VisTA into sequence query and detection, as well as the potential for the visualizations to be useful for encouraging self-directed learning in students."],"url":"http://arxiv.org/abs/2404.12944v1","category":"cs.HC"}
{"created":"2024-04-19 15:20:29","title":"Symmetry: a General Structure in Nonparametric Regression","abstract":"In this paper we present the framework of symmetry in nonparametric regression. This generalises the framework of covariate sparsity, where the regression function depends only on at most $s < d$ of the covariates, which is a special case of translation symmetry with linear orbits. In general this extends to other types of functions that capture lower dimensional behavior even when these structures are non-linear. We show both that known symmetries of regression functions can be exploited to give similarly faster rates, and that unknown symmetries with Lipschitz actions can be estimated sufficiently quickly to obtain the same rates. This is done by explicit constructions of partial symmetrisation operators that are then applied to usual estimators, and with a two step M-estimator of the maximal symmetry of the regression function. We also demonstrate the finite sample performance of these estimators on synthetic data.","sentences":["In this paper we present the framework of symmetry in nonparametric regression.","This generalises the framework of covariate sparsity, where the regression function depends only on at most $s < d$ of the covariates, which is a special case of translation symmetry with linear orbits.","In general this extends to other types of functions that capture lower dimensional behavior even when these structures are non-linear.","We show both that known symmetries of regression functions can be exploited to give similarly faster rates, and that unknown symmetries with Lipschitz actions can be estimated sufficiently quickly to obtain the same rates.","This is done by explicit constructions of partial symmetrisation operators that are then applied to usual estimators, and with a two step M-estimator of the maximal symmetry of the regression function.","We also demonstrate the finite sample performance of these estimators on synthetic data."],"url":"http://arxiv.org/abs/2404.12943v1","category":"math.ST"}
{"created":"2024-04-19 15:16:04","title":"Purposer: Putting Human Motion Generation in Context","abstract":"We present a novel method to generate human motion to populate 3D indoor scenes. It can be controlled with various combinations of conditioning signals such as a path in a scene, target poses, past motions, and scenes represented as 3D point clouds. State-of-the-art methods are either models specialized to one single setting, require vast amounts of high-quality and diverse training data, or are unconditional models that do not integrate scene or other contextual information. As a consequence, they have limited applicability and rely on costly training data. To address these limitations, we propose a new method ,dubbed Purposer, based on neural discrete representation learning. Our model is capable of exploiting, in a flexible manner, different types of information already present in open access large-scale datasets such as AMASS. First, we encode unconditional human motion into a discrete latent space. Second, an autoregressive generative model, conditioned with key contextual information, either with prompting or additive tokens, and trained for next-step prediction in this space, synthesizes sequences of latent indices. We further design a novel conditioning block to handle future conditioning information in such a causal model by using a network with two branches to compute separate stacks of features. In this manner, Purposer can generate realistic motion sequences in diverse test scenes. Through exhaustive evaluation, we demonstrate that our multi-contextual solution outperforms existing specialized approaches for specific contextual information, both in terms of quality and diversity. Our model is trained with short sequences, but a byproduct of being able to use various conditioning signals is that at test time different combinations can be used to chain short sequences together and generate long motions within a context scene.","sentences":["We present a novel method to generate human motion to populate 3D indoor scenes.","It can be controlled with various combinations of conditioning signals such as a path in a scene, target poses, past motions, and scenes represented as 3D point clouds.","State-of-the-art methods are either models specialized to one single setting, require vast amounts of high-quality and diverse training data, or are unconditional models that do not integrate scene or other contextual information.","As a consequence, they have limited applicability and rely on costly training data.","To address these limitations, we propose a new method ,dubbed Purposer, based on neural discrete representation learning.","Our model is capable of exploiting, in a flexible manner, different types of information already present in open access large-scale datasets such as AMASS.","First, we encode unconditional human motion into a discrete latent space.","Second, an autoregressive generative model, conditioned with key contextual information, either with prompting or additive tokens, and trained for next-step prediction in this space, synthesizes sequences of latent indices.","We further design a novel conditioning block to handle future conditioning information in such a causal model by using a network with two branches to compute separate stacks of features.","In this manner, Purposer can generate realistic motion sequences in diverse test scenes.","Through exhaustive evaluation, we demonstrate that our multi-contextual solution outperforms existing specialized approaches for specific contextual information, both in terms of quality and diversity.","Our model is trained with short sequences, but a byproduct of being able to use various conditioning signals is that at test time different combinations can be used to chain short sequences together and generate long motions within a context scene."],"url":"http://arxiv.org/abs/2404.12942v1","category":"cs.CV"}
{"created":"2024-04-19 15:10:54","title":"Neural Flow Diffusion Models: Learnable Forward Process for Improved Diffusion Modelling","abstract":"Conventional diffusion models typically relies on a fixed forward process, which implicitly defines complex marginal distributions over latent variables. This can often complicate the reverse process' task in learning generative trajectories, and results in costly inference for diffusion models. To address these limitations, we introduce Neural Flow Diffusion Models (NFDM), a novel framework that enhances diffusion models by supporting a broader range of forward processes beyond the fixed linear Gaussian. We also propose a novel parameterization technique for learning the forward process. Our framework provides an end-to-end, simulation-free optimization objective, effectively minimizing a variational upper bound on the negative log-likelihood. Experimental results demonstrate NFDM's strong performance, evidenced by state-of-the-art likelihood estimation. Furthermore, we investigate NFDM's capacity for learning generative dynamics with specific characteristics, such as deterministic straight lines trajectories. This exploration underscores NFDM's versatility and its potential for a wide range of applications.","sentences":["Conventional diffusion models typically relies on a fixed forward process, which implicitly defines complex marginal distributions over latent variables.","This can often complicate the reverse process' task in learning generative trajectories, and results in costly inference for diffusion models.","To address these limitations, we introduce Neural Flow Diffusion Models (NFDM), a novel framework that enhances diffusion models by supporting a broader range of forward processes beyond the fixed linear Gaussian.","We also propose a novel parameterization technique for learning the forward process.","Our framework provides an end-to-end, simulation-free optimization objective, effectively minimizing a variational upper bound on the negative log-likelihood.","Experimental results demonstrate NFDM's strong performance, evidenced by state-of-the-art likelihood estimation.","Furthermore, we investigate NFDM's capacity for learning generative dynamics with specific characteristics, such as deterministic straight lines trajectories.","This exploration underscores NFDM's versatility and its potential for a wide range of applications."],"url":"http://arxiv.org/abs/2404.12940v1","category":"stat.ML"}
{"created":"2024-04-19 15:08:06","title":"MAiDE-up: Multilingual Deception Detection of GPT-generated Hotel Reviews","abstract":"Deceptive reviews are becoming increasingly common, especially given the increase in performance and the prevalence of LLMs. While work to date has addressed the development of models to differentiate between truthful and deceptive human reviews, much less is known about the distinction between real reviews and AI-authored fake reviews. Moreover, most of the research so far has focused primarily on English, with very little work dedicated to other languages. In this paper, we compile and make publicly available the MAiDE-up dataset, consisting of 10,000 real and 10,000 AI-generated fake hotel reviews, balanced across ten languages. Using this dataset, we conduct extensive linguistic analyses to (1) compare the AI fake hotel reviews to real hotel reviews, and (2) identify the factors that influence the deception detection model performance. We explore the effectiveness of several models for deception detection in hotel reviews across three main dimensions: sentiment, location, and language. We find that these dimensions influence how well we can detect AI-generated fake reviews.","sentences":["Deceptive reviews are becoming increasingly common, especially given the increase in performance and the prevalence of LLMs.","While work to date has addressed the development of models to differentiate between truthful and deceptive human reviews, much less is known about the distinction between real reviews and AI-authored fake reviews.","Moreover, most of the research so far has focused primarily on English, with very little work dedicated to other languages.","In this paper, we compile and make publicly available the MAiDE-up dataset, consisting of 10,000 real and 10,000 AI-generated fake hotel reviews, balanced across ten languages.","Using this dataset, we conduct extensive linguistic analyses to (1) compare the AI fake hotel reviews to real hotel reviews, and (2) identify the factors that influence the deception detection model performance.","We explore the effectiveness of several models for deception detection in hotel reviews across three main dimensions: sentiment, location, and language.","We find that these dimensions influence how well we can detect AI-generated fake reviews."],"url":"http://arxiv.org/abs/2404.12938v1","category":"cs.CL"}
{"created":"2024-04-19 15:07:51","title":"Coupled $\\operatorname{G}_2$-instantons","abstract":"We introduce the coupled instanton equations for a metric, a spinor, a three-form, and a connection on a bundle, over a spin manifold. Special solutions in dimensions $6$ and $7$ arise, respectively, from the Hull--Strominger and heterotic $\\operatorname{G}_2$ systems. The equations are motivated by recent developments in theoretical physics and can be recast using generalized geometry; we investigate how coupled instantons relate to generalized Ricci-flat metrics and also to Killing spinors on a Courant algebroid. In this respect, we present two open questions regarding how these different geometric conditions are intertwined. A positive answer is expected from recent developments in the physics literature in work by De la Ossa, Larfors and Svanes, and in the mathematics literature, for the case of Calabi--Yau manifolds, in recent work by the second author jointly with Gonzalez Molina. We give a complete solution to the first of these two problems, for $\\operatorname{G}_2$-structures with torsion coupled to $\\operatorname{G}_2$-instantons, in the seven-dimensional case, and also establish some partial results for the second problem. The last part of the present work carefully studies the approximate solutions to the heterotic $\\operatorname{G}_2$-system constructed by the third and fourth authors on contact Calabi--Yau $7$-manifolds, for which we prove the existence of approximate coupled $\\operatorname{G}_2$-instantons and generalized Ricci-flat metrics.","sentences":["We introduce the coupled instanton equations for a metric, a spinor, a three-form, and a connection on a bundle, over a spin manifold.","Special solutions in dimensions $6$ and $7$ arise, respectively, from the Hull--Strominger and heterotic $\\operatorname{G}_2$ systems.","The equations are motivated by recent developments in theoretical physics and can be recast using generalized geometry; we investigate how coupled instantons relate to generalized Ricci-flat metrics and also to Killing spinors on a Courant algebroid.","In this respect, we present two open questions regarding how these different geometric conditions are intertwined.","A positive answer is expected from recent developments in the physics literature in work by De la Ossa, Larfors and Svanes, and in the mathematics literature, for the case of Calabi--Yau manifolds, in recent work by the second author jointly with Gonzalez Molina.","We give a complete solution to the first of these two problems, for $\\operatorname{G}_2$-structures with torsion coupled to $\\operatorname{G}_2$-instantons, in the seven-dimensional case, and also establish some partial results for the second problem.","The last part of the present work carefully studies the approximate solutions to the heterotic $\\operatorname{G}_2$-system constructed by the third and fourth authors on contact Calabi--Yau $7$-manifolds, for which we prove the existence of approximate coupled $\\operatorname{G}_2$-instantons and generalized Ricci-flat metrics."],"url":"http://arxiv.org/abs/2404.12937v1","category":"math.DG"}
{"created":"2024-04-19 15:04:30","title":"Cross-cultural Inspiration Detection and Analysis in Real and LLM-generated Social Media Data","abstract":"Inspiration is linked to various positive outcomes, such as increased creativity, productivity, and happiness. Although inspiration has great potential, there has been limited effort toward identifying content that is inspiring, as opposed to just engaging or positive. Additionally, most research has concentrated on Western data, with little attention paid to other cultures. This work is the first to study cross-cultural inspiration through machine learning methods. We aim to identify and analyze real and AI-generated cross-cultural inspiring posts. To this end, we compile and make publicly available the InspAIred dataset, which consists of 2,000 real inspiring posts, 2,000 real non-inspiring posts, and 2,000 generated inspiring posts evenly distributed across India and the UK. The real posts are sourced from Reddit, while the generated posts are created using the GPT-4 model. Using this dataset, we conduct extensive computational linguistic analyses to (1) compare inspiring content across cultures, (2) compare AI-generated inspiring posts to real inspiring posts, and (3) determine if detection models can accurately distinguish between inspiring content across cultures and data sources.","sentences":["Inspiration is linked to various positive outcomes, such as increased creativity, productivity, and happiness.","Although inspiration has great potential, there has been limited effort toward identifying content that is inspiring, as opposed to just engaging or positive.","Additionally, most research has concentrated on Western data, with little attention paid to other cultures.","This work is the first to study cross-cultural inspiration through machine learning methods.","We aim to identify and analyze real and AI-generated cross-cultural inspiring posts.","To this end, we compile and make publicly available the InspAIred dataset, which consists of 2,000 real inspiring posts, 2,000 real non-inspiring posts, and 2,000 generated inspiring posts evenly distributed across India and the UK.","The real posts are sourced from Reddit, while the generated posts are created using the GPT-4 model.","Using this dataset, we conduct extensive computational linguistic analyses to (1) compare inspiring content across cultures, (2) compare AI-generated inspiring posts to real inspiring posts, and (3) determine if detection models can accurately distinguish between inspiring content across cultures and data sources."],"url":"http://arxiv.org/abs/2404.12933v1","category":"cs.CL"}
{"created":"2024-04-19 14:55:21","title":"The Positivity of the Neural Tangent Kernel","abstract":"The Neural Tangent Kernel (NTK) has emerged as a fundamental concept in the study of wide Neural Networks. In particular, it is known that the positivity of the NTK is directly related to the memorization capacity of sufficiently wide networks, i.e., to the possibility of reaching zero loss in training, via gradient descent. Here we will improve on previous works and obtain a sharp result concerning the positivity of the NTK of feedforward networks of any depth. More precisely, we will show that, for any non-polynomial activation function, the NTK is strictly positive definite. Our results are based on a novel characterization of polynomial functions which is of independent interest.","sentences":["The Neural Tangent Kernel (NTK) has emerged as a fundamental concept in the study of wide Neural Networks.","In particular, it is known that the positivity of the NTK is directly related to the memorization capacity of sufficiently wide networks, i.e., to the possibility of reaching zero loss in training, via gradient descent.","Here we will improve on previous works and obtain a sharp result concerning the positivity of the NTK of feedforward networks of any depth.","More precisely, we will show that, for any non-polynomial activation function, the NTK is strictly positive definite.","Our results are based on a novel characterization of polynomial functions which is of independent interest."],"url":"http://arxiv.org/abs/2404.12928v1","category":"cs.LG"}
{"created":"2024-04-19 14:52:57","title":"MM-PhyRLHF: Reinforcement Learning Framework for Multimodal Physics Question-Answering","abstract":"Recent advancements in LLMs have shown their significant potential in tasks like text summarization and generation. Yet, they often encounter difficulty while solving complex physics problems that require arithmetic calculation and a good understanding of concepts. Moreover, many physics problems include images that contain important details required to understand the problem's context. We propose an LMM-based chatbot to answer multimodal physics MCQs. For domain adaptation, we utilize the MM-PhyQA dataset comprising Indian high school-level multimodal physics problems. To improve the LMM's performance, we experiment with two techniques, RLHF (Reinforcement Learning from Human Feedback) and Image Captioning. In image captioning, we add a detailed explanation of the diagram in each image, minimizing hallucinations and image processing errors. We further explore the integration of Reinforcement Learning from Human Feedback (RLHF) methodology inspired by the ranking approach in RLHF to enhance the human-like problem-solving abilities of the models. The RLHF approach incorporates human feedback into the learning process of LLMs, improving the model's problem-solving skills, truthfulness, and reasoning capabilities, minimizing the hallucinations in the answers, and improving the quality instead of using vanilla-supervised fine-tuned models. We employ the LLaVA open-source model to answer multimodal physics MCQs and compare the performance with and without using RLHF.","sentences":["Recent advancements in LLMs have shown their significant potential in tasks like text summarization and generation.","Yet, they often encounter difficulty while solving complex physics problems that require arithmetic calculation and a good understanding of concepts.","Moreover, many physics problems include images that contain important details required to understand the problem's context.","We propose an LMM-based chatbot to answer multimodal physics MCQs.","For domain adaptation, we utilize the MM-PhyQA dataset comprising Indian high school-level multimodal physics problems.","To improve the LMM's performance, we experiment with two techniques, RLHF (Reinforcement Learning from Human Feedback) and Image Captioning.","In image captioning, we add a detailed explanation of the diagram in each image, minimizing hallucinations and image processing errors.","We further explore the integration of Reinforcement Learning from Human Feedback (RLHF) methodology inspired by the ranking approach in RLHF to enhance the human-like problem-solving abilities of the models.","The RLHF approach incorporates human feedback into the learning process of LLMs, improving the model's problem-solving skills, truthfulness, and reasoning capabilities, minimizing the hallucinations in the answers, and improving the quality instead of using vanilla-supervised fine-tuned models.","We employ the LLaVA open-source model to answer multimodal physics MCQs and compare the performance with and without using RLHF."],"url":"http://arxiv.org/abs/2404.12926v1","category":"cs.AI"}
{"created":"2024-04-19 14:52:25","title":"A Hybrid Generative and Discriminative PointNet on Unordered Point Sets","abstract":"As point cloud provides a natural and flexible representation usable in myriad applications (e.g., robotics and self-driving cars), the ability to synthesize point clouds for analysis becomes crucial. Recently, Xie et al. propose a generative model for unordered point sets in the form of an energy-based model (EBM). Despite the model achieving an impressive performance for point cloud generation, one separate model needs to be trained for each category to capture the complex point set distributions. Besides, their method is unable to classify point clouds directly and requires additional fine-tuning for classification. One interesting question is: Can we train a single network for a hybrid generative and discriminative model of point clouds? A similar question has recently been answered in the affirmative for images, introducing the framework of Joint Energy-based Model (JEM), which achieves high performance in image classification and generation simultaneously. This paper proposes GDPNet, the first hybrid Generative and Discriminative PointNet that extends JEM for point cloud classification and generation. Our GDPNet retains strong discriminative power of modern PointNet classifiers, while generating point cloud samples rivaling state-of-the-art generative approaches.","sentences":["As point cloud provides a natural and flexible representation usable in myriad applications (e.g., robotics and self-driving cars), the ability to synthesize point clouds for analysis becomes crucial.","Recently, Xie et al. propose a generative model for unordered point sets in the form of an energy-based model (EBM).","Despite the model achieving an impressive performance for point cloud generation, one separate model needs to be trained for each category to capture the complex point set distributions.","Besides, their method is unable to classify point clouds directly and requires additional fine-tuning for classification.","One interesting question is: Can we train a single network for a hybrid generative and discriminative model of point clouds?","A similar question has recently been answered in the affirmative for images, introducing the framework of Joint Energy-based Model (JEM), which achieves high performance in image classification and generation simultaneously.","This paper proposes GDPNet, the first hybrid Generative and Discriminative PointNet that extends JEM for point cloud classification and generation.","Our GDPNet retains strong discriminative power of modern PointNet classifiers, while generating point cloud samples rivaling state-of-the-art generative approaches."],"url":"http://arxiv.org/abs/2404.12925v1","category":"cs.CV"}
{"created":"2024-04-19 14:45:27","title":"Is Retain Set All You Need in Machine Unlearning? Restoring Performance of Unlearned Models with Out-Of-Distribution Images","abstract":"In this paper, we introduce Selective-distillation for Class and Architecture-agnostic unleaRning (SCAR), a novel approximate unlearning method. SCAR efficiently eliminates specific information while preserving the model's test accuracy without using a retain set, which is a key component in state-of-the-art approximate unlearning algorithms. Our approach utilizes a modified Mahalanobis distance to guide the unlearning of the feature vectors of the instances to be forgotten, aligning them to the nearest wrong class distribution. Moreover, we propose a distillation-trick mechanism that distills the knowledge of the original model into the unlearning model with out-of-distribution images for retaining the original model's test performance without using any retain set. Importantly, we propose a self-forget version of SCAR that unlearns without having access to the forget set. We experimentally verified the effectiveness of our method, on three public datasets, comparing it with state-of-the-art methods. Our method obtains performance higher than methods that operate without the retain set and comparable w.r.t the best methods that rely on the retain set.","sentences":["In this paper, we introduce Selective-distillation for Class and Architecture-agnostic unleaRning (SCAR), a novel approximate unlearning method.","SCAR efficiently eliminates specific information while preserving the model's test accuracy without using a retain set, which is a key component in state-of-the-art approximate unlearning algorithms.","Our approach utilizes a modified Mahalanobis distance to guide the unlearning of the feature vectors of the instances to be forgotten, aligning them to the nearest wrong class distribution.","Moreover, we propose a distillation-trick mechanism that distills the knowledge of the original model into the unlearning model with out-of-distribution images for retaining the original model's test performance without using any retain set.","Importantly, we propose a self-forget version of SCAR that unlearns without having access to the forget set.","We experimentally verified the effectiveness of our method, on three public datasets, comparing it with state-of-the-art methods.","Our method obtains performance higher than methods that operate without the retain set and comparable w.r.t the best methods that rely on the retain set."],"url":"http://arxiv.org/abs/2404.12922v1","category":"cs.CV"}
{"created":"2024-04-19 14:43:48","title":"Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models","abstract":"Localizing the exact pathological regions in a given medical scan is an important imaging problem that requires a large amount of bounding box ground truth annotations to be accurately solved. However, there exist alternative, potentially weaker, forms of supervision, such as accompanying free-text reports, which are readily available. The task of performing localization with textual guidance is commonly referred to as phrase grounding. In this work, we use a publicly available Foundation Model, namely the Latent Diffusion Model, to solve this challenging task. This choice is supported by the fact that the Latent Diffusion Model, despite being generative in nature, contains mechanisms (cross-attention) that implicitly align visual and textual features, thus leading to intermediate representations that are suitable for the task at hand. In addition, we aim to perform this task in a zero-shot manner, i.e., without any further training on target data, meaning that the model's weights remain frozen. To this end, we devise strategies to select features and also refine them via post-processing without extra learnable parameters. We compare our proposed method with state-of-the-art approaches which explicitly enforce image-text alignment in a joint embedding space via contrastive learning. Results on a popular chest X-ray benchmark indicate that our method is competitive wih SOTA on different types of pathology, and even outperforms them on average in terms of two metrics (mean IoU and AUC-ROC). Source code will be released upon acceptance.","sentences":["Localizing the exact pathological regions in a given medical scan is an important imaging problem that requires a large amount of bounding box ground truth annotations to be accurately solved.","However, there exist alternative, potentially weaker, forms of supervision, such as accompanying free-text reports, which are readily available.","The task of performing localization with textual guidance is commonly referred to as phrase grounding.","In this work, we use a publicly available Foundation Model, namely the Latent Diffusion Model, to solve this challenging task.","This choice is supported by the fact that the Latent Diffusion Model, despite being generative in nature, contains mechanisms (cross-attention) that implicitly align visual and textual features, thus leading to intermediate representations that are suitable for the task at hand.","In addition, we aim to perform this task in a zero-shot manner, i.e., without any further training on target data, meaning that the model's weights remain frozen.","To this end, we devise strategies to select features and also refine them via post-processing without extra learnable parameters.","We compare our proposed method with state-of-the-art approaches which explicitly enforce image-text alignment in a joint embedding space via contrastive learning.","Results on a popular chest X-ray benchmark indicate that our method is competitive wih SOTA on different types of pathology, and even outperforms them on average in terms of two metrics (mean IoU and AUC-ROC).","Source code will be released upon acceptance."],"url":"http://arxiv.org/abs/2404.12920v1","category":"cs.CV"}
{"created":"2024-04-19 14:42:42","title":"Zero-Shot Stitching in Reinforcement Learning using Relative Representations","abstract":"Visual Reinforcement Learning is a popular and powerful framework that takes full advantage of the Deep Learning breakthrough. However, it is also known that variations in the input (e.g., different colors of the panorama due to the season of the year) or the task (e.g., changing the speed limit for a car to respect) could require complete retraining of the agents. In this work, we leverage recent developments in unifying latent representations to demonstrate that it is possible to combine the components of an agent, rather than retrain it from scratch. We build upon the recent relative representations framework and adapt it for Visual RL. This allows us to create completely new agents capable of handling environment-task combinations never seen during training. Our work paves the road toward a more accessible and flexible use of reinforcement learning.","sentences":["Visual Reinforcement Learning is a popular and powerful framework that takes full advantage of the Deep Learning breakthrough.","However, it is also known that variations in the input (e.g., different colors of the panorama due to the season of the year) or the task (e.g., changing the speed limit for a car to respect) could require complete retraining of the agents.","In this work, we leverage recent developments in unifying latent representations to demonstrate that it is possible to combine the components of an agent, rather than retrain it from scratch.","We build upon the recent relative representations framework and adapt it for Visual RL.","This allows us to create completely new agents capable of handling environment-task combinations never seen during training.","Our work paves the road toward a more accessible and flexible use of reinforcement learning."],"url":"http://arxiv.org/abs/2404.12917v1","category":"cs.LG"}
{"created":"2024-04-19 14:42:42","title":"Interpreting neural operators: how nonlinear waves propagate in non-reciprocal solids","abstract":"We present a data-driven pipeline for model building that combines interpretable machine learning, hydrodynamic theories, and microscopic models. The goal is to uncover the underlying processes governing nonlinear dynamics experiments. We exemplify our method with data from microfluidic experiments where crystals of streaming droplets support the propagation of nonlinear waves absent in passive crystals. By combining physics-inspired neural networks, known as neural operators, with symbolic regression tools, we generate the solution, as well as the mathematical form, of a nonlinear dynamical system that accurately models the experimental data. Finally, we interpret this continuum model from fundamental physics principles. Informed by machine learning, we coarse grain a microscopic model of interacting droplets and discover that non-reciprocal hydrodynamic interactions stabilise and promote nonlinear wave propagation.","sentences":["We present a data-driven pipeline for model building that combines interpretable machine learning, hydrodynamic theories, and microscopic models.","The goal is to uncover the underlying processes governing nonlinear dynamics experiments.","We exemplify our method with data from microfluidic experiments where crystals of streaming droplets support the propagation of nonlinear waves absent in passive crystals.","By combining physics-inspired neural networks, known as neural operators, with symbolic regression tools, we generate the solution, as well as the mathematical form, of a nonlinear dynamical system that accurately models the experimental data.","Finally, we interpret this continuum model from fundamental physics principles.","Informed by machine learning, we coarse grain a microscopic model of interacting droplets and discover that non-reciprocal hydrodynamic interactions stabilise and promote nonlinear wave propagation."],"url":"http://arxiv.org/abs/2404.12918v1","category":"cond-mat.soft"}
{"created":"2024-04-19 14:40:38","title":"Physical Backdoor Attack can Jeopardize Driving with Vision-Large-Language Models","abstract":"Vision-Large-Language-models(VLMs) have great application prospects in autonomous driving. Despite the ability of VLMs to comprehend and make decisions in complex scenarios, their integration into safety-critical autonomous driving systems poses serious security risks. In this paper, we propose BadVLMDriver, the first backdoor attack against VLMs for autonomous driving that can be launched in practice using physical objects. Unlike existing backdoor attacks against VLMs that rely on digital modifications, BadVLMDriver uses common physical items, such as a red balloon, to induce unsafe actions like sudden acceleration, highlighting a significant real-world threat to autonomous vehicle safety. To execute BadVLMDriver, we develop an automated pipeline utilizing natural language instructions to generate backdoor training samples with embedded malicious behaviors. This approach allows for flexible trigger and behavior selection, enhancing the stealth and practicality of the attack in diverse scenarios. We conduct extensive experiments to evaluate BadVLMDriver for two representative VLMs, five different trigger objects, and two types of malicious backdoor behaviors. BadVLMDriver achieves a 92% attack success rate in inducing a sudden acceleration when coming across a pedestrian holding a red balloon. Thus, BadVLMDriver not only demonstrates a critical security risk but also emphasizes the urgent need for developing robust defense mechanisms to protect against such vulnerabilities in autonomous driving technologies.","sentences":["Vision-Large-Language-models(VLMs) have great application prospects in autonomous driving.","Despite the ability of VLMs to comprehend and make decisions in complex scenarios, their integration into safety-critical autonomous driving systems poses serious security risks.","In this paper, we propose BadVLMDriver, the first backdoor attack against VLMs for autonomous driving that can be launched in practice using physical objects.","Unlike existing backdoor attacks against VLMs that rely on digital modifications, BadVLMDriver uses common physical items, such as a red balloon, to induce unsafe actions like sudden acceleration, highlighting a significant real-world threat to autonomous vehicle safety.","To execute BadVLMDriver, we develop an automated pipeline utilizing natural language instructions to generate backdoor training samples with embedded malicious behaviors.","This approach allows for flexible trigger and behavior selection, enhancing the stealth and practicality of the attack in diverse scenarios.","We conduct extensive experiments to evaluate BadVLMDriver for two representative VLMs, five different trigger objects, and two types of malicious backdoor behaviors.","BadVLMDriver achieves a 92% attack success rate in inducing a sudden acceleration when coming across a pedestrian holding a red balloon.","Thus, BadVLMDriver not only demonstrates a critical security risk but also emphasizes the urgent need for developing robust defense mechanisms to protect against such vulnerabilities in autonomous driving technologies."],"url":"http://arxiv.org/abs/2404.12916v1","category":"cs.CR"}
{"created":"2024-04-19 14:33:16","title":"Saturating a Fundamental Bound on Quantum Measurements' Accuracy","abstract":"A quantum system is usually measured through observations performed on a second quantum system, or meter, to which it is coupled. In this scenario, fundamental limitations arise as stated by the celebrated Wigner-Araki-Yanase theorem and its generalizations, predicting an upper-bound on the measurement's accuracy (Ozawa's bound). Here, we show it is possible to saturate this fundamental bound. We propose a simple interferometric setup, arguably within reach of present technology, in which a flying particle (the quantum meter) is used to measure the state of a qubit (the target system). We show that the bound can be saturated and that this happens only if the flying particle is prepared in a Gaussian wavepacket.","sentences":["A quantum system is usually measured through observations performed on a second quantum system, or meter, to which it is coupled.","In this scenario, fundamental limitations arise as stated by the celebrated Wigner-Araki-Yanase theorem and its generalizations, predicting an upper-bound on the measurement's accuracy (Ozawa's bound).","Here, we show it is possible to saturate this fundamental bound.","We propose a simple interferometric setup, arguably within reach of present technology, in which a flying particle (the quantum meter) is used to measure the state of a qubit (the target system).","We show that the bound can be saturated and that this happens only if the flying particle is prepared in a Gaussian wavepacket."],"url":"http://arxiv.org/abs/2404.12910v1","category":"quant-ph"}
{"created":"2024-04-19 14:30:41","title":"Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images","abstract":"Diffusion models (DMs) have revolutionized image generation, producing high-quality images with applications spanning various fields. However, their ability to create hyper-realistic images poses significant challenges in distinguishing between real and synthetic content, raising concerns about digital authenticity and potential misuse in creating deepfakes. This work introduces a robust detection framework that integrates image and text features extracted by CLIP model with a Multilayer Perceptron (MLP) classifier. We propose a novel loss that can improve the detector's robustness and handle imbalanced datasets. Additionally, we flatten the loss landscape during the model training to improve the detector's generalization capabilities. The effectiveness of our method, which outperforms traditional detection techniques, is demonstrated through extensive experiments, underscoring its potential to set a new state-of-the-art approach in DM-generated image detection. The code is available at https://github.com/Purdue-M2/Robust_DM_Generated_Image_Detection.","sentences":["Diffusion models (DMs) have revolutionized image generation, producing high-quality images with applications spanning various fields.","However, their ability to create hyper-realistic images poses significant challenges in distinguishing between real and synthetic content, raising concerns about digital authenticity and potential misuse in creating deepfakes.","This work introduces a robust detection framework that integrates image and text features extracted by CLIP model with a Multilayer Perceptron (MLP) classifier.","We propose a novel loss that can improve the detector's robustness and handle imbalanced datasets.","Additionally, we flatten the loss landscape during the model training to improve the detector's generalization capabilities.","The effectiveness of our method, which outperforms traditional detection techniques, is demonstrated through extensive experiments, underscoring its potential to set a new state-of-the-art approach in DM-generated image detection.","The code is available at https://github.com/Purdue-M2/Robust_DM_Generated_Image_Detection."],"url":"http://arxiv.org/abs/2404.12908v1","category":"cs.CV"}
{"created":"2024-04-19 14:28:00","title":"Dynamic Parameterized Feedback Problems in Tournaments","abstract":"In this paper we present the first dynamic algorithms for the problem of Feedback Arc Set in Tournaments (FAST) and the problem of Feedback Vertex Set in Tournaments (FVST). Our algorithms maintain a dynamic tournament on n vertices altered by redirecting the arcs, and answer if the tournament admits a feedback arc set (or respectively feedback vertex set) of size at most K, for some chosen parameter K. For dynamic FAST we offer two algorithms. In the promise model, where we are guaranteed, that the size of the solution does not exceed g(K) for some computable function g, we give an $O(\\sqrt{g(K)})$ update and $O(3^K K \\sqrt{K})$ query algorithm. In the general setting without any promise, we offer an $O(\\log^2 n)$ update and $O(3^K K \\log^2 n)$ query time algorithm for dynamic FAST. For dynamic FVST we offer an algorithm working in the promise model, which admits $O(g^5(K))$ update and $O(3^K K^3 g(K))$ query time.","sentences":["In this paper we present the first dynamic algorithms for the problem of Feedback Arc Set in Tournaments (FAST) and the problem of Feedback Vertex Set in Tournaments (FVST).","Our algorithms maintain a dynamic tournament on n vertices altered by redirecting the arcs, and answer if the tournament admits a feedback arc set (or respectively feedback vertex set) of size at most K, for some chosen parameter K. For dynamic FAST we offer two algorithms.","In the promise model, where we are guaranteed, that the size of the solution does not exceed g(K) for some computable function g, we give an $O(\\sqrt{g(K)})$ update and $O(3^K K \\sqrt{K})$ query algorithm.","In the general setting without any promise, we offer an $O(\\log^2 n)$ update and $O(3^K K \\log^2 n)$ query time algorithm for dynamic FAST.","For dynamic FVST we offer an algorithm working in the promise model, which admits $O(g^5(K))$ update and $O(3^K K^3 g(K))$ query time."],"url":"http://arxiv.org/abs/2404.12907v1","category":"cs.DS"}
{"created":"2024-04-19 14:27:34","title":"A Pythagorean triangle in which the hypotenuse and the sum of the arms are squares","abstract":"In this paper, show that the Diophantine equation $ x^2+(x+1)^2=w^4 $ has only two solutions $ (0,1) $ and $ (119,13)$ in non-negative integers $ x $ and $ w $. This equation concerned a classic problem posed by Pierre de Fermat, wonders about finding a Pythagorean triangle in which the hypotenuse and the sum of the arms are square. We review the method of finding the smallest solution presented by Fermat, and the relationship between the primitive Pythagorean triples and the Pell's equation, Finally, we present an algorithm for finding primitive solutions, which actually enabled us to find a second solution.","sentences":["In this paper, show that the Diophantine equation $ x^2+(x+1)^2=w^4 $ has only two solutions $ (0,1) $ and $ (119,13)$ in non-negative integers $ x $ and $ w $.","This equation concerned a classic problem posed by Pierre de Fermat, wonders about finding a Pythagorean triangle in which the hypotenuse and the sum of the arms are square.","We review the method of finding the smallest solution presented by Fermat, and the relationship between the primitive Pythagorean triples and the Pell's equation, Finally, we present an algorithm for finding primitive solutions, which actually enabled us to find a second solution."],"url":"http://arxiv.org/abs/2404.12906v1","category":"math.GM"}
{"created":"2024-04-19 14:19:39","title":"The MEMENTO code for modelling of macroscopic melt motion in fusion devices","abstract":"The MEMENTO (MEtallic Melt Evolution in Next-step TOkamaks) code is a new numerical implementation of the physics model originally developed for the MEMOS-U code with the objective to self-consistently describe the generation of melt and its subsequent large scale dynamics in fusion devices and to assess the damage of metallic reactor armor under powerful normal and off-normal plasma events. The model has been validated in multiple dedicated EUROfusion experiments. MEMENTO solves the heat and phase transfer problem coupled with the incompressible Navier-Stokes equations in the shallow water approximation for the thin liquid film over the solid metal and with the current propagation equations on a domain that features a time-evolving deforming metal-plasma interface. The code utilizes non-uniform and adaptive meshing along with sub-cycling in time facilitated by the AMReX open-source framework as well as AMReX's built-in parallelization capabilities.","sentences":["The MEMENTO (MEtallic Melt Evolution in Next-step TOkamaks) code is a new numerical implementation of the physics model originally developed for the MEMOS-U code with the objective to self-consistently describe the generation of melt and its subsequent large scale dynamics in fusion devices and to assess the damage of metallic reactor armor under powerful normal and off-normal plasma events.","The model has been validated in multiple dedicated EUROfusion experiments.","MEMENTO solves the heat and phase transfer problem coupled with the incompressible Navier-Stokes equations in the shallow water approximation for the thin liquid film over the solid metal and with the current propagation equations on a domain that features a time-evolving deforming metal-plasma interface.","The code utilizes non-uniform and adaptive meshing along with sub-cycling in time facilitated by the AMReX open-source framework as well as AMReX's built-in parallelization capabilities."],"url":"http://arxiv.org/abs/2404.12904v1","category":"physics.plasm-ph"}
{"created":"2024-04-19 14:19:13","title":"ConCLVD: Controllable Chinese Landscape Video Generation via Diffusion Model","abstract":"Chinese landscape painting is a gem of Chinese cultural and artistic heritage that showcases the splendor of nature through the deep observations and imaginations of its painters. Limited by traditional techniques, these artworks were confined to static imagery in ancient times, leaving the dynamism of landscapes and the subtleties of artistic sentiment to the viewer's imagination. Recently, emerging text-to-video (T2V) diffusion methods have shown significant promise in video generation, providing hope for the creation of dynamic Chinese landscape paintings. However, challenges such as the lack of specific datasets, the intricacy of artistic styles, and the creation of extensive, high-quality videos pose difficulties for these models in generating Chinese landscape painting videos. In this paper, we propose CLV-HD (Chinese Landscape Video-High Definition), a novel T2V dataset for Chinese landscape painting videos, and ConCLVD (Controllable Chinese Landscape Video Diffusion), a T2V model that utilizes Stable Diffusion. Specifically, we present a motion module featuring a dual attention mechanism to capture the dynamic transformations of landscape imageries, alongside a noise adapter to leverage unsupervised contrastive learning in the latent space. Following the generation of keyframes, we employ optical flow for frame interpolation to enhance video smoothness. Our method not only retains the essence of the landscape painting imageries but also achieves dynamic transitions, significantly advancing the field of artistic video generation. The source code and dataset are available at https://anonymous.4open.science/r/ConCLVD-EFE3.","sentences":["Chinese landscape painting is a gem of Chinese cultural and artistic heritage that showcases the splendor of nature through the deep observations and imaginations of its painters.","Limited by traditional techniques, these artworks were confined to static imagery in ancient times, leaving the dynamism of landscapes and the subtleties of artistic sentiment to the viewer's imagination.","Recently, emerging text-to-video (T2V) diffusion methods have shown significant promise in video generation, providing hope for the creation of dynamic Chinese landscape paintings.","However, challenges such as the lack of specific datasets, the intricacy of artistic styles, and the creation of extensive, high-quality videos pose difficulties for these models in generating Chinese landscape painting videos.","In this paper, we propose CLV-HD (Chinese Landscape Video-High Definition), a novel T2V dataset for Chinese landscape painting videos, and ConCLVD (Controllable Chinese Landscape Video Diffusion), a T2V model that utilizes Stable Diffusion.","Specifically, we present a motion module featuring a dual attention mechanism to capture the dynamic transformations of landscape imageries, alongside a noise adapter to leverage unsupervised contrastive learning in the latent space.","Following the generation of keyframes, we employ optical flow for frame interpolation to enhance video smoothness.","Our method not only retains the essence of the landscape painting imageries but also achieves dynamic transitions, significantly advancing the field of artistic video generation.","The source code and dataset are available at https://anonymous.4open.science/r/ConCLVD-EFE3."],"url":"http://arxiv.org/abs/2404.12903v1","category":"cs.MM"}
{"created":"2024-04-19 14:17:02","title":"Large Language Models for Networking: Workflow, Advances and Challenges","abstract":"The networking field is characterized by its high complexity and rapid iteration, requiring extensive expertise to accomplish network tasks, ranging from network design, diagnosis, configuration and security. The inherent complexity of these tasks, coupled with the ever-changing landscape of networking technologies and protocols, poses significant hurdles for traditional machine learning-based methods. These methods often struggle to generalize and automate complex tasks in networking, as they require extensive labeled data, domain-specific feature engineering, and frequent retraining to adapt to new scenarios. However, the recent emergence of large language models (LLMs) has sparked a new wave of possibilities in addressing these challenges. LLMs have demonstrated remarkable capabilities in natural language understanding, generation, and reasoning. These models, trained on extensive data, can benefit the networking domain. Some efforts have already explored the application of LLMs in the networking domain and revealed promising results. By reviewing recent advances, we present an abstract workflow to describe the fundamental process involved in applying LLM for Networking. We introduce the highlights of existing works by category and explain in detail how they operate at different stages of the workflow. Furthermore, we delve into the challenges encountered, discuss potential solutions, and outline future research prospects. We hope that this survey will provide insight for researchers and practitioners, promoting the development of this interdisciplinary research field.","sentences":["The networking field is characterized by its high complexity and rapid iteration, requiring extensive expertise to accomplish network tasks, ranging from network design, diagnosis, configuration and security.","The inherent complexity of these tasks, coupled with the ever-changing landscape of networking technologies and protocols, poses significant hurdles for traditional machine learning-based methods.","These methods often struggle to generalize and automate complex tasks in networking, as they require extensive labeled data, domain-specific feature engineering, and frequent retraining to adapt to new scenarios.","However, the recent emergence of large language models (LLMs) has sparked a new wave of possibilities in addressing these challenges.","LLMs have demonstrated remarkable capabilities in natural language understanding, generation, and reasoning.","These models, trained on extensive data, can benefit the networking domain.","Some efforts have already explored the application of LLMs in the networking domain and revealed promising results.","By reviewing recent advances, we present an abstract workflow to describe the fundamental process involved in applying LLM for Networking.","We introduce the highlights of existing works by category and explain in detail how they operate at different stages of the workflow.","Furthermore, we delve into the challenges encountered, discuss potential solutions, and outline future research prospects.","We hope that this survey will provide insight for researchers and practitioners, promoting the development of this interdisciplinary research field."],"url":"http://arxiv.org/abs/2404.12901v1","category":"cs.NI"}
{"created":"2024-04-19 14:13:46","title":"Training-and-prompt-free General Painterly Harmonization Using Image-wise Attention Sharing","abstract":"Painterly Image Harmonization aims at seamlessly blending disparate visual elements within a single coherent image. However, previous approaches often encounter significant limitations due to training data constraints, the need for time-consuming fine-tuning, or reliance on additional prompts. To surmount these hurdles, we design a Training-and-prompt-Free General Painterly Harmonization method using image-wise attention sharing (TF-GPH), which integrates a novel \"share-attention module\". This module redefines the traditional self-attention mechanism by allowing for comprehensive image-wise attention, facilitating the use of a state-of-the-art pretrained latent diffusion model without the typical training data limitations. Additionally, we further introduce \"similarity reweighting\" mechanism enhances performance by effectively harnessing cross-image information, surpassing the capabilities of fine-tuning or prompt-based approaches. At last, we recognize the deficiencies in existing benchmarks and propose the \"General Painterly Harmonization Benchmark\", which employs range-based evaluation metrics to more accurately reflect real-world application. Extensive experiments demonstrate the superior efficacy of our method across various benchmarks. The code and web demo are available at https://github.com/BlueDyee/TF-GPH.","sentences":["Painterly Image Harmonization aims at seamlessly blending disparate visual elements within a single coherent image.","However, previous approaches often encounter significant limitations due to training data constraints, the need for time-consuming fine-tuning, or reliance on additional prompts.","To surmount these hurdles, we design a Training-and-prompt-Free General Painterly Harmonization method using image-wise attention sharing (TF-GPH), which integrates a novel \"share-attention module\".","This module redefines the traditional self-attention mechanism by allowing for comprehensive image-wise attention, facilitating the use of a state-of-the-art pretrained latent diffusion model without the typical training data limitations.","Additionally, we further introduce \"similarity reweighting\" mechanism enhances performance by effectively harnessing cross-image information, surpassing the capabilities of fine-tuning or prompt-based approaches.","At last, we recognize the deficiencies in existing benchmarks and propose the \"General Painterly Harmonization Benchmark\", which employs range-based evaluation metrics to more accurately reflect real-world application.","Extensive experiments demonstrate the superior efficacy of our method across various benchmarks.","The code and web demo are available at https://github.com/BlueDyee/TF-GPH."],"url":"http://arxiv.org/abs/2404.12900v1","category":"cs.CV"}
{"created":"2024-04-19 14:10:17","title":"Hall mass and transverse Noether spin currents in noncollinear antiferromagnets","abstract":"Noncollinear antiferromagnets (AFMs) in the family of Mn$_3X$ ($X$=Ir, Sn, Ge, Pt, etc.) have recently attracted attention in the emerging field of antiferromagnetic spintronics because of their various interesting transport, magnetic, and optical properties. Due to the noncollinear magnetic order, the localized electron spins on different magnetic sublattices are not conserved even when spin-orbit coupling is neglected, making it difficult to understand the transport of spin angular momentum. Here we study the conserved Noether current associated with spin-rotation symmetry of the local spins in noncollinear AFMs. We found that a Hall component of the d.c. spin current can be created by a longitudinal driving force associated with a propagating spin wave, and is proportional to a response coefficient that we denote as the Hall (inverse) mass. Such a Hall spin current can be generated by spin pumping in a ferromagnet (FM)-noncollinear AFM bilayer structure as we demonstrated numerically. Finally we showed that the Hall mass is an isotropic quantity, similar to the isotropic spin Hall conductivity, and should generally exist in noncollinear AFMs and their polycrystals. Our results shed light on the potential of noncollinear AFMs in manipulating the polarization and flow of spin currents in general spintronic devices.","sentences":["Noncollinear antiferromagnets (AFMs) in the family of Mn$_3X$ ($X$=Ir, Sn, Ge, Pt, etc.) have recently attracted attention in the emerging field of antiferromagnetic spintronics because of their various interesting transport, magnetic, and optical properties.","Due to the noncollinear magnetic order, the localized electron spins on different magnetic sublattices are not conserved even when spin-orbit coupling is neglected, making it difficult to understand the transport of spin angular momentum.","Here we study the conserved Noether current associated with spin-rotation symmetry of the local spins in noncollinear AFMs.","We found that a Hall component of the d.c. spin current can be created by a longitudinal driving force associated with a propagating spin wave, and is proportional to a response coefficient that we denote as the Hall (inverse) mass.","Such a Hall spin current can be generated by spin pumping in a ferromagnet (FM)-noncollinear AFM bilayer structure as we demonstrated numerically.","Finally we showed that the Hall mass is an isotropic quantity, similar to the isotropic spin Hall conductivity, and should generally exist in noncollinear AFMs and their polycrystals.","Our results shed light on the potential of noncollinear AFMs in manipulating the polarization and flow of spin currents in general spintronic devices."],"url":"http://arxiv.org/abs/2404.12898v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-19 14:05:03","title":"Enabling Natural Zero-Shot Prompting on Encoder Models via Statement-Tuning","abstract":"While Large Language Models (LLMs) exhibit remarkable capabilities in zero-shot and few-shot scenarios, they often require computationally prohibitive sizes. Conversely, smaller Masked Language Models (MLMs) like BERT and RoBERTa achieve state-of-the-art results through fine-tuning but struggle with extending to few-shot and zero-shot settings due to their architectural constraints. Hence, we propose Statement-Tuning, a technique that models discriminative tasks as a set of finite statements and trains an Encoder model to discriminate between the potential statements to determine the label. We do Statement-Tuning on multiple tasks to enable cross-task generalization. Experimental results demonstrate that Statement Tuning achieves competitive performance compared to state-of-the-art LLMs with significantly fewer parameters. Moreover, the study investigates the impact of several design choices on few-shot and zero-shot generalization, revealing that Statement Tuning can achieve sufficient performance with modest training data and benefits from task and statement diversity for unseen task generalizability.","sentences":["While Large Language Models (LLMs) exhibit remarkable capabilities in zero-shot and few-shot scenarios, they often require computationally prohibitive sizes.","Conversely, smaller Masked Language Models (MLMs) like BERT and RoBERTa achieve state-of-the-art results through fine-tuning but struggle with extending to few-shot and zero-shot settings due to their architectural constraints.","Hence, we propose Statement-Tuning, a technique that models discriminative tasks as a set of finite statements and trains an Encoder model to discriminate between the potential statements to determine the label.","We do Statement-Tuning on multiple tasks to enable cross-task generalization.","Experimental results demonstrate that Statement Tuning achieves competitive performance compared to state-of-the-art LLMs with significantly fewer parameters.","Moreover, the study investigates the impact of several design choices on few-shot and zero-shot generalization, revealing that Statement Tuning can achieve sufficient performance with modest training data and benefits from task and statement diversity for unseen task generalizability."],"url":"http://arxiv.org/abs/2404.12897v1","category":"cs.CL"}
{"created":"2024-04-19 13:58:34","title":"The emergence of the width of subjective temporality: the self-simulational theory of temporal extension from the perspective of the free energy principle","abstract":"The self-simulational theory of temporal extension describes an information-theoretically formalized mechanism by which the width of subjective temporality emerges from the architecture of self-modelling. In this paper, the perspective of the free energy principle will be assumed, to cast the emergence of subjective temporality, along with a mechanism for duration estimation, from first principles of the physics of self-organization. Building on the transparent inferential format of self-modelling, it will be explained why subjective temporality feels like a genuine dimension, in which our life unfolds, as opposed to a mere mental construct, such as the mental number line. Using active inference, a deep parametric generative model of temporal inference is simulated, which realizes the described dynamics on a computational level. Two biases (i.e. variations) of time-perception naturally emerge from the simulation. This concerns the intentional binding effect (i.e. the subjective compression of the temporal interval between voluntarily initiated actions and subsequent sensory consequences) and empirically documented alterations of subjective time experience in deep and concentrated states of meditative absorption. Generally, numerous systematic and domain-specific variations of subjective time experience are computationally explained, as enabled by integration with current active inference accounts mapping onto the respective domains. This concerns the temporality modulating role of negative valence, impulsivity, boredom, flow-states, and near death-experiences, amongst others. The self-simulational theory of temporal extension, from the perspective of the free energy principle, explains how the subjective temporal Now emerges and varies from first principles, accounting for why sometimes, subjective time seems to fly, and sometimes, moments feel like eternities.","sentences":["The self-simulational theory of temporal extension describes an information-theoretically formalized mechanism by which the width of subjective temporality emerges from the architecture of self-modelling.","In this paper, the perspective of the free energy principle will be assumed, to cast the emergence of subjective temporality, along with a mechanism for duration estimation, from first principles of the physics of self-organization.","Building on the transparent inferential format of self-modelling, it will be explained why subjective temporality feels like a genuine dimension, in which our life unfolds, as opposed to a mere mental construct, such as the mental number line.","Using active inference, a deep parametric generative model of temporal inference is simulated, which realizes the described dynamics on a computational level.","Two biases (i.e. variations) of time-perception naturally emerge from the simulation.","This concerns the intentional binding effect (i.e. the subjective compression of the temporal interval between voluntarily initiated actions and subsequent sensory consequences) and empirically documented alterations of subjective time experience in deep and concentrated states of meditative absorption.","Generally, numerous systematic and domain-specific variations of subjective time experience are computationally explained, as enabled by integration with current active inference accounts mapping onto the respective domains.","This concerns the temporality modulating role of negative valence, impulsivity, boredom, flow-states, and near death-experiences, amongst others.","The self-simulational theory of temporal extension, from the perspective of the free energy principle, explains how the subjective temporal Now emerges and varies from first principles, accounting for why sometimes, subjective time seems to fly, and sometimes, moments feel like eternities."],"url":"http://arxiv.org/abs/2404.12895v1","category":"q-bio.NC"}
{"created":"2024-04-19 13:54:34","title":"The Power of Words: Generating PowerShell Attacks from Natural Language","abstract":"As the Windows OS stands out as one of the most targeted systems, the PowerShell language has become a key tool for malicious actors and cybersecurity professionals (e.g., for penetration testing). This work explores an uncharted domain in AI code generation by automatically generating offensive PowerShell code from natural language descriptions using Neural Machine Translation (NMT). For training and evaluation purposes, we propose two novel datasets with PowerShell code samples, one with manually curated descriptions in natural language and another code-only dataset for reinforcing the training. We present an extensive evaluation of state-of-the-art NMT models and analyze the generated code both statically and dynamically. Results indicate that tuning NMT using our dataset is effective at generating offensive PowerShell code. Comparative analysis against the most widely used LLM service ChatGPT reveals the specialized strengths of our fine-tuned models.","sentences":["As the Windows OS stands out as one of the most targeted systems, the PowerShell language has become a key tool for malicious actors and cybersecurity professionals (e.g., for penetration testing).","This work explores an uncharted domain in AI code generation by automatically generating offensive PowerShell code from natural language descriptions using Neural Machine Translation (NMT).","For training and evaluation purposes, we propose two novel datasets with PowerShell code samples, one with manually curated descriptions in natural language and another code-only dataset for reinforcing the training.","We present an extensive evaluation of state-of-the-art NMT models and analyze the generated code both statically and dynamically.","Results indicate that tuning NMT using our dataset is effective at generating offensive PowerShell code.","Comparative analysis against the most widely used LLM service ChatGPT reveals the specialized strengths of our fine-tuned models."],"url":"http://arxiv.org/abs/2404.12893v1","category":"cs.CR"}
{"created":"2024-04-19 13:46:33","title":"On the probability of linear separability through intrinsic volumes","abstract":"A dataset with two labels is linearly separable if it can be split into its two classes with a hyperplane. This inflicts a curse on some statistical tools (such as logistic regression) but forms a blessing for others (e.g. support vector machines). Recently, the following question has regained interest: What is the probability that the data are linearly separable?   We provide a formula for the probability of linear separability for Gaussian features and labels depending only on one marginal of the features (as in generalized linear models). In this setting, we derive an upper bound that complements the recent result by Hayakawa, Lyons, and Oberhauser [2023], and a sharp upper bound for sign-flip noise.   To prove our results, we exploit that this probability can be expressed as a sum of the intrinsic volumes of a polyhedral cone of the form $\\text{span}\\{v\\}\\oplus[0,\\infty)^n$, as shown in Cand\\`es and Sur [2020]. After providing the inequality description for this cone, and an algorithm to project onto it, we calculate its intrinsic volumes. In doing so, we encounter Youden's demon problem, for which we provide a formula following Kabluchko and Zaporozhets [2020]. The key insight of this work is the following: The number of correctly labeled observations in the data affects the structure of this polyhedral cone, allowing the translation of insights from geometry into statistics.","sentences":["A dataset with two labels is linearly separable if it can be split into its two classes with a hyperplane.","This inflicts a curse on some statistical tools (such as logistic regression) but forms a blessing for others (e.g. support vector machines).","Recently, the following question has regained interest: What is the probability that the data are linearly separable?   ","We provide a formula for the probability of linear separability for Gaussian features and labels depending only on one marginal of the features (as in generalized linear models).","In this setting, we derive an upper bound that complements the recent result by Hayakawa, Lyons, and Oberhauser","[2023], and a sharp upper bound for sign-flip noise.   ","To prove our results, we exploit that this probability can be expressed as a sum of the intrinsic volumes of a polyhedral cone of the form $\\text{span}\\{v\\}\\oplus[0,\\infty)^n$, as shown in Cand\\`es and Sur [2020].","After providing the inequality description for this cone, and an algorithm to project onto it, we calculate its intrinsic volumes.","In doing so, we encounter Youden's demon problem, for which we provide a formula following Kabluchko and Zaporozhets [2020].","The key insight of this work is the following: The number of correctly labeled observations in the data affects the structure of this polyhedral cone, allowing the translation of insights from geometry into statistics."],"url":"http://arxiv.org/abs/2404.12889v1","category":"math.ST"}
{"created":"2024-04-19 13:43:14","title":"3D Multi-frame Fusion for Video Stabilization","abstract":"In this paper, we present RStab, a novel framework for video stabilization that integrates 3D multi-frame fusion through volume rendering. Departing from conventional methods, we introduce a 3D multi-frame perspective to generate stabilized images, addressing the challenge of full-frame generation while preserving structure. The core of our approach lies in Stabilized Rendering (SR), a volume rendering module, which extends beyond the image fusion by incorporating feature fusion. The core of our RStab framework lies in Stabilized Rendering (SR), a volume rendering module, fusing multi-frame information in 3D space. Specifically, SR involves warping features and colors from multiple frames by projection, fusing them into descriptors to render the stabilized image. However, the precision of warped information depends on the projection accuracy, a factor significantly influenced by dynamic regions. In response, we introduce the Adaptive Ray Range (ARR) module to integrate depth priors, adaptively defining the sampling range for the projection process. Additionally, we propose Color Correction (CC) assisting geometric constraints with optical flow for accurate color aggregation. Thanks to the three modules, our RStab demonstrates superior performance compared with previous stabilizers in the field of view (FOV), image quality, and video stability across various datasets.","sentences":["In this paper, we present RStab, a novel framework for video stabilization that integrates 3D multi-frame fusion through volume rendering.","Departing from conventional methods, we introduce a 3D multi-frame perspective to generate stabilized images, addressing the challenge of full-frame generation while preserving structure.","The core of our approach lies in Stabilized Rendering (SR), a volume rendering module, which extends beyond the image fusion by incorporating feature fusion.","The core of our RStab framework lies in Stabilized Rendering (SR), a volume rendering module, fusing multi-frame information in 3D space.","Specifically, SR involves warping features and colors from multiple frames by projection, fusing them into descriptors to render the stabilized image.","However, the precision of warped information depends on the projection accuracy, a factor significantly influenced by dynamic regions.","In response, we introduce the Adaptive Ray Range (ARR) module to integrate depth priors, adaptively defining the sampling range for the projection process.","Additionally, we propose Color Correction (CC) assisting geometric constraints with optical flow for accurate color aggregation.","Thanks to the three modules, our RStab demonstrates superior performance compared with previous stabilizers in the field of view (FOV), image quality, and video stability across various datasets."],"url":"http://arxiv.org/abs/2404.12887v1","category":"cs.CV"}
{"created":"2024-04-19 13:40:25","title":"MCM: Multi-condition Motion Synthesis Framework","abstract":"Conditional human motion synthesis (HMS) aims to generate human motion sequences that conform to specific conditions. Text and audio represent the two predominant modalities employed as HMS control conditions. While existing research has primarily focused on single conditions, the multi-condition human motion synthesis remains underexplored. In this study, we propose a multi-condition HMS framework, termed MCM, based on a dual-branch structure composed of a main branch and a control branch. This framework effectively extends the applicability of the diffusion model, which is initially predicated solely on textual conditions, to auditory conditions. This extension encompasses both music-to-dance and co-speech HMS while preserving the intrinsic quality of motion and the capabilities for semantic association inherent in the original model. Furthermore, we propose the implementation of a Transformer-based diffusion model, designated as MWNet, as the main branch. This model adeptly apprehends the spatial intricacies and inter-joint correlations inherent in motion sequences, facilitated by the integration of multi-wise self-attention modules. Extensive experiments show that our method achieves competitive results in single-condition and multi-condition HMS tasks.","sentences":["Conditional human motion synthesis (HMS) aims to generate human motion sequences that conform to specific conditions.","Text and audio represent the two predominant modalities employed as HMS control conditions.","While existing research has primarily focused on single conditions, the multi-condition human motion synthesis remains underexplored.","In this study, we propose a multi-condition HMS framework, termed MCM, based on a dual-branch structure composed of a main branch and a control branch.","This framework effectively extends the applicability of the diffusion model, which is initially predicated solely on textual conditions, to auditory conditions.","This extension encompasses both music-to-dance and co-speech HMS while preserving the intrinsic quality of motion and the capabilities for semantic association inherent in the original model.","Furthermore, we propose the implementation of a Transformer-based diffusion model, designated as MWNet, as the main branch.","This model adeptly apprehends the spatial intricacies and inter-joint correlations inherent in motion sequences, facilitated by the integration of multi-wise self-attention modules.","Extensive experiments show that our method achieves competitive results in single-condition and multi-condition HMS tasks."],"url":"http://arxiv.org/abs/2404.12886v1","category":"cs.CV"}
{"created":"2024-04-19 13:27:38","title":"Unlocking Multi-View Insights in Knowledge-Dense Retrieval-Augmented Generation","abstract":"While Retrieval-Augmented Generation (RAG) plays a crucial role in the application of Large Language Models (LLMs), existing retrieval methods in knowledge-dense domains like law and medicine still suffer from a lack of multi-perspective views, which are essential for improving interpretability and reliability. Previous research on multi-view retrieval often focused solely on different semantic forms of queries, neglecting the expression of specific domain knowledge perspectives. This paper introduces a novel multi-view RAG framework, MVRAG, tailored for knowledge-dense domains that utilizes intention-aware query rewriting from multiple domain viewpoints to enhance retrieval precision, thereby improving the effectiveness of the final inference. Experiments conducted on legal and medical case retrieval demonstrate significant improvements in recall and precision rates with our framework. Our multi-perspective retrieval approach unleashes the potential of multi-view information enhancing RAG tasks, accelerating the further application of LLMs in knowledge-intensive fields.","sentences":["While Retrieval-Augmented Generation (RAG) plays a crucial role in the application of Large Language Models (LLMs), existing retrieval methods in knowledge-dense domains like law and medicine still suffer from a lack of multi-perspective views, which are essential for improving interpretability and reliability.","Previous research on multi-view retrieval often focused solely on different semantic forms of queries, neglecting the expression of specific domain knowledge perspectives.","This paper introduces a novel multi-view RAG framework, MVRAG, tailored for knowledge-dense domains that utilizes intention-aware query rewriting from multiple domain viewpoints to enhance retrieval precision, thereby improving the effectiveness of the final inference.","Experiments conducted on legal and medical case retrieval demonstrate significant improvements in recall and precision rates with our framework.","Our multi-perspective retrieval approach unleashes the potential of multi-view information enhancing RAG tasks, accelerating the further application of LLMs in knowledge-intensive fields."],"url":"http://arxiv.org/abs/2404.12879v1","category":"cs.CL"}
{"created":"2024-04-19 13:25:27","title":"A Large-scale Medical Visual Task Adaptation Benchmark","abstract":"Visual task adaptation has been demonstrated to be effective in adapting pre-trained Vision Transformers (ViTs) to general downstream visual tasks using specialized learnable layers or tokens. However, there is yet a large-scale benchmark to fully explore the effect of visual task adaptation on the realistic and important medical domain, particularly across diverse medical visual modalities, such as color images, X-ray, and CT. To close this gap, we present Med-VTAB, a large-scale Medical Visual Task Adaptation Benchmark consisting of 1.68 million medical images for diverse organs, modalities, and adaptation approaches. Based on Med-VTAB, we explore the scaling law of medical prompt tuning concerning tunable parameters and the generalizability of medical visual adaptation using non-medical/medical pre-train weights. Besides, we study the impact of patient ID out-of-distribution on medical visual adaptation, which is a real and challenging scenario. Furthermore, results from Med-VTAB indicate that a single pre-trained model falls short in medical task adaptation. Therefore, we introduce GMoE-Adapter, a novel method that combines medical and general pre-training weights through a gated mixture-of-experts adapter, achieving state-of-the-art results in medical visual task adaptation.","sentences":["Visual task adaptation has been demonstrated to be effective in adapting pre-trained Vision Transformers (ViTs) to general downstream visual tasks using specialized learnable layers or tokens.","However, there is yet a large-scale benchmark to fully explore the effect of visual task adaptation on the realistic and important medical domain, particularly across diverse medical visual modalities, such as color images, X-ray, and CT.","To close this gap, we present Med-VTAB, a large-scale Medical Visual Task Adaptation Benchmark consisting of 1.68 million medical images for diverse organs, modalities, and adaptation approaches.","Based on Med-VTAB, we explore the scaling law of medical prompt tuning concerning tunable parameters and the generalizability of medical visual adaptation using non-medical/medical pre-train weights.","Besides, we study the impact of patient ID out-of-distribution on medical visual adaptation, which is a real and challenging scenario.","Furthermore, results from Med-VTAB indicate that a single pre-trained model falls short in medical task adaptation.","Therefore, we introduce GMoE-Adapter, a novel method that combines medical and general pre-training weights through a gated mixture-of-experts adapter, achieving state-of-the-art results in medical visual task adaptation."],"url":"http://arxiv.org/abs/2404.12876v1","category":"cs.CV"}
{"created":"2024-04-19 13:25:22","title":"A comprehensive study of compact stars with dark matter","abstract":"We present a comprehensive study of compact stars admixed with non-self annihilating self-interacting fermionic dark matter, delineating the dependence on the nuclear equation of state by considering the two limiting parametrized equations of state for neutron star matter obtained by smoothly matching the low-density chiral effective theory and the high-density perturbative QCD. These two parametrizations are the limiting cases of a wide variety of smooth equations of state, i.e. the softest and stiffest possible one without a phase transition, that generate masses and radii compatible with 2M$_\\odot$ observations and the tidal constraint from GW170817. With an exhaustive analysis of the possible stable mass-radius configurations, we determine the quantity of dark matter contained in stars with masses and radii compatible with the aforementioned astrophysical constraints. We find that for dark particle masses of a few tenths of GeV, the dark core collapses and no stable solutions are found irrespective on the chosen nuclear equation of state. For lower masses, the dark matter fraction is limited to 10%, being at most 1% for masses ranging from 0.1 to 10 GeV for the limiting soft nuclear equation of state. For the limiting stiff nuclear equation of state, the dark matter fraction can reach values of more than 10%, but the dark particle mass is being constrained to 0.3 GeV and 10 GeV for the weak self-interacting case and has to be at least 5 GeV for the strong self-interacting one. For dark particle masses of less than 0.1 GeV, stable neutron star configurations should have less than 1% of self-interacting dark matter to be compatible with the constraint of the tidal deformability from GW170817 irrespective on the chosen nuclear equation of state.","sentences":["We present a comprehensive study of compact stars admixed with non-self annihilating self-interacting fermionic dark matter, delineating the dependence on the nuclear equation of state by considering the two limiting parametrized equations of state for neutron star matter obtained by smoothly matching the low-density chiral effective theory and the high-density perturbative QCD.","These two parametrizations are the limiting cases of a wide variety of smooth equations of state, i.e. the softest and stiffest possible one without a phase transition, that generate masses and radii compatible with 2M$_\\odot$ observations and the tidal constraint from GW170817.","With an exhaustive analysis of the possible stable mass-radius configurations, we determine the quantity of dark matter contained in stars with masses and radii compatible with the aforementioned astrophysical constraints.","We find that for dark particle masses of a few tenths of GeV, the dark core collapses and no stable solutions are found irrespective on the chosen nuclear equation of state.","For lower masses, the dark matter fraction is limited to 10%, being at most 1% for masses ranging from 0.1 to 10 GeV for the limiting soft nuclear equation of state.","For the limiting stiff nuclear equation of state, the dark matter fraction can reach values of more than 10%, but the dark particle mass is being constrained to 0.3 GeV and 10 GeV for the weak self-interacting case and has to be at least 5 GeV for the strong self-interacting one.","For dark particle masses of less than 0.1 GeV, stable neutron star configurations should have less than 1% of self-interacting dark matter to be compatible with the constraint of the tidal deformability from GW170817 irrespective on the chosen nuclear equation of state."],"url":"http://arxiv.org/abs/2404.12875v1","category":"astro-ph.HE"}
{"created":"2024-04-19 13:19:47","title":"Wrinkling instability of 3D auxetic bilayers in tension","abstract":"Bilayers (soft substrates coated with stiff films) are commonly found in nature with examples including skin tissue, vesicles, or organ membranes. They exhibit various types of instabilities when subjected to compression, depending on the contrast in material properties between the two components. We present wrinkling instabilities for 3D hyperelastic bilayer systems, including auxetics (materials with negative Poisson's ratio), under uni-axial tension. In tension, a soft bilayer can experience large lateral contraction, and we find that with an adequate contrast in the Poisson ratios, compressive stresses may develop and generate wrinkles aligned with the tensile direction. We rely on an analytic modelling of the phenomenon, and validate it with a user-defined Python script with periodic boundary conditions and constitutive relation implementation in advanced Finite Element simulations. Our findings reveal that wrinkles are observed when the Poisson ratio of the substrate is greater than that of the film. As the two Poisson ratios converge to a common value, the critical stretch of instability shoots up rapidly, and the wrinkling disappears. We also confirm these results by asymptotic analysis. This wrinkling analysis has significant potential in controlling surface patterns of auxetic skin grafts and hydrogel organ patches under mechanical loads. Moreover, the asymptotic expressions in this work can be used under finite strain for buckling-based metrology applications.","sentences":["Bilayers (soft substrates coated with stiff films) are commonly found in nature with examples including skin tissue, vesicles, or organ membranes.","They exhibit various types of instabilities when subjected to compression, depending on the contrast in material properties between the two components.","We present wrinkling instabilities for 3D hyperelastic bilayer systems, including auxetics (materials with negative Poisson's ratio), under uni-axial tension.","In tension, a soft bilayer can experience large lateral contraction, and we find that with an adequate contrast in the Poisson ratios, compressive stresses may develop and generate wrinkles aligned with the tensile direction.","We rely on an analytic modelling of the phenomenon, and validate it with a user-defined Python script with periodic boundary conditions and constitutive relation implementation in advanced Finite Element simulations.","Our findings reveal that wrinkles are observed when the Poisson ratio of the substrate is greater than that of the film.","As the two Poisson ratios converge to a common value, the critical stretch of instability shoots up rapidly, and the wrinkling disappears.","We also confirm these results by asymptotic analysis.","This wrinkling analysis has significant potential in controlling surface patterns of auxetic skin grafts and hydrogel organ patches under mechanical loads.","Moreover, the asymptotic expressions in this work can be used under finite strain for buckling-based metrology applications."],"url":"http://arxiv.org/abs/2404.12873v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-19 13:17:07","title":"LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency","abstract":"Query rewrite, which aims to generate more efficient queries by altering a SQL query's structure without changing the query result, has been an important research problem. In order to maintain equivalence between the rewritten query and the original one during rewriting, traditional query rewrite methods always rewrite the queries following certain rewrite rules. However, some problems still remain. Firstly, existing methods of finding the optimal choice or sequence of rewrite rules are still limited and the process always costs a lot of resources. Methods involving discovering new rewrite rules typically require complicated proofs of structural logic or extensive user interactions. Secondly, current query rewrite methods usually rely highly on DBMS cost estimators which are often not accurate. In this paper, we address these problems by proposing a novel method of query rewrite named LLM-R2, adopting a large language model (LLM) to propose possible rewrite rules for a database rewrite system. To further improve the inference ability of LLM in recommending rewrite rules, we train a contrastive model by curriculum to learn query representations and select effective query demonstrations for the LLM. Experimental results have shown that our method can significantly improve the query execution efficiency and outperform the baseline methods. In addition, our method enjoys high robustness across different datasets.","sentences":["Query rewrite, which aims to generate more efficient queries by altering a SQL query's structure without changing the query result, has been an important research problem.","In order to maintain equivalence between the rewritten query and the original one during rewriting, traditional query rewrite methods always rewrite the queries following certain rewrite rules.","However, some problems still remain.","Firstly, existing methods of finding the optimal choice or sequence of rewrite rules are still limited and the process always costs a lot of resources.","Methods involving discovering new rewrite rules typically require complicated proofs of structural logic or extensive user interactions.","Secondly, current query rewrite methods usually rely highly on DBMS cost estimators which are often not accurate.","In this paper, we address these problems by proposing a novel method of query rewrite named LLM-R2, adopting a large language model (LLM) to propose possible rewrite rules for a database rewrite system.","To further improve the inference ability of LLM in recommending rewrite rules, we train a contrastive model by curriculum to learn query representations and select effective query demonstrations for the LLM.","Experimental results have shown that our method can significantly improve the query execution efficiency and outperform the baseline methods.","In addition, our method enjoys high robustness across different datasets."],"url":"http://arxiv.org/abs/2404.12872v1","category":"cs.DB"}
{"created":"2024-04-19 13:15:18","title":"Grid-aware Scheduling and Control of Electric Vehicle Charging Stations for Dispatching Active Distribution Networks. Part-II: Intra-day and Experimental Validation","abstract":"In Part-I, we presented an optimal day-ahead scheduling scheme for dispatching active distribution networks accounting for the flexibility provided by electric vehicle charging stations (EVCSs) and other controllable resources such as battery energy storage systems (BESSs). Part-II presents the intra-day control layer for tracking the dispatch plan computed from the day-ahead scheduling stage. The control problem is formulated as model predictive control (MPC) with an objective to track the dispatch plan setpoint every 5 minutes, while actuated every 30 seconds. MPC accounts for the uncertainty of the power injections from stochastic resources (such as demand and generation from photovoltaic - PV plants) by short-term forecasts. MPC also accounts for the grid's operational constraints (i.e., the limits on the nodal voltages and the line power-flows) by a linearized optimal power flow (LOPF) model based on the power-flow sensitivity coefficients, and for the operational constraints of the controllable resources (i.e., BESSs and EVCSs). The proposed framework is experimentally validated on a real-life ADN at the EPFL's Distributed Electrical Systems Laboratory and is composed of a medium voltage (MV) bus connected to three low voltage distribution networks. It hosts two controllable EVCSs (172 kWp and 32 F~kWp), multiple PV plants (aggregated generation of 42~kWp), uncontrollable demand from office buildings (20 kWp), and two controllable BESSs (150kW/300kWh and 25kW/25kWh).","sentences":["In Part-I, we presented an optimal day-ahead scheduling scheme for dispatching active distribution networks accounting for the flexibility provided by electric vehicle charging stations (EVCSs) and other controllable resources such as battery energy storage systems (BESSs).","Part-II presents the intra-day control layer for tracking the dispatch plan computed from the day-ahead scheduling stage.","The control problem is formulated as model predictive control (MPC) with an objective to track the dispatch plan setpoint every 5 minutes, while actuated every 30 seconds.","MPC accounts for the uncertainty of the power injections from stochastic resources (such as demand and generation from photovoltaic - PV plants) by short-term forecasts.","MPC also accounts for the grid's operational constraints (i.e., the limits on the nodal voltages and the line power-flows) by a linearized optimal power flow (LOPF) model based on the power-flow sensitivity coefficients, and for the operational constraints of the controllable resources (i.e., BESSs and EVCSs).","The proposed framework is experimentally validated on a real-life ADN at the EPFL's Distributed Electrical Systems Laboratory and is composed of a medium voltage (MV) bus connected to three low voltage distribution networks.","It hosts two controllable EVCSs (172 kWp and 32 F~kWp), multiple PV plants (aggregated generation of 42~kWp), uncontrollable demand from office buildings (20 kWp), and two controllable BESSs (150kW/300kWh and 25kW/25kWh)."],"url":"http://arxiv.org/abs/2404.12870v1","category":"eess.SY"}
{"created":"2024-04-19 13:12:13","title":"Tribo-piezoelectric Nanogenerators for Energy Harvesting: a first-principles study","abstract":"Two-dimensional transition metal dichalcogenides (TMDs) are highly promising candidates for various applications due to their unique electrical, optical, mechanical, and chemical properties. Furthermore, heterostructures consisting of TMDs with metals, oxides, and conductive materials have attracted significant research interest due to their exceptional electronic properties. In this study, we utilized density functional theory to investigate those electronic and transport properties, which are relevant for the application of tribo-piezoelectricity in creating novel nanogenerators: an interdisciplinary approach with promising implications. The results of the study demonstrate that the enhancement of charge transfer between layers and the orbital contribution to the Fermi level under applied strain in MoS/IrO, MoS/TiO, MoS/WTe, and MoTe/WS heterostructures is noteworthy. Additionally, non-equilibrium Green's function calculations of electron transport properties provide valuable insights into the behavior of these materials under different conditions. While MoS/IrO and MoS/TiO hetero-bilayers are unsuitable due to their tendency to exhibit large current flow with increasing voltage, others like MoS/WTe and MoTe/WS hetero bilayers show promise due to their ability to prevent voltage drop. The presented innovative concept of utilizing compressive strain of TMD bilayers to generate a tribo-piezoelectric effect for nanogenerators has a potential to contribute to the development of efficient and sustainable energy harvesting devices.","sentences":["Two-dimensional transition metal dichalcogenides (TMDs) are highly promising candidates for various applications due to their unique electrical, optical, mechanical, and chemical properties.","Furthermore, heterostructures consisting of TMDs with metals, oxides, and conductive materials have attracted significant research interest due to their exceptional electronic properties.","In this study, we utilized density functional theory to investigate those electronic and transport properties, which are relevant for the application of tribo-piezoelectricity in creating novel nanogenerators: an interdisciplinary approach with promising implications.","The results of the study demonstrate that the enhancement of charge transfer between layers and the orbital contribution to the Fermi level under applied strain in MoS/IrO, MoS/TiO, MoS/WTe, and MoTe/WS heterostructures is noteworthy.","Additionally, non-equilibrium Green's function calculations of electron transport properties provide valuable insights into the behavior of these materials under different conditions.","While MoS/IrO and MoS/TiO hetero-bilayers are unsuitable due to their tendency to exhibit large current flow with increasing voltage, others like MoS/WTe and MoTe/WS hetero bilayers show promise due to their ability to prevent voltage drop.","The presented innovative concept of utilizing compressive strain of TMD bilayers to generate a tribo-piezoelectric effect for nanogenerators has a potential to contribute to the development of efficient and sustainable energy harvesting devices."],"url":"http://arxiv.org/abs/2404.12869v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-19 13:05:37","title":"How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?","abstract":"The increase in parameter size of multimodal large language models (MLLMs) introduces significant capabilities, particularly in-context learning, where MLLMs enhance task performance without updating pre-trained parameters. This effectiveness, however, hinges on the appropriate selection of in-context examples, a process that is currently biased towards visual data, overlooking textual information. Furthermore, the area of supervised retrievers for MLLMs, crucial for optimal in-context example selection, continues to be uninvestigated. Our study offers an in-depth evaluation of the impact of textual information on the unsupervised selection of in-context examples in multimodal contexts, uncovering a notable sensitivity of retriever performance to the employed modalities. Responding to this, we introduce a novel supervised MLLM-retriever MSIER that employs a neural network to select examples that enhance multimodal in-context learning efficiency. This approach is validated through extensive testing across three distinct tasks, demonstrating the method's effectiveness. Additionally, we investigate the influence of modalities on our supervised retrieval method's training and pinpoint factors contributing to our model's success. This exploration paves the way for future advancements, highlighting the potential for refined in-context learning in MLLMs through the strategic use of multimodal data.","sentences":["The increase in parameter size of multimodal large language models (MLLMs) introduces significant capabilities, particularly in-context learning, where MLLMs enhance task performance without updating pre-trained parameters.","This effectiveness, however, hinges on the appropriate selection of in-context examples, a process that is currently biased towards visual data, overlooking textual information.","Furthermore, the area of supervised retrievers for MLLMs, crucial for optimal in-context example selection, continues to be uninvestigated.","Our study offers an in-depth evaluation of the impact of textual information on the unsupervised selection of in-context examples in multimodal contexts, uncovering a notable sensitivity of retriever performance to the employed modalities.","Responding to this, we introduce a novel supervised MLLM-retriever MSIER that employs a neural network to select examples that enhance multimodal in-context learning efficiency.","This approach is validated through extensive testing across three distinct tasks, demonstrating the method's effectiveness.","Additionally, we investigate the influence of modalities on our supervised retrieval method's training and pinpoint factors contributing to our model's success.","This exploration paves the way for future advancements, highlighting the potential for refined in-context learning in MLLMs through the strategic use of multimodal data."],"url":"http://arxiv.org/abs/2404.12866v1","category":"cs.CL"}
{"created":"2024-04-19 13:03:14","title":"Nyon Unchained: Forensic Analysis of Bosch's eBike Board Computers","abstract":"Modern eBike on-board computers are basically small PCs that not only offer motor control, navigation, and performance monitoring, but also store lots of sensitive user data. The Bosch Nyon series of board computers are cutting-edge devices from one of the market leaders in the eBike business, which is why they are especially interesting for forensics. Therefore, we conducted an in-depth forensic analysis of the two available Nyon models released in 2014 and 2021. On a first-generation Nyon device, Telnet access could be established by abusing a design flaw in the update procedure, which allowed the acquisition of relevant data without risking damage to the hardware. Besides the user's personal information, the data analysis revealed databases containing user activities, including timestamps and GPS coordinates. Furthermore, it was possible to forge the data on the device and transfer it to Bosch's servers to be persisted across their online service and smartphone app. On a current second-generation Nyon device, no software-based access could be obtained. For this reason, more intrusive hardware-based options were considered, and the data could be extracted via chip-off eventually. Despite encryption, the user data could be accessed and evaluated. Besides location and user information, the newer model holds even more forensically relevant data, such as nearby Bluetooth devices.","sentences":["Modern eBike on-board computers are basically small PCs that not only offer motor control, navigation, and performance monitoring, but also store lots of sensitive user data.","The Bosch Nyon series of board computers are cutting-edge devices from one of the market leaders in the eBike business, which is why they are especially interesting for forensics.","Therefore, we conducted an in-depth forensic analysis of the two available Nyon models released in 2014 and 2021.","On a first-generation Nyon device, Telnet access could be established by abusing a design flaw in the update procedure, which allowed the acquisition of relevant data without risking damage to the hardware.","Besides the user's personal information, the data analysis revealed databases containing user activities, including timestamps and GPS coordinates.","Furthermore, it was possible to forge the data on the device and transfer it to Bosch's servers to be persisted across their online service and smartphone app.","On a current second-generation Nyon device, no software-based access could be obtained.","For this reason, more intrusive hardware-based options were considered, and the data could be extracted via chip-off eventually.","Despite encryption, the user data could be accessed and evaluated.","Besides location and user information, the newer model holds even more forensically relevant data, such as nearby Bluetooth devices."],"url":"http://arxiv.org/abs/2404.12864v1","category":"cs.CR"}
{"created":"2024-04-19 13:01:59","title":"A Guide to Feature Importance Methods for Scientific Inference","abstract":"While machine learning (ML) models are increasingly used due to their high predictive power, their use in understanding the data-generating process (DGP) is limited. Understanding the DGP requires insights into feature-target associations, which many ML models cannot directly provide, due to their opaque internal mechanisms. Feature importance (FI) methods provide useful insights into the DGP under certain conditions. Since the results of different FI methods have different interpretations, selecting the correct FI method for a concrete use case is crucial and still requires expert knowledge. This paper serves as a comprehensive guide to help understand the different interpretations of FI methods. Through an extensive review of FI methods and providing new proofs regarding their interpretation, we facilitate a thorough understanding of these methods and formulate concrete recommendations for scientific inference. We conclude by discussing options for FI uncertainty estimation and point to directions for future research aiming at full statistical inference from black-box ML models.","sentences":["While machine learning (ML) models are increasingly used due to their high predictive power, their use in understanding the data-generating process (DGP) is limited.","Understanding the DGP requires insights into feature-target associations, which many ML models cannot directly provide, due to their opaque internal mechanisms.","Feature importance (FI) methods provide useful insights into the DGP under certain conditions.","Since the results of different FI methods have different interpretations, selecting the correct FI method for a concrete use case is crucial and still requires expert knowledge.","This paper serves as a comprehensive guide to help understand the different interpretations of FI methods.","Through an extensive review of FI methods and providing new proofs regarding their interpretation, we facilitate a thorough understanding of these methods and formulate concrete recommendations for scientific inference.","We conclude by discussing options for FI uncertainty estimation and point to directions for future research aiming at full statistical inference from black-box ML models."],"url":"http://arxiv.org/abs/2404.12862v1","category":"stat.ML"}
{"created":"2024-04-19 13:01:30","title":"Foundation Model assisted Weakly Supervised LiDAR Semantic Segmentation","abstract":"Current point cloud semantic segmentation has achieved great advances when given sufficient labels. However, the dense annotation of LiDAR point clouds remains prohibitively expensive and time-consuming, unable to keep up with the continuously growing volume of data. In this paper, we propose annotating images with scattered points, followed by utilizing SAM (a Foundation model) to generate semantic segmentation labels for the images. Finally, by mapping the segmentation labels of the images to the LiDAR space using the intrinsic and extrinsic parameters of the camera and LiDAR, we obtain labels for point cloud semantic segmentation, and release Scatter-KITTI and Scatter-nuScenes, which are the first works to utilize image segmentation-based SAM for weakly supervised point cloud semantic segmentation. Furthermore, to mitigate the influence of erroneous pseudo labels obtained from sparse annotations on point cloud features, we propose a multi-modal weakly supervised network for LiDAR semantic segmentation, called MM-ScatterNet. This network combines features from both point cloud and image modalities, enhancing the representation learning of point clouds by introducing consistency constraints between multi-modal features and point cloud features. On the SemanticKITTI dataset, we achieve 66\\% of fully supervised performance using only 0.02% of annotated data, and on the NuScenes dataset, we achieve 95% of fully supervised performance using only 0.1% labeled points.","sentences":["Current point cloud semantic segmentation has achieved great advances when given sufficient labels.","However, the dense annotation of LiDAR point clouds remains prohibitively expensive and time-consuming, unable to keep up with the continuously growing volume of data.","In this paper, we propose annotating images with scattered points, followed by utilizing SAM (a Foundation model) to generate semantic segmentation labels for the images.","Finally, by mapping the segmentation labels of the images to the LiDAR space using the intrinsic and extrinsic parameters of the camera and LiDAR, we obtain labels for point cloud semantic segmentation, and release Scatter-KITTI and Scatter-nuScenes, which are the first works to utilize image segmentation-based SAM for weakly supervised point cloud semantic segmentation.","Furthermore, to mitigate the influence of erroneous pseudo labels obtained from sparse annotations on point cloud features, we propose a multi-modal weakly supervised network for LiDAR semantic segmentation, called MM-ScatterNet.","This network combines features from both point cloud and image modalities, enhancing the representation learning of point clouds by introducing consistency constraints between multi-modal features and point cloud features.","On the SemanticKITTI dataset, we achieve 66\\% of fully supervised performance using only 0.02% of annotated data, and on the NuScenes dataset, we achieve 95% of fully supervised performance using only 0.1% labeled points."],"url":"http://arxiv.org/abs/2404.12861v1","category":"cs.CV"}
{"created":"2024-04-19 12:56:45","title":"Circular Photocurrents in Centrosymmetric Semiconductors with Hidden Spin Polarization","abstract":"Centrosymmetric materials with site inversion asymmetries possess hidden spin polarization, which remains challenging to be converted into spin currents because the global inversion symmetry is still conserved. This study demonstrates the spin-polarized DC circular photocurrents (CPC) in centrosymmetric transition metal dichalcogenides (TMDCs) at normal incidence without applying electric bias. The global inversion symmetry is broken by using a spatially-varying circularly polarized light beam, which could generate spin gradient owing to the hidden spin polarization. The dependences of CPC on electrode configuration, illumination position, and beam spot size indicate an emergence of circulating electric current under spatially inhomogeneous light, which is associated with the spin-to-charge conversion through the inverse spin Hall effect (ISHE). The CPC is subsequently utilized to probe the spin polarization and ISHE under different excitation wavelengths and temperatures. The results of this study demonstrate the feasibility of using centrosymmetric materials with hidden spin polarization and spin-orbit coupling (SOC) for spintronic device applications.","sentences":["Centrosymmetric materials with site inversion asymmetries possess hidden spin polarization, which remains challenging to be converted into spin currents because the global inversion symmetry is still conserved.","This study demonstrates the spin-polarized DC circular photocurrents (CPC) in centrosymmetric transition metal dichalcogenides (TMDCs) at normal incidence without applying electric bias.","The global inversion symmetry is broken by using a spatially-varying circularly polarized light beam, which could generate spin gradient owing to the hidden spin polarization.","The dependences of CPC on electrode configuration, illumination position, and beam spot size indicate an emergence of circulating electric current under spatially inhomogeneous light, which is associated with the spin-to-charge conversion through the inverse spin Hall effect (ISHE).","The CPC is subsequently utilized to probe the spin polarization and ISHE under different excitation wavelengths and temperatures.","The results of this study demonstrate the feasibility of using centrosymmetric materials with hidden spin polarization and spin-orbit coupling (SOC) for spintronic device applications."],"url":"http://arxiv.org/abs/2404.12859v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-19 12:50:43","title":"Language-Driven Active Learning for Diverse Open-Set 3D Object Detection","abstract":"Object detection is crucial for ensuring safe autonomous driving. However, data-driven approaches face challenges when encountering minority or novel objects in the 3D driving scene. In this paper, we propose VisLED, a language-driven active learning framework for diverse open-set 3D Object Detection. Our method leverages active learning techniques to query diverse and informative data samples from an unlabeled pool, enhancing the model's ability to detect underrepresented or novel objects. Specifically, we introduce the Vision-Language Embedding Diversity Querying (VisLED-Querying) algorithm, which operates in both open-world exploring and closed-world mining settings. In open-world exploring, VisLED-Querying selects data points most novel relative to existing data, while in closed-world mining, it mines new instances of known classes. We evaluate our approach on the nuScenes dataset and demonstrate its effectiveness compared to random sampling and entropy-querying methods. Our results show that VisLED-Querying consistently outperforms random sampling and offers competitive performance compared to entropy-querying despite the latter's model-optimality, highlighting the potential of VisLED for improving object detection in autonomous driving scenarios.","sentences":["Object detection is crucial for ensuring safe autonomous driving.","However, data-driven approaches face challenges when encountering minority or novel objects in the 3D driving scene.","In this paper, we propose VisLED, a language-driven active learning framework for diverse open-set 3D Object Detection.","Our method leverages active learning techniques to query diverse and informative data samples from an unlabeled pool, enhancing the model's ability to detect underrepresented or novel objects.","Specifically, we introduce the Vision-Language Embedding Diversity Querying (VisLED-Querying) algorithm, which operates in both open-world exploring and closed-world mining settings.","In open-world exploring, VisLED-Querying selects data points most novel relative to existing data, while in closed-world mining, it mines new instances of known classes.","We evaluate our approach on the nuScenes dataset and demonstrate its effectiveness compared to random sampling and entropy-querying methods.","Our results show that VisLED-Querying consistently outperforms random sampling and offers competitive performance compared to entropy-querying despite the latter's model-optimality, highlighting the potential of VisLED for improving object detection in autonomous driving scenarios."],"url":"http://arxiv.org/abs/2404.12856v1","category":"cs.CV"}
{"created":"2024-04-19 12:42:31","title":"LSP Framework: A Compensatory Model for Defeating Trigger Reverse Engineering via Label Smoothing Poisoning","abstract":"Deep neural networks are vulnerable to backdoor attacks. Among the existing backdoor defense methods, trigger reverse engineering based approaches, which reconstruct the backdoor triggers via optimizations, are the most versatile and effective ones compared to other types of methods. In this paper, we summarize and construct a generic paradigm for the typical trigger reverse engineering process. Based on this paradigm, we propose a new perspective to defeat trigger reverse engineering by manipulating the classification confidence of backdoor samples. To determine the specific modifications of classification confidence, we propose a compensatory model to compute the lower bound of the modification. With proper modifications, the backdoor attack can easily bypass the trigger reverse engineering based methods. To achieve this objective, we propose a Label Smoothing Poisoning (LSP) framework, which leverages label smoothing to specifically manipulate the classification confidences of backdoor samples. Extensive experiments demonstrate that the proposed work can defeat the state-of-the-art trigger reverse engineering based methods, and possess good compatibility with a variety of existing backdoor attacks.","sentences":["Deep neural networks are vulnerable to backdoor attacks.","Among the existing backdoor defense methods, trigger reverse engineering based approaches, which reconstruct the backdoor triggers via optimizations, are the most versatile and effective ones compared to other types of methods.","In this paper, we summarize and construct a generic paradigm for the typical trigger reverse engineering process.","Based on this paradigm, we propose a new perspective to defeat trigger reverse engineering by manipulating the classification confidence of backdoor samples.","To determine the specific modifications of classification confidence, we propose a compensatory model to compute the lower bound of the modification.","With proper modifications, the backdoor attack can easily bypass the trigger reverse engineering based methods.","To achieve this objective, we propose a Label Smoothing Poisoning (LSP) framework, which leverages label smoothing to specifically manipulate the classification confidences of backdoor samples.","Extensive experiments demonstrate that the proposed work can defeat the state-of-the-art trigger reverse engineering based methods, and possess good compatibility with a variety of existing backdoor attacks."],"url":"http://arxiv.org/abs/2404.12852v1","category":"cs.CR"}
{"created":"2024-04-19 12:39:26","title":"A Semi-orthogonal Sequence in the Derived Category of the Hilbert Scheme of Three Points","abstract":"For a smooth projective variety $X$ of dimension $d \\geq 5$ over an algebraically closed field $k$ of characteristic zero, it is shown in this paper that the bounded derived category of the Hilbert scheme of three points $X^{[3]}$ admits a semi-orthogonal sequence of length $\\binom{d-3}{2}$. Each subcategory in this sequence is equivalent to the derived category of $X$ and realized as the image of a Fourier-Mukai transform along a Grassmannian bundle $\\mathbb{G}$ over $X$ parametrizing planar subschemes in $X^{[3]}$. The main ingredient in the proof is the computation of the normal bundle of $\\mathbb{G}$ in $X^{[3]}$. An analogous result for generalized Kummer varieties is deduced at the end.","sentences":["For a smooth projective variety $X$ of dimension $d \\geq 5$ over an algebraically closed field $k$ of characteristic zero, it is shown in this paper that the bounded derived category of the Hilbert scheme of three points $X^{[3]}$ admits a semi-orthogonal sequence of length $\\binom{d-3}{2}$. Each subcategory in this sequence is equivalent to the derived category of $X$ and realized as the image of a Fourier-Mukai transform along a Grassmannian bundle $\\mathbb{G}$ over $X$ parametrizing planar subschemes in $X^{[3]}$. The main ingredient in the proof is the computation of the normal bundle of $\\mathbb{G}$ in $X^{[3]}$. An analogous result for generalized Kummer varieties is deduced at the end."],"url":"http://arxiv.org/abs/2404.12851v1","category":"math.AG"}
{"created":"2024-04-19 12:39:11","title":"CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and Feature Balance","abstract":"Federated Learning (FL) as a promising distributed machine learning paradigm has been widely adopted in Artificial Intelligence of Things (AIoT) applications. However, the efficiency and inference capability of FL is seriously limited due to the presence of stragglers and data imbalance across massive AIoT devices, respectively. To address the above challenges, we present a novel asynchronous FL approach named CaBaFL, which includes a hierarchical Cache-based aggregation mechanism and a feature Balance-guided device selection strategy. CaBaFL maintains multiple intermediate models simultaneously for local training. The hierarchical cache-based aggregation mechanism enables each intermediate model to be trained on multiple devices to align the training time and mitigate the straggler issue. In specific, each intermediate model is stored in a low-level cache for local training and when it is trained by sufficient local devices, it will be stored in a high-level cache for aggregation. To address the problem of imbalanced data, the feature balance-guided device selection strategy in CaBaFL adopts the activation distribution as a metric, which enables each intermediate model to be trained across devices with totally balanced data distributions before aggregation. Experimental results show that compared with the state-of-the-art FL methods, CaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy improvements.","sentences":["Federated Learning (FL) as a promising distributed machine learning paradigm has been widely adopted in Artificial Intelligence of Things (AIoT) applications.","However, the efficiency and inference capability of FL is seriously limited due to the presence of stragglers and data imbalance across massive AIoT devices, respectively.","To address the above challenges, we present a novel asynchronous FL approach named CaBaFL, which includes a hierarchical Cache-based aggregation mechanism and a feature Balance-guided device selection strategy.","CaBaFL maintains multiple intermediate models simultaneously for local training.","The hierarchical cache-based aggregation mechanism enables each intermediate model to be trained on multiple devices to align the training time and mitigate the straggler issue.","In specific, each intermediate model is stored in a low-level cache for local training and when it is trained by sufficient local devices, it will be stored in a high-level cache for aggregation.","To address the problem of imbalanced data, the feature balance-guided device selection strategy in CaBaFL adopts the activation distribution as a metric, which enables each intermediate model to be trained across devices with totally balanced data distributions before aggregation.","Experimental results show that compared with the state-of-the-art FL methods, CaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy improvements."],"url":"http://arxiv.org/abs/2404.12850v1","category":"cs.LG"}
{"created":"2024-04-19 12:31:46","title":"Boundary regularity for a general nonlinear parabolic equation in non-divergence form","abstract":"We characterize regular boundary points in terms of a barrier family for a general form of a parabolic equation that generalizes both the standard parabolic $p$-Laplace equation and the normalized version arising from stochastic game theory. Using this result we prove geometric conditions that ensure regularity by constructing suitable barrier families. We also prove that when $q<2$, a single barrier does not suffice to guarantee regularity.","sentences":["We characterize regular boundary points in terms of a barrier family for a general form of a parabolic equation that generalizes both the standard parabolic $p$-Laplace equation and the normalized version arising from stochastic game theory.","Using this result we prove geometric conditions that ensure regularity by constructing suitable barrier families.","We also prove that when $q<2$, a single barrier does not suffice to guarantee regularity."],"url":"http://arxiv.org/abs/2404.12848v1","category":"math.AP"}
{"created":"2024-04-19 12:23:57","title":"Towards Logically Consistent Language Models via Probabilistic Reasoning","abstract":"Large language models (LLMs) are a promising venue for natural language understanding and generation tasks. However, current LLMs are far from reliable: they are prone to generate non-factual information and, more crucially, to contradict themselves when prompted to reason about beliefs of the world. These problems are currently addressed with large scale fine-tuning or by delegating consistent reasoning to external tools. In this work, we strive for a middle ground and introduce a training objective based on principled probabilistic reasoning that teaches a LLM to be consistent with external knowledge in the form of a set of facts and rules. Fine-tuning with our loss on a limited set of facts enables our LLMs to be more logically consistent than previous baselines and allows them to extrapolate to unseen but semantically similar factual knowledge more systematically.","sentences":["Large language models (LLMs) are a promising venue for natural language understanding and generation tasks.","However, current LLMs are far from reliable: they are prone to generate non-factual information and, more crucially, to contradict themselves when prompted to reason about beliefs of the world.","These problems are currently addressed with large scale fine-tuning or by delegating consistent reasoning to external tools.","In this work, we strive for a middle ground and introduce a training objective based on principled probabilistic reasoning that teaches a LLM to be consistent with external knowledge in the form of a set of facts and rules.","Fine-tuning with our loss on a limited set of facts enables our LLMs to be more logically consistent than previous baselines and allows them to extrapolate to unseen but semantically similar factual knowledge more systematically."],"url":"http://arxiv.org/abs/2404.12843v1","category":"cs.LG"}
{"created":"2024-04-19 12:21:27","title":"Explainable Deepfake Video Detection using Convolutional Neural Network and CapsuleNet","abstract":"Deepfake technology, derived from deep learning, seamlessly inserts individuals into digital media, irrespective of their actual participation. Its foundation lies in machine learning and Artificial Intelligence (AI). Initially, deepfakes served research, industry, and entertainment. While the concept has existed for decades, recent advancements render deepfakes nearly indistinguishable from reality. Accessibility has soared, empowering even novices to create convincing deepfakes. However, this accessibility raises security concerns.The primary deepfake creation algorithm, GAN (Generative Adversarial Network), employs machine learning to craft realistic images or videos. Our objective is to utilize CNN (Convolutional Neural Network) and CapsuleNet with LSTM to differentiate between deepfake-generated frames and originals. Furthermore, we aim to elucidate our model's decision-making process through Explainable AI, fostering transparent human-AI relationships and offering practical examples for real-life scenarios.","sentences":["Deepfake technology, derived from deep learning, seamlessly inserts individuals into digital media, irrespective of their actual participation.","Its foundation lies in machine learning and Artificial Intelligence (AI).","Initially, deepfakes served research, industry, and entertainment.","While the concept has existed for decades, recent advancements render deepfakes nearly indistinguishable from reality.","Accessibility has soared, empowering even novices to create convincing deepfakes.","However, this accessibility raises security concerns.","The primary deepfake creation algorithm, GAN (Generative Adversarial Network), employs machine learning to craft realistic images or videos.","Our objective is to utilize CNN (Convolutional Neural Network) and CapsuleNet with LSTM to differentiate between deepfake-generated frames and originals.","Furthermore, we aim to elucidate our model's decision-making process through Explainable AI, fostering transparent human-AI relationships and offering practical examples for real-life scenarios."],"url":"http://arxiv.org/abs/2404.12841v1","category":"cs.CV"}
{"created":"2024-04-19 12:20:49","title":"ECOR: Explainable CLIP for Object Recognition","abstract":"Large Vision Language Models (VLMs), such as CLIP, have significantly contributed to various computer vision tasks, including object recognition and object detection. Their open vocabulary feature enhances their value. However, their black-box nature and lack of explainability in predictions make them less trustworthy in critical domains. Recently, some work has been done to force VLMs to provide reasonable rationales for object recognition, but this often comes at the expense of classification accuracy. In this paper, we first propose a mathematical definition of explainability in the object recognition task based on the joint probability distribution of categories and rationales, then leverage this definition to fine-tune CLIP in an explainable manner. Through evaluations of different datasets, our method demonstrates state-of-the-art performance in explainable classification. Notably, it excels in zero-shot settings, showcasing its adaptability. This advancement improves explainable object recognition, enhancing trust across diverse applications. The code will be made available online upon publication.","sentences":["Large Vision Language Models (VLMs), such as CLIP, have significantly contributed to various computer vision tasks, including object recognition and object detection.","Their open vocabulary feature enhances their value.","However, their black-box nature and lack of explainability in predictions make them less trustworthy in critical domains.","Recently, some work has been done to force VLMs to provide reasonable rationales for object recognition, but this often comes at the expense of classification accuracy.","In this paper, we first propose a mathematical definition of explainability in the object recognition task based on the joint probability distribution of categories and rationales, then leverage this definition to fine-tune CLIP in an explainable manner.","Through evaluations of different datasets, our method demonstrates state-of-the-art performance in explainable classification.","Notably, it excels in zero-shot settings, showcasing its adaptability.","This advancement improves explainable object recognition, enhancing trust across diverse applications.","The code will be made available online upon publication."],"url":"http://arxiv.org/abs/2404.12839v1","category":"cs.CV"}
{"created":"2024-04-19 12:09:49","title":"COIN: Counterfactual inpainting for weakly supervised semantic segmentation for medical images","abstract":"Deep learning is dramatically transforming the field of medical imaging and radiology, enabling the identification of pathologies in medical images, including computed tomography (CT) and X-ray scans. However, the performance of deep learning models, particularly in segmentation tasks, is often limited by the need for extensive annotated datasets. To address this challenge, the capabilities of weakly supervised semantic segmentation are explored through the lens of Explainable AI and the generation of counterfactual explanations. The scope of this research is development of a novel counterfactual inpainting approach (COIN) that flips the predicted classification label from abnormal to normal by using a generative model. For instance, if the classifier deems an input medical image X as abnormal, indicating the presence of a pathology, the generative model aims to inpaint the abnormal region, thus reversing the classifier's original prediction label. The approach enables us to produce precise segmentations for pathologies without depending on pre-existing segmentation masks. Crucially, image-level labels are utilized, which are substantially easier to acquire than creating detailed segmentation masks. The effectiveness of the method is demonstrated by segmenting synthetic targets and actual kidney tumors from CT images acquired from Tartu University Hospital in Estonia. The findings indicate that COIN greatly surpasses established attribution methods, such as RISE, ScoreCAM, and LayerCAM, as well as an alternative counterfactual explanation method introduced by Singla et al. This evidence suggests that COIN is a promising approach for semantic segmentation of tumors in CT images, and presents a step forward in making deep learning applications more accessible and effective in healthcare, where annotated data is scarce.","sentences":["Deep learning is dramatically transforming the field of medical imaging and radiology, enabling the identification of pathologies in medical images, including computed tomography (CT) and X-ray scans.","However, the performance of deep learning models, particularly in segmentation tasks, is often limited by the need for extensive annotated datasets.","To address this challenge, the capabilities of weakly supervised semantic segmentation are explored through the lens of Explainable AI and the generation of counterfactual explanations.","The scope of this research is development of a novel counterfactual inpainting approach (COIN) that flips the predicted classification label from abnormal to normal by using a generative model.","For instance, if the classifier deems an input medical image X as abnormal, indicating the presence of a pathology, the generative model aims to inpaint the abnormal region, thus reversing the classifier's original prediction label.","The approach enables us to produce precise segmentations for pathologies without depending on pre-existing segmentation masks.","Crucially, image-level labels are utilized, which are substantially easier to acquire than creating detailed segmentation masks.","The effectiveness of the method is demonstrated by segmenting synthetic targets and actual kidney tumors from CT images acquired from Tartu University Hospital in Estonia.","The findings indicate that COIN greatly surpasses established attribution methods, such as RISE, ScoreCAM, and LayerCAM, as well as an alternative counterfactual explanation method introduced by Singla et al.","This evidence suggests that COIN is a promising approach for semantic segmentation of tumors in CT images, and presents a step forward in making deep learning applications more accessible and effective in healthcare, where annotated data is scarce."],"url":"http://arxiv.org/abs/2404.12832v1","category":"cs.CV"}
{"created":"2024-04-19 12:08:29","title":"On extremal points for some vectorial total variation seminorms","abstract":"We consider the set of extremal points of the generalized unit ball induced by gradient total variation seminorms for vector-valued functions on bounded Euclidean domains. These extremal points are central to the understanding of sparse solutions and sparse optimization algorithms for variational regularization problems posed among such functions. For not fully vectorial cases in which either the domain or the target are one dimensional, or the sum of the total variations of each component is used, we prove that these extremals are fully characterized as in the scalar-valued case, that is, they consist of piecewise constant functions with two regions. For definitions involving more involved matrix norms and in particular spectral norms, which are of interest in image processing, we produce families of examples to show that the resulting set of extremal points is larger and includes piecewise constant functions with more than two regions. We also consider the total deformation induced by the symmetrized gradient, for which minimization with linear constraints appears in problems of determination of limit loads in a number of continuum mechanical models involving plasticity, bringing relevance to the corresponding extremal points. For this case, we show piecewise infinitesimally rigid functions with two pieces to be extremal under mild assumptions. Finally, as an example of an extremal which is not piecewise constant, we prove that unit radial vector fields are extremal for the Frobenius total variation in the plane.","sentences":["We consider the set of extremal points of the generalized unit ball induced by gradient total variation seminorms for vector-valued functions on bounded Euclidean domains.","These extremal points are central to the understanding of sparse solutions and sparse optimization algorithms for variational regularization problems posed among such functions.","For not fully vectorial cases in which either the domain or the target are one dimensional, or the sum of the total variations of each component is used, we prove that these extremals are fully characterized as in the scalar-valued case, that is, they consist of piecewise constant functions with two regions.","For definitions involving more involved matrix norms and in particular spectral norms, which are of interest in image processing, we produce families of examples to show that the resulting set of extremal points is larger and includes piecewise constant functions with more than two regions.","We also consider the total deformation induced by the symmetrized gradient, for which minimization with linear constraints appears in problems of determination of limit loads in a number of continuum mechanical models involving plasticity, bringing relevance to the corresponding extremal points.","For this case, we show piecewise infinitesimally rigid functions with two pieces to be extremal under mild assumptions.","Finally, as an example of an extremal which is not piecewise constant, we prove that unit radial vector fields are extremal for the Frobenius total variation in the plane."],"url":"http://arxiv.org/abs/2404.12831v1","category":"math.FA"}
{"created":"2024-04-19 12:06:28","title":"LiMe: a Latin Corpus of Late Medieval Criminal Sentences","abstract":"The Latin language has received attention from the computational linguistics research community, which has built, over the years, several valuable resources, ranging from detailed annotated corpora to sophisticated tools for linguistic analysis. With the recent advent of large language models, researchers have also started developing models capable of generating vector representations of Latin texts. The performances of such models remain behind the ones for modern languages, given the disparity in available data. In this paper, we present the LiMe dataset, a corpus of 325 documents extracted from a series of medieval manuscripts called Libri sententiarum potestatis Mediolani, and thoroughly annotated by experts, in order to be employed for masked language model, as well as supervised natural language processing tasks.","sentences":["The Latin language has received attention from the computational linguistics research community, which has built, over the years, several valuable resources, ranging from detailed annotated corpora to sophisticated tools for linguistic analysis.","With the recent advent of large language models, researchers have also started developing models capable of generating vector representations of Latin texts.","The performances of such models remain behind the ones for modern languages, given the disparity in available data.","In this paper, we present the LiMe dataset, a corpus of 325 documents extracted from a series of medieval manuscripts called Libri sententiarum potestatis Mediolani, and thoroughly annotated by experts, in order to be employed for masked language model, as well as supervised natural language processing tasks."],"url":"http://arxiv.org/abs/2404.12829v1","category":"cs.CL"}
{"created":"2024-04-19 12:04:32","title":"CT-ADE: An Evaluation Benchmark for Adverse Drug Event Prediction from Clinical Trial Results","abstract":"Adverse drug events (ADEs) significantly impact clinical research and public health, contributing to failures in clinical trials and leading to increased healthcare costs. The accurate prediction and management of ADEs are crucial for improving the development of safer, more effective medications, and enhancing patient outcomes. To support this effort, we introduce CT-ADE, a novel dataset compiled to enhance the predictive modeling of ADEs. Encompassing over 12,000 instances extracted from clinical trial results, the CT-ADE dataset integrates drug, patient population, and contextual information for multilabel ADE classification tasks in monopharmacy treatments, providing a comprehensive resource for developing advanced predictive models. To mirror the complex nature of ADEs, annotations are standardized at the system organ class level of the Medical Dictionary for Regulatory Activities (MedDRA) ontology. Preliminary analyses using baseline models have demonstrated promising results, achieving 73.33% F1 score and 81.54% balanced accuracy, highlighting CT-ADE's potential to advance ADE prediction. CT-ADE provides an essential tool for researchers aiming to leverage the power of artificial intelligence and machine learning to enhance patient safety and minimize the impact of ADEs on pharmaceutical research and development. Researchers interested in using the CT-ADE dataset can find all necessary resources at https://github.com/xxxx/xxxx.","sentences":["Adverse drug events (ADEs) significantly impact clinical research and public health, contributing to failures in clinical trials and leading to increased healthcare costs.","The accurate prediction and management of ADEs are crucial for improving the development of safer, more effective medications, and enhancing patient outcomes.","To support this effort, we introduce CT-ADE, a novel dataset compiled to enhance the predictive modeling of ADEs.","Encompassing over 12,000 instances extracted from clinical trial results, the CT-ADE dataset integrates drug, patient population, and contextual information for multilabel ADE classification tasks in monopharmacy treatments, providing a comprehensive resource for developing advanced predictive models.","To mirror the complex nature of ADEs, annotations are standardized at the system organ class level of the Medical Dictionary for Regulatory Activities (MedDRA) ontology.","Preliminary analyses using baseline models have demonstrated promising results, achieving 73.33% F1 score and 81.54% balanced accuracy, highlighting CT-ADE's potential to advance ADE prediction.","CT-ADE provides an essential tool for researchers aiming to leverage the power of artificial intelligence and machine learning to enhance patient safety and minimize the impact of ADEs on pharmaceutical research and development.","Researchers interested in using the CT-ADE dataset can find all necessary resources at https://github.com/xxxx/xxxx."],"url":"http://arxiv.org/abs/2404.12827v1","category":"cs.CL"}
{"created":"2024-04-19 12:00:10","title":"MAexp: A Generic Platform for RL-based Multi-Agent Exploration","abstract":"The sim-to-real gap poses a significant challenge in RL-based multi-agent exploration due to scene quantization and action discretization. Existing platforms suffer from the inefficiency in sampling and the lack of diversity in Multi-Agent Reinforcement Learning (MARL) algorithms across different scenarios, restraining their widespread applications. To fill these gaps, we propose MAexp, a generic platform for multi-agent exploration that integrates a broad range of state-of-the-art MARL algorithms and representative scenarios. Moreover, we employ point clouds to represent our exploration scenarios, leading to high-fidelity environment mapping and a sampling speed approximately 40 times faster than existing platforms. Furthermore, equipped with an attention-based Multi-Agent Target Generator and a Single-Agent Motion Planner, MAexp can work with arbitrary numbers of agents and accommodate various types of robots. Extensive experiments are conducted to establish the first benchmark featuring several high-performance MARL algorithms across typical scenarios for robots with continuous actions, which highlights the distinct strengths of each algorithm in different scenarios.","sentences":["The sim-to-real gap poses a significant challenge in RL-based multi-agent exploration due to scene quantization and action discretization.","Existing platforms suffer from the inefficiency in sampling and the lack of diversity in Multi-Agent Reinforcement Learning (MARL) algorithms across different scenarios, restraining their widespread applications.","To fill these gaps, we propose MAexp, a generic platform for multi-agent exploration that integrates a broad range of state-of-the-art MARL algorithms and representative scenarios.","Moreover, we employ point clouds to represent our exploration scenarios, leading to high-fidelity environment mapping and a sampling speed approximately 40 times faster than existing platforms.","Furthermore, equipped with an attention-based Multi-Agent Target Generator and a Single-Agent Motion Planner, MAexp can work with arbitrary numbers of agents and accommodate various types of robots.","Extensive experiments are conducted to establish the first benchmark featuring several high-performance MARL algorithms across typical scenarios for robots with continuous actions, which highlights the distinct strengths of each algorithm in different scenarios."],"url":"http://arxiv.org/abs/2404.12824v1","category":"cs.RO"}
{"created":"2024-04-19 11:57:41","title":"Nodal auxiliary space preconditioners for mixed virtual element methods","abstract":"We propose nodal auxiliary space preconditioners for facet and edge virtual elements of lowest order by deriving discrete regular decompositions on polytopal grids and generalizing the Hiptmair-Xu preconditioner to the virtual element framework. The preconditioner consists of solving a sequence of elliptic problems on the nodal virtual element space, combined with appropriate smoother steps. Under assumed regularity of the mesh, the preconditioned system is proven to have bounded spectral condition number independent of the mesh size and this is verified by numerical experiments on a sequence of polygonal meshes. Moreover, we observe numerically that the preconditioner is robust on meshes containing elements with high aspect ratios.","sentences":["We propose nodal auxiliary space preconditioners for facet and edge virtual elements of lowest order by deriving discrete regular decompositions on polytopal grids and generalizing the Hiptmair-Xu preconditioner to the virtual element framework.","The preconditioner consists of solving a sequence of elliptic problems on the nodal virtual element space, combined with appropriate smoother steps.","Under assumed regularity of the mesh, the preconditioned system is proven to have bounded spectral condition number independent of the mesh size and this is verified by numerical experiments on a sequence of polygonal meshes.","Moreover, we observe numerically that the preconditioner is robust on meshes containing elements with high aspect ratios."],"url":"http://arxiv.org/abs/2404.12823v1","category":"math.NA"}
{"created":"2024-04-19 11:57:07","title":"Introducing spontaneous curvature to the Helfrich flow: Singularities and convergence","abstract":"While there are various results on the long-time behavior of the Willmore flow, the Helfrich flow with non-zero spontaneous curvature as its natural generalization is not yet well-understood. Past results for the gradient flow of a locally area- and volume-constrained Willmore flow indicate the existence of finite-time singularities which corresponds to the scaling-behavior of the underlying energy. However, for a non-vanishing spontaneous curvature, the scaling behavior is not quite as conclusive.   Indeed, in this article, we find that a negative spontaneous curvature corresponds to finite-time singularities of the locally constrained Helfrich flow if the initial surface is energetically close to a round sphere. Conversely however, in the case of a positive spontaneous curvature, we find a positive result in terms of the convergence behavior: The locally area-constrained Helfrich flow starting in a spherical immersion with suitably small Helfrich energy exists globally and converges to a Helfrich immersion after reparametrization. Moreover, this energetic smallness assumption is given by an explicit energy threshold depending on the spontaneous curvature and the local area constraint of the energy.","sentences":["While there are various results on the long-time behavior of the Willmore flow, the Helfrich flow with non-zero spontaneous curvature as its natural generalization is not yet well-understood.","Past results for the gradient flow of a locally area- and volume-constrained Willmore flow indicate the existence of finite-time singularities which corresponds to the scaling-behavior of the underlying energy.","However, for a non-vanishing spontaneous curvature, the scaling behavior is not quite as conclusive.   ","Indeed, in this article, we find that a negative spontaneous curvature corresponds to finite-time singularities of the locally constrained Helfrich flow if the initial surface is energetically close to a round sphere.","Conversely however, in the case of a positive spontaneous curvature, we find a positive result in terms of the convergence behavior: The locally area-constrained Helfrich flow starting in a spherical immersion with suitably small Helfrich energy exists globally and converges to a Helfrich immersion after reparametrization.","Moreover, this energetic smallness assumption is given by an explicit energy threshold depending on the spontaneous curvature and the local area constraint of the energy."],"url":"http://arxiv.org/abs/2404.12820v1","category":"math.AP"}
{"created":"2024-04-19 11:56:29","title":"Unveiling the Ambiguity in Neural Inverse Rendering: A Parameter Compensation Analysis","abstract":"Inverse rendering aims to reconstruct the scene properties of objects solely from multiview images. However, it is an ill-posed problem prone to producing ambiguous estimations deviating from physically accurate representations. In this paper, we utilize Neural Microfacet Fields (NMF), a state-of-the-art neural inverse rendering method to illustrate the inherent ambiguity. We propose an evaluation framework to assess the degree of compensation or interaction between the estimated scene properties, aiming to explore the mechanisms behind this ill-posed problem and potential mitigation strategies. Specifically, we introduce artificial perturbations to one scene property and examine how adjusting another property can compensate for these perturbations. To facilitate such experiments, we introduce a disentangled NMF where material properties are independent. The experimental findings underscore the intrinsic ambiguity present in neural inverse rendering and highlight the importance of providing additional guidance through geometry, material, and illumination priors.","sentences":["Inverse rendering aims to reconstruct the scene properties of objects solely from multiview images.","However, it is an ill-posed problem prone to producing ambiguous estimations deviating from physically accurate representations.","In this paper, we utilize Neural Microfacet Fields (NMF), a state-of-the-art neural inverse rendering method to illustrate the inherent ambiguity.","We propose an evaluation framework to assess the degree of compensation or interaction between the estimated scene properties, aiming to explore the mechanisms behind this ill-posed problem and potential mitigation strategies.","Specifically, we introduce artificial perturbations to one scene property and examine how adjusting another property can compensate for these perturbations.","To facilitate such experiments, we introduce a disentangled NMF where material properties are independent.","The experimental findings underscore the intrinsic ambiguity present in neural inverse rendering and highlight the importance of providing additional guidance through geometry, material, and illumination priors."],"url":"http://arxiv.org/abs/2404.12819v1","category":"cs.CV"}
{"created":"2024-04-19 11:51:13","title":"Making the invisible visible: Magnetic fields in accretion flows revealed by X-ray polarization","abstract":"Large scale, strong magnetic fields are often evoked in black hole accretion flows, for jet launching in the low/hard state and to circumvent the thermal instability in the high/soft state. Here we show how these ideas are strongly challenged by X-ray polarization measurements from IXPE. Quite general arguments show that equipartition fields in the accretion flow should be of order $10^{6-8}$ G. These produce substantial Faraday rotation and/or depolarization for photons escaping the flow in the 2-8 keV IXPE bandpass, which is not consistent with the observed data. While we stress that Faraday rotation should be calculated for each individual simulation (density, field geometry and emissivity), it seems most likely that there are no equipartition strength large scale ordered fields inside the photosphere of the X-ray emitting gas. Strong poloidal fields can still be present if they thread the black hole horizon rather than the X-ray emitting flow, so Blandford-Znajek jets are still possible, but an alternative solution is that the low/hard state jet is dominated by pairs so can be accelerated by lower fields. Fundamentally, polarization data from IXPE means that magnetic fields in black hole accretion flows are no longer invisible and unconstrained.","sentences":["Large scale, strong magnetic fields are often evoked in black hole accretion flows, for jet launching in the low/hard state and to circumvent the thermal instability in the high/soft state.","Here we show how these ideas are strongly challenged by X-ray polarization measurements from IXPE.","Quite general arguments show that equipartition fields in the accretion flow should be of order $10^{6-8}$ G. These produce substantial Faraday rotation and/or depolarization for photons escaping the flow in the 2-8 keV IXPE bandpass, which is not consistent with the observed data.","While we stress that Faraday rotation should be calculated for each individual simulation (density, field geometry and emissivity), it seems most likely that there are no equipartition strength large scale ordered fields inside the photosphere of the X-ray emitting gas.","Strong poloidal fields can still be present if they thread the black hole horizon rather than the X-ray emitting flow, so Blandford-Znajek jets are still possible, but an alternative solution is that the low/hard state jet is dominated by pairs so can be accelerated by lower fields.","Fundamentally, polarization data from IXPE means that magnetic fields in black hole accretion flows are no longer invisible and unconstrained."],"url":"http://arxiv.org/abs/2404.12815v1","category":"astro-ph.HE"}
{"created":"2024-04-19 11:49:01","title":"Generative Modelling with High-Order Langevin Dynamics","abstract":"Diffusion generative modelling (DGM) based on stochastic   differential equations (SDEs) with   score matching has achieved unprecedented results in data   generation.   In this paper, we propose a novel fast high-quality   generative modelling method   based on high-order   Langevin dynamics (HOLD) with score matching.   This motive is proved by third-order   Langevin dynamics. By augmenting the   previous SDEs, e.g.   variance exploding or variance preserving SDEs   for single-data variable processes, HOLD can simultaneously   model position, velocity, and   acceleration, thereby improving the quality   and speed of the data   generation at the same time.   HOLD is composed of one Ornstein-Uhlenbeck process   and two Hamiltonians,   which reduce the mixing time by two orders of magnitude.   Empirical experiments for unconditional image generation on the   public data set CIFAR-10 and CelebA-HQ show that the effect is significant in   both Frechet inception distance (FID) and negative log-likelihood,   and achieves the   state-of-the-art FID of 1.85 on CIFAR-10.","sentences":["Diffusion generative modelling (DGM) based on stochastic   differential equations (SDEs) with   score matching has achieved unprecedented results in data   generation.   ","In this paper, we propose a novel fast high-quality   generative modelling method   based on high-order   Langevin dynamics (HOLD) with score matching.   ","This motive is proved by third-order   Langevin dynamics.","By augmenting the   previous SDEs, e.g.   variance exploding or variance preserving SDEs   for single-data variable processes, HOLD can simultaneously   model position, velocity, and   acceleration, thereby improving the quality   and speed of the data   generation at the same time.   ","HOLD is composed of one Ornstein-Uhlenbeck process   and two Hamiltonians,   which reduce the mixing time by two orders of magnitude.   ","Empirical experiments for unconditional image generation on the   public data set CIFAR-10 and CelebA-HQ show that the effect is significant in   both Frechet inception distance (FID) and negative log-likelihood,   and achieves the   state-of-the-art FID of 1.85 on CIFAR-10."],"url":"http://arxiv.org/abs/2404.12814v1","category":"cs.LG"}
{"created":"2024-04-19 11:48:54","title":"Open Datasets for AI-Enabled Radio Resource Control in Non-Terrestrial Networks","abstract":"By effectively implementing the strategies for resource allocation, the capabilities, and reliability of non-terrestrial networks (NTN) can be enhanced. This leads to enhance spectrum utilization performance while minimizing the unmet system capacity, meeting quality of service (QoS) requirements and overall system optimization. In turn, a wide range of applications and services in various domains can be supported. However, allocating resources in a multi-constellation system with heterogeneous satellite links and highly dynamic user traffic demand pose challenges in ensuring sufficient and fair resource distribution. To mitigate these complexities and minimize the overhead, there is a growing shift towards utilizing artificial intelligence (AI) for its ability to handle such problems effectively. This calls for the development of an intelligent decision-making controller using AI to efficiently manage resources in this complex environment. In this context, real-world open datasets play a pivotal role in the development of AI models addressing radio control optimization problems. As a matter of fact, acquiring suitable datasets can be arduous. Therefore, this paper identifies pertinent real-world open datasets representing realistic traffic pattern, network performances and demand for fixed and dynamic user terminals, enabling a variety of uses cases. The aim of gathering and publishing the information of these datasets are to inspire and assist the research community in crafting the advance resource management solutions. In a nutshell, this paper establishes a solid foundation of commercially accessible data, with the potential to set benchmarks and accelerate the resolution of resource allocation optimization challenges.","sentences":["By effectively implementing the strategies for resource allocation, the capabilities, and reliability of non-terrestrial networks (NTN) can be enhanced.","This leads to enhance spectrum utilization performance while minimizing the unmet system capacity, meeting quality of service (QoS) requirements and overall system optimization.","In turn, a wide range of applications and services in various domains can be supported.","However, allocating resources in a multi-constellation system with heterogeneous satellite links and highly dynamic user traffic demand pose challenges in ensuring sufficient and fair resource distribution.","To mitigate these complexities and minimize the overhead, there is a growing shift towards utilizing artificial intelligence (AI) for its ability to handle such problems effectively.","This calls for the development of an intelligent decision-making controller using AI to efficiently manage resources in this complex environment.","In this context, real-world open datasets play a pivotal role in the development of AI models addressing radio control optimization problems.","As a matter of fact, acquiring suitable datasets can be arduous.","Therefore, this paper identifies pertinent real-world open datasets representing realistic traffic pattern, network performances and demand for fixed and dynamic user terminals, enabling a variety of uses cases.","The aim of gathering and publishing the information of these datasets are to inspire and assist the research community in crafting the advance resource management solutions.","In a nutshell, this paper establishes a solid foundation of commercially accessible data, with the potential to set benchmarks and accelerate the resolution of resource allocation optimization challenges."],"url":"http://arxiv.org/abs/2404.12813v1","category":"cs.NI"}
{"created":"2024-04-19 11:48:29","title":"Black Hole shadows of $\u03b1'$-corrected black holes","abstract":"In this paper we study the qualitative features induces by corrections to GR coming from String Theory, on the shadows of rotating black holes. We deal with the slowly rotating black hole solutions up to order $\\mathcal{O}(a^3)$, to first order in $\\alpha'$, including also the dilaton. We provide a detailed characterization of the geometry, as well as the ISCO and photon ring, and then we proceed to obtain the black hole images within the relativistic thin-disk model. We characterize the images by computing the diameter, displacement and asymmetry. A comparison with the Kerr case, indicates that all these quantities grow due to the $\\alpha'$ correction, and that the departure from GR for different observable is enhanced depending on the angle of view, namely for the diameter the maximum departure is obtained when the system is face-on, while for the displacement and asymmetry the departure from GR is maximized for edge-on point of view.","sentences":["In this paper we study the qualitative features induces by corrections to GR coming from String Theory, on the shadows of rotating black holes.","We deal with the slowly rotating black hole solutions up to order $\\mathcal{O}(a^3)$, to first order in $\\alpha'$, including also the dilaton.","We provide a detailed characterization of the geometry, as well as the ISCO and photon ring, and then we proceed to obtain the black hole images within the relativistic thin-disk model.","We characterize the images by computing the diameter, displacement and asymmetry.","A comparison with the Kerr case, indicates that all these quantities grow due to the $\\alpha'$ correction, and that the departure from GR for different observable is enhanced depending on the angle of view, namely for the diameter the maximum departure is obtained when the system is face-on, while for the displacement and asymmetry the departure from GR is maximized for edge-on point of view."],"url":"http://arxiv.org/abs/2404.12811v1","category":"gr-qc"}
{"created":"2024-04-19 11:47:17","title":"Enhancing Counterfactual Explanation Search with Diffusion Distance and Directional Coherence","abstract":"A pressing issue in the adoption of AI models is the increasing demand for more human-centric explanations of their predictions. To advance towards more human-centric explanations, understanding how humans produce and select explanations has been beneficial. In this work, inspired by insights of human cognition we propose and test the incorporation of two novel biases to enhance the search for effective counterfactual explanations. Central to our methodology is the application of diffusion distance, which emphasizes data connectivity and actionability in the search for feasible counterfactual explanations. In particular, diffusion distance effectively weights more those points that are more interconnected by numerous short-length paths. This approach brings closely connected points nearer to each other, identifying a feasible path between them. We also introduce a directional coherence term that allows the expression of a preference for the alignment between the joint and marginal directional changes in feature space to reach a counterfactual. This term enables the generation of counterfactual explanations that align with a set of marginal predictions based on expectations of how the outcome of the model varies by changing one feature at a time. We evaluate our method, named Coherent Directional Counterfactual Explainer (CoDiCE), and the impact of the two novel biases against existing methods such as DiCE, FACE, Prototypes, and Growing Spheres. Through a series of ablation experiments on both synthetic and real datasets with continuous and mixed-type features, we demonstrate the effectiveness of our method.","sentences":["A pressing issue in the adoption of AI models is the increasing demand for more human-centric explanations of their predictions.","To advance towards more human-centric explanations, understanding how humans produce and select explanations has been beneficial.","In this work, inspired by insights of human cognition we propose and test the incorporation of two novel biases to enhance the search for effective counterfactual explanations.","Central to our methodology is the application of diffusion distance, which emphasizes data connectivity and actionability in the search for feasible counterfactual explanations.","In particular, diffusion distance effectively weights more those points that are more interconnected by numerous short-length paths.","This approach brings closely connected points nearer to each other, identifying a feasible path between them.","We also introduce a directional coherence term that allows the expression of a preference for the alignment between the joint and marginal directional changes in feature space to reach a counterfactual.","This term enables the generation of counterfactual explanations that align with a set of marginal predictions based on expectations of how the outcome of the model varies by changing one feature at a time.","We evaluate our method, named Coherent Directional Counterfactual Explainer (CoDiCE), and the impact of the two novel biases against existing methods such as DiCE, FACE, Prototypes, and Growing Spheres.","Through a series of ablation experiments on both synthetic and real datasets with continuous and mixed-type features, we demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.12810v1","category":"cs.LG"}
{"created":"2024-04-19 11:41:51","title":"Systematic Evaluation of Forensic Data Acquisition using Smartphone Local Backup","abstract":"Due to the increasing security standards of modern smartphones, forensic data acquisition from such devices is a growing challenge. One rather generic way to access data on smartphones in practice is to use the local backup mechanism offered by the mobile operating systems. We study the suitability of such mechanisms for forensic data acquisition by performing a thorough evaluation of iOS's and Android's local backup mechanisms on two mobile devices. Based on a systematic and generic evaluation procedure comparing the contents of local backup to the original storage, we show that in our exemplary practical evaluations, in most cases (but not all) local backup actually yields a correct copy of the original data from storage. Our study also highlights corner cases, such as database files with pending changes, that need to be considered when assessing the integrity and authenticity of evidence acquired through local backup.","sentences":["Due to the increasing security standards of modern smartphones, forensic data acquisition from such devices is a growing challenge.","One rather generic way to access data on smartphones in practice is to use the local backup mechanism offered by the mobile operating systems.","We study the suitability of such mechanisms for forensic data acquisition by performing a thorough evaluation of iOS's and Android's local backup mechanisms on two mobile devices.","Based on a systematic and generic evaluation procedure comparing the contents of local backup to the original storage, we show that in our exemplary practical evaluations, in most cases (but not all) local backup actually yields a correct copy of the original data from storage.","Our study also highlights corner cases, such as database files with pending changes, that need to be considered when assessing the integrity and authenticity of evidence acquired through local backup."],"url":"http://arxiv.org/abs/2404.12808v1","category":"cs.CR"}
{"created":"2024-04-19 11:39:23","title":"Bloch equations in Terahertz magnetic-resonance ellipsometry","abstract":"A generalized approach derived from Blochs equation of motion of nuclear magnetic moments is presented to model the frequency, magnetic field, spin density, and temperature dependencies in the electromagnetic permeability tensor for materials with magnetic resonances. The resulting tensor model predicts characteristic polarization signatures which can be observed, for example, in fully polarization-resolved Mueller matrix element spectra measured across magnetic resonances as a function of frequency, magnetic field, magnetic moment density, and temperature. When augmented with thermodynamic considerations and suitable Hamiltonian description of the magnetic eigenvalue spectrum, important parameters such as zero-frequency magnetization, spectral amplitude distribution, relaxation time constants, and geometrical orientation parameters of the magnetic moment density can be obtained from comparing the generalized model approach to experimental data. We demonstrate our approach by comparing model calculations with full Mueller matrix element spectra measured at oblique angle of incidence in the terahertz spectral range, across electron spin resonance quintuplet transitions observed in wurtzite-structure GaN doped with iron. Measurements were performed by ellipsometry, using a superconducting cryostat magnet at magnetic fields of 7.23 T and at temperatures of 20 K and 30 K. We detail the occurrence of linear and circular birefringence and dichroism associated with each of the zero-field split spin transitions in the S = 5/2 defect system. We derive the spectral dependence of the magnetic susceptibility function and obtain the temperature and magnetic field dependence of the spin Hamiltonian. Our model correctly predicts the complexity of the polarization signatures observed in the 15 independent elements of the normalized Mueller matrix for both positive and negative magnetic fields.","sentences":["A generalized approach derived from Blochs equation of motion of nuclear magnetic moments is presented to model the frequency, magnetic field, spin density, and temperature dependencies in the electromagnetic permeability tensor for materials with magnetic resonances.","The resulting tensor model predicts characteristic polarization signatures which can be observed, for example, in fully polarization-resolved Mueller matrix element spectra measured across magnetic resonances as a function of frequency, magnetic field, magnetic moment density, and temperature.","When augmented with thermodynamic considerations and suitable Hamiltonian description of the magnetic eigenvalue spectrum, important parameters such as zero-frequency magnetization, spectral amplitude distribution, relaxation time constants, and geometrical orientation parameters of the magnetic moment density can be obtained from comparing the generalized model approach to experimental data.","We demonstrate our approach by comparing model calculations with full Mueller matrix element spectra measured at oblique angle of incidence in the terahertz spectral range, across electron spin resonance quintuplet transitions observed in wurtzite-structure GaN doped with iron.","Measurements were performed by ellipsometry, using a superconducting cryostat magnet at magnetic fields of 7.23 T and at temperatures of 20 K and 30 K. We detail the occurrence of linear and circular birefringence and dichroism associated with each of the zero-field split spin transitions in the S = 5/2 defect system.","We derive the spectral dependence of the magnetic susceptibility function and obtain the temperature and magnetic field dependence of the spin Hamiltonian.","Our model correctly predicts the complexity of the polarization signatures observed in the 15 independent elements of the normalized Mueller matrix for both positive and negative magnetic fields."],"url":"http://arxiv.org/abs/2404.12805v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-19 11:38:08","title":"TextSquare: Scaling up Text-Centric Visual Instruction Tuning","abstract":"Text-centric visual question answering (VQA) has made great strides with the development of Multimodal Large Language Models (MLLMs), yet open-source models still fall short of leading models like GPT4V and Gemini, partly due to a lack of extensive, high-quality instruction tuning data. To this end, we introduce a new approach for creating a massive, high-quality instruction-tuning dataset, Square-10M, which is generated using closed-source MLLMs. The data construction process, termed Square, consists of four steps: Self-Questioning, Answering, Reasoning, and Evaluation. Our experiments with Square-10M led to three key findings: 1) Our model, TextSquare, considerably surpasses open-source previous state-of-the-art Text-centric MLLMs and sets a new standard on OCRBench(62.2%). It even outperforms top-tier models like GPT4V and Gemini in 6 of 10 text-centric benchmarks. 2) Additionally, we demonstrate the critical role of VQA reasoning data in offering comprehensive contextual insights for specific questions. This not only improves accuracy but also significantly mitigates hallucinations. Specifically, TextSquare scores an average of 75.1% across four general VQA and hallucination evaluation datasets, outperforming previous state-of-the-art models. 3) Notably, the phenomenon observed in scaling text-centric VQA datasets reveals a vivid pattern: the exponential increase of instruction tuning data volume is directly proportional to the improvement in model performance, thereby validating the necessity of the dataset scale and the high quality of Square-10M.","sentences":["Text-centric visual question answering (VQA) has made great strides with the development of Multimodal Large Language Models (MLLMs), yet open-source models still fall short of leading models like GPT4V and Gemini, partly due to a lack of extensive, high-quality instruction tuning data.","To this end, we introduce a new approach for creating a massive, high-quality instruction-tuning dataset, Square-10M, which is generated using closed-source MLLMs.","The data construction process, termed Square, consists of four steps: Self-Questioning, Answering, Reasoning, and Evaluation.","Our experiments with Square-10M led to three key findings: 1) Our model, TextSquare, considerably surpasses open-source previous state-of-the-art Text-centric MLLMs and sets a new standard on OCRBench(62.2%).","It even outperforms top-tier models like GPT4V and Gemini in 6 of 10 text-centric benchmarks.","2) Additionally, we demonstrate the critical role of VQA reasoning data in offering comprehensive contextual insights for specific questions.","This not only improves accuracy but also significantly mitigates hallucinations.","Specifically, TextSquare scores an average of 75.1% across four general VQA and hallucination evaluation datasets, outperforming previous state-of-the-art models.","3) Notably, the phenomenon observed in scaling text-centric VQA datasets reveals a vivid pattern: the exponential increase of instruction tuning data volume is directly proportional to the improvement in model performance, thereby validating the necessity of the dataset scale and the high quality of Square-10M."],"url":"http://arxiv.org/abs/2404.12803v1","category":"cs.CV"}
{"created":"2024-04-19 11:37:51","title":"Enhancing Interval Type-2 Fuzzy Logic Systems: Learning for Precision and Prediction Intervals","abstract":"In this paper, we tackle the task of generating Prediction Intervals (PIs) in high-risk scenarios by proposing enhancements for learning Interval Type-2 (IT2) Fuzzy Logic Systems (FLSs) to address their learning challenges. In this context, we first provide extra design flexibility to the Karnik-Mendel (KM) and Nie-Tan (NT) center of sets calculation methods to increase their flexibility for generating PIs. These enhancements increase the flexibility of KM in the defuzzification stage while the NT in the fuzzification stage. To address the large-scale learning challenge, we transform the IT2-FLS's constraint learning problem into an unconstrained form via parameterization tricks, enabling the direct application of deep learning optimizers. To address the curse of dimensionality issue, we expand the High-Dimensional Takagi-Sugeno-Kang (HTSK) method proposed for type-1 FLS to IT2-FLSs, resulting in the HTSK2 approach. Additionally, we introduce a framework to learn the enhanced IT2-FLS with a dual focus, aiming for high precision and PI generation. Through exhaustive statistical results, we reveal that HTSK2 effectively addresses the dimensionality challenge, while the enhanced KM and NT methods improved learning and enhanced uncertainty quantification performances of IT2-FLSs.","sentences":["In this paper, we tackle the task of generating Prediction Intervals (PIs) in high-risk scenarios by proposing enhancements for learning Interval Type-2 (IT2) Fuzzy Logic Systems (FLSs) to address their learning challenges.","In this context, we first provide extra design flexibility to the Karnik-Mendel (KM) and Nie-Tan (NT) center of sets calculation methods to increase their flexibility for generating PIs.","These enhancements increase the flexibility of KM in the defuzzification stage while the NT in the fuzzification stage.","To address the large-scale learning challenge, we transform the IT2-FLS's constraint learning problem into an unconstrained form via parameterization tricks, enabling the direct application of deep learning optimizers.","To address the curse of dimensionality issue, we expand the High-Dimensional Takagi-Sugeno-Kang (HTSK) method proposed for type-1 FLS to IT2-FLSs, resulting in the HTSK2 approach.","Additionally, we introduce a framework to learn the enhanced IT2-FLS with a dual focus, aiming for high precision and PI generation.","Through exhaustive statistical results, we reveal that HTSK2 effectively addresses the dimensionality challenge, while the enhanced KM and NT methods improved learning and enhanced uncertainty quantification performances of IT2-FLSs."],"url":"http://arxiv.org/abs/2404.12802v1","category":"cs.LG"}
{"created":"2024-04-19 11:29:10","title":"Zadeh's Type-2 Fuzzy Logic Systems: Precision and High-Quality Prediction Intervals","abstract":"General Type-2 (GT2) Fuzzy Logic Systems (FLSs) are perfect candidates to quantify uncertainty, which is crucial for informed decisions in high-risk tasks, as they are powerful tools in representing uncertainty. In this paper, we travel back in time to provide a new look at GT2-FLSs by adopting Zadeh's (Z) GT2 Fuzzy Set (FS) definition, intending to learn GT2-FLSs that are capable of achieving reliable High-Quality Prediction Intervals (HQ-PI) alongside precision. By integrating Z-GT2-FS with the \\(\\alpha\\)-plane representation, we show that the design flexibility of GT2-FLS is increased as it takes away the dependency of the secondary membership function from the primary membership function. After detailing the construction of Z-GT2-FLSs, we provide solutions to challenges while learning from high-dimensional data: the curse of dimensionality, and integrating Deep Learning (DL) optimizers. We develop a DL framework for learning dual-focused Z-GT2-FLSs with high performances. Our study includes statistical analyses, highlighting that the Z-GT2-FLS not only exhibits high-precision performance but also produces HQ-PIs in comparison to its GT2 and IT2 fuzzy counterparts which have more learnable parameters. The results show that the Z-GT2-FLS has a huge potential in uncertainty quantification.","sentences":["General Type-2 (GT2)","Fuzzy Logic Systems (FLSs) are perfect candidates to quantify uncertainty, which is crucial for informed decisions in high-risk tasks, as they are powerful tools in representing uncertainty.","In this paper, we travel back in time to provide a new look at GT2-FLSs by adopting Zadeh's (Z) GT2 Fuzzy Set (FS) definition, intending to learn GT2-FLSs that are capable of achieving reliable High-Quality Prediction Intervals (HQ-PI) alongside precision.","By integrating Z-GT2-FS with the \\(\\alpha\\)-plane representation, we show that the design flexibility of GT2-FLS is increased as it takes away the dependency of the secondary membership function from the primary membership function.","After detailing the construction of Z-GT2-FLSs, we provide solutions to challenges while learning from high-dimensional data: the curse of dimensionality, and integrating Deep Learning (DL) optimizers.","We develop a DL framework for learning dual-focused Z-GT2-FLSs with high performances.","Our study includes statistical analyses, highlighting that the Z-GT2-FLS not only exhibits high-precision performance but also produces HQ-PIs in comparison to its GT2 and IT2 fuzzy counterparts which have more learnable parameters.","The results show that the Z-GT2-FLS has a huge potential in uncertainty quantification."],"url":"http://arxiv.org/abs/2404.12800v1","category":"cs.LG"}
{"created":"2024-04-19 11:29:07","title":"On Global Symmetries and Fayet-Iliopoulos Terms","abstract":"We revisit the genuine Fayet-Iliopoulos terms of 4D N=1 supergravity. Such terms are commonly believed to preserve a global symmetry, and therefore they are in conflict with the principles of quantum gravity. However, we find that generically there do exist supersymmetric terms that break explicitly the specific global symmetry, while preserving gauge invariance. We illustrate this, by providing a series of examples, including superpotentials and superspace higher order terms, along with a general prescription for their construction.","sentences":["We revisit the genuine Fayet-Iliopoulos terms of 4D N=1 supergravity.","Such terms are commonly believed to preserve a global symmetry, and therefore they are in conflict with the principles of quantum gravity.","However, we find that generically there do exist supersymmetric terms that break explicitly the specific global symmetry, while preserving gauge invariance.","We illustrate this, by providing a series of examples, including superpotentials and superspace higher order terms, along with a general prescription for their construction."],"url":"http://arxiv.org/abs/2404.12799v1","category":"hep-th"}
{"created":"2024-04-19 11:24:03","title":"Conversion of Boolean and Integer FlatZinc Builtins to Quadratic or Linear Integer Problems","abstract":"Constraint satisfaction or optimisation models -- even if they are formulated in high-level modelling languages -- need to be reduced into an equivalent format before they can be solved by the use of Quantum Computing. In this paper we show how Boolean and integer FlatZinc builtins over finite-domain integer variables can be equivalently reformulated as linear equations, linear inequalities or binary products of those variables, i.e. as finite-domain quadratic integer programs. Those quadratic integer programs can be further transformed into equivalent Quadratic Unconstrained Binary Optimisation problem models, i.e. a general format for optimisation problems to be solved on Quantum Computers especially on Quantum Annealers.","sentences":["Constraint satisfaction or optimisation models -- even if they are formulated in high-level modelling languages -- need to be reduced into an equivalent format before they can be solved by the use of Quantum Computing.","In this paper we show how Boolean and integer FlatZinc builtins over finite-domain integer variables can be equivalently reformulated as linear equations, linear inequalities or binary products of those variables, i.e. as finite-domain quadratic integer programs.","Those quadratic integer programs can be further transformed into equivalent Quadratic Unconstrained Binary Optimisation problem models, i.e. a general format for optimisation problems to be solved on Quantum Computers especially on Quantum Annealers."],"url":"http://arxiv.org/abs/2404.12797v1","category":"cs.MS"}
{"created":"2024-04-19 11:22:58","title":"Harnessing cardiac power: heart kinetic motion analysis for energy harvesters","abstract":"Accurately estimating the complex motion of the heart can unlock enormous potential for kinetic energy harvesting. This paper presents a foundational dataset for heart kinetic motion through in-vivo tests and investigates the most influential factors in heart kinetic motion. In-vivo tests on a living pig's heart, with signal processing, were carried out to study the heart movement by heart beating and respiration motions. A network of nine points on the heart was employed for in vivo measurements. These measurements illustrated the kinetic energy signals in displacement, velocity, and acceleration. The results indicated that the motion level varies in distinct locations over epicardium. The statistical features and autocorrelations were reported for these points, illustrating the highest displacement and acceleration. Each heartbeat generated an energy of 14.35 mJ and a power of 1.03 W. However, this available energy is not uniformly distributed. The results illustrated that not only is cardiac movement location-dependent, but the speed of cardiac displacement cycles is also location-dependent. The right atrium has the highest cardiac kinetic movement with an amplitude of 16.19 mm displacement and 16.3 m/s2 acceleration. To evaluate the energy harvesting possibility from the heart's motion, a piezoelectric energy harvester was simulated by the finite element method, implying that the energy harvesting level significantly depends on implant location over epicardium. The results of this study open the potential of designing novel energy harvesters based on accurate heart movements and provide a foundation for future investigations of energy harvesting for leadless pacemaker energy systems.","sentences":["Accurately estimating the complex motion of the heart can unlock enormous potential for kinetic energy harvesting.","This paper presents a foundational dataset for heart kinetic motion through in-vivo tests and investigates the most influential factors in heart kinetic motion.","In-vivo tests on a living pig's heart, with signal processing, were carried out to study the heart movement by heart beating and respiration motions.","A network of nine points on the heart was employed for in vivo measurements.","These measurements illustrated the kinetic energy signals in displacement, velocity, and acceleration.","The results indicated that the motion level varies in distinct locations over epicardium.","The statistical features and autocorrelations were reported for these points, illustrating the highest displacement and acceleration.","Each heartbeat generated an energy of 14.35 mJ and a power of 1.03 W. However, this available energy is not uniformly distributed.","The results illustrated that not only is cardiac movement location-dependent, but the speed of cardiac displacement cycles is also location-dependent.","The right atrium has the highest cardiac kinetic movement with an amplitude of 16.19 mm displacement and 16.3 m/s2 acceleration.","To evaluate the energy harvesting possibility from the heart's motion, a piezoelectric energy harvester was simulated by the finite element method, implying that the energy harvesting level significantly depends on implant location over epicardium.","The results of this study open the potential of designing novel energy harvesters based on accurate heart movements and provide a foundation for future investigations of energy harvesting for leadless pacemaker energy systems."],"url":"http://arxiv.org/abs/2404.12796v1","category":"physics.med-ph"}
{"created":"2024-04-19 11:09:55","title":"Efficient Learning of Fuzzy Logic Systems for Large-Scale Data Using Deep Learning","abstract":"Type-1 and Interval Type-2 (IT2) Fuzzy Logic Systems (FLS) excel in handling uncertainty alongside their parsimonious rule-based structure. Yet, in learning large-scale data challenges arise, such as the curse of dimensionality and training complexity of FLSs. The complexity is due mainly to the constraints to be satisfied as the learnable parameters define FSs and the complexity of the center of the sets calculation method, especially of IT2-FLSs. This paper explicitly focuses on the learning problem of FLSs and presents a computationally efficient learning method embedded within the realm of Deep Learning (DL). The proposed method tackles the learning challenges of FLSs by presenting computationally efficient implementations of FLSs, thereby minimizing training time while leveraging mini-batched DL optimizers and automatic differentiation provided within the DL frameworks. We illustrate the efficiency of the DL framework for FLSs on benchmark datasets.","sentences":["Type-1 and Interval Type-2 (IT2) Fuzzy Logic Systems (FLS) excel in handling uncertainty alongside their parsimonious rule-based structure.","Yet, in learning large-scale data challenges arise, such as the curse of dimensionality and training complexity of FLSs.","The complexity is due mainly to the constraints to be satisfied as the learnable parameters define FSs and the complexity of the center of the sets calculation method, especially of IT2-FLSs.","This paper explicitly focuses on the learning problem of FLSs and presents a computationally efficient learning method embedded within the realm of Deep Learning (DL).","The proposed method tackles the learning challenges of FLSs by presenting computationally efficient implementations of FLSs, thereby minimizing training time while leveraging mini-batched DL optimizers and automatic differentiation provided within the DL frameworks.","We illustrate the efficiency of the DL framework for FLSs on benchmark datasets."],"url":"http://arxiv.org/abs/2404.12792v1","category":"cs.LG"}
{"created":"2024-04-19 10:55:07","title":"Unlocking the Potential of Local CSI in Cell-Free Networks with Channel Aging and Fronthaul Delays","abstract":"It is generally believed that downlink cell-free networks perform best under centralized implementations where the local channel state information (CSI) acquired by the access-points (AP) is forwarded to one or more central processing units (CPU) for the computation of the joint precoders based on global CSI. However, mostly due to limited fronthaul capabilities, this procedure incurs some delay that may lead to partially outdated precoding decisions and hence performance degradation. In some scenarios, this may even lead to worse performance than distributed implementations where the precoders are locally computed by the APs based on partial yet timely local CSI. To address this issue, this study considers the problem of robust precoding design merging the benefits of timely local CSI and delayed global CSI. As main result, we provide a novel distributed precoding design based on the recently proposed team minimum mean-square error method. As a byproduct, we also obtain novel insights related to the AP-CPU functional split problem. Our main conclusion, corroborated by simulations, is that the opportunity of performing some local precoding computations at the APs should not be neglected, even in centralized implementations.","sentences":["It is generally believed that downlink cell-free networks perform best under centralized implementations where the local channel state information (CSI) acquired by the access-points (AP) is forwarded to one or more central processing units (CPU) for the computation of the joint precoders based on global CSI.","However, mostly due to limited fronthaul capabilities, this procedure incurs some delay that may lead to partially outdated precoding decisions and hence performance degradation.","In some scenarios, this may even lead to worse performance than distributed implementations where the precoders are locally computed by the APs based on partial yet timely local CSI.","To address this issue, this study considers the problem of robust precoding design merging the benefits of timely local CSI and delayed global CSI.","As main result, we provide a novel distributed precoding design based on the recently proposed team minimum mean-square error method.","As a byproduct, we also obtain novel insights related to the AP-CPU functional split problem.","Our main conclusion, corroborated by simulations, is that the opportunity of performing some local precoding computations at the APs should not be neglected, even in centralized implementations."],"url":"http://arxiv.org/abs/2404.12786v1","category":"cs.IT"}
{"created":"2024-04-19 10:47:53","title":"Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation","abstract":"We introduce Contrastive Gaussian Clustering, a novel approach capable of provide segmentation masks from any viewpoint and of enabling 3D segmentation of the scene. Recent works in novel-view synthesis have shown how to model the appearance of a scene via a cloud of 3D Gaussians, and how to generate accurate images from a given viewpoint by projecting on it the Gaussians before $\\alpha$ blending their color. Following this example, we train a model to include also a segmentation feature vector for each Gaussian. These can then be used for 3D scene segmentation, by clustering Gaussians according to their feature vectors; and to generate 2D segmentation masks, by projecting the Gaussians on a plane and $\\alpha$ blending over their segmentation features. Using a combination of contrastive learning and spatial regularization, our method can be trained on inconsistent 2D segmentation masks, and still learn to generate segmentation masks consistent across all views. Moreover, the resulting model is extremely accurate, improving the IoU accuracy of the predicted masks by $+8\\%$ over the state of the art. Code and trained models will be released soon.","sentences":["We introduce Contrastive Gaussian Clustering, a novel approach capable of provide segmentation masks from any viewpoint and of enabling 3D segmentation of the scene.","Recent works in novel-view synthesis have shown how to model the appearance of a scene via a cloud of 3D Gaussians, and how to generate accurate images from a given viewpoint by projecting on it the Gaussians before $\\alpha$ blending their color.","Following this example, we train a model to include also a segmentation feature vector for each Gaussian.","These can then be used for 3D scene segmentation, by clustering Gaussians according to their feature vectors; and to generate 2D segmentation masks, by projecting the Gaussians on a plane and $\\alpha$ blending over their segmentation features.","Using a combination of contrastive learning and spatial regularization, our method can be trained on inconsistent 2D segmentation masks, and still learn to generate segmentation masks consistent across all views.","Moreover, the resulting model is extremely accurate, improving the IoU accuracy of the predicted masks by $+8\\%$ over the state of the art.","Code and trained models will be released soon."],"url":"http://arxiv.org/abs/2404.12784v1","category":"cs.CV"}
{"created":"2024-04-19 10:45:05","title":"A Proactive Decoy Selection Scheme for Cyber Deception using MITRE ATT&CK","abstract":"Cyber deception allows compensating the late response of defenders countermeasures to the ever evolving tactics, techniques, and procedures (TTPs) of attackers. This proactive defense strategy employs decoys resembling legitimate system components to lure stealthy attackers within the defender environment, slowing and/or denying the accomplishment of their goals. In this regard, the selection of decoys that can expose the techniques used by malicious users plays a central role to incentivize their engagement. However, this is a difficult task to achieve in practice, since it requires an accurate and realistic modeling of the attacker capabilities and his possible targets. In this work, we tackle this challenge and we design a decoy selection scheme that is supported by an adversarial modeling based on empirical observation of real-world attackers. We take advantage of a domain-specific threat modelling language using MITRE ATT&CK framework as source of attacker TTPs targeting enterprise systems. In detail, we extract the information about the execution preconditions of each technique as well as its possible effects on the environment to generate attack graphs modeling the adversary capabilities. Based on this, we formulate a graph partition problem that minimizes the number of decoys detecting a corresponding number of techniques employed in various attack paths directed to specific targets. We compare our optimization-based decoy selection approach against several benchmark schemes that ignore the preconditions between the various attack steps. Results reveal that the proposed scheme provides the highest interception rate of attack paths using the lowest amount of decoys.","sentences":["Cyber deception allows compensating the late response of defenders countermeasures to the ever evolving tactics, techniques, and procedures (TTPs) of attackers.","This proactive defense strategy employs decoys resembling legitimate system components to lure stealthy attackers within the defender environment, slowing and/or denying the accomplishment of their goals.","In this regard, the selection of decoys that can expose the techniques used by malicious users plays a central role to incentivize their engagement.","However, this is a difficult task to achieve in practice, since it requires an accurate and realistic modeling of the attacker capabilities and his possible targets.","In this work, we tackle this challenge and we design a decoy selection scheme that is supported by an adversarial modeling based on empirical observation of real-world attackers.","We take advantage of a domain-specific threat modelling language using MITRE ATT&CK framework as source of attacker TTPs targeting enterprise systems.","In detail, we extract the information about the execution preconditions of each technique as well as its possible effects on the environment to generate attack graphs modeling the adversary capabilities.","Based on this, we formulate a graph partition problem that minimizes the number of decoys detecting a corresponding number of techniques employed in various attack paths directed to specific targets.","We compare our optimization-based decoy selection approach against several benchmark schemes that ignore the preconditions between the various attack steps.","Results reveal that the proposed scheme provides the highest interception rate of attack paths using the lowest amount of decoys."],"url":"http://arxiv.org/abs/2404.12783v1","category":"cs.CR"}
{"created":"2024-04-19 10:43:25","title":"Sentiment-oriented Transformer-based Variational Autoencoder Network for Live Video Commenting","abstract":"Automatic live video commenting is with increasing attention due to its significance in narration generation, topic explanation, etc. However, the diverse sentiment consideration of the generated comments is missing from the current methods. Sentimental factors are critical in interactive commenting, and lack of research so far. Thus, in this paper, we propose a Sentiment-oriented Transformer-based Variational Autoencoder (So-TVAE) network which consists of a sentiment-oriented diversity encoder module and a batch attention module, to achieve diverse video commenting with multiple sentiments and multiple semantics. Specifically, our sentiment-oriented diversity encoder elegantly combines VAE and random mask mechanism to achieve semantic diversity under sentiment guidance, which is then fused with cross-modal features to generate live video comments. Furthermore, a batch attention module is also proposed in this paper to alleviate the problem of missing sentimental samples, caused by the data imbalance, which is common in live videos as the popularity of videos varies. Extensive experiments on Livebot and VideoIC datasets demonstrate that the proposed So-TVAE outperforms the state-of-the-art methods in terms of the quality and diversity of generated comments. Related code is available at https://github.com/fufy1024/So-TVAE.","sentences":["Automatic live video commenting is with increasing attention due to its significance in narration generation, topic explanation, etc.","However, the diverse sentiment consideration of the generated comments is missing from the current methods.","Sentimental factors are critical in interactive commenting, and lack of research so far.","Thus, in this paper, we propose a Sentiment-oriented Transformer-based Variational Autoencoder (So-TVAE) network which consists of a sentiment-oriented diversity encoder module and a batch attention module, to achieve diverse video commenting with multiple sentiments and multiple semantics.","Specifically, our sentiment-oriented diversity encoder elegantly combines VAE and random mask mechanism to achieve semantic diversity under sentiment guidance, which is then fused with cross-modal features to generate live video comments.","Furthermore, a batch attention module is also proposed in this paper to alleviate the problem of missing sentimental samples, caused by the data imbalance, which is common in live videos as the popularity of videos varies.","Extensive experiments on Livebot and VideoIC datasets demonstrate that the proposed So-TVAE outperforms the state-of-the-art methods in terms of the quality and diversity of generated comments.","Related code is available at https://github.com/fufy1024/So-TVAE."],"url":"http://arxiv.org/abs/2404.12782v1","category":"cs.CV"}
{"created":"2024-04-19 10:30:02","title":"The open-source sunbather code: modeling escaping planetary atmospheres and their transit spectra","abstract":"Atmospheric escape is thought to significantly influence the evolution of exoplanets, especially for sub-Jupiter planets on short orbital periods. Theoretical models predict that hydrodynamic escape could erode the atmospheres of such gaseous planets, leaving only a rocky core. Deriving atmospheric mass-loss rates from observations is necessary to check these predictions. One of the ways to obtain mass-loss rate estimates is to fit transit spectra of the 10830 {\\AA} helium or UV metal lines with Parker wind models. We aim to provide the community with a tool that enables performing this type of analysis, and present sunbather, an open-source Python code to model escaping exoplanet atmospheres and their transit spectra. sunbather incorporates the Parker wind code p-winds and the photoionization code Cloudy, with the ability to calculate any currently known spectral tracer at an arbitrary atmospheric composition. With sunbather, we investigate how the atmospheric structure of a generic hot Neptune planet depends on the metallicity. We find that the mass-loss rate drops by roughly one order of magnitude as we increase the metallicity from solar to 50 times solar. Line cooling by metal species is important already for a solar composition, and more so at higher metallicity. We then demonstrate how sunbather can be used to interpret observations of spectral lines that form in the upper atmosphere. We fit the observed helium spectrum of the mini-Neptune TOI-2134 b and show how even for helium data, the inferred mass-loss rate depends on the metallicity by up to a factor of three.","sentences":["Atmospheric escape is thought to significantly influence the evolution of exoplanets, especially for sub-Jupiter planets on short orbital periods.","Theoretical models predict that hydrodynamic escape could erode the atmospheres of such gaseous planets, leaving only a rocky core.","Deriving atmospheric mass-loss rates from observations is necessary to check these predictions.","One of the ways to obtain mass-loss rate estimates is to fit transit spectra of the 10830 {\\AA} helium or UV metal lines with Parker wind models.","We aim to provide the community with a tool that enables performing this type of analysis, and present sunbather, an open-source Python code to model escaping exoplanet atmospheres and their transit spectra.","sunbather incorporates the Parker wind code p-winds and the photoionization code Cloudy, with the ability to calculate any currently known spectral tracer at an arbitrary atmospheric composition.","With sunbather, we investigate how the atmospheric structure of a generic hot Neptune planet depends on the metallicity.","We find that the mass-loss rate drops by roughly one order of magnitude as we increase the metallicity from solar to 50 times solar.","Line cooling by metal species is important already for a solar composition, and more so at higher metallicity.","We then demonstrate how sunbather can be used to interpret observations of spectral lines that form in the upper atmosphere.","We fit the observed helium spectrum of the mini-Neptune TOI-2134 b and show how even for helium data, the inferred mass-loss rate depends on the metallicity by up to a factor of three."],"url":"http://arxiv.org/abs/2404.12775v1","category":"astro-ph.EP"}
{"created":"2024-04-19 10:29:40","title":"Recent Advancements in Battery State of Power Estimation Technology: A Comprehensive Overview and Error Source Analysis","abstract":"Accurate state of power (SOP) estimation is of great importance for lithium-ion batteries in safety-critical and power-intensive applications for electric vehicles. This review article delves deeply into the entire development flow of current SOP estimation technology, offering a systematic breakdown of all key aspects with their recent advancements. First, we review the design of battery safe operation area, summarizing diverse limitation factors and furnishing a profound comprehension of battery safety across a broad operational scale. Second, we illustrate the unique discharge and charge characteristics of various peak operation modes, such as constant current, constant voltage, constant current-constant voltage, and constant power, and explore their impacts on battery peak power performance. Third, we extensively survey the aspects of battery modelling and algorithm development in current SOP estimation technology, highlighting their technical contributions and specific considerations. Fourth, we present an in-depth dissection of all error sources to unveil their propagation pathways, providing insightful analysis into how each type of error impacts the SOP estimation performance. Finally, the technical challenges and complexities inherent in this field of research are addressed, suggesting potential directions for future development. Our goal is to inspire further efforts towards developing more accurate and intelligent SOP estimation technology for next-generation battery management systems.","sentences":["Accurate state of power (SOP) estimation is of great importance for lithium-ion batteries in safety-critical and power-intensive applications for electric vehicles.","This review article delves deeply into the entire development flow of current SOP estimation technology, offering a systematic breakdown of all key aspects with their recent advancements.","First, we review the design of battery safe operation area, summarizing diverse limitation factors and furnishing a profound comprehension of battery safety across a broad operational scale.","Second, we illustrate the unique discharge and charge characteristics of various peak operation modes, such as constant current, constant voltage, constant current-constant voltage, and constant power, and explore their impacts on battery peak power performance.","Third, we extensively survey the aspects of battery modelling and algorithm development in current SOP estimation technology, highlighting their technical contributions and specific considerations.","Fourth, we present an in-depth dissection of all error sources to unveil their propagation pathways, providing insightful analysis into how each type of error impacts the SOP estimation performance.","Finally, the technical challenges and complexities inherent in this field of research are addressed, suggesting potential directions for future development.","Our goal is to inspire further efforts towards developing more accurate and intelligent SOP estimation technology for next-generation battery management systems."],"url":"http://arxiv.org/abs/2404.12774v1","category":"eess.SY"}
{"created":"2024-04-19 10:28:54","title":"LayeredMAPF: a decomposition of MAPF instance without compromising solvability","abstract":"Generally, the calculation and memory space required for multi-agent path finding (MAPF) grows exponentially as the number of agents increases. This often results in some MAPF instances being unsolvable under limited computational resources and memory space, thereby limiting the application of MAPF in complex scenarios. Hence, we propose a decomposition approach for MAPF instances, which breaks down instances involving a large number of agents into multiple isolated subproblems involving fewer agents. Moreover, we present a framework to enable general MAPF algorithms to solve each subproblem independently and merge their solutions into one conflict-free final solution, without compromising on solvability. Unlike existing works that propose isolated methods aimed at reducing the time cost of MAPF, our method is applicable to all MAPF methods. In our results, we apply decomposition to multiple state-of-the-art MAPF methods using a classic MAPF benchmark (https://movingai.com/benchmarks/mapf.html). The decomposition of MAPF instances is completed on average within 1s, and its application to seven MAPF methods reduces the memory usage and time cost significantly, particularly for serial methods. To facilitate further research within the community, we have made the source code of the proposed algorithm publicly available (https://github.com/JoeYao-bit/LayeredMAPF).","sentences":["Generally, the calculation and memory space required for multi-agent path finding (MAPF) grows exponentially as the number of agents increases.","This often results in some MAPF instances being unsolvable under limited computational resources and memory space, thereby limiting the application of MAPF in complex scenarios.","Hence, we propose a decomposition approach for MAPF instances, which breaks down instances involving a large number of agents into multiple isolated subproblems involving fewer agents.","Moreover, we present a framework to enable general MAPF algorithms to solve each subproblem independently and merge their solutions into one conflict-free final solution, without compromising on solvability.","Unlike existing works that propose isolated methods aimed at reducing the time cost of MAPF, our method is applicable to all MAPF methods.","In our results, we apply decomposition to multiple state-of-the-art MAPF methods using a classic MAPF benchmark (https://movingai.com/benchmarks/mapf.html).","The decomposition of MAPF instances is completed on average within 1s, and its application to seven MAPF methods reduces the memory usage and time cost significantly, particularly for serial methods.","To facilitate further research within the community, we have made the source code of the proposed algorithm publicly available (https://github.com/JoeYao-bit/LayeredMAPF)."],"url":"http://arxiv.org/abs/2404.12773v1","category":"cs.RO"}
{"created":"2024-04-19 10:27:40","title":"Generating Test Scenarios from NL Requirements using Retrieval-Augmented LLMs: An Industrial Study","abstract":"Test scenarios are specific instances of test cases that describe actions to validate a particular software functionality. By outlining the conditions under which the software operates and the expected outcomes, test scenarios ensure that the software functionality is tested in an integrated manner. Test scenarios are crucial for systematically testing an application under various conditions, including edge cases, to identify potential issues and guarantee overall performance and reliability. Specifying test scenarios is tedious and requires a deep understanding of software functionality and the underlying domain. It further demands substantial effort and investment from already time- and budget-constrained requirements engineers and testing teams. This paper presents an automated approach (RAGTAG) for test scenario generation using Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs). RAG allows the integration of specific domain knowledge with LLMs' generation capabilities. We evaluate RAGTAG on two industrial projects from Austrian Post with bilingual requirements in German and English. Our results from an interview survey conducted with four experts on five dimensions -- relevance, coverage, correctness, coherence and feasibility, affirm the potential of RAGTAG in automating test scenario generation. Specifically, our results indicate that, despite the difficult task of analyzing bilingual requirements, RAGTAG is able to produce scenarios that are well-aligned with the underlying requirements and provide coverage of different aspects of the intended functionality. The generated scenarios are easily understandable to experts and feasible for testing in the project environment. The overall correctness is deemed satisfactory; however, gaps in capturing exact action sequences and domain nuances remain, underscoring the need for domain expertise when applying LLMs.","sentences":["Test scenarios are specific instances of test cases that describe actions to validate a particular software functionality.","By outlining the conditions under which the software operates and the expected outcomes, test scenarios ensure that the software functionality is tested in an integrated manner.","Test scenarios are crucial for systematically testing an application under various conditions, including edge cases, to identify potential issues and guarantee overall performance and reliability.","Specifying test scenarios is tedious and requires a deep understanding of software functionality and the underlying domain.","It further demands substantial effort and investment from already time- and budget-constrained requirements engineers and testing teams.","This paper presents an automated approach (RAGTAG) for test scenario generation using Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs).","RAG allows the integration of specific domain knowledge with LLMs' generation capabilities.","We evaluate RAGTAG on two industrial projects from Austrian Post with bilingual requirements in German and English.","Our results from an interview survey conducted with four experts on five dimensions -- relevance, coverage, correctness, coherence and feasibility, affirm the potential of RAGTAG in automating test scenario generation.","Specifically, our results indicate that, despite the difficult task of analyzing bilingual requirements, RAGTAG is able to produce scenarios that are well-aligned with the underlying requirements and provide coverage of different aspects of the intended functionality.","The generated scenarios are easily understandable to experts and feasible for testing in the project environment.","The overall correctness is deemed satisfactory; however, gaps in capturing exact action sequences and domain nuances remain, underscoring the need for domain expertise when applying LLMs."],"url":"http://arxiv.org/abs/2404.12772v1","category":"cs.SE"}
{"created":"2024-04-19 10:23:12","title":"Phase-space analysis of a two-section InP laser as an all-optical spiking neuron: dependency on control and design parameters","abstract":"Using a rate-equation model we numerically evaluate the carrier concentration and photon number in an integrated two-section semiconductor laser, and analyse its dynamics in three-dimensional phase space. The simulation comprises compact model descriptions extracted from a commercially-available generic InP technology platform, allowing us to model an applied reverse-bias voltage to the saturable absorber. We use the model to study the influence of the injected gain current, reverse-bias voltage, and cavity mirror reflectivity on the excitable operation state, which is the operation mode desired for the laser to act as an all-optical integrated neuron. We show in phase-space that our model is capable of demonstrating four different operation modes, i.e. cw, self-pulsating and an on-set and excitable mode under optical pulse injection. In addition, we show that lowering the reflectivity of one of the cavity mirrors greatly enhances the control parameter space for excitable operation, enabling more relaxed operation parameter control and lower power consumption of an integrated two-section laser neuron.","sentences":["Using a rate-equation model we numerically evaluate the carrier concentration and photon number in an integrated two-section semiconductor laser, and analyse its dynamics in three-dimensional phase space.","The simulation comprises compact model descriptions extracted from a commercially-available generic InP technology platform, allowing us to model an applied reverse-bias voltage to the saturable absorber.","We use the model to study the influence of the injected gain current, reverse-bias voltage, and cavity mirror reflectivity on the excitable operation state, which is the operation mode desired for the laser to act as an all-optical integrated neuron.","We show in phase-space that our model is capable of demonstrating four different operation modes, i.e. cw, self-pulsating and an on-set and excitable mode under optical pulse injection.","In addition, we show that lowering the reflectivity of one of the cavity mirrors greatly enhances the control parameter space for excitable operation, enabling more relaxed operation parameter control and lower power consumption of an integrated two-section laser neuron."],"url":"http://arxiv.org/abs/2404.12771v1","category":"physics.optics"}
{"created":"2024-04-19 10:17:10","title":"MixLight: Borrowing the Best of both Spherical Harmonics and Gaussian Models","abstract":"Accurately estimating scene lighting is critical for applications such as mixed reality. Existing works estimate illumination by generating illumination maps or regressing illumination parameters. However, the method of generating illumination maps has poor generalization performance and parametric models such as Spherical Harmonic (SH) and Spherical Gaussian (SG) fall short in capturing high-frequency or low-frequency components. This paper presents MixLight, a joint model that utilizes the complementary characteristics of SH and SG to achieve a more complete illumination representation, which uses SH and SG to capture low-frequency ambient and high-frequency light sources respectively. In addition, a special spherical light source sparsemax (SLSparsemax) module that refers to the position and brightness relationship between spherical light sources is designed to improve their sparsity, which is significant but omitted by prior works. Extensive experiments demonstrate that MixLight surpasses state-of-the-art (SOTA) methods on multiple metrics. In addition, experiments on Web Dataset also show that MixLight as a parametric method has better generalization performance than non-parametric methods.","sentences":["Accurately estimating scene lighting is critical for applications such as mixed reality.","Existing works estimate illumination by generating illumination maps or regressing illumination parameters.","However, the method of generating illumination maps has poor generalization performance and parametric models such as Spherical Harmonic (SH) and Spherical Gaussian (SG) fall short in capturing high-frequency or low-frequency components.","This paper presents MixLight, a joint model that utilizes the complementary characteristics of SH and SG to achieve a more complete illumination representation, which uses SH and SG to capture low-frequency ambient and high-frequency light sources respectively.","In addition, a special spherical light source sparsemax (SLSparsemax) module that refers to the position and brightness relationship between spherical light sources is designed to improve their sparsity, which is significant but omitted by prior works.","Extensive experiments demonstrate that MixLight surpasses state-of-the-art (SOTA) methods on multiple metrics.","In addition, experiments on Web Dataset also show that MixLight as a parametric method has better generalization performance than non-parametric methods."],"url":"http://arxiv.org/abs/2404.12768v1","category":"cs.CV"}
{"created":"2024-04-19 10:08:28","title":"How should AI decisions be explained? Requirements for Explanations from the Perspective of European Law","abstract":"This paper investigates the relationship between law and eXplainable Artificial Intelligence (XAI). While there is much discussion about the AI Act, for which the trilogue of the European Parliament, Council and Commission recently concluded, other areas of law seem underexplored. This paper focuses on European (and in part German) law, although with international concepts and regulations such as fiduciary plausibility checks, the General Data Protection Regulation (GDPR), and product safety and liability. Based on XAI-taxonomies, requirements for XAI-methods are derived from each of the legal bases, resulting in the conclusion that each legal basis requires different XAI properties and that the current state of the art does not fulfill these to full satisfaction, especially regarding the correctness (sometimes called fidelity) and confidence estimates of XAI-methods.","sentences":["This paper investigates the relationship between law and eXplainable Artificial Intelligence (XAI).","While there is much discussion about the AI Act, for which the trilogue of the European Parliament, Council and Commission recently concluded, other areas of law seem underexplored.","This paper focuses on European (and in part German) law, although with international concepts and regulations such as fiduciary plausibility checks, the General Data Protection Regulation (GDPR), and product safety and liability.","Based on XAI-taxonomies, requirements for XAI-methods are derived from each of the legal bases, resulting in the conclusion that each legal basis requires different XAI properties and that the current state of the art does not fulfill these to full satisfaction, especially regarding the correctness (sometimes called fidelity) and confidence estimates of XAI-methods."],"url":"http://arxiv.org/abs/2404.12762v1","category":"cs.AI"}
{"created":"2024-04-19 10:03:59","title":"Food Development through Co-creation with AI: bread with a \"taste of love\"","abstract":"This study explores a new method in food development by utilizing AI including generative AI, aiming to craft products that delight the senses and resonate with consumers' emotions. The food ingredient recommendation approach used in this study can be considered as a form of multimodal generation in a broad sense, as it takes text as input and outputs food ingredient candidates. This Study focused on producing \"Romance Bread,\" a collection of breads infused with flavors that reflect the nuances of a romantic Japanese television program. We analyzed conversations from TV programs and lyrics from songs featuring fruits and sweets to recommend ingredients that express romantic feelings. Based on these recommendations, the bread developers then considered the flavoring of the bread and developed new bread varieties. The research included a tasting evaluation involving 31 participants and interviews with the product developers. Findings indicate a notable correlation between tastes generated by AI and human preferences. This study validates the concept of using AI in food innovation and highlights the broad potential for developing unique consumer experiences that focus on emotional engagement through AI and human collaboration.","sentences":["This study explores a new method in food development by utilizing AI including generative AI, aiming to craft products that delight the senses and resonate with consumers' emotions.","The food ingredient recommendation approach used in this study can be considered as a form of multimodal generation in a broad sense, as it takes text as input and outputs food ingredient candidates.","This Study focused on producing \"Romance Bread,\" a collection of breads infused with flavors that reflect the nuances of a romantic Japanese television program.","We analyzed conversations from TV programs and lyrics from songs featuring fruits and sweets to recommend ingredients that express romantic feelings.","Based on these recommendations, the bread developers then considered the flavoring of the bread and developed new bread varieties.","The research included a tasting evaluation involving 31 participants and interviews with the product developers.","Findings indicate a notable correlation between tastes generated by AI and human preferences.","This study validates the concept of using AI in food innovation and highlights the broad potential for developing unique consumer experiences that focus on emotional engagement through AI and human collaboration."],"url":"http://arxiv.org/abs/2404.12760v1","category":"cs.AI"}
{"created":"2024-04-19 10:00:34","title":"Adaptive Regularization of Representation Rank as an Implicit Constraint of Bellman Equation","abstract":"Representation rank is an important concept for understanding the role of Neural Networks (NNs) in Deep Reinforcement learning (DRL), which measures the expressive capacity of value networks. Existing studies focus on unboundedly maximizing this rank; nevertheless, that approach would introduce overly complex models in the learning, thus undermining performance. Hence, fine-tuning representation rank presents a challenging and crucial optimization problem. To address this issue, we find a guiding principle for adaptive control of the representation rank. We employ the Bellman equation as a theoretical foundation and derive an upper bound on the cosine similarity of consecutive state-action pairs representations of value networks. We then leverage this upper bound to propose a novel regularizer, namely BEllman Equation-based automatic rank Regularizer (BEER). This regularizer adaptively regularizes the representation rank, thus improving the DRL agent's performance. We first validate the effectiveness of automatic control of rank on illustrative experiments. Then, we scale up BEER to complex continuous control tasks by combining it with the deterministic policy gradient method. Among 12 challenging DeepMind control tasks, BEER outperforms the baselines by a large margin. Besides, BEER demonstrates significant advantages in Q-value approximation. Our code is available at https://github.com/sweetice/BEER-ICLR2024.","sentences":["Representation rank is an important concept for understanding the role of Neural Networks (NNs) in Deep Reinforcement learning (DRL), which measures the expressive capacity of value networks.","Existing studies focus on unboundedly maximizing this rank; nevertheless, that approach would introduce overly complex models in the learning, thus undermining performance.","Hence, fine-tuning representation rank presents a challenging and crucial optimization problem.","To address this issue, we find a guiding principle for adaptive control of the representation rank.","We employ the Bellman equation as a theoretical foundation and derive an upper bound on the cosine similarity of consecutive state-action pairs representations of value networks.","We then leverage this upper bound to propose a novel regularizer, namely BEllman Equation-based automatic rank Regularizer (BEER).","This regularizer adaptively regularizes the representation rank, thus improving the DRL agent's performance.","We first validate the effectiveness of automatic control of rank on illustrative experiments.","Then, we scale up BEER to complex continuous control tasks by combining it with the deterministic policy gradient method.","Among 12 challenging DeepMind control tasks, BEER outperforms the baselines by a large margin.","Besides, BEER demonstrates significant advantages in Q-value approximation.","Our code is available at https://github.com/sweetice/BEER-ICLR2024."],"url":"http://arxiv.org/abs/2404.12754v1","category":"cs.LG"}
{"created":"2024-04-19 09:59:44","title":"AutoCrawler: A Progressive Understanding Web Agent for Web Crawler Generation","abstract":"Web automation is a significant technique that accomplishes complicated web tasks by automating common web actions, enhancing operational efficiency, and reducing the need for manual intervention. Traditional methods, such as wrappers, suffer from limited adaptability and scalability when faced with a new website. On the other hand, generative agents empowered by large language models (LLMs) exhibit poor performance and reusability in open-world scenarios. In this work, we introduce a crawler generation task for vertical information web pages and the paradigm of combining LLMs with crawlers, which helps crawlers handle diverse and changing web environments more efficiently. We propose AutoCrawler, a two-stage framework that leverages the hierarchical structure of HTML for progressive understanding. Through top-down and step-back operations, AutoCrawler can learn from erroneous actions and continuously prune HTML for better action generation. We conduct comprehensive experiments with multiple LLMs and demonstrate the effectiveness of our framework. Resources of this paper can be found at \\url{https://github.com/EZ-hwh/AutoCrawler}","sentences":["Web automation is a significant technique that accomplishes complicated web tasks by automating common web actions, enhancing operational efficiency, and reducing the need for manual intervention.","Traditional methods, such as wrappers, suffer from limited adaptability and scalability when faced with a new website.","On the other hand, generative agents empowered by large language models (LLMs) exhibit poor performance and reusability in open-world scenarios.","In this work, we introduce a crawler generation task for vertical information web pages and the paradigm of combining LLMs with crawlers, which helps crawlers handle diverse and changing web environments more efficiently.","We propose AutoCrawler, a two-stage framework that leverages the hierarchical structure of HTML for progressive understanding.","Through top-down and step-back operations, AutoCrawler can learn from erroneous actions and continuously prune HTML for better action generation.","We conduct comprehensive experiments with multiple LLMs and demonstrate the effectiveness of our framework.","Resources of this paper can be found at \\url{https://github.com/EZ-hwh/AutoCrawler}"],"url":"http://arxiv.org/abs/2404.12753v1","category":"cs.CL"}
{"created":"2024-04-19 09:57:06","title":"Immersive Analysis: Enhancing Material Inspection of X-Ray Computed Tomography Datasets in Augmented Reality","abstract":"This work introduces a novel Augmented Reality (AR) approach to visualize material data alongside real objects in order to facilitate detailed material analyses based on spatial non-destructive testing (NDT) data as generated in X-ray computed tomography (XCT) imaging. For this purpose, we introduce a framework that leverages the potential of AR devices, visualization and interaction techniques to seamlessly explore complex primary and secondary XCT data matched with real-world objects. The overall goal of the proposed analysis scheme is to enable researchers and analysts to inspect material properties and structures onsite and in-place. Coupling immersive visualization techniques with real physical objects allows for highly intuitive workflows in material analysis and inspection, which enables the identification of anomalies and accelerates informed decision making. As a result, this framework generates an immersive experience, which provides a more engaging and more natural analysis of material data. A case study on fiber-reinforced polymer datasets was used to validate the AR framework and its new workflow. Initial results revealed positive feedback from experts, in particular regarding improved understanding of spatial data and a more natural interaction with material samples, which may have significant potential when combined with conventional analysis systems.","sentences":["This work introduces a novel Augmented Reality (AR) approach to visualize material data alongside real objects in order to facilitate detailed material analyses based on spatial non-destructive testing (NDT) data as generated in X-ray computed tomography (XCT) imaging.","For this purpose, we introduce a framework that leverages the potential of AR devices, visualization and interaction techniques to seamlessly explore complex primary and secondary XCT data matched with real-world objects.","The overall goal of the proposed analysis scheme is to enable researchers and analysts to inspect material properties and structures onsite and in-place.","Coupling immersive visualization techniques with real physical objects allows for highly intuitive workflows in material analysis and inspection, which enables the identification of anomalies and accelerates informed decision making.","As a result, this framework generates an immersive experience, which provides a more engaging and more natural analysis of material data.","A case study on fiber-reinforced polymer datasets was used to validate the AR framework and its new workflow.","Initial results revealed positive feedback from experts, in particular regarding improved understanding of spatial data and a more natural interaction with material samples, which may have significant potential when combined with conventional analysis systems."],"url":"http://arxiv.org/abs/2404.12751v1","category":"cs.HC"}
{"created":"2024-04-19 09:44:51","title":"Beyond Human Norms: Unveiling Unique Values of Large Language Models through Interdisciplinary Approaches","abstract":"Recent advancements in Large Language Models (LLMs) have revolutionized the AI field but also pose potential safety and ethical risks. Deciphering LLMs' embedded values becomes crucial for assessing and mitigating their risks. Despite extensive investigation into LLMs' values, previous studies heavily rely on human-oriented value systems in social sciences. Then, a natural question arises: Do LLMs possess unique values beyond those of humans? Delving into it, this work proposes a novel framework, ValueLex, to reconstruct LLMs' unique value system from scratch, leveraging psychological methodologies from human personality/value research. Based on Lexical Hypothesis, ValueLex introduces a generative approach to elicit diverse values from 30+ LLMs, synthesizing a taxonomy that culminates in a comprehensive value framework via factor analysis and semantic clustering. We identify three core value dimensions, Competence, Character, and Integrity, each with specific subdimensions, revealing that LLMs possess a structured, albeit non-human, value system. Based on this system, we further develop tailored projective tests to evaluate and analyze the value inclinations of LLMs across different model sizes, training methods, and data sources. Our framework fosters an interdisciplinary paradigm of understanding LLMs, paving the way for future AI alignment and regulation.","sentences":["Recent advancements in Large Language Models (LLMs) have revolutionized the AI field but also pose potential safety and ethical risks.","Deciphering LLMs' embedded values becomes crucial for assessing and mitigating their risks.","Despite extensive investigation into LLMs' values, previous studies heavily rely on human-oriented value systems in social sciences.","Then, a natural question arises: Do LLMs possess unique values beyond those of humans?","Delving into it, this work proposes a novel framework, ValueLex, to reconstruct LLMs' unique value system from scratch, leveraging psychological methodologies from human personality/value research.","Based on Lexical Hypothesis, ValueLex introduces a generative approach to elicit diverse values from 30+ LLMs, synthesizing a taxonomy that culminates in a comprehensive value framework via factor analysis and semantic clustering.","We identify three core value dimensions, Competence, Character, and Integrity, each with specific subdimensions, revealing that LLMs possess a structured, albeit non-human, value system.","Based on this system, we further develop tailored projective tests to evaluate and analyze the value inclinations of LLMs across different model sizes, training methods, and data sources.","Our framework fosters an interdisciplinary paradigm of understanding LLMs, paving the way for future AI alignment and regulation."],"url":"http://arxiv.org/abs/2404.12744v1","category":"cs.CL"}
{"created":"2024-04-19 09:32:16","title":"The Solution for the CVPR2024 NICE Image Captioning Challenge","abstract":"This report introduces a solution to the Topic 1 Zero-shot Image Captioning of 2024 NICE : New frontiers for zero-shot Image Captioning Evaluation. In contrast to NICE 2023 datasets, this challenge involves new annotations by humans with significant differences in caption style and content. Therefore, we enhance image captions effectively through retrieval augmentation and caption grading methods. At the data level, we utilize high-quality captions generated by image caption models as training data to address the gap in text styles. At the model level, we employ OFA (a large-scale visual-language pre-training model based on handcrafted templates) to perform the image captioning task. Subsequently, we propose caption-level strategy for the high-quality caption data generated by the image caption models and integrate them with retrieval augmentation strategy into the template to compel the model to generate higher quality, more matching, and semantically enriched captions based on the retrieval augmentation prompts. Our approach ranks first on the leaderboard, achieving a CIDEr score of 234.11 and 1st in all other metrics.","sentences":["This report introduces a solution to the Topic 1 Zero-shot Image Captioning of 2024 NICE : New frontiers for zero-shot Image Captioning Evaluation.","In contrast to NICE 2023 datasets, this challenge involves new annotations by humans with significant differences in caption style and content.","Therefore, we enhance image captions effectively through retrieval augmentation and caption grading methods.","At the data level, we utilize high-quality captions generated by image caption models as training data to address the gap in text styles.","At the model level, we employ OFA (a large-scale visual-language pre-training model based on handcrafted templates) to perform the image captioning task.","Subsequently, we propose caption-level strategy for the high-quality caption data generated by the image caption models and integrate them with retrieval augmentation strategy into the template to compel the model to generate higher quality, more matching, and semantically enriched captions based on the retrieval augmentation prompts.","Our approach ranks first on the leaderboard, achieving a CIDEr score of 234.11 and 1st in all other metrics."],"url":"http://arxiv.org/abs/2404.12739v1","category":"cs.CV"}
{"created":"2024-04-19 09:29:53","title":"Large Language Model Supply Chain: A Research Agenda","abstract":"The rapid advancements in pre-trained Large Language Models (LLMs) and Large Multimodal Models (LMMs) have ushered in a new era of intelligent applications, transforming fields ranging from natural language processing to content generation. The LLM supply chain represents a crucial aspect of the contemporary artificial intelligence landscape. It encompasses the entire lifecycle of pre-trained models, from its initial development and training to its final deployment and application in various domains. This paper presents a comprehensive overview of the LLM supply chain, highlighting its three core elements: 1) the model infrastructure, encompassing datasets and toolchain for training, optimization, and deployment; 2) the model lifecycle, covering training, testing, releasing, and ongoing maintenance; and 3) the downstream application ecosystem, enabling the integration of pre-trained models into a wide range of intelligent applications. However, this rapidly evolving field faces numerous challenges across these key components, including data privacy and security, model interpretability and fairness, infrastructure scalability, and regulatory compliance. Addressing these challenges is essential for harnessing the full potential of LLMs and ensuring their ethical and responsible use. This paper provides a future research agenda for the LLM supply chain, aiming at driving the continued advancement and responsible deployment of these transformative LLMs.","sentences":["The rapid advancements in pre-trained Large Language Models (LLMs) and Large Multimodal Models (LMMs) have ushered in a new era of intelligent applications, transforming fields ranging from natural language processing to content generation.","The LLM supply chain represents a crucial aspect of the contemporary artificial intelligence landscape.","It encompasses the entire lifecycle of pre-trained models, from its initial development and training to its final deployment and application in various domains.","This paper presents a comprehensive overview of the LLM supply chain, highlighting its three core elements: 1) the model infrastructure, encompassing datasets and toolchain for training, optimization, and deployment; 2) the model lifecycle, covering training, testing, releasing, and ongoing maintenance; and 3) the downstream application ecosystem, enabling the integration of pre-trained models into a wide range of intelligent applications.","However, this rapidly evolving field faces numerous challenges across these key components, including data privacy and security, model interpretability and fairness, infrastructure scalability, and regulatory compliance.","Addressing these challenges is essential for harnessing the full potential of LLMs and ensuring their ethical and responsible use.","This paper provides a future research agenda for the LLM supply chain, aiming at driving the continued advancement and responsible deployment of these transformative LLMs."],"url":"http://arxiv.org/abs/2404.12736v1","category":"cs.SE"}
{"created":"2024-04-19 09:28:48","title":"Enumerating low-frequency nonphononic vibrations in computer glasses","abstract":"In addition to Goldstone phonons that generically emerge in the low-frequency vibrational spectrum of any solid, crystalline or glassy, structural glasses also feature other low-frequency vibrational modes. The nature and statistical properties of these modes -- often termed `excess modes' -- have been the subject of decades-long investigation. Studying them, even using well-controlled computer glasses, has proven challenging due to strong spatial hybridization effects between phononic and nonphononic excitations, which hinder quantitative analyses of the nonphononic contribution ${\\cal D}_{\\rm G}(\\omega)$ to the total spectrum ${\\cal D}(\\omega)$, per frequency $\\omega$. Here, using recent advances indicating that ${\\cal D}_{\\rm G}(\\omega)\\!=\\!{\\cal D}(\\omega)-{\\cal D}_{\\rm D}(\\omega)$, where ${\\cal D}_{\\rm D}(\\omega)$ is Debye's spectrum of phonons, we present a simple and straightforward scheme to enumerate nonphononic modes in computer glasses. Our analysis establishes that nonphononic modes in computer glasses indeed make an additive contribution to the total spectrum, including in the presence of strong hybridizations. Moreover, it cleanly reveals the universal ${\\cal D}_{\\rm G}(\\omega)\\!\\sim\\!\\omega^4$ tail of the nonphononic spectrum, and opens the way for related analyses of experimental spectra of glasses.","sentences":["In addition to Goldstone phonons that generically emerge in the low-frequency vibrational spectrum of any solid, crystalline or glassy, structural glasses also feature other low-frequency vibrational modes.","The nature and statistical properties of these modes -- often termed `excess modes' -- have been the subject of decades-long investigation.","Studying them, even using well-controlled computer glasses, has proven challenging due to strong spatial hybridization effects between phononic and nonphononic excitations, which hinder quantitative analyses of the nonphononic contribution ${\\cal D}_{\\rm G}(\\omega)$ to the total spectrum ${\\cal D}(\\omega)$, per frequency $\\omega$. Here, using recent advances indicating that ${\\cal D}_{\\rm G}(\\omega)\\!=\\!{\\cal D}(\\omega)-{\\cal D}_{\\rm D}(\\omega)$, where ${\\cal D}_{\\rm D}(\\omega)$ is Debye's spectrum of phonons, we present a simple and straightforward scheme to enumerate nonphononic modes in computer glasses.","Our analysis establishes that nonphononic modes in computer glasses indeed make an additive contribution to the total spectrum, including in the presence of strong hybridizations.","Moreover, it cleanly reveals the universal ${\\cal D}_{\\rm G}(\\omega)\\!\\sim\\!\\omega^4$ tail of the nonphononic spectrum, and opens the way for related analyses of experimental spectra of glasses."],"url":"http://arxiv.org/abs/2404.12735v1","category":"cond-mat.soft"}
{"created":"2024-04-19 09:28:16","title":"DLoRA-TrOCR: Mixed Text Mode Optical Character Recognition Based On Transformer","abstract":"With the continuous development of OCR technology and the expansion of application fields, text recognition in complex scenes has become a key challenge. Factors such as multiple fonts, mixed scenes and complex layouts seriously affect the recognition accuracy of traditional OCR models. Although OCR models based on deep learning have performed well in specific fields or similar data sets in recent years, the generalization ability and robustness of the model are still a big challenge when facing complex environments with multiple scenes. Furthermore, training an OCR model from scratch or fine-tuning all parameters is very demanding on computing resources and inference time, which limits the flexibility of its application. This study focuses on a fundamental aspect of mixed text recognition in response to the challenges mentioned above, which involves effectively fine-tuning the pre-trained basic OCR model to demonstrate exceptional performance across various downstream tasks. To this end, we propose a parameter-efficient hybrid text recognition method based on pre-trained OCR Transformer, namely DLoRA-TrOCR. This method embeds DoRA into the image encoder and LoRA into the internal structure of the text decoder, enabling efficient parameter fine-tuning for downstream tasks. Experimental results show that compared to similar parameter adjustment methods, our model DLoRA-TrOCR has the smallest number of parameters and performs better. It can achieve state-of-the-art performance on complex scene data sets involving simultaneous recognition of mixed handwritten, printed and street view texts.","sentences":["With the continuous development of OCR technology and the expansion of application fields, text recognition in complex scenes has become a key challenge.","Factors such as multiple fonts, mixed scenes and complex layouts seriously affect the recognition accuracy of traditional OCR models.","Although OCR models based on deep learning have performed well in specific fields or similar data sets in recent years, the generalization ability and robustness of the model are still a big challenge when facing complex environments with multiple scenes.","Furthermore, training an OCR model from scratch or fine-tuning all parameters is very demanding on computing resources and inference time, which limits the flexibility of its application.","This study focuses on a fundamental aspect of mixed text recognition in response to the challenges mentioned above, which involves effectively fine-tuning the pre-trained basic OCR model to demonstrate exceptional performance across various downstream tasks.","To this end, we propose a parameter-efficient hybrid text recognition method based on pre-trained OCR Transformer, namely DLoRA-TrOCR.","This method embeds DoRA into the image encoder and LoRA into the internal structure of the text decoder, enabling efficient parameter fine-tuning for downstream tasks.","Experimental results show that compared to similar parameter adjustment methods, our model DLoRA-TrOCR has the smallest number of parameters and performs better.","It can achieve state-of-the-art performance on complex scene data sets involving simultaneous recognition of mixed handwritten, printed and street view texts."],"url":"http://arxiv.org/abs/2404.12734v1","category":"cs.CV"}
{"created":"2024-04-19 09:22:20","title":"PATE-TripleGAN: Privacy-Preserving Image Synthesis with Gaussian Differential Privacy","abstract":"Conditional Generative Adversarial Networks (CGANs) exhibit significant potential in supervised learning model training by virtue of their ability to generate realistic labeled images. However, numerous studies have indicated the privacy leakage risk in CGANs models. The solution DPCGAN, incorporating the differential privacy framework, faces challenges such as heavy reliance on labeled data for model training and potential disruptions to original gradient information due to excessive gradient clipping, making it difficult to ensure model accuracy. To address these challenges, we present a privacy-preserving training framework called PATE-TripleGAN. This framework incorporates a classifier to pre-classify unlabeled data, establishing a three-party min-max game to reduce dependence on labeled data. Furthermore, we present a hybrid gradient desensitization algorithm based on the Private Aggregation of Teacher Ensembles (PATE) framework and Differential Private Stochastic Gradient Descent (DPSGD) method. This algorithm allows the model to retain gradient information more effectively while ensuring privacy protection, thereby enhancing the model's utility. Privacy analysis and extensive experiments affirm that the PATE-TripleGAN model can generate a higher quality labeled image dataset while ensuring the privacy of the training data.","sentences":["Conditional Generative Adversarial Networks (CGANs) exhibit significant potential in supervised learning model training by virtue of their ability to generate realistic labeled images.","However, numerous studies have indicated the privacy leakage risk in CGANs models.","The solution DPCGAN, incorporating the differential privacy framework, faces challenges such as heavy reliance on labeled data for model training and potential disruptions to original gradient information due to excessive gradient clipping, making it difficult to ensure model accuracy.","To address these challenges, we present a privacy-preserving training framework called PATE-TripleGAN.","This framework incorporates a classifier to pre-classify unlabeled data, establishing a three-party min-max game to reduce dependence on labeled data.","Furthermore, we present a hybrid gradient desensitization algorithm based on the Private Aggregation of Teacher Ensembles (PATE) framework and Differential Private Stochastic Gradient Descent (DPSGD) method.","This algorithm allows the model to retain gradient information more effectively while ensuring privacy protection, thereby enhancing the model's utility.","Privacy analysis and extensive experiments affirm that the PATE-TripleGAN model can generate a higher quality labeled image dataset while ensuring the privacy of the training data."],"url":"http://arxiv.org/abs/2404.12730v1","category":"cs.CV"}
{"created":"2024-04-19 09:15:07","title":"Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?","abstract":"Analogical reasoning is a unique ability of humans to address unfamiliar challenges by transferring strategies from relevant past experiences. One key finding in psychology is that compared with irrelevant past experiences, recalling relevant ones can help humans better handle new tasks. Coincidentally, the NLP community has also recently found that self-generating relevant examples in the context can help large language models (LLMs) better solve a given problem than hand-crafted prompts. However, it is yet not clear whether relevance is the key factor eliciting such capability, i.e., can LLMs benefit more from self-generated relevant examples than irrelevant ones? In this work, we systematically explore whether LLMs can truly perform analogical reasoning on a diverse set of reasoning tasks. With extensive experiments and analysis, we show that self-generated random examples can surprisingly achieve comparable or even better performance, e.g., 4% performance boost on GSM8K with random biological examples. We find that the accuracy of self-generated examples is the key factor and subsequently design two improved methods with significantly reduced inference costs. Overall, we aim to advance a deeper understanding of LLM analogical reasoning and hope this work stimulates further research in the design of self-generated contexts.","sentences":["Analogical reasoning is a unique ability of humans to address unfamiliar challenges by transferring strategies from relevant past experiences.","One key finding in psychology is that compared with irrelevant past experiences, recalling relevant ones can help humans better handle new tasks.","Coincidentally, the NLP community has also recently found that self-generating relevant examples in the context can help large language models (LLMs) better solve a given problem than hand-crafted prompts.","However, it is yet not clear whether relevance is the key factor eliciting such capability, i.e., can LLMs benefit more from self-generated relevant examples than irrelevant ones?","In this work, we systematically explore whether LLMs can truly perform analogical reasoning on a diverse set of reasoning tasks.","With extensive experiments and analysis, we show that self-generated random examples can surprisingly achieve comparable or even better performance, e.g., 4% performance boost on GSM8K with random biological examples.","We find that the accuracy of self-generated examples is the key factor and subsequently design two improved methods with significantly reduced inference costs.","Overall, we aim to advance a deeper understanding of LLM analogical reasoning and hope this work stimulates further research in the design of self-generated contexts."],"url":"http://arxiv.org/abs/2404.12728v1","category":"cs.CL"}
{"created":"2024-04-19 09:13:19","title":"Characterizations of open and semi-open maps of compact Hausdorff spaces by induced maps","abstract":"Let $f\\colon X\\rightarrow Y$ be a continuous surjection of compact Hausdorff spaces. By $$f_*\\colon\\mathfrak{M}(X)\\rightarrow\\mathfrak{M}(Y),\\ \\mu\\mapsto \\mu\\circ f^{-1} \\quad{\\rm and}\\quad 2^f\\colon2^X\\rightarrow2^Y,\\ A\\mapsto f[A]$$ we denote the induced continuous surjections on the probability measure spaces and hyperspaces, respectively. In this paper we mainly show the following facts:   (1) If $f_*$ is semi-open, then $f$ is semi-open.   (2) If $f$ is semi-open densely open, then $f_*$ is semi-open densely open.   (3) $f$ is open iff $2^f$ is open.   (4) $f$ is semi-open iff $2^f$ is semi-open.   (5) $f$ is irreducible iff $2^f$ is irreducible.","sentences":["Let $f\\colon X\\rightarrow Y$ be a continuous surjection of compact Hausdorff spaces.","By $$f_*\\colon\\mathfrak{M}(X)\\rightarrow\\mathfrak{M}(Y),\\ \\mu\\mapsto \\mu\\circ f^{-1} \\quad{\\rm and}\\quad 2^f\\colon2^X\\rightarrow2^Y,\\ A\\mapsto","f[A]$$ we denote the induced continuous surjections on the probability measure spaces and hyperspaces, respectively.","In this paper we mainly show the following facts:   (1) If $f_*$ is semi-open, then $f$ is semi-open.   ","(2) If $f$ is semi-open densely open, then $f_*$ is semi-open densely open.   ","(3) $f$ is open iff","$2^f$ is open.   ","(4) $f$ is semi-open iff $2^f$ is semi-open.   ","(5) $f$ is irreducible iff","$2^f$ is irreducible."],"url":"http://arxiv.org/abs/2404.12727v1","category":"math.DS"}
{"created":"2024-04-19 09:10:29","title":"Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works","abstract":"Large language models (LLMs) have demonstrated impressive performance and spurred numerous AI applications, in which role-playing agents (RPAs) are particularly popular, especially for fictional characters. The prerequisite for these RPAs lies in the capability of LLMs to understand characters from fictional works. Previous efforts have evaluated this capability via basic classification tasks or characteristic imitation, failing to capture the nuanced character understanding with LLMs. In this paper, we propose evaluating LLMs' character understanding capability via the character profiling task, i.e., summarizing character profiles from corresponding materials, a widely adopted yet understudied practice for RPA development. Specifically, we construct the CroSS dataset from literature experts and assess the generated profiles by comparing ground truth references and their applicability in downstream tasks. Our experiments, which cover various summarization methods and LLMs, have yielded promising results. These results strongly validate the character understanding capability of LLMs. We believe our constructed resource will promote further research in this field. Resources are available at https://github.com/Joanna0123/character_profiling.","sentences":["Large language models (LLMs) have demonstrated impressive performance and spurred numerous AI applications, in which role-playing agents (RPAs) are particularly popular, especially for fictional characters.","The prerequisite for these RPAs lies in the capability of LLMs to understand characters from fictional works.","Previous efforts have evaluated this capability via basic classification tasks or characteristic imitation, failing to capture the nuanced character understanding with LLMs.","In this paper, we propose evaluating LLMs' character understanding capability via the character profiling task, i.e., summarizing character profiles from corresponding materials, a widely adopted yet understudied practice for RPA development.","Specifically, we construct the CroSS dataset from literature experts and assess the generated profiles by comparing ground truth references and their applicability in downstream tasks.","Our experiments, which cover various summarization methods and LLMs, have yielded promising results.","These results strongly validate the character understanding capability of LLMs.","We believe our constructed resource will promote further research in this field.","Resources are available at https://github.com/Joanna0123/character_profiling."],"url":"http://arxiv.org/abs/2404.12726v1","category":"cs.CL"}
{"created":"2024-04-19 09:08:44","title":"Separate in the Speech Chain: Cross-Modal Conditional Audio-Visual Target Speech Extraction","abstract":"The integration of visual cues has revitalized the performance of the target speech extraction task, elevating it to the forefront of the field. Nevertheless, this multi-modal learning paradigm often encounters the challenge of modality imbalance. In audio-visual target speech extraction tasks, the audio modality tends to dominate, potentially overshadowing the importance of visual guidance. To tackle this issue, we propose AVSepChain, drawing inspiration from the speech chain concept. Our approach partitions the audio-visual target speech extraction task into two stages: speech perception and speech production. In the speech perception stage, audio serves as the dominant modality, while visual information acts as the conditional modality. Conversely, in the speech production stage, the roles are reversed. This transformation of modality status aims to alleviate the problem of modality imbalance. Additionally, we introduce a contrastive semantic matching loss to ensure that the semantic information conveyed by the generated speech aligns with the semantic information conveyed by lip movements during the speech production stage. Through extensive experiments conducted on multiple benchmark datasets for audio-visual target speech extraction, we showcase the superior performance achieved by our proposed method.","sentences":["The integration of visual cues has revitalized the performance of the target speech extraction task, elevating it to the forefront of the field.","Nevertheless, this multi-modal learning paradigm often encounters the challenge of modality imbalance.","In audio-visual target speech extraction tasks, the audio modality tends to dominate, potentially overshadowing the importance of visual guidance.","To tackle this issue, we propose AVSepChain, drawing inspiration from the speech chain concept.","Our approach partitions the audio-visual target speech extraction task into two stages: speech perception and speech production.","In the speech perception stage, audio serves as the dominant modality, while visual information acts as the conditional modality.","Conversely, in the speech production stage, the roles are reversed.","This transformation of modality status aims to alleviate the problem of modality imbalance.","Additionally, we introduce a contrastive semantic matching loss to ensure that the semantic information conveyed by the generated speech aligns with the semantic information conveyed by lip movements during the speech production stage.","Through extensive experiments conducted on multiple benchmark datasets for audio-visual target speech extraction, we showcase the superior performance achieved by our proposed method."],"url":"http://arxiv.org/abs/2404.12725v1","category":"cs.SD"}
{"created":"2024-04-19 09:01:58","title":"Generalized Few-Shot Meets Remote Sensing: Discovering Novel Classes in Land Cover Mapping via Hybrid Semantic Segmentation Framework","abstract":"Land-cover mapping is one of the vital applications in Earth observation, aiming at classifying each pixel's land-cover type of remote-sensing images. As natural and human activities change the landscape, the land-cover map needs to be rapidly updated. However, discovering newly appeared land-cover types in existing classification systems is still a non-trivial task hindered by various scales of complex land objects and insufficient labeled data over a wide-span geographic area. In this paper, we propose a generalized few-shot segmentation-based framework, named SegLand, to update novel classes in high-resolution land-cover mapping. Specifically, the proposed framework is designed in three parts: (a) Data pre-processing: the base training set and the few-shot support sets of novel classes are analyzed and augmented; (b) Hybrid segmentation structure; Multiple base learners and a modified Projection onto Orthogonal Prototypes (POP) network are combined to enhance the base-class recognition and to dig novel classes from insufficient labels data; (c) Ultimate fusion: the semantic segmentation results of the base learners and POP network are reasonably fused. The proposed framework has won first place in the leaderboard of the OpenEarthMap Land Cover Mapping Few-Shot Challenge. Experiments demonstrate the superiority of the framework for automatically updating novel land-cover classes with limited labeled data.","sentences":["Land-cover mapping is one of the vital applications in Earth observation, aiming at classifying each pixel's land-cover type of remote-sensing images.","As natural and human activities change the landscape, the land-cover map needs to be rapidly updated.","However, discovering newly appeared land-cover types in existing classification systems is still a non-trivial task hindered by various scales of complex land objects and insufficient labeled data over a wide-span geographic area.","In this paper, we propose a generalized few-shot segmentation-based framework, named SegLand, to update novel classes in high-resolution land-cover mapping.","Specifically, the proposed framework is designed in three parts: (a) Data pre-processing: the base training set and the few-shot support sets of novel classes are analyzed and augmented; (b) Hybrid segmentation structure; Multiple base learners and a modified Projection onto Orthogonal Prototypes (POP) network are combined to enhance the base-class recognition and to dig novel classes from insufficient labels data; (c) Ultimate fusion: the semantic segmentation results of the base learners and POP network are reasonably fused.","The proposed framework has won first place in the leaderboard of the OpenEarthMap Land Cover Mapping Few-Shot Challenge.","Experiments demonstrate the superiority of the framework for automatically updating novel land-cover classes with limited labeled data."],"url":"http://arxiv.org/abs/2404.12721v1","category":"cs.CV"}
{"created":"2024-04-19 08:58:53","title":"Improving Prediction Accuracy of Semantic Segmentation Methods Using Convolutional Autoencoder Based Pre-processing Layers","abstract":"In this paper, we propose a method to improve prediction accuracy of semantic segmentation methods as follows: (1) construct a neural network that has pre-processing layers based on a convolutional autoencoder ahead of a semantic segmentation network, and (2) train the entire network initialized by the weights of the pre-trained autoencoder. We applied this method to the fully convolutional network (FCN) and experimentally compared its prediction accuracy on the cityscapes dataset. The Mean IoU of the proposed target model with the He normal initialization is 18.7% higher than that of FCN with the He normal initialization. In addition, those of the modified models of the target model are significantly higher than that of FCN with the He normal initialization. The accuracy and loss curves during the training showed that these are resulting from the improvement of the generalization ability. All of these results provide strong evidence that the proposed method is significantly effective in improving the prediction accuracy of FCN. The proposed method has the following features: it is comparatively simple, whereas the effect on improving the generalization ability and prediction accuracy of FCN is significant; the increase in the number of parameters by using it is very small, and that in the computation time is substantially large. In principle, the proposed method can be applied to other semantic segmentation methods. For semantic segmentation, at present, there is no effective way to improve the prediction accuracy of existing methods. None have published a method which is the same as or similar to our method and none have used such a method in practice. Therefore, we believe that our method is useful in practice and worthy of being widely known and used.","sentences":["In this paper, we propose a method to improve prediction accuracy of semantic segmentation methods as follows: (1) construct a neural network that has pre-processing layers based on a convolutional autoencoder ahead of a semantic segmentation network, and (2) train the entire network initialized by the weights of the pre-trained autoencoder.","We applied this method to the fully convolutional network (FCN) and experimentally compared its prediction accuracy on the cityscapes dataset.","The Mean IoU of the proposed target model with the He normal initialization is 18.7% higher than that of FCN with the He normal initialization.","In addition, those of the modified models of the target model are significantly higher than that of FCN with the He normal initialization.","The accuracy and loss curves during the training showed that these are resulting from the improvement of the generalization ability.","All of these results provide strong evidence that the proposed method is significantly effective in improving the prediction accuracy of FCN.","The proposed method has the following features: it is comparatively simple, whereas the effect on improving the generalization ability and prediction accuracy of FCN is significant; the increase in the number of parameters by using it is very small, and that in the computation time is substantially large.","In principle, the proposed method can be applied to other semantic segmentation methods.","For semantic segmentation, at present, there is no effective way to improve the prediction accuracy of existing methods.","None have published a method which is the same as or similar to our method and none have used such a method in practice.","Therefore, we believe that our method is useful in practice and worthy of being widely known and used."],"url":"http://arxiv.org/abs/2404.12718v1","category":"cs.CV"}
{"created":"2024-04-19 08:58:52","title":"Show and Grasp: Few-shot Semantic Segmentation for Robot Grasping through Zero-shot Foundation Models","abstract":"The ability of a robot to pick an object, known as robot grasping, is crucial for several applications, such as assembly or sorting. In such tasks, selecting the right target to pick is as essential as inferring a correct configuration of the gripper. A common solution to this problem relies on semantic segmentation models, which often show poor generalization to unseen objects and require considerable time and massive data to be trained. To reduce the need for large datasets, some grasping pipelines exploit few-shot semantic segmentation models, which are capable of recognizing new classes given a few examples. However, this often comes at the cost of limited performance and fine-tuning is required to be effective in robot grasping scenarios. In this work, we propose to overcome all these limitations by combining the impressive generalization capability reached by foundation models with a high-performing few-shot classifier, working as a score function to select the segmentation that is closer to the support set. The proposed model is designed to be embedded in a grasp synthesis pipeline. The extensive experiments using one or five examples show that our novel approach overcomes existing performance limitations, improving the state of the art both in few-shot semantic segmentation on the Graspnet-1B (+10.5% mIoU) and Ocid-grasp (+1.6% AP) datasets, and real-world few-shot grasp synthesis (+21.7% grasp accuracy). The project page is available at: https://leobarcellona.github.io/showandgrasp.github.io/","sentences":["The ability of a robot to pick an object, known as robot grasping, is crucial for several applications, such as assembly or sorting.","In such tasks, selecting the right target to pick is as essential as inferring a correct configuration of the gripper.","A common solution to this problem relies on semantic segmentation models, which often show poor generalization to unseen objects and require considerable time and massive data to be trained.","To reduce the need for large datasets, some grasping pipelines exploit few-shot semantic segmentation models, which are capable of recognizing new classes given a few examples.","However, this often comes at the cost of limited performance and fine-tuning is required to be effective in robot grasping scenarios.","In this work, we propose to overcome all these limitations by combining the impressive generalization capability reached by foundation models with a high-performing few-shot classifier, working as a score function to select the segmentation that is closer to the support set.","The proposed model is designed to be embedded in a grasp synthesis pipeline.","The extensive experiments using one or five examples show that our novel approach overcomes existing performance limitations, improving the state of the art both in few-shot semantic segmentation on the Graspnet-1B (+10.5% mIoU) and Ocid-grasp (+1.6% AP) datasets, and real-world few-shot grasp synthesis (+21.7% grasp accuracy).","The project page is available at: https://leobarcellona.github.io/showandgrasp.github.io/"],"url":"http://arxiv.org/abs/2404.12717v1","category":"cs.RO"}
{"created":"2024-04-19 08:52:22","title":"Enabling Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel Collaboration","abstract":"Large language models (LLMs) have shown complementary strengths in various tasks and instances, motivating the research of ensembling LLMs to push the frontier leveraging the wisdom of the crowd. Existing work achieves this objective via training the extra reward model or fusion model to select or fuse all candidate answers. However, these methods pose a great challenge to the generalizability of the trained models. Besides, existing methods use the textual responses as communication media, ignoring the rich information in the inner representations of neural networks. Therefore, we propose a training-free ensemble framework DEEPEN, averaging the probability distributions outputted by different LLMs. A key challenge in this paradigm is the vocabulary discrepancy between heterogeneous LLMs, which hinders the operation of probability distribution averaging. To address this challenge, DEEPEN maps the probability distribution of each model from the probability space to a universe relative space based on the relative representation theory, and performs aggregation. Then, the result of aggregation is mapped back to the probability space of one LLM via a search-based inverse transformation to determine the generated token. We conduct experiments on the ensemble of various LLMs of 6B to 70B. Experimental results show that DEEPEN achieves consistent improvements across six popular benchmarks involving subject examination, reasoning and knowledge-QA, proving the effectiveness of our approach.","sentences":["Large language models (LLMs) have shown complementary strengths in various tasks and instances, motivating the research of ensembling LLMs to push the frontier leveraging the wisdom of the crowd.","Existing work achieves this objective via training the extra reward model or fusion model to select or fuse all candidate answers.","However, these methods pose a great challenge to the generalizability of the trained models.","Besides, existing methods use the textual responses as communication media, ignoring the rich information in the inner representations of neural networks.","Therefore, we propose a training-free ensemble framework DEEPEN, averaging the probability distributions outputted by different LLMs.","A key challenge in this paradigm is the vocabulary discrepancy between heterogeneous LLMs, which hinders the operation of probability distribution averaging.","To address this challenge, DEEPEN maps the probability distribution of each model from the probability space to a universe relative space based on the relative representation theory, and performs aggregation.","Then, the result of aggregation is mapped back to the probability space of one LLM via a search-based inverse transformation to determine the generated token.","We conduct experiments on the ensemble of various LLMs of 6B to 70B. Experimental results show that DEEPEN achieves consistent improvements across six popular benchmarks involving subject examination, reasoning and knowledge-QA, proving the effectiveness of our approach."],"url":"http://arxiv.org/abs/2404.12715v1","category":"cs.CL"}
{"created":"2024-04-19 08:46:33","title":"uTRAND: Unsupervised Anomaly Detection in Traffic Trajectories","abstract":"Deep learning-based approaches have achieved significant improvements on public video anomaly datasets, but often do not perform well in real-world applications. This paper addresses two issues: the lack of labeled data and the difficulty of explaining the predictions of a neural network. To this end, we present a framework called uTRAND, that shifts the problem of anomalous trajectory prediction from the pixel space to a semantic-topological domain. The framework detects and tracks all types of traffic agents in bird's-eye-view videos of traffic cameras mounted at an intersection. By conceptualizing the intersection as a patch-based graph, it is shown that the framework learns and models the normal behaviour of traffic agents without costly manual labeling. Furthermore, uTRAND allows to formulate simple rules to classify anomalous trajectories in a way suited for human interpretation. We show that uTRAND outperforms other state-of-the-art approaches on a dataset of anomalous trajectories collected in a real-world setting, while producing explainable detection results.","sentences":["Deep learning-based approaches have achieved significant improvements on public video anomaly datasets, but often do not perform well in real-world applications.","This paper addresses two issues: the lack of labeled data and the difficulty of explaining the predictions of a neural network.","To this end, we present a framework called uTRAND, that shifts the problem of anomalous trajectory prediction from the pixel space to a semantic-topological domain.","The framework detects and tracks all types of traffic agents in bird's-eye-view videos of traffic cameras mounted at an intersection.","By conceptualizing the intersection as a patch-based graph, it is shown that the framework learns and models the normal behaviour of traffic agents without costly manual labeling.","Furthermore, uTRAND allows to formulate simple rules to classify anomalous trajectories in a way suited for human interpretation.","We show that uTRAND outperforms other state-of-the-art approaches on a dataset of anomalous trajectories collected in a real-world setting, while producing explainable detection results."],"url":"http://arxiv.org/abs/2404.12712v1","category":"cs.CV"}
{"created":"2024-04-19 08:40:52","title":"Dynamic Temperature Knowledge Distillation","abstract":"Temperature plays a pivotal role in moderating label softness in the realm of knowledge distillation (KD). Traditional approaches often employ a static temperature throughout the KD process, which fails to address the nuanced complexities of samples with varying levels of difficulty and overlooks the distinct capabilities of different teacher-student pairings. This leads to a less-than-ideal transfer of knowledge. To improve the process of knowledge propagation, we proposed Dynamic Temperature Knowledge Distillation (DTKD) which introduces a dynamic, cooperative temperature control for both teacher and student models simultaneously within each training iterafion. In particular, we proposed \"\\textbf{sharpness}\" as a metric to quantify the smoothness of a model's output distribution. By minimizing the sharpness difference between the teacher and the student, we can derive sample-specific temperatures for them respectively. Extensive experiments on CIFAR-100 and ImageNet-2012 demonstrate that DTKD performs comparably to leading KD techniques, with added robustness in Target Class KD and None-target Class KD scenarios.The code is available at https://github.com/JinYu1998/DTKD.","sentences":["Temperature plays a pivotal role in moderating label softness in the realm of knowledge distillation (KD).","Traditional approaches often employ a static temperature throughout the KD process, which fails to address the nuanced complexities of samples with varying levels of difficulty and overlooks the distinct capabilities of different teacher-student pairings.","This leads to a less-than-ideal transfer of knowledge.","To improve the process of knowledge propagation, we proposed Dynamic Temperature Knowledge Distillation (DTKD) which introduces a dynamic, cooperative temperature control for both teacher and student models simultaneously within each training iterafion.","In particular, we proposed \"\\textbf{sharpness}\" as a metric to quantify the smoothness of a model's output distribution.","By minimizing the sharpness difference between the teacher and the student, we can derive sample-specific temperatures for them respectively.","Extensive experiments on CIFAR-100 and ImageNet-2012 demonstrate that DTKD performs comparably to leading KD techniques, with added robustness in Target Class KD and None-target Class KD scenarios.","The code is available at https://github.com/JinYu1998/DTKD."],"url":"http://arxiv.org/abs/2404.12711v1","category":"cs.LG"}
{"created":"2024-04-19 08:32:43","title":"Cosmology from String T-duality and zero-point length","abstract":"Inspired by String T-duality and taking into account the zero-point length correction, $l_0$, to the gravitational potential, we construct modified Friedmann equations by applying the first law of thermodynamics on the apparent horizon of the Friedmann-Robertson-Walker (FRW) Universe. The cosmological viability of this extended scenario is investigated by studying influences on the evolution of the early Universe and performing the cosmographic analysis. Furthermore, we explore the inflationary paradigm under the slow-roll condition. By testing the model against observational data, the zero-point length is constrained around the Planck scale, in compliance with the original assumption from String T-duality. We also study the growth of density perturbations in the linear regime. It is shown that the zero-point length stands out as an alternative characterization for the broken-power-law spectrum, which provides a better fitting for the experimental measurements comparing to the simple power-law potential. Finally, we focus on implications for the primordial gravitational waves (PGWs) spectrum. Should the zero-point length be running over energy scales, as is the case for all parameters and coupling constants in quantum gravity under renormalization group considerations, deviations from General Relativity (GR) might be potentially tested through upcoming PGW observatories.","sentences":["Inspired by String T-duality and taking into account the zero-point length correction, $l_0$, to the gravitational potential, we construct modified Friedmann equations by applying the first law of thermodynamics on the apparent horizon of the Friedmann-Robertson-Walker (FRW) Universe.","The cosmological viability of this extended scenario is investigated by studying influences on the evolution of the early Universe and performing the cosmographic analysis.","Furthermore, we explore the inflationary paradigm under the slow-roll condition.","By testing the model against observational data, the zero-point length is constrained around the Planck scale, in compliance with the original assumption from String T-duality.","We also study the growth of density perturbations in the linear regime.","It is shown that the zero-point length stands out as an alternative characterization for the broken-power-law spectrum, which provides a better fitting for the experimental measurements comparing to the simple power-law potential.","Finally, we focus on implications for the primordial gravitational waves (PGWs) spectrum.","Should the zero-point length be running over energy scales, as is the case for all parameters and coupling constants in quantum gravity under renormalization group considerations, deviations from General Relativity (GR) might be potentially tested through upcoming PGW observatories."],"url":"http://arxiv.org/abs/2404.12707v1","category":"gr-qc"}
{"created":"2024-04-19 08:21:54","title":"A Clean-graph Backdoor Attack against Graph Convolutional Networks with Poisoned Label Only","abstract":"Graph Convolutional Networks (GCNs) have shown excellent performance in dealing with various graph structures such as node classification, graph classification and other tasks. However,recent studies have shown that GCNs are vulnerable to a novel threat known as backdoor attacks. However, all existing backdoor attacks in the graph domain require modifying the training samples to accomplish the backdoor injection, which may not be practical in many realistic scenarios where adversaries have no access to modify the training samples and may leads to the backdoor attack being detected easily. In order to explore the backdoor vulnerability of GCNs and create a more practical and stealthy backdoor attack method, this paper proposes a clean-graph backdoor attack against GCNs (CBAG) in the node classification task,which only poisons the training labels without any modification to the training samples, revealing that GCNs have this security vulnerability. Specifically, CBAG designs a new trigger exploration method to find important feature dimensions as the trigger patterns to improve the attack performance. By poisoning the training labels, a hidden backdoor is injected into the GCNs model. Experimental results show that our clean graph backdoor can achieve 99% attack success rate while maintaining the functionality of the GCNs model on benign samples.","sentences":["Graph Convolutional Networks (GCNs) have shown excellent performance in dealing with various graph structures such as node classification, graph classification and other tasks.","However,recent studies have shown that GCNs are vulnerable to a novel threat known as backdoor attacks.","However, all existing backdoor attacks in the graph domain require modifying the training samples to accomplish the backdoor injection, which may not be practical in many realistic scenarios where adversaries have no access to modify the training samples and may leads to the backdoor attack being detected easily.","In order to explore the backdoor vulnerability of GCNs and create a more practical and stealthy backdoor attack method, this paper proposes a clean-graph backdoor attack against GCNs (CBAG) in the node classification task,which only poisons the training labels without any modification to the training samples, revealing that GCNs have this security vulnerability.","Specifically, CBAG designs a new trigger exploration method to find important feature dimensions as the trigger patterns to improve the attack performance.","By poisoning the training labels, a hidden backdoor is injected into the GCNs model.","Experimental results show that our clean graph backdoor can achieve 99% attack success rate while maintaining the functionality of the GCNs model on benign samples."],"url":"http://arxiv.org/abs/2404.12704v1","category":"cs.AI"}
{"created":"2024-04-19 08:21:05","title":"GAL\u00c6XI: Solving complex compressible flows with high-order discontinuous Galerkin methods on accelerator-based systems","abstract":"This work presents GAL{\\AE}XI as a novel, energy-efficient flow solver for the simulation of compressible flows on unstructured meshes leveraging the parallel computing power of modern Graphics Processing Units (GPUs). GAL{\\AE}XI implements the high-order Discontinuous Galerkin Spectral Element Method (DGSEM) using shock capturing with a finite-volume subcell approach to ensure the stability of the high-order scheme near shocks. This work provides details on the general code design, the parallelization strategy, and the implementation approach for the compute kernels with a focus on the element local mappings between volume and surface data due to the unstructured mesh. GAL{\\AE}XI exhibits excellent strong scaling properties up to 1024 GPUs if each GPU is assigned a minimum of one million degrees of freedom degrees of freedom. To verify its implementation, a convergence study is performed that recovers the theoretical order of convergence of the implemented numerical schemes. Moreover, the solver is validated using both the incompressible and compressible formulation of the Taylor-Green-Vortex at a Mach number of 0.1 and 1.25, respectively. A mesh convergence study shows that the results converge to the high-fidelity reference solution and that the results match the original CPU implementation. Finally, GAL{\\AE}XI is applied to a large-scale wall-resolved large eddy simulation of a linear cascade of the NASA Rotor 37. Here, the supersonic region and shocks at the leading edge are captured accurately and robustly by the implemented shock-capturing approach. It is demonstrated that GAL{\\AE}XI requires less than half of the energy to carry out this simulation in comparison to the reference CPU implementation. This renders GAL{\\AE}XI as a potent tool for accurate and efficient simulations of compressible flows in the realm of exascale computing and the associated new HPC architectures.","sentences":["This work presents GAL{\\AE}XI as a novel, energy-efficient flow solver for the simulation of compressible flows on unstructured meshes leveraging the parallel computing power of modern Graphics Processing Units (GPUs).","GAL{\\AE}XI","implements the high-order Discontinuous Galerkin Spectral Element Method (DGSEM) using shock capturing with a finite-volume subcell approach to ensure the stability of the high-order scheme near shocks.","This work provides details on the general code design, the parallelization strategy, and the implementation approach for the compute kernels with a focus on the element local mappings between volume and surface data due to the unstructured mesh.","GAL{\\AE}XI exhibits excellent strong scaling properties up to 1024 GPUs if each GPU is assigned a minimum of one million degrees of freedom degrees of freedom.","To verify its implementation, a convergence study is performed that recovers the theoretical order of convergence of the implemented numerical schemes.","Moreover, the solver is validated using both the incompressible and compressible formulation of the Taylor-Green-Vortex at a Mach number of 0.1 and 1.25, respectively.","A mesh convergence study shows that the results converge to the high-fidelity reference solution and that the results match the original CPU implementation.","Finally, GAL{\\AE}XI is applied to a large-scale wall-resolved large eddy simulation of a linear cascade of the NASA Rotor 37.","Here, the supersonic region and shocks at the leading edge are captured accurately and robustly by the implemented shock-capturing approach.","It is demonstrated that GAL{\\AE}XI requires less than half of the energy to carry out this simulation in comparison to the reference CPU implementation.","This renders GAL{\\AE}XI as a potent tool for accurate and efficient simulations of compressible flows in the realm of exascale computing and the associated new HPC architectures."],"url":"http://arxiv.org/abs/2404.12703v1","category":"cs.MS"}
{"created":"2024-04-19 16:55:49","title":"The Hydrophobic Interaction Induced Strengthening of Hydrogen Bond in Water-DMSO Binary Mixture","abstract":"The lifetime of a hydrogen bond between water and dimethyl sulfoxide (DMSO) is found to be considerably longer than that between two water molecules in the neat water. This is counter-intuitive because the charge on the oxygen in DMSO is considerably less than that in water. Additionally, the strength of the water-dimethyl sulfoxide (w-D) hydrogen bond is found to be strongly composition dependent; the lifetime of the hydrogen bond is ten times larger at 30% over that at very low concentration. Using computer simulations, we perform microscopic structural and dynamic analysis to find that these anomalies arise at least partly from an action-at-a-distance effect where the attraction between the hydrophobic methyl groups results in self-aggregation of DMSO molecules that cages both rotational and linear motions of molecules involved. This is reflected in the observed strong correlation of the lifetime with the local coordination number of the associated methyl groups. The elongated w-D h-bond lifetime causes a slowdown of collective dynamics and affects the lifetime of the w-w h-bond. This nonlinear feedback mechanism explains the strong composition dependence of viscosity and anticipated to play a dominant role in many self-assemblies. Furthermore, the w-D hydrogen bond breaking mechanism changes from low to high DMSO concentration, a phenomenon not anticipated a priori. We introduce a new order parameter-based free energy surface of the bond breaking pathway. A two-dimensional transition state rate theory (TSRT) calculation is performed for the lifetime of the w-D h-bond that is found to be semi-quantitatively accurate","sentences":["The lifetime of a hydrogen bond between water and dimethyl sulfoxide (DMSO) is found to be considerably longer than that between two water molecules in the neat water.","This is counter-intuitive because the charge on the oxygen in DMSO is considerably less than that in water.","Additionally, the strength of the water-dimethyl sulfoxide (w-D) hydrogen bond is found to be strongly composition dependent; the lifetime of the hydrogen bond is ten times larger at 30% over that at very low concentration.","Using computer simulations, we perform microscopic structural and dynamic analysis to find that these anomalies arise at least partly from an action-at-a-distance effect where the attraction between the hydrophobic methyl groups results in self-aggregation of DMSO molecules that cages both rotational and linear motions of molecules involved.","This is reflected in the observed strong correlation of the lifetime with the local coordination number of the associated methyl groups.","The elongated w-D h-bond lifetime causes a slowdown of collective dynamics and affects the lifetime of the w-w h-bond.","This nonlinear feedback mechanism explains the strong composition dependence of viscosity and anticipated to play a dominant role in many self-assemblies.","Furthermore, the w-D hydrogen bond breaking mechanism changes from low to high DMSO concentration, a phenomenon not anticipated a priori.","We introduce a new order parameter-based free energy surface of the bond breaking pathway.","A two-dimensional transition state rate theory (TSRT) calculation is performed for the lifetime of the w-D h-bond that is found to be semi-quantitatively accurate"],"url":"http://arxiv.org/abs/2404.13001v1","category":"cond-mat.soft"}
{"created":"2024-04-19 16:18:16","title":"Ring-a-Pose: A Ring for Continuous Hand Pose Tracking","abstract":"We present Ring-a-Pose, a single untethered ring that tracks continuous 3D hand poses. Located in the center of the hand, the ring emits an inaudible acoustic signal that each hand pose reflects differently. Ring-a-Pose imposes minimal obtrusions on the hand, unlike multi-ring or glove systems. It is not affected by the choice of clothing that may cover wrist-worn systems. In a series of three user studies with a total of 30 participants, we evaluate Ring-a-Pose's performance on pose tracking and micro-finger gesture recognition. Without collecting any training data from a user, Ring-a-Pose tracks continuous hand poses with a joint error of 14.1mm. The joint error decreases to 10.3mm for fine-tuned user-dependent models. Ring-a-Pose recognizes 7-class micro-gestures with a 90.60% and 99.27% accuracy for user-independent and user-dependent models, respectively. Furthermore, the ring exhibits promising performance when worn on any finger. Ring-a-Pose enables the future of smart rings to track and recognize hand poses using relatively low-power acoustic sensing.","sentences":["We present Ring-a-Pose, a single untethered ring that tracks continuous 3D hand poses.","Located in the center of the hand, the ring emits an inaudible acoustic signal that each hand pose reflects differently.","Ring-a-Pose imposes minimal obtrusions on the hand, unlike multi-ring or glove systems.","It is not affected by the choice of clothing that may cover wrist-worn systems.","In a series of three user studies with a total of 30 participants, we evaluate Ring-a-Pose's performance on pose tracking and micro-finger gesture recognition.","Without collecting any training data from a user, Ring-a-Pose tracks continuous hand poses with a joint error of 14.1mm.","The joint error decreases to 10.3mm for fine-tuned user-dependent models.","Ring-a-Pose recognizes 7-class micro-gestures with a 90.60% and 99.27% accuracy for user-independent and user-dependent models, respectively.","Furthermore, the ring exhibits promising performance when worn on any finger.","Ring-a-Pose enables the future of smart rings to track and recognize hand poses using relatively low-power acoustic sensing."],"url":"http://arxiv.org/abs/2404.12980v1","category":"cs.HC"}
{"created":"2024-04-19 15:08:15","title":"Superradiant phase transition in a large interacting driven atomic ensemble in free space","abstract":"Atomic ensembles strongly interacting with light constitute rich quantum-optical many-body systems, with the potential for observing cooperative effects and dissipative nonequilibrium phase transitions. We theoretically analyze the conditions under which a driven atomic ensemble in free space, characterized by strong dipole-dipole interactions and large spatial extent, can undergo a superradiant phase transition, also known as cooperative resonance fluorescence. In an atomic array, stationary states that conserve the collective pseudospin exhibit completely cooperative decay and undergo a second-order phase transition in the large atom number limit. In contrast, decay mechanisms on longer timescales that fail to conserve pseudospin can lead to discontinuous first-order phase transition at a critical finite atom number, disrupting cooperation despite sharing many similar observable characteristics. A hallmark of the superradiant phase transition is an abrupt shift from total light reflection off the atoms to rapidly increasing transmission, accompanied by significant quantum fluctuations, as a function of light intensity.","sentences":["Atomic ensembles strongly interacting with light constitute rich quantum-optical many-body systems, with the potential for observing cooperative effects and dissipative nonequilibrium phase transitions.","We theoretically analyze the conditions under which a driven atomic ensemble in free space, characterized by strong dipole-dipole interactions and large spatial extent, can undergo a superradiant phase transition, also known as cooperative resonance fluorescence.","In an atomic array, stationary states that conserve the collective pseudospin exhibit completely cooperative decay and undergo a second-order phase transition in the large atom number limit.","In contrast, decay mechanisms on longer timescales that fail to conserve pseudospin can lead to discontinuous first-order phase transition at a critical finite atom number, disrupting cooperation despite sharing many similar observable characteristics.","A hallmark of the superradiant phase transition is an abrupt shift from total light reflection off the atoms to rapidly increasing transmission, accompanied by significant quantum fluctuations, as a function of light intensity."],"url":"http://arxiv.org/abs/2404.12939v1","category":"quant-ph"}
{"created":"2024-04-19 15:07:23","title":"FAIR Jupyter: a knowledge graph approach to semantic sharing and granular exploration of a computational notebook reproducibility dataset","abstract":"The way in which data are shared can affect their utility and reusability. Here, we demonstrate how data that we had previously shared in bulk can be mobilized further through a knowledge graph that allows for much more granular exploration and interrogation. The original dataset is about the computational reproducibility of GitHub-hosted Jupyter notebooks associated with biomedical publications. It contains rich metadata about the publications, associated GitHub repositories and Jupyter notebooks, and the notebooks' reproducibility. We took this dataset, converted it into semantic triples and loaded these into a triple store to create a knowledge graph, FAIR Jupyter, that we made accessible via a web service. This enables granular data exploration and analysis through queries that can be tailored to specific use cases. Such queries may provide details about any of the variables from the original dataset, highlight relationships between them or combine some of the graph's content with materials from corresponding external resources. We provide a collection of example queries addressing a range of use cases in research and education. We also outline how sets of such queries can be used to profile specific content types, either individually or by class. We conclude by discussing how such a semantically enhanced sharing of complex datasets can both enhance their FAIRness, i.e., their findability, accessibility, interoperability, and reusability, and help identify and communicate best practices, particularly with regards to data quality, standardization, automation and reproducibility.","sentences":["The way in which data are shared can affect their utility and reusability.","Here, we demonstrate how data that we had previously shared in bulk can be mobilized further through a knowledge graph that allows for much more granular exploration and interrogation.","The original dataset is about the computational reproducibility of GitHub-hosted Jupyter notebooks associated with biomedical publications.","It contains rich metadata about the publications, associated GitHub repositories and Jupyter notebooks, and the notebooks' reproducibility.","We took this dataset, converted it into semantic triples and loaded these into a triple store to create a knowledge graph, FAIR Jupyter, that we made accessible via a web service.","This enables granular data exploration and analysis through queries that can be tailored to specific use cases.","Such queries may provide details about any of the variables from the original dataset, highlight relationships between them or combine some of the graph's content with materials from corresponding external resources.","We provide a collection of example queries addressing a range of use cases in research and education.","We also outline how sets of such queries can be used to profile specific content types, either individually or by class.","We conclude by discussing how such a semantically enhanced sharing of complex datasets can both enhance their FAIRness, i.e., their findability, accessibility, interoperability, and reusability, and help identify and communicate best practices, particularly with regards to data quality, standardization, automation and reproducibility."],"url":"http://arxiv.org/abs/2404.12935v1","category":"cs.CE"}
{"created":"2024-04-19 14:58:58","title":"Collective entanglement in quantum materials with competing orders","abstract":"We investigate entanglement detection in quantum materials through criteria based on the simultaneous suppression of collective matter excitations. Unlike other detection schemes, these criteria can be applied to continuous and unbounded variables. By considering a system of interacting dipoles on a lattice, we show the detection of collective entanglement arising from two different physical mechanisms, namely, the ferroelectric ordering and the dressing of matter degrees of freedom by light. In the latter case, the detection shows the formation of a collective entangled phase not directly related to spontaneous symmetry breaking. These results open a new perspective for the entanglement characterization of competing orders in quantum materials, and have direct application to quantum paraelectrics with large polariton splittings.","sentences":["We investigate entanglement detection in quantum materials through criteria based on the simultaneous suppression of collective matter excitations.","Unlike other detection schemes, these criteria can be applied to continuous and unbounded variables.","By considering a system of interacting dipoles on a lattice, we show the detection of collective entanglement arising from two different physical mechanisms, namely, the ferroelectric ordering and the dressing of matter degrees of freedom by light.","In the latter case, the detection shows the formation of a collective entangled phase not directly related to spontaneous symmetry breaking.","These results open a new perspective for the entanglement characterization of competing orders in quantum materials, and have direct application to quantum paraelectrics with large polariton splittings."],"url":"http://arxiv.org/abs/2404.12931v1","category":"cond-mat.str-el"}
{"created":"2024-04-19 14:37:11","title":"Simple and efficient methods for local structural analysis in polydisperse hard disk systems","abstract":"In nonequilibrium statistical physics, quantifying the nearest (and higher-order) neighbors and free volumes of particles in many-body systems is crucial to elucidating the origin of macroscopic collective phenomena, such as glass/granular jamming transitions and various aspects of the behavior of active matter. However, conventional techniques (based on a fixed-distance cutoff or the Voronoi construction) have mainly been applied to equilibrated, homogeneous, and monodisperse particle systems. In this paper, we implement simple and efficient methods for local structure analysis in nonequilibrium, inhomogeneous, and polydisperse hard disk systems. We show how these novel methods can overcome the difficulties encountered by conventional techniques, as well as demonstrating some applications.","sentences":["In nonequilibrium statistical physics, quantifying the nearest (and higher-order) neighbors and free volumes of particles in many-body systems is crucial to elucidating the origin of macroscopic collective phenomena, such as glass/granular jamming transitions and various aspects of the behavior of active matter.","However, conventional techniques (based on a fixed-distance cutoff or the Voronoi construction) have mainly been applied to equilibrated, homogeneous, and monodisperse particle systems.","In this paper, we implement simple and efficient methods for local structure analysis in nonequilibrium, inhomogeneous, and polydisperse hard disk systems.","We show how these novel methods can overcome the difficulties encountered by conventional techniques, as well as demonstrating some applications."],"url":"http://arxiv.org/abs/2404.12912v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-19 14:31:01","title":"Cloud-based Digital Twin for Cognitive Robotics","abstract":"The paper presents a novel cloud-based digital twin learning platform for teaching and training concepts of cognitive robotics. Instead of forcing interested learners or students to install a new operating system and bulky, fragile software onto their personal laptops just to solve tutorials or coding assignments of a single lecture on robotics, it would be beneficial to avoid technical setups and directly dive into the content of cognitive robotics. To achieve this, the authors utilize containerization technologies and Kubernetes to deploy and operate containerized applications, including robotics simulation environments and software collections based on the Robot operating System (ROS). The web-based Integrated Development Environment JupyterLab is integrated with RvizWeb and XPRA to provide real-time visualization of sensor data and robot behavior in a user-friendly environment for interacting with robotics software. The paper also discusses the application of the platform in teaching Knowledge Representation, Reasoning, Acquisition and Retrieval, and Task-Executives. The authors conclude that the proposed platform is a valuable tool for education and research in cognitive robotics, and that it has the potential to democratize access to these fields. The platform has already been successfully employed in various academic courses, demonstrating its effectiveness in fostering knowledge and skill development.","sentences":["The paper presents a novel cloud-based digital twin learning platform for teaching and training concepts of cognitive robotics.","Instead of forcing interested learners or students to install a new operating system and bulky, fragile software onto their personal laptops just to solve tutorials or coding assignments of a single lecture on robotics, it would be beneficial to avoid technical setups and directly dive into the content of cognitive robotics.","To achieve this, the authors utilize containerization technologies and Kubernetes to deploy and operate containerized applications, including robotics simulation environments and software collections based on the Robot operating System (ROS).","The web-based Integrated Development Environment JupyterLab is integrated with RvizWeb and XPRA to provide real-time visualization of sensor data and robot behavior in a user-friendly environment for interacting with robotics software.","The paper also discusses the application of the platform in teaching Knowledge Representation, Reasoning, Acquisition and Retrieval, and Task-Executives.","The authors conclude that the proposed platform is a valuable tool for education and research in cognitive robotics, and that it has the potential to democratize access to these fields.","The platform has already been successfully employed in various academic courses, demonstrating its effectiveness in fostering knowledge and skill development."],"url":"http://arxiv.org/abs/2404.12909v1","category":"cs.RO"}
{"created":"2024-04-19 13:33:30","title":"TimelinePTC: Development of a unified interface for pathways to care collection, visualization, and collaboration in first episode psychosis","abstract":"This paper presents TimelinePTC, a web-based tool developed to improve the collection and analysis of Pathways to Care (PTC) data in first episode psychosis (FEP) research. Accurately measuring the duration of untreated psychosis (DUP) is essential for effective FEP treatment, requiring detailed understanding of the patient's journey to care. However, traditional PTC data collection methods, mainly manual and paper-based, are time-consuming and often fail to capture the full complexity of care pathways.   TimelinePTC addresses these limitations by providing a digital platform for collaborative, real-time data entry and visualization, thereby enhancing data accuracy and collection efficiency. Initially created for the Specialized Treatment Early in Psychosis (STEP) program in New Haven, Connecticut, its design allows for straightforward adaptation to other healthcare contexts, facilitated by its open-source codebase.   The tool significantly simplifies the data collection process, making it more efficient and user-friendly. It automates the conversion of collected data into a format ready for analysis, reducing manual transcription errors and saving time. By enabling more detailed and consistent data collection, TimelinePTC has the potential to improve healthcare access research, supporting the development of targeted interventions to reduce DUP and improve patient outcomes.","sentences":["This paper presents TimelinePTC, a web-based tool developed to improve the collection and analysis of Pathways to Care (PTC) data in first episode psychosis (FEP) research.","Accurately measuring the duration of untreated psychosis (DUP) is essential for effective FEP treatment, requiring detailed understanding of the patient's journey to care.","However, traditional PTC data collection methods, mainly manual and paper-based, are time-consuming and often fail to capture the full complexity of care pathways.   ","TimelinePTC addresses these limitations by providing a digital platform for collaborative, real-time data entry and visualization, thereby enhancing data accuracy and collection efficiency.","Initially created for the Specialized Treatment Early in Psychosis (STEP) program in New Haven, Connecticut, its design allows for straightforward adaptation to other healthcare contexts, facilitated by its open-source codebase.   ","The tool significantly simplifies the data collection process, making it more efficient and user-friendly.","It automates the conversion of collected data into a format ready for analysis, reducing manual transcription errors and saving time.","By enabling more detailed and consistent data collection, TimelinePTC has the potential to improve healthcare access research, supporting the development of targeted interventions to reduce DUP and improve patient outcomes."],"url":"http://arxiv.org/abs/2404.12883v1","category":"cs.HC"}
{"created":"2024-04-19 11:51:14","title":"Coexistence of Push Wireless Access with Pull Communication for Content-based Wake-up Radios","abstract":"This paper considers energy-efficient connectivity for Internet of Things (IoT) devices in a coexistence scenario between two distinctive communication models: pull- and push-based. In pull-based, the base station (BS) decides when to retrieve a specific type of data from the IoT devices, while in push-based, the IoT device decides when and which data to transmit. To this end, this paper advocates introducing the content-based wake-up (CoWu), which enables the BS to remotely activate only a subset of pull-based nodes equipped with wake-up receivers, observing the relevant data. In this setup, a BS pulls data with CoWu at a specific time instance to fulfill its tasks while collecting data from the nodes operating with a push-based communication model. The resource allocation plays an important role: longer data collection duration for pull-based nodes can lead to high retrieval accuracy while decreasing the probability of data transmission success for push-based nodes, and vice versa. Numerical results show that CoWu can manage communication requirements for both pull-based and push-based nodes while realizing the high energy efficiency (up to 38%) of IoT devices, compared to the baseline scheduling method.","sentences":["This paper considers energy-efficient connectivity for Internet of Things (IoT) devices in a coexistence scenario between two distinctive communication models: pull- and push-based.","In pull-based, the base station (BS) decides when to retrieve a specific type of data from the IoT devices, while in push-based, the IoT device decides when and which data to transmit.","To this end, this paper advocates introducing the content-based wake-up (CoWu), which enables the BS to remotely activate only a subset of pull-based nodes equipped with wake-up receivers, observing the relevant data.","In this setup, a BS pulls data with CoWu at a specific time instance to fulfill its tasks while collecting data from the nodes operating with a push-based communication model.","The resource allocation plays an important role: longer data collection duration for pull-based nodes can lead to high retrieval accuracy while decreasing the probability of data transmission success for push-based nodes, and vice versa.","Numerical results show that CoWu can manage communication requirements for both pull-based and push-based nodes while realizing the high energy efficiency (up to 38%) of IoT devices, compared to the baseline scheduling method."],"url":"http://arxiv.org/abs/2404.12816v1","category":"cs.NI"}
{"created":"2024-04-19 09:25:31","title":"Near-Quantum-limited Haloscope Detection of Dark Photon Dark Matter Enhanced by a High-Q Superconducting Cavit","abstract":"We report new experimental results on the search for dark photons based on a near-quantum-limited haloscope equipped with a superconducting cavity. The loaded quality factor of the superconducting cavity is $6\\times10^{5}$, so that the expected signal from dark photon dark matter can be enhanced by more than one order compared to a copper cavity. A Josephson parametric amplifier with a near-quantum-limited noise temperature has been utilized to minimize the noise during the search. Furthermore, a digital acquisition card based on field programmable gate arrays has been utilized to maximize data collection efficiency with a duty cycle being 100$\\%$. This work has established the most stringent constraints on dark photons at around 26.965 $\\mu$eV. In the future, our apparatus can be extended to search for other dark matter candidates, such as axions and axion-like particles, and scrutinize new physics beyond the Standard Model.","sentences":["We report new experimental results on the search for dark photons based on a near-quantum-limited haloscope equipped with a superconducting cavity.","The loaded quality factor of the superconducting cavity is $6\\times10^{5}$, so that the expected signal from dark photon dark matter can be enhanced by more than one order compared to a copper cavity.","A Josephson parametric amplifier with a near-quantum-limited noise temperature has been utilized to minimize the noise during the search.","Furthermore, a digital acquisition card based on field programmable gate arrays has been utilized to maximize data collection efficiency with a duty cycle being 100$\\%$.","This work has established the most stringent constraints on dark photons at around 26.965 $\\mu$eV. In the future, our apparatus can be extended to search for other dark matter candidates, such as axions and axion-like particles, and scrutinize new physics beyond the Standard Model."],"url":"http://arxiv.org/abs/2404.12731v1","category":"hep-ex"}
{"created":"2024-04-19 17:59:59","title":"Formation of a $33\\,M_{\\odot}$ black hole in a low-metallicity binary","abstract":"A $33\\,M_\\odot$ black hole (BH) was recently discovered in an 11.6-year binary only 590 pc from the Sun. The system, Gaia BH3, contains a $0.8\\,M_\\odot$ low-metallicity giant ($\\rm [M/H]=-2.2$) and is kinematically part of the Galactic halo, suggesting that the BH formed from a low-metallicity massive star. I show that orbits similar to that of Gaia BH3 are naturally produced through isolated binary evolution. The system's period and eccentricity can result from a broad range of initial orbits with a modest natal kick ($v_{\\rm kick}\\lesssim 75\\,\\rm km\\,s^{-1}$) to the BH. I construct MESA models for metal-poor massive stars with initial masses ranging from $35-55\\,M_{\\odot}$, which reach maximum radii of $1150-1800\\,R_{\\odot}$ as red supergiants. Stars of this size would fit inside most plausible pre-supernova orbits for the system without overflowing their Roche lobes. In addition, models with moderately rapid initial rotation ($\\Omega/\\Omega_{\\rm crit} \\gtrsim 0.45$) undergo chemically homogeneous evolution and never expand to radii larger than $10\\,R_{\\odot}$. There are thus multiple channels through which a low-metallicity, extreme-mass ratio binary could produce a system like Gaia BH3. Dynamical formation scenarios are also viable, and there is little doubt that both isolated and dynamically-formed BH binaries with orbits similar to Gaia BH3 will be discovered in Gaia DR4. Only about 1 in 10,000 stars in the solar neighborhood have metallicities as low as Gaia BH3. This suggests that BH companions are dramatically over-represented at low-metallicity, though caveats related to small number statistics apply. The fact that the luminous star in Gaia BH3 has been a giant - greatly boosting its detectability - only for $\\sim$1% of the time since the system's formation implies that additional massive BHs remain to be discovered with only moderately fainter companions.","sentences":["A $33\\,M_\\odot$ black hole (BH) was recently discovered in an 11.6-year binary only 590 pc from the Sun.","The system, Gaia BH3, contains a $0.8\\,M_\\odot$ low-metallicity giant ($\\rm [M/H]=-2.2$) and is kinematically part of the Galactic halo, suggesting that the BH formed from a low-metallicity massive star.","I show that orbits similar to that of Gaia BH3 are naturally produced through isolated binary evolution.","The system's period and eccentricity can result from a broad range of initial orbits with a modest natal kick ($v_{\\rm kick}\\lesssim 75\\,\\rm km\\,s^{-1}$) to the BH.","I construct MESA models for metal-poor massive stars with initial masses ranging from $35-55\\,M_{\\odot}$, which reach maximum radii of $1150-1800\\,R_{\\odot}$ as red supergiants.","Stars of this size would fit inside most plausible pre-supernova orbits for the system without overflowing their Roche lobes.","In addition, models with moderately rapid initial rotation ($\\Omega/\\Omega_{\\rm crit} \\gtrsim 0.45$) undergo chemically homogeneous evolution and never expand to radii larger than $10\\,R_{\\odot}$. There are thus multiple channels through which a low-metallicity, extreme-mass ratio binary could produce a system like Gaia BH3.","Dynamical formation scenarios are also viable, and there is little doubt that both isolated and dynamically-formed BH binaries with orbits similar to Gaia BH3 will be discovered in Gaia DR4.","Only about 1 in 10,000 stars in the solar neighborhood have metallicities as low as Gaia BH3.","This suggests that BH companions are dramatically over-represented at low-metallicity, though caveats related to small number statistics apply.","The fact that the luminous star in Gaia BH3 has been a giant - greatly boosting its detectability - only for $\\sim$1% of the time since the system's formation implies that additional massive BHs remain to be discovered with only moderately fainter companions."],"url":"http://arxiv.org/abs/2404.13047v1","category":"astro-ph.SR"}
{"created":"2024-04-19 17:48:08","title":"A Mobile Additive Manufacturing Robot Framework for Smart Manufacturing Systems","abstract":"Recent technological innovations in the areas of additive manufacturing and collaborative robotics have paved the way toward realizing the concept of on-demand, personalized production on the shop floor. Additive manufacturing process can provide the capability of printing highly customized parts based on various customer requirements. Autonomous, mobile systems provide the flexibility to move custom parts around the shop floor to various manufacturing operations, as needed by product requirements. In this work, we proposed a mobile additive manufacturing robot framework for merging an additive manufacturing process system with an autonomous mobile base. Two case studies showcase the potential benefits of the proposed mobile additive manufacturing framework. The first case study overviews the effect that a mobile system can have on a fused deposition modeling process. The second case study showcases how integrating a mobile additive manufacturing machine can improve the throughput of the manufacturing system. The major findings of this study are that the proposed mobile robotic AM has increased throughput by taking advantage of the travel time between operations/processing sites. It is particularly suited to perform intermittent operations (e.g., preparing feedstock) during the travel time of the robotic AM. One major implication of this study is its application in manufacturing structural components (e.g., concrete construction, and feedstock preparation during reconnaissance missions) in remote or extreme terrains with on-site or on-demand feedstocks.","sentences":["Recent technological innovations in the areas of additive manufacturing and collaborative robotics have paved the way toward realizing the concept of on-demand, personalized production on the shop floor.","Additive manufacturing process can provide the capability of printing highly customized parts based on various customer requirements.","Autonomous, mobile systems provide the flexibility to move custom parts around the shop floor to various manufacturing operations, as needed by product requirements.","In this work, we proposed a mobile additive manufacturing robot framework for merging an additive manufacturing process system with an autonomous mobile base.","Two case studies showcase the potential benefits of the proposed mobile additive manufacturing framework.","The first case study overviews the effect that a mobile system can have on a fused deposition modeling process.","The second case study showcases how integrating a mobile additive manufacturing machine can improve the throughput of the manufacturing system.","The major findings of this study are that the proposed mobile robotic AM has increased throughput by taking advantage of the travel time between operations/processing sites.","It is particularly suited to perform intermittent operations (e.g., preparing feedstock) during the travel time of the robotic AM.","One major implication of this study is its application in manufacturing structural components (e.g., concrete construction, and feedstock preparation during reconnaissance missions) in remote or extreme terrains with on-site or on-demand feedstocks."],"url":"http://arxiv.org/abs/2404.13034v1","category":"cs.RO"}
{"created":"2024-04-19 17:47:02","title":"Sample Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs","abstract":"In the burgeoning field of Large Language Models (LLMs) like ChatGPT and LLaMA, Prompt Engineering (PE) is renowned for boosting zero-shot or in-context learning (ICL) through prompt modifications. Yet, the realm of the sample design for downstream fine-tuning, crucial for task-specific LLM adaptation, is largely unexplored. This paper introduces Sample Design Engineering (SDE), a methodical approach to enhancing LLMs' post-tuning performance by refining input, output, and reasoning designs. We conduct a series of in-domain (ID) and out-of-domain (OOD) experiments to assess the impact of various design options on LLMs' downstream performance, revealing several intriguing patterns that hold consistently across different LLMs. Based on these insights, we propose an integrated SDE strategy, combining the most effective options, and validate its consistent superiority over heuristic sample designs in complex downstream tasks like multi-aspect sentiment analysis, event extraction, and nested entity recognition. Additionally, analyses of LLMs' inherent prompt/output perplexity, zero-shot, and ICL abilities illustrate that good PE strategies may not always translate to good SDE strategies. Code available at https://github.com/beyondguo/LLM-Tuning.","sentences":["In the burgeoning field of Large Language Models (LLMs) like ChatGPT and LLaMA, Prompt Engineering (PE) is renowned for boosting zero-shot or in-context learning (ICL) through prompt modifications.","Yet, the realm of the sample design for downstream fine-tuning, crucial for task-specific LLM adaptation, is largely unexplored.","This paper introduces Sample Design Engineering (SDE), a methodical approach to enhancing LLMs' post-tuning performance by refining input, output, and reasoning designs.","We conduct a series of in-domain (ID) and out-of-domain (OOD) experiments to assess the impact of various design options on LLMs' downstream performance, revealing several intriguing patterns that hold consistently across different LLMs.","Based on these insights, we propose an integrated SDE strategy, combining the most effective options, and validate its consistent superiority over heuristic sample designs in complex downstream tasks like multi-aspect sentiment analysis, event extraction, and nested entity recognition.","Additionally, analyses of LLMs' inherent prompt/output perplexity, zero-shot, and ICL abilities illustrate that good PE strategies may not always translate to good SDE strategies.","Code available at https://github.com/beyondguo/LLM-Tuning."],"url":"http://arxiv.org/abs/2404.13033v1","category":"cs.CL"}
{"created":"2024-04-19 17:45:34","title":"The James Webb Interferometer: Space-based interferometric detections of PDS 70 b and c at 4.8 $\u03bc$m","abstract":"We observed the planet-hosting system PDS 70 with the James Webb Interferometer, JWST's Aperture Masking Interferometric (AMI) mode within NIRISS. Observing with the F480M filter centered at 4.8 $\\mu$m, we simultaneously fit a geometric model to the outer disk and the two known planetary companions. We re-detect the protoplanets PDS 70 b and c at an SNR of 21 and 11, respectively. Our photometry of both PDS 70 b and c provide evidence for circumplanetary disk emission through fitting SED models to these new measurements and those found in the literature. We also newly detect emission within the disk gap at an SNR of $\\sim$4, at a position angle of $207^{+11}_{-10}$ degrees, and an unconstrained separation within $\\sim$200 mas. Follow-up observations will be needed to determine the nature of this emission. We place a 5$\\sigma$ upper limit of $\\Delta$mag = 7.56 on the contrast of the candidate PDS 70 d at 4.8 $\\mu$m, which indicates that if the previously observed emission at shorter wavelengths is due to a planet, this putative planet has a different atmospheric composition than PDS 70 b or c. Finally, we place upper limits on emission from any additional planets in the disk gap. We find an azimuthally averaged 5$\\sigma$ upper limit of $\\Delta$mag $\\approx$ 7.5 at separations greater than 125 mas. These are the deepest limits to date within $\\sim$250 mas at 4.8 $\\mu$m and the first space-based interferometric observations of this system.","sentences":["We observed the planet-hosting system PDS 70 with the James Webb Interferometer, JWST's Aperture Masking Interferometric (AMI) mode within NIRISS.","Observing with the F480M filter centered at 4.8 $\\mu$m, we simultaneously fit a geometric model to the outer disk and the two known planetary companions.","We re-detect the protoplanets PDS 70 b and c at an SNR of 21 and 11, respectively.","Our photometry of both PDS 70 b and c provide evidence for circumplanetary disk emission through fitting SED models to these new measurements and those found in the literature.","We also newly detect emission within the disk gap at an SNR of $\\sim$4, at a position angle of $207^{+11}_{-10}$ degrees, and an unconstrained separation within $\\sim$200 mas.","Follow-up observations will be needed to determine the nature of this emission.","We place a 5$\\sigma$ upper limit of $\\Delta$mag = 7.56 on the contrast of the candidate PDS 70 d at 4.8 $\\mu$m, which indicates that if the previously observed emission at shorter wavelengths is due to a planet, this putative planet has a different atmospheric composition than PDS 70 b or c. Finally, we place upper limits on emission from any additional planets in the disk gap.","We find an azimuthally averaged 5$\\sigma$ upper limit of $\\Delta$mag $\\approx$ 7.5 at separations greater than 125 mas.","These are the deepest limits to date within $\\sim$250 mas at 4.8 $\\mu$m and the first space-based interferometric observations of this system."],"url":"http://arxiv.org/abs/2404.13032v1","category":"astro-ph.EP"}
{"created":"2024-04-19 17:45:27","title":"OGLE-2015-BLG-0845L: A low-mass M dwarf from the microlensing parallax and xallarap effects","abstract":"We present the analysis of the microlensing event OGLE-2015-BLG-0845, which was affected by both the microlensing parallax and xallarap effects. The former was detected via the simultaneous observations from the ground and Spitzer, and the latter was caused by the orbital motion of the source star in a relatively close binary. The combination of these two effects led to a direct mass measurement of the lens object, revealing a low-mass ($0.14 \\pm 0.05 M_{\\odot}$) M-dwarf at the bulge distance ($7.6 \\pm 1.0$ kpc). The source binary consists of a late F-type subgiant and a K-type dwarf of $\\sim1.2 M_{\\odot}$ and $\\sim 0.9 M_{\\odot}$, respectively, and the orbital period is $70 \\pm 10$ days. OGLE-2015-BLG-0845 is the first single-lens event in which the lens mass is measured via the binarity of the source. Given the abundance of binary systems as potential microlensing sources, the xallarap effect may not be a rare phenomenon. Our work thus highlights the application of the xallarap effect in the mass determination of microlenses, and the same method can be used to identify isolated dark lenses.","sentences":["We present the analysis of the microlensing event OGLE-2015-BLG-0845, which was affected by both the microlensing parallax and xallarap effects.","The former was detected via the simultaneous observations from the ground and Spitzer, and the latter was caused by the orbital motion of the source star in a relatively close binary.","The combination of these two effects led to a direct mass measurement of the lens object, revealing a low-mass ($0.14 \\pm 0.05 M_{\\odot}$) M-dwarf at the bulge distance ($7.6 \\pm 1.0$ kpc).","The source binary consists of a late F-type subgiant and a K-type dwarf of $\\sim1.2 M_{\\odot}$ and $\\sim 0.9 M_{\\odot}$, respectively, and the orbital period is $70 \\pm 10$ days.","OGLE-2015-BLG-0845 is the first single-lens event in which the lens mass is measured via the binarity of the source.","Given the abundance of binary systems as potential microlensing sources, the xallarap effect may not be a rare phenomenon.","Our work thus highlights the application of the xallarap effect in the mass determination of microlenses, and the same method can be used to identify isolated dark lenses."],"url":"http://arxiv.org/abs/2404.13031v1","category":"astro-ph.SR"}
{"created":"2024-04-19 17:41:56","title":"An Analysis of Driver-Initiated Takeovers during Assisted Driving and their Effect on Driver Satisfaction","abstract":"During the use of Advanced Driver Assistance Systems (ADAS), drivers can intervene in the active function and take back control due to various reasons. However, the specific reasons for driver-initiated takeovers in naturalistic driving are still not well understood. In order to get more information on the reasons behind these takeovers, a test group study was conducted. There, 17 participants used a predictive longitudinal driving function for their daily commutes and annotated the reasons for their takeovers during active function use. In this paper, the recorded takeovers are analyzed and the different reasons for them are highlighted. The results show that the reasons can be divided into three main categories. The most common category consists of takeovers which aim to adjust the behavior of the ADAS within its Operational Design Domain (ODD) in order to better match the drivers' personal preferences. Other reasons include takeovers due to leaving the ADAS's ODD and corrections of incorrect sensing state information. Using the questionnaire results of the test group study, it was found that the number and frequency of takeovers especially within the ADAS's ODD have a significant negative impact on driver satisfaction. Therefore, the driver satisfaction with the ADAS could be increased by adapting its behavior to the drivers' wishes and thereby lowering the number of takeovers within the ODD. The information contained in the takeover behavior of the drivers could be used as feedback for the ADAS. Finally, it is shown that there are considerable differences in the takeover behavior of different drivers, which shows a need for ADAS individualization.","sentences":["During the use of Advanced Driver Assistance Systems (ADAS), drivers can intervene in the active function and take back control due to various reasons.","However, the specific reasons for driver-initiated takeovers in naturalistic driving are still not well understood.","In order to get more information on the reasons behind these takeovers, a test group study was conducted.","There, 17 participants used a predictive longitudinal driving function for their daily commutes and annotated the reasons for their takeovers during active function use.","In this paper, the recorded takeovers are analyzed and the different reasons for them are highlighted.","The results show that the reasons can be divided into three main categories.","The most common category consists of takeovers which aim to adjust the behavior of the ADAS within its Operational Design Domain (ODD) in order to better match the drivers' personal preferences.","Other reasons include takeovers due to leaving the ADAS's ODD and corrections of incorrect sensing state information.","Using the questionnaire results of the test group study, it was found that the number and frequency of takeovers especially within the ADAS's ODD have a significant negative impact on driver satisfaction.","Therefore, the driver satisfaction with the ADAS could be increased by adapting its behavior to the drivers' wishes and thereby lowering the number of takeovers within the ODD.","The information contained in the takeover behavior of the drivers could be used as feedback for the ADAS.","Finally, it is shown that there are considerable differences in the takeover behavior of different drivers, which shows a need for ADAS individualization."],"url":"http://arxiv.org/abs/2404.13027v1","category":"cs.RO"}
{"created":"2024-04-19 17:24:49","title":"A high-fidelity finite volume scheme for ideal magnetohydrodynamics equations using boundary variation diminishing algorithm","abstract":"A high-fidelity finite volume scheme based on the BVD (boundary variation diminishing) concept is proposed in this study to solve the ideal magnetohydrodynamics (MHD) equations. A hybrid spatial reconstruction profile, consisting of a quadratic polynomial and a steepness-adjustable hyperbolic tangent function, is adopted to reproduce the accurate solutions of the complex magnetohydrodynamics flows. The BVD principle is used to find a optimal combination of these two types of spatial reconstructions by comparing the variations of the interface values interpolated in two adjacent cells, aiming to remove the non-physical oscillations around discontinuities by switching the quadratic polynomial to a step-shaped function. Additionally, a constrained transport (CT) method is applied in this study to assure the non-divergent solution of the magnetic field. The widely used numerical tests in one and two dimensional cases were checked in this study. The numerical results can retrieve the accuracy of a $3^{rd}$-order linear scheme in the convergence test for both advection and MHD equations and capture the strong shock waves in MHD flows without spurious oscillations. In comparison with the results of a $3^{rd}$-order WENO (weighted essentially non-oscillatory) scheme, the proposed scheme gains more accurate solutions not only for the strong discontinuities but also the smooth structures across scales. To our knowledge, this is the first attempt to build a high-fidelity model for ideal MHD equations by a BVD algorithm. Numerical results are competitive to those of existing advanced models and thus the BVD algorithm has promising potentials to build practical models for various MHD flows.","sentences":["A high-fidelity finite volume scheme based on the BVD (boundary variation diminishing) concept is proposed in this study to solve the ideal magnetohydrodynamics (MHD) equations.","A hybrid spatial reconstruction profile, consisting of a quadratic polynomial and a steepness-adjustable hyperbolic tangent function, is adopted to reproduce the accurate solutions of the complex magnetohydrodynamics flows.","The BVD principle is used to find a optimal combination of these two types of spatial reconstructions by comparing the variations of the interface values interpolated in two adjacent cells, aiming to remove the non-physical oscillations around discontinuities by switching the quadratic polynomial to a step-shaped function.","Additionally, a constrained transport (CT) method is applied in this study to assure the non-divergent solution of the magnetic field.","The widely used numerical tests in one and two dimensional cases were checked in this study.","The numerical results can retrieve the accuracy of a $3^{rd}$-order linear scheme in the convergence test for both advection and MHD equations and capture the strong shock waves in MHD flows without spurious oscillations.","In comparison with the results of a $3^{rd}$-order WENO (weighted essentially non-oscillatory) scheme, the proposed scheme gains more accurate solutions not only for the strong discontinuities but also the smooth structures across scales.","To our knowledge, this is the first attempt to build a high-fidelity model for ideal MHD equations by a BVD algorithm.","Numerical results are competitive to those of existing advanced models and thus the BVD algorithm has promising potentials to build practical models for various MHD flows."],"url":"http://arxiv.org/abs/2404.13015v1","category":"physics.flu-dyn"}
{"created":"2024-04-19 17:24:40","title":"Mean-field Potts and random-cluster dynamics from high-entropy initializations","abstract":"A common obstruction to efficient sampling from high-dimensional distributions is the multimodality of the target distribution because Markov chains may get trapped far from stationarity. Still, one hopes that this is only a barrier to the mixing of Markov chains from worst-case initializations and can be overcome by choosing high-entropy initializations, e.g., a product or weakly correlated distribution. Ideally, from such initializations, the dynamics would escape from the saddle points separating modes quickly and spread its mass between the dominant modes.   In this paper, we study convergence from high-entropy initializations for the random-cluster and Potts models on the complete graph -- two extensively studied high-dimensional landscapes that pose many complexities like discontinuous phase transitions and asymmetric metastable modes. We study the Chayes--Machta and Swendsen--Wang dynamics for the mean-field random-cluster model and the Glauber dynamics for the Potts model. We sharply characterize the set of product measure initializations from which these Markov chains mix rapidly, even though their mixing times from worst-case initializations are exponentially slow. Our proofs require careful approximations of projections of high-dimensional Markov chains (which are not themselves Markovian) by tractable 1-dimensional random processes, followed by analysis of the latter's escape from saddle points separating stable modes.","sentences":["A common obstruction to efficient sampling from high-dimensional distributions is the multimodality of the target distribution because Markov chains may get trapped far from stationarity.","Still, one hopes that this is only a barrier to the mixing of Markov chains from worst-case initializations and can be overcome by choosing high-entropy initializations, e.g., a product or weakly correlated distribution.","Ideally, from such initializations, the dynamics would escape from the saddle points separating modes quickly and spread its mass between the dominant modes.   ","In this paper, we study convergence from high-entropy initializations for the random-cluster and Potts models on the complete graph -- two extensively studied high-dimensional landscapes that pose many complexities like discontinuous phase transitions and asymmetric metastable modes.","We study the Chayes--Machta and Swendsen--","Wang dynamics for the mean-field random-cluster model and the Glauber dynamics for the Potts model.","We sharply characterize the set of product measure initializations from which these Markov chains mix rapidly, even though their mixing times from worst-case initializations are exponentially slow.","Our proofs require careful approximations of projections of high-dimensional Markov chains (which are not themselves Markovian) by tractable 1-dimensional random processes, followed by analysis of the latter's escape from saddle points separating stable modes."],"url":"http://arxiv.org/abs/2404.13014v1","category":"math.PR"}
{"created":"2024-04-19 17:17:34","title":"Asymptotic behavior of solutions of the nonlinear Beltrami equation with the Jacobian","abstract":"We investigate the asymptotic behavior at infinity of regular homeomorphic solutions of the nonlinear Beltrami equation with the Jacobian on the right-hand side. The sharpness of the above bounds is illustrated by several examples.","sentences":["We investigate the asymptotic behavior at infinity of regular homeomorphic solutions of the nonlinear Beltrami equation with the Jacobian on the right-hand side.","The sharpness of the above bounds is illustrated by several examples."],"url":"http://arxiv.org/abs/2404.13012v1","category":"math.AP"}
{"created":"2024-04-19 17:16:25","title":"A multigrain-multilayer astrochemical model with variable desorption energy for surface species","abstract":"Context. Interstellar surface chemistry is a complex process that occurs in icy layers accumulated onto grains of different sizes. Efficiency of surface processes often depends on the immediate environment of adsorbed molecules. Aims. We investigate how gas-grain chemistry changes when surface molecule desorption is made explicitly dependent to the molecular binding energy, which is modified, depending on the properties of the surface. Methods. Molecular binding energy changes gradually for three different environments - bare grain, where polar, water-dominated ices and non-polar, carbon monoxide-dominated ices. In addition to diffusion, evaporation and chemical desorption, photodesorption was also made binding energy-dependent, in line with experimental results. These phenomena occur in a collapsing prestellar core model that considers five grain sizes with ices arranged into four layers. Results. Efficient chemical desorption from bare grains significantly delays ice accumulation. Easier surface diffusion of molecules on non-polar ices promotes the production of carbon dioxide and other species. Conclusions. The composition of interstellar ices is regulated by several binding-energy dependent desorption mechanisms. Their actions overlap in time and space, which explains the ubiquitous proportions of major ice components (water and carbon oxides), observed to be similar in all directions.","sentences":["Context.","Interstellar surface chemistry is a complex process that occurs in icy layers accumulated onto grains of different sizes.","Efficiency of surface processes often depends on the immediate environment of adsorbed molecules.","Aims.","We investigate how gas-grain chemistry changes when surface molecule desorption is made explicitly dependent to the molecular binding energy, which is modified, depending on the properties of the surface.","Methods.","Molecular binding energy changes gradually for three different environments - bare grain, where polar, water-dominated ices and non-polar, carbon monoxide-dominated ices.","In addition to diffusion, evaporation and chemical desorption, photodesorption was also made binding energy-dependent, in line with experimental results.","These phenomena occur in a collapsing prestellar core model that considers five grain sizes with ices arranged into four layers.","Results.","Efficient chemical desorption from bare grains significantly delays ice accumulation.","Easier surface diffusion of molecules on non-polar ices promotes the production of carbon dioxide and other species.","Conclusions.","The composition of interstellar ices is regulated by several binding-energy dependent desorption mechanisms.","Their actions overlap in time and space, which explains the ubiquitous proportions of major ice components (water and carbon oxides), observed to be similar in all directions."],"url":"http://arxiv.org/abs/2404.13011v1","category":"astro-ph.GA"}
{"created":"2024-04-19 17:14:03","title":"High-rate quantum LDPC codes for long-range-connected neutral atom registers","abstract":"High-rate quantum error correcting (QEC) codes with moderate overheads in qubit number and control complexity are highly desirable for achieving fault-tolerant quantum computing. Recently, quantum error correction has experienced significant progress both in code development and experimental realizations, with neutral atom qubit architecture rapidly establishing itself as a leading platform in the field. Scalable quantum computing will require processing with QEC codes that have low qubit overhead and large error suppression, and while such codes do exist, they involve a degree of non-locality that has yet to be integrated into experimental platforms. In this work, we analyze a family of high-rate Low-Density Parity-Check (LDPC) codes with limited long-range interactions and outline a near-term implementation in neutral atom registers. By means of circuit-level simulations, we find that these codes outperform surface codes in all respects when the two-qubit nearest neighbour gate error probability is below $\\sim 0.1\\%$. We show how these codes can be natively integrated in two-dimensional static neutral atom qubit architectures with open boundaries, where the desired long-range connectivity can be targeted via Rydberg-blockade interaction. Our protocol solely requires multiple laser colors to enable transitions to different Rydberg states for different interatomic distances.","sentences":["High-rate quantum error correcting (QEC) codes with moderate overheads in qubit number and control complexity are highly desirable for achieving fault-tolerant quantum computing.","Recently, quantum error correction has experienced significant progress both in code development and experimental realizations, with neutral atom qubit architecture rapidly establishing itself as a leading platform in the field.","Scalable quantum computing will require processing with QEC codes that have low qubit overhead and large error suppression, and while such codes do exist, they involve a degree of non-locality that has yet to be integrated into experimental platforms.","In this work, we analyze a family of high-rate Low-Density Parity-Check (LDPC) codes with limited long-range interactions and outline a near-term implementation in neutral atom registers.","By means of circuit-level simulations, we find that these codes outperform surface codes in all respects when the two-qubit nearest neighbour gate error probability is below $\\sim 0.1\\%$.","We show how these codes can be natively integrated in two-dimensional static neutral atom qubit architectures with open boundaries, where the desired long-range connectivity can be targeted via Rydberg-blockade interaction.","Our protocol solely requires multiple laser colors to enable transitions to different Rydberg states for different interatomic distances."],"url":"http://arxiv.org/abs/2404.13010v1","category":"quant-ph"}
{"created":"2024-04-19 17:09:43","title":"Influence of strain and point defects on the electronic structure and related properties of (111)NiO epitaxial films","abstract":"(111)NiO epitaxial films are grown on c-sapphire substrates at various growth temperatures ranging from room-temperature to 600C using pulsed laser deposition (PLD) technique. Two series of samples, where different laser fluences are used to ablate the target, are studied here. Films grown with higher laser fluence, are found to be embedded with Ni-clusters crystallographically aligned with the (111)NiO matrix. While the layers grown with lower laser energy density exhibit p-type conductivity specially at low growth temperatures. X-ray diffraction study shows the coexistence of biaxial compressive and tensile hydrostatic strains in these samples, which results in an expansion of the lattice primarily along the growth direction. This effective uniaxial expansion {epsilon}_perpendicular increases with the reduction of the growth temperature. Band gap of these samples is found to decrease linearly with {epsilon}_perpendicular. This result is validated by density functional theory (DFT) calculations. Experimental findings and the theoretical study further indicate that V_Ni + O_I and V_O + Ni_I complexes exist as the dominant native defects in samples grown with Ni-deficient (low laser fluence) and Ni-rich (high laser fluence) conditions, respectively. P-type conductivity observed in the samples grown in Ni-deficient condition is more likely to be resulting from V_Ni + O_I defects than Ni-vacancies (V_Ni).","sentences":["(111)NiO epitaxial films are grown on c-sapphire substrates at various growth temperatures ranging from room-temperature to 600C using pulsed laser deposition (PLD) technique.","Two series of samples, where different laser fluences are used to ablate the target, are studied here.","Films grown with higher laser fluence, are found to be embedded with Ni-clusters crystallographically aligned with the (111)NiO matrix.","While the layers grown with lower laser energy density exhibit p-type conductivity specially at low growth temperatures.","X-ray diffraction study shows the coexistence of biaxial compressive and tensile hydrostatic strains in these samples, which results in an expansion of the lattice primarily along the growth direction.","This effective uniaxial expansion {epsilon}_perpendicular increases with the reduction of the growth temperature.","Band gap of these samples is found to decrease linearly with {epsilon}_perpendicular.","This result is validated by density functional theory (DFT) calculations.","Experimental findings and the theoretical study further indicate that V_Ni","+ O_I and V_O + Ni_I complexes exist as the dominant native defects in samples grown with Ni-deficient (low laser fluence) and Ni-rich (high laser fluence) conditions, respectively.","P-type conductivity observed in the samples grown in Ni-deficient condition is more likely to be resulting from V_Ni + O_I defects than Ni-vacancies (V_Ni)."],"url":"http://arxiv.org/abs/2404.13007v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-19 16:59:50","title":"Deep Reinforcement Learning-Based Active Flow Control of an Elliptical Cylinder: Transitioning from an Elliptical Cylinder to a Circular Cylinder and a Flat Plate","abstract":"This study investigates the effectiveness of Active Flow Control (AFC) technology supported by Deep Reinforcement Learning (DRL) in suppressing vortex shedding, reducing drag, and minimizing lift coefficients around elliptical cylinders. The aspect ratio Ar of the elliptical cylinder transitions from an ellipsoid Ar=2 to a circular shape Ar=1, and ultimately to a flat plate Ar=0. We utilize the Proximal Policy Optimization (PPO) algorithm to precisely control the mass flow rates of synthetic jets located on both the upper and lower surfaces of the cylinder. The focus is on optimizing fluid dynamics and examining the scalability of these control techniques across different geometric configurations. Our research findings indicate that, for elliptical cylinders with aspect ratios between 1.75 and 0.75, the reduction in drag coefficient ranges from 0.9% to 15.7%, and the reduction in lift coefficient ranges from 95.2% to 99.7%. Notably, the DRL-based control strategy not only significantly reduces lift and drag, but also completely suppresses vortex shedding while using less than 1% of external excitation energy, demonstrating its efficiency and energy-saving capabilities. Additionally, for aspect ratios from 0.5 to 0, the reduction in drag coefficient ranges from 26.9% to 43.6%, and the reduction in lift coefficient from 50.2% to 68.0%. This reflects the control strategy's significant reduction in both drag and lift coefficients, while also alleviating vortex shedding. This underscores the adaptability and potential of DRL-based AFC in managing complex fluid dynamics across diverse elliptical cylinder configurations.","sentences":["This study investigates the effectiveness of Active Flow Control (AFC) technology supported by Deep Reinforcement Learning (DRL) in suppressing vortex shedding, reducing drag, and minimizing lift coefficients around elliptical cylinders.","The aspect ratio Ar of the elliptical cylinder transitions from an ellipsoid Ar=2 to a circular shape Ar=1, and ultimately to a flat plate Ar=0.","We utilize the Proximal Policy Optimization (PPO) algorithm to precisely control the mass flow rates of synthetic jets located on both the upper and lower surfaces of the cylinder.","The focus is on optimizing fluid dynamics and examining the scalability of these control techniques across different geometric configurations.","Our research findings indicate that, for elliptical cylinders with aspect ratios between 1.75 and 0.75, the reduction in drag coefficient ranges from 0.9% to 15.7%, and the reduction in lift coefficient ranges from 95.2% to 99.7%.","Notably, the DRL-based control strategy not only significantly reduces lift and drag, but also completely suppresses vortex shedding while using less than 1% of external excitation energy, demonstrating its efficiency and energy-saving capabilities.","Additionally, for aspect ratios from 0.5 to 0, the reduction in drag coefficient ranges from 26.9% to 43.6%, and the reduction in lift coefficient from 50.2% to 68.0%.","This reflects the control strategy's significant reduction in both drag and lift coefficients, while also alleviating vortex shedding.","This underscores the adaptability and potential of DRL-based AFC in managing complex fluid dynamics across diverse elliptical cylinder configurations."],"url":"http://arxiv.org/abs/2404.13003v1","category":"physics.flu-dyn"}
{"created":"2024-04-19 16:45:50","title":"Rethinking the Evaluation of Dialogue Systems: Effects of User Feedback on Crowdworkers and LLMs","abstract":"In ad-hoc retrieval, evaluation relies heavily on user actions, including implicit feedback. In a conversational setting such signals are usually unavailable due to the nature of the interactions, and, instead, the evaluation often relies on crowdsourced evaluation labels. The role of user feedback in annotators' assessment of turns in a conversational perception has been little studied. We focus on how the evaluation of task-oriented dialogue systems (TDSs), is affected by considering user feedback, explicit or implicit, as provided through the follow-up utterance of a turn being evaluated. We explore and compare two methodologies for assessing TDSs: one includes the user's follow-up utterance and one without. We use both crowdworkers and large language models (LLMs) as annotators to assess system responses across four aspects: relevance, usefulness, interestingness, and explanation quality. Our findings indicate that there is a distinct difference in ratings assigned by both annotator groups in the two setups, indicating user feedback does influence system evaluation. Workers are more susceptible to user feedback on usefulness and interestingness compared to LLMs on interestingness and relevance. User feedback leads to a more personalized assessment of usefulness by workers, aligning closely with the user's explicit feedback. Additionally, in cases of ambiguous or complex user requests, user feedback improves agreement among crowdworkers. These findings emphasize the significance of user feedback in refining system evaluations and suggest the potential for automated feedback integration in future research. We publicly release the annotated data to foster research in this area.","sentences":["In ad-hoc retrieval, evaluation relies heavily on user actions, including implicit feedback.","In a conversational setting such signals are usually unavailable due to the nature of the interactions, and, instead, the evaluation often relies on crowdsourced evaluation labels.","The role of user feedback in annotators' assessment of turns in a conversational perception has been little studied.","We focus on how the evaluation of task-oriented dialogue systems (TDSs), is affected by considering user feedback, explicit or implicit, as provided through the follow-up utterance of a turn being evaluated.","We explore and compare two methodologies for assessing TDSs: one includes the user's follow-up utterance and one without.","We use both crowdworkers and large language models (LLMs) as annotators to assess system responses across four aspects: relevance, usefulness, interestingness, and explanation quality.","Our findings indicate that there is a distinct difference in ratings assigned by both annotator groups in the two setups, indicating user feedback does influence system evaluation.","Workers are more susceptible to user feedback on usefulness and interestingness compared to LLMs on interestingness and relevance.","User feedback leads to a more personalized assessment of usefulness by workers, aligning closely with the user's explicit feedback.","Additionally, in cases of ambiguous or complex user requests, user feedback improves agreement among crowdworkers.","These findings emphasize the significance of user feedback in refining system evaluations and suggest the potential for automated feedback integration in future research.","We publicly release the annotated data to foster research in this area."],"url":"http://arxiv.org/abs/2404.12994v1","category":"cs.IR"}
{"created":"2024-04-19 16:42:43","title":"Quantum entanglement in the multicritical disordered Ising model","abstract":"Here, the entanglement entropy is calculated at the quantum multicritical point of the random transverse-field Ising model (RTIM). We use an efficient implementation of the strong disorder renormalization group method in two and three dimensions for two types of disorder. For cubic subsystems we find a universal logarithmic corner contribution to the area law b*ln(l) that is independent of the form of disorder. Our results agree qualitatively with those at the quantum critical points of the RTIM, but with new b prefactors due to having both geometric and quantum fluctuations at play. By studying the vicinity of the multicritical point, we demonstrate that the corner contribution serves as an `entanglement susceptibility', a useful tool to locate the phase transition and to measure the correlation length critical exponents.","sentences":["Here, the entanglement entropy is calculated at the quantum multicritical point of the random transverse-field Ising model (RTIM).","We use an efficient implementation of the strong disorder renormalization group method in two and three dimensions for two types of disorder.","For cubic subsystems we find a universal logarithmic corner contribution to the area law b*ln(l) that is independent of the form of disorder.","Our results agree qualitatively with those at the quantum critical points of the RTIM, but with new b prefactors due to having both geometric and quantum fluctuations at play.","By studying the vicinity of the multicritical point, we demonstrate that the corner contribution serves as an `entanglement susceptibility', a useful tool to locate the phase transition and to measure the correlation length critical exponents."],"url":"http://arxiv.org/abs/2404.12990v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-19 16:30:40","title":"Private Agent-Based Modeling","abstract":"The practical utility of agent-based models in decision-making relies on their capacity to accurately replicate populations while seamlessly integrating real-world data streams. Yet, the incorporation of such data poses significant challenges due to privacy concerns. To address this issue, we introduce a paradigm for private agent-based modeling wherein the simulation, calibration, and analysis of agent-based models can be achieved without centralizing the agents attributes or interactions. The key insight is to leverage techniques from secure multi-party computation to design protocols for decentralized computation in agent-based models. This ensures the confidentiality of the simulated agents without compromising on simulation accuracy. We showcase our protocols on a case study with an epidemiological simulation comprising over 150,000 agents. We believe this is a critical step towards deploying agent-based models to real-world applications.","sentences":["The practical utility of agent-based models in decision-making relies on their capacity to accurately replicate populations while seamlessly integrating real-world data streams.","Yet, the incorporation of such data poses significant challenges due to privacy concerns.","To address this issue, we introduce a paradigm for private agent-based modeling wherein the simulation, calibration, and analysis of agent-based models can be achieved without centralizing the agents attributes or interactions.","The key insight is to leverage techniques from secure multi-party computation to design protocols for decentralized computation in agent-based models.","This ensures the confidentiality of the simulated agents without compromising on simulation accuracy.","We showcase our protocols on a case study with an epidemiological simulation comprising over 150,000 agents.","We believe this is a critical step towards deploying agent-based models to real-world applications."],"url":"http://arxiv.org/abs/2404.12983v1","category":"cs.MA"}
{"created":"2024-04-19 16:21:26","title":"Spinor-valued Higgs fields","abstract":"We investigate the geometry of holomorphic vector bundles $E$ over a Riemann surface $C$ together with a section of the endomorphism bundle tensored with $K^{1/2}$ -- a square root of the canonical bundle $K$. These parallel to some extent the various features of usual Higgs bundles, such as spectral curve constructions, but some features are radically different. We make essential use of the mod 2 index to distinguish two families of moduli spaces, and provide examples in low genus.","sentences":["We investigate the geometry of holomorphic vector bundles $E$ over a Riemann surface $C$ together with a section of the endomorphism bundle tensored with $K^{1/2}$ -- a square root of the canonical bundle $K$. These parallel to some extent the various features of usual Higgs bundles, such as spectral curve constructions, but some features are radically different.","We make essential use of the mod 2 index to distinguish two families of moduli spaces, and provide examples in low genus."],"url":"http://arxiv.org/abs/2404.12981v1","category":"math.AG"}
{"created":"2024-04-19 16:09:17","title":"TRNet: Two-level Refinement Network leveraging Speech Enhancement for Noise Robust Speech Emotion Recognition","abstract":"One persistent challenge in Speech Emotion Recognition (SER) is the ubiquitous environmental noise, which frequently results in diminished SER performance in practical use. In this paper, we introduce a Two-level Refinement Network, dubbed TRNet, to address this challenge. Specifically, a pre-trained speech enhancement module is employed for front-end noise reduction and noise level estimation. Later, we utilize clean speech spectrograms and their corresponding deep representations as reference signals to refine the spectrogram distortion and representation shift of enhanced speech during model training. Experimental results validate that the proposed TRNet substantially increases the system's robustness in both matched and unmatched noisy environments, without compromising its performance in clean environments.","sentences":["One persistent challenge in Speech Emotion Recognition (SER) is the ubiquitous environmental noise, which frequently results in diminished SER performance in practical use.","In this paper, we introduce a Two-level Refinement Network, dubbed TRNet, to address this challenge.","Specifically, a pre-trained speech enhancement module is employed for front-end noise reduction and noise level estimation.","Later, we utilize clean speech spectrograms and their corresponding deep representations as reference signals to refine the spectrogram distortion and representation shift of enhanced speech during model training.","Experimental results validate that the proposed TRNet substantially increases the system's robustness in both matched and unmatched noisy environments, without compromising its performance in clean environments."],"url":"http://arxiv.org/abs/2404.12979v1","category":"cs.SD"}
{"created":"2024-04-19 16:09:11","title":"Strengthening Community Resilience by Modeling Transportation and Electric Power Network Interdependencies","abstract":"This study presents an agent-based model (ABM) developed to simulate the resilience of a community to hurricane-induced infrastructure disruptions, focusing on the interdependencies between electric power and transportation networks. In this ABM approach, agents represent the components of a system, where interactions within a system shape intra-dependency of a system and interactions among systems shape interdependencies. To study household resilience subject to a hurricane, a library of agents has been created including electric power network, transportation network, wind/flooding hazards, and household agents. The ABM is applied over the household and infrastructure data from a community (Zip code 33147) in Miami-Dade County, Florida. Interdependencies between the two networks are modeled in two ways, (i) representing the role of transportation in fuel delivery to power plants and restoration teams' access, (ii) impact of power outage on transportation network components. Restoring traffic signals quickly is crucial as their outage can slow down traffic and increase the chance of crashes. We simulate three restoration strategies: component based, distance based, and traffic lights based restoration. The model is validated against Hurricane Irma data, showing consistent behavior with varying hazard intensities. Scenario analyses explore the impact of restoration strategies, road accessibility, and wind speed intensities on power restoration. Results demonstrate that a traffic lights based restoration strategy efficiently prioritizes signal recovery without delaying household power restoration time. Restoration of power services will be faster if restoration teams do not need to wait due to inaccessible roads and fuel transportation to power plants is not delayed.","sentences":["This study presents an agent-based model (ABM) developed to simulate the resilience of a community to hurricane-induced infrastructure disruptions, focusing on the interdependencies between electric power and transportation networks.","In this ABM approach, agents represent the components of a system, where interactions within a system shape intra-dependency of a system and interactions among systems shape interdependencies.","To study household resilience subject to a hurricane, a library of agents has been created including electric power network, transportation network, wind/flooding hazards, and household agents.","The ABM is applied over the household and infrastructure data from a community (Zip code 33147) in Miami-Dade County, Florida.","Interdependencies between the two networks are modeled in two ways, (i) representing the role of transportation in fuel delivery to power plants and restoration teams' access, (ii) impact of power outage on transportation network components.","Restoring traffic signals quickly is crucial as their outage can slow down traffic and increase the chance of crashes.","We simulate three restoration strategies: component based, distance based, and traffic lights based restoration.","The model is validated against Hurricane Irma data, showing consistent behavior with varying hazard intensities.","Scenario analyses explore the impact of restoration strategies, road accessibility, and wind speed intensities on power restoration.","Results demonstrate that a traffic lights based restoration strategy efficiently prioritizes signal recovery without delaying household power restoration time.","Restoration of power services will be faster if restoration teams do not need to wait due to inaccessible roads and fuel transportation to power plants is not delayed."],"url":"http://arxiv.org/abs/2404.12978v1","category":"cs.CE"}
{"created":"2024-04-19 16:05:03","title":"Insights from the Gaussian Processes Method for the FRB-associated X-ray Burst of SGR 1935+2154","abstract":"Gaussian processes method is employed to analyze the light curves of bursts detected by Insight-HXMT, NICER, and GECAM from SGR 1935+2154 between 2020 to 2022. It is found that a stochastically driven damped simple harmonic oscillator (SHO) is necessary to capture the characteristics of the X-ray bursts. Variability timescale of the X-ray bursts, corresponding to the broken frequencies in the SHO power spectral densities (PSDs), are extracted. In particular, a high broken frequency of 35 Hz where the index of the SHO PSD changes from -4 to -2 is constrained by the HXMT-HE burst associated with FRB 200428. It is suggested that the corresponding timescale of 0.03 s could be the retarding timescale of the system driven by some kind of energy release, and the production of the HE photon should be quasi-simultaneous with the response. The other special event is a NICER burst with a retarding timescale of 1/39 Hz (0.02 s). In the normal X-ray bursts, no retarding timescale is constrained; a long relax/equilibrium timescale (corresponding to a broken frequency of 1-10 Hz where the index of the SHO PSD changing from -4/-2 to 0 in the SHO PSD) is obtained. The results indicate that the FRB-associated HXMT-HE X-ray burst could be produced immediately when the system is responding to the energy disturbance, far before the equilibrium state.","sentences":["Gaussian processes method is employed to analyze the light curves of bursts detected by Insight-HXMT, NICER, and GECAM from SGR 1935+2154 between 2020 to 2022.","It is found that a stochastically driven damped simple harmonic oscillator (SHO) is necessary to capture the characteristics of the X-ray bursts.","Variability timescale of the X-ray bursts, corresponding to the broken frequencies in the SHO power spectral densities (PSDs), are extracted.","In particular, a high broken frequency of 35 Hz where the index of the SHO PSD changes from -4 to -2 is constrained by the HXMT-HE burst associated with FRB 200428.","It is suggested that the corresponding timescale of 0.03 s could be the retarding timescale of the system driven by some kind of energy release, and the production of the HE photon should be quasi-simultaneous with the response.","The other special event is a NICER burst with a retarding timescale of 1/39 Hz (0.02 s).","In the normal X-ray bursts, no retarding timescale is constrained; a long relax/equilibrium timescale (corresponding to a broken frequency of 1-10 Hz where the index of the SHO PSD changing from -4/-2 to 0 in the SHO PSD) is obtained.","The results indicate that the FRB-associated HXMT-HE X-ray burst could be produced immediately when the system is responding to the energy disturbance, far before the equilibrium state."],"url":"http://arxiv.org/abs/2404.12976v1","category":"astro-ph.HE"}
{"created":"2024-04-19 16:01:00","title":"Cross-modal Diffusion Modelling for Super-resolved Spatial Transcriptomics","abstract":"The recent advancement of spatial transcriptomics (ST) allows to characterize spatial gene expression within tissue for discovery research. However, current ST platforms suffer from low resolution, hindering in-depth understanding of spatial gene expression. Super-resolution approaches promise to enhance ST maps by integrating histology images with gene expressions of profiled tissue spots. However, current super-resolution methods are limited by restoration uncertainty and mode collapse. Although diffusion models have shown promise in capturing complex interactions between multi-modal conditions, it remains a challenge to integrate histology images and gene expression for super-resolved ST maps. This paper proposes a cross-modal conditional diffusion model for super-resolving ST maps with the guidance of histology images. Specifically, we design a multi-modal disentangling network with cross-modal adaptive modulation to utilize complementary information from histology images and spatial gene expression. Moreover, we propose a dynamic cross-attention modelling strategy to extract hierarchical cell-to-tissue information from histology images. Lastly, we propose a co-expression-based gene-correlation graph network to model the co-expression relationship of multiple genes. Experiments show that our method outperforms other state-of-the-art methods in ST super-resolution on three public datasets.","sentences":["The recent advancement of spatial transcriptomics (ST) allows to characterize spatial gene expression within tissue for discovery research.","However, current ST platforms suffer from low resolution, hindering in-depth understanding of spatial gene expression.","Super-resolution approaches promise to enhance ST maps by integrating histology images with gene expressions of profiled tissue spots.","However, current super-resolution methods are limited by restoration uncertainty and mode collapse.","Although diffusion models have shown promise in capturing complex interactions between multi-modal conditions, it remains a challenge to integrate histology images and gene expression for super-resolved ST maps.","This paper proposes a cross-modal conditional diffusion model for super-resolving ST maps with the guidance of histology images.","Specifically, we design a multi-modal disentangling network with cross-modal adaptive modulation to utilize complementary information from histology images and spatial gene expression.","Moreover, we propose a dynamic cross-attention modelling strategy to extract hierarchical cell-to-tissue information from histology images.","Lastly, we propose a co-expression-based gene-correlation graph network to model the co-expression relationship of multiple genes.","Experiments show that our method outperforms other state-of-the-art methods in ST super-resolution on three public datasets."],"url":"http://arxiv.org/abs/2404.12973v1","category":"eess.IV"}
{"created":"2024-04-19 15:56:54","title":"FlyNeRF: NeRF-Based Aerial Mapping for High-Quality 3D Scene Reconstruction","abstract":"Current methods for 3D reconstruction and environmental mapping frequently face challenges in achieving high precision, highlighting the need for practical and effective solutions. In response to this issue, our study introduces FlyNeRF, a system integrating Neural Radiance Fields (NeRF) with drone-based data acquisition for high-quality 3D reconstruction. Utilizing unmanned aerial vehicle (UAV) for capturing images and corresponding spatial coordinates, the obtained data is subsequently used for the initial NeRF-based 3D reconstruction of the environment. Further evaluation of the reconstruction render quality is accomplished by the image evaluation neural network developed within the scope of our system. According to the results of the image evaluation module, an autonomous algorithm determines the position for additional image capture, thereby improving the reconstruction quality. The neural network introduced for render quality assessment demonstrates an accuracy of 97%. Furthermore, our adaptive methodology enhances the overall reconstruction quality, resulting in an average improvement of 2.5 dB in Peak Signal-to-Noise Ratio (PSNR) for the 10% quantile. The FlyNeRF demonstrates promising results, offering advancements in such fields as environmental monitoring, surveillance, and digital twins, where high-fidelity 3D reconstructions are crucial.","sentences":["Current methods for 3D reconstruction and environmental mapping frequently face challenges in achieving high precision, highlighting the need for practical and effective solutions.","In response to this issue, our study introduces FlyNeRF, a system integrating Neural Radiance Fields (NeRF) with drone-based data acquisition for high-quality 3D reconstruction.","Utilizing unmanned aerial vehicle (UAV) for capturing images and corresponding spatial coordinates, the obtained data is subsequently used for the initial NeRF-based 3D reconstruction of the environment.","Further evaluation of the reconstruction render quality is accomplished by the image evaluation neural network developed within the scope of our system.","According to the results of the image evaluation module, an autonomous algorithm determines the position for additional image capture, thereby improving the reconstruction quality.","The neural network introduced for render quality assessment demonstrates an accuracy of 97%.","Furthermore, our adaptive methodology enhances the overall reconstruction quality, resulting in an average improvement of 2.5 dB in Peak Signal-to-Noise Ratio (PSNR) for the 10% quantile.","The FlyNeRF demonstrates promising results, offering advancements in such fields as environmental monitoring, surveillance, and digital twins, where high-fidelity 3D reconstructions are crucial."],"url":"http://arxiv.org/abs/2404.12970v1","category":"cs.RO"}
{"created":"2024-04-19 15:54:15","title":"Scalable Data Assimilation with Message Passing","abstract":"Data assimilation is a core component of numerical weather prediction systems. The large quantity of data processed during assimilation requires the computation to be distributed across increasingly many compute nodes, yet existing approaches suffer from synchronisation overhead in this setting. In this paper, we exploit the formulation of data assimilation as a Bayesian inference problem and apply a message-passing algorithm to solve the spatial inference problem. Since message passing is inherently based on local computations, this approach lends itself to parallel and distributed computation. In combination with a GPU-accelerated implementation, we can scale the algorithm to very large grid sizes while retaining good accuracy and compute and memory requirements.","sentences":["Data assimilation is a core component of numerical weather prediction systems.","The large quantity of data processed during assimilation requires the computation to be distributed across increasingly many compute nodes, yet existing approaches suffer from synchronisation overhead in this setting.","In this paper, we exploit the formulation of data assimilation as a Bayesian inference problem and apply a message-passing algorithm to solve the spatial inference problem.","Since message passing is inherently based on local computations, this approach lends itself to parallel and distributed computation.","In combination with a GPU-accelerated implementation, we can scale the algorithm to very large grid sizes while retaining good accuracy and compute and memory requirements."],"url":"http://arxiv.org/abs/2404.12968v1","category":"cs.LG"}
{"created":"2024-04-19 15:45:41","title":"A comparison between single-stage and two-stage 3D tracking algorithms for greenhouse robotics","abstract":"With the current demand for automation in the agro-food industry, accurately detecting and localizing relevant objects in 3D is essential for successful robotic operations. However, this is a challenge due the presence of occlusions. Multi-view perception approaches allow robots to overcome occlusions, but a tracking component is needed to associate the objects detected by the robot over multiple viewpoints. Multi-object tracking (MOT) algorithms can be categorized between two-stage and single-stage methods. Two-stage methods tend to be simpler to adapt and implement to custom applications, while single-stage methods present a more complex end-to-end tracking method that can yield better results in occluded situations at the cost of more training data. The potential advantages of single-stage methods over two-stage methods depends on the complexity of the sequence of viewpoints that a robot needs to process. In this work, we compare a 3D two-stage MOT algorithm, 3D-SORT, against a 3D single-stage MOT algorithm, MOT-DETR, in three different types of sequences with varying levels of complexity. The sequences represent simpler and more complex motions that a robot arm can perform in a tomato greenhouse. Our experiments in a tomato greenhouse show that the single-stage algorithm consistently yields better tracking accuracy, especially in the more challenging sequences where objects are fully occluded or non-visible during several viewpoints.","sentences":["With the current demand for automation in the agro-food industry, accurately detecting and localizing relevant objects in 3D is essential for successful robotic operations.","However, this is a challenge due the presence of occlusions.","Multi-view perception approaches allow robots to overcome occlusions, but a tracking component is needed to associate the objects detected by the robot over multiple viewpoints.","Multi-object tracking (MOT) algorithms can be categorized between two-stage and single-stage methods.","Two-stage methods tend to be simpler to adapt and implement to custom applications, while single-stage methods present a more complex end-to-end tracking method that can yield better results in occluded situations at the cost of more training data.","The potential advantages of single-stage methods over two-stage methods depends on the complexity of the sequence of viewpoints that a robot needs to process.","In this work, we compare a 3D two-stage MOT algorithm, 3D-SORT, against a 3D single-stage MOT algorithm, MOT-DETR, in three different types of sequences with varying levels of complexity.","The sequences represent simpler and more complex motions that a robot arm can perform in a tomato greenhouse.","Our experiments in a tomato greenhouse show that the single-stage algorithm consistently yields better tracking accuracy, especially in the more challenging sequences where objects are fully occluded or non-visible during several viewpoints."],"url":"http://arxiv.org/abs/2404.12963v1","category":"cs.RO"}
{"created":"2024-04-19 15:41:22","title":"Intrinsic Limits of Charge Carrier Mobilities in Layered Halide Perovskites","abstract":"Layered halide perovskites have emerged as potential alternatives to three-dimensional halide perovskites due to their improved stability and larger material phase space, allowing fine-tuning of structural, electronic, and optical properties. However, their charge carrier mobilities are significantly smaller than that of three-dimensional halide perovskites, which has a considerable impact on their application in optoelectronic devices. Here, we employ state-of-the-art ab initio approaches to unveil the electron-phonon mechanisms responsible for the diminished transport properties of layered halide perovskites. Starting from a prototypical ABX$_{3}$ halide perovskite, we model the case of $n=1$ and $n=2$ layered structures and compare their electronic and transport properties to the three-dimensional reference. The electronic and phononic properties are investigated within density functional theory (DFT) and density functional perturbation theory (DFPT), while transport properties are obtained via the ab initio Boltzmann transport equation. The vibrational modes contributing to charge carrier scattering are investigated and associated with polar-phonon scattering mechanisms arising from the long-range Fr\\\"ohlich coupling and deformation potential scattering processes. Our investigation reveals that the lower mobilities in layered systems primarily originates from the increased electronic density of states at the vicinity of the band edges, while the electron-phonon coupling strength remains similar. Such increase is caused by the dimensionality reduction and the break in octahedra connectivity along the stacking direction. Our findings provide a fundamental understanding of the electron-phonon coupling mechanisms in layered perovskites and highlight the intrinsic limitations of the charge carrier transport in these materials.","sentences":["Layered halide perovskites have emerged as potential alternatives to three-dimensional halide perovskites due to their improved stability and larger material phase space, allowing fine-tuning of structural, electronic, and optical properties.","However, their charge carrier mobilities are significantly smaller than that of three-dimensional halide perovskites, which has a considerable impact on their application in optoelectronic devices.","Here, we employ state-of-the-art ab initio approaches to unveil the electron-phonon mechanisms responsible for the diminished transport properties of layered halide perovskites.","Starting from a prototypical ABX$_{3}$ halide perovskite, we model the case of $n=1$ and $n=2$ layered structures and compare their electronic and transport properties to the three-dimensional reference.","The electronic and phononic properties are investigated within density functional theory (DFT) and density functional perturbation theory (DFPT), while transport properties are obtained via the ab initio Boltzmann transport equation.","The vibrational modes contributing to charge carrier scattering are investigated and associated with polar-phonon scattering mechanisms arising from the long-range Fr\\\"ohlich coupling and deformation potential scattering processes.","Our investigation reveals that the lower mobilities in layered systems primarily originates from the increased electronic density of states at the vicinity of the band edges, while the electron-phonon coupling strength remains similar.","Such increase is caused by the dimensionality reduction and the break in octahedra connectivity along the stacking direction.","Our findings provide a fundamental understanding of the electron-phonon coupling mechanisms in layered perovskites and highlight the intrinsic limitations of the charge carrier transport in these materials."],"url":"http://arxiv.org/abs/2404.12960v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-19 14:58:37","title":"Fast Broadcast in Highly Connected Networks","abstract":"We revisit the classic broadcast problem, wherein we have $k$ messages, each composed of $O(\\log{n})$ bits, distributed arbitrarily across a network. The objective is to broadcast these messages to all nodes in the network. In the distributed CONGEST model, a textbook algorithm solves this problem in $O(D+k)$ rounds, where $D$ is the diameter of the graph. While the $O(D)$ term in the round complexity is unavoidable$\\unicode{x2014}$given that $\\Omega(D)$ rounds are necessary to solve broadcast in any graph$\\unicode{x2014}$it remains unclear whether the $O(k)$ term is needed in all graphs. In cases where the minimum cut size is one, simply transmitting messages from one side of the cut to the other would require $\\Omega(k)$ rounds. However, if the size of the minimum cut is larger, it may be possible to develop faster algorithms. This motivates the exploration of the broadcast problem in networks with high edge connectivity.   In this work, we present a simple randomized distributed algorithm for performing $k$-message broadcast in $O(((n+k)/\\lambda)\\log n)$ rounds in any $n$-node simple graph with edge connectivity $\\lambda$. When $k = \\Omega(n)$, our algorithm is universally optimal, up to an $O(\\log n)$ factor, as its complexity nearly matches an information-theoretic $\\Omega(k/\\lambda)$ lower bound that applies to all graphs, even when the network topology is known to the algorithm.   The setting $k = \\Omega(n)$ is particularly interesting because several fundamental problems can be reduced to broadcasting $\\Omega(n)$ messages. Our broadcast algorithm finds several applications in distributed computing, enabling $O(1)$-approximation for all distances and $(1+\\epsilon)$-approximation for all cut sizes in $\\tilde{O}(n/\\lambda)$ rounds.","sentences":["We revisit the classic broadcast problem, wherein we have $k$ messages, each composed of $O(\\log{n})$ bits, distributed arbitrarily across a network.","The objective is to broadcast these messages to all nodes in the network.","In the distributed CONGEST model, a textbook algorithm solves this problem in $O(D+k)$ rounds, where $D$ is the diameter of the graph.","While the $O(D)$ term in the round complexity is unavoidable$\\unicode{x2014}$given that $\\Omega(D)$ rounds are necessary to solve broadcast in any graph$\\unicode{x2014}$it remains unclear whether the $O(k)$ term is needed in all graphs.","In cases where the minimum cut size is one, simply transmitting messages from one side of the cut to the other would require $\\Omega(k)$ rounds.","However, if the size of the minimum cut is larger, it may be possible to develop faster algorithms.","This motivates the exploration of the broadcast problem in networks with high edge connectivity.   ","In this work, we present a simple randomized distributed algorithm for performing $k$-message broadcast in $O(((n+k)/\\lambda)\\log n)$ rounds in any $n$-node simple graph with edge connectivity $\\lambda$. When $k = \\Omega(n)$, our algorithm is universally optimal, up to an $O(\\log n)$ factor, as its complexity nearly matches an information-theoretic $\\Omega(k/\\lambda)$ lower bound that applies to all graphs, even when the network topology is known to the algorithm.   ","The setting $k = \\Omega(n)$ is particularly interesting because several fundamental problems can be reduced to broadcasting $\\Omega(n)$ messages.","Our broadcast algorithm finds several applications in distributed computing, enabling $O(1)$-approximation for all distances and $(1+\\epsilon)$-approximation for all cut sizes in $\\tilde{O}(n/\\lambda)$ rounds."],"url":"http://arxiv.org/abs/2404.12930v1","category":"cs.DC"}
{"created":"2024-04-19 14:58:20","title":"Diffusive contact between randomly driven colloidal suspensions","abstract":"We study the relaxation process of two driven colloidal suspensions in diffusive contact to a steady state, similar to thermalization. We start by studying a single suspension, subjecting it to random driving forces via holographic optical tweezers, which agitate it to a higher effective temperature. Interestingly, the effective temperature of the suspension, defined by the Einstein relation, exhibits a non-monotonic dependence on the driving frequency. Next, we follow the flux of particles between two such suspensions in diffusive contact, starting from a uniform density and relaxing to a state with zero net particle flux. The density remains uniform for systems with different frequencies but equal effective temperatures. At high driving frequencies, we show that the density distribution at steady state is determined by equating the ratio of the chemical potential to the effective temperature in both systems, mirroring thermal equilibrium behavior.","sentences":["We study the relaxation process of two driven colloidal suspensions in diffusive contact to a steady state, similar to thermalization.","We start by studying a single suspension, subjecting it to random driving forces via holographic optical tweezers, which agitate it to a higher effective temperature.","Interestingly, the effective temperature of the suspension, defined by the Einstein relation, exhibits a non-monotonic dependence on the driving frequency.","Next, we follow the flux of particles between two such suspensions in diffusive contact, starting from a uniform density and relaxing to a state with zero net particle flux.","The density remains uniform for systems with different frequencies but equal effective temperatures.","At high driving frequencies, we show that the density distribution at steady state is determined by equating the ratio of the chemical potential to the effective temperature in both systems, mirroring thermal equilibrium behavior."],"url":"http://arxiv.org/abs/2404.12929v1","category":"cond-mat.soft"}
{"created":"2024-04-19 14:55:15","title":"The Localized Active Space Method with Unitary Selective Coupled Cluster","abstract":"We introduce a hybrid quantum-classical algorithm, the localized active space unitary selective coupled cluster singles and doubles (LAS-USCCSD) method. Derived from the localized active space unitary coupled cluster (LAS-UCCSD) method, LAS-USCCSD first performs a classical LASSCF calculation, then selectively identifies the most important parameters (cluster amplitudes used to build the multireference UCC ansatz) for restoring inter-fragment interaction energy using this reduced set of parameters with the variational quantum eigensolver method. We benchmark LAS-USCCSD against LAS-UCCSD by calculating the total energies of $(\\mathrm{H}_2)_2$, $(\\mathrm{H}_2)_4$ and \\textit{trans}-butadiene, and the magnetic coupling constant for a bimetallic compound [Cr$_2$(OH)$_3$(NH$_3$)$_6$]$^{3+}$. For these systems, we find that LAS-USCCSD reduces the number of required parameters and thus the circuit depth by at least one order of magnitude, an aspect which is important for the practical implementation of multireference hybrid quantum-classical algorithms like LAS-UCCSD on near-term quantum computers.","sentences":["We introduce a hybrid quantum-classical algorithm, the localized active space unitary selective coupled cluster singles and doubles (LAS-USCCSD) method.","Derived from the localized active space unitary coupled cluster (LAS-UCCSD) method, LAS-USCCSD first performs a classical LASSCF calculation, then selectively identifies the most important parameters (cluster amplitudes used to build the multireference UCC ansatz) for restoring inter-fragment interaction energy using this reduced set of parameters with the variational quantum eigensolver method.","We benchmark LAS-USCCSD against LAS-UCCSD by calculating the total energies of $(\\mathrm{H}_2)_2$, $(\\mathrm{H}_2)_4$ and \\textit{trans}-butadiene, and the magnetic coupling constant for a bimetallic compound","[Cr$_2$(OH)$_3$(NH$_3$)$_6$]$^{3+}$. For these systems, we find that LAS-USCCSD reduces the number of required parameters and thus the circuit depth by at least one order of magnitude, an aspect which is important for the practical implementation of multireference hybrid quantum-classical algorithms like LAS-UCCSD on near-term quantum computers."],"url":"http://arxiv.org/abs/2404.12927v1","category":"quant-ph"}
{"created":"2024-04-19 14:52:14","title":"Probabilistic-Numeric SMC Sampling for Bayesian Nonlinear System Identification in Continuous Time","abstract":"In engineering, accurately modeling nonlinear dynamic systems from data contaminated by noise is both essential and complex. Established Sequential Monte Carlo (SMC) methods, used for the Bayesian identification of these systems, facilitate the quantification of uncertainty in the parameter identification process. A significant challenge in this context is the numerical integration of continuous-time ordinary differential equations (ODEs), crucial for aligning theoretical models with discretely sampled data. This integration introduces additional numerical uncertainty, a factor that is often over looked. To address this issue, the field of probabilistic numerics combines numerical methods, such as numerical integration, with probabilistic modeling to offer a more comprehensive analysis of total uncertainty. By retaining the accuracy of classical deterministic methods, these probabilistic approaches offer a deeper understanding of the uncertainty inherent in the inference process. This paper demonstrates the application of a probabilistic numerical method for solving ODEs in the joint parameter-state identification of nonlinear dynamic systems. The presented approach efficiently identifies latent states and system parameters from noisy measurements. Simultaneously incorporating probabilistic solutions to the ODE in the identification challenge. The methodology's primary advantage lies in its capability to produce posterior distributions over system parameters, thereby representing the inherent uncertainties in both the data and the identification process.","sentences":["In engineering, accurately modeling nonlinear dynamic systems from data contaminated by noise is both essential and complex.","Established Sequential Monte Carlo (SMC) methods, used for the Bayesian identification of these systems, facilitate the quantification of uncertainty in the parameter identification process.","A significant challenge in this context is the numerical integration of continuous-time ordinary differential equations (ODEs), crucial for aligning theoretical models with discretely sampled data.","This integration introduces additional numerical uncertainty, a factor that is often over looked.","To address this issue, the field of probabilistic numerics combines numerical methods, such as numerical integration, with probabilistic modeling to offer a more comprehensive analysis of total uncertainty.","By retaining the accuracy of classical deterministic methods, these probabilistic approaches offer a deeper understanding of the uncertainty inherent in the inference process.","This paper demonstrates the application of a probabilistic numerical method for solving ODEs in the joint parameter-state identification of nonlinear dynamic systems.","The presented approach efficiently identifies latent states and system parameters from noisy measurements.","Simultaneously incorporating probabilistic solutions to the ODE in the identification challenge.","The methodology's primary advantage lies in its capability to produce posterior distributions over system parameters, thereby representing the inherent uncertainties in both the data and the identification process."],"url":"http://arxiv.org/abs/2404.12923v1","category":"stat.ML"}
{"created":"2024-04-19 14:44:50","title":"Hydrodynamic Attractor in Ultracold Atoms","abstract":"The hydrodynamic attractor is a concept that describes universal equilibration behavior in which systems lose microscopic details before hydrodynamics becomes applicable. We propose a setup to observe hydrodynamic attractors in ultracold atomic gases, taking advantage of the fact that driving the two-body $s$-wave scattering length causes phenomena equivalent to isotropic fluid expansions. We specifically consider two-component fermions with contact interactions in three dimensions and discuss their dynamics under a power-law drive of the scattering length in a uniform system, employing a hydrodynamic relaxation model. We analytically solve their dynamics and find the hydrodynamic attractor solution. Our results establish the cold atom systems as a new platform for exploring hydrodynamic attractors.","sentences":["The hydrodynamic attractor is a concept that describes universal equilibration behavior in which systems lose microscopic details before hydrodynamics becomes applicable.","We propose a setup to observe hydrodynamic attractors in ultracold atomic gases, taking advantage of the fact that driving the two-body $s$-wave scattering length causes phenomena equivalent to isotropic fluid expansions.","We specifically consider two-component fermions with contact interactions in three dimensions and discuss their dynamics under a power-law drive of the scattering length in a uniform system, employing a hydrodynamic relaxation model.","We analytically solve their dynamics and find the hydrodynamic attractor solution.","Our results establish the cold atom systems as a new platform for exploring hydrodynamic attractors."],"url":"http://arxiv.org/abs/2404.12921v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-19 14:27:30","title":"Complexity of Weighted First-Order Model Counting in the Two-Variable Fragment with Counting Quantifiers: A Bound to Beat","abstract":"We study the time complexity of the weighted first-order model counting (WFOMC) over the logical language with two variables and counting quantifiers. The problem is known to be solvable in time polynomial in the domain size. However, the degree of the polynomial, which turns out to be relatively high for most practical applications, has never been properly addressed. First, we formulate a time complexity bound for the existing techniques for solving WFOMC with counting quantifiers. The bound is already known to be a polynomial with its degree depending on the number of cells of the input formula. We observe that the number of cells depends, in turn, exponentially on the parameter of the counting quantifiers appearing in the formula. Second, we propose a new approach to dealing with counting quantifiers, reducing the exponential dependency to a quadratic one, therefore obtaining a tighter upper bound. It remains an open question whether the dependency of the polynomial degree on the counting quantifiers can be reduced further, thus making our new bound a bound to beat.","sentences":["We study the time complexity of the weighted first-order model counting (WFOMC) over the logical language with two variables and counting quantifiers.","The problem is known to be solvable in time polynomial in the domain size.","However, the degree of the polynomial, which turns out to be relatively high for most practical applications, has never been properly addressed.","First, we formulate a time complexity bound for the existing techniques for solving WFOMC with counting quantifiers.","The bound is already known to be a polynomial with its degree depending on the number of cells of the input formula.","We observe that the number of cells depends, in turn, exponentially on the parameter of the counting quantifiers appearing in the formula.","Second, we propose a new approach to dealing with counting quantifiers, reducing the exponential dependency to a quadratic one, therefore obtaining a tighter upper bound.","It remains an open question whether the dependency of the polynomial degree on the counting quantifiers can be reduced further, thus making our new bound a bound to beat."],"url":"http://arxiv.org/abs/2404.12905v1","category":"cs.LO"}
{"created":"2024-04-19 14:11:32","title":"Bayesian Co-navigation: Dynamic Designing of the Materials Digital Twins via Active Learning","abstract":"Scientific advancement is universally based on the dynamic interplay between theoretical insights, modelling, and experimental discoveries. However, this feedback loop is often slow, including delayed community interactions and the gradual integration of experimental data into theoretical frameworks. This challenge is particularly exacerbated in domains dealing with high-dimensional object spaces, such as molecules and complex microstructures. Hence, the integration of theory within automated and autonomous experimental setups, or theory in the loop automated experiment, is emerging as a crucial objective for accelerating scientific research. The critical aspect is not only to use theory but also on-the-fly theory updates during the experiment. Here, we introduce a method for integrating theory into the loop through Bayesian co-navigation of theoretical model space and experimentation. Our approach leverages the concurrent development of surrogate models for both simulation and experimental domains at the rates determined by latencies and costs of experiments and computation, alongside the adjustment of control parameters within theoretical models to minimize epistemic uncertainty over the experimental object spaces. This methodology facilitates the creation of digital twins of material structures, encompassing both the surrogate model of behavior that includes the correlative part and the theoretical model itself. While demonstrated here within the context of functional responses in ferroelectric materials, our approach holds promise for broader applications, the exploration of optical properties in nanoclusters, microstructure-dependent properties in complex materials, and properties of molecular systems. The analysis code that supports the funding is publicly available at https://github.com/Slautin/2024_Co-navigation/tree/main","sentences":["Scientific advancement is universally based on the dynamic interplay between theoretical insights, modelling, and experimental discoveries.","However, this feedback loop is often slow, including delayed community interactions and the gradual integration of experimental data into theoretical frameworks.","This challenge is particularly exacerbated in domains dealing with high-dimensional object spaces, such as molecules and complex microstructures.","Hence, the integration of theory within automated and autonomous experimental setups, or theory in the loop automated experiment, is emerging as a crucial objective for accelerating scientific research.","The critical aspect is not only to use theory but also on-the-fly theory updates during the experiment.","Here, we introduce a method for integrating theory into the loop through Bayesian co-navigation of theoretical model space and experimentation.","Our approach leverages the concurrent development of surrogate models for both simulation and experimental domains at the rates determined by latencies and costs of experiments and computation, alongside the adjustment of control parameters within theoretical models to minimize epistemic uncertainty over the experimental object spaces.","This methodology facilitates the creation of digital twins of material structures, encompassing both the surrogate model of behavior that includes the correlative part and the theoretical model itself.","While demonstrated here within the context of functional responses in ferroelectric materials, our approach holds promise for broader applications, the exploration of optical properties in nanoclusters, microstructure-dependent properties in complex materials, and properties of molecular systems.","The analysis code that supports the funding is publicly available at https://github.com/Slautin/2024_Co-navigation/tree/main"],"url":"http://arxiv.org/abs/2404.12899v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-19 13:51:40","title":"A Machine Learning-Based Error Mitigation Approach For Reliable Software Development On IBM'S Quantum Computers","abstract":"Quantum computers have the potential to outperform classical computers for some complex computational problems. However, current quantum computers (e.g., from IBM and Google) have inherent noise that results in errors in the outputs of quantum software executing on the quantum computers, affecting the reliability of quantum software development. The industry is increasingly interested in machine learning (ML)--based error mitigation techniques, given their scalability and practicality. However, existing ML-based techniques have limitations, such as only targeting specific noise types or specific quantum circuits. This paper proposes a practical ML-based approach, called Q-LEAR, with a novel feature set, to mitigate noise errors in quantum software outputs. We evaluated Q-LEAR on eight quantum computers and their corresponding noisy simulators, all from IBM, and compared Q-LEAR with a state-of-the-art ML-based approach taken as baseline. Results show that, compared to the baseline, Q-LEAR achieved a 25% average improvement in error mitigation on both real quantum computers and simulators. We also discuss the implications and practicality of Q-LEAR, which, we believe, is valuable for practitioners.","sentences":["Quantum computers have the potential to outperform classical computers for some complex computational problems.","However, current quantum computers (e.g., from IBM and Google) have inherent noise that results in errors in the outputs of quantum software executing on the quantum computers, affecting the reliability of quantum software development.","The industry is increasingly interested in machine learning (ML)--based error mitigation techniques, given their scalability and practicality.","However, existing ML-based techniques have limitations, such as only targeting specific noise types or specific quantum circuits.","This paper proposes a practical ML-based approach, called Q-LEAR, with a novel feature set, to mitigate noise errors in quantum software outputs.","We evaluated Q-LEAR on eight quantum computers and their corresponding noisy simulators, all from IBM, and compared Q-LEAR with a state-of-the-art ML-based approach taken as baseline.","Results show that, compared to the baseline, Q-LEAR achieved a 25% average improvement in error mitigation on both real quantum computers and simulators.","We also discuss the implications and practicality of Q-LEAR, which, we believe, is valuable for practitioners."],"url":"http://arxiv.org/abs/2404.12892v1","category":"cs.SE"}
{"created":"2024-04-19 13:47:18","title":"Orbital Stability of Optical Solitons in 2d","abstract":"We present a stability result for ground states of a Schrodinger-Poisson system in $(2+1)$ dimension, modelling the propagation of a light beam through a liquid crystal with nonlocal nonlinear response. A new estimate for perturbations of the medium configuration allows to explicitly prove strict positivity of the second derivative of the action on a ground state. In addition we prove existence of a ground state with frequency $\\sigma$ for any $\\sigma \\in (0,1)$ by a continuity method.","sentences":["We present a stability result for ground states of a Schrodinger-Poisson system in $(2+1)$ dimension, modelling the propagation of a light beam through a liquid crystal with nonlocal nonlinear response.","A new estimate for perturbations of the medium configuration allows to explicitly prove strict positivity of the second derivative of the action on a ground state.","In addition we prove existence of a ground state with frequency $\\sigma$ for any $\\sigma \\in (0,1)$ by a continuity method."],"url":"http://arxiv.org/abs/2404.12890v1","category":"math.AP"}
{"created":"2024-04-19 13:26:04","title":"Torsors on moduli spaces of principal $G$-bundles","abstract":"Let $G$ be a semisimple complex algebraic group with a simple Lie algebra $\\mathfrak{g}$, and let $\\mathcal{M}^0_{G}$ denote the moduli stack of topologically trivial stable $G$-bundles on a smooth projective curve $C$. Fix a theta characteristic $\\kappa$ on $C$ which is even in case $\\dim{\\mathfrak{g}}$ is odd. We show that there is a nonempty Zariski open substack ${\\mathcal U}_\\kappa$ of $\\mathcal{M}^0_{G}$ such that $H^i(C,\\, \\text{ad}(E_G)\\otimes\\kappa) \\,=\\, 0$, $i\\,=\\, 1,\\, 2$, for all $E_G\\,\\in\\, {\\mathcal U}_\\kappa$. It is shown that any such $E_G$ has a canonical connection. It is also shown that the tangent bundle $T{U}_\\kappa$ has a natural splitting, where $U_{\\kappa}$ is the restriction of $\\mathcal{U}_{\\kappa}$ to the semi-stable locus. We also produce an isomorphism between two naturally occurring $\\Omega^1_{{M}^{rs}_{G}}$--torsors on the moduli space of regularly stable ${M}^{rs}_{G}$.","sentences":["Let $G$ be a semisimple complex algebraic group with a simple Lie algebra $\\mathfrak{g}$, and let $\\mathcal{M}^0_{G}$ denote the moduli stack of topologically trivial stable $G$-bundles on a smooth projective curve $C$.","Fix a theta characteristic $\\kappa$ on $C$ which is even in case $\\dim{\\mathfrak{g}}$ is odd.","We show that there is a nonempty Zariski open substack ${\\mathcal U}_\\kappa$ of $\\mathcal{M}^0_{G}$ such that $H^i(C,\\, \\text{ad}(E_G)\\otimes\\kappa) \\,=\\, 0$, $i\\,=\\, 1,\\, 2$, for all $E_G\\,\\in\\, {\\mathcal U}_\\kappa$.","It is shown that any such $E_G$ has a canonical connection.","It is also shown that the tangent bundle $T{U}_\\kappa$ has a natural splitting, where $U_{\\kappa}$ is the restriction of $\\mathcal{U}_{\\kappa}$ to the semi-stable locus.","We also produce an isomorphism between two naturally occurring $\\Omega^1_{{M}^{rs}_{G}}$--torsors on the moduli space of regularly stable ${M}^{rs}_{G}$."],"url":"http://arxiv.org/abs/2404.12877v1","category":"math.AG"}
{"created":"2024-04-19 13:20:13","title":"Physical Layer Authentication Using Information Reconciliation","abstract":"User authentication in future wireless communication networks is expected to become more complicated due to their large scale and heterogeneity. Furthermore, the computational complexity of classical cryptographic approaches based on public key distribution can be a limiting factor for using in simple, low-end Internet of things (IoT) devices. This paper proposes physical layer authentication (PLA) expected to complement existing traditional approaches, e.g., in multi-factor authentication protocols. The precision and consistency of PLA is impacted because of random variations of wireless channel realizations between different time slots, which can impair authentication performance. In order to address this, a method based on error-correcting codes in the form of reconciliation is considered in this work. In particular, we adopt distributed source coding (Slepian-Wolf) reconciliation using polar codes to reconcile channel measurements spread in time. Hypothesis testing is then applied to the reconciled vectors to accept or reject the device as authenticated. Simulation results show that the proposed PLA using reconciliation outperforms prior schemes even in low signal-to-noise ratio scenarios.","sentences":["User authentication in future wireless communication networks is expected to become more complicated due to their large scale and heterogeneity.","Furthermore, the computational complexity of classical cryptographic approaches based on public key distribution can be a limiting factor for using in simple, low-end Internet of things (IoT) devices.","This paper proposes physical layer authentication (PLA) expected to complement existing traditional approaches, e.g., in multi-factor authentication protocols.","The precision and consistency of PLA is impacted because of random variations of wireless channel realizations between different time slots, which can impair authentication performance.","In order to address this, a method based on error-correcting codes in the form of reconciliation is considered in this work.","In particular, we adopt distributed source coding (Slepian-Wolf) reconciliation using polar codes to reconcile channel measurements spread in time.","Hypothesis testing is then applied to the reconciled vectors to accept or reject the device as authenticated.","Simulation results show that the proposed PLA using reconciliation outperforms prior schemes even in low signal-to-noise ratio scenarios."],"url":"http://arxiv.org/abs/2404.12874v1","category":"eess.SP"}
{"created":"2024-04-19 13:17:06","title":"Expanding the Katz Index for Link Prediction: A Case Study on a Live Fish Movement Network","abstract":"In aquaculture, disease spread models often neglect the dynamic interactions between farms, hindering accuracy. This study enhances the Katz index (KI) to incorporate spatial and temporal patterns of fish movement, improving the prediction of farms susceptible to disease via live fish transfers. We modified the Katz index to create models like the Weighted Katz Index (WKI), Edge Weighted Katz Index (EWKI), and combined models (e.g., KIEWKI). These incorporate spatial distances and temporal movement patterns for a comprehensive aquaculture network connection prediction framework. Model performance was evaluated using precision, recall, F1-scores, AUPR, and AUROC. The EWKI model significantly outperformed the traditional KI and other variations. It achieved high precision (0.988), recall (0.712), F1-score (0.827), and AUPR (0.970). Combined models (KIEWKI, WKIEWKI) approached, but couldn't surpass, EWKI performance. This study highlights the value of extending Katz index models to improve disease spread predictions in aquaculture networks. The EWKI model's performance demonstrates an innovative and flexible approach to tackling spatial challenges within network analysis.","sentences":["In aquaculture, disease spread models often neglect the dynamic interactions between farms, hindering accuracy.","This study enhances the Katz index (KI) to incorporate spatial and temporal patterns of fish movement, improving the prediction of farms susceptible to disease via live fish transfers.","We modified the Katz index to create models like the Weighted Katz Index (WKI), Edge Weighted Katz Index (EWKI), and combined models (e.g., KIEWKI).","These incorporate spatial distances and temporal movement patterns for a comprehensive aquaculture network connection prediction framework.","Model performance was evaluated using precision, recall, F1-scores, AUPR, and AUROC.","The EWKI model significantly outperformed the traditional KI and other variations.","It achieved high precision (0.988), recall (0.712), F1-score (0.827), and AUPR (0.970).","Combined models (KIEWKI, WKIEWKI) approached, but couldn't surpass, EWKI performance.","This study highlights the value of extending Katz index models to improve disease spread predictions in aquaculture networks.","The EWKI model's performance demonstrates an innovative and flexible approach to tackling spatial challenges within network analysis."],"url":"http://arxiv.org/abs/2404.12871v1","category":"cs.SI"}
{"created":"2024-04-19 13:08:43","title":"FipTR: A Simple yet Effective Transformer Framework for Future Instance Prediction in Autonomous Driving","abstract":"The future instance prediction from a Bird's Eye View(BEV) perspective is a vital component in autonomous driving, which involves future instance segmentation and instance motion prediction. Existing methods usually rely on a redundant and complex pipeline which requires multiple auxiliary outputs and post-processing procedures. Moreover, estimated errors on each of the auxiliary predictions will lead to degradation of the prediction performance. In this paper, we propose a simple yet effective fully end-to-end framework named Future Instance Prediction Transformer(FipTR), which views the task as BEV instance segmentation and prediction for future frames. We propose to adopt instance queries representing specific traffic participants to directly estimate the corresponding future occupied masks, and thus get rid of complex post-processing procedures. Besides, we devise a flow-aware BEV predictor for future BEV feature prediction composed of a flow-aware deformable attention that takes backward flow guiding the offset sampling. A novel future instance matching strategy is also proposed to further improve the temporal coherence. Extensive experiments demonstrate the superiority of FipTR and its effectiveness under different temporal BEV encoders.","sentences":["The future instance prediction from a Bird's Eye View(BEV) perspective is a vital component in autonomous driving, which involves future instance segmentation and instance motion prediction.","Existing methods usually rely on a redundant and complex pipeline which requires multiple auxiliary outputs and post-processing procedures.","Moreover, estimated errors on each of the auxiliary predictions will lead to degradation of the prediction performance.","In this paper, we propose a simple yet effective fully end-to-end framework named Future Instance Prediction Transformer(FipTR), which views the task as BEV instance segmentation and prediction for future frames.","We propose to adopt instance queries representing specific traffic participants to directly estimate the corresponding future occupied masks, and thus get rid of complex post-processing procedures.","Besides, we devise a flow-aware BEV predictor for future BEV feature prediction composed of a flow-aware deformable attention that takes backward flow guiding the offset sampling.","A novel future instance matching strategy is also proposed to further improve the temporal coherence.","Extensive experiments demonstrate the superiority of FipTR and its effectiveness under different temporal BEV encoders."],"url":"http://arxiv.org/abs/2404.12867v1","category":"cs.CV"}
{"created":"2024-04-19 13:02:04","title":"Grid-aware Scheduling and Control of Electric Vehicle Charging Stations for Dispatching Active Distribution Networks. Part-I: Day-ahead and Numerical Validation","abstract":"This paper proposes a grid-aware scheduling and control framework for Electric Vehicle Charging Stations (EVCSs) for dispatching the operation of an active power distribution network. The framework consists of two stages. In the first stage, we determine an optimal day-ahead power schedule at the grid connection point (GCP), referred to as the dispatch plan. Then, in the second stage, a real-time model predictive control is proposed to track the day-ahead dispatch plan using flexibility from EVCSs. The dispatch plan accounts for the uncertainties of vehicles connected to the EVCS along with other uncontrollable power injections, by day-ahead predicted scenarios. We propose using a Gaussian-Mixture-Model (GMM) for the forecasting of EVCS demand using the historical dataset on arrival, departure times, EV battery capacity, State-of-Charge (SoC) targets, etc. The framework ensures that the grid is operated within its voltage and branches power-flow operational bounds, modeled by a linearized optimal power-flow model, maintaining the tractability of the problem formulation. The scheme is numerically and experimentally validated on a real-life distribution network at the EPFL connected to two EVCSs, two batteries, three photovoltaic plants, and multiple heterogeneous loads. The day-ahead and real-time stages are described in Part-I and Part-II papers respectively.","sentences":["This paper proposes a grid-aware scheduling and control framework for Electric Vehicle Charging Stations (EVCSs) for dispatching the operation of an active power distribution network.","The framework consists of two stages.","In the first stage, we determine an optimal day-ahead power schedule at the grid connection point (GCP), referred to as the dispatch plan.","Then, in the second stage, a real-time model predictive control is proposed to track the day-ahead dispatch plan using flexibility from EVCSs.","The dispatch plan accounts for the uncertainties of vehicles connected to the EVCS along with other uncontrollable power injections, by day-ahead predicted scenarios.","We propose using a Gaussian-Mixture-Model (GMM) for the forecasting of EVCS demand using the historical dataset on arrival, departure times, EV battery capacity, State-of-Charge (SoC) targets, etc.","The framework ensures that the grid is operated within its voltage and branches power-flow operational bounds, modeled by a linearized optimal power-flow model, maintaining the tractability of the problem formulation.","The scheme is numerically and experimentally validated on a real-life distribution network at the EPFL connected to two EVCSs, two batteries, three photovoltaic plants, and multiple heterogeneous loads.","The day-ahead and real-time stages are described in Part-I and Part-II papers respectively."],"url":"http://arxiv.org/abs/2404.12863v1","category":"eess.SY"}
{"created":"2024-04-19 13:00:44","title":"Nonreciprocal PT-symmetric phase transition in a non-Hermitian chiral quantum optical system","abstract":"Phase transitions, non-Hermiticity and nonreciprocity play central roles in fundamental physics. However, the triple interplay of these three fields is of lack in the quantum domain. Here, we show nonreciprocal parity-time-symmetric phase transition in a non-Hermitian chiral quantum electrodynamical system, caused by the directional system dissipation. In remarkable contrast to previously reported nonreciprocal phase transitions, the nonreciprocal parity-time-symmetric phases appear even when the atom-resonator coupling is reciprocal. Nonreciprocal photon blockade is obtained in the nonreciprocal phase region. These results may deepen the fundamental insight of nonreciprocal and non-Hermitian quantum physics, and also open a new door for unconventional quantum manipulation.","sentences":["Phase transitions, non-Hermiticity and nonreciprocity play central roles in fundamental physics.","However, the triple interplay of these three fields is of lack in the quantum domain.","Here, we show nonreciprocal parity-time-symmetric phase transition in a non-Hermitian chiral quantum electrodynamical system, caused by the directional system dissipation.","In remarkable contrast to previously reported nonreciprocal phase transitions, the nonreciprocal parity-time-symmetric phases appear even when the atom-resonator coupling is reciprocal.","Nonreciprocal photon blockade is obtained in the nonreciprocal phase region.","These results may deepen the fundamental insight of nonreciprocal and non-Hermitian quantum physics, and also open a new door for unconventional quantum manipulation."],"url":"http://arxiv.org/abs/2404.12860v1","category":"quant-ph"}
{"created":"2024-04-19 12:43:32","title":"Migrating Software Systems towards Post-Quantum-Cryptography -- A Systematic Literature Review","abstract":"Networks such as the Internet are essential for our connected world. Quantum computing poses a threat to this heterogeneous infrastructure since it threatens fundamental security mechanisms. Therefore, a migration to post-quantum-cryptography (PQC) is necessary for networks and their components. At the moment, there is little knowledge on how such migrations should be structured and implemented in practice. Our systematic literature review addresses migration approaches for IP networks towards PQC. It surveys papers about the migration process and exemplary real-world software system migrations. On the process side, we found that terminology, migration steps, and roles are not defined precisely or consistently across the literature. Still, we identified four major phases and appropriate substeps which we matched with also emerging archetypes of roles. In terms of real-world migrations, we see that reports used several different PQC implementations and hybrid solutions for migrations of systems belonging to a wide range of system types. Across all papers we noticed three major challenges for adopters: missing experience of PQC and a high realization effort, concerns about the security of the upcoming system, and finally, high complexity. Our findings indicate that recent standardization efforts already push quantum-safe networking forward. However, the literature is still not in consensus about definitions and best practices. Implementations are mostly experimental and not necessarily practical, leading to an overall chaotic situation. To better grasp this fast moving field of (applied) research, our systematic literature review provides a comprehensive overview of its current state and serves as a starting point for delving into the matter of PQC migration.","sentences":["Networks such as the Internet are essential for our connected world.","Quantum computing poses a threat to this heterogeneous infrastructure since it threatens fundamental security mechanisms.","Therefore, a migration to post-quantum-cryptography (PQC) is necessary for networks and their components.","At the moment, there is little knowledge on how such migrations should be structured and implemented in practice.","Our systematic literature review addresses migration approaches for IP networks towards PQC.","It surveys papers about the migration process and exemplary real-world software system migrations.","On the process side, we found that terminology, migration steps, and roles are not defined precisely or consistently across the literature.","Still, we identified four major phases and appropriate substeps which we matched with also emerging archetypes of roles.","In terms of real-world migrations, we see that reports used several different PQC implementations and hybrid solutions for migrations of systems belonging to a wide range of system types.","Across all papers we noticed three major challenges for adopters: missing experience of PQC and a high realization effort, concerns about the security of the upcoming system, and finally, high complexity.","Our findings indicate that recent standardization efforts already push quantum-safe networking forward.","However, the literature is still not in consensus about definitions and best practices.","Implementations are mostly experimental and not necessarily practical, leading to an overall chaotic situation.","To better grasp this fast moving field of (applied) research, our systematic literature review provides a comprehensive overview of its current state and serves as a starting point for delving into the matter of PQC migration."],"url":"http://arxiv.org/abs/2404.12854v1","category":"cs.CR"}
{"created":"2024-04-19 12:21:00","title":"On the $L^2$ volume of Bergman spaces","abstract":"In this paper, we show that the Calabi volume and Mabuchi volume of Bergman spaces on the product of a projective manifold and a projective space is infinite. Our result is inspired by a conjecture of Shiffman-Zelditch in [arXiv:2303.11559].","sentences":["In this paper, we show that the Calabi volume and Mabuchi volume of Bergman spaces on the product of a projective manifold and a projective space is infinite.","Our result is inspired by a conjecture of Shiffman-Zelditch in [arXiv:2303.11559]."],"url":"http://arxiv.org/abs/2404.12840v1","category":"math.CV"}
{"created":"2024-04-19 12:07:49","title":"Optimal Training Design for Over-the-Air Polynomial Power Amplifier Model Estimation","abstract":"The current evolution towards a massive number of antennas and a large variety of transceiver architectures forces to revisit the conventional techniques used to improve the fundamental power amplifier (PA) linearity-efficiency trade-off. Most of the digital linearization techniques rely on PA measurements using a dedicated feedback receiver. However, in modern systems with large amount of RF chains and high carrier frequency, dedicated receiver per RF chain is costly and complex to implement. This issue can be addressed by measuring PAs over the air, but in that case, this extra signalling is sharing resources with the actual data transmission. In this paper, we look at the problem from an estimation theory point of view so as to minimize pilot overhead while optimizing estimation performance. We show that conventional results in the mathematical statistics community can be used. We find the least squares (LS) optimal training design, minimizing the maximal mean squared error (MSE) of the reconstructed PA response over its whole input range. As compared to uniform training, simulations demonstrate a factor 10 reduction of the maximal MSE for a L = 7 PA polynomial order. Using prior information, the LMMSE estimator can achieve an additional gain of a factor up to 300 at low signal-to-noise ratio (SNR).","sentences":["The current evolution towards a massive number of antennas and a large variety of transceiver architectures forces to revisit the conventional techniques used to improve the fundamental power amplifier (PA) linearity-efficiency trade-off.","Most of the digital linearization techniques rely on PA measurements using a dedicated feedback receiver.","However, in modern systems with large amount of RF chains and high carrier frequency, dedicated receiver per RF chain is costly and complex to implement.","This issue can be addressed by measuring PAs over the air, but in that case, this extra signalling is sharing resources with the actual data transmission.","In this paper, we look at the problem from an estimation theory point of view so as to minimize pilot overhead while optimizing estimation performance.","We show that conventional results in the mathematical statistics community can be used.","We find the least squares (LS) optimal training design, minimizing the maximal mean squared error (MSE) of the reconstructed PA response over its whole input range.","As compared to uniform training, simulations demonstrate a factor 10 reduction of the maximal MSE for a L = 7 PA polynomial order.","Using prior information, the LMMSE estimator can achieve an additional gain of a factor up to 300 at low signal-to-noise ratio (SNR)."],"url":"http://arxiv.org/abs/2404.12830v1","category":"eess.SP"}
{"created":"2024-04-19 12:00:11","title":"360\u00b0 phase detector cell for measurement systems based on switched dual multipliers","abstract":"This letter presents a 360{\\deg} phase detector cell for performing phase-shift measurements on multiple output systems. An analog phase detector, capable of detecting a maximum range of {\\pm}90{\\deg}, has been used to perform a double multiplication of two signals, both in-phase and phase-shifted. The proposed solution broadens the frequency range beyond other solutions that require to fulfill the quadrature condition. Subsequently, the possibility of reaching the theoretical limit of phase shift within a hybrid coupler ({\\Phi} < 90{\\deg} {\\pm} 90{\\deg}) is discussed by using four straight-line equations to characterize the phase detector response. The proposed solution allows to extend up to 360{\\deg} the phase detection range and provide an increased immunity with respect to both impedance mismatching and phase deviations within the hybrid coupler. To demonstrate the feasibility of the proposed design, a phase detector cell prototype has been implemented using a commercial hybrid coupler with a phase shift of 92.5{\\deg} {\\pm} 0.5{\\deg} at 3.1-5.9 GHz, an external switch and a microcontroller with 2 kB of memory. Measurements show a range of detection of 360{\\deg} ({\\pm}180{\\deg}) across the tested frequency band of 2.7-6 GHz.","sentences":["This letter presents a 360{\\deg} phase detector cell for performing phase-shift measurements on multiple output systems.","An analog phase detector, capable of detecting a maximum range of {\\pm}90{\\deg}, has been used to perform a double multiplication of two signals, both in-phase and phase-shifted.","The proposed solution broadens the frequency range beyond other solutions that require to fulfill the quadrature condition.","Subsequently, the possibility of reaching the theoretical limit of phase shift within a hybrid coupler ({\\Phi} < 90{\\deg} {\\pm} 90{\\deg}) is discussed by using four straight-line equations to characterize the phase detector response.","The proposed solution allows to extend up to 360{\\deg} the phase detection range and provide an increased immunity with respect to both impedance mismatching and phase deviations within the hybrid coupler.","To demonstrate the feasibility of the proposed design, a phase detector cell prototype has been implemented using a commercial hybrid coupler with a phase shift of 92.5{\\deg} {\\pm} 0.5{\\deg} at 3.1-5.9 GHz, an external switch and a microcontroller with 2 kB of memory.","Measurements show a range of detection of 360{\\deg} ({\\pm}180{\\deg}) across the tested frequency band of 2.7-6 GHz."],"url":"http://arxiv.org/abs/2404.12825v1","category":"eess.SY"}
{"created":"2024-04-19 11:57:32","title":"Benchmarking the performance of a self-custody, non-ledger-based, obliviously managed digital payment system","abstract":"As global governments intensify efforts to operationalize retail central bank digital currencies (CBDCs), the imperative for architectures that preserve user privacy has never been more pronounced. This paper advances an existing retail CBDC framework developed at University College London. Utilizing the capabilities of the Comet research framework, our proposed design allows users to retain direct custody of their assets without the need for intermediary service providers, all while preserving transactional anonymity. The study unveils a novel technique to expedite the retrieval of Proof of Provenance, significantly accelerating the verification of transaction legitimacy through the refinement of Merkle Trie structures. In parallel, we introduce a streamlined Digital Ledger designed to offer fast, immutable, and decentralized transaction validation within a permissioned ecosystem. The ultimate objective of this research is to benchmark the performance of the legacy system formulated by the original Comet research team against the newly devised system elucidated in this paper. Our endeavour is to establish a foundational design for a scalable national infrastructure proficient in seamlessly processing thousands of transactions in real-time, without compromising consumer privacy or data integrity.","sentences":["As global governments intensify efforts to operationalize retail central bank digital currencies (CBDCs), the imperative for architectures that preserve user privacy has never been more pronounced.","This paper advances an existing retail CBDC framework developed at University College London.","Utilizing the capabilities of the Comet research framework, our proposed design allows users to retain direct custody of their assets without the need for intermediary service providers, all while preserving transactional anonymity.","The study unveils a novel technique to expedite the retrieval of Proof of Provenance, significantly accelerating the verification of transaction legitimacy through the refinement of Merkle Trie structures.","In parallel, we introduce a streamlined Digital Ledger designed to offer fast, immutable, and decentralized transaction validation within a permissioned ecosystem.","The ultimate objective of this research is to benchmark the performance of the legacy system formulated by the original Comet research team against the newly devised system elucidated in this paper.","Our endeavour is to establish a foundational design for a scalable national infrastructure proficient in seamlessly processing thousands of transactions in real-time, without compromising consumer privacy or data integrity."],"url":"http://arxiv.org/abs/2404.12821v1","category":"cs.CY"}
{"created":"2024-04-19 11:56:02","title":"Aggregator of Electric Vehicles Bidding in Nordic FCR-D Markets: A Chance-Constrained Program","abstract":"Recently, two new innovative regulations in the Nordic ancillary service markets, the P90 rule and LER classification, were introduced to make the market more attractive for flexible stochastic resources. The regulations respectively relax market requirements related to the security and volume of flexible capacity from such resources. However, this incentivizes aggregators to exploit the rules when bidding flexible capacity. Considering the Nordic ancillary service Frequency Containment Reserve - Disturbance (FCR-D), we consider an aggregator with a portfolio of Electric Vehicles (EVs) using real-life data and present an optimization model that, new to the literature, uses Joint Chance-Constraints (JCCs) for bidding its flexible capacity while adhering to the new market regulations. Using different bundle sizes within the portfolio and the approximation methods of the JCCs, ALSO-X and Conditional Value at Risk (CVaR), we show that a significant synergy effect emerges when aggregating a portfolio of EVs, especially when applying ALSO-X which exploits the rules more than CVaR. We show that EV owners can earn a significant profit when participating in the aggregator portfolio.","sentences":["Recently, two new innovative regulations in the Nordic ancillary service markets, the P90 rule and LER classification, were introduced to make the market more attractive for flexible stochastic resources.","The regulations respectively relax market requirements related to the security and volume of flexible capacity from such resources.","However, this incentivizes aggregators to exploit the rules when bidding flexible capacity.","Considering the Nordic ancillary service Frequency Containment Reserve - Disturbance (FCR-D), we consider an aggregator with a portfolio of Electric Vehicles (EVs) using real-life data and present an optimization model that, new to the literature, uses Joint Chance-Constraints (JCCs) for bidding its flexible capacity while adhering to the new market regulations.","Using different bundle sizes within the portfolio and the approximation methods of the JCCs, ALSO-X and Conditional Value at Risk (CVaR), we show that a significant synergy effect emerges when aggregating a portfolio of EVs, especially when applying ALSO-X which exploits the rules more than CVaR. We show that EV owners can earn a significant profit when participating in the aggregator portfolio."],"url":"http://arxiv.org/abs/2404.12818v1","category":"eess.SY"}
{"created":"2024-04-19 11:48:45","title":"Algorithmic Changes Are Not Enough: Evaluating the Removal of Race Adjustment from the eGFR Equation","abstract":"Changing clinical algorithms to remove race adjustment has been proposed and implemented for multiple health conditions. Removing race adjustment from estimated glomerular filtration rate (eGFR) equations may reduce disparities in chronic kidney disease (CKD), but has not been studied in clinical practice after implementation. Here, we assessed whether implementing an eGFR equation (CKD-EPI 2021) without adjustment for Black or African American race modified quarterly rates of nephrology referrals and visits within a single healthcare system, Stanford Health Care (SHC). Our cohort study analyzed 547,194 adult patients aged 21 and older who had at least one recorded serum creatinine or serum cystatin C between January 1, 2019 and September 1, 2023. During the study period, implementation of CKD-EPI 2021 did not modify rates of quarterly nephrology referrals in those documented as Black or African American or in the overall cohort. After adjusting for capacity at SHC nephrology clinics, estimated rates of nephrology referrals and visits with CKD-EPI 2021 were 34 (95% CI 29, 39) and 188 (175, 201) per 10,000 patients documented as Black or African American. If race adjustment had not been removed, estimated rates were nearly identical: 38 (95% CI: 28, 53) and 189 (165, 218) per 10,000 patients. Changes to the eGFR equation are likely insufficient to achieve health equity in CKD care decision-making as many other structural inequities remain.","sentences":["Changing clinical algorithms to remove race adjustment has been proposed and implemented for multiple health conditions.","Removing race adjustment from estimated glomerular filtration rate (eGFR) equations may reduce disparities in chronic kidney disease (CKD), but has not been studied in clinical practice after implementation.","Here, we assessed whether implementing an eGFR equation (CKD-EPI 2021) without adjustment for Black or African American race modified quarterly rates of nephrology referrals and visits within a single healthcare system, Stanford Health Care (SHC).","Our cohort study analyzed 547,194 adult patients aged 21 and older who had at least one recorded serum creatinine or serum cystatin C between January 1, 2019 and September 1, 2023.","During the study period, implementation of CKD-EPI 2021 did not modify rates of quarterly nephrology referrals in those documented as Black or African American or in the overall cohort.","After adjusting for capacity at SHC nephrology clinics, estimated rates of nephrology referrals and visits with CKD-EPI 2021 were 34 (95% CI 29, 39) and 188 (175, 201) per 10,000 patients documented as Black or African American.","If race adjustment had not been removed, estimated rates were nearly identical: 38 (95% CI: 28, 53) and 189 (165, 218) per 10,000 patients.","Changes to the eGFR equation are likely insufficient to achieve health equity in CKD care decision-making as many other structural inequities remain."],"url":"http://arxiv.org/abs/2404.12812v1","category":"cs.CY"}
{"created":"2024-04-19 11:41:50","title":"Leveraging P90 Requirement: Flexible Resources Bidding in Nordic Ancillary Service Markets","abstract":"The P90 requirement of the Danish transmission system operator, Energinet, incentivizes flexible resources with stochastic power consumption/production baseline to bid in Nordic ancillary service markets with the minimum reliability of 90%, i.e., letting them cause reserve shortfall with the probability of up to 10%. Leveraging this requirement, we develop a distributionally robust joint chance-constrained optimization model for aggregators of flexible resources to optimize their volume of reserve capacity to be offered. Having an aggregator of electric vehicles as a case study, we show how distributional robustness is key for the aggregator when making bidding decisions in a non-stationary uncertain environment. We also develop a heuristic based on a grid search for the system operator to adjust the P90 requirement and the level of conservativeness, aiming to procure the maximum reserve capacity from stochastic resources with least expected shortfall.","sentences":["The P90 requirement of the Danish transmission system operator, Energinet, incentivizes flexible resources with stochastic power consumption/production baseline to bid in Nordic ancillary service markets with the minimum reliability of 90%, i.e., letting them cause reserve shortfall with the probability of up to 10%.","Leveraging this requirement, we develop a distributionally robust joint chance-constrained optimization model for aggregators of flexible resources to optimize their volume of reserve capacity to be offered.","Having an aggregator of electric vehicles as a case study, we show how distributional robustness is key for the aggregator when making bidding decisions in a non-stationary uncertain environment.","We also develop a heuristic based on a grid search for the system operator to adjust the P90 requirement and the level of conservativeness, aiming to procure the maximum reserve capacity from stochastic resources with least expected shortfall."],"url":"http://arxiv.org/abs/2404.12807v1","category":"eess.SY"}
{"created":"2024-04-19 11:11:18","title":"Optimal transport maps as flows of control-affine systems","abstract":"We consider the controllability problem for the continuity equation where the dynamics is governed by a driftless control-affine system. Under suitable regularity conditions, the controllability of the system is a sufficient condition for the controllability of the continuity equation by means of time-varying feedback controls. Moreover, we show that there exist controls such that the flow of the control system is the optimal transport map, for the 2-Wasserstein distance, between two given probability measures.","sentences":["We consider the controllability problem for the continuity equation where the dynamics is governed by a driftless control-affine system.","Under suitable regularity conditions, the controllability of the system is a sufficient condition for the controllability of the continuity equation by means of time-varying feedback controls.","Moreover, we show that there exist controls such that the flow of the control system is the optimal transport map, for the 2-Wasserstein distance, between two given probability measures."],"url":"http://arxiv.org/abs/2404.12793v1","category":"math.OC"}
{"created":"2024-04-19 11:04:27","title":"REXEL: An End-to-end Model for Document-Level Relation Extraction and Entity Linking","abstract":"Extracting structured information from unstructured text is critical for many downstream NLP applications and is traditionally achieved by closed information extraction (cIE). However, existing approaches for cIE suffer from two limitations: (i) they are often pipelines which makes them prone to error propagation, and/or (ii) they are restricted to sentence level which prevents them from capturing long-range dependencies and results in expensive inference time. We address these limitations by proposing REXEL, a highly efficient and accurate model for the joint task of document level cIE (DocIE). REXEL performs mention detection, entity typing, entity disambiguation, coreference resolution and document-level relation classification in a single forward pass to yield facts fully linked to a reference knowledge graph. It is on average 11 times faster than competitive existing approaches in a similar setting and performs competitively both when optimised for any of the individual subtasks and a variety of combinations of different joint tasks, surpassing the baselines by an average of more than 6 F1 points. The combination of speed and accuracy makes REXEL an accurate cost-efficient system for extracting structured information at web-scale. We also release an extension of the DocRED dataset to enable benchmarking of future work on DocIE, which is available at https://github.com/amazon-science/e2e-docie.","sentences":["Extracting structured information from unstructured text is critical for many downstream NLP applications and is traditionally achieved by closed information extraction (cIE).","However, existing approaches for cIE suffer from two limitations: (i) they are often pipelines which makes them prone to error propagation, and/or (ii) they are restricted to sentence level which prevents them from capturing long-range dependencies and results in expensive inference time.","We address these limitations by proposing REXEL, a highly efficient and accurate model for the joint task of document level cIE (DocIE).","REXEL performs mention detection, entity typing, entity disambiguation, coreference resolution and document-level relation classification in a single forward pass to yield facts fully linked to a reference knowledge graph.","It is on average 11 times faster than competitive existing approaches in a similar setting and performs competitively both when optimised for any of the individual subtasks and a variety of combinations of different joint tasks, surpassing the baselines by an average of more than 6 F1 points.","The combination of speed and accuracy makes REXEL an accurate cost-efficient system for extracting structured information at web-scale.","We also release an extension of the DocRED dataset to enable benchmarking of future work on DocIE, which is available at https://github.com/amazon-science/e2e-docie."],"url":"http://arxiv.org/abs/2404.12788v1","category":"cs.CL"}
{"created":"2024-04-19 10:49:33","title":"AutoInspect: Towards Long-Term Autonomous Industrial Inspection","abstract":"We give an overview of AutoInspect, a ROS-based software system for robust and extensible mission-level autonomy. Over the past three years AutoInspect has been deployed in a variety of environments, including at a mine, a chemical plant, a mock oil rig, decommissioned nuclear power plants, and a fusion reactor for durations ranging from hours to weeks. The system combines robust mapping and localisation with graph-based autonomous navigation, mission execution, and scheduling to achieve a complete autonomous inspection system. The time from arrival at a new site to autonomous mission execution can be under an hour. It is deployed on a Boston Dynamics Spot robot using a custom sensing and compute payload called Frontier. In this work we go into detail of the system's performance in two long-term deployments of 49 days at a robotics test facility, and 35 days at the Joint European Torus (JET) fusion reactor in Oxfordshire, UK.","sentences":["We give an overview of AutoInspect, a ROS-based software system for robust and extensible mission-level autonomy.","Over the past three years AutoInspect has been deployed in a variety of environments, including at a mine, a chemical plant, a mock oil rig, decommissioned nuclear power plants, and a fusion reactor for durations ranging from hours to weeks.","The system combines robust mapping and localisation with graph-based autonomous navigation, mission execution, and scheduling to achieve a complete autonomous inspection system.","The time from arrival at a new site to autonomous mission execution can be under an hour.","It is deployed on a Boston Dynamics Spot robot using a custom sensing and compute payload called Frontier.","In this work we go into detail of the system's performance in two long-term deployments of 49 days at a robotics test facility, and 35 days at the Joint European Torus (JET) fusion reactor in Oxfordshire, UK."],"url":"http://arxiv.org/abs/2404.12785v1","category":"cs.RO"}
{"created":"2024-04-19 10:40:24","title":"Piecewise Semi-Analytical Formulation for the Analysis of Coupled-Oscillator Systems","abstract":"A new simulation technique to obtain the synchronized steady-state solutions existing in coupled oscillator systems is presented. The technique departs from a semi-analytical formulation presented in previous works. It extends the model of the admittance function describing each individual oscillator to a piecewise linear one. This provides a global formulation of the coupled system, considering the whole characteristic of each voltage-controlled oscillator (VCO) in the array. In comparison with the previous local formulation, the new formulation significantly improves the accuracy in the prediction of the system synchronization ranges. The technique has been tested by comparison with computationally demanding circuit-level Harmonic Balance simulations in an array of Van der Pol-type oscillators and then applied to a coupled system of FET based oscillators at 5 GHz, with very good agreement with measurements.","sentences":["A new simulation technique to obtain the synchronized steady-state solutions existing in coupled oscillator systems is presented.","The technique departs from a semi-analytical formulation presented in previous works.","It extends the model of the admittance function describing each individual oscillator to a piecewise linear one.","This provides a global formulation of the coupled system, considering the whole characteristic of each voltage-controlled oscillator (VCO) in the array.","In comparison with the previous local formulation, the new formulation significantly improves the accuracy in the prediction of the system synchronization ranges.","The technique has been tested by comparison with computationally demanding circuit-level Harmonic Balance simulations in an array of Van der Pol-type oscillators and then applied to a coupled system of FET based oscillators at 5 GHz, with very good agreement with measurements."],"url":"http://arxiv.org/abs/2404.12780v1","category":"eess.SY"}
{"created":"2024-04-19 10:32:11","title":"SIR models with vital dynamics, reinfection and randomness to investigate the spread of infectious diseases","abstract":"We investigate SIR models with vital dynamics, reinfection, and randomness at the transmission coefficient and recruitment rate. Initially, we conduct an extensive analysis of the autonomous scenario, covering aspects such as local and global well-posedness, the existence and internal structure of attractors, and the presence of gradient dynamics. Subsequently, we explore the implications of small nonautonomous random perturbations, establishing the continuity of attractors and ensuring their topological structural stability. Additionally, we study scenarios in which both the transmission coefficient and the recruitment rate exhibit time-dependent or random behavior. For each scenario, we establish the existence of attractors and delineate conditions that determine whether the disease is eradicated or reaches an endemic state. Finally, we depict numerical simulations to illustrate the theoretical results.","sentences":["We investigate SIR models with vital dynamics, reinfection, and randomness at the transmission coefficient and recruitment rate.","Initially, we conduct an extensive analysis of the autonomous scenario, covering aspects such as local and global well-posedness, the existence and internal structure of attractors, and the presence of gradient dynamics.","Subsequently, we explore the implications of small nonautonomous random perturbations, establishing the continuity of attractors and ensuring their topological structural stability.","Additionally, we study scenarios in which both the transmission coefficient and the recruitment rate exhibit time-dependent or random behavior.","For each scenario, we establish the existence of attractors and delineate conditions that determine whether the disease is eradicated or reaches an endemic state.","Finally, we depict numerical simulations to illustrate the theoretical results."],"url":"http://arxiv.org/abs/2404.12776v1","category":"math.DS"}
{"created":"2024-04-19 10:21:33","title":"Camera Agnostic Two-Head Network for Ego-Lane Inference","abstract":"Vision-based ego-lane inference using High-Definition (HD) maps is essential in autonomous driving and advanced driver assistance systems. The traditional approach necessitates well-calibrated cameras, which confines variation of camera configuration, as the algorithm relies on intrinsic and extrinsic calibration. In this paper, we propose a learning-based ego-lane inference by directly estimating the ego-lane index from a single image. To enhance robust performance, our model incorporates the two-head structure inferring ego-lane in two perspectives simultaneously. Furthermore, we utilize an attention mechanism guided by vanishing point-and-line to adapt to changes in viewpoint without requiring accurate calibration. The high adaptability of our model was validated in diverse environments, devices, and camera mounting points and orientations.","sentences":["Vision-based ego-lane inference using High-Definition (HD) maps is essential in autonomous driving and advanced driver assistance systems.","The traditional approach necessitates well-calibrated cameras, which confines variation of camera configuration, as the algorithm relies on intrinsic and extrinsic calibration.","In this paper, we propose a learning-based ego-lane inference by directly estimating the ego-lane index from a single image.","To enhance robust performance, our model incorporates the two-head structure inferring ego-lane in two perspectives simultaneously.","Furthermore, we utilize an attention mechanism guided by vanishing point-and-line to adapt to changes in viewpoint without requiring accurate calibration.","The high adaptability of our model was validated in diverse environments, devices, and camera mounting points and orientations."],"url":"http://arxiv.org/abs/2404.12770v1","category":"cs.CV"}
{"created":"2024-04-19 10:20:10","title":"Towards Accurate and Efficient Sorting of Retired Lithium-ion Batteries: A Data Driven Based Electrode Aging Assessment Approach","abstract":"Retired batteries (RBs) for second-life applications offer promising economic and environmental benefits. However, accurate and efficient sorting of RBs with discrepant characteristics persists as a pressing challenge. In this study, we introduce a data driven based electrode aging assessment approach to address this concern. To this end, a number of 15 feature points are extracted from battery open circuit voltage (OCV) curves to capture their characteristics at different levels of aging, and a convolutional neural network with an optimized structure and minimized input size is established to relocate the relative positions of these OCV feature points. Next, a rapid estimation algorithm is proposed to identify the three electrode aging parameters (EAPs) which best reconstruct the 15 OCV feature points over the entire usable capacity range. Utilizing the three EAPs as sorting indices, we employ an adaptive affinity propagation algorithm to cluster RBs without the need for pre-determining the clustering number. Unlike conventional sorting methods based solely on battery capacity, the proposed method provides profound insights into electrode aging behaviors, minimizes the need for constant-current charging data, and supports module/pack-level tests for the simultaneous processing of high volumes of RBs.","sentences":["Retired batteries (RBs) for second-life applications offer promising economic and environmental benefits.","However, accurate and efficient sorting of RBs with discrepant characteristics persists as a pressing challenge.","In this study, we introduce a data driven based electrode aging assessment approach to address this concern.","To this end, a number of 15 feature points are extracted from battery open circuit voltage (OCV) curves to capture their characteristics at different levels of aging, and a convolutional neural network with an optimized structure and minimized input size is established to relocate the relative positions of these OCV feature points.","Next, a rapid estimation algorithm is proposed to identify the three electrode aging parameters (EAPs) which best reconstruct the 15 OCV feature points over the entire usable capacity range.","Utilizing the three EAPs as sorting indices, we employ an adaptive affinity propagation algorithm to cluster RBs without the need for pre-determining the clustering number.","Unlike conventional sorting methods based solely on battery capacity, the proposed method provides profound insights into electrode aging behaviors, minimizes the need for constant-current charging data, and supports module/pack-level tests for the simultaneous processing of high volumes of RBs."],"url":"http://arxiv.org/abs/2404.12769v1","category":"eess.SY"}
{"created":"2024-04-19 10:02:13","title":"Nonclassicality in Two-Mode Stabilized Squeezed Coherent State: Quantum-to-Classical transition","abstract":"We consider a two-mode stabilized squeezed coherent state (SSCS) of light and introduce the $\\Pi_{\\rm N}$ indicator, a novel measure for characterizing nonclassicality in the resulting EPR-entangled state. Unlike existing methods based on Cauchy-Schwarz or Murihead inequalities, $\\Pi_{\\rm N}$ leverages analytical solutions to the quantum Langevin equations to directly analyze nonclassicality arising from key processes like bichromatic injection, frequency conversion, and parametric down-conversion (both spontaneous and stimulated). This approach not only identifies the optimal phase for maximum nonclassicality but also reveals two new phenomena: first, both intra-cavity and extra-cavity fields exhibit the same degree of nonclassicality, and second, balanced seeding in phase-mismatched configurations induces nonclassicality across a broad range of squeezing and seeding parameters. Our work deepens the understanding of the intricate dependence of nonclassicality on system parameters in the context of SSCS, paving the way for investigations into the quantum-to-classical transition in entangled systems. The potential of $\\Pi_{\\rm N}$ holds significant promise for advancements in quantum optics and information science.","sentences":["We consider a two-mode stabilized squeezed coherent state (SSCS) of light and introduce the $\\Pi_{\\rm N}$ indicator, a novel measure for characterizing nonclassicality in the resulting EPR-entangled state.","Unlike existing methods based on Cauchy-Schwarz or Murihead inequalities, $\\Pi_{\\rm N}$ leverages analytical solutions to the quantum Langevin equations to directly analyze nonclassicality arising from key processes like bichromatic injection, frequency conversion, and parametric down-conversion (both spontaneous and stimulated).","This approach not only identifies the optimal phase for maximum nonclassicality but also reveals two new phenomena: first, both intra-cavity and extra-cavity fields exhibit the same degree of nonclassicality, and second, balanced seeding in phase-mismatched configurations induces nonclassicality across a broad range of squeezing and seeding parameters.","Our work deepens the understanding of the intricate dependence of nonclassicality on system parameters in the context of SSCS, paving the way for investigations into the quantum-to-classical transition in entangled systems.","The potential of $\\Pi_{\\rm N}$ holds significant promise for advancements in quantum optics and information science."],"url":"http://arxiv.org/abs/2404.12758v1","category":"quant-ph"}
{"created":"2024-04-19 10:01:06","title":"Why not a thin plate spline for spatial models? A comparative study using Bayesian inference","abstract":"Spatial modelling often uses Gaussian random fields to capture the stochastic nature of studied phenomena. However, this approach incurs significant computational burdens (O(n3)), primarily due to covariance matrix computations. In this study, we propose to use a low-rank approximation of a thin plate spline as a spatial random effect in Bayesian spatial models. We compare its statistical performance and computational efficiency with the approximated Gaussian random field (by the SPDE method). In this case, the dense matrix of the thin plate spline is approximated using a truncated spectral decomposition, resulting in computational complexity of O(kn2) operations, where k is the number of knots. Bayesian inference is conducted via the Hamiltonian Monte Carlo algorithm of the probabilistic software Stan, which allows us to evaluate performance and diagnostics for the proposed models. A simulation study reveals that both models accurately recover the parameters used to simulate data. However, models using a thin plate spline demonstrate superior execution time to achieve the convergence of chains compared to the models utilizing an approximated Gaussian random field. Furthermore, thin plate spline models exhibited better computational efficiency for simulated data coming from different spatial locations. In a real application, models using a thin plate spline as spatial random effect produced similar results in estimating a relative index of abundance for a benthic marine species when compared to models incorporating an approximated Gaussian random field. Although they were not the more computational efficient models, their simplicity in parametrization, execution time and predictive performance make them a valid alternative for spatial modelling under Bayesian inference.","sentences":["Spatial modelling often uses Gaussian random fields to capture the stochastic nature of studied phenomena.","However, this approach incurs significant computational burdens (O(n3)), primarily due to covariance matrix computations.","In this study, we propose to use a low-rank approximation of a thin plate spline as a spatial random effect in Bayesian spatial models.","We compare its statistical performance and computational efficiency with the approximated Gaussian random field (by the SPDE method).","In this case, the dense matrix of the thin plate spline is approximated using a truncated spectral decomposition, resulting in computational complexity of O(kn2) operations, where k is the number of knots.","Bayesian inference is conducted via the Hamiltonian Monte Carlo algorithm of the probabilistic software Stan, which allows us to evaluate performance and diagnostics for the proposed models.","A simulation study reveals that both models accurately recover the parameters used to simulate data.","However, models using a thin plate spline demonstrate superior execution time to achieve the convergence of chains compared to the models utilizing an approximated Gaussian random field.","Furthermore, thin plate spline models exhibited better computational efficiency for simulated data coming from different spatial locations.","In a real application, models using a thin plate spline as spatial random effect produced similar results in estimating a relative index of abundance for a benthic marine species when compared to models incorporating an approximated Gaussian random field.","Although they were not the more computational efficient models, their simplicity in parametrization, execution time and predictive performance make them a valid alternative for spatial modelling under Bayesian inference."],"url":"http://arxiv.org/abs/2404.12756v1","category":"stat.ME"}
{"created":"2024-04-19 10:00:50","title":"Expanding Ricci solitons coming out of weakly PIC1 metric cones","abstract":"Motivated by recent work of Deruelle-Schulze-Simon, we study complete weakly PIC1 Ricci flows with Euclidean volume growth coming out of metric cones. We show that such a Ricci flow must be an expanding gradient Ricci soliton, and as a consequence, any metric cone at infinity of a complete weakly PIC1 K\\\"ahler manifold with Euclidean volume growth is biholomorphic to complex Euclidean space in a canonical way.","sentences":["Motivated by recent work of Deruelle-Schulze-Simon, we study complete weakly PIC1 Ricci flows with Euclidean volume growth coming out of metric cones.","We show that such a Ricci flow must be an expanding gradient Ricci soliton, and as a consequence, any metric cone at infinity of a complete weakly PIC1 K\\\"ahler manifold with Euclidean volume growth is biholomorphic to complex Euclidean space in a canonical way."],"url":"http://arxiv.org/abs/2404.12755v1","category":"math.DG"}
{"created":"2024-04-19 09:58:54","title":"User-Centric Cell-Free (UCCF) Wireless Systems: Principles and Optimization","abstract":"User-centric cell-free (UCCF) wireless networks have a range of distinguished characteristics, which can be exploited for meeting some challenges that the conventional cellular systems are hard to. This chapter is devoted to delivering the fundamentals of wireless communications in UCCF systems, including channel modeling and estimation, uplink (UL) detection, downlink (DL) transmission, and resource optimization. Specifically, the advantages of cell-free networking are examined in contrast to the conventional celluar systems. The global and location-aware distributed UL detection are explored in the principles of minimum mean-square error (MMSE) and brief propagation. Correspondingly, the global and distributed DL transmission schemes are designed based on the MMSE precoding. The optimization of both UL and DL is analyzed with respect to system design and resource-allocation. Furthermore, some challenges for the implementation of UCCF systems in practice are identified and analyzed.","sentences":["User-centric cell-free (UCCF) wireless networks have a range of distinguished characteristics, which can be exploited for meeting some challenges that the conventional cellular systems are hard to.","This chapter is devoted to delivering the fundamentals of wireless communications in UCCF systems, including channel modeling and estimation, uplink (UL) detection, downlink (DL) transmission, and resource optimization.","Specifically, the advantages of cell-free networking are examined in contrast to the conventional celluar systems.","The global and location-aware distributed UL detection are explored in the principles of minimum mean-square error (MMSE) and brief propagation.","Correspondingly, the global and distributed DL transmission schemes are designed based on the MMSE precoding.","The optimization of both UL and DL is analyzed with respect to system design and resource-allocation.","Furthermore, some challenges for the implementation of UCCF systems in practice are identified and analyzed."],"url":"http://arxiv.org/abs/2404.12752v1","category":"eess.SP"}
{"created":"2024-04-19 09:50:21","title":"Moduli spaces of 3-manifolds with boundary are finite","abstract":"We study the classifying space B Diff(M) of the diffeomorphism group of a connected, compact, orientable 3-manifold M. In the case that M is reducible we build a contractible space parametrising the systems of reducing spheres. We use this to prove that if M has non-empty boundary, then B Diff(M rel boundary) has the homotopy type of a finite CW complex. This was conjectured by Kontsevich and appears on the Kirby problem list as Problem 3.48. As a consequence, we are able to show that for every compact, orientable 3-manifold M, B Diff(M) has finite type.","sentences":["We study the classifying space B Diff(M) of the diffeomorphism group of a connected, compact, orientable 3-manifold M. In the case that M is reducible we build a contractible space parametrising the systems of reducing spheres.","We use this to prove that if M has non-empty boundary, then B Diff(M rel boundary) has the homotopy type of a finite CW complex.","This was conjectured by Kontsevich and appears on the Kirby problem list as Problem 3.48.","As a consequence, we are able to show that for every compact, orientable 3-manifold M, B Diff(M) has finite type."],"url":"http://arxiv.org/abs/2404.12748v1","category":"math.GT"}
{"created":"2024-04-19 09:50:02","title":"Customizing Static Analysis using Codesearch","abstract":"Static analysis is a growing application of software engineering, leading to a range of essential security tools, bug-finding tools, as well as software verification. Recent years show an increase of universal static analysis tools that validate a range of properties and allow customizing parts of the scanner to validate additional properties or \"static analysis rules\". A commonly used language to describe a range of static analysis applications is Datalog. Unfortunately, the language is still non-trivial to use, leading to analysis that is difficult to implement in a precise but performant way. In this work, we aim to make building custom static analysis tools much easier for developers, while at the same time, providing a familiar framework for application security and static analysis experts. Our approach introduces a language called StarLang, a variant of Datalog which only includes programs with a fast runtime by the means of having low time complexity of its decision procedure.","sentences":["Static analysis is a growing application of software engineering, leading to a range of essential security tools, bug-finding tools, as well as software verification.","Recent years show an increase of universal static analysis tools that validate a range of properties and allow customizing parts of the scanner to validate additional properties or \"static analysis rules\".","A commonly used language to describe a range of static analysis applications is Datalog.","Unfortunately, the language is still non-trivial to use, leading to analysis that is difficult to implement in a precise but performant way.","In this work, we aim to make building custom static analysis tools much easier for developers, while at the same time, providing a familiar framework for application security and static analysis experts.","Our approach introduces a language called StarLang, a variant of Datalog which only includes programs with a fast runtime by the means of having low time complexity of its decision procedure."],"url":"http://arxiv.org/abs/2404.12747v1","category":"cs.PL"}
{"created":"2024-04-19 09:42:38","title":"Laplace--Beltrami Equations and Numerical Conformal Mappings on Surfaces","abstract":"The conjugate function method is an algorithm for numerical computation of conformal mappings for simply and multiply connected domains. In this paper, the conjugate function method is extended to cover conformal mappings between Riemannian surfaces. The main challenge addressed here is the connection between Laplace--Beltrami equations on surfaces and the computation of the conformal modulus of a quadrilateral. We consider mappings of simply, doubly, and multiply connected domains. The numerical computation is based on an $hp$-adaptive finite element method. The key advantage of our approach is that it allows highly accurate computations of mappings on surfaces, including domains of complex boundary geometry involving strong singularities and cusps. The efficacy of the proposed method is illustrated via an extensive set of numerical experiments including error estimates.","sentences":["The conjugate function method is an algorithm for numerical computation of conformal mappings for simply and multiply connected domains.","In this paper, the conjugate function method is extended to cover conformal mappings between Riemannian surfaces.","The main challenge addressed here is the connection between Laplace--Beltrami equations on surfaces and the computation of the conformal modulus of a quadrilateral.","We consider mappings of simply, doubly, and multiply connected domains.","The numerical computation is based on an $hp$-adaptive finite element method.","The key advantage of our approach is that it allows highly accurate computations of mappings on surfaces, including domains of complex boundary geometry involving strong singularities and cusps.","The efficacy of the proposed method is illustrated via an extensive set of numerical experiments including error estimates."],"url":"http://arxiv.org/abs/2404.12743v1","category":"math.NA"}
{"created":"2024-04-19 09:39:07","title":"Kitaev-Heisenberg cobaltates: Coulomb exchange as leading nearest-neighbor interaction mechanism","abstract":"A range of honeycomb Co oxide compounds has been proposed and investigated in the search for a topological Kitaev spin liquid. Analyzing the quantum chemistry of interacting magnetic moments in Na$_3$Co$_2$SbO$_6$, a representative $LS$-coupled $t_{2g}^5e_g^2$ magnet, we find that the Kitaev and off-diagonal $\\Gamma$ interactions are sizable and antiferromagnetic but still weaker than the Heisenberg contribution. Except $\\Gamma$', all nearest-neighbor couplings are mainly determined by Coulomb exchange, different from current representations of anisotropic interaction terms. This highlights the limitations of existing anisotropic models and the need for systematic wave-function quantum chemical studies to clarify exchange mechanisms in Kitaev-Heisenberg systems.","sentences":["A range of honeycomb Co oxide compounds has been proposed and investigated in the search for a topological Kitaev spin liquid.","Analyzing the quantum chemistry of interacting magnetic moments in Na$_3$Co$_2$SbO$_6$, a representative $LS$-coupled $t_{2g}^5e_g^2$ magnet, we find that the Kitaev and off-diagonal $\\Gamma$ interactions are sizable and antiferromagnetic but still weaker than the Heisenberg contribution.","Except $\\Gamma$', all nearest-neighbor couplings are mainly determined by Coulomb exchange, different from current representations of anisotropic interaction terms.","This highlights the limitations of existing anisotropic models and the need for systematic wave-function quantum chemical studies to clarify exchange mechanisms in Kitaev-Heisenberg systems."],"url":"http://arxiv.org/abs/2404.12742v1","category":"cond-mat.str-el"}
{"created":"2024-04-19 09:04:12","title":"TOI-4336 A b: A temperate sub-Neptune ripe for atmospheric characterization in a nearby triple M-dwarf system","abstract":"Small planets transiting bright nearby stars are essential to our understanding of the formation and evolution of exoplanetary systems. However, few constitute prime targets for atmospheric characterization, and even fewer are part of multiple star systems. This work aims to validate TOI-4336 A b, a sub-Neptune-sized exoplanet candidate identified by the TESS space-based transit survey around a nearby M-dwarf. We validate the planetary nature of TOI-4336 A b through the global analysis of TESS and follow-up multi-band high-precision photometric data from ground-based telescopes, medium- and high-resolution spectroscopy of the host star, high-resolution speckle imaging, and archival images. The newly discovered exoplanet TOI-4336 A b has a radius of 2.1$\\pm$0.1R$_{\\oplus}$. Its host star is an M3.5-dwarf star of mass 0.33$\\pm$0.01M$_{\\odot}$ and radius 0.33$\\pm$0.02R$_{\\odot}$ member of a hierarchical triple M-dwarf system 22 pc away from the Sun. The planet's orbital period of 16.3 days places it at the inner edge of the Habitable Zone of its host star, the brightest of the inner binary pair. The parameters of the system make TOI-4336 A b an extremely promising target for the detailed atmospheric characterization of a temperate sub-Neptune by transit transmission spectroscopy with JWST.","sentences":["Small planets transiting bright nearby stars are essential to our understanding of the formation and evolution of exoplanetary systems.","However, few constitute prime targets for atmospheric characterization, and even fewer are part of multiple star systems.","This work aims to validate TOI-4336 A b, a sub-Neptune-sized exoplanet candidate identified by the TESS space-based transit survey around a nearby M-dwarf.","We validate the planetary nature of TOI-4336 A b through the global analysis of TESS and follow-up multi-band high-precision photometric data from ground-based telescopes, medium- and high-resolution spectroscopy of the host star, high-resolution speckle imaging, and archival images.","The newly discovered exoplanet TOI-4336 A b has a radius of 2.1$\\pm$0.1R$_{\\oplus}$. Its host star is an M3.5-dwarf star of mass 0.33$\\pm$0.01M$_{\\odot}$ and radius 0.33$\\pm$0.02R$_{\\odot}$ member of a hierarchical triple M-dwarf system 22 pc away from the Sun.","The planet's orbital period of 16.3 days places it at the inner edge of the Habitable Zone of its host star, the brightest of the inner binary pair.","The parameters of the system make TOI-4336 A b an extremely promising target for the detailed atmospheric characterization of a temperate sub-Neptune by transit transmission spectroscopy with JWST."],"url":"http://arxiv.org/abs/2404.12722v1","category":"astro-ph.EP"}
{"created":"2024-04-19 08:47:11","title":"Energy Conserved Failure Detection for NS-IoT Systems","abstract":"Nowadays, network slicing (NS) technology has gained widespread adoption within Internet of Things (IoT) systems to meet diverse customized requirements. In the NS based IoT systems, the detection of equipment failures necessitates comprehensive equipment monitoring, which leads to significant resource utilization, particularly within large-scale IoT ecosystems. Thus, the imperative task of reducing failure rates while optimizing monitoring costs has emerged. In this paper, we propose a monitor application function (MAF) based dynamic dormancy monitoring mechanism for the novel NS-IoT system, which is based on a network data analysis function (NWDAF) framework defined in Rel-17. Within the NS-IoT system, all nodes are organized into groups, and multiple MAFs are deployed to monitor each group of nodes. We also propose a dormancy monitor mechanism to mitigate the monitoring energy consumption by placing the MAFs, which is monitoring non-failure devices, in a dormant state. We propose a reinforcement learning based PPO algorithm to guide the dynamic dormancy of MAFs. Simulation results demonstrate that our dynamic dormancy strategy maximizes energy conservation, while proposed algorithm outperforms alternatives in terms of efficiency and stability.","sentences":["Nowadays, network slicing (NS) technology has gained widespread adoption within Internet of Things (IoT) systems to meet diverse customized requirements.","In the NS based IoT systems, the detection of equipment failures necessitates comprehensive equipment monitoring, which leads to significant resource utilization, particularly within large-scale IoT ecosystems.","Thus, the imperative task of reducing failure rates while optimizing monitoring costs has emerged.","In this paper, we propose a monitor application function (MAF) based dynamic dormancy monitoring mechanism for the novel NS-IoT system, which is based on a network data analysis function (NWDAF) framework defined in Rel-17.","Within the NS-IoT system, all nodes are organized into groups, and multiple MAFs are deployed to monitor each group of nodes.","We also propose a dormancy monitor mechanism to mitigate the monitoring energy consumption by placing the MAFs, which is monitoring non-failure devices, in a dormant state.","We propose a reinforcement learning based PPO algorithm to guide the dynamic dormancy of MAFs.","Simulation results demonstrate that our dynamic dormancy strategy maximizes energy conservation, while proposed algorithm outperforms alternatives in terms of efficiency and stability."],"url":"http://arxiv.org/abs/2404.12713v1","category":"cs.NI"}
{"created":"2024-04-19 08:20:18","title":"Modeling Multi-Granularity Context Information Flow for Pavement Crack Detection","abstract":"Crack detection has become an indispensable, interesting yet challenging task in the computer vision community. Specially, pavement cracks have a highly complex spatial structure, a low contrasting background and a weak spatial continuity, posing a significant challenge to an effective crack detection method. In this paper, we address these problems from a view that utilizes contexts of the cracks and propose an end-to-end deep learning method to model the context information flow. To precisely localize crack from an image, it is critical to effectively extract and aggregate multi-granularity context, including the fine-grained local context around the cracks (in spatial-level) and the coarse-grained semantics (in segment-level). Concretely, in Convolutional Neural Network (CNN), low-level features extracted by the shallow layers represent the local information, while the deep layers extract the semantic features. Additionally, a second main insight in this work is that the semantic context should be an guidance to local context feature. By the above insights, the proposed method we first apply the dilated convolution as the backbone feature extractor to model local context, then we build a context guidance module to leverage semantic context to guide local feature extraction at multiple stages. To handle label alignment between stages, we apply the Multiple Instance Learning (MIL) strategy to align the high-level feature to the low-level ones in the stage-wise context flow. In addition, compared with these public crack datasets, to our best knowledge, we release the largest, most complex and most challenging Bitumen Pavement Crack (BPC) dataset. The experimental results on the three crack datasets demonstrate that the proposed method performs well and outperforms the current state-of-the-art methods.","sentences":["Crack detection has become an indispensable, interesting yet challenging task in the computer vision community.","Specially, pavement cracks have a highly complex spatial structure, a low contrasting background and a weak spatial continuity, posing a significant challenge to an effective crack detection method.","In this paper, we address these problems from a view that utilizes contexts of the cracks and propose an end-to-end deep learning method to model the context information flow.","To precisely localize crack from an image, it is critical to effectively extract and aggregate multi-granularity context, including the fine-grained local context around the cracks (in spatial-level) and the coarse-grained semantics (in segment-level).","Concretely, in Convolutional Neural Network (CNN), low-level features extracted by the shallow layers represent the local information, while the deep layers extract the semantic features.","Additionally, a second main insight in this work is that the semantic context should be an guidance to local context feature.","By the above insights, the proposed method we first apply the dilated convolution as the backbone feature extractor to model local context, then we build a context guidance module to leverage semantic context to guide local feature extraction at multiple stages.","To handle label alignment between stages, we apply the Multiple Instance Learning (MIL) strategy to align the high-level feature to the low-level ones in the stage-wise context flow.","In addition, compared with these public crack datasets, to our best knowledge, we release the largest, most complex and most challenging Bitumen Pavement Crack (BPC) dataset.","The experimental results on the three crack datasets demonstrate that the proposed method performs well and outperforms the current state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.12702v1","category":"cs.CV"}
{"created":"2024-04-19 17:58:09","title":"NIRSpec View of the Appearance and Evolution of Balmer Breaks and the Transition from Bursty to Smooth Star Formation Histories from Deep Within the Epoch of Reionization to Cosmic Noon","abstract":"Theoretical models and observational evidence suggest that high-redshift galaxies grow under the bursty mode of star formation, with large temporal star formation rate (SFR) fluctuations around some mean value. From an observational perspective, it has not been clear at which redshift and stellar population characteristics the transition from bursty to smooth star formation occurs. Here, we investigate these using a uniformly reduced sample of NIRSpec prism spectra of 631 galaxies at $3 < z_{\\rm spec} < 14$, stacked in 8 redshift and 8 UV slope bins. We evaluate the burstiness of star formation histories using the Balmer break strengths as well as the ratios of SFRs as measured from the emission lines to those measured from the UV continua. The break strength increases monotonically from $z = 10$ to $z = 3$, and from $\\beta_{\\rm UV} = -3.0$ to $\\beta_{\\rm UV} = 0.0$. The break strength is tightly anti-correlated with specific SFR (sSFR), and in dusty galaxies, strongly correlated with dust attenuation. Based on the SFR ratios, we find that bursty star formation thrives in the highest redshift, bluest, and lowest stellar mass galaxies, which exhibit the highest sSFRs. The burstiness appears to plateau at $z > 6$, suggesting that we might be observing the peak of star formation burstiness at these redshifts. The $z < 4$ galaxies do not appear particularly bursty, suggesting that the smooth mode of star formation starts taking over right before cosmic noon. As galaxies mature and develop redder UV colors and more pronounced Balmer breaks, their ability to sustain star formation over longer timescales increases, signalling their transition from bursty to smooth star formation.","sentences":["Theoretical models and observational evidence suggest that high-redshift galaxies grow under the bursty mode of star formation, with large temporal star formation rate (SFR) fluctuations around some mean value.","From an observational perspective, it has not been clear at which redshift and stellar population characteristics the transition from bursty to smooth star formation occurs.","Here, we investigate these using a uniformly reduced sample of NIRSpec prism spectra of 631 galaxies at $3 < z_{\\rm spec} < 14$, stacked in 8 redshift and 8 UV slope bins.","We evaluate the burstiness of star formation histories using the Balmer break strengths as well as the ratios of SFRs as measured from the emission lines to those measured from the UV continua.","The break strength increases monotonically from $z = 10$ to $z = 3$, and from $\\beta_{\\rm UV} = -3.0$ to $\\beta_{\\rm UV} = 0.0$.","The break strength is tightly anti-correlated with specific SFR (sSFR), and in dusty galaxies, strongly correlated with dust attenuation.","Based on the SFR ratios, we find that bursty star formation thrives in the highest redshift, bluest, and lowest stellar mass galaxies, which exhibit the highest sSFRs.","The burstiness appears to plateau at $z > 6$, suggesting that we might be observing the peak of star formation burstiness at these redshifts.","The $z < 4$ galaxies do not appear particularly bursty, suggesting that the smooth mode of star formation starts taking over right before cosmic noon.","As galaxies mature and develop redder UV colors and more pronounced Balmer breaks, their ability to sustain star formation over longer timescales increases, signalling their transition from bursty to smooth star formation."],"url":"http://arxiv.org/abs/2404.13045v1","category":"astro-ph.GA"}
{"created":"2024-04-19 17:07:15","title":"Spin-Wave Self-Imaging: Experimental and Numerical Demonstration of Caustic and Talbot-like Diffraction Patterns","abstract":"Extending the scope of the self-imaging phenomenon, traditionally associated with linear optics, to the domain of magnonics, this study presents the experimental demonstration and numerical analysis of spin-wave (SW) self-imaging in an in-plane magnetized yttrium iron garnet film. We explore this phenomenon using a setup in which a plane SW passes through a diffraction grating, and the resulting interference pattern is detected using Brillouin light scattering. We have varied the frequencies of the source dynamic magnetic field to discern the influence of the anisotropic dispersion relation and the caustic effect on the analyzed phenomenon. We found that at low frequencies and diffraction fields, the caustics determine the interference pattern. However, at large distances from the grating, when the waves of high diffraction order and number of slits contribute to the interference pattern, the self-imaging phenomenon and Talbot-like patterns are formed. This methodological approach not only sheds light on the behavior of SW interference under different conditions but also enhances our understanding of the SW self-imaging process in both isotropic and anisotropic media.","sentences":["Extending the scope of the self-imaging phenomenon, traditionally associated with linear optics, to the domain of magnonics, this study presents the experimental demonstration and numerical analysis of spin-wave (SW) self-imaging in an in-plane magnetized yttrium iron garnet film.","We explore this phenomenon using a setup in which a plane SW passes through a diffraction grating, and the resulting interference pattern is detected using Brillouin light scattering.","We have varied the frequencies of the source dynamic magnetic field to discern the influence of the anisotropic dispersion relation and the caustic effect on the analyzed phenomenon.","We found that at low frequencies and diffraction fields, the caustics determine the interference pattern.","However, at large distances from the grating, when the waves of high diffraction order and number of slits contribute to the interference pattern, the self-imaging phenomenon and Talbot-like patterns are formed.","This methodological approach not only sheds light on the behavior of SW interference under different conditions but also enhances our understanding of the SW self-imaging process in both isotropic and anisotropic media."],"url":"http://arxiv.org/abs/2404.13006v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-19 16:52:19","title":"On Commuting automorphisms of nilpotent Lie algebras","abstract":"We investigate the commuting automorphisms of nilpotent Lie algebras $L$ with coclass $\\leq 3$. Our examination exposes the conditions under which the set of commuting automorphisms of $L$ forms a subgroup within its automorphism group.","sentences":["We investigate the commuting automorphisms of nilpotent Lie algebras $L$ with coclass $\\leq 3$. Our examination exposes the conditions under which the set of commuting automorphisms of $L$ forms a subgroup within its automorphism group."],"url":"http://arxiv.org/abs/2404.12998v1","category":"math.RA"}
{"created":"2024-04-19 16:50:22","title":"On the Asymmetric Volatility Connectedness","abstract":"Connectedness measures the degree at which a time-series variable spills over volatility to other variables compared to the rate that it is receiving. The idea is based on the percentage of variance decomposition from one variable to the others, which is estimated by making use of a VAR model. Diebold and Yilmaz (2012, 2014) suggested estimating this simple and useful measure of percentage risk spillover impact. Their method is symmetric by nature, however. The current paper offers an alternative asymmetric approach for measuring the volatility spillover direction, which is based on estimating the asymmetric variance decompositions introduced by Hatemi-J (2011, 2014). This approach accounts explicitly for the asymmetric property in the estimations, which accords better with reality. An application is provided to capture the potential asymmetric volatility spillover impacts between the three largest financial markets in the world.","sentences":["Connectedness measures the degree at which a time-series variable spills over volatility to other variables compared to the rate that it is receiving.","The idea is based on the percentage of variance decomposition from one variable to the others, which is estimated by making use of a VAR model.","Diebold and Yilmaz (2012, 2014) suggested estimating this simple and useful measure of percentage risk spillover impact.","Their method is symmetric by nature, however.","The current paper offers an alternative asymmetric approach for measuring the volatility spillover direction, which is based on estimating the asymmetric variance decompositions introduced by Hatemi-J (2011, 2014).","This approach accounts explicitly for the asymmetric property in the estimations, which accords better with reality.","An application is provided to capture the potential asymmetric volatility spillover impacts between the three largest financial markets in the world."],"url":"http://arxiv.org/abs/2404.12997v1","category":"econ.EM"}
{"created":"2024-04-19 16:42:44","title":"RedactBuster: Entity Type Recognition from Redacted Documents","abstract":"The widespread exchange of digital documents in various domains has resulted in abundant private information being shared. This proliferation necessitates redaction techniques to protect sensitive content and user privacy. While numerous redaction methods exist, their effectiveness varies, with some proving more robust than others. As such, the literature proposes several deanonymization techniques, raising awareness of potential privacy threats. However, while none of these methods are successful against the most effective redaction techniques, these attacks only focus on the anonymized tokens and ignore the sentence context.   In this paper, we propose RedactBuster, the first deanonymization model using sentence context to perform Named Entity Recognition on reacted text. Our methodology leverages fine-tuned state-of-the-art Transformers and Deep Learning models to determine the anonymized entity types in a document. We test RedactBuster against the most effective redaction technique and evaluate it using the publicly available Text Anonymization Benchmark (TAB). Our results show accuracy values up to 0.985 regardless of the document nature or entity type. In raising awareness of this privacy issue, we propose a countermeasure we call character evasion that helps strengthen the secrecy of sensitive information. Furthermore, we make our model and testbed open-source to aid researchers and practitioners in evaluating the resilience of novel redaction techniques and enhancing document privacy.","sentences":["The widespread exchange of digital documents in various domains has resulted in abundant private information being shared.","This proliferation necessitates redaction techniques to protect sensitive content and user privacy.","While numerous redaction methods exist, their effectiveness varies, with some proving more robust than others.","As such, the literature proposes several deanonymization techniques, raising awareness of potential privacy threats.","However, while none of these methods are successful against the most effective redaction techniques, these attacks only focus on the anonymized tokens and ignore the sentence context.   ","In this paper, we propose RedactBuster, the first deanonymization model using sentence context to perform Named Entity Recognition on reacted text.","Our methodology leverages fine-tuned state-of-the-art Transformers and Deep Learning models to determine the anonymized entity types in a document.","We test RedactBuster against the most effective redaction technique and evaluate it using the publicly available Text Anonymization Benchmark (TAB).","Our results show accuracy values up to 0.985 regardless of the document nature or entity type.","In raising awareness of this privacy issue, we propose a countermeasure we call character evasion that helps strengthen the secrecy of sensitive information.","Furthermore, we make our model and testbed open-source to aid researchers and practitioners in evaluating the resilience of novel redaction techniques and enhancing document privacy."],"url":"http://arxiv.org/abs/2404.12991v1","category":"cs.CR"}
{"created":"2024-04-19 16:34:38","title":"A practical global existence and uniqueness result for stochastic differential equations on Riemannian manifolds of bounded geometry","abstract":"In this paper, we establish a result for existence and uniqueness of stochastic differential equations on Riemannian manifolds, for regular inhomogeneous tensor coefficients with stochastic drift, under geometrical-only hypothesis on the manifold, so-called manifolds of bounded geometry, this hypothesis is consistent with the maximal regularity result for parabolic equations obtained by Herbert Amann. Furthermore, we provide a stochastic flow estimate for the solutions.","sentences":["In this paper, we establish a result for existence and uniqueness of stochastic differential equations on Riemannian manifolds, for regular inhomogeneous tensor coefficients with stochastic drift, under geometrical-only hypothesis on the manifold, so-called manifolds of bounded geometry, this hypothesis is consistent with the maximal regularity result for parabolic equations obtained by Herbert Amann.","Furthermore, we provide a stochastic flow estimate for the solutions."],"url":"http://arxiv.org/abs/2404.12985v1","category":"math.PR"}
{"created":"2024-04-19 14:37:37","title":"Influential Billboard Slot Selection under Zonal Influence Constraint","abstract":"Given billboard and trajectory database, finding a limited number of billboard slots for maximizing the influence is an important problem in the context of billboard advertisement. Most of the existing literature focused on the influential slot selection problem without considering any specific zonal influence constraint. To bridge this gap in this paper, we introduce and study the Influential Billboard Slot Selection Problem Under Zonal Influence Constraint. We propose a simple greedy approach to solve this problem. Though this method is easy to understand and simple to implement due to the excessive number of marginal gain computations, this method is not scalable. We design a branch and bound framework with two bound estimation techniques that divide the problem into different zones and integrate the zone-specific solutions to obtain a solution for the whole. We implement both the solution methodologies with real-world billboard and trajectory datasets and several experiments have been reported. We compare the performance of the proposed solution approaches with several baseline methods. The results show that the proposed approaches lead to more effective solutions with reasonable computational overhead than the baseline methods.","sentences":["Given billboard and trajectory database, finding a limited number of billboard slots for maximizing the influence is an important problem in the context of billboard advertisement.","Most of the existing literature focused on the influential slot selection problem without considering any specific zonal influence constraint.","To bridge this gap in this paper, we introduce and study the Influential Billboard Slot Selection Problem Under Zonal Influence Constraint.","We propose a simple greedy approach to solve this problem.","Though this method is easy to understand and simple to implement due to the excessive number of marginal gain computations, this method is not scalable.","We design a branch and bound framework with two bound estimation techniques that divide the problem into different zones and integrate the zone-specific solutions to obtain a solution for the whole.","We implement both the solution methodologies with real-world billboard and trajectory datasets and several experiments have been reported.","We compare the performance of the proposed solution approaches with several baseline methods.","The results show that the proposed approaches lead to more effective solutions with reasonable computational overhead than the baseline methods."],"url":"http://arxiv.org/abs/2404.12913v1","category":"cs.DB"}
{"created":"2024-04-19 14:18:54","title":"Urban topology and dynamics can assess green areas importance","abstract":"Green areas are a crucial element in evolution of a city, contributing to improve citizens' life, to reduce effects of climate change, and to make possible the survival of other species in urban areas. Unfortunately, the above effects are difficult to assess quantitatively for regulators, stakeholders and experts, making troublesome the planning of city development. Here we present a method to estimate the impact of these areas in the city life based on the network topology of the city itself and on a simple model of dynamics on this structure. Movements between various areas of the city are simulated by means of an agent-based biased-diffusion process where citizens try to reach the nearest Public Green Area (PGA) from their position and the model is fed with real data about the density of populations in the cases of study. Firstly, we define a centrality measure of PGA's based on average farness measured on the city network; this approach outperforms information based on the simple topology. We then improve this quantity by taking into account the occupation of PGA's, thereby providing a quantitative measure of PGA usage for regulators.","sentences":["Green areas are a crucial element in evolution of a city, contributing to improve citizens' life, to reduce effects of climate change, and to make possible the survival of other species in urban areas.","Unfortunately, the above effects are difficult to assess quantitatively for regulators, stakeholders and experts, making troublesome the planning of city development.","Here we present a method to estimate the impact of these areas in the city life based on the network topology of the city itself and on a simple model of dynamics on this structure.","Movements between various areas of the city are simulated by means of an agent-based biased-diffusion process where citizens try to reach the nearest Public Green Area (PGA) from their position and the model is fed with real data about the density of populations in the cases of study.","Firstly, we define a centrality measure of PGA's based on average farness measured on the city network; this approach outperforms information based on the simple topology.","We then improve this quantity by taking into account the occupation of PGA's, thereby providing a quantitative measure of PGA usage for regulators."],"url":"http://arxiv.org/abs/2404.12902v1","category":"physics.soc-ph"}
{"created":"2024-04-19 13:28:44","title":"Semantic Security with Unreliable Entanglement Assistance: Interception and Loss","abstract":"Semantic security is considered with unreliable entanglement assistance, due to one of two reasons: Interception or loss. We consider two corresponding models. In the first model, Eve may intercept the entanglement resource. In the second model, Eve is passive, and the resource may dissipate to the environment beyond her reach. We derive achievable rates for both models, subject to a maximal error criterion and semantic security. As an example, we consider the amplitude damping channel. Under interception, time division is not necessarily possible, and the boundary of our achievable region is disconnected. In the passive model, our rate region outperforms time division.","sentences":["Semantic security is considered with unreliable entanglement assistance, due to one of two reasons: Interception or loss.","We consider two corresponding models.","In the first model, Eve may intercept the entanglement resource.","In the second model, Eve is passive, and the resource may dissipate to the environment beyond her reach.","We derive achievable rates for both models, subject to a maximal error criterion and semantic security.","As an example, we consider the amplitude damping channel.","Under interception, time division is not necessarily possible, and the boundary of our achievable region is disconnected.","In the passive model, our rate region outperforms time division."],"url":"http://arxiv.org/abs/2404.12880v1","category":"quant-ph"}
{"created":"2024-04-19 12:29:51","title":"Banach Lie groupoid of partial isometries over restricted Grassmannian","abstract":"The differential structure on the set of partial isometries over the restricted Grassmannian is constructed, which makes it a Banach Lie groupoid.","sentences":["The differential structure on the set of partial isometries over the restricted Grassmannian is constructed, which makes it a Banach Lie groupoid."],"url":"http://arxiv.org/abs/2404.12847v1","category":"math.DG"}
{"created":"2024-04-19 12:05:50","title":"Low solution rank of the matrix LASSO under RIP with consequences for rank-constrained algorithms","abstract":"We show that solutions to the popular convex matrix LASSO problem (nuclear-norm--penalized linear least-squares) have low rank under similar assumptions as required by classical low-rank matrix sensing error bounds. Although the purpose of the nuclear norm penalty is to promote low solution rank, a proof has not yet (to our knowledge) been provided outside very specific circumstances. Furthermore, we show that this result has significant theoretical consequences for nonconvex rank-constrained optimization approaches. Specifically, we show that if (a) the ground truth matrix has low rank, (b) the (linear) measurement operator has the matrix restricted isometry property (RIP), and (c) the measurement error is small enough relative to the nuclear norm penalty, then the (unique) LASSO solution has rank (approximately) bounded by that of the ground truth. From this, we show (a) that a low-rank--projected proximal gradient descent algorithm will converge linearly to the LASSO solution from any initialization, and (b) that the nonconvex landscape of the low-rank Burer-Monteiro--factored problem formulation is benign in the sense that all second-order critical points are globally optimal and yield the LASSO solution.","sentences":["We show that solutions to the popular convex matrix LASSO problem (nuclear-norm--penalized linear least-squares) have low rank under similar assumptions as required by classical low-rank matrix sensing error bounds.","Although the purpose of the nuclear norm penalty is to promote low solution rank, a proof has not yet (to our knowledge) been provided outside very specific circumstances.","Furthermore, we show that this result has significant theoretical consequences for nonconvex rank-constrained optimization approaches.","Specifically, we show that if (a) the ground truth matrix has low rank, (b) the (linear) measurement operator has the matrix restricted isometry property (RIP), and (c) the measurement error is small enough relative to the nuclear norm penalty, then the (unique) LASSO solution has rank (approximately) bounded by that of the ground truth.","From this, we show (a) that a low-rank--projected proximal gradient descent algorithm will converge linearly to the LASSO solution from any initialization, and (b) that the nonconvex landscape of the low-rank Burer-Monteiro--factored problem formulation is benign in the sense that all second-order critical points are globally optimal and yield the LASSO solution."],"url":"http://arxiv.org/abs/2404.12828v1","category":"math.OC"}
{"created":"2024-04-19 11:24:34","title":"A Point-Based Approach to Efficient LiDAR Multi-Task Perception","abstract":"Multi-task networks can potentially improve performance and computational efficiency compared to single-task networks, facilitating online deployment. However, current multi-task architectures in point cloud perception combine multiple task-specific point cloud representations, each requiring a separate feature encoder and making the network structures bulky and slow. We propose PAttFormer, an efficient multi-task architecture for joint semantic segmentation and object detection in point clouds that only relies on a point-based representation. The network builds on transformer-based feature encoders using neighborhood attention and grid-pooling and a query-based detection decoder using a novel 3D deformable-attention detection head design. Unlike other LiDAR-based multi-task architectures, our proposed PAttFormer does not require separate feature encoders for multiple task-specific point cloud representations, resulting in a network that is 3x smaller and 1.4x faster while achieving competitive performance on the nuScenes and KITTI benchmarks for autonomous driving perception. Our extensive evaluations show substantial gains from multi-task learning, improving LiDAR semantic segmentation by +1.7% in mIou and 3D object detection by +1.7% in mAP on the nuScenes benchmark compared to the single-task models.","sentences":["Multi-task networks can potentially improve performance and computational efficiency compared to single-task networks, facilitating online deployment.","However, current multi-task architectures in point cloud perception combine multiple task-specific point cloud representations, each requiring a separate feature encoder and making the network structures bulky and slow.","We propose PAttFormer, an efficient multi-task architecture for joint semantic segmentation and object detection in point clouds that only relies on a point-based representation.","The network builds on transformer-based feature encoders using neighborhood attention and grid-pooling and a query-based detection decoder using a novel 3D deformable-attention detection head design.","Unlike other LiDAR-based multi-task architectures, our proposed PAttFormer does not require separate feature encoders for multiple task-specific point cloud representations, resulting in a network that is 3x smaller and 1.4x faster while achieving competitive performance on the nuScenes and KITTI benchmarks for autonomous driving perception.","Our extensive evaluations show substantial gains from multi-task learning, improving LiDAR semantic segmentation by +1.7% in mIou and 3D object detection by +1.7% in mAP on the nuScenes benchmark compared to the single-task models."],"url":"http://arxiv.org/abs/2404.12798v1","category":"cs.CV"}
{"created":"2024-04-19 11:05:52","title":"Quantum non-classicality in the simplest causal network","abstract":"Bell's theorem prompts us with a fundamental inquiry: what is the simplest scenario leading to the incompatibility between quantum correlations and the classical theory of causality? Here we demonstrate that quantum non-classicality is possible in a network consisting of only three dichotomic variables, without the need of the locality assumption neither external measurement choices. We also show that the use of interventions, a central tool in the field of causal inference, significantly improves the noise robustness of this new kind of non-classical behaviour, making it feasible for experimental tests with current technology.","sentences":["Bell's theorem prompts us with a fundamental inquiry: what is the simplest scenario leading to the incompatibility between quantum correlations and the classical theory of causality?","Here we demonstrate that quantum non-classicality is possible in a network consisting of only three dichotomic variables, without the need of the locality assumption neither external measurement choices.","We also show that the use of interventions, a central tool in the field of causal inference, significantly improves the noise robustness of this new kind of non-classical behaviour, making it feasible for experimental tests with current technology."],"url":"http://arxiv.org/abs/2404.12790v1","category":"quant-ph"}
{"created":"2024-04-19 11:04:54","title":"Efficient scaling and squaring method for the matrix exponential","abstract":"This work presents a new algorithm to compute the matrix exponential within a given tolerance. Combined with the scaling and squaring procedure, the algorithm incorporates Taylor, partitioned and classical Pad\\'e methods shown to be superior in performance to the approximants used in state-of-the-art software. The algorithm computes matrix--matrix products and also matrix inverses, but it can be implemented to avoid the computation of inverses, making it convenient for some problems. If the matrix A belongs to a Lie algebra, then exp(A) belongs to its associated Lie group, being a property which is preserved by diagonal Pad\\'e approximants, and the algorithm has another option to use only these. Numerical experiments show the superior performance with respect to state-of-the-art implementations.","sentences":["This work presents a new algorithm to compute the matrix exponential within a given tolerance.","Combined with the scaling and squaring procedure, the algorithm incorporates Taylor, partitioned and classical Pad\\'e methods shown to be superior in performance to the approximants used in state-of-the-art software.","The algorithm computes matrix--matrix products and also matrix inverses, but it can be implemented to avoid the computation of inverses, making it convenient for some problems.","If the matrix A belongs to a Lie algebra, then exp(A) belongs to its associated Lie group, being a property which is preserved by diagonal Pad\\'e approximants, and the algorithm has another option to use only these.","Numerical experiments show the superior performance with respect to state-of-the-art implementations."],"url":"http://arxiv.org/abs/2404.12789v1","category":"math.NA"}
{"created":"2024-04-19 10:41:44","title":"Search for Z' Radiating from the Dark Matter at the LHC","abstract":"We discuss a collider probe of a dark sector model in which the dark matter is charged under a new, hidden $U(1)$ gauge group. In particular, we look for the so-called Darkstrahlung process, in which the final states dark matter radiates a new $Z'$ gauge boson and it manifests as dilepton resonances. This work emphasizes the potential of dilepton final states with missing transverse energy in probing the darkstrahlung process. We recast the ATLAS Run 2 search for dilepton resonances in association with missing energy in addition to the ATLAS and CMS searches for sleptons. We find that the recasted searches put strong constraints on the coupling between the dark matter and the $Z'$. Moreover, we evaluate refined search strategies for $Z'$ production and propose an analysis employing constraints on the lepton invariant mass and higher missing energy cut related to the darkstrahlung process. Simulation outcomes indicate substantial enhancements, particularly by a factor of 6 in regions featuring lower $Z'$ masses. Finally, we also discuss the case when the $Z'$ has a long lifetime, resulting in displaced decay of the boson.","sentences":["We discuss a collider probe of a dark sector model in which the dark matter is charged under a new, hidden $U(1)$ gauge group.","In particular, we look for the so-called Darkstrahlung process, in which the final states dark matter radiates a new $Z'$ gauge boson and it manifests as dilepton resonances.","This work emphasizes the potential of dilepton final states with missing transverse energy in probing the darkstrahlung process.","We recast the ATLAS Run 2 search for dilepton resonances in association with missing energy in addition to the ATLAS and CMS searches for sleptons.","We find that the recasted searches put strong constraints on the coupling between the dark matter and the $","Z'$. Moreover, we evaluate refined search strategies for $Z'$ production and propose an analysis employing constraints on the lepton invariant mass and higher missing energy cut related to the darkstrahlung process.","Simulation outcomes indicate substantial enhancements, particularly by a factor of 6 in regions featuring lower $Z'$ masses.","Finally, we also discuss the case when the $Z'$ has a long lifetime, resulting in displaced decay of the boson."],"url":"http://arxiv.org/abs/2404.12781v1","category":"hep-ph"}
{"created":"2024-04-19 10:36:37","title":"Inflation, the Hubble Tension and Early Dark Energy: an alternative overview","abstract":"I review and discuss the possible implications for inflation resulting from considering new physics in light of the Hubble tension. My study is motivated by a simple argument that the constraints on inflationary parameters, most typically the spectral index $n_s$, depend to some extent on the cosmological framework. To avoid broadening the uncertainties resulting from marginalizing over additional parameters (typical in many alternative models), I first adopt the same alternative viewpoint of previous studies and analyze what happens if a physical theory can fix extra parameters to non-standard values. Focusing on the dark energy equation of state $w$ and the effective number of relativistic species $N_{\\rm{eff}}$, I confirm that physical theories able to fix $w \\approx -1.2$ or $N_{\\rm{eff}} \\approx 3.9$ produce values of $H_0$ from CMB and BAO in line with the local distance ladder estimate. While in the former case I do not find any relevant implications for inflation, in the latter scenarios, I observe a shift towards $n_s \\approx 1$. From a model-selection perspective, both cases are strongly disfavored compared to $\\Lambda$CDM. However, models with $N_{\\rm{eff}} \\approx 3.3 - 3.4$ could bring the $H_0$ tension down to $\\sim 3\\sigma$ while being moderately disfavored. Yet, this is enough to change the constraints on inflation so that the most accredited models (e.g., Starobinsky inflation) would no longer be favored by data. I then focus on Early Dark Energy (EDE), arguing that an EDE fraction $f_{\\rm{EDE}}\\sim 0.04 - 0.06$ (only able to mildly reduce the $H_0$-tension down to $\\sim 3\\sigma$) could already require a similar change in perspective on inflation. In fact, performing a full joint analysis of EDE and Starobinsky inflation, I find that the two models can hardly coexist for $f_{\\rm{EDE}}\\gtrsim 0.06$.","sentences":["I review and discuss the possible implications for inflation resulting from considering new physics in light of the Hubble tension.","My study is motivated by a simple argument that the constraints on inflationary parameters, most typically the spectral index $n_s$, depend to some extent on the cosmological framework.","To avoid broadening the uncertainties resulting from marginalizing over additional parameters (typical in many alternative models), I first adopt the same alternative viewpoint of previous studies and analyze what happens if a physical theory can fix extra parameters to non-standard values.","Focusing on the dark energy equation of state $w$ and the effective number of relativistic species $N_{\\rm{eff}}$, I confirm that physical theories able to fix $w \\approx -1.2$ or $N_{\\rm{eff}} \\approx 3.9$ produce values of $H_0$ from CMB and BAO in line with the local distance ladder estimate.","While in the former case I do not find any relevant implications for inflation, in the latter scenarios, I observe a shift towards $n_s","\\approx 1$. From a model-selection perspective, both cases are strongly disfavored compared to $\\Lambda$CDM.","However, models with $N_{\\rm{eff}} \\approx 3.3 - 3.4$ could bring the $H_0$ tension down to $\\sim 3\\sigma$ while being moderately disfavored.","Yet, this is enough to change the constraints on inflation so that the most accredited models (e.g., Starobinsky inflation) would no longer be favored by data.","I then focus on Early Dark Energy (EDE), arguing that an EDE fraction $f_{\\rm{EDE}}\\sim 0.04 - 0.06$ (only able to mildly reduce the $H_0$-tension down to $\\sim 3\\sigma$) could already require a similar change in perspective on inflation.","In fact, performing a full joint analysis of EDE and Starobinsky inflation, I find that the two models can hardly coexist for $f_{\\rm{EDE}}\\gtrsim 0.06$."],"url":"http://arxiv.org/abs/2404.12779v1","category":"astro-ph.CO"}
{"created":"2024-04-19 10:10:39","title":"Continual Learning on a Diet: Learning from Sparsely Labeled Streams Under Constrained Computation","abstract":"We propose and study a realistic Continual Learning (CL) setting where learning algorithms are granted a restricted computational budget per time step while training. We apply this setting to large-scale semi-supervised Continual Learning scenarios with sparse label rates. Previous proficient CL methods perform very poorly in this challenging setting. Overfitting to the sparse labeled data and insufficient computational budget are the two main culprits for such a poor performance. Our new setting encourages learning methods to effectively and efficiently utilize the unlabeled data during training. To that end, we propose a simple but highly effective baseline, DietCL, which utilizes both unlabeled and labeled data jointly. DietCL meticulously allocates computational budget for both types of data. We validate our baseline, at scale, on several datasets, e.g., CLOC, ImageNet10K, and CGLM, under constraint budget setups. DietCL outperforms, by a large margin, all existing supervised CL algorithms as well as more recent continual semi-supervised methods. Our extensive analysis and ablations demonstrate that DietCL is stable under a full spectrum of label sparsity, computational budget, and various other ablations.","sentences":["We propose and study a realistic Continual Learning (CL) setting where learning algorithms are granted a restricted computational budget per time step while training.","We apply this setting to large-scale semi-supervised Continual Learning scenarios with sparse label rates.","Previous proficient CL methods perform very poorly in this challenging setting.","Overfitting to the sparse labeled data and insufficient computational budget are the two main culprits for such a poor performance.","Our new setting encourages learning methods to effectively and efficiently utilize the unlabeled data during training.","To that end, we propose a simple but highly effective baseline, DietCL, which utilizes both unlabeled and labeled data jointly.","DietCL meticulously allocates computational budget for both types of data.","We validate our baseline, at scale, on several datasets, e.g., CLOC, ImageNet10K, and CGLM, under constraint budget setups.","DietCL outperforms, by a large margin, all existing supervised CL algorithms as well as more recent continual semi-supervised methods.","Our extensive analysis and ablations demonstrate that DietCL is stable under a full spectrum of label sparsity, computational budget, and various other ablations."],"url":"http://arxiv.org/abs/2404.12766v1","category":"cs.LG"}
{"created":"2024-04-19 10:10:15","title":"$ G $-Bessel processes and related properties","abstract":"In this paper, for a class of $ d $-dimensional $ G $-Brownian motions, we introduce $ G $-Bessel processes. Under the condition of dimensionality $ d $, we obtain that the $ G $-Bessel process is the solution of a stochastic differential equation. Moreover, under the more strict condition of dimensionality $ d $, we get the uniqueness of the solution of stochastic differential equations governing $ G $-Bessel processes and the nonattainability of the origin of $ G $-Brownian motion.","sentences":["In this paper, for a class of $ d $-dimensional $ G $-Brownian motions, we introduce $ G $-Bessel processes.","Under the condition of dimensionality $ d $, we obtain that the $ G $-Bessel process is the solution of a stochastic differential equation.","Moreover, under the more strict condition of dimensionality $ d $, we get the uniqueness of the solution of stochastic differential equations governing $ G $-Bessel processes and the nonattainability of the origin of $ G $-Brownian motion."],"url":"http://arxiv.org/abs/2404.12764v1","category":"math.PR"}
{"created":"2024-04-19 10:08:23","title":"Universality of giant diffusion in tilted periodic potentials","abstract":"Giant diffusion, where the diffusion coefficient of a Brownian particle in a periodic potential with an external force is significantly enhanced by the external force, is a non-trivial non-equilibrium phenomenon. We propose a simple stochastic model of giant diffusion, which is based on a biased continuous-time random walk (CTRW). In this model, we introduce a flight time in the biased CTRW. We derive the diffusion coefficients of this model by the renewal theory and find that there is a maximum diffusion coefficient when the bias is changed. Giant diffusion is universally observed in the sense that there is a peak of the diffusion coefficient for any tilted periodic potentials and the degree of the diffusivity is greatly enhanced especially for low-temperature regimes. The biased CTRW models with flight times are applied to diffusion under three tilted periodic potentials. Furthermore, the temperature dependence of the maximum diffusion coefficient and the external force that attains the maximum are presented for diffusion under a tilted sawtooth potential.","sentences":["Giant diffusion, where the diffusion coefficient of a Brownian particle in a periodic potential with an external force is significantly enhanced by the external force, is a non-trivial non-equilibrium phenomenon.","We propose a simple stochastic model of giant diffusion, which is based on a biased continuous-time random walk (CTRW).","In this model, we introduce a flight time in the biased CTRW.","We derive the diffusion coefficients of this model by the renewal theory and find that there is a maximum diffusion coefficient when the bias is changed.","Giant diffusion is universally observed in the sense that there is a peak of the diffusion coefficient for any tilted periodic potentials and the degree of the diffusivity is greatly enhanced especially for low-temperature regimes.","The biased CTRW models with flight times are applied to diffusion under three tilted periodic potentials.","Furthermore, the temperature dependence of the maximum diffusion coefficient and the external force that attains the maximum are presented for diffusion under a tilted sawtooth potential."],"url":"http://arxiv.org/abs/2404.12761v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-19 10:02:53","title":"decoupleQ: Towards 2-bit Post-Training Uniform Quantization via decoupling Parameters into Integer and Floating Points","abstract":"Quantization emerges as one of the most promising compression technologies for deploying efficient large models for various real time application in recent years. Considering that the storage and IO of weights take up the vast majority of the overhead inside a large model, weight only quantization can lead to large gains. However, existing quantization schemes suffer from significant accuracy degradation at very low bits, or require some additional computational overhead when deployed, making it difficult to be applied to large-scale applications in industry. In this paper, we propose decoupleQ, achieving a substantial increase in model accuracy, especially at very low bits. decoupleQ abandons the traditional heuristic quantization paradigm and decouples the model parameters into integer and floating-point parts, thus transforming the quantization problem into a traditional mathematical optimization problem with constraints, which is then solved alternatively by off-the-shelf optimization methods.   Quantization via decoupleQ is linear and uniform, making it hardware-friendlier than non-uniform counterpart, and enabling the idea to be migrated to high-bit quantization to enhance its robustness. Our method has achieved well on-line accuracy near fp16/bf16 on the 2-bit quantization of large speech models in ByteDance. The code is available at https://github.com/bytedance/decoupleQ","sentences":["Quantization emerges as one of the most promising compression technologies for deploying efficient large models for various real time application in recent years.","Considering that the storage and IO of weights take up the vast majority of the overhead inside a large model, weight only quantization can lead to large gains.","However, existing quantization schemes suffer from significant accuracy degradation at very low bits, or require some additional computational overhead when deployed, making it difficult to be applied to large-scale applications in industry.","In this paper, we propose decoupleQ, achieving a substantial increase in model accuracy, especially at very low bits.","decoupleQ abandons the traditional heuristic quantization paradigm and decouples the model parameters into integer and floating-point parts, thus transforming the quantization problem into a traditional mathematical optimization problem with constraints, which is then solved alternatively by off-the-shelf optimization methods.   ","Quantization via decoupleQ is linear and uniform, making it hardware-friendlier than non-uniform counterpart, and enabling the idea to be migrated to high-bit quantization to enhance its robustness.","Our method has achieved well on-line accuracy near fp16/bf16 on the 2-bit quantization of large speech models in ByteDance.","The code is available at https://github.com/bytedance/decoupleQ"],"url":"http://arxiv.org/abs/2404.12759v1","category":"cs.LG"}
{"created":"2024-04-19 09:25:43","title":"Stability, convergence, and pressure-robustness of numerical schemes for incompressible flows with hybrid velocity and pressure","abstract":"In this work we study the stability, convergence, and pressure-robustness of discretization methods for incompressible flows with hybrid velocity and pressure. Specifically, focusing on the Stokes problem, we identify a set of assumptions that yield inf-sup stability as well as error estimates which distinguish the velocity- and pressure-related contributions to the error. We additionally identify the key properties under which the pressure-related contributions vanish in the estimate of the velocity, thus leading to pressure-robustness. Several examples of existing and new schemes that fit into the framework are provided, and extensive numerical validation of the theoretical properties is provided.","sentences":["In this work we study the stability, convergence, and pressure-robustness of discretization methods for incompressible flows with hybrid velocity and pressure.","Specifically, focusing on the Stokes problem, we identify a set of assumptions that yield inf-sup stability as well as error estimates which distinguish the velocity- and pressure-related contributions to the error.","We additionally identify the key properties under which the pressure-related contributions vanish in the estimate of the velocity, thus leading to pressure-robustness.","Several examples of existing and new schemes that fit into the framework are provided, and extensive numerical validation of the theoretical properties is provided."],"url":"http://arxiv.org/abs/2404.12732v1","category":"math.NA"}
{"created":"2024-04-19 09:17:15","title":"Demonstration of quantum projective simulation on a single-photon-based quantum computer","abstract":"Variational quantum algorithms show potential in effectively operating on noisy intermediate-scale quantum devices. A novel variational approach to reinforcement learning has been recently proposed, incorporating linear-optical interferometers and a classical learning model known as projective simulation (PS). PS is a decision-making tool for reinforcement learning and can be classically represented as a random walk on a graph that describes the agent's memory. In its optical quantum version, this approach utilizes quantum walks of single photons on a mesh of tunable beamsplitters and phase shifters to select actions. In this work, we present the implementation of this algorithm on Ascella, a single-photon-based quantum computer from Quandela. The focus is drawn on solving a test bed task to showcase the potential of the quantum agent with respect to the classical agent.","sentences":["Variational quantum algorithms show potential in effectively operating on noisy intermediate-scale quantum devices.","A novel variational approach to reinforcement learning has been recently proposed, incorporating linear-optical interferometers and a classical learning model known as projective simulation (PS).","PS is a decision-making tool for reinforcement learning and can be classically represented as a random walk on a graph that describes the agent's memory.","In its optical quantum version, this approach utilizes quantum walks of single photons on a mesh of tunable beamsplitters and phase shifters to select actions.","In this work, we present the implementation of this algorithm on Ascella, a single-photon-based quantum computer from Quandela.","The focus is drawn on solving a test bed task to showcase the potential of the quantum agent with respect to the classical agent."],"url":"http://arxiv.org/abs/2404.12729v1","category":"quant-ph"}
{"created":"2024-04-19 08:33:30","title":"Magnetic-field driven evolution of zero-energy mode on Bi islands deposited on Fe(Te,Se)","abstract":"We investigate the magnetic-field dependent evolution of the zero-bias conductance peaks (ZBCPs) on the nanoscale bismuth islands grown on the FeTe$_{0.55}$Se$_{0.45}$ substrate. The ZBCPs can be observed throughout the entire region on these islands, and their characteristics align with the signatures of Majorana zero modes. Remarkably, the evolution of ZBCPs on these islands exhibits anomalous behavior under varying magnetic fields: The magnitude of ZBCPs is first enhanced at weak fields lower than 2 T and then suppressed as the fields further increase. We attribute the non-monotonic evolution of the ZBCPs to the magnetic-field-enhanced topological edge states on these Bi islands. Our findings provide valuable insights into the probable origin of the Majorana zero modes in the Bi-island platform and the magnetic-field response of topological edge states.","sentences":["We investigate the magnetic-field dependent evolution of the zero-bias conductance peaks (ZBCPs) on the nanoscale bismuth islands grown on the FeTe$_{0.55}$Se$_{0.45}$ substrate.","The ZBCPs can be observed throughout the entire region on these islands, and their characteristics align with the signatures of Majorana zero modes.","Remarkably, the evolution of ZBCPs on these islands exhibits anomalous behavior under varying magnetic fields:","The magnitude of ZBCPs is first enhanced at weak fields lower than 2 T and then suppressed as the fields further increase.","We attribute the non-monotonic evolution of the ZBCPs to the magnetic-field-enhanced topological edge states on these Bi islands.","Our findings provide valuable insights into the probable origin of the Majorana zero modes in the Bi-island platform and the magnetic-field response of topological edge states."],"url":"http://arxiv.org/abs/2404.12708v1","category":"cond-mat.supr-con"}
{"created":"2024-04-19 12:26:28","title":"TartuNLP @ SIGTYP 2024 Shared Task: Adapting XLM-RoBERTa for Ancient and Historical Languages","abstract":"We present our submission to the unconstrained subtask of the SIGTYP 2024 Shared Task on Word Embedding Evaluation for Ancient and Historical Languages for morphological annotation, POS-tagging, lemmatization, character- and word-level gap-filling. We developed a simple, uniform, and computationally lightweight approach based on the adapters framework using parameter-efficient fine-tuning. We applied the same adapter-based approach uniformly to all tasks and 16 languages by fine-tuning stacked language- and task-specific adapters. Our submission obtained an overall second place out of three submissions, with the first place in word-level gap-filling. Our results show the feasibility of adapting language models pre-trained on modern languages to historical and ancient languages via adapter training.","sentences":["We present our submission to the unconstrained subtask of the SIGTYP 2024 Shared Task on Word Embedding Evaluation for Ancient and Historical Languages for morphological annotation, POS-tagging, lemmatization, character- and word-level gap-filling.","We developed a simple, uniform, and computationally lightweight approach based on the adapters framework using parameter-efficient fine-tuning.","We applied the same adapter-based approach uniformly to all tasks and 16 languages by fine-tuning stacked language-","and task-specific adapters.","Our submission obtained an overall second place out of three submissions, with the first place in word-level gap-filling.","Our results show the feasibility of adapting language models pre-trained on modern languages to historical and ancient languages via adapter training."],"url":"http://arxiv.org/abs/2404.12845v1","category":"cs.CL"}
{"created":"2024-04-19 11:41:39","title":"A Hybrid Process for Integration of Organic Electrochemical Transistors for High Uniformity & Reliability","abstract":"Photolithography is believed to be a complementary technique to large-area printing, allowing for nanometer-scale integration and offering cost-efficiency. For organic electronics though, adapting photolithography is very challenging due to chemical incompatibilities. However, with the help of Alexander Zakhidov, orthogonal resins opened up the prospect of adapting the well-established process of photolithography for organic electronics. Here, we present a hybrid fabrication method for organic electrochemical transistors by combining orthogonal photolithography and inkjet printing, enabling high uniformity and reliability. We demonstrate how the resolution of each process affects the uniformity, and we explore the advantages of this process for device scaling and circuit integration.","sentences":["Photolithography is believed to be a complementary technique to large-area printing, allowing for nanometer-scale integration and offering cost-efficiency.","For organic electronics though, adapting photolithography is very challenging due to chemical incompatibilities.","However, with the help of Alexander Zakhidov, orthogonal resins opened up the prospect of adapting the well-established process of photolithography for organic electronics.","Here, we present a hybrid fabrication method for organic electrochemical transistors by combining orthogonal photolithography and inkjet printing, enabling high uniformity and reliability.","We demonstrate how the resolution of each process affects the uniformity, and we explore the advantages of this process for device scaling and circuit integration."],"url":"http://arxiv.org/abs/2404.12806v1","category":"physics.app-ph"}
{"created":"2024-04-19 17:39:50","title":"BANF: Band-limited Neural Fields for Levels of Detail Reconstruction","abstract":"Largely due to their implicit nature, neural fields lack a direct mechanism for filtering, as Fourier analysis from discrete signal processing is not directly applicable to these representations. Effective filtering of neural fields is critical to enable level-of-detail processing in downstream applications, and support operations that involve sampling the field on regular grids (e.g. marching cubes). Existing methods that attempt to decompose neural fields in the frequency domain either resort to heuristics or require extensive modifications to the neural field architecture. We show that via a simple modification, one can obtain neural fields that are low-pass filtered, and in turn show how this can be exploited to obtain a frequency decomposition of the entire signal. We demonstrate the validity of our technique by investigating level-of-detail reconstruction, and showing how coarser representations can be computed effectively.","sentences":["Largely due to their implicit nature, neural fields lack a direct mechanism for filtering, as Fourier analysis from discrete signal processing is not directly applicable to these representations.","Effective filtering of neural fields is critical to enable level-of-detail processing in downstream applications, and support operations that involve sampling the field on regular grids (e.g. marching cubes).","Existing methods that attempt to decompose neural fields in the frequency domain either resort to heuristics or require extensive modifications to the neural field architecture.","We show that via a simple modification, one can obtain neural fields that are low-pass filtered, and in turn show how this can be exploited to obtain a frequency decomposition of the entire signal.","We demonstrate the validity of our technique by investigating level-of-detail reconstruction, and showing how coarser representations can be computed effectively."],"url":"http://arxiv.org/abs/2404.13024v1","category":"cs.CV"}
{"created":"2024-04-19 14:38:49","title":"Search for a resonance decaying into a scalar particle and a Higgs boson in the final state with two bottom quarks and two photons in proton-proton collisions at a center of mass energy of 13 TeV with the ATLAS detector","abstract":"A search for the resonant production of a heavy scalar $X$ decaying into a Higgs boson and a new lighter scalar $S$, through the process $X \\to S(\\to bb) H(\\to \\gamma\\gamma)$, where the two photons are consistent with the Higgs boson decay, is performed. The search is conducted using an integrated luminosity of 140 fb$^{-1}$ of proton-proton collision data at a centre-of-mass energy of 13 TeV recorded with the ATLAS detector at the Large Hadron Collider. The search is performed over the mass range 170 $\\leq$ $m_{X}$ $\\leq$ 1000 GeV and 15 $\\leq$ $m_{S}$ $\\leq$ 500 GeV. Parameterised neural networks are used to enhance the signal purity and to achieve continuous sensitivity in a domain of the ($m_{X}$, $m_{S}$) plane. No significant excess above the expected background is found and 95% CL upper limits are set on the cross section times branching ratio, ranging from 39 fb to 0.09 fb. The largest deviation from the background-only expectation occurs for ($m_{X}$, $m_{S}$) = (575, 200) GeV with a local (global) significance of 3.5 (2.0) standard deviations.","sentences":["A search for the resonant production of a heavy scalar $X$ decaying into a Higgs boson and a new lighter scalar $S$, through the process $X \\to S(\\to bb) H(\\to \\gamma\\gamma)$, where the two photons are consistent with the Higgs boson decay, is performed.","The search is conducted using an integrated luminosity of 140 fb$^{-1}$ of proton-proton collision data at a centre-of-mass energy of 13 TeV recorded with the ATLAS detector at the Large Hadron Collider.","The search is performed over the mass range 170 $\\leq$ $m_{X}$ $\\leq$ 1000 GeV and 15 $\\leq$ $m_{S}$ $\\leq$ 500 GeV. Parameterised neural networks are used to enhance the signal purity and to achieve continuous sensitivity in a domain of the ($m_{X}$, $m_{S}$) plane.","No significant excess above the expected background is found and 95% CL upper limits are set on the cross section times branching ratio, ranging from 39 fb to 0.09 fb.","The largest deviation from the background-only expectation occurs for ($m_{X}$, $m_{S}$) = (575, 200) GeV with a local (global) significance of 3.5 (2.0) standard deviations."],"url":"http://arxiv.org/abs/2404.12915v1","category":"hep-ex"}
{"created":"2024-04-19 13:26:54","title":"John's blow up examples and scattering solutions for semi-linear wave equations","abstract":"In light of recent work of the third author, we revisit a classic example given by Fritz John of a semi-linear wave equation which exhibits finite in time blow up for all compactly supported data. We present the construction of future global solutions from asymptotic data given in arXiv:2204.12870(2022) for this specific example, and clarify the relation of this result of Yu to John's theorem. Furthermore we present a novel blow up result for finite energy solutions satisfying a sign condition due to the first author, and invoke this result to show that the constructed backwards in time solutions blow up in the past.","sentences":["In light of recent work of the third author, we revisit a classic example given by Fritz John of a semi-linear wave equation which exhibits finite in time blow up for all compactly supported data.","We present the construction of future global solutions from asymptotic data given in arXiv:2204.12870(2022) for this specific example, and clarify the relation of this result of Yu to John's theorem.","Furthermore we present a novel blow up result for finite energy solutions satisfying a sign condition due to the first author, and invoke this result to show that the constructed backwards in time solutions blow up in the past."],"url":"http://arxiv.org/abs/2404.12878v1","category":"math.AP"}
{"created":"2024-04-19 12:54:17","title":"Relative Energy Method For Weak-Strong Uniqueness Of The Inhomogeneous Navier-Stokes Equations","abstract":"We present a weak-strong uniqueness result for the inhomogeneous Navier-Stokes (INS) equations in $\\mathbb{R}^d$ ($d=2,3$) for bounded initial densities that are far from vacuum. Given a strong solution within the class employed in Paicu, Zhang and Zhang (2013) and Chen, Zhang and Zhao (2016), and a Leray-Hopf weak solution, we establish that they coincide if the initial data agree. The strategy of our proof is based on the relative energy method and new $W^{-1,p}$-type stability estimates for the density. A key point lies in proving that every Leray-Hopf weak solution originating from initial densities far from vacuum remains distant from vacuum at all times.","sentences":["We present a weak-strong uniqueness result for the inhomogeneous Navier-Stokes (INS) equations in $\\mathbb{R}^d$ ($d=2,3$) for bounded initial densities that are far from vacuum.","Given a strong solution within the class employed in Paicu, Zhang and Zhang (2013) and Chen, Zhang and Zhao (2016), and a Leray-Hopf weak solution, we establish that they coincide if the initial data agree.","The strategy of our proof is based on the relative energy method and new $W^{-1,p}$-type stability estimates for the density.","A key point lies in proving that every Leray-Hopf weak solution originating from initial densities far from vacuum remains distant from vacuum at all times."],"url":"http://arxiv.org/abs/2404.12858v1","category":"math.AP"}
{"created":"2024-04-19 11:43:33","title":"Mapping the SMEFT at High-Energy Colliders: from LEP and the (HL-)LHC to the FCC-ee","abstract":"We present SMEFiT3.0, an updated global SMEFT analysis of Higgs, top quark, and diboson production data from the LHC complemented by electroweak precision observables (EWPOs) from LEP and SLD. We consider recent inclusive and differential measurements from the LHC Run II, alongside with a novel implementation of the EWPOs based on independent calculations of the relevant EFT contributions. We estimate the impact of HL-LHC measurements on the SMEFT parameter space when added on top of SMEFiT3.0, through dedicated projections extrapolating from Run II data. We quantify the significant constraints that measurements from two proposed high-energy circular $e^+e^-$ colliders, the FCC-ee and the CEPC, would impose on both the SMEFT parameter space and on representative UV-complete models. Our analysis considers projections for the FCC-ee and the CEPC based on the latest running scenarios and includes $Z$-pole EWPOs, fermion-pair, Higgs, diboson, and top quark production, using optimal observables for both the $W^+W^-$ and the $t\\bar{t}$ channels. The framework presented in this work may be extended to other future colliders and running scenarios, providing timely input to ongoing studies towards future high-energy particle physics facilities.","sentences":["We present SMEFiT3.0, an updated global SMEFT analysis of Higgs, top quark, and diboson production data from the LHC complemented by electroweak precision observables (EWPOs) from LEP and SLD.","We consider recent inclusive and differential measurements from the LHC Run II, alongside with a novel implementation of the EWPOs based on independent calculations of the relevant EFT contributions.","We estimate the impact of HL-LHC measurements on the SMEFT parameter space when added on top of SMEFiT3.0, through dedicated projections extrapolating from Run II data.","We quantify the significant constraints that measurements from two proposed high-energy circular $e^+e^-$ colliders, the FCC-ee and the CEPC, would impose on both the SMEFT parameter space and on representative UV-complete models.","Our analysis considers projections for the FCC-ee and the CEPC based on the latest running scenarios and includes $Z$-pole EWPOs, fermion-pair, Higgs, diboson, and top quark production, using optimal observables for both the $W^+W^-$ and the $t\\bar{t}$ channels.","The framework presented in this work may be extended to other future colliders and running scenarios, providing timely input to ongoing studies towards future high-energy particle physics facilities."],"url":"http://arxiv.org/abs/2404.12809v1","category":"hep-ph"}
{"created":"2024-04-19 11:18:34","title":"Stability for a class of three-tori with small negative scalar curvature","abstract":"We define a flexible class of Riemmanian metrics on the three-torus. Then, using Stern's inequality relating scalar curvature to harmonic one-forms, we show that any sequence of metrics in this family has a subsequence which converges to some flat metric on the torus in the sense of Dong-Song.","sentences":["We define a flexible class of Riemmanian metrics on the three-torus.","Then, using Stern's inequality relating scalar curvature to harmonic one-forms, we show that any sequence of metrics in this family has a subsequence which converges to some flat metric on the torus in the sense of Dong-Song."],"url":"http://arxiv.org/abs/2404.12795v1","category":"math.DG"}
{"created":"2024-04-19 10:36:00","title":"Defending against Data Poisoning Attacks in Federated Learning via User Elimination","abstract":"In the evolving landscape of Federated Learning (FL), a new type of attacks concerns the research community, namely Data Poisoning Attacks, which threaten the model integrity by maliciously altering training data. This paper introduces a novel defensive framework focused on the strategic elimination of adversarial users within a federated model. We detect those anomalies in the aggregation phase of the Federated Algorithm, by integrating metadata gathered by the local training instances with Differential Privacy techniques, to ensure that no data leakage is possible. To our knowledge, this is the first proposal in the field of FL that leverages metadata other than the model's gradients in order to ensure honesty in the reported local models. Our extensive experiments demonstrate the efficacy of our methods, significantly mitigating the risk of data poisoning while maintaining user privacy and model performance. Our findings suggest that this new user elimination approach serves us with a great balance between privacy and utility, thus contributing to the arsenal of arguments in favor of the safe adoption of FL in safe domains, both in academic setting and in the industry.","sentences":["In the evolving landscape of Federated Learning (FL), a new type of attacks concerns the research community, namely Data Poisoning Attacks, which threaten the model integrity by maliciously altering training data.","This paper introduces a novel defensive framework focused on the strategic elimination of adversarial users within a federated model.","We detect those anomalies in the aggregation phase of the Federated Algorithm, by integrating metadata gathered by the local training instances with Differential Privacy techniques, to ensure that no data leakage is possible.","To our knowledge, this is the first proposal in the field of FL that leverages metadata other than the model's gradients in order to ensure honesty in the reported local models.","Our extensive experiments demonstrate the efficacy of our methods, significantly mitigating the risk of data poisoning while maintaining user privacy and model performance.","Our findings suggest that this new user elimination approach serves us with a great balance between privacy and utility, thus contributing to the arsenal of arguments in favor of the safe adoption of FL in safe domains, both in academic setting and in the industry."],"url":"http://arxiv.org/abs/2404.12778v1","category":"cs.CR"}
{"created":"2024-04-19 09:56:56","title":"Leveraging Symbolic Regression for Heuristic Design in the Traveling Thief Problem","abstract":"The Traveling Thief Problem is an NP-hard combination of the well known traveling salesman and knapsack packing problems. In this paper, we use symbolic regression to learn useful features of near-optimal packing plans, which we then use to design efficient metaheuristic genetic algorithms for the traveling thief algorithm. By using symbolic regression again to initialize the metaheuristic GA with near-optimal individuals, we are able to design a fast, interpretable, and effective packing initialization scheme. Comparisons against previous initialization schemes validates our algorithm design.","sentences":["The Traveling Thief Problem is an NP-hard combination of the well known traveling salesman and knapsack packing problems.","In this paper, we use symbolic regression to learn useful features of near-optimal packing plans, which we then use to design efficient metaheuristic genetic algorithms for the traveling thief algorithm.","By using symbolic regression again to initialize the metaheuristic GA with near-optimal individuals, we are able to design a fast, interpretable, and effective packing initialization scheme.","Comparisons against previous initialization schemes validates our algorithm design."],"url":"http://arxiv.org/abs/2404.12750v1","category":"cs.NE"}
{"created":"2024-04-19 09:46:59","title":"Near-Tight Runtime Guarantees for Many-Objective Evolutionary Algorithms","abstract":"Despite significant progress in the field of mathematical runtime analysis of multi-objective evolutionary algorithms (MOEAs), the performance of MOEAs on discrete many-objective problems is little understood. In particular, the few existing bounds for the SEMO, global SEMO, and SMS-EMOA algorithms on classic benchmarks are all roughly quadratic in the size of the Pareto front. In this work, we prove near-tight runtime guarantees for these three algorithms on the four most common benchmark problems OneMinMax, CountingOnesCountingZeros, LeadingOnesTrailingZeros, and OneJumpZeroJump, and this for arbitrary numbers of objectives. Our bounds depend only linearly on the Pareto front size, showing that these MOEAs on these benchmarks cope much better with many objectives than what previous works suggested. Our bounds are tight apart from small polynomial factors in the number of objectives and length of bitstrings. This is the first time that such tight bounds are proven for many-objective uses of these MOEAs. While it is known that such results cannot hold for the NSGA-II, we do show that our bounds, via a recent structural result, transfer to the NSGA-III algorithm.","sentences":["Despite significant progress in the field of mathematical runtime analysis of multi-objective evolutionary algorithms (MOEAs), the performance of MOEAs on discrete many-objective problems is little understood.","In particular, the few existing bounds for the SEMO, global SEMO, and SMS-EMOA algorithms on classic benchmarks are all roughly quadratic in the size of the Pareto front.","In this work, we prove near-tight runtime guarantees for these three algorithms on the four most common benchmark problems OneMinMax, CountingOnesCountingZeros, LeadingOnesTrailingZeros, and OneJumpZeroJump, and this for arbitrary numbers of objectives.","Our bounds depend only linearly on the Pareto front size, showing that these MOEAs on these benchmarks cope much better with many objectives than what previous works suggested.","Our bounds are tight apart from small polynomial factors in the number of objectives and length of bitstrings.","This is the first time that such tight bounds are proven for many-objective uses of these MOEAs.","While it is known that such results cannot hold for the NSGA-II, we do show that our bounds, via a recent structural result, transfer to the NSGA-III algorithm."],"url":"http://arxiv.org/abs/2404.12746v1","category":"cs.NE"}
{"created":"2024-04-19 09:46:45","title":"Recurrent Neural Networks for Modelling Gross Primary Production","abstract":"Accurate quantification of Gross Primary Production (GPP) is crucial for understanding terrestrial carbon dynamics. It represents the largest atmosphere-to-land CO$_2$ flux, especially significant for forests. Eddy Covariance (EC) measurements are widely used for ecosystem-scale GPP quantification but are globally sparse. In areas lacking local EC measurements, remote sensing (RS) data are typically utilised to estimate GPP after statistically relating them to in-situ data. Deep learning offers novel perspectives, and the potential of recurrent neural network architectures for estimating daily GPP remains underexplored. This study presents a comparative analysis of three architectures: Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs), and Long-Short Term Memory (LSTMs). Our findings reveal comparable performance across all models for full-year and growing season predictions. Notably, LSTMs outperform in predicting climate-induced GPP extremes. Furthermore, our analysis highlights the importance of incorporating radiation and RS inputs (optical, temperature, and radar) for accurate GPP predictions, particularly during climate extremes.","sentences":["Accurate quantification of Gross Primary Production (GPP) is crucial for understanding terrestrial carbon dynamics.","It represents the largest atmosphere-to-land CO$_2$ flux, especially significant for forests.","Eddy Covariance (EC) measurements are widely used for ecosystem-scale GPP quantification but are globally sparse.","In areas lacking local EC measurements, remote sensing (RS) data are typically utilised to estimate GPP after statistically relating them to in-situ data.","Deep learning offers novel perspectives, and the potential of recurrent neural network architectures for estimating daily GPP remains underexplored.","This study presents a comparative analysis of three architectures: Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs), and Long-Short Term Memory (LSTMs).","Our findings reveal comparable performance across all models for full-year and growing season predictions.","Notably, LSTMs outperform in predicting climate-induced GPP extremes.","Furthermore, our analysis highlights the importance of incorporating radiation and RS inputs (optical, temperature, and radar) for accurate GPP predictions, particularly during climate extremes."],"url":"http://arxiv.org/abs/2404.12745v1","category":"cs.LG"}
{"created":"2024-04-19 09:36:48","title":"Multi-Class Quantum Convolutional Neural Networks","abstract":"Classification is particularly relevant to Information Retrieval, as it is used in various subtasks of the search pipeline. In this work, we propose a quantum convolutional neural network (QCNN) for multi-class classification of classical data. The model is implemented using PennyLane. The optimization process is conducted by minimizing the cross-entropy loss through parameterized quantum circuit optimization. The QCNN is tested on the MNIST dataset with 4, 6, 8 and 10 classes. The results show that with 4 classes, the performance is slightly lower compared to the classical CNN, while with a higher number of classes, the QCNN outperforms the classical neural network.","sentences":["Classification is particularly relevant to Information Retrieval, as it is used in various subtasks of the search pipeline.","In this work, we propose a quantum convolutional neural network (QCNN) for multi-class classification of classical data.","The model is implemented using PennyLane.","The optimization process is conducted by minimizing the cross-entropy loss through parameterized quantum circuit optimization.","The QCNN is tested on the MNIST dataset with 4, 6, 8 and 10 classes.","The results show that with 4 classes, the performance is slightly lower compared to the classical CNN, while with a higher number of classes, the QCNN outperforms the classical neural network."],"url":"http://arxiv.org/abs/2404.12741v1","category":"quant-ph"}
{"created":"2024-04-19 09:08:12","title":"Graph Learning Dual Graph Convolutional Network For Semi-Supervised Node Classification With Subgraph Sketch","abstract":"In this paper, we propose the G raph Learning D ual G raph Convolutional Neural Network called GLDGCN based on the classical Graph Convolutional Neural Network by introducing dual convolutional layer and graph learning layer. We apply GLDGCN to the semi-supervised node classification task. Compared with the baseline methods, we achieve higher classification accuracy on three citation networks Citeseer, Cora and Pubmed, and we also analyze and discussabout selection of the hyperparameters and network depth. GLDGCN also perform well on the classic social network KarateClub and the new Wiki-CS dataset.   For the insufficient ability of our algorithm to process large graph data during the experiment, we also introduce subgraph clustering and stochastic gradient descent technology into GCN and design a semi-supervised node classification algorithm based on the CLustering G raph Convolutional neural Network, which enables GCN to process large graph and improves its application value. We complete semi-supervised node classification experiments on two classical large graph which are PPI data sets (more than 50,000 nodes) and Reddit data sets (more than 200,000 nodes), and also perform well.","sentences":["In this paper, we propose the G raph Learning D ual G raph Convolutional Neural Network called GLDGCN based on the classical Graph Convolutional Neural Network by introducing dual convolutional layer and graph learning layer.","We apply GLDGCN to the semi-supervised node classification task.","Compared with the baseline methods, we achieve higher classification accuracy on three citation networks Citeseer, Cora and Pubmed, and we also analyze and discussabout selection of the hyperparameters and network depth.","GLDGCN also perform well on the classic social network KarateClub and the new Wiki-CS dataset.   ","For the insufficient ability of our algorithm to process large graph data during the experiment, we also introduce subgraph clustering and stochastic gradient descent technology into GCN and design a semi-supervised node classification algorithm based on the CLustering G raph Convolutional neural Network, which enables GCN to process large graph and improves its application value.","We complete semi-supervised node classification experiments on two classical large graph which are PPI data sets (more than 50,000 nodes) and Reddit data sets (more than 200,000 nodes), and also perform well."],"url":"http://arxiv.org/abs/2404.12724v1","category":"cs.LG"}
{"created":"2024-04-19 17:58:04","title":"Unified Scene Representation and Reconstruction for 3D Large Language Models","abstract":"Enabling Large Language Models (LLMs) to interact with 3D environments is challenging. Existing approaches extract point clouds either from ground truth (GT) geometry or 3D scenes reconstructed by auxiliary models. Text-image aligned 2D features from CLIP are then lifted to point clouds, which serve as inputs for LLMs. However, this solution lacks the establishment of 3D point-to-point connections, leading to a deficiency of spatial structure information. Concurrently, the absence of integration and unification between the geometric and semantic representations of the scene culminates in a diminished level of 3D scene understanding. In this paper, we demonstrate the importance of having a unified scene representation and reconstruction framework, which is essential for LLMs in 3D scenes. Specifically, we introduce Uni3DR^2 extracts 3D geometric and semantic aware representation features via the frozen pre-trained 2D foundation models (e.g., CLIP and SAM) and a multi-scale aggregate 3D decoder. Our learned 3D representations not only contribute to the reconstruction process but also provide valuable knowledge for LLMs. Experimental results validate that our Uni3DR^2 yields convincing gains over the baseline on the 3D reconstruction dataset ScanNet (increasing F-Score by +1.8\\%). When applied to LLMs, our Uni3DR^2-LLM exhibits superior performance over the baseline on the 3D vision-language understanding dataset ScanQA (increasing BLEU-1 by +4.0\\% and +4.2\\% on the val set and test set, respectively). Furthermore, it outperforms the state-of-the-art method that uses additional GT point clouds on both ScanQA and 3DMV-VQA.","sentences":["Enabling Large Language Models (LLMs) to interact with 3D environments is challenging.","Existing approaches extract point clouds either from ground truth (GT) geometry or 3D scenes reconstructed by auxiliary models.","Text-image aligned 2D features from CLIP are then lifted to point clouds, which serve as inputs for LLMs.","However, this solution lacks the establishment of 3D point-to-point connections, leading to a deficiency of spatial structure information.","Concurrently, the absence of integration and unification between the geometric and semantic representations of the scene culminates in a diminished level of 3D scene understanding.","In this paper, we demonstrate the importance of having a unified scene representation and reconstruction framework, which is essential for LLMs in 3D scenes.","Specifically, we introduce Uni3DR^2 extracts 3D geometric and semantic aware representation features via the frozen pre-trained 2D foundation models (e.g., CLIP and SAM) and a multi-scale aggregate 3D decoder.","Our learned 3D representations not only contribute to the reconstruction process but also provide valuable knowledge for LLMs.","Experimental results validate that our Uni3DR^2 yields convincing gains over the baseline on the 3D reconstruction dataset ScanNet (increasing F-Score by +1.8\\%).","When applied to LLMs, our Uni3DR^2-LLM exhibits superior performance over the baseline on the 3D vision-language understanding dataset ScanQA (increasing BLEU-1 by +4.0\\% and +4.2\\% on the val set and test set, respectively).","Furthermore, it outperforms the state-of-the-art method that uses additional GT point clouds on both ScanQA and 3DMV-VQA."],"url":"http://arxiv.org/abs/2404.13044v1","category":"cs.CV"}
{"created":"2024-04-19 17:35:35","title":"Machine Learning-guided accelerated discovery of structure-property correlations in lean magnesium alloys for biomedical applications","abstract":"Magnesium alloys are emerging as promising alternatives to traditional orthopedic implant materials thanks to their biodegradability, biocompatibility, and impressive mechanical characteristics. However, their rapid in-vivo degradation presents challenges, notably in upholding mechanical integrity over time. This study investigates the impact of high-temperature thermal processing on the mechanical and degradation attributes of a lean Mg-Zn-Ca-Mn alloy, ZX10. Utilizing rapid, cost-efficient characterization methods like X-ray diffraction and optical, we swiftly examine microstructural changes post-thermal treatment. Employing Pearson correlation coefficient analysis, we unveil the relationship between microstructural properties and critical targets (properties): hardness and corrosion resistance. Additionally, leveraging the least absolute shrinkage and selection operator (LASSO), we pinpoint the dominant microstructural factors among closely correlated variables. Our findings underscore the significant role of grain size refinement in strengthening and the predominance of the ternary Ca2Mg6Zn3 phase in corrosion behavior. This suggests that achieving an optimal blend of strength and corrosion resistance is attainable through fine grains and reduced concentration of ternary phases. This thorough investigation furnishes valuable insights into the intricate interplay of processing, structure, and properties in magnesium alloys, thereby advancing the development of superior biodegradable implant materials.","sentences":["Magnesium alloys are emerging as promising alternatives to traditional orthopedic implant materials thanks to their biodegradability, biocompatibility, and impressive mechanical characteristics.","However, their rapid in-vivo degradation presents challenges, notably in upholding mechanical integrity over time.","This study investigates the impact of high-temperature thermal processing on the mechanical and degradation attributes of a lean Mg-Zn-Ca-Mn alloy, ZX10.","Utilizing rapid, cost-efficient characterization methods like X-ray diffraction and optical, we swiftly examine microstructural changes post-thermal treatment.","Employing Pearson correlation coefficient analysis, we unveil the relationship between microstructural properties and critical targets (properties): hardness and corrosion resistance.","Additionally, leveraging the least absolute shrinkage and selection operator (LASSO), we pinpoint the dominant microstructural factors among closely correlated variables.","Our findings underscore the significant role of grain size refinement in strengthening and the predominance of the ternary Ca2Mg6Zn3 phase in corrosion behavior.","This suggests that achieving an optimal blend of strength and corrosion resistance is attainable through fine grains and reduced concentration of ternary phases.","This thorough investigation furnishes valuable insights into the intricate interplay of processing, structure, and properties in magnesium alloys, thereby advancing the development of superior biodegradable implant materials."],"url":"http://arxiv.org/abs/2404.13022v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-19 17:30:10","title":"Stronger Random Baselines for In-Context Learning","abstract":"Evaluating the in-context learning classification performance of language models poses challenges due to small dataset sizes, extensive prompt-selection using the validation set, and intentionally difficult tasks that lead to near-random performance. The standard random baseline -- the expected accuracy of guessing labels uniformly at random -- is stable when the evaluation set is used only once or when the dataset is large. We account for the common practice of validation set reuse and existing small datasets with a stronger random baseline: the expected maximum accuracy across multiple random classifiers. When choosing the best prompt demonstrations across six quantized language models applied to 16 BIG-bench Lite tasks, more than 20\\% of the few-shot results that exceed the standard baseline do not exceed this stronger random baseline. When held-out test sets are available, this stronger baseline is also a better predictor of held-out performance than the standard baseline, avoiding unnecessary test set evaluations. This maximum random baseline provides an easily calculated drop-in replacement for the standard baseline.","sentences":["Evaluating the in-context learning classification performance of language models poses challenges due to small dataset sizes, extensive prompt-selection using the validation set, and intentionally difficult tasks that lead to near-random performance.","The standard random baseline -- the expected accuracy of guessing labels uniformly at random -- is stable when the evaluation set is used only once or when the dataset is large.","We account for the common practice of validation set reuse and existing small datasets with a stronger random baseline: the expected maximum accuracy across multiple random classifiers.","When choosing the best prompt demonstrations across six quantized language models applied to 16 BIG-bench Lite tasks, more than 20\\% of the few-shot results that exceed the standard baseline do not exceed this stronger random baseline.","When held-out test sets are available, this stronger baseline is also a better predictor of held-out performance than the standard baseline, avoiding unnecessary test set evaluations.","This maximum random baseline provides an easily calculated drop-in replacement for the standard baseline."],"url":"http://arxiv.org/abs/2404.13020v1","category":"cs.CL"}
{"created":"2024-04-19 17:26:43","title":"A New Multi-Picture Architecture for Learned Video Deinterlacing and Demosaicing with Parallel Deformable Convolution and Self-Attention Blocks","abstract":"Despite the fact real-world video deinterlacing and demosaicing are well-suited to supervised learning from synthetically degraded data because the degradation models are known and fixed, learned video deinterlacing and demosaicing have received much less attention compared to denoising and super-resolution tasks. We propose a new multi-picture architecture for video deinterlacing or demosaicing by aligning multiple supporting pictures with missing data to a reference picture to be reconstructed, benefiting from both local and global spatio-temporal correlations in the feature space using modified deformable convolution blocks and a novel residual efficient top-$k$ self-attention (kSA) block, respectively. Separate reconstruction blocks are used to estimate different types of missing data. Our extensive experimental results, on synthetic or real-world datasets, demonstrate that the proposed novel architecture provides superior results that significantly exceed the state-of-the-art for both tasks in terms of PSNR, SSIM, and perceptual quality. Ablation studies are provided to justify and show the benefit of each novel modification made to the deformable convolution and residual efficient kSA blocks. Code is available: https://github.com/KUIS-AI-Tekalp-Research-Group/Video-Deinterlacing.","sentences":["Despite the fact real-world video deinterlacing and demosaicing are well-suited to supervised learning from synthetically degraded data because the degradation models are known and fixed, learned video deinterlacing and demosaicing have received much less attention compared to denoising and super-resolution tasks.","We propose a new multi-picture architecture for video deinterlacing or demosaicing by aligning multiple supporting pictures with missing data to a reference picture to be reconstructed, benefiting from both local and global spatio-temporal correlations in the feature space using modified deformable convolution blocks and a novel residual efficient top-$k$ self-attention (kSA) block, respectively.","Separate reconstruction blocks are used to estimate different types of missing data.","Our extensive experimental results, on synthetic or real-world datasets, demonstrate that the proposed novel architecture provides superior results that significantly exceed the state-of-the-art for both tasks in terms of PSNR, SSIM, and perceptual quality.","Ablation studies are provided to justify and show the benefit of each novel modification made to the deformable convolution and residual efficient kSA blocks.","Code is available: https://github.com/KUIS-AI-Tekalp-Research-Group/Video-Deinterlacing."],"url":"http://arxiv.org/abs/2404.13018v1","category":"eess.IV"}
{"created":"2024-04-19 17:25:43","title":"Optimizing Calibration by Gaining Aware of Prediction Correctness","abstract":"Model calibration aims to align confidence with prediction correctness. The Cross-Entropy CE) loss is widely used for calibrator training, which enforces the model to increase confidence on the ground truth class. However, we find the CE loss has intrinsic limitations. For example, for a narrow misclassification, a calibrator trained by the CE loss often produces high confidence on the wrongly predicted class (e.g., a test sample is wrongly classified and its softmax score on the ground truth class is around 0.4), which is undesirable. In this paper, we propose a new post-hoc calibration objective derived from the aim of calibration. Intuitively, the proposed objective function asks that the calibrator decrease model confidence on wrongly predicted samples and increase confidence on correctly predicted samples. Because a sample itself has insufficient ability to indicate correctness, we use its transformed versions (e.g., rotated, greyscaled and color-jittered) during calibrator training. Trained on an in-distribution validation set and tested with isolated, individual test samples, our method achieves competitive calibration performance on both in-distribution and out-of-distribution test sets compared with the state of the art. Further, our analysis points out the difference between our method and commonly used objectives such as CE loss and mean square error loss, where the latters sometimes deviates from the calibration aim.","sentences":["Model calibration aims to align confidence with prediction correctness.","The Cross-Entropy CE) loss is widely used for calibrator training, which enforces the model to increase confidence on the ground truth class.","However, we find the CE loss has intrinsic limitations.","For example, for a narrow misclassification, a calibrator trained by the CE loss often produces high confidence on the wrongly predicted class (e.g., a test sample is wrongly classified and its softmax score on the ground truth class is around 0.4), which is undesirable.","In this paper, we propose a new post-hoc calibration objective derived from the aim of calibration.","Intuitively, the proposed objective function asks that the calibrator decrease model confidence on wrongly predicted samples and increase confidence on correctly predicted samples.","Because a sample itself has insufficient ability to indicate correctness, we use its transformed versions (e.g., rotated, greyscaled and color-jittered) during calibrator training.","Trained on an in-distribution validation set and tested with isolated, individual test samples, our method achieves competitive calibration performance on both in-distribution and out-of-distribution test sets compared with the state of the art.","Further, our analysis points out the difference between our method and commonly used objectives such as CE loss and mean square error loss, where the latters sometimes deviates from the calibration aim."],"url":"http://arxiv.org/abs/2404.13016v1","category":"cs.CV"}
{"created":"2024-04-19 16:46:29","title":"Aquaculture field robotics: Applications, lessons learned and future prospects","abstract":"Aquaculture is a big marine industry and contributes to securing global food demands. Underwater vehicles such as remotely operated vehicles (ROVs) are commonly used for inspection, maintenance, and intervention (IMR) tasks in fish farms. However, underwater vehicle operations in aquaculture face several unique and demanding challenges, such as navigation in dynamically changing environments with time-varying sealoads and poor hydroacoustic sensor capabilities, challenges yet to be properly addressed in research. This paper will present various endeavors to address these questions and improve the overall autonomy level in aquaculture robotics, with a focus on field experiments. We will also discuss lessons learned during field trials and potential future prospects in aquaculture robotics.","sentences":["Aquaculture is a big marine industry and contributes to securing global food demands.","Underwater vehicles such as remotely operated vehicles (ROVs) are commonly used for inspection, maintenance, and intervention (IMR) tasks in fish farms.","However, underwater vehicle operations in aquaculture face several unique and demanding challenges, such as navigation in dynamically changing environments with time-varying sealoads and poor hydroacoustic sensor capabilities, challenges yet to be properly addressed in research.","This paper will present various endeavors to address these questions and improve the overall autonomy level in aquaculture robotics, with a focus on field experiments.","We will also discuss lessons learned during field trials and potential future prospects in aquaculture robotics."],"url":"http://arxiv.org/abs/2404.12995v1","category":"cs.RO"}
{"created":"2024-04-19 15:40:39","title":"Towards Reliable Latent Knowledge Estimation in LLMs: In-Context Learning vs. Prompting Based Factual Knowledge Extraction","abstract":"We propose an approach for estimating the latent knowledge embedded inside large language models (LLMs). We leverage the in-context learning (ICL) abilities of LLMs to estimate the extent to which an LLM knows the facts stored in a knowledge base. Our knowledge estimator avoids reliability concerns with previous prompting-based methods, is both conceptually simpler and easier to apply, and we demonstrate that it can surface more of the latent knowledge embedded in LLMs. We also investigate how different design choices affect the performance of ICL-based knowledge estimation. Using the proposed estimator, we perform a large-scale evaluation of the factual knowledge of a variety of open source LLMs, like OPT, Pythia, Llama(2), Mistral, Gemma, etc. over a large set of relations and facts from the Wikidata knowledge base. We observe differences in the factual knowledge between different model families and models of different sizes, that some relations are consistently better known than others but that models differ in the precise facts they know, and differences in the knowledge of base models and their finetuned counterparts.","sentences":["We propose an approach for estimating the latent knowledge embedded inside large language models (LLMs).","We leverage the in-context learning (ICL) abilities of LLMs to estimate the extent to which an LLM knows the facts stored in a knowledge base.","Our knowledge estimator avoids reliability concerns with previous prompting-based methods, is both conceptually simpler and easier to apply, and we demonstrate that it can surface more of the latent knowledge embedded in LLMs.","We also investigate how different design choices affect the performance of ICL-based knowledge estimation.","Using the proposed estimator, we perform a large-scale evaluation of the factual knowledge of a variety of open source LLMs, like OPT, Pythia, Llama(2), Mistral, Gemma, etc. over a large set of relations and facts from the Wikidata knowledge base.","We observe differences in the factual knowledge between different model families and models of different sizes, that some relations are consistently better known than others but that models differ in the precise facts they know, and differences in the knowledge of base models and their finetuned counterparts."],"url":"http://arxiv.org/abs/2404.12957v1","category":"cs.CL"}
{"created":"2024-04-19 13:45:14","title":"Learn2Talk: 3D Talking Face Learns from 2D Talking Face","abstract":"Speech-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years. However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and speech perception. To mind the gap between the two sub-fields, we propose a learning framework named Learn2Talk, which can construct a better 3D talking face network by exploiting two expertise points from the field of 2D talking face. Firstly, inspired by the audio-video sync network, a 3D sync-lip expert model is devised for the pursuit of lip-sync between audio and 3D facial motion. Secondly, a teacher model selected from 2D talking face methods is used to guide the training of the audio-to-3D motions regression network to yield more 3D vertex accuracy. Extensive experiments show the advantages of the proposed framework in terms of lip-sync, vertex accuracy and speech perception, compared with state-of-the-arts. Finally, we show two applications of the proposed framework: audio-visual speech recognition and speech-driven 3D Gaussian Splatting based avatar animation.","sentences":["Speech-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years.","However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and speech perception.","To mind the gap between the two sub-fields, we propose a learning framework named Learn2Talk, which can construct a better 3D talking face network by exploiting two expertise points from the field of 2D talking face.","Firstly, inspired by the audio-video sync network, a 3D sync-lip expert model is devised for the pursuit of lip-sync between audio and 3D facial motion.","Secondly, a teacher model selected from 2D talking face methods is used to guide the training of the audio-to-3D motions regression network to yield more 3D vertex accuracy.","Extensive experiments show the advantages of the proposed framework in terms of lip-sync, vertex accuracy and speech perception, compared with state-of-the-arts.","Finally, we show two applications of the proposed framework: audio-visual speech recognition and speech-driven 3D Gaussian Splatting based avatar animation."],"url":"http://arxiv.org/abs/2404.12888v1","category":"cs.CV"}
{"created":"2024-04-19 12:50:03","title":"Ransomware Detection and Classification Using Random Forest: A Case Study with the UGRansome2024 Dataset","abstract":"Cybersecurity faces challenges in identifying and mitigating ransomware, which is important for protecting critical infrastructures. The absence of datasets for distinguishing normal versus abnormal network behaviour hinders the development of proactive detection strategies against ransomware. An obstacle in proactive prevention methods is the absence of comprehensive datasets for contrasting normal versus abnormal network behaviours. The dataset enabling such contrasts would significantly expedite threat anomaly mitigation. In this study, we introduce UGRansome2024, an optimised dataset for ransomware detection in network traffic. This dataset is derived from the UGRansome data using an intuitionistic feature engineering approach that considers only relevant patterns in network behaviour analysis. The study presents an analysis of ransomware detection using the UGRansome2024 dataset and the Random Forest algorithm. Through encoding and feature relevance determination, the Random Forest achieved a classification accuracy of 96% and effectively identified unusual ransomware transactions. Findings indicate that certain ransomware variants, such as those utilising Encrypt Decrypt Algorithms (EDA) and Globe ransomware, have the highest financial impact. These insights have significant implications for real-world cybersecurity practices, highlighting the importance of machine learning in ransomware detection and mitigation. Further research is recommended to expand datasets, explore alternative detection methods, and address limitations in current approaches.","sentences":["Cybersecurity faces challenges in identifying and mitigating ransomware, which is important for protecting critical infrastructures.","The absence of datasets for distinguishing normal versus abnormal network behaviour hinders the development of proactive detection strategies against ransomware.","An obstacle in proactive prevention methods is the absence of comprehensive datasets for contrasting normal versus abnormal network behaviours.","The dataset enabling such contrasts would significantly expedite threat anomaly mitigation.","In this study, we introduce UGRansome2024, an optimised dataset for ransomware detection in network traffic.","This dataset is derived from the UGRansome data using an intuitionistic feature engineering approach that considers only relevant patterns in network behaviour analysis.","The study presents an analysis of ransomware detection using the UGRansome2024 dataset and the Random Forest algorithm.","Through encoding and feature relevance determination, the Random Forest achieved a classification accuracy of 96% and effectively identified unusual ransomware transactions.","Findings indicate that certain ransomware variants, such as those utilising Encrypt Decrypt Algorithms (EDA) and Globe ransomware, have the highest financial impact.","These insights have significant implications for real-world cybersecurity practices, highlighting the importance of machine learning in ransomware detection and mitigation.","Further research is recommended to expand datasets, explore alternative detection methods, and address limitations in current approaches."],"url":"http://arxiv.org/abs/2404.12855v1","category":"cs.CR"}
{"created":"2024-04-19 12:26:45","title":"KoReA-SFL: Knowledge Replay-based Split Federated Learning Against Catastrophic Forgetting","abstract":"Although Split Federated Learning (SFL) is good at enabling knowledge sharing among resource-constrained clients, it suffers from the problem of low training accuracy due to the neglect of data heterogeneity and catastrophic forgetting. To address this issue, we propose a novel SFL approach named KoReA-SFL, which adopts a multi-model aggregation mechanism to alleviate gradient divergence caused by heterogeneous data and a knowledge replay strategy to deal with catastrophic forgetting. Specifically, in KoReA-SFL cloud servers (i.e., fed server and main server) maintain multiple branch model portions rather than a global portion for local training and an aggregated master-model portion for knowledge sharing among branch portions. To avoid catastrophic forgetting, the main server of KoReA-SFL selects multiple assistant devices for knowledge replay according to the training data distribution of each server-side branch-model portion. Experimental results obtained from non-IID and IID scenarios demonstrate that KoReA-SFL significantly outperforms conventional SFL methods (by up to 23.25\\% test accuracy improvement).","sentences":["Although Split Federated Learning (SFL) is good at enabling knowledge sharing among resource-constrained clients, it suffers from the problem of low training accuracy due to the neglect of data heterogeneity and catastrophic forgetting.","To address this issue, we propose a novel SFL approach named KoReA-SFL, which adopts a multi-model aggregation mechanism to alleviate gradient divergence caused by heterogeneous data and a knowledge replay strategy to deal with catastrophic forgetting.","Specifically, in KoReA-SFL cloud servers (i.e., fed server and main server) maintain multiple branch model portions rather than a global portion for local training and an aggregated master-model portion for knowledge sharing among branch portions.","To avoid catastrophic forgetting, the main server of KoReA-SFL selects multiple assistant devices for knowledge replay according to the training data distribution of each server-side branch-model portion.","Experimental results obtained from non-IID and IID scenarios demonstrate that KoReA-SFL significantly outperforms conventional SFL methods (by up to 23.25\\% test accuracy improvement)."],"url":"http://arxiv.org/abs/2404.12846v1","category":"cs.LG"}
{"created":"2024-04-19 12:14:09","title":"How Far Can We Go with Practical Function-Level Program Repair?","abstract":"Recently, multiple Automated Program Repair (APR) techniques based on Large Language Models (LLMs) have been proposed to enhance the repair performance. While these techniques mainly focus on the single-line or hunk-level repair, they face significant challenges in real-world application due to the limited repair task scope and costly statement-level fault localization. However, the more practical function-level APR, which broadens the scope of APR task to fix entire buggy functions and requires only cost-efficient function-level fault localization, remains underexplored. In this paper, we conduct the first comprehensive study of LLM-based function-level APR including investigating the effect of the few-shot learning mechanism and the auxiliary repair-relevant information. Specifically, we adopt six widely-studied LLMs and construct a benchmark in both the Defects4J 1.2 and 2.0 datasets. Our study demonstrates that LLMs with zero-shot learning are already powerful function-level APR techniques, while applying the few-shot learning mechanism leads to disparate repair performance. Moreover, we find that directly applying the auxiliary repair-relevant information to LLMs significantly increases function-level repair performance. Inspired by our findings, we propose an LLM-based function-level APR technique, namely SRepair, which adopts a dual-LLM framework to leverage the power of the auxiliary repair-relevant information for advancing the repair performance. The evaluation results demonstrate that SRepair can correctly fix 300 single-function bugs in the Defects4J dataset, largely surpassing all previous APR techniques by at least 85%, without the need for the costly statement-level fault location information. Furthermore, SRepair successfully fixes 32 multi-function bugs in the Defects4J dataset, which is the first time achieved by any APR technique ever to our best knowledge.","sentences":["Recently, multiple Automated Program Repair (APR) techniques based on Large Language Models (LLMs) have been proposed to enhance the repair performance.","While these techniques mainly focus on the single-line or hunk-level repair, they face significant challenges in real-world application due to the limited repair task scope and costly statement-level fault localization.","However, the more practical function-level APR, which broadens the scope of APR task to fix entire buggy functions and requires only cost-efficient function-level fault localization, remains underexplored.","In this paper, we conduct the first comprehensive study of LLM-based function-level APR including investigating the effect of the few-shot learning mechanism and the auxiliary repair-relevant information.","Specifically, we adopt six widely-studied LLMs and construct a benchmark in both the Defects4J 1.2 and 2.0 datasets.","Our study demonstrates that LLMs with zero-shot learning are already powerful function-level APR techniques, while applying the few-shot learning mechanism leads to disparate repair performance.","Moreover, we find that directly applying the auxiliary repair-relevant information to LLMs significantly increases function-level repair performance.","Inspired by our findings, we propose an LLM-based function-level APR technique, namely SRepair, which adopts a dual-LLM framework to leverage the power of the auxiliary repair-relevant information for advancing the repair performance.","The evaluation results demonstrate that SRepair can correctly fix 300 single-function bugs in the Defects4J dataset, largely surpassing all previous APR techniques by at least 85%, without the need for the costly statement-level fault location information.","Furthermore, SRepair successfully fixes 32 multi-function bugs in the Defects4J dataset, which is the first time achieved by any APR technique ever to our best knowledge."],"url":"http://arxiv.org/abs/2404.12833v1","category":"cs.SE"}
{"created":"2024-04-19 09:31:11","title":"DeviceRadar: Online IoT Device Fingerprinting in ISPs using Programmable Switches","abstract":"Device fingerprinting can be used by Internet Service Providers (ISPs) to identify vulnerable IoT devices for early prevention of threats. However, due to the wide deployment of middleboxes in ISP networks, some important data, e.g., 5-tuples and flow statistics, are often obscured, rendering many existing approaches invalid. It is further challenged by the high-speed traffic of hundreds of terabytes per day in ISP networks. This paper proposes DeviceRadar, an online IoT device fingerprinting framework that achieves accurate, real-time processing in ISPs using programmable switches. We innovatively exploit \"key packets\" as a basis of fingerprints only using packet sizes and directions, which appear periodically while exhibiting differences across different IoT devices. To utilize them, we propose a packet size embedding model to discover the spatial relationships between packets. Meanwhile, we design an algorithm to extract the \"key packets\" of each device, and propose an approach that jointly considers the spatial relationships and the key packets to produce a neighboring key packet distribution, which can serve as a feature vector for machine learning models for inference. Last, we design a model transformation method and a feature extraction process to deploy the model on a programmable data plane within its constrained arithmetic operations and memory to achieve line-speed processing. Our experiments show that DeviceRadar can achieve state-of-the-art accuracy across 77 IoT devices with 40 Gbps throughput, and requires only 1.3% of the processing time compared to GPU-accelerated approaches.","sentences":["Device fingerprinting can be used by Internet Service Providers (ISPs) to identify vulnerable IoT devices for early prevention of threats.","However, due to the wide deployment of middleboxes in ISP networks, some important data, e.g., 5-tuples and flow statistics, are often obscured, rendering many existing approaches invalid.","It is further challenged by the high-speed traffic of hundreds of terabytes per day in ISP networks.","This paper proposes DeviceRadar, an online IoT device fingerprinting framework that achieves accurate, real-time processing in ISPs using programmable switches.","We innovatively exploit \"key packets\" as a basis of fingerprints only using packet sizes and directions, which appear periodically while exhibiting differences across different IoT devices.","To utilize them, we propose a packet size embedding model to discover the spatial relationships between packets.","Meanwhile, we design an algorithm to extract the \"key packets\" of each device, and propose an approach that jointly considers the spatial relationships and the key packets to produce a neighboring key packet distribution, which can serve as a feature vector for machine learning models for inference.","Last, we design a model transformation method and a feature extraction process to deploy the model on a programmable data plane within its constrained arithmetic operations and memory to achieve line-speed processing.","Our experiments show that DeviceRadar can achieve state-of-the-art accuracy across 77 IoT devices with 40 Gbps throughput, and requires only 1.3% of the processing time compared to GPU-accelerated approaches."],"url":"http://arxiv.org/abs/2404.12738v1","category":"cs.NI"}
{"created":"2024-04-19 08:36:52","title":"FedMeS: Personalized Federated Continual Learning Leveraging Local Memory","abstract":"We focus on the problem of Personalized Federated Continual Learning (PFCL): a group of distributed clients, each with a sequence of local tasks on arbitrary data distributions, collaborate through a central server to train a personalized model at each client, with the model expected to achieve good performance on all local tasks. We propose a novel PFCL framework called Federated Memory Strengthening FedMeS to address the challenges of client drift and catastrophic forgetting. In FedMeS, each client stores samples from previous tasks using a small amount of local memory, and leverages this information to both 1) calibrate gradient updates in training process; and 2) perform KNN-based Gaussian inference to facilitate personalization. FedMeS is designed to be task-oblivious, such that the same inference process is applied to samples from all tasks to achieve good performance. FedMeS is analyzed theoretically and evaluated experimentally. It is shown to outperform all baselines in average accuracy and forgetting rate, over various combinations of datasets, task distributions, and client numbers.","sentences":["We focus on the problem of Personalized Federated Continual Learning (PFCL): a group of distributed clients, each with a sequence of local tasks on arbitrary data distributions, collaborate through a central server to train a personalized model at each client, with the model expected to achieve good performance on all local tasks.","We propose a novel PFCL framework called Federated Memory Strengthening FedMeS to address the challenges of client drift and catastrophic forgetting.","In FedMeS, each client stores samples from previous tasks using a small amount of local memory, and leverages this information to both 1) calibrate gradient updates in training process; and 2) perform KNN-based Gaussian inference to facilitate personalization.","FedMeS is designed to be task-oblivious, such that the same inference process is applied to samples from all tasks to achieve good performance.","FedMeS is analyzed theoretically and evaluated experimentally.","It is shown to outperform all baselines in average accuracy and forgetting rate, over various combinations of datasets, task distributions, and client numbers."],"url":"http://arxiv.org/abs/2404.12710v1","category":"cs.LG"}
{"created":"2024-04-19 15:33:59","title":"Low-Depth Spatial Tree Algorithms","abstract":"Contemporary accelerator designs exhibit a high degree of spatial localization, wherein two-dimensional physical distance determines communication costs between processing elements. This situation presents considerable algorithmic challenges, particularly when managing sparse data, a pivotal component in progressing data science. The spatial computer model quantifies communication locality by weighting processor communication costs by distance, introducing a term named energy. Moreover, it integrates depth, a widely-utilized metric, to promote high parallelism. We propose and analyze a framework for efficient spatial tree algorithms within the spatial computer model. Our primary method constructs a spatial tree layout that optimizes the locality of the neighbors in the compute grid. This approach thereby enables locality-optimized messaging within the tree. Our layout achieves a polynomial factor improvement in energy compared to utilizing a PRAM approach. Using this layout, we develop energy-efficient treefix sum and lowest common ancestor algorithms, which are both fundamental building blocks for other graph algorithms. With high probability, our algorithms exhibit near-linear energy and poly-logarithmic depth. Our contributions augment a growing body of work demonstrating that computations can have both high spatial locality and low depth. Moreover, our work constitutes an advancement in the spatial layout of irregular and sparse computations.","sentences":["Contemporary accelerator designs exhibit a high degree of spatial localization, wherein two-dimensional physical distance determines communication costs between processing elements.","This situation presents considerable algorithmic challenges, particularly when managing sparse data, a pivotal component in progressing data science.","The spatial computer model quantifies communication locality by weighting processor communication costs by distance, introducing a term named energy.","Moreover, it integrates depth, a widely-utilized metric, to promote high parallelism.","We propose and analyze a framework for efficient spatial tree algorithms within the spatial computer model.","Our primary method constructs a spatial tree layout that optimizes the locality of the neighbors in the compute grid.","This approach thereby enables locality-optimized messaging within the tree.","Our layout achieves a polynomial factor improvement in energy compared to utilizing a PRAM approach.","Using this layout, we develop energy-efficient treefix sum and lowest common ancestor algorithms, which are both fundamental building blocks for other graph algorithms.","With high probability, our algorithms exhibit near-linear energy and poly-logarithmic depth.","Our contributions augment a growing body of work demonstrating that computations can have both high spatial locality and low depth.","Moreover, our work constitutes an advancement in the spatial layout of irregular and sparse computations."],"url":"http://arxiv.org/abs/2404.12953v1","category":"cs.DC"}
{"created":"2024-04-19 15:27:54","title":"Optimal single threshold stopping rules and sharp prophet inequalities","abstract":"This paper considers a finite horizon optimal stopping problem for a sequence of independent and identically distributed random variables. The objective is to design stopping rules that attempt to select the random variable with the highest value in the sequence. The performance of any stopping rule may be benchmarked relative to the selection of a \"prophet\" that has perfect foreknowledge of the largest value. Such comparisons are typically stated in the form of \"prophet inequalities.\" In this paper we characterize sharp prophet inequalities for single threshold stopping rules as solutions to infinite two person zero sum games on the unit square with special payoff kernels. The proposed game theoretic characterization allows one to derive sharp non-asymptotic prophet inequalities for different classes of distributions. This, in turn, gives rise to a simple and computationally tractable algorithmic paradigm for deriving optimal single threshold stopping rules. Our results also indicate that several classical observations in the literature are either incorrect or incomplete in treating this problem.","sentences":["This paper considers a finite horizon optimal stopping problem for a sequence of independent and identically distributed random variables.","The objective is to design stopping rules that attempt to select the random variable with the highest value in the sequence.","The performance of any stopping rule may be benchmarked relative to the selection of a \"prophet\" that has perfect foreknowledge of the largest value.","Such comparisons are typically stated in the form of \"prophet inequalities.\"","In this paper we characterize sharp prophet inequalities for single threshold stopping rules as solutions to infinite two person zero sum games on the unit square with special payoff kernels.","The proposed game theoretic characterization allows one to derive sharp non-asymptotic prophet inequalities for different classes of distributions.","This, in turn, gives rise to a simple and computationally tractable algorithmic paradigm for deriving optimal single threshold stopping rules.","Our results also indicate that several classical observations in the literature are either incorrect or incomplete in treating this problem."],"url":"http://arxiv.org/abs/2404.12949v1","category":"math.PR"}
{"created":"2024-04-19 13:05:08","title":"A minimal model of boosting and waning iin a recurrent seasonal epidemic","abstract":"We propose a model of the immunity to a cyclical epidemic disease taking account not only of seasonal boosts during the infectious season, but also of residual immunity remaining from one season to the next. The focus is on the exponential waning process over successive cycles, imposed on the temporal distribution of infections or exposures over a season. This distribution, interacting with the waning function, is all that is necessary to reproduce, in mathematically closed form, the mechanical cycle of boosting and waning immunity characteristic of recurrent seasonal infectious disease. Distinct from epidemiological models predicting numbers of individuals moving between infectivity compartments, our result enables us to directly estimate parameters of waning and the infectivity distribution. We can naturally iterate the cyclical process to simulate immunity trajectories over many years and thus to quantify the strong relationship between residual immunity and the time elapsed between annual infectivity peaks.","sentences":["We propose a model of the immunity to a cyclical epidemic disease taking account not only of seasonal boosts during the infectious season, but also of residual immunity remaining from one season to the next.","The focus is on the exponential waning process over successive cycles, imposed on the temporal distribution of infections or exposures over a season.","This distribution, interacting with the waning function, is all that is necessary to reproduce, in mathematically closed form, the mechanical cycle of boosting and waning immunity characteristic of recurrent seasonal infectious disease.","Distinct from epidemiological models predicting numbers of individuals moving between infectivity compartments, our result enables us to directly estimate parameters of waning and the infectivity distribution.","We can naturally iterate the cyclical process to simulate immunity trajectories over many years and thus to quantify the strong relationship between residual immunity and the time elapsed between annual infectivity peaks."],"url":"http://arxiv.org/abs/2404.12865v1","category":"q-bio.PE"}
{"created":"2024-04-19 12:21:55","title":"Accurate and Fast Geometry Optimization with Time Estimation and Method Switching","abstract":"Geometry optimization is an important task in quantum chemical calculations to analyze the characteristics of molecules. A top concern on it is a long execution time because time-consuming energy and gradient calculations are repeated across several to tens of steps. In this work, we present a scheme to estimate the execution times of geometry optimization of a target molecule at different accuracy levels (i.e., the combinations of ab initio methods and basis sets). It enables to identify the accuracy levels where geometry optimization will finish in an acceptable time. In addition, we propose a gradient-based method switching (GMS) technique that reduces the execution time by dynamically switching multiple methods during geometry optimization. Our evaluation using 46 molecules in total shows that the geometry optimization times at 20 accuracy levels are estimated with a mean error of 29.5%, and GMS reduces the execution time by up to 42.7% without affecting the accuracy of geometry optimization.","sentences":["Geometry optimization is an important task in quantum chemical calculations to analyze the characteristics of molecules.","A top concern on it is a long execution time because time-consuming energy and gradient calculations are repeated across several to tens of steps.","In this work, we present a scheme to estimate the execution times of geometry optimization of a target molecule at different accuracy levels (i.e., the combinations of ab initio methods and basis sets).","It enables to identify the accuracy levels where geometry optimization will finish in an acceptable time.","In addition, we propose a gradient-based method switching (GMS) technique that reduces the execution time by dynamically switching multiple methods during geometry optimization.","Our evaluation using 46 molecules in total shows that the geometry optimization times at 20 accuracy levels are estimated with a mean error of 29.5%, and GMS reduces the execution time by up to 42.7% without affecting the accuracy of geometry optimization."],"url":"http://arxiv.org/abs/2404.12842v1","category":"physics.chem-ph"}
{"created":"2024-04-19 12:20:26","title":"Getting to the Root of the Problem: Sums of Squares for Infinite Trees","abstract":"The inducibility of a graph represents its maximum density as an induced subgraph over all possible sequences of graphs of size growing to infinity. This invariant of graphs has been extensively studied since its introduction in $1975$ by Pippenger and Golumbic. In $2017$, Czabarka, Sz\\'ekely and Wagner extended this notion to leaf-labeled rooted binary trees, which are objects widely studied in the field of phylogenetics. They obtain the first results and bounds for the densities and inducibilities of such trees. Following up on their work, we apply Razborov's flag algebra theory to this setting, introducing the flag algebra of rooted leaf-labeled binary trees. This framework allows us to use polynomial optimization methods, based on semidefinite programming, to efficiently obtain new upper bounds for the inducibility of trees and to improve existing ones. Additionally, we obtain the first outer approximations of profiles of trees, which represent all possible simultaneous densities of a pair of trees. Finally, we are able to prove the non-convexity of some of these profiles.","sentences":["The inducibility of a graph represents its maximum density as an induced subgraph over all possible sequences of graphs of size growing to infinity.","This invariant of graphs has been extensively studied since its introduction in $1975$ by Pippenger and Golumbic.","In $2017$, Czabarka, Sz\\'ekely and Wagner extended this notion to leaf-labeled rooted binary trees, which are objects widely studied in the field of phylogenetics.","They obtain the first results and bounds for the densities and inducibilities of such trees.","Following up on their work, we apply Razborov's flag algebra theory to this setting, introducing the flag algebra of rooted leaf-labeled binary trees.","This framework allows us to use polynomial optimization methods, based on semidefinite programming, to efficiently obtain new upper bounds for the inducibility of trees and to improve existing ones.","Additionally, we obtain the first outer approximations of profiles of trees, which represent all possible simultaneous densities of a pair of trees.","Finally, we are able to prove the non-convexity of some of these profiles."],"url":"http://arxiv.org/abs/2404.12838v1","category":"math.OC"}
{"created":"2024-04-19 10:32:30","title":"EfficientGS: Streamlining Gaussian Splatting for Large-Scale High-Resolution Scene Representation","abstract":"In the domain of 3D scene representation, 3D Gaussian Splatting (3DGS) has emerged as a pivotal technology. However, its application to large-scale, high-resolution scenes (exceeding 4k$\\times$4k pixels) is hindered by the excessive computational requirements for managing a large number of Gaussians. Addressing this, we introduce 'EfficientGS', an advanced approach that optimizes 3DGS for high-resolution, large-scale scenes. We analyze the densification process in 3DGS and identify areas of Gaussian over-proliferation. We propose a selective strategy, limiting Gaussian increase to key primitives, thereby enhancing the representational efficiency. Additionally, we develop a pruning mechanism to remove redundant Gaussians, those that are merely auxiliary to adjacent ones. For further enhancement, we integrate a sparse order increment for Spherical Harmonics (SH), designed to alleviate storage constraints and reduce training overhead. Our empirical evaluations, conducted on a range of datasets including extensive 4K+ aerial images, demonstrate that 'EfficientGS' not only expedites training and rendering times but also achieves this with a model size approximately tenfold smaller than conventional 3DGS while maintaining high rendering fidelity.","sentences":["In the domain of 3D scene representation, 3D Gaussian Splatting (3DGS) has emerged as a pivotal technology.","However, its application to large-scale, high-resolution scenes (exceeding 4k$\\times$4k pixels) is hindered by the excessive computational requirements for managing a large number of Gaussians.","Addressing this, we introduce 'EfficientGS', an advanced approach that optimizes 3DGS for high-resolution, large-scale scenes.","We analyze the densification process in 3DGS and identify areas of Gaussian over-proliferation.","We propose a selective strategy, limiting Gaussian increase to key primitives, thereby enhancing the representational efficiency.","Additionally, we develop a pruning mechanism to remove redundant Gaussians, those that are merely auxiliary to adjacent ones.","For further enhancement, we integrate a sparse order increment for Spherical Harmonics (SH), designed to alleviate storage constraints and reduce training overhead.","Our empirical evaluations, conducted on a range of datasets including extensive 4K+ aerial images, demonstrate that 'EfficientGS' not only expedites training and rendering times but also achieves this with a model size approximately tenfold smaller than conventional 3DGS while maintaining high rendering fidelity."],"url":"http://arxiv.org/abs/2404.12777v1","category":"cs.CV"}
{"created":"2024-04-19 08:54:24","title":"Plasmonic nanoprism distributions to promote enhanced and uniform energy deposition in passive and active targets","abstract":"Passive and active targets, implanted with gold nanoprisms, were designed to achieve enhanced and uniform power absorption during two-sided illumination by short laser pulses. The target length was adjusted to match the short laser pulse-length. Capabilities of three different, uniform, single-peaked Gaussian and adjusted, nanoresonator number density distributions were compared. The average local E-field inside the gain medium and on the surface of the nanoprisms were mapped as a function of the pump E-field strength and dye concentration, assuming a uniform nanoresonator distribution. The optimal parameters were adopted to each inspected nanoprism distributions. The time-evolution of the near-field enhancement (NFE), integrated power-loss and deposited energy were determined, additionally, the time-evolution of the standard deviation of these quantities was monitored. A comparative study was performed on passive and active targets, to determine the most advantageous nanoprism number density distribution type and to consider the advantages of dye doping. Based on the results, the adjusted distribution is proposed both in passive and active targets. Doping with the dye is advantageous in every inspected distribution in decreasing the minimal standard deviation of the NFE. It is advantageous in decreasing the delay of the minimal standard deviation in the power-loss and deposited energy, the standard deviation of the NFE as well as in increasing the FOM of the NFE in the uniform and adjusted distributions. In addition, doping allows for decreasing the delay of the minimal standard deviation in the NFE / increasing the mean NFE / decreasing the standard deviation of the power-loss and deposited energy in the uniform / Gaussian / adjusted distribution.","sentences":["Passive and active targets, implanted with gold nanoprisms, were designed to achieve enhanced and uniform power absorption during two-sided illumination by short laser pulses.","The target length was adjusted to match the short laser pulse-length.","Capabilities of three different, uniform, single-peaked Gaussian and adjusted, nanoresonator number density distributions were compared.","The average local E-field inside the gain medium and on the surface of the nanoprisms were mapped as a function of the pump E-field strength and dye concentration, assuming a uniform nanoresonator distribution.","The optimal parameters were adopted to each inspected nanoprism distributions.","The time-evolution of the near-field enhancement (NFE), integrated power-loss and deposited energy were determined, additionally, the time-evolution of the standard deviation of these quantities was monitored.","A comparative study was performed on passive and active targets, to determine the most advantageous nanoprism number density distribution type and to consider the advantages of dye doping.","Based on the results, the adjusted distribution is proposed both in passive and active targets.","Doping with the dye is advantageous in every inspected distribution in decreasing the minimal standard deviation of the NFE.","It is advantageous in decreasing the delay of the minimal standard deviation in the power-loss and deposited energy, the standard deviation of the NFE as well as in increasing the FOM of the NFE in the uniform and adjusted distributions.","In addition, doping allows for decreasing the delay of the minimal standard deviation in the NFE / increasing the mean NFE / decreasing the standard deviation of the power-loss and deposited energy in the uniform / Gaussian / adjusted distribution."],"url":"http://arxiv.org/abs/2404.12716v1","category":"physics.optics"}
{"created":"2024-04-19 08:50:03","title":"Lasing and spasing with active individual core-shell plasmonic nanoresonators","abstract":"Active core-shell nanoresonators were designed in order to achieve large near-field enhancement, large power-outflow and minimal spaser threshold in the pump E-field strength. Gain-metal-dielectric (GMD) and gain-metal-gain (GMG) nanoresonator compositions were optimized with corresponding objective functions. The average local E-field, power-outflow and extinction cross-section were mapped above the pump E-field strength and dye concentration parameter plane with the criterion that the local E-field is smaller than the damage threshold of the nanoresonator. Regions, corresponding to the maxima in the average local E-field, the highest power-outflow, or to the zero-crossing of the extinction cross-section, were selected for detailed studies. The spectral distribution of the near-field enhancement, optical cross-sections, optical responses, quantum efficiencies, as well as the polar angle distribution of the far-field radiated power and the local charge distribution of dominant modes were inspected. Based on the results the GMD nanoresonator composition is proposed to maximize local E-field in near-field amplifiers, to maximize power-outflow in far-field-emitting lasers and to minimize threshold E-field in spasers. Comparing the complete characteristics, both compositions are suitable for different operation regions, the GMG is proposed as near-field amplifier and far-field out-coupling nanolaser, whereas the GMD is unambiguously preferable to achieve optimal spaser properties.","sentences":["Active core-shell nanoresonators were designed in order to achieve large near-field enhancement, large power-outflow and minimal spaser threshold in the pump E-field strength.","Gain-metal-dielectric (GMD) and gain-metal-gain (GMG) nanoresonator compositions were optimized with corresponding objective functions.","The average local E-field, power-outflow and extinction cross-section were mapped above the pump E-field strength and dye concentration parameter plane with the criterion that the local E-field is smaller than the damage threshold of the nanoresonator.","Regions, corresponding to the maxima in the average local E-field, the highest power-outflow, or to the zero-crossing of the extinction cross-section, were selected for detailed studies.","The spectral distribution of the near-field enhancement, optical cross-sections, optical responses, quantum efficiencies, as well as the polar angle distribution of the far-field radiated power and the local charge distribution of dominant modes were inspected.","Based on the results the GMD nanoresonator composition is proposed to maximize local E-field in near-field amplifiers, to maximize power-outflow in far-field-emitting lasers and to minimize threshold E-field in spasers.","Comparing the complete characteristics, both compositions are suitable for different operation regions, the GMG is proposed as near-field amplifier and far-field out-coupling nanolaser, whereas the GMD is unambiguously preferable to achieve optimal spaser properties."],"url":"http://arxiv.org/abs/2404.12714v1","category":"physics.optics"}
{"created":"2024-04-19 17:54:34","title":"Ab initio tight-binding Models for Mono- and Bilayer Hexagonal Boron Nitride (h-BN)","abstract":"We provide effective tight-binding models of monolayer and bilayer hexagonal boron nitride (h-BN) informed by ab initio density functional theory calculations within the local density approximation using maximally localized Wannier functions centered at the boron and nitrogen sites. An effective intra-layer tight-binding model with up to four distant hopping neighbors captures the band energies near the K-point and M-point in the Brillouin zone, including the indirect nature of the band gap for certain stackings. We then propose a two-center interlayer tight-binding model that can be used for any stacking in bilayer h-BN based on the relative distance between two atomic sites which can be used to model twisted h-BN structures.","sentences":["We provide effective tight-binding models of monolayer and bilayer hexagonal boron nitride (h-BN) informed by ab initio density functional theory calculations within the local density approximation using maximally localized Wannier functions centered at the boron and nitrogen sites.","An effective intra-layer tight-binding model with up to four distant hopping neighbors captures the band energies near the K-point and M-point in the Brillouin zone, including the indirect nature of the band gap for certain stackings.","We then propose a two-center interlayer tight-binding model that can be used for any stacking in bilayer h-BN based on the relative distance between two atomic sites which can be used to model twisted h-BN structures."],"url":"http://arxiv.org/abs/2404.13041v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-19 16:49:48","title":"Axion-induced Casimir force between nuclei and dynamical axion pair creation","abstract":"We study the interaction between axions and nuclei by combining the Peccei-Quinn mechanism with results from quantum chromo-dynamics (QCD) which imply that the QCD condensates are reduced within nuclear matter. Thus, the effective axion mass is also reduced, yielding a finite axion-nucleon scattering cross section. Even in the absence of real axions, this interaction would manifest itself in a Casimir type attraction between two nuclei. Finally, accelerated nuclei can create entangled pairs of axions via the dynamical Casimir effect (or as signatures of the Unruh effect).","sentences":["We study the interaction between axions and nuclei by combining the Peccei-Quinn mechanism with results from quantum chromo-dynamics (QCD) which imply that the QCD condensates are reduced within nuclear matter.","Thus, the effective axion mass is also reduced, yielding a finite axion-nucleon scattering cross section.","Even in the absence of real axions, this interaction would manifest itself in a Casimir type attraction between two nuclei.","Finally, accelerated nuclei can create entangled pairs of axions via the dynamical Casimir effect (or as signatures of the Unruh effect)."],"url":"http://arxiv.org/abs/2404.12996v1","category":"hep-ph"}
{"created":"2024-04-19 16:44:31","title":"Buoyancy glitches in pulsating stars revisited","abstract":"Sharp structural variations induce specific signatures on stellar pulsations that can be studied to infer localised information on the stratification of the star. This information is key to improve our understanding of the physical processes that lead to the structural variations and how to model them. Here we revisit and extend the analysis of the signature of different types of buoyancy glitches in gravity-mode and mixed-mode pulsators presented in earlier works, including glitches with step-like, Gaussian-like, and Dirac-$\\delta$-like shapes. In particular, we provide analytical expressions for the perturbations to the periods and show that these can be reliably used in place of the expressions provided for the period spacings, with the advantage that the use of the new expressions does not require modes with consecutive radial orders to be observed. Based on a comparison with two limit cases and on simulated data, we further tested the accuracy of the expression for the Gaussian-like glitch signature whose derivation in an earlier work involved a significant approximation. We find that the least reliable glitch parameter inferred from fitting that expression is the amplitude, which can be up to a factor of two larger than the true amplitude, reaching this limit when the glitch is small. We further discuss the impact on the glitch signature of considering a glitch in the inner and outer half of the g-mode cavity, emphasising the break of symmetry that takes place in the case of mixed-mode pulsators.","sentences":["Sharp structural variations induce specific signatures on stellar pulsations that can be studied to infer localised information on the stratification of the star.","This information is key to improve our understanding of the physical processes that lead to the structural variations and how to model them.","Here we revisit and extend the analysis of the signature of different types of buoyancy glitches in gravity-mode and mixed-mode pulsators presented in earlier works, including glitches with step-like, Gaussian-like, and Dirac-$\\delta$-like shapes.","In particular, we provide analytical expressions for the perturbations to the periods and show that these can be reliably used in place of the expressions provided for the period spacings, with the advantage that the use of the new expressions does not require modes with consecutive radial orders to be observed.","Based on a comparison with two limit cases and on simulated data, we further tested the accuracy of the expression for the Gaussian-like glitch signature whose derivation in an earlier work involved a significant approximation.","We find that the least reliable glitch parameter inferred from fitting that expression is the amplitude, which can be up to a factor of two larger than the true amplitude, reaching this limit when the glitch is small.","We further discuss the impact on the glitch signature of considering a glitch in the inner and outer half of the g-mode cavity, emphasising the break of symmetry that takes place in the case of mixed-mode pulsators."],"url":"http://arxiv.org/abs/2404.12992v1","category":"astro-ph.SR"}
{"created":"2024-04-19 16:36:26","title":"When will fusion energy truly become a reality?","abstract":"This abstract provides up-to-date insights into fusion (thermonuclear) research, detailing ongoing projects and planned devices. The document also explores alternative sources of energy, offering a comprehensive overview of the current landscape. Additionally, notable comments and observations are provided to illuminate key aspects of the discussed topics. Stay informed as we delve into the latest advancements and initiatives in the dynamic field of energy research.","sentences":["This abstract provides up-to-date insights into fusion (thermonuclear) research, detailing ongoing projects and planned devices.","The document also explores alternative sources of energy, offering a comprehensive overview of the current landscape.","Additionally, notable comments and observations are provided to illuminate key aspects of the discussed topics.","Stay informed as we delve into the latest advancements and initiatives in the dynamic field of energy research."],"url":"http://arxiv.org/abs/2404.12987v1","category":"physics.soc-ph"}
{"created":"2024-04-19 15:45:33","title":"Asymmetric $\\mathbb{Z}_4$ orbifolds of type IIB string theory revisited","abstract":"We construct freely acting asymmetric $\\mathbb{Z}_4$ orbifolds of type IIB string theory on $T^5$ preserving 24,16 or 8 supercharges in five dimensions. We show that these models are well-defined if the SO(8) lattice is chosen, while the SU(2)$^4$ lattice, which was previously considered in the literature, does not lead to a modular invariant partition function. Also, we clarify some issues related to the coefficients in the $q\\bar{q}$-expansion of the partition function in the twisted sectors of asymmetric orbifolds.","sentences":["We construct freely acting asymmetric $\\mathbb{Z}_4$ orbifolds of type IIB string theory on $T^5$ preserving 24,16 or 8 supercharges in five dimensions.","We show that these models are well-defined if the SO(8) lattice is chosen, while the SU(2)$^4$ lattice, which was previously considered in the literature, does not lead to a modular invariant partition function.","Also, we clarify some issues related to the coefficients in the $q\\bar{q}$-expansion of the partition function in the twisted sectors of asymmetric orbifolds."],"url":"http://arxiv.org/abs/2404.12962v1","category":"hep-th"}
{"created":"2024-04-19 15:24:14","title":"Hadronic Spectrum in the Chiral Large $N_c$ Extension of Quantum Chromodynamics","abstract":"We study Quantum Chromodynamics in the chiral large $N_c$ limit which contains a left-handed Weyl fermion in the fundamental representation, a left-handed Weyl fermion in the two index antisymmetric representation and $(N_c-3)$ left-handed Weyl fermions in the antifundamental representation of the $SU(N_c)$ gauge group. We construct gauge singlet composite operators and study their masses and correlation functions at large $N_c$.   It is shown that all hadron masses scale as $\\sim N_c^0 n_q$ where $n_q$ is the number of constituent quarks in the hadron. In addition by simple gluon exchange considerations it is seen that scattering amplitudes between hadrons have the same $N_c$ scaling as the mass of the lightest hadron involved. This is the case provided the hadrons in the scattering amplitude share a sufficiently large number of constituent quarks.   The chiral large $N_c$ extension also allows for other non-trivial processes. For instance we consider two different baryonium states that are unique to this extension and that decay via emissions of two- and three-quark hadrons. Also other non-trivial scattering processes are considered. Finally, we study composites made of a mix of left- and right-handed fields. We categorize multiple groups of hadrons within the full spectrum according to their flavor structure. Within these groups all $n$-point functions scale the same.","sentences":["We study Quantum Chromodynamics in the chiral large $N_c$ limit which contains a left-handed Weyl fermion in the fundamental representation, a left-handed Weyl fermion in the two index antisymmetric representation and $(N_c-3)$ left-handed Weyl fermions in the antifundamental representation of the $SU(N_c)$ gauge group.","We construct gauge singlet composite operators and study their masses and correlation functions at large $N_c$.   It is shown that all hadron masses scale as $\\sim N_c^0 n_q$ where $n_q$ is the number of constituent quarks in the hadron.","In addition by simple gluon exchange considerations it is seen that scattering amplitudes between hadrons have the same $N_c$ scaling as the mass of the lightest hadron involved.","This is the case provided the hadrons in the scattering amplitude share a sufficiently large number of constituent quarks.   ","The chiral large $N_c$ extension also allows for other non-trivial processes.","For instance we consider two different baryonium states that are unique to this extension and that decay via emissions of two- and three-quark hadrons.","Also other non-trivial scattering processes are considered.","Finally, we study composites made of a mix of left- and right-handed fields.","We categorize multiple groups of hadrons within the full spectrum according to their flavor structure.","Within these groups all $n$-point functions scale the same."],"url":"http://arxiv.org/abs/2404.12947v1","category":"hep-ph"}
{"created":"2024-04-19 14:52:14","title":"Partial orders are the free conservative cocompletion of total orders","abstract":"We show that the category of partially ordered sets $\\mathsf{Pos}$ is equivalent to the free conservative cocompletion of the category of finite non-empty totally ordered sets $\\Delta$, which is also known as the simplex category.","sentences":["We show that the category of partially ordered sets $\\mathsf{Pos}$ is equivalent to the free conservative cocompletion of the category of finite non-empty totally ordered sets $\\Delta$, which is also known as the simplex category."],"url":"http://arxiv.org/abs/2404.12924v1","category":"math.CT"}
{"created":"2024-04-19 14:37:58","title":"Reduced cross-section in Electron-Ion Colliders at small $x$","abstract":"The nuclear reduced cross section $\\sigma^{A}_{r}$, in the kinematic range of the electron-Ion collider with center-of-mass energy $\\sqrt{s}=140~\\mathrm{GeV}$ and $y{\\leq}1$, is discussed. The importance of the nuclear longitudinal structure function $F^{A}_{L}$ and its behavior owing to the impact parameter for the heavy and light nucleus of Pb-208 and C-12 at $Q^2=5$ and $10~\\mathrm{GeV}^2$ is considered. The dependence of the ratios $R^{A}_{F_{L}}$ and $R^{A}_{\\sigma}$ on the impact parameter and the expanding point of the gluon density at small $x$ are investigated. The factorized form of parton distributions in nuclei is used in HIJING2.0 model.","sentences":["The nuclear reduced cross section $\\sigma^{A}_{r}$, in the kinematic range of the electron-Ion collider with center-of-mass energy $\\sqrt{s}=140~\\mathrm{GeV}$ and $y{\\leq}1$, is discussed.","The importance of the nuclear longitudinal structure function $F^{A}_{L}$ and its behavior owing to the impact parameter for the heavy and light nucleus of Pb-208 and C-12 at $Q^2=5$ and $10~\\mathrm{GeV}^2$ is considered.","The dependence of the ratios $R^{A}_{F_{L}}$ and $R^{A}_{\\sigma}$ on the impact parameter and the expanding point of the gluon density at small $x$ are investigated.","The factorized form of parton distributions in nuclei is used in HIJING2.0 model."],"url":"http://arxiv.org/abs/2404.12914v1","category":"hep-ph"}
{"created":"2024-04-19 13:56:55","title":"Mapping the path to Cryogenic Atom Probe Tomography Analysis of biomolecules","abstract":"The understanding of protein structure, folding, and interaction with other proteins remains one of the grand challenges of modern biology. Tremendous progress has been made thanks to X-ray- or electron-based techniques that have provided atomic configurations of proteins, and their solvation shell. These techniques though require a large number of similar molecules to provide an average view, and lack detailed compositional information that might play a major role in the biochemical activity of these macromolecules. Based on its intrinsic performance and recent impact in materials science, atom probe tomography (APT) has been touted as a potential novel tool to analyse biological materials, including proteins. However, analysis of biomolecules in their native, hydrated state by APT have not yet been routinely achieved, and the technique's true capabilities remain to be demonstrated. Here, we present and discuss systematic analyses of individual amino-acids in frozen aqueous solutions on two different nanoporous metal supports across a wide range of analysis conditions. Using a ratio of the molecular ions of water as a descriptor for the conditions of electrostatic field, we study the fragmentation and behavior of those amino acids. We discuss the importance sample support, specimen preparation route, acquisition conditions and data analysis, to pave the way towards establishing guidelines for cryo-APT analysis of biomolecules.","sentences":["The understanding of protein structure, folding, and interaction with other proteins remains one of the grand challenges of modern biology.","Tremendous progress has been made thanks to X-ray- or electron-based techniques that have provided atomic configurations of proteins, and their solvation shell.","These techniques though require a large number of similar molecules to provide an average view, and lack detailed compositional information that might play a major role in the biochemical activity of these macromolecules.","Based on its intrinsic performance and recent impact in materials science, atom probe tomography (APT) has been touted as a potential novel tool to analyse biological materials, including proteins.","However, analysis of biomolecules in their native, hydrated state by APT have not yet been routinely achieved, and the technique's true capabilities remain to be demonstrated.","Here, we present and discuss systematic analyses of individual amino-acids in frozen aqueous solutions on two different nanoporous metal supports across a wide range of analysis conditions.","Using a ratio of the molecular ions of water as a descriptor for the conditions of electrostatic field, we study the fragmentation and behavior of those amino acids.","We discuss the importance sample support, specimen preparation route, acquisition conditions and data analysis, to pave the way towards establishing guidelines for cryo-APT analysis of biomolecules."],"url":"http://arxiv.org/abs/2404.12894v1","category":"physics.bio-ph"}
{"created":"2024-04-19 13:30:08","title":"On the field independent additive constant in Wilson actions","abstract":"We discuss the field independent additive constant in Wilson actions carefully within the exact renormalization group formalism. The additive constant does not affect the correlation functions of fields normalized by the partition function, and for that reason it is often ignored. But it is an essential part of the partition function, and in the limit where the UV cutoff goes to zero, the constant gives a renormalized vacuum energy density. We discuss two concrete examples: the Gaussian theory and the linear sigma model in the large $N$ limit.","sentences":["We discuss the field independent additive constant in Wilson actions carefully within the exact renormalization group formalism.","The additive constant does not affect the correlation functions of fields normalized by the partition function, and for that reason it is often ignored.","But it is an essential part of the partition function, and in the limit where the UV cutoff goes to zero, the constant gives a renormalized vacuum energy density.","We discuss two concrete examples: the Gaussian theory and the linear sigma model in the large $N$ limit."],"url":"http://arxiv.org/abs/2404.12881v1","category":"hep-th"}
{"created":"2024-04-19 12:50:51","title":"Development of Two-Dimensional Neutron Imager with a Sandwich Configuration","abstract":"We have developed a two-dimensional neutron imager based on a semiconductor pixelated sensor, especially designed for experiments measuring of a spatial and a temporal behavior of quantum bound states of ultra-cold neutrons. Through these measurements, we expect to measure the ratio between the inertial and gravitational masses of neutrons and to test the equivalence principle in the quantum regime. As one of the principal neutron imagers, we fabricated a sensor with a sandwich configuration, named 10B-INTPIX4-sw, and tested its response to ultra-cold neutrons at the Los Alamos Neutron Science Center (LANSCE). We observed simultaneous events on both sandwiching sensors without significant loss of detection efficiency. The efficiency was evaluated to be about 16%, relative to the 10B/ZnS reference detector. The coincidence condition reduces its efficiency by a factor of about 3.","sentences":["We have developed a two-dimensional neutron imager based on a semiconductor pixelated sensor, especially designed for experiments measuring of a spatial and a temporal behavior of quantum bound states of ultra-cold neutrons.","Through these measurements, we expect to measure the ratio between the inertial and gravitational masses of neutrons and to test the equivalence principle in the quantum regime.","As one of the principal neutron imagers, we fabricated a sensor with a sandwich configuration, named 10B-INTPIX4-sw, and tested its response to ultra-cold neutrons at the Los Alamos Neutron Science Center (LANSCE).","We observed simultaneous events on both sandwiching sensors without significant loss of detection efficiency.","The efficiency was evaluated to be about 16%, relative to the 10B/ZnS reference detector.","The coincidence condition reduces its efficiency by a factor of about 3."],"url":"http://arxiv.org/abs/2404.12857v1","category":"physics.ins-det"}
{"created":"2024-04-19 12:17:05","title":"Distinguishing radiation mechanisms and particle populations in blazar jets through long-term multi-band monitoring with RINGO3 and Fermi","abstract":"We present the results of seven years of multicolour photometric monitoring of a sample of 31 $\\gamma$-ray bright blazars using the RINGO3 polarimeter on the Liverpool Telescope from 2013--2020. We explore the relationships between simultaneous observations of flux in three optical wavebands along with Fermi $\\gamma$-ray data in order to explore the radiation mechanisms and particle populations in blazar jets. We find significant correlations between optical and $\\gamma$-ray flux with no detectable time lag, suggesting leptonic emission processes in the jets of these sources. Furthermore, we find the spectral behaviour against optical and $\\gamma$-ray flux for many sources is best fit logarithmically. This is suggestive of a transition between bluer-/redder-when-brighter into stable-when-brighter behaviour during high activity states; a behaviour that might be missed in poorly sampled data, resulting in apparent linear relationships.","sentences":["We present the results of seven years of multicolour photometric monitoring of a sample of 31 $\\gamma$-ray bright blazars using the RINGO3 polarimeter on the Liverpool Telescope from 2013--2020.","We explore the relationships between simultaneous observations of flux in three optical wavebands along with Fermi $\\gamma$-ray data in order to explore the radiation mechanisms and particle populations in blazar jets.","We find significant correlations between optical and $\\gamma$-ray flux with no detectable time lag, suggesting leptonic emission processes in the jets of these sources.","Furthermore, we find the spectral behaviour against optical and $\\gamma$-ray flux for many sources is best fit logarithmically.","This is suggestive of a transition between bluer-/redder-when-brighter into stable-when-brighter behaviour during high activity states; a behaviour that might be missed in poorly sampled data, resulting in apparent linear relationships."],"url":"http://arxiv.org/abs/2404.12835v1","category":"astro-ph.HE"}
{"created":"2024-04-19 11:57:39","title":"Mixed Polyanionic NaFe$_{1.6}$V$_{0.4}$(PO$_{4}$)(SO$_{4}$)$_{2}$@CNT Cathode for Sodium-ion Batteries: Electrochemical Diffusion Kinetics and Distribution of Relaxation Time Analysis at Different Temperatures","abstract":"We report the electrochemical sodium-ion kiinetics and distribution of relaxation time (DRT) analysis of a newly designed mixed polyanionic NaFe$_{1.6}$V$_{0.4}$(PO$_{4}$)(SO$_{4}$)$_{2}$@CNT composite as a cathode. The specific capacity of 104 mAhg$^{-1}$ is observed at 0.1~C with the average working voltage of $\\sim$3~V. Intriguingly, a remarkable rate capability and reversibility are demonstrated up to very high current rate of 25~C. The long cycling test up to 10~C shows high capacity retention even after 2000 cycles. The detailed analysis of galvanostatic intermittent titration technique (GITT) and cyclic voltammetry (CV) data reveal the diffusion coefficient of 10$^{-8}$--10$^{-11}$ cm$^{2}$s$^{-1}$. We find excellent stability in the thermal testing between 25--55$^\\circ$C temperatures and 80\\% capacity retention up to 100 cycles at 5~C. Further, we analyse the individual electrochemical processes in the time domain using the novel DRT technique at different temperatures. The {\\it ex-situ} investigation shows the stable and reversible structure, morphology and electronic states of the long cycled cathode material. More importantly, we demonstrate relatively high specific energy of $\\approx$155 Wh kg$^{-1}$ (considering the total active material loading of both the electrodes) at 0.2~C for full cell battery having excellent rate capability up to 10~C and long cyclic stability at 1~C.","sentences":["We report the electrochemical sodium-ion kiinetics and distribution of relaxation time (DRT) analysis of a newly designed mixed polyanionic NaFe$_{1.6}$V$_{0.4}$(PO$_{4}$)(SO$_{4}$)$_{2}$@CNT composite as a cathode.","The specific capacity of 104 mAhg$^{-1}$ is observed at 0.1~C with the average working voltage of $\\sim$3~V. Intriguingly, a remarkable rate capability and reversibility are demonstrated up to very high current rate of 25~C. The long cycling test up to 10~C shows high capacity retention even after 2000 cycles.","The detailed analysis of galvanostatic intermittent titration technique (GITT) and cyclic voltammetry (CV) data reveal the diffusion coefficient of 10$^{-8}$--10$^{-11}$ cm$^{2}$s$^{-1}$. We find excellent stability in the thermal testing between 25--55$^\\circ$C temperatures and 80\\% capacity retention up to 100 cycles at 5~C. Further, we analyse the individual electrochemical processes in the time domain using the novel DRT technique at different temperatures.","The {\\it ex-situ} investigation shows the stable and reversible structure, morphology and electronic states of the long cycled cathode material.","More importantly, we demonstrate relatively high specific energy of $\\approx$155 Wh kg$^{-1}$ (considering the total active material loading of both the electrodes) at 0.2~C for full cell battery having excellent rate capability up to 10~C and long cyclic stability at 1~C."],"url":"http://arxiv.org/abs/2404.12822v1","category":"physics.chem-ph"}
{"created":"2024-04-19 11:55:36","title":"Determination of the CKM angle $\u03c6_{3}$ from a combination of Belle and Belle II results","abstract":"We report a determination of the CKM angle $\\phi_{3}$, also known as $\\gamma$, from a combination of measurements using samples of up to 711~fb$^{-1}$ from the Belle experiment and up to 362~fb$^{-1}$ from the Belle II experiment. We combine results from analyses of $B^+\\to DK^+, B^+\\to D\\pi^+$, and $B^+ \\to D^{*}K^+$ decays, where $D$ is an admixture of $D^0$ and $\\overline{D}{}^{0}$ mesons, in a likelihood fit to obtain $\\phi_{3} = (78.6^{+7.2}_{-7.3})^{\\circ}$. We also briefly discuss the interpretation of this result.","sentences":["We report a determination of the CKM angle $\\phi_{3}$, also known as $\\gamma$, from a combination of measurements using samples of up to 711~fb$^{-1}$ from the Belle experiment and up to 362~fb$^{-1}$ from the Belle II experiment.","We combine results from analyses of $B^+\\to DK^+, B^+\\to D\\pi^+$, and $B^+ \\to D^{*}K^+$ decays, where $D$ is an admixture of $D^0$ and $\\overline{D}{}^{0}$ mesons, in a likelihood fit to obtain $\\phi_{3} = (78.6^{+7.2}_{-7.3})^{\\circ}$. We also briefly discuss the interpretation of this result."],"url":"http://arxiv.org/abs/2404.12817v1","category":"hep-ex"}
{"created":"2024-04-19 11:29:33","title":"Hypernuclear constraints on the existence and lifetime of a deeply bound $H$ dibaryon","abstract":"We study to what extent the unique observation of $\\Lambda\\Lambda$ hypernuclei by their weak decay into known $\\Lambda$ hypernuclei, with lifetimes of order 10$^{-10}$ s, rules out the existence of a deeply bound doubly-strange (${\\cal S}$=$-$2) $H$ dibaryon. Treating ${_{\\Lambda\\Lambda}^{~~6}}{\\rm He}$ (the Nagara emulsion event) in a realistic $\\Lambda-\\Lambda-{^4}$He three-body model, we find that the ${_{\\Lambda\\Lambda}^{~~6}}{\\rm He}\\to H + {^4{\\rm He}}$ strong-interaction lifetime increases beyond 10$^{-10}$ s for $m_H < m_{\\Lambda}+m_n$, about 176 MeV below the $\\Lambda\\Lambda$ threshold, so that such a deeply bound $H$ is not in conflict with hypernuclear data. Constrained by $\\Lambda$ hypernuclear $\\Delta{\\cal S}$=1 nonmesonic weak-interaction decay rates, we evaluate the $\\Delta{\\cal S}$=2 $H\\to nn$ weak-decay lifetime of $H$ in the mass range $2m_n \\lesssim m_H < m_{\\Lambda}+m_n$. The resulting $H$ lifetime is of order 10$^4$ s, many orders of magnitude shorter than required to qualify for a dark-matter candidate. A lower-mass absolutely stable $H$, $m_H\\lesssim 2m_n$, is likely to be ruled out by established limits of nuclear stability such as for $^{16}$O.","sentences":["We study to what extent the unique observation of $\\Lambda\\Lambda$ hypernuclei by their weak decay into known $\\Lambda$ hypernuclei, with lifetimes of order 10$^{-10}$ s, rules out the existence of a deeply bound doubly-strange (${\\cal S}$=$-$2) $H$ dibaryon.","Treating ${_{\\Lambda\\Lambda}^{~~6}}{\\rm He}$ (the Nagara emulsion event) in a realistic $\\Lambda-\\Lambda-{^4}$He three-body model, we find that the ${_{\\Lambda\\Lambda}^{~~6}}{\\rm He}\\to H + {^4{\\rm He}}$ strong-interaction lifetime increases beyond 10$^{-10}$ s for $m_H < m_{\\Lambda}+m_n$, about 176 MeV below the $\\Lambda\\Lambda$ threshold, so that such a deeply bound $H$ is not in conflict with hypernuclear data.","Constrained by $\\Lambda$ hypernuclear $\\Delta{\\cal S}$=1 nonmesonic weak-interaction decay rates, we evaluate the $\\Delta{\\cal S}$=2 $H\\to nn$ weak-decay lifetime of $H$ in the mass range $2m_n \\lesssim m_H < m_{\\Lambda}+m_n$.","The resulting $H$ lifetime is of order 10$^4$ s, many orders of magnitude shorter than required to qualify for a dark-matter candidate.","A lower-mass absolutely stable $H$, $m_H\\lesssim 2m_n$, is likely to be ruled out by established limits of nuclear stability such as for $^{16}$O."],"url":"http://arxiv.org/abs/2404.12801v1","category":"nucl-th"}
{"created":"2024-04-19 11:05:53","title":"Observation of Floquet states in graphene","abstract":"Recent advances in the field of condensed-matter physics have unlocked the potential to realize and control emergent material phases that do not exist in thermal equilibrium. One of the most promising concepts in this regard is Floquet engineering, the coherent dressing of matter via time-periodic perturbations. However, the broad applicability of Floquet engineering to quantum materials is still unclear. For the paradigmatic case of monolayer graphene, the theoretically predicted Floquet-induced effects, despite a seminal report of the light-induced anomalous Hall effect, have been put into question. Here, we overcome this problem by using electronic structure measurements to provide direct experimental evidence of Floquet engineering in graphene. We report light-matter-dressed Dirac bands by measuring the contribution of Floquet sidebands, Volkov sidebands, and their quantum path interference to graphene's photoemission spectral function. Our results finally demonstrate that Floquet engineering in graphene is possible, paving the way for the experimental realization of the many theoretical proposals on Floquet-engineered band structures and topological phases.","sentences":["Recent advances in the field of condensed-matter physics have unlocked the potential to realize and control emergent material phases that do not exist in thermal equilibrium.","One of the most promising concepts in this regard is Floquet engineering, the coherent dressing of matter via time-periodic perturbations.","However, the broad applicability of Floquet engineering to quantum materials is still unclear.","For the paradigmatic case of monolayer graphene, the theoretically predicted Floquet-induced effects, despite a seminal report of the light-induced anomalous Hall effect, have been put into question.","Here, we overcome this problem by using electronic structure measurements to provide direct experimental evidence of Floquet engineering in graphene.","We report light-matter-dressed Dirac bands by measuring the contribution of Floquet sidebands, Volkov sidebands, and their quantum path interference to graphene's photoemission spectral function.","Our results finally demonstrate that Floquet engineering in graphene is possible, paving the way for the experimental realization of the many theoretical proposals on Floquet-engineered band structures and topological phases."],"url":"http://arxiv.org/abs/2404.12791v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-19 10:59:55","title":"Enhanced interlayer electron transfer by surface treatments in mixed-dimensional van der Waals semiconductor heterostructures","abstract":"We investigate the excitonic species in WS$_{2}$ monolayers transferred onto III-V semiconductor substrates with different surface treatments. When the III-V substrates were covered with amorphous native oxides, negatively charged excitons dominate the spectral weight in low-temperature near-resonance photoluminescence (PL) measurements. However, when the native oxides of the III-V substrates were reduced, neutral excitons begin to dominate the spectral weight, indicating a reduction in the electron density in the WS$_{2}$ monolayers. The removal of the native oxides enhanced the electron transfer from the WS$_{2}$ monolayer to the III-V substrate. In addition, an additional shoulder-like PL feature appeared $\\sim$50 meV below the emission of neutral excitons, which can be attributed to the emission of localized excitons. When the III-V substrate surface was passivated by sulfur after the reduction of the native oxides, neutral excitons still dominated the spectral weight. However, the low energy PL shoulder disappeared again, suggesting the effective delocalization of excitons through the substrate surface passivation. Surface engineering of the semiconductor substrates for two-dimensional (2D) materials can provide a novel approach to control the carrier density of the 2D materials, to implement deterministic carrier localization or delocalization for the 2D materials, and to facilitate the interlayer transfer of charge, spin, and valley currents. These findings open the avenue for novel device concepts and phenomena in mixed-dimensional semiconductor heterostructures.","sentences":["We investigate the excitonic species in WS$_{2}$ monolayers transferred onto III-V semiconductor substrates with different surface treatments.","When the III-V substrates were covered with amorphous native oxides, negatively charged excitons dominate the spectral weight in low-temperature near-resonance photoluminescence (PL) measurements.","However, when the native oxides of the III-V substrates were reduced, neutral excitons begin to dominate the spectral weight, indicating a reduction in the electron density in the WS$_{2}$ monolayers.","The removal of the native oxides enhanced the electron transfer from the WS$_{2}$ monolayer to the III-V substrate.","In addition, an additional shoulder-like PL feature appeared $\\sim$50 meV below the emission of neutral excitons, which can be attributed to the emission of localized excitons.","When the III-V substrate surface was passivated by sulfur after the reduction of the native oxides, neutral excitons still dominated the spectral weight.","However, the low energy PL shoulder disappeared again, suggesting the effective delocalization of excitons through the substrate surface passivation.","Surface engineering of the semiconductor substrates for two-dimensional (2D) materials can provide a novel approach to control the carrier density of the 2D materials, to implement deterministic carrier localization or delocalization for the 2D materials, and to facilitate the interlayer transfer of charge, spin, and valley currents.","These findings open the avenue for novel device concepts and phenomena in mixed-dimensional semiconductor heterostructures."],"url":"http://arxiv.org/abs/2404.12787v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-19 10:11:57","title":"On the Path to High-temperature Josephson Multi-junction Devices","abstract":"We report our progress in the high-temperature superconductor (HTS) Josephson junction fabrication process founded on using a focused helium ion beam damaging technique and discuss the expected device performance attainable with the HTS multi-junction device technology. Both the achievable high value of characteristic voltage $V_c=I_cR_N$ of Josephson junctions and the ability to design a large number of arbitrary located Josephson junctions allow narrowing the existing gap in design abilities for LTS and HTS circuits even with using a single YBCO film layer. A one-layer topology of active electrically small antenna is suggested and its voltage response characteristics are considered.","sentences":["We report our progress in the high-temperature superconductor (HTS) Josephson junction fabrication process founded on using a focused helium ion beam damaging technique and discuss the expected device performance attainable with the HTS multi-junction device technology.","Both the achievable high value of characteristic voltage $V_c=I_cR_N$ of Josephson junctions and the ability to design a large number of arbitrary located Josephson junctions allow narrowing the existing gap in design abilities for LTS and HTS circuits even with using a single YBCO film layer.","A one-layer topology of active electrically small antenna is suggested and its voltage response characteristics are considered."],"url":"http://arxiv.org/abs/2404.12767v1","category":"cond-mat.supr-con"}
{"created":"2024-04-19 09:26:59","title":"The free energy of Dirac's vacuum in purely magnetic fields","abstract":"The QED vacuum is a non-linear polarisable medium rather than an empty space. In this work, we present the first rigorous derivation of the one-loop effective magnetic Lagrangian at positive temperature, a non-linear functional describing the free energy of quantum vacuum in a classical magnetic field. We start by properly defining the free energy functional using the Pauli-Villars regularisation technique in order to remove the worst ultraviolet divergence which represents a well known issue of the theory. Then, we study the limit of slowly varying classical magnetic fields. In this regime, we prove the convergence of this functional to the Euler-Heisenberg formula with thermal corrections, by recovering the effective Lagrangian first derived by Dittrich in 1979.","sentences":["The QED vacuum is a non-linear polarisable medium rather than an empty space.","In this work, we present the first rigorous derivation of the one-loop effective magnetic Lagrangian at positive temperature, a non-linear functional describing the free energy of quantum vacuum in a classical magnetic field.","We start by properly defining the free energy functional using the Pauli-Villars regularisation technique in order to remove the worst ultraviolet divergence which represents a well known issue of the theory.","Then, we study the limit of slowly varying classical magnetic fields.","In this regime, we prove the convergence of this functional to the Euler-Heisenberg formula with thermal corrections, by recovering the effective Lagrangian first derived by Dittrich in 1979."],"url":"http://arxiv.org/abs/2404.12733v1","category":"math-ph"}
{"created":"2024-04-19 09:06:49","title":"Virasoro constraints for K3 surfaces and monodromy operators","abstract":"The Virasoro constraints for moduli spaces of stable torsion free sheaves on a surface with only $(p,p)$-cohomology were recently proved by Bojko-Moreira-Lim. The rank 1 case, which is not restricted to surfaces with only $(p,p)$-cohomology, was established by Moreira. We formulate conjectural Virasoro constraints in any positive rank without requiring only $(p,p)$-cohomology. We prove our conjecture for K3 surfaces using Markman monodromy operators, which allow us to reduce to the rank 1 case. We also prove new Virasoro constraints in rank 0. Finally, for K3 surfaces, we introduce new Virasoro operators in negative degree which, together with the previous Virasoro operators, give a representation of Virasoro algebra with central charge $24$.","sentences":["The Virasoro constraints for moduli spaces of stable torsion free sheaves on a surface with only $(p,p)$-cohomology were recently proved by Bojko-Moreira-Lim.","The rank 1 case, which is not restricted to surfaces with only $(p,p)$-cohomology, was established by Moreira.","We formulate conjectural Virasoro constraints in any positive rank without requiring only $(p,p)$-cohomology.","We prove our conjecture for K3 surfaces using Markman monodromy operators, which allow us to reduce to the rank 1 case.","We also prove new Virasoro constraints in rank 0.","Finally, for K3 surfaces, we introduce new Virasoro operators in negative degree which, together with the previous Virasoro operators, give a representation of Virasoro algebra with central charge $24$."],"url":"http://arxiv.org/abs/2404.12723v1","category":"math.AG"}
