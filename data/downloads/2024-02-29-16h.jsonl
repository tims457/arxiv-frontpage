{"created":"2024-02-27 18:59:18","title":"Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning","abstract":"A common failure mode for policies trained with imitation is compounding execution errors at test time. When the learned policy encounters states that were not present in the expert demonstrations, the policy fails, leading to degenerate behavior. The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states. However, in practice, this is often prohibitively expensive. In this work, we propose Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without the cost for eye-in-hand imitation learning problems. Instead of collecting new samples to cover out-of-distribution states, DMD uses recent advances in diffusion models to create these samples with diffusion models. This leads to robust performance from few demonstrations. In experiments conducted for non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a success rate of 80% with as few as 8 expert demonstrations, where naive behavior cloning reaches only 20%. DMD also outperform competing NeRF-based augmentation schemes by 50%.","sentences":["A common failure mode for policies trained with imitation is compounding execution errors at test time.","When the learned policy encounters states that were not present in the expert demonstrations, the policy fails, leading to degenerate behavior.","The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states.","However, in practice, this is often prohibitively expensive.","In this work, we propose Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without the cost for eye-in-hand imitation learning problems.","Instead of collecting new samples to cover out-of-distribution states, DMD uses recent advances in diffusion models to create these samples with diffusion models.","This leads to robust performance from few demonstrations.","In experiments conducted for non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a success rate of 80% with as few as 8 expert demonstrations, where naive behavior cloning reaches only 20%.","DMD also outperform competing NeRF-based augmentation schemes by 50%."],"url":"http://arxiv.org/abs/2402.17768v1","category":"cs.RO"}
{"created":"2024-02-27 18:58:54","title":"Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator","abstract":"Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment). In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments. We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments. Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot. An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system. We will open source code and models for others to replicate and build upon our system.","sentences":["Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment).","In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments.","We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments.","Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot.","An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system.","We will open source code and models for others to replicate and build upon our system."],"url":"http://arxiv.org/abs/2402.17767v1","category":"cs.RO"}
{"created":"2024-02-27 18:53:18","title":"Learning to Program Variational Quantum Circuits with Fast Weights","abstract":"Quantum Machine Learning (QML) has surfaced as a pioneering framework addressing sequential control tasks and time-series modeling. It has demonstrated empirical quantum advantages notably within domains such as Reinforcement Learning (RL) and time-series prediction. A significant advancement lies in Quantum Recurrent Neural Networks (QRNNs), specifically tailored for memory-intensive tasks encompassing partially observable environments and non-linear time-series prediction. Nevertheless, QRNN-based models encounter challenges, notably prolonged training duration stemming from the necessity to compute quantum gradients using backpropagation-through-time (BPTT). This predicament exacerbates when executing the complete model on quantum devices, primarily due to the substantial demand for circuit evaluation arising from the parameter-shift rule. This paper introduces the Quantum Fast Weight Programmers (QFWP) as a solution to the temporal or sequential learning challenge. The QFWP leverages a classical neural network (referred to as the 'slow programmer') functioning as a quantum programmer to swiftly modify the parameters of a variational quantum circuit (termed the 'fast programmer'). Instead of completely overwriting the fast programmer at each time-step, the slow programmer generates parameter changes or updates for the quantum circuit parameters. This approach enables the fast programmer to incorporate past observations or information. Notably, the proposed QFWP model achieves learning of temporal dependencies without necessitating the use of quantum recurrent neural networks. Numerical simulations conducted in this study showcase the efficacy of the proposed QFWP model in both time-series prediction and RL tasks. The model exhibits performance levels either comparable to or surpassing those achieved by QLSTM-based models.","sentences":["Quantum Machine Learning (QML) has surfaced as a pioneering framework addressing sequential control tasks and time-series modeling.","It has demonstrated empirical quantum advantages notably within domains such as Reinforcement Learning (RL) and time-series prediction.","A significant advancement lies in Quantum Recurrent Neural Networks (QRNNs), specifically tailored for memory-intensive tasks encompassing partially observable environments and non-linear time-series prediction.","Nevertheless, QRNN-based models encounter challenges, notably prolonged training duration stemming from the necessity to compute quantum gradients using backpropagation-through-time (BPTT).","This predicament exacerbates when executing the complete model on quantum devices, primarily due to the substantial demand for circuit evaluation arising from the parameter-shift rule.","This paper introduces the Quantum Fast Weight Programmers (QFWP) as a solution to the temporal or sequential learning challenge.","The QFWP leverages a classical neural network (referred to as the 'slow programmer') functioning as a quantum programmer to swiftly modify the parameters of a variational quantum circuit (termed the 'fast programmer').","Instead of completely overwriting the fast programmer at each time-step, the slow programmer generates parameter changes or updates for the quantum circuit parameters.","This approach enables the fast programmer to incorporate past observations or information.","Notably, the proposed QFWP model achieves learning of temporal dependencies without necessitating the use of quantum recurrent neural networks.","Numerical simulations conducted in this study showcase the efficacy of the proposed QFWP model in both time-series prediction and RL tasks.","The model exhibits performance levels either comparable to or surpassing those achieved by QLSTM-based models."],"url":"http://arxiv.org/abs/2402.17760v1","category":"quant-ph"}
{"created":"2024-02-27 18:42:31","title":"Evaluating Very Long-Term Conversational Memory of LLM Agents","abstract":"Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.","sentences":["Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions.","Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored.","To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs.","Moreover, we equip each agent with the capability of sharing and reacting to images.","The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs.","Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions.","Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks.","Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues.","Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance."],"url":"http://arxiv.org/abs/2402.17753v1","category":"cs.CL"}
{"created":"2024-02-27 18:32:11","title":"When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning","abstract":"Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observable settings and propose research directions to help tackle these challenges.","sentences":["Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment.","What happens when human feedback is based only on partial observations?","We formally define two failure cases: deception and overjustification.","Modeling the human as Boltzmann-rational w.r.t.","a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both.","To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function.","In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity.","We caution against blindly applying RLHF in partially observable settings and propose research directions to help tackle these challenges."],"url":"http://arxiv.org/abs/2402.17747v1","category":"cs.LG"}
{"created":"2024-02-27 18:21:42","title":"Approaching Periodic Systems in Ensemble Density Functional Theory \\textit{via} Finite One-Dimensional Models","abstract":"Ensemble Density Functional Theory (EDFT) is a generalization of ground-state Density Functional Theory (GS DFT), which is based on an exact formal theory of finite collections of a system's ground and excited states. EDFT in various forms has been shown to improve the accuracy of calculated energy level differences in isolated model systems, atoms, and molecules, but it is not yet clear how EDFT could be used to calculate band gaps for periodic systems. We extend the application of EDFT toward periodic systems by estimating the thermodynamic limit with increasingly large finite one-dimensional \\enquote{particle in a box} systems, which approach the uniform electron gas. Using ensemble-generalized Hartree and Local Spin Density Approximation (LSDA) exchange-correlation functionals, we find that corrections go to zero in the infinite limit, as expected for a metallic system. However, there is a correction to the effective mass, as estimated from bi- and tri-ensembles, indicating promise for non-trivial results for EDFT on periodic systems. Singlet excitation energies are found to be positive, but triplet excitation energies are sometimes negative (a triplet instability), pointing to deficiencies of the approximations.","sentences":["Ensemble Density Functional Theory (EDFT) is a generalization of ground-state Density Functional Theory (GS DFT), which is based on an exact formal theory of finite collections of a system's ground and excited states.","EDFT in various forms has been shown to improve the accuracy of calculated energy level differences in isolated model systems, atoms, and molecules, but it is not yet clear how EDFT could be used to calculate band gaps for periodic systems.","We extend the application of EDFT toward periodic systems by estimating the thermodynamic limit with increasingly large finite one-dimensional \\enquote{particle in a box} systems, which approach the uniform electron gas.","Using ensemble-generalized Hartree and Local Spin Density Approximation (LSDA) exchange-correlation functionals, we find that corrections go to zero in the infinite limit, as expected for a metallic system.","However, there is a correction to the effective mass, as estimated from bi- and tri-ensembles, indicating promise for non-trivial results for EDFT on periodic systems.","Singlet excitation energies are found to be positive, but triplet excitation energies are sometimes negative (a triplet instability), pointing to deficiencies of the approximations."],"url":"http://arxiv.org/abs/2402.17742v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-27 18:18:23","title":"reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use","abstract":"The escalating prevalence of cannabis use, and associated cannabis-use disorder (CUD), poses a significant public health challenge globally. With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objective within the 2030 United Nations Agenda for Sustainable Development Goals (SDG). In this work, we develop an online reinforcement learning (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver personalized mobile health interventions aimed at reducing cannabis use among EAs. reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments. Moreover, reBandit employs Empirical Bayes and optimization techniques to autonomously update its hyper-parameters online. To evaluate the performance of our algorithm, we construct a simulation testbed using data from a prior study, and compare against commonly used algorithms in mobile health studies. We show that reBandit performs equally well or better than all the baseline algorithms, and the performance gap widens as population heterogeneity increases in the simulation environment, proving its adeptness to adapt to diverse population of study participants.","sentences":["The escalating prevalence of cannabis use, and associated cannabis-use disorder (CUD), poses a significant public health challenge globally.","With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objective within the 2030 United Nations Agenda for Sustainable Development Goals (SDG).","In this work, we develop an online reinforcement learning (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver personalized mobile health interventions aimed at reducing cannabis use among EAs.","reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments.","Moreover, reBandit employs Empirical Bayes and optimization techniques to autonomously update its hyper-parameters online.","To evaluate the performance of our algorithm, we construct a simulation testbed using data from a prior study, and compare against commonly used algorithms in mobile health studies.","We show that reBandit performs equally well or better than all the baseline algorithms, and the performance gap widens as population heterogeneity increases in the simulation environment, proving its adeptness to adapt to diverse population of study participants."],"url":"http://arxiv.org/abs/2402.17739v1","category":"cs.AI"}
{"created":"2024-02-27 18:15:22","title":"Observation of the $\u039e^-_\\mathrm{b}$ $\\to$ $\u03c8$(2S)$\u039e^-$ decay and studies of the $\u039e_\\mathrm{b}^{\\ast{}0}$ baryon in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"The first observation of the decay $\\Xi^-_\\mathrm{b}$ $\\to$ $\\psi$(2S)$\\Xi^-$ and measurement of the branching ratio of $\\Xi^-_\\mathrm{b}$ $\\to$ $\\psi$(2S)$\\Xi^-$ to $\\Xi^-_\\mathrm{b}$ $\\to$ J/$\\psi$$\\Xi^-$ are presented. The J/$\\psi$ and $\\psi$(2S) mesons are reconstructed using their dimuon decay modes. The results are based on proton-proton colliding beam data from the LHC collected by the CMS experiment at $\\sqrt{s}$ = 13 TeV in 2016-2018, corresponding to an integrated luminosity of 140 fb$^{-1}$. The branching fraction ratio is measured to be $\\mathcal{B}$($\\Xi^-_\\mathrm{b}$ $\\to$ $\\psi$(2S)$\\Xi^-$)/$\\mathcal{B}$($\\Xi^-_\\mathrm{b}$ $\\to$ J/$\\psi$$\\Xi^-$) = 0.84$^{+0.21}_{-0.19}$ (stat) $\\pm$ 0.10 (syst) $\\pm$ 0.02 ($\\mathcal{B}$), where the last uncertainty comes from the uncertainties in the branching fractions of the charmonium states. New measurements of the $\\Xi_\\mathrm{b}^{\\ast{}0}$ baryon mass and natural width are also presented, using the $\\Xi_\\mathrm{b}^-\\pi^+$ final state, where the $\\Xi^-_\\mathrm{b}$ baryon is reconstructed through the decays J/$\\psi \\Xi^-$, $\\psi$(2S)$\\Xi^-$, J/$\\psi \\Lambda$K$^-$, and J/$\\psi \\Sigma^0$K$^-$. Finally, the fraction of the $\\Xi^-_\\mathrm{b}$ baryons produced from $\\Xi_\\mathrm{b}^{\\ast{}0}$ decays is determined.","sentences":["The first observation of the decay $\\Xi^-_\\mathrm{b}$ $\\to$ $\\psi$(2S)$\\Xi^-$ and measurement of the branching ratio of $\\Xi^-_\\mathrm{b}$ $\\to$ $\\psi$(2S)$\\Xi^-$ to $\\Xi^-_\\mathrm{b}$ $\\to$ J/$\\psi$$\\Xi^-$ are presented.","The J/$\\psi$ and $\\psi$(2S) mesons are reconstructed using their dimuon decay modes.","The results are based on proton-proton colliding beam data from the LHC collected by the CMS experiment at $\\sqrt{s}$ = 13 TeV in 2016-2018, corresponding to an integrated luminosity of 140 fb$^{-1}$. The branching fraction ratio is measured to be $\\mathcal{B}$($\\Xi^-_\\mathrm{b}$ $\\to$ $\\psi$(2S)$\\Xi^-$)/$\\mathcal{B}$($\\Xi^-_\\mathrm{b}$ $\\to$ J/$\\psi$$\\Xi^-$) = 0.84$^{+0.21}_{-0.19}$ (stat) $\\pm$ 0.10 (syst) $\\pm$ 0.02 ($\\mathcal{B}$), where the last uncertainty comes from the uncertainties in the branching fractions of the charmonium states.","New measurements of the $\\Xi_\\mathrm{b}^{\\ast{}0}$ baryon mass and natural width are also presented, using the $\\Xi_\\mathrm{b}^-\\pi^+$ final state, where the $\\Xi^-_\\mathrm{b}$ baryon is reconstructed through the decays J/$\\psi \\Xi^-$, $\\psi$(2S)$\\Xi^-$, J/$\\psi \\Lambda$K$^-$, and J/$\\psi \\Sigma^0$K$^-$. Finally, the fraction of the $\\Xi^-_\\mathrm{b}$ baryons produced from $\\Xi_\\mathrm{b}^{\\ast{}0}$ decays is determined."],"url":"http://arxiv.org/abs/2402.17738v1","category":"hep-ex"}
{"created":"2024-02-27 18:12:58","title":"Learning-Based Algorithms for Graph Searching Problems","abstract":"We consider the problem of graph searching with prediction recently introduced by Banerjee et al. (2022). In this problem, an agent, starting at some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a hidden goal node $g$ while minimizing the total distance travelled. We study a setting in which at any node $v$, the agent receives a noisy estimate of the distance from $v$ to $g$. We design algorithms for this search task on unknown graphs. We establish the first formal guarantees on unknown weighted graphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error. Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic. Finally, we provide alternative simpler performance bounds on the algorithms of Banerjee et al. (2022) for the case of searching on a known graph, and establish new lower bounds for this setting.","sentences":["We consider the problem of graph searching with prediction recently introduced by Banerjee et al. (2022).","In this problem, an agent, starting at some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a hidden goal node $g$ while minimizing the total distance travelled.","We study a setting in which at any node $v$, the agent receives a noisy estimate of the distance from $v$ to $g$. We design algorithms for this search task on unknown graphs.","We establish the first formal guarantees on unknown weighted graphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error.","Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic.","Finally, we provide alternative simpler performance bounds on the algorithms of Banerjee et al. (2022) for the case of searching on a known graph, and establish new lower bounds for this setting."],"url":"http://arxiv.org/abs/2402.17736v1","category":"cs.DS"}
{"created":"2024-02-27 18:09:36","title":"Tower: An Open Multilingual Large Language Model for Translation-Related Tasks","abstract":"While general-purpose large language models (LLMs) demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open LLMs are competitive only when specializing on a single task. In this paper, we propose a recipe for tailoring LLMs to multiple tasks present in translation workflows. We perform continued pretraining on a multilingual mixture of monolingual and parallel data, creating TowerBase, followed by finetuning on instructions relevant for translation processes, creating TowerInstruct. Our final model surpasses open alternatives on several tasks relevant to translation workflows and is competitive with general-purpose closed LLMs. To facilitate future research, we release the Tower models, our specialization dataset, an evaluation framework for LLMs focusing on the translation ecosystem, and a collection of model generations, including ours, on our benchmark.","sentences":["While general-purpose large language models (LLMs) demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open LLMs are competitive only when specializing on a single task.","In this paper, we propose a recipe for tailoring LLMs to multiple tasks present in translation workflows.","We perform continued pretraining on a multilingual mixture of monolingual and parallel data, creating TowerBase, followed by finetuning on instructions relevant for translation processes, creating TowerInstruct.","Our final model surpasses open alternatives on several tasks relevant to translation workflows and is competitive with general-purpose closed LLMs.","To facilitate future research, we release the Tower models, our specialization dataset, an evaluation framework for LLMs focusing on the translation ecosystem, and a collection of model generations, including ours, on our benchmark."],"url":"http://arxiv.org/abs/2402.17733v1","category":"cs.CL"}
{"created":"2024-02-27 18:04:59","title":"Markovletics: Methods and A Novel Application for Learning Continuous-Time Markov Chain Mixtures","abstract":"Sequential data naturally arises from user engagement on digital platforms like social media, music streaming services, and web navigation, encapsulating evolving user preferences and behaviors through continuous information streams. A notable unresolved query in stochastic processes is learning mixtures of continuous-time Markov chains (CTMCs). While there is progress in learning mixtures of discrete-time Markov chains with recovery guarantees [GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexplored challenges. The intrigue in CTMC mixtures stems from their potential to model intricate continuous-time stochastic processes prevalent in various fields including social media, finance, and biology.   In this study, we introduce a novel framework for exploring CTMCs, emphasizing the influence of observed trails' length and mixture parameters on problem regimes, which demands specific algorithms. Through thorough experimentation, we examine the impact of discretizing continuous-time trails on the learnability of the continuous-time mixture, given that these processes are often observed via discrete, resource-demanding observations. Our comparative analysis with leading methods explores sample complexity and the trade-off between the number of trails and their lengths, offering crucial insights for method selection in different problem instances. We apply our algorithms on an extensive collection of Lastfm's user-generated trails spanning three years, demonstrating the capability of our algorithms to differentiate diverse user preferences. We pioneer the use of CTMC mixtures on a basketball passing dataset to unveil intricate offensive tactics of NBA teams. This underscores the pragmatic utility and versatility of our proposed framework. All results presented in this study are replicable, and we provide the implementations to facilitate reproducibility.","sentences":["Sequential data naturally arises from user engagement on digital platforms like social media, music streaming services, and web navigation, encapsulating evolving user preferences and behaviors through continuous information streams.","A notable unresolved query in stochastic processes is learning mixtures of continuous-time Markov chains (CTMCs).","While there is progress in learning mixtures of discrete-time Markov chains with recovery guarantees [GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexplored challenges.","The intrigue in CTMC mixtures stems from their potential to model intricate continuous-time stochastic processes prevalent in various fields including social media, finance, and biology.   ","In this study, we introduce a novel framework for exploring CTMCs, emphasizing the influence of observed trails' length and mixture parameters on problem regimes, which demands specific algorithms.","Through thorough experimentation, we examine the impact of discretizing continuous-time trails on the learnability of the continuous-time mixture, given that these processes are often observed via discrete, resource-demanding observations.","Our comparative analysis with leading methods explores sample complexity and the trade-off between the number of trails and their lengths, offering crucial insights for method selection in different problem instances.","We apply our algorithms on an extensive collection of Lastfm's user-generated trails spanning three years, demonstrating the capability of our algorithms to differentiate diverse user preferences.","We pioneer the use of CTMC mixtures on a basketball passing dataset to unveil intricate offensive tactics of NBA teams.","This underscores the pragmatic utility and versatility of our proposed framework.","All results presented in this study are replicable, and we provide the implementations to facilitate reproducibility."],"url":"http://arxiv.org/abs/2402.17730v1","category":"cs.LG"}
{"created":"2024-02-27 17:41:58","title":"Case-Based or Rule-Based: How Do Transformers Do the Math?","abstract":"Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar \"cases\" seen in the training corpus for help. We define these two different reasoning mechanisms as \"rule-based reasoning\" and \"case-based reasoning\". Since rule-based reasoning is essential for acquiring the systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason. To mitigate such problems, we propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform rule-based reasoning. Specifically, we provide explicit rules in the input and then instruct transformers to recite and follow the rules step by step. Through RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up to 12-digit addition with over 95% accuracy, which is over 40% higher than scratchpad. The significant improvement demonstrates that teaching LLMs to explicitly use rules helps them learn rule-based reasoning and generalize better in length.","sentences":["Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition.","While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same.","Instead, they may rely on similar \"cases\" seen in the training corpus for help.","We define these two different reasoning mechanisms as \"rule-based reasoning\" and \"case-based reasoning\".","Since rule-based reasoning is essential for acquiring the systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems.","Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason.","To mitigate such problems, we propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform rule-based reasoning.","Specifically, we provide explicit rules in the input and then instruct transformers to recite and follow the rules step by step.","Through RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up to 12-digit addition with over 95% accuracy, which is over 40% higher than scratchpad.","The significant improvement demonstrates that teaching LLMs to explicitly use rules helps them learn rule-based reasoning and generalize better in length."],"url":"http://arxiv.org/abs/2402.17709v1","category":"cs.AI"}
{"created":"2024-02-27 17:33:23","title":"Federated Learning for Estimating Heterogeneous Treatment Effects","abstract":"Machine learning methods for estimating heterogeneous treatment effects (HTE) facilitate large-scale personalized decision-making across various domains such as healthcare, policy making, education, and more. Current machine learning approaches for HTE require access to substantial amounts of data per treatment, and the high costs associated with interventions makes centrally collecting so much data for each intervention a formidable challenge. To overcome this obstacle, in this work, we propose a novel framework for collaborative learning of HTE estimators across institutions via Federated Learning. We show that even under a diversity of interventions and subject populations across clients, one can jointly learn a common feature representation, while concurrently and privately learning the specific predictive functions for outcomes under distinct interventions across institutions. Our framework and the associated algorithm are based on this insight, and leverage tabular transformers to map multiple input data to feature representations which are then used for outcome prediction via multi-task learning. We also propose a novel way of federated training of personalised transformers that can work with heterogeneous input feature spaces. Experimental results on real-world clinical trial data demonstrate the effectiveness of our method.","sentences":["Machine learning methods for estimating heterogeneous treatment effects (HTE) facilitate large-scale personalized decision-making across various domains such as healthcare, policy making, education, and more.","Current machine learning approaches for HTE require access to substantial amounts of data per treatment, and the high costs associated with interventions makes centrally collecting so much data for each intervention a formidable challenge.","To overcome this obstacle, in this work, we propose a novel framework for collaborative learning of HTE estimators across institutions via Federated Learning.","We show that even under a diversity of interventions and subject populations across clients, one can jointly learn a common feature representation, while concurrently and privately learning the specific predictive functions for outcomes under distinct interventions across institutions.","Our framework and the associated algorithm are based on this insight, and leverage tabular transformers to map multiple input data to feature representations which are then used for outcome prediction via multi-task learning.","We also propose a novel way of federated training of personalised transformers that can work with heterogeneous input feature spaces.","Experimental results on real-world clinical trial data demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2402.17705v1","category":"cs.LG"}
{"created":"2024-02-27 17:27:47","title":"The SCIP Optimization Suite 9.0","abstract":"The SCIP Optimization Suite provides a collection of software packages for mathematical optimization, centered around the constraint integer programming (CIP) framework SCIP. This report discusses the enhancements and extensions included in the SCIP Optimization Suite 9.0. The updates in SCIP 9.0 include improved symmetry handling, additions and improvements of nonlinear handlers and primal heuristics, a new cut generator and two new cut selection schemes, a new branching rule, a new LP interface, and several bug fixes. The SCIP Optimization Suite 9.0 also features new Rust and C++ interfaces for SCIP, new Python interface for SoPlex, along with enhancements to existing interfaces. The SCIP Optimization Suite 9.0 also includes new and improved features in the LP solver SoPlex, the presolving library PaPILO, the parallel framework UG, the decomposition framework GCG, and the SCIP extension SCIP-SDP. These additions and enhancements have resulted in an overall performance improvement of SCIP in terms of solving time, number of nodes in the branch-and-bound tree, as well as the reliability of the solver.","sentences":["The SCIP Optimization Suite provides a collection of software packages for mathematical optimization, centered around the constraint integer programming (CIP) framework SCIP.","This report discusses the enhancements and extensions included in the SCIP Optimization Suite 9.0.","The updates in SCIP 9.0 include improved symmetry handling, additions and improvements of nonlinear handlers and primal heuristics, a new cut generator and two new cut selection schemes, a new branching rule, a new LP interface, and several bug fixes.","The SCIP Optimization Suite 9.0 also features new Rust and C++ interfaces for SCIP, new Python interface for SoPlex, along with enhancements to existing interfaces.","The SCIP Optimization Suite 9.0 also includes new and improved features in the LP solver SoPlex, the presolving library PaPILO, the parallel framework UG, the decomposition framework GCG, and the SCIP extension SCIP-SDP.","These additions and enhancements have resulted in an overall performance improvement of SCIP in terms of solving time, number of nodes in the branch-and-bound tree, as well as the reliability of the solver."],"url":"http://arxiv.org/abs/2402.17702v1","category":"math.OC"}
{"created":"2024-02-27 17:07:18","title":"Autonomous Vehicles: Evolution of Artificial Intelligence and Learning Algorithms","abstract":"The advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies. Central to this evolution is the integration of Artificial Intelligence (AI) and learning algorithms, propelling vehicles into realms of unprecedented autonomy. This paper provides a comprehensive exploration of the evolutionary trajectory of AI within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements. Commencing with a current landscape overview, the paper delves into the fundamental role of AI in shaping the autonomous decision-making capabilities of vehicles. It elucidates the steps involved in the AI-powered development life cycle in vehicles, addressing ethical considerations and bias in AI-driven software development for autonomous vehicles. The study presents statistical insights into the usage and types of AI/learning algorithms over the years, showcasing the evolving research landscape within the automotive industry. Furthermore, the paper highlights the pivotal role of parameters in refining algorithms for both trucks and cars, facilitating vehicles to adapt, learn, and improve performance over time. It concludes by outlining different levels of autonomy, elucidating the nuanced usage of AI and learning algorithms, and automating key tasks at each level. Additionally, the document discusses the variation in software package sizes across different autonomy levels","sentences":["The advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies.","Central to this evolution is the integration of Artificial Intelligence (AI) and learning algorithms, propelling vehicles into realms of unprecedented autonomy.","This paper provides a comprehensive exploration of the evolutionary trajectory of AI within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements.","Commencing with a current landscape overview, the paper delves into the fundamental role of AI in shaping the autonomous decision-making capabilities of vehicles.","It elucidates the steps involved in the AI-powered development life cycle in vehicles, addressing ethical considerations and bias in AI-driven software development for autonomous vehicles.","The study presents statistical insights into the usage and types of AI/learning algorithms over the years, showcasing the evolving research landscape within the automotive industry.","Furthermore, the paper highlights the pivotal role of parameters in refining algorithms for both trucks and cars, facilitating vehicles to adapt, learn, and improve performance over time.","It concludes by outlining different levels of autonomy, elucidating the nuanced usage of AI and learning algorithms, and automating key tasks at each level.","Additionally, the document discusses the variation in software package sizes across different autonomy levels"],"url":"http://arxiv.org/abs/2402.17690v2","category":"cs.LG"}
{"created":"2024-02-27 17:05:41","title":"QoS prediction in radio vehicular environments via prior user information","abstract":"Reliable wireless communications play an important role in the automotive industry as it helps to enhance current use cases and enable new ones such as connected autonomous driving, platooning, cooperative maneuvering, teleoperated driving, and smart navigation. These and other use cases often rely on specific quality of service (QoS) levels for communication. Recently, the area of predictive quality of service (QoS) has received a great deal of attention as a key enabler to forecast communication quality well enough in advance. However, predicting QoS in a reliable manner is a notoriously difficult task. In this paper, we evaluate ML tree-ensemble methods to predict QoS in the range of minutes with data collected from a cellular test network. We discuss radio environment characteristics and we showcase how these can be used to improve ML performance and further support the uptake of ML in commercial networks. Specifically, we use the correlations of the measurements coming from the radio environment by including information of prior vehicles to enhance the prediction of the target vehicles. Moreover, we are extending prior art by showing how longer prediction horizons can be supported.","sentences":["Reliable wireless communications play an important role in the automotive industry as it helps to enhance current use cases and enable new ones such as connected autonomous driving, platooning, cooperative maneuvering, teleoperated driving, and smart navigation.","These and other use cases often rely on specific quality of service (QoS) levels for communication.","Recently, the area of predictive quality of service (QoS) has received a great deal of attention as a key enabler to forecast communication quality well enough in advance.","However, predicting QoS in a reliable manner is a notoriously difficult task.","In this paper, we evaluate ML tree-ensemble methods to predict QoS in the range of minutes with data collected from a cellular test network.","We discuss radio environment characteristics and we showcase how these can be used to improve ML performance and further support the uptake of ML in commercial networks.","Specifically, we use the correlations of the measurements coming from the radio environment by including information of prior vehicles to enhance the prediction of the target vehicles.","Moreover, we are extending prior art by showing how longer prediction horizons can be supported."],"url":"http://arxiv.org/abs/2402.17689v1","category":"cs.LG"}
{"created":"2024-02-27 16:21:09","title":"CSI-Free Optimization of Reconfigurable Intelligent Surfaces with Interference by Using Multiport Network Theory","abstract":"Reconfigurable Intelligent Surfaces (RIS) will play a pivotal role in next-generation wireless systems. Despite efforts to minimize pilot overhead associated with channel estimation, the necessity of configuring the RIS multiple times before obtaining reliable Channel State Information (CSI) may significantly diminish their benefits. Therefore, we propose a CSI-free approach that explores the feasibility of optimizing the RIS for the uplink of a communication system in the presence of interfering users without relying on CSI estimation but leveraging solely some a priori statistical knowledge of the channel. In this context, we consider a multiport network model that accounts for several aspects overlooked by traditional RIS models used in Communication Theory, such as mutual coupling among scattering elements and the presence of structural scattering. The proposed approach targets the maximization of the average achievable rate and is shown to achieve performance that, in some cases, can be very close to the case where the RIS is optimized leveraging perfect CSI.","sentences":["Reconfigurable Intelligent Surfaces (RIS) will play a pivotal role in next-generation wireless systems.","Despite efforts to minimize pilot overhead associated with channel estimation, the necessity of configuring the RIS multiple times before obtaining reliable Channel State Information (CSI) may significantly diminish their benefits.","Therefore, we propose a CSI-free approach that explores the feasibility of optimizing the RIS for the uplink of a communication system in the presence of interfering users without relying on CSI estimation but leveraging solely some a priori statistical knowledge of the channel.","In this context, we consider a multiport network model that accounts for several aspects overlooked by traditional RIS models used in Communication Theory, such as mutual coupling among scattering elements and the presence of structural scattering.","The proposed approach targets the maximization of the average achievable rate and is shown to achieve performance that, in some cases, can be very close to the case where the RIS is optimized leveraging perfect CSI."],"url":"http://arxiv.org/abs/2402.17651v1","category":"cs.IT"}
{"created":"2024-02-27 16:20:46","title":"Comparison of the Effects of Interaction with Intentional Agent and Artificial Intelligence using fNIRS","abstract":"As societal interactions increasingly involve both intentional and unintentional agents, understanding their effects on human cognition becomes paramount. This study investigates the neural correlates of interacting with intentional versus artificial agents in a simulated tennis game scenario. Employing functional near-infrared imaging spectroscopy (fNIRS), we analyzed brain activity in 50 male participants during gameplay against both types of opponents. Our methodological approach ensures ecological validity by simulating real-world decision-making scenarios while participants undergo fNIRS scanning, avoiding the constraints of traditional neuroimaging methods. We focus on six prefrontal cortex channels, leveraging the 10-20 system, to capture nuanced differences in brain activity. Utilizing wavelet analysis, we dissected the data into frequency-specific differences, revealing subtle variations across different channels and frequency bands. Moreover, we quantified activity by comparing average data signals between rest and play modes across all points. Our findings unveil significant differences in neural activation patterns, particularly in one specific channel and frequency range, suggesting distinct cognitive processing when interacting with intentional agents. These results align with previous neuroimaging studies and contribute to understanding the neural underpinnings of human-agent interactions in naturalistic settings. While acknowledging study limitations, including sample homogeneity and spatial accuracy constraints, our findings underscore the potential of fNIRS in exploring complex cognitive phenomena beyond laboratory confines.","sentences":["As societal interactions increasingly involve both intentional and unintentional agents, understanding their effects on human cognition becomes paramount.","This study investigates the neural correlates of interacting with intentional versus artificial agents in a simulated tennis game scenario.","Employing functional near-infrared imaging spectroscopy (fNIRS), we analyzed brain activity in 50 male participants during gameplay against both types of opponents.","Our methodological approach ensures ecological validity by simulating real-world decision-making scenarios while participants undergo fNIRS scanning, avoiding the constraints of traditional neuroimaging methods.","We focus on six prefrontal cortex channels, leveraging the 10-20 system, to capture nuanced differences in brain activity.","Utilizing wavelet analysis, we dissected the data into frequency-specific differences, revealing subtle variations across different channels and frequency bands.","Moreover, we quantified activity by comparing average data signals between rest and play modes across all points.","Our findings unveil significant differences in neural activation patterns, particularly in one specific channel and frequency range, suggesting distinct cognitive processing when interacting with intentional agents.","These results align with previous neuroimaging studies and contribute to understanding the neural underpinnings of human-agent interactions in naturalistic settings.","While acknowledging study limitations, including sample homogeneity and spatial accuracy constraints, our findings underscore the potential of fNIRS in exploring complex cognitive phenomena beyond laboratory confines."],"url":"http://arxiv.org/abs/2402.17650v1","category":"q-bio.NC"}
{"created":"2024-02-27 16:19:37","title":"Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs","abstract":"Due to the widespread use of large language models (LLMs) in ubiquitous systems, we need to understand whether they embed a specific worldview and what these views reflect. Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings. However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning. We propose a series of tests which assess the reliability and consistency of LLMs' stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy domains. We study LLMs ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count. Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They evince a (left-wing) positive stance towards environment protection, social welfare but also (right-wing) law and order, with no consistent preferences in foreign policy, migration, and economy.","sentences":["Due to the widespread use of large language models (LLMs) in ubiquitous systems, we need to understand whether they embed a specific worldview and what these views reflect.","Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings.","However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning.","We propose a series of tests which assess the reliability and consistency of LLMs' stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy domains.","We study LLMs ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count.","Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They evince a (left-wing) positive stance towards environment protection, social welfare but also (right-wing) law and order, with no consistent preferences in foreign policy, migration, and economy."],"url":"http://arxiv.org/abs/2402.17649v1","category":"cs.CL"}
{"created":"2024-02-27 16:15:28","title":"SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation","abstract":"We present SongComposer, an innovative LLM designed for song composition. It could understand and generate melodies and lyrics in symbolic song representations, by leveraging the capability of LLM. Existing music-related LLM treated the music as quantized audio signals, while such implicit encoding leads to inefficient encoding and poor flexibility. In contrast, we resort to symbolic song representation, the mature and efficient way humans designed for music, and enable LLM to explicitly compose songs like humans. In practice, we design a novel tuple design to format lyric and three note attributes (pitch, duration, and rest duration) in the melody, which guarantees the correct LLM understanding of musical symbols and realizes precise alignment between lyrics and melody. To impart basic music understanding to LLM, we carefully collected SongCompose-PT, a large-scale song pretraining dataset that includes lyrics, melodies, and paired lyrics-melodies in either Chinese or English. After adequate pre-training, 10K carefully crafted QA pairs are used to empower the LLM with the instruction-following capability and solve diverse tasks. With extensive experiments, SongComposer demonstrates superior performance in lyric-to-melody generation, melody-to-lyric generation, song continuation, and text-to-song creation, outperforming advanced LLMs like GPT-4.","sentences":["We present SongComposer, an innovative LLM designed for song composition.","It could understand and generate melodies and lyrics in symbolic song representations, by leveraging the capability of LLM.","Existing music-related LLM treated the music as quantized audio signals, while such implicit encoding leads to inefficient encoding and poor flexibility.","In contrast, we resort to symbolic song representation, the mature and efficient way humans designed for music, and enable LLM to explicitly compose songs like humans.","In practice, we design a novel tuple design to format lyric and three note attributes (pitch, duration, and rest duration) in the melody, which guarantees the correct LLM understanding of musical symbols and realizes precise alignment between lyrics and melody.","To impart basic music understanding to LLM, we carefully collected SongCompose-PT, a large-scale song pretraining dataset that includes lyrics, melodies, and paired lyrics-melodies in either Chinese or English.","After adequate pre-training, 10K carefully crafted QA pairs are used to empower the LLM with the instruction-following capability and solve diverse tasks.","With extensive experiments, SongComposer demonstrates superior performance in lyric-to-melody generation, melody-to-lyric generation, song continuation, and text-to-song creation, outperforming advanced LLMs like GPT-4."],"url":"http://arxiv.org/abs/2402.17645v1","category":"cs.SD"}
{"created":"2024-02-27 16:15:03","title":"Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data","abstract":"Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement. Among open-source models, Deepseek-coder-instruct, a code LLM pretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously. Code and data are in https://github.com/xxxiaol/QRData.","sentences":["Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited.","To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data.","The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers.","To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText.","We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models.","The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement.","Among open-source models, Deepseek-coder-instruct, a code LLM pretrained on 2T tokens, gets the highest accuracy of 37%.","Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously.","Code and data are in https://github.com/xxxiaol/QRData."],"url":"http://arxiv.org/abs/2402.17644v1","category":"cs.CL"}
{"created":"2024-02-27 16:11:05","title":"Variational Learning is Effective for Large Deep Networks","abstract":"We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence in support of effectiveness of variational learning.","sentences":["We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks.","We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch.","IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better.","We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data.","We find overwhelming evidence in support of effectiveness of variational learning."],"url":"http://arxiv.org/abs/2402.17641v1","category":"cs.LG"}
{"created":"2024-02-27 15:33:20","title":"Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem","abstract":"Existing learning-based methods for solving job shop scheduling problem (JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and neglect the rich and meaningful topological structures of disjunctive graphs (DGs). This paper proposes the topology-aware bidirectional graph attention network (TBGAT), a novel GNN architecture based on the attention mechanism, to embed the DG for solving JSSP in a local search framework. Specifically, TBGAT embeds the DG from a forward and a backward view, respectively, where the messages are propagated by following the different topologies of the views and aggregated via graph attention. Then, we propose a novel operator based on the message-passing mechanism to calculate the forward and backward topological sorts of the DG, which are the features for characterizing the topological structures and exploited by our model. In addition, we theoretically and experimentally show that TBGAT has linear computational complexity to the number of jobs and machines, respectively, which strengthens the practical value of our method. Besides, extensive experiments on five synthetic datasets and seven classic benchmarks show that TBGAT achieves new SOTA results by outperforming a wide range of neural methods by a large margin.","sentences":["Existing learning-based methods for solving job shop scheduling problem (JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and neglect the rich and meaningful topological structures of disjunctive graphs (DGs).","This paper proposes the topology-aware bidirectional graph attention network (TBGAT), a novel GNN architecture based on the attention mechanism, to embed the DG for solving JSSP in a local search framework.","Specifically, TBGAT embeds the DG from a forward and a backward view, respectively, where the messages are propagated by following the different topologies of the views and aggregated via graph attention.","Then, we propose a novel operator based on the message-passing mechanism to calculate the forward and backward topological sorts of the DG, which are the features for characterizing the topological structures and exploited by our model.","In addition, we theoretically and experimentally show that TBGAT has linear computational complexity to the number of jobs and machines, respectively, which strengthens the practical value of our method.","Besides, extensive experiments on five synthetic datasets and seven classic benchmarks show that TBGAT achieves new SOTA results by outperforming a wide range of neural methods by a large margin."],"url":"http://arxiv.org/abs/2402.17606v1","category":"cs.LG"}
{"created":"2024-02-27 15:29:12","title":"Optimal shielding for Einstein gravity","abstract":"In order to construct asymptotically Euclidean, Einstein's initial data sets, we introduce the localized seed-to-solution method and establish the existence of classes of data sets that exhibit the gravity shielding phenomenon (or localization). We achieve optimal shielding in the sense that the gluing domain can be a collection of arbitrarily narrow nested cones while the metric and extrinsic curvature may be controlled at a super-harmonic rate, and may have arbitrarily slow decay (possibly beyond the standard ADM formalism). We also uncover several notions of physical and mathematical interest: normalized asymptotic kernel elements, localized energy functionals, localized ADM modulators, and relative energy-momentum vectors.","sentences":["In order to construct asymptotically Euclidean, Einstein's initial data sets, we introduce the localized seed-to-solution method and establish the existence of classes of data sets that exhibit the gravity shielding phenomenon (or localization).","We achieve optimal shielding in the sense that the gluing domain can be a collection of arbitrarily narrow nested cones while the metric and extrinsic curvature may be controlled at a super-harmonic rate, and may have arbitrarily slow decay (possibly beyond the standard ADM formalism).","We also uncover several notions of physical and mathematical interest: normalized asymptotic kernel elements, localized energy functionals, localized ADM modulators, and relative energy-momentum vectors."],"url":"http://arxiv.org/abs/2402.17598v1","category":"gr-qc"}
{"created":"2024-02-27 15:28:01","title":"Implicit Regularization via Spectral Neural Networks and Non-linear Matrix Sensing","abstract":"The phenomenon of implicit regularization has attracted interest in recent years as a fundamental aspect of the remarkable generalizing ability of neural networks. In a nutshell, it entails that gradient descent dynamics in many neural nets, even without any explicit regularizer in the loss function, converges to the solution of a regularized learning problem. However, known results attempting to theoretically explain this phenomenon focus overwhelmingly on the setting of linear neural nets, and the simplicity of the linear structure is particularly crucial to existing arguments. In this paper, we explore this problem in the context of more realistic neural networks with a general class of non-linear activation functions, and rigorously demonstrate the implicit regularization phenomenon for such networks in the setting of matrix sensing problems, together with rigorous rate guarantees that ensure exponentially fast convergence of gradient descent.In this vein, we contribute a network architecture called Spectral Neural Networks (abbrv. SNN) that is particularly suitable for matrix learning problems. Conceptually, this entails coordinatizing the space of matrices by their singular values and singular vectors, as opposed to by their entries, a potentially fruitful perspective for matrix learning. We demonstrate that the SNN architecture is inherently much more amenable to theoretical analysis than vanilla neural nets and confirm its effectiveness in the context of matrix sensing, via both mathematical guarantees and empirical investigations. We believe that the SNN architecture has the potential to be of wide applicability in a broad class of matrix learning scenarios.","sentences":["The phenomenon of implicit regularization has attracted interest in recent years as a fundamental aspect of the remarkable generalizing ability of neural networks.","In a nutshell, it entails that gradient descent dynamics in many neural nets, even without any explicit regularizer in the loss function, converges to the solution of a regularized learning problem.","However, known results attempting to theoretically explain this phenomenon focus overwhelmingly on the setting of linear neural nets, and the simplicity of the linear structure is particularly crucial to existing arguments.","In this paper, we explore this problem in the context of more realistic neural networks with a general class of non-linear activation functions, and rigorously demonstrate the implicit regularization phenomenon for such networks in the setting of matrix sensing problems, together with rigorous rate guarantees that ensure exponentially fast convergence of gradient descent.","In this vein, we contribute a network architecture called Spectral Neural Networks (abbrv.","SNN) that is particularly suitable for matrix learning problems.","Conceptually, this entails coordinatizing the space of matrices by their singular values and singular vectors, as opposed to by their entries, a potentially fruitful perspective for matrix learning.","We demonstrate that the SNN architecture is inherently much more amenable to theoretical analysis than vanilla neural nets and confirm its effectiveness in the context of matrix sensing, via both mathematical guarantees and empirical investigations.","We believe that the SNN architecture has the potential to be of wide applicability in a broad class of matrix learning scenarios."],"url":"http://arxiv.org/abs/2402.17595v1","category":"cs.LG"}
{"created":"2024-02-27 15:20:11","title":"Chronicles of CI/CD: A Deep Dive into its Usage Over Time","abstract":"DevOps is a combination of methodologies and tools that improves the software development, build, deployment, and monitoring processes by shortening its lifecycle and improving software quality. Part of this process is CI/CD, which embodies mostly the first parts, right up to the deployment. Despite the many benefits of DevOps and CI/CD, it still presents many challenges promoted by the tremendous proliferation of different tools, languages, and syntaxes, which makes the field quite challenging to learn and keep up to date. Software repositories contain data regarding various software practices, tools, and uses. This data can help gather multiple insights that inform technical and academic decision-making. GitHub is currently the most popular software hosting platform and provides a search API that lets users query its repositories. Our goal with this paper is to gain insights into the technologies developers use for CI/CD by analyzing GitHub repositories. Using a list of the state-of-the-art CI/CD technologies, we use the GitHub search API to find repositories using each of these technologies. We also use the API to extract various insights regarding those repositories. We then organize and analyze the data collected. From our analysis, we provide an overview of the use of CI/CD technologies in our days, but also what happened in the last 12 years. We also show developers use several technologies simultaneously in the same project and that the change between technologies is quite common. From these insights, we find several research paths, from how to support the use of multiple technologies, both in terms of techniques, but also in terms of human-computer interaction, to aiding developers in evolving their CI/CD pipelines, again considering the various dimensions of the problem.","sentences":["DevOps is a combination of methodologies and tools that improves the software development, build, deployment, and monitoring processes by shortening its lifecycle and improving software quality.","Part of this process is CI/CD, which embodies mostly the first parts, right up to the deployment.","Despite the many benefits of DevOps and CI/CD, it still presents many challenges promoted by the tremendous proliferation of different tools, languages, and syntaxes, which makes the field quite challenging to learn and keep up to date.","Software repositories contain data regarding various software practices, tools, and uses.","This data can help gather multiple insights that inform technical and academic decision-making.","GitHub is currently the most popular software hosting platform and provides a search API that lets users query its repositories.","Our goal with this paper is to gain insights into the technologies developers use for CI/CD by analyzing GitHub repositories.","Using a list of the state-of-the-art CI/CD technologies, we use the GitHub search API to find repositories using each of these technologies.","We also use the API to extract various insights regarding those repositories.","We then organize and analyze the data collected.","From our analysis, we provide an overview of the use of CI/CD technologies in our days, but also what happened in the last 12 years.","We also show developers use several technologies simultaneously in the same project and that the change between technologies is quite common.","From these insights, we find several research paths, from how to support the use of multiple technologies, both in terms of techniques, but also in terms of human-computer interaction, to aiding developers in evolving their CI/CD pipelines, again considering the various dimensions of the problem."],"url":"http://arxiv.org/abs/2402.17588v1","category":"cs.SE"}
{"created":"2024-02-27 15:09:20","title":"Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization","abstract":"Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.","sentences":["Large Language Models exhibit robust problem-solving capabilities for diverse tasks.","However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions.","These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games.","In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy.","Specifically, it involves a dynamic belief generation and reflection process for policy evolution.","Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy.","Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs.","Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models.","Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications."],"url":"http://arxiv.org/abs/2402.17574v1","category":"cs.AI"}
{"created":"2024-02-27 14:48:07","title":"Evaluation of Predictive Reliability to Foster Trust in Artificial Intelligence. A case study in Multiple Sclerosis","abstract":"Applying Artificial Intelligence (AI) and Machine Learning (ML) in critical contexts, such as medicine, requires the implementation of safety measures to reduce risks of harm in case of prediction errors. Spotting ML failures is of paramount importance when ML predictions are used to drive clinical decisions. ML predictive reliability measures the degree of trust of a ML prediction on a new instance, thus allowing decision-makers to accept or reject it based on its reliability. To assess reliability, we propose a method that implements two principles. First, our approach evaluates whether an instance to be classified is coming from the same distribution of the training set. To do this, we leverage Autoencoders (AEs) ability to reconstruct the training set with low error. An instance is considered Out-of-Distribution (OOD) if the AE reconstructs it with a high error. Second, it is evaluated whether the ML classifier has good performances on samples similar to the newly classified instance by using a proxy model. We show that this approach is able to assess reliability both in a simulated scenario and on a model trained to predict disease progression of Multiple Sclerosis patients. We also developed a Python package, named relAI, to embed reliability measures into ML pipelines. We propose a simple approach that can be used in the deployment phase of any ML model to suggest whether to trust predictions or not. Our method holds the promise to provide effective support to clinicians by spotting potential ML failures during deployment.","sentences":["Applying Artificial Intelligence (AI) and Machine Learning (ML) in critical contexts, such as medicine, requires the implementation of safety measures to reduce risks of harm in case of prediction errors.","Spotting ML failures is of paramount importance when ML predictions are used to drive clinical decisions.","ML predictive reliability measures the degree of trust of a ML prediction on a new instance, thus allowing decision-makers to accept or reject it based on its reliability.","To assess reliability, we propose a method that implements two principles.","First, our approach evaluates whether an instance to be classified is coming from the same distribution of the training set.","To do this, we leverage Autoencoders (AEs) ability to reconstruct the training set with low error.","An instance is considered Out-of-Distribution (OOD) if the AE reconstructs it with a high error.","Second, it is evaluated whether the ML classifier has good performances on samples similar to the newly classified instance by using a proxy model.","We show that this approach is able to assess reliability both in a simulated scenario and on a model trained to predict disease progression of Multiple Sclerosis patients.","We also developed a Python package, named relAI, to embed reliability measures into ML pipelines.","We propose a simple approach that can be used in the deployment phase of any ML model to suggest whether to trust predictions or not.","Our method holds the promise to provide effective support to clinicians by spotting potential ML failures during deployment."],"url":"http://arxiv.org/abs/2402.17554v1","category":"cs.LG"}
{"created":"2024-02-27 14:47:53","title":"OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web","abstract":"For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as \"Play the next song\", as well as longer horizon tasks such as \"Send an email to John Doe mentioning the time and place to meet\". Specifically, given a pair of screen image and a visually-grounded natural language task, the goal is to generate a script capable of fully executing the task. We run several strong baseline language model agents on our benchmark. The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents. Our benchmark provides a platform to measure and evaluate the progress of language model agents in automating computer tasks and motivates future work towards building multimodal models that bridge large language models and the visual grounding of computer screens.","sentences":["For decades, human-computer interaction has fundamentally been manual.","Even today, almost all productive work done on the computer necessitates human input at every step.","Autonomous virtual agents represent an exciting step in automating many of these menial tasks.","Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems.","They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention.","In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks.","Our scope extends beyond traditional web automation, covering a diverse range of desktop applications.","The dataset consists of fundamental tasks such as \"Play the next song\", as well as longer horizon tasks such as \"Send an email to John Doe mentioning the time and place to meet\".","Specifically, given a pair of screen image and a visually-grounded natural language task, the goal is to generate a script capable of fully executing the task.","We run several strong baseline language model agents on our benchmark.","The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents.","Our benchmark provides a platform to measure and evaluate the progress of language model agents in automating computer tasks and motivates future work towards building multimodal models that bridge large language models and the visual grounding of computer screens."],"url":"http://arxiv.org/abs/2402.17553v2","category":"cs.AI"}
{"created":"2024-02-27 14:44:11","title":"Emergency Caching: Coded Caching-based Reliable Map Transmission in Emergency Networks","abstract":"Many rescue missions demand effective perception and real-time decision making, which highly rely on effective data collection and processing. In this study, we propose a three-layer architecture of emergency caching networks focusing on data collection and reliable transmission, by leveraging efficient perception and edge caching technologies. Based on this architecture, we propose a disaster map collection framework that integrates coded caching technologies. Our framework strategically caches coded fragments of maps across unmanned aerial vehicles (UAVs), fostering collaborative uploading for augmented transmission reliability. Additionally, we establish a comprehensive probability model to assess the effective recovery area of disaster maps. Towards the goal of utility maximization, we propose a deep reinforcement learning (DRL) based algorithm that jointly makes decisions about cooperative UAVs selection, bandwidth allocation and coded caching parameter adjustment, accommodating the real-time map updates in a dynamic disaster situation. Our proposed scheme is more effective than the non-coding caching scheme, as validated by simulation.","sentences":["Many rescue missions demand effective perception and real-time decision making, which highly rely on effective data collection and processing.","In this study, we propose a three-layer architecture of emergency caching networks focusing on data collection and reliable transmission, by leveraging efficient perception and edge caching technologies.","Based on this architecture, we propose a disaster map collection framework that integrates coded caching technologies.","Our framework strategically caches coded fragments of maps across unmanned aerial vehicles (UAVs), fostering collaborative uploading for augmented transmission reliability.","Additionally, we establish a comprehensive probability model to assess the effective recovery area of disaster maps.","Towards the goal of utility maximization, we propose a deep reinforcement learning (DRL) based algorithm that jointly makes decisions about cooperative UAVs selection, bandwidth allocation and coded caching parameter adjustment, accommodating the real-time map updates in a dynamic disaster situation.","Our proposed scheme is more effective than the non-coding caching scheme, as validated by simulation."],"url":"http://arxiv.org/abs/2402.17550v1","category":"cs.NI"}
{"created":"2024-02-27 14:38:47","title":"COCOA: CBT-based Conversational Counseling Agent using Memory Specialized in Cognitive Distortions and Dynamic Prompt","abstract":"The demand for conversational agents that provide mental health care is consistently increasing. In this work, we develop a psychological counseling agent, referred to as CoCoA, that applies Cognitive Behavioral Therapy (CBT) techniques to identify and address cognitive distortions inherent in the client's statements. Specifically, we construct a memory system to efficiently manage information necessary for counseling while extracting high-level insights about the client from their utterances. Additionally, to ensure that the counseling agent generates appropriate responses, we introduce dynamic prompting to flexibly apply CBT techniques and facilitate the appropriate retrieval of information. We conducted dialogues between CoCoA and characters from Character.ai, creating a dataset for evaluation. Then, we asked GPT to evaluate the constructed counseling dataset, and our model demonstrated a statistically significant difference from other models.","sentences":["The demand for conversational agents that provide mental health care is consistently increasing.","In this work, we develop a psychological counseling agent, referred to as CoCoA, that applies Cognitive Behavioral Therapy (CBT) techniques to identify and address cognitive distortions inherent in the client's statements.","Specifically, we construct a memory system to efficiently manage information necessary for counseling while extracting high-level insights about the client from their utterances.","Additionally, to ensure that the counseling agent generates appropriate responses, we introduce dynamic prompting to flexibly apply CBT techniques and facilitate the appropriate retrieval of information.","We conducted dialogues between CoCoA and characters from Character.ai, creating a dataset for evaluation.","Then, we asked GPT to evaluate the constructed counseling dataset, and our model demonstrated a statistically significant difference from other models."],"url":"http://arxiv.org/abs/2402.17546v1","category":"cs.AI"}
{"created":"2024-02-27 14:16:19","title":"Retrieval is Accurate Generation","abstract":"Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from 42.61% to 81.58% in open-ended text generation. Remarkably, our model also achieves the best performance and the lowest latency among several retrieval-augmented baselines. In conclusion, we assert that retrieval is more accurate generation and hope that our work will encourage further research on this new paradigm shift.","sentences":["Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary.","We introduce a novel method that selects context-aware phrases from a collection of supporting documents.","One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents.","To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement.","Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation.","For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from 42.61% to 81.58% in open-ended text generation.","Remarkably, our model also achieves the best performance and the lowest latency among several retrieval-augmented baselines.","In conclusion, we assert that retrieval is more accurate generation and hope that our work will encourage further research on this new paradigm shift."],"url":"http://arxiv.org/abs/2402.17532v1","category":"cs.CL"}
{"created":"2024-02-27 14:14:23","title":"Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides","abstract":"Effective incident management is pivotal for the smooth operation of enterprises-level cloud services. In order to expedite incident mitigation, service teams compile troubleshooting knowledge into Troubleshooting Guides (TSGs) accessible to on-call engineers (OCEs). While automated pipelines are enabled to resolve the most frequent and easy incidents, there still exist complex incidents that require OCEs' intervention. However, TSGs are often unstructured and incomplete, which requires manual interpretation by OCEs, leading to on-call fatigue and decreased productivity, especially among new-hire OCEs. In this work, we propose Nissist which leverages TSGs and incident mitigation histories to provide proactive suggestions, reducing human intervention. Leveraging Large Language Models (LLM), Nissist extracts insights from unstructured TSGs and historical incident mitigation discussions, forming a comprehensive knowledge base. Its multi-agent system design enhances proficiency in precisely discerning user queries, retrieving relevant information, and delivering systematic plans consecutively. Through our user case and experiment, we demonstrate that Nissist significant reduce Time to Mitigate (TTM) in incident mitigation, alleviating operational burdens on OCEs and improving service reliability. Our demo is available at https://aka.ms/nissist_demo.","sentences":["Effective incident management is pivotal for the smooth operation of enterprises-level cloud services.","In order to expedite incident mitigation, service teams compile troubleshooting knowledge into Troubleshooting Guides (TSGs) accessible to on-call engineers (OCEs).","While automated pipelines are enabled to resolve the most frequent and easy incidents, there still exist complex incidents that require OCEs' intervention.","However, TSGs are often unstructured and incomplete, which requires manual interpretation by OCEs, leading to on-call fatigue and decreased productivity, especially among new-hire OCEs.","In this work, we propose Nissist which leverages TSGs and incident mitigation histories to provide proactive suggestions, reducing human intervention.","Leveraging Large Language Models (LLM), Nissist extracts insights from unstructured TSGs and historical incident mitigation discussions, forming a comprehensive knowledge base.","Its multi-agent system design enhances proficiency in precisely discerning user queries, retrieving relevant information, and delivering systematic plans consecutively.","Through our user case and experiment, we demonstrate that Nissist significant reduce Time to Mitigate (TTM) in incident mitigation, alleviating operational burdens on OCEs and improving service reliability.","Our demo is available at https://aka.ms/nissist_demo."],"url":"http://arxiv.org/abs/2402.17531v1","category":"cs.SE"}
{"created":"2024-02-27 14:11:32","title":"Predict the Next Word: <Humans exhibit uncertainty in this task and language models _____>","abstract":"Language models (LMs) are statistical models trained to assign probability to human-generated text. As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgements (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM's ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the 'next word prediction' task. This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertainty. We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty. We also verify the failure of expected calibration error (ECE) to reflect this, and as such, advise the community against relying on it in this setting.","sentences":["Language models (LMs) are statistical models trained to assign probability to human-generated text.","As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well.","This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgements (i.e., human evaluation) or a robust automated proxy (which is non-trivial).","At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context.","We exploit this fact and evaluate the LM's ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the 'next word prediction' task.","This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertainty.","We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty.","We also verify the failure of expected calibration error (ECE) to reflect this, and as such, advise the community against relying on it in this setting."],"url":"http://arxiv.org/abs/2402.17527v1","category":"cs.CL"}
{"created":"2024-02-27 14:00:08","title":"QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations","abstract":"Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal. In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting explanations but also generates more certain counterfactual examples. We showcase the performance of the QUCE method by comparing it with competing methods for both path-based explanations and generative counterfactual examples. The code repository for the QUCE method is available at: https://github.com/jamie-duell/QUCE.","sentences":["Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain.","The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data.","However, as the complexity of DNN models rises, interpretability diminishes.","In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions.","Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal.","In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty.","QUCE not only quantifies uncertainty when presenting explanations but also generates more certain counterfactual examples.","We showcase the performance of the QUCE method by comparing it with competing methods for both path-based explanations and generative counterfactual examples.","The code repository for the QUCE method is available at: https://github.com/jamie-duell/QUCE."],"url":"http://arxiv.org/abs/2402.17516v1","category":"cs.LG"}
{"created":"2024-02-27 13:53:52","title":"Rethinking Mutual Information for Language Conditioned Skill Discovery on Imitation Learning","abstract":"Language-conditioned robot behavior plays a vital role in executing complex tasks by associating human commands or instructions with perception and actions. The ability to compose long-horizon tasks based on unconstrained language instructions necessitates the acquisition of a diverse set of general-purpose skills. However, acquiring inherent primitive skills in a coupled and long-horizon environment without external rewards or human supervision presents significant challenges. In this paper, we evaluate the relationship between skills and language instructions from a mathematical perspective, employing two forms of mutual information within the framework of language-conditioned policy learning. To maximize the mutual information between language and skills in an unsupervised manner, we propose an end-to-end imitation learning approach known as Language Conditioned Skill Discovery (LCSD). Specifically, we utilize vector quantization to learn discrete latent skills and leverage skill sequences of trajectories to reconstruct high-level semantic instructions. Through extensive experiments on language-conditioned robotic navigation and manipulation tasks, encompassing BabyAI, LORel, and CALVIN, we demonstrate the superiority of our method over prior works. Our approach exhibits enhanced generalization capabilities towards unseen tasks, improved skill interpretability, and notably higher rates of task completion success.","sentences":["Language-conditioned robot behavior plays a vital role in executing complex tasks by associating human commands or instructions with perception and actions.","The ability to compose long-horizon tasks based on unconstrained language instructions necessitates the acquisition of a diverse set of general-purpose skills.","However, acquiring inherent primitive skills in a coupled and long-horizon environment without external rewards or human supervision presents significant challenges.","In this paper, we evaluate the relationship between skills and language instructions from a mathematical perspective, employing two forms of mutual information within the framework of language-conditioned policy learning.","To maximize the mutual information between language and skills in an unsupervised manner, we propose an end-to-end imitation learning approach known as Language Conditioned Skill Discovery (LCSD).","Specifically, we utilize vector quantization to learn discrete latent skills and leverage skill sequences of trajectories to reconstruct high-level semantic instructions.","Through extensive experiments on language-conditioned robotic navigation and manipulation tasks, encompassing BabyAI, LORel, and CALVIN, we demonstrate the superiority of our method over prior works.","Our approach exhibits enhanced generalization capabilities towards unseen tasks, improved skill interpretability, and notably higher rates of task completion success."],"url":"http://arxiv.org/abs/2402.17511v1","category":"cs.RO"}
{"created":"2024-02-27 13:50:34","title":"Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning","abstract":"Vision-language models (VLMs) mainly rely on contrastive training to learn general-purpose representations of images and captions. We focus on the situation when one image is associated with several captions, each caption containing both information shared among all captions and unique information per caption about the scene depicted in the image. In such cases, it is unclear whether contrastive losses are sufficient for learning task-optimal representations that contain all the information provided by the captions or whether the contrastive learning setup encourages the learning of a simple shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for vision-language: a training and evaluation framework where we inject synthetic shortcuts into image-text data. We show that contrastive VLMs trained from scratch or fine-tuned with data containing these synthetic shortcuts mainly learn features that represent the shortcut. Hence, contrastive losses are not sufficient to learn task-optimal representations, i.e., representations that contain all task-relevant information shared between the image and associated captions. We examine two methods to reduce shortcut learning in our training and evaluation framework: (i) latent target decoding and (ii) implicit feature modification. We show empirically that both methods improve performance on the evaluation task, but only partly reduce shortcut learning when training and evaluating with our shortcut learning framework. Hence, we show the difficulty and challenge of our shortcut learning framework for contrastive vision-language representation learning.","sentences":["Vision-language models (VLMs) mainly rely on contrastive training to learn general-purpose representations of images and captions.","We focus on the situation when one image is associated with several captions, each caption containing both information shared among all captions and unique information per caption about the scene depicted in the image.","In such cases, it is unclear whether contrastive losses are sufficient for learning task-optimal representations that contain all the information provided by the captions or whether the contrastive learning setup encourages the learning of a simple shortcut that minimizes contrastive loss.","We introduce synthetic shortcuts for vision-language: a training and evaluation framework where we inject synthetic shortcuts into image-text data.","We show that contrastive VLMs trained from scratch or fine-tuned with data containing these synthetic shortcuts mainly learn features that represent the shortcut.","Hence, contrastive losses are not sufficient to learn task-optimal representations, i.e., representations that contain all task-relevant information shared between the image and associated captions.","We examine two methods to reduce shortcut learning in our training and evaluation framework: (i) latent target decoding and (ii) implicit feature modification.","We show empirically that both methods improve performance on the evaluation task, but only partly reduce shortcut learning when training and evaluating with our shortcut learning framework.","Hence, we show the difficulty and challenge of our shortcut learning framework for contrastive vision-language representation learning."],"url":"http://arxiv.org/abs/2402.17510v1","category":"cs.CV"}
{"created":"2024-02-27 13:36:55","title":"Intensive Care as One Big Sequence Modeling Problem","abstract":"Reinforcement Learning in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control. However, previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning. To enable training of foundation models for Healthcare as well as leverage the capabilities of state of the art Transformer architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream. To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous clinical records from MIMIC-IV dataset into a uniform event stream format, train a baseline model and explore its capabilities.","sentences":["Reinforcement Learning in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control.","However, previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning.","To enable training of foundation models for Healthcare as well as leverage the capabilities of state of the art Transformer architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream.","To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous clinical records from MIMIC-IV dataset into a uniform event stream format, train a baseline model and explore its capabilities."],"url":"http://arxiv.org/abs/2402.17501v1","category":"cs.LG"}
{"created":"2024-02-27 13:34:08","title":"Predicting Instability in Complex Oscillator Networks: Limitations and Potentials of Network Measures and Machine Learning","abstract":"A central question of network science is how functional properties of systems arise from their structure. For networked dynamical systems, structure is typically quantified with network measures. A functional property that is of theoretical and practical interest for oscillatory systems is the stability of synchrony to localized perturbations. Recently, Graph Neural Networks (GNNs) have been shown to predict this stability successfully; at the same time, network measures have struggled to paint a clear picture. Here we collect 46 relevant network measures and find that no small subset can reliably predict stability. The performance of GNNs can only be matched by combining all network measures and nodewise machine learning. However, unlike GNNs, this approach fails to extrapolate from network ensembles to several real power grid topologies. This suggests that correlations of network measures and function may be misleading, and that GNNs capture the causal relationship between structure and stability substantially better.","sentences":["A central question of network science is how functional properties of systems arise from their structure.","For networked dynamical systems, structure is typically quantified with network measures.","A functional property that is of theoretical and practical interest for oscillatory systems is the stability of synchrony to localized perturbations.","Recently, Graph Neural Networks (GNNs) have been shown to predict this stability successfully; at the same time, network measures have struggled to paint a clear picture.","Here we collect 46 relevant network measures and find that no small subset can reliably predict stability.","The performance of GNNs can only be matched by combining all network measures and nodewise machine learning.","However, unlike GNNs, this approach fails to extrapolate from network ensembles to several real power grid topologies.","This suggests that correlations of network measures and function may be misleading, and that GNNs capture the causal relationship between structure and stability substantially better."],"url":"http://arxiv.org/abs/2402.17500v1","category":"nlin.AO"}
{"created":"2024-02-27 13:22:47","title":"Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages","abstract":"Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced. Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment. Audios were labeled in valence and arousal dimensions by three non-experts and two experts, which were then combined to obtain a final label per dimension. The experts also provided an extra label corresponding to seven emotion categories. To set a baseline for future investigations using EMOVOME, we implemented emotion recognition models using both speech and audio transcriptions. For speech, we used the standard eGeMAPS feature set and support vector machines, obtaining 49.27% and 44.71% unweighted accuracy for valence and arousal respectively. For text, we fine-tuned a multilingual BERT model and achieved 61.15% and 47.43% unweighted accuracy for valence and arousal respectively. This database will significantly contribute to research on emotion recognition in the wild, while also providing a unique natural and freely accessible resource for Spanish.","sentences":["Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced.","Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment.","Audios were labeled in valence and arousal dimensions by three non-experts and two experts, which were then combined to obtain a final label per dimension.","The experts also provided an extra label corresponding to seven emotion categories.","To set a baseline for future investigations using EMOVOME, we implemented emotion recognition models using both speech and audio transcriptions.","For speech, we used the standard eGeMAPS feature set and support vector machines, obtaining 49.27% and 44.71% unweighted accuracy for valence and arousal respectively.","For text, we fine-tuned a multilingual BERT model and achieved 61.15% and 47.43% unweighted accuracy for valence and arousal respectively.","This database will significantly contribute to research on emotion recognition in the wild, while also providing a unique natural and freely accessible resource for Spanish."],"url":"http://arxiv.org/abs/2402.17496v1","category":"cs.SD"}
{"created":"2024-02-27 13:16:50","title":"The Mechanical Turkness: Tactical Media Art and the Critique of Corporate AI","abstract":"The extensive industrialization of artificial intelligence (AI) since the mid-2010s has increasingly motivated artists to address its economic and sociopolitical consequences. In this chapter, I discuss interrelated art practices that thematize creative agency, crowdsourced labor, and delegated artmaking to reveal the social rootage of AI technologies and underline the productive human roles in their development. I focus on works whose poetic features indicate broader issues of contemporary AI-influenced science, technology, economy, and society. By exploring the conceptual, methodological, and ethical aspects of their effectiveness in disrupting the political regime of corporate AI, I identify several problems that affect their tactical impact and outline potential avenues for tackling the challenges and advancing the field.","sentences":["The extensive industrialization of artificial intelligence (AI) since the mid-2010s has increasingly motivated artists to address its economic and sociopolitical consequences.","In this chapter, I discuss interrelated art practices that thematize creative agency, crowdsourced labor, and delegated artmaking to reveal the social rootage of AI technologies and underline the productive human roles in their development.","I focus on works whose poetic features indicate broader issues of contemporary AI-influenced science, technology, economy, and society.","By exploring the conceptual, methodological, and ethical aspects of their effectiveness in disrupting the political regime of corporate AI, I identify several problems that affect their tactical impact and outline potential avenues for tackling the challenges and advancing the field."],"url":"http://arxiv.org/abs/2402.17490v1","category":"cs.CY"}
{"created":"2024-02-27 13:08:34","title":"Automated Classification of Phonetic Segments in Child Speech Using Raw Ultrasound Imaging","abstract":"Speech sound disorder (SSD) is defined as a persistent impairment in speech sound production leading to reduced speech intelligibility and hindered verbal communication. Early recognition and intervention of children with SSD and timely referral to speech and language therapists (SLTs) for treatment are crucial. Automated detection of speech impairment is regarded as an efficient method for examining and screening large populations. This study focuses on advancing the automatic diagnosis of SSD in early childhood by proposing a technical solution that integrates ultrasound tongue imaging (UTI) with deep-learning models. The introduced FusionNet model combines UTI data with the extracted texture features to classify UTI. The overarching aim is to elevate the accuracy and efficiency of UTI analysis, particularly for classifying speech sounds associated with SSD. This study compared the FusionNet approach with standard deep-learning methodologies, highlighting the excellent improvement results of the FusionNet model in UTI classification and the potential of multi-learning in improving UTI classification in speech therapy clinics.","sentences":["Speech sound disorder (SSD) is defined as a persistent impairment in speech sound production leading to reduced speech intelligibility and hindered verbal communication.","Early recognition and intervention of children with SSD and timely referral to speech and language therapists (SLTs) for treatment are crucial.","Automated detection of speech impairment is regarded as an efficient method for examining and screening large populations.","This study focuses on advancing the automatic diagnosis of SSD in early childhood by proposing a technical solution that integrates ultrasound tongue imaging (UTI) with deep-learning models.","The introduced FusionNet model combines UTI data with the extracted texture features to classify UTI.","The overarching aim is to elevate the accuracy and efficiency of UTI analysis, particularly for classifying speech sounds associated with SSD.","This study compared the FusionNet approach with standard deep-learning methodologies, highlighting the excellent improvement results of the FusionNet model in UTI classification and the potential of multi-learning in improving UTI classification in speech therapy clinics."],"url":"http://arxiv.org/abs/2402.17482v1","category":"cs.SD"}
{"created":"2024-02-27 12:53:15","title":"Fraud Detection with Binding Global and Local Relational Interaction","abstract":"Graph Neural Network has been proved to be effective for fraud detection for its capability to encode node interaction and aggregate features in a holistic view. Recently, Transformer network with great sequence encoding ability, has also outperformed other GNN-based methods in literatures. However, both GNN-based and Transformer-based networks only encode one perspective of the whole graph, while GNN encodes global features and Transformer network encodes local ones. Furthermore, previous works ignored encoding global interaction features of the heterogeneous graph with separate networks, thus leading to suboptimal performance. In this work, we present a novel framework called Relation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds local and global features into a target node. The simple yet effective network applies a modified GAGA module where each transformer layer is followed by a cross-relation aggregation layer, to encode local embeddings and node interactions across different relations. Apart from the Transformer-based network, we further introduce a Relation-Aware GNN module to learn global embeddings, which is later merged into the local embeddings by an attention fusion module and a skip connection. Extensive experiments on two popular public datasets and an industrial dataset demonstrate that RAGFormer achieves the state-of-the-art performance. Substantial analysis experiments validate the effectiveness of each submodule of RAGFormer and its high efficiency in utilizing small-scale data and low hyper-parameter sensitivity.","sentences":["Graph Neural Network has been proved to be effective for fraud detection for its capability to encode node interaction and aggregate features in a holistic view.","Recently, Transformer network with great sequence encoding ability, has also outperformed other GNN-based methods in literatures.","However, both GNN-based and Transformer-based networks only encode one perspective of the whole graph, while GNN encodes global features and Transformer network encodes local ones.","Furthermore, previous works ignored encoding global interaction features of the heterogeneous graph with separate networks, thus leading to suboptimal performance.","In this work, we present a novel framework called Relation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds local and global features into a target node.","The simple yet effective network applies a modified GAGA module where each transformer layer is followed by a cross-relation aggregation layer, to encode local embeddings and node interactions across different relations.","Apart from the Transformer-based network, we further introduce a Relation-Aware GNN module to learn global embeddings, which is later merged into the local embeddings by an attention fusion module and a skip connection.","Extensive experiments on two popular public datasets and an industrial dataset demonstrate that RAGFormer achieves the state-of-the-art performance.","Substantial analysis experiments validate the effectiveness of each submodule of RAGFormer and its high efficiency in utilizing small-scale data and low hyper-parameter sensitivity."],"url":"http://arxiv.org/abs/2402.17472v1","category":"cs.LG"}
{"created":"2024-02-27 12:48:01","title":"Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: a Survey","abstract":"Several adaptations of Transformers models have been developed in various domains since its breakthrough in Natural Language Processing (NLP). This trend has spread into the field of Music Information Retrieval (MIR), including studies processing music data. However, the practice of leveraging NLP tools for symbolic music data is not novel in MIR. Music has been frequently compared to language, as they share several similarities, including sequential representations of text and music. These analogies are also reflected through similar tasks in MIR and NLP. This survey reviews NLP methods applied to symbolic music generation and information retrieval studies following two axes. We first propose an overview of representations of symbolic music adapted from natural language sequential representations. Such representations are designed by considering the specificities of symbolic music. These representations are then processed by models. Such models, possibly originally developed for text and adapted for symbolic music, are trained on various tasks. We describe these models, in particular deep learning models, through different prisms, highlighting music-specialized mechanisms. We finally present a discussion surrounding the effective use of NLP tools for symbolic music data. This includes technical issues regarding NLP methods and fundamental differences between text and music, which may open several doors for further research into more effectively adapting NLP tools to symbolic MIR.","sentences":["Several adaptations of Transformers models have been developed in various domains since its breakthrough in Natural Language Processing (NLP).","This trend has spread into the field of Music Information Retrieval (MIR), including studies processing music data.","However, the practice of leveraging NLP tools for symbolic music data is not novel in MIR.","Music has been frequently compared to language, as they share several similarities, including sequential representations of text and music.","These analogies are also reflected through similar tasks in MIR and NLP.","This survey reviews NLP methods applied to symbolic music generation and information retrieval studies following two axes.","We first propose an overview of representations of symbolic music adapted from natural language sequential representations.","Such representations are designed by considering the specificities of symbolic music.","These representations are then processed by models.","Such models, possibly originally developed for text and adapted for symbolic music, are trained on various tasks.","We describe these models, in particular deep learning models, through different prisms, highlighting music-specialized mechanisms.","We finally present a discussion surrounding the effective use of NLP tools for symbolic music data.","This includes technical issues regarding NLP methods and fundamental differences between text and music, which may open several doors for further research into more effectively adapting NLP tools to symbolic MIR."],"url":"http://arxiv.org/abs/2402.17467v1","category":"cs.IR"}
{"created":"2024-02-27 12:27:51","title":"A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education","abstract":"Cyberbullying harms teenagers' mental health, and teaching them upstanding intervention is crucial. Wizard-of-Oz studies show chatbots can scale up personalized and interactive cyberbullying education, but implementing such chatbots is a challenging and delicate task. We created a no-code chatbot design tool for K-12 teachers. Using large language models and prompt chaining, our tool allows teachers to prototype bespoke dialogue flows and chatbot utterances. In offering this tool, we explore teachers' distinctive needs when designing chatbots to assist their teaching, and how chatbot design tools might better support them. Our findings reveal that teachers welcome the tool enthusiastically. Moreover, they see themselves as playwrights guiding both the students' and the chatbot's behaviors, while allowing for some improvisation. Their goal is to enable students to rehearse both desirable and undesirable reactions to cyberbullying in a safe environment. We discuss the design opportunities LLM-Chains offer for empowering teachers and the research opportunities this work opens up.","sentences":["Cyberbullying harms teenagers' mental health, and teaching them upstanding intervention is crucial.","Wizard-of-Oz studies show chatbots can scale up personalized and interactive cyberbullying education, but implementing such chatbots is a challenging and delicate task.","We created a no-code chatbot design tool for K-12 teachers.","Using large language models and prompt chaining, our tool allows teachers to prototype bespoke dialogue flows and chatbot utterances.","In offering this tool, we explore teachers' distinctive needs when designing chatbots to assist their teaching, and how chatbot design tools might better support them.","Our findings reveal that teachers welcome the tool enthusiastically.","Moreover, they see themselves as playwrights guiding both the students' and the chatbot's behaviors, while allowing for some improvisation.","Their goal is to enable students to rehearse both desirable and undesirable reactions to cyberbullying in a safe environment.","We discuss the design opportunities LLM-Chains offer for empowering teachers and the research opportunities this work opens up."],"url":"http://arxiv.org/abs/2402.17456v1","category":"cs.HC"}
{"created":"2024-02-27 12:11:56","title":"Conformal Shield: A Novel Adversarial Attack Detection Framework for Automatic Modulation Classification","abstract":"Deep learning algorithms have become an essential component in the field of cognitive radio, especially playing a pivotal role in automatic modulation classification. However, Deep learning also present risks and vulnerabilities. Despite their outstanding classification performance, they exhibit fragility when confronted with meticulously crafted adversarial examples, posing potential risks to the reliability of modulation recognition results. Addressing this issue, this letter pioneers the development of an intelligent modulation classification framework based on conformal theory, named the Conformal Shield, aimed at detecting the presence of adversarial examples in unknown signals and assessing the reliability of recognition results. Utilizing conformal mapping from statistical learning theory, introduces a custom-designed Inconsistency Soft-solution Set, enabling multiple validity assessments of the recognition outcomes. Experimental results demonstrate that the Conformal Shield maintains robust detection performance against a variety of typical adversarial sample attacks in the received signals under different perturbation-to-signal power ratio conditions.","sentences":["Deep learning algorithms have become an essential component in the field of cognitive radio, especially playing a pivotal role in automatic modulation classification.","However, Deep learning also present risks and vulnerabilities.","Despite their outstanding classification performance, they exhibit fragility when confronted with meticulously crafted adversarial examples, posing potential risks to the reliability of modulation recognition results.","Addressing this issue, this letter pioneers the development of an intelligent modulation classification framework based on conformal theory, named the Conformal Shield, aimed at detecting the presence of adversarial examples in unknown signals and assessing the reliability of recognition results.","Utilizing conformal mapping from statistical learning theory, introduces a custom-designed Inconsistency Soft-solution Set, enabling multiple validity assessments of the recognition outcomes.","Experimental results demonstrate that the Conformal Shield maintains robust detection performance against a variety of typical adversarial sample attacks in the received signals under different perturbation-to-signal power ratio conditions."],"url":"http://arxiv.org/abs/2402.17450v1","category":"eess.SP"}
{"created":"2024-02-27 12:03:56","title":"Deep Learning Based Named Entity Recognition Models for Recipes","abstract":"Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. Recipes are cultural capsules transmitted across generations via unstructured text. Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation. Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels. Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER. Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity to create the machine-annotated dataset. A thorough investigation of NER approaches on these three datasets involving statistical, fine-tuning of deep learning-based language models and few-shot prompting on large language models (LLMs) provides deep insights. We conclude that few-shot prompting on LLMs has abysmal performance, whereas the fine-tuned spaCy-transformer emerges as the best model with macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated, augmented, and machine-annotated datasets, respectively.","sentences":["Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability.","Recipes are cultural capsules transmitted across generations via unstructured text.","Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation.","Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels.","Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively.","Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER.","Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity to create the machine-annotated dataset.","A thorough investigation of NER approaches on these three datasets involving statistical, fine-tuning of deep learning-based language models and few-shot prompting on large language models (LLMs) provides deep insights.","We conclude that few-shot prompting on LLMs has abysmal performance, whereas the fine-tuned spaCy-transformer emerges as the best model with macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated, augmented, and machine-annotated datasets, respectively."],"url":"http://arxiv.org/abs/2402.17447v1","category":"cs.CL"}
{"created":"2024-02-27 12:00:08","title":"Constraining the average magnetic field in galaxy clusters with current and upcoming CMB surveys","abstract":"Galaxy clusters that host radio halos indicate the presence of population(s) of non-thermal electrons. These electrons can scatter low-energy photons of the Cosmic Microwave Background, resulting in the non-thermal Sunyaev-Zeldovich (ntSZ) effect. We measure the average ntSZ signal from 62 radio-halo hosting clusters using the $Planck$ multi-frequency all-sky maps. We find no direct evidence of the ntSZ signal in the $Planck$ data. Combining the upper limits on the non-thermal electron density with the average measured synchrotron power collected from the literature, we place lower limits on the average magnetic field strength in our sample. The lower limit on the volume-averaged magnetic field is $0.1-0.01\\,\\mu$G, depending on the assumed power-law distribution of electron energies. We further explore the potential improvement of these constraints from the upcoming Simons Observatory and Fred Young Submillimeter Telescope (FYST) of the CCAT-prime collaboration. We find that combining these two experiments, the constraints will improve by a factor of $3-4$, which can be sufficient to rule out some power-law models.","sentences":["Galaxy clusters that host radio halos indicate the presence of population(s) of non-thermal electrons.","These electrons can scatter low-energy photons of the Cosmic Microwave Background, resulting in the non-thermal Sunyaev-Zeldovich (ntSZ) effect.","We measure the average ntSZ signal from 62 radio-halo hosting clusters using the $Planck$ multi-frequency all-sky maps.","We find no direct evidence of the ntSZ signal in the $Planck$ data.","Combining the upper limits on the non-thermal electron density with the average measured synchrotron power collected from the literature, we place lower limits on the average magnetic field strength in our sample.","The lower limit on the volume-averaged magnetic field is $0.1-0.01\\,\\mu$G, depending on the assumed power-law distribution of electron energies.","We further explore the potential improvement of these constraints from the upcoming Simons Observatory and Fred Young Submillimeter Telescope (FYST) of the CCAT-prime collaboration.","We find that combining these two experiments, the constraints will improve by a factor of $3-4$, which can be sufficient to rule out some power-law models."],"url":"http://arxiv.org/abs/2402.17445v1","category":"astro-ph.CO"}
{"created":"2024-02-27 11:57:28","title":"Ansible Lightspeed: A Code Generation Service for IT Automation","abstract":"The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been released, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for IT automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Red Hat Ansible Lightspeed with IBM Watson Code Assistant, further referred to as Ansible Lightspeed, is an LLM-based service designed explicitly for natural language to Ansible code generation.   In this paper, we describe the design and implementation of the Ansible Lightspeed service and analyze feedback from thousands of real users. We examine diverse performance indicators, classified according to both immediate and extended utilization patterns along with user sentiments. The analysis shows that the user acceptance rate of Ansible Lightspeed suggestions is higher than comparable tools that are more general and not specific to a programming language. This remains true even after we use much more stringent criteria for what is considered an accepted model suggestion, discarding suggestions which were heavily edited after being accepted. The relatively high acceptance rate results in higher-than-expected user retention and generally positive user feedback. This paper provides insights on how a comparatively small, dedicated model performs on a domain-specific language and more importantly, how it is received by users.","sentences":["The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity.","Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs.","Although many such tools have been released, almost all of them focus on general-purpose programming languages.","Domain-specific languages, such as those crucial for IT automation, have not received much attention.","Ansible is one such YAML-based IT automation-specific language.","Red Hat Ansible Lightspeed with IBM Watson Code Assistant, further referred to as Ansible Lightspeed, is an LLM-based service designed explicitly for natural language to Ansible code generation.   ","In this paper, we describe the design and implementation of the Ansible Lightspeed service and analyze feedback from thousands of real users.","We examine diverse performance indicators, classified according to both immediate and extended utilization patterns along with user sentiments.","The analysis shows that the user acceptance rate of Ansible Lightspeed suggestions is higher than comparable tools that are more general and not specific to a programming language.","This remains true even after we use much more stringent criteria for what is considered an accepted model suggestion, discarding suggestions which were heavily edited after being accepted.","The relatively high acceptance rate results in higher-than-expected user retention and generally positive user feedback.","This paper provides insights on how a comparatively small, dedicated model performs on a domain-specific language and more importantly, how it is received by users."],"url":"http://arxiv.org/abs/2402.17442v1","category":"cs.SE"}
{"created":"2024-02-27 11:50:05","title":"Exploiting Emotion-Semantic Correlations for Empathetic Response Generation","abstract":"Empathetic response generation aims to generate empathetic responses by understanding the speaker's emotional feelings from the language of dialogue. Recent methods capture emotional words in the language of communicators and construct them as static vectors to perceive nuanced emotions. However, linguistic research has shown that emotional words in language are dynamic and have correlations with other grammar semantic roles, i.e., words with semantic meanings, in grammar. Previous methods overlook these two characteristics, which easily lead to misunderstandings of emotions and neglect of key semantics. To address this issue, we propose a dynamical Emotion-Semantic Correlation Model (ESCM) for empathetic dialogue generation tasks. ESCM constructs dynamic emotion-semantic vectors through the interaction of context and emotions. We introduce dependency trees to reflect the correlations between emotions and semantics. Based on dynamic emotion-semantic vectors and dependency trees, we propose a dynamic correlation graph convolutional network to guide the model in learning context meanings in dialogue and generating empathetic responses. Experimental results on the EMPATHETIC-DIALOGUES dataset show that ESCM understands semantics and emotions more accurately and expresses fluent and informative empathetic responses. Our analysis results also indicate that the correlations between emotions and semantics are frequently used in dialogues, which is of great significance for empathetic perception and expression.","sentences":["Empathetic response generation aims to generate empathetic responses by understanding the speaker's emotional feelings from the language of dialogue.","Recent methods capture emotional words in the language of communicators and construct them as static vectors to perceive nuanced emotions.","However, linguistic research has shown that emotional words in language are dynamic and have correlations with other grammar semantic roles, i.e., words with semantic meanings, in grammar.","Previous methods overlook these two characteristics, which easily lead to misunderstandings of emotions and neglect of key semantics.","To address this issue, we propose a dynamical Emotion-Semantic Correlation Model (ESCM) for empathetic dialogue generation tasks.","ESCM constructs dynamic emotion-semantic vectors through the interaction of context and emotions.","We introduce dependency trees to reflect the correlations between emotions and semantics.","Based on dynamic emotion-semantic vectors and dependency trees, we propose a dynamic correlation graph convolutional network to guide the model in learning context meanings in dialogue and generating empathetic responses.","Experimental results on the EMPATHETIC-DIALOGUES dataset show that ESCM understands semantics and emotions more accurately and expresses fluent and informative empathetic responses.","Our analysis results also indicate that the correlations between emotions and semantics are frequently used in dialogues, which is of great significance for empathetic perception and expression."],"url":"http://arxiv.org/abs/2402.17437v1","category":"cs.CL"}
{"created":"2024-02-27 11:47:46","title":"Wykorzystanie rekonfigurowalnych matryc antenowych wraz z informacj\u0105 kontekstow\u0105","abstract":"Reconfigurable intelligent surfaces can be successfully used to control the radio environment. Simple control of the reflection angle of the signal from the surface allows maximization or minimization of the received power in specific places. The paper presents simulations where it is possible to receive a signal in a place where it was not possible, to detect the occupancy of the spectrum in a place where the sensor was unable to make correct detection or to minimize interference in a specific receiver.","sentences":["Reconfigurable intelligent surfaces can be successfully used to control the radio environment.","Simple control of the reflection angle of the signal from the surface allows maximization or minimization of the received power in specific places.","The paper presents simulations where it is possible to receive a signal in a place where it was not possible, to detect the occupancy of the spectrum in a place where the sensor was unable to make correct detection or to minimize interference in a specific receiver."],"url":"http://arxiv.org/abs/2402.17436v1","category":"cs.NI"}
{"created":"2024-02-27 11:43:41","title":"The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns","abstract":"Artificial intelligence is continuously seeking novel challenges and benchmarks to effectively measure performance and to advance the state-of-the-art. In this paper we introduce KANDY, a benchmarking framework that can be used to generate a variety of learning and reasoning tasks inspired by Kandinsky patterns. By creating curricula of binary classification tasks with increasing complexity and with sparse supervisions, KANDY can be used to implement benchmarks for continual and semi-supervised learning, with a specific focus on symbol compositionality. Classification rules are also provided in the ground truth to enable analysis of interpretable solutions. Together with the benchmark generation pipeline, we release two curricula, an easier and a harder one, that we propose as new challenges for the research community. With a thorough experimental evaluation, we show how both state-of-the-art neural models and purely symbolic approaches struggle with solving most of the tasks, thus calling for the application of advanced neuro-symbolic methods trained over time.","sentences":["Artificial intelligence is continuously seeking novel challenges and benchmarks to effectively measure performance and to advance the state-of-the-art.","In this paper we introduce KANDY, a benchmarking framework that can be used to generate a variety of learning and reasoning tasks inspired by Kandinsky patterns.","By creating curricula of binary classification tasks with increasing complexity and with sparse supervisions, KANDY can be used to implement benchmarks for continual and semi-supervised learning, with a specific focus on symbol compositionality.","Classification rules are also provided in the ground truth to enable analysis of interpretable solutions.","Together with the benchmark generation pipeline, we release two curricula, an easier and a harder one, that we propose as new challenges for the research community.","With a thorough experimental evaluation, we show how both state-of-the-art neural models and purely symbolic approaches struggle with solving most of the tasks, thus calling for the application of advanced neuro-symbolic methods trained over time."],"url":"http://arxiv.org/abs/2402.17431v1","category":"cs.AI"}
{"created":"2024-02-27 11:32:14","title":"Reinforced In-Context Black-Box Optimization","abstract":"Black-Box Optimization (BBO) has found successful applications in many fields of science and engineering. Recently, there has been a growing interest in meta-learning particular components of BBO algorithms to speed up optimization and get rid of tedious hand-crafted heuristics. As an extension, learning the entire algorithm from data requires the least labor from experts and can provide the most flexibility. In this paper, we propose RIBBO, a method to reinforce-learn a BBO algorithm from offline data in an end-to-end fashion. RIBBO employs expressive sequence models to learn the optimization histories produced by multiple behavior algorithms and tasks, leveraging the in-context learning ability of large models to extract task information and make decisions accordingly. Central to our method is to augment the optimization histories with regret-to-go tokens, which are designed to represent the performance of an algorithm based on cumulative regret of the histories. The integration of regret-to-go tokens enables RIBBO to automatically generate sequences of query points that satisfy the user-desired regret, which is verified by its universally good empirical performance on diverse problems, including BBOB functions, hyper-parameter optimization and robot control problems.","sentences":["Black-Box Optimization (BBO) has found successful applications in many fields of science and engineering.","Recently, there has been a growing interest in meta-learning particular components of BBO algorithms to speed up optimization and get rid of tedious hand-crafted heuristics.","As an extension, learning the entire algorithm from data requires the least labor from experts and can provide the most flexibility.","In this paper, we propose RIBBO, a method to reinforce-learn a BBO algorithm from offline data in an end-to-end fashion.","RIBBO employs expressive sequence models to learn the optimization histories produced by multiple behavior algorithms and tasks, leveraging the in-context learning ability of large models to extract task information and make decisions accordingly.","Central to our method is to augment the optimization histories with regret-to-go tokens, which are designed to represent the performance of an algorithm based on cumulative regret of the histories.","The integration of regret-to-go tokens enables RIBBO to automatically generate sequences of query points that satisfy the user-desired regret, which is verified by its universally good empirical performance on diverse problems, including BBOB functions, hyper-parameter optimization and robot control problems."],"url":"http://arxiv.org/abs/2402.17423v1","category":"cs.LG"}
{"created":"2024-02-27 11:23:39","title":"PANDAS: Prototype-based Novel Class Discovery and Detection","abstract":"Object detectors are typically trained once and for all on a fixed set of classes. However, this closed-world assumption is unrealistic in practice, as new classes will inevitably emerge after the detector is deployed in the wild. In this work, we look at ways to extend a detector trained for a set of base classes so it can i) spot the presence of novel classes, and ii) automatically enrich its repertoire to be able to detect those newly discovered classes together with the base ones. We propose PANDAS, a method for novel class discovery and detection. It discovers clusters representing novel classes from unlabeled data, and represents old and new classes with prototypes. During inference, a distance-based classifier uses these prototypes to assign a label to each detected object instance. The simplicity of our method makes it widely applicable. We experimentally demonstrate the effectiveness of PANDAS on the VOC 2012 and COCO-to-LVIS benchmarks. It performs favorably against the state of the art for this task while being computationally more affordable.","sentences":["Object detectors are typically trained once and for all on a fixed set of classes.","However, this closed-world assumption is unrealistic in practice, as new classes will inevitably emerge after the detector is deployed in the wild.","In this work, we look at ways to extend a detector trained for a set of base classes so it can i) spot the presence of novel classes, and ii) automatically enrich its repertoire to be able to detect those newly discovered classes together with the base ones.","We propose PANDAS, a method for novel class discovery and detection.","It discovers clusters representing novel classes from unlabeled data, and represents old and new classes with prototypes.","During inference, a distance-based classifier uses these prototypes to assign a label to each detected object instance.","The simplicity of our method makes it widely applicable.","We experimentally demonstrate the effectiveness of PANDAS on the VOC 2012 and COCO-to-LVIS benchmarks.","It performs favorably against the state of the art for this task while being computationally more affordable."],"url":"http://arxiv.org/abs/2402.17420v1","category":"cs.CV"}
{"created":"2024-02-27 11:01:58","title":"A novel image space formalism of Fourier domain interpolation neural networks for noise propagation analysis","abstract":"Purpose: To develop an image space formalism of multi-layer convolutional neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions and analytically estimate noise propagation during CNN inference. Theory and Methods: Nonlinear activations in the Fourier domain (also known as k-space) using complex-valued Rectifier Linear Units are expressed as elementwise multiplication with activation masks. This operation is transformed into a convolution in the image space. After network training in k-space, this approach provides an algebraic expression for the derivative of the reconstructed image with respect to the aliased coil images, which serve as the input tensors to the network in the image space. This allows the variance in the network inference to be estimated analytically and to be used to describe noise characteristics. Monte-Carlo simulations and numerical approaches based on auto-differentiation were used for validation. The framework was tested on retrospectively undersampled invivo brain images. Results: Inferences conducted in the image domain are quasi-identical to inferences in the k-space, underlined by corresponding quantitative metrics. Noise variance maps obtained from the analytical expression correspond with those obtained via Monte-Carlo simulations, as well as via an auto-differentiation approach. The noise resilience is well characterized, as in the case of classical Parallel Imaging. Komolgorov-Smirnov tests demonstrate Gaussian distributions of voxel magnitudes in variance maps obtained via Monte-Carlo simulations. Conclusion: The quasi-equivalent image space formalism for neural networks for k-space interpolation enables fast and accurate description of the noise characteristics during CNN inference, analogous to geometry-factor maps in traditional parallel imaging methods.","sentences":["Purpose: To develop an image space formalism of multi-layer convolutional neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions and analytically estimate noise propagation during CNN inference.","Theory and Methods: Nonlinear activations in the Fourier domain (also known as k-space) using complex-valued Rectifier Linear Units are expressed as elementwise multiplication with activation masks.","This operation is transformed into a convolution in the image space.","After network training in k-space, this approach provides an algebraic expression for the derivative of the reconstructed image with respect to the aliased coil images, which serve as the input tensors to the network in the image space.","This allows the variance in the network inference to be estimated analytically and to be used to describe noise characteristics.","Monte-Carlo simulations and numerical approaches based on auto-differentiation were used for validation.","The framework was tested on retrospectively undersampled invivo brain images.","Results:","Inferences conducted in the image domain are quasi-identical to inferences in the k-space, underlined by corresponding quantitative metrics.","Noise variance maps obtained from the analytical expression correspond with those obtained via Monte-Carlo simulations, as well as via an auto-differentiation approach.","The noise resilience is well characterized, as in the case of classical Parallel Imaging.","Komolgorov-Smirnov tests demonstrate Gaussian distributions of voxel magnitudes in variance maps obtained via Monte-Carlo simulations.","Conclusion: The quasi-equivalent image space formalism for neural networks for k-space interpolation enables fast and accurate description of the noise characteristics during CNN inference, analogous to geometry-factor maps in traditional parallel imaging methods."],"url":"http://arxiv.org/abs/2402.17410v1","category":"cs.CV"}
{"created":"2024-02-27 10:57:07","title":"A Neural Rewriting System to Solve Algorithmic Problems","abstract":"Modern neural network architectures still struggle to learn algorithmic procedures that require to systematically apply compositional rules to solve out-of-distribution problem instances. In this work, we propose an original approach to learn algorithmic tasks inspired by rewriting systems, a classic framework in symbolic artificial intelligence. We show that a rewriting system can be implemented as a neural architecture composed by specialized modules: the Selector identifies the target sub-expression to process, the Solver simplifies the sub-expression by computing the corresponding result, and the Combiner produces a new version of the original expression by replacing the sub-expression with the solution provided. We evaluate our model on three types of algorithmic tasks that require simplifying symbolic formulas involving lists, arithmetic, and algebraic expressions. We test the extrapolation capabilities of the proposed architecture using formulas involving a higher number of operands and nesting levels than those seen during training, and we benchmark its performance against the Neural Data Router, a recent model specialized for systematic generalization, and a state-of-the-art large language model (GPT-4) probed with advanced prompting strategies.","sentences":["Modern neural network architectures still struggle to learn algorithmic procedures that require to systematically apply compositional rules to solve out-of-distribution problem instances.","In this work, we propose an original approach to learn algorithmic tasks inspired by rewriting systems, a classic framework in symbolic artificial intelligence.","We show that a rewriting system can be implemented as a neural architecture composed by specialized modules: the Selector identifies the target sub-expression to process, the Solver simplifies the sub-expression by computing the corresponding result, and the Combiner produces a new version of the original expression by replacing the sub-expression with the solution provided.","We evaluate our model on three types of algorithmic tasks that require simplifying symbolic formulas involving lists, arithmetic, and algebraic expressions.","We test the extrapolation capabilities of the proposed architecture using formulas involving a higher number of operands and nesting levels than those seen during training, and we benchmark its performance against the Neural Data Router, a recent model specialized for systematic generalization, and a state-of-the-art large language model (GPT-4) probed with advanced prompting strategies."],"url":"http://arxiv.org/abs/2402.17407v1","category":"cs.NE"}
{"created":"2024-02-27 10:55:07","title":"LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning","abstract":"Visual Prompt Tuning (VPT) techniques have gained prominence for their capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual tasks using specialized learnable tokens termed as prompts. Contemporary VPT methodologies, especially when employed with self-supervised vision transformers, often default to the introduction of new learnable prompts or gated prompt tokens predominantly sourced from the model's previous block. A pivotal oversight in such approaches is their failure to harness the potential of long-range previous blocks as sources of prompts within each self-supervised ViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning (LSPT) - a revolutionary approach to visual representation learning. Drawing inspiration from the intricacies of the human brain, LSPT ingeniously incorporates long-term gated prompts. This feature serves as temporal coding, curbing the risk of forgetting parameters acquired from earlier blocks. Further enhancing its prowess, LSPT brings into play patch tokens, serving as spatial coding. This is strategically designed to perpetually amass class-conscious features, thereby fortifying the model's prowess in distinguishing and identifying visual categories. To validate the efficacy of our proposed method, we engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks. Our empirical findings underscore the superiority of LSPT, showcasing its ability to set new benchmarks in visual prompt tuning performance.","sentences":["Visual Prompt Tuning (VPT) techniques have gained prominence for their capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual tasks using specialized learnable tokens termed as prompts.","Contemporary VPT methodologies, especially when employed with self-supervised vision transformers, often default to the introduction of new learnable prompts or gated prompt tokens predominantly sourced from the model's previous block.","A pivotal oversight in such approaches is their failure to harness the potential of long-range previous blocks as sources of prompts within each self-supervised ViT.","To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning (LSPT) - a revolutionary approach to visual representation learning.","Drawing inspiration from the intricacies of the human brain, LSPT ingeniously incorporates long-term gated prompts.","This feature serves as temporal coding, curbing the risk of forgetting parameters acquired from earlier blocks.","Further enhancing its prowess, LSPT brings into play patch tokens, serving as spatial coding.","This is strategically designed to perpetually amass class-conscious features, thereby fortifying the model's prowess in distinguishing and identifying visual categories.","To validate the efficacy of our proposed method, we engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks.","Our empirical findings underscore the superiority of LSPT, showcasing its ability to set new benchmarks in visual prompt tuning performance."],"url":"http://arxiv.org/abs/2402.17406v1","category":"cs.CV"}
{"created":"2024-02-27 10:46:36","title":"A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)","abstract":"The paper proposes the Quantum-SMOTE method, a novel solution that uses quantum computing techniques to solve the prevalent problem of class imbalance in machine learning datasets. Quantum-SMOTE, inspired by the Synthetic Minority Oversampling Technique (SMOTE), generates synthetic data points using quantum processes such as swap tests and quantum rotation. The process varies from the conventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean distances, enabling synthetic instances to be generated from minority class data points without relying on neighbor proximity. The algorithm asserts greater control over the synthetic data generation process by introducing hyperparameters such as rotation angle, minority percentage, and splitting factor, which allow for customization to specific dataset requirements. The approach is tested on a public dataset of TelecomChurn and evaluated alongside two prominent classification algorithms, Random Forest and Logistic Regression, to determine its impact along with varying proportions of synthetic data.","sentences":["The paper proposes the Quantum-SMOTE method, a novel solution that uses quantum computing techniques to solve the prevalent problem of class imbalance in machine learning datasets.","Quantum-SMOTE, inspired by the Synthetic Minority Oversampling Technique (SMOTE), generates synthetic data points using quantum processes such as swap tests and quantum rotation.","The process varies from the conventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean distances, enabling synthetic instances to be generated from minority class data points without relying on neighbor proximity.","The algorithm asserts greater control over the synthetic data generation process by introducing hyperparameters such as rotation angle, minority percentage, and splitting factor, which allow for customization to specific dataset requirements.","The approach is tested on a public dataset of TelecomChurn and evaluated alongside two prominent classification algorithms, Random Forest and Logistic Regression, to determine its impact along with varying proportions of synthetic data."],"url":"http://arxiv.org/abs/2402.17398v1","category":"quant-ph"}
{"created":"2024-02-27 10:44:52","title":"Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies","abstract":"Large Language Models (LLMs) have revolutionized the field of Natural Language Processing thanks to their ability to reuse knowledge acquired on massive text corpora on a wide variety of downstream tasks, with minimal (if any) tuning steps. At the same time, it has been repeatedly shown that LLMs lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution. In this work, we offer a systematic benchmarking of GPT-4, one of the most advanced LLMs available, on three algorithmic tasks characterized by the possibility to control the problem difficulty with two parameters. We compare the performance of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the Transformer-Encoder architecture recently introduced to solve similar tasks, the Neural Data Router. We find that the deployment of advanced prompting techniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating that state-of-the-art LLMs constitute a very strong baseline also in challenging tasks that require systematic generalization.","sentences":["Large Language Models (LLMs) have revolutionized the field of Natural Language Processing thanks to their ability to reuse knowledge acquired on massive text corpora on a wide variety of downstream tasks, with minimal (if any) tuning steps.","At the same time, it has been repeatedly shown that LLMs lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution.","In this work, we offer a systematic benchmarking of GPT-4, one of the most advanced LLMs available, on three algorithmic tasks characterized by the possibility to control the problem difficulty with two parameters.","We compare the performance of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the Transformer-Encoder architecture recently introduced to solve similar tasks, the Neural Data Router.","We find that the deployment of advanced prompting techniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating that state-of-the-art LLMs constitute a very strong baseline also in challenging tasks that require systematic generalization."],"url":"http://arxiv.org/abs/2402.17396v1","category":"cs.CL"}
{"created":"2024-02-27 10:41:39","title":"A Survey of Network Protocol Fuzzing: Model, Techniques and Directions","abstract":"As one of the most successful and effective software testing techniques in recent years, fuzz testing has uncovered numerous bugs and vulnerabilities in modern software, including network protocol software. In contrast to other fuzzing targets, network protocol software exhibits its distinct characteristics and challenges, introducing a plethora of research questions that need to be addressed in the design and implementation of network protocol fuzzers. While some research work has evaluated and systematized the knowledge of general fuzzing techniques at a high level, there is a lack of similar analysis and summarization for fuzzing research specific to network protocols. This paper offers a comprehensive exposition of network protocol software's fuzzing-related features and conducts a systematic review of some representative advancements in network protocol fuzzing since its inception. We summarize state-of-the-art strategies and solutions in various aspects, propose a unified protocol fuzzing process model, and introduce the techniques involved in each stage of the model. At the same time, this paper also summarizes the promising research directions in the landscape of protocol fuzzing to foster exploration within the community for more efficient and intelligent modern network protocol fuzzing techniques.","sentences":["As one of the most successful and effective software testing techniques in recent years, fuzz testing has uncovered numerous bugs and vulnerabilities in modern software, including network protocol software.","In contrast to other fuzzing targets, network protocol software exhibits its distinct characteristics and challenges, introducing a plethora of research questions that need to be addressed in the design and implementation of network protocol fuzzers.","While some research work has evaluated and systematized the knowledge of general fuzzing techniques at a high level, there is a lack of similar analysis and summarization for fuzzing research specific to network protocols.","This paper offers a comprehensive exposition of network protocol software's fuzzing-related features and conducts a systematic review of some representative advancements in network protocol fuzzing since its inception.","We summarize state-of-the-art strategies and solutions in various aspects, propose a unified protocol fuzzing process model, and introduce the techniques involved in each stage of the model.","At the same time, this paper also summarizes the promising research directions in the landscape of protocol fuzzing to foster exploration within the community for more efficient and intelligent modern network protocol fuzzing techniques."],"url":"http://arxiv.org/abs/2402.17394v1","category":"cs.NI"}
{"created":"2024-02-27 10:40:15","title":"Designing Chatbots to Support Victims and Survivors of Domestic Abuse","abstract":"Objective: Domestic abuse cases have risen significantly over the last four years, in part due to the COVID-19 pandemic and the challenges for victims and survivors in accessing support. In this study, we investigate the role that chatbots - Artificial Intelligence (AI) and rule-based - may play in supporting victims/survivors in situations such as these or where direct access to help is limited. Methods: Interviews were conducted with experts working in domestic abuse support services and organizations (e.g., charities, law enforcement) and the content of websites of related support-service providers was collected. Thematic content analysis was then applied to assess and extract insights from the interview data and the content on victim-support websites. We also reviewed pertinent chatbot literature to reflect on studies that may inform design principles and interaction patterns for agents used to support victims/survivors. Results: From our analysis, we outlined a set of design considerations/practices for chatbots that consider potential use cases and target groups, dialog structure, personality traits that might be useful for chatbots to possess, and finally, safety and privacy issues that should be addressed. Of particular note are situations where AI systems (e.g., ChatGPT, CoPilot, Gemini) are not recommended for use, the value of conveying emotional support, the importance of transparency, and the need for a safe and confidential space. Conclusion: It is our hope that these considerations/practices will stimulate debate among chatbots and AI developers and service providers and - for situations where chatbots are deemed appropriate for use - inspire efficient use of chatbots in the support of survivors of domestic abuse.","sentences":["Objective: Domestic abuse cases have risen significantly over the last four years, in part due to the COVID-19 pandemic and the challenges for victims and survivors in accessing support.","In this study, we investigate the role that chatbots - Artificial Intelligence (AI) and rule-based - may play in supporting victims/survivors in situations such as these or where direct access to help is limited.","Methods: Interviews were conducted with experts working in domestic abuse support services and organizations (e.g., charities, law enforcement) and the content of websites of related support-service providers was collected.","Thematic content analysis was then applied to assess and extract insights from the interview data and the content on victim-support websites.","We also reviewed pertinent chatbot literature to reflect on studies that may inform design principles and interaction patterns for agents used to support victims/survivors.","Results: From our analysis, we outlined a set of design considerations/practices for chatbots that consider potential use cases and target groups, dialog structure, personality traits that might be useful for chatbots to possess, and finally, safety and privacy issues that should be addressed.","Of particular note are situations where AI systems (e.g., ChatGPT, CoPilot, Gemini) are not recommended for use, the value of conveying emotional support, the importance of transparency, and the need for a safe and confidential space.","Conclusion: It is our hope that these considerations/practices will stimulate debate among chatbots and AI developers and service providers and - for situations where chatbots are deemed appropriate for use - inspire efficient use of chatbots in the support of survivors of domestic abuse."],"url":"http://arxiv.org/abs/2402.17393v1","category":"cs.CY"}
{"created":"2024-02-27 10:38:37","title":"Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and Humans","abstract":"Nowadays, technology is rapidly advancing: bots are writing comments, articles, and reviews. Due to this fact, it is crucial to know if the text was written by a human or by a bot. This paper focuses on comparing structures of the coarse-grained partitions of semantic paths for human-written and bot-generated texts. We compare the clusterizations of datasets of n-grams from literary texts and texts generated by several bots. The hypothesis is that the structures and clusterizations are different. Our research supports the hypothesis. As the semantic structure may be different for different languages, we investigate Russian, English, German, and Vietnamese languages.","sentences":["Nowadays, technology is rapidly advancing: bots are writing comments, articles, and reviews.","Due to this fact, it is crucial to know if the text was written by a human or by a bot.","This paper focuses on comparing structures of the coarse-grained partitions of semantic paths for human-written and bot-generated texts.","We compare the clusterizations of datasets of n-grams from literary texts and texts generated by several bots.","The hypothesis is that the structures and clusterizations are different.","Our research supports the hypothesis.","As the semantic structure may be different for different languages, we investigate Russian, English, German, and Vietnamese languages."],"url":"http://arxiv.org/abs/2402.17392v1","category":"cs.CL"}
{"created":"2024-02-27 10:31:00","title":"FairBelief - Assessing Harmful Beliefs in Language Models","abstract":"Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real-world applications without careful fairness auditing. This paper proposes FairBelief, an analytical approach to capture and assess beliefs, i.e., propositions that an LM may embed with different degrees of confidence and that covertly influence its predictions. With FairBelief, we leverage prompting to study the behavior of several state-of-the-art LMs across different previously neglected axes, such as model scale and likelihood, assessing predictions on a fairness dataset specifically designed to quantify LMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative assessment of the beliefs emitted by the models. We apply FairBelief to English LMs, revealing that, although these architectures enable high performances on diverse natural language processing tasks, they show hurtful beliefs about specific genders. Interestingly, training procedure and dataset, model scale, and architecture induce beliefs of different degrees of hurtfulness.","sentences":["Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real-world applications without careful fairness auditing.","This paper proposes FairBelief, an analytical approach to capture and assess beliefs, i.e., propositions that an LM may embed with different degrees of confidence and that covertly influence its predictions.","With FairBelief, we leverage prompting to study the behavior of several state-of-the-art LMs across different previously neglected axes, such as model scale and likelihood, assessing predictions on a fairness dataset specifically designed to quantify LMs' outputs' hurtfulness.","Finally, we conclude with an in-depth qualitative assessment of the beliefs emitted by the models.","We apply FairBelief to English LMs, revealing that, although these architectures enable high performances on diverse natural language processing tasks, they show hurtful beliefs about specific genders.","Interestingly, training procedure and dataset, model scale, and architecture induce beliefs of different degrees of hurtfulness."],"url":"http://arxiv.org/abs/2402.17389v1","category":"cs.CL"}
{"created":"2024-02-27 10:26:25","title":"A case study of sending graph neural networks back to the test bench for applications in high-energy particle physics","abstract":"In high-energy particle collisions, the primary collision products usually decay further resulting in tree-like, hierarchical structures with a priori unknown multiplicity. At the stable-particle level all decay products of a collision form permutation invariant sets of final state objects. The analogy to mathematical graphs gives rise to the idea that graph neural networks (GNNs), which naturally resemble these properties, should be best-suited to address many tasks related to high-energy particle physics. In this paper we describe a benchmark test of a typical GNN against neural networks of the well-established deep fully-connected feed-forward architecture. We aim at performing this comparison maximally unbiased in terms of nodes, hidden layers, or trainable parameters of the neural networks under study. As physics case we use the classification of the final state X produced in association with top quark-antiquark pairs in proton-proton collisions at the Large Hadron Collider at CERN, where X stands for a bottom quark-antiquark pair produced either non-resonantly or through the decay of an intermediately produced Z or Higgs boson.","sentences":["In high-energy particle collisions, the primary collision products usually decay further resulting in tree-like, hierarchical structures with a priori unknown multiplicity.","At the stable-particle level all decay products of a collision form permutation invariant sets of final state objects.","The analogy to mathematical graphs gives rise to the idea that graph neural networks (GNNs), which naturally resemble these properties, should be best-suited to address many tasks related to high-energy particle physics.","In this paper we describe a benchmark test of a typical GNN against neural networks of the well-established deep fully-connected feed-forward architecture.","We aim at performing this comparison maximally unbiased in terms of nodes, hidden layers, or trainable parameters of the neural networks under study.","As physics case we use the classification of the final state X produced in association with top quark-antiquark pairs in proton-proton collisions at the Large Hadron Collider at CERN, where X stands for a bottom quark-antiquark pair produced either non-resonantly or through the decay of an intermediately produced Z or Higgs boson."],"url":"http://arxiv.org/abs/2402.17386v1","category":"hep-ph"}
{"created":"2024-02-27 18:55:50","title":"Reducing Unnecessary Alerts in Pedestrian Protection Systems Based on P2V Communications","abstract":"There are different proposals in the literature on how to protect pedestrians using warning systems to alert drivers of their presence. They can be based on onboard perception systems or wireless communications. The evaluation of these systems has been focused on testing their ability to detect pedestrians. A problem that has received much less attention is the possibility of generating too many alerts in the warning systems. In this paper, we propose and analyze four different algorithms to take the decision on generating alerts in a warning system that is based on direct wireless communications between vehicles and pedestrians. With the algorithms, we explore different strategies to reduce unnecessary alerts. The feasibility of the implementation of the algorithms was evaluated with a deployment using real equipment, and tests were carried out to verify their behavior in real scenarios. The ability of each algorithm to reduce unnecessary alerts was evaluated with realistic simulations in an urban scenario, using a traffic simulator with vehicular and pedestrian flows. The results show the importance of tackling the problem of driver overload in warning systems, and that it is not straightforward to predict the load of alerts generated by an algorithm in a large-scale deployment, in which there are multiple interactions between vehicles and pedestrians.","sentences":["There are different proposals in the literature on how to protect pedestrians using warning systems to alert drivers of their presence.","They can be based on onboard perception systems or wireless communications.","The evaluation of these systems has been focused on testing their ability to detect pedestrians.","A problem that has received much less attention is the possibility of generating too many alerts in the warning systems.","In this paper, we propose and analyze four different algorithms to take the decision on generating alerts in a warning system that is based on direct wireless communications between vehicles and pedestrians.","With the algorithms, we explore different strategies to reduce unnecessary alerts.","The feasibility of the implementation of the algorithms was evaluated with a deployment using real equipment, and tests were carried out to verify their behavior in real scenarios.","The ability of each algorithm to reduce unnecessary alerts was evaluated with realistic simulations in an urban scenario, using a traffic simulator with vehicular and pedestrian flows.","The results show the importance of tackling the problem of driver overload in warning systems, and that it is not straightforward to predict the load of alerts generated by an algorithm in a large-scale deployment, in which there are multiple interactions between vehicles and pedestrians."],"url":"http://arxiv.org/abs/2402.17763v1","category":"cs.NI"}
{"created":"2024-02-27 18:52:19","title":"Towards Optimal Learning of Language Models","abstract":"This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an \"LM-training-as-lossless-compression\" view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods. Our code can be found at https://aka.ms/LearningLaw.","sentences":["This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance.","Specifically, we present a theory for the optimal learning of LMs.","We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an \"LM-training-as-lossless-compression\" view.","Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective.","The theorem is then validated by experiments on a linear classification and a real-world language modeling task.","Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods.","Our code can be found at https://aka.ms/LearningLaw."],"url":"http://arxiv.org/abs/2402.17759v1","category":"cs.CL"}
{"created":"2024-02-27 18:48:07","title":"Robustly Learning Single-Index Models via Alignment Sharpness","abstract":"We study the problem of learning Single-Index Models under the $L_2^2$ loss in the agnostic model. We give an efficient learning algorithm, achieving a constant factor approximation to the optimal loss, that succeeds under a range of distributions (including log-concave distributions) and a broad class of monotone and Lipschitz link functions. This is the first efficient constant factor approximate agnostic learner, even for Gaussian data and for any nontrivial class of link functions. Prior work for the case of unknown link function either works in the realizable setting or does not attain constant factor approximation. The main technical ingredient enabling our algorithm and analysis is a novel notion of a local error bound in optimization that we term alignment sharpness and that may be of broader interest.","sentences":["We study the problem of learning Single-Index Models under the $L_2^2$ loss in the agnostic model.","We give an efficient learning algorithm, achieving a constant factor approximation to the optimal loss, that succeeds under a range of distributions (including log-concave distributions) and a broad class of monotone and Lipschitz link functions.","This is the first efficient constant factor approximate agnostic learner, even for Gaussian data and for any nontrivial class of link functions.","Prior work for the case of unknown link function either works in the realizable setting or does not attain constant factor approximation.","The main technical ingredient enabling our algorithm and analysis is a novel notion of a local error bound in optimization that we term alignment sharpness and that may be of broader interest."],"url":"http://arxiv.org/abs/2402.17756v1","category":"cs.LG"}
{"created":"2024-02-27 18:45:31","title":"Increasing the Diversity of Investment Portfolio with Integration of Gamified Components in the FinTech Applications Lifecycle","abstract":"Gamification has the potential to make significant contributions to financial product delivery, Fintech services, and inclusive growth. The integration of gamification into FinTech applications has shown a positive correlation with the social impact theory. Utilizing gamification in a sustainable and effective manner can be crucial for long-term prospects in the FinTech industry. Therefore, it is essential to develop efficient and modern financial software that improves the customer experience. The current literature aims to contribute to this area by highlighting the relationship between interrelated theories and the key factors to consider when designing a gamified element. This study aims to explore the effects of gamification on altering user intention and its significant influence on customer value propositions.","sentences":["Gamification has the potential to make significant contributions to financial product delivery, Fintech services, and inclusive growth.","The integration of gamification into FinTech applications has shown a positive correlation with the social impact theory.","Utilizing gamification in a sustainable and effective manner can be crucial for long-term prospects in the FinTech industry.","Therefore, it is essential to develop efficient and modern financial software that improves the customer experience.","The current literature aims to contribute to this area by highlighting the relationship between interrelated theories and the key factors to consider when designing a gamified element.","This study aims to explore the effects of gamification on altering user intention and its significant influence on customer value propositions."],"url":"http://arxiv.org/abs/2402.17754v1","category":"cs.GT"}
{"created":"2024-02-27 18:38:05","title":"An Eye Gaze Heatmap Analysis of Uncertainty Head-Up Display Designs for Conditional Automated Driving","abstract":"This paper reports results from a high-fidelity driving simulator study (N=215) about a head-up display (HUD) that conveys a conditional automated vehicle's dynamic \"uncertainty\" about the current situation while fallback drivers watch entertaining videos. We compared (between-group) three design interventions: display (a bar visualisation of uncertainty close to the video), interruption (interrupting the video during uncertain situations), and combination (a combination of both), against a baseline (video-only). We visualised eye-tracking data to conduct a heatmap analysis of the four groups' gaze behaviour over time. We found interruptions initiated a phase during which participants interleaved their attention between monitoring and entertainment. This improved monitoring behaviour was more pronounced in combination compared to interruption, suggesting pre-warning interruptions have positive effects. The same addition had negative effects without interruptions (comparing baseline & display). Intermittent interruptions may have safety benefits over placing additional peripheral displays without compromising usability.","sentences":["This paper reports results from a high-fidelity driving simulator study (N=215) about a head-up display (HUD) that conveys a conditional automated vehicle's dynamic \"uncertainty\" about the current situation while fallback drivers watch entertaining videos.","We compared (between-group) three design interventions: display (a bar visualisation of uncertainty close to the video), interruption (interrupting the video during uncertain situations), and combination (a combination of both), against a baseline (video-only).","We visualised eye-tracking data to conduct a heatmap analysis of the four groups' gaze behaviour over time.","We found interruptions initiated a phase during which participants interleaved their attention between monitoring and entertainment.","This improved monitoring behaviour was more pronounced in combination compared to interruption, suggesting pre-warning interruptions have positive effects.","The same addition had negative effects without interruptions (comparing baseline & display).","Intermittent interruptions may have safety benefits over placing additional peripheral displays without compromising usability."],"url":"http://arxiv.org/abs/2402.17751v1","category":"cs.HC"}
{"created":"2024-02-27 18:37:22","title":"Scaling on-chip photonic neural processors using arbitrarily programmable wave propagation","abstract":"On-chip photonic processors for neural networks have potential benefits in both speed and energy efficiency but have not yet reached the scale at which they can outperform electronic processors. The dominant paradigm for designing on-chip photonics is to make networks of relatively bulky discrete components connected by one-dimensional waveguides. A far more compact alternative is to avoid explicitly defining any components and instead sculpt the continuous substrate of the photonic processor to directly perform the computation using waves freely propagating in two dimensions. We propose and demonstrate a device whose refractive index as a function of space, $n(x,z)$, can be rapidly reprogrammed, allowing arbitrary control over the wave propagation in the device. Our device, a 2D-programmable waveguide, combines photoconductive gain with the electro-optic effect to achieve massively parallel modulation of the refractive index of a slab waveguide, with an index modulation depth of $10^{-3}$ and approximately $10^4$ programmable degrees of freedom. We used a prototype device with a functional area of $12\\,\\text{mm}^2$ to perform neural-network inference with up to 49-dimensional input vectors in a single pass, achieving 96% accuracy on vowel classification and 86% accuracy on $7 \\times 7$-pixel MNIST handwritten-digit classification. This is a scale beyond that of previous photonic chips relying on discrete components, illustrating the benefit of the continuous-waves paradigm. In principle, with large enough chip area, the reprogrammability of the device's refractive index distribution enables the reconfigurable realization of any passive, linear photonic circuit or device. This promises the development of more compact and versatile photonic systems for a wide range of applications, including optical processing, smart sensing, spectroscopy, and optical communications.","sentences":["On-chip photonic processors for neural networks have potential benefits in both speed and energy efficiency but have not yet reached the scale at which they can outperform electronic processors.","The dominant paradigm for designing on-chip photonics is to make networks of relatively bulky discrete components connected by one-dimensional waveguides.","A far more compact alternative is to avoid explicitly defining any components and instead sculpt the continuous substrate of the photonic processor to directly perform the computation using waves freely propagating in two dimensions.","We propose and demonstrate a device whose refractive index as a function of space, $n(x,z)$, can be rapidly reprogrammed, allowing arbitrary control over the wave propagation in the device.","Our device, a 2D-programmable waveguide, combines photoconductive gain with the electro-optic effect to achieve massively parallel modulation of the refractive index of a slab waveguide, with an index modulation depth of $10^{-3}$ and approximately $10^4$ programmable degrees of freedom.","We used a prototype device with a functional area of $12\\,\\text{mm}^2$ to perform neural-network inference with up to 49-dimensional input vectors in a single pass, achieving 96% accuracy on vowel classification and 86% accuracy on $7 \\times 7$-pixel MNIST handwritten-digit classification.","This is a scale beyond that of previous photonic chips relying on discrete components, illustrating the benefit of the continuous-waves paradigm.","In principle, with large enough chip area, the reprogrammability of the device's refractive index distribution enables the reconfigurable realization of any passive, linear photonic circuit or device.","This promises the development of more compact and versatile photonic systems for a wide range of applications, including optical processing, smart sensing, spectroscopy, and optical communications."],"url":"http://arxiv.org/abs/2402.17750v1","category":"physics.optics"}
{"created":"2024-02-27 18:37:01","title":"$\u03b6$-QVAE: A Quantum Variational Autoencoder utilizing Regularized Mixed-state Latent Representations","abstract":"A major challenge in near-term quantum computing is its application to large real-world datasets due to scarce quantum hardware resources. One approach to enabling tractable quantum models for such datasets involves compressing the original data to manageable dimensions while still representing essential information for downstream analysis. In classical machine learning, variational autoencoders (VAEs) facilitate efficient data compression, representation learning for subsequent tasks, and novel data generation. However, no model has been proposed that exactly captures all of these features for direct application to quantum data on quantum computers. Some existing quantum models for data compression lack regularization of latent representations, thus preventing direct use for generation and control of generalization. Others are hybrid models with only some internal quantum components, impeding direct training on quantum data. To bridge this gap, we present a fully quantum framework, $\\zeta$-QVAE, which encompasses all the capabilities of classical VAEs and can be directly applied for both classical and quantum data compression. Our model utilizes regularized mixed states to attain optimal latent representations. It accommodates various divergences for reconstruction and regularization. Furthermore, by accommodating mixed states at every stage, it can utilize the full-data density matrix and allow for a \"global\" training objective. Doing so, in turn, makes efficient optimization possible and has potential implications for private and federated learning. In addition to exploring the theoretical properties of $\\zeta$-QVAE, we demonstrate its performance on representative genomics and synthetic data. Our results consistently indicate that $\\zeta$-QVAE exhibits similar or better performance compared to matched classical models.","sentences":["A major challenge in near-term quantum computing is its application to large real-world datasets due to scarce quantum hardware resources.","One approach to enabling tractable quantum models for such datasets involves compressing the original data to manageable dimensions while still representing essential information for downstream analysis.","In classical machine learning, variational autoencoders (VAEs) facilitate efficient data compression, representation learning for subsequent tasks, and novel data generation.","However, no model has been proposed that exactly captures all of these features for direct application to quantum data on quantum computers.","Some existing quantum models for data compression lack regularization of latent representations, thus preventing direct use for generation and control of generalization.","Others are hybrid models with only some internal quantum components, impeding direct training on quantum data.","To bridge this gap, we present a fully quantum framework, $\\zeta$-QVAE, which encompasses all the capabilities of classical VAEs and can be directly applied for both classical and quantum data compression.","Our model utilizes regularized mixed states to attain optimal latent representations.","It accommodates various divergences for reconstruction and regularization.","Furthermore, by accommodating mixed states at every stage, it can utilize the full-data density matrix and allow for a \"global\" training objective.","Doing so, in turn, makes efficient optimization possible and has potential implications for private and federated learning.","In addition to exploring the theoretical properties of $\\zeta$-QVAE, we demonstrate its performance on representative genomics and synthetic data.","Our results consistently indicate that $\\zeta$-QVAE exhibits similar or better performance compared to matched classical models."],"url":"http://arxiv.org/abs/2402.17749v1","category":"quant-ph"}
{"created":"2024-02-27 18:25:16","title":"Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using Contrastive Learning and Geometric Unfolding","abstract":"Understanding the cortical organization of the human brain requires interpretable descriptors for distinct structural and functional imaging data. 3D polarized light imaging (3D-PLI) is an imaging modality for visualizing fiber architecture in postmortem brains with high resolution that also captures the presence of cell bodies, for example, to identify hippocampal subfields. The rich texture in 3D-PLI images, however, makes this modality particularly difficult to analyze and best practices for characterizing architectonic patterns still need to be established. In this work, we demonstrate a novel method to analyze the regional organization of the human hippocampus in 3D-PLI by combining recent advances in unfolding methods with deep texture features obtained using a self-supervised contrastive learning approach. We identify clusters in the representations that correspond well with classical descriptions of hippocampal subfields, lending validity to the developed methodology.","sentences":["Understanding the cortical organization of the human brain requires interpretable descriptors for distinct structural and functional imaging data.","3D polarized light imaging (3D-PLI) is an imaging modality for visualizing fiber architecture in postmortem brains with high resolution that also captures the presence of cell bodies, for example, to identify hippocampal subfields.","The rich texture in 3D-PLI images, however, makes this modality particularly difficult to analyze and best practices for characterizing architectonic patterns still need to be established.","In this work, we demonstrate a novel method to analyze the regional organization of the human hippocampus in 3D-PLI by combining recent advances in unfolding methods with deep texture features obtained using a self-supervised contrastive learning approach.","We identify clusters in the representations that correspond well with classical descriptions of hippocampal subfields, lending validity to the developed methodology."],"url":"http://arxiv.org/abs/2402.17744v1","category":"cs.CV"}
{"created":"2024-02-27 18:18:56","title":"Multistable Kuramoto splay states in a crystal of mode-locked laser pulses","abstract":"We demonstrate the existence of a multiplicity of co-existing frequency combs in a harmonically mode-locked laser that we link to the splay phases of the Kuramoto model with short range interactions. These splay states are multistable and the laser may wander between them under the influence of stochastic forces. Consequently, the many pulses circulating in the cavity are not necessarily coherent with each other. We show that this partially disordered state for the phase of the optical field features regular train of pulses in the field intensity, a state that we term an incoherent crystal of optical pulses. We provide evidence that the notion of coherence should be interpreted by comparing the duration of the measurement time with the Kramers' escape time of each splay state. Our results are confirmed experimentally by studying a passively mode-locked vertical external-cavity surface-emitting laser.","sentences":["We demonstrate the existence of a multiplicity of co-existing frequency combs in a harmonically mode-locked laser that we link to the splay phases of the Kuramoto model with short range interactions.","These splay states are multistable and the laser may wander between them under the influence of stochastic forces.","Consequently, the many pulses circulating in the cavity are not necessarily coherent with each other.","We show that this partially disordered state for the phase of the optical field features regular train of pulses in the field intensity, a state that we term an incoherent crystal of optical pulses.","We provide evidence that the notion of coherence should be interpreted by comparing the duration of the measurement time with the Kramers' escape time of each splay state.","Our results are confirmed experimentally by studying a passively mode-locked vertical external-cavity surface-emitting laser."],"url":"http://arxiv.org/abs/2402.17740v1","category":"physics.optics"}
{"created":"2024-02-27 18:06:20","title":"Batched Nonparametric Contextual Bandits","abstract":"We study nonparametric contextual bandits under batch constraints, where the expected reward for each action is modeled as a smooth function of covariates, and the policy updates are made at the end of each batch of observations. We establish a minimax regret lower bound for this setting and propose Batched Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal regret (up to logarithmic factors). In essence, BaSEDB dynamically splits the covariate space into smaller bins, carefully aligning their widths with the batch size. We also show the suboptimality of static binning under batch constraints, highlighting the necessity of dynamic binning. Additionally, our results suggest that a nearly constant number of policy updates can attain optimal regret in the fully online setting.","sentences":["We study nonparametric contextual bandits under batch constraints, where the expected reward for each action is modeled as a smooth function of covariates, and the policy updates are made at the end of each batch of observations.","We establish a minimax regret lower bound for this setting and propose Batched Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal regret (up to logarithmic factors).","In essence, BaSEDB dynamically splits the covariate space into smaller bins, carefully aligning their widths with the batch size.","We also show the suboptimality of static binning under batch constraints, highlighting the necessity of dynamic binning.","Additionally, our results suggest that a nearly constant number of policy updates can attain optimal regret in the fully online setting."],"url":"http://arxiv.org/abs/2402.17732v1","category":"math.ST"}
{"created":"2024-02-27 17:58:05","title":"MedContext: Learning Contextual Cues for Efficient Volumetric Medical Segmentation","abstract":"Volumetric medical segmentation is a critical component of 3D medical image analysis that delineates different semantic regions. Deep neural networks have significantly improved volumetric medical segmentation, but they generally require large-scale annotated data to achieve better performance, which can be expensive and prohibitive to obtain. To address this limitation, existing works typically perform transfer learning or design dedicated pretraining-finetuning stages to learn representative features. However, the mismatch between the source and target domain can make it challenging to learn optimal representation for volumetric data, while the multi-stage training demands higher compute as well as careful selection of stage-specific design choices. In contrast, we propose a universal training framework called MedContext that is architecture-agnostic and can be incorporated into any existing training framework for 3D medical segmentation. Our approach effectively learns self supervised contextual cues jointly with the supervised voxel segmentation task without requiring large-scale annotated volumetric medical data or dedicated pretraining-finetuning stages. The proposed approach induces contextual knowledge in the network by learning to reconstruct the missing organ or parts of an organ in the output segmentation space. The effectiveness of MedContext is validated across multiple 3D medical datasets and four state-of-the-art model architectures. Our approach demonstrates consistent gains in segmentation performance across datasets and different architectures even in few-shot data scenarios. Our code and pretrained models are available at https://github.com/hananshafi/MedContext","sentences":["Volumetric medical segmentation is a critical component of 3D medical image analysis that delineates different semantic regions.","Deep neural networks have significantly improved volumetric medical segmentation, but they generally require large-scale annotated data to achieve better performance, which can be expensive and prohibitive to obtain.","To address this limitation, existing works typically perform transfer learning or design dedicated pretraining-finetuning stages to learn representative features.","However, the mismatch between the source and target domain can make it challenging to learn optimal representation for volumetric data, while the multi-stage training demands higher compute as well as careful selection of stage-specific design choices.","In contrast, we propose a universal training framework called MedContext that is architecture-agnostic and can be incorporated into any existing training framework for 3D medical segmentation.","Our approach effectively learns self supervised contextual cues jointly with the supervised voxel segmentation task without requiring large-scale annotated volumetric medical data or dedicated pretraining-finetuning stages.","The proposed approach induces contextual knowledge in the network by learning to reconstruct the missing organ or parts of an organ in the output segmentation space.","The effectiveness of MedContext is validated across multiple 3D medical datasets and four state-of-the-art model architectures.","Our approach demonstrates consistent gains in segmentation performance across datasets and different architectures even in few-shot data scenarios.","Our code and pretrained models are available at https://github.com/hananshafi/MedContext"],"url":"http://arxiv.org/abs/2402.17725v1","category":"eess.IV"}
{"created":"2024-02-27 17:56:49","title":"Taming Nonconvex Stochastic Mirror Descent with General Bregman Divergence","abstract":"This paper revisits the convergence of Stochastic Mirror Descent (SMD) in the contemporary nonconvex optimization setting. Existing results for batch-free nonconvex SMD restrict the choice of the distance generating function (DGF) to be differentiable with Lipschitz continuous gradients, thereby excluding important setups such as Shannon entropy. In this work, we present a new convergence analysis of nonconvex SMD supporting general DGF, that overcomes the above limitations and relies solely on the standard assumptions. Moreover, our convergence is established with respect to the Bregman Forward-Backward envelope, which is a stronger measure than the commonly used squared norm of gradient mapping. We further extend our results to guarantee high probability convergence under sub-Gaussian noise and global convergence under the generalized Bregman Proximal Polyak-{\\L}ojasiewicz condition. Additionally, we illustrate the advantages of our improved SMD theory in various nonconvex machine learning tasks by harnessing nonsmooth DGFs. Notably, in the context of nonconvex differentially private (DP) learning, our theory yields a simple algorithm with a (nearly) dimension-independent utility bound. For the problem of training linear neural networks, we develop provably convergent stochastic algorithms.","sentences":["This paper revisits the convergence of Stochastic Mirror Descent (SMD) in the contemporary nonconvex optimization setting.","Existing results for batch-free nonconvex SMD restrict the choice of the distance generating function (DGF) to be differentiable with Lipschitz continuous gradients, thereby excluding important setups such as Shannon entropy.","In this work, we present a new convergence analysis of nonconvex SMD supporting general DGF, that overcomes the above limitations and relies solely on the standard assumptions.","Moreover, our convergence is established with respect to the Bregman Forward-Backward envelope, which is a stronger measure than the commonly used squared norm of gradient mapping.","We further extend our results to guarantee high probability convergence under sub-Gaussian noise and global convergence under the generalized Bregman Proximal Polyak-{\\L}ojasiewicz condition.","Additionally, we illustrate the advantages of our improved SMD theory in various nonconvex machine learning tasks by harnessing nonsmooth DGFs.","Notably, in the context of nonconvex differentially private (DP) learning, our theory yields a simple algorithm with a (nearly) dimension-independent utility bound.","For the problem of training linear neural networks, we develop provably convergent stochastic algorithms."],"url":"http://arxiv.org/abs/2402.17722v1","category":"math.OC"}
{"created":"2024-02-27 18:55:13","title":"Quantum Circuit Discovery for Fault-Tolerant Logical State Preparation with Reinforcement Learning","abstract":"One of the key aspects in the realization of large-scale fault-tolerant quantum computers is quantum error correction (QEC). The first essential step of QEC is to encode the logical state into physical qubits in a fault-tolerant manner. Recently, flag-based protocols have been introduced that use ancillary qubits to flag harmful errors. However, there is no clear recipe for finding a compact quantum circuit with flag-based protocols for fault-tolerant logical state preparation. It is even more difficult when we consider the hardware constraints, such as qubit connectivity and gate set. In this work, we propose and explore reinforcement learning (RL) to automatically discover compact and hardware-adapted quantum circuits that fault-tolerantly prepare the logical state of a QEC code. We show that RL discovers circuits with fewer gates and ancillary qubits than published results without and with hardware constraints of up to 15 physical qubits. Furthermore, RL allows for straightforward exploration of different qubit connectivities and the use of transfer learning to accelerate the discovery. More generally, our work opens the door towards the use of RL for the discovery of fault-tolerant quantum circuits for addressing tasks beyond state preparation, including magic state preparation, logical gate synthesis, or syndrome measurement.","sentences":["One of the key aspects in the realization of large-scale fault-tolerant quantum computers is quantum error correction (QEC).","The first essential step of QEC is to encode the logical state into physical qubits in a fault-tolerant manner.","Recently, flag-based protocols have been introduced that use ancillary qubits to flag harmful errors.","However, there is no clear recipe for finding a compact quantum circuit with flag-based protocols for fault-tolerant logical state preparation.","It is even more difficult when we consider the hardware constraints, such as qubit connectivity and gate set.","In this work, we propose and explore reinforcement learning (RL) to automatically discover compact and hardware-adapted quantum circuits that fault-tolerantly prepare the logical state of a QEC code.","We show that RL discovers circuits with fewer gates and ancillary qubits than published results without and with hardware constraints of up to 15 physical qubits.","Furthermore, RL allows for straightforward exploration of different qubit connectivities and the use of transfer learning to accelerate the discovery.","More generally, our work opens the door towards the use of RL for the discovery of fault-tolerant quantum circuits for addressing tasks beyond state preparation, including magic state preparation, logical gate synthesis, or syndrome measurement."],"url":"http://arxiv.org/abs/2402.17761v1","category":"quant-ph"}
{"created":"2024-02-27 18:29:07","title":"LoDIP: Low light phase retrieval with deep image prior","abstract":"Phase retrieval (PR) is a fundamental challenge in scientific imaging, enabling nanoscale techniques like coherent diffractive imaging (CDI). Imaging at low radiation doses becomes important in applications where samples are susceptible to radiation damage. However, most PR methods struggle in low dose scenario due to the presence of very high shot noise. Advancements in the optical data acquisition setup, exemplified by in-situ CDI, have shown potential for low-dose imaging. But these depend on a time series of measurements, rendering them unsuitable for single-image applications. Similarly, on the computational front, data-driven phase retrieval techniques are not readily adaptable to the single-image context. Deep learning based single-image methods, such as deep image prior, have been effective for various imaging tasks but have exhibited limited success when applied to PR. In this work, we propose LoDIP which combines the in-situ CDI setup with the power of implicit neural priors to tackle the problem of single-image low-dose phase retrieval. Quantitative evaluations demonstrate the superior performance of LoDIP on this task as well as applicability to real experimental scenarios.","sentences":["Phase retrieval (PR) is a fundamental challenge in scientific imaging, enabling nanoscale techniques like coherent diffractive imaging (CDI).","Imaging at low radiation doses becomes important in applications where samples are susceptible to radiation damage.","However, most PR methods struggle in low dose scenario due to the presence of very high shot noise.","Advancements in the optical data acquisition setup, exemplified by in-situ CDI, have shown potential for low-dose imaging.","But these depend on a time series of measurements, rendering them unsuitable for single-image applications.","Similarly, on the computational front, data-driven phase retrieval techniques are not readily adaptable to the single-image context.","Deep learning based single-image methods, such as deep image prior, have been effective for various imaging tasks but have exhibited limited success when applied to PR.","In this work, we propose LoDIP which combines the in-situ CDI setup with the power of implicit neural priors to tackle the problem of single-image low-dose phase retrieval.","Quantitative evaluations demonstrate the superior performance of LoDIP on this task as well as applicability to real experimental scenarios."],"url":"http://arxiv.org/abs/2402.17745v1","category":"physics.comp-ph"}
{"created":"2024-02-27 17:55:33","title":"The SMART approach to instance-optimal online learning","abstract":"We devise an online learning algorithm -- titled Switching via Monotone Adapted Regret Traces (SMART) -- that adapts to the data and achieves regret that is instance optimal, i.e., simultaneously competitive on every input sequence compared to the performance of the follow-the-leader (FTL) policy and the worst case guarantee of any other input policy. We show that the regret of the SMART policy on any input sequence is within a multiplicative factor $e/(e-1) \\approx 1.58$ of the smaller of: 1) the regret obtained by FTL on the sequence, and 2) the upper bound on regret guaranteed by the given worst-case policy. This implies a strictly stronger guarantee than typical `best-of-both-worlds' bounds as the guarantee holds for every input sequence regardless of how it is generated. SMART is simple to implement as it begins by playing FTL and switches at most once during the time horizon to the worst-case algorithm. Our approach and results follow from an operational reduction of instance optimal online learning to competitive analysis for the ski-rental problem. We complement our competitive ratio upper bounds with a fundamental lower bound showing that over all input sequences, no algorithm can get better than a $1.43$-fraction of the minimum regret achieved by FTL and the minimax-optimal policy. We also present a modification of SMART that combines FTL with a ``small-loss\" algorithm to achieve instance optimality between the regret of FTL and the small loss regret bound.","sentences":["We devise an online learning algorithm -- titled Switching via Monotone Adapted Regret Traces (SMART) -- that adapts to the data and achieves regret that is instance optimal, i.e., simultaneously competitive on every input sequence compared to the performance of the follow-the-leader (FTL) policy and the worst case guarantee of any other input policy.","We show that the regret of the SMART policy on any input sequence is within a multiplicative factor $e/(e-1) \\approx 1.58$ of the smaller of: 1) the regret obtained by FTL on the sequence, and 2) the upper bound on regret guaranteed by the given worst-case policy.","This implies a strictly stronger guarantee than typical `best-of-both-worlds' bounds as the guarantee holds for every input sequence regardless of how it is generated.","SMART is simple to implement as it begins by playing FTL and switches at most once during the time horizon to the worst-case algorithm.","Our approach and results follow from an operational reduction of instance optimal online learning to competitive analysis for the ski-rental problem.","We complement our competitive ratio upper bounds with a fundamental lower bound showing that over all input sequences, no algorithm can get better than a $1.43$-fraction of the minimum regret achieved by FTL and the minimax-optimal policy.","We also present a modification of SMART that combines FTL with a ``small-loss\" algorithm to achieve instance optimality between the regret of FTL and the small loss regret bound."],"url":"http://arxiv.org/abs/2402.17720v1","category":"cs.LG"}
{"created":"2024-02-27 17:36:01","title":"Adaptive quantization with mixed-precision based on low-cost proxy","abstract":"It is critical to deploy complicated neural network models on hardware with limited resources. This paper proposes a novel model quantization method, named the Low-Cost Proxy-Based Adaptive Mixed-Precision Model Quantization (LCPAQ), which contains three key modules. The hardware-aware module is designed by considering the hardware limitations, while an adaptive mixed-precision quantization module is developed to evaluate the quantization sensitivity by using the Hessian matrix and Pareto frontier techniques. Integer linear programming is used to fine-tune the quantization across different layers. Then the low-cost proxy neural architecture search module efficiently explores the ideal quantization hyperparameters. Experiments on the ImageNet demonstrate that the proposed LCPAQ achieves comparable or superior quantization accuracy to existing mixed-precision models. Notably, LCPAQ achieves 1/200 of the search time compared with existing methods, which provides a shortcut in practical quantization use for resource-limited devices.","sentences":["It is critical to deploy complicated neural network models on hardware with limited resources.","This paper proposes a novel model quantization method, named the Low-Cost Proxy-Based Adaptive Mixed-Precision Model Quantization (LCPAQ), which contains three key modules.","The hardware-aware module is designed by considering the hardware limitations, while an adaptive mixed-precision quantization module is developed to evaluate the quantization sensitivity by using the Hessian matrix and Pareto frontier techniques.","Integer linear programming is used to fine-tune the quantization across different layers.","Then the low-cost proxy neural architecture search module efficiently explores the ideal quantization hyperparameters.","Experiments on the ImageNet demonstrate that the proposed LCPAQ achieves comparable or superior quantization accuracy to existing mixed-precision models.","Notably, LCPAQ achieves 1/200 of the search time compared with existing methods, which provides a shortcut in practical quantization use for resource-limited devices."],"url":"http://arxiv.org/abs/2402.17706v1","category":"cs.CV"}
{"created":"2024-02-27 17:26:33","title":"Real-time Low-latency Music Source Separation using Hybrid Spectrogram-TasNet","abstract":"There have been significant advances in deep learning for music demixing in recent years. However, there has been little attention given to how these neural networks can be adapted for real-time low-latency applications, which could be helpful for hearing aids, remixing audio streams and live shows. In this paper, we investigate the various challenges involved in adapting current demixing models in the literature for this use case. Subsequently, inspired by the Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domain Audio Separation Network HS-TasNet, which utilises the advantages of spectral and waveform domains. For a latency of 23 ms, the HS-TasNet obtains an overall signal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increases to 5.55 with additional training data. These results demonstrate the potential of efficient demixing for real-time low-latency music applications.","sentences":["There have been significant advances in deep learning for music demixing in recent years.","However, there has been little attention given to how these neural networks can be adapted for real-time low-latency applications, which could be helpful for hearing aids, remixing audio streams and live shows.","In this paper, we investigate the various challenges involved in adapting current demixing models in the literature for this use case.","Subsequently, inspired by the Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domain Audio Separation Network HS-TasNet, which utilises the advantages of spectral and waveform domains.","For a latency of 23 ms, the HS-TasNet obtains an overall signal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increases to 5.55 with additional training data.","These results demonstrate the potential of efficient demixing for real-time low-latency music applications."],"url":"http://arxiv.org/abs/2402.17701v1","category":"eess.AS"}
{"created":"2024-02-27 17:23:40","title":"Gradient-based Discrete Sampling with Automatic Cyclical Scheduling","abstract":"Discrete distributions, particularly in high-dimensional deep models, are often highly multimodal due to inherent discontinuities. While gradient-based discrete sampling has proven effective, it is susceptible to becoming trapped in local modes due to the gradient information. To tackle this challenge, we propose an automatic cyclical scheduling, designed for efficient and accurate sampling in multimodal discrete distributions. Our method contains three key components: (1) a cyclical step size schedule where large steps discover new modes and small steps exploit each mode; (2) a cyclical balancing schedule, ensuring ``balanced\" proposals for given step sizes and high efficiency of the Markov chain; and (3) an automatic tuning scheme for adjusting the hyperparameters in the cyclical schedules, allowing adaptability across diverse datasets with minimal tuning. We prove the non-asymptotic convergence and inference guarantee for our method in general discrete distributions. Extensive experiments demonstrate the superiority of our method in sampling complex multimodal discrete distributions.","sentences":["Discrete distributions, particularly in high-dimensional deep models, are often highly multimodal due to inherent discontinuities.","While gradient-based discrete sampling has proven effective, it is susceptible to becoming trapped in local modes due to the gradient information.","To tackle this challenge, we propose an automatic cyclical scheduling, designed for efficient and accurate sampling in multimodal discrete distributions.","Our method contains three key components: (1) a cyclical step size schedule where large steps discover new modes and small steps exploit each mode; (2) a cyclical balancing schedule, ensuring ``balanced\" proposals for given step sizes and high efficiency of the Markov chain; and (3) an automatic tuning scheme for adjusting the hyperparameters in the cyclical schedules, allowing adaptability across diverse datasets with minimal tuning.","We prove the non-asymptotic convergence and inference guarantee for our method in general discrete distributions.","Extensive experiments demonstrate the superiority of our method in sampling complex multimodal discrete distributions."],"url":"http://arxiv.org/abs/2402.17699v1","category":"cs.LG"}
{"created":"2024-02-27 17:11:39","title":"Adaptive waveform inversion for transmitted wave data","abstract":"Adaptive Waveform Inversion applied to transient transmitted wave data yields estimates of index of refraction (or wave velocity) similar to those obtained by travel time inversion, provided that the data contain a single smooth wavefront.","sentences":["Adaptive Waveform Inversion applied to transient transmitted wave data yields estimates of index of refraction (or wave velocity) similar to those obtained by travel time inversion, provided that the data contain a single smooth wavefront."],"url":"http://arxiv.org/abs/2402.17696v1","category":"math.OC"}
{"created":"2024-02-27 16:36:53","title":"Multi-Agent Deep Reinforcement Learning for Distributed Satellite Routing","abstract":"This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL) approach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Each satellite is an independent decision-making agent with a partial knowledge of the environment, and supported by feedback received from the nearby agents. Building on our previous work that introduced a Q-routing solution, the contribution of this paper is to extend it to a deep learning framework able to quickly adapt to the network and traffic changes, and based on two phases: (1) An offline exploration learning phase that relies on a global Deep Neural Network (DNN) to learn the optimal paths at each possible position and congestion level; (2) An online exploitation phase with local, on-board, pre-trained DNNs. Results show that MA-DRL efficiently learns optimal routes offline that are then loaded for an efficient distributed routing online.","sentences":["This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL) approach for routing in Low Earth Orbit Satellite Constellations (LSatCs).","Each satellite is an independent decision-making agent with a partial knowledge of the environment, and supported by feedback received from the nearby agents.","Building on our previous work that introduced a Q-routing solution, the contribution of this paper is to extend it to a deep learning framework able to quickly adapt to the network and traffic changes, and based on two phases: (1) An offline exploration learning phase that relies on a global Deep Neural Network (DNN) to learn the optimal paths at each possible position and congestion level; (2) An online exploitation phase with local, on-board, pre-trained DNNs.","Results show that MA-DRL efficiently learns optimal routes offline that are then loaded for an efficient distributed routing online."],"url":"http://arxiv.org/abs/2402.17666v1","category":"cs.LG"}
{"created":"2024-02-27 16:24:28","title":"Confidence-Aware Multi-Field Model Calibration","abstract":"Accurately predicting the probabilities of user feedback, such as clicks and conversions, is critical for ad ranking and bidding. However, there often exist unwanted mismatches between predicted probabilities and true likelihoods due to the shift of data distributions and intrinsic model biases. Calibration aims to address this issue by post-processing model predictions, and field-aware calibration can adjust model output on different feature field values to satisfy fine-grained advertising demands. Unfortunately, the observed samples corresponding to certain field values can be too limited to make confident calibrations, which may yield bias amplification and online disturbance. In this paper, we propose a confidence-aware multi-field calibration method, which adaptively adjusts the calibration intensity based on the confidence levels derived from sample statistics. It also utilizes multiple feature fields for joint model calibration with awareness of their importance to mitigate the data sparsity effect of a single field. Extensive offline and online experiments show the superiority of our method in boosting advertising performance and reducing prediction deviations.","sentences":["Accurately predicting the probabilities of user feedback, such as clicks and conversions, is critical for ad ranking and bidding.","However, there often exist unwanted mismatches between predicted probabilities and true likelihoods due to the shift of data distributions and intrinsic model biases.","Calibration aims to address this issue by post-processing model predictions, and field-aware calibration can adjust model output on different feature field values to satisfy fine-grained advertising demands.","Unfortunately, the observed samples corresponding to certain field values can be too limited to make confident calibrations, which may yield bias amplification and online disturbance.","In this paper, we propose a confidence-aware multi-field calibration method, which adaptively adjusts the calibration intensity based on the confidence levels derived from sample statistics.","It also utilizes multiple feature fields for joint model calibration with awareness of their importance to mitigate the data sparsity effect of a single field.","Extensive offline and online experiments show the superiority of our method in boosting advertising performance and reducing prediction deviations."],"url":"http://arxiv.org/abs/2402.17655v1","category":"cs.LG"}
{"created":"2024-02-27 16:16:00","title":"Recurrent chaotic clustering and slow chaos in adaptive networks","abstract":"Adaptive dynamical networks are network systems in which the structure co-evolves and interacts with the dynamical state of the nodes. We study an adaptive dynamical network in which the structure changes on a slower time scale relative to the fast dynamics of the nodes. We identify a phenomenon we refer to as recurrent adaptive chaotic clustering (RACC), in which chaos is observed on a slow time scale, while the fast time scale exhibits regular dynamics. Such slow chaos is further characterized by long (relative to the fast time scale) regimes of frequency clusters or frequency-synchronized dynamics, interrupted by fast jumps between these regimes. We also determine parameter values where the time intervals between jumps are chaotic and show that such a state is robust to changes in parameters and initial conditions.","sentences":["Adaptive dynamical networks are network systems in which the structure co-evolves and interacts with the dynamical state of the nodes.","We study an adaptive dynamical network in which the structure changes on a slower time scale relative to the fast dynamics of the nodes.","We identify a phenomenon we refer to as recurrent adaptive chaotic clustering (RACC), in which chaos is observed on a slow time scale, while the fast time scale exhibits regular dynamics.","Such slow chaos is further characterized by long (relative to the fast time scale) regimes of frequency clusters or frequency-synchronized dynamics, interrupted by fast jumps between these regimes.","We also determine parameter values where the time intervals between jumps are chaotic and show that such a state is robust to changes in parameters and initial conditions."],"url":"http://arxiv.org/abs/2402.17646v1","category":"nlin.AO"}
{"created":"2024-02-27 15:52:59","title":"CustomSketching: Sketch Concept Extraction for Sketch-based Image Synthesis and Editing","abstract":"Personalization techniques for large text-to-image (T2I) models allow users to incorporate new concepts from reference images. However, existing methods primarily rely on textual descriptions, leading to limited control over customized images and failing to support fine-grained and local editing (e.g., shape, pose, and details). In this paper, we identify sketches as an intuitive and versatile representation that can facilitate such control, e.g., contour lines capturing shape information and flow lines representing texture. This motivates us to explore a novel task of sketch concept extraction: given one or more sketch-image pairs, we aim to extract a special sketch concept that bridges the correspondence between the images and sketches, thus enabling sketch-based image synthesis and editing at a fine-grained level. To accomplish this, we introduce CustomSketching, a two-stage framework for extracting novel sketch concepts. Considering that an object can often be depicted by a contour for general shapes and additional strokes for internal details, we introduce a dual-sketch representation to reduce the inherent ambiguity in sketch depiction. We employ a shape loss and a regularization loss to balance fidelity and editability during optimization. Through extensive experiments, a user study, and several applications, we show our method is effective and superior to the adapted baselines.","sentences":["Personalization techniques for large text-to-image (T2I) models allow users to incorporate new concepts from reference images.","However, existing methods primarily rely on textual descriptions, leading to limited control over customized images and failing to support fine-grained and local editing (e.g., shape, pose, and details).","In this paper, we identify sketches as an intuitive and versatile representation that can facilitate such control, e.g., contour lines capturing shape information and flow lines representing texture.","This motivates us to explore a novel task of sketch concept extraction: given one or more sketch-image pairs, we aim to extract a special sketch concept that bridges the correspondence between the images and sketches, thus enabling sketch-based image synthesis and editing at a fine-grained level.","To accomplish this, we introduce CustomSketching, a two-stage framework for extracting novel sketch concepts.","Considering that an object can often be depicted by a contour for general shapes and additional strokes for internal details, we introduce a dual-sketch representation to reduce the inherent ambiguity in sketch depiction.","We employ a shape loss and a regularization loss to balance fidelity and editability during optimization.","Through extensive experiments, a user study, and several applications, we show our method is effective and superior to the adapted baselines."],"url":"http://arxiv.org/abs/2402.17624v1","category":"cs.CV"}
{"created":"2024-02-27 15:43:53","title":"Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation","abstract":"Few-shot segmentation performance declines substantially when facing images from a domain different than the training domain, effectively limiting real-world use cases. To alleviate this, recently cross-domain few-shot segmentation (CD-FSS) has emerged. Works that address this task mainly attempted to learn segmentation on a source domain in a manner that generalizes across domains. Surprisingly, we can outperform these approaches while eliminating the training stage and removing their main segmentation network. We show test-time task-adaption is the key for successful CD-FSS instead. Task-adaption is achieved by appending small networks to the feature pyramid of a conventionally classification-pretrained backbone. To avoid overfitting to the few labeled samples in supervised fine-tuning, consistency across augmented views of input images serves as guidance while learning the parameters of the attached layers. Despite our self-restriction not to use any images other than the few labeled samples at test time, we achieve new state-of-the-art performance in CD-FSS, evidencing the need to rethink approaches for the task.","sentences":["Few-shot segmentation performance declines substantially when facing images from a domain different than the training domain, effectively limiting real-world use cases.","To alleviate this, recently cross-domain few-shot segmentation (CD-FSS) has emerged.","Works that address this task mainly attempted to learn segmentation on a source domain in a manner that generalizes across domains.","Surprisingly, we can outperform these approaches while eliminating the training stage and removing their main segmentation network.","We show test-time task-adaption is the key for successful CD-FSS instead.","Task-adaption is achieved by appending small networks to the feature pyramid of a conventionally classification-pretrained backbone.","To avoid overfitting to the few labeled samples in supervised fine-tuning, consistency across augmented views of input images serves as guidance while learning the parameters of the attached layers.","Despite our self-restriction not to use any images other than the few labeled samples at test time, we achieve new state-of-the-art performance in CD-FSS, evidencing the need to rethink approaches for the task."],"url":"http://arxiv.org/abs/2402.17614v1","category":"cs.CV"}
{"created":"2024-02-27 15:33:26","title":"Radar Resource Management for Active Tracking Using Split-Aperture Phased Arrays","abstract":"Flexible front-end technology will become available in future multifunction radar systems to improve adaptability to the operational theatre. A potential concept to utilize this flexibility is to subdivide radar tasks spatially over the array, the so-called split-aperture phased array (SAPA) concept. As radars are generally designed for their worst-case scenario, e.g., small targets at a large range, the power-aperture budget can be excessive for targets that do not fall within that class. To increase efficiency of the time budget of the radar front-end, the SAPA concept could be applied. In this paper, the SAPA concept is explored to assign radar resources for active tracking tasks of many targets. To do so, we formulate and solve the radar resource management problem for the SAPA concept by employing the quality of service based resource allocation model (Q-RAM) framework. It will be demonstrated by a simulation example that a radar can maintain a larger numbers of active tracking tasks when using the SAPA concept compared to the case that only the full array can be used per task.","sentences":["Flexible front-end technology will become available in future multifunction radar systems to improve adaptability to the operational theatre.","A potential concept to utilize this flexibility is to subdivide radar tasks spatially over the array, the so-called split-aperture phased array (SAPA) concept.","As radars are generally designed for their worst-case scenario, e.g., small targets at a large range, the power-aperture budget can be excessive for targets that do not fall within that class.","To increase efficiency of the time budget of the radar front-end, the SAPA concept could be applied.","In this paper, the SAPA concept is explored to assign radar resources for active tracking tasks of many targets.","To do so, we formulate and solve the radar resource management problem for the SAPA concept by employing the quality of service based resource allocation model (Q-RAM) framework.","It will be demonstrated by a simulation example that a radar can maintain a larger numbers of active tracking tasks when using the SAPA concept compared to the case that only the full array can be used per task."],"url":"http://arxiv.org/abs/2402.17607v1","category":"eess.SP"}
{"created":"2024-02-27 15:02:17","title":"An Empirical Study of the Generalization Ability of Lidar 3D Object Detectors to Unseen Domains","abstract":"3D Object Detectors (3D-OD) are crucial for understanding the environment in many robotic tasks, especially autonomous driving. Including 3D information via Lidar sensors improves accuracy greatly. However, such detectors perform poorly on domains they were not trained on, i.e. different locations, sensors, weather, etc., limiting their reliability in safety-critical applications. There exist methods to adapt 3D-ODs to these domains; however, these methods treat 3D-ODs as a black box, neglecting underlying architectural decisions and source-domain training strategies. Instead, we dive deep into the details of 3D-ODs, focusing our efforts on fundamental factors that influence robustness prior to domain adaptation.   We systematically investigate four design choices (and the interplay between them) often overlooked in 3D-OD robustness and domain adaptation: architecture, voxel encoding, data augmentations, and anchor strategies. We assess their impact on the robustness of nine state-of-the-art 3D-ODs across six benchmarks encompassing three types of domain gaps - sensor type, weather, and location.   Our main findings are: (1) transformer backbones with local point features are more robust than 3D CNNs, (2) test-time anchor size adjustment is crucial for adaptation across geographical locations, significantly boosting scores without retraining, (3) source-domain augmentations allow the model to generalize to low-resolution sensors, and (4) surprisingly, robustness to bad weather is improved when training directly on more clean weather data than on training with bad weather data. We outline our main conclusions and findings to provide practical guidance on developing more robust 3D-ODs.","sentences":["3D Object Detectors (3D-OD) are crucial for understanding the environment in many robotic tasks, especially autonomous driving.","Including 3D information via Lidar sensors improves accuracy greatly.","However, such detectors perform poorly on domains they were not trained on, i.e. different locations, sensors, weather, etc., limiting their reliability in safety-critical applications.","There exist methods to adapt 3D-ODs to these domains; however, these methods treat 3D-ODs as a black box, neglecting underlying architectural decisions and source-domain training strategies.","Instead, we dive deep into the details of 3D-ODs, focusing our efforts on fundamental factors that influence robustness prior to domain adaptation.   ","We systematically investigate four design choices (and the interplay between them) often overlooked in 3D-OD robustness and domain adaptation: architecture, voxel encoding, data augmentations, and anchor strategies.","We assess their impact on the robustness of nine state-of-the-art 3D-ODs across six benchmarks encompassing three types of domain gaps - sensor type, weather, and location.   ","Our main findings are: (1) transformer backbones with local point features are more robust than 3D CNNs, (2) test-time anchor size adjustment is crucial for adaptation across geographical locations, significantly boosting scores without retraining, (3) source-domain augmentations allow the model to generalize to low-resolution sensors, and (4) surprisingly, robustness to bad weather is improved when training directly on more clean weather data than on training with bad weather data.","We outline our main conclusions and findings to provide practical guidance on developing more robust 3D-ODs."],"url":"http://arxiv.org/abs/2402.17562v1","category":"cs.CV"}
{"created":"2024-02-27 14:51:56","title":"Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label","abstract":"Scribble-based weakly-supervised semantic segmentation using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives. Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision. However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation. In this study, we propose a class-driven scribble promotion network, which utilizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision. Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space. To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction, which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label's boundary. Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness of our method.The code is available at https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network.","sentences":["Scribble-based weakly-supervised semantic segmentation using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives.","Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision.","However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation.","In this study, we propose a class-driven scribble promotion network, which utilizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision.","Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space.","To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction, which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label's boundary.","Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness of our method.","The code is available at https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network."],"url":"http://arxiv.org/abs/2402.17555v1","category":"cs.CV"}
{"created":"2024-02-27 14:34:14","title":"Adapting Learned Image Codecs to Screen Content via Adjustable Transformations","abstract":"As learned image codecs (LICs) become more prevalent, their low coding efficiency for out-of-distribution data becomes a bottleneck for some applications. To improve the performance of LICs for screen content (SC) images without breaking backwards compatibility, we propose to introduce parameterized and invertible linear transformations into the coding pipeline without changing the underlying baseline codec's operation flow. We design two neural networks to act as prefilters and postfilters in our setup to increase the coding efficiency and help with the recovery from coding artifacts. Our end-to-end trained solution achieves up to 10% bitrate savings on SC compression compared to the baseline LICs while introducing only 1% extra parameters.","sentences":["As learned image codecs (LICs) become more prevalent, their low coding efficiency for out-of-distribution data becomes a bottleneck for some applications.","To improve the performance of LICs for screen content (SC) images without breaking backwards compatibility, we propose to introduce parameterized and invertible linear transformations into the coding pipeline without changing the underlying baseline codec's operation flow.","We design two neural networks to act as prefilters and postfilters in our setup to increase the coding efficiency and help with the recovery from coding artifacts.","Our end-to-end trained solution achieves up to 10% bitrate savings on SC compression compared to the baseline LICs while introducing only 1% extra parameters."],"url":"http://arxiv.org/abs/2402.17544v1","category":"eess.IV"}
{"created":"2024-02-27 14:05:05","title":"AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Point Cloud Analysis","abstract":"Efficient downsampling plays a crucial role in point cloud learning, particularly for large-scale 3D scenes. Existing downsampling methods either require a huge computational burden or sacrifice fine-grained geometric information. This paper presents an advanced sampler that achieves both high accuracy and efficiency. The proposed method utilizes voxel-based sampling as a foundation, but effectively addresses the challenges regarding voxel size determination and the preservation of critical geometric cues. Specifically, we propose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the reference of point-based downsampling ratio. This ensures the sampling results exhibit a favorable distribution for comprehending various 3D objects or scenes. Additionally, we introduce a network compatible with arbitrary voxel sizes for sampling and feature extraction while maintaining high efficiency. Our method achieves state-of-the-art accuracy on the ShapeNetPart and ScanNet benchmarks with promising efficiency. Code will be available at https://github.com/yhc2021/AVS-Net.","sentences":["Efficient downsampling plays a crucial role in point cloud learning, particularly for large-scale 3D scenes.","Existing downsampling methods either require a huge computational burden or sacrifice fine-grained geometric information.","This paper presents an advanced sampler that achieves both high accuracy and efficiency.","The proposed method utilizes voxel-based sampling as a foundation, but effectively addresses the challenges regarding voxel size determination and the preservation of critical geometric cues.","Specifically, we propose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the reference of point-based downsampling ratio.","This ensures the sampling results exhibit a favorable distribution for comprehending various 3D objects or scenes.","Additionally, we introduce a network compatible with arbitrary voxel sizes for sampling and feature extraction while maintaining high efficiency.","Our method achieves state-of-the-art accuracy on the ShapeNetPart and ScanNet benchmarks with promising efficiency.","Code will be available at https://github.com/yhc2021/AVS-Net."],"url":"http://arxiv.org/abs/2402.17521v1","category":"cs.CV"}
{"created":"2024-02-27 13:57:16","title":"Integrated, bright, broadband parametric down-conversion source for quantum metrology and spectroscopy","abstract":"Broadband quantum light is a vital resource for quantum metrology and spectroscopy applications such as quantum optical coherence tomography or entangled two photon absorption. For entangled two photon absorption in particular, very high photon flux combined with high time-frequency entanglement is crucial for observing a signal. So far these conditions could be met by using high power lasers driving degenerate, type 0 bulk-crystal spontaneous parametric down conversion (SPDC) sources. This naturally limits the available wavelength ranges and precludes deterministic splitting of the generated output photons. In this work we demonstrate an integrated two-colour SPDC source utilising a group-velocity matched lithium niobate waveguide, reaching both exceptional brightness $1.52\\cdot10^6\\frac{\\mathrm{pairs}}{\\mathrm{s\\,mW\\,GHz}}$ and large bandwidth ($7.8\\,$THz FWHM) while pumped with a few mW of continuous wave (CW) laser light. By converting a narrow band pump to broadband pulses the created photon pairs show correlation times of $\\Delta \\tau \\approx 120\\,\\text{fs}$ while maintaining the narrow bandwidth $\\Delta \\omega_p \\ll 1\\,\\text{MHz}$ of the CW pump light, yielding strong time-frequency entanglement. Furthermore our process can be adapted to a wide range of central wavelengths.","sentences":["Broadband quantum light is a vital resource for quantum metrology and spectroscopy applications such as quantum optical coherence tomography or entangled two photon absorption.","For entangled two photon absorption in particular, very high photon flux combined with high time-frequency entanglement is crucial for observing a signal.","So far these conditions could be met by using high power lasers driving degenerate, type 0 bulk-crystal spontaneous parametric down conversion (SPDC) sources.","This naturally limits the available wavelength ranges and precludes deterministic splitting of the generated output photons.","In this work we demonstrate an integrated two-colour SPDC source utilising a group-velocity matched lithium niobate waveguide, reaching both exceptional brightness $1.52\\cdot10^6\\frac{\\mathrm{pairs}}{\\mathrm{s\\,mW\\,GHz}}$ and large bandwidth ($7.8\\,$THz FWHM) while pumped with a few mW of continuous wave (CW) laser light.","By converting a narrow band pump to broadband pulses the created photon pairs show correlation times of $\\Delta \\tau \\approx 120\\,\\text{fs}$ while maintaining the narrow bandwidth $\\Delta \\omega_p \\ll 1\\,\\text{MHz}$ of the CW pump light, yielding strong time-frequency entanglement.","Furthermore our process can be adapted to a wide range of central wavelengths."],"url":"http://arxiv.org/abs/2402.17515v1","category":"quant-ph"}
{"created":"2024-02-27 13:55:17","title":"Robust Unsupervised Crowd Counting and Localization with Adaptive Resolution SAM","abstract":"The existing crowd counting models require extensive training data, which is time-consuming to annotate. To tackle this issue, we propose a simple yet effective crowd counting method by utilizing the Segment-Everything-Everywhere Model (SEEM), an adaptation of the Segmentation Anything Model (SAM), to generate pseudo-labels for training crowd counting models. However, our initial investigation reveals that SEEM's performance in dense crowd scenes is limited, primarily due to the omission of many persons in high-density areas. To overcome this limitation, we propose an adaptive resolution SEEM to handle the scale variations, occlusions, and overlapping of people within crowd scenes. Alongside this, we introduce a robust localization method, based on Gaussian Mixture Models, for predicting the head positions in the predicted people masks. Given the mask and point pseudo-labels, we propose a robust loss function, which is designed to exclude uncertain regions based on SEEM's predictions, thereby enhancing the training process of the counting networks. Finally, we propose an iterative method for generating pseudo-labels. This method aims at improving the quality of the segmentation masks by identifying more tiny persons in high-density regions, which are often missed in the first pseudo-labeling stage. Overall, our proposed method achieves the best unsupervised performance in crowd counting, while also being comparable results to some supervised methods. This makes it a highly effective and versatile tool for crowd counting, especially in situations where labeled data is not available.","sentences":["The existing crowd counting models require extensive training data, which is time-consuming to annotate.","To tackle this issue, we propose a simple yet effective crowd counting method by utilizing the Segment-Everything-Everywhere Model (SEEM), an adaptation of the Segmentation Anything Model (SAM), to generate pseudo-labels for training crowd counting models.","However, our initial investigation reveals that SEEM's performance in dense crowd scenes is limited, primarily due to the omission of many persons in high-density areas.","To overcome this limitation, we propose an adaptive resolution SEEM to handle the scale variations, occlusions, and overlapping of people within crowd scenes.","Alongside this, we introduce a robust localization method, based on Gaussian Mixture Models, for predicting the head positions in the predicted people masks.","Given the mask and point pseudo-labels, we propose a robust loss function, which is designed to exclude uncertain regions based on SEEM's predictions, thereby enhancing the training process of the counting networks.","Finally, we propose an iterative method for generating pseudo-labels.","This method aims at improving the quality of the segmentation masks by identifying more tiny persons in high-density regions, which are often missed in the first pseudo-labeling stage.","Overall, our proposed method achieves the best unsupervised performance in crowd counting, while also being comparable results to some supervised methods.","This makes it a highly effective and versatile tool for crowd counting, especially in situations where labeled data is not available."],"url":"http://arxiv.org/abs/2402.17514v1","category":"cs.CV"}
{"created":"2024-02-27 13:41:32","title":"FedLPPA: Learning Personalized Prompt and Aggregation for Federated Weakly-supervised Medical Image Segmentation","abstract":"Federated learning (FL) effectively mitigates the data silo challenge brought about by policies and privacy concerns, implicitly harnessing more data for deep model training. However, traditional centralized FL models grapple with diverse multi-center data, especially in the face of significant data heterogeneity, notably in medical contexts. In the realm of medical image segmentation, the growing imperative to curtail annotation costs has amplified the importance of weakly-supervised techniques which utilize sparse annotations such as points, scribbles, etc. A pragmatic FL paradigm shall accommodate diverse annotation formats across different sites, which research topic remains under-investigated. In such context, we propose a novel personalized FL framework with learnable prompt and aggregation (FedLPPA) to uniformly leverage heterogeneous weak supervision for medical image segmentation. In FedLPPA, a learnable universal knowledge prompt is maintained, complemented by multiple learnable personalized data distribution prompts and prompts representing the supervision sparsity. Integrated with sample features through a dual-attention mechanism, those prompts empower each local task decoder to adeptly adjust to both the local distribution and the supervision form. Concurrently, a dual-decoder strategy, predicated on prompt similarity, is introduced for enhancing the generation of pseudo-labels in weakly-supervised learning, alleviating overfitting and noise accumulation inherent to local data, while an adaptable aggregation method is employed to customize the task decoder on a parameter-wise basis. Extensive experiments on three distinct medical image segmentation tasks involving different modalities underscore the superiority of FedLPPA, with its efficacy closely parallels that of fully supervised centralized training. Our code and data will be available.","sentences":["Federated learning (FL) effectively mitigates the data silo challenge brought about by policies and privacy concerns, implicitly harnessing more data for deep model training.","However, traditional centralized FL models grapple with diverse multi-center data, especially in the face of significant data heterogeneity, notably in medical contexts.","In the realm of medical image segmentation, the growing imperative to curtail annotation costs has amplified the importance of weakly-supervised techniques which utilize sparse annotations such as points, scribbles, etc.","A pragmatic FL paradigm shall accommodate diverse annotation formats across different sites, which research topic remains under-investigated.","In such context, we propose a novel personalized FL framework with learnable prompt and aggregation (FedLPPA) to uniformly leverage heterogeneous weak supervision for medical image segmentation.","In FedLPPA, a learnable universal knowledge prompt is maintained, complemented by multiple learnable personalized data distribution prompts and prompts representing the supervision sparsity.","Integrated with sample features through a dual-attention mechanism, those prompts empower each local task decoder to adeptly adjust to both the local distribution and the supervision form.","Concurrently, a dual-decoder strategy, predicated on prompt similarity, is introduced for enhancing the generation of pseudo-labels in weakly-supervised learning, alleviating overfitting and noise accumulation inherent to local data, while an adaptable aggregation method is employed to customize the task decoder on a parameter-wise basis.","Extensive experiments on three distinct medical image segmentation tasks involving different modalities underscore the superiority of FedLPPA, with its efficacy closely parallels that of fully supervised centralized training.","Our code and data will be available."],"url":"http://arxiv.org/abs/2402.17502v1","category":"cs.CV"}
{"created":"2024-02-27 13:22:51","title":"REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering","abstract":"Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness of source relevance for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a new architecture for LLM based RAG system, by incorporating a specially designed rank head that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training. By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents. Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches. Our code and data can be accessed at https://github.com/RUCAIBox/REAR.","sentences":["Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs).","Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents).","To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA).","As the key motivation, we aim to enhance the self-awareness of source relevance for LLMs, so as to adaptively utilize external knowledge in RAG systems.","Specially, we develop a new architecture for LLM based RAG system, by incorporating a specially designed rank head that precisely assesses the relevance of retrieved documents.","Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training.","By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents.","Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches.","Our code and data can be accessed at https://github.com/RUCAIBox/REAR."],"url":"http://arxiv.org/abs/2402.17497v1","category":"cs.CL"}
{"created":"2024-02-27 13:18:00","title":"Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?","abstract":"Postoperative risk predictions can inform effective perioperative care management and planning. We aimed to assess whether clinical large language models (LLMs) can predict postoperative risks using clinical texts with various training strategies. The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth Israel Deaconess's MIMIC dataset. Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days. For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia. Three domain adaptation and finetuning strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives; incorporating labels with semi-supervised fine-tuning; and foundational modelling through multi-task learning. Model performance was compared using the area under the receiver operating characteristic curve (AUROC) and the area under the precision recall curve (AUPRC) for classification tasks, and mean squared error (MSE) and R2 for regression tasks. Pre-trained LLMs outperformed traditional word embeddings, with absolute maximal gains of 38.3% for AUROC and 14% for AUPRC. Adapting models further improved performance: (1) self-supervised finetuning by 3.2% for AUROC and 1.5% for AUPRC; (2) semi-supervised finetuning by 1.8% for AUROC and 2% for AUPRC, compared to self-supervised finetuning; (3) foundational modelling by 3.6% for AUROC and 2.6% for AUPRC, compared to self-supervised finetuning. Pre-trained clinical LLMs offer opportunities for postoperative risk predictions in unforeseen data, with peaks in foundational models indicating the potential of task-agnostic learning towards the generalizability of LLMs in perioperative care.","sentences":["Postoperative risk predictions can inform effective perioperative care management and planning.","We aimed to assess whether clinical large language models (LLMs) can predict postoperative risks using clinical texts with various training strategies.","The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021.","Methods were replicated on Beth Israel Deaconess's MIMIC dataset.","Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days.","For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia.","Three domain adaptation and finetuning strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives; incorporating labels with semi-supervised fine-tuning; and foundational modelling through multi-task learning.","Model performance was compared using the area under the receiver operating characteristic curve (AUROC) and the area under the precision recall curve (AUPRC) for classification tasks, and mean squared error (MSE) and R2 for regression tasks.","Pre-trained LLMs outperformed traditional word embeddings, with absolute maximal gains of 38.3% for AUROC and 14% for AUPRC.","Adapting models further improved performance: (1) self-supervised finetuning by 3.2% for AUROC and 1.5% for AUPRC; (2) semi-supervised finetuning by 1.8% for AUROC and 2% for AUPRC, compared to self-supervised finetuning; (3) foundational modelling by 3.6% for AUROC and 2.6% for AUPRC, compared to self-supervised finetuning.","Pre-trained clinical LLMs offer opportunities for postoperative risk predictions in unforeseen data, with peaks in foundational models indicating the potential of task-agnostic learning towards the generalizability of LLMs in perioperative care."],"url":"http://arxiv.org/abs/2402.17493v2","category":"cs.CL"}
{"created":"2024-02-27 13:08:26","title":"Numerical Schemes for 3-Wave Kinetic Equations: A Complete Treatment of the Collision Operator","abstract":"In our previous work, numerical schemes for a simplified version of 3-wave kinetic equations, in which only the simple forward-cascade terms of the collision operators are kept, have been successfully designed, especially to capture the long time dynamics of the equation given the multiple blow-up time phenomenon. In this second work in the series, we propose numerical treatments for the complete 3-wave kinetic equations, in which the complete, much more complicated collision operators are fully considered based on a novel conservative form of the equation. We then derive an implicit finite volume scheme to solve the equation. The new discretization uses an adaptive time-stepping method which allows for the simulations to be carried to very long times. Our computed solutions are compared with previously derived long-time asymptotic estimates for the decay rate of total energy of time-dependent solutions of 3-wave kinetic equations and found to be in excellent agreement.","sentences":["In our previous work, numerical schemes for a simplified version of 3-wave kinetic equations, in which only the simple forward-cascade terms of the collision operators are kept, have been successfully designed, especially to capture the long time dynamics of the equation given the multiple blow-up time phenomenon.","In this second work in the series, we propose numerical treatments for the complete 3-wave kinetic equations, in which the complete, much more complicated collision operators are fully considered based on a novel conservative form of the equation.","We then derive an implicit finite volume scheme to solve the equation.","The new discretization uses an adaptive time-stepping method which allows for the simulations to be carried to very long times.","Our computed solutions are compared with previously derived long-time asymptotic estimates for the decay rate of total energy of time-dependent solutions of 3-wave kinetic equations and found to be in excellent agreement."],"url":"http://arxiv.org/abs/2402.17481v1","category":"math.NA"}
{"created":"2024-02-27 12:52:44","title":"Bit Distribution Study and Implementation of Spatial Quality Map in the JPEG-AI Standardization","abstract":"Currently, there is a high demand for neural network-based image compression codecs. These codecs employ non-linear transforms to create compact bit representations and facilitate faster coding speeds on devices compared to the hand-crafted transforms used in classical frameworks. The scientific and industrial communities are highly interested in these properties, leading to the standardization effort of JPEG-AI. The JPEG-AI verification model has been released and is currently under development for standardization. Utilizing neural networks, it can outperform the classic codec VVC intra by over 10% BD-rate operating at base operation point. Researchers attribute this success to the flexible bit distribution in the spatial domain, in contrast to VVC intra's anchor that is generated with a constant quality point. However, our study reveals that VVC intra displays a more adaptable bit distribution structure through the implementation of various block sizes. As a result of our observations, we have proposed a spatial bit allocation method to optimize the JPEG-AI verification model's bit distribution and enhance the visual quality. Furthermore, by applying the VVC bit distribution strategy, the objective performance of JPEG-AI verification mode can be further improved, resulting in a maximum gain of 0.45 dB in PSNR-Y.","sentences":["Currently, there is a high demand for neural network-based image compression codecs.","These codecs employ non-linear transforms to create compact bit representations and facilitate faster coding speeds on devices compared to the hand-crafted transforms used in classical frameworks.","The scientific and industrial communities are highly interested in these properties, leading to the standardization effort of JPEG-AI.","The JPEG-AI verification model has been released and is currently under development for standardization.","Utilizing neural networks, it can outperform the classic codec VVC intra by over 10% BD-rate operating at base operation point.","Researchers attribute this success to the flexible bit distribution in the spatial domain, in contrast to VVC intra's anchor that is generated with a constant quality point.","However, our study reveals that VVC intra displays a more adaptable bit distribution structure through the implementation of various block sizes.","As a result of our observations, we have proposed a spatial bit allocation method to optimize the JPEG-AI verification model's bit distribution and enhance the visual quality.","Furthermore, by applying the VVC bit distribution strategy, the objective performance of JPEG-AI verification mode can be further improved, resulting in a maximum gain of 0.45 dB in PSNR-Y."],"url":"http://arxiv.org/abs/2402.17470v1","category":"cs.CV"}
{"created":"2024-02-27 12:26:07","title":"DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning","abstract":"In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success rate in the development stage, while attaining 36% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per run with GPT-4, respectively.","sentences":["In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models.","Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario.","To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR).","In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism.","Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs.","Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success rate in the development stage, while attaining 36% improvement on average one pass rate across alternative LLMs in the deployment stage.","In both stages, DS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per run with GPT-4, respectively."],"url":"http://arxiv.org/abs/2402.17453v1","category":"cs.LG"}
{"created":"2024-02-27 10:48:56","title":"Beacon, a lightweight deep reinforcement learning benchmark library for flow control","abstract":"Recently, the increasing use of deep reinforcement learning for flow control problems has led to a new area of research, focused on the coupling and the adaptation of the existing algorithms to the control of numerical fluid dynamics environments. Although still in its infancy, the field has seen multiple successes in a short time span, and its fast development pace can certainly be partly imparted to the open-source effort that drives the expansion of the community. Yet, this emerging domain still misses a common ground to (i) ensure the reproducibility of the results, and (ii) offer a proper ad-hoc benchmarking basis. To this end, we propose Beacon, an open-source benchmark library composed of seven lightweight 1D and 2D flow control problems with various characteristics, action and observation space characteristics, and CPU requirements. In this contribution, the seven considered problems are described, and reference control solutions are provided. The sources for the following work are available at https://github.com/jviquerat/beacon.","sentences":["Recently, the increasing use of deep reinforcement learning for flow control problems has led to a new area of research, focused on the coupling and the adaptation of the existing algorithms to the control of numerical fluid dynamics environments.","Although still in its infancy, the field has seen multiple successes in a short time span, and its fast development pace can certainly be partly imparted to the open-source effort that drives the expansion of the community.","Yet, this emerging domain still misses a common ground to (i) ensure the reproducibility of the results, and (ii) offer a proper ad-hoc benchmarking basis.","To this end, we propose Beacon, an open-source benchmark library composed of seven lightweight 1D and 2D flow control problems with various characteristics, action and observation space characteristics, and CPU requirements.","In this contribution, the seven considered problems are described, and reference control solutions are provided.","The sources for the following work are available at https://github.com/jviquerat/beacon."],"url":"http://arxiv.org/abs/2402.17402v1","category":"physics.comp-ph"}
{"created":"2024-02-27 10:47:24","title":"Investigating Continual Pretraining in Large Language Models: Insights and Implications","abstract":"This paper studies the evolving domain of Continual Learning (CL) in large language models (LLMs), with a focus on developing strategies for efficient and sustainable training. Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge and enhancing cross-domain knowledge transfer without relying on domain-specific identification. Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting, our research evaluates the adaptability and capabilities of LLMs to changing data landscapes in practical scenarios. To this end, we introduce a new benchmark designed to measure the adaptability of LLMs to these evolving data environments, offering a comprehensive framework for evaluation. We examine the impact of model size on learning efficacy and forgetting, as well as how the progression and similarity of emerging domains affect the knowledge transfer within these models. Our findings uncover several key insights: (i) when the sequence of domains shows semantic similarity, continual pretraining enables LLMs to better specialize in the current domain compared to stand-alone fine-tuning, (ii) training across a diverse range of domains enhances both backward and forward knowledge transfer, and (iii) smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both forgetting and learning. We posit that our research marks a shift towards establishing a more realistic benchmark for investigating CL in LLMs, and has the potential to play a key role in guiding the direction of future research in the field.","sentences":["This paper studies the evolving domain of Continual Learning (CL) in large language models (LLMs), with a focus on developing strategies for efficient and sustainable training.","Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge and enhancing cross-domain knowledge transfer without relying on domain-specific identification.","Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting, our research evaluates the adaptability and capabilities of LLMs to changing data landscapes in practical scenarios.","To this end, we introduce a new benchmark designed to measure the adaptability of LLMs to these evolving data environments, offering a comprehensive framework for evaluation.","We examine the impact of model size on learning efficacy and forgetting, as well as how the progression and similarity of emerging domains affect the knowledge transfer within these models.","Our findings uncover several key insights: (i) when the sequence of domains shows semantic similarity, continual pretraining enables LLMs to better specialize in the current domain compared to stand-alone fine-tuning, (ii) training across a diverse range of domains enhances both backward and forward knowledge transfer, and (iii) smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both forgetting and learning.","We posit that our research marks a shift towards establishing a more realistic benchmark for investigating CL in LLMs, and has the potential to play a key role in guiding the direction of future research in the field."],"url":"http://arxiv.org/abs/2402.17400v1","category":"cs.CL"}
{"created":"2024-02-27 09:55:34","title":"CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks","abstract":"Dynamic graphs are extensively employed for detecting anomalous behavior in nodes within the Internet of Things (IoT). Generative models are often used to address the issue of imbalanced node categories in dynamic graphs. Nevertheless, the constraints it faces include the monotonicity of adjacency relationships, the difficulty in constructing multi-dimensional features for nodes, and the lack of a method for end-to-end generation of multiple categories of nodes. This paper presents a novel graph generation model, called CGGM, designed specifically to generate a larger number of nodes belonging to the minority class. The mechanism for generating an adjacency matrix, through adaptive sparsity, enhances flexibility in its structure. The feature generation module, called multidimensional features generator (MFG) to generate node features along with topological information. Labels are transformed into embedding vectors, serving as conditional constraints to control the generation of synthetic data across multiple categories. Using a multi-stage loss, the distribution of synthetic data is adjusted to closely resemble that of real data. In extensive experiments, we show that CGGM's synthetic data outperforms state-of-the-art methods across various metrics. Our results demonstrate efficient generation of diverse data categories, robustly enhancing multi-category classification model performance.","sentences":["Dynamic graphs are extensively employed for detecting anomalous behavior in nodes within the Internet of Things (IoT).","Generative models are often used to address the issue of imbalanced node categories in dynamic graphs.","Nevertheless, the constraints it faces include the monotonicity of adjacency relationships, the difficulty in constructing multi-dimensional features for nodes, and the lack of a method for end-to-end generation of multiple categories of nodes.","This paper presents a novel graph generation model, called CGGM, designed specifically to generate a larger number of nodes belonging to the minority class.","The mechanism for generating an adjacency matrix, through adaptive sparsity, enhances flexibility in its structure.","The feature generation module, called multidimensional features generator (MFG) to generate node features along with topological information.","Labels are transformed into embedding vectors, serving as conditional constraints to control the generation of synthetic data across multiple categories.","Using a multi-stage loss, the distribution of synthetic data is adjusted to closely resemble that of real data.","In extensive experiments, we show that CGGM's synthetic data outperforms state-of-the-art methods across various metrics.","Our results demonstrate efficient generation of diverse data categories, robustly enhancing multi-category classification model performance."],"url":"http://arxiv.org/abs/2402.17363v1","category":"cs.RO"}
{"created":"2024-02-27 09:52:27","title":"SoFA: Shielded On-the-fly Alignment via Priority Rule Following","abstract":"The alignment problem in Large Language Models (LLMs) involves adapting them to the broad spectrum of human values. This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards. This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions. Our preliminary analysis reveals that even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules. Therefore, we present PriorityDistill, a semi-automated approach for distilling priority following signals from LLM simulations to ensure robust rule integration and adherence. Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that the model responds appropriately.","sentences":["The alignment problem in Large Language Models (LLMs) involves adapting them to the broad spectrum of human values.","This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards.","This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions.","Our preliminary analysis reveals that even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules.","Therefore, we present PriorityDistill, a semi-automated approach for distilling priority following signals from LLM simulations to ensure robust rule integration and adherence.","Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that the model responds appropriately."],"url":"http://arxiv.org/abs/2402.17358v1","category":"cs.CL"}
{"created":"2024-02-27 08:47:19","title":"Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation","abstract":"The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices. Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides. However, in many real-world scenarios, the test environments may change dynamically (known as distribution shifts), which often results in degraded performance. Thus, one has to adapt the edge models promptly to attain promising performance. Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance. To address these, we encounter two primary challenges: 1) the edge model has limited computation power and may only support forward propagation; 2) the data transmission budget between cloud and edge devices is limited in latency-sensitive scenarios. In this paper, we establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online. In our CEMA, to reduce the communication burden, we devise two criteria to exclude unnecessary samples from uploading to the cloud, i.e., dynamic unreliable and low-informative sample exclusion. Based on the uploaded samples, we update and distribute the affine parameters of normalization layers by distilling from the stronger foundation model to the edge model with a sample replay strategy. Extensive experimental results on ImageNet-C and ImageNet-R verify the effectiveness of our CEMA.","sentences":["The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices.","Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides.","However, in many real-world scenarios, the test environments may change dynamically (known as distribution shifts), which often results in degraded performance.","Thus, one has to adapt the edge models promptly to attain promising performance.","Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance.","To address these, we encounter two primary challenges: 1) the edge model has limited computation power and may only support forward propagation; 2) the data transmission budget between cloud and edge devices is limited in latency-sensitive scenarios.","In this paper, we establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online.","In our CEMA, to reduce the communication burden, we devise two criteria to exclude unnecessary samples from uploading to the cloud, i.e., dynamic unreliable and low-informative sample exclusion.","Based on the uploaded samples, we update and distribute the affine parameters of normalization layers by distilling from the stronger foundation model to the edge model with a sample replay strategy.","Extensive experimental results on ImageNet-C and ImageNet-R verify the effectiveness of our CEMA."],"url":"http://arxiv.org/abs/2402.17316v1","category":"cs.CV"}
{"created":"2024-02-27 08:20:45","title":"ArcSin: Adaptive ranged cosine Similarity injected noise for Language-Driven Visual Tasks","abstract":"In this study, we address the challenging task of bridging the modality gap between learning from language and inference for visual tasks, including Visual Question Answering (VQA), Image Captioning (IC) and Visual Entailment (VE). We train models for these tasks in a zero-shot cross-modal transfer setting, a domain where the previous state-of-the-art method relied on the fixed scale noise injection, often compromising the semantic content of the original modality embedding. To combat it, we propose a novel method called Adaptive ranged cosine Similarity injected noise (ArcSin). First, we introduce an innovative adaptive noise scale that effectively generates the textual elements with more variability while preserving the original text feature's integrity. Second, a similarity pool strategy is employed, expanding the domain generalization potential by broadening the overall noise scale. This dual strategy effectively widens the scope of the original domain while safeguarding content integrity. Our empirical results demonstrate that these models closely rival those trained on images in terms of performance. Specifically, our method exhibits substantial improvements over the previous state-of-the-art, achieving gains of 1.9 and 1.1 CIDEr points in S-Cap and M-Cap, respectively. Additionally, we observe increases of 1.5 percentage points (pp), 1.4 pp, and 1.4 pp in accuracy for VQA, VQA-E, and VE, respectively, pushing the boundaries of what is achievable within the constraints of image-trained model benchmarks. The code will be released.","sentences":["In this study, we address the challenging task of bridging the modality gap between learning from language and inference for visual tasks, including Visual Question Answering (VQA), Image Captioning (IC) and Visual Entailment (VE).","We train models for these tasks in a zero-shot cross-modal transfer setting, a domain where the previous state-of-the-art method relied on the fixed scale noise injection, often compromising the semantic content of the original modality embedding.","To combat it, we propose a novel method called Adaptive ranged cosine Similarity injected noise (ArcSin).","First, we introduce an innovative adaptive noise scale that effectively generates the textual elements with more variability while preserving the original text feature's integrity.","Second, a similarity pool strategy is employed, expanding the domain generalization potential by broadening the overall noise scale.","This dual strategy effectively widens the scope of the original domain while safeguarding content integrity.","Our empirical results demonstrate that these models closely rival those trained on images in terms of performance.","Specifically, our method exhibits substantial improvements over the previous state-of-the-art, achieving gains of 1.9 and 1.1 CIDEr points in S-Cap and M-Cap, respectively.","Additionally, we observe increases of 1.5 percentage points (pp), 1.4 pp, and 1.4 pp in accuracy for VQA, VQA-E, and VE, respectively, pushing the boundaries of what is achievable within the constraints of image-trained model benchmarks.","The code will be released."],"url":"http://arxiv.org/abs/2402.17298v1","category":"cs.CV"}
{"created":"2024-02-27 08:14:50","title":"Advancing Continuous Distribution Generation: An Exponentiated Odds Ratio Generator Approach","abstract":"This paper presents a new methodology for generating continuous statistical distributions, integrating the exponentiated odds ratio within the framework of survival analysis. This new method enhances the flexibility and adaptability of distribution models to effectively address the complexities inherent in contemporary datasets. The core of this advancement is illustrated by introducing a particular subfamily, the \"Type-2 Gumbel Weibull-G Family of Distributions.\" We provide a comprehensive analysis of the mathematical properties of these distributions, encompassing statistical properties such as density functions, moments, hazard rate and quantile functions, R\\'enyi entropy, order statistics, and the concept of stochastic ordering. To establish the robustness of our approach, we apply five distinct methods for parameter estimation. The practical applicability of the Type-2 Gumbel Weibull-G distributions is further supported through the analysis of three real-world datasets. These empirical applications illustrate the exceptional statistical precision of our distributions compared to existing models, thereby reinforcing their significant value in both theoretical and practical statistical applications.","sentences":["This paper presents a new methodology for generating continuous statistical distributions, integrating the exponentiated odds ratio within the framework of survival analysis.","This new method enhances the flexibility and adaptability of distribution models to effectively address the complexities inherent in contemporary datasets.","The core of this advancement is illustrated by introducing a particular subfamily, the \"Type-2 Gumbel Weibull-G Family of Distributions.\"","We provide a comprehensive analysis of the mathematical properties of these distributions, encompassing statistical properties such as density functions, moments, hazard rate and quantile functions, R\\'enyi entropy, order statistics, and the concept of stochastic ordering.","To establish the robustness of our approach, we apply five distinct methods for parameter estimation.","The practical applicability of the Type-2 Gumbel Weibull-G distributions is further supported through the analysis of three real-world datasets.","These empirical applications illustrate the exceptional statistical precision of our distributions compared to existing models, thereby reinforcing their significant value in both theoretical and practical statistical applications."],"url":"http://arxiv.org/abs/2402.17294v1","category":"math.ST"}
{"created":"2024-02-27 07:53:32","title":"Distribution of number of peaks within a long gamma-ray burst","abstract":"The variety of long duration gamma-ray burst (LGRB) light curves (LCs) encode a wealth of information on how LGRB engines release energy following the collapse of the progenitor star. Attempts to characterise GRB LCs focused on a number of properties, such as the minimum variability timescale, power density spectra (both ensemble average and individual), or with different definitions of variability. In parallel, a characterisation as a stochastic process was pursued by studying the distributions of waiting times, peak flux, fluence of individual peaks within GRB time profiles. Yet, the question remains as to whether the diversity of profiles can be described in terms of a common stochastic process. Here we address this issue by studying for the first time the distribution of the number of peaks in a GRB profile. We used four different GRB catalogues: CGRO/BATSE, Swift/BAT, BeppoSAX/GRBM, and Insight-HXMT. The statistically significant peaks were identified by means of well tested algorithm MEPSA and further selected by applying a set of thresholds on signal-to-noise ratio. We then extracted the corresponding distributions of number of peaks per GRB. Among the different models considered (power-law, simple or stretched exponential) only a mixture of two exponentials models all the observed distributions, suggesting the existence of two distinct behaviours: (i) an average number of 2.1+-0.1 peaks per GRB (\"peak poor\") and accounting for about 80% of the observed population of GRBs; (ii) an average number of 8.3+-1.0 peaks per GRB (\"peak rich\") and accounting for the remaining 20% of the observed population. We associate the class of peak-rich GRBs with the presence of sub-second variability, which seems to be absent among peak-poor GRBs. The two classes could result from two different regimes through which GRB engines release energy or through which energy is dissipated into gamma-rays.","sentences":["The variety of long duration gamma-ray burst (LGRB) light curves (LCs) encode a wealth of information on how LGRB engines release energy following the collapse of the progenitor star.","Attempts to characterise GRB LCs focused on a number of properties, such as the minimum variability timescale, power density spectra (both ensemble average and individual), or with different definitions of variability.","In parallel, a characterisation as a stochastic process was pursued by studying the distributions of waiting times, peak flux, fluence of individual peaks within GRB time profiles.","Yet, the question remains as to whether the diversity of profiles can be described in terms of a common stochastic process.","Here we address this issue by studying for the first time the distribution of the number of peaks in a GRB profile.","We used four different GRB catalogues: CGRO/BATSE, Swift/BAT, BeppoSAX/GRBM, and Insight-HXMT.","The statistically significant peaks were identified by means of well tested algorithm MEPSA and further selected by applying a set of thresholds on signal-to-noise ratio.","We then extracted the corresponding distributions of number of peaks per GRB.","Among the different models considered (power-law, simple or stretched exponential) only a mixture of two exponentials models all the observed distributions, suggesting the existence of two distinct behaviours: (i) an average number of 2.1+-0.1 peaks per GRB (\"peak poor\") and accounting for about 80% of the observed population of GRBs; (ii) an average number of 8.3+-1.0 peaks per GRB (\"peak rich\") and accounting for the remaining 20% of the observed population.","We associate the class of peak-rich GRBs with the presence of sub-second variability, which seems to be absent among peak-poor GRBs.","The two classes could result from two different regimes through which GRB engines release energy or through which energy is dissipated into gamma-rays."],"url":"http://arxiv.org/abs/2402.17282v1","category":"astro-ph.HE"}
{"created":"2024-02-27 07:46:54","title":"RISAR: RIS-assisted Human Activity Recognition with Commercial Wi-Fi Devices","abstract":"Human activity recognition (HAR) is integral to smart homes, security, and healthcare. Existing systems face challenges such as the absence of line-of-sight links, insufficient details regarding the sensing subject, and inefficiencies in noise reduction and feature extraction from channel state information. To address these issues, this study builds a reconfigurable intelligent surface (RIS)-assisted passive human activity recognition (RISAR) system, compatible with commercial Wi-Fi devices. RISAR employs a RIS to augment the orientation and spatial diversity of Wi-Fi signals, thereby facilitating improved detection capabilities. A new denoising and feature extraction technique, the high-dimensional factor model, based on random matrix theory, is proposed for noise elimination and salient temporal feature extraction. On this basis, a dual-stream spatial-temporal attention network model is developed for assigning variable weights to different characteristics and sequences, mimicking human cognitive processes in prioritizing essential information. Experimental analysis shows that RISAR significantly outperforms existing HAR systems in accuracy and efficiency, achieving an average accuracy of 97.26%. These findings not only highlight the adaptability of the RISAR system but also underscore its potential as a robust solution for activity recognition across complex environments.","sentences":["Human activity recognition (HAR) is integral to smart homes, security, and healthcare.","Existing systems face challenges such as the absence of line-of-sight links, insufficient details regarding the sensing subject, and inefficiencies in noise reduction and feature extraction from channel state information.","To address these issues, this study builds a reconfigurable intelligent surface (RIS)-assisted passive human activity recognition (RISAR) system, compatible with commercial Wi-Fi devices.","RISAR employs a RIS to augment the orientation and spatial diversity of Wi-Fi signals, thereby facilitating improved detection capabilities.","A new denoising and feature extraction technique, the high-dimensional factor model, based on random matrix theory, is proposed for noise elimination and salient temporal feature extraction.","On this basis, a dual-stream spatial-temporal attention network model is developed for assigning variable weights to different characteristics and sequences, mimicking human cognitive processes in prioritizing essential information.","Experimental analysis shows that RISAR significantly outperforms existing HAR systems in accuracy and efficiency, achieving an average accuracy of 97.26%.","These findings not only highlight the adaptability of the RISAR system but also underscore its potential as a robust solution for activity recognition across complex environments."],"url":"http://arxiv.org/abs/2402.17277v1","category":"eess.SY"}
{"created":"2024-02-27 07:25:02","title":"Reinforcement Learning Based Robust Volt/Var Control in Active Distribution Networks With Imprecisely Known Delay","abstract":"Active distribution networks (ADNs) incorporating massive photovoltaic (PV) devices encounter challenges of rapid voltage fluctuations and potential violations. Due to the fluctuation and intermittency of PV generation, the state gap, arising from time-inconsistent states and exacerbated by imprecisely known system delays, significantly impacts the accuracy of voltage control. This paper addresses this challenge by introducing a framework for delay adaptive Volt/Var control (VVC) in the presence of imprecisely known system delays to regulate the reactive power of PV inverters. The proposed approach formulates the voltage control, based on predicted system operation states, as a robust VVC problem. It employs sample selection from the state prediction interval to promptly identify the worst-performing system operation state. Furthermore, we leverage the decentralized partially observable Markov decision process (Dec-POMDP) to reformulate the robust VVC problem. We design Multiple Policy Networks and employ Multiple Policy Networks and Reward Shaping-based Multi-agent Twin Delayed Deep Deterministic Policy Gradient (MPNRS-MATD3) algorithm to efficiently address and solve the Dec-POMDP model-based problem. Simulation results show the delay adaption characteristic of our proposed framework, and the MPNRS-MATD3 outperforms other multi-agent reinforcement learning algorithms in robust voltage control.","sentences":["Active distribution networks (ADNs) incorporating massive photovoltaic (PV) devices encounter challenges of rapid voltage fluctuations and potential violations.","Due to the fluctuation and intermittency of PV generation, the state gap, arising from time-inconsistent states and exacerbated by imprecisely known system delays, significantly impacts the accuracy of voltage control.","This paper addresses this challenge by introducing a framework for delay adaptive Volt/Var control (VVC) in the presence of imprecisely known system delays to regulate the reactive power of PV inverters.","The proposed approach formulates the voltage control, based on predicted system operation states, as a robust VVC problem.","It employs sample selection from the state prediction interval to promptly identify the worst-performing system operation state.","Furthermore, we leverage the decentralized partially observable Markov decision process (Dec-POMDP) to reformulate the robust VVC problem.","We design Multiple Policy Networks and employ Multiple Policy Networks and Reward Shaping-based Multi-agent Twin Delayed Deep Deterministic Policy Gradient (MPNRS-MATD3) algorithm to efficiently address and solve the Dec-POMDP model-based problem.","Simulation results show the delay adaption characteristic of our proposed framework, and the MPNRS-MATD3 outperforms other multi-agent reinforcement learning algorithms in robust voltage control."],"url":"http://arxiv.org/abs/2402.17268v1","category":"eess.SY"}
{"created":"2024-02-27 07:14:12","title":"Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning","abstract":"Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring pre-trained large language models (LLMs), especially as the models' scale and the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters. However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning. We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential. The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters. This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability. We conduct a theoretical analysis and empirical studies on various NLP tasks. Our experimental results show that, compared to LoRA, MELoRA achieves better performance with 8 times fewer trainable parameters on natural language understanding tasks and 36 times fewer trainable parameters on instruction following tasks, which demonstrates the effectiveness of MELoRA.","sentences":["Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring pre-trained large language models (LLMs), especially as the models' scale and the diversity of tasks increase.","Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters.","However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning.","We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential.","The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters.","This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability.","We conduct a theoretical analysis and empirical studies on various NLP tasks.","Our experimental results show that, compared to LoRA, MELoRA achieves better performance with 8 times fewer trainable parameters on natural language understanding tasks and 36 times fewer trainable parameters on instruction following tasks, which demonstrates the effectiveness of MELoRA."],"url":"http://arxiv.org/abs/2402.17263v1","category":"cs.CL"}
{"created":"2024-02-27 06:47:52","title":"Deep Learning-Based Speech and Vision Synthesis to Improve Phishing Attack Detection through a Multi-layer Adaptive Framework","abstract":"The ever-evolving ways attacker continues to im prove their phishing techniques to bypass existing state-of-the-art phishing detection methods pose a mountain of challenges to researchers in both industry and academia research due to the inability of current approaches to detect complex phishing attack. Thus, current anti-phishing methods remain vulnerable to complex phishing because of the increasingly sophistication tactics adopted by attacker coupled with the rate at which new tactics are being developed to evade detection. In this research, we proposed an adaptable framework that combines Deep learning and Randon Forest to read images, synthesize speech from deep-fake videos, and natural language processing at various predictions layered to significantly increase the performance of machine learning models for phishing attack detection.","sentences":["The ever-evolving ways attacker continues to im prove their phishing techniques to bypass existing state-of-the-art phishing detection methods pose a mountain of challenges to researchers in both industry and academia research due to the inability of current approaches to detect complex phishing attack.","Thus, current anti-phishing methods remain vulnerable to complex phishing because of the increasingly sophistication tactics adopted by attacker coupled with the rate at which new tactics are being developed to evade detection.","In this research, we proposed an adaptable framework that combines Deep learning and Randon Forest to read images, synthesize speech from deep-fake videos, and natural language processing at various predictions layered to significantly increase the performance of machine learning models for phishing attack detection."],"url":"http://arxiv.org/abs/2402.17249v1","category":"cs.CR"}
{"created":"2024-02-27 06:32:56","title":"SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion Classification Using 3D Multi-Phase Imaging","abstract":"Automated classification of liver lesions in multi-phase CT and MR scans is of clinical significance but challenging. This study proposes a novel Siamese Dual-Resolution Transformer (SDR-Former) framework, specifically designed for liver lesion classification in 3D multi-phase CT and MR imaging with varying phase counts. The proposed SDR-Former utilizes a streamlined Siamese Neural Network (SNN) to process multi-phase imaging inputs, possessing robust feature representations while maintaining computational efficiency. The weight-sharing feature of the SNN is further enriched by a hybrid Dual-Resolution Transformer (DR-Former), comprising a 3D Convolutional Neural Network (CNN) and a tailored 3D Transformer for processing high- and low-resolution images, respectively. This hybrid sub-architecture excels in capturing detailed local features and understanding global contextual information, thereby, boosting the SNN's feature extraction capabilities. Additionally, a novel Adaptive Phase Selection Module (APSM) is introduced, promoting phase-specific intercommunication and dynamically adjusting each phase's influence on the diagnostic outcome. The proposed SDR-Former framework has been validated through comprehensive experiments on two clinical datasets: a three-phase CT dataset and an eight-phase MR dataset. The experimental results affirm the efficacy of the proposed framework. To support the scientific community, we are releasing our extensive multi-phase MR dataset for liver lesion analysis to the public. This pioneering dataset, being the first publicly available multi-phase MR dataset in this field, also underpins the MICCAI LLD-MMRI Challenge. The dataset is accessible at:https://bit.ly/3IyYlgN.","sentences":["Automated classification of liver lesions in multi-phase CT and MR scans is of clinical significance but challenging.","This study proposes a novel Siamese Dual-Resolution Transformer (SDR-Former) framework, specifically designed for liver lesion classification in 3D multi-phase CT and MR imaging with varying phase counts.","The proposed SDR-Former utilizes a streamlined Siamese Neural Network (SNN) to process multi-phase imaging inputs, possessing robust feature representations while maintaining computational efficiency.","The weight-sharing feature of the SNN is further enriched by a hybrid Dual-Resolution Transformer (DR-Former), comprising a 3D Convolutional Neural Network (CNN) and a tailored 3D Transformer for processing high- and low-resolution images, respectively.","This hybrid sub-architecture excels in capturing detailed local features and understanding global contextual information, thereby, boosting the SNN's feature extraction capabilities.","Additionally, a novel Adaptive Phase Selection Module (APSM) is introduced, promoting phase-specific intercommunication and dynamically adjusting each phase's influence on the diagnostic outcome.","The proposed SDR-Former framework has been validated through comprehensive experiments on two clinical datasets: a three-phase CT dataset and an eight-phase MR dataset.","The experimental results affirm the efficacy of the proposed framework.","To support the scientific community, we are releasing our extensive multi-phase MR dataset for liver lesion analysis to the public.","This pioneering dataset, being the first publicly available multi-phase MR dataset in this field, also underpins the MICCAI LLD-MMRI Challenge.","The dataset is accessible at:https://bit.ly/3IyYlgN."],"url":"http://arxiv.org/abs/2402.17246v1","category":"eess.IV"}
{"created":"2024-02-27 05:42:38","title":"Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology","abstract":"Multiple instance learning (MIL) is the most widely used framework in computational pathology, encompassing sub-typing, diagnosis, prognosis, and more. However, the existing MIL paradigm typically requires an offline instance feature extractor, such as a pre-trained ResNet or a foundation model. This approach lacks the capability for feature fine-tuning within the specific downstream tasks, limiting its adaptability and performance. To address this issue, we propose a Re-embedded Regional Transformer (R$^2$T) for re-embedding the instance features online, which captures fine-grained local features and establishes connections across different regions. Unlike existing works that focus on pre-training powerful feature extractor or designing sophisticated instance aggregator, R$^2$T is tailored to re-embed instance features online. It serves as a portable module that can seamlessly integrate into mainstream MIL models. Extensive experimental results on common computational pathology tasks validate that: 1) feature re-embedding improves the performance of MIL models based on ResNet-50 features to the level of foundation model features, and further enhances the performance of foundation model features; 2) the R$^2$T can introduce more significant performance improvements to various MIL models; 3) R$^2$T-MIL, as an R$^2$T-enhanced AB-MIL, outperforms other latest methods by a large margin. The code is available at:~\\href{https://github.com/DearCaat/RRT-MIL}{https://github.com/DearCaat/RRT-MIL}.","sentences":["Multiple instance learning (MIL) is the most widely used framework in computational pathology, encompassing sub-typing, diagnosis, prognosis, and more.","However, the existing MIL paradigm typically requires an offline instance feature extractor, such as a pre-trained ResNet or a foundation model.","This approach lacks the capability for feature fine-tuning within the specific downstream tasks, limiting its adaptability and performance.","To address this issue, we propose a Re-embedded Regional Transformer (R$^2$T) for re-embedding the instance features online, which captures fine-grained local features and establishes connections across different regions.","Unlike existing works that focus on pre-training powerful feature extractor or designing sophisticated instance aggregator, R$^2$T is tailored to re-embed instance features online.","It serves as a portable module that can seamlessly integrate into mainstream MIL models.","Extensive experimental results on common computational pathology tasks validate that: 1) feature re-embedding improves the performance of MIL models based on ResNet-50 features to the level of foundation model features, and further enhances the performance of foundation model features; 2) the R$^2$T can introduce more significant performance improvements to various MIL models; 3) R$^2$T-MIL, as an R$^2$T-enhanced AB-MIL, outperforms other latest methods by a large margin.","The code is available at:~\\href{https://github.com/DearCaat/RRT-MIL}{https://github.com/DearCaat/RRT-MIL}."],"url":"http://arxiv.org/abs/2402.17228v1","category":"cs.CV"}
{"created":"2024-02-27 05:40:36","title":"Efficient Backpropagation with Variance-Controlled Adaptive Sampling","abstract":"Sampling-based algorithms, which eliminate ''unimportant'' computations during forward and/or back propagation (BP), offer potential solutions to accelerate neural network training. However, since sampling introduces approximations to training, such algorithms may not consistently maintain accuracy across various tasks. In this work, we introduce a variance-controlled adaptive sampling (VCAS) method designed to accelerate BP. VCAS computes an unbiased stochastic gradient with fine-grained layerwise importance sampling in data dimension for activation gradient calculation and leverage score sampling in token dimension for weight gradient calculation. To preserve accuracy, we control the additional variance by learning the sample ratio jointly with model parameters during training. We assessed VCAS on multiple fine-tuning and pre-training tasks in both vision and natural language domains. On all the tasks, VCAS can preserve the original training loss trajectory and validation accuracy with an up to 73.87% FLOPs reduction of BP and 49.58% FLOPs reduction of the whole training process. The implementation is available at https://github.com/thu-ml/VCAS .","sentences":["Sampling-based algorithms, which eliminate ''unimportant'' computations during forward and/or back propagation (BP), offer potential solutions to accelerate neural network training.","However, since sampling introduces approximations to training, such algorithms may not consistently maintain accuracy across various tasks.","In this work, we introduce a variance-controlled adaptive sampling (VCAS) method designed to accelerate BP.","VCAS computes an unbiased stochastic gradient with fine-grained layerwise importance sampling in data dimension for activation gradient calculation and leverage score sampling in token dimension for weight gradient calculation.","To preserve accuracy, we control the additional variance by learning the sample ratio jointly with model parameters during training.","We assessed VCAS on multiple fine-tuning and pre-training tasks in both vision and natural language domains.","On all the tasks, VCAS can preserve the original training loss trajectory and validation accuracy with an up to 73.87% FLOPs reduction of BP and 49.58% FLOPs reduction of the whole training process.","The implementation is available at https://github.com/thu-ml/VCAS ."],"url":"http://arxiv.org/abs/2402.17227v1","category":"cs.LG"}
{"created":"2024-02-27 05:31:11","title":"Deadzone-Adapted Disturbance Suppression Control for Strict-Feedback Systems","abstract":"In this paper we extend our recently proposed Deadzone-Adapted Disturbance Suppression (DADS) Control approach from systems with matched uncertainties to general systems in parametric strict feedback form. The DADS approach prevents gain and state drift regardless of the size of the disturbance and unknown parameter and achieves an attenuation of the plant output to an assignable small level, despite the presence of persistent disturbances and unknown parameters of arbitrary and unknown bounds. The controller is designed by means of a step-by-step backstepping procedure which can be applied in an algorithmic fashion. Examples are provided which illustrate the efficiency of the DADS controller compared to existing adaptive control schemes.","sentences":["In this paper we extend our recently proposed Deadzone-Adapted Disturbance Suppression (DADS)","Control approach from systems with matched uncertainties to general systems in parametric strict feedback form.","The DADS approach prevents gain and state drift regardless of the size of the disturbance and unknown parameter and achieves an attenuation of the plant output to an assignable small level, despite the presence of persistent disturbances and unknown parameters of arbitrary and unknown bounds.","The controller is designed by means of a step-by-step backstepping procedure which can be applied in an algorithmic fashion.","Examples are provided which illustrate the efficiency of the DADS controller compared to existing adaptive control schemes."],"url":"http://arxiv.org/abs/2402.17222v1","category":"math.OC"}
{"created":"2024-02-27 03:58:39","title":"PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning","abstract":"Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems. These modalities provide intuitive semantics that facilitate modality-aware user preference modeling. However, two key challenges in multi-modal recommenders remain unresolved: i) The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT). ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference. To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation. Specifically, PromptMM conducts model compression through distilling u-i edge relationship and multi-modal node content from cumbersome teachers to relieve students from the additional feature reduction parameters. To bridge the semantic gap between multi-modal context and collaborative signals for empowering the overfitting teacher, soft prompt-tuning is introduced to perform student task-adaptive. Additionally, to adjust the impact of inaccuracies in multimedia data, a disentangled multi-modal list-wise distillation is developed with modality-aware re-weighting mechanism. Experiments on real-world data demonstrate PromptMM's superiority over existing techniques. Ablation tests confirm the effectiveness of key components. Additional tests show the efficiency and effectiveness.","sentences":["Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems.","These modalities provide intuitive semantics that facilitate modality-aware user preference modeling.","However, two key challenges in multi-modal recommenders remain unresolved: i)","The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT).","ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference.","To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation.","Specifically, PromptMM conducts model compression through distilling u-i edge relationship and multi-modal node content from cumbersome teachers to relieve students from the additional feature reduction parameters.","To bridge the semantic gap between multi-modal context and collaborative signals for empowering the overfitting teacher, soft prompt-tuning is introduced to perform student task-adaptive.","Additionally, to adjust the impact of inaccuracies in multimedia data, a disentangled multi-modal list-wise distillation is developed with modality-aware re-weighting mechanism.","Experiments on real-world data demonstrate PromptMM's superiority over existing techniques.","Ablation tests confirm the effectiveness of key components.","Additional tests show the efficiency and effectiveness."],"url":"http://arxiv.org/abs/2402.17188v1","category":"cs.IR"}
{"created":"2024-02-27 03:44:55","title":"Inpainting Computational Fluid Dynamics with Deep Learning","abstract":"Fluid data completion is a research problem with high potential benefit for both experimental and computational fluid dynamics. An effective fluid data completion method reduces the required number of sensors in a fluid dynamics experiment, and allows a coarser and more adaptive mesh for a Computational Fluid Dynamics (CFD) simulation. However, the ill-posed nature of the fluid data completion problem makes it prohibitively difficult to obtain a theoretical solution and presents high numerical uncertainty and instability for a data-driven approach (e.g., a neural network model). To address these challenges, we leverage recent advancements in computer vision, employing the vector quantization technique to map both complete and incomplete fluid data spaces onto discrete-valued lower-dimensional representations via a two-stage learning procedure. We demonstrated the effectiveness of our approach on Kolmogorov flow data (Reynolds number: 1000) occluded by masks of different size and arrangement. Experimental results show that our proposed model consistently outperforms benchmark models under different occlusion settings in terms of point-wise reconstruction accuracy as well as turbulent energy spectrum and vorticity distribution.","sentences":["Fluid data completion is a research problem with high potential benefit for both experimental and computational fluid dynamics.","An effective fluid data completion method reduces the required number of sensors in a fluid dynamics experiment, and allows a coarser and more adaptive mesh for a Computational Fluid Dynamics (CFD) simulation.","However, the ill-posed nature of the fluid data completion problem makes it prohibitively difficult to obtain a theoretical solution and presents high numerical uncertainty and instability for a data-driven approach (e.g., a neural network model).","To address these challenges, we leverage recent advancements in computer vision, employing the vector quantization technique to map both complete and incomplete fluid data spaces onto discrete-valued lower-dimensional representations via a two-stage learning procedure.","We demonstrated the effectiveness of our approach on Kolmogorov flow data (Reynolds number: 1000) occluded by masks of different size and arrangement.","Experimental results show that our proposed model consistently outperforms benchmark models under different occlusion settings in terms of point-wise reconstruction accuracy as well as turbulent energy spectrum and vorticity distribution."],"url":"http://arxiv.org/abs/2402.17185v1","category":"cs.LG"}
{"created":"2024-02-27 02:54:22","title":"Few-shot adaptation for morphology-independent cell instance segmentation","abstract":"Microscopy data collections are becoming larger and more frequent. Accurate and precise quantitative analysis tools like cell instance segmentation are necessary to benefit from them. This is challenging due to the variability in the data, which requires retraining the segmentation model to maintain high accuracy on new collections. This is needed especially for segmenting cells with elongated and non-convex morphology like bacteria. We propose to reduce the amount of annotation and computing power needed for retraining the model by introducing a few-shot domain adaptation approach that requires annotating only one to five cells of the new data to process and that quickly adapts the model to maintain high accuracy. Our results show a significant boost in accuracy after adaptation to very challenging bacteria datasets.","sentences":["Microscopy data collections are becoming larger and more frequent.","Accurate and precise quantitative analysis tools like cell instance segmentation are necessary to benefit from them.","This is challenging due to the variability in the data, which requires retraining the segmentation model to maintain high accuracy on new collections.","This is needed especially for segmenting cells with elongated and non-convex morphology like bacteria.","We propose to reduce the amount of annotation and computing power needed for retraining the model by introducing a few-shot domain adaptation approach that requires annotating only one to five cells of the new data to process and that quickly adapts the model to maintain high accuracy.","Our results show a significant boost in accuracy after adaptation to very challenging bacteria datasets."],"url":"http://arxiv.org/abs/2402.17165v1","category":"cs.CV"}
{"created":"2024-02-27 01:59:02","title":"Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings","abstract":"Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods. Code for this project is provided at: https://github.com/kvfrans/fre","sentences":["Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner?","In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem.","Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder.","This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples.","We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods.","Code for this project is provided at: https://github.com/kvfrans/fre"],"url":"http://arxiv.org/abs/2402.17135v1","category":"cs.LG"}
{"created":"2024-02-27 01:57:02","title":"SAM-DiffSR: Structure-Modulated Diffusion Model for Image Super-Resolution","abstract":"Diffusion-based super-resolution (SR) models have recently garnered significant attention due to their potent restoration capabilities. But conventional diffusion models perform noise sampling from a single distribution, constraining their ability to handle real-world scenes and complex textures across semantic regions. With the success of segment anything model (SAM), generating sufficiently fine-grained region masks can enhance the detail recovery of diffusion-based SR model. However, directly integrating SAM into SR models will result in much higher computational cost. In this paper, we propose the SAM-DiffSR model, which can utilize the fine-grained structure information from SAM in the process of sampling noise to improve the image quality without additional computational cost during inference. In the process of training, we encode structural position information into the segmentation mask from SAM. Then the encoded mask is integrated into the forward diffusion process by modulating it to the sampled noise. This adjustment allows us to independently adapt the noise mean within each corresponding segmentation area. The diffusion model is trained to estimate this modulated noise. Crucially, our proposed framework does NOT change the reverse diffusion process and does NOT require SAM at inference. Experimental results demonstrate the effectiveness of our proposed method, showcasing superior performance in suppressing artifacts, and surpassing existing diffusion-based methods by 0.74 dB at the maximum in terms of PSNR on DIV2K dataset. The code and dataset are available at https://github.com/lose4578/SAM-DiffSR.","sentences":["Diffusion-based super-resolution (SR) models have recently garnered significant attention due to their potent restoration capabilities.","But conventional diffusion models perform noise sampling from a single distribution, constraining their ability to handle real-world scenes and complex textures across semantic regions.","With the success of segment anything model (SAM), generating sufficiently fine-grained region masks can enhance the detail recovery of diffusion-based SR model.","However, directly integrating SAM into SR models will result in much higher computational cost.","In this paper, we propose the SAM-DiffSR model, which can utilize the fine-grained structure information from SAM in the process of sampling noise to improve the image quality without additional computational cost during inference.","In the process of training, we encode structural position information into the segmentation mask from SAM.","Then the encoded mask is integrated into the forward diffusion process by modulating it to the sampled noise.","This adjustment allows us to independently adapt the noise mean within each corresponding segmentation area.","The diffusion model is trained to estimate this modulated noise.","Crucially, our proposed framework does NOT change the reverse diffusion process and does NOT require SAM at inference.","Experimental results demonstrate the effectiveness of our proposed method, showcasing superior performance in suppressing artifacts, and surpassing existing diffusion-based methods by 0.74 dB at the maximum in terms of PSNR on DIV2K dataset.","The code and dataset are available at https://github.com/lose4578/SAM-DiffSR."],"url":"http://arxiv.org/abs/2402.17133v1","category":"cs.CV"}
{"created":"2024-02-27 01:22:08","title":"CharNeRF: 3D Character Generation from Concept Art","abstract":"3D modeling holds significant importance in the realms of AR/VR and gaming, allowing for both artistic creativity and practical applications. However, the process is often time-consuming and demands a high level of skill. In this paper, we present a novel approach to create volumetric representations of 3D characters from consistent turnaround concept art, which serves as the standard input in the 3D modeling industry. While Neural Radiance Field (NeRF) has been a game-changer in image-based 3D reconstruction, to the best of our knowledge, there is no known research that optimizes the pipeline for concept art. To harness the potential of concept art, with its defined body poses and specific view angles, we propose encoding it as priors for our model. We train the network to make use of these priors for various 3D points through a learnable view-direction-attended multi-head self-attention layer. Additionally, we demonstrate that a combination of ray sampling and surface sampling enhances the inference capabilities of our network. Our model is able to generate high-quality 360-degree views of characters. Subsequently, we provide a simple guideline to better leverage our model to extract the 3D mesh. It is important to note that our model's inferencing capabilities are influenced by the training data's characteristics, primarily focusing on characters with a single head, two arms, and two legs. Nevertheless, our methodology remains versatile and adaptable to concept art from diverse subject matters, without imposing any specific assumptions on the data.","sentences":["3D modeling holds significant importance in the realms of AR/VR and gaming, allowing for both artistic creativity and practical applications.","However, the process is often time-consuming and demands a high level of skill.","In this paper, we present a novel approach to create volumetric representations of 3D characters from consistent turnaround concept art, which serves as the standard input in the 3D modeling industry.","While Neural Radiance Field (NeRF) has been a game-changer in image-based 3D reconstruction, to the best of our knowledge, there is no known research that optimizes the pipeline for concept art.","To harness the potential of concept art, with its defined body poses and specific view angles, we propose encoding it as priors for our model.","We train the network to make use of these priors for various 3D points through a learnable view-direction-attended multi-head self-attention layer.","Additionally, we demonstrate that a combination of ray sampling and surface sampling enhances the inference capabilities of our network.","Our model is able to generate high-quality 360-degree views of characters.","Subsequently, we provide a simple guideline to better leverage our model to extract the 3D mesh.","It is important to note that our model's inferencing capabilities are influenced by the training data's characteristics, primarily focusing on characters with a single head, two arms, and two legs.","Nevertheless, our methodology remains versatile and adaptable to concept art from diverse subject matters, without imposing any specific assumptions on the data."],"url":"http://arxiv.org/abs/2402.17115v1","category":"cs.CV"}
{"created":"2024-02-27 01:19:53","title":"Transparent Image Layer Diffusion using Latent Transparency","abstract":"We present LayerDiffusion, an approach enabling large-scale pretrained latent diffusion models to generate transparent images. The method allows generation of single transparent images or of multiple transparent layers. The method learns a \"latent transparency\" that encodes alpha channel transparency into the latent manifold of a pretrained latent diffusion model. It preserves the production-ready quality of the large diffusion model by regulating the added transparency as a latent offset with minimal changes to the original latent distribution of the pretrained model. In this way, any latent diffusion model can be converted into a transparent image generator by finetuning it with the adjusted latent space. We train the model with 1M transparent image layer pairs collected using a human-in-the-loop collection scheme. We show that latent transparency can be applied to different open source image generators, or be adapted to various conditional control systems to achieve applications like foreground/background-conditioned layer generation, joint layer generation, structural control of layer contents, etc. A user study finds that in most cases (97%) users prefer our natively generated transparent content over previous ad-hoc solutions such as generating and then matting. Users also report the quality of our generated transparent images is comparable to real commercial transparent assets like Adobe Stock.","sentences":["We present LayerDiffusion, an approach enabling large-scale pretrained latent diffusion models to generate transparent images.","The method allows generation of single transparent images or of multiple transparent layers.","The method learns a \"latent transparency\" that encodes alpha channel transparency into the latent manifold of a pretrained latent diffusion model.","It preserves the production-ready quality of the large diffusion model by regulating the added transparency as a latent offset with minimal changes to the original latent distribution of the pretrained model.","In this way, any latent diffusion model can be converted into a transparent image generator by finetuning it with the adjusted latent space.","We train the model with 1M transparent image layer pairs collected using a human-in-the-loop collection scheme.","We show that latent transparency can be applied to different open source image generators, or be adapted to various conditional control systems to achieve applications like foreground/background-conditioned layer generation, joint layer generation, structural control of layer contents, etc.","A user study finds that in most cases (97%) users prefer our natively generated transparent content over previous ad-hoc solutions such as generating and then matting.","Users also report the quality of our generated transparent images is comparable to real commercial transparent assets like Adobe Stock."],"url":"http://arxiv.org/abs/2402.17113v2","category":"cs.CV"}
{"created":"2024-02-27 01:15:28","title":"SwiftCache: Model-Based Learning for Dynamic Content Caching in CDNs","abstract":"We introduce SwiftCache, a \"fresh\" learning-based caching framework designed for content distribution networks (CDNs) featuring distributed front-end local caches and a dynamic back-end database. Users prefer the most recent version of the dynamically updated content, while the local caches lack knowledge of item popularity and refresh rates. We first explore scenarios with requests arriving at a local cache following a Poisson process, whereby we prove that the optimal policy features a threshold-based structure with updates occurring solely at request arrivals. Leveraging these findings, SwiftCache is proposed as a model-based learning framework for dynamic content caching. The simulation demonstrates near-optimal cost for Poisson process arrivals and strong performance with limited cache sizes. For more general environments, we present a model-free Reinforcement Learning (RL) based caching policy without prior statistical assumptions. The model-based policy performs well compared to the model-free policy when the variance of interarrival times remains moderate. However, as the variance increases, RL slightly outperforms model-based learning at the cost of longer training times, and higher computational resource consumption. Model-based learning's adaptability to environmental changes without retraining positions it as a practical choice for dynamic network environments. Distributed edge caches can utilize this approach in a decentralized manner to effectively meet the evolving behaviors of users.","sentences":["We introduce SwiftCache, a \"fresh\" learning-based caching framework designed for content distribution networks (CDNs) featuring distributed front-end local caches and a dynamic back-end database.","Users prefer the most recent version of the dynamically updated content, while the local caches lack knowledge of item popularity and refresh rates.","We first explore scenarios with requests arriving at a local cache following a Poisson process, whereby we prove that the optimal policy features a threshold-based structure with updates occurring solely at request arrivals.","Leveraging these findings, SwiftCache is proposed as a model-based learning framework for dynamic content caching.","The simulation demonstrates near-optimal cost for Poisson process arrivals and strong performance with limited cache sizes.","For more general environments, we present a model-free Reinforcement Learning (RL) based caching policy without prior statistical assumptions.","The model-based policy performs well compared to the model-free policy when the variance of interarrival times remains moderate.","However, as the variance increases, RL slightly outperforms model-based learning at the cost of longer training times, and higher computational resource consumption.","Model-based learning's adaptability to environmental changes without retraining positions it as a practical choice for dynamic network environments.","Distributed edge caches can utilize this approach in a decentralized manner to effectively meet the evolving behaviors of users."],"url":"http://arxiv.org/abs/2402.17111v1","category":"math.OC"}
{"created":"2024-02-27 01:01:59","title":"Repeated Contracting with Multiple Non-Myopic Agents: Policy Regret and Limited Liability","abstract":"We study a repeated contracting setting in which a Principal adaptively chooses amongst $k$ Agents at each of $T$ rounds. The Agents are non-myopic, and so a mechanism for the Principal induces a $T$-round extensive form game amongst the Agents. We give several results aimed at understanding an under-explored aspect of contract theory -- the game induced when choosing an Agent to contract with. First, we show that this game admits a pure-strategy \\emph{non-responsive} equilibrium amongst the Agents -- informally an equilibrium in which the Agent's actions depend on the history of realized states of nature, but not on the history of each other's actions, and so avoids the complexities of collusion and threats. Next, we show that if the Principal selects Agents using a \\emph{monotone} bandit algorithm, then for any concave contract, in any such equilibrium, the Principal obtains no regret to contracting with the best Agent in hindsight -- not just given their realized actions, but also to the counterfactual world in which they had offered a guaranteed $T$-round contract to the best Agent in hindsight, which would have induced a different sequence of actions. Finally, we show that if the Principal selects Agents using a monotone bandit algorithm which guarantees no swap-regret, then the Principal can additionally offer only limited liability contracts (in which the Agent never needs to pay the Principal) while getting no-regret to the counterfactual world in which she offered a linear contract to the best Agent in hindsight -- despite the fact that linear contracts are not limited liability. We instantiate this theorem by demonstrating the existence of a monotone no swap-regret bandit algorithm, which to our knowledge has not previously appeared in the literature.","sentences":["We study a repeated contracting setting in which a Principal adaptively chooses amongst $k$ Agents at each of $T$ rounds.","The Agents are non-myopic, and so a mechanism for the Principal induces a $T$-round extensive form game amongst the Agents.","We give several results aimed at understanding an under-explored aspect of contract theory -- the game induced when choosing an Agent to contract with.","First, we show that this game admits a pure-strategy \\emph{non-responsive} equilibrium amongst the Agents -- informally an equilibrium in which the Agent's actions depend on the history of realized states of nature, but not on the history of each other's actions, and so avoids the complexities of collusion and threats.","Next, we show that if the Principal selects Agents using a \\emph{monotone} bandit algorithm, then for any concave contract, in any such equilibrium, the Principal obtains no regret to contracting with the best Agent in hindsight -- not just given their realized actions, but also to the counterfactual world in which they had offered a guaranteed $T$-round contract to the best Agent in hindsight, which would have induced a different sequence of actions.","Finally, we show that if the Principal selects Agents using a monotone bandit algorithm which guarantees no swap-regret, then the Principal can additionally offer only limited liability contracts (in which the Agent never needs to pay the Principal) while getting no-regret to the counterfactual world in which she offered a linear contract to the best Agent in hindsight -- despite the fact that linear contracts are not limited liability.","We instantiate this theorem by demonstrating the existence of a monotone no swap-regret bandit algorithm, which to our knowledge has not previously appeared in the literature."],"url":"http://arxiv.org/abs/2402.17108v1","category":"cs.GT"}
{"created":"2024-02-26 23:37:59","title":"A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI Judge","abstract":"This study presents an innovative enhancement to retrieval-augmented generation (RAG) systems by seamlessly integrating fine-tuned large language models (LLMs) with vector databases. This integration capitalizes on the combined strengths of structured data retrieval and the nuanced comprehension provided by advanced LLMs. Central to our approach are the LoRA and QLoRA methodologies, which stand at the forefront of model refinement through parameter-efficient fine-tuning and memory optimization. A novel feature of our research is the incorporation of user feedback directly into the training process, ensuring the model's continuous adaptation to user expectations and thus, improving its performance and applicability. Additionally, we introduce a Quantized Influence Measure (QIM) as an innovative \"AI Judge\" mechanism to enhance the precision of result selection, further refining the system's accuracy. Accompanied by an executive diagram and a detailed algorithm for fine-tuning QLoRA, our work provides a comprehensive framework for implementing these advancements within chatbot technologies. This research contributes significant insights into LLM optimization for specific uses and heralds new directions for further development in retrieval-augmented models. Through extensive experimentation and analysis, our findings lay a robust foundation for future advancements in chatbot technology and retrieval systems, marking a significant step forward in the creation of more sophisticated, precise, and user-centric conversational AI systems.","sentences":["This study presents an innovative enhancement to retrieval-augmented generation (RAG) systems by seamlessly integrating fine-tuned large language models (LLMs) with vector databases.","This integration capitalizes on the combined strengths of structured data retrieval and the nuanced comprehension provided by advanced LLMs.","Central to our approach are the LoRA and QLoRA methodologies, which stand at the forefront of model refinement through parameter-efficient fine-tuning and memory optimization.","A novel feature of our research is the incorporation of user feedback directly into the training process, ensuring the model's continuous adaptation to user expectations and thus, improving its performance and applicability.","Additionally, we introduce a Quantized Influence Measure (QIM) as an innovative \"AI Judge\" mechanism to enhance the precision of result selection, further refining the system's accuracy.","Accompanied by an executive diagram and a detailed algorithm for fine-tuning QLoRA, our work provides a comprehensive framework for implementing these advancements within chatbot technologies.","This research contributes significant insights into LLM optimization for specific uses and heralds new directions for further development in retrieval-augmented models.","Through extensive experimentation and analysis, our findings lay a robust foundation for future advancements in chatbot technology and retrieval systems, marking a significant step forward in the creation of more sophisticated, precise, and user-centric conversational AI systems."],"url":"http://arxiv.org/abs/2402.17081v1","category":"cs.IR"}
{"created":"2024-02-26 22:16:06","title":"Extension of the Best Polynomial Operator in Generalized Orlicz spaces","abstract":"In this paper, we consider the best multivalued polynomial approximation operator for functions in an Orlicz Space $L^{\\varphi}(\\Omega)$. We obtain its characterization involving $\\psi^-$ and $\\psi^+$, which are the left and right derivatives functions of $\\varphi$. And then, we extend the operator to $L^{\\psi^+}(\\Omega)$. We also get pointwise convergence of this extension, where the Calder\\'on-Zygmund class $t_m^p (x)$ adapted to $L^{\\psi^+}(\\Omega)$ plays an important role.","sentences":["In this paper, we consider the best multivalued polynomial approximation operator for functions in an Orlicz Space $L^{\\varphi}(\\Omega)$.","We obtain its characterization involving $\\psi^-$ and $\\psi^+$, which are the left and right derivatives functions of $\\varphi$. And then, we extend the operator to $L^{\\psi^+}(\\Omega)$. We also get pointwise convergence of this extension, where the Calder\\'on-Zygmund class $t_m^p (x)$ adapted to $L^{\\psi^+}(\\Omega)$ plays an important role."],"url":"http://arxiv.org/abs/2402.17048v1","category":"math.CA"}
{"created":"2024-02-26 22:04:07","title":"Mind the Gap: Nonlocal Cascades and Preferential Heating in High-$\u03b2$ Alfv\u00e9nic Turbulence","abstract":"Characterizing the thermodynamics of turbulent plasmas is key to decoding observable signatures from astrophysical systems. In magnetohydrodynamic (MHD) turbulence, nonlinear interactions between counter-propagating Alfv\\'en waves cascade energy to smaller spatial scales where dissipation heats the protons and electrons. When the thermal pressure far exceeds the magnetic pressure, linear theory predicts a spectral gap at perpendicular scales near the proton gyroradius where Alfv\\'en waves become non-propagating. For simple models of an MHD turbulent cascade that assume only local nonlinear interactions, the cascade halts at this gap, preventing energy from reaching smaller scales where electron dissipation dominates, leading to an overestimate of the proton heating rate. In this work, we demonstrate that nonlocal contributions to the cascade, specifically large scale shearing and small scale diffusion, can bridge the non-propagating gap, allowing the cascade to continue to smaller scales. We provide an updated functional form for the proton-to-electron heating ratio accounting for this nonlocal energy transfer by evaluating a nonlocal weakened cascade model over a range of temperature and pressure ratios. In plasmas where the thermal pressure dominates the magnetic pressure, we observe that the proton heating is moderated compared to the significant enhancement predicted by local models.","sentences":["Characterizing the thermodynamics of turbulent plasmas is key to decoding observable signatures from astrophysical systems.","In magnetohydrodynamic (MHD) turbulence, nonlinear interactions between counter-propagating Alfv\\'en waves cascade energy to smaller spatial scales where dissipation heats the protons and electrons.","When the thermal pressure far exceeds the magnetic pressure, linear theory predicts a spectral gap at perpendicular scales near the proton gyroradius where Alfv\\'en waves become non-propagating.","For simple models of an MHD turbulent cascade that assume only local nonlinear interactions, the cascade halts at this gap, preventing energy from reaching smaller scales where electron dissipation dominates, leading to an overestimate of the proton heating rate.","In this work, we demonstrate that nonlocal contributions to the cascade, specifically large scale shearing and small scale diffusion, can bridge the non-propagating gap, allowing the cascade to continue to smaller scales.","We provide an updated functional form for the proton-to-electron heating ratio accounting for this nonlocal energy transfer by evaluating a nonlocal weakened cascade model over a range of temperature and pressure ratios.","In plasmas where the thermal pressure dominates the magnetic pressure, we observe that the proton heating is moderated compared to the significant enhancement predicted by local models."],"url":"http://arxiv.org/abs/2402.17044v1","category":"physics.space-ph"}
{"created":"2024-02-26 22:03:24","title":"Traffic Control via Connected and Automated Vehicles: An Open-Road Field Experiment with 100 CAVs","abstract":"The CIRCLES project aims to reduce instabilities in traffic flow, which are naturally occurring phenomena due to human driving behavior. These \"phantom jams\" or \"stop-and-go waves,\"are a significant source of wasted energy. Toward this goal, the CIRCLES project designed a control system referred to as the MegaController by the CIRCLES team, that could be deployed in real traffic. Our field experiment leveraged a heterogeneous fleet of 100 longitudinally-controlled vehicles as Lagrangian traffic actuators, each of which ran a controller with the architecture described in this paper. The MegaController is a hierarchical control architecture, which consists of two main layers. The upper layer is called Speed Planner, and is a centralized optimal control algorithm. It assigns speed targets to the vehicles, conveyed through the LTE cellular network. The lower layer is a control layer, running on each vehicle. It performs local actuation by overriding the stock adaptive cruise controller, using the stock on-board sensors. The Speed Planner ingests live data feeds provided by third parties, as well as data from our own control vehicles, and uses both to perform the speed assignment. The architecture of the speed planner allows for modular use of standard control techniques, such as optimal control, model predictive control, kernel methods and others, including Deep RL, model predictive control and explicit controllers. Depending on the vehicle architecture, all onboard sensing data can be accessed by the local controllers, or only some. Control inputs vary across different automakers, with inputs ranging from torque or acceleration requests for some cars, and electronic selection of ACC set points in others. The proposed architecture allows for the combination of all possible settings proposed above. Most configurations were tested throughout the ramp up to the MegaVandertest.","sentences":["The CIRCLES project aims to reduce instabilities in traffic flow, which are naturally occurring phenomena due to human driving behavior.","These \"phantom jams\" or \"stop-and-go waves,\"are a significant source of wasted energy.","Toward this goal, the CIRCLES project designed a control system referred to as the MegaController by the CIRCLES team, that could be deployed in real traffic.","Our field experiment leveraged a heterogeneous fleet of 100 longitudinally-controlled vehicles as Lagrangian traffic actuators, each of which ran a controller with the architecture described in this paper.","The MegaController is a hierarchical control architecture, which consists of two main layers.","The upper layer is called Speed Planner, and is a centralized optimal control algorithm.","It assigns speed targets to the vehicles, conveyed through the LTE cellular network.","The lower layer is a control layer, running on each vehicle.","It performs local actuation by overriding the stock adaptive cruise controller, using the stock on-board sensors.","The Speed Planner ingests live data feeds provided by third parties, as well as data from our own control vehicles, and uses both to perform the speed assignment.","The architecture of the speed planner allows for modular use of standard control techniques, such as optimal control, model predictive control, kernel methods and others, including Deep RL, model predictive control and explicit controllers.","Depending on the vehicle architecture, all onboard sensing data can be accessed by the local controllers, or only some.","Control inputs vary across different automakers, with inputs ranging from torque or acceleration requests for some cars, and electronic selection of ACC set points in others.","The proposed architecture allows for the combination of all possible settings proposed above.","Most configurations were tested throughout the ramp up to the MegaVandertest."],"url":"http://arxiv.org/abs/2402.17043v1","category":"eess.SY"}
{"created":"2024-02-26 20:57:35","title":"Deep Learning Algorithms Used in Intrusion Detection Systems -- A Review","abstract":"The increase in network attacks has necessitated the development of robust and efficient intrusion detection systems (IDS) capable of identifying malicious activities in real-time. In the last five years, deep learning algorithms have emerged as powerful tools in this domain, offering enhanced detection capabilities compared to traditional methods. This review paper studies recent advancements in the application of deep learning techniques, including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Deep Belief Networks (DBN), Deep Neural Networks (DNN), Long Short-Term Memory (LSTM), autoencoders (AE), Multi-Layer Perceptrons (MLP), Self-Normalizing Networks (SNN) and hybrid models, within network intrusion detection systems. we delve into the unique architectures, training models, and classification methodologies tailored for network traffic analysis and anomaly detection. Furthermore, we analyze the strengths and limitations of each deep learning approach in terms of detection accuracy, computational efficiency, scalability, and adaptability to evolving threats. Additionally, this paper highlights prominent datasets and benchmarking frameworks commonly utilized for evaluating the performance of deep learning-based IDS. This review will provide researchers and industry practitioners with valuable insights into the state-of-the-art deep learning algorithms for enhancing the security framework of network environments through intrusion detection.","sentences":["The increase in network attacks has necessitated the development of robust and efficient intrusion detection systems (IDS) capable of identifying malicious activities in real-time.","In the last five years, deep learning algorithms have emerged as powerful tools in this domain, offering enhanced detection capabilities compared to traditional methods.","This review paper studies recent advancements in the application of deep learning techniques, including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Deep Belief Networks (DBN), Deep Neural Networks (DNN), Long Short-Term Memory (LSTM), autoencoders (AE), Multi-Layer Perceptrons (MLP), Self-Normalizing Networks (SNN) and hybrid models, within network intrusion detection systems.","we delve into the unique architectures, training models, and classification methodologies tailored for network traffic analysis and anomaly detection.","Furthermore, we analyze the strengths and limitations of each deep learning approach in terms of detection accuracy, computational efficiency, scalability, and adaptability to evolving threats.","Additionally, this paper highlights prominent datasets and benchmarking frameworks commonly utilized for evaluating the performance of deep learning-based IDS.","This review will provide researchers and industry practitioners with valuable insights into the state-of-the-art deep learning algorithms for enhancing the security framework of network environments through intrusion detection."],"url":"http://arxiv.org/abs/2402.17020v1","category":"cs.CR"}
{"created":"2024-02-26 20:55:47","title":"A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection","abstract":"We tested front-end enhanced neural models where a frozen classifier was prepended by a differentiable and fully convolutional model with a skip connection. By training them using a small learning rate for about one epoch, we obtained models that retained the accuracy of the backbone classifier while being unusually resistant to gradient attacks including APGD and FAB-T attacks from the AutoAttack package, which we attributed to gradient masking. The gradient masking phenomenon is not new, but the degree of masking was quite remarkable for fully differentiable models that did not have gradient-shattering components such as JPEG compression or components that are expected to cause diminishing gradients.   Though black box attacks can be partially effective against gradient masking, they are easily defeated by combining models into randomized ensembles. We estimate that such ensembles achieve near-SOTA AutoAttack accuracy on CIFAR10, CIFAR100, and ImageNet despite having virtually zero accuracy under adaptive attacks. Adversarial training of the backbone classifier can further increase resistance of the front-end enhanced model to gradient attacks. On CIFAR10, the respective randomized ensemble achieved 90.8$\\pm 2.5$% (99% CI) accuracy under AutoAttack while having only 18.2$\\pm 3.6$% accuracy under the adaptive attack.   We do not establish SOTA in adversarial robustness. Instead, we make methodological contributions and further supports the thesis that adaptive attacks designed with the complete knowledge of model architecture are crucial in demonstrating model robustness and that even the so-called white-box gradient attacks can have limited applicability. Although gradient attacks can be complemented with black-box attack such as the SQUARE attack or the zero-order PGD, black-box attacks can be weak against randomized ensembles, e.g., when ensemble models mask gradients.","sentences":["We tested front-end enhanced neural models where a frozen classifier was prepended by a differentiable and fully convolutional model with a skip connection.","By training them using a small learning rate for about one epoch, we obtained models that retained the accuracy of the backbone classifier while being unusually resistant to gradient attacks including APGD and FAB-T attacks from the AutoAttack package, which we attributed to gradient masking.","The gradient masking phenomenon is not new, but the degree of masking was quite remarkable for fully differentiable models that did not have gradient-shattering components such as JPEG compression or components that are expected to cause diminishing gradients.   ","Though black box attacks can be partially effective against gradient masking, they are easily defeated by combining models into randomized ensembles.","We estimate that such ensembles achieve near-SOTA AutoAttack accuracy on CIFAR10, CIFAR100, and ImageNet despite having virtually zero accuracy under adaptive attacks.","Adversarial training of the backbone classifier can further increase resistance of the front-end enhanced model to gradient attacks.","On CIFAR10, the respective randomized ensemble achieved 90.8$\\pm 2.5$% (99% CI) accuracy under AutoAttack while having only 18.2$\\pm 3.6$% accuracy under the adaptive attack.   ","We do not establish SOTA in adversarial robustness.","Instead, we make methodological contributions and further supports the thesis that adaptive attacks designed with the complete knowledge of model architecture are crucial in demonstrating model robustness and that even the so-called white-box gradient attacks can have limited applicability.","Although gradient attacks can be complemented with black-box attack such as the SQUARE attack or the zero-order PGD, black-box attacks can be weak against randomized ensembles, e.g., when ensemble models mask gradients."],"url":"http://arxiv.org/abs/2402.17018v1","category":"cs.LG"}
{"created":"2024-02-26 20:01:29","title":"Thermodynamic Overfitting and Generalization: Energetic Limits on Predictive Complexity","abstract":"Efficiently harvesting thermodynamic resources requires a precise understanding of their structure. This becomes explicit through the lens of information engines -- thermodynamic engines that use information as fuel. Maximizing the work harvested using available information is a form of physically-instantiated machine learning that drives information engines to develop complex predictive memory to store an environment's temporal correlations. We show that an information engine's complex predictive memory poses both energetic benefits and risks. While increasing memory facilitates detection of hidden patterns in an environment, it also opens the possibility of thermodynamic overfitting, where the engine dissipates additional energy in testing. To address overfitting, we introduce thermodynamic regularizers that incur a cost to engine complexity in training due to the physical constraints on the information engine. We demonstrate that regularized thermodynamic machine learning generalizes effectively. In particular, the physical constraints from which regularizers are derived improve the performance of learned predictive models. This suggests that the laws of physics jointly create the conditions for emergent complexity and predictive intelligence.","sentences":["Efficiently harvesting thermodynamic resources requires a precise understanding of their structure.","This becomes explicit through the lens of information engines -- thermodynamic engines that use information as fuel.","Maximizing the work harvested using available information is a form of physically-instantiated machine learning that drives information engines to develop complex predictive memory to store an environment's temporal correlations.","We show that an information engine's complex predictive memory poses both energetic benefits and risks.","While increasing memory facilitates detection of hidden patterns in an environment, it also opens the possibility of thermodynamic overfitting, where the engine dissipates additional energy in testing.","To address overfitting, we introduce thermodynamic regularizers that incur a cost to engine complexity in training due to the physical constraints on the information engine.","We demonstrate that regularized thermodynamic machine learning generalizes effectively.","In particular, the physical constraints from which regularizers are derived improve the performance of learned predictive models.","This suggests that the laws of physics jointly create the conditions for emergent complexity and predictive intelligence."],"url":"http://arxiv.org/abs/2402.16995v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-26 19:48:38","title":"Magnetic filaments: formation, stability, and feedback","abstract":"As well known, magnetic fields in space are distributed very inhomogeneously. Some-times field distributions have forms of filaments with high magnetic field values. As many ob-servations show, such a filamentation takes place in convective cells in the Sun and other astro-physical objects. This effect is associated with the frozenness of the magnetic field into a medium with high conductivity that leads to compression of magnetic field lines and forming magnetic filaments. We show analytically, based on the general analysis, that the magnetic field intensifies in the regions of downward flows in both two-dimensional and three-dimensional convective cells. These regions of the hyperbolic type for magnetic fields play a role of a specific attractor. This analysis was confirmed by numerical simulations for 2D convective cells of the roll-type. Without dissipation the magnetic field grows exponentially in time and does not depend on the aspect ratio between horizontal and vertical scale of the cell. An increase due to compression in the magnetic field in the high conductive plasma is saturated due to the natural limitation associated with dissipative effects when the maximum magnitude of the magnetic field is of the order of the root of the magnetic Reynolds number Rem. For the solar convective zone the mean kinetic energy density exceeds mean magnetic energy density at least for two orders of magnitude that allows one to use the kinematic approximation for the MHD induction equation. In this paper based on the stability analysis we explain why downward flows influence magnetic filaments from making them more flat with orientation along interfaces between convective cells.","sentences":["As well known, magnetic fields in space are distributed very inhomogeneously.","Some-times field distributions have forms of filaments with high magnetic field values.","As many ob-servations show, such a filamentation takes place in convective cells in the Sun and other astro-physical objects.","This effect is associated with the frozenness of the magnetic field into a medium with high conductivity that leads to compression of magnetic field lines and forming magnetic filaments.","We show analytically, based on the general analysis, that the magnetic field intensifies in the regions of downward flows in both two-dimensional and three-dimensional convective cells.","These regions of the hyperbolic type for magnetic fields play a role of a specific attractor.","This analysis was confirmed by numerical simulations for 2D convective cells of the roll-type.","Without dissipation the magnetic field grows exponentially in time and does not depend on the aspect ratio between horizontal and vertical scale of the cell.","An increase due to compression in the magnetic field in the high conductive plasma is saturated due to the natural limitation associated with dissipative effects when the maximum magnitude of the magnetic field is of the order of the root of the magnetic Reynolds number Rem.","For the solar convective zone the mean kinetic energy density exceeds mean magnetic energy density at least for two orders of magnitude that allows one to use the kinematic approximation for the MHD induction equation.","In this paper based on the stability analysis we explain why downward flows influence magnetic filaments from making them more flat with orientation along interfaces between convective cells."],"url":"http://arxiv.org/abs/2402.16989v1","category":"astro-ph.SR"}
{"created":"2024-02-26 19:04:57","title":"Imaging Spectropolarimetry -- A New Observing Mode on the Hubble Space Telescope's Advanced Camera for Surveys","abstract":"Imaging spectropolarimetry is a new observing mode on the Advanced Camera for Surveys (ACS) aboard the Hubble Space Telescope (HST) that was commissioned in Cycle 30 and is available to HST observers starting in Cycle 31 (i.e., from 2023). It is a technique that is accessible from ground-based observatories, but the superb spatial resolution afforded by HST/ACS combined with the slitless nature of HST/ACS grism spectroscopy opens up the possibility of studying polarized extended emission in a way that is not currently possible even with Adaptive Optics facilities on the ground. This mode could help to study interesting targets including (but not limited to) QSOs, AGN and Radio Galaxies, ISM Dust Properties, Pre-Planetary Nebulae, Proto-Planetary and Debris Disks, Supernovae/Supernova Remnants, and Solar System objects. This research note presents the preliminary results from the calibration programs used to calibrate imaging spectropolarimetry on HST/ACS.","sentences":["Imaging spectropolarimetry is a new observing mode on the Advanced Camera for Surveys (ACS) aboard the Hubble Space Telescope (HST) that was commissioned in Cycle 30 and is available to HST observers starting in Cycle 31 (i.e., from 2023).","It is a technique that is accessible from ground-based observatories, but the superb spatial resolution afforded by HST/ACS combined with the slitless nature of HST/ACS grism spectroscopy opens up the possibility of studying polarized extended emission in a way that is not currently possible even with Adaptive Optics facilities on the ground.","This mode could help to study interesting targets including (but not limited to) QSOs, AGN and Radio Galaxies, ISM Dust Properties, Pre-Planetary Nebulae, Proto-Planetary and Debris Disks, Supernovae/Supernova Remnants, and Solar System objects.","This research note presents the preliminary results from the calibration programs used to calibrate imaging spectropolarimetry on HST/ACS."],"url":"http://arxiv.org/abs/2402.16967v1","category":"astro-ph.IM"}
{"created":"2024-02-26 18:59:18","title":"Multi-LoRA Composition for Image Generation","abstract":"Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models for the accurate rendition of specific elements like distinct characters or unique styles in generated images. Nonetheless, existing methods face challenges in effectively composing multiple LoRAs, especially as the number of LoRAs to be integrated grows, thus hindering the creation of complex imagery. In this paper, we study multi-LoRA composition through a decoding-centric perspective. We present two training-free methods: LoRA Switch, which alternates between different LoRAs at each denoising step, and LoRA Composite, which simultaneously incorporates all LoRAs to guide more cohesive image synthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new comprehensive testbed as part of this research. It features a diverse range of LoRA categories with 480 composition sets. Utilizing an evaluation framework based on GPT-4V, our findings demonstrate a clear improvement in performance with our methods over the prevalent baseline, particularly evident when increasing the number of LoRAs in a composition.","sentences":["Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models for the accurate rendition of specific elements like distinct characters or unique styles in generated images.","Nonetheless, existing methods face challenges in effectively composing multiple LoRAs, especially as the number of LoRAs to be integrated grows, thus hindering the creation of complex imagery.","In this paper, we study multi-LoRA composition through a decoding-centric perspective.","We present two training-free methods: LoRA Switch, which alternates between different LoRAs at each denoising step, and LoRA Composite, which simultaneously incorporates all LoRAs to guide more cohesive image synthesis.","To evaluate the proposed approaches, we establish ComposLoRA, a new comprehensive testbed as part of this research.","It features a diverse range of LoRA categories with 480 composition sets.","Utilizing an evaluation framework based on GPT-4V, our findings demonstrate a clear improvement in performance with our methods over the prevalent baseline, particularly evident when increasing the number of LoRAs in a composition."],"url":"http://arxiv.org/abs/2402.16843v1","category":"cs.CV"}
{"created":"2024-02-26 18:59:12","title":"Asymmetry in Low-Rank Adapters of Foundation Models","abstract":"Parameter-efficient fine-tuning optimizes large, pre-trained foundation models by updating a subset of parameters; in this class, Low-Rank Adaptation (LoRA) is particularly effective. Inspired by an effort to investigate the different roles of LoRA matrices during fine-tuning, this paper characterizes and leverages unexpected asymmetry in the importance of low-rank adapter matrices. Specifically, when updating the parameter matrices of a neural network by adding a product $BA$, we observe that the $B$ and $A$ matrices have distinct functions: $A$ extracts features from the input, while $B$ uses these features to create the desired output. Based on this observation, we demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning $A$, and that a random untrained $A$ should perform nearly as well as a fine-tuned one. Using an information-theoretic lens, we also bound the generalization of low-rank adapters, showing that the parameter savings of exclusively training $B$ improves the bound. We support our conclusions with experiments on RoBERTa, BART-Large, LLaMA-2, and ViTs.","sentences":["Parameter-efficient fine-tuning optimizes large, pre-trained foundation models by updating a subset of parameters; in this class, Low-Rank Adaptation (LoRA) is particularly effective.","Inspired by an effort to investigate the different roles of LoRA matrices during fine-tuning, this paper characterizes and leverages unexpected asymmetry in the importance of low-rank adapter matrices.","Specifically, when updating the parameter matrices of a neural network by adding a product $BA$, we observe that the $B$ and $A$ matrices have distinct functions: $A$ extracts features from the input, while $B$ uses these features to create the desired output.","Based on this observation, we demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning $A$, and that a random untrained $A$ should perform nearly as well as a fine-tuned one.","Using an information-theoretic lens, we also bound the generalization of low-rank adapters, showing that the parameter savings of exclusively training $B$ improves the bound.","We support our conclusions with experiments on RoBERTa, BART-Large, LLaMA-2, and ViTs."],"url":"http://arxiv.org/abs/2402.16842v2","category":"cs.LG"}
{"created":"2024-02-26 18:55:13","title":"Training Neural Networks from Scratch with Parallel Low-Rank Adapters","abstract":"The scalability of deep learning models is fundamentally limited by computing resources, memory, and communication. Although methods like low-rank adaptation (LoRA) have reduced the cost of model finetuning, its application in model pre-training remains largely unexplored. This paper explores extending LoRA to model pre-training, identifying the inherent constraints and limitations of standard LoRA in this context. We introduce LoRA-the-Explorer (LTE), a novel bi-level optimization algorithm designed to enable parallel training of multiple low-rank heads across computing nodes, thereby reducing the need for frequent synchronization. Our approach includes extensive experimentation on vision transformers using various vision datasets, demonstrating that LTE is competitive with standard pre-training.","sentences":["The scalability of deep learning models is fundamentally limited by computing resources, memory, and communication.","Although methods like low-rank adaptation (LoRA) have reduced the cost of model finetuning, its application in model pre-training remains largely unexplored.","This paper explores extending LoRA to model pre-training, identifying the inherent constraints and limitations of standard LoRA in this context.","We introduce LoRA-the-Explorer (LTE), a novel bi-level optimization algorithm designed to enable parallel training of multiple low-rank heads across computing nodes, thereby reducing the need for frequent synchronization.","Our approach includes extensive experimentation on vision transformers using various vision datasets, demonstrating that LTE is competitive with standard pre-training."],"url":"http://arxiv.org/abs/2402.16828v1","category":"cs.LG"}
{"created":"2024-02-26 18:51:15","title":"Weighted Monte Carlo augmented spherical Fourier-Bessel convolutional layers for 3D abdominal organ segmentation","abstract":"Filter-decomposition-based group equivariant convolutional neural networks show promising stability and data efficiency for 3D image feature extraction. However, the existing filter-decomposition-based 3D group equivariant neural networks rely on parameter-sharing designs and are mostly limited to rotation transform groups, where the chosen spherical harmonic filter bases consider only angular orthogonality. These limitations hamper its application to deep neural network architectures for medical image segmentation. To address these issues, this paper describes a non-parameter-sharing affine group equivariant neural network for 3D medical image segmentation based on an adaptive aggregation of Monte Carlo augmented spherical Fourier Bessel filter bases. The efficiency and flexibility of the adopted non-parameter strategy enable for the first time an efficient implementation of 3D affine group equivariant convolutional neural networks for volumetric data. The introduced spherical Bessel Fourier filter basis combines both angular and radial orthogonality for better feature extraction. The 3D image segmentation experiments on two abdominal image sets, BTCV and the NIH Pancreas datasets, show that the proposed methods excel the state-of-the-art 3D neural networks with high training stability and data efficiency. The code will be available at https://github.com/ZhaoWenzhao/WVMS.","sentences":["Filter-decomposition-based group equivariant convolutional neural networks show promising stability and data efficiency for 3D image feature extraction.","However, the existing filter-decomposition-based 3D group equivariant neural networks rely on parameter-sharing designs and are mostly limited to rotation transform groups, where the chosen spherical harmonic filter bases consider only angular orthogonality.","These limitations hamper its application to deep neural network architectures for medical image segmentation.","To address these issues, this paper describes a non-parameter-sharing affine group equivariant neural network for 3D medical image segmentation based on an adaptive aggregation of Monte Carlo augmented spherical Fourier Bessel filter bases.","The efficiency and flexibility of the adopted non-parameter strategy enable for the first time an efficient implementation of 3D affine group equivariant convolutional neural networks for volumetric data.","The introduced spherical Bessel Fourier filter basis combines both angular and radial orthogonality for better feature extraction.","The 3D image segmentation experiments on two abdominal image sets, BTCV and the NIH Pancreas datasets, show that the proposed methods excel the state-of-the-art 3D neural networks with high training stability and data efficiency.","The code will be available at https://github.com/ZhaoWenzhao/WVMS."],"url":"http://arxiv.org/abs/2402.16825v2","category":"cs.CV"}
{"created":"2024-02-26 18:19:07","title":"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning","abstract":"Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms. We identify that existing benchmarks used for research into open-ended learning fall into one of two categories. Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original. A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90% of the optimal reward. To provide a more compelling challenge we present the main Craftax benchmark, a significant extension of the Crafter mechanics with elements inspired from NetHack. Solving Craftax requires deep exploration, long term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered. We show that existing methods including global and episodic exploration, as well as unsupervised environment design fail to make material progress on the benchmark. We believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources.","sentences":["Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms.","We identify that existing benchmarks used for research into open-ended learning fall into one of two categories.","Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen.","To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original.","A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90% of the optimal reward.","To provide a more compelling challenge we present the main Craftax benchmark, a significant extension of the Crafter mechanics with elements inspired from NetHack.","Solving Craftax requires deep exploration, long term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered.","We show that existing methods including global and episodic exploration, as well as unsupervised environment design fail to make material progress on the benchmark.","We believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources."],"url":"http://arxiv.org/abs/2402.16801v1","category":"cs.LG"}
{"created":"2024-02-26 17:07:51","title":"Asymptotic-preserving and energy stable dynamical low-rank approximation for thermal radiative transfer equations","abstract":"The thermal radiative transfer equations model temperature evolution through a background medium as a result of radiation. When a large number of particles are absorbed in a short time scale, the dynamics tend to a non-linear diffusion-type equation called the Rosseland approximation. The main challenges for constructing numerical schemes that exhibit the correct limiting behavior are posed by the solution's high-dimensional phase space and multi-scale effects. In this work, we propose an asymptotic-preserving and rank-adaptive dynamical low-rank approximation scheme based on the macro-micro decomposition of the particle density and a modified augmented basis-update \\& Galerkin integrator. We show that this scheme, for linear particle emission by the material, dissipates energy over time under a step size restriction that captures the hyperbolic and parabolic CFL conditions. We demonstrate the efficacy of the proposed method in a series of numerical experiments.","sentences":["The thermal radiative transfer equations model temperature evolution through a background medium as a result of radiation.","When a large number of particles are absorbed in a short time scale, the dynamics tend to a non-linear diffusion-type equation called the Rosseland approximation.","The main challenges for constructing numerical schemes that exhibit the correct limiting behavior are posed by the solution's high-dimensional phase space and multi-scale effects.","In this work, we propose an asymptotic-preserving and rank-adaptive dynamical low-rank approximation scheme based on the macro-micro decomposition of the particle density and a modified augmented basis-update \\& Galerkin integrator.","We show that this scheme, for linear particle emission by the material, dissipates energy over time under a step size restriction that captures the hyperbolic and parabolic CFL conditions.","We demonstrate the efficacy of the proposed method in a series of numerical experiments."],"url":"http://arxiv.org/abs/2402.16746v1","category":"math.NA"}
{"created":"2024-02-26 16:50:08","title":"Auto Tuning for OpenMP Dynamic Scheduling applied to FWI","abstract":"Because Full Waveform Inversion (FWI) works with a massive amount of data, its execution requires much time and computational resources, being restricted to large-scale computer systems such as supercomputers. Techniques such as FWI adapt well to parallel computing and can be parallelized in shared memory systems using the application programming interface (API) OpenMP. The management of parallel tasks can be performed through loop schedulers contained in OpenMP. The dynamic scheduler stands out for distributing predefined fixed-size chunk sizes to idle processing cores at runtime. It can better adapt to FWI, where data processing can be irregular. However, the relationship between the size of the chunk size and the runtime is unknown. Optimization techniques can employ meta-heuristics to explore the parameter search space, avoiding testing all possible solutions. Here, we propose a strategy to use the Parameter Auto Tuning for Shared Memory Algorithms (PATSMA), with Coupled Simulated Annealing (CSA) as its optimization method, to automatically adjust the chunk size for the dynamic scheduling of wave propagation, one of the most expensive steps in FWI. Since testing each candidate chunk size in the complete FWI is unpractical, our approach consists of running a PATSMA where the objective function is the runtime of the first time iteration of the first seismic shot of the first FWI iteration. The resulting chunk size is then employed in all wave propagations involved in an FWI. We conducted tests to measure the runtime of an FWI using the proposed autotuning, varying the problem size and running on different computational environments, such as supercomputers and cloud computing instances. The results show that applying the proposed autotuning in an FWI reduces its runtime by up to 70.46% compared to standard OpenMP schedulers.","sentences":["Because Full Waveform Inversion (FWI) works with a massive amount of data, its execution requires much time and computational resources, being restricted to large-scale computer systems such as supercomputers.","Techniques such as FWI adapt well to parallel computing and can be parallelized in shared memory systems using the application programming interface (API) OpenMP.","The management of parallel tasks can be performed through loop schedulers contained in OpenMP.","The dynamic scheduler stands out for distributing predefined fixed-size chunk sizes to idle processing cores at runtime.","It can better adapt to FWI, where data processing can be irregular.","However, the relationship between the size of the chunk size and the runtime is unknown.","Optimization techniques can employ meta-heuristics to explore the parameter search space, avoiding testing all possible solutions.","Here, we propose a strategy to use the Parameter Auto Tuning for Shared Memory Algorithms (PATSMA), with Coupled Simulated Annealing (CSA) as its optimization method, to automatically adjust the chunk size for the dynamic scheduling of wave propagation, one of the most expensive steps in FWI.","Since testing each candidate chunk size in the complete FWI is unpractical, our approach consists of running a PATSMA where the objective function is the runtime of the first time iteration of the first seismic shot of the first FWI iteration.","The resulting chunk size is then employed in all wave propagations involved in an FWI.","We conducted tests to measure the runtime of an FWI using the proposed autotuning, varying the problem size and running on different computational environments, such as supercomputers and cloud computing instances.","The results show that applying the proposed autotuning in an FWI reduces its runtime by up to 70.46% compared to standard OpenMP schedulers."],"url":"http://arxiv.org/abs/2402.16728v1","category":"cs.DC"}
{"created":"2024-02-26 16:48:34","title":"Modeling error correction with Lindblad dynamics and approximate channels","abstract":"We analyze the performance of a quantum error correction code subject to physically-motivated noise modeled by a Lindblad master equation. Working within the code-capacity framework, we consider dissipative and coherent single-qubit terms and two-qubit crosstalk, studying how different approximations of the noise capture the performance of the five-qubit code. A composite-channel approximation where every noise term is considered separately, captures the behavior in many physical cases up to considerably-long timescales, and we analyze its eventual failure due to the effect of noncommuting terms. In contrast, we find that single-qubit approximations do not properly capture the error correction dynamics with two-qubit noise, even for short times. A Pauli approximation going beyond a single-qubit channel, is sensitive to the details of the noise, state, and decoder, and succeeds in many cases at short timescales relative to the noise strength, beyond which it fails. We calculate the code pseudo-threshold emerging within this model, and demonstrate how knowledge of the qubit parameters and connectivity can be used to design better decoders. These results shed light on the performance of error correction codes in the presence of realistic noise and can advance the ongoing efforts toward useful quantum error correction.","sentences":["We analyze the performance of a quantum error correction code subject to physically-motivated noise modeled by a Lindblad master equation.","Working within the code-capacity framework, we consider dissipative and coherent single-qubit terms and two-qubit crosstalk, studying how different approximations of the noise capture the performance of the five-qubit code.","A composite-channel approximation where every noise term is considered separately, captures the behavior in many physical cases up to considerably-long timescales, and we analyze its eventual failure due to the effect of noncommuting terms.","In contrast, we find that single-qubit approximations do not properly capture the error correction dynamics with two-qubit noise, even for short times.","A Pauli approximation going beyond a single-qubit channel, is sensitive to the details of the noise, state, and decoder, and succeeds in many cases at short timescales relative to the noise strength, beyond which it fails.","We calculate the code pseudo-threshold emerging within this model, and demonstrate how knowledge of the qubit parameters and connectivity can be used to design better decoders.","These results shed light on the performance of error correction codes in the presence of realistic noise and can advance the ongoing efforts toward useful quantum error correction."],"url":"http://arxiv.org/abs/2402.16727v1","category":"quant-ph"}
{"created":"2024-02-26 16:24:39","title":"Performance of high-order Godunov-type methods in simulations of astrophysical low Mach number flows","abstract":"High-order Godunov methods for gas dynamics have become a standard tool for simulating different classes of astrophysical flows. Their accuracy is mostly determined by the spatial interpolant used to reconstruct the pair of Riemann states at cell interfaces and by the Riemann solver that computes the interface fluxes. In most Godunov-type methods, these two steps can be treated independently, so that many different schemes can in principle be built from the same numerical framework. In this work, we use our fully compressible Seven-League Hydro (SLH) code to test the accuracy of six reconstruction methods and three approximate Riemann solvers on two- and three-dimensional (2D and 3D) problems involving subsonic flows only. We consider Mach numbers in the range from $10^{-3}$ to $10^{-1}$ in a well-posed, 2D, Kelvin--Helmholtz instability problem and a 3D turbulent convection zone that excites internal gravity waves in an overlying stable layer. We find that (i) there is a spread of almost four orders of magnitude in computational cost per fixed accuracy between the methods tested in this study, with the most performant method being a combination of a \"low-dissipation\" Riemann solver and a sextic reconstruction scheme, (ii) the low-dissipation solver always outperforms conventional Riemann solvers on a fixed grid when the reconstruction scheme is kept the same, (iii) in simulations of turbulent flows, increasing the order of spatial reconstruction reduces the characteristic dissipation length scale achieved on a given grid even if the overall scheme is only second order accurate, (iv) reconstruction methods based on slope-limiting techniques tend to generate artificial, high-frequency acoustic waves during the evolution of the flow, (v) unlimited reconstruction methods introduce oscillations in the thermal stratification near the convective boundary, where the entropy gradient is steep.","sentences":["High-order Godunov methods for gas dynamics have become a standard tool for simulating different classes of astrophysical flows.","Their accuracy is mostly determined by the spatial interpolant used to reconstruct the pair of Riemann states at cell interfaces and by the Riemann solver that computes the interface fluxes.","In most Godunov-type methods, these two steps can be treated independently, so that many different schemes can in principle be built from the same numerical framework.","In this work, we use our fully compressible Seven-League Hydro (SLH) code to test the accuracy of six reconstruction methods and three approximate Riemann solvers on two-","and three-dimensional (2D and 3D) problems involving subsonic flows only.","We consider Mach numbers in the range from $10^{-3}$ to $10^{-1}$ in a well-posed, 2D, Kelvin--Helmholtz instability problem and a 3D turbulent convection zone that excites internal gravity waves in an overlying stable layer.","We find that (i) there is a spread of almost four orders of magnitude in computational cost per fixed accuracy between the methods tested in this study, with the most performant method being a combination of a \"low-dissipation\" Riemann solver and a sextic reconstruction scheme, (ii) the low-dissipation solver always outperforms conventional Riemann solvers on a fixed grid when the reconstruction scheme is kept the same, (iii) in simulations of turbulent flows, increasing the order of spatial reconstruction reduces the characteristic dissipation length scale achieved on a given grid even if the overall scheme is only second order accurate, (iv) reconstruction methods based on slope-limiting techniques tend to generate artificial, high-frequency acoustic waves during the evolution of the flow, (v) unlimited reconstruction methods introduce oscillations in the thermal stratification near the convective boundary, where the entropy gradient is steep."],"url":"http://arxiv.org/abs/2402.16706v1","category":"astro-ph.SR"}
{"created":"2024-02-27 18:59:29","title":"The Strominger System and Flows by the Ricci Tensor","abstract":"This is a survey on the Strominger system and a geometric flow known as the anomaly flow. We will discuss various aspects of non-K\\\"ahler geometry on Calabi-Yau threefolds. Along the way, we discuss balanced metrics and balanced classes, the Aeppli cohomology class associated to a solution to the Strominger system, the equations of motion of heterotic supergravity, and a version of Ricci flow in this special geometry.","sentences":["This is a survey on the Strominger system and a geometric flow known as the anomaly flow.","We will discuss various aspects of non-K\\\"ahler geometry on Calabi-Yau threefolds.","Along the way, we discuss balanced metrics and balanced classes, the Aeppli cohomology class associated to a solution to the Strominger system, the equations of motion of heterotic supergravity, and a version of Ricci flow in this special geometry."],"url":"http://arxiv.org/abs/2402.17770v1","category":"math.DG"}
{"created":"2024-02-27 18:30:17","title":"A geometric characterization of $\\mathbb{N}$-manifolds and the Frobenius theorem","abstract":"This paper studies graded manifolds with local coordinates concentrated in non-negative degrees. We provide a canonical description of these objects in terms of classical geometric data and, building on this geometric viewpoint, we prove the Frobenius theorem for distributions in this graded setting.","sentences":["This paper studies graded manifolds with local coordinates concentrated in non-negative degrees.","We provide a canonical description of these objects in terms of classical geometric data and, building on this geometric viewpoint, we prove the Frobenius theorem for distributions in this graded setting."],"url":"http://arxiv.org/abs/2402.17746v1","category":"math.DG"}
{"created":"2024-02-27 18:22:48","title":"Rose: Efficient and Extensible Autodiff on the Web","abstract":"Automatic differentiation (AD) has become the backbone for a new wave of optimization-driven domains such as computer graphics and machine learning over the past decade. However, existing AD systems face limitations, either lacking support for in-browser development or failing to harness more recent, compilerbased approaches to achieve both expressiveness and size-preserving differentiation. This work introduces Rose, a portable, extensible AD library that runs on the web. Rose allows users to write opaque functions with custom derivatives and supports dynamic construction of AD functions. We integrated Rose into two differentiable simulations and a diagram authoring tool to demonstrate the utility of Rose's design. Finally, we show that Rose is 173x as fast as TensorFlow.js in compiling and running a benchmark suite of optimized diagrams.","sentences":["Automatic differentiation (AD) has become the backbone for a new wave of optimization-driven domains such as computer graphics and machine learning over the past decade.","However, existing AD systems face limitations, either lacking support for in-browser development or failing to harness more recent, compilerbased approaches to achieve both expressiveness and size-preserving differentiation.","This work introduces Rose, a portable, extensible AD library that runs on the web.","Rose allows users to write opaque functions with custom derivatives and supports dynamic construction of AD functions.","We integrated Rose into two differentiable simulations and a diagram authoring tool to demonstrate the utility of Rose's design.","Finally, we show that Rose is 173x as fast as TensorFlow.js in compiling and running a benchmark suite of optimized diagrams."],"url":"http://arxiv.org/abs/2402.17743v1","category":"cs.PL"}
{"created":"2024-02-27 18:12:43","title":"High-Fidelity Neural Phonetic Posteriorgrams","abstract":"A phonetic posteriorgram (PPG) is a time-varying categorical distribution over acoustic units of speech (e.g., phonemes). PPGs are a popular representation in speech generation due to their ability to disentangle pronunciation features from speaker identity, allowing accurate reconstruction of pronunciation (e.g., voice conversion) and coarse-grained pronunciation editing (e.g., foreign accent conversion). In this paper, we demonstrably improve the quality of PPGs to produce a state-of-the-art interpretable PPG representation. We train an off-the-shelf speech synthesizer using our PPG representation and show that high-quality PPGs yield independent control over pitch and pronunciation. We further demonstrate novel uses of PPGs, such as an acoustic pronunciation distance and fine-grained pronunciation control.","sentences":["A phonetic posteriorgram (PPG) is a time-varying categorical distribution over acoustic units of speech (e.g., phonemes).","PPGs are a popular representation in speech generation due to their ability to disentangle pronunciation features from speaker identity, allowing accurate reconstruction of pronunciation (e.g., voice conversion) and coarse-grained pronunciation editing (e.g., foreign accent conversion).","In this paper, we demonstrably improve the quality of PPGs to produce a state-of-the-art interpretable PPG representation.","We train an off-the-shelf speech synthesizer using our PPG representation and show that high-quality PPGs yield independent control over pitch and pronunciation.","We further demonstrate novel uses of PPGs, such as an acoustic pronunciation distance and fine-grained pronunciation control."],"url":"http://arxiv.org/abs/2402.17735v1","category":"eess.AS"}
{"created":"2024-02-27 17:55:26","title":"Computation of the expectation value of the spin operator $\\hat{S}^2$ for the Spin-Flip Bethe-Salpeter Equation","abstract":"Spin-flip methods applied to excited-state approaches like the Bethe-Salpeter Equation allow access to the excitation energies of open-shell systems, such as molecules and defects in solids. The eigenstates of these solutions, however, are generally not eigenstates of the spin operator $\\hat{S}^2$. Even for simple cases where the excitation vector is expected to be, for example, a triplet state, the value of $\\langle \\hat{S}^2 \\rangle$ may be found to differ from 2.00; this difference is called ``spin contamination.'' The expectation values $\\langle \\hat{S}^2 \\rangle$ must be computed for each excitation vector, to assist with the characterization of the particular excitation and to determine the amount of spin contamination of the state. Our aim is to provide for the first time in the spin-flip methods literature a comprehensive resource on the derivation of the formulas for $\\langle \\hat{S}^2 \\rangle$ as well as its computational implementation. After a brief discussion of the theory of the Spin-Flip Bethe-Salpeter Equation and some examples further illustrating the need for calculating $\\langle \\hat{S}^2 \\rangle$, we present the derivation for the general equation for computing $\\langle \\hat{S}^2 \\rangle$ with the eigenvectors from an SF-BSE calculation, how it is implemented in a Python script, and timing information on how this calculation scales with the size of the SF-BSE Hamiltonian.","sentences":["Spin-flip methods applied to excited-state approaches like the Bethe-Salpeter Equation allow access to the excitation energies of open-shell systems, such as molecules and defects in solids.","The eigenstates of these solutions, however, are generally not eigenstates of the spin operator $\\hat{S}^2$. Even for simple cases where the excitation vector is expected to be, for example, a triplet state, the value of $\\langle \\hat{S}^2 \\rangle$ may be found to differ from 2.00; this difference is called ``spin contamination.''","The expectation values $\\langle \\hat{S}^2 \\rangle$ must be computed for each excitation vector, to assist with the characterization of the particular excitation and to determine the amount of spin contamination of the state.","Our aim is to provide for the first time in the spin-flip methods literature a comprehensive resource on the derivation of the formulas for $\\langle \\hat{S}^2 \\rangle$ as well as its computational implementation.","After a brief discussion of the theory of the Spin-Flip Bethe-Salpeter Equation and some examples further illustrating the need for calculating $\\langle \\hat{S}^2 \\rangle$, we present the derivation for the general equation for computing $\\langle \\hat{S}^2 \\rangle$ with the eigenvectors from an SF-BSE calculation, how it is implemented in a Python script, and timing information on how this calculation scales with the size of the SF-BSE Hamiltonian."],"url":"http://arxiv.org/abs/2402.17719v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-27 17:50:47","title":"Attention-based Neural Network Emulators for Multi-Probe Data Vectors Part I: Forecasting the Growth-Geometry split","abstract":"We present a new class of machine-learning emulators that accurately model the cosmic shear, galaxy-galaxy lensing, and galaxy clustering real space correlation functions in the context of Rubin Observatory year one simulated data. To illustrate its capabilities in forecasting models beyond the standard $\\Lambda$CDM, we forecast how well LSST Year 1 data will be able to probe the consistency between geometry $\\Omega^{\\rm geo}_\\mathrm{m}$ and growth $\\Omega^{\\rm growth}_\\mathrm{m}$ dark matter densities in the so-called split $\\Lambda$CDM parameterization. When trained with a few million samples, our emulator shows uniform accuracy across a wide range in an 18-dimensional parameter space. We provide a detailed comparison of three neural network designs, illustrating the importance of adopting state-of-the-art Transformer blocks. Our study also details their performance when computing Bayesian evidence for cosmic shear on three fiducial cosmologies. The transformers-based emulator is always accurate within PolyChord's precision. As an application, we use our emulator to study the degeneracies between dark energy models and growth geometry split parameterizations. We find that the growth-geometry split remains to be a meaningful test of the smooth dark energy assumption.","sentences":["We present a new class of machine-learning emulators that accurately model the cosmic shear, galaxy-galaxy lensing, and galaxy clustering real space correlation functions in the context of Rubin Observatory year one simulated data.","To illustrate its capabilities in forecasting models beyond the standard $\\Lambda$CDM, we forecast how well LSST","Year 1 data will be able to probe the consistency between geometry $\\Omega^{\\rm geo}_\\mathrm{m}$ and growth $\\Omega^{\\rm growth}_\\mathrm{m}$ dark matter densities in the so-called split $\\Lambda$CDM parameterization.","When trained with a few million samples, our emulator shows uniform accuracy across a wide range in an 18-dimensional parameter space.","We provide a detailed comparison of three neural network designs, illustrating the importance of adopting state-of-the-art Transformer blocks.","Our study also details their performance when computing Bayesian evidence for cosmic shear on three fiducial cosmologies.","The transformers-based emulator is always accurate within PolyChord's precision.","As an application, we use our emulator to study the degeneracies between dark energy models and growth geometry split parameterizations.","We find that the growth-geometry split remains to be a meaningful test of the smooth dark energy assumption."],"url":"http://arxiv.org/abs/2402.17716v1","category":"astro-ph.CO"}
{"created":"2024-02-27 17:48:56","title":"An all-frequency stable surface integral equation algorithm for electromagnetism in 3-D unbounded penetrable media: Continuous and fully-discrete model analysis","abstract":"We use the time-harmonic Maxwell partial differential equations (PDEs) to model the wave propagation in 3-D space, which comprises a closed penetrable scatterer and its unbounded free-space complement. Surface integral equations (SIEs) that are equivalent to the time-harmonic Maxwell PDEs provide an efficient framework to directly model the surface electromagnetic fields and hence the RCS.The equivalent SIE system on the interface has the advantages that: (a) it avoids truncation of the unbounded region and the solution exactly satisfies the radiation condition; and (b) the surface-fields solution yields the unknowns in the Maxwell PDEs through surface potential representations of the interior and exterior fields. The Maxwell PDE system has been proven (several decades ago) to be stable for all frequencies, that is, (i) it does not possess eigenfrequencies (it is well-posed); and (ii) it does not suffer from low-frequency. However, weakly-singular SIE reformulations of the PDE satisfying these two properties, subject to a stabilization constraint, were derived and mathematically proven only about a decade ago (see {J. Math. Anal. Appl. 412 (2014) 277-300}). The aim of this article is two-fold: (I) To effect a robust coupling of the stabilization constraint to the weakly singular SIE and use mathematical analysis to establish that the resulting continuous weakly-singular second-kind self-adjoint SIE system (without constraints) retains all-frequency stability; and (II) To apply a fully-discrete spectral algorithm for the all-frequency-stable weakly-singular second-kind SIE, and prove spectral accuracy of the algorithm. We numerically demonstrate the high-order accuracy of the algorithm using several dielectric and absorbing benchmark scatterers with curved surfaces.","sentences":["We use the time-harmonic Maxwell partial differential equations (PDEs) to model the wave propagation in 3-D space, which comprises a closed penetrable scatterer and its unbounded free-space complement.","Surface integral equations (SIEs) that are equivalent to the time-harmonic Maxwell PDEs provide an efficient framework to directly model the surface electromagnetic fields and hence the RCS.The equivalent SIE system on the interface has the advantages that: (a) it avoids truncation of the unbounded region and the solution exactly satisfies the radiation condition; and (b) the surface-fields solution yields the unknowns in the Maxwell PDEs through surface potential representations of the interior and exterior fields.","The Maxwell PDE system has been proven (several decades ago) to be stable for all frequencies, that is, (i) it does not possess eigenfrequencies (it is well-posed); and (ii) it does not suffer from low-frequency.","However, weakly-singular SIE reformulations of the PDE satisfying these two properties, subject to a stabilization constraint, were derived and mathematically proven only about a decade ago (see {J. Math.","Anal.","Appl.","412 (2014) 277-300}).","The aim of this article is two-fold: (I) To effect a robust coupling of the stabilization constraint to the weakly singular SIE and use mathematical analysis to establish that the resulting continuous weakly-singular second-kind self-adjoint SIE system (without constraints) retains all-frequency stability; and (II) To apply a fully-discrete spectral algorithm for the all-frequency-stable weakly-singular second-kind SIE, and prove spectral accuracy of the algorithm.","We numerically demonstrate the high-order accuracy of the algorithm using several dielectric and absorbing benchmark scatterers with curved surfaces."],"url":"http://arxiv.org/abs/2402.17713v1","category":"math.NA"}
{"created":"2024-02-27 17:43:51","title":"Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers","abstract":"In neural network binarization, BinaryConnect (BC) and its variants are considered the standard. These methods apply the sign function in their forward pass and their respective gradients are backpropagated to update the weights. However, the derivative of the sign function is zero whenever defined, which consequently freezes training. Therefore, implementations of BC (e.g., BNN) usually replace the derivative of sign in the backward computation with identity or other approximate gradient alternatives. Although such practice works well empirically, it is largely a heuristic or ''training trick.'' We aim at shedding some light on these training tricks from the optimization perspective. Building from existing theory on ProxConnect (PC, a generalization of BC), we (1) equip PC with different forward-backward quantizers and obtain ProxConnect++ (PC++) that includes existing binarization techniques as special cases; (2) derive a principled way to synthesize forward-backward quantizers with automatic theoretical guarantees; (3) illustrate our theory by proposing an enhanced binarization algorithm BNN++; (4) conduct image classification experiments on CNNs and vision transformers, and empirically verify that BNN++ generally achieves competitive results on binarizing these models.","sentences":["In neural network binarization, BinaryConnect (BC) and its variants are considered the standard.","These methods apply the sign function in their forward pass and their respective gradients are backpropagated to update the weights.","However, the derivative of the sign function is zero whenever defined, which consequently freezes training.","Therefore, implementations of BC (e.g., BNN) usually replace the derivative of sign in the backward computation with identity or other approximate gradient alternatives.","Although such practice works well empirically, it is largely a heuristic or ''training trick.''","We aim at shedding some light on these training tricks from the optimization perspective.","Building from existing theory on ProxConnect (PC, a generalization of BC), we (1) equip PC with different forward-backward quantizers and obtain ProxConnect++ (PC++) that includes existing binarization techniques as special cases; (2) derive a principled way to synthesize forward-backward quantizers with automatic theoretical guarantees; (3) illustrate our theory by proposing an enhanced binarization algorithm BNN++; (4) conduct image classification experiments on CNNs and vision transformers, and empirically verify that BNN++ generally achieves competitive results on binarizing these models."],"url":"http://arxiv.org/abs/2402.17710v1","category":"cs.LG"}
{"created":"2024-02-27 17:08:47","title":"A Complete Graphical Language for Linear Optical Circuits with Finite-Photon-Number Sources and Detectors","abstract":"Linear optical circuits can be used to manipulate the quantum states of photons as they pass through components including beam splitters and phase shifters. Those photonic states possess a particularly high level of expressiveness, as they reside within the bosonic Fock space, an infinite-dimensional Hilbert space. However, in the domain of linear optical quantum computation, these basic components may not be sufficient to efficiently perform all computations of interest, such as universal quantum computation. To address this limitation it is common to add auxiliary sources and detectors, which enable projections onto auxiliary photonic states and thus increase the versatility of the processes. In this paper, we introduce the $\\textbf{LO}_{fi}$-calculus, a graphical language to reason on the infinite-dimensional bosonic Fock space with circuits composed of four core elements of linear optics: the phase shifter, the beam splitter, and auxiliary sources and detectors with bounded photon number. We present an equational theory that we prove to be complete: two $\\textbf{LO}_{fi}$-circuits represent the same quantum process if and only if one can be transformed into the other with the rules of the $\\textbf{LO}_{fi}$-calculus. We give a unique and compact universal form for such circuits.","sentences":["Linear optical circuits can be used to manipulate the quantum states of photons as they pass through components including beam splitters and phase shifters.","Those photonic states possess a particularly high level of expressiveness, as they reside within the bosonic Fock space, an infinite-dimensional Hilbert space.","However, in the domain of linear optical quantum computation, these basic components may not be sufficient to efficiently perform all computations of interest, such as universal quantum computation.","To address this limitation it is common to add auxiliary sources and detectors, which enable projections onto auxiliary photonic states and thus increase the versatility of the processes.","In this paper, we introduce the $\\textbf{LO}_{fi}$-calculus, a graphical language to reason on the infinite-dimensional bosonic Fock space with circuits composed of four core elements of linear optics: the phase shifter, the beam splitter, and auxiliary sources and detectors with bounded photon number.","We present an equational theory that we prove to be complete: two $\\textbf{LO}_{fi}$-circuits represent the same quantum process if and only if one can be transformed into the other with the rules of the $\\textbf{LO}_{fi}$-calculus.","We give a unique and compact universal form for such circuits."],"url":"http://arxiv.org/abs/2402.17693v1","category":"quant-ph"}
{"created":"2024-02-27 17:07:25","title":"Expansion dynamics of Bose-Einstein condensates in a synthetic magnetic field","abstract":"We investigate the expansion dynamics of spin-orbit-coupled Bose-Einstein condensates subjected to a synthetic magnetic field, after their release from an external harmonic trap. Our findings reveal that the condensate experiences a spin-dependent rotation and separation due to the rigid-like rotational velocity field, which leads to a spin density deflection. The deflection angle reaches a peak at a time that is inversely related to the frequency of the harmonic trap. When the detuning gradient is below a critical value for vortex nucleation, our analytical results derived from a spinor hydrodynamic theory align closely with numerical results using the coupled Gross-Pitaevskii equations. Beyond this critical value, we also numerically simulated the expansion dynamics of the condensates containing vortices with negative circulation. Our findings highlight the pivotal role of the rigid-like rotational velocity field on the dynamics of the condensate and may stimulate further experimental investigations into the rich superfluid dynamics induced by synthetic magnetic fields.","sentences":["We investigate the expansion dynamics of spin-orbit-coupled Bose-Einstein condensates subjected to a synthetic magnetic field, after their release from an external harmonic trap.","Our findings reveal that the condensate experiences a spin-dependent rotation and separation due to the rigid-like rotational velocity field, which leads to a spin density deflection.","The deflection angle reaches a peak at a time that is inversely related to the frequency of the harmonic trap.","When the detuning gradient is below a critical value for vortex nucleation, our analytical results derived from a spinor hydrodynamic theory align closely with numerical results using the coupled Gross-Pitaevskii equations.","Beyond this critical value, we also numerically simulated the expansion dynamics of the condensates containing vortices with negative circulation.","Our findings highlight the pivotal role of the rigid-like rotational velocity field on the dynamics of the condensate and may stimulate further experimental investigations into the rich superfluid dynamics induced by synthetic magnetic fields."],"url":"http://arxiv.org/abs/2402.17691v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-27 17:05:06","title":"Novel spectral methods for shock capturing and the removal of tygers in computational fluid dynamics","abstract":"Spectral methods yield numerical solutions of the Galerkin-truncated versions of nonlinear partial differential equations involved especially in fluid dynamics. In the presence of discontinuities, such as shocks, spectral approximations develop Gibbs oscillations near the discontinuity. This causes the numerical solution to deviate quickly from the true solution. For spectral approximations of the 1D inviscid Burgers equation, nonlinear wave resonances lead to the formation of tygers in well-resolved areas of the flow, far from the shock. Recently, Besse(to be published) has proposed novel spectral relaxation (SR) and spectral purging (SP) schemes for the removal of tygers and Gibbs oscillations in spectral approximations of nonlinear conservation laws. For the 1D inviscid Burgers equation, it is shown that the novel SR and SP approximations of the solution converge strongly in L2 norm to the entropic weak solution, under an appropriate choice of kernels and related parameters. In this work, we carry out a detailed numerical investigation of SR and SP schemes when applied to the 1D inviscid Burgers equation and report the efficiency of shock capture and the removal of tygers. We then extend our study to systems of nonlinear hyperbolic conservation laws - such as the 2x2 system of the shallow water equations and the standard 3x3 system of 1D compressible Euler equations. For the latter, we generalise the implementation of SR methods to non-periodic problems using Chebyshev polynomials. We then turn to singular flow in the 1D wall approximation of the 3D-axisymmetric wall-bounded incompressible Euler equation. Here, in order to determine the blowup time of the solution, we compare the decay of the width of the analyticity strip, obtained from the pure pseudospectral method, with the improved estimate obtained using the novel spectral relaxation scheme.","sentences":["Spectral methods yield numerical solutions of the Galerkin-truncated versions of nonlinear partial differential equations involved especially in fluid dynamics.","In the presence of discontinuities, such as shocks, spectral approximations develop Gibbs oscillations near the discontinuity.","This causes the numerical solution to deviate quickly from the true solution.","For spectral approximations of the 1D inviscid Burgers equation, nonlinear wave resonances lead to the formation of tygers in well-resolved areas of the flow, far from the shock.","Recently, Besse(to be published) has proposed novel spectral relaxation (SR) and spectral purging (SP) schemes for the removal of tygers and Gibbs oscillations in spectral approximations of nonlinear conservation laws.","For the 1D inviscid Burgers equation, it is shown that the novel SR and SP approximations of the solution converge strongly in L2 norm to the entropic weak solution, under an appropriate choice of kernels and related parameters.","In this work, we carry out a detailed numerical investigation of SR and SP schemes when applied to the 1D inviscid Burgers equation and report the efficiency of shock capture and the removal of tygers.","We then extend our study to systems of nonlinear hyperbolic conservation laws - such as the 2x2 system of the shallow water equations and the standard 3x3 system of 1D compressible Euler equations.","For the latter, we generalise the implementation of SR methods to non-periodic problems using Chebyshev polynomials.","We then turn to singular flow in the 1D wall approximation of the 3D-axisymmetric wall-bounded incompressible Euler equation.","Here, in order to determine the blowup time of the solution, we compare the decay of the width of the analyticity strip, obtained from the pure pseudospectral method, with the improved estimate obtained using the novel spectral relaxation scheme."],"url":"http://arxiv.org/abs/2402.17688v1","category":"math.NA"}
{"created":"2024-02-27 17:01:21","title":"Outlier-Detection for Reactive Machine Learned Potential Energy Surfaces","abstract":"Uncertainty quantification (UQ) to detect samples with large expected errors (outliers) is applied to reactive molecular potential energy surfaces (PESs). Three methods - Ensembles, Deep Evidential Regression (DER), and Gaussian Mixture Models (GMM) - were applied to the H-transfer reaction between ${\\it syn-}$Criegee and vinyl hydroxyperoxide. The results indicate that ensemble models provide the best results for detecting outliers, followed by GMM. For example, from a pool of 1000 structures with the largest uncertainty, the detection quality for outliers is $\\sim 90$ \\% and $\\sim 50$ \\%, respectively, if 25 or 1000 structures with large errors are sought. On the contrary, the limitations of the statistical assumptions of DER greatly impacted its prediction capabilities. Finally, a structure-based indicator was found to be correlated with large average error, which may help to rapidly classify new structures into those that provide an advantage for refining the neural network.","sentences":["Uncertainty quantification (UQ) to detect samples with large expected errors (outliers) is applied to reactive molecular potential energy surfaces (PESs).","Three methods - Ensembles, Deep Evidential Regression (DER), and Gaussian Mixture Models (GMM) - were applied to the H-transfer reaction between ${\\it syn-}$Criegee and vinyl hydroxyperoxide.","The results indicate that ensemble models provide the best results for detecting outliers, followed by GMM.","For example, from a pool of 1000 structures with the largest uncertainty, the detection quality for outliers is $\\sim 90$ \\% and $\\sim 50$ \\%, respectively, if 25 or 1000 structures with large errors are sought.","On the contrary, the limitations of the statistical assumptions of DER greatly impacted its prediction capabilities.","Finally, a structure-based indicator was found to be correlated with large average error, which may help to rapidly classify new structures into those that provide an advantage for refining the neural network."],"url":"http://arxiv.org/abs/2402.17686v1","category":"physics.chem-ph"}
{"created":"2024-02-27 16:54:56","title":"JKO schemes with general transport costs","abstract":"We modify the JKO scheme, which is a time discretization of Wasserstein gradient flows, by replacing the Wasserstein distance with more general transport costs on manifolds. We show when the cost function has a mixed Hessian which defines a Riemannian metric, our modified JKO scheme converges under suitable conditions to the corresponding Riemannian Fokker--Planck equation. Thus on a Riemannian manifold one may replace the (squared) Riemannian distance with any cost function which induces the metric. Of interest is when the Riemannian distance is computationally intractable, but a suitable cost has a simple analytic expression. We consider the Fokker--Planck equation on compact submanifolds with the Neumann boundary condition and on complete Riemannian manifolds with a finite drift condition. As an application we consider Hessian manifolds, taking as a cost the Bregman divergence.","sentences":["We modify the JKO scheme, which is a time discretization of Wasserstein gradient flows, by replacing the Wasserstein distance with more general transport costs on manifolds.","We show when the cost function has a mixed Hessian which defines a Riemannian metric, our modified JKO scheme converges under suitable conditions to the corresponding Riemannian Fokker--Planck equation.","Thus on a Riemannian manifold one may replace the (squared) Riemannian distance with any cost function which induces the metric.","Of interest is when the Riemannian distance is computationally intractable, but a suitable cost has a simple analytic expression.","We consider the Fokker--Planck equation on compact submanifolds with the Neumann boundary condition and on complete Riemannian manifolds with a finite drift condition.","As an application we consider Hessian manifolds, taking as a cost the Bregman divergence."],"url":"http://arxiv.org/abs/2402.17681v1","category":"math.AP"}
{"created":"2024-02-27 16:46:21","title":"SDF2Net: Shallow to Deep Feature Fusion Network for PolSAR Image Classification","abstract":"Polarimetric synthetic aperture radar (PolSAR) images encompass valuable information that can facilitate extensive land cover interpretation and generate diverse output products. Extracting meaningful features from PolSAR data poses challenges distinct from those encountered in optical imagery. Deep learning (DL) methods offer effective solutions for overcoming these challenges in PolSAR feature extraction. Convolutional neural networks (CNNs) play a crucial role in capturing PolSAR image characteristics by leveraging kernel capabilities to consider local information and the complex-valued nature of PolSAR data. In this study, a novel three-branch fusion of complex-valued CNN, named the Shallow to Deep Feature Fusion Network (SDF2Net), is proposed for PolSAR image classification. To validate the performance of the proposed method, classification results are compared against multiple state-of-the-art approaches using the airborne synthetic aperture radar (AIRSAR) datasets of Flevoland and San Francisco, as well as the ESAR Oberpfaffenhofen dataset. The results indicate that the proposed approach demonstrates improvements in overallaccuracy, with a 1.3% and 0.8% enhancement for the AIRSAR datasets and a 0.5% improvement for the ESAR dataset. Analyses conducted on the Flevoland data underscore the effectiveness of the SDF2Net model, revealing a promising overall accuracy of 96.01% even with only a 1% sampling ratio.","sentences":["Polarimetric synthetic aperture radar (PolSAR) images encompass valuable information that can facilitate extensive land cover interpretation and generate diverse output products.","Extracting meaningful features from PolSAR data poses challenges distinct from those encountered in optical imagery.","Deep learning (DL) methods offer effective solutions for overcoming these challenges in PolSAR feature extraction.","Convolutional neural networks (CNNs) play a crucial role in capturing PolSAR image characteristics by leveraging kernel capabilities to consider local information and the complex-valued nature of PolSAR data.","In this study, a novel three-branch fusion of complex-valued CNN, named the Shallow to Deep Feature Fusion Network (SDF2Net), is proposed for PolSAR image classification.","To validate the performance of the proposed method, classification results are compared against multiple state-of-the-art approaches using the airborne synthetic aperture radar (AIRSAR) datasets of Flevoland and San Francisco, as well as the ESAR Oberpfaffenhofen dataset.","The results indicate that the proposed approach demonstrates improvements in overallaccuracy, with a 1.3% and 0.8% enhancement for the AIRSAR datasets and a 0.5% improvement for the ESAR dataset.","Analyses conducted on the Flevoland data underscore the effectiveness of the SDF2Net model, revealing a promising overall accuracy of 96.01% even with only a 1% sampling ratio."],"url":"http://arxiv.org/abs/2402.17672v1","category":"cs.CV"}
{"created":"2024-02-27 16:35:07","title":"Bayesian Differentiable Physics for Cloth Digitalization","abstract":"We propose a new method for cloth digitalization. Deviating from existing methods which learn from data captured under relatively casual settings, we propose to learn from data captured in strictly tested measuring protocols, and find plausible physical parameters of the cloths. However, such data is currently absent, so we first propose a new dataset with accurate cloth measurements. Further, the data size is considerably smaller than the ones in current deep learning, due to the nature of the data capture process. To learn from small data, we propose a new Bayesian differentiable cloth model to estimate the complex material heterogeneity of real cloths. It can provide highly accurate digitalization from very limited data samples. Through exhaustive evaluation and comparison, we show our method is accurate in cloth digitalization, efficient in learning from limited data samples, and general in capturing material variations. Code and data are available https://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization","sentences":["We propose a new method for cloth digitalization.","Deviating from existing methods which learn from data captured under relatively casual settings, we propose to learn from data captured in strictly tested measuring protocols, and find plausible physical parameters of the cloths.","However, such data is currently absent, so we first propose a new dataset with accurate cloth measurements.","Further, the data size is considerably smaller than the ones in current deep learning, due to the nature of the data capture process.","To learn from small data, we propose a new Bayesian differentiable cloth model to estimate the complex material heterogeneity of real cloths.","It can provide highly accurate digitalization from very limited data samples.","Through exhaustive evaluation and comparison, we show our method is accurate in cloth digitalization, efficient in learning from limited data samples, and general in capturing material variations.","Code and data are available https://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization"],"url":"http://arxiv.org/abs/2402.17664v1","category":"cs.CV"}
{"created":"2024-02-27 16:31:10","title":"Exact solutions and automorphic systems of the geopotential forecast equation","abstract":"The study of the recently constructed group foliation for the geopotential forecast equation is continued. The group foliation consists of two systems, namely the automorphic and resolving systems, the analysis of which facilitates the derivation of invariant solutions for the original equation. As obtaining a general solution to the resolving system (even to its reductions on subgroups) is problematic, its various particular solutions are considered. Consequently, the question arises concerning the specific forms of automorphic systems that correspond to exact solutions obtained through alternative methods. This is of interest for both comparing solutions derived through different approaches and for the integration of specific automorphic systems. The problem is discussed in a number of examples.","sentences":["The study of the recently constructed group foliation for the geopotential forecast equation is continued.","The group foliation consists of two systems, namely the automorphic and resolving systems, the analysis of which facilitates the derivation of invariant solutions for the original equation.","As obtaining a general solution to the resolving system (even to its reductions on subgroups) is problematic, its various particular solutions are considered.","Consequently, the question arises concerning the specific forms of automorphic systems that correspond to exact solutions obtained through alternative methods.","This is of interest for both comparing solutions derived through different approaches and for the integration of specific automorphic systems.","The problem is discussed in a number of examples."],"url":"http://arxiv.org/abs/2402.17663v1","category":"math-ph"}
{"created":"2024-02-27 16:27:06","title":"TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations","abstract":"Achieving a balance between computational speed, prediction accuracy, and universal applicability in molecular simulations has been a persistent challenge. This paper presents substantial advancements in the TorchMD-Net software, a pivotal step forward in the shift from conventional force fields to neural network-based potentials. The evolution of TorchMD-Net into a more comprehensive and versatile framework is highlighted, incorporating cutting-edge architectures such as TensorNet. This transformation is achieved through a modular design approach, encouraging customized applications within the scientific community. The most notable enhancement is a significant improvement in computational efficiency, achieving a very remarkable acceleration in the computation of energy and forces for TensorNet models, with performance gains ranging from 2-fold to 10-fold over previous iterations. Other enhancements include highly optimized neighbor search algorithms that support periodic boundary conditions and the smooth integration with existing molecular dynamics frameworks. Additionally, the updated version introduces the capability to integrate physical priors, further enriching its application spectrum and utility in research. The software is available at https://github.com/torchmd/torchmd-net.","sentences":["Achieving a balance between computational speed, prediction accuracy, and universal applicability in molecular simulations has been a persistent challenge.","This paper presents substantial advancements in the TorchMD-Net software, a pivotal step forward in the shift from conventional force fields to neural network-based potentials.","The evolution of TorchMD-Net into a more comprehensive and versatile framework is highlighted, incorporating cutting-edge architectures such as TensorNet.","This transformation is achieved through a modular design approach, encouraging customized applications within the scientific community.","The most notable enhancement is a significant improvement in computational efficiency, achieving a very remarkable acceleration in the computation of energy and forces for TensorNet models, with performance gains ranging from 2-fold to 10-fold over previous iterations.","Other enhancements include highly optimized neighbor search algorithms that support periodic boundary conditions and the smooth integration with existing molecular dynamics frameworks.","Additionally, the updated version introduces the capability to integrate physical priors, further enriching its application spectrum and utility in research.","The software is available at https://github.com/torchmd/torchmd-net."],"url":"http://arxiv.org/abs/2402.17660v1","category":"cs.LG"}
{"created":"2024-02-27 16:25:46","title":"Heisenberg Soft Hair on Robinson-Trautman Spacetimes","abstract":"We study 4 dimensional $(4d$) gravitational waves (GWs) with compact wavefronts, generalizing Robinson-Trautman (RT) solutions in Einstein gravity with an arbitrary cosmological constant. We construct the most general solution of the GWs in the presence of a causal, timelike, or null boundary when the usual tensor modes are turned off. Our solution space besides the shape and topology of the wavefront which is a generic compact, smooth, and orientable $2d$ surface $\\Sigma$, is specified by a vector over $\\Sigma$ satisfying the conformal Killing equation and two scalars that are arbitrary functions over the causal boundary, the boundary modes (soft hair). We work out the symplectic form over the solution space using covariant phase space formalism and analyze the boundary symmetries and charges. The algebra of surface charges is a Heisenberg algebra. Only the overall size of the compact wavefront and not the details of its shape appears in the boundary symplectic form and is canonical conjugate to the overall mass of the GW. Hence, the information about the shape of the wavefront can't be probed by the boundary observer. We construct a boundary energy-momentum tensor and a boundary current, whose conservation yields the RT equation for both asymptotically AdS and flat spacetimes. The latter provides a hydrodynamic description for our RT solutions.","sentences":["We study 4 dimensional $(4d$) gravitational waves (GWs) with compact wavefronts, generalizing Robinson-Trautman (RT) solutions in Einstein gravity with an arbitrary cosmological constant.","We construct the most general solution of the GWs in the presence of a causal, timelike, or null boundary when the usual tensor modes are turned off.","Our solution space besides the shape and topology of the wavefront which is a generic compact, smooth, and orientable $2d$ surface $\\Sigma$, is specified by a vector over $\\Sigma$ satisfying the conformal Killing equation and two scalars that are arbitrary functions over the causal boundary, the boundary modes (soft hair).","We work out the symplectic form over the solution space using covariant phase space formalism and analyze the boundary symmetries and charges.","The algebra of surface charges is a Heisenberg algebra.","Only the overall size of the compact wavefront and not the details of its shape appears in the boundary symplectic form and is canonical conjugate to the overall mass of the GW.","Hence, the information about the shape of the wavefront can't be probed by the boundary observer.","We construct a boundary energy-momentum tensor and a boundary current, whose conservation yields the RT equation for both asymptotically AdS and flat spacetimes.","The latter provides a hydrodynamic description for our RT solutions."],"url":"http://arxiv.org/abs/2402.17658v1","category":"hep-th"}
{"created":"2024-02-27 16:18:40","title":"Computing eigenfrequency sensitivities near exceptional points","abstract":"Exceptional points are spectral degeneracies of non-Hermitian systems where both eigenfrequencies and eigenmodes coalesce. The eigenfrequency sensitivities near an exceptional point are significantly enhanced, whereby they diverge directly at the exceptional point. Capturing this enhanced sensitivity is crucial for the investigation and optimization of exceptional-point-based applications, such as optical sensors. We present a numerical framework, based on contour integration and algorithmic differentiation, to accurately and efficiently compute eigenfrequency sensitivities near exceptional points. We demonstrate the framework to an optical microdisk cavity and derive a semi-analytical solution to validate the numerical results. The computed eigenfrequency sensitivities are used to track the exceptional point along an exceptional surface in the parameter space. The presented framework can be applied to any kind of resonance problem, e.g., with arbitrary geometry or with exceptional points of arbitrary order.","sentences":["Exceptional points are spectral degeneracies of non-Hermitian systems where both eigenfrequencies and eigenmodes coalesce.","The eigenfrequency sensitivities near an exceptional point are significantly enhanced, whereby they diverge directly at the exceptional point.","Capturing this enhanced sensitivity is crucial for the investigation and optimization of exceptional-point-based applications, such as optical sensors.","We present a numerical framework, based on contour integration and algorithmic differentiation, to accurately and efficiently compute eigenfrequency sensitivities near exceptional points.","We demonstrate the framework to an optical microdisk cavity and derive a semi-analytical solution to validate the numerical results.","The computed eigenfrequency sensitivities are used to track the exceptional point along an exceptional surface in the parameter space.","The presented framework can be applied to any kind of resonance problem, e.g., with arbitrary geometry or with exceptional points of arbitrary order."],"url":"http://arxiv.org/abs/2402.17648v1","category":"physics.comp-ph"}
{"created":"2024-02-27 16:12:51","title":"The critical disordered pinning measure","abstract":"In this paper, we study a disordered pinning model induced by a random walk whose increments have a finite fourth moment and vanishing first and third moments. It is known that this model is marginally relevant, and moreover, it undergoes a phase transition in an intermediate disorder regime. We show that, in the critical window, the point-to-point partition functions converge to a unique limiting random measure, which we call the critical disordered pinning measure. We also obtain an analogous result for a continuous counterpart to the pinning model, which is closely related to two other models: one is a critical stochastic Volterra equation that gives rise to a rough volatility model, and the other is a critical stochastic heat equation with multiplicative noise that is white in time and delta in space.","sentences":["In this paper, we study a disordered pinning model induced by a random walk whose increments have a finite fourth moment and vanishing first and third moments.","It is known that this model is marginally relevant, and moreover, it undergoes a phase transition in an intermediate disorder regime.","We show that, in the critical window, the point-to-point partition functions converge to a unique limiting random measure, which we call the critical disordered pinning measure.","We also obtain an analogous result for a continuous counterpart to the pinning model, which is closely related to two other models: one is a critical stochastic Volterra equation that gives rise to a rough volatility model, and the other is a critical stochastic heat equation with multiplicative noise that is white in time and delta in space."],"url":"http://arxiv.org/abs/2402.17642v1","category":"math.PR"}
{"created":"2024-02-27 15:49:54","title":"Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image Modeling","abstract":"This work proposes a semantic segmentation network that produces high-quality uncertainty estimates in a single forward pass. We exploit general representations from foundation models and unlabelled datasets through a Masked Image Modeling (MIM) approach, which is robust to augmentation hyper-parameters and simpler than previous techniques. For neural networks used in safety-critical applications, bias in the training data can lead to errors; therefore it is crucial to understand a network's limitations at run time and act accordingly. To this end, we test our proposed method on a number of test domains including the SAX Segmentation benchmark, which includes labelled test data from dense urban, rural and off-road driving domains. The proposed method consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark.","sentences":["This work proposes a semantic segmentation network that produces high-quality uncertainty estimates in a single forward pass.","We exploit general representations from foundation models and unlabelled datasets through a Masked Image Modeling (MIM) approach, which is robust to augmentation hyper-parameters and simpler than previous techniques.","For neural networks used in safety-critical applications, bias in the training data can lead to errors; therefore it is crucial to understand a network's limitations at run time and act accordingly.","To this end, we test our proposed method on a number of test domains including the SAX Segmentation benchmark, which includes labelled test data from dense urban, rural and off-road driving domains.","The proposed method consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark."],"url":"http://arxiv.org/abs/2402.17622v1","category":"cs.CV"}
{"created":"2024-02-27 15:46:51","title":"On the blow-up for a Kuramoto-Velarde type equation","abstract":"It is known that the Kuramoto-Velarde equation is globally well-posed on Sobolev spaces in the case when the parameters $\\gamma_1$ and $\\gamma_2$ involved in the non-linear terms verify $ \\gamma_1=\\frac{\\gamma_1}{2}$ or $\\gamma_2=0$. In the complementary case of these parameters, the global existence or blow-up of solutions is a completely open (and hard) problem. Motivated by this fact, in this work we consider a non-local version of the Kuramoto-Velarde equation. This equation allows us to apply a Fourier-based method and, within the framework $\\gamma_2\\neq \\frac{\\gamma_1}{2}$ and $\\gamma_2\\neq 0$, we show that large values of these parameters yield a blow-up in finite time of solutions in the Sobolev norm.","sentences":["It is known that the Kuramoto-Velarde equation is globally well-posed on Sobolev spaces in the case when the parameters $\\gamma_1$ and $\\gamma_2$ involved in the non-linear terms verify $ \\gamma_1=\\frac{\\gamma_1}{2}$ or $\\gamma_2=0$. In the complementary case of these parameters, the global existence or blow-up of solutions is a completely open (and hard) problem.","Motivated by this fact, in this work we consider a non-local version of the Kuramoto-Velarde equation.","This equation allows us to apply a Fourier-based method and, within the framework $\\gamma_2\\neq \\frac{\\gamma_1}{2}$ and $\\gamma_2\\neq 0$, we show that large values of these parameters yield a blow-up in finite time of solutions in the Sobolev norm."],"url":"http://arxiv.org/abs/2402.17619v1","category":"math.AP"}
{"created":"2024-02-27 15:42:33","title":"Neural Automated Writing Evaluation with Corrective Feedback","abstract":"The utilization of technology in second language learning and teaching has become ubiquitous. For the assessment of writing specifically, automated writing evaluation (AWE) and grammatical error correction (GEC) have become immensely popular and effective methods for enhancing writing proficiency and delivering instant and individualized feedback to learners. By leveraging the power of natural language processing (NLP) and machine learning algorithms, AWE and GEC systems have been developed separately to provide language learners with automated corrective feedback and more accurate and unbiased scoring that would otherwise be subject to examiners. In this paper, we propose an integrated system for automated writing evaluation with corrective feedback as a means of bridging the gap between AWE and GEC results for second language learners. This system enables language learners to simulate the essay writing tests: a student writes and submits an essay, and the system returns the assessment of the writing along with suggested grammatical error corrections. Given that automated scoring and grammatical correction are more efficient and cost-effective than human grading, this integrated system would also alleviate the burden of manually correcting innumerable essays.","sentences":["The utilization of technology in second language learning and teaching has become ubiquitous.","For the assessment of writing specifically, automated writing evaluation (AWE) and grammatical error correction (GEC) have become immensely popular and effective methods for enhancing writing proficiency and delivering instant and individualized feedback to learners.","By leveraging the power of natural language processing (NLP) and machine learning algorithms, AWE and GEC systems have been developed separately to provide language learners with automated corrective feedback and more accurate and unbiased scoring that would otherwise be subject to examiners.","In this paper, we propose an integrated system for automated writing evaluation with corrective feedback as a means of bridging the gap between AWE and GEC results for second language learners.","This system enables language learners to simulate the essay writing tests: a student writes and submits an essay, and the system returns the assessment of the writing along with suggested grammatical error corrections.","Given that automated scoring and grammatical correction are more efficient and cost-effective than human grading, this integrated system would also alleviate the burden of manually correcting innumerable essays."],"url":"http://arxiv.org/abs/2402.17613v1","category":"cs.CL"}
{"created":"2024-02-27 15:31:00","title":"Equivariant ideals of polynomials","abstract":"We study existence and computability of finite bases for ideals of polynomials over infinitely many variables. In our setting, variables come from a countable logical structure A, and embeddings from A to A act on polynomials by renaming variables. First, we give a sufficient and necessary condition for A to guarantee the following generalisation of Hilbert's Basis Theorem: every polynomial ideal which is equivariant, i.e. invariant under renaming of variables, is finitely generated. Second, we develop an extension of classical Buchberger's algorithm to compute a Gr\\\"obner basis of a given equivariant ideal. This implies decidability of the membership problem for equivariant ideals. Finally, we sketch upon various applications of these results to register automata, Petri nets with data, orbit-finitely generated vector spaces, and orbit-finite systems of linear equations.","sentences":["We study existence and computability of finite bases for ideals of polynomials over infinitely many variables.","In our setting, variables come from a countable logical structure A, and embeddings from A to A act on polynomials by renaming variables.","First, we give a sufficient and necessary condition for A to guarantee the following generalisation of Hilbert's Basis Theorem: every polynomial ideal which is equivariant, i.e. invariant under renaming of variables, is finitely generated.","Second, we develop an extension of classical Buchberger's algorithm to compute a Gr\\\"obner basis of a given equivariant ideal.","This implies decidability of the membership problem for equivariant ideals.","Finally, we sketch upon various applications of these results to register automata, Petri nets with data, orbit-finitely generated vector spaces, and orbit-finite systems of linear equations."],"url":"http://arxiv.org/abs/2402.17604v1","category":"cs.LO"}
{"created":"2024-02-27 15:30:34","title":"Classification Theorem For Positive Critical Points Of Sobolev Trace Inequality","abstract":"We consider the Euler-Lagrange equation of Sobolev trace inequality and prove several classification results. Exploiting the moving sphere method, it has been shown, when $p=2$, positive solutions of Euler-Lagrange equation of Sobolev trace inequality are classified. Since the moving sphere method strongly relies on the symmetries of the equation, in this paper we use asymptotic estimates and two important integral identities to classify positive solutions of Euler-Langrange equation of Sobolev trace inequality under finite energy when $1<p<n$.","sentences":["We consider the Euler-Lagrange equation of Sobolev trace inequality and prove several classification results.","Exploiting the moving sphere method, it has been shown, when $p=2$, positive solutions of Euler-Lagrange equation of Sobolev trace inequality are classified.","Since the moving sphere method strongly relies on the symmetries of the equation, in this paper we use asymptotic estimates and two important integral identities to classify positive solutions of Euler-Langrange equation of Sobolev trace inequality under finite energy when $1<p<n$."],"url":"http://arxiv.org/abs/2402.17602v1","category":"math.AP"}
{"created":"2024-02-27 15:30:01","title":"Advancing sleep detection by modelling weak label sets: A novel weakly supervised learning approach","abstract":"Understanding sleep and activity patterns plays a crucial role in physical and mental health. This study introduces a novel approach for sleep detection using weakly supervised learning for scenarios where reliable ground truth labels are unavailable. The proposed method relies on a set of weak labels, derived from the predictions generated by conventional sleep detection algorithms. Introducing a novel approach, we suggest a novel generalised non-linear statistical model in which the number of weak sleep labels is modelled as outcome of a binomial distribution. The probability of sleep in the binomial distribution is linked to the outcomes of neural networks trained to detect sleep based on actigraphy. We show that maximizing the likelihood function of the model, is equivalent to minimizing the soft cross-entropy loss. Additionally, we explored the use of the Brier score as a loss function for weak labels. The efficacy of the suggested modelling framework was demonstrated using the Multi-Ethnic Study of Atherosclerosis dataset. A \\gls{lstm} trained on the soft cross-entropy outperformed conventional sleep detection algorithms, other neural network architectures and loss functions in accuracy and model calibration. This research not only advances sleep detection techniques in scenarios where ground truth data is scarce but also contributes to the broader field of weakly supervised learning by introducing innovative approach in modelling sets of weak labels.","sentences":["Understanding sleep and activity patterns plays a crucial role in physical and mental health.","This study introduces a novel approach for sleep detection using weakly supervised learning for scenarios where reliable ground truth labels are unavailable.","The proposed method relies on a set of weak labels, derived from the predictions generated by conventional sleep detection algorithms.","Introducing a novel approach, we suggest a novel generalised non-linear statistical model in which the number of weak sleep labels is modelled as outcome of a binomial distribution.","The probability of sleep in the binomial distribution is linked to the outcomes of neural networks trained to detect sleep based on actigraphy.","We show that maximizing the likelihood function of the model, is equivalent to minimizing the soft cross-entropy loss.","Additionally, we explored the use of the Brier score as a loss function for weak labels.","The efficacy of the suggested modelling framework was demonstrated using the Multi-Ethnic Study of Atherosclerosis dataset.","A \\gls{lstm} trained on the soft cross-entropy outperformed conventional sleep detection algorithms, other neural network architectures and loss functions in accuracy and model calibration.","This research not only advances sleep detection techniques in scenarios where ground truth data is scarce but also contributes to the broader field of weakly supervised learning by introducing innovative approach in modelling sets of weak labels."],"url":"http://arxiv.org/abs/2402.17601v1","category":"cs.LG"}
{"created":"2024-02-27 15:08:57","title":"Sparse Variational Contaminated Noise Gaussian Process Regression for Forecasting Geomagnetic Perturbations","abstract":"Gaussian Processes (GP) have become popular machine learning methods for kernel based learning on datasets with complicated covariance structures. In this paper, we present a novel extension to the GP framework using a contaminated normal likelihood function to better account for heteroscedastic variance and outlier noise. We propose a scalable inference algorithm based on the Sparse Variational Gaussian Process (SVGP) method for fitting sparse Gaussian process regression models with contaminated normal noise on large datasets. We examine an application to geomagnetic ground perturbations, where the state-of-art prediction model is based on neural networks. We show that our approach yields shorter predictions intervals for similar coverage and accuracy when compared to an artificial dense neural network baseline.","sentences":["Gaussian Processes (GP) have become popular machine learning methods for kernel based learning on datasets with complicated covariance structures.","In this paper, we present a novel extension to the GP framework using a contaminated normal likelihood function to better account for heteroscedastic variance and outlier noise.","We propose a scalable inference algorithm based on the Sparse Variational Gaussian Process (SVGP) method for fitting sparse Gaussian process regression models with contaminated normal noise on large datasets.","We examine an application to geomagnetic ground perturbations, where the state-of-art prediction model is based on neural networks.","We show that our approach yields shorter predictions intervals for similar coverage and accuracy when compared to an artificial dense neural network baseline."],"url":"http://arxiv.org/abs/2402.17570v2","category":"cs.LG"}
{"created":"2024-02-27 15:06:45","title":"Quasi-Classical Gluon Fields and Low's Soft Theorem at Small $x$","abstract":"In the high energy limit, soft gluons can be approximately described by quasi-classical gluon fields. It is well-known that the gluon field is a pure gauge field on the transverse plane at eikonal order. We derived the complete next-to-eikonal order solutions of the classical Yang-Mills equations for soft gluons in the dense nuclear regime. Utilizing these solutions, it is shown that Low's soft theorem at small $x$ can be obtained by considering off-diagonal matrix elements of quasi-classical chromoelectric field between single gluon states in the dilute regime. Furthermore, we extend Low's soft theorem at small $x$ to incorporate the effects of gluon saturation in the dense regime.","sentences":["In the high energy limit, soft gluons can be approximately described by quasi-classical gluon fields.","It is well-known that the gluon field is a pure gauge field on the transverse plane at eikonal order.","We derived the complete next-to-eikonal order solutions of the classical Yang-Mills equations for soft gluons in the dense nuclear regime.","Utilizing these solutions, it is shown that Low's soft theorem at small $x$ can be obtained by considering off-diagonal matrix elements of quasi-classical chromoelectric field between single gluon states in the dilute regime.","Furthermore, we extend Low's soft theorem at small $x$ to incorporate the effects of gluon saturation in the dense regime."],"url":"http://arxiv.org/abs/2402.17568v1","category":"hep-ph"}
{"created":"2024-02-27 15:06:26","title":"Third order estimates and the regularity of the stress field for solutions to $p$-Laplace equations","abstract":"We consider solutions to   $$ - \\Delta_{p} u = f(x) \\quad \\text{in } \\Omega\\, ,$$   when $p$ approaches the semilinear limiting case $p=2$ and we get third order estimates. As a consequence we deduce improved regularity properties of the stress field.","sentences":["We consider solutions to   $$ - \\Delta_{p} u = f(x) \\quad \\text{in } \\Omega\\, ,$$   when $p$ approaches the semilinear limiting case $p=2$","and we get third order estimates.","As a consequence we deduce improved regularity properties of the stress field."],"url":"http://arxiv.org/abs/2402.17566v1","category":"math.AP"}
{"created":"2024-02-27 15:05:57","title":"Willmore-type variational problem for foliated hypersurfaces","abstract":"We study new Willmore-type variational problem for a hypersurface $M$ in $\\mathbb{R}^{n+1}$ equipped with an $s$-dimensional foliation ${\\cal F}$. Its general version is the Reilly-type functional $WF_{n,s}=\\int_M F(\\sigma^{\\cal F}_1,\\ldots,\\sigma^{\\cal F}_s)\\,{\\rm d}V$, where $\\sigma^{\\cal F}_i$ are elementary symmetric functions of the eigenvalues of the second fundamental form restricted on the leaves of $\\cal F$. The first and second variations of such functionals are calculated, conformal invariance of some of $WF_{n,s}$ is also shown. The Euler-Lagrange equation for a critical hypersurface with a transversally harmonic (e.g., Riemannian) foliation $\\cal F$ is found and examples with $s\\le2$ and $s=n$ are considered. Critical hypersurfaces of revolution are found, and it is shown that they are a local minimum for special variations.","sentences":["We study new Willmore-type variational problem for a hypersurface $M$ in $\\mathbb{R}^{n+1}$ equipped with an $s$-dimensional foliation ${\\cal F}$.","Its general version is the Reilly-type functional $WF_{n,s}=\\int_M F(\\sigma^{\\cal F}_1,\\ldots,\\sigma^{\\cal F}_s)\\,{\\rm d}V$, where $\\sigma^{\\cal F}_i$ are elementary symmetric functions of the eigenvalues of the second fundamental form restricted on the leaves of $\\cal F$. The first and second variations of such functionals are calculated, conformal invariance of some of $WF_{n,s}$ is also shown.","The Euler-Lagrange equation for a critical hypersurface with a transversally harmonic (e.g., Riemannian) foliation $\\cal F$ is found and examples with $s\\le2$ and $s=n$ are considered.","Critical hypersurfaces of revolution are found, and it is shown that they are a local minimum for special variations."],"url":"http://arxiv.org/abs/2402.17565v1","category":"math.DG"}
{"created":"2024-02-27 14:58:49","title":"On the Weierstra\u00df form of infinite dimensional differential algebraic equations","abstract":"The solvability for infinite dimensional differential algebraic equations possessing a resolvent index and a Weierstra{\\ss} form is studied. In particular, the concept of integrated semigroups is used to determine a subset on which solutions exist and are unique. This information is later used for a important class of systems, namely, port-Hamiltonian differential algebraic equations.","sentences":["The solvability for infinite dimensional differential algebraic equations possessing a resolvent index and a Weierstra{\\ss} form is studied.","In particular, the concept of integrated semigroups is used to determine a subset on which solutions exist and are unique.","This information is later used for a important class of systems, namely, port-Hamiltonian differential algebraic equations."],"url":"http://arxiv.org/abs/2402.17560v1","category":"math.AP"}
{"created":"2024-02-27 14:43:55","title":"FlipHash: A Constant-Time Consistent Range-Hashing Algorithm","abstract":"Consistent range-hashing is a technique used in distributed systems, either directly or as a subroutine for consistent hashing, commonly to realize an even and stable data distribution over a variable number of resources. We introduce FlipHash, a consistent range-hashing algorithm with constant time complexity and low memory requirements. Like Jump Consistent Hash, FlipHash is intended for applications where resources can be indexed sequentially. Under this condition, it ensures that keys are hashed evenly across resources and that changing the number of resources only causes keys to be remapped from a removed resource or to an added one, but never shuffled across persisted ones. FlipHash differentiates itself with its low computational cost, achieving constant-time complexity. We show that FlipHash beats Jump Consistent Hash's cost, which is logarithmic in the number of resources, both theoretically and in experiments over practical settings.","sentences":["Consistent range-hashing is a technique used in distributed systems, either directly or as a subroutine for consistent hashing, commonly to realize an even and stable data distribution over a variable number of resources.","We introduce FlipHash, a consistent range-hashing algorithm with constant time complexity and low memory requirements.","Like Jump Consistent Hash, FlipHash is intended for applications where resources can be indexed sequentially.","Under this condition, it ensures that keys are hashed evenly across resources and that changing the number of resources only causes keys to be remapped from a removed resource or to an added one, but never shuffled across persisted ones.","FlipHash differentiates itself with its low computational cost, achieving constant-time complexity.","We show that FlipHash beats Jump Consistent Hash's cost, which is logarithmic in the number of resources, both theoretically and in experiments over practical settings."],"url":"http://arxiv.org/abs/2402.17549v1","category":"cs.DS"}
{"created":"2024-02-27 14:41:09","title":"On geodesic orbit nilmanifolds","abstract":"The paper is devoted to the study of geodesic orbit Riemannian metrics on nilpotent Lie groups. The main result is the construction of continuous families of pairwise non-isomorphic connected and simply connected nilpotent Lie groups, every of which admits geodesic orbit metrics. The minimum dimension of groups in the constructed families is $10$.","sentences":["The paper is devoted to the study of geodesic orbit Riemannian metrics on nilpotent Lie groups.","The main result is the construction of continuous families of pairwise non-isomorphic connected and simply connected nilpotent Lie groups, every of which admits geodesic orbit metrics.","The minimum dimension of groups in the constructed families is $10$."],"url":"http://arxiv.org/abs/2402.17548v1","category":"math.DG"}
{"created":"2024-02-27 14:36:49","title":"Accuracy of the Gross-Pitaevskii Equation in a Double-Well Potential","abstract":"The Gross-Pitaevskii equation (GPE) in a double well potential produces solutions that break the symmetry of the underlying non-interacting Hamiltonian, i.e., asymmetric solutions. The GPE is derived from the more general second-quantized Fock Schroedinger equation (FSE). We investigate whether such solutions appear in the more general case or are artifacts of the GPE. We use two-mode analyses for a variational treatment of the GPE and to treat the Fock equation. An exact diagonalization of the FSE in dual-condensates yields degenerate ground states that are very accurately fitted by phase-state representations of the degenerate asymmetric states found in the GPE. The superposition of degenerate asymmetrical states forms a cat state. An alternative form of cat state results from a change of the two-mode basis set.","sentences":["The Gross-Pitaevskii equation (GPE) in a double well potential produces solutions that break the symmetry of the underlying non-interacting Hamiltonian, i.e., asymmetric solutions.","The GPE is derived from the more general second-quantized Fock Schroedinger equation (FSE).","We investigate whether such solutions appear in the more general case or are artifacts of the GPE.","We use two-mode analyses for a variational treatment of the GPE and to treat the Fock equation.","An exact diagonalization of the FSE in dual-condensates yields degenerate ground states that are very accurately fitted by phase-state representations of the degenerate asymmetric states found in the GPE.","The superposition of degenerate asymmetrical states forms a cat state.","An alternative form of cat state results from a change of the two-mode basis set."],"url":"http://arxiv.org/abs/2402.17545v1","category":"quant-ph"}
{"created":"2024-02-27 14:30:12","title":"Computational imaging of small-amplitude biperiodic surfaces with negative index material","abstract":"This paper presents an innovative approach to computational acoustic imaging of biperiodic surfaces, exploiting the capabilities of an acoustic superlens to overcome the diffraction limit. We address the challenge of imaging physical entities in complex environments by considering the partial differential equations that govern the physics and solving the corresponding inverse problem. We focus on imaging infinite rough surfaces, specifically 2D diffraction gratings, and propose a method that leverages the transformed field expansion (TFE). We derive a reconstruction formula connecting the Fourier coefficients of the surface and the measured field, demonstrating the potential for unlimited resolution under ideal conditions. We also introduce an approximate discrepancy principle to determine the cut-off frequency for the truncated Fourier series expansion in surface profile reconstruction. Furthermore, we elucidate the resolution enhancement effect of the superlens by deriving the discrete Fourier transform of white Gaussian noise. Our numerical experiments confirm the effectiveness of the proposed method, demonstrating high subwavelength resolution even under slightly non-ideal conditions. This study extends the current understanding of superlens-based imaging and provides a robust framework for future research.","sentences":["This paper presents an innovative approach to computational acoustic imaging of biperiodic surfaces, exploiting the capabilities of an acoustic superlens to overcome the diffraction limit.","We address the challenge of imaging physical entities in complex environments by considering the partial differential equations that govern the physics and solving the corresponding inverse problem.","We focus on imaging infinite rough surfaces, specifically 2D diffraction gratings, and propose a method that leverages the transformed field expansion (TFE).","We derive a reconstruction formula connecting the Fourier coefficients of the surface and the measured field, demonstrating the potential for unlimited resolution under ideal conditions.","We also introduce an approximate discrepancy principle to determine the cut-off frequency for the truncated Fourier series expansion in surface profile reconstruction.","Furthermore, we elucidate the resolution enhancement effect of the superlens by deriving the discrete Fourier transform of white Gaussian noise.","Our numerical experiments confirm the effectiveness of the proposed method, demonstrating high subwavelength resolution even under slightly non-ideal conditions.","This study extends the current understanding of superlens-based imaging and provides a robust framework for future research."],"url":"http://arxiv.org/abs/2402.17543v1","category":"math.AP"}
{"created":"2024-02-27 14:25:47","title":"Optimal Stopping of BSDEs with Constrained Jumps and Related Double Obstacle PDEs","abstract":"We consider partial differential equations (PDEs) characterized by an upper barrier that depends on the solution itself and a fixed lower barrier, while accommodating a non-local driver. First, we show a Feynman-Kac representation for the PDE when the driver is local. Specifically, we relate the non-linear Snell envelope for an optimal stopping problem, where the underlying process is the first component in the solution to a stopped backward stochastic differential equation (BSDE) with jumps and a constraint on the jumps process, to a viscosity solution for the PDE. Leveraging this Feynman-Kac representation, we subsequently prove existence and uniqueness of viscosity solutions in the non-local setting by employing a contraction argument. In addition, the contraction argument yields existence of a new type of non-linear Snell envelope and extends the theory of probabilistic representation for PDEs.","sentences":["We consider partial differential equations (PDEs) characterized by an upper barrier that depends on the solution itself and a fixed lower barrier, while accommodating a non-local driver.","First, we show a Feynman-Kac representation for the PDE when the driver is local.","Specifically, we relate the non-linear Snell envelope for an optimal stopping problem, where the underlying process is the first component in the solution to a stopped backward stochastic differential equation (BSDE) with jumps and a constraint on the jumps process, to a viscosity solution for the PDE.","Leveraging this Feynman-Kac representation, we subsequently prove existence and uniqueness of viscosity solutions in the non-local setting by employing a contraction argument.","In addition, the contraction argument yields existence of a new type of non-linear Snell envelope and extends the theory of probabilistic representation for PDEs."],"url":"http://arxiv.org/abs/2402.17541v1","category":"math.PR"}
{"created":"2024-02-27 14:22:41","title":"Electrically driven cascaded photon-emission in a single molecule","abstract":"Controlling electrically-stimulated quantum light sources (QLS) is key for developing integrated and low-scale quantum devices. The mechanisms leading to quantum emission are complex, as a large number of electronic states of the system impacts the emission dynamics. Here, we use a scanning tunneling microscope (STM) to excite a model QLS, namely a single molecule. The luminescence spectra reveal two lines, associated to the emission of the neutral and positively charged molecule, both exhibiting single-photon source behavior. In addition, we find a correlation between the charged and neutral molecule's emission, the signature of a photon cascade. By adjusting the charging/discharging rate, we can control these emission statistics. This generic strategy is further established by a rate equation model revealing the complex internal dynamics of the molecular junction.","sentences":["Controlling electrically-stimulated quantum light sources (QLS) is key for developing integrated and low-scale quantum devices.","The mechanisms leading to quantum emission are complex, as a large number of electronic states of the system impacts the emission dynamics.","Here, we use a scanning tunneling microscope (STM) to excite a model QLS, namely a single molecule.","The luminescence spectra reveal two lines, associated to the emission of the neutral and positively charged molecule, both exhibiting single-photon source behavior.","In addition, we find a correlation between the charged and neutral molecule's emission, the signature of a photon cascade.","By adjusting the charging/discharging rate, we can control these emission statistics.","This generic strategy is further established by a rate equation model revealing the complex internal dynamics of the molecular junction."],"url":"http://arxiv.org/abs/2402.17536v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-27 14:21:56","title":"Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control","abstract":"Learned sparse retrieval (LSR) is a family of neural methods that encode queries and documents into sparse lexical vectors that can be indexed and retrieved efficiently with an inverted index. We explore the application of LSR to the multi-modal domain, with a focus on text-image retrieval. While LSR has seen success in text retrieval, its application in multimodal retrieval remains underexplored. Current approaches like LexLIP and STAIR require complex multi-step training on massive datasets. Our proposed approach efficiently transforms dense vectors from a frozen dense model into sparse lexical vectors. We address issues of high dimension co-activation and semantic deviation through a new training algorithm, using Bernoulli random variables to control query expansion. Experiments with two dense models (BLIP, ALBEF) and two datasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively reduces co-activation and semantic deviation. Our best-performing sparsified model outperforms state-of-the-art text-image LSR models with a shorter training time and lower GPU memory requirements. Our approach offers an effective solution for training LSR retrieval models in multimodal settings. Our code and model checkpoints are available at github.com/thongnt99/lsr-multimodal","sentences":["Learned sparse retrieval (LSR) is a family of neural methods that encode queries and documents into sparse lexical vectors that can be indexed and retrieved efficiently with an inverted index.","We explore the application of LSR to the multi-modal domain, with a focus on text-image retrieval.","While LSR has seen success in text retrieval, its application in multimodal retrieval remains underexplored.","Current approaches like LexLIP and STAIR require complex multi-step training on massive datasets.","Our proposed approach efficiently transforms dense vectors from a frozen dense model into sparse lexical vectors.","We address issues of high dimension co-activation and semantic deviation through a new training algorithm, using Bernoulli random variables to control query expansion.","Experiments with two dense models (BLIP, ALBEF) and two datasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively reduces co-activation and semantic deviation.","Our best-performing sparsified model outperforms state-of-the-art text-image LSR models with a shorter training time and lower GPU memory requirements.","Our approach offers an effective solution for training LSR retrieval models in multimodal settings.","Our code and model checkpoints are available at github.com/thongnt99/lsr-multimodal"],"url":"http://arxiv.org/abs/2402.17535v1","category":"cs.IR"}
{"created":"2024-02-27 14:21:32","title":"The ghost-gluon vertex in the presence of the Gribov horizon: general kinematics","abstract":"Correlation functions are important probes for the behavior of quantum field theories. Already at tree-level, the Refined Gribov Zwanziger (RGZ) effective action for Yang-Mills theories provides a good approximation for the gluon propagator, as compared to that calculated by nonperturbative methods such as Lattice Field Theory and Dyson-Schwinger Equations. However, the study of higher correlation functions of the RGZ theory is still at its beginning. In this work we evaluate the ghost-antighost-gluon vertex function in Landau gauge at one-loop level, in $d=4$ space-time dimensions for the gauge groups SU(2) and SU(3). More precisely, we extend the analysis conducted in [1] for the soft-gluon limit to an arbitrary kinematic configuration. We introduce renormalization group effects by means of a toy model for the running coupling and investigate the impact of such a model in the ultraviolet tails of our results. We find that RGZ results match fairly closely those from lattice simulations, Schwinger-Dyson equations and the Curci-Ferrari model for three different kinematic configurations. This is compatible with RGZ being a feasible theory for the strong interaction in the infrared regime.","sentences":["Correlation functions are important probes for the behavior of quantum field theories.","Already at tree-level, the Refined Gribov Zwanziger (RGZ) effective action for Yang-Mills theories provides a good approximation for the gluon propagator, as compared to that calculated by nonperturbative methods such as Lattice Field Theory and Dyson-Schwinger Equations.","However, the study of higher correlation functions of the RGZ theory is still at its beginning.","In this work we evaluate the ghost-antighost-gluon vertex function in Landau gauge at one-loop level, in $d=4$ space-time dimensions for the gauge groups SU(2) and SU(3).","More precisely, we extend the analysis conducted in [1] for the soft-gluon limit to an arbitrary kinematic configuration.","We introduce renormalization group effects by means of a toy model for the running coupling and investigate the impact of such a model in the ultraviolet tails of our results.","We find that RGZ results match fairly closely those from lattice simulations, Schwinger-Dyson equations and the Curci-Ferrari model for three different kinematic configurations.","This is compatible with RGZ being a feasible theory for the strong interaction in the infrared regime."],"url":"http://arxiv.org/abs/2402.17534v1","category":"hep-ph"}
{"created":"2024-02-27 14:14:06","title":"The reverse Burnett conjecture for null dusts","abstract":"Given a generic solution $\\mathbf{g}_0$ of the Einstein-null dusts system without restriction on the number of dusts, we construct families of solutions $(\\mathbf{g}_\\lambda)_{\\lambda\\in(0,1]}$ of the Einstein vacuum equations such that $\\mathbf{g}_\\lambda-\\mathbf{g}_0$ and $\\partial(\\mathbf{g}_\\lambda-\\mathbf{g}_0)$ converges respectively strongly and weakly to 0 when $\\lambda\\to0$. Our construction, based on a multiphase geometric optics ansatz, thus extends the validity of the reverse Burnett conjecture without symmetry to a large class of massless kinetic spacetimes. In order to deal with the finite but arbitrary number of direction of oscillations we work in a generalised wave gauge and control precisely the self-interaction of each wave but also the interaction of waves propagating in different null directions, relying crucially on the non-linear structure of the Einstein vacuum equations. We also provide the construction of oscillating initial data solving the vacuum constraint equations and which are consistent with the spacetime ansatz.","sentences":["Given a generic solution $\\mathbf{g}_0$ of the Einstein-null dusts system without restriction on the number of dusts, we construct families of solutions $(\\mathbf{g}_\\lambda)_{\\lambda\\in(0,1]}$ of the Einstein vacuum equations such that $\\mathbf{g}_\\lambda-\\mathbf{g}_0$ and $\\partial(\\mathbf{g}_\\lambda-\\mathbf{g}_0)$ converges respectively strongly and weakly to 0 when $\\lambda\\to0$. Our construction, based on a multiphase geometric optics ansatz, thus extends the validity of the reverse Burnett conjecture without symmetry to a large class of massless kinetic spacetimes.","In order to deal with the finite but arbitrary number of direction of oscillations we work in a generalised wave gauge and control precisely the self-interaction of each wave but also the interaction of waves propagating in different null directions, relying crucially on the non-linear structure of the Einstein vacuum equations.","We also provide the construction of oscillating initial data solving the vacuum constraint equations and which are consistent with the spacetime ansatz."],"url":"http://arxiv.org/abs/2402.17530v1","category":"math.AP"}
{"created":"2024-02-27 14:13:41","title":"Evaluation of block encoding for sparse matrix inversion using QSVT","abstract":"Three block encoding methods are evaluated for solving linear systems of equations using QSVT (Quantum Singular Value Transformation). These are ARCSIN, FABLE and PREPARE-SELECT. The performance of the encoders is evaluated using a suite of 30 test cases including 1D, 2D and 3D Laplacians and 2D CFD matrices. A subset of cases is used to characterise how the degree of the polynomial approximation to $1/x$ influences the performance of QSVT. The results are used to guide the evaluation of QSVT as the linear solver in hybrid non-linear pressure correction and coupled implicit CFD solvers. The performance of QSVT is shown to be resilient to polynomial approximation errors. For both CFD solvers, error tolerances of $10^{-2}$ are more than sufficient in most cases and in some cases $10^{-1}$ is sufficient. The pressure correction solver allows subnormalised condition numbers, $\\kappa_s$, as low as half the theoretical values to be used, reducing the number of phase factors needed. PREPARE-SELECT encoding relies on a unitary decomposition, e.g. Pauli strings, that has significant classical preprocessing costs. Both ARCSIN and FABLE have much lower costs, particularly for coupled solvers. However, their subnormalisation factors, which are based on the rank of the matrix, can be many times higher than PREPARE-SELECT leading to more phase factors being needed. For both the pressure correction and coupled CFD calculations, QSVT is more stable than previous HHL results due to the polynomial approximation errors only affecting long wavelength CFD errors. Given that lowering $\\kappa_s$ increases the success probability, optimising the performance of QSVT within a CFD code is a function of the number QSVT phase factors, the number of non-linear iterations and the number of shots. Although phase factor files can be reused, the time taken to generate them impedes scaling QSVT to larger test cases.","sentences":["Three block encoding methods are evaluated for solving linear systems of equations using QSVT (Quantum Singular Value Transformation).","These are ARCSIN, FABLE and PREPARE-SELECT.","The performance of the encoders is evaluated using a suite of 30 test cases including 1D, 2D and 3D Laplacians and 2D CFD matrices.","A subset of cases is used to characterise how the degree of the polynomial approximation to $1/x$ influences the performance of QSVT.","The results are used to guide the evaluation of QSVT as the linear solver in hybrid non-linear pressure correction and coupled implicit CFD solvers.","The performance of QSVT is shown to be resilient to polynomial approximation errors.","For both CFD solvers, error tolerances of $10^{-2}$ are more than sufficient in most cases and in some cases $10^{-1}$ is sufficient.","The pressure correction solver allows subnormalised condition numbers, $\\kappa_s$, as low as half the theoretical values to be used, reducing the number of phase factors needed.","PREPARE-SELECT encoding relies on a unitary decomposition, e.g. Pauli strings, that has significant classical preprocessing costs.","Both ARCSIN and FABLE have much lower costs, particularly for coupled solvers.","However, their subnormalisation factors, which are based on the rank of the matrix, can be many times higher than PREPARE-SELECT leading to more phase factors being needed.","For both the pressure correction and coupled CFD calculations, QSVT is more stable than previous HHL results due to the polynomial approximation errors only affecting long wavelength CFD errors.","Given that lowering $\\kappa_s$ increases the success probability, optimising the performance of QSVT within a CFD code is a function of the number QSVT phase factors, the number of non-linear iterations and the number of shots.","Although phase factor files can be reused, the time taken to generate them impedes scaling QSVT to larger test cases."],"url":"http://arxiv.org/abs/2402.17529v1","category":"quant-ph"}
{"created":"2024-02-27 14:03:20","title":"Chromatic defect, Wood's theorem, and higher real $K$-theories","abstract":"Let $\\mathrm{X(n)}$ be Ravenel's Thom spectrum over $\\Omega \\mathrm{SU}(n)$. We say a spectrum $E$ has chromatic defect $n$ if $n$ is the smallest positive integer such that $E\\otimes \\mathrm{X(n)}$ is complex orientable. We compute the chromatic defect of various examples of interest: finite spectra, the Real Johnson--Wilson theories $\\mathrm{ER(n)}$, the fixed points $\\mathrm{EO}_n(G)$ of Morava $E$-theories with respect to a finite subgroup $G$ of the Morava stabilizer group, and the connective image of $J$ spectrum j. Having finite chromatic defect is closely related to the existence of analogues of the classical Wood splitting $\\mathrm{ko}\\otimes C(\\eta)\\simeq \\mathrm{ku}$. We show that such splittings exist in quite a wide generality for fp spectra $E$. When $E$ participates in such a splitting, $E$ admits a $\\mathbb Z$-indexed Adams--Novikov tower, which may be used to deduce differentials in the Adams--Novikov spectral sequence of $E$.","sentences":["Let $\\mathrm{X(n)}$ be Ravenel's Thom spectrum over $\\Omega","\\mathrm{SU}(n)$.","We say a spectrum $E$ has chromatic defect $n$ if $n$ is the smallest positive integer such that $E\\otimes \\mathrm{X(n)}$ is complex orientable.","We compute the chromatic defect of various examples of interest: finite spectra, the Real Johnson--Wilson theories $\\mathrm{ER(n)}$, the fixed points $\\mathrm{EO}_n(G)$ of Morava $E$-theories with respect to a finite subgroup $G$ of the Morava stabilizer group, and the connective image of $J$ spectrum j.","Having finite chromatic defect is closely related to the existence of analogues of the classical Wood splitting $\\mathrm{ko}\\otimes C(\\eta)\\simeq","\\mathrm{ku}$.","We show that such splittings exist in quite a wide generality for fp spectra $E$. When $E$ participates in such a splitting, $E$ admits a $\\mathbb Z$-indexed Adams--Novikov tower, which may be used to deduce differentials in the Adams--Novikov spectral sequence of $E$."],"url":"http://arxiv.org/abs/2402.17519v1","category":"math.AT"}
{"created":"2024-02-27 14:01:21","title":"The Strong CP Problem in the Quantum Rotor","abstract":"Recent studies have claimed that the strong CP problem does not occur in QCD, proposing a new order of limits in volume and topological sectors when studying observables on the lattice. In order to shed light on this issue, we study the effect of the topological $\\theta$-term on a simple quantum mechanical rotor that allows a lattice description. The topological susceptibility and the $\\theta$-dependence of the energy spectrum are both computed using local lattice correlation functions. The sign problem is overcome by considering Taylor expansions in $\\theta$ exploiting automatic differentiation methods for Monte Carlo processes. Our findings confirm the conventional wisdom on the strong CP problem.","sentences":["Recent studies have claimed that the strong CP problem does not occur in QCD, proposing a new order of limits in volume and topological sectors when studying observables on the lattice.","In order to shed light on this issue, we study the effect of the topological $\\theta$-term on a simple quantum mechanical rotor that allows a lattice description.","The topological susceptibility and the $\\theta$-dependence of the energy spectrum are both computed using local lattice correlation functions.","The sign problem is overcome by considering Taylor expansions in $\\theta$ exploiting automatic differentiation methods for Monte Carlo processes.","Our findings confirm the conventional wisdom on the strong CP problem."],"url":"http://arxiv.org/abs/2402.17518v1","category":"hep-lat"}
{"created":"2024-02-27 18:56:19","title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","abstract":"Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.","sentences":["Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs).","In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}.","It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption.","More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective.","Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs."],"url":"http://arxiv.org/abs/2402.17764v1","category":"cs.CL"}
{"created":"2024-02-27 18:55:17","title":"Massive Activations in Large Language Models","abstract":"We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.","sentences":["We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger).","We call them massive activations.","First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations.","Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs.","Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output.","Last, we also study massive activations in Vision Transformers."],"url":"http://arxiv.org/abs/2402.17762v1","category":"cs.CL"}
{"created":"2024-02-27 18:51:52","title":"ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily Living","abstract":"Hand-Object Interactions (HOIs) are conditioned on spatial and temporal contexts like surrounding objects, pre- vious actions, and future intents (for example, grasping and handover actions vary greatly based on objects proximity and trajectory obstruction). However, existing datasets for 4D HOI (3D HOI over time) are limited to one subject inter- acting with one object only. This restricts the generalization of learning-based HOI methods trained on those datasets. We introduce ADL4D, a dataset of up to two subjects inter- acting with different sets of objects performing Activities of Daily Living (ADL) like breakfast or lunch preparation ac- tivities. The transition between multiple objects to complete a certain task over time introduces a unique context lacking in existing datasets. Our dataset consists of 75 sequences with a total of 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained action annotations. We develop an automatic system for multi-view multi-hand 3D pose an- notation capable of tracking hand poses over time. We inte- grate and test it against publicly available datasets. Finally, we evaluate our dataset on the tasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS).","sentences":["Hand-Object Interactions (HOIs) are conditioned on spatial and temporal contexts like surrounding objects, pre- vious actions, and future intents (for example, grasping and handover actions vary greatly based on objects proximity and trajectory obstruction).","However, existing datasets for 4D HOI (3D HOI over time) are limited to one subject inter- acting with one object only.","This restricts the generalization of learning-based HOI methods trained on those datasets.","We introduce ADL4D, a dataset of up to two subjects inter- acting with different sets of objects performing Activities of Daily Living (ADL) like breakfast or lunch preparation ac- tivities.","The transition between multiple objects to complete a certain task over time introduces a unique context lacking in existing datasets.","Our dataset consists of 75 sequences with a total of 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained action annotations.","We develop an automatic system for multi-view multi-hand 3D pose an- notation capable of tracking hand poses over time.","We inte- grate and test it against publicly available datasets.","Finally, we evaluate our dataset on the tasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS)."],"url":"http://arxiv.org/abs/2402.17758v1","category":"cs.CV"}
{"created":"2024-02-27 18:12:22","title":"Automated Scheduling of Doppler Exoplanet Observations at Keck Observatory","abstract":"Precise Doppler studies of extrasolar planets require fine-grained control of observational cadence, i.e. the timing of and spacing between observations. We present a novel framework for scheduling a set of Doppler campaigns with different cadence requirements at the W. M. Keck Observatory (WMKO). For a set of observing programs and allocated nights on an instrument, our software optimizes the timing and ordering of ~1000 observations within a given observing semester. We achieve a near-optimal solution in real-time using a hierarchical Integer Linear Programming (ILP) framework. Our scheduling formulation optimizes over the roughly 10^3000 possible orderings. A top level optimization finds the most regular sequence of allocated nights by which to observe each host star in the request catalog based on a frequency specified in the request. A second optimization scheme minimizes the slews and downtime of the instrument. We have assessed our algorithms performance with simulated data and with the real suite of Doppler observations of the California Planet Search in 2023.","sentences":["Precise Doppler studies of extrasolar planets require fine-grained control of observational cadence, i.e. the timing of and spacing between observations.","We present a novel framework for scheduling a set of Doppler campaigns with different cadence requirements at the W. M. Keck Observatory (WMKO).","For a set of observing programs and allocated nights on an instrument, our software optimizes the timing and ordering of ~1000 observations within a given observing semester.","We achieve a near-optimal solution in real-time using a hierarchical Integer Linear Programming (ILP) framework.","Our scheduling formulation optimizes over the roughly 10^3000 possible orderings.","A top level optimization finds the most regular sequence of allocated nights by which to observe each host star in the request catalog based on a frequency specified in the request.","A second optimization scheme minimizes the slews and downtime of the instrument.","We have assessed our algorithms performance with simulated data and with the real suite of Doppler observations of the California Planet Search in 2023."],"url":"http://arxiv.org/abs/2402.17734v1","category":"astro-ph.IM"}
{"created":"2024-02-27 18:01:59","title":"Towards Fairness-Aware Adversarial Learning","abstract":"Although adversarial training (AT) has proven effective in enhancing the model's robustness, the recently revealed issue of fairness in robustness has not been well addressed, i.e. the robust accuracy varies significantly among different categories. In this paper, instead of uniformly evaluating the model's average class performance, we delve into the issue of robust fairness, by considering the worst-case distribution across various classes. We propose a novel learning paradigm, named Fairness-Aware Adversarial Learning (FAAL). As a generalization of conventional AT, we re-define the problem of adversarial training as a min-max-max framework, to ensure both robustness and fairness of the trained model. Specifically, by taking advantage of distributional robust optimization, our method aims to find the worst distribution among different categories, and the solution is guaranteed to obtain the upper bound performance with high probability. In particular, FAAL can fine-tune an unfair robust model to be fair within only two epochs, without compromising the overall clean and robust accuracies. Extensive experiments on various image datasets validate the superior performance and efficiency of the proposed FAAL compared to other state-of-the-art methods.","sentences":["Although adversarial training (AT) has proven effective in enhancing the model's robustness, the recently revealed issue of fairness in robustness has not been well addressed, i.e. the robust accuracy varies significantly among different categories.","In this paper, instead of uniformly evaluating the model's average class performance, we delve into the issue of robust fairness, by considering the worst-case distribution across various classes.","We propose a novel learning paradigm, named Fairness-Aware Adversarial Learning (FAAL).","As a generalization of conventional AT, we re-define the problem of adversarial training as a min-max-max framework, to ensure both robustness and fairness of the trained model.","Specifically, by taking advantage of distributional robust optimization, our method aims to find the worst distribution among different categories, and the solution is guaranteed to obtain the upper bound performance with high probability.","In particular, FAAL can fine-tune an unfair robust model to be fair within only two epochs, without compromising the overall clean and robust accuracies.","Extensive experiments on various image datasets validate the superior performance and efficiency of the proposed FAAL compared to other state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.17729v1","category":"cs.CV"}
{"created":"2024-02-27 17:57:33","title":"Elliptic Reconstruction and A Posteriori Error Estimates for Parabolic Variational Inequalities","abstract":"Elliptic reconstruction property, originally introduced by Makridakis and Nochetto for linear parabolic problems, is a well-known tool to derive optimal a posteriori error estimates. No such results are known for nonlinear and nonsmooth problems such as parabolic variational inequalities (VIs). This article establishes the elliptic reconstruction property for parabolic VIs and derives a posteriori error estimates in $L^{\\infty}(0,T;L^{2}(\\Omega))$ and $L^{\\infty}(0,T;L^{\\infty}(\\Omega))$, respectively. As an application, the residual-type error estimates are presented.","sentences":["Elliptic reconstruction property, originally introduced by Makridakis and Nochetto for linear parabolic problems, is a well-known tool to derive optimal a posteriori error estimates.","No such results are known for nonlinear and nonsmooth problems such as parabolic variational inequalities (VIs).","This article establishes the elliptic reconstruction property for parabolic VIs and derives a posteriori error estimates in $L^{\\infty}(0,T;L^{2}(\\Omega))$ and $L^{\\infty}(0,T;L^{\\infty}(\\Omega))$, respectively.","As an application, the residual-type error estimates are presented."],"url":"http://arxiv.org/abs/2402.17724v1","category":"math.NA"}
{"created":"2024-02-27 17:57:04","title":"Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners","abstract":"Video and audio content creation serves as the core technique for the movie industry and professional users. Recently, existing diffusion-based methods tackle video and audio generation separately, which hinders the technique transfer from academia to industry. In this work, we aim at filling the gap, with a carefully designed optimization-based framework for cross-visual-audio and joint-visual-audio generation. We observe the powerful generation ability of off-the-shelf video or audio generation models. Thus, instead of training the giant models from scratch, we propose to bridge the existing strong models with a shared latent representation space. Specifically, we propose a multimodality latent aligner with the pre-trained ImageBind model. Our latent aligner shares a similar core as the classifier guidance that guides the diffusion denoising process during inference time. Through carefully designed optimization strategy and loss functions, we show the superior performance of our method on joint video-audio generation, visual-steered audio generation, and audio-steered visual generation tasks. The project website can be found at https://yzxing87.github.io/Seeing-and-Hearing/","sentences":["Video and audio content creation serves as the core technique for the movie industry and professional users.","Recently, existing diffusion-based methods tackle video and audio generation separately, which hinders the technique transfer from academia to industry.","In this work, we aim at filling the gap, with a carefully designed optimization-based framework for cross-visual-audio and joint-visual-audio generation.","We observe the powerful generation ability of off-the-shelf video or audio generation models.","Thus, instead of training the giant models from scratch, we propose to bridge the existing strong models with a shared latent representation space.","Specifically, we propose a multimodality latent aligner with the pre-trained ImageBind model.","Our latent aligner shares a similar core as the classifier guidance that guides the diffusion denoising process during inference time.","Through carefully designed optimization strategy and loss functions, we show the superior performance of our method on joint video-audio generation, visual-steered audio generation, and audio-steered visual generation tasks.","The project website can be found at https://yzxing87.github.io/Seeing-and-Hearing/"],"url":"http://arxiv.org/abs/2402.17723v1","category":"cs.CV"}
{"created":"2024-02-27 17:53:13","title":"Towards a Digital Twin Framework in Additive Manufacturing: Machine Learning and Bayesian Optimization for Time Series Process Optimization","abstract":"Laser-directed-energy deposition (DED) offers advantages in additive manufacturing (AM) for creating intricate geometries and material grading. Yet, challenges like material inconsistency and part variability remain, mainly due to its layer-wise fabrication. A key issue is heat accumulation during DED, which affects the material microstructure and properties. While closed-loop control methods for heat management are common in DED research, few integrate real-time monitoring, physics-based modeling, and control in a unified framework. Our work presents a digital twin (DT) framework for real-time predictive control of DED process parameters to meet specific design objectives. We develop a surrogate model using Long Short-Term Memory (LSTM)-based machine learning with Bayesian Inference to predict temperatures in DED parts. This model predicts future temperature states in real time. We also introduce Bayesian Optimization (BO) for Time Series Process Optimization (BOTSPO), based on traditional BO but featuring a unique time series process profile generator with reduced dimensions. BOTSPO dynamically optimizes processes, identifying optimal laser power profiles to attain desired mechanical properties. The established process trajectory guides online optimizations, aiming to enhance performance. This paper outlines the digital twin framework's components, promoting its integration into a comprehensive system for AM.","sentences":["Laser-directed-energy deposition (DED) offers advantages in additive manufacturing (AM) for creating intricate geometries and material grading.","Yet, challenges like material inconsistency and part variability remain, mainly due to its layer-wise fabrication.","A key issue is heat accumulation during DED, which affects the material microstructure and properties.","While closed-loop control methods for heat management are common in DED research, few integrate real-time monitoring, physics-based modeling, and control in a unified framework.","Our work presents a digital twin (DT) framework for real-time predictive control of DED process parameters to meet specific design objectives.","We develop a surrogate model using Long Short-Term Memory (LSTM)-based machine learning with Bayesian Inference to predict temperatures in DED parts.","This model predicts future temperature states in real time.","We also introduce Bayesian Optimization (BO) for Time Series Process Optimization (BOTSPO), based on traditional BO but featuring a unique time series process profile generator with reduced dimensions.","BOTSPO dynamically optimizes processes, identifying optimal laser power profiles to attain desired mechanical properties.","The established process trajectory guides online optimizations, aiming to enhance performance.","This paper outlines the digital twin framework's components, promoting its integration into a comprehensive system for AM."],"url":"http://arxiv.org/abs/2402.17718v1","category":"cs.LG"}
{"created":"2024-02-27 17:39:34","title":"Noise Aware Path Planning and Power Management of Hybrid Fuel UAVs","abstract":"Hybrid fuel Unmanned Aerial Vehicles (UAV), through their combination of multiple energy sources, offer several advantages over the standard single fuel source configuration, the primary one being increased range and efficiency. Multiple power or fuel sources also allow the distinct pitfalls of each source to be mitigated while exploiting the advantages within the mission or path planning. We consider here a UAV equipped with a combustion engine-generator and battery pack as energy sources. We consider the path planning and power-management of this platform in a noise-aware manner. To solve the path planning problem, we first present the Mixed Integer Linear Program (MILP) formulation of the problem. We then present and analyze a label-correcting algorithm, for which a pseudo-polynomial running time is proven. Results of extensive numerical testing are presented which analyze the performance and scalability of the labeling algorithm for various graph structures, problem parameters, and search heuristics. It is shown that the algorithm can solve instances on graphs as large as twenty thousand nodes in only a few seconds.","sentences":["Hybrid fuel Unmanned Aerial Vehicles (UAV), through their combination of multiple energy sources, offer several advantages over the standard single fuel source configuration, the primary one being increased range and efficiency.","Multiple power or fuel sources also allow the distinct pitfalls of each source to be mitigated while exploiting the advantages within the mission or path planning.","We consider here a UAV equipped with a combustion engine-generator and battery pack as energy sources.","We consider the path planning and power-management of this platform in a noise-aware manner.","To solve the path planning problem, we first present the Mixed Integer Linear Program (MILP) formulation of the problem.","We then present and analyze a label-correcting algorithm, for which a pseudo-polynomial running time is proven.","Results of extensive numerical testing are presented which analyze the performance and scalability of the labeling algorithm for various graph structures, problem parameters, and search heuristics.","It is shown that the algorithm can solve instances on graphs as large as twenty thousand nodes in only a few seconds."],"url":"http://arxiv.org/abs/2402.17708v1","category":"math.OC"}
{"created":"2024-02-27 17:30:33","title":"Transfer Learning Bayesian Optimization to Design Competitor DNA Molecules for Use in Diagnostic Assays","abstract":"With the rise in engineered biomolecular devices, there is an increased need for tailor-made biological sequences. Often, many similar biological sequences need to be made for a specific application meaning numerous, sometimes prohibitively expensive, lab experiments are necessary for their optimization. This paper presents a transfer learning design of experiments workflow to make this development feasible. By combining a transfer learning surrogate model with Bayesian optimization, we show how the total number of experiments can be reduced by sharing information between optimization tasks. We demonstrate the reduction in the number of experiments using data from the development of DNA competitors for use in an amplification-based diagnostic assay. We use cross-validation to compare the predictive accuracy of different transfer learning models, and then compare the performance of the models for both single objective and penalized optimization tasks.","sentences":["With the rise in engineered biomolecular devices, there is an increased need for tailor-made biological sequences.","Often, many similar biological sequences need to be made for a specific application meaning numerous, sometimes prohibitively expensive, lab experiments are necessary for their optimization.","This paper presents a transfer learning design of experiments workflow to make this development feasible.","By combining a transfer learning surrogate model with Bayesian optimization, we show how the total number of experiments can be reduced by sharing information between optimization tasks.","We demonstrate the reduction in the number of experiments using data from the development of DNA competitors for use in an amplification-based diagnostic assay.","We use cross-validation to compare the predictive accuracy of different transfer learning models, and then compare the performance of the models for both single objective and penalized optimization tasks."],"url":"http://arxiv.org/abs/2402.17704v1","category":"q-bio.QM"}
{"created":"2024-02-27 17:11:35","title":"Geometric Deep Learning for Computer-Aided Design: A Survey","abstract":"Geometric Deep Learning techniques have become a transformative force in the field of Computer-Aided Design (CAD), and have the potential to revolutionize how designers and engineers approach and enhance the design process. By harnessing the power of machine learning-based methods, CAD designers can optimize their workflows, save time and effort while making better informed decisions, and create designs that are both innovative and practical. The ability to process the CAD designs represented by geometric data and to analyze their encoded features enables the identification of similarities among diverse CAD models, the proposition of alternative designs and enhancements, and even the generation of novel design alternatives. This survey offers a comprehensive overview of learning-based methods in computer-aided design across various categories, including similarity analysis and retrieval, 2D and 3D CAD model synthesis, and CAD generation from point clouds. Additionally, it provides a complete list of benchmark datasets and their characteristics, along with open-source codes that have propelled research in this domain. The final discussion delves into the challenges prevalent in this field, followed by potential future research directions in this rapidly evolving field.","sentences":["Geometric Deep Learning techniques have become a transformative force in the field of Computer-Aided Design (CAD), and have the potential to revolutionize how designers and engineers approach and enhance the design process.","By harnessing the power of machine learning-based methods, CAD designers can optimize their workflows, save time and effort while making better informed decisions, and create designs that are both innovative and practical.","The ability to process the CAD designs represented by geometric data and to analyze their encoded features enables the identification of similarities among diverse CAD models, the proposition of alternative designs and enhancements, and even the generation of novel design alternatives.","This survey offers a comprehensive overview of learning-based methods in computer-aided design across various categories, including similarity analysis and retrieval, 2D and 3D CAD model synthesis, and CAD generation from point clouds.","Additionally, it provides a complete list of benchmark datasets and their characteristics, along with open-source codes that have propelled research in this domain.","The final discussion delves into the challenges prevalent in this field, followed by potential future research directions in this rapidly evolving field."],"url":"http://arxiv.org/abs/2402.17695v1","category":"cs.CG"}
{"created":"2024-02-27 17:09:45","title":"Optimal Control Barrier Functions: Maximizing the Action Space Subject to Control Bounds","abstract":"This letter addresses the constraint compatibility problem of control barrier functions (CBFs), which occurs when a safety-critical CBF requires a system to apply more control effort than it is capable of generating. This inevitably leads to a safety violation, which transitions the system to an unsafe (and possibly dangerous) trajectory. We resolve the constraint compatibility problem by constructing a control barrier function that maximizes the feasible action space for first and second-order constraints, and we prove that the optimal CBF encodes a dynamical motion primitive. Furthermore, we show that this dynamical motion primitive contains an implicit model for the future trajectory for time-varying components of the system. We validate our optimal CBF in simulation, and compare its behavior with a linear CBF.","sentences":["This letter addresses the constraint compatibility problem of control barrier functions (CBFs), which occurs when a safety-critical CBF requires a system to apply more control effort than it is capable of generating.","This inevitably leads to a safety violation, which transitions the system to an unsafe (and possibly dangerous) trajectory.","We resolve the constraint compatibility problem by constructing a control barrier function that maximizes the feasible action space for first and second-order constraints, and we prove that the optimal CBF encodes a dynamical motion primitive.","Furthermore, we show that this dynamical motion primitive contains an implicit model for the future trajectory for time-varying components of the system.","We validate our optimal CBF in simulation, and compare its behavior with a linear CBF."],"url":"http://arxiv.org/abs/2402.17694v1","category":"math.OC"}
{"created":"2024-02-27 16:07:56","title":"Exergetic Port-Hamiltonian Systems Modeling Language","abstract":"Mathematical modeling of real-world physical systems requires the consistent combination of a multitude of physical laws and phenomenological models. This challenging task can be greatly simplified by hierarchically decomposing systems. Moreover, the use of diagrams for expressing such decompositions helps make the process more intuitive and facilitates communication, even with non-experts. As an important requirement, models have to respect fundamental physical laws such as the first and the second law of thermodynamics. While some existing modeling frameworks can make such guarantees based on structural properties of their models, they lack a formal graphical syntax. We present a compositional and thermodynamically consistent modeling language with a graphical syntax. As its semantics, port-Hamiltonian systems are endowed with further structural properties and a fixed physical interpretation such that thermodynamic consistency is ensured in a way that is closely related to the GENERIC framework. While port-Hamiltonian systems are inspired by graphical modeling with bond graphs, neither the link between the two, nor bond graphs themselves, can be easily formalized. In contrast, our syntax is based on a refinement of the well-studied operad of undirected wiring diagrams. The language effectively decouples the construction of complex models via the graphical syntax from physical concerns, which are dealt with only at the level of primitive subsystems that represent elementary physical behaviors. As a consequence, reuse of models and substitution of their parts becomes possible. Finally, by construction, systems interact by exchanging exergy, i.e. energy that is available for doing work, so the language is particularly well suited for thermodynamic analysis and optimization.","sentences":["Mathematical modeling of real-world physical systems requires the consistent combination of a multitude of physical laws and phenomenological models.","This challenging task can be greatly simplified by hierarchically decomposing systems.","Moreover, the use of diagrams for expressing such decompositions helps make the process more intuitive and facilitates communication, even with non-experts.","As an important requirement, models have to respect fundamental physical laws such as the first and the second law of thermodynamics.","While some existing modeling frameworks can make such guarantees based on structural properties of their models, they lack a formal graphical syntax.","We present a compositional and thermodynamically consistent modeling language with a graphical syntax.","As its semantics, port-Hamiltonian systems are endowed with further structural properties and a fixed physical interpretation such that thermodynamic consistency is ensured in a way that is closely related to the GENERIC framework.","While port-Hamiltonian systems are inspired by graphical modeling with bond graphs, neither the link between the two, nor bond graphs themselves, can be easily formalized.","In contrast, our syntax is based on a refinement of the well-studied operad of undirected wiring diagrams.","The language effectively decouples the construction of complex models via the graphical syntax from physical concerns, which are dealt with only at the level of primitive subsystems that represent elementary physical behaviors.","As a consequence, reuse of models and substitution of their parts becomes possible.","Finally, by construction, systems interact by exchanging exergy, i.e. energy that is available for doing work, so the language is particularly well suited for thermodynamic analysis and optimization."],"url":"http://arxiv.org/abs/2402.17640v1","category":"eess.SY"}
{"created":"2024-02-27 15:57:31","title":"Deterministic Cache-Oblivious Funnelselect","abstract":"In the multiple-selection problem one is given an unsorted array $S$ of $N$ elements and an array of $q$ query ranks $r_1<\\cdots<r_q$, and the task is to return, in sorted order, the $q$ elements in $S$ of rank $r_1, \\ldots, r_q$, respectively. The asymptotic deterministic comparison complexity of the problem was settled by Dobkin and Munro [JACM 1981]. In the I/O model an optimal I/O complexity was achieved by Hu et al. [SPAA 2014]. Recently [ESA 2023], we presented a cache-oblivious algorithm with matching I/O complexity, named funnelselect, since it heavily borrows ideas from the cache-oblivious sorting algorithm funnelsort from the seminal paper by Frigo, Leiserson, Prokop and Ramachandran [FOCS 1999]. Funnelselect is inherently randomized as it relies on sampling for cheaply finding many good pivots. In this paper we present deterministic funnelselect, achieving the same optional I/O complexity cache-obliviously without randomization. Our new algorithm essentially replaces a single (in expectation) reversed-funnel computation using random pivots by a recursive algorithm using multiple reversed-funnel computations. To meet the I/O bound, this requires a carefully chosen subproblem size based on the entropy of the sequence of query ranks; deterministic funnelselect thus raises distinct technical challenges not met by randomized funnelselect. The resulting worst-case I/O bound is $O\\bigl(\\sum_{i=1}^{q+1} \\frac{\\Delta_i}{B} \\cdot \\log_{M/B} \\frac{N}{\\Delta_i} + \\frac{N}{B}\\bigr)$, where $B$ is the external memory block size, $M\\geq B^{1+\\epsilon}$ is the internal memory size, for some constant $\\epsilon>0$, and $\\Delta_i = r_{i} - r_{i-1}$ (assuming $r_0=0$ and $r_{q+1}=N + 1$).","sentences":["In the multiple-selection problem one is given an unsorted array $S$ of $N$ elements and an array of $q$ query ranks $r_1<\\cdots<r_q$, and the task is to return, in sorted order, the $q$ elements in $S$ of rank $r_1, \\ldots, r_q$, respectively.","The asymptotic deterministic comparison complexity of the problem was settled by Dobkin and Munro","[JACM 1981].","In the I/O model an optimal I/O complexity was achieved by Hu et al.","[SPAA 2014].","Recently [ESA 2023], we presented a cache-oblivious algorithm with matching I/O complexity, named funnelselect, since it heavily borrows ideas from the cache-oblivious sorting algorithm funnelsort from the seminal paper by Frigo, Leiserson, Prokop and Ramachandran","[FOCS 1999].","Funnelselect is inherently randomized as it relies on sampling for cheaply finding many good pivots.","In this paper we present deterministic funnelselect, achieving the same optional I/O complexity cache-obliviously without randomization.","Our new algorithm essentially replaces a single (in expectation) reversed-funnel computation using random pivots by a recursive algorithm using multiple reversed-funnel computations.","To meet the I/O bound, this requires a carefully chosen subproblem size based on the entropy of the sequence of query ranks; deterministic funnelselect thus raises distinct technical challenges not met by randomized funnelselect.","The resulting worst-case I/O bound is $O\\bigl(\\sum_{i=1}^{q+1} \\frac{\\Delta_i}{B} \\cdot \\log_{M/B} \\frac{N}{\\Delta_i} + \\frac{N}{B}\\bigr)$, where $B$ is the external memory block size, $M\\geq B^{1+\\epsilon}$ is the internal memory size, for some constant $\\epsilon>0$, and $\\Delta_i = r_{i} - r_{i-1}$ (assuming $r_0=0$ and $r_{q+1}=N + 1$)."],"url":"http://arxiv.org/abs/2402.17631v1","category":"cs.DS"}
{"created":"2024-02-27 15:57:11","title":"Fine-Grained Natural Language Inference Based Faithfulness Evaluation for Diverse Summarisation Tasks","abstract":"We study existing approaches to leverage off-the-shelf Natural Language Inference (NLI) models for the evaluation of summary faithfulness and argue that these are sub-optimal due to the granularity level considered for premises and hypotheses. That is, the smaller content unit considered as hypothesis is a sentence and premises are made up of a fixed number of document sentences. We propose a novel approach, namely InFusE, that uses a variable premise size and simplifies summary sentences into shorter hypotheses. Departing from previous studies which focus on single short document summarisation, we analyse NLI based faithfulness evaluation for diverse summarisation tasks. We introduce DiverSumm, a new benchmark comprising long form summarisation (long documents and summaries) and diverse summarisation tasks (e.g., meeting and multi-document summarisation). In experiments, InFusE obtains superior performance across the different summarisation tasks. Our code and data are available at https://github.com/HJZnlp/infuse.","sentences":["We study existing approaches to leverage off-the-shelf Natural Language Inference (NLI) models for the evaluation of summary faithfulness and argue that these are sub-optimal due to the granularity level considered for premises and hypotheses.","That is, the smaller content unit considered as hypothesis is a sentence and premises are made up of a fixed number of document sentences.","We propose a novel approach, namely InFusE, that uses a variable premise size and simplifies summary sentences into shorter hypotheses.","Departing from previous studies which focus on single short document summarisation, we analyse NLI based faithfulness evaluation for diverse summarisation tasks.","We introduce DiverSumm, a new benchmark comprising long form summarisation (long documents and summaries) and diverse summarisation tasks (e.g., meeting and multi-document summarisation).","In experiments, InFusE obtains superior performance across the different summarisation tasks.","Our code and data are available at https://github.com/HJZnlp/infuse."],"url":"http://arxiv.org/abs/2402.17630v1","category":"cs.CL"}
{"created":"2024-02-27 15:35:55","title":"Out-of-time-ordered correlators for Wigner matrices","abstract":"We consider the time evolution of the out-of-time-ordered correlator (OTOC) of two general observables $A$ and $B$ in a mean field chaotic quantum system described by a random Wigner matrix as its Hamiltonian. We rigorously identify three time regimes separated by the physically relevant scrambling and relaxation times. The main feature of our analysis is that we express the error terms in the optimal Schatten (tracial) norms of the observables, allowing us to track the exact dependence of the errors on their rank. In particular, for significantly overlapping observables with low rank the OTOC is shown to exhibit a significant local maximum at the scrambling time, a feature that may not have been noticed in the physics literature before. Our main tool is a novel multi-resolvent local law with Schatten norms that unifies and improves previous local laws involving either the much cruder operator norm (cf. [G. Cipolloni, L. Erd\\H{o}s, D. Schr\\\"oder. Elect. J. Prob. 27, 1-38, 2022]) or the Hilbert-Schmidt norm (cf. [G. Cipolloni, L. Erd\\H{o}s, D. Schr\\\"oder. Forum Math., Sigma 10, E96, 2022]).","sentences":["We consider the time evolution of the out-of-time-ordered correlator (OTOC) of two general observables $A$ and $B$ in a mean field chaotic quantum system described by a random Wigner matrix as its Hamiltonian.","We rigorously identify three time regimes separated by the physically relevant scrambling and relaxation times.","The main feature of our analysis is that we express the error terms in the optimal Schatten (tracial) norms of the observables, allowing us to track the exact dependence of the errors on their rank.","In particular, for significantly overlapping observables with low rank the OTOC is shown to exhibit a significant local maximum at the scrambling time, a feature that may not have been noticed in the physics literature before.","Our main tool is a novel multi-resolvent local law with Schatten norms that unifies and improves previous local laws involving either the much cruder operator norm (cf.","[G. Cipolloni, L. Erd\\H{o}s, D. Schr\\\"oder. Elect.","J. Prob.","27, 1-38, 2022]) or the Hilbert-Schmidt norm (cf.","[G. Cipolloni, L. Erd\\H{o}s, D. Schr\\\"oder.","Forum Math., Sigma 10, E96, 2022])."],"url":"http://arxiv.org/abs/2402.17609v1","category":"math-ph"}
{"created":"2024-02-27 15:28:19","title":"Corridor MPC for Multi-Agent Inspection of Orbiting Structures","abstract":"In this work, we propose an extension of the previously introduced Corridor Model Predictive Control scheme for high-order and distributed systems, with an application for on-orbit inspection. To this end, we leverage high order control barrier function (HOCBF) constraints as a suitable control approach to maintain each agent in the formation within a safe corridor from its reference trajectory. The recursive feasibility of the designed MPC scheme is tested numerically, while suitable modifications of the classical HOCBF constraint definition are introduced such that safety is guaranteed both in sampled and continuous time. The designed controller is validated through computer simulation in a realistic inspection scenario of the International Space Station.","sentences":["In this work, we propose an extension of the previously introduced Corridor Model Predictive Control scheme for high-order and distributed systems, with an application for on-orbit inspection.","To this end, we leverage high order control barrier function (HOCBF) constraints as a suitable control approach to maintain each agent in the formation within a safe corridor from its reference trajectory.","The recursive feasibility of the designed MPC scheme is tested numerically, while suitable modifications of the classical HOCBF constraint definition are introduced such that safety is guaranteed both in sampled and continuous time.","The designed controller is validated through computer simulation in a realistic inspection scenario of the International Space Station."],"url":"http://arxiv.org/abs/2402.17596v1","category":"cs.SY"}
{"created":"2024-02-27 15:22:20","title":"PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive Representation Learning","abstract":"Recently, the application of Contrastive Representation Learning (CRL) in learning with noisy labels (LNL) has shown promising advancements due to its remarkable ability to learn well-distributed representations for better distinguishing noisy labels. However, CRL is mainly used as a pre-training technique, leading to a complicated multi-stage training pipeline. We also observed that trivially combining CRL with supervised LNL methods decreases performance. Using different images from the same class as negative pairs in CRL creates optimization conflicts between CRL and the supervised loss. To address these two issues, we propose an end-to-end PLReMix framework that avoids the complicated pipeline by introducing a Pseudo-Label Relaxed (PLR) contrastive loss to alleviate the conflicts between losses. This PLR loss constructs a reliable negative set of each sample by filtering out its inappropriate negative pairs that overlap at the top k indices of prediction probabilities, leading to more compact semantic clusters than vanilla CRL. Furthermore, a two-dimensional Gaussian Mixture Model (GMM) is adopted to distinguish clean and noisy samples by leveraging semantic information and model outputs simultaneously, which is expanded on the previously widely used one-dimensional form. The PLR loss and a semi-supervised loss are simultaneously applied to train on the GMM divided clean and noisy samples. Experiments on multiple benchmark datasets demonstrate the effectiveness of the proposed method. Our proposed PLR loss is scalable, which can be easily integrated into other LNL methods and boost their performance. Codes will be available.","sentences":["Recently, the application of Contrastive Representation Learning (CRL) in learning with noisy labels (LNL) has shown promising advancements due to its remarkable ability to learn well-distributed representations for better distinguishing noisy labels.","However, CRL is mainly used as a pre-training technique, leading to a complicated multi-stage training pipeline.","We also observed that trivially combining CRL with supervised LNL methods decreases performance.","Using different images from the same class as negative pairs in CRL creates optimization conflicts between CRL and the supervised loss.","To address these two issues, we propose an end-to-end PLReMix framework that avoids the complicated pipeline by introducing a Pseudo-Label Relaxed (PLR) contrastive loss to alleviate the conflicts between losses.","This PLR loss constructs a reliable negative set of each sample by filtering out its inappropriate negative pairs that overlap at the top k indices of prediction probabilities, leading to more compact semantic clusters than vanilla CRL.","Furthermore, a two-dimensional Gaussian Mixture Model (GMM) is adopted to distinguish clean and noisy samples by leveraging semantic information and model outputs simultaneously, which is expanded on the previously widely used one-dimensional form.","The PLR loss and a semi-supervised loss are simultaneously applied to train on the GMM divided clean and noisy samples.","Experiments on multiple benchmark datasets demonstrate the effectiveness of the proposed method.","Our proposed PLR loss is scalable, which can be easily integrated into other LNL methods and boost their performance.","Codes will be available."],"url":"http://arxiv.org/abs/2402.17589v1","category":"cs.CV"}
{"created":"2024-02-27 15:17:05","title":"Communication-Constrained STL Task Decomposition through Convex Optimization","abstract":"In this work, we propose a method to decompose signal temporal logic (STL) tasks for multi-agent systems subject to constraints imposed by the communication graph. Specifically, we propose to decompose tasks defined over multiple agents which require multi-hop communication, by a set of sub-tasks defined over the states of agents with 1-hop distance over the communication graph. To this end, we parameterize the predicates of the tasks to be decomposed as suitable hyper-rectangles. Then, we show that by solving a constrained convex optimization, optimal parameters maximising the volume of the predicate's super-level sets can be computed for the decomposed tasks. In addition, we provide a formal definition of conflicting conjunctions of tasks for the considered STL fragment and a formal procedure to exclude such conjunctions from the solution set of possible decompositions. The proposed approach is demonstrated through simulations.","sentences":["In this work, we propose a method to decompose signal temporal logic (STL) tasks for multi-agent systems subject to constraints imposed by the communication graph.","Specifically, we propose to decompose tasks defined over multiple agents which require multi-hop communication, by a set of sub-tasks defined over the states of agents with 1-hop distance over the communication graph.","To this end, we parameterize the predicates of the tasks to be decomposed as suitable hyper-rectangles.","Then, we show that by solving a constrained convex optimization, optimal parameters maximising the volume of the predicate's super-level sets can be computed for the decomposed tasks.","In addition, we provide a formal definition of conflicting conjunctions of tasks for the considered STL fragment and a formal procedure to exclude such conjunctions from the solution set of possible decompositions.","The proposed approach is demonstrated through simulations."],"url":"http://arxiv.org/abs/2402.17585v1","category":"eess.SY"}
{"created":"2024-02-27 15:13:05","title":"A highly efficient computational approach for part-scale microstructure predictions in Ti-6Al-4V additive manufacturing","abstract":"Fast and efficient simulations of metal additive manufacturing (AM) processes are highly relevant to exploring the full potential of this promising manufacturing technique. The microstructure composition plays an important role in characterizing the part quality and deriving mechanical properties. When complete parts are simulated, one often needs to resort to strong simplifications such as layer-wise heating due to the large number of simulated time steps compared to the small time step sizes. This article proposes a scan-resolved approach to the coupled thermo-microstructural problem. Building on a highly efficient thermal model, we discuss the implementation of a phenomenological microstructure model for the evolution of the three main constituents of Ti-6Al-4V: stable $\\alpha_s$-phase, martensite $\\alpha_m$-phase and $\\beta$-phase. The implementation is tailored to modern hardware features using vectorization and fast approximations of transcendental functions. A performance model and numerical examples verify the high degree of optimization. We demonstrate the applicability and predictive power of the approach and the influence of scan strategy and geometry. Depending on the specific example, results can be obtained with moderate computational resources in a few hours to days. The numerical examples include a prediction of the microstructure on the full NIST AM Benchmark cantilever specimen.","sentences":["Fast and efficient simulations of metal additive manufacturing (AM) processes are highly relevant to exploring the full potential of this promising manufacturing technique.","The microstructure composition plays an important role in characterizing the part quality and deriving mechanical properties.","When complete parts are simulated, one often needs to resort to strong simplifications such as layer-wise heating due to the large number of simulated time steps compared to the small time step sizes.","This article proposes a scan-resolved approach to the coupled thermo-microstructural problem.","Building on a highly efficient thermal model, we discuss the implementation of a phenomenological microstructure model for the evolution of the three main constituents of Ti-6Al-4V: stable $\\alpha_s$-phase, martensite $\\alpha_m$-phase and $\\beta$-phase.","The implementation is tailored to modern hardware features using vectorization and fast approximations of transcendental functions.","A performance model and numerical examples verify the high degree of optimization.","We demonstrate the applicability and predictive power of the approach and the influence of scan strategy and geometry.","Depending on the specific example, results can be obtained with moderate computational resources in a few hours to days.","The numerical examples include a prediction of the microstructure on the full NIST AM Benchmark cantilever specimen."],"url":"http://arxiv.org/abs/2402.17580v1","category":"cs.CE"}
{"created":"2024-02-27 15:12:04","title":"Optimization of foreground moment deprojection for semi-blind CMB polarization reconstruction","abstract":"Upcoming Cosmic Microwave Background (CMB) experiments, aimed at measuring primordial CMB B-modes, require exquisite control of Galactic foreground contamination. Minimum-variance techniques, like the Needlet Internal Linear Combination (NILC), have proven effective in reconstructing the CMB polarization signal and mitigating foregrounds across diverse sky models without suffering from mismodelling errors. Still, residual contamination may bias the recovered CMB polarization at large angular scales when confronted with the most complex foreground scenarios. By adding constraints to NILC to deproject moments of the Galactic emission, the Constrained Moment ILC (cMILC) method has proven to enhance foreground subtraction, albeit with an associated increase in overall noise variance. Faced with this trade-off between foreground bias reduction and overall variance minimization, there is still no recipe on which moments to deproject and which are better suited for blind variance minimization. To address this, we introduce the optimized cMILC (ocMILC) pipeline, which performs full optimization of the required number and set of foreground moments to deproject, pivot parameter values, and deprojection coefficients across the sky and angular scales, depending on the actual sky complexity, available frequency coverage, and experiment sensitivity. The optimal number of deprojected moments, before paying significant noise penalty, is determined through a data diagnosis inspired by the Generalized NILC (GNILC) method. Validated on B-mode simulations of the PICO space mission concept with four challenging foreground models, ocMILC exhibits lower foreground contamination compared to NILC and cMILC at all angular scales, with limited noise penalty. This multi-layer optimization enables the ocMILC pipeline to achieve unbiased posteriors of the tensor-to-scalar ratio, regardless of foreground complexity.","sentences":["Upcoming Cosmic Microwave Background (CMB) experiments, aimed at measuring primordial CMB B-modes, require exquisite control of Galactic foreground contamination.","Minimum-variance techniques, like the Needlet Internal Linear Combination (NILC), have proven effective in reconstructing the CMB polarization signal and mitigating foregrounds across diverse sky models without suffering from mismodelling errors.","Still, residual contamination may bias the recovered CMB polarization at large angular scales when confronted with the most complex foreground scenarios.","By adding constraints to NILC to deproject moments of the Galactic emission, the Constrained Moment ILC (cMILC) method has proven to enhance foreground subtraction, albeit with an associated increase in overall noise variance.","Faced with this trade-off between foreground bias reduction and overall variance minimization, there is still no recipe on which moments to deproject and which are better suited for blind variance minimization.","To address this, we introduce the optimized cMILC (ocMILC) pipeline, which performs full optimization of the required number and set of foreground moments to deproject, pivot parameter values, and deprojection coefficients across the sky and angular scales, depending on the actual sky complexity, available frequency coverage, and experiment sensitivity.","The optimal number of deprojected moments, before paying significant noise penalty, is determined through a data diagnosis inspired by the Generalized NILC (GNILC) method.","Validated on B-mode simulations of the PICO space mission concept with four challenging foreground models, ocMILC exhibits lower foreground contamination compared to NILC and cMILC at all angular scales, with limited noise penalty.","This multi-layer optimization enables the ocMILC pipeline to achieve unbiased posteriors of the tensor-to-scalar ratio, regardless of foreground complexity."],"url":"http://arxiv.org/abs/2402.17579v1","category":"astro-ph.CO"}
{"created":"2024-02-27 15:09:20","title":"HBF MU-MIMO with Interference-Aware Beam Pair Link Allocation for Beyond-5G mm-Wave Networks","abstract":"Hybrid beamforming (HBF) multi-user multiple-input multiple-output (MU-MIMO) is a key technology for unlocking the directional millimeter-wave (mm-wave) nature for spatial multiplexing beyond current codebook-based 5G-NR networks. In order to suppress co-scheduled users' interference, HBF MU-MIMO is predicated on having sufficient radio frequency chains and accurate channel state information (CSI), which can otherwise lead to performance losses due to imperfect interference cancellation. In this work, we propose IABA, a 5G-NR standard-compliant beam pair link (BPL) allocation scheme for mitigating spatial interference in practical HBF MU-MIMO networks. IABA solves the network sum throughput optimization via either a distributed or a centralized BPL allocation using dedicated CSI reference signals for candidate BPL monitoring. We present a comprehensive study of practical multi-cell mm-wave networks and demonstrate that HBF MU-MIMO without interference-aware BPL allocation experiences strong residual interference which limits the achievable network performance. Our results show that IABA offers significant performance gains over the default interference-agnostic 5G-NR BPL allocation, and even allows HBF MU-MIMO to outperform the fully digital MU-MIMO baseline, by facilitating allocation of secondary BPLs other than the strongest BPL found during initial access. We further demonstrate the scalability of IABA with increased gNB antennas and densification for beyond-5G mm-wave networks.","sentences":["Hybrid beamforming (HBF) multi-user multiple-input multiple-output (MU-MIMO) is a key technology for unlocking the directional millimeter-wave (mm-wave) nature for spatial multiplexing beyond current codebook-based 5G-NR networks.","In order to suppress co-scheduled users' interference, HBF MU-MIMO is predicated on having sufficient radio frequency chains and accurate channel state information (CSI), which can otherwise lead to performance losses due to imperfect interference cancellation.","In this work, we propose IABA, a 5G-NR standard-compliant beam pair link (BPL) allocation scheme for mitigating spatial interference in practical HBF MU-MIMO networks.","IABA solves the network sum throughput optimization via either a distributed or a centralized BPL allocation using dedicated CSI reference signals for candidate BPL monitoring.","We present a comprehensive study of practical multi-cell mm-wave networks and demonstrate that HBF MU-MIMO without interference-aware BPL allocation experiences strong residual interference which limits the achievable network performance.","Our results show that IABA offers significant performance gains over the default interference-agnostic 5G-NR BPL allocation, and even allows HBF MU-MIMO to outperform the fully digital MU-MIMO baseline, by facilitating allocation of secondary BPLs other than the strongest BPL found during initial access.","We further demonstrate the scalability of IABA with increased gNB antennas and densification for beyond-5G mm-wave networks."],"url":"http://arxiv.org/abs/2402.17573v1","category":"cs.NI"}
{"created":"2024-02-27 15:08:14","title":"Backpropagation-Based Analytical Derivatives of EKF Covariance for Active Sensing","abstract":"To enhance accuracy of robot state estimation, perception-aware (or active sensing) methods seek trajectories that minimize uncertainty. To this aim, one possibility is to seek trajectories that minimize the final covariance of an extended Kalman filter (EKF), w.r.t. its control inputs over a given horizon. However, this can be computationally demanding. In this article, we derive novel backpropagation analytical formulas for the derivatives of the final covariance of an EKF w.r.t. its inputs. We then leverage the obtained gradients as an enabling technology to derive perception-aware optimal motion plans. Simulations validate the approach, showcasing improvements in both estimation accuracy and execution time. Experimental results on a real large ground vehicle also support the method.","sentences":["To enhance accuracy of robot state estimation, perception-aware (or active sensing) methods seek trajectories that minimize uncertainty.","To this aim, one possibility is to seek trajectories that minimize the final covariance of an extended Kalman filter (EKF), w.r.t.","its control inputs over a given horizon.","However, this can be computationally demanding.","In this article, we derive novel backpropagation analytical formulas for the derivatives of the final covariance of an EKF w.r.t.","its inputs.","We then leverage the obtained gradients as an enabling technology to derive perception-aware optimal motion plans.","Simulations validate the approach, showcasing improvements in both estimation accuracy and execution time.","Experimental results on a real large ground vehicle also support the method."],"url":"http://arxiv.org/abs/2402.17569v1","category":"cs.RO"}
{"created":"2024-02-27 15:06:40","title":"Coherence generation with Hamiltonians","abstract":"We explore methods to generate quantum coherence through unitary evolutions, by introducing and studying the coherence generating capacity of Hamiltonians. This quantity is defined as the maximum derivative of coherence that can be achieved by a Hamiltonian. By adopting the relative entropy of coherence as our figure of merit, we evaluate the maximal coherence generating capacity with the constraint of a bounded Hilbert-Schmidt norm for the Hamiltonian. Our investigation yields closed-form expressions for both Hamiltonians and quantum states that induce the maximal derivative of coherence under these conditions. Specifically, for qubit systems, we solve this problem comprehensively for any given Hamiltonian, identifying the quantum states that lead to the largest coherence derivative induced by the Hamiltonian. Our investigation enables a precise identification of conditions under which quantum coherence is optimally enhanced, offering valuable insights for the manipulation and control of quantum coherence in quantum systems.","sentences":["We explore methods to generate quantum coherence through unitary evolutions, by introducing and studying the coherence generating capacity of Hamiltonians.","This quantity is defined as the maximum derivative of coherence that can be achieved by a Hamiltonian.","By adopting the relative entropy of coherence as our figure of merit, we evaluate the maximal coherence generating capacity with the constraint of a bounded Hilbert-Schmidt norm for the Hamiltonian.","Our investigation yields closed-form expressions for both Hamiltonians and quantum states that induce the maximal derivative of coherence under these conditions.","Specifically, for qubit systems, we solve this problem comprehensively for any given Hamiltonian, identifying the quantum states that lead to the largest coherence derivative induced by the Hamiltonian.","Our investigation enables a precise identification of conditions under which quantum coherence is optimally enhanced, offering valuable insights for the manipulation and control of quantum coherence in quantum systems."],"url":"http://arxiv.org/abs/2402.17567v1","category":"quant-ph"}
{"created":"2024-02-27 15:05:32","title":"Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers","abstract":"Automatic prompt optimization is an important approach to improving the performance of large language models (LLMs). Recent research demonstrates the potential of using LLMs as prompt optimizers, which can generate improved task prompts via iterative refinement. In this paper, we propose a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers. To connect these two approaches, we identify two pivotal factors in model parameter learning: update direction and update method. Focused on the two aspects, we borrow the theoretical framework and learning methods from gradient-based optimization to design improved strategies for LLM-based prompt optimizers. By systematically analyzing a rich set of improvement strategies, we further develop a capable Gradient-inspired LLM-based Prompt Optimizer called GPO. At each step, it first retrieves relevant prompts from the optimization trajectory as the update direction. Then, it utilizes the generation-based refinement strategy to perform the update, while controlling the edit distance through a cosine-based decay strategy. Extensive experiments demonstrate the effectiveness and efficiency of GPO. In particular, GPO brings an additional improvement of up to 56.8% on Big-Bench Hard and 55.3% on MMLU compared to baseline methods.","sentences":["Automatic prompt optimization is an important approach to improving the performance of large language models (LLMs).","Recent research demonstrates the potential of using LLMs as prompt optimizers, which can generate improved task prompts via iterative refinement.","In this paper, we propose a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers.","To connect these two approaches, we identify two pivotal factors in model parameter learning: update direction and update method.","Focused on the two aspects, we borrow the theoretical framework and learning methods from gradient-based optimization to design improved strategies for LLM-based prompt optimizers.","By systematically analyzing a rich set of improvement strategies, we further develop a capable Gradient-inspired LLM-based Prompt Optimizer called GPO.","At each step, it first retrieves relevant prompts from the optimization trajectory as the update direction.","Then, it utilizes the generation-based refinement strategy to perform the update, while controlling the edit distance through a cosine-based decay strategy.","Extensive experiments demonstrate the effectiveness and efficiency of GPO.","In particular, GPO brings an additional improvement of up to 56.8% on Big-Bench Hard and 55.3% on MMLU compared to baseline methods."],"url":"http://arxiv.org/abs/2402.17564v1","category":"cs.CL"}
{"created":"2024-02-27 15:05:13","title":"Structure-Guided Adversarial Training of Diffusion Models","abstract":"Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transformers (DiT) and outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512, respectively.","sentences":["Diffusion models have demonstrated exceptional efficacy in various generative applications.","While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples.","To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM).","In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch.","To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones.","SADM substantially improves existing diffusion transformers (DiT) and outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512, respectively."],"url":"http://arxiv.org/abs/2402.17563v1","category":"cs.CV"}
{"created":"2024-02-27 14:58:35","title":"GraphMatch: Subgraph Query Processing on FPGAs","abstract":"Efficiently finding subgraph embeddings in large graphs is crucial for many application areas like biology and social network analysis. Set intersections are the predominant and most challenging aspect of current join-based subgraph query processing systems for CPUs. Previous work has shown the viability of utilizing FPGAs for acceleration of graph and join processing.   In this work, we propose GraphMatch, the first genearl-purpose stand-alone subgraph query processing accelerator based on worst-case optimal joins (WCOJ) that is fully designed for modern, field programmable gate array (FPGA) hardware. For efficient processing of various graph data sets and query graph patterns, it leverages a novel set intersection approach, called AllCompare, tailor-made for FPGAs. We show that this set intersection approach efficiently solves multi-set intersections in subgraph query processing, superior to CPU-based approaches. Overall, GraphMatch achieves a speedup of over 2.68x and 5.16x, compared to the state-of-the-art systems GraphFlow and RapidMatch, respectively.","sentences":["Efficiently finding subgraph embeddings in large graphs is crucial for many application areas like biology and social network analysis.","Set intersections are the predominant and most challenging aspect of current join-based subgraph query processing systems for CPUs.","Previous work has shown the viability of utilizing FPGAs for acceleration of graph and join processing.   ","In this work, we propose GraphMatch, the first genearl-purpose stand-alone subgraph query processing accelerator based on worst-case optimal joins (WCOJ) that is fully designed for modern, field programmable gate array (FPGA) hardware.","For efficient processing of various graph data sets and query graph patterns, it leverages a novel set intersection approach, called AllCompare, tailor-made for FPGAs.","We show that this set intersection approach efficiently solves multi-set intersections in subgraph query processing, superior to CPU-based approaches.","Overall, GraphMatch achieves a speedup of over 2.68x and 5.16x, compared to the state-of-the-art systems GraphFlow and RapidMatch, respectively."],"url":"http://arxiv.org/abs/2402.17559v1","category":"cs.DB"}
{"created":"2024-02-27 14:27:05","title":"A heuristic for solving the irregular strip packing problem with quantum optimization","abstract":"We introduce a novel quantum computing heuristic for solving the irregular strip packing problem, a significant challenge in optimizing material usage across various industries. This problem involves arranging a set of irregular polygonal pieces within a fixed-height, rectangular container to minimize waste. Traditional methods heavily rely on manual optimization by specialists, highlighting the complexity and computational difficulty of achieving quasi-optimal layouts. The proposed algorithm employs a quantum-inspired heuristic that decomposes the strip packing problem into two sub-problems: ordering pieces via the traveling salesman problem and spatially arranging them in a rectangle packing problem. This strategy facilitates a novel application of quantum computing to industrial optimization, aiming to minimize waste and enhance material efficiency. Experimental evaluations using both classical and quantum computational methods demonstrate the algorithm's efficacy. We evaluate the algorithm's performance using the quantum approximate optimization algorithm and the quantum alternating operator ansatz, through simulations and real quantum computers, and compare it to classical approaches.","sentences":["We introduce a novel quantum computing heuristic for solving the irregular strip packing problem, a significant challenge in optimizing material usage across various industries.","This problem involves arranging a set of irregular polygonal pieces within a fixed-height, rectangular container to minimize waste.","Traditional methods heavily rely on manual optimization by specialists, highlighting the complexity and computational difficulty of achieving quasi-optimal layouts.","The proposed algorithm employs a quantum-inspired heuristic that decomposes the strip packing problem into two sub-problems: ordering pieces via the traveling salesman problem and spatially arranging them in a rectangle packing problem.","This strategy facilitates a novel application of quantum computing to industrial optimization, aiming to minimize waste and enhance material efficiency.","Experimental evaluations using both classical and quantum computational methods demonstrate the algorithm's efficacy.","We evaluate the algorithm's performance using the quantum approximate optimization algorithm and the quantum alternating operator ansatz, through simulations and real quantum computers, and compare it to classical approaches."],"url":"http://arxiv.org/abs/2402.17542v1","category":"quant-ph"}
{"created":"2024-02-27 14:25:20","title":"The optimizing mode classification stabilization of sampled stochastic jump systems via an improved hill-climbing algorithm based on Q-learning","abstract":"This paper addresses the stabilization problem of stochastic jump systems (SJSs) closed by a generally sampled controller. Because of the controller's switching and state both sampled, it is challenging to study its stabilization. A new stabilizing method deeply depending on the mode classifications is proposed to deal with the above sampling situation, whose quantity is equal to a Stirling number of the second kind. For the sake of finding the best stabilization effect among all the classifications, a convex optimization problem is developed, whose globally solution is proved to be existent and can be computed by an augmented Lagrangian function. More importantly, in order to further reduce the computation complexity but retaining a better performance as much as possible, a novelly improved hill-climbing algorithm is established by applying the Q-learning technique to provide an optimal attenuation coefficient. A numerical example is offered so as to verify the effectiveness and superiority of the methods proposed in this study.","sentences":["This paper addresses the stabilization problem of stochastic jump systems (SJSs) closed by a generally sampled controller.","Because of the controller's switching and state both sampled, it is challenging to study its stabilization.","A new stabilizing method deeply depending on the mode classifications is proposed to deal with the above sampling situation, whose quantity is equal to a Stirling number of the second kind.","For the sake of finding the best stabilization effect among all the classifications, a convex optimization problem is developed, whose globally solution is proved to be existent and can be computed by an augmented Lagrangian function.","More importantly, in order to further reduce the computation complexity but retaining a better performance as much as possible, a novelly improved hill-climbing algorithm is established by applying the Q-learning technique to provide an optimal attenuation coefficient.","A numerical example is offered so as to verify the effectiveness and superiority of the methods proposed in this study."],"url":"http://arxiv.org/abs/2402.17539v1","category":"math.OC"}
{"created":"2024-02-27 14:05:46","title":"Highway Discretionary Lane-change Decision and Control Using Model Predictive Control","abstract":"To enable vehicles to perform automatic lane change amidst the random traffic flow on highways, this paper introduces a decision-making and control method for vehicle lane-change based on Model Predictive Control (MPC). This approach divides the driving control of vehicles on highways into two parts: lane-change decision and lane-change control, both of which are solved using the MPC method. In the lane-change decision module, the minimum driving costs for each lane are computed and compared by solving the MPC problem to make lane-change decisions. In the lane-change control module, a dynamic bicycle model is incorporated, and a multi-objective cost function is designed to obtain the optimal control inputs for the lane-change process. The proposed lane-change decision and control methods are simulated and validated within the SUMO platform under random highway traffic conditions.","sentences":["To enable vehicles to perform automatic lane change amidst the random traffic flow on highways, this paper introduces a decision-making and control method for vehicle lane-change based on Model Predictive Control (MPC).","This approach divides the driving control of vehicles on highways into two parts: lane-change decision and lane-change control, both of which are solved using the MPC method.","In the lane-change decision module, the minimum driving costs for each lane are computed and compared by solving the MPC problem to make lane-change decisions.","In the lane-change control module, a dynamic bicycle model is incorporated, and a multi-objective cost function is designed to obtain the optimal control inputs for the lane-change process.","The proposed lane-change decision and control methods are simulated and validated within the SUMO platform under random highway traffic conditions."],"url":"http://arxiv.org/abs/2402.17524v1","category":"eess.SY"}
{"created":"2024-02-27 13:46:45","title":"Thermodynamics-informed super-resolution of scarce temporal dynamics data","abstract":"We present a method to increase the resolution of measurements of a physical system and subsequently predict its time evolution using thermodynamics-aware neural networks. Our method uses adversarial autoencoders, which reduce the dimensionality of the full order model to a set of latent variables that are enforced to match a prior, for example a normal distribution. Adversarial autoencoders are seen as generative models, and they can be trained to generate high-resolution samples from low-resoution inputs, meaning they can address the so-called super-resolution problem. Then, a second neural network is trained to learn the physical structure of the latent variables and predict their temporal evolution. This neural network is known as an structure-preserving neural network. It learns the metriplectic-structure of the system and applies a physical bias to ensure that the first and second principles of thermodynamics are fulfilled. The integrated trajectories are decoded to their original dimensionality, as well as to the higher dimensionality space produced by the adversarial autoencoder and they are compared to the ground truth solution. The method is tested with two examples of flow over a cylinder, where the fluid properties are varied between both examples.","sentences":["We present a method to increase the resolution of measurements of a physical system and subsequently predict its time evolution using thermodynamics-aware neural networks.","Our method uses adversarial autoencoders, which reduce the dimensionality of the full order model to a set of latent variables that are enforced to match a prior, for example a normal distribution.","Adversarial autoencoders are seen as generative models, and they can be trained to generate high-resolution samples from low-resoution inputs, meaning they can address the so-called super-resolution problem.","Then, a second neural network is trained to learn the physical structure of the latent variables and predict their temporal evolution.","This neural network is known as an structure-preserving neural network.","It learns the metriplectic-structure of the system and applies a physical bias to ensure that the first and second principles of thermodynamics are fulfilled.","The integrated trajectories are decoded to their original dimensionality, as well as to the higher dimensionality space produced by the adversarial autoencoder and they are compared to the ground truth solution.","The method is tested with two examples of flow over a cylinder, where the fluid properties are varied between both examples."],"url":"http://arxiv.org/abs/2402.17506v1","category":"physics.comp-ph"}
{"created":"2024-02-27 13:12:18","title":"Bit Rate Matching Algorithm Optimization in JPEG-AI Verification Model","abstract":"The research on neural network (NN) based image compression has shown superior performance compared to classical compression frameworks. Unlike the hand-engineered transforms in the classical frameworks, NN-based models learn the non-linear transforms providing more compact bit representations, and achieve faster coding speed on parallel devices over their classical counterparts. Those properties evoked the attention of both scientific and industrial communities, resulting in the standardization activity JPEG-AI. The verification model for the standardization process of JPEG-AI is already in development and has surpassed the advanced VVC intra codec. To generate reconstructed images with the desired bits per pixel and assess the BD-rate performance of both the JPEG-AI verification model and VVC intra, bit rate matching is employed. However, the current state of the JPEG-AI verification model experiences significant slowdowns during bit rate matching, resulting in suboptimal performance due to an unsuitable model. The proposed methodology offers a gradual algorithmic optimization for matching bit rates, resulting in a fourfold acceleration and over 1% improvement in BD-rate at the base operation point. At the high operation point, the acceleration increases up to sixfold.","sentences":["The research on neural network (NN) based image compression has shown superior performance compared to classical compression frameworks.","Unlike the hand-engineered transforms in the classical frameworks, NN-based models learn the non-linear transforms providing more compact bit representations, and achieve faster coding speed on parallel devices over their classical counterparts.","Those properties evoked the attention of both scientific and industrial communities, resulting in the standardization activity JPEG-AI.","The verification model for the standardization process of JPEG-AI is already in development and has surpassed the advanced VVC intra codec.","To generate reconstructed images with the desired bits per pixel and assess the BD-rate performance of both the JPEG-AI verification model and VVC intra, bit rate matching is employed.","However, the current state of the JPEG-AI verification model experiences significant slowdowns during bit rate matching, resulting in suboptimal performance due to an unsuitable model.","The proposed methodology offers a gradual algorithmic optimization for matching bit rates, resulting in a fourfold acceleration and over 1% improvement in BD-rate at the base operation point.","At the high operation point, the acceleration increases up to sixfold."],"url":"http://arxiv.org/abs/2402.17487v1","category":"cs.CV"}
{"created":"2024-02-27 13:08:47","title":"AlignMiF: Geometry-Aligned Multimodal Implicit Field for LiDAR-Camera Joint Synthesis","abstract":"Neural implicit fields have been a de facto standard in novel view synthesis. Recently, there exist some methods exploring fusing multiple modalities within a single field, aiming to share implicit features from different modalities to enhance reconstruction performance. However, these modalities often exhibit misaligned behaviors: optimizing for one modality, such as LiDAR, can adversely affect another, like camera performance, and vice versa. In this work, we conduct comprehensive analyses on the multimodal implicit field of LiDAR-camera joint synthesis, revealing the underlying issue lies in the misalignment of different sensors. Furthermore, we introduce AlignMiF, a geometrically aligned multimodal implicit field with two proposed modules: Geometry-Aware Alignment (GAA) and Shared Geometry Initialization (SGI). These modules effectively align the coarse geometry across different modalities, significantly enhancing the fusion process between LiDAR and camera data. Through extensive experiments across various datasets and scenes, we demonstrate the effectiveness of our approach in facilitating better interaction between LiDAR and camera modalities within a unified neural field. Specifically, our proposed AlignMiF, achieves remarkable improvement over recent implicit fusion methods (+2.01 and +3.11 image PSNR on the KITTI-360 and Waymo datasets) and consistently surpasses single modality performance (13.8% and 14.2% reduction in LiDAR Chamfer Distance on the respective datasets).","sentences":["Neural implicit fields have been a de facto standard in novel view synthesis.","Recently, there exist some methods exploring fusing multiple modalities within a single field, aiming to share implicit features from different modalities to enhance reconstruction performance.","However, these modalities often exhibit misaligned behaviors: optimizing for one modality, such as LiDAR, can adversely affect another, like camera performance, and vice versa.","In this work, we conduct comprehensive analyses on the multimodal implicit field of LiDAR-camera joint synthesis, revealing the underlying issue lies in the misalignment of different sensors.","Furthermore, we introduce AlignMiF, a geometrically aligned multimodal implicit field with two proposed modules: Geometry-Aware Alignment (GAA) and Shared Geometry Initialization (SGI).","These modules effectively align the coarse geometry across different modalities, significantly enhancing the fusion process between LiDAR and camera data.","Through extensive experiments across various datasets and scenes, we demonstrate the effectiveness of our approach in facilitating better interaction between LiDAR and camera modalities within a unified neural field.","Specifically, our proposed AlignMiF, achieves remarkable improvement over recent implicit fusion methods (+2.01 and +3.11 image PSNR on the KITTI-360 and Waymo datasets) and consistently surpasses single modality performance (13.8% and 14.2% reduction in LiDAR Chamfer Distance on the respective datasets)."],"url":"http://arxiv.org/abs/2402.17483v1","category":"cs.CV"}
{"created":"2024-02-27 12:55:36","title":"A Holistic Approach for Bitcoin Confirmation Times & Optimal Fee Selection","abstract":"Bitcoin is currently subject to a significant pay-for-speed trade-off. This is caused by lengthy and highly variable transaction confirmation times, especially during times of congestion. Users can reduce their transaction confirmation times by increasing their transaction fee. In this paper, based on the inner workings of Bitcoin, we propose a model-based approach (based on the Cram\\'er-Lundberg model) that can be used to determine the optimal fee, via, for example, the mean or quantiles, and models accurately the confirmation time distribution for a given fee. The proposed model is highly suitable as it arises as the limiting model for the mempool process (that tracks the unconfirmed transactions), which we rigorously show via a fluid limit and we extend this to the diffusion limit (an approximation of the Cram\\'er-Lundberg model for fast computations in highly congested instances). We also propose methods (incorporating the real-time data) to estimate the model parameters, thereby combining model and data-driven approaches. The model-based approach is validated on real-world data and the resulting transaction fees outperform, in most instances, the data-driven ones.","sentences":["Bitcoin is currently subject to a significant pay-for-speed trade-off.","This is caused by lengthy and highly variable transaction confirmation times, especially during times of congestion.","Users can reduce their transaction confirmation times by increasing their transaction fee.","In this paper, based on the inner workings of Bitcoin, we propose a model-based approach (based on the Cram\\'er-Lundberg model) that can be used to determine the optimal fee, via, for example, the mean or quantiles, and models accurately the confirmation time distribution for a given fee.","The proposed model is highly suitable as it arises as the limiting model for the mempool process (that tracks the unconfirmed transactions), which we rigorously show via a fluid limit and we extend this to the diffusion limit (an approximation of the Cram\\'er-Lundberg model for fast computations in highly congested instances).","We also propose methods (incorporating the real-time data) to estimate the model parameters, thereby combining model and data-driven approaches.","The model-based approach is validated on real-world data and the resulting transaction fees outperform, in most instances, the data-driven ones."],"url":"http://arxiv.org/abs/2402.17474v1","category":"math.PR"}
{"created":"2024-02-27 12:48:31","title":"Noether inequality for irregular threefolds of general type","abstract":"Let $X$ be a smooth irregular $3$-fold of general type over $\\mathbb{C}$. We prove that the optimal Noether inequality $$ \\mathrm{vol}(X) \\ge \\frac{4}{3}p_g(X) $$ holds if $p_g(X) \\ge 16$ or if $X$ has a Gorenstein minimal model. Moreover, when $X$ attains the equality and $p_g(X) \\ge 16$, its canonical model can be explicitly described.","sentences":["Let $X$ be a smooth irregular $3$-fold of general type over $\\mathbb{C}$. We prove that the optimal Noether inequality $$ \\mathrm{vol}(X) \\ge \\frac{4}{3}p_g(X) $$ holds if $p_g(X) \\ge 16$ or if $X$ has a Gorenstein minimal model.","Moreover, when $X$ attains the equality and $p_g(X) \\ge 16$, its canonical model can be explicitly described."],"url":"http://arxiv.org/abs/2402.17468v1","category":"math.AG"}
{"created":"2024-02-27 12:39:22","title":"On the upper and lower covariances under multiple probabilities","abstract":"In this paper, we define the upper (resp. lower) covariance under multiple probabilities via a corresponding max-min-max (resp. min-max-min) optimization problem and the related properties of covariances are obtained. In particular, we propose a fast algorithm of calculation for upper and lower covariances under the finite number of probabilities. As an application, our algorithm can be used to solve a class of quadratic programming problem exactly, and we obtain a probabilistic representation of such quadratic programming problem.","sentences":["In this paper, we define the upper (resp.","lower) covariance under multiple probabilities via a corresponding max-min-max (resp.","min-max-min) optimization problem and the related properties of covariances are obtained.","In particular, we propose a fast algorithm of calculation for upper and lower covariances under the finite number of probabilities.","As an application, our algorithm can be used to solve a class of quadratic programming problem exactly, and we obtain a probabilistic representation of such quadratic programming problem."],"url":"http://arxiv.org/abs/2402.17462v1","category":"math.PR"}
{"created":"2024-02-27 12:28:01","title":"Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning","abstract":"Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit ($\\mu$P and its depth extension), then some hyperparameters - such as the learning rate - exhibit transfer from small to very large models, thus reducing the cost of hyperparameter tuning. From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is remarkably consistent across very different model sizes. In this work, we find empirical evidence that learning rate transfer can be attributed to the fact that under $\\mu$P and its depth extension, the largest eigenvalue of the training loss Hessian (i.e. the sharpness) is largely independent of the width and depth of the network for a sustained period of training time. On the other hand, we show that under the neural tangent kernel (NTK) regime, the sharpness exhibits very different dynamics at different scales, thus preventing learning rate transfer. But what causes these differences in the sharpness dynamics? Through a connection between the spectra of the Hessian and the NTK matrix, we argue that the cause lies in the presence (for $\\mu$P) or progressive absence (for the NTK regime) of feature learning, which results in a different evolution of the NTK, and thus of the sharpness. We corroborate our claims with a substantial suite of experiments, covering a wide range of datasets and architectures: from ResNets and Vision Transformers trained on benchmark vision datasets to Transformers-based language models trained on WikiText","sentences":["Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit ($\\mu$P and its depth extension), then some hyperparameters - such as the learning rate - exhibit transfer from small to very large models, thus reducing the cost of hyperparameter tuning.","From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is remarkably consistent across very different model sizes.","In this work, we find empirical evidence that learning rate transfer can be attributed to the fact that under $\\mu$P and its depth extension, the largest eigenvalue of the training loss Hessian (i.e. the sharpness) is largely independent of the width and depth of the network for a sustained period of training time.","On the other hand, we show that under the neural tangent kernel (NTK) regime, the sharpness exhibits very different dynamics at different scales, thus preventing learning rate transfer.","But what causes these differences in the sharpness dynamics?","Through a connection between the spectra of the Hessian and the NTK matrix, we argue that the cause lies in the presence (for $\\mu$P) or progressive absence (for the NTK regime) of feature learning, which results in a different evolution of the NTK, and thus of the sharpness.","We corroborate our claims with a substantial suite of experiments, covering a wide range of datasets and architectures: from ResNets and Vision Transformers trained on benchmark vision datasets to Transformers-based language models trained on WikiText"],"url":"http://arxiv.org/abs/2402.17457v1","category":"cs.LG"}
{"created":"2024-02-27 11:52:49","title":"Principled Architecture-aware Scaling of Hyperparameters","abstract":"Training a high-quality deep neural network requires choosing suitable hyperparameters, which is a non-trivial and expensive process. Current works try to automatically optimize or design principles of hyperparameters, such that they can generalize to diverse unseen scenarios. However, most designs or optimization methods are agnostic to the choice of network structures, and thus largely ignore the impact of neural architectures on hyperparameters. In this work, we precisely characterize the dependence of initializations and maximal learning rates on the network architecture, which includes the network depth, width, convolutional kernel size, and connectivity patterns. By pursuing every parameter to be maximally updated with the same mean squared change in pre-activations, we can generalize our initialization and learning rates across MLPs (multi-layer perception) and CNNs (convolutional neural network) with sophisticated graph topologies. We verify our principles with comprehensive experiments. More importantly, our strategy further sheds light on advancing current benchmarks for architecture design. A fair comparison of AutoML algorithms requires accurate network rankings. However, we demonstrate that network rankings can be easily changed by better training networks in benchmarks with our architecture-aware learning rates and initialization.","sentences":["Training a high-quality deep neural network requires choosing suitable hyperparameters, which is a non-trivial and expensive process.","Current works try to automatically optimize or design principles of hyperparameters, such that they can generalize to diverse unseen scenarios.","However, most designs or optimization methods are agnostic to the choice of network structures, and thus largely ignore the impact of neural architectures on hyperparameters.","In this work, we precisely characterize the dependence of initializations and maximal learning rates on the network architecture, which includes the network depth, width, convolutional kernel size, and connectivity patterns.","By pursuing every parameter to be maximally updated with the same mean squared change in pre-activations, we can generalize our initialization and learning rates across MLPs (multi-layer perception) and CNNs (convolutional neural network) with sophisticated graph topologies.","We verify our principles with comprehensive experiments.","More importantly, our strategy further sheds light on advancing current benchmarks for architecture design.","A fair comparison of AutoML algorithms requires accurate network rankings.","However, we demonstrate that network rankings can be easily changed by better training networks in benchmarks with our architecture-aware learning rates and initialization."],"url":"http://arxiv.org/abs/2402.17440v1","category":"cs.LG"}
{"created":"2024-02-27 11:47:02","title":"Nuclear spin relaxation mediated by donor-bound and free electrons in wide CdTe quantum wells","abstract":"The nuclear spin systems in CdTe/(Cd,Zn)Te and CdTe/(Cd,Mg)Te quantum wells (QW) are studied using a multistage technique combining optical pumping and Hanle effect-based detection. The samples demonstrate drastically different nuclear spin dynamics in zero and weak magnetic fields. In CdTe/(Cd,Zn)Te, the nuclear spin relaxation time is found to strongly increase with the magnetic field, growing from 3 s in zero field to tens of seconds in a field of 25 G. In CdTe/(Cd,Mg)Te the relaxation is an order of magnitude slower, and it is field-independent up to at least 70 G. The differences are attributed to the nuclear spin relaxation being mediated by different kinds of resident electrons in these QWs. In CdTe/(Cd,Mg)Te, a residual electron gas trapped in the QW largely determines the relaxation dynamics. In CdTe/(Cd,Zn)Te, the fast relaxation in zero field is due to interaction with localized donor-bound electrons. Nuclear spin diffusion barriers form around neutral donors when the external magnetic field exceeds the local nuclear field, which is about $B_L\\approx $0.4 G in CdTe. This inhibits nuclear spin diffusion towards the donors, slowing down relaxation. These findings are supported by theoretical modeling. In particular, we show that the formation of the diffusion barrier is made possible by several features specific to CdTe: (i) the large donor binding energy (about 10 meV), (ii) the low abundance of magnetic isotopes (only $\\approx$30% of nuclei have nonzero spin), and (iii) the absence of nuclear quadrupole interactions between nuclei. The two latter properties are also favorable to nuclear spin cooling via optical pumping followed by adiabatic demagnetization. Under non-optimized conditions we have reached sub-microkelvin nuclear spin temperatures in both samples, lower than all previous results obtained in GaAs.","sentences":["The nuclear spin systems in CdTe/(Cd,Zn)Te and CdTe/(Cd,Mg)Te quantum wells (QW) are studied using a multistage technique combining optical pumping and Hanle effect-based detection.","The samples demonstrate drastically different nuclear spin dynamics in zero and weak magnetic fields.","In CdTe/(Cd,Zn)Te, the nuclear spin relaxation time is found to strongly increase with the magnetic field, growing from 3 s in zero field to tens of seconds in a field of 25 G. In CdTe/(Cd,Mg)Te the relaxation is an order of magnitude slower, and it is field-independent up to at least 70 G. The differences are attributed to the nuclear spin relaxation being mediated by different kinds of resident electrons in these QWs.","In CdTe/(Cd,Mg)Te, a residual electron gas trapped in the QW largely determines the relaxation dynamics.","In CdTe/(Cd,Zn)Te, the fast relaxation in zero field is due to interaction with localized donor-bound electrons.","Nuclear spin diffusion barriers form around neutral donors when the external magnetic field exceeds the local nuclear field, which is about $B_L\\approx $0.4 G in CdTe.","This inhibits nuclear spin diffusion towards the donors, slowing down relaxation.","These findings are supported by theoretical modeling.","In particular, we show that the formation of the diffusion barrier is made possible by several features specific to CdTe: (i) the large donor binding energy (about 10 meV), (ii) the low abundance of magnetic isotopes (only $\\approx$30% of nuclei have nonzero spin), and (iii) the absence of nuclear quadrupole interactions between nuclei.","The two latter properties are also favorable to nuclear spin cooling via optical pumping followed by adiabatic demagnetization.","Under non-optimized conditions we have reached sub-microkelvin nuclear spin temperatures in both samples, lower than all previous results obtained in GaAs."],"url":"http://arxiv.org/abs/2402.17435v2","category":"cond-mat.mes-hall"}
{"created":"2024-02-27 11:40:50","title":"VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction","abstract":"Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed. While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations. To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion. These cells are merged into a complete scene after parallel optimization. We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images. Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering.","sentences":["Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed.","While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations.","To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting.","We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion.","These cells are merged into a complete scene after parallel optimization.","We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images.","Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering."],"url":"http://arxiv.org/abs/2402.17427v1","category":"cs.CV"}
{"created":"2024-02-27 10:27:01","title":"RACP: Risk-Aware Contingency Planning with Multi-Modal Predictions","abstract":"For an autonomous vehicle to operate reliably within real-world traffic scenarios, it is imperative to assess the repercussions of its prospective actions by anticipating the uncertain intentions exhibited by other participants in the traffic environment. Driven by the pronounced multi-modal nature of human driving behavior, this paper presents an approach that leverages Bayesian beliefs over the distribution of potential policies of other road users to construct a novel risk-aware probabilistic motion planning framework. In particular, we propose a novel contingency planner that outputs long-term contingent plans conditioned on multiple possible intents for other actors in the traffic scene. The Bayesian belief is incorporated into the optimization cost function to influence the behavior of the short-term plan based on the likelihood of other agents' policies. Furthermore, a probabilistic risk metric is employed to fine-tune the balance between efficiency and robustness. Through a series of closed-loop safety-critical simulated traffic scenarios shared with human-driven vehicles, we demonstrate the practical efficacy of our proposed approach that can handle multi-vehicle scenarios.","sentences":["For an autonomous vehicle to operate reliably within real-world traffic scenarios, it is imperative to assess the repercussions of its prospective actions by anticipating the uncertain intentions exhibited by other participants in the traffic environment.","Driven by the pronounced multi-modal nature of human driving behavior, this paper presents an approach that leverages Bayesian beliefs over the distribution of potential policies of other road users to construct a novel risk-aware probabilistic motion planning framework.","In particular, we propose a novel contingency planner that outputs long-term contingent plans conditioned on multiple possible intents for other actors in the traffic scene.","The Bayesian belief is incorporated into the optimization cost function to influence the behavior of the short-term plan based on the likelihood of other agents' policies.","Furthermore, a probabilistic risk metric is employed to fine-tune the balance between efficiency and robustness.","Through a series of closed-loop safety-critical simulated traffic scenarios shared with human-driven vehicles, we demonstrate the practical efficacy of our proposed approach that can handle multi-vehicle scenarios."],"url":"http://arxiv.org/abs/2402.17387v1","category":"cs.RO"}
{"created":"2024-02-27 10:15:25","title":"Warm-Starting the VQE with Approximate Complex Amplitude Encoding","abstract":"The Variational Quantum Eigensolver (VQE) is a Variational Quantum Algorithm (VQA) to determine the ground state of quantum-mechanical systems. As a VQA, it makes use of a classical computer to optimize parameter values for its quantum circuit. However, each iteration of the VQE requires a multitude of measurements, and the optimization is subject to obstructions, such as barren plateaus, local minima, and subsequently slow convergence. We propose a warm-starting technique, that utilizes an approximation to generate beneficial initial parameter values for the VQE aiming to mitigate these effects. The warm-start is based on Approximate Complex Amplitude Encoding, a VQA using fidelity estimations from classical shadows to encode complex amplitude vectors into quantum states. Such warm-starts open the path to fruitful combinations of classical approximation algorithms and quantum algorithms. In particular, the evaluation of our approach shows that the warm-started VQE reaches higher quality solutions earlier than the original VQE.","sentences":["The Variational Quantum Eigensolver (VQE) is a Variational Quantum Algorithm (VQA) to determine the ground state of quantum-mechanical systems.","As a VQA, it makes use of a classical computer to optimize parameter values for its quantum circuit.","However, each iteration of the VQE requires a multitude of measurements, and the optimization is subject to obstructions, such as barren plateaus, local minima, and subsequently slow convergence.","We propose a warm-starting technique, that utilizes an approximation to generate beneficial initial parameter values for the VQE aiming to mitigate these effects.","The warm-start is based on Approximate Complex Amplitude Encoding, a VQA using fidelity estimations from classical shadows to encode complex amplitude vectors into quantum states.","Such warm-starts open the path to fruitful combinations of classical approximation algorithms and quantum algorithms.","In particular, the evaluation of our approach shows that the warm-started VQE reaches higher quality solutions earlier than the original VQE."],"url":"http://arxiv.org/abs/2402.17378v1","category":"quant-ph"}
{"created":"2024-02-27 10:13:30","title":"Accelerating Diffusion Sampling with Optimized Time Steps","abstract":"Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than $15$ seconds. Our extensive experiments on both unconditional and conditional sampling using pixel- and latent-space DPMs demonstrate that, when combined with the state-of-the-art sampling method UniPC, our optimized time steps significantly improve image generation performance in terms of FID scores for datasets such as CIFAR-10 and ImageNet, compared to using uniform time steps.","sentences":["Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps.","Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps.","While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps.","To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs.","This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver.","It can be efficiently solved using the constrained trust region method, taking less than $15$ seconds.","Our extensive experiments on both unconditional and conditional sampling using pixel- and latent-space DPMs demonstrate that, when combined with the state-of-the-art sampling method UniPC, our optimized time steps significantly improve image generation performance in terms of FID scores for datasets such as CIFAR-10 and ImageNet, compared to using uniform time steps."],"url":"http://arxiv.org/abs/2402.17376v1","category":"cs.CV"}
{"created":"2024-02-27 10:12:47","title":"Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control","abstract":"Integral reinforcement learning (IntRL) demands the precise computation of the utility function's integral at its policy evaluation (PEV) stage. This is achieved through quadrature rules, which are weighted sums of utility functions evaluated from state samples obtained in discrete time. Our research reveals a critical yet underexplored phenomenon: the choice of the computational method -- in this case, the quadrature rule -- can significantly impact control performance. This impact is traced back to the fact that computational errors introduced in the PEV stage can affect the policy iteration's convergence behavior, which in turn affects the learned controller. To elucidate how computation impacts control, we draw a parallel between IntRL's policy iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation. In this light, computational error in PEV manifests as an extra error term in each iteration of Newton's method, with its upper bound proportional to the computational error. Further, we demonstrate that when the utility function resides in a reproducing kernel Hilbert space (RKHS), the optimal quadrature is achievable by employing Bayesian quadrature with the RKHS-inducing kernel function. We prove that the local convergence rates for IntRL using the trapezoidal rule and Bayesian quadrature with a Mat\\'ern kernel to be $O(N^{-2})$ and $O(N^{-b})$, where $N$ is the number of evenly-spaced samples and $b$ is the Mat\\'ern kernel's smoothness parameter. These theoretical findings are finally validated by two canonical control tasks.","sentences":["Integral reinforcement learning (IntRL) demands the precise computation of the utility function's integral at its policy evaluation (PEV) stage.","This is achieved through quadrature rules, which are weighted sums of utility functions evaluated from state samples obtained in discrete time.","Our research reveals a critical yet underexplored phenomenon: the choice of the computational method -- in this case, the quadrature rule -- can significantly impact control performance.","This impact is traced back to the fact that computational errors introduced in the PEV stage can affect the policy iteration's convergence behavior, which in turn affects the learned controller.","To elucidate how computation impacts control, we draw a parallel between IntRL's policy iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation.","In this light, computational error in PEV manifests as an extra error term in each iteration of Newton's method, with its upper bound proportional to the computational error.","Further, we demonstrate that when the utility function resides in a reproducing kernel Hilbert space (RKHS), the optimal quadrature is achievable by employing Bayesian quadrature with the RKHS-inducing kernel function.","We prove that the local convergence rates for IntRL using the trapezoidal rule and Bayesian quadrature with a Mat\\'ern kernel to be $O(N^{-2})$ and $O(N^{-b})$, where $N$ is the number of evenly-spaced samples and $b$ is the Mat\\'ern kernel's smoothness parameter.","These theoretical findings are finally validated by two canonical control tasks."],"url":"http://arxiv.org/abs/2402.17375v1","category":"eess.SY"}
{"created":"2024-02-27 10:02:12","title":"Superconducting Electronic Device Response Linearity Calculation Method Using Discrete Fourier Analysis","abstract":"A method for estimating the linearity of superconducting electronic devices is discussed. The method allows the obtaining of expected spurious-free dynamic range values based on the results of preliminary numerical modeling and solving optimization problems. A description of the method and an analysis of measurement errors are provided.","sentences":["A method for estimating the linearity of superconducting electronic devices is discussed.","The method allows the obtaining of expected spurious-free dynamic range values based on the results of preliminary numerical modeling and solving optimization problems.","A description of the method and an analysis of measurement errors are provided."],"url":"http://arxiv.org/abs/2402.17367v1","category":"cond-mat.supr-con"}
{"created":"2024-02-27 09:41:59","title":"ICP-Flow: LiDAR Scene Flow Estimation with ICP","abstract":"Scene flow characterizes the 3D motion between two LiDAR scans captured by an autonomous vehicle at nearby timesteps. Prevalent methods consider scene flow as point-wise unconstrained flow vectors that can be learned by either large-scale training beforehand or time-consuming optimization at inference. However, these methods do not take into account that objects in autonomous driving often move rigidly. We incorporate this rigid-motion assumption into our design, where the goal is to associate objects over scans and then estimate the locally rigid transformations. We propose ICP-Flow, a learning-free flow estimator. The core of our design is the conventional Iterative Closest Point (ICP) algorithm, which aligns the objects over time and outputs the corresponding rigid transformations. Crucially, to aid ICP, we propose a histogram-based initialization that discovers the most likely translation, thus providing a good starting point for ICP. The complete scene flow is then recovered from the rigid transformations. We outperform state-of-the-art baselines, including supervised models, on the Waymo dataset and perform competitively on Argoverse-v2 and nuScenes. Further, we train a feedforward neural network, supervised by the pseudo labels from our model, and achieve top performance among all models capable of real-time inference. We validate the advantage of our model on scene flow estimation with longer temporal gaps, up to 0.5 seconds where other models fail to deliver meaningful results.","sentences":["Scene flow characterizes the 3D motion between two LiDAR scans captured by an autonomous vehicle at nearby timesteps.","Prevalent methods consider scene flow as point-wise unconstrained flow vectors that can be learned by either large-scale training beforehand or time-consuming optimization at inference.","However, these methods do not take into account that objects in autonomous driving often move rigidly.","We incorporate this rigid-motion assumption into our design, where the goal is to associate objects over scans and then estimate the locally rigid transformations.","We propose ICP-Flow, a learning-free flow estimator.","The core of our design is the conventional Iterative Closest Point (ICP) algorithm, which aligns the objects over time and outputs the corresponding rigid transformations.","Crucially, to aid ICP, we propose a histogram-based initialization that discovers the most likely translation, thus providing a good starting point for ICP.","The complete scene flow is then recovered from the rigid transformations.","We outperform state-of-the-art baselines, including supervised models, on the Waymo dataset and perform competitively on Argoverse-v2 and nuScenes.","Further, we train a feedforward neural network, supervised by the pseudo labels from our model, and achieve top performance among all models capable of real-time inference.","We validate the advantage of our model on scene flow estimation with longer temporal gaps, up to 0.5 seconds where other models fail to deliver meaningful results."],"url":"http://arxiv.org/abs/2402.17351v1","category":"cs.CV"}
{"created":"2024-02-27 09:33:51","title":"Ab initio simulations of neutron stars' oblique electrospheres with realistic neutron star parameters","abstract":"Electrospheres are environments with the same origin as pulsars; a highly magnetized rotating neutron star. In pulsars, a cascade of electron-positron pair creation enriches the plasma. The plasma surrounding an electrosphere consists only of particles that have escaped from the neutron star's surface. Electrospheres with a magnetic axis aligned with the rotation axis have been well described for decades. Models of electrospheres with an oblique magnetic axis relative to the rotation axis have resisted most theoretical investigations. Some electrospheres and pulsars have been simulated using particle-in-cell codes, but the numerical constraints did not allow the use of realistic neutron star parameters. We aimed to develop a numerical simulation code optimized for understanding the physics of electrospheres and pulsars, with realistic neutron star parameters. As a first step, presented in this paper, we focused on the simulation of oblique electrospheres with realistic physical parameters. A specific code was developed for the computation of stationary solutions. The resolution of Maxwell's equations was based on spectral methods. Particle motions included their finite inertia. No hypothesis was made in relation to the force-free behavior of the electrospheric plasma. The numerical code is called Pulsar ARoMa (pulsar asymmetric rotating magnetosphere). Various numerical simulations were conducted using realistic neutron star parameters. We find that oblique electrospheres possess the same global structure as aligned force-free electrospheres, with two domes of electrons and a torus of positively charged particles. The domes are not centered on the magnetic axis; nor are they symmetric. Yet, the solutions do not exhibit a force-free behavior. The simulations performed with the Pulsar ARoMa code require modest resources and little computing time. This code will be upgraded for more ambitious investigations into pulsar physics.","sentences":["Electrospheres are environments with the same origin as pulsars; a highly magnetized rotating neutron star.","In pulsars, a cascade of electron-positron pair creation enriches the plasma.","The plasma surrounding an electrosphere consists only of particles that have escaped from the neutron star's surface.","Electrospheres with a magnetic axis aligned with the rotation axis have been well described for decades.","Models of electrospheres with an oblique magnetic axis relative to the rotation axis have resisted most theoretical investigations.","Some electrospheres and pulsars have been simulated using particle-in-cell codes, but the numerical constraints did not allow the use of realistic neutron star parameters.","We aimed to develop a numerical simulation code optimized for understanding the physics of electrospheres and pulsars, with realistic neutron star parameters.","As a first step, presented in this paper, we focused on the simulation of oblique electrospheres with realistic physical parameters.","A specific code was developed for the computation of stationary solutions.","The resolution of Maxwell's equations was based on spectral methods.","Particle motions included their finite inertia.","No hypothesis was made in relation to the force-free behavior of the electrospheric plasma.","The numerical code is called Pulsar ARoMa (pulsar asymmetric rotating magnetosphere).","Various numerical simulations were conducted using realistic neutron star parameters.","We find that oblique electrospheres possess the same global structure as aligned force-free electrospheres, with two domes of electrons and a torus of positively charged particles.","The domes are not centered on the magnetic axis; nor are they symmetric.","Yet, the solutions do not exhibit a force-free behavior.","The simulations performed with the Pulsar ARoMa code require modest resources and little computing time.","This code will be upgraded for more ambitious investigations into pulsar physics."],"url":"http://arxiv.org/abs/2402.17348v1","category":"astro-ph.HE"}
{"created":"2024-02-27 09:23:13","title":"Enhanced Bayesian Optimization via Preferential Modeling of Abstract Properties","abstract":"Experimental (design) optimization is a key driver in designing and discovering new products and processes. Bayesian Optimization (BO) is an effective tool for optimizing expensive and black-box experimental design processes. While Bayesian optimization is a principled data-driven approach to experimental optimization, it learns everything from scratch and could greatly benefit from the expertise of its human (domain) experts who often reason about systems at different abstraction levels using physical properties that are not necessarily directly measured (or measurable). In this paper, we propose a human-AI collaborative Bayesian framework to incorporate expert preferences about unmeasured abstract properties into the surrogate modeling to further boost the performance of BO. We provide an efficient strategy that can also handle any incorrect/misleading expert bias in preferential judgments. We discuss the convergence behavior of our proposed framework. Our experimental results involving synthetic functions and real-world datasets show the superiority of our method against the baselines.","sentences":["Experimental (design) optimization is a key driver in designing and discovering new products and processes.","Bayesian Optimization (BO) is an effective tool for optimizing expensive and black-box experimental design processes.","While Bayesian optimization is a principled data-driven approach to experimental optimization, it learns everything from scratch and could greatly benefit from the expertise of its human (domain) experts who often reason about systems at different abstraction levels using physical properties that are not necessarily directly measured (or measurable).","In this paper, we propose a human-AI collaborative Bayesian framework to incorporate expert preferences about unmeasured abstract properties into the surrogate modeling to further boost the performance of BO.","We provide an efficient strategy that can also handle any incorrect/misleading expert bias in preferential judgments.","We discuss the convergence behavior of our proposed framework.","Our experimental results involving synthetic functions and real-world datasets show the superiority of our method against the baselines."],"url":"http://arxiv.org/abs/2402.17343v1","category":"cs.LG"}
{"created":"2024-02-27 09:13:27","title":"SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned Latents","abstract":"Pedestrian trajectory prediction is the key technology in many applications for providing insights into human behavior and anticipating human future motions. Most existing empirical models are explicitly formulated by observed human behaviors using explicable mathematical terms with a deterministic nature, while recent work has focused on developing hybrid models combined with learning-based techniques for powerful expressiveness while maintaining explainability. However, the deterministic nature of the learned steering behaviors from the empirical models limits the models' practical performance. To address this issue, this work proposes the social conditional variational autoencoder (SocialCVAE) for predicting pedestrian trajectories, which employs a CVAE to explore behavioral uncertainty in human motion decisions. SocialCVAE learns socially reasonable motion randomness by utilizing a socially explainable interaction energy map as the CVAE's condition, which illustrates the future occupancy of each pedestrian's local neighborhood area. The energy map is generated using an energy-based interaction model, which anticipates the energy cost (i.e., repulsion intensity) of pedestrians' interactions with neighbors. Experimental results on two public benchmarks including 25 scenes demonstrate that SocialCVAE significantly improves prediction accuracy compared with the state-of-the-art methods, with up to 16.85% improvement in Average Displacement Error (ADE) and 69.18% improvement in Final Displacement Error (FDE).","sentences":["Pedestrian trajectory prediction is the key technology in many applications for providing insights into human behavior and anticipating human future motions.","Most existing empirical models are explicitly formulated by observed human behaviors using explicable mathematical terms with a deterministic nature, while recent work has focused on developing hybrid models combined with learning-based techniques for powerful expressiveness while maintaining explainability.","However, the deterministic nature of the learned steering behaviors from the empirical models limits the models' practical performance.","To address this issue, this work proposes the social conditional variational autoencoder (SocialCVAE) for predicting pedestrian trajectories, which employs a CVAE to explore behavioral uncertainty in human motion decisions.","SocialCVAE learns socially reasonable motion randomness by utilizing a socially explainable interaction energy map as the CVAE's condition, which illustrates the future occupancy of each pedestrian's local neighborhood area.","The energy map is generated using an energy-based interaction model, which anticipates the energy cost (i.e., repulsion intensity) of pedestrians' interactions with neighbors.","Experimental results on two public benchmarks including 25 scenes demonstrate that SocialCVAE significantly improves prediction accuracy compared with the state-of-the-art methods, with up to 16.85% improvement in Average Displacement Error (ADE) and 69.18% improvement in Final Displacement Error (FDE)."],"url":"http://arxiv.org/abs/2402.17339v1","category":"cs.CV"}
{"created":"2024-02-27 09:02:10","title":"Nodal precession of a hot Jupiter transiting the edge of a late A-type star TOI-1518","abstract":"TOI-1518b, a hot Jupiter around a late A-type star, is one of the few planetary systems that transit the edge of the stellar surface (the impact parameter $b\\sim0.9 $) among hot Jupiters around hot stars (Cabot et al. 2021). The high rotation speed of the host star ($\\sim85$ km s$^{-1}$) and the nearly polar orbit of the planet ($\\sim 120$ deg) may cause a nodal precession. In this study, we report the nodal precession undergone by TOI-1518\\,b. This system is the fourth planetary system in which nodal precession is detected. We investigate the time change in $b$ from the photometric data of TOI-1518 acquired in 2019 and 2022 with TESS and from the spectral transit data of TOI-1518b obtained in 2020 with two high-dispersion spectrographs; CARMENES and EXPRES. We find that the value of $b$ is decreasing with $db/dt=-0.0116\\pm0.0036$\\,year$^{-1}$, indicating that the transit trajectory is moving toward the center of the stellar surface. We also estimate the minimum value of the quadrupole mass moment of TOI-1518 $J_{2,\\mathrm{min}}=4.41\\times 10^{-5}$ and the logarithm of the Love number of TOI-1518 $\\log{k_2}= -2.17\\pm 0.33$ from the nodal precession.","sentences":["TOI-1518b, a hot Jupiter around a late A-type star, is one of the few planetary systems that transit the edge of the stellar surface (the impact parameter $b\\sim0.9 $) among hot Jupiters around hot stars (Cabot et al. 2021).","The high rotation speed of the host star ($\\sim85$ km s$^{-1}$) and the nearly polar orbit of the planet ($\\sim 120$ deg) may cause a nodal precession.","In this study, we report the nodal precession undergone by TOI-1518\\,b.","This system is the fourth planetary system in which nodal precession is detected.","We investigate the time change in $b$ from the photometric data of TOI-1518 acquired in 2019 and 2022 with TESS and from the spectral transit data of TOI-1518b obtained in 2020 with two high-dispersion spectrographs; CARMENES and EXPRES.","We find that the value of $b$ is decreasing with $db/dt=-0.0116\\pm0.0036$\\,year$^{-1}$, indicating that the transit trajectory is moving toward the center of the stellar surface.","We also estimate the minimum value of the quadrupole mass moment of TOI-1518 $J_{2,\\mathrm{min}}=4.41\\times 10^{-5}$ and the logarithm of the Love number of TOI-1518 $\\log{k_2}= -2.17\\pm 0.33$ from the nodal precession."],"url":"http://arxiv.org/abs/2402.17325v1","category":"astro-ph.EP"}
