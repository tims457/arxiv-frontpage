{"created":"2024-06-12 17:59:52","title":"ICE-G: Image Conditional Editing of 3D Gaussian Splats","abstract":"Recently many techniques have emerged to create high quality 3D assets and scenes. When it comes to editing of these objects, however, existing approaches are either slow, compromise on quality, or do not provide enough customization. We introduce a novel approach to quickly edit a 3D model from a single reference view. Our technique first segments the edit image, and then matches semantically corresponding regions across chosen segmented dataset views using DINO features. A color or texture change from a particular region of the edit image can then be applied to other views automatically in a semantically sensible manner. These edited views act as an updated dataset to further train and re-style the 3D scene. The end-result is therefore an edited 3D model. Our framework enables a wide variety of editing tasks such as manual local edits, correspondence based style transfer from any example image, and a combination of different styles from multiple example images. We use Gaussian Splats as our primary 3D representation due to their speed and ease of local editing, but our technique works for other methods such as NeRFs as well. We show through multiple examples that our method produces higher quality results while offering fine-grained control of editing. Project page: ice-gaussian.github.io","sentences":["Recently many techniques have emerged to create high quality 3D assets and scenes.","When it comes to editing of these objects, however, existing approaches are either slow, compromise on quality, or do not provide enough customization.","We introduce a novel approach to quickly edit a 3D model from a single reference view.","Our technique first segments the edit image, and then matches semantically corresponding regions across chosen segmented dataset views using DINO features.","A color or texture change from a particular region of the edit image can then be applied to other views automatically in a semantically sensible manner.","These edited views act as an updated dataset to further train and re-style the 3D scene.","The end-result is therefore an edited 3D model.","Our framework enables a wide variety of editing tasks such as manual local edits, correspondence based style transfer from any example image, and a combination of different styles from multiple example images.","We use Gaussian Splats as our primary 3D representation due to their speed and ease of local editing, but our technique works for other methods such as NeRFs as well.","We show through multiple examples that our method produces higher quality results while offering fine-grained control of editing.","Project page: ice-gaussian.github.io"],"url":"http://arxiv.org/abs/2406.08488v1","category":"cs.CV"}
{"created":"2024-06-12 17:59:36","title":"Celestial Topology, Symmetry Theories, and Evidence for a Non-SUSY D3-Brane CFT","abstract":"Symmetry Theories (SymThs) provide a flexible framework for analyzing the global categorical symmetries of a $D$-dimensional QFT$_{D}$ in terms of a $(D+1)$-dimensional bulk system SymTh$_{D+1}$. In QFTs realized via local string backgrounds, these SymThs naturally arise from dimensional reduction of the linking boundary geometry. To track possible time dependent effects we introduce a celestial generalization of the standard \"boundary at infinity\" of a SymTh. As an application of these considerations we revisit large $N$ quiver gauge theories realized by spacetime filling D3-branes probing a non-supersymmetric orbifold $\\mathbb{R}^6 / \\Gamma$. Comparing the imprint of symmetry breaking on the celestial geometry at small and large 't Hooft coupling we find evidence for an intermediate symmetry preserving conformal fixed point.","sentences":["Symmetry Theories (SymThs) provide a flexible framework for analyzing the global categorical symmetries of a $D$-dimensional QFT$_{D}$ in terms of a $(D+1)$-dimensional bulk system SymTh$_{D+1}$.","In QFTs realized via local string backgrounds, these SymThs naturally arise from dimensional reduction of the linking boundary geometry.","To track possible time dependent effects we introduce a celestial generalization of the standard \"boundary at infinity\" of a SymTh.","As an application of these considerations we revisit large $N$ quiver gauge theories realized by spacetime filling D3-branes probing a non-supersymmetric orbifold $\\mathbb{R}^6 / \\Gamma$.","Comparing the imprint of symmetry breaking on the celestial geometry at small and large 't Hooft coupling we find evidence for an intermediate symmetry preserving conformal fixed point."],"url":"http://arxiv.org/abs/2406.08485v1","category":"hep-th"}
{"created":"2024-06-12 17:59:34","title":"Exploiting the diversity of modeling methods to probe systematic biases in strong lensing analyses","abstract":"In the past decade, the diversity of strong lens modeling methods has exploded, from being purely analytical to pixelated and non-parametric, or based on deep learning. We embrace this diversity by selecting different software packages and use them to blindly model independently simulated Hubble Space Telescope imaging data. To overcome the difficulties arising from using different codes and conventions, we use the COde-independent Organized LEns STandard (COOLEST) to store, compare and release all models in a self-consistent and human-readable manner. From an ensemble of six modeling methods, we study the recovery of the lens potential parameters and properties of the reconstructed source. In particular, we simulate and infer parameters of an elliptical power-law mass distribution with external shear for the lens while each modeling method reconstructs the source differently. We find that overall, both lens and source properties are recovered reasonably well, but systematic biases arise in all methods. Interestingly, we do not observe that a single method is significantly more accurate than others, and the amount of bias largely depends on the specific lens or source property of interest. By combining posterior distributions from individual methods using equal weights, the maximal systematic biases on lens model parameters inferred from individual models are reduced by a factor of 5.4 on average. We investigate a selection of modeling effects that partly explain the observed biases, such as the cuspy nature of the background source and the accuracy of the point spread function. This work introduces, for the first time, a generic framework to compare and ease the combination of models obtained from different codes and methods, which will be key to retain accuracy in future strong lensing analyses.","sentences":["In the past decade, the diversity of strong lens modeling methods has exploded, from being purely analytical to pixelated and non-parametric, or based on deep learning.","We embrace this diversity by selecting different software packages and use them to blindly model independently simulated Hubble Space Telescope imaging data.","To overcome the difficulties arising from using different codes and conventions, we use the COde-independent Organized LEns STandard (COOLEST) to store, compare and release all models in a self-consistent and human-readable manner.","From an ensemble of six modeling methods, we study the recovery of the lens potential parameters and properties of the reconstructed source.","In particular, we simulate and infer parameters of an elliptical power-law mass distribution with external shear for the lens while each modeling method reconstructs the source differently.","We find that overall, both lens and source properties are recovered reasonably well, but systematic biases arise in all methods.","Interestingly, we do not observe that a single method is significantly more accurate than others, and the amount of bias largely depends on the specific lens or source property of interest.","By combining posterior distributions from individual methods using equal weights, the maximal systematic biases on lens model parameters inferred from individual models are reduced by a factor of 5.4 on average.","We investigate a selection of modeling effects that partly explain the observed biases, such as the cuspy nature of the background source and the accuracy of the point spread function.","This work introduces, for the first time, a generic framework to compare and ease the combination of models obtained from different codes and methods, which will be key to retain accuracy in future strong lensing analyses."],"url":"http://arxiv.org/abs/2406.08484v1","category":"astro-ph.CO"}
{"created":"2024-06-12 17:59:27","title":"Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation","abstract":"Diffusion models are the state of the art in text-to-image generation, but their perceptual variability remains understudied. In this paper, we examine how prompts affect image variability in black-box diffusion-based models. We propose W1KP, a human-calibrated measure of variability in a set of images, bootstrapped from existing image-pair perceptual distances. Current datasets do not cover recent diffusion models, thus we curate three test sets for evaluation. Our best perceptual distance outperforms nine baselines by up to 18 points in accuracy, and our calibration matches graded human judgements 78% of the time. Using W1KP, we study prompt reusability and show that Imagen prompts can be reused for 10-50 random seeds before new images become too similar to already generated images, while Stable Diffusion XL and DALL-E 3 can be reused 50-200 times. Lastly, we analyze 56 linguistic features of real prompts, finding that the prompt's length, CLIP embedding norm, concreteness, and word senses influence variability most. As far as we are aware, we are the first to analyze diffusion variability from a visuolinguistic perspective. Our project page is at http://w1kp.com","sentences":["Diffusion models are the state of the art in text-to-image generation, but their perceptual variability remains understudied.","In this paper, we examine how prompts affect image variability in black-box diffusion-based models.","We propose W1KP, a human-calibrated measure of variability in a set of images, bootstrapped from existing image-pair perceptual distances.","Current datasets do not cover recent diffusion models, thus we curate three test sets for evaluation.","Our best perceptual distance outperforms nine baselines by up to 18 points in accuracy, and our calibration matches graded human judgements 78% of the time.","Using W1KP, we study prompt reusability and show that Imagen prompts can be reused for 10-50 random seeds before new images become too similar to already generated images, while Stable Diffusion XL and DALL-E 3 can be reused 50-200 times.","Lastly, we analyze 56 linguistic features of real prompts, finding that the prompt's length, CLIP embedding norm, concreteness, and word senses influence variability most.","As far as we are aware, we are the first to analyze diffusion variability from a visuolinguistic perspective.","Our project page is at http://w1kp.com"],"url":"http://arxiv.org/abs/2406.08482v1","category":"cs.CV"}
{"created":"2024-06-12 17:59:14","title":"Linear equations with monomial constraints and decision problems in abelian-by-cyclic groups","abstract":"We show that it is undecidable whether a system of linear equations over the Laurent polynomial ring $\\mathbb{Z}[X^{\\pm}]$ admit solutions where a specified subset of variables take value in the set of monomials $\\{X^z \\mid z \\in \\mathbb{Z}\\}$. In particular, we construct a finitely presented $\\mathbb{Z}[X^{\\pm}]$-module, where it is undecidable whether a linear equation $X^{z_1} \\boldsymbol{f}_1 + \\cdots + X^{z_n} \\boldsymbol{f}_n = \\boldsymbol{f}_0$ has solutions $z_1, \\ldots, z_n \\in \\mathbb{Z}$. This contrasts the decidability of the case $n = 1$, which can be deduced from Noskov's Lemma.   As applications, we show that there exists a finitely generated abelian-by-cyclic group in which the Knapsack Problem is undecidable, and in which the problem of solving quadratic equations is also undecidable. In contrast, we show that the problem of Coset Intersection is decidable in all finitely generated abelian-by-cyclic groups.","sentences":["We show that it is undecidable whether a system of linear equations over the Laurent polynomial ring $\\mathbb{Z}[X^{\\pm}]$ admit solutions where a specified subset of variables take value in the set of monomials $\\{X^z \\mid z \\in \\mathbb{Z}\\}$. In particular, we construct a finitely presented $\\mathbb{Z}[X^{\\pm}]$-module, where it is undecidable whether a linear equation $X^{z_1} \\boldsymbol{f}_1 + \\cdots + X^{z_n} \\boldsymbol{f}_n = \\boldsymbol{f}_0$ has solutions $z_1, \\ldots, z_n","\\in \\mathbb{Z}$. This contrasts the decidability of the case $n = 1$, which can be deduced from Noskov's Lemma.   ","As applications, we show that there exists a finitely generated abelian-by-cyclic group in which the Knapsack Problem is undecidable, and in which the problem of solving quadratic equations is also undecidable.","In contrast, we show that the problem of Coset Intersection is decidable in all finitely generated abelian-by-cyclic groups."],"url":"http://arxiv.org/abs/2406.08480v1","category":"cs.SC"}
{"created":"2024-06-12 17:59:07","title":"What If We Recaption Billions of Web Images with LLaMA-3?","abstract":"Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate that semantically aligning and enriching textual descriptions of these pairs can significantly enhance model training across various vision-language tasks, particularly text-to-image generation. However, large-scale investigations in this area remain predominantly closed-source. Our paper aims to bridge this community effort, leveraging the powerful and \\textit{open-sourced} LLaMA-3, a GPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images from the DataComp-1B dataset. Our empirical results confirm that this enhanced dataset, Recap-DataComp-1B, offers substantial benefits in training advanced vision-language models. For discriminative models like CLIP, we observe enhanced zero-shot performance in cross-modal retrieval tasks. For generative models like text-to-image Diffusion Transformers, the generated images exhibit a significant improvement in alignment with users' text instructions, especially in following complex queries. Our project page is https://www.haqtu.me/Recap-Datacomp-1B/","sentences":["Web-crawled image-text pairs are inherently noisy.","Prior studies demonstrate that semantically aligning and enriching textual descriptions of these pairs can significantly enhance model training across various vision-language tasks, particularly text-to-image generation.","However, large-scale investigations in this area remain predominantly closed-source.","Our paper aims to bridge this community effort, leveraging the powerful and \\textit{open-sourced} LLaMA-3, a GPT-4 level LLM.","Our recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images from the DataComp-1B dataset.","Our empirical results confirm that this enhanced dataset, Recap-DataComp-1B, offers substantial benefits in training advanced vision-language models.","For discriminative models like CLIP, we observe enhanced zero-shot performance in cross-modal retrieval tasks.","For generative models like text-to-image Diffusion Transformers, the generated images exhibit a significant improvement in alignment with users' text instructions, especially in following complex queries.","Our project page is https://www.haqtu.me/Recap-Datacomp-1B/"],"url":"http://arxiv.org/abs/2406.08478v1","category":"cs.CV"}
{"created":"2024-06-12 17:59:04","title":"RMem: Restricted Memory Banks Improve Video Object Segmentation","abstract":"With recent video object segmentation (VOS) benchmarks evolving to challenging scenarios, we revisit a simple but overlooked strategy: restricting the size of memory banks. This diverges from the prevalent practice of expanding memory banks to accommodate extensive historical information. Our specially designed \"memory deciphering\" study offers a pivotal insight underpinning such a strategy: expanding memory banks, while seemingly beneficial, actually increases the difficulty for VOS modules to decode relevant features due to the confusion from redundant information. By restricting memory banks to a limited number of essential frames, we achieve a notable improvement in VOS accuracy. This process balances the importance and freshness of frames to maintain an informative memory bank within a bounded capacity. Additionally, restricted memory banks reduce the training-inference discrepancy in memory lengths compared with continuous expansion. This fosters new opportunities in temporal reasoning and enables us to introduce the previously overlooked \"temporal positional embedding.\" Finally, our insights are embodied in \"RMem\" (\"R\" for restricted), a simple yet effective VOS modification that excels at challenging VOS scenarios and establishes new state of the art for object state changes (on the VOST dataset) and long videos (on the Long Videos dataset). Our code and demo are available at https://restricted-memory.github.io/.","sentences":["With recent video object segmentation (VOS) benchmarks evolving to challenging scenarios, we revisit a simple but overlooked strategy: restricting the size of memory banks.","This diverges from the prevalent practice of expanding memory banks to accommodate extensive historical information.","Our specially designed \"memory deciphering\" study offers a pivotal insight underpinning such a strategy: expanding memory banks, while seemingly beneficial, actually increases the difficulty for VOS modules to decode relevant features due to the confusion from redundant information.","By restricting memory banks to a limited number of essential frames, we achieve a notable improvement in VOS accuracy.","This process balances the importance and freshness of frames to maintain an informative memory bank within a bounded capacity.","Additionally, restricted memory banks reduce the training-inference discrepancy in memory lengths compared with continuous expansion.","This fosters new opportunities in temporal reasoning and enables us to introduce the previously overlooked \"temporal positional embedding.\"","Finally, our insights are embodied in \"RMem\" (\"R\" for restricted), a simple yet effective VOS modification that excels at challenging VOS scenarios and establishes new state of the art for object state changes (on the VOST dataset) and long videos (on the Long Videos dataset).","Our code and demo are available at https://restricted-memory.github.io/."],"url":"http://arxiv.org/abs/2406.08476v1","category":"cs.CV"}
{"created":"2024-06-12 17:57:25","title":"Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models","abstract":"Creating realistic avatars from a single RGB image is an attractive yet challenging problem. Due to its ill-posed nature, recent works leverage powerful prior from 2D diffusion models pretrained on large datasets. Although 2D diffusion models demonstrate strong generalization capability, they cannot provide multi-view shape priors with guaranteed 3D consistency. We propose Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion. Our key insight is that 2D multi-view diffusion and 3D reconstruction models provide complementary information for each other, and by coupling them in a tight manner, we can fully leverage the potential of both models. We introduce a novel image-conditioned generative 3D Gaussian Splats reconstruction model that leverages the priors from 2D multi-view diffusion models, and provides an explicit 3D representation, which further guides the 2D reverse sampling process to have better 3D consistency. Experiments show that our proposed framework outperforms state-of-the-art methods and enables the creation of realistic avatars from a single RGB image, achieving high-fidelity in both geometry and appearance. Extensive ablations also validate the efficacy of our design, (1) multi-view 2D priors conditioning in generative 3D reconstruction and (2) consistency refinement of sampling trajectory via the explicit 3D representation. Our code and models will be released on https://yuxuan-xue.com/human-3diffusion.","sentences":["Creating realistic avatars from a single RGB image is an attractive yet challenging problem.","Due to its ill-posed nature, recent works leverage powerful prior from 2D diffusion models pretrained on large datasets.","Although 2D diffusion models demonstrate strong generalization capability, they cannot provide multi-view shape priors with guaranteed 3D consistency.","We propose Human 3Diffusion:","Realistic Avatar Creation via Explicit 3D Consistent Diffusion.","Our key insight is that 2D multi-view diffusion and 3D reconstruction models provide complementary information for each other, and by coupling them in a tight manner, we can fully leverage the potential of both models.","We introduce a novel image-conditioned generative 3D Gaussian Splats reconstruction model that leverages the priors from 2D multi-view diffusion models, and provides an explicit 3D representation, which further guides the 2D reverse sampling process to have better 3D consistency.","Experiments show that our proposed framework outperforms state-of-the-art methods and enables the creation of realistic avatars from a single RGB image, achieving high-fidelity in both geometry and appearance.","Extensive ablations also validate the efficacy of our design, (1) multi-view 2D priors conditioning in generative 3D reconstruction and (2) consistency refinement of sampling trajectory via the explicit 3D representation.","Our code and models will be released on https://yuxuan-xue.com/human-3diffusion."],"url":"http://arxiv.org/abs/2406.08475v1","category":"cs.CV"}
{"created":"2024-06-12 17:57:06","title":"Real2Code: Reconstruct Articulated Objects via Code Generation","abstract":"We present Real2Code, a novel approach to reconstructing articulated objects via code generation. Given visual observations of an object, we first reconstruct its part geometry using an image segmentation model and a shape completion model. We then represent the object parts with oriented bounding boxes, which are input to a fine-tuned large language model (LLM) to predict joint articulation as code. By leveraging pre-trained vision and language models, our approach scales elegantly with the number of articulated parts, and generalizes from synthetic training data to real world objects in unstructured environments. Experimental results demonstrate that Real2Code significantly outperforms previous state-of-the-art in reconstruction accuracy, and is the first approach to extrapolate beyond objects' structural complexity in the training set, and reconstructs objects with up to 10 articulated parts. When incorporated with a stereo reconstruction model, Real2Code also generalizes to real world objects from a handful of multi-view RGB images, without the need for depth or camera information.","sentences":["We present Real2Code, a novel approach to reconstructing articulated objects via code generation.","Given visual observations of an object, we first reconstruct its part geometry using an image segmentation model and a shape completion model.","We then represent the object parts with oriented bounding boxes, which are input to a fine-tuned large language model (LLM) to predict joint articulation as code.","By leveraging pre-trained vision and language models, our approach scales elegantly with the number of articulated parts, and generalizes from synthetic training data to real world objects in unstructured environments.","Experimental results demonstrate that Real2Code significantly outperforms previous state-of-the-art in reconstruction accuracy, and is the first approach to extrapolate beyond objects' structural complexity in the training set, and reconstructs objects with up to 10 articulated parts.","When incorporated with a stereo reconstruction model, Real2Code also generalizes to real world objects from a handful of multi-view RGB images, without the need for depth or camera information."],"url":"http://arxiv.org/abs/2406.08474v1","category":"cs.CV"}
{"created":"2024-06-12 17:56:46","title":"Strategies for Pretraining Neural Operators","abstract":"Pretraining for partial differential equation (PDE) modeling has recently shown promise in scaling neural operators across datasets to improve generalizability and performance. Despite these advances, our understanding of how pretraining affects neural operators is still limited; studies generally propose tailored architectures and datasets that make it challenging to compare or examine different pretraining frameworks. To address this, we compare various pretraining methods without optimizing architecture choices to characterize pretraining dynamics on different models and datasets as well as to understand its scaling and generalization behavior. We find that pretraining is highly dependent on model and dataset choices, but in general transfer learning or physics-based pretraining strategies work best. In addition, pretraining performance can be further improved by using data augmentations. Lastly, pretraining is additionally beneficial when fine-tuning in scarce data regimes or when generalizing to downstream data similar to the pretraining distribution. Through providing insights into pretraining neural operators for physics prediction, we hope to motivate future work in developing and evaluating pretraining methods for PDEs.","sentences":["Pretraining for partial differential equation (PDE) modeling has recently shown promise in scaling neural operators across datasets to improve generalizability and performance.","Despite these advances, our understanding of how pretraining affects neural operators is still limited; studies generally propose tailored architectures and datasets that make it challenging to compare or examine different pretraining frameworks.","To address this, we compare various pretraining methods without optimizing architecture choices to characterize pretraining dynamics on different models and datasets as well as to understand its scaling and generalization behavior.","We find that pretraining is highly dependent on model and dataset choices, but in general transfer learning or physics-based pretraining strategies work best.","In addition, pretraining performance can be further improved by using data augmentations.","Lastly, pretraining is additionally beneficial when fine-tuning in scarce data regimes or when generalizing to downstream data similar to the pretraining distribution.","Through providing insights into pretraining neural operators for physics prediction, we hope to motivate future work in developing and evaluating pretraining methods for PDEs."],"url":"http://arxiv.org/abs/2406.08473v1","category":"cs.LG"}
{"created":"2024-06-12 17:56:31","title":"RILe: Reinforced Imitation Learning","abstract":"Reinforcement Learning has achieved significant success in generating complex behavior but often requires extensive reward function engineering. Adversarial variants of Imitation Learning and Inverse Reinforcement Learning offer an alternative by learning policies from expert demonstrations via a discriminator. Employing discriminators increases their data- and computational efficiency over the standard approaches; however, results in sensitivity to imperfections in expert data. We propose RILe, a teacher-student system that achieves both robustness to imperfect data and efficiency. In RILe, the student learns an action policy while the teacher dynamically adjusts a reward function based on the student's performance and its alignment with expert demonstrations. By tailoring the reward function to both performance of the student and expert similarity, our system reduces dependence on the discriminator and, hence, increases robustness against data imperfections. Experiments show that RILe outperforms existing methods by 2x in settings with limited or noisy expert data.","sentences":["Reinforcement Learning has achieved significant success in generating complex behavior but often requires extensive reward function engineering.","Adversarial variants of Imitation Learning and Inverse Reinforcement Learning offer an alternative by learning policies from expert demonstrations via a discriminator.","Employing discriminators increases their data- and computational efficiency over the standard approaches; however, results in sensitivity to imperfections in expert data.","We propose RILe, a teacher-student system that achieves both robustness to imperfect data and efficiency.","In RILe, the student learns an action policy while the teacher dynamically adjusts a reward function based on the student's performance and its alignment with expert demonstrations.","By tailoring the reward function to both performance of the student and expert similarity, our system reduces dependence on the discriminator and, hence, increases robustness against data imperfections.","Experiments show that RILe outperforms existing methods by 2x in settings with limited or noisy expert data."],"url":"http://arxiv.org/abs/2406.08472v1","category":"cs.LG"}
{"created":"2024-06-12 17:56:15","title":"Surprise! Using Physiological Stress for Allostatic Regulation Under the Active Inference Framework [Pre-Print]","abstract":"Allostasis proposes that long-term viability of a living system is achieved through anticipatory adjustments of its physiology and behaviour: emphasising physiological and affective stress as an adaptive state of adaptation that minimizes long-term prediction errors. More recently, the active inference framework (AIF) has also sought to explain action and long-term adaptation through the minimization of future errors (free energy), through the learning of statistical contingencies of the world, offering a formalism for allostatic regulation. We suggest that framing prediction errors through the lens of biological hormonal dynamics proposed by allostasis offers a way to integrate these two models together in a biologically-plausible manner. In this paper, we describe our initial work in developing a model that grounds prediction errors (surprisal) into the secretion of a physiological stress hormone (cortisol) acting as an adaptive, allostatic mediator on a homeostatically-controlled physiology. We evaluate this using a computational model in simulations using an active inference agent endowed with an artificial physiology, regulated through homeostatic and allostatic control in a stochastic environment. Our results find that allostatic functions of cortisol (stress), secreted as a function of prediction errors, provide adaptive advantages to the agent's long-term physiological regulation. We argue that the coupling of information-theoretic prediction errors to low-level, biological hormonal dynamics of stress can provide a computationally efficient model to long-term regulation for embodied intelligent systems.","sentences":["Allostasis proposes that long-term viability of a living system is achieved through anticipatory adjustments of its physiology and behaviour: emphasising physiological and affective stress as an adaptive state of adaptation that minimizes long-term prediction errors.","More recently, the active inference framework (AIF) has also sought to explain action and long-term adaptation through the minimization of future errors (free energy), through the learning of statistical contingencies of the world, offering a formalism for allostatic regulation.","We suggest that framing prediction errors through the lens of biological hormonal dynamics proposed by allostasis offers a way to integrate these two models together in a biologically-plausible manner.","In this paper, we describe our initial work in developing a model that grounds prediction errors (surprisal) into the secretion of a physiological stress hormone (cortisol) acting as an adaptive, allostatic mediator on a homeostatically-controlled physiology.","We evaluate this using a computational model in simulations using an active inference agent endowed with an artificial physiology, regulated through homeostatic and allostatic control in a stochastic environment.","Our results find that allostatic functions of cortisol (stress), secreted as a function of prediction errors, provide adaptive advantages to the agent's long-term physiological regulation.","We argue that the coupling of information-theoretic prediction errors to low-level, biological hormonal dynamics of stress can provide a computationally efficient model to long-term regulation for embodied intelligent systems."],"url":"http://arxiv.org/abs/2406.08471v1","category":"cs.AI"}
{"created":"2024-06-12 17:55:44","title":"Charged pion condensation and color superconductivity phenomena in chirally asymmetric dense quark matter","abstract":"In this paper, the question of the influence of color superconductivity (CSC) on the formation of a phase with condensation of charged pions in dense chirally asymmetric quark matter is studied. We consider it within the framework of the massless NJL model with a diquark interaction channel at zero temperature, but in the presence of baryon $\\mu_B$, isospin $\\mu_I$, chiral $\\mu_{5}$ and chiral isospin $\\mu_{I5}$ chemical potentials. It has been shown in the mean-field approximation that in the presence of chiral imbalance, when $\\mu_{5}\\ne 0$ and/or $\\mu_{I5}\\ne 0$, CSC phenomenon does not hinder the generation of charged pion condensation in dense quark matter even at largest baryon densities attainable in heavy ion collisions experiments and in cores of neutron stars and its mergers. So charged pion condensation in dense quark matter with chiral imbalance predicted earlier is not in a bit suppressed by the presence of CSC, highlighting the resilience of this phase in dense quark matter. This makes phase structure of dense quark matter even more rich, multifaceted and interesting, and shows that pion condensation in dense quark matter is viable phenomenon to be explored, for example, in intermediate energy heavy ion collisions experiments.","sentences":["In this paper, the question of the influence of color superconductivity (CSC) on the formation of a phase with condensation of charged pions in dense chirally asymmetric quark matter is studied.","We consider it within the framework of the massless NJL model with a diquark interaction channel at zero temperature, but in the presence of baryon $\\mu_B$, isospin $\\mu_I$, chiral $\\mu_{5}$ and chiral isospin $\\mu_{I5}$ chemical potentials.","It has been shown in the mean-field approximation that in the presence of chiral imbalance, when $\\mu_{5}\\ne 0$ and/or $\\mu_{I5}\\ne 0$, CSC phenomenon does not hinder the generation of charged pion condensation in dense quark matter even at largest baryon densities attainable in heavy ion collisions experiments and in cores of neutron stars and its mergers.","So charged pion condensation in dense quark matter with chiral imbalance predicted earlier is not in a bit suppressed by the presence of CSC, highlighting the resilience of this phase in dense quark matter.","This makes phase structure of dense quark matter even more rich, multifaceted and interesting, and shows that pion condensation in dense quark matter is viable phenomenon to be explored, for example, in intermediate energy heavy ion collisions experiments."],"url":"http://arxiv.org/abs/2406.08470v1","category":"hep-ph"}
{"created":"2024-06-12 17:54:54","title":"PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences","abstract":"Large foundation models pretrained on raw web-scale data are not readily deployable without additional step of extensive alignment to human preferences. Such alignment is typically done by collecting large amounts of pairwise comparisons from humans (\"Do you prefer output A or B?\") and learning a reward model or a policy with the Bradley-Terry-Luce (BTL) model as a proxy for a human's underlying implicit preferences. These methods generally suffer from assuming a universal preference shared by all humans, which lacks the flexibility of adapting to plurality of opinions and preferences. In this work, we propose PAL, a framework to model human preference complementary to existing pretraining strategies, which incorporates plurality from the ground up. We propose using the ideal point model as a lens to view alignment using preference comparisons. Together with our novel reformulation and using mixture modeling, our framework captures the plurality of population preferences while simultaneously learning a common preference latent space across different preferences, which can few-shot generalize to new, unseen users. Our approach enables us to use the penultimate-layer representation of large foundation models and simple MLP layers to learn reward functions that are on-par with the existing large state-of-the-art reward models, thereby enhancing efficiency of reward modeling significantly. We show that PAL achieves competitive reward model accuracy compared to strong baselines on 1) Language models with Summary dataset ; 2) Image Generative models with Pick-a-Pic dataset ; 3) A new semisynthetic heterogeneous dataset generated using Anthropic Personas. Finally, our experiments also highlight the shortcoming of current preference datasets that are created using rigid rubrics which wash away heterogeneity, and call for more nuanced data collection approaches.","sentences":["Large foundation models pretrained on raw web-scale data are not readily deployable without additional step of extensive alignment to human preferences.","Such alignment is typically done by collecting large amounts of pairwise comparisons from humans (\"Do you prefer output A or B?\") and learning a reward model or a policy with the Bradley-Terry-Luce (BTL) model as a proxy for a human's underlying implicit preferences.","These methods generally suffer from assuming a universal preference shared by all humans, which lacks the flexibility of adapting to plurality of opinions and preferences.","In this work, we propose PAL, a framework to model human preference complementary to existing pretraining strategies, which incorporates plurality from the ground up.","We propose using the ideal point model as a lens to view alignment using preference comparisons.","Together with our novel reformulation and using mixture modeling, our framework captures the plurality of population preferences while simultaneously learning a common preference latent space across different preferences, which can few-shot generalize to new, unseen users.","Our approach enables us to use the penultimate-layer representation of large foundation models and simple MLP layers to learn reward functions that are on-par with the existing large state-of-the-art reward models, thereby enhancing efficiency of reward modeling significantly.","We show that PAL achieves competitive reward model accuracy compared to strong baselines on 1) Language models with Summary dataset ; 2) Image Generative models with Pick-a-Pic dataset ; 3) A new semisynthetic heterogeneous dataset generated using Anthropic Personas.","Finally, our experiments also highlight the shortcoming of current preference datasets that are created using rigid rubrics which wash away heterogeneity, and call for more nuanced data collection approaches."],"url":"http://arxiv.org/abs/2406.08469v1","category":"cs.LG"}
{"created":"2024-06-12 17:53:31","title":"DafnyBench: A Benchmark for Formal Software Verification","abstract":"We introduce DafnyBench, the largest benchmark of its kind for training and evaluating machine learning systems for formal software verification. We test the ability of LLMs such as GPT-4 and Claude 3 to auto-generate enough hints for the Dafny formal verification engine to successfully verify over 750 programs with about 53,000 lines of code. The best model and prompting scheme achieved 68% success rate, and we quantify how this rate improves when retrying with error message feedback and how it deteriorates with the amount of required code and hints. We hope that DafnyBench will enable rapid improvements from this baseline as LLMs and verification techniques grow in quality.","sentences":["We introduce DafnyBench, the largest benchmark of its kind for training and evaluating machine learning systems for formal software verification.","We test the ability of LLMs such as GPT-4 and Claude 3 to auto-generate enough hints for the Dafny formal verification engine to successfully verify over 750 programs with about 53,000 lines of code.","The best model and prompting scheme achieved 68% success rate, and we quantify how this rate improves when retrying with error message feedback and how it deteriorates with the amount of required code and hints.","We hope that DafnyBench will enable rapid improvements from this baseline as LLMs and verification techniques grow in quality."],"url":"http://arxiv.org/abs/2406.08467v1","category":"cs.SE"}
{"created":"2024-06-12 17:53:29","title":"Scaling Laws in Linear Regression: Compute, Parameters, and Data","abstract":"Empirically, large-scale deep learning models often satisfy a neural scaling law: the test error of the trained model improves polynomially as the model size and data size grow. However, conventional wisdom suggests the test error consists of approximation, bias, and variance errors, where the variance error increases with model size. This disagrees with the general form of neural scaling laws, which predict that increasing model size monotonically improves performance.   We study the theory of scaling laws in an infinite dimensional linear regression setup. Specifically, we consider a model with $M$ parameters as a linear function of sketched covariates. The model is trained by one-pass stochastic gradient descent (SGD) using $N$ data. Assuming the optimal parameter satisfies a Gaussian prior and the data covariance matrix has a power-law spectrum of degree $a>1$, we show that the reducible part of the test error is $\\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which increases with $M$, is dominated by the other errors due to the implicit regularization of SGD, thus disappearing from the bound. Our theory is consistent with the empirical neural scaling laws and verified by numerical simulation.","sentences":["Empirically, large-scale deep learning models often satisfy a neural scaling law: the test error of the trained model improves polynomially as the model size and data size grow.","However, conventional wisdom suggests the test error consists of approximation, bias, and variance errors, where the variance error increases with model size.","This disagrees with the general form of neural scaling laws, which predict that increasing model size monotonically improves performance.   ","We study the theory of scaling laws in an infinite dimensional linear regression setup.","Specifically, we consider a model with $M$ parameters as a linear function of sketched covariates.","The model is trained by one-pass stochastic gradient descent (SGD) using $N$ data.","Assuming the optimal parameter satisfies a Gaussian prior and the data covariance matrix has a power-law spectrum of degree $a>1$, we show that the reducible part of the test error is $\\Theta(M^{-(a-1)}","+ N^{-(a-1)/a})$. The variance error, which increases with $M$, is dominated by the other errors due to the implicit regularization of SGD, thus disappearing from the bound.","Our theory is consistent with the empirical neural scaling laws and verified by numerical simulation."],"url":"http://arxiv.org/abs/2406.08466v1","category":"cs.LG"}
{"created":"2024-06-12 17:52:30","title":"Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing","abstract":"High-quality instruction data is critical for aligning large language models (LLMs). Although some models, such as Llama-3-Instruct, have open weights, their alignment data remain private, which hinders the democratization of AI. High human labor costs and a limited, predefined scope for prompting prevent existing open-source data creation methods from scaling effectively, potentially limiting the diversity and quality of public alignment datasets. Is it possible to synthesize high-quality instruction data at scale by extracting it directly from an aligned LLM? We present a self-synthesis method for generating large-scale alignment data named Magpie. Our key observation is that aligned LLMs like Llama-3-Instruct can generate a user query when we input only the left-side templates up to the position reserved for user messages, thanks to their auto-regressive nature. We use this method to prompt Llama-3-Instruct and generate 4 million instructions along with their corresponding responses. We perform a comprehensive analysis of the extracted data and select 300K high-quality instances. To compare Magpie data with other public instruction datasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the performance of the fine-tuned models. Our results indicate that in some tasks, models fine-tuned with Magpie perform comparably to the official Llama-3-8B-Instruct, despite the latter being enhanced with 10 million data points through supervised fine-tuning (SFT) and subsequent feedback learning. We also show that using Magpie solely for SFT can surpass the performance of previous public datasets utilized for both SFT and preference optimization, such as direct preference optimization with UltraFeedback. This advantage is evident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.","sentences":["High-quality instruction data is critical for aligning large language models (LLMs).","Although some models, such as Llama-3-Instruct, have open weights, their alignment data remain private, which hinders the democratization of AI.","High human labor costs and a limited, predefined scope for prompting prevent existing open-source data creation methods from scaling effectively, potentially limiting the diversity and quality of public alignment datasets.","Is it possible to synthesize high-quality instruction data at scale by extracting it directly from an aligned LLM?","We present a self-synthesis method for generating large-scale alignment data named Magpie.","Our key observation is that aligned LLMs like Llama-3-Instruct can generate a user query when we input only the left-side templates up to the position reserved for user messages, thanks to their auto-regressive nature.","We use this method to prompt Llama-3-Instruct and generate 4 million instructions along with their corresponding responses.","We perform a comprehensive analysis of the extracted data and select 300K high-quality instances.","To compare Magpie data with other public instruction datasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the performance of the fine-tuned models.","Our results indicate that in some tasks, models fine-tuned with Magpie perform comparably to the official Llama-3-8B-Instruct, despite the latter being enhanced with 10 million data points through supervised fine-tuning (SFT) and subsequent feedback learning.","We also show that using Magpie solely for SFT can surpass the performance of previous public datasets utilized for both SFT and preference optimization, such as direct preference optimization with UltraFeedback.","This advantage is evident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench."],"url":"http://arxiv.org/abs/2406.08464v1","category":"cs.CL"}
{"created":"2024-06-12 17:50:05","title":"Braided Logarithmic Vertex Algebras","abstract":"We study a family of algebras defined using a locally-finite endomorphism called a braiding map. When the braiding map is semi-simple, the algebra is a generalized vertex algebra, while when the braiding map is locally-nilpotent we have a logarithmic vertex algebra. We describe a method that associates to these algebras non-local Poisson vertex algebras, and we use this relation to build a new example of a generalized vertex algebra motivated by the non-linear Schr\\\"odinger non-local Poisson vertex algebra","sentences":["We study a family of algebras defined using a locally-finite endomorphism called a braiding map.","When the braiding map is semi-simple, the algebra is a generalized vertex algebra, while when the braiding map is locally-nilpotent we have a logarithmic vertex algebra.","We describe a method that associates to these algebras non-local Poisson vertex algebras, and we use this relation to build a new example of a generalized vertex algebra motivated by the non-linear Schr\\\"odinger non-local Poisson vertex algebra"],"url":"http://arxiv.org/abs/2406.08458v1","category":"math.QA"}
{"created":"2024-06-12 17:47:04","title":"AToM-Bot: Embodied Fulfillment of Unspoken Human Needs with Affective Theory of Mind","abstract":"We propose AToM-Bot, a novel task generation and execution framework for proactive robot-human interaction, which leverages the human mental and physical state inference capabilities of the Vision Language Model (VLM) prompted by the Affective Theory of Mind (AToM). Without requiring explicit commands by humans, AToM-Bot proactively generates and follows feasible tasks to improve general human well-being. When around humans, AToM-Bot first detects current human needs based on inferred human states and observations of the surrounding environment. It then generates tasks to fulfill these needs, taking into account its embodied constraints. We designed 16 daily life scenarios spanning 4 common scenes and tasked the same visual stimulus to 59 human subjects and our robot. We used the similarity between human open-ended answers and robot output, and the human satisfaction scores to metric robot performance. AToM-Bot received high human evaluations in need detection (6.42/7, 91.7%), embodied solution (6.15/7, 87.8%) and task execution (6.17/7, 88.1%). We show that AToM-Bot excels in generating and executing feasible plans to fulfill unspoken human needs. Videos and code are available at https://affective-tom-bot.github.io.","sentences":["We propose AToM-Bot, a novel task generation and execution framework for proactive robot-human interaction, which leverages the human mental and physical state inference capabilities of the Vision Language Model (VLM) prompted by the Affective Theory of Mind (AToM).","Without requiring explicit commands by humans, AToM-Bot proactively generates and follows feasible tasks to improve general human well-being.","When around humans, AToM-Bot first detects current human needs based on inferred human states and observations of the surrounding environment.","It then generates tasks to fulfill these needs, taking into account its embodied constraints.","We designed 16 daily life scenarios spanning 4 common scenes and tasked the same visual stimulus to 59 human subjects and our robot.","We used the similarity between human open-ended answers and robot output, and the human satisfaction scores to metric robot performance.","AToM-Bot received high human evaluations in need detection (6.42/7, 91.7%), embodied solution (6.15/7, 87.8%) and task execution (6.17/7, 88.1%).","We show that AToM-Bot excels in generating and executing feasible plans to fulfill unspoken human needs.","Videos and code are available at https://affective-tom-bot.github.io."],"url":"http://arxiv.org/abs/2406.08455v1","category":"cs.RO"}
{"created":"2024-06-12 17:44:32","title":"Einstein Gravity from Einstein Action: Counterterms and Covariance","abstract":"The field equations of general relativity can be derived from the Einstein action, which is quadratic in connection coefficients, rather than the standard action involving the Gibbons-Hawking-York term and counterterm. We show that it is possible to construct a new counterterm directly for the Einstein action, which removes divergences and naturally introduces a flat reference spacetime. The total action is then covariant under simultaneous transformation of both the spacetime and reference tetrads, and argue that this is analogous to the Gibbons-Hawking action. We then explore different perspectives arising naturally from different uses of the reference tetrad, and explore implications of viewing gravity as fundamentally described in terms of non-covariant connection coefficients.","sentences":["The field equations of general relativity can be derived from the Einstein action, which is quadratic in connection coefficients, rather than the standard action involving the Gibbons-Hawking-York term and counterterm.","We show that it is possible to construct a new counterterm directly for the Einstein action, which removes divergences and naturally introduces a flat reference spacetime.","The total action is then covariant under simultaneous transformation of both the spacetime and reference tetrads, and argue that this is analogous to the Gibbons-Hawking action.","We then explore different perspectives arising naturally from different uses of the reference tetrad, and explore implications of viewing gravity as fundamentally described in terms of non-covariant connection coefficients."],"url":"http://arxiv.org/abs/2406.08452v1","category":"gr-qc"}
{"created":"2024-06-12 17:39:36","title":"Heterogeneous Beliefs Model of Stock Market Predictability","abstract":"This paper proposes a theory of stock market predictability patterns based on a model of heterogeneous beliefs. In a discrete finite time framework, some agents receive news about an asset's fundamental value through a noisy signal. The investors are heterogeneous in that they have different beliefs about the stochastic supply. A momentum in the stock price arises from those agents who incorrectly underestimate the signal accuracy, dampening the initial price impact of the signal. A reversal in price occurs because the price reverts to the fundamental value in the long run. An extension of the model to multiple assets case predicts co-movement and lead-lag effect, in addition to cross-sectional momentum and reversal. The heterogeneous beliefs of investors about news demonstrate how the main predictability anomalies arise endogenously in a model of bounded rationality.","sentences":["This paper proposes a theory of stock market predictability patterns based on a model of heterogeneous beliefs.","In a discrete finite time framework, some agents receive news about an asset's fundamental value through a noisy signal.","The investors are heterogeneous in that they have different beliefs about the stochastic supply.","A momentum in the stock price arises from those agents who incorrectly underestimate the signal accuracy, dampening the initial price impact of the signal.","A reversal in price occurs because the price reverts to the fundamental value in the long run.","An extension of the model to multiple assets case predicts co-movement and lead-lag effect, in addition to cross-sectional momentum and reversal.","The heterogeneous beliefs of investors about news demonstrate how the main predictability anomalies arise endogenously in a model of bounded rationality."],"url":"http://arxiv.org/abs/2406.08448v1","category":"q-fin.PR"}
{"created":"2024-06-12 17:38:20","title":"The Impact of Initialization on LoRA Finetuning Dynamics","abstract":"In this paper, we study the role of initialization in Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021). Essentially, to start from the pretrained model as initialization for finetuning, one can either initialize B to zero and A to random (default initialization in PEFT package), or vice-versa. In both cases, the product BA is equal to zero at initialization, which makes finetuning starts from the pretrained model. These two initialization schemes are seemingly similar. They should in-principle yield the same performance and share the same optimal learning rate. We demonstrate that this is an incorrect intuition and that the first scheme (initializing B to zero and A to random) on average yields better performance compared to the other scheme. Our theoretical analysis shows that the reason behind this might be that the first initialization allows the use of larger learning rates (without causing output instability) compared to the second initialization, resulting in more efficient learning of the first scheme. We validate our results with extensive experiments on LLMs.","sentences":["In this paper, we study the role of initialization in Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021).","Essentially, to start from the pretrained model as initialization for finetuning, one can either initialize B to zero and A to random (default initialization in PEFT package), or vice-versa.","In both cases, the product BA is equal to zero at initialization, which makes finetuning starts from the pretrained model.","These two initialization schemes are seemingly similar.","They should in-principle yield the same performance and share the same optimal learning rate.","We demonstrate that this is an incorrect intuition and that the first scheme (initializing B to zero and A to random) on average yields better performance compared to the other scheme.","Our theoretical analysis shows that the reason behind this might be that the first initialization allows the use of larger learning rates (without causing output instability) compared to the second initialization, resulting in more efficient learning of the first scheme.","We validate our results with extensive experiments on LLMs."],"url":"http://arxiv.org/abs/2406.08447v1","category":"cs.LG"}
{"created":"2024-06-12 17:37:09","title":"SVSNet+: Enhancing Speaker Voice Similarity Assessment Models with Representations from Speech Foundation Models","abstract":"Representations from pre-trained speech foundation models (SFMs) have shown impressive performance in many downstream tasks. However, the potential benefits of incorporating pre-trained SFM representations into speaker voice similarity assessment have not been thoroughly investigated. In this paper, we propose SVSNet+, a model that integrates pre-trained SFM representations to improve performance in assessing speaker voice similarity. Experimental results on the Voice Conversion Challenge 2018 and 2020 datasets show that SVSNet+ incorporating WavLM representations shows significant improvements compared to baseline models. In addition, while fine-tuning WavLM with a small dataset of the downstream task does not improve performance, using the same dataset to learn a weighted-sum representation of WavLM can substantially improve performance. Furthermore, when WavLM is replaced by other SFMs, SVSNet+ still outperforms the baseline models and exhibits strong generalization ability.","sentences":["Representations from pre-trained speech foundation models (SFMs) have shown impressive performance in many downstream tasks.","However, the potential benefits of incorporating pre-trained SFM representations into speaker voice similarity assessment have not been thoroughly investigated.","In this paper, we propose SVSNet+, a model that integrates pre-trained SFM representations to improve performance in assessing speaker voice similarity.","Experimental results on the Voice Conversion Challenge 2018 and 2020 datasets show that SVSNet+ incorporating WavLM representations shows significant improvements compared to baseline models.","In addition, while fine-tuning WavLM with a small dataset of the downstream task does not improve performance, using the same dataset to learn a weighted-sum representation of WavLM can substantially improve performance.","Furthermore, when WavLM is replaced by other SFMs, SVSNet+ still outperforms the baseline models and exhibits strong generalization ability."],"url":"http://arxiv.org/abs/2406.08445v1","category":"eess.AS"}
{"created":"2024-06-12 17:37:09","title":"OLMES: A Standard for Language Model Evaluations","abstract":"Progress in AI is often demonstrated by new models claiming improved performance on tasks measuring model capabilities. Evaluating language models in particular is challenging, as small changes to how a model is evaluated on a task can lead to large changes in measured performance. There is no common standard setup, so different models are evaluated on the same tasks in different ways, leading to claims about which models perform best not being reproducible. We propose OLMES, a completely documented, practical, open standard for reproducible LLM evaluations. In developing this standard, we identify and review the varying factors in evaluation practices adopted by the community - such as details of prompt formatting, choice of in-context examples, probability normalizations, and task formulation. In particular, OLMES supports meaningful comparisons between smaller base models that require the unnatural \"cloze\" formulation of multiple-choice questions against larger models that can utilize the original formulation. OLMES includes well-considered recommendations guided by results from existing literature as well as new experiments investigating open questions.","sentences":["Progress in AI is often demonstrated by new models claiming improved performance on tasks measuring model capabilities.","Evaluating language models in particular is challenging, as small changes to how a model is evaluated on a task can lead to large changes in measured performance.","There is no common standard setup, so different models are evaluated on the same tasks in different ways, leading to claims about which models perform best not being reproducible.","We propose OLMES, a completely documented, practical, open standard for reproducible LLM evaluations.","In developing this standard, we identify and review the varying factors in evaluation practices adopted by the community - such as details of prompt formatting, choice of in-context examples, probability normalizations, and task formulation.","In particular, OLMES supports meaningful comparisons between smaller base models that require the unnatural \"cloze\" formulation of multiple-choice questions against larger models that can utilize the original formulation.","OLMES includes well-considered recommendations guided by results from existing literature as well as new experiments investigating open questions."],"url":"http://arxiv.org/abs/2406.08446v1","category":"cs.CL"}
{"created":"2024-06-12 17:28:32","title":"$\\texttt{DiffLense}$: A Conditional Diffusion Model for Super-Resolution of Gravitational Lensing Data","abstract":"Gravitational lensing data is frequently collected at low resolution due to instrumental limitations and observing conditions. Machine learning-based super-resolution techniques offer a method to enhance the resolution of these images, enabling more precise measurements of lensing effects and a better understanding of the matter distribution in the lensing system. This enhancement can significantly improve our knowledge of the distribution of mass within the lensing galaxy and its environment, as well as the properties of the background source being lensed. Traditional super-resolution techniques typically learn a mapping function from lower-resolution to higher-resolution samples. However, these methods are often constrained by their dependence on optimizing a fixed distance function, which can result in the loss of intricate details crucial for astrophysical analysis. In this work, we introduce $\\texttt{DiffLense}$, a novel super-resolution pipeline based on a conditional diffusion model specifically designed to enhance the resolution of gravitational lensing images obtained from the Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP). Our approach adopts a generative model, leveraging the detailed structural information present in Hubble Space Telescope (HST) counterparts. The diffusion model, trained to generate HST data, is conditioned on HSC data pre-processed with denoising techniques and thresholding to significantly reduce noise and background interference. This process leads to a more distinct and less overlapping conditional distribution during the model's training phase. We demonstrate that $\\texttt{DiffLense}$ outperforms existing state-of-the-art single-image super-resolution techniques, particularly in retaining the fine details necessary for astrophysical analyses.","sentences":["Gravitational lensing data is frequently collected at low resolution due to instrumental limitations and observing conditions.","Machine learning-based super-resolution techniques offer a method to enhance the resolution of these images, enabling more precise measurements of lensing effects and a better understanding of the matter distribution in the lensing system.","This enhancement can significantly improve our knowledge of the distribution of mass within the lensing galaxy and its environment, as well as the properties of the background source being lensed.","Traditional super-resolution techniques typically learn a mapping function from lower-resolution to higher-resolution samples.","However, these methods are often constrained by their dependence on optimizing a fixed distance function, which can result in the loss of intricate details crucial for astrophysical analysis.","In this work, we introduce $\\texttt{DiffLense}$, a novel super-resolution pipeline based on a conditional diffusion model specifically designed to enhance the resolution of gravitational lensing images obtained from the Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP).","Our approach adopts a generative model, leveraging the detailed structural information present in Hubble Space Telescope (HST) counterparts.","The diffusion model, trained to generate HST data, is conditioned on HSC data pre-processed with denoising techniques and thresholding to significantly reduce noise and background interference.","This process leads to a more distinct and less overlapping conditional distribution during the model's training phase.","We demonstrate that $\\texttt{DiffLense}$ outperforms existing state-of-the-art single-image super-resolution techniques, particularly in retaining the fine details necessary for astrophysical analyses."],"url":"http://arxiv.org/abs/2406.08442v1","category":"astro-ph.IM"}
{"created":"2024-06-12 17:26:54","title":"Adaptive Swarm Mesh Refinement using Deep Reinforcement Learning with Local Rewards","abstract":"Simulating physical systems is essential in engineering, but analytical solutions are limited to straightforward problems. Consequently, numerical methods like the Finite Element Method (FEM) are widely used. However, the FEM becomes computationally expensive as problem complexity and accuracy demands increase. Adaptive Mesh Refinement (AMR) improves the FEM by dynamically allocating mesh elements on the domain, balancing computational speed and accuracy. Classical AMR depends on heuristics or expensive error estimators, limiting its use in complex simulations. While learning-based AMR methods are promising, they currently only scale to simple problems. In this work, we formulate AMR as a system of collaborating, homogeneous agents that iteratively split into multiple new agents. This agent-wise perspective enables a spatial reward formulation focused on reducing the maximum mesh element error. Our approach, Adaptive Swarm Mesh Refinement (ASMR), offers efficient, stable optimization and generates highly adaptive meshes at user-defined resolution during inference. Extensive experiments, including volumetric meshes and Neumann boundary conditions, demonstrate that ASMR exceeds heuristic approaches and learned baselines, matching the performance of expensive error-based oracle AMR strategies. ASMR additionally generalizes to different domains during inference, and produces meshes that simulate up to 2 orders of magnitude faster than uniform refinements in more demanding settings.","sentences":["Simulating physical systems is essential in engineering, but analytical solutions are limited to straightforward problems.","Consequently, numerical methods like the Finite Element Method (FEM) are widely used.","However, the FEM becomes computationally expensive as problem complexity and accuracy demands increase.","Adaptive Mesh Refinement (AMR) improves the FEM by dynamically allocating mesh elements on the domain, balancing computational speed and accuracy.","Classical AMR depends on heuristics or expensive error estimators, limiting its use in complex simulations.","While learning-based AMR methods are promising, they currently only scale to simple problems.","In this work, we formulate AMR as a system of collaborating, homogeneous agents that iteratively split into multiple new agents.","This agent-wise perspective enables a spatial reward formulation focused on reducing the maximum mesh element error.","Our approach, Adaptive Swarm Mesh Refinement (ASMR), offers efficient, stable optimization and generates highly adaptive meshes at user-defined resolution during inference.","Extensive experiments, including volumetric meshes and Neumann boundary conditions, demonstrate that ASMR exceeds heuristic approaches and learned baselines, matching the performance of expensive error-based oracle AMR strategies.","ASMR additionally generalizes to different domains during inference, and produces meshes that simulate up to 2 orders of magnitude faster than uniform refinements in more demanding settings."],"url":"http://arxiv.org/abs/2406.08440v1","category":"cs.LG"}
{"created":"2024-06-12 17:23:45","title":"Energy Spectrum of Primary Knock-on Atoms and Atomic Displacement Calculations in Metallic Alloys Under Neutron Irradiation","abstract":"Materials subjected to neutron irradiation experience damage due to displacement cascades triggered by nuclear reactions. This paper presents a practical method to calculate primary atomic recoil events (PKAs), which lead to cascade damage, based on energy and recoiling species. We developed a custom code to identify PKAs and extract their properties using MCNPX and SRIM. This code determines the specifications of recoil atoms from the data provided by the PTRAC card in MCNPX. Consequently, the energy spectrum of PKAs generated through various reaction channels, including elastic/inelastic scattering and transmutations such as (n, {\\alpha}), (n, p), and (n, {\\gamma}), is calculated. This PKA spectrum is then input into SRIM, which calculates the total number of atomic displacements using the binary collision approximation (BCA) and provides crucial information about the spatial distribution of defects within the irradiated material. Our results indicate that elastic scattering is the predominant reaction, producing most PKAs with energies in the range of several keV. In contrast, inelastic scattering becomes the dominant interaction for generating high-energy PKAs (~EPKA>1 MeV). Additionally, we observed that the number of Frenkel pairs versus ion energy curves for light particle ion implantation (such as H and He) is significantly smaller than for heavier ions.","sentences":["Materials subjected to neutron irradiation experience damage due to displacement cascades triggered by nuclear reactions.","This paper presents a practical method to calculate primary atomic recoil events (PKAs), which lead to cascade damage, based on energy and recoiling species.","We developed a custom code to identify PKAs and extract their properties using MCNPX and SRIM.","This code determines the specifications of recoil atoms from the data provided by the PTRAC card in MCNPX.","Consequently, the energy spectrum of PKAs generated through various reaction channels, including elastic/inelastic scattering and transmutations such as (n, {\\alpha}), (n, p), and (n, {\\gamma}), is calculated.","This PKA spectrum is then input into SRIM, which calculates the total number of atomic displacements using the binary collision approximation (BCA) and provides crucial information about the spatial distribution of defects within the irradiated material.","Our results indicate that elastic scattering is the predominant reaction, producing most PKAs with energies in the range of several keV.","In contrast, inelastic scattering becomes the dominant interaction for generating high-energy PKAs (~EPKA>1 MeV).","Additionally, we observed that the number of Frenkel pairs versus ion energy curves for light particle ion implantation (such as H and He) is significantly smaller than for heavier ions."],"url":"http://arxiv.org/abs/2406.08438v1","category":"physics.comp-ph"}
{"created":"2024-06-12 17:22:00","title":"Wiki Entity Summarization Benchmark","abstract":"Entity summarization aims to compute concise summaries for entities in knowledge graphs. Existing datasets and benchmarks are often limited to a few hundred entities and discard graph structure in source knowledge graphs. This limitation is particularly pronounced when it comes to ground-truth summaries, where there exist only a few labeled summaries for evaluation and training. We propose WikES, a comprehensive benchmark comprising of entities, their summaries, and their connections. Additionally, WikES features a dataset generator to test entity summarization algorithms in different areas of the knowledge graph. Importantly, our approach combines graph algorithms and NLP models as well as different data sources such that WikES does not require human annotation, rendering the approach cost-effective and generalizable to multiple domains. Finally, WikES is scalable and capable of capturing the complexities of knowledge graphs in terms of topology and semantics. WikES features existing datasets for comparison. Empirical studies of entity summarization methods confirm the usefulness of our benchmark. Data, code, and models are available at: https://github.com/msorkhpar/wiki-entity-summarization.","sentences":["Entity summarization aims to compute concise summaries for entities in knowledge graphs.","Existing datasets and benchmarks are often limited to a few hundred entities and discard graph structure in source knowledge graphs.","This limitation is particularly pronounced when it comes to ground-truth summaries, where there exist only a few labeled summaries for evaluation and training.","We propose WikES, a comprehensive benchmark comprising of entities, their summaries, and their connections.","Additionally, WikES features a dataset generator to test entity summarization algorithms in different areas of the knowledge graph.","Importantly, our approach combines graph algorithms and NLP models as well as different data sources such that WikES does not require human annotation, rendering the approach cost-effective and generalizable to multiple domains.","Finally, WikES is scalable and capable of capturing the complexities of knowledge graphs in terms of topology and semantics.","WikES features existing datasets for comparison.","Empirical studies of entity summarization methods confirm the usefulness of our benchmark.","Data, code, and models are available at: https://github.com/msorkhpar/wiki-entity-summarization."],"url":"http://arxiv.org/abs/2406.08435v1","category":"cs.IR"}
{"created":"2024-06-12 17:21:21","title":"TasTe: Teaching Large Language Models to Translate through Self-Reflection","abstract":"Large language models (LLMs) have exhibited remarkable performance in various natural language processing tasks. Techniques like instruction tuning have effectively enhanced the proficiency of LLMs in the downstream task of machine translation. However, the existing approaches fail to yield satisfactory translation outputs that match the quality of supervised neural machine translation (NMT) systems. One plausible explanation for this discrepancy is that the straightforward prompts employed in these methodologies are unable to fully exploit the acquired instruction-following capabilities. To this end, we propose the TasTe framework, which stands for translating through self-reflection. The self-reflection process includes two stages of inference. In the first stage, LLMs are instructed to generate preliminary translations and conduct self-assessments on these translations simultaneously. In the second stage, LLMs are tasked to refine these preliminary translations according to the evaluation results. The evaluation results in four language directions on the WMT22 benchmark reveal the effectiveness of our approach compared to existing methods. Our work presents a promising approach to unleash the potential of LLMs and enhance their capabilities in MT. The codes and datasets are open-sourced at https://github.com/YutongWang1216/ReflectionLLMMT.","sentences":["Large language models (LLMs) have exhibited remarkable performance in various natural language processing tasks.","Techniques like instruction tuning have effectively enhanced the proficiency of LLMs in the downstream task of machine translation.","However, the existing approaches fail to yield satisfactory translation outputs that match the quality of supervised neural machine translation (NMT) systems.","One plausible explanation for this discrepancy is that the straightforward prompts employed in these methodologies are unable to fully exploit the acquired instruction-following capabilities.","To this end, we propose the TasTe framework, which stands for translating through self-reflection.","The self-reflection process includes two stages of inference.","In the first stage, LLMs are instructed to generate preliminary translations and conduct self-assessments on these translations simultaneously.","In the second stage, LLMs are tasked to refine these preliminary translations according to the evaluation results.","The evaluation results in four language directions on the WMT22 benchmark reveal the effectiveness of our approach compared to existing methods.","Our work presents a promising approach to unleash the potential of LLMs and enhance their capabilities in MT.","The codes and datasets are open-sourced at https://github.com/YutongWang1216/ReflectionLLMMT."],"url":"http://arxiv.org/abs/2406.08434v1","category":"cs.CL"}
{"created":"2024-06-12 17:16:16","title":"Diffusion Soup: Model Merging for Text-to-Image Diffusion Models","abstract":"We present Diffusion Soup, a compartmentalization method for Text-to-Image Generation that averages the weights of diffusion models trained on sharded data. By construction, our approach enables training-free continual learning and unlearning with no additional memory or inference costs, since models corresponding to data shards can be added or removed by re-averaging. We show that Diffusion Soup samples from a point in weight space that approximates the geometric mean of the distributions of constituent datasets, which offers anti-memorization guarantees and enables zero-shot style mixing. Empirically, Diffusion Soup outperforms a paragon model trained on the union of all data shards and achieves a 30% improvement in Image Reward (.34 $\\to$ .44) on domain sharded data, and a 59% improvement in IR (.37 $\\to$ .59) on aesthetic data. In both cases, souping also prevails in TIFA score (respectively, 85.5 $\\to$ 86.5 and 85.6 $\\to$ 86.8). We demonstrate robust unlearning -- removing any individual domain shard only lowers performance by 1% in IR (.45 $\\to$ .44) -- and validate our theoretical insights on anti-memorization using real data. Finally, we showcase Diffusion Soup's ability to blend the distinct styles of models finetuned on different shards, resulting in the zero-shot generation of hybrid styles.","sentences":["We present Diffusion Soup, a compartmentalization method for Text-to-Image Generation that averages the weights of diffusion models trained on sharded data.","By construction, our approach enables training-free continual learning and unlearning with no additional memory or inference costs, since models corresponding to data shards can be added or removed by re-averaging.","We show that Diffusion Soup samples from a point in weight space that approximates the geometric mean of the distributions of constituent datasets, which offers anti-memorization guarantees and enables zero-shot style mixing.","Empirically, Diffusion Soup outperforms a paragon model trained on the union of all data shards and achieves a 30% improvement in Image Reward (.34 $\\to$ .44) on domain sharded data, and a 59% improvement in IR (.37 $\\to$ .59) on aesthetic data.","In both cases, souping also prevails in TIFA score (respectively, 85.5 $\\to$ 86.5 and 85.6 $\\to$ 86.8).","We demonstrate robust unlearning -- removing any individual domain shard only lowers performance by 1% in IR (.45 $\\to$ .44) -- and validate our theoretical insights on anti-memorization using real data.","Finally, we showcase Diffusion Soup's ability to blend the distinct styles of models finetuned on different shards, resulting in the zero-shot generation of hybrid styles."],"url":"http://arxiv.org/abs/2406.08431v1","category":"cs.CV"}
{"created":"2024-06-12 17:14:44","title":"Improving Noise Robustness through Abstractions and its Impact on Machine Learning","abstract":"Noise is a fundamental problem in learning theory with huge effects in the application of Machine Learning (ML) methods, due to real world data tendency to be noisy. Additionally, introduction of malicious noise can make ML methods fail critically, as is the case with adversarial attacks. Thus, finding and developing alternatives to improve robustness to noise is a fundamental problem in ML. In this paper, we propose a method to deal with noise: mitigating its effect through the use of data abstractions. The goal is to reduce the effect of noise over the model's performance through the loss of information produced by the abstraction. However, this information loss comes with a cost: it can result in an accuracy reduction due to the missing information. First, we explored multiple methodologies to create abstractions, using the training dataset, for the specific case of numerical data and binary classification tasks. We also tested how these abstractions can affect robustness to noise with several experiments that explore the robustness of an Artificial Neural Network to noise when trained using raw data \\emph{vs} when trained using abstracted data. The results clearly show that using abstractions is a viable approach for developing noise robust ML methods.","sentences":["Noise is a fundamental problem in learning theory with huge effects in the application of Machine Learning (ML) methods, due to real world data tendency to be noisy.","Additionally, introduction of malicious noise can make ML methods fail critically, as is the case with adversarial attacks.","Thus, finding and developing alternatives to improve robustness to noise is a fundamental problem in ML.","In this paper, we propose a method to deal with noise: mitigating its effect through the use of data abstractions.","The goal is to reduce the effect of noise over the model's performance through the loss of information produced by the abstraction.","However, this information loss comes with a cost: it can result in an accuracy reduction due to the missing information.","First, we explored multiple methodologies to create abstractions, using the training dataset, for the specific case of numerical data and binary classification tasks.","We also tested how these abstractions can affect robustness to noise with several experiments that explore the robustness of an Artificial Neural Network to noise when trained using raw data \\emph{vs} when trained using abstracted data.","The results clearly show that using abstractions is a viable approach for developing noise robust ML methods."],"url":"http://arxiv.org/abs/2406.08428v1","category":"cs.LG"}
{"created":"2024-06-12 17:13:17","title":"Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL","abstract":"Generating accurate SQL according to natural language questions (text-to-SQL) is a long-standing problem since it is challenging in user question understanding, database schema comprehension, and SQL generation. Conventional text-to-SQL systems include human engineering and deep neural networks. Subsequently, pre-trained language models (PLMs) have been developed and utilized for text-to-SQL tasks, achieving promising performance. As modern databases become more complex and corresponding user questions more challenging, PLMs with limited comprehension capabilities can lead to incorrect SQL generation. This necessitates more sophisticated and tailored optimization methods, which, in turn, restricts the applications of PLM-based systems. Most recently, large language models (LLMs) have demonstrated significant abilities in natural language understanding as the model scale remains increasing. Therefore, integrating the LLM-based implementation can bring unique opportunities, challenges, and solutions to text-to-SQL research. In this survey, we present a comprehensive review of LLM-based text-to-SQL. Specifically, we propose a brief overview of the current challenges and the evolutionary process of text-to-SQL. Then, we provide a detailed introduction to the datasets and metrics designed to evaluate text-to-SQL systems. After that, we present a systematic analysis of recent advances in LLM-based text-to-SQL. Finally, we discuss the remaining challenges in this field and propose expectations for future directions.","sentences":["Generating accurate SQL according to natural language questions (text-to-SQL) is a long-standing problem since it is challenging in user question understanding, database schema comprehension, and SQL generation.","Conventional text-to-SQL systems include human engineering and deep neural networks.","Subsequently, pre-trained language models (PLMs) have been developed and utilized for text-to-SQL tasks, achieving promising performance.","As modern databases become more complex and corresponding user questions more challenging, PLMs with limited comprehension capabilities can lead to incorrect SQL generation.","This necessitates more sophisticated and tailored optimization methods, which, in turn, restricts the applications of PLM-based systems.","Most recently, large language models (LLMs) have demonstrated significant abilities in natural language understanding as the model scale remains increasing.","Therefore, integrating the LLM-based implementation can bring unique opportunities, challenges, and solutions to text-to-SQL research.","In this survey, we present a comprehensive review of LLM-based text-to-SQL.","Specifically, we propose a brief overview of the current challenges and the evolutionary process of text-to-SQL.","Then, we provide a detailed introduction to the datasets and metrics designed to evaluate text-to-SQL systems.","After that, we present a systematic analysis of recent advances in LLM-based text-to-SQL.","Finally, we discuss the remaining challenges in this field and propose expectations for future directions."],"url":"http://arxiv.org/abs/2406.08426v1","category":"cs.CL"}
{"created":"2024-06-12 17:10:27","title":"AWGUNET: Attention-Aided Wavelet Guided U-Net for Nuclei Segmentation in Histopathology Images","abstract":"Accurate nuclei segmentation in histopathological images is crucial for cancer diagnosis. Automating this process offers valuable support to clinical experts, as manual annotation is time-consuming and prone to human errors. However, automating nuclei segmentation presents challenges due to uncertain cell boundaries, intricate staining, and diverse structures. In this paper, we present a segmentation approach that combines the U-Net architecture with a DenseNet-121 backbone, harnessing the strengths of both to capture comprehensive contextual and spatial information. Our model introduces the Wavelet-guided channel attention module to enhance cell boundary delineation, along with a learnable weighted global attention module for channel-specific attention. The decoder module, composed of an upsample block and convolution block, further refines segmentation in handling staining patterns. The experimental results conducted on two publicly accessible histopathology datasets, namely Monuseg and TNBC, underscore the superiority of our proposed model, demonstrating its potential to advance histopathological image analysis and cancer diagnosis. The code is made available at: https://github.com/AyushRoy2001/AWGUNET.","sentences":["Accurate nuclei segmentation in histopathological images is crucial for cancer diagnosis.","Automating this process offers valuable support to clinical experts, as manual annotation is time-consuming and prone to human errors.","However, automating nuclei segmentation presents challenges due to uncertain cell boundaries, intricate staining, and diverse structures.","In this paper, we present a segmentation approach that combines the U-Net architecture with a DenseNet-121 backbone, harnessing the strengths of both to capture comprehensive contextual and spatial information.","Our model introduces the Wavelet-guided channel attention module to enhance cell boundary delineation, along with a learnable weighted global attention module for channel-specific attention.","The decoder module, composed of an upsample block and convolution block, further refines segmentation in handling staining patterns.","The experimental results conducted on two publicly accessible histopathology datasets, namely Monuseg and TNBC, underscore the superiority of our proposed model, demonstrating its potential to advance histopathological image analysis and cancer diagnosis.","The code is made available at: https://github.com/AyushRoy2001/AWGUNET."],"url":"http://arxiv.org/abs/2406.08425v1","category":"cs.CV"}
{"created":"2024-06-12 17:06:07","title":"State Soup: In-Context Skill Learning, Retrieval and Mixing","abstract":"A new breed of gated-linear recurrent neural networks has reached state-of-the-art performance on a range of sequence modeling problems. Such models naturally handle long sequences efficiently, as the cost of processing a new input is independent of sequence length. Here, we explore another advantage of these stateful sequence models, inspired by the success of model merging through parameter interpolation. Building on parallels between fine-tuning and in-context learning, we investigate whether we can treat internal states as task vectors that can be stored, retrieved, and then linearly combined, exploiting the linearity of recurrence. We study this form of fast model merging on Mamba-2.8b, a pretrained recurrent model, and present preliminary evidence that simple linear state interpolation methods suffice to improve next-token perplexity as well as downstream in-context learning task performance.","sentences":["A new breed of gated-linear recurrent neural networks has reached state-of-the-art performance on a range of sequence modeling problems.","Such models naturally handle long sequences efficiently, as the cost of processing a new input is independent of sequence length.","Here, we explore another advantage of these stateful sequence models, inspired by the success of model merging through parameter interpolation.","Building on parallels between fine-tuning and in-context learning, we investigate whether we can treat internal states as task vectors that can be stored, retrieved, and then linearly combined, exploiting the linearity of recurrence.","We study this form of fast model merging on Mamba-2.8b, a pretrained recurrent model, and present preliminary evidence that simple linear state interpolation methods suffice to improve next-token perplexity as well as downstream in-context learning task performance."],"url":"http://arxiv.org/abs/2406.08423v1","category":"cs.LG"}
{"created":"2024-06-12 17:05:21","title":"KSW criterion in large field models","abstract":"We extend the analytic description of complex no-boundary solutions in the context of inflation to large field models. We discuss the Kontsevich-Segal-Witten (KSW) criterion and find it is satisfied in small field models, while in large field models it depends on an integral involving $V'(\\phi)$ over the range of inflation. It follows that the criterion does not truly constrain inflationary phenomenology since one can complete any inflaton potential beyond observable scales so as to satisfy KSW.","sentences":["We extend the analytic description of complex no-boundary solutions in the context of inflation to large field models.","We discuss the Kontsevich-Segal-Witten (KSW) criterion and find it is satisfied in small field models, while in large field models it depends on an integral involving $V'(\\phi)$ over the range of inflation.","It follows that the criterion does not truly constrain inflationary phenomenology since one can complete any inflaton potential beyond observable scales so as to satisfy KSW."],"url":"http://arxiv.org/abs/2406.08422v1","category":"hep-th"}
{"created":"2024-06-12 17:05:08","title":"PRIBOOT: A New Data-Driven Expert for Improved Driving Simulations","abstract":"The development of Autonomous Driving (AD) systems in simulated environments like CARLA is crucial for advancing real-world automotive technologies. To drive innovation, CARLA introduced Leaderboard 2.0, significantly more challenging than its predecessor. However, current AD methods have struggled to achieve satisfactory outcomes due to a lack of sufficient ground truth data. Human driving logs provided by CARLA are insufficient, and previously successful expert agents like Autopilot and Roach, used for collecting datasets, have seen reduced effectiveness under these more demanding conditions. To overcome these data limitations, we introduce PRIBOOT, an expert agent that leverages limited human logs with privileged information. We have developed a novel BEV representation specifically tailored to meet the demands of this new benchmark and processed it as an RGB image to facilitate the application of transfer learning techniques, instead of using a set of masks. Additionally, we propose the Infraction Rate Score (IRS), a new evaluation metric designed to provide a more balanced assessment of driving performance over extended routes. PRIBOOT is the first model to achieve a Route Completion (RC) of 75% in Leaderboard 2.0, along with a Driving Score (DS) and IRS of 20% and 45%, respectively. With PRIBOOT, researchers can now generate extensive datasets, potentially solving the data availability issues that have hindered progress in this benchmark.","sentences":["The development of Autonomous Driving (AD) systems in simulated environments like CARLA is crucial for advancing real-world automotive technologies.","To drive innovation, CARLA introduced Leaderboard 2.0, significantly more challenging than its predecessor.","However, current AD methods have struggled to achieve satisfactory outcomes due to a lack of sufficient ground truth data.","Human driving logs provided by CARLA are insufficient, and previously successful expert agents like Autopilot and Roach, used for collecting datasets, have seen reduced effectiveness under these more demanding conditions.","To overcome these data limitations, we introduce PRIBOOT, an expert agent that leverages limited human logs with privileged information.","We have developed a novel BEV representation specifically tailored to meet the demands of this new benchmark and processed it as an RGB image to facilitate the application of transfer learning techniques, instead of using a set of masks.","Additionally, we propose the Infraction Rate Score (IRS), a new evaluation metric designed to provide a more balanced assessment of driving performance over extended routes.","PRIBOOT is the first model to achieve a Route Completion (RC) of 75% in Leaderboard 2.0, along with a Driving Score (DS) and IRS of 20% and 45%, respectively.","With PRIBOOT, researchers can now generate extensive datasets, potentially solving the data availability issues that have hindered progress in this benchmark."],"url":"http://arxiv.org/abs/2406.08421v1","category":"cs.RO"}
{"created":"2024-06-12 17:03:16","title":"Identification and Inference on Treatment Effects under Covariate-Adaptive Randomization and Imperfect Compliance","abstract":"Randomized controlled trials (RCTs) frequently utilize covariate-adaptive randomization (CAR) (e.g., stratified block randomization) and commonly suffer from imperfect compliance. This paper studies the identification and inference for the average treatment effect (ATE) and the average treatment effect on the treated (ATT) in such RCTs with a binary treatment.   We first develop characterizations of the identified sets for both estimands. Since data are generally not i.i.d. under CAR, these characterizations do not follow from existing results. We then provide consistent estimators of the identified sets and asymptotically valid confidence intervals for the parameters. Our asymptotic analysis leads to concrete practical recommendations regarding how to estimate the treatment assignment probabilities that enter in estimated bounds. In the case of the ATE, using sample analog assignment frequencies is more efficient than using the true assignment probabilities. On the contrary, using the true assignment probabilities is preferable for the ATT.","sentences":["Randomized controlled trials (RCTs) frequently utilize covariate-adaptive randomization (CAR) (e.g., stratified block randomization) and commonly suffer from imperfect compliance.","This paper studies the identification and inference for the average treatment effect (ATE) and the average treatment effect on the treated (ATT) in such RCTs with a binary treatment.   ","We first develop characterizations of the identified sets for both estimands.","Since data are generally not i.i.d.","under CAR, these characterizations do not follow from existing results.","We then provide consistent estimators of the identified sets and asymptotically valid confidence intervals for the parameters.","Our asymptotic analysis leads to concrete practical recommendations regarding how to estimate the treatment assignment probabilities that enter in estimated bounds.","In the case of the ATE, using sample analog assignment frequencies is more efficient than using the true assignment probabilities.","On the contrary, using the true assignment probabilities is preferable for the ATT."],"url":"http://arxiv.org/abs/2406.08419v1","category":"econ.EM"}
{"created":"2024-06-12 17:01:04","title":"OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text","abstract":"Image-text interleaved data, consisting of multiple images and texts arranged in a natural document format, aligns with the presentation paradigm of internet data and closely resembles human reading habits. Recent studies have shown that such data aids multimodal in-context learning and maintains the capabilities of large language models during multimodal fine-tuning. However, the limited scale and diversity of current image-text interleaved data restrict the development of multimodal large language models. In this paper, we introduce OmniCorpus, a 10 billion-scale image-text interleaved dataset. Using an efficient data engine, we filter and extract large-scale high-quality documents, which contain 8.6 billion images and 1,696 billion text tokens. Compared to counterparts (e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales while maintaining good data quality; 2) features more diverse sources, including both English and non-English websites as well as video-centric websites; 3) is more flexible, easily degradable from an image-text interleaved format to pure text corpus and image-text pairs. Through comprehensive analysis and experiments, we validate the quality, usability, and effectiveness of the proposed dataset. We hope this could provide a solid data foundation for future multimodal model research. Code and data are released at https://github.com/OpenGVLab/OmniCorpus.","sentences":["Image-text interleaved data, consisting of multiple images and texts arranged in a natural document format, aligns with the presentation paradigm of internet data and closely resembles human reading habits.","Recent studies have shown that such data aids multimodal in-context learning and maintains the capabilities of large language models during multimodal fine-tuning.","However, the limited scale and diversity of current image-text interleaved data restrict the development of multimodal large language models.","In this paper, we introduce OmniCorpus, a 10 billion-scale image-text interleaved dataset.","Using an efficient data engine, we filter and extract large-scale high-quality documents, which contain 8.6 billion images and 1,696 billion text tokens.","Compared to counterparts (e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales while maintaining good data quality; 2) features more diverse sources, including both English and non-English websites as well as video-centric websites; 3) is more flexible, easily degradable from an image-text interleaved format to pure text corpus and image-text pairs.","Through comprehensive analysis and experiments, we validate the quality, usability, and effectiveness of the proposed dataset.","We hope this could provide a solid data foundation for future multimodal model research.","Code and data are released at https://github.com/OpenGVLab/OmniCorpus."],"url":"http://arxiv.org/abs/2406.08418v1","category":"cs.CV"}
{"created":"2024-06-12 16:57:58","title":"Memory Is All You Need: An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference","abstract":"Large language models (LLMs) have recently transformed natural language processing, enabling machines to generate human-like text and engage in meaningful conversations. This development necessitates speed, efficiency, and accessibility in LLM inference as the computational and memory requirements of these systems grow exponentially. Meanwhile, advancements in computing and memory capabilities are lagging behind, exacerbated by the discontinuation of Moore's law. With LLMs exceeding the capacity of single GPUs, they require complex, expert-level configurations for parallel processing. Memory accesses become significantly more expensive than computation, posing a challenge for efficient scaling, known as the memory wall. Here, compute-in-memory (CIM) technologies offer a promising solution for accelerating AI inference by directly performing analog computations in memory, potentially reducing latency and power consumption. By closely integrating memory and compute elements, CIM eliminates the von Neumann bottleneck, reducing data movement and improving energy efficiency. This survey paper provides an overview and analysis of transformer-based models, reviewing various CIM architectures and exploring how they can address the imminent challenges of modern AI computing systems. We discuss transformer-related operators and their hardware acceleration schemes and highlight challenges, trends, and insights in corresponding CIM designs.","sentences":["Large language models (LLMs) have recently transformed natural language processing, enabling machines to generate human-like text and engage in meaningful conversations.","This development necessitates speed, efficiency, and accessibility in LLM inference as the computational and memory requirements of these systems grow exponentially.","Meanwhile, advancements in computing and memory capabilities are lagging behind, exacerbated by the discontinuation of Moore's law.","With LLMs exceeding the capacity of single GPUs, they require complex, expert-level configurations for parallel processing.","Memory accesses become significantly more expensive than computation, posing a challenge for efficient scaling, known as the memory wall.","Here, compute-in-memory (CIM) technologies offer a promising solution for accelerating AI inference by directly performing analog computations in memory, potentially reducing latency and power consumption.","By closely integrating memory and compute elements, CIM eliminates the von Neumann bottleneck, reducing data movement and improving energy efficiency.","This survey paper provides an overview and analysis of transformer-based models, reviewing various CIM architectures and exploring how they can address the imminent challenges of modern AI computing systems.","We discuss transformer-related operators and their hardware acceleration schemes and highlight challenges, trends, and insights in corresponding CIM designs."],"url":"http://arxiv.org/abs/2406.08413v1","category":"cs.AR"}
{"created":"2024-06-12 16:57:28","title":"Tailoring Generative AI Chatbots for Multiethnic Communities in Disaster Preparedness Communication: Extending the CASA Paradigm","abstract":"This study is among the first to develop different prototypes of generative AI (GenAI) chatbots powered by GPT 4 to communicate hurricane preparedness information to diverse residents. Drawing from the Computers Are Social Actors (CASA) paradigm and the literature on disaster vulnerability and cultural tailoring, this study conducted a between-subjects experiment with 441 Black, Hispanic, and Caucasian residents of Florida. A computational analysis of chat logs (N = 7,848) shows that anthropomorphism and personalization are key communication topics in GenAI chatbot-user interactions. SEM results (N = 441) suggest that GenAI chatbots varying in tone formality and cultural tailoring significantly predict bot perceptions and, subsequently, hurricane preparedness outcomes. These results highlight the potential of using GenAI chatbots to improve diverse communities' disaster preparedness.","sentences":["This study is among the first to develop different prototypes of generative AI (GenAI) chatbots powered by GPT 4 to communicate hurricane preparedness information to diverse residents.","Drawing from the Computers Are Social Actors (CASA) paradigm and the literature on disaster vulnerability and cultural tailoring, this study conducted a between-subjects experiment with 441 Black, Hispanic, and Caucasian residents of Florida.","A computational analysis of chat logs (N = 7,848) shows that anthropomorphism and personalization are key communication topics in GenAI chatbot-user interactions.","SEM results (N = 441) suggest that GenAI chatbots varying in tone formality and cultural tailoring significantly predict bot perceptions and, subsequently, hurricane preparedness outcomes.","These results highlight the potential of using GenAI chatbots to improve diverse communities' disaster preparedness."],"url":"http://arxiv.org/abs/2406.08411v1","category":"cs.CL"}
{"created":"2024-06-12 16:56:45","title":"Quasistationary hair for binary black hole initial data in scalar Gauss-Bonnet gravity","abstract":"Recent efforts to numerically simulate compact objects in alternative theories of gravity have largely focused on the time-evolution equations. Another critical aspect is the construction of constraint-satisfying initial data with precise control over the properties of the systems under consideration. Here, we augment the extended conformal thin sandwich framework to construct quasistationary initial data for black hole systems in scalar Gauss-Bonnet theory and numerically implement it in the open-source SpECTRE code. Despite the resulting elliptic system being singular at black hole horizons, we demonstrate how to construct numerical solutions that extend smoothly across the horizon. We obtain quasistationary scalar hair configurations in the test-field limit for black holes with linear/angular momentum as well as for black hole binaries. For isolated black holes, we explicitly show that the scalar profile obtained is stationary by evolving the system in time and compare against previous formulations of scalar Gauss-Bonnet initial data. In the case of the binary, we find that the scalar hair near the black holes can be markedly altered by the presence of the other black hole. The initial data constructed here enables targeted simulations in scalar Gauss-Bonnet simulations with reduced initial transients.","sentences":["Recent efforts to numerically simulate compact objects in alternative theories of gravity have largely focused on the time-evolution equations.","Another critical aspect is the construction of constraint-satisfying initial data with precise control over the properties of the systems under consideration.","Here, we augment the extended conformal thin sandwich framework to construct quasistationary initial data for black hole systems in scalar Gauss-Bonnet theory and numerically implement it in the open-source SpECTRE code.","Despite the resulting elliptic system being singular at black hole horizons, we demonstrate how to construct numerical solutions that extend smoothly across the horizon.","We obtain quasistationary scalar hair configurations in the test-field limit for black holes with linear/angular momentum as well as for black hole binaries.","For isolated black holes, we explicitly show that the scalar profile obtained is stationary by evolving the system in time and compare against previous formulations of scalar Gauss-Bonnet initial data.","In the case of the binary, we find that the scalar hair near the black holes can be markedly altered by the presence of the other black hole.","The initial data constructed here enables targeted simulations in scalar Gauss-Bonnet simulations with reduced initial transients."],"url":"http://arxiv.org/abs/2406.08410v1","category":"gr-qc"}
{"created":"2024-06-12 16:54:54","title":"MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos","abstract":"Multimodal Language Language Models (MLLMs) demonstrate the emerging abilities of \"world models\" -- interpreting and reasoning about complex real-world dynamics. To assess these abilities, we posit videos are the ideal medium, as they encapsulate rich representations of real-world dynamics and causalities. To this end, we introduce MMWorld, a new benchmark for multi-discipline, multi-faceted multimodal video understanding. MMWorld distinguishes itself from previous video understanding benchmarks with two unique advantages: (1) multi-discipline, covering various disciplines that often require domain expertise for comprehensive understanding; (2) multi-faceted reasoning, including explanation, counterfactual thinking, future prediction, etc. MMWorld consists of a human-annotated dataset to evaluate MLLMs with questions about the whole videos and a synthetic dataset to analyze MLLMs within a single modality of perception. Together, MMWorld encompasses 1,910 videos across seven broad disciplines and 69 subdisciplines, complete with 6,627 question-answer pairs and associated captions. The evaluation includes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld (e.g., GPT-4V performs the best with only 52.3\\% accuracy), showing large room for improvement. Further ablation studies reveal other interesting findings such as models' different skill sets from humans. We hope MMWorld can serve as an essential step towards world model evaluation in videos.","sentences":["Multimodal Language Language Models (MLLMs) demonstrate the emerging abilities of \"world models\" -- interpreting and reasoning about complex real-world dynamics.","To assess these abilities, we posit videos are the ideal medium, as they encapsulate rich representations of real-world dynamics and causalities.","To this end, we introduce MMWorld, a new benchmark for multi-discipline, multi-faceted multimodal video understanding.","MMWorld distinguishes itself from previous video understanding benchmarks with two unique advantages: (1) multi-discipline, covering various disciplines that often require domain expertise for comprehensive understanding; (2) multi-faceted reasoning, including explanation, counterfactual thinking, future prediction, etc.","MMWorld consists of a human-annotated dataset to evaluate MLLMs with questions about the whole videos and a synthetic dataset to analyze MLLMs within a single modality of perception.","Together, MMWorld encompasses 1,910 videos across seven broad disciplines and 69 subdisciplines, complete with 6,627 question-answer pairs and associated captions.","The evaluation includes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld (e.g., GPT-4V performs the best with only 52.3\\% accuracy), showing large room for improvement.","Further ablation studies reveal other interesting findings such as models' different skill sets from humans.","We hope MMWorld can serve as an essential step towards world model evaluation in videos."],"url":"http://arxiv.org/abs/2406.08407v1","category":"cs.CV"}
{"created":"2024-06-12 16:52:54","title":"Scaling Value Iteration Networks to 5000 Layers for Extreme Long-Term Planning","abstract":"The Value Iteration Network (VIN) is an end-to-end differentiable architecture that performs value iteration on a latent MDP for planning in reinforcement learning (RL). However, VINs struggle to scale to long-term and large-scale planning tasks, such as navigating a $100\\times 100$ maze -- a task which typically requires thousands of planning steps to solve. We observe that this deficiency is due to two issues: the representation capacity of the latent MDP and the planning module's depth. We address these by augmenting the latent MDP with a dynamic transition kernel, dramatically improving its representational capacity, and, to mitigate the vanishing gradient problem, introducing an \"adaptive highway loss\" that constructs skip connections to improve gradient flow. We evaluate our method on both 2D maze navigation environments and the ViZDoom 3D navigation benchmark. We find that our new method, named Dynamic Transition VIN (DT-VIN), easily scales to 5000 layers and casually solves challenging versions of the above tasks. Altogether, we believe that DT-VIN represents a concrete step forward in performing long-term large-scale planning in RL environments.","sentences":["The Value Iteration Network (VIN) is an end-to-end differentiable architecture that performs value iteration on a latent MDP for planning in reinforcement learning (RL).","However, VINs struggle to scale to long-term and large-scale planning tasks, such as navigating a $100\\times 100$ maze -- a task which typically requires thousands of planning steps to solve.","We observe that this deficiency is due to two issues: the representation capacity of the latent MDP and the planning module's depth.","We address these by augmenting the latent MDP with a dynamic transition kernel, dramatically improving its representational capacity, and, to mitigate the vanishing gradient problem, introducing an \"adaptive highway loss\" that constructs skip connections to improve gradient flow.","We evaluate our method on both 2D maze navigation environments and the ViZDoom 3D navigation benchmark.","We find that our new method, named Dynamic Transition VIN (DT-VIN), easily scales to 5000 layers and casually solves challenging versions of the above tasks.","Altogether, we believe that DT-VIN represents a concrete step forward in performing long-term large-scale planning in RL environments."],"url":"http://arxiv.org/abs/2406.08404v1","category":"cs.LG"}
{"created":"2024-06-12 16:47:54","title":"Differentiable Cost-Parameterized Monge Map Estimators","abstract":"Within the field of optimal transport (OT), the choice of ground cost is crucial to ensuring that the optimality of a transport map corresponds to usefulness in real-world applications. It is therefore desirable to use known information to tailor cost functions and hence learn OT maps which are adapted to the problem at hand. By considering a class of neural ground costs whose Monge maps have a known form, we construct a differentiable Monge map estimator which can be optimized to be consistent with known information about an OT map. In doing so, we simultaneously learn both an OT map estimator and a corresponding adapted cost function. Through suitable choices of loss function, our method provides a general approach for incorporating prior information about the Monge map itself when learning adapted OT maps and cost functions.","sentences":["Within the field of optimal transport (OT), the choice of ground cost is crucial to ensuring that the optimality of a transport map corresponds to usefulness in real-world applications.","It is therefore desirable to use known information to tailor cost functions and hence learn OT maps which are adapted to the problem at hand.","By considering a class of neural ground costs whose Monge maps have a known form, we construct a differentiable Monge map estimator which can be optimized to be consistent with known information about an OT map.","In doing so, we simultaneously learn both an OT map estimator and a corresponding adapted cost function.","Through suitable choices of loss function, our method provides a general approach for incorporating prior information about the Monge map itself when learning adapted OT maps and cost functions."],"url":"http://arxiv.org/abs/2406.08399v1","category":"stat.ML"}
{"created":"2024-06-12 16:46:12","title":"cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers","abstract":"An emerging area of research in situated and multimodal interactive conversations (SIMMC) includes interactions in scientific papers. Since scientific papers are primarily composed of text, equations, figures, and tables, SIMMC methods must be developed specifically for each component to support the depth of inquiry and interactions required by research scientists. This work introduces Conversational Papers (cPAPERS), a dataset of conversational question-answer pairs from reviews of academic papers grounded in these paper components and their associated references from scientific documents available on arXiv. We present a data collection strategy to collect these question-answer pairs from OpenReview and associate them with contextual information from LaTeX source files. Additionally, we present a series of baseline approaches utilizing Large Language Models (LLMs) in both zero-shot and fine-tuned configurations to address the cPAPERS dataset.","sentences":["An emerging area of research in situated and multimodal interactive conversations (SIMMC) includes interactions in scientific papers.","Since scientific papers are primarily composed of text, equations, figures, and tables, SIMMC methods must be developed specifically for each component to support the depth of inquiry and interactions required by research scientists.","This work introduces Conversational Papers (cPAPERS), a dataset of conversational question-answer pairs from reviews of academic papers grounded in these paper components and their associated references from scientific documents available on arXiv.","We present a data collection strategy to collect these question-answer pairs from OpenReview and associate them with contextual information from LaTeX source files.","Additionally, we present a series of baseline approaches utilizing Large Language Models (LLMs) in both zero-shot and fine-tuned configurations to address the cPAPERS dataset."],"url":"http://arxiv.org/abs/2406.08398v1","category":"cs.CL"}
{"created":"2024-06-12 16:45:37","title":"A Note on a Generalized Two Component Camassa-Holm System in Sobolev Spaces","abstract":"In this paper, we consider a generalized two component Camassa-Holm system. Based on local well-posedness results and lifespan estimates, we establish sharpness of continuity on the data-to-solution map by showing that it is not uniformly continuous from product Sobolev spaces $H^s \\times H^{s}$ to $C([0,T]; H^s \\times H^{s})$. The proof of nonuniform dependence is based upon approximate solutions.","sentences":["In this paper, we consider a generalized two component Camassa-Holm system.","Based on local well-posedness results and lifespan estimates, we establish sharpness of continuity on the data-to-solution map by showing that it is not uniformly continuous from product Sobolev spaces $H^s \\times H^{s}$ to $C([0,T]; H^s \\times","H^{s})$.","The proof of nonuniform dependence is based upon approximate solutions."],"url":"http://arxiv.org/abs/2406.08397v1","category":"math.AP"}
{"created":"2024-06-12 16:45:35","title":"Neural Blind Source Separation and Diarization for Distant Speech Recognition","abstract":"This paper presents a neural method for distant speech recognition (DSR) that jointly separates and diarizes speech mixtures without supervision by isolated signals. A standard separation method for multi-talker DSR is a statistical multichannel method called guided source separation (GSS). While GSS does not require signal-level supervision, it relies on speaker diarization results to handle unknown numbers of active speakers. To overcome this limitation, we introduce and train a neural inference model in a weakly-supervised manner, employing the objective function of a statistical separation method. This training requires only multichannel mixtures and their temporal annotations of speaker activities. In contrast to GSS, the trained model can jointly separate and diarize speech mixtures without any auxiliary information. The experiments with the AMI corpus show that our method outperforms GSS with oracle diarization results regarding word error rates. The code is available online.","sentences":["This paper presents a neural method for distant speech recognition (DSR) that jointly separates and diarizes speech mixtures without supervision by isolated signals.","A standard separation method for multi-talker DSR is a statistical multichannel method called guided source separation (GSS).","While GSS does not require signal-level supervision, it relies on speaker diarization results to handle unknown numbers of active speakers.","To overcome this limitation, we introduce and train a neural inference model in a weakly-supervised manner, employing the objective function of a statistical separation method.","This training requires only multichannel mixtures and their temporal annotations of speaker activities.","In contrast to GSS, the trained model can jointly separate and diarize speech mixtures without any auxiliary information.","The experiments with the AMI corpus show that our method outperforms GSS with oracle diarization results regarding word error rates.","The code is available online."],"url":"http://arxiv.org/abs/2406.08396v1","category":"eess.AS"}
{"created":"2024-06-12 16:44:50","title":"VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks","abstract":"We present VisionLLM v2, an end-to-end generalist multimodal large model (MLLM) that unifies visual perception, understanding, and generation within a single framework. Unlike traditional MLLMs limited to text output, VisionLLM v2 significantly broadens its application scope. It excels not only in conventional visual question answering (VQA) but also in open-ended, cross-domain vision tasks such as object localization, pose estimation, and image generation and editing. To this end, we propose a new information transmission mechanism termed \"super link\", as a medium to connect MLLM with task-specific decoders. It not only allows flexible transmission of task information and gradient feedback between the MLLM and multiple downstream decoders but also effectively resolves training conflicts in multi-tasking scenarios. In addition, to support the diverse range of tasks, we carefully collected and combed training data from hundreds of public vision and vision-language tasks. In this way, our model can be joint-trained end-to-end on hundreds of vision language tasks and generalize to these tasks using a set of shared parameters through different user prompts, achieving performance comparable to task-specific models. We believe VisionLLM v2 will offer a new perspective on the generalization of MLLMs.","sentences":["We present VisionLLM v2, an end-to-end generalist multimodal large model (MLLM) that unifies visual perception, understanding, and generation within a single framework.","Unlike traditional MLLMs limited to text output, VisionLLM v2 significantly broadens its application scope.","It excels not only in conventional visual question answering (VQA) but also in open-ended, cross-domain vision tasks such as object localization, pose estimation, and image generation and editing.","To this end, we propose a new information transmission mechanism termed \"super link\", as a medium to connect MLLM with task-specific decoders.","It not only allows flexible transmission of task information and gradient feedback between the MLLM and multiple downstream decoders but also effectively resolves training conflicts in multi-tasking scenarios.","In addition, to support the diverse range of tasks, we carefully collected and combed training data from hundreds of public vision and vision-language tasks.","In this way, our model can be joint-trained end-to-end on hundreds of vision language tasks and generalize to these tasks using a set of shared parameters through different user prompts, achieving performance comparable to task-specific models.","We believe VisionLLM v2 will offer a new perspective on the generalization of MLLMs."],"url":"http://arxiv.org/abs/2406.08394v1","category":"cs.CV"}
{"created":"2024-06-12 16:43:47","title":"FontStudio: Shape-Adaptive Diffusion Model for Coherent and Consistent Font Effect Generation","abstract":"Recently, the application of modern diffusion-based text-to-image generation models for creating artistic fonts, traditionally the domain of professional designers, has garnered significant interest. Diverging from the majority of existing studies that concentrate on generating artistic typography, our research aims to tackle a novel and more demanding challenge: the generation of text effects for multilingual fonts. This task essentially requires generating coherent and consistent visual content within the confines of a font-shaped canvas, as opposed to a traditional rectangular canvas. To address this task, we introduce a novel shape-adaptive diffusion model capable of interpreting the given shape and strategically planning pixel distributions within the irregular canvas. To achieve this, we curate a high-quality shape-adaptive image-text dataset and incorporate the segmentation mask as a visual condition to steer the image generation process within the irregular-canvas. This approach enables the traditionally rectangle canvas-based diffusion model to produce the desired concepts in accordance with the provided geometric shapes. Second, to maintain consistency across multiple letters, we also present a training-free, shape-adaptive effect transfer method for transferring textures from a generated reference letter to others. The key insights are building a font effect noise prior and propagating the font effect information in a concatenated latent space. The efficacy of our FontStudio system is confirmed through user preference studies, which show a marked preference (78% win-rates on aesthetics) for our system even when compared to the latest unrivaled commercial product, Adobe Firefly.","sentences":["Recently, the application of modern diffusion-based text-to-image generation models for creating artistic fonts, traditionally the domain of professional designers, has garnered significant interest.","Diverging from the majority of existing studies that concentrate on generating artistic typography, our research aims to tackle a novel and more demanding challenge: the generation of text effects for multilingual fonts.","This task essentially requires generating coherent and consistent visual content within the confines of a font-shaped canvas, as opposed to a traditional rectangular canvas.","To address this task, we introduce a novel shape-adaptive diffusion model capable of interpreting the given shape and strategically planning pixel distributions within the irregular canvas.","To achieve this, we curate a high-quality shape-adaptive image-text dataset and incorporate the segmentation mask as a visual condition to steer the image generation process within the irregular-canvas.","This approach enables the traditionally rectangle canvas-based diffusion model to produce the desired concepts in accordance with the provided geometric shapes.","Second, to maintain consistency across multiple letters, we also present a training-free, shape-adaptive effect transfer method for transferring textures from a generated reference letter to others.","The key insights are building a font effect noise prior and propagating the font effect information in a concatenated latent space.","The efficacy of our FontStudio system is confirmed through user preference studies, which show a marked preference (78% win-rates on aesthetics) for our system even when compared to the latest unrivaled commercial product, Adobe Firefly."],"url":"http://arxiv.org/abs/2406.08392v1","category":"cs.CV"}
{"created":"2024-06-12 16:41:31","title":"Large Language Models Must Be Taught to Know What They Don't Know","abstract":"When using large language models (LLMs) in high-stakes applications, we need to know when we can trust their predictions. Some works argue that prompting high-performance LLMs is sufficient to produce calibrated uncertainties, while others introduce sampling methods that can be prohibitively expensive. In this work, we first argue that prompting on its own is insufficient to achieve good calibration and then show that fine-tuning on a small dataset of correct and incorrect answers can create an uncertainty estimate with good generalization and small computational overhead. We show that a thousand graded examples are sufficient to outperform baseline methods and that training through the features of a model is necessary for good performance and tractable for large open-source models when using LoRA. We also investigate the mechanisms that enable reliable LLM uncertainty estimation, finding that many models can be used as general-purpose uncertainty estimators, applicable not just to their own uncertainties but also the uncertainty of other models. Lastly, we show that uncertainty estimates inform human use of LLMs in human-AI collaborative settings through a user study.","sentences":["When using large language models (LLMs) in high-stakes applications, we need to know when we can trust their predictions.","Some works argue that prompting high-performance LLMs is sufficient to produce calibrated uncertainties, while others introduce sampling methods that can be prohibitively expensive.","In this work, we first argue that prompting on its own is insufficient to achieve good calibration and then show that fine-tuning on a small dataset of correct and incorrect answers can create an uncertainty estimate with good generalization and small computational overhead.","We show that a thousand graded examples are sufficient to outperform baseline methods and that training through the features of a model is necessary for good performance and tractable for large open-source models when using LoRA.","We also investigate the mechanisms that enable reliable LLM uncertainty estimation, finding that many models can be used as general-purpose uncertainty estimators, applicable not just to their own uncertainties but also the uncertainty of other models.","Lastly, we show that uncertainty estimates inform human use of LLMs in human-AI collaborative settings through a user study."],"url":"http://arxiv.org/abs/2406.08391v1","category":"cs.LG"}
{"created":"2024-06-12 16:38:43","title":"Topological linear response of hyperbolic Chern insulators","abstract":"We establish a connection between the electromagnetic Hall response and band topological invariants in hyperbolic Chern insulators by deriving a hyperbolic analog of the Thouless-Kohmoto-Nightingale-den Nijs (TKNN) formula. By generalizing the Kubo formula to hyperbolic lattices, we show that the Hall conductivity is quantized to $-e^2C_{ij}/h$, where $C_{ij}$ is the first Chern number. Through a flux-threading argument, we provide an interpretation of the Chern number as a topological invariant in hyperbolic band theory. We demonstrate that, although it receives contributions from both Abelian and non-Abelian Bloch states, the Chern number can be calculated solely from Abelian states, resulting in a tremendous simplification of the topological band theory. Finally, we verify our results numerically by computing various Chern numbers in the hyperbolic Haldane model.","sentences":["We establish a connection between the electromagnetic Hall response and band topological invariants in hyperbolic Chern insulators by deriving a hyperbolic analog of the Thouless-Kohmoto-Nightingale-den Nijs (TKNN) formula.","By generalizing the Kubo formula to hyperbolic lattices, we show that the Hall conductivity is quantized to $-e^2C_{ij}/h$, where $C_{ij}$ is the first Chern number.","Through a flux-threading argument, we provide an interpretation of the Chern number as a topological invariant in hyperbolic band theory.","We demonstrate that, although it receives contributions from both Abelian and non-Abelian Bloch states, the Chern number can be calculated solely from Abelian states, resulting in a tremendous simplification of the topological band theory.","Finally, we verify our results numerically by computing various Chern numbers in the hyperbolic Haldane model."],"url":"http://arxiv.org/abs/2406.08388v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-12 16:36:06","title":"Banal Deception Human-AI Ecosystems: A Study of People's Perceptions of LLM-generated Deceptive Behaviour","abstract":"Large language models (LLMs) can provide users with false, inaccurate, or misleading information, and we consider the output of this type of information as what Natale (2021) calls `banal' deceptive behaviour. Here, we investigate peoples' perceptions of ChatGPT-generated deceptive behaviour and how this affects peoples' own behaviour and trust. To do this, we use a mixed-methods approach comprising of (i) an online survey with 220 participants and (ii) semi-structured interviews with 12 participants. Our results show that (i) the most common types of deceptive information encountered were over-simplifications and outdated information; (ii) humans' perceptions of trust and `worthiness' of talking to ChatGPT are impacted by `banal' deceptive behaviour; (iii) the perceived responsibility for deception is influenced by education level and the frequency of deceptive information; and (iv) users become more cautious after encountering deceptive information, but they come to trust the technology more when they identify advantages of using it. Our findings contribute to the understanding of human-AI interaction dynamics in the context of \\textit{Deceptive AI Ecosystems}, and highlight the importance of user-centric approaches to mitigating the potential harms of deceptive AI technologies.","sentences":["Large language models (LLMs) can provide users with false, inaccurate, or misleading information, and we consider the output of this type of information as what Natale (2021) calls `banal' deceptive behaviour.","Here, we investigate peoples' perceptions of ChatGPT-generated deceptive behaviour and how this affects peoples' own behaviour and trust.","To do this, we use a mixed-methods approach comprising of (i) an online survey with 220 participants and (ii) semi-structured interviews with 12 participants.","Our results show that (i) the most common types of deceptive information encountered were over-simplifications and outdated information; (ii) humans' perceptions of trust and `worthiness' of talking to ChatGPT are impacted by `banal' deceptive behaviour; (iii) the perceived responsibility for deception is influenced by education level and the frequency of deceptive information; and (iv) users become more cautious after encountering deceptive information, but they come to trust the technology more when they identify advantages of using it.","Our findings contribute to the understanding of human-AI interaction dynamics in the context of \\textit{Deceptive AI Ecosystems}, and highlight the importance of user-centric approaches to mitigating the potential harms of deceptive AI technologies."],"url":"http://arxiv.org/abs/2406.08386v1","category":"cs.CY"}
{"created":"2024-06-12 16:34:26","title":"Diff-A-Riff: Musical Accompaniment Co-creation via Latent Diffusion Models","abstract":"Recent advancements in deep generative models present new opportunities for music production but also pose challenges, such as high computational demands and limited audio quality. Moreover, current systems frequently rely solely on text input and typically focus on producing complete musical pieces, which is incompatible with existing workflows in music production. To address these issues, we introduce \"Diff-A-Riff,\" a Latent Diffusion Model designed to generate high-quality instrumental accompaniments adaptable to any musical context. This model offers control through either audio references, text prompts, or both, and produces 48kHz pseudo-stereo audio while significantly reducing inference time and memory usage. We demonstrate the model's capabilities through objective metrics and subjective listening tests, with extensive examples available on the accompanying website: sonycslparis.github.io/diffariff-companion/","sentences":["Recent advancements in deep generative models present new opportunities for music production but also pose challenges, such as high computational demands and limited audio quality.","Moreover, current systems frequently rely solely on text input and typically focus on producing complete musical pieces, which is incompatible with existing workflows in music production.","To address these issues, we introduce \"Diff-A-Riff,\" a Latent Diffusion Model designed to generate high-quality instrumental accompaniments adaptable to any musical context.","This model offers control through either audio references, text prompts, or both, and produces 48kHz pseudo-stereo audio while significantly reducing inference time and memory usage.","We demonstrate the model's capabilities through objective metrics and subjective listening tests, with extensive examples available on the accompanying website: sonycslparis.github.io/diffariff-companion/"],"url":"http://arxiv.org/abs/2406.08384v1","category":"cs.SD"}
{"created":"2024-06-12 16:31:08","title":"Moving Manifolds and General Relativity","abstract":"We revise general relativity (GR) from the perspective of calculus for moving surfaces (CMS). While GR is intrinsically constructed in pseudo-Riemannian geometry, a complete understanding of moving manifolds requires embedding in a higher dimension. It can only be defined by extrinsic Gaussian differential geometry and its extension to moving surfaces, known as CMS. Following the recent developments in CMS, we present a new derivation for the Einstein field equation and demonstrate the fundamental limitations of GR. Explicitly, we show that GR is an approximation of moving manifold equations and only stands for dominantly compressible space-time. While GR, with a cosmological constant, predicts an expanding universe, CMS shows fluctuation between inflation and collapse. We also show that the specific solution to GR with cosmological constant is constant mean curvature shapes. In the end, by presenting calculations for incompressible but deforming two-dimensional spheres, we indicate that material points moving with constant spherical velocities move like waves, strongly suggesting a resolution of the wave-corpuscular dualism problem.","sentences":["We revise general relativity (GR) from the perspective of calculus for moving surfaces (CMS).","While GR is intrinsically constructed in pseudo-Riemannian geometry, a complete understanding of moving manifolds requires embedding in a higher dimension.","It can only be defined by extrinsic Gaussian differential geometry and its extension to moving surfaces, known as CMS.","Following the recent developments in CMS, we present a new derivation for the Einstein field equation and demonstrate the fundamental limitations of GR.","Explicitly, we show that GR is an approximation of moving manifold equations and only stands for dominantly compressible space-time.","While GR, with a cosmological constant, predicts an expanding universe, CMS shows fluctuation between inflation and collapse.","We also show that the specific solution to GR with cosmological constant is constant mean curvature shapes.","In the end, by presenting calculations for incompressible but deforming two-dimensional spheres, we indicate that material points moving with constant spherical velocities move like waves, strongly suggesting a resolution of the wave-corpuscular dualism problem."],"url":"http://arxiv.org/abs/2406.08382v1","category":"gr-qc"}
{"created":"2024-06-12 16:23:52","title":"The unresolved mystery of dust particle swarms within the magnetosphere","abstract":"Early-generation in-situ dust detectors in near-Earth space have reported the occurrence of clusters of sub-micron dust particles that seemed unrelated to human spaceflight activities. In particular, data from the impact ionization detector onboard the HEOS-2 satellite indicate that such swarms of particles occur throughout the Earth's magnetosphere up to altitudes of 60,000 km -- far beyond regions typically used by spacecraft. Further account of high-altitude clusters has since been given by the GEO-deployed GORID detector, however, explanations for the latter have so far only been sought in GEO spaceflight activity.   This perspective piece reviews dust cluster detections in near-Earth space, emphasizing the natural swarm creation mechanism conjectured to explain the HEOS-2 data -- that is, the electrostatic disruption of meteoroids. Highlighting this mechanism offers a novel viewpoint on more recent near-Earth dust measurements. We further show that the impact clusters observed by both HEOS-2 and GORID are correlated with increased geomagnetic activity. This consistent correlation supports the notion that both sets of observations stem from the same underlying phenomenon and aligns with the hypothesis of the electrostatic breakup origin. We conclude that the nature of these peculiar swarms remains highly uncertain, advocating for their concerted investigation by forthcoming dust science endeavours, such as the JAXA/DLR DESTINY+ mission.","sentences":["Early-generation in-situ dust detectors in near-Earth space have reported the occurrence of clusters of sub-micron dust particles that seemed unrelated to human spaceflight activities.","In particular, data from the impact ionization detector onboard the HEOS-2 satellite indicate that such swarms of particles occur throughout the Earth's magnetosphere up to altitudes of 60,000 km -- far beyond regions typically used by spacecraft.","Further account of high-altitude clusters has since been given by the GEO-deployed GORID detector, however, explanations for the latter have so far only been sought in GEO spaceflight activity.   ","This perspective piece reviews dust cluster detections in near-Earth space, emphasizing the natural swarm creation mechanism conjectured to explain the HEOS-2 data -- that is, the electrostatic disruption of meteoroids.","Highlighting this mechanism offers a novel viewpoint on more recent near-Earth dust measurements.","We further show that the impact clusters observed by both HEOS-2 and GORID are correlated with increased geomagnetic activity.","This consistent correlation supports the notion that both sets of observations stem from the same underlying phenomenon and aligns with the hypothesis of the electrostatic breakup origin.","We conclude that the nature of these peculiar swarms remains highly uncertain, advocating for their concerted investigation by forthcoming dust science endeavours, such as the JAXA/DLR DESTINY+ mission."],"url":"http://arxiv.org/abs/2406.08376v1","category":"astro-ph.EP"}
{"created":"2024-06-12 16:22:41","title":"2.5D Multi-view Averaging Diffusion Model for 3D Medical Image Translation: Application to Low-count PET Reconstruction with CT-less Attenuation Correction","abstract":"Positron Emission Tomography (PET) is an important clinical imaging tool but inevitably introduces radiation hazards to patients and healthcare providers. Reducing the tracer injection dose and eliminating the CT acquisition for attenuation correction can reduce the overall radiation dose, but often results in PET with high noise and bias. Thus, it is desirable to develop 3D methods to translate the non-attenuation-corrected low-dose PET (NAC-LDPET) into attenuation-corrected standard-dose PET (AC-SDPET). Recently, diffusion models have emerged as a new state-of-the-art deep learning method for image-to-image translation, better than traditional CNN-based methods. However, due to the high computation cost and memory burden, it is largely limited to 2D applications. To address these challenges, we developed a novel 2.5D Multi-view Averaging Diffusion Model (MADM) for 3D image-to-image translation with application on NAC-LDPET to AC-SDPET translation. Specifically, MADM employs separate diffusion models for axial, coronal, and sagittal views, whose outputs are averaged in each sampling step to ensure the 3D generation quality from multiple views. To accelerate the 3D sampling process, we also proposed a strategy to use the CNN-based 3D generation as a prior for the diffusion model. Our experimental results on human patient studies suggested that MADM can generate high-quality 3D translation images, outperforming previous CNN-based and Diffusion-based baseline methods.","sentences":["Positron Emission Tomography (PET) is an important clinical imaging tool but inevitably introduces radiation hazards to patients and healthcare providers.","Reducing the tracer injection dose and eliminating the CT acquisition for attenuation correction can reduce the overall radiation dose, but often results in PET with high noise and bias.","Thus, it is desirable to develop 3D methods to translate the non-attenuation-corrected low-dose PET (NAC-LDPET) into attenuation-corrected standard-dose PET (AC-SDPET).","Recently, diffusion models have emerged as a new state-of-the-art deep learning method for image-to-image translation, better than traditional CNN-based methods.","However, due to the high computation cost and memory burden, it is largely limited to 2D applications.","To address these challenges, we developed a novel 2.5D Multi-view Averaging Diffusion Model (MADM) for 3D image-to-image translation with application on NAC-LDPET to AC-SDPET translation.","Specifically, MADM employs separate diffusion models for axial, coronal, and sagittal views, whose outputs are averaged in each sampling step to ensure the 3D generation quality from multiple views.","To accelerate the 3D sampling process, we also proposed a strategy to use the CNN-based 3D generation as a prior for the diffusion model.","Our experimental results on human patient studies suggested that MADM can generate high-quality 3D translation images, outperforming previous CNN-based and Diffusion-based baseline methods."],"url":"http://arxiv.org/abs/2406.08374v1","category":"cs.CV"}
{"created":"2024-06-12 16:21:11","title":"Deep Learning Based Joint Multi-User MISO Power Allocation and Beamforming Design","abstract":"The evolution of fifth generation (5G) wireless communication networks has led to an increased need for wireless resource management solutions that provide higher data rates, wide coverage, low latency, and power efficiency. Yet, many of existing traditional approaches remain non-practical due to computational limitations, and unrealistic presumptions of static network conditions and algorithm initialization dependencies. This creates an important gap between theoretical analysis and real-time processing of algorithms. To bridge this gap, deep learning based techniques offer promising solutions with their representational capabilities for universal function approximation. We propose a novel unsupervised deep learning based joint power allocation and beamforming design for multi-user multiple-input single-output (MU-MISO) system. The objective is to enhance the spectral efficiency by maximizing the sum-rate with the proposed joint design framework, NNBF-P while also offering computationally efficient solution in contrast to conventional approaches. We conduct experiments for diverse settings to compare the performance of NNBF-P with zero-forcing beamforming (ZFBF), minimum mean square error (MMSE) beamforming, and NNBF, which is also our deep learning based beamforming design without joint power allocation scheme. Experiment results demonstrate the superiority of NNBF-P compared to ZFBF, and MMSE while NNBF can have lower performances than MMSE and ZFBF in some experiment settings. It can also demonstrate the effectiveness of joint design framework with respect to NNBF.","sentences":["The evolution of fifth generation (5G) wireless communication networks has led to an increased need for wireless resource management solutions that provide higher data rates, wide coverage, low latency, and power efficiency.","Yet, many of existing traditional approaches remain non-practical due to computational limitations, and unrealistic presumptions of static network conditions and algorithm initialization dependencies.","This creates an important gap between theoretical analysis and real-time processing of algorithms.","To bridge this gap, deep learning based techniques offer promising solutions with their representational capabilities for universal function approximation.","We propose a novel unsupervised deep learning based joint power allocation and beamforming design for multi-user multiple-input single-output (MU-MISO) system.","The objective is to enhance the spectral efficiency by maximizing the sum-rate with the proposed joint design framework, NNBF-P while also offering computationally efficient solution in contrast to conventional approaches.","We conduct experiments for diverse settings to compare the performance of NNBF-P with zero-forcing beamforming (ZFBF), minimum mean square error (MMSE) beamforming, and NNBF, which is also our deep learning based beamforming design without joint power allocation scheme.","Experiment results demonstrate the superiority of NNBF-P compared to ZFBF, and MMSE while NNBF can have lower performances than MMSE and ZFBF in some experiment settings.","It can also demonstrate the effectiveness of joint design framework with respect to NNBF."],"url":"http://arxiv.org/abs/2406.08373v1","category":"cs.IT"}
{"created":"2024-06-12 16:20:58","title":"APSeg: Auto-Prompt Network for Cross-Domain Few-Shot Semantic Segmentatio","abstract":"Few-shot semantic segmentation (FSS) endeavors to segment unseen classes with only a few labeled samples. Current FSS methods are commonly built on the assumption that their training and application scenarios share similar domains, and their performances degrade significantly while applied to a distinct domain. To this end, we propose to leverage the cutting-edge foundation model, the Segment Anything Model (SAM), for generalization enhancement. The SAM however performs unsatisfactorily on domains that are distinct from its training data, which primarily comprise natural scene images, and it does not support automatic segmentation of specific semantics due to its interactive prompting mechanism. In our work, we introduce APSeg, a novel auto-prompt network for cross-domain few-shot semantic segmentation (CD-FSS), which is designed to be auto-prompted for guiding cross-domain segmentation. Specifically, we propose a Dual Prototype Anchor Transformation (DPAT) module that fuses pseudo query prototypes extracted based on cycle-consistency with support prototypes, allowing features to be transformed into a more stable domain-agnostic space. Additionally, a Meta Prompt Generator (MPG) module is introduced to automatically generate prompt embeddings, eliminating the need for manual visual prompts. We build an efficient model which can be applied directly to target domains without fine-tuning. Extensive experiments on four cross-domain datasets show that our model outperforms the state-of-the-art CD-FSS method by 5.24% and 3.10% in average accuracy on 1-shot and 5-shot settings, respectively.","sentences":["Few-shot semantic segmentation (FSS) endeavors to segment unseen classes with only a few labeled samples.","Current FSS methods are commonly built on the assumption that their training and application scenarios share similar domains, and their performances degrade significantly while applied to a distinct domain.","To this end, we propose to leverage the cutting-edge foundation model, the Segment Anything Model (SAM), for generalization enhancement.","The SAM however performs unsatisfactorily on domains that are distinct from its training data, which primarily comprise natural scene images, and it does not support automatic segmentation of specific semantics due to its interactive prompting mechanism.","In our work, we introduce APSeg, a novel auto-prompt network for cross-domain few-shot semantic segmentation (CD-FSS), which is designed to be auto-prompted for guiding cross-domain segmentation.","Specifically, we propose a Dual Prototype Anchor Transformation (DPAT) module that fuses pseudo query prototypes extracted based on cycle-consistency with support prototypes, allowing features to be transformed into a more stable domain-agnostic space.","Additionally, a Meta Prompt Generator (MPG) module is introduced to automatically generate prompt embeddings, eliminating the need for manual visual prompts.","We build an efficient model which can be applied directly to target domains without fine-tuning.","Extensive experiments on four cross-domain datasets show that our model outperforms the state-of-the-art CD-FSS method by 5.24% and 3.10% in average accuracy on 1-shot and 5-shot settings, respectively."],"url":"http://arxiv.org/abs/2406.08372v1","category":"cs.CV"}
{"created":"2024-06-12 16:18:26","title":"A law of the iterated logarithm for the number of blocks in regenerative compositions generated by gamma-like subordinators","abstract":"The points of the closed range of a drift-free subordinator with no killing are used for separating into blocks the elements of a sample of size $n$ from the standard exponential distribution. This gives rise to a random composition of $n$. Assuming that the subordinator has the L\\'{e}vy measure, which behaves near zero like the gamma subordinator, we prove a law of the iterated logarithm for the number of blocks in the composition as $n$ tends to infinity. Along the way we prove a law of the iterated logarithm for the Lebesgue convolution of a standard Brownian motion and a deterministic regularly varying function. This result may be of independent interest.","sentences":["The points of the closed range of a drift-free subordinator with no killing are used for separating into blocks the elements of a sample of size $n$ from the standard exponential distribution.","This gives rise to a random composition of $n$. Assuming that the subordinator has the L\\'{e}vy measure, which behaves near zero like the gamma subordinator, we prove a law of the iterated logarithm for the number of blocks in the composition as $n$ tends to infinity.","Along the way we prove a law of the iterated logarithm for the Lebesgue convolution of a standard Brownian motion and a deterministic regularly varying function.","This result may be of independent interest."],"url":"http://arxiv.org/abs/2406.08370v1","category":"math.PR"}
{"created":"2024-06-12 16:13:39","title":"High order momentum topological insulator in 2D semi-Dirac materials","abstract":"Semi-Dirac materials in 2D present an anisotropic dispersion relation, linear along one direction and quadratic along the perpendicular one. This study explores the topological properties and the influence of disorder in a 2D semi-Dirac Hamiltonian. Anisotropic edge states appear only in one direction. Their topological protection can be rigorously founded on the Zak phase of the one-dimensional reduction of the semi-Dirac Hamiltonian, parametrically depending on one of the momenta. In general, only a single value of the momentum is topologically protected so these systems can be considered as high order momentum topological insulators. We explore the dependence on the disorder of the edge states and the robustness of the topological protection in these materials. We also explore the consequences of the high order topological protection in momentum space for the transport properties in a two-terminal configuration.","sentences":["Semi-Dirac materials in 2D present an anisotropic dispersion relation, linear along one direction and quadratic along the perpendicular one.","This study explores the topological properties and the influence of disorder in a 2D semi-Dirac Hamiltonian.","Anisotropic edge states appear only in one direction.","Their topological protection can be rigorously founded on the Zak phase of the one-dimensional reduction of the semi-Dirac Hamiltonian, parametrically depending on one of the momenta.","In general, only a single value of the momentum is topologically protected so these systems can be considered as high order momentum topological insulators.","We explore the dependence on the disorder of the edge states and the robustness of the topological protection in these materials.","We also explore the consequences of the high order topological protection in momentum space for the transport properties in a two-terminal configuration."],"url":"http://arxiv.org/abs/2406.08365v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-12 16:13:13","title":"Black hole scattering near the transition to plunge: Self-force and resummation of post-Minkowskian theory","abstract":"Geodesic scattering of a test particle off a Schwarzschild black hole can be parameterized by the speed-at-infinity $v$ and the impact parameter $b$, with a \"separatrix\", $b=b_c(v)$, marking the threshold between scattering and plunge. Near the separatrix, the scattering angle diverges as $\\sim\\log(b-b_c)$. The self-force correction to the scattering angle (at fixed $v,b$) diverges even faster, like $\\sim A_1(v)b_c/(b-b_c)$. Here we numerically calculate the divergence coefficient $A_1(v)$ in a scalar-charge toy model. We then use our knowledge of $A_1(v)$ to inform a resummation of the post-Minkowskian expansion for the scattering angle, and demonstrate that the resummed series agrees remarkably well with numerical self-force results even in the strong-field regime. We propose that a similar resummation technique, applied to a mass particle subject to a gravitational self-force, can significantly enhance the utility and regime of validity of post-Minkowsian calculations for black-hole scattering.","sentences":["Geodesic scattering of a test particle off a Schwarzschild black hole can be parameterized by the speed-at-infinity $v$ and the impact parameter $b$, with a \"separatrix\", $b=b_c(v)$, marking the threshold between scattering and plunge.","Near the separatrix, the scattering angle diverges as $\\sim\\log(b-b_c)$. The self-force correction to the scattering angle (at fixed $v,b$) diverges even faster, like $\\sim A_1(v)b_c/(b-b_c)$.","Here we numerically calculate the divergence coefficient $A_1(v)$ in a scalar-charge toy model.","We then use our knowledge of $A_1(v)$ to inform a resummation of the post-Minkowskian expansion for the scattering angle, and demonstrate that the resummed series agrees remarkably well with numerical self-force results even in the strong-field regime.","We propose that a similar resummation technique, applied to a mass particle subject to a gravitational self-force, can significantly enhance the utility and regime of validity of post-Minkowsian calculations for black-hole scattering."],"url":"http://arxiv.org/abs/2406.08363v1","category":"gr-qc"}
{"created":"2024-06-12 16:02:28","title":"From a Social Cognitive Perspective: Context-aware Visual Social Relationship Recognition","abstract":"People's social relationships are often manifested through their surroundings, with certain objects or interactions acting as symbols for specific relationships, e.g., wedding rings, roses, hugs, or holding hands. This brings unique challenges to recognizing social relationships, requiring understanding and capturing the essence of these contexts from visual appearances. However, current methods of social relationship understanding rely on the basic classification paradigm of detected persons and objects, which fails to understand the comprehensive context and often overlooks decisive social factors, especially subtle visual cues. To highlight the social-aware context and intricate details, we propose a novel approach that recognizes \\textbf{Con}textual \\textbf{So}cial \\textbf{R}elationships (\\textbf{ConSoR}) from a social cognitive perspective. Specifically, to incorporate social-aware semantics, we build a lightweight adapter upon the frozen CLIP to learn social concepts via our novel multi-modal side adapter tuning mechanism. Further, we construct social-aware descriptive language prompts (e.g., scene, activity, objects, emotions) with social relationships for each image, and then compel ConSoR to concentrate more intensively on the decisive visual social factors via visual-linguistic contrasting. Impressively, ConSoR outperforms previous methods with a 12.2\\% gain on the People-in-Social-Context (PISC) dataset and a 9.8\\% increase on the People-in-Photo-Album (PIPA) benchmark. Furthermore, we observe that ConSoR excels at finding critical visual evidence to reveal social relationships.","sentences":["People's social relationships are often manifested through their surroundings, with certain objects or interactions acting as symbols for specific relationships, e.g., wedding rings, roses, hugs, or holding hands.","This brings unique challenges to recognizing social relationships, requiring understanding and capturing the essence of these contexts from visual appearances.","However, current methods of social relationship understanding rely on the basic classification paradigm of detected persons and objects, which fails to understand the comprehensive context and often overlooks decisive social factors, especially subtle visual cues.","To highlight the social-aware context and intricate details, we propose a novel approach that recognizes \\textbf{Con}textual \\textbf{So}cial \\textbf{R}elationships (\\textbf{ConSoR}) from a social cognitive perspective.","Specifically, to incorporate social-aware semantics, we build a lightweight adapter upon the frozen CLIP to learn social concepts via our novel multi-modal side adapter tuning mechanism.","Further, we construct social-aware descriptive language prompts (e.g., scene, activity, objects, emotions) with social relationships for each image, and then compel ConSoR to concentrate more intensively on the decisive visual social factors via visual-linguistic contrasting.","Impressively, ConSoR outperforms previous methods with a 12.2\\% gain on the People-in-Social-Context (PISC) dataset and a 9.8\\% increase on the People-in-Photo-Album (PIPA) benchmark.","Furthermore, we observe that ConSoR excels at finding critical visual evidence to reveal social relationships."],"url":"http://arxiv.org/abs/2406.08358v1","category":"cs.CV"}
{"created":"2024-06-12 16:02:13","title":"Long-Term Effects of Hiring Subsidies for Low-Educated Unemployed Youths","abstract":"We use regression discontinuity design and difference-in-differences methods to estimate the impact of a one-time hiring subsidy for low-educated unemployed youths in Belgium during the recovery from the Great Recession. Within a year of unemployment, the subsidy increases job-finding in the private sector by 10 percentage points. Over six years, high school graduates secure 2.8 more quarters of private employment. However, they transition from public jobs and self-employment, resulting in no net increase in overall employment, albeit with better wages. High school dropouts experience no lasting benefits. Additionally, in tight labor markets near Luxembourg's employment hub, the subsidy results in a complete deadweight loss.","sentences":["We use regression discontinuity design and difference-in-differences methods to estimate the impact of a one-time hiring subsidy for low-educated unemployed youths in Belgium during the recovery from the Great Recession.","Within a year of unemployment, the subsidy increases job-finding in the private sector by 10 percentage points.","Over six years, high school graduates secure 2.8 more quarters of private employment.","However, they transition from public jobs and self-employment, resulting in no net increase in overall employment, albeit with better wages.","High school dropouts experience no lasting benefits.","Additionally, in tight labor markets near Luxembourg's employment hub, the subsidy results in a complete deadweight loss."],"url":"http://arxiv.org/abs/2406.08357v1","category":"econ.GN"}
{"created":"2024-06-12 16:02:09","title":"An open dataset of article processing charges from six large scholarly publishers (2019-2023)","abstract":"This paper introduces a dataset of article processing charges (APCs) produced from the price lists of six large scholarly publishers - Elsevier, Frontiers, PLOS, MDPI, Springer Nature and Wiley - between 2019 and 2023. APC price lists were downloaded from publisher websites each year as well as via Wayback Machine snapshots to retrieve fees per journal per year. The dataset includes journal metadata, APC collection method, and annual APC price list information in several currencies (USD, EUR, GBP, CHF, JPY, CAD) for 8,712 unique journals and 36,618 journal-year combinations. The dataset was generated to allow for more precise analysis of APCs and can support library collection development and scientometric analysis estimating APCs paid in gold and hybrid OA journals.","sentences":["This paper introduces a dataset of article processing charges (APCs) produced from the price lists of six large scholarly publishers - Elsevier, Frontiers, PLOS, MDPI, Springer Nature and Wiley - between 2019 and 2023.","APC price lists were downloaded from publisher websites each year as well as via Wayback Machine snapshots to retrieve fees per journal per year.","The dataset includes journal metadata, APC collection method, and annual APC price list information in several currencies (USD, EUR, GBP, CHF, JPY, CAD) for 8,712 unique journals and 36,618 journal-year combinations.","The dataset was generated to allow for more precise analysis of APCs and can support library collection development and scientometric analysis estimating APCs paid in gold and hybrid OA journals."],"url":"http://arxiv.org/abs/2406.08356v1","category":"cs.DL"}
{"created":"2024-06-12 16:00:16","title":"DocSynthv2: A Practical Autoregressive Modeling for Document Generation","abstract":"While the generation of document layouts has been extensively explored, comprehensive document generation encompassing both layout and content presents a more complex challenge. This paper delves into this advanced domain, proposing a novel approach called DocSynthv2 through the development of a simple yet effective autoregressive structured model. Our model, distinct in its integration of both layout and textual cues, marks a step beyond existing layout-generation approaches. By focusing on the relationship between the structural elements and the textual content within documents, we aim to generate cohesive and contextually relevant documents without any reliance on visual components. Through experimental studies on our curated benchmark for the new task, we demonstrate the ability of our model combining layout and textual information in enhancing the generation quality and relevance of documents, opening new pathways for research in document creation and automated design. Our findings emphasize the effectiveness of autoregressive models in handling complex document generation tasks.","sentences":["While the generation of document layouts has been extensively explored, comprehensive document generation encompassing both layout and content presents a more complex challenge.","This paper delves into this advanced domain, proposing a novel approach called DocSynthv2 through the development of a simple yet effective autoregressive structured model.","Our model, distinct in its integration of both layout and textual cues, marks a step beyond existing layout-generation approaches.","By focusing on the relationship between the structural elements and the textual content within documents, we aim to generate cohesive and contextually relevant documents without any reliance on visual components.","Through experimental studies on our curated benchmark for the new task, we demonstrate the ability of our model combining layout and textual information in enhancing the generation quality and relevance of documents, opening new pathways for research in document creation and automated design.","Our findings emphasize the effectiveness of autoregressive models in handling complex document generation tasks."],"url":"http://arxiv.org/abs/2406.08354v1","category":"cs.CV"}
{"created":"2024-06-12 15:56:12","title":"Enhancing Cosmological Model Selection with Interpretable Machine Learning","abstract":"We propose a novel approach using neural networks (NNs) to differentiate between cosmological models, especially in the case where they are nested and the additional model parameters are close to zero, making it difficult to discriminate them with traditional approaches. Our method complements Bayesian analyses for cosmological model selection, which heavily depend on the chosen priors and average the unnormalized posterior over potentially large prior volumes. By analyzing simulated realistic data sets of the growth rate of the large scale structure (LSS) of the Universe, based on current galaxy-clustering survey specifications, for the cosmological constant and cold dark matter ($\\Lambda$CDM) model and the Hu-Sawicki $f(R)$ model, we demonstrate the potential of NNs to enhance the extraction of meaningful information from cosmological LSS data. We find that the NN can successfully distinguish between $\\Lambda$CDM and the $f(R)$ models, by predicting the correct model with approximately $97\\%$ overall accuracy, thus demonstrating that NNs can maximise the potential of current and next generation surveys to probe for deviations from general relativity.","sentences":["We propose a novel approach using neural networks (NNs) to differentiate between cosmological models, especially in the case where they are nested and the additional model parameters are close to zero, making it difficult to discriminate them with traditional approaches.","Our method complements Bayesian analyses for cosmological model selection, which heavily depend on the chosen priors and average the unnormalized posterior over potentially large prior volumes.","By analyzing simulated realistic data sets of the growth rate of the large scale structure (LSS) of the Universe, based on current galaxy-clustering survey specifications, for the cosmological constant and cold dark matter ($\\Lambda$CDM) model and the Hu-Sawicki $f(R)$ model, we demonstrate the potential of NNs to enhance the extraction of meaningful information from cosmological LSS data.","We find that the NN can successfully distinguish between $\\Lambda$CDM and the $f(R)$ models, by predicting the correct model with approximately $97\\%$ overall accuracy, thus demonstrating that NNs can maximise the potential of current and next generation surveys to probe for deviations from general relativity."],"url":"http://arxiv.org/abs/2406.08351v1","category":"astro-ph.CO"}
{"created":"2024-06-12 15:55:42","title":"Utilizing Navigation Path to Generate Target Point for Enhanced End-to-End Autonomous Driving Planning","abstract":"In recent years, end-to-end autonomous driving frameworks have been shown to not only enhance perception performance but also improve planning capabilities. However, most previous end-to-end autonomous driving frameworks have primarily focused on enhancing environment perception while neglecting the learning of autonomous vehicle planning intent. Within the end-to-end framework, this paper proposes a method termed NTT, which obtains explicit planning intent through the navigation path. NTT first generates the future target point for the autonomous vehicle based on the navigation path, thereby enhancing planning performance within the end-to-end framework. On one hand, the generation of the target point allows the autonomous vehicle to learn explicit intention from the navigation path, enhancing the practicality of planning. On the other hand, planning trajectory generated based on the target point can adapt more flexibly to environmental changes, thus effectively improving planning safety. We achieved excellent planning performance on the widely used nuScenes dataset and validated the effectiveness of our method through ablation experiments.","sentences":["In recent years, end-to-end autonomous driving frameworks have been shown to not only enhance perception performance but also improve planning capabilities.","However, most previous end-to-end autonomous driving frameworks have primarily focused on enhancing environment perception while neglecting the learning of autonomous vehicle planning intent.","Within the end-to-end framework, this paper proposes a method termed NTT, which obtains explicit planning intent through the navigation path.","NTT first generates the future target point for the autonomous vehicle based on the navigation path, thereby enhancing planning performance within the end-to-end framework.","On one hand, the generation of the target point allows the autonomous vehicle to learn explicit intention from the navigation path, enhancing the practicality of planning.","On the other hand, planning trajectory generated based on the target point can adapt more flexibly to environmental changes, thus effectively improving planning safety.","We achieved excellent planning performance on the widely used nuScenes dataset and validated the effectiveness of our method through ablation experiments."],"url":"http://arxiv.org/abs/2406.08349v1","category":"cs.RO"}
{"created":"2024-06-12 15:55:39","title":"Trajectory optimization of tail-sitter considering speed constraints","abstract":"Tail-sitters combine the advantages of fixed-wing unmanned aerial vehicles (UAVs) and vertical take-off and landing UAVs, and have been widely designed and researched in recent years. With the change in modern UAV application scenarios, it is required that UAVs have fast maneuverable three-dimensional flight capabilities. Due to the highly nonlinear aerodynamics produced by the fuselage and wings of the tail-sitter, how to quickly generate a smooth and executable trajectory is a problem that needs to be solved urgently. We constrain the speed of the tail-sitter, eliminate the differential dynamics constraints in the trajectory generation process of the tail-sitter through differential flatness, and allocate the time variable of the trajectory through the state-of-the-art trajectory generation method named MINCO. Because we discretize the trajectory in time, we convert the speed constraint on the vehicle into a soft constraint, thereby achieving the time-optimal trajectory for the tail-sitter to fly through any given waypoints.","sentences":["Tail-sitters combine the advantages of fixed-wing unmanned aerial vehicles (UAVs) and vertical take-off and landing UAVs, and have been widely designed and researched in recent years.","With the change in modern UAV application scenarios, it is required that UAVs have fast maneuverable three-dimensional flight capabilities.","Due to the highly nonlinear aerodynamics produced by the fuselage and wings of the tail-sitter, how to quickly generate a smooth and executable trajectory is a problem that needs to be solved urgently.","We constrain the speed of the tail-sitter, eliminate the differential dynamics constraints in the trajectory generation process of the tail-sitter through differential flatness, and allocate the time variable of the trajectory through the state-of-the-art trajectory generation method named MINCO.","Because we discretize the trajectory in time, we convert the speed constraint on the vehicle into a soft constraint, thereby achieving the time-optimal trajectory for the tail-sitter to fly through any given waypoints."],"url":"http://arxiv.org/abs/2406.08347v1","category":"cs.RO"}
{"created":"2024-06-12 15:51:39","title":"Blind Image Deblurring using FFT-ReLU with Deep Learning Pipeline Integration","abstract":"Blind image deblurring is the process of deriving a sharp image and a blur kernel from a blurred image. Blurry images are typically modeled as the convolution of a sharp image with a blur kernel, necessitating the estimation of the unknown blur kernel to perform blind image deblurring effectively. Existing approaches primarily focus on domain-specific features of images, such as salient edges, dark channels, and light streaks. These features serve as probabilistic priors to enhance the estimation of the blur kernel. For improved generality, we propose a novel prior (ReLU sparsity prior) that estimates blur kernel effectively across all distributions of images (natural, facial, text, low-light, saturated etc). Our approach demonstrates superior efficiency, with inference times up to three times faster, while maintaining high accuracy in PSNR, SSIM, and error ratio metrics. We also observe noticeable improvement in the performance of the state-of-the-art architectures (in terms of aforementioned metrics) in deep learning based approaches when our method is used as a post-processing unit.","sentences":["Blind image deblurring is the process of deriving a sharp image and a blur kernel from a blurred image.","Blurry images are typically modeled as the convolution of a sharp image with a blur kernel, necessitating the estimation of the unknown blur kernel to perform blind image deblurring effectively.","Existing approaches primarily focus on domain-specific features of images, such as salient edges, dark channels, and light streaks.","These features serve as probabilistic priors to enhance the estimation of the blur kernel.","For improved generality, we propose a novel prior (ReLU sparsity prior) that estimates blur kernel effectively across all distributions of images (natural, facial, text, low-light, saturated etc).","Our approach demonstrates superior efficiency, with inference times up to three times faster, while maintaining high accuracy in PSNR, SSIM, and error ratio metrics.","We also observe noticeable improvement in the performance of the state-of-the-art architectures (in terms of aforementioned metrics) in deep learning based approaches when our method is used as a post-processing unit."],"url":"http://arxiv.org/abs/2406.08344v1","category":"cs.CV"}
{"created":"2024-06-12 15:50:35","title":"Continuous-Time Digital Twin with Analogue Memristive Neural Ordinary Differential Equation Solver","abstract":"Digital twins, the cornerstone of Industry 4.0, replicate real-world entities through computer models, revolutionising fields such as manufacturing management and industrial automation. Recent advances in machine learning provide data-driven methods for developing digital twins using discrete-time data and finite-depth models on digital computers. However, this approach fails to capture the underlying continuous dynamics and struggles with modelling complex system behaviour. Additionally, the architecture of digital computers, with separate storage and processing units, necessitates frequent data transfers and Analogue-Digital (A/D) conversion, thereby significantly increasing both time and energy costs. Here, we introduce a memristive neural ordinary differential equation (ODE) solver for digital twins, which is capable of capturing continuous-time dynamics and facilitates the modelling of complex systems using an infinite-depth model. By integrating storage and computation within analogue memristor arrays, we circumvent the von Neumann bottleneck, thus enhancing both speed and energy efficiency. We experimentally validate our approach by developing a digital twin of the HP memristor, which accurately extrapolates its nonlinear dynamics, achieving a 4.2-fold projected speedup and a 41.4-fold projected decrease in energy consumption compared to state-of-the-art digital hardware, while maintaining an acceptable error margin. Additionally, we demonstrate scalability through experimentally grounded simulations of Lorenz96 dynamics, exhibiting projected performance improvements of 12.6-fold in speed and 189.7-fold in energy efficiency relative to traditional digital approaches. By harnessing the capabilities of fully analogue computing, our breakthrough accelerates the development of digital twins, offering an efficient and rapid solution to meet the demands of Industry 4.0.","sentences":["Digital twins, the cornerstone of Industry 4.0, replicate real-world entities through computer models, revolutionising fields such as manufacturing management and industrial automation.","Recent advances in machine learning provide data-driven methods for developing digital twins using discrete-time data and finite-depth models on digital computers.","However, this approach fails to capture the underlying continuous dynamics and struggles with modelling complex system behaviour.","Additionally, the architecture of digital computers, with separate storage and processing units, necessitates frequent data transfers and Analogue-Digital (A/D) conversion, thereby significantly increasing both time and energy costs.","Here, we introduce a memristive neural ordinary differential equation (ODE) solver for digital twins, which is capable of capturing continuous-time dynamics and facilitates the modelling of complex systems using an infinite-depth model.","By integrating storage and computation within analogue memristor arrays, we circumvent the von Neumann bottleneck, thus enhancing both speed and energy efficiency.","We experimentally validate our approach by developing a digital twin of the HP memristor, which accurately extrapolates its nonlinear dynamics, achieving a 4.2-fold projected speedup and a 41.4-fold projected decrease in energy consumption compared to state-of-the-art digital hardware, while maintaining an acceptable error margin.","Additionally, we demonstrate scalability through experimentally grounded simulations of Lorenz96 dynamics, exhibiting projected performance improvements of 12.6-fold in speed and 189.7-fold in energy efficiency relative to traditional digital approaches.","By harnessing the capabilities of fully analogue computing, our breakthrough accelerates the development of digital twins, offering an efficient and rapid solution to meet the demands of Industry 4.0."],"url":"http://arxiv.org/abs/2406.08343v1","category":"cs.AR"}
{"created":"2024-06-12 15:48:39","title":"Practical, Automated Scenario-based Mobile App Testing","abstract":"The importance of mobile application (app) quality insurance is increasing with the rapid development of the mobile Internet. Automated test generation approaches, as a dominant direction of app quality insurance, follow specific models or strategies, targeting at optimizing the code coverage. Such approaches lead to a huge gap between testing execution and app business logic. Test scripts developed by human testers consider business logic by focusing on testing scenarios. Due to the GUI-intensive feature of mobile apps, human testers always understand app GUI to organize test scripts for scenarios. This inspires us to utilize domain knowledge from app GUI understanding for scenario-based test generation.   In this paper, we propose a novel approach, ScenTest, for scenario-based mobile app testing with event knowledge graph (EKG) via GUI image understanding. ScenTest tries to start automated testing by imitating human practices and integrating domain knowledge into scenario-based mobile app testing, realizing fully automated testing on target testing scenarios for the first time. ScenTest extracts four kinds of entities and five kinds of corresponding relationships from crowdsourced test reports, where the test events and app GUI information are presented, and constructs the EKGs for specific scenarios. Then, ScenTest conducts test generation for specific scenarios on different apps with the guidance of EKG with the combination consideration of app current state and testing context. We conduct an evaluation on ScenTest on different aspects. The results show that the test generation of ScenTest on the basis of EKG is effective, and ScenTest can reveal 80+ distinct real-world bugs in specific scenarios compared with representative baselines.","sentences":["The importance of mobile application (app) quality insurance is increasing with the rapid development of the mobile Internet.","Automated test generation approaches, as a dominant direction of app quality insurance, follow specific models or strategies, targeting at optimizing the code coverage.","Such approaches lead to a huge gap between testing execution and app business logic.","Test scripts developed by human testers consider business logic by focusing on testing scenarios.","Due to the GUI-intensive feature of mobile apps, human testers always understand app GUI to organize test scripts for scenarios.","This inspires us to utilize domain knowledge from app GUI understanding for scenario-based test generation.   ","In this paper, we propose a novel approach, ScenTest, for scenario-based mobile app testing with event knowledge graph (EKG) via GUI image understanding.","ScenTest tries to start automated testing by imitating human practices and integrating domain knowledge into scenario-based mobile app testing, realizing fully automated testing on target testing scenarios for the first time.","ScenTest extracts four kinds of entities and five kinds of corresponding relationships from crowdsourced test reports, where the test events and app GUI information are presented, and constructs the EKGs for specific scenarios.","Then, ScenTest conducts test generation for specific scenarios on different apps with the guidance of EKG with the combination consideration of app current state and testing context.","We conduct an evaluation on ScenTest on different aspects.","The results show that the test generation of ScenTest on the basis of EKG is effective, and ScenTest can reveal 80+ distinct real-world bugs in specific scenarios compared with representative baselines."],"url":"http://arxiv.org/abs/2406.08340v1","category":"cs.SE"}
{"created":"2024-06-12 15:42:52","title":"WMAdapter: Adding WaterMark Control to Latent Diffusion Models","abstract":"Watermarking is crucial for protecting the copyright of AI-generated images. We propose WMAdapter, a diffusion model watermark plugin that takes user-specified watermark information and allows for seamless watermark imprinting during the diffusion generation process. WMAdapter is efficient and robust, with a strong emphasis on high generation quality. To achieve this, we make two key designs: (1) We develop a contextual adapter structure that is lightweight and enables effective knowledge transfer from heavily pretrained post-hoc watermarking models. (2) We introduce an extra finetuning step and design a hybrid finetuning strategy to further improve image quality and eliminate tiny artifacts. Empirical results demonstrate that WMAdapter offers strong flexibility, exceptional image generation quality and competitive watermark robustness.","sentences":["Watermarking is crucial for protecting the copyright of AI-generated images.","We propose WMAdapter, a diffusion model watermark plugin that takes user-specified watermark information and allows for seamless watermark imprinting during the diffusion generation process.","WMAdapter is efficient and robust, with a strong emphasis on high generation quality.","To achieve this, we make two key designs: (1) We develop a contextual adapter structure that is lightweight and enables effective knowledge transfer from heavily pretrained post-hoc watermarking models.","(2) We introduce an extra finetuning step and design a hybrid finetuning strategy to further improve image quality and eliminate tiny artifacts.","Empirical results demonstrate that WMAdapter offers strong flexibility, exceptional image generation quality and competitive watermark robustness."],"url":"http://arxiv.org/abs/2406.08337v1","category":"cs.CV"}
{"created":"2024-06-12 15:41:06","title":"A Survey of Pipeline Tools for Data Engineering","abstract":"Currently, a variety of pipeline tools are available for use in data engineering. Data scientists can use these tools to resolve data wrangling issues associated with data and accomplish some data engineering tasks from data ingestion through data preparation to utilization as input for machine learning (ML). Some of these tools have essential built-in components or can be combined with other tools to perform desired data engineering operations. While some tools are wholly or partly commercial, several open-source tools are available to perform expert-level data engineering tasks. This survey examines the broad categories and examples of pipeline tools based on their design and data engineering intentions. These categories are Extract Transform Load/Extract Load Transform (ETL/ELT), pipelines for Data Integration, Ingestion, and Transformation, Data Pipeline Orchestration and Workflow Management, and Machine Learning Pipelines. The survey also provides a broad outline of the utilization with examples within these broad groups and finally, a discussion is presented with case studies indicating the usage of pipeline tools for data engineering. The studies present some first-user application experiences with sample data, some complexities of the applied pipeline, and a summary note of approaches to using these tools to prepare data for machine learning.","sentences":["Currently, a variety of pipeline tools are available for use in data engineering.","Data scientists can use these tools to resolve data wrangling issues associated with data and accomplish some data engineering tasks from data ingestion through data preparation to utilization as input for machine learning (ML).","Some of these tools have essential built-in components or can be combined with other tools to perform desired data engineering operations.","While some tools are wholly or partly commercial, several open-source tools are available to perform expert-level data engineering tasks.","This survey examines the broad categories and examples of pipeline tools based on their design and data engineering intentions.","These categories are Extract Transform Load/Extract Load Transform (ETL/ELT), pipelines for Data Integration, Ingestion, and Transformation, Data Pipeline Orchestration and Workflow Management, and Machine Learning Pipelines.","The survey also provides a broad outline of the utilization with examples within these broad groups and finally, a discussion is presented with case studies indicating the usage of pipeline tools for data engineering.","The studies present some first-user application experiences with sample data, some complexities of the applied pipeline, and a summary note of approaches to using these tools to prepare data for machine learning."],"url":"http://arxiv.org/abs/2406.08335v1","category":"cs.LG"}
{"created":"2024-06-12 15:40:06","title":"ProTrain: Efficient LLM Training via Memory-Aware Techniques","abstract":"It is extremely memory-hungry to train Large Language Models (LLM). To solve this problem, existing work exploits the combination of CPU and GPU for the training process, such as ZeRO-Offload. Such a technique largely democratizes billion-scale model training, making it possible to train with few consumer graphics cards. However, based on our observation, existing frameworks often provide coarse-grained memory management and require experienced experts in configuration tuning, leading to suboptimal hardware utilization and performance. This paper proposes ProTrain, a novel training system that intelligently balances memory usage and performance by coordinating memory, computation, and IO. ProTrain achieves adaptive memory management through Chunk-Based Model State Management and Block-Wise Activation Management, guided by a Memory-Aware Runtime Profiler without user intervention. ProTrain does not change the training algorithm and thus does not compromise accuracy. Experiments show that ProTrain improves training throughput by 1.43$\\times$ to 2.71$\\times$ compared to the SOTA training systems.","sentences":["It is extremely memory-hungry to train Large Language Models (LLM).","To solve this problem, existing work exploits the combination of CPU and GPU for the training process, such as ZeRO-Offload.","Such a technique largely democratizes billion-scale model training, making it possible to train with few consumer graphics cards.","However, based on our observation, existing frameworks often provide coarse-grained memory management and require experienced experts in configuration tuning, leading to suboptimal hardware utilization and performance.","This paper proposes ProTrain, a novel training system that intelligently balances memory usage and performance by coordinating memory, computation, and IO.","ProTrain achieves adaptive memory management through Chunk-Based Model State Management and Block-Wise Activation Management, guided by a Memory-Aware Runtime Profiler without user intervention.","ProTrain does not change the training algorithm and thus does not compromise accuracy.","Experiments show that ProTrain improves training throughput by 1.43$\\times$ to 2.71$\\times$ compared to the SOTA training systems."],"url":"http://arxiv.org/abs/2406.08334v1","category":"cs.DC"}
{"created":"2024-06-12 15:36:30","title":"UDON: Universal Dynamic Online distillatioN for generic image representations","abstract":"Universal image representations are critical in enabling real-world fine-grained and instance-level recognition applications, where objects and entities from any domain must be identified at large scale. Despite recent advances, existing methods fail to capture important domain-specific knowledge, while also ignoring differences in data distribution across different domains. This leads to a large performance gap between efficient universal solutions and expensive approaches utilising a collection of specialist models, one for each domain. In this work, we make significant strides towards closing this gap, by introducing a new learning technique, dubbed UDON (Universal Dynamic Online DistillatioN). UDON employs multi-teacher distillation, where each teacher is specialized in one domain, to transfer detailed domain-specific knowledge into the student universal embedding. UDON's distillation approach is not only effective, but also very efficient, by sharing most model parameters between the student and all teachers, where all models are jointly trained in an online manner. UDON also comprises a sampling technique which adapts the training process to dynamically allocate batches to domains which are learned slower and require more frequent processing. This boosts significantly the learning of complex domains which are characterised by a large number of classes and long-tail distributions. With comprehensive experiments, we validate each component of UDON, and showcase significant improvements over the state of the art in the recent UnED benchmark. Code: https://github.com/nikosips/UDON .","sentences":["Universal image representations are critical in enabling real-world fine-grained and instance-level recognition applications, where objects and entities from any domain must be identified at large scale.","Despite recent advances, existing methods fail to capture important domain-specific knowledge, while also ignoring differences in data distribution across different domains.","This leads to a large performance gap between efficient universal solutions and expensive approaches utilising a collection of specialist models, one for each domain.","In this work, we make significant strides towards closing this gap, by introducing a new learning technique, dubbed UDON (Universal Dynamic Online DistillatioN).","UDON employs multi-teacher distillation, where each teacher is specialized in one domain, to transfer detailed domain-specific knowledge into the student universal embedding.","UDON's distillation approach is not only effective, but also very efficient, by sharing most model parameters between the student and all teachers, where all models are jointly trained in an online manner.","UDON also comprises a sampling technique which adapts the training process to dynamically allocate batches to domains which are learned slower and require more frequent processing.","This boosts significantly the learning of complex domains which are characterised by a large number of classes and long-tail distributions.","With comprehensive experiments, we validate each component of UDON, and showcase significant improvements over the state of the art in the recent UnED benchmark.","Code: https://github.com/nikosips/UDON ."],"url":"http://arxiv.org/abs/2406.08332v1","category":"cs.CV"}
{"created":"2024-06-12 15:34:42","title":"Genetic Column Generation for Computing Lower Bounds for Adversarial Classification","abstract":"Recent theoretical results on adversarial multi-class classification showed a similarity to the multi-marginal formulation of Wasserstein-barycenter in optimal transport. Unfortunately, both problems suffer from the curse of dimension, making it hard to exploit the nice linear program structure of the problems for numerical calculations. We investigate how ideas from Genetic Column Generation for multi-marginal optimal transport can be used to overcome the curse of dimension in computing the minimal adversarial risk in multi-class classification.","sentences":["Recent theoretical results on adversarial multi-class classification showed a similarity to the multi-marginal formulation of Wasserstein-barycenter in optimal transport.","Unfortunately, both problems suffer from the curse of dimension, making it hard to exploit the nice linear program structure of the problems for numerical calculations.","We investigate how ideas from Genetic Column Generation for multi-marginal optimal transport can be used to overcome the curse of dimension in computing the minimal adversarial risk in multi-class classification."],"url":"http://arxiv.org/abs/2406.08331v1","category":"math.NA"}
{"created":"2024-06-12 15:34:28","title":"It's all about PR -- Smart Benchmarking AI Accelerators using Performance Representatives","abstract":"Statistical models are widely used to estimate the performance of commercial off-the-shelf (COTS) AI hardware accelerators. However, training of statistical performance models often requires vast amounts of data, leading to a significant time investment and can be difficult in case of limited hardware availability. To alleviate this problem, we propose a novel performance modeling methodology that significantly reduces the number of training samples while maintaining good accuracy. Our approach leverages knowledge of the target hardware architecture and initial parameter sweeps to identify a set of Performance Representatives (PR) for deep neural network (DNN) layers. These PRs are then used for benchmarking, building a statistical performance model, and making estimations. This targeted approach drastically reduces the number of training samples needed, opposed to random sampling, to achieve a better estimation accuracy. We achieve a Mean Absolute Percentage Error (MAPE) of as low as 0.02% for single-layer estimations and 0.68% for whole DNN estimations with less than 10000 training samples. The results demonstrate the superiority of our method for single-layer estimations compared to models trained with randomly sampled datasets of the same size.","sentences":["Statistical models are widely used to estimate the performance of commercial off-the-shelf (COTS)","AI hardware accelerators.","However, training of statistical performance models often requires vast amounts of data, leading to a significant time investment and can be difficult in case of limited hardware availability.","To alleviate this problem, we propose a novel performance modeling methodology that significantly reduces the number of training samples while maintaining good accuracy.","Our approach leverages knowledge of the target hardware architecture and initial parameter sweeps to identify a set of Performance Representatives (PR) for deep neural network (DNN) layers.","These PRs are then used for benchmarking, building a statistical performance model, and making estimations.","This targeted approach drastically reduces the number of training samples needed, opposed to random sampling, to achieve a better estimation accuracy.","We achieve a Mean Absolute Percentage Error (MAPE) of as low as 0.02% for single-layer estimations and 0.68% for whole DNN estimations with less than 10000 training samples.","The results demonstrate the superiority of our method for single-layer estimations compared to models trained with randomly sampled datasets of the same size."],"url":"http://arxiv.org/abs/2406.08330v1","category":"cs.PF"}
{"created":"2024-06-12 15:33:59","title":"Highly Connected Graph Partitioning: Exact Formulation and Solution Methods","abstract":"Graph partitioning (GP) and vertex connectivity have traditionally been two distinct fields of study. This paper introduces the highly connected graph partitioning (HCGP) problem, which partitions a graph into compact, size balanced, and $Q$-(vertex) connected parts for any $Q\\geq 1$. This problem is valuable in applications that seek cohesion and fault-tolerance within their parts, such as community detection in social networks and resiliency-focused partitioning of power networks. Existing research in this fundamental interconnection primarily focuses on providing theoretical existence guarantees of highly connected partitions for a limited set of dense graphs, and do not include canonical GP considerations such as size balance and compactness. This paper's key contribution is providing a general modeling and algorithmic approach for HCGP, inspired by recent work in the political districting problem, a special case of HCGP with $Q=1$. This approach models $Q$-connectivity constraints as mixed integer programs for any $Q\\geq 1$ and provides an efficient branch-and-cut method to solve HCGP. When solution time is a priority over optimality, this paper provides a heuristic method specifically designed for HCGP with $Q=2$. A computational analysis evaluates these methods using a test bed of instances from various real-world graphs. In this analysis, the branch-and-cut method finds an optimal solution within one hour in $82.8\\%$ of the instances solved. For $Q=2$, small and sparse instances are challenging for the heuristic, whereas large and sparse instances are challenging for the exact method. Furthermore, this study quantifies the computational cost of ensuring higher connectivity using the branch-and-cut approach, compared to a baseline of ensuring $1$-connectivity. Overall, this work serves as an effective tool to partition a graph into resilient and cohesive parts.","sentences":["Graph partitioning (GP) and vertex connectivity have traditionally been two distinct fields of study.","This paper introduces the highly connected graph partitioning (HCGP) problem, which partitions a graph into compact, size balanced, and $Q$-(vertex) connected parts for any $Q\\geq 1$.","This problem is valuable in applications that seek cohesion and fault-tolerance within their parts, such as community detection in social networks and resiliency-focused partitioning of power networks.","Existing research in this fundamental interconnection primarily focuses on providing theoretical existence guarantees of highly connected partitions for a limited set of dense graphs, and do not include canonical GP considerations such as size balance and compactness.","This paper's key contribution is providing a general modeling and algorithmic approach for HCGP, inspired by recent work in the political districting problem, a special case of HCGP with $Q=1$. This approach models $Q$-connectivity constraints as mixed integer programs for any $Q\\geq 1$ and provides an efficient branch-and-cut method to solve HCGP.","When solution time is a priority over optimality, this paper provides a heuristic method specifically designed for HCGP with $Q=2$. A computational analysis evaluates these methods using a test bed of instances from various real-world graphs.","In this analysis, the branch-and-cut method finds an optimal solution within one hour in $82.8\\%$ of the instances solved.","For $Q=2$, small and sparse instances are challenging for the heuristic, whereas large and sparse instances are challenging for the exact method.","Furthermore, this study quantifies the computational cost of ensuring higher connectivity using the branch-and-cut approach, compared to a baseline of ensuring $1$-connectivity.","Overall, this work serves as an effective tool to partition a graph into resilient and cohesive parts."],"url":"http://arxiv.org/abs/2406.08329v1","category":"cs.DM"}
{"created":"2024-06-12 15:23:50","title":"Illustrating the benefits of efficient creation and adaption of behavior models in intelligent Digital Twins over the machine life cycle","abstract":"The concept of the Digital Twin, which in the context of this paper is the virtual representation of a production system or its components, can be used as a \"digital playground\" to master the increasing complexity of these assets. Central subcomponents of the Digital Twin are behavior models that can provide benefits over the entire machine life cycle. However, the creation, adaption and use of behavior models throughout the machine life cycle is very time-consuming, which is why approaches to improve the cost-benefit ratio are needed. Furthermore, there is a lack of specific use cases that illustrate the application and added benefit of behavior models over the machine life cycle, which is why the universal application of behavior models in industry is still lacking compared to research. This paper first presents the fundamentals, challenges and related work on Digital Twins and behavior models in the context of the machine life cycle. Then, concepts for low-effort creation and automatic adaption of Digital Twins are presented, with a focus on behavior models. Finally, the aforementioned gap between research and industry is addressed by demonstrating various realized use cases over the machine life cycle, in which the advantages as well as the application of behavior models in the different life phases are shown.","sentences":["The concept of the Digital Twin, which in the context of this paper is the virtual representation of a production system or its components, can be used as a \"digital playground\" to master the increasing complexity of these assets.","Central subcomponents of the Digital Twin are behavior models that can provide benefits over the entire machine life cycle.","However, the creation, adaption and use of behavior models throughout the machine life cycle is very time-consuming, which is why approaches to improve the cost-benefit ratio are needed.","Furthermore, there is a lack of specific use cases that illustrate the application and added benefit of behavior models over the machine life cycle, which is why the universal application of behavior models in industry is still lacking compared to research.","This paper first presents the fundamentals, challenges and related work on Digital Twins and behavior models in the context of the machine life cycle.","Then, concepts for low-effort creation and automatic adaption of Digital Twins are presented, with a focus on behavior models.","Finally, the aforementioned gap between research and industry is addressed by demonstrating various realized use cases over the machine life cycle, in which the advantages as well as the application of behavior models in the different life phases are shown."],"url":"http://arxiv.org/abs/2406.08323v1","category":"cs.CE"}
{"created":"2024-06-12 15:22:56","title":"MMIL: A novel algorithm for disease associated cell type discovery","abstract":"Single-cell datasets often lack individual cell labels, making it challenging to identify cells associated with disease. To address this, we introduce Mixture Modeling for Multiple Instance Learning (MMIL), an expectation maximization method that enables the training and calibration of cell-level classifiers using patient-level labels. Our approach can be used to train e.g. lasso logistic regression models, gradient boosted trees, and neural networks. When applied to clinically-annotated, primary patient samples in Acute Myeloid Leukemia (AML) and Acute Lymphoblastic Leukemia (ALL), our method accurately identifies cancer cells, generalizes across tissues and treatment timepoints, and selects biologically relevant features. In addition, MMIL is capable of incorporating cell labels into model training when they are known, providing a powerful framework for leveraging both labeled and unlabeled data simultaneously. Mixture Modeling for MIL offers a novel approach for cell classification, with significant potential to advance disease understanding and management, especially in scenarios with unknown gold-standard labels and high dimensionality.","sentences":["Single-cell datasets often lack individual cell labels, making it challenging to identify cells associated with disease.","To address this, we introduce Mixture Modeling for Multiple Instance Learning (MMIL), an expectation maximization method that enables the training and calibration of cell-level classifiers using patient-level labels.","Our approach can be used to train e.g. lasso logistic regression models, gradient boosted trees, and neural networks.","When applied to clinically-annotated, primary patient samples in Acute Myeloid Leukemia (AML) and Acute Lymphoblastic Leukemia (ALL), our method accurately identifies cancer cells, generalizes across tissues and treatment timepoints, and selects biologically relevant features.","In addition, MMIL is capable of incorporating cell labels into model training when they are known, providing a powerful framework for leveraging both labeled and unlabeled data simultaneously.","Mixture Modeling for MIL offers a novel approach for cell classification, with significant potential to advance disease understanding and management, especially in scenarios with unknown gold-standard labels and high dimensionality."],"url":"http://arxiv.org/abs/2406.08322v1","category":"q-bio.QM"}
{"created":"2024-06-12 15:21:51","title":"Deep learning from strongly mixing observations: Sparse-penalized regularization and minimax optimality","abstract":"The explicit regularization and optimality of deep neural networks estimators from independent data have made considerable progress recently. The study of such properties on dependent data is still a challenge. In this paper, we carry out deep learning from strongly mixing observations, and deal with the squared and a broad class of loss functions. We consider sparse-penalized regularization for deep neural network predictor. For a general framework that includes, regression estimation, classification, time series prediction,$\\cdots$, oracle inequality for the expected excess risk is established and a bound on the class of H\\\"older smooth functions is provided. For nonparametric regression from strong mixing data and sub-exponentially error, we provide an oracle inequality for the $L_2$ error and investigate an upper bound of this error on a class of H\\\"older composition functions. For the specific case of nonparametric autoregression with Gaussian and Laplace errors, a lower bound of the $L_2$ error on this H\\\"older composition class is established. Up to logarithmic factor, this bound matches its upper bound; so, the deep neural network estimator attains the minimax optimal rate.","sentences":["The explicit regularization and optimality of deep neural networks estimators from independent data have made considerable progress recently.","The study of such properties on dependent data is still a challenge.","In this paper, we carry out deep learning from strongly mixing observations, and deal with the squared and a broad class of loss functions.","We consider sparse-penalized regularization for deep neural network predictor.","For a general framework that includes, regression estimation, classification, time series prediction,$\\cdots$, oracle inequality for the expected excess risk is established and a bound on the class of H\\\"older smooth functions is provided.","For nonparametric regression from strong mixing data and sub-exponentially error, we provide an oracle inequality for the $L_2$ error and investigate an upper bound of this error on a class of H\\\"older composition functions.","For the specific case of nonparametric autoregression with Gaussian and Laplace errors, a lower bound of the $L_2$ error on this H\\\"older composition class is established.","Up to logarithmic factor, this bound matches its upper bound; so, the deep neural network estimator attains the minimax optimal rate."],"url":"http://arxiv.org/abs/2406.08321v1","category":"stat.ML"}
{"created":"2024-06-12 15:16:40","title":"Is Programming by Example solved by LLMs?","abstract":"Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples. Such systems are practically and theoretically important: from an end-user perspective, they are deployed to millions of people, and from an AI perspective, PBE corresponds to a very general form of few-shot inductive inference. Given the success of Large Language Models (LLMs) in code-generation tasks, we investigate here the extent to which LLMs can be said to have `solved' PBE. We experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data. We find that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution. We analyze empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization. Collectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short.","sentences":["Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples.","Such systems are practically and theoretically important: from an end-user perspective, they are deployed to millions of people, and from an AI perspective, PBE corresponds to a very general form of few-shot inductive inference.","Given the success of Large Language Models (LLMs) in code-generation tasks, we investigate here the extent to which LLMs can be said to have `solved' PBE.","We experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data.","We find that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution.","We analyze empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization.","Collectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short."],"url":"http://arxiv.org/abs/2406.08316v1","category":"cs.CL"}
{"created":"2024-06-12 15:16:26","title":"Improving Policy Optimization via $\\varepsilon$-Retrain","abstract":"We present $\\varepsilon$-retrain, an exploration strategy designed to encourage a behavioral preference while optimizing policies with monotonic improvement guarantees. To this end, we introduce an iterative procedure for collecting retrain areas -- parts of the state space where an agent did not follow the behavioral preference. Our method then switches between the typical uniform restart state distribution and the retrain areas using a decaying factor $\\varepsilon$, allowing agents to retrain on situations where they violated the preference. Experiments over hundreds of seeds across locomotion, navigation, and power network tasks show that our method yields agents that exhibit significant performance and sample efficiency improvements. Moreover, we employ formal verification of neural networks to provably quantify the degree to which agents adhere to behavioral preferences.","sentences":["We present $\\varepsilon$-retrain, an exploration strategy designed to encourage a behavioral preference while optimizing policies with monotonic improvement guarantees.","To this end, we introduce an iterative procedure for collecting retrain areas -- parts of the state space where an agent did not follow the behavioral preference.","Our method then switches between the typical uniform restart state distribution and the retrain areas using a decaying factor $\\varepsilon$, allowing agents to retrain on situations where they violated the preference.","Experiments over hundreds of seeds across locomotion, navigation, and power network tasks show that our method yields agents that exhibit significant performance and sample efficiency improvements.","Moreover, we employ formal verification of neural networks to provably quantify the degree to which agents adhere to behavioral preferences."],"url":"http://arxiv.org/abs/2406.08315v1","category":"cs.AI"}
{"created":"2024-06-12 15:14:41","title":"Searching for bound states in the open strangeness systems","abstract":"Inspired by the recent findings of $Z_{cs}$ and $P_{cs}$ states, we investigate the strong interactions of the systems with open strangeness(es) from the light sector to the heavy sector (no beauty quark), where the interaction potential is derived from the vector meson exchange mechanism in $t$- and $u$-channels. In the current work, we discuss all of single channel cases for the open strangeness in the systemic framework, where the resonances $X_0(2866)$, $D^*_{s0}(2317)$ and $D_{s1}(2460)$ are dynamically generated. Furthermore, there are many new exotics predicted. In addition, the left-hand cut problem in $t$- and $u$-channels is discussed in detail.","sentences":["Inspired by the recent findings of $Z_{cs}$ and $P_{cs}$ states, we investigate the strong interactions of the systems with open strangeness(es) from the light sector to the heavy sector (no beauty quark), where the interaction potential is derived from the vector meson exchange mechanism in $t$- and $u$-channels.","In the current work, we discuss all of single channel cases for the open strangeness in the systemic framework, where the resonances $X_0(2866)$, $D^*_{s0}(2317)$ and $D_{s1}(2460)$ are dynamically generated.","Furthermore, there are many new exotics predicted.","In addition, the left-hand cut problem in $t$- and $u$-channels is discussed in detail."],"url":"http://arxiv.org/abs/2406.08313v1","category":"hep-ph"}
{"created":"2024-06-12 15:12:49","title":"Causality for Tabular Data Synthesis: A High-Order Structure Causal Benchmark Framework","abstract":"Tabular synthesis models remain ineffective at capturing complex dependencies, and the quality of synthetic data is still insufficient for comprehensive downstream tasks, such as prediction under distribution shifts, automated decision-making, and cross-table understanding. A major challenge is the lack of prior knowledge about underlying structures and high-order relationships in tabular data. We argue that a systematic evaluation on high-order structural information for tabular data synthesis is the first step towards solving the problem. In this paper, we introduce high-order structural causal information as natural prior knowledge and provide a benchmark framework for the evaluation of tabular synthesis models. The framework allows us to generate benchmark datasets with a flexible range of data generation processes and to train tabular synthesis models using these datasets for further evaluation. We propose multiple benchmark tasks, high-order metrics, and causal inference tasks as downstream tasks for evaluating the quality of synthetic data generated by the trained models. Our experiments demonstrate to leverage the benchmark framework for evaluating the model capability of capturing high-order structural causal information. Furthermore, our benchmarking results provide an initial assessment of state-of-the-art tabular synthesis models. They have clearly revealed significant gaps between ideal and actual performance and how baseline methods differ. Our benchmark framework is available at URL https://github.com/TURuibo/CauTabBench.","sentences":["Tabular synthesis models remain ineffective at capturing complex dependencies, and the quality of synthetic data is still insufficient for comprehensive downstream tasks, such as prediction under distribution shifts, automated decision-making, and cross-table understanding.","A major challenge is the lack of prior knowledge about underlying structures and high-order relationships in tabular data.","We argue that a systematic evaluation on high-order structural information for tabular data synthesis is the first step towards solving the problem.","In this paper, we introduce high-order structural causal information as natural prior knowledge and provide a benchmark framework for the evaluation of tabular synthesis models.","The framework allows us to generate benchmark datasets with a flexible range of data generation processes and to train tabular synthesis models using these datasets for further evaluation.","We propose multiple benchmark tasks, high-order metrics, and causal inference tasks as downstream tasks for evaluating the quality of synthetic data generated by the trained models.","Our experiments demonstrate to leverage the benchmark framework for evaluating the model capability of capturing high-order structural causal information.","Furthermore, our benchmarking results provide an initial assessment of state-of-the-art tabular synthesis models.","They have clearly revealed significant gaps between ideal and actual performance and how baseline methods differ.","Our benchmark framework is available at URL https://github.com/TURuibo/CauTabBench."],"url":"http://arxiv.org/abs/2406.08311v1","category":"cs.LG"}
{"created":"2024-06-12 15:10:44","title":"GraphFM: A Comprehensive Benchmark for Graph Foundation Model","abstract":"Foundation Models (FMs) serve as a general class for the development of artificial intelligence systems, offering broad potential for generalization across a spectrum of downstream tasks. Despite extensive research into self-supervised learning as the cornerstone of FMs, several outstanding issues persist in Graph Foundation Models that rely on graph self-supervised learning, namely: 1) Homogenization. The extent of generalization capability on downstream tasks remains unclear. 2) Scalability. It is unknown how effectively these models can scale to large datasets. 3) Efficiency. The training time and memory usage of these models require evaluation. 4) Training Stop Criteria. Determining the optimal stopping strategy for pre-training across multiple tasks to maximize performance on downstream tasks. To address these questions, we have constructed a rigorous benchmark that thoroughly analyzes and studies the generalization and scalability of self-supervised Graph Neural Network (GNN) models. Regarding generalization, we have implemented and compared the performance of various self-supervised GNN models, trained to generate node representations, across tasks such as node classification, link prediction, and node clustering. For scalability, we have compared the performance of various models after training using full-batch and mini-batch strategies. Additionally, we have assessed the training efficiency of these models by conducting experiments to test their GPU memory usage and throughput. Through these experiments, we aim to provide insights to motivate future research. The code for this benchmark is publicly available at https://github.com/NYUSHCS/GraphFM.","sentences":["Foundation Models (FMs) serve as a general class for the development of artificial intelligence systems, offering broad potential for generalization across a spectrum of downstream tasks.","Despite extensive research into self-supervised learning as the cornerstone of FMs, several outstanding issues persist in Graph Foundation Models that rely on graph self-supervised learning, namely: 1) Homogenization.","The extent of generalization capability on downstream tasks remains unclear.","2) Scalability.","It is unknown how effectively these models can scale to large datasets.","3) Efficiency.","The training time and memory usage of these models require evaluation.","4) Training Stop Criteria.","Determining the optimal stopping strategy for pre-training across multiple tasks to maximize performance on downstream tasks.","To address these questions, we have constructed a rigorous benchmark that thoroughly analyzes and studies the generalization and scalability of self-supervised Graph Neural Network (GNN) models.","Regarding generalization, we have implemented and compared the performance of various self-supervised GNN models, trained to generate node representations, across tasks such as node classification, link prediction, and node clustering.","For scalability, we have compared the performance of various models after training using full-batch and mini-batch strategies.","Additionally, we have assessed the training efficiency of these models by conducting experiments to test their GPU memory usage and throughput.","Through these experiments, we aim to provide insights to motivate future research.","The code for this benchmark is publicly available at https://github.com/NYUSHCS/GraphFM."],"url":"http://arxiv.org/abs/2406.08310v1","category":"cs.LG"}
{"created":"2024-06-12 15:10:28","title":"Dynamical Lorentz Symmetry Breaking in a Scale-free Theory of Gravity","abstract":"This paper explores the renormalization of scale-free quadratic gravity coupled to the bumblebee field and its potential for dynamically breaking Lorentz symmetry. We conduct one-loop renormalization of the model and calculate the associated renormalization group functions. Additionally, we compute the one-loop effective potential for the bumblebee field, revealing that it acquires a non-trivial vacuum expectation value induced by radiative corrections - a phenomenon known as the Coleman-Weinberg mechanism. This spontaneous breaking of scale invariance arises from the non-vanishing vacuum expectation value of the bumblebee field, implicating Lorentz symmetry violation. Consequently, the non-minimal coupling between the bumblebee and gravitational fields results in a spontaneous generation of the Einstein-Hilbert term due to radiative corrections, thereby linking the Planck scale to Lorentz violation phenomena.","sentences":["This paper explores the renormalization of scale-free quadratic gravity coupled to the bumblebee field and its potential for dynamically breaking Lorentz symmetry.","We conduct one-loop renormalization of the model and calculate the associated renormalization group functions.","Additionally, we compute the one-loop effective potential for the bumblebee field, revealing that it acquires a non-trivial vacuum expectation value induced by radiative corrections - a phenomenon known as the Coleman-Weinberg mechanism.","This spontaneous breaking of scale invariance arises from the non-vanishing vacuum expectation value of the bumblebee field, implicating Lorentz symmetry violation.","Consequently, the non-minimal coupling between the bumblebee and gravitational fields results in a spontaneous generation of the Einstein-Hilbert term due to radiative corrections, thereby linking the Planck scale to Lorentz violation phenomena."],"url":"http://arxiv.org/abs/2406.08309v1","category":"hep-th"}
{"created":"2024-06-12 15:04:50","title":"Large Language Model(LLM) assisted End-to-End Network Health Management based on Multi-Scale Semanticization","abstract":"Network device and system health management is the foundation of modern network operations and maintenance. Traditional health management methods, relying on expert identification or simple rule-based algorithms, struggle to cope with the dynamic heterogeneous networks (DHNs) environment. Moreover, current state-of-the-art distributed anomaly detection methods, which utilize specific machine learning techniques, lack multi-scale adaptivity for heterogeneous device information, resulting in unsatisfactory diagnostic accuracy for DHNs. In this paper, we develop an LLM-assisted end-to-end intelligent network health management framework. The framework first proposes a Multi-Scale Semanticized Anomaly Detection Model (MSADM), incorporating semantic rule trees with an attention mechanism to address the multi-scale anomaly detection problem in DHNs. Secondly, a chain-of-thought-based large language model is embedded in downstream to adaptively analyze the fault detection results and produce an analysis report with detailed fault information and optimization strategies. Experimental results show that the accuracy of our proposed MSADM for heterogeneous network entity anomaly detection is as high as 91.31\\%.","sentences":["Network device and system health management is the foundation of modern network operations and maintenance.","Traditional health management methods, relying on expert identification or simple rule-based algorithms, struggle to cope with the dynamic heterogeneous networks (DHNs) environment.","Moreover, current state-of-the-art distributed anomaly detection methods, which utilize specific machine learning techniques, lack multi-scale adaptivity for heterogeneous device information, resulting in unsatisfactory diagnostic accuracy for DHNs.","In this paper, we develop an LLM-assisted end-to-end intelligent network health management framework.","The framework first proposes a Multi-Scale Semanticized Anomaly Detection Model (MSADM), incorporating semantic rule trees with an attention mechanism to address the multi-scale anomaly detection problem in DHNs.","Secondly, a chain-of-thought-based large language model is embedded in downstream to adaptively analyze the fault detection results and produce an analysis report with detailed fault information and optimization strategies.","Experimental results show that the accuracy of our proposed MSADM for heterogeneous network entity anomaly detection is as high as 91.31\\%."],"url":"http://arxiv.org/abs/2406.08305v1","category":"cs.NI"}
{"created":"2024-06-12 15:03:19","title":"The cosmology of $f(R, L_m)$ gravity: constraining the background and perturbed dynamics","abstract":"This paper delves into the late-time accelerated expansion of the universe and the evolution of cosmic structures within the context of a specific \\( f(R, L_m) \\) gravity model, formulated as \\( f(R, L_m) = \\lambda R + \\beta L_m^\\alpha + \\eta \\). To study the cosmological viability of the model, we employed the latest cosmic measurement datasets: i) 57 observational Hubble parameter data points (\\texttt{OHD}); ii) 1048 distance moduli data points (\\texttt{SNIa}); iii) a combined dataset (\\texttt{OHD+SNIa}); and large scale structure datasets, including iv) 14 growth rate data points (\\texttt{f}); and v) 30 redshift space distortion data points (\\texttt{f}$\\sigma_8$). These datasets facilitated the constraint of the \\( f(R, L_m) \\)-gravity model via MCMC simulations, followed by a comparative analysis with the \\(\\Lambda\\)CDM model. A comprehensive statistical analysis has been conducted to evaluate the \\( f(R, L_m) \\)-gravity model's efficacy in explaining both the accelerated expansion of the universe and the growth of cosmic structures.","sentences":["This paper delves into the late-time accelerated expansion of the universe and the evolution of cosmic structures within the context of a specific \\( f(R, L_m) \\) gravity model, formulated as \\( f(R, L_m) = \\lambda R","+","\\beta L_m^\\alpha +","\\eta \\).","To study the cosmological viability of the model, we employed the latest cosmic measurement datasets: i) 57 observational Hubble parameter data points (\\texttt{OHD}); ii) 1048 distance moduli data points (\\texttt{SNIa}); iii) a combined dataset (\\texttt{OHD+SNIa}); and large scale structure datasets, including iv) 14 growth rate data points (\\texttt{f}); and v) 30 redshift space distortion data points (\\texttt{f}$\\sigma_8$).","These datasets facilitated the constraint of the \\( f(R, L_m) \\)-gravity model via MCMC simulations, followed by a comparative analysis with the \\(\\Lambda\\)CDM model.","A comprehensive statistical analysis has been conducted to evaluate the \\( f(R, L_m) \\)-gravity model's efficacy in explaining both the accelerated expansion of the universe and the growth of cosmic structures."],"url":"http://arxiv.org/abs/2406.08303v1","category":"astro-ph.CO"}
{"created":"2024-06-12 14:59:12","title":"AdaNCA: Neural Cellular Automata As Adaptors For More Robust Vision Transformer","abstract":"Vision Transformers (ViTs) have demonstrated remarkable performance in image classification tasks, particularly when equipped with local information via region attention or convolutions. While such architectures improve the feature aggregation from different granularities, they often fail to contribute to the robustness of the networks. Neural Cellular Automata (NCA) enables the modeling of global cell representations through local interactions, with its training strategies and architecture design conferring strong generalization ability and robustness against noisy inputs. In this paper, we propose Adaptor Neural Cellular Automata (AdaNCA) for Vision Transformer that uses NCA as plug-in-play adaptors between ViT layers, enhancing ViT's performance and robustness against adversarial samples as well as out-of-distribution inputs. To overcome the large computational overhead of standard NCAs, we propose Dynamic Interaction for more efficient interaction learning. Furthermore, we develop an algorithm for identifying the most effective insertion points for AdaNCA based on our analysis of AdaNCA placement and robustness improvement. With less than a 3% increase in parameters, AdaNCA contributes to more than 10% absolute improvement in accuracy under adversarial attacks on the ImageNet1K benchmark. Moreover, we demonstrate with extensive evaluations across 8 robustness benchmarks and 4 ViT architectures that AdaNCA, as a plug-in-play module, consistently improves the robustness of ViTs.","sentences":["Vision Transformers (ViTs) have demonstrated remarkable performance in image classification tasks, particularly when equipped with local information via region attention or convolutions.","While such architectures improve the feature aggregation from different granularities, they often fail to contribute to the robustness of the networks.","Neural Cellular Automata (NCA) enables the modeling of global cell representations through local interactions, with its training strategies and architecture design conferring strong generalization ability and robustness against noisy inputs.","In this paper, we propose Adaptor Neural Cellular Automata (AdaNCA) for Vision Transformer that uses NCA as plug-in-play adaptors between ViT layers, enhancing ViT's performance and robustness against adversarial samples as well as out-of-distribution inputs.","To overcome the large computational overhead of standard NCAs, we propose Dynamic Interaction for more efficient interaction learning.","Furthermore, we develop an algorithm for identifying the most effective insertion points for AdaNCA based on our analysis of AdaNCA placement and robustness improvement.","With less than a 3% increase in parameters, AdaNCA contributes to more than 10% absolute improvement in accuracy under adversarial attacks on the ImageNet1K benchmark.","Moreover, we demonstrate with extensive evaluations across 8 robustness benchmarks and 4 ViT architectures that AdaNCA, as a plug-in-play module, consistently improves the robustness of ViTs."],"url":"http://arxiv.org/abs/2406.08298v1","category":"cs.CV"}
{"created":"2024-06-12 14:57:30","title":"A minimalistic and general weighted averaging method for inconsistent data","abstract":"The weighted average of inconsistent data is a common and tedious problem that many scientists have encountered. The standard weighted average is not recommended for these cases, and different alternative methods are proposed in the literature. Here, we introduce a new method based on Bayesian statistics for a broad application that keeps the number of assumptions to a minimum. The uncertainty associated with each input value is considered just a lower bound of the true unknown uncertainty. By assuming a non-informative (Jeffreys') prior for true uncertainty and marginalising over its value, a modified Gaussian distribution is obtained with smoothly decreasing wings, which allows for a better treatment of scattered data and outliers. The proposed method is tested on a series of data sets: simulations, CODATA recommended value of the Newtonian gravitational constant, and some particle properties from the Particle Data Group, including the proton charge radius and the mass of the W boson. For the latter in particular, contrary to other works, our prediction lies in good agreement with the Standard Model. A freely available Python library is also provided for a simple implementation of our averaging method.","sentences":["The weighted average of inconsistent data is a common and tedious problem that many scientists have encountered.","The standard weighted average is not recommended for these cases, and different alternative methods are proposed in the literature.","Here, we introduce a new method based on Bayesian statistics for a broad application that keeps the number of assumptions to a minimum.","The uncertainty associated with each input value is considered just a lower bound of the true unknown uncertainty.","By assuming a non-informative (Jeffreys') prior for true uncertainty and marginalising over its value, a modified Gaussian distribution is obtained with smoothly decreasing wings, which allows for a better treatment of scattered data and outliers.","The proposed method is tested on a series of data sets: simulations, CODATA recommended value of the Newtonian gravitational constant, and some particle properties from the Particle Data Group, including the proton charge radius and the mass of the W boson.","For the latter in particular, contrary to other works, our prediction lies in good agreement with the Standard Model.","A freely available Python library is also provided for a simple implementation of our averaging method."],"url":"http://arxiv.org/abs/2406.08293v1","category":"physics.data-an"}
{"created":"2024-06-12 14:56:56","title":"Outdoor Scene Extrapolation with Hierarchical Generative Cellular Automata","abstract":"We aim to generate fine-grained 3D geometry from large-scale sparse LiDAR scans, abundantly captured by autonomous vehicles (AV). Contrary to prior work on AV scene completion, we aim to extrapolate fine geometry from unlabeled and beyond spatial limits of LiDAR scans, taking a step towards generating realistic, high-resolution simulation-ready 3D street environments. We propose hierarchical Generative Cellular Automata (hGCA), a spatially scalable conditional 3D generative model, which grows geometry recursively with local kernels following, in a coarse-to-fine manner, equipped with a light-weight planner to induce global consistency. Experiments on synthetic scenes show that hGCA generates plausible scene geometry with higher fidelity and completeness compared to state-of-the-art baselines. Our model generalizes strongly from sim-to-real, qualitatively outperforming baselines on the Waymo-open dataset. We also show anecdotal evidence of the ability to create novel objects from real-world geometric cues even when trained on limited synthetic content. More results and details can be found on https://research.nvidia.com/labs/toronto-ai/hGCA/.","sentences":["We aim to generate fine-grained 3D geometry from large-scale sparse LiDAR scans, abundantly captured by autonomous vehicles (AV).","Contrary to prior work on AV scene completion, we aim to extrapolate fine geometry from unlabeled and beyond spatial limits of LiDAR scans, taking a step towards generating realistic, high-resolution simulation-ready 3D street environments.","We propose hierarchical Generative Cellular Automata (hGCA), a spatially scalable conditional 3D generative model, which grows geometry recursively with local kernels following, in a coarse-to-fine manner, equipped with a light-weight planner to induce global consistency.","Experiments on synthetic scenes show that hGCA generates plausible scene geometry with higher fidelity and completeness compared to state-of-the-art baselines.","Our model generalizes strongly from sim-to-real, qualitatively outperforming baselines on the Waymo-open dataset.","We also show anecdotal evidence of the ability to create novel objects from real-world geometric cues even when trained on limited synthetic content.","More results and details can be found on https://research.nvidia.com/labs/toronto-ai/hGCA/."],"url":"http://arxiv.org/abs/2406.08292v1","category":"cs.CV"}
{"created":"2024-06-12 14:56:28","title":"Unlabeled Compressed Sensing from Multiple Measurement Vectors","abstract":"This paper introduces an algorithmic solution to a broader class of unlabeled sensing problems with multiple measurement vectors (MMV). The goal is to recover an unknown structured signal matrix, $\\mathbf{X}$, from its noisy linear observation matrix, $\\mathbf{Y}$, whose rows are further randomly shuffled by an unknown permutation matrix $\\mathbf{U}$. A new Bayes-optimal unlabeled compressed sensing (UCS) recovery algorithm is developed from the bilinear approximate message passing (Bi-VAMP) framework using non-separable and coupled priors on the rows and columns of the permutation matrix $\\mathbf{U}$. In particular, standard unlabeled sensing is a special case of the proposed framework, and UCS further generalizes it by neither assuming a partially shuffled signal matrix $\\mathbf{X}$ nor a small-sized permutation matrix $\\mathbf{U}$. For the sake of theoretical performance prediction, we also conduct a state evolution (SE) analysis of the proposed algorithm and show its consistency with the asymptotic empirical mean-squared error (MSE). Numerical results demonstrate the effectiveness of the proposed UCS algorithm and its advantage over state-of-the-art baseline approaches in various applications. We also numerically examine the phase transition diagrams of UCS, thereby characterizing the detectability region as a function of the signal-to-noise ratio (SNR).","sentences":["This paper introduces an algorithmic solution to a broader class of unlabeled sensing problems with multiple measurement vectors (MMV).","The goal is to recover an unknown structured signal matrix, $\\mathbf{X}$, from its noisy linear observation matrix, $\\mathbf{Y}$, whose rows are further randomly shuffled by an unknown permutation matrix $\\mathbf{U}$. A new Bayes-optimal unlabeled compressed sensing (UCS) recovery algorithm is developed from the bilinear approximate message passing (Bi-VAMP) framework using non-separable and coupled priors on the rows and columns of the permutation matrix $\\mathbf{U}$. In particular, standard unlabeled sensing is a special case of the proposed framework, and UCS further generalizes it by neither assuming a partially shuffled signal matrix $\\mathbf{X}$ nor a small-sized permutation matrix $\\mathbf{U}$. For the sake of theoretical performance prediction, we also conduct a state evolution (SE) analysis of the proposed algorithm and show its consistency with the asymptotic empirical mean-squared error (MSE).","Numerical results demonstrate the effectiveness of the proposed UCS algorithm and its advantage over state-of-the-art baseline approaches in various applications.","We also numerically examine the phase transition diagrams of UCS, thereby characterizing the detectability region as a function of the signal-to-noise ratio (SNR)."],"url":"http://arxiv.org/abs/2406.08290v1","category":"cs.IT"}
{"created":"2024-06-12 14:54:21","title":"Wobbling and Migrating Ferrofluid Droplets","abstract":"Active components incorporated in materials generate motion by inducing conformational changes in response to external fields. Magnetic fields are particularly interesting as they can actuate materials remotely. Millimeter-sized ferrofluid droplets placed on a solid surface, surrounded by an ambient gas phase, are shown here to migrate under a rotating magnetic field due to the periodic deformation of the liquid-gas interface. This interface wobbling leads to droplet migration with speeds that increase as the amplitude and frequency of the magnetic field increase. In addition to migrating in a controlled manner, we demonstrate the ability of magnetic droplets to clean surface impurities and transport cargo.","sentences":["Active components incorporated in materials generate motion by inducing conformational changes in response to external fields.","Magnetic fields are particularly interesting as they can actuate materials remotely.","Millimeter-sized ferrofluid droplets placed on a solid surface, surrounded by an ambient gas phase, are shown here to migrate under a rotating magnetic field due to the periodic deformation of the liquid-gas interface.","This interface wobbling leads to droplet migration with speeds that increase as the amplitude and frequency of the magnetic field increase.","In addition to migrating in a controlled manner, we demonstrate the ability of magnetic droplets to clean surface impurities and transport cargo."],"url":"http://arxiv.org/abs/2406.08289v1","category":"physics.flu-dyn"}
{"created":"2024-06-12 14:53:30","title":"Decoupling the Class Label and the Target Concept in Machine Unlearning","abstract":"Machine unlearning as an emerging research topic for data regulations, aims to adjust a trained model to approximate a retrained one that excludes a portion of training data. Previous studies showed that class-wise unlearning is successful in forgetting the knowledge of a target class, through gradient ascent on the forgetting data or fine-tuning with the remaining data. However, while these methods are useful, they are insufficient as the class label and the target concept are often considered to coincide. In this work, we decouple them by considering the label domain mismatch and investigate three problems beyond the conventional all matched forgetting, e.g., target mismatch, model mismatch, and data mismatch forgetting. We systematically analyze the new challenges in restrictively forgetting the target concept and also reveal crucial forgetting dynamics in the representation level to realize these tasks. Based on that, we propose a general framework, namely, TARget-aware Forgetting (TARF). It enables the additional tasks to actively forget the target concept while maintaining the rest part, by simultaneously conducting annealed gradient ascent on the forgetting data and selected gradient descent on the hard-to-affect remaining data. Empirically, various experiments under the newly introduced settings are conducted to demonstrate the effectiveness of our TARF.","sentences":["Machine unlearning as an emerging research topic for data regulations, aims to adjust a trained model to approximate a retrained one that excludes a portion of training data.","Previous studies showed that class-wise unlearning is successful in forgetting the knowledge of a target class, through gradient ascent on the forgetting data or fine-tuning with the remaining data.","However, while these methods are useful, they are insufficient as the class label and the target concept are often considered to coincide.","In this work, we decouple them by considering the label domain mismatch and investigate three problems beyond the conventional all matched forgetting, e.g., target mismatch, model mismatch, and data mismatch forgetting.","We systematically analyze the new challenges in restrictively forgetting the target concept and also reveal crucial forgetting dynamics in the representation level to realize these tasks.","Based on that, we propose a general framework, namely, TARget-aware Forgetting (TARF).","It enables the additional tasks to actively forget the target concept while maintaining the rest part, by simultaneously conducting annealed gradient ascent on the forgetting data and selected gradient descent on the hard-to-affect remaining data.","Empirically, various experiments under the newly introduced settings are conducted to demonstrate the effectiveness of our TARF."],"url":"http://arxiv.org/abs/2406.08288v1","category":"cs.LG"}
{"created":"2024-06-12 14:53:23","title":"Pre-Training Identification of Graph Winning Tickets in Adaptive Spatial-Temporal Graph Neural Networks","abstract":"In this paper, we present a novel method to significantly enhance the computational efficiency of Adaptive Spatial-Temporal Graph Neural Networks (ASTGNNs) by introducing the concept of the Graph Winning Ticket (GWT), derived from the Lottery Ticket Hypothesis (LTH). By adopting a pre-determined star topology as a GWT prior to training, we balance edge reduction with efficient information propagation, reducing computational demands while maintaining high model performance. Both the time and memory computational complexity of generating adaptive spatial-temporal graphs is significantly reduced from $\\mathcal{O}(N^2)$ to $\\mathcal{O}(N)$. Our approach streamlines the ASTGNN deployment by eliminating the need for exhaustive training, pruning, and retraining cycles, and demonstrates empirically across various datasets that it is possible to achieve comparable performance to full models with substantially lower computational costs. Specifically, our approach enables training ASTGNNs on the largest scale spatial-temporal dataset using a single A6000 equipped with 48 GB of memory, overcoming the out-of-memory issue encountered during original training and even achieving state-of-the-art performance. {Furthermore, we delve into the effectiveness of the GWT from the perspective of spectral graph theory, providing substantial theoretical support.} This advancement not only proves the existence of efficient sub-networks within ASTGNNs but also broadens the applicability of the LTH in resource-constrained settings, marking a significant step forward in the field of graph neural networks. Code is available at https://anonymous.4open.science/r/paper-1430.","sentences":["In this paper, we present a novel method to significantly enhance the computational efficiency of Adaptive Spatial-Temporal Graph Neural Networks (ASTGNNs) by introducing the concept of the Graph Winning Ticket (GWT), derived from the Lottery Ticket Hypothesis (LTH).","By adopting a pre-determined star topology as a GWT prior to training, we balance edge reduction with efficient information propagation, reducing computational demands while maintaining high model performance.","Both the time and memory computational complexity of generating adaptive spatial-temporal graphs is significantly reduced from $\\mathcal{O}(N^2)$ to $\\mathcal{O}(N)$. Our approach streamlines the ASTGNN deployment by eliminating the need for exhaustive training, pruning, and retraining cycles, and demonstrates empirically across various datasets that it is possible to achieve comparable performance to full models with substantially lower computational costs.","Specifically, our approach enables training ASTGNNs on the largest scale spatial-temporal dataset using a single A6000 equipped with 48 GB of memory, overcoming the out-of-memory issue encountered during original training and even achieving state-of-the-art performance.","{Furthermore, we delve into the effectiveness of the GWT from the perspective of spectral graph theory, providing substantial theoretical support.}","This advancement not only proves the existence of efficient sub-networks within ASTGNNs but also broadens the applicability of the LTH in resource-constrained settings, marking a significant step forward in the field of graph neural networks.","Code is available at https://anonymous.4open.science/r/paper-1430."],"url":"http://arxiv.org/abs/2406.08287v1","category":"cs.LG"}
{"created":"2024-06-12 14:50:51","title":"Copy-composition for probabilistic graphical models","abstract":"In probabilistic modelling, joint distributions are often of more interest than their marginals, but the standard composition of stochastic channels is defined by marginalization. Recently, the notion of 'copy-composition' was introduced in order to circumvent this problem and express the chain rule of the relative entropy fibrationally, but while that goal was achieved, copy-composition lacked a satisfactory origin story. Here, we supply such a story for two standard probabilistic tools: directed and undirected graphical models. We explain that (directed) Bayesian networks may be understood as \"stochastic terms\" of product type, in which context copy-composition amounts to a pull-push operation. Likewise, we show that (undirected) factor graphs compose by copy-composition. In each case, our construction yields a double fibration of decorated (co)spans. Along the way, we introduce a useful bifibration of measure kernels, to provide semantics for the notion of stochastic term, which allows us to generalize probabilistic modelling from product to dependent types.","sentences":["In probabilistic modelling, joint distributions are often of more interest than their marginals, but the standard composition of stochastic channels is defined by marginalization.","Recently, the notion of 'copy-composition' was introduced in order to circumvent this problem and express the chain rule of the relative entropy fibrationally, but while that goal was achieved, copy-composition lacked a satisfactory origin story.","Here, we supply such a story for two standard probabilistic tools: directed and undirected graphical models.","We explain that (directed) Bayesian networks may be understood as \"stochastic terms\" of product type, in which context copy-composition amounts to a pull-push operation.","Likewise, we show that (undirected) factor graphs compose by copy-composition.","In each case, our construction yields a double fibration of decorated (co)spans.","Along the way, we introduce a useful bifibration of measure kernels, to provide semantics for the notion of stochastic term, which allows us to generalize probabilistic modelling from product to dependent types."],"url":"http://arxiv.org/abs/2406.08286v1","category":"math.CT"}
{"created":"2024-06-12 14:48:40","title":"A Hybrid Task-Constrained Motion Planning for Collaborative Robots in Intelligent Remanufacturing","abstract":"Industrial manipulators have extensively collaborated with human operators to execute tasks, e.g., disassembly of end-of-use products, in intelligent remanufacturing. A safety task execution requires real-time path planning for the manipulator's end-effector to autonomously avoid human operators. This is even more challenging when the end-effector needs to follow a planned path while avoiding the collision between the manipulator body and human operators, which is usually computationally expensive and limits real-time application. This paper proposes an efficient hybrid motion planning algorithm that consists of an A$^*$ algorithm and an online manipulator reconfiguration mechanism (OMRM) to tackle such challenges in task and configuration spaces respectively. The A$^*$ algorithm is first leveraged to plan the shortest collision-free path of the end-effector in task space. When the manipulator body is risky to the human operator, our OMRM then selects an alternative joint configuration with minimum reconfiguration effort from a database to assist the manipulator to follow the planned path and avoid the human operator simultaneously. The database of manipulator reconfiguration establishes the relationship between the task and configuration space offline using forward kinematics, and is able to provide multiple reconfiguration candidates for a desired end-effector's position. The proposed new hybrid algorithm plans safe manipulator motion during the whole task execution. Extensive numerical and experimental studies, as well as comparison studies between the proposed one and the state-of-the-art ones, have been conducted to validate the proposed motion planning algorithm.","sentences":["Industrial manipulators have extensively collaborated with human operators to execute tasks, e.g., disassembly of end-of-use products, in intelligent remanufacturing.","A safety task execution requires real-time path planning for the manipulator's end-effector to autonomously avoid human operators.","This is even more challenging when the end-effector needs to follow a planned path while avoiding the collision between the manipulator body and human operators, which is usually computationally expensive and limits real-time application.","This paper proposes an efficient hybrid motion planning algorithm that consists of an A$^*$ algorithm and an online manipulator reconfiguration mechanism (OMRM) to tackle such challenges in task and configuration spaces respectively.","The A$^*$ algorithm is first leveraged to plan the shortest collision-free path of the end-effector in task space.","When the manipulator body is risky to the human operator, our OMRM then selects an alternative joint configuration with minimum reconfiguration effort from a database to assist the manipulator to follow the planned path and avoid the human operator simultaneously.","The database of manipulator reconfiguration establishes the relationship between the task and configuration space offline using forward kinematics, and is able to provide multiple reconfiguration candidates for a desired end-effector's position.","The proposed new hybrid algorithm plans safe manipulator motion during the whole task execution.","Extensive numerical and experimental studies, as well as comparison studies between the proposed one and the state-of-the-art ones, have been conducted to validate the proposed motion planning algorithm."],"url":"http://arxiv.org/abs/2406.08283v1","category":"cs.RO"}
{"created":"2024-06-12 14:47:51","title":"Interpretable Representation Learning of Cardiac MRI via Attribute Regularization","abstract":"Interpretability is essential in medical imaging to ensure that clinicians can comprehend and trust artificial intelligence models. Several approaches have been recently considered to encode attributes in the latent space to enhance its interpretability. Notably, attribute regularization aims to encode a set of attributes along the dimensions of a latent representation. However, this approach is based on Variational AutoEncoder and suffers from blurry reconstruction. In this paper, we propose an Attributed-regularized Soft Introspective Variational Autoencoder that combines attribute regularization of the latent space within the framework of an adversarially trained variational autoencoder. We demonstrate on short-axis cardiac Magnetic Resonance images of the UK Biobank the ability of the proposed method to address blurry reconstruction issues of variational autoencoder methods while preserving the latent space interpretability.","sentences":["Interpretability is essential in medical imaging to ensure that clinicians can comprehend and trust artificial intelligence models.","Several approaches have been recently considered to encode attributes in the latent space to enhance its interpretability.","Notably, attribute regularization aims to encode a set of attributes along the dimensions of a latent representation.","However, this approach is based on Variational AutoEncoder and suffers from blurry reconstruction.","In this paper, we propose an Attributed-regularized Soft Introspective Variational Autoencoder that combines attribute regularization of the latent space within the framework of an adversarially trained variational autoencoder.","We demonstrate on short-axis cardiac Magnetic Resonance images of the UK Biobank the ability of the proposed method to address blurry reconstruction issues of variational autoencoder methods while preserving the latent space interpretability."],"url":"http://arxiv.org/abs/2406.08282v1","category":"eess.IV"}
{"created":"2024-06-12 14:42:37","title":"Positive and negative word of mouth in the United States","abstract":"Word of mouth is a process by which consumers transmit positive or negative sentiment to other consumers about a business. While this process has long been recognized as a type of promotion for businesses, the value of word of mouth is questionable. This study will examine the various correlates of word of mouth to demographic variables, including the role of the trust of business owners. Education level, region of residence, and income level were found to be significant predictors of positive word of mouth. Although the results generally suggest that the majority of respondents do not engage in word of mouth, there are valuable insights to be learned.","sentences":["Word of mouth is a process by which consumers transmit positive or negative sentiment to other consumers about a business.","While this process has long been recognized as a type of promotion for businesses, the value of word of mouth is questionable.","This study will examine the various correlates of word of mouth to demographic variables, including the role of the trust of business owners.","Education level, region of residence, and income level were found to be significant predictors of positive word of mouth.","Although the results generally suggest that the majority of respondents do not engage in word of mouth, there are valuable insights to be learned."],"url":"http://arxiv.org/abs/2406.08279v1","category":"econ.EM"}
{"created":"2024-06-12 14:40:28","title":"An accurate and transferable machine learning interatomic potential for equimolar and non-equimolar high-entropy diborides","abstract":"Machine learning interatomic potentials have become a powerful tool to achieve molecular dynamics (MD) simulations with the accuracy of ab initio methods while beyond their length and timescale limitations. Here, we develop an efficient moment tensor potential (MTP) for high-entropy diborides (HEBs) based on unary and binary diborides with Ti-V-Cr-Zr-Nb-Mo-Hf-Ta-W principal elements. Notably, the trained MTP exhibits exceptional generalization across both equimolar and non-equimolar HEBs, with testing errors in energy and force of 2.6 meV/atom and 155 meV/{\\AA} for equimolar HEBs, and 3.7 meV/atom and 172 meV/{\\AA} for non-equimolar HEBs, respectively, indicating its remarkable accuracy and transferability. The reliability of the established MTP is further confirmed by a comparative analysis with first-principles calculations, where our MTP accurately reproduces the structural and mechanical properties of various HEBs. The work presents a significant advancement in the simulation of high-entropy ceramics with enhanced efficiency and accuracy.","sentences":["Machine learning interatomic potentials have become a powerful tool to achieve molecular dynamics (MD) simulations with the accuracy of ab initio methods while beyond their length and timescale limitations.","Here, we develop an efficient moment tensor potential (MTP) for high-entropy diborides (HEBs) based on unary and binary diborides with Ti-V-Cr-Zr-Nb-Mo-Hf-Ta-W principal elements.","Notably, the trained MTP exhibits exceptional generalization across both equimolar and non-equimolar HEBs, with testing errors in energy and force of 2.6 meV/atom and 155 meV/{\\AA} for equimolar HEBs, and 3.7 meV/atom and 172 meV/{\\AA} for non-equimolar HEBs, respectively, indicating its remarkable accuracy and transferability.","The reliability of the established MTP is further confirmed by a comparative analysis with first-principles calculations, where our MTP accurately reproduces the structural and mechanical properties of various HEBs.","The work presents a significant advancement in the simulation of high-entropy ceramics with enhanced efficiency and accuracy."],"url":"http://arxiv.org/abs/2406.08275v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-12 14:37:29","title":"The Importance of Positional Encoding Initialization in Transformers for Relational Reasoning","abstract":"Relational reasoning refers to the ability to infer and understand the relations between multiple entities. In humans, this ability underpins many higher cognitive functions, such as problem solving and decision-making, and has been reliably linked to fluid intelligence. Despite machine learning models making impressive advances across various domains, such as natural language processing and vision, the extent to which such models can perform relational reasoning tasks remains unclear. Here we study the importance of positional encoding (PE) for relational reasoning in the Transformer, and find that a learnable PE outperforms all other commonly-used PEs (e.g., absolute, relative, rotary, etc.). Moreover, we find that when using a PE with a learnable parameter, the choice of initialization greatly influences the learned representations and its downstream generalization performance. Specifically, we find that a learned PE initialized from a small-norm distribution can 1) uncover ground-truth position information, 2) generalize in the presence of noisy inputs, and 3) produce behavioral patterns that are consistent with human performance. Our results shed light on the importance of learning high-performing and robust PEs during relational reasoning tasks, which will prove useful for tasks in which ground truth positions are not provided or not known.","sentences":["Relational reasoning refers to the ability to infer and understand the relations between multiple entities.","In humans, this ability underpins many higher cognitive functions, such as problem solving and decision-making, and has been reliably linked to fluid intelligence.","Despite machine learning models making impressive advances across various domains, such as natural language processing and vision, the extent to which such models can perform relational reasoning tasks remains unclear.","Here we study the importance of positional encoding (PE) for relational reasoning in the Transformer, and find that a learnable PE outperforms all other commonly-used PEs (e.g., absolute, relative, rotary, etc.).","Moreover, we find that when using a PE with a learnable parameter, the choice of initialization greatly influences the learned representations and its downstream generalization performance.","Specifically, we find that a learned PE initialized from a small-norm distribution can 1) uncover ground-truth position information, 2) generalize in the presence of noisy inputs, and 3) produce behavioral patterns that are consistent with human performance.","Our results shed light on the importance of learning high-performing and robust PEs during relational reasoning tasks, which will prove useful for tasks in which ground truth positions are not provided or not known."],"url":"http://arxiv.org/abs/2406.08272v1","category":"cs.LG"}
{"created":"2024-06-12 14:36:22","title":"Explainable AI improves task performance in human-AI collaboration","abstract":"Artificial intelligence (AI) provides considerable opportunities to assist human work. However, one crucial challenge of human-AI collaboration is that many AI algorithms operate in a black-box manner where the way how the AI makes predictions remains opaque. This makes it difficult for humans to validate a prediction made by AI against their own domain knowledge. For this reason, we hypothesize that augmenting humans with explainable AI as a decision aid improves task performance in human-AI collaboration. To test this hypothesis, we analyze the effect of augmenting domain experts with explainable AI in the form of visual heatmaps. We then compare participants that were either supported by (a) black-box AI or (b) explainable AI, where the latter supports them to follow AI predictions when the AI is accurate or overrule the AI when the AI predictions are wrong. We conducted two preregistered experiments with representative, real-world visual inspection tasks from manufacturing and medicine. The first experiment was conducted with factory workers from an electronics factory, who performed $N=9,600$ assessments of whether electronic products have defects. The second experiment was conducted with radiologists, who performed $N=5,650$ assessments of chest X-ray images to identify lung lesions. The results of our experiments with domain experts performing real-world tasks show that task performance improves when participants are supported by explainable AI instead of black-box AI. For example, in the manufacturing setting, we find that augmenting participants with explainable AI (as opposed to black-box AI) leads to a five-fold decrease in the median error rate of human decisions, which gives a significant improvement in task performance.","sentences":["Artificial intelligence (AI) provides considerable opportunities to assist human work.","However, one crucial challenge of human-AI collaboration is that many AI algorithms operate in a black-box manner where the way how the AI makes predictions remains opaque.","This makes it difficult for humans to validate a prediction made by AI against their own domain knowledge.","For this reason, we hypothesize that augmenting humans with explainable AI as a decision aid improves task performance in human-AI collaboration.","To test this hypothesis, we analyze the effect of augmenting domain experts with explainable AI in the form of visual heatmaps.","We then compare participants that were either supported by (a) black-box AI or (b) explainable AI, where the latter supports them to follow AI predictions when the AI is accurate or overrule the AI when the AI predictions are wrong.","We conducted two preregistered experiments with representative, real-world visual inspection tasks from manufacturing and medicine.","The first experiment was conducted with factory workers from an electronics factory, who performed $N=9,600$ assessments of whether electronic products have defects.","The second experiment was conducted with radiologists, who performed $N=5,650$ assessments of chest X-ray images to identify lung lesions.","The results of our experiments with domain experts performing real-world tasks show that task performance improves when participants are supported by explainable AI instead of black-box AI.","For example, in the manufacturing setting, we find that augmenting participants with explainable AI (as opposed to black-box AI) leads to a five-fold decrease in the median error rate of human decisions, which gives a significant improvement in task performance."],"url":"http://arxiv.org/abs/2406.08271v1","category":"cs.HC"}
{"created":"2024-06-12 14:35:43","title":"Boosting Multimedia Recommendation via Separate Generic and Unique Awareness","abstract":"Multimedia recommendation, which incorporates various modalities (e.g., images, texts, etc.) into user or item representation to improve recommendation quality, has received widespread attention. Recent methods mainly focus on cross-modal alignment with self-supervised learning to obtain higher quality representation. Despite remarkable performance, we argue that there is still a limitation: completely aligning representation undermines modality-unique information. We consider that cross-modal alignment is right, but it should not be the entirety, as different modalities contain generic information between them, and each modality also contains unique information. Simply aligning each modality may ignore modality-unique features, thus degrading the performance of multimedia recommendation. To tackle the above limitation, we propose a Separate Alignment aNd Distancing framework (SAND) for multimedia recommendation, which concurrently learns both modal-unique and -generic representation to achieve more comprehensive items representation. First, we split each modal feature into generic and unique part. Then, in the alignment module, for better integration of semantic information between different modalities , we design a SoloSimLoss to align generic modalities. Furthermore, in the distancing module, we aim to distance the unique modalities from the modal-generic so that each modality retains its unique and complementary information. In the light of the flexibility of our framework, we give two technical solutions, the more capable mutual information minimization and the simple negative l2 distance. Finally, extensive experimental results on three popular datasets demonstrate the effectiveness and generalization of our proposed framework.","sentences":["Multimedia recommendation, which incorporates various modalities (e.g., images, texts, etc.) into user or item representation to improve recommendation quality, has received widespread attention.","Recent methods mainly focus on cross-modal alignment with self-supervised learning to obtain higher quality representation.","Despite remarkable performance, we argue that there is still a limitation: completely aligning representation undermines modality-unique information.","We consider that cross-modal alignment is right, but it should not be the entirety, as different modalities contain generic information between them, and each modality also contains unique information.","Simply aligning each modality may ignore modality-unique features, thus degrading the performance of multimedia recommendation.","To tackle the above limitation, we propose a Separate Alignment aNd Distancing framework (SAND) for multimedia recommendation, which concurrently learns both modal-unique and -generic representation to achieve more comprehensive items representation.","First, we split each modal feature into generic and unique part.","Then, in the alignment module, for better integration of semantic information between different modalities , we design a SoloSimLoss to align generic modalities.","Furthermore, in the distancing module, we aim to distance the unique modalities from the modal-generic so that each modality retains its unique and complementary information.","In the light of the flexibility of our framework, we give two technical solutions, the more capable mutual information minimization and the simple negative l2 distance.","Finally, extensive experimental results on three popular datasets demonstrate the effectiveness and generalization of our proposed framework."],"url":"http://arxiv.org/abs/2406.08270v1","category":"cs.IR"}
{"created":"2024-06-12 14:35:19","title":"Analyzing constrained LLM through PDFA-learning","abstract":"We define a congruence that copes with null next-symbol probabilities that arise when the output of a language model is constrained by some means during text generation. We develop an algorithm for efficiently learning the quotient with respect to this congruence and evaluate it on case studies for analyzing statistical properties of LLM.","sentences":["We define a congruence that copes with null next-symbol probabilities that arise when the output of a language model is constrained by some means during text generation.","We develop an algorithm for efficiently learning the quotient with respect to this congruence and evaluate it on case studies for analyzing statistical properties of LLM."],"url":"http://arxiv.org/abs/2406.08269v1","category":"cs.FL"}
{"created":"2024-06-12 14:35:13","title":"A deep cut into Split Federated Self-supervised Learning","abstract":"Collaborative self-supervised learning has recently become feasible in highly distributed environments by dividing the network layers between client devices and a central server. However, state-of-the-art methods, such as MocoSFL, are optimized for network division at the initial layers, which decreases the protection of the client data and increases communication overhead. In this paper, we demonstrate that splitting depth is crucial for maintaining privacy and communication efficiency in distributed training. We also show that MocoSFL suffers from a catastrophic quality deterioration for the minimal communication overhead. As a remedy, we introduce Momentum-Aligned contrastive Split Federated Learning (MonAcoSFL), which aligns online and momentum client models during training procedure. Consequently, we achieve state-of-the-art accuracy while significantly reducing the communication overhead, making MonAcoSFL more practical in real-world scenarios.","sentences":["Collaborative self-supervised learning has recently become feasible in highly distributed environments by dividing the network layers between client devices and a central server.","However, state-of-the-art methods, such as MocoSFL, are optimized for network division at the initial layers, which decreases the protection of the client data and increases communication overhead.","In this paper, we demonstrate that splitting depth is crucial for maintaining privacy and communication efficiency in distributed training.","We also show that MocoSFL suffers from a catastrophic quality deterioration for the minimal communication overhead.","As a remedy, we introduce Momentum-Aligned contrastive Split Federated Learning (MonAcoSFL), which aligns online and momentum client models during training procedure.","Consequently, we achieve state-of-the-art accuracy while significantly reducing the communication overhead, making MonAcoSFL more practical in real-world scenarios."],"url":"http://arxiv.org/abs/2406.08267v1","category":"cs.LG"}
{"created":"2024-06-12 14:24:21","title":"The Mock Alexander Polynomial for Knotoids and Linkoids","abstract":"The mock Alexander polynomial is an extension of the classical Alexander polynomial, defined and studied for (virtual) knots and knotoids by the second and third authors. In this paper we consider the mock Alexander polynomial for generalizations of knotoids. We prove a conjecture on the mock Alexander polynomial for knotoids, which generalizes to uni-linkoids. Afterwards we give constructions for canonical invariants of linkoids derived from the mock Alexander polynomial, using the formalism of generalized knotoids due to Adams et al.","sentences":["The mock Alexander polynomial is an extension of the classical Alexander polynomial, defined and studied for (virtual) knots and knotoids by the second and third authors.","In this paper we consider the mock Alexander polynomial for generalizations of knotoids.","We prove a conjecture on the mock Alexander polynomial for knotoids, which generalizes to uni-linkoids.","Afterwards we give constructions for canonical invariants of linkoids derived from the mock Alexander polynomial, using the formalism of generalized knotoids due to Adams et al."],"url":"http://arxiv.org/abs/2406.08253v1","category":"math.GT"}
{"created":"2024-06-12 14:20:20","title":"Light-induced fictitious magnetic fields for quantum storage in cold atomic ensembles","abstract":"In this work, we have demonstrated that optically generated fictitious magnetic fields can be utilized to extend the lifetime of quantum memories in cold atomic ensembles. All the degrees of freedom of an AC Stark shift such as polarization, spatial profile, and temporal waveform can be readily controlled in a precise manner. Temporal fluctuations over several experimental cycles, and spatial inhomogeneities along a cold atomic gas have been compensated by an optical beam. The advantage of the use of fictitious magnetic fields for quantum storage stems from the speed and spatial precision that these fields can be synthesized. Our simple and versatile technique can find widespread application in coherent pulse and single-photon storage in any atomic species.","sentences":["In this work, we have demonstrated that optically generated fictitious magnetic fields can be utilized to extend the lifetime of quantum memories in cold atomic ensembles.","All the degrees of freedom of an AC Stark shift such as polarization, spatial profile, and temporal waveform can be readily controlled in a precise manner.","Temporal fluctuations over several experimental cycles, and spatial inhomogeneities along a cold atomic gas have been compensated by an optical beam.","The advantage of the use of fictitious magnetic fields for quantum storage stems from the speed and spatial precision that these fields can be synthesized.","Our simple and versatile technique can find widespread application in coherent pulse and single-photon storage in any atomic species."],"url":"http://arxiv.org/abs/2406.08251v1","category":"quant-ph"}
{"created":"2024-06-12 14:18:36","title":"Casimir Wormholes with GUP Correction in the Loop Quantum Cosmology","abstract":"In this paper, we obtain novel traversable, static, and spherically symmetric wormhole solutions, derived from the effective energy density and isotropic pressure resulting from the Casimir effect, corrected by the Generalized Uncertainty Principle (GUP) within the framework of Loop Quantum Cosmology (LQC). The goal is to explore the interplay between competing quantum gravity effects and quantum vacuum phenomena in the emergence of non-trivial spacetime structures. We examine features such as traversability, embedding diagrams, energy conditions, curvature, and stability of the obtained solutions. Additionally, we analyze the junction conditions required to integrate the wormhole spacetime with an external Schwarzschild spacetime and calculate the amount of exotic matter needed to maintain the wormhole. Finally, we evaluate the conditions under which this latter remains visible or is hidden by the event horizon associated with the Schwarzschild spacetime.","sentences":["In this paper, we obtain novel traversable, static, and spherically symmetric wormhole solutions, derived from the effective energy density and isotropic pressure resulting from the Casimir effect, corrected by the Generalized Uncertainty Principle (GUP) within the framework of Loop Quantum Cosmology (LQC).","The goal is to explore the interplay between competing quantum gravity effects and quantum vacuum phenomena in the emergence of non-trivial spacetime structures.","We examine features such as traversability, embedding diagrams, energy conditions, curvature, and stability of the obtained solutions.","Additionally, we analyze the junction conditions required to integrate the wormhole spacetime with an external Schwarzschild spacetime and calculate the amount of exotic matter needed to maintain the wormhole.","Finally, we evaluate the conditions under which this latter remains visible or is hidden by the event horizon associated with the Schwarzschild spacetime."],"url":"http://arxiv.org/abs/2406.08250v1","category":"gr-qc"}
{"created":"2024-06-12 14:18:07","title":"Dataset Enhancement with Instance-Level Augmentations","abstract":"We present a method for expanding a dataset by incorporating knowledge from the wide distribution of pre-trained latent diffusion models. Data augmentations typically incorporate inductive biases about the image formation process into the training (e.g. translation, scaling, colour changes, etc.). Here, we go beyond simple pixel transformations and introduce the concept of instance-level data augmentation by repainting parts of the image at the level of object instances. The method combines a conditional diffusion model with depth and edge maps control conditioning to seamlessly repaint individual objects inside the scene, being applicable to any segmentation or detection dataset. Used as a data augmentation method, it improves the performance and generalization of the state-of-the-art salient object detection, semantic segmentation and object detection models. By redrawing all privacy-sensitive instances (people, license plates, etc.), the method is also applicable for data anonymization. We also release fully synthetic and anonymized expansions for popular datasets: COCO, Pascal VOC and DUTS.","sentences":["We present a method for expanding a dataset by incorporating knowledge from the wide distribution of pre-trained latent diffusion models.","Data augmentations typically incorporate inductive biases about the image formation process into the training (e.g. translation, scaling, colour changes, etc.).","Here, we go beyond simple pixel transformations and introduce the concept of instance-level data augmentation by repainting parts of the image at the level of object instances.","The method combines a conditional diffusion model with depth and edge maps control conditioning to seamlessly repaint individual objects inside the scene, being applicable to any segmentation or detection dataset.","Used as a data augmentation method, it improves the performance and generalization of the state-of-the-art salient object detection, semantic segmentation and object detection models.","By redrawing all privacy-sensitive instances (people, license plates, etc.), the method is also applicable for data anonymization.","We also release fully synthetic and anonymized expansions for popular datasets: COCO, Pascal VOC and DUTS."],"url":"http://arxiv.org/abs/2406.08249v1","category":"cs.CV"}
{"created":"2024-06-12 14:15:15","title":"Leveraging Large Language Models for Web Scraping","abstract":"Large Language Models (LLMs) demonstrate remarkable capabilities in replicating human tasks and boosting productivity. However, their direct application for data extraction presents limitations due to a prioritisation of fluency over factual accuracy and a restricted ability to manipulate specific information. Therefore to overcome these limitations, this research leverages the knowledge representation power of pre-trained LLMs and the targeted information access enabled by RAG models, this research investigates a general-purpose accurate data scraping recipe for RAG models designed for language generation. To capture knowledge in a more modular and interpretable way, we use pre trained language models with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus. We utilised RAG model architecture and did an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Chunking HTML text for effective understanding, and (iii) comparing results from different LLMs and ranking algorithms. While previous work has developed dedicated architectures and training procedures for HTML understanding and extraction, we show that LLMs pre-trained on standard natural language with an addition of effective chunking, searching and ranking algorithms, can prove to be efficient data scraping tool to extract complex data from unstructured text. Future research directions include addressing the challenges of provenance tracking and dynamic knowledge updates within the proposed RAG-based data extraction framework. By overcoming these limitations, this approach holds the potential to revolutionise data extraction from vast repositories of textual information.","sentences":["Large Language Models (LLMs) demonstrate remarkable capabilities in replicating human tasks and boosting productivity.","However, their direct application for data extraction presents limitations due to a prioritisation of fluency over factual accuracy and a restricted ability to manipulate specific information.","Therefore to overcome these limitations, this research leverages the knowledge representation power of pre-trained LLMs and the targeted information access enabled by RAG models, this research investigates a general-purpose accurate data scraping recipe for RAG models designed for language generation.","To capture knowledge in a more modular and interpretable way, we use pre trained language models with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus.","We utilised RAG model architecture and did an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Chunking HTML text for effective understanding, and (iii) comparing results from different LLMs and ranking algorithms.","While previous work has developed dedicated architectures and training procedures for HTML understanding and extraction, we show that LLMs pre-trained on standard natural language with an addition of effective chunking, searching and ranking algorithms, can prove to be efficient data scraping tool to extract complex data from unstructured text.","Future research directions include addressing the challenges of provenance tracking and dynamic knowledge updates within the proposed RAG-based data extraction framework.","By overcoming these limitations, this approach holds the potential to revolutionise data extraction from vast repositories of textual information."],"url":"http://arxiv.org/abs/2406.08246v1","category":"cs.CL"}
{"created":"2024-06-12 14:12:53","title":"An efficient strategy to construct general machine learning potentials for high-entropy ceramics","abstract":"Molecular dynamics (MD) simulations powered by machine learning potentials (MLPs) have become an effective way to investigate complex systems. However, the construction of transferable MLPs with broad elemental applicability for HECs remains a challenge due to the vast compositional space. Taking high-entropy carbides (HECs) as the model, we propose a strategy to efficiently construct general neuroevolution potentials (NEPs) for HECs with up to ten principal elements based on carbides with low entropy. The trained NEP exhibits high accuracy and transferability for 3-10HECs with low testing errors of 15.7 meV/atom and 301 meV/{\\AA} for energy and force, respectively. Moreover, the accuracy, generalization, and reliability of our established NEP are further validated through the accurate predictions on structural, mechanical, and thermal properties of HECs with the comparison to first-principles calculations, experimental measurements, or the rule of mixture. Our work provides an efficient solution to developing general MLPs for high-entropy ceramics.","sentences":["Molecular dynamics (MD) simulations powered by machine learning potentials (MLPs) have become an effective way to investigate complex systems.","However, the construction of transferable MLPs with broad elemental applicability for HECs remains a challenge due to the vast compositional space.","Taking high-entropy carbides (HECs) as the model, we propose a strategy to efficiently construct general neuroevolution potentials (NEPs) for HECs with up to ten principal elements based on carbides with low entropy.","The trained NEP exhibits high accuracy and transferability for 3-10HECs with low testing errors of 15.7 meV/atom and 301 meV/{\\AA} for energy and force, respectively.","Moreover, the accuracy, generalization, and reliability of our established NEP are further validated through the accurate predictions on structural, mechanical, and thermal properties of HECs with the comparison to first-principles calculations, experimental measurements, or the rule of mixture.","Our work provides an efficient solution to developing general MLPs for high-entropy ceramics."],"url":"http://arxiv.org/abs/2406.08243v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-12 14:08:52","title":"PTHelper: An open source tool to support the Penetration Testing process","abstract":"Offensive security is one of the state of the art measures to protect enterprises and organizations. Penetration testing, broadly called pentesting, is a branch of offensive security designed to find, rate and exploit these vulnerabilities, in order to assess the security posture of an organization. This process is often time-consuming and the quantity of information that pentesters need to manage might also be difficult to handle. This project takes a practical approach to solve the automation of pentesting and proposes a usable tool, called PTHelper. This open-source tool has been designed in a modular way to be easily upgradable by the pentesting community, and uses state of the art tools and artificial intelligence to achieve its objective.","sentences":["Offensive security is one of the state of the art measures to protect enterprises and organizations.","Penetration testing, broadly called pentesting, is a branch of offensive security designed to find, rate and exploit these vulnerabilities, in order to assess the security posture of an organization.","This process is often time-consuming and the quantity of information that pentesters need to manage might also be difficult to handle.","This project takes a practical approach to solve the automation of pentesting and proposes a usable tool, called PTHelper.","This open-source tool has been designed in a modular way to be easily upgradable by the pentesting community, and uses state of the art tools and artificial intelligence to achieve its objective."],"url":"http://arxiv.org/abs/2406.08242v1","category":"cs.CR"}
{"created":"2024-06-12 14:06:28","title":"On categories with arbitrary 2-cell structures","abstract":"When a category is equipped with a 2-cell structure it becomes a sesquicategory but not necessarily a 2-category. It is widely accepted that the latter property is equivalent to the middle interchange law. However, little attention has been given to the study of the category of all 2-cell structures (seen as sesquicategories with a fixed underlying base category) other than as a generalization for 2-categories. The purpose of this work is to highlight the significance of such a study, which can prove valuable in identifying intrinsic features pertaining to the base category. These ideas are expanded upon through the guiding example of the category of monoids. Specifically, when a monoid is viewed as a one-object category, its 2-cell structures resemble semibimodules.","sentences":["When a category is equipped with a 2-cell structure it becomes a sesquicategory but not necessarily a 2-category.","It is widely accepted that the latter property is equivalent to the middle interchange law.","However, little attention has been given to the study of the category of all 2-cell structures (seen as sesquicategories with a fixed underlying base category) other than as a generalization for 2-categories.","The purpose of this work is to highlight the significance of such a study, which can prove valuable in identifying intrinsic features pertaining to the base category.","These ideas are expanded upon through the guiding example of the category of monoids.","Specifically, when a monoid is viewed as a one-object category, its 2-cell structures resemble semibimodules."],"url":"http://arxiv.org/abs/2406.08240v1","category":"math.CT"}
{"created":"2024-06-12 14:04:26","title":"Residual Learning and Context Encoding for Adaptive Offline-to-Online Reinforcement Learning","abstract":"Offline reinforcement learning (RL) allows learning sequential behavior from fixed datasets. Since offline datasets do not cover all possible situations, many methods collect additional data during online fine-tuning to improve performance. In general, these methods assume that the transition dynamics remain the same during both the offline and online phases of training. However, in many real-world applications, such as outdoor construction and navigation over rough terrain, it is common for the transition dynamics to vary between the offline and online phases. Moreover, the dynamics may vary during the online fine-tuning. To address this problem of changing dynamics from offline to online RL we propose a residual learning approach that infers dynamics changes to correct the outputs of the offline solution. At the online fine-tuning phase, we train a context encoder to learn a representation that is consistent inside the current online learning environment while being able to predict dynamic transitions. Experiments in D4RL MuJoCo environments, modified to support dynamics' changes upon environment resets, show that our approach can adapt to these dynamic changes and generalize to unseen perturbations in a sample-efficient way, whilst comparison methods cannot.","sentences":["Offline reinforcement learning (RL) allows learning sequential behavior from fixed datasets.","Since offline datasets do not cover all possible situations, many methods collect additional data during online fine-tuning to improve performance.","In general, these methods assume that the transition dynamics remain the same during both the offline and online phases of training.","However, in many real-world applications, such as outdoor construction and navigation over rough terrain, it is common for the transition dynamics to vary between the offline and online phases.","Moreover, the dynamics may vary during the online fine-tuning.","To address this problem of changing dynamics from offline to online RL we propose a residual learning approach that infers dynamics changes to correct the outputs of the offline solution.","At the online fine-tuning phase, we train a context encoder to learn a representation that is consistent inside the current online learning environment while being able to predict dynamic transitions.","Experiments in D4RL MuJoCo environments, modified to support dynamics' changes upon environment resets, show that our approach can adapt to these dynamic changes and generalize to unseen perturbations in a sample-efficient way, whilst comparison methods cannot."],"url":"http://arxiv.org/abs/2406.08238v1","category":"cs.LG"}
{"created":"2024-06-12 14:03:57","title":"Smith homomorphisms and Spin$^h$ structures","abstract":"In this article, we answer two questions of Buchanan-McKean (arXiv:2312.08209) about bordism for manifolds with spin$^h$ structures: we establish a Smith isomorphism between the reduced spin$^h$ bordism of $\\mathbb{RP}^\\infty$ and pin$^{h-}$ bordism, and we provide a geometric explanation for the isomorphism $\\Omega_{4k}^{\\mathrm{Spin}^c} \\otimes\\mathbb Z[1/2] \\cong \\Omega_{4k}^{\\mathrm{Spin}^h} \\otimes\\mathbb Z[1/2]$. Our proofs use the general theory of twisted spin structures and Smith homomorphisms that we developed in arXiv:2405.04649 joint with Devalapurkar, Liu, Pacheco-Tallaj, and Thorngren, specifically that the Smith homomorphism participates in a long exact sequence with explicit, computable terms.","sentences":["In this article, we answer two questions of Buchanan-McKean (arXiv:2312.08209) about bordism for manifolds with spin$^h$ structures: we establish a Smith isomorphism between the reduced spin$^h$ bordism of $\\mathbb{RP}^\\infty$ and pin$^{h-}$ bordism, and we provide a geometric explanation for the isomorphism $\\Omega_{4k}^{\\mathrm{Spin}^c} \\otimes\\mathbb Z[1/2]","\\cong \\Omega_{4k}^{\\mathrm{Spin}^h} \\otimes\\mathbb Z[1/2]$. Our proofs use the general theory of twisted spin structures and Smith homomorphisms that we developed in arXiv:2405.04649 joint with Devalapurkar, Liu, Pacheco-Tallaj, and Thorngren, specifically that the Smith homomorphism participates in a long exact sequence with explicit, computable terms."],"url":"http://arxiv.org/abs/2406.08237v1","category":"math.AT"}
{"created":"2024-06-12 14:00:07","title":"OpenCOLE: Towards Reproducible Automatic Graphic Design Generation","abstract":"Automatic generation of graphic designs has recently received considerable attention. However, the state-of-the-art approaches are complex and rely on proprietary datasets, which creates reproducibility barriers. In this paper, we propose an open framework for automatic graphic design called OpenCOLE, where we build a modified version of the pioneering COLE and train our model exclusively on publicly available datasets. Based on GPT4V evaluations, our model shows promising performance comparable to the original COLE. We release the pipeline and training results to encourage open development.","sentences":["Automatic generation of graphic designs has recently received considerable attention.","However, the state-of-the-art approaches are complex and rely on proprietary datasets, which creates reproducibility barriers.","In this paper, we propose an open framework for automatic graphic design called OpenCOLE, where we build a modified version of the pioneering COLE and train our model exclusively on publicly available datasets.","Based on GPT4V evaluations, our model shows promising performance comparable to the original COLE.","We release the pipeline and training results to encourage open development."],"url":"http://arxiv.org/abs/2406.08232v1","category":"cs.CV"}
{"created":"2024-06-12 13:59:45","title":"Using Deep Convolutional Neural Networks to Detect Rendered Glitches in Video Games","abstract":"In this paper, we present a method using Deep Convolutional Neural Networks (DCNNs) to detect common glitches in video games. The problem setting consists of an image (800x800 RGB) as input to be classified into one of five defined classes, normal image, or one of four different kinds of glitches (stretched, low resolution, missing and placeholder textures). Using a supervised approach, we train a ShuffleNetV2 using generated data. This work focuses on detecting texture graphical anomalies achieving arguably good performance with an accuracy of 86.8\\%, detecting 88\\% of the glitches with a false positive rate of 8.7\\%, and with the models being able to generalize and detect glitches even in unseen objects. We apply a confidence measure as well to tackle the issue with false positives as well as an effective way of aggregating images to achieve better detection in production. The main use of this work is the partial automatization of graphical testing in the final stages of video game development.","sentences":["In this paper, we present a method using Deep Convolutional Neural Networks (DCNNs) to detect common glitches in video games.","The problem setting consists of an image (800x800 RGB) as input to be classified into one of five defined classes, normal image, or one of four different kinds of glitches (stretched, low resolution, missing and placeholder textures).","Using a supervised approach, we train a ShuffleNetV2 using generated data.","This work focuses on detecting texture graphical anomalies achieving arguably good performance with an accuracy of 86.8\\%, detecting 88\\% of the glitches with a false positive rate of 8.7\\%, and with the models being able to generalize and detect glitches even in unseen objects.","We apply a confidence measure as well to tackle the issue with false positives as well as an effective way of aggregating images to achieve better detection in production.","The main use of this work is the partial automatization of graphical testing in the final stages of video game development."],"url":"http://arxiv.org/abs/2406.08231v1","category":"cs.CV"}
{"created":"2024-06-12 13:59:37","title":"Skyrmion blinking from the conical phase","abstract":"While the transition between skyrmionic and non-topological states has been widely explored as a bit operation for information transport and storage in spintronic devices, the ultrafast dynamics of such transitions remains challenging to observe and understand. Here, we utilize spin-dynamics simulations and harmonic transition state theory (HTST) to provide an in-depth analysis of the nucleation of skyrmionic states in helimagnets. We reveal a persistent blinking (creation-annihilation) phenomenon of these topological states under specific conditions near the phase boundary between skyrmion and conical states. Through a minimum-energy path analysis, we elucidate that this blinking behavior is favored by the formation of chiral bobber (CB) surface states and that the collapse of CBs differs from that of skyrmions in thin films due to their different oscillation modes. We further employ HTST to estimate the typical blinking time as a function of the applied magnetic field and temperature. Finally, we illustrate the practical use of skyrmion blinking for controlled probabilistic computing, exemplified by a skyrmion-based random-number generator.","sentences":["While the transition between skyrmionic and non-topological states has been widely explored as a bit operation for information transport and storage in spintronic devices, the ultrafast dynamics of such transitions remains challenging to observe and understand.","Here, we utilize spin-dynamics simulations and harmonic transition state theory (HTST) to provide an in-depth analysis of the nucleation of skyrmionic states in helimagnets.","We reveal a persistent blinking (creation-annihilation) phenomenon of these topological states under specific conditions near the phase boundary between skyrmion and conical states.","Through a minimum-energy path analysis, we elucidate that this blinking behavior is favored by the formation of chiral bobber (CB) surface states and that the collapse of CBs differs from that of skyrmions in thin films due to their different oscillation modes.","We further employ HTST to estimate the typical blinking time as a function of the applied magnetic field and temperature.","Finally, we illustrate the practical use of skyrmion blinking for controlled probabilistic computing, exemplified by a skyrmion-based random-number generator."],"url":"http://arxiv.org/abs/2406.08230v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-12 13:56:55","title":"Qualitative Data Analysis in Software Engineering: Techniques and Teaching Insights","abstract":"Software repositories are rich sources of qualitative artifacts, including source code comments, commit messages, issue descriptions, and documentation. These artifacts offer many interesting insights when analyzed through quantitative methods, as outlined in the chapter on mining software repositories. This chapter shifts the focus towards interpreting these artifacts using various qualitative data analysis techniques. We introduce qualitative coding as an iterative process, which is crucial not only for educational purposes but also to enhance the credibility and depth of research findings. Various coding methods are discussed along with the strategic design of a coding guide to ensure consistency and accuracy in data interpretation. The chapter also discusses quality assurance in qualitative data analysis, emphasizing principles such as credibility, transferability, dependability, and confirmability. These principles are vital to ensure that the findings are robust and can be generalized in different contexts. By sharing best practices and lessons learned, we aim to equip all readers with the tools necessary to conduct rigorous qualitative research in the field of software engineering.","sentences":["Software repositories are rich sources of qualitative artifacts, including source code comments, commit messages, issue descriptions, and documentation.","These artifacts offer many interesting insights when analyzed through quantitative methods, as outlined in the chapter on mining software repositories.","This chapter shifts the focus towards interpreting these artifacts using various qualitative data analysis techniques.","We introduce qualitative coding as an iterative process, which is crucial not only for educational purposes but also to enhance the credibility and depth of research findings.","Various coding methods are discussed along with the strategic design of a coding guide to ensure consistency and accuracy in data interpretation.","The chapter also discusses quality assurance in qualitative data analysis, emphasizing principles such as credibility, transferability, dependability, and confirmability.","These principles are vital to ensure that the findings are robust and can be generalized in different contexts.","By sharing best practices and lessons learned, we aim to equip all readers with the tools necessary to conduct rigorous qualitative research in the field of software engineering."],"url":"http://arxiv.org/abs/2406.08228v1","category":"cs.SE"}
{"created":"2024-06-12 13:55:29","title":"Measurement of the Imperceptible Threshold for Color Vibration Pairs Selected by using MacAdam Ellipse","abstract":"We propose an efficient method for searching for color vibration pairs that are imperceptible to the human eye based on the MacAdam ellipse, an experimentally determined color-difference range that is indistinguishable to the human eye. We created color pairs by selecting eight colors within the sRGB color space specified by the ellipse, and conducted experiments to confirm the threshold of the amplitude of color vibration amplitude at which flicker becomes imperceptible to the human eye. The experimental results indicate a general guideline for acceptable amplitudes for pair selection.","sentences":["We propose an efficient method for searching for color vibration pairs that are imperceptible to the human eye based on the MacAdam ellipse, an experimentally determined color-difference range that is indistinguishable to the human eye.","We created color pairs by selecting eight colors within the sRGB color space specified by the ellipse, and conducted experiments to confirm the threshold of the amplitude of color vibration amplitude at which flicker becomes imperceptible to the human eye.","The experimental results indicate a general guideline for acceptable amplitudes for pair selection."],"url":"http://arxiv.org/abs/2406.08227v1","category":"cs.HC"}
{"created":"2024-06-12 13:55:12","title":"DistilDoc: Knowledge Distillation for Visually-Rich Document Applications","abstract":"This work explores knowledge distillation (KD) for visually-rich document (VRD) applications such as document layout analysis (DLA) and document image classification (DIC). While VRD research is dependent on increasingly sophisticated and cumbersome models, the field has neglected to study efficiency via model compression. Here, we design a KD experimentation methodology for more lean, performant models on document understanding (DU) tasks that are integral within larger task pipelines. We carefully selected KD strategies (response-based, feature-based) for distilling knowledge to and from backbones with different architectures (ResNet, ViT, DiT) and capacities (base, small, tiny). We study what affects the teacher-student knowledge gap and find that some methods (tuned vanilla KD, MSE, SimKD with an apt projector) can consistently outperform supervised student training. Furthermore, we design downstream task setups to evaluate covariate shift and the robustness of distilled DLA models on zero-shot layout-aware document visual question answering (DocVQA). DLA-KD experiments result in a large mAP knowledge gap, which unpredictably translates to downstream robustness, accentuating the need to further explore how to efficiently obtain more semantic document layout awareness.","sentences":["This work explores knowledge distillation (KD) for visually-rich document (VRD) applications such as document layout analysis (DLA) and document image classification (DIC).","While VRD research is dependent on increasingly sophisticated and cumbersome models, the field has neglected to study efficiency via model compression.","Here, we design a KD experimentation methodology for more lean, performant models on document understanding (DU) tasks that are integral within larger task pipelines.","We carefully selected KD strategies (response-based, feature-based) for distilling knowledge to and from backbones with different architectures (ResNet, ViT, DiT) and capacities (base, small, tiny).","We study what affects the teacher-student knowledge gap and find that some methods (tuned vanilla KD, MSE, SimKD with an apt projector) can consistently outperform supervised student training.","Furthermore, we design downstream task setups to evaluate covariate shift and the robustness of distilled DLA models on zero-shot layout-aware document visual question answering (DocVQA).","DLA-KD experiments result in a large mAP knowledge gap, which unpredictably translates to downstream robustness, accentuating the need to further explore how to efficiently obtain more semantic document layout awareness."],"url":"http://arxiv.org/abs/2406.08226v1","category":"cs.CV"}
{"created":"2024-06-12 13:52:38","title":"Research Trends for the Interplay between Large Language Models and Knowledge Graphs","abstract":"This survey investigates the synergistic relationship between Large Language Models (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's capabilities in understanding, reasoning, and language processing. It aims to address gaps in current research by exploring areas such as KG Question Answering, ontology generation, KG validation, and the enhancement of KG accuracy and consistency through LLMs. The paper further examines the roles of LLMs in generating descriptive texts and natural language queries for KGs. Through a structured analysis that includes categorizing LLM-KG interactions, examining methodologies, and investigating collaborative uses and potential biases, this study seeks to provide new insights into the combined potential of LLMs and KGs. It highlights the importance of their interaction for improving AI applications and outlines future research directions.","sentences":["This survey investigates the synergistic relationship between Large Language Models (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's capabilities in understanding, reasoning, and language processing.","It aims to address gaps in current research by exploring areas such as KG Question Answering, ontology generation, KG validation, and the enhancement of KG accuracy and consistency through LLMs.","The paper further examines the roles of LLMs in generating descriptive texts and natural language queries for KGs.","Through a structured analysis that includes categorizing LLM-KG interactions, examining methodologies, and investigating collaborative uses and potential biases, this study seeks to provide new insights into the combined potential of LLMs and KGs.","It highlights the importance of their interaction for improving AI applications and outlines future research directions."],"url":"http://arxiv.org/abs/2406.08223v1","category":"cs.AI"}
{"created":"2024-06-12 13:52:30","title":"A Sociotechnical Lens for Evaluating Computer Vision Models: A Case Study on Detecting and Reasoning about Gender and Emotion","abstract":"In the evolving landscape of computer vision (CV) technologies, the automatic detection and interpretation of gender and emotion in images is a critical area of study. This paper investigates social biases in CV models, emphasizing the limitations of traditional evaluation metrics such as precision, recall, and accuracy. These metrics often fall short in capturing the complexities of gender and emotion, which are fluid and culturally nuanced constructs. Our study proposes a sociotechnical framework for evaluating CV models, incorporating both technical performance measures and considerations of social fairness. Using a dataset of 5,570 images related to vaccination and climate change, we empirically compared the performance of various CV models, including traditional models like DeepFace and FER, and generative models like GPT-4 Vision. Our analysis involved manually validating the gender and emotional expressions in a subset of images to serve as benchmarks. Our findings reveal that while GPT-4 Vision outperforms other models in technical accuracy for gender classification, it exhibits discriminatory biases, particularly in response to transgender and non-binary personas. Furthermore, the model's emotion detection skew heavily towards positive emotions, with a notable bias towards associating female images with happiness, especially when prompted by male personas. These findings underscore the necessity of developing more comprehensive evaluation criteria that address both validity and discriminatory biases in CV models. Our proposed framework provides guidelines for researchers to critically assess CV tools, ensuring their application in communication research is both ethical and effective. The significant contribution of this study lies in its emphasis on a sociotechnical approach, advocating for CV technologies that support social good and mitigate biases rather than perpetuate them.","sentences":["In the evolving landscape of computer vision (CV) technologies, the automatic detection and interpretation of gender and emotion in images is a critical area of study.","This paper investigates social biases in CV models, emphasizing the limitations of traditional evaluation metrics such as precision, recall, and accuracy.","These metrics often fall short in capturing the complexities of gender and emotion, which are fluid and culturally nuanced constructs.","Our study proposes a sociotechnical framework for evaluating CV models, incorporating both technical performance measures and considerations of social fairness.","Using a dataset of 5,570 images related to vaccination and climate change, we empirically compared the performance of various CV models, including traditional models like DeepFace and FER, and generative models like GPT-4 Vision.","Our analysis involved manually validating the gender and emotional expressions in a subset of images to serve as benchmarks.","Our findings reveal that while GPT-4 Vision outperforms other models in technical accuracy for gender classification, it exhibits discriminatory biases, particularly in response to transgender and non-binary personas.","Furthermore, the model's emotion detection skew heavily towards positive emotions, with a notable bias towards associating female images with happiness, especially when prompted by male personas.","These findings underscore the necessity of developing more comprehensive evaluation criteria that address both validity and discriminatory biases in CV models.","Our proposed framework provides guidelines for researchers to critically assess CV tools, ensuring their application in communication research is both ethical and effective.","The significant contribution of this study lies in its emphasis on a sociotechnical approach, advocating for CV technologies that support social good and mitigate biases rather than perpetuate them."],"url":"http://arxiv.org/abs/2406.08222v1","category":"cs.CV"}
{"created":"2024-06-12 13:51:51","title":"Can Large Language Models Analyze Software Failures in the News? An End-to-End Automated Pipeline with FAIL","abstract":"Software failures inform engineering work, standards, regulations. For example, the Log4J vulnerability brought government and industry attention to evaluating and securing software supply chains. Accessing private engineering records is difficult, so failure analyses tend to use information reported by the news media. However, prior works in this direction have relied on manual analysis. That has limited the scale of their analyses. The community lacks automated support to enable such analyses to consider a wide range of news sources and incidents.   In this paper, we propose the Failure Analysis Investigation with LLMs (FAIL) system to fill this gap. FAIL collects, analyzes, and summarizes software failures as reported in the news. FAIL groups articles that describe the same incidents. It then analyzes incidents using existing taxonomies for postmortems, faults, and system characteristics. To tune and evaluate FAIL, we followed the methods of prior works by manually analyzing 31 software failures. FAIL achieved an F1 score of 90% for collecting news about software failures, a V-measure of 0.98 for merging articles reporting on the same incident, and extracted 90% of the facts about failures. We then applied FAIL to a total of 137,427 news articles from 11 providers published between 2010 and 2022. FAIL identified and analyzed 2457 distinct failures reported across 4,184 articles. Our findings include: (1) current generation of large language models are capable of identifying news articles that describe failures, and analyzing them according to structured taxonomies; (2) high recurrences of similar failures within organizations and across organizations; and (3) severity of the consequences of software failures have increased over the past decade. The full FAIL database is available so that researchers, engineers, and policymakers can learn from a diversity of software failures.","sentences":["Software failures inform engineering work, standards, regulations.","For example, the Log4J vulnerability brought government and industry attention to evaluating and securing software supply chains.","Accessing private engineering records is difficult, so failure analyses tend to use information reported by the news media.","However, prior works in this direction have relied on manual analysis.","That has limited the scale of their analyses.","The community lacks automated support to enable such analyses to consider a wide range of news sources and incidents.   ","In this paper, we propose the Failure Analysis Investigation with LLMs (FAIL) system to fill this gap.","FAIL collects, analyzes, and summarizes software failures as reported in the news.","FAIL groups articles that describe the same incidents.","It then analyzes incidents using existing taxonomies for postmortems, faults, and system characteristics.","To tune and evaluate FAIL, we followed the methods of prior works by manually analyzing 31 software failures.","FAIL achieved an F1 score of 90% for collecting news about software failures, a V-measure of 0.98 for merging articles reporting on the same incident, and extracted 90% of the facts about failures.","We then applied FAIL to a total of 137,427 news articles from 11 providers published between 2010 and 2022.","FAIL identified and analyzed 2457 distinct failures reported across 4,184 articles.","Our findings include: (1) current generation of large language models are capable of identifying news articles that describe failures, and analyzing them according to structured taxonomies; (2) high recurrences of similar failures within organizations and across organizations; and (3) severity of the consequences of software failures have increased over the past decade.","The full FAIL database is available so that researchers, engineers, and policymakers can learn from a diversity of software failures."],"url":"http://arxiv.org/abs/2406.08221v1","category":"cs.SE"}
{"created":"2024-06-12 13:50:42","title":"Efficient Communication and Powering for Smart Contact Lens with Resonant Magneto-Quasistatic Coupling","abstract":"A two-coil wearable system is proposed for wireless communication and powering between a transmitter coil in a necklace and a receiver coil in a smart contact lens, where the necklace is invisible in contrast to coils embedded in wearables like spectacles or headbands. Magneto-quasistatic(MQS) field coupling facilitates communication between the transmitter in the necklace and the contact lens receiver, enabling AR/VR and health monitoring. As long as the receiver coil remains within the magnetic field generated by the transmitter, continuous communication is sustained through MQS field coupling despite the misalignments present. Resonant frequency tuning enhances system efficiency. The system's performance was tested for coil misalignments, showing a maximum path loss variation within $10 dB$ across scenarios, indicating robustness. Finite Element Method(FEM) analysis has been used to study the system for efficient wireless data transfer and powering. A communication channel capacity is $4.5 Mbps$ over a $1 MHz$ bandwidth. Simulations show negligible path loss differences with or without human tissues, as magnetic coupling remains unaffected at MQS frequencies below $30 MHz$ due to similar magnetic permeability of tissues and air. Therefore, the possibility of efficient communication and powering of smart contact lenses through a necklace is shown for the first time using resonant MQS coupling at an axial distance of $15cm$ and lateral distance of over $9cm$ to enable AR/VR and health monitoring on the contact lens.","sentences":["A two-coil wearable system is proposed for wireless communication and powering between a transmitter coil in a necklace and a receiver coil in a smart contact lens, where the necklace is invisible in contrast to coils embedded in wearables like spectacles or headbands.","Magneto-quasistatic(MQS) field coupling facilitates communication between the transmitter in the necklace and the contact lens receiver, enabling AR/VR and health monitoring.","As long as the receiver coil remains within the magnetic field generated by the transmitter, continuous communication is sustained through MQS field coupling despite the misalignments present.","Resonant frequency tuning enhances system efficiency.","The system's performance was tested for coil misalignments, showing a maximum path loss variation within $10 dB$ across scenarios, indicating robustness.","Finite Element Method(FEM) analysis has been used to study the system for efficient wireless data transfer and powering.","A communication channel capacity is $4.5 Mbps$ over a $1 MHz$ bandwidth.","Simulations show negligible path loss differences with or without human tissues, as magnetic coupling remains unaffected at MQS frequencies below $30 MHz$ due to similar magnetic permeability of tissues and air.","Therefore, the possibility of efficient communication and powering of smart contact lenses through a necklace is shown for the first time using resonant MQS coupling at an axial distance of $15cm$ and lateral distance of over $9cm$ to enable AR/VR and health monitoring on the contact lens."],"url":"http://arxiv.org/abs/2406.08220v1","category":"eess.SP"}
{"created":"2024-06-12 13:45:45","title":"A Software Engineering Perspective on Testing Large Language Models: Research, Practice, Tools and Benchmarks","abstract":"Large Language Models (LLMs) are rapidly becoming ubiquitous both as stand-alone tools and as components of current and future software systems. To enable usage of LLMs in the high-stake or safety-critical systems of 2030, they need to undergo rigorous testing. Software Engineering (SE) research on testing Machine Learning (ML) components and ML-based systems has systematically explored many topics such as test input generation and robustness. We believe knowledge about tools, benchmarks, research and practitioner views related to LLM testing needs to be similarly organized. To this end, we present a taxonomy of LLM testing topics and conduct preliminary studies of state of the art and practice approaches to research, open-source tools and benchmarks for LLM testing, mapping results onto this taxonomy. Our goal is to identify gaps requiring more research and engineering effort and inspire a clearer communication between LLM practitioners and the SE research community.","sentences":["Large Language Models (LLMs) are rapidly becoming ubiquitous both as stand-alone tools and as components of current and future software systems.","To enable usage of LLMs in the high-stake or safety-critical systems of 2030, they need to undergo rigorous testing.","Software Engineering (SE) research on testing Machine Learning (ML) components and ML-based systems has systematically explored many topics such as test input generation and robustness.","We believe knowledge about tools, benchmarks, research and practitioner views related to LLM testing needs to be similarly organized.","To this end, we present a taxonomy of LLM testing topics and conduct preliminary studies of state of the art and practice approaches to research, open-source tools and benchmarks for LLM testing, mapping results onto this taxonomy.","Our goal is to identify gaps requiring more research and engineering effort and inspire a clearer communication between LLM practitioners and the SE research community."],"url":"http://arxiv.org/abs/2406.08216v1","category":"cs.SE"}
{"created":"2024-06-12 13:44:58","title":"SumHiS: Extractive Summarization Exploiting Hidden Structure","abstract":"Extractive summarization is a task of highlighting the most important parts of the text. We introduce a new approach to extractive summarization task using hidden clustering structure of the text. Experimental results on CNN/DailyMail demonstrate that our approach generates more accurate summaries than both extractive and abstractive methods, achieving state-of-the-art results in terms of ROUGE-2 metric exceeding the previous approaches by 10%. Additionally, we show that hidden structure of the text could be interpreted as aspects.","sentences":["Extractive summarization is a task of highlighting the most important parts of the text.","We introduce a new approach to extractive summarization task using hidden clustering structure of the text.","Experimental results on CNN/DailyMail demonstrate that our approach generates more accurate summaries than both extractive and abstractive methods, achieving state-of-the-art results in terms of ROUGE-2 metric exceeding the previous approaches by 10%.","Additionally, we show that hidden structure of the text could be interpreted as aspects."],"url":"http://arxiv.org/abs/2406.08215v1","category":"cs.CL"}
{"created":"2024-06-12 13:44:22","title":"Graph Bottlenecked Social Recommendation","abstract":"With the emergence of social networks, social recommendation has become an essential technique for personalized services. Recently, graph-based social recommendations have shown promising results by capturing the high-order social influence. Most empirical studies of graph-based social recommendations directly take the observed social networks into formulation, and produce user preferences based on social homogeneity. Despite the effectiveness, we argue that social networks in the real-world are inevitably noisy~(existing redundant social relations), which may obstruct precise user preference characterization. Nevertheless, identifying and removing redundant social relations is challenging due to a lack of labels. In this paper, we focus on learning the denoised social structure to facilitate recommendation tasks from an information bottleneck perspective. Specifically, we propose a novel Graph Bottlenecked Social Recommendation (GBSR) framework to tackle the social noise issue.GBSR is a model-agnostic social denoising framework, that aims to maximize the mutual information between the denoised social graph and recommendation labels, meanwhile minimizing it between the denoised social graph and the original one. This enables GBSR to learn the minimal yet sufficient social structure, effectively reducing redundant social relations and enhancing social recommendations. Technically, GBSR consists of two elaborate components, preference-guided social graph refinement, and HSIC-based bottleneck learning. Extensive experimental results demonstrate the superiority of the proposed GBSR, including high performances and good generality combined with various backbones. Our code is available at: https://github.com/yimutianyang/KDD24-GBSR.","sentences":["With the emergence of social networks, social recommendation has become an essential technique for personalized services.","Recently, graph-based social recommendations have shown promising results by capturing the high-order social influence.","Most empirical studies of graph-based social recommendations directly take the observed social networks into formulation, and produce user preferences based on social homogeneity.","Despite the effectiveness, we argue that social networks in the real-world are inevitably noisy~(existing redundant social relations), which may obstruct precise user preference characterization.","Nevertheless, identifying and removing redundant social relations is challenging due to a lack of labels.","In this paper, we focus on learning the denoised social structure to facilitate recommendation tasks from an information bottleneck perspective.","Specifically, we propose a novel Graph Bottlenecked Social Recommendation (GBSR) framework to tackle the social noise issue.","GBSR is a model-agnostic social denoising framework, that aims to maximize the mutual information between the denoised social graph and recommendation labels, meanwhile minimizing it between the denoised social graph and the original one.","This enables GBSR to learn the minimal yet sufficient social structure, effectively reducing redundant social relations and enhancing social recommendations.","Technically, GBSR consists of two elaborate components, preference-guided social graph refinement, and HSIC-based bottleneck learning.","Extensive experimental results demonstrate the superiority of the proposed GBSR, including high performances and good generality combined with various backbones.","Our code is available at: https://github.com/yimutianyang/KDD24-GBSR."],"url":"http://arxiv.org/abs/2406.08214v1","category":"cs.IR"}
{"created":"2024-06-12 13:43:01","title":"Designing metasurface optical interfaces for solid-state qubits using many-body adjoint shape optimization","abstract":"We present a general strategy for the inverse design of metasurfaces composed of elementary shapes. We use it to design a structure that collects and collimates light from nitrogen-vacancy centers in diamond. Such metasurfaces constitute scalable optical interfaces for solid-state qubits, enabling efficient photon coupling into optical fibers and eliminating free-space collection optics. The many-body shape optimization strategy is a practical alternative to topology optimization that explicitly enforces material and fabrication constraints throughout the optimization, while still achieving high performance. The metasurface is easily adaptable to other solid-state qubits, and the optimization method is broadly applicable to fabrication-constrained photonic design problems.","sentences":["We present a general strategy for the inverse design of metasurfaces composed of elementary shapes.","We use it to design a structure that collects and collimates light from nitrogen-vacancy centers in diamond.","Such metasurfaces constitute scalable optical interfaces for solid-state qubits, enabling efficient photon coupling into optical fibers and eliminating free-space collection optics.","The many-body shape optimization strategy is a practical alternative to topology optimization that explicitly enforces material and fabrication constraints throughout the optimization, while still achieving high performance.","The metasurface is easily adaptable to other solid-state qubits, and the optimization method is broadly applicable to fabrication-constrained photonic design problems."],"url":"http://arxiv.org/abs/2406.08212v1","category":"physics.optics"}
{"created":"2024-06-12 13:41:39","title":"A Thermodynamic Study of $\\textbf{(2+1)}$-Dimensional Analytic Charged Hairy Black Holes with Born-Infeld Electrodynamics","abstract":"This work presents analytical black hole solutions for a coupled Einstein-Born-Infeld-Scalar gravity system in AdS spacetime with two different non-minimal coupling functions $f(z)$. For both solutions, we establish the regularity of the scalar field and curvature scalars outside the horizon. For one of the considered coupling cases, thermodynamic analysis in the canonical ensemble reveals stability across all temperatures, while the other case exhibits the Hawking/Page phase transition between the stable large phase of the black hole and thermal-AdS. We investigate the effect of the scalar hair parameter and black hole charge on the phase transition temperature and observe that the critical values of the scalar hair and the charge parameters constrain the feasibility of Hawking/Page phase transition.","sentences":["This work presents analytical black hole solutions for a coupled Einstein-Born-Infeld-Scalar gravity system in AdS spacetime with two different non-minimal coupling functions $f(z)$. For both solutions, we establish the regularity of the scalar field and curvature scalars outside the horizon.","For one of the considered coupling cases, thermodynamic analysis in the canonical ensemble reveals stability across all temperatures, while the other case exhibits the Hawking/Page phase transition between the stable large phase of the black hole and","thermal-AdS.","We investigate the effect of the scalar hair parameter and black hole charge on the phase transition temperature and observe that the critical values of the scalar hair and the charge parameters constrain the feasibility of Hawking/Page phase transition."],"url":"http://arxiv.org/abs/2406.08211v1","category":"gr-qc"}
{"created":"2024-06-12 13:41:07","title":"Expressivity and Generalization: Fragment-Biases for Molecular GNNs","abstract":"Although recent advances in higher-order Graph Neural Networks (GNNs) improve the theoretical expressiveness and molecular property predictive performance, they often fall short of the empirical performance of models that explicitly use fragment information as inductive bias. However, for these approaches, there exists no theoretic expressivity study. In this work, we propose the Fragment-WL test, an extension to the well-known Weisfeiler & Leman (WL) test, which enables the theoretic analysis of these fragment-biased GNNs. Building on the insights gained from the Fragment-WL test, we develop a new GNN architecture and a fragmentation with infinite vocabulary that significantly boosts expressiveness. We show the effectiveness of our model on synthetic and real-world data where we outperform all GNNs on Peptides and have 12% lower error than all GNNs on ZINC and 34% lower error than other fragment-biased models. Furthermore, we show that our model exhibits superior generalization capabilities compared to the latest transformer-based architectures, positioning it as a robust solution for a range of molecular modeling tasks.","sentences":["Although recent advances in higher-order Graph Neural Networks (GNNs) improve the theoretical expressiveness and molecular property predictive performance, they often fall short of the empirical performance of models that explicitly use fragment information as inductive bias.","However, for these approaches, there exists no theoretic expressivity study.","In this work, we propose the Fragment-WL test, an extension to the well-known Weisfeiler & Leman (WL) test, which enables the theoretic analysis of these fragment-biased GNNs.","Building on the insights gained from the Fragment-WL test, we develop a new GNN architecture and a fragmentation with infinite vocabulary that significantly boosts expressiveness.","We show the effectiveness of our model on synthetic and real-world data where we outperform all GNNs on Peptides and have 12% lower error than all GNNs on ZINC and 34% lower error than other fragment-biased models.","Furthermore, we show that our model exhibits superior generalization capabilities compared to the latest transformer-based architectures, positioning it as a robust solution for a range of molecular modeling tasks."],"url":"http://arxiv.org/abs/2406.08210v1","category":"cs.LG"}
{"created":"2024-06-12 13:38:10","title":"Diffusion-Promoted HDR Video Reconstruction","abstract":"High dynamic range (HDR) video reconstruction aims to generate HDR videos from low dynamic range (LDR) frames captured with alternating exposures. Most existing works solely rely on the regression-based paradigm, leading to adverse effects such as ghosting artifacts and missing details in saturated regions. In this paper, we propose a diffusion-promoted method for HDR video reconstruction, termed HDR-V-Diff, which incorporates a diffusion model to capture the HDR distribution. As such, HDR-V-Diff can reconstruct HDR videos with realistic details while alleviating ghosting artifacts. However, the direct introduction of video diffusion models would impose massive computational burden. Instead, to alleviate this burden, we first propose an HDR Latent Diffusion Model (HDR-LDM) to learn the distribution prior of single HDR frames. Specifically, HDR-LDM incorporates a tonemapping strategy to compress HDR frames into the latent space and a novel exposure embedding to aggregate the exposure information into the diffusion process. We then propose a Temporal-Consistent Alignment Module (TCAM) to learn the temporal information as a complement for HDR-LDM, which conducts coarse-to-fine feature alignment at different scales among video frames. Finally, we design a Zero-Init Cross-Attention (ZiCA) mechanism to effectively integrate the learned distribution prior and temporal information for generating HDR frames. Extensive experiments validate that HDR-V-Diff achieves state-of-the-art results on several representative datasets.","sentences":["High dynamic range (HDR) video reconstruction aims to generate HDR videos from low dynamic range (LDR) frames captured with alternating exposures.","Most existing works solely rely on the regression-based paradigm, leading to adverse effects such as ghosting artifacts and missing details in saturated regions.","In this paper, we propose a diffusion-promoted method for HDR video reconstruction, termed HDR-V-Diff, which incorporates a diffusion model to capture the HDR distribution.","As such, HDR-V-Diff can reconstruct HDR videos with realistic details while alleviating ghosting artifacts.","However, the direct introduction of video diffusion models would impose massive computational burden.","Instead, to alleviate this burden, we first propose an HDR Latent Diffusion Model (HDR-LDM) to learn the distribution prior of single HDR frames.","Specifically, HDR-LDM incorporates a tonemapping strategy to compress HDR frames into the latent space and a novel exposure embedding to aggregate the exposure information into the diffusion process.","We then propose a Temporal-Consistent Alignment Module (TCAM) to learn the temporal information as a complement for HDR-LDM, which conducts coarse-to-fine feature alignment at different scales among video frames.","Finally, we design a Zero-Init Cross-Attention (ZiCA) mechanism to effectively integrate the learned distribution prior and temporal information for generating HDR frames.","Extensive experiments validate that HDR-V-Diff achieves state-of-the-art results on several representative datasets."],"url":"http://arxiv.org/abs/2406.08204v1","category":"cs.CV"}
{"created":"2024-06-12 13:36:03","title":"LAFMA: A Latent Flow Matching Model for Text-to-Audio Generation","abstract":"Recently, the application of diffusion models has facilitated the significant development of speech and audio generation. Nevertheless, the quality of samples generated by diffusion models still needs improvement. And the effectiveness of the method is accompanied by the extensive number of sampling steps, leading to an extended synthesis time necessary for generating high-quality audio. Previous Text-to-Audio (TTA) methods mostly used diffusion models in the latent space for audio generation. In this paper, we explore the integration of the Flow Matching (FM) model into the audio latent space for audio generation. The FM is an alternative simulation-free method that trains continuous normalization flows (CNF) based on regressing vector fields. We demonstrate that our model significantly enhances the quality of generated audio samples, achieving better performance than prior models. Moreover, it reduces the number of inference steps to ten steps almost without sacrificing performance.","sentences":["Recently, the application of diffusion models has facilitated the significant development of speech and audio generation.","Nevertheless, the quality of samples generated by diffusion models still needs improvement.","And the effectiveness of the method is accompanied by the extensive number of sampling steps, leading to an extended synthesis time necessary for generating high-quality audio.","Previous Text-to-Audio (TTA) methods mostly used diffusion models in the latent space for audio generation.","In this paper, we explore the integration of the Flow Matching (FM) model into the audio latent space for audio generation.","The FM is an alternative simulation-free method that trains continuous normalization flows (CNF) based on regressing vector fields.","We demonstrate that our model significantly enhances the quality of generated audio samples, achieving better performance than prior models.","Moreover, it reduces the number of inference steps to ten steps almost without sacrificing performance."],"url":"http://arxiv.org/abs/2406.08203v1","category":"eess.AS"}
{"created":"2024-06-12 13:35:10","title":"A Dialogue Game for Eliciting Balanced Collaboration","abstract":"Collaboration is an integral part of human dialogue. Typical task-oriented dialogue games assign asymmetric roles to the participants, which limits their ability to elicit naturalistic role-taking in collaboration and its negotiation. We present a novel and simple online setup that favors balanced collaboration: a two-player 2D object placement game in which the players must negotiate the goal state themselves. We show empirically that human players exhibit a variety of role distributions, and that balanced collaboration improves task performance. We also present an LLM-based baseline agent which demonstrates that automatic playing of our game is an interesting challenge for artificial systems.","sentences":["Collaboration is an integral part of human dialogue.","Typical task-oriented dialogue games assign asymmetric roles to the participants, which limits their ability to elicit naturalistic role-taking in collaboration and its negotiation.","We present a novel and simple online setup that favors balanced collaboration: a two-player 2D object placement game in which the players must negotiate the goal state themselves.","We show empirically that human players exhibit a variety of role distributions, and that balanced collaboration improves task performance.","We also present an LLM-based baseline agent which demonstrates that automatic playing of our game is an interesting challenge for artificial systems."],"url":"http://arxiv.org/abs/2406.08202v1","category":"cs.CL"}
{"created":"2024-06-12 13:33:24","title":"Asynchronous Voice Anonymization Using Adversarial Perturbation On Speaker Embedding","abstract":"Voice anonymization has been developed as a technique for preserving privacy by replacing the speaker's voice in a speech signal with that of a pseudo-speaker, thereby obscuring the original voice attributes from machine recognition and human perception. In this paper, we focus on altering the voice attributes against machine recognition while retaining human perception. We referred to this as the asynchronous voice anonymization. To this end, a speech generation framework incorporating a speaker disentanglement mechanism is employed to generate the anonymized speech. The speaker attributes are altered through adversarial perturbation applied on the speaker embedding, while human perception is preserved by controlling the intensity of perturbation. Experiments conducted on the LibriSpeech dataset showed that the speaker attributes were obscured with their human perception preserved for 60.71% of the processed utterances.","sentences":["Voice anonymization has been developed as a technique for preserving privacy by replacing the speaker's voice in a speech signal with that of a pseudo-speaker, thereby obscuring the original voice attributes from machine recognition and human perception.","In this paper, we focus on altering the voice attributes against machine recognition while retaining human perception.","We referred to this as the asynchronous voice anonymization.","To this end, a speech generation framework incorporating a speaker disentanglement mechanism is employed to generate the anonymized speech.","The speaker attributes are altered through adversarial perturbation applied on the speaker embedding, while human perception is preserved by controlling the intensity of perturbation.","Experiments conducted on the LibriSpeech dataset showed that the speaker attributes were obscured with their human perception preserved for 60.71% of the processed utterances."],"url":"http://arxiv.org/abs/2406.08200v1","category":"cs.SD"}
{"created":"2024-06-12 13:29:30","title":"On the equivalence of quasirandomness and exchangeable representations independent from lower-order variables","abstract":"It is often convenient to represent a process for randomly generating a graph as a graphon. (More precisely, these give \\emph{vertex exchangeable} processes -- those processes in which each vertex is treated the same way.) Other structures can be treated by generalizations like hypergraphons, permutatons, and, for a very general class, theons. These representations are not unique: different representations can lead to the same probability distribution on graphs. This naturally leads to questions (going back at least to Hoover's proof of the Aldous--Hoover Theorem on the existence of such representations) that ask when quasirandomness properties on the distribution guarantee the existence of particularly simple representations.   We extend the usual theon representation by adding an additional datum of a random permutation to each tuple, which we call a $\\ast$-representation. We show that if a process satisfies the \\emph{unique coupling} property UCouple[$\\ell$], which says roughly that all $\\ell$-tuples of vertices ``look the same'', then the process is $\\ast$-$\\ell$-independent: there is a $\\ast$-representation that does not make use of any random information about $\\ell$-tuples (including tuples of length $<\\ell$). Simple examples show that the use of $\\ast$-representations is necessary.   This resolves a question of Coregliano and Razborov, since it easily follows that UCouple[l] implies Independence[\\ell'] (the existence of an $\\ell'$-independent ordinary representation) for $\\ell'<\\ell$.","sentences":["It is often convenient to represent a process for randomly generating a graph as a graphon.","(More precisely, these give \\emph{vertex exchangeable} processes -- those processes in which each vertex is treated the same way.)","Other structures can be treated by generalizations like hypergraphons, permutatons, and, for a very general class, theons.","These representations are not unique: different representations can lead to the same probability distribution on graphs.","This naturally leads to questions (going back at least to Hoover's proof of the Aldous--Hoover Theorem on the existence of such representations) that ask when quasirandomness properties on the distribution guarantee the existence of particularly simple representations.   ","We extend the usual theon representation by adding an additional datum of a random permutation to each tuple, which we call a $\\ast$-representation.","We show that if a process satisfies the \\emph{unique coupling} property UCouple[$\\ell$], which says roughly that all $\\ell$-tuples of vertices ``look the same'', then the process is $\\ast$-$\\ell$-independent: there is a $\\ast$-representation that does not make use of any random information about $\\ell$-tuples (including tuples of length $<\\ell$).","Simple examples show that the use of $\\ast$-representations is necessary.   ","This resolves a question of Coregliano and Razborov, since it easily follows that UCouple[l] implies Independence[\\ell'] (the existence of an $\\ell'$-independent ordinary representation) for $\\ell'<\\ell$."],"url":"http://arxiv.org/abs/2406.08195v1","category":"math.CO"}
{"created":"2024-06-12 13:22:26","title":"Minimal Communication-Cost Statistical Learning","abstract":"A client device which has access to $n$ training data samples needs to obtain a statistical hypothesis or model $W$ and then to send it to a remote server. The client and the server devices share some common randomness sequence as well as a prior on the hypothesis space. In this problem a suitable hypothesis or model $W$ should meet two distinct design criteria simultaneously: (i) small (population) risk during the inference phase and (ii) small 'complexity' for it to be conveyed to the server with minimum communication cost. In this paper, we propose a joint training and source coding scheme with provable in-expectation guarantees, where the expectation is over the encoder's output message. Specifically, we show that by imposing a constraint on a suitable Kullback-Leibler divergence between the conditional distribution induced by a compressed learning model $\\widehat{W}$ given $W$ and the prior, one guarantees simultaneously small average empirical risk (aka training loss), small average generalization error and small average communication cost. We also consider a one-shot scenario in which the guarantees on the empirical risk and generalization error are obtained for every encoder's output message.","sentences":["A client device which has access to $n$ training data samples needs to obtain a statistical hypothesis or model $W$ and then to send it to a remote server.","The client and the server devices share some common randomness sequence as well as a prior on the hypothesis space.","In this problem a suitable hypothesis or model $W$ should meet two distinct design criteria simultaneously: (i) small (population) risk during the inference phase and (ii) small 'complexity' for it to be conveyed to the server with minimum communication cost.","In this paper, we propose a joint training and source coding scheme with provable in-expectation guarantees, where the expectation is over the encoder's output message.","Specifically, we show that by imposing a constraint on a suitable Kullback-Leibler divergence between the conditional distribution induced by a compressed learning model $\\widehat{W}$ given $W$ and the prior, one guarantees simultaneously small average empirical risk (aka training loss), small average generalization error and small average communication cost.","We also consider a one-shot scenario in which the guarantees on the empirical risk and generalization error are obtained for every encoder's output message."],"url":"http://arxiv.org/abs/2406.08193v1","category":"stat.ML"}
{"created":"2024-06-12 13:21:33","title":"2nd Place Solution for MOSE Track in CVPR 2024 PVUW workshop: Complex Video Object Segmentation","abstract":"Complex video object segmentation serves as a fundamental task for a wide range of downstream applications such as video editing and automatic data annotation. Here we present the 2nd place solution in the MOSE track of PVUW 2024. To mitigate problems caused by tiny objects, similar objects and fast movements in MOSE. We use instance segmentation to generate extra pretraining data from the valid and test set of MOSE. The segmented instances are combined with objects extracted from COCO to augment the training data and enhance semantic representation of the baseline model. Besides, motion blur is added during training to increase robustness against image blur induced by motion. Finally, we apply test time augmentation (TTA) and memory strategy to the inference stage. Our method ranked 2nd in the MOSE track of PVUW 2024, with a $\\mathcal{J}$ of 0.8007, a $\\mathcal{F}$ of 0.8683 and a $\\mathcal{J}$\\&$\\mathcal{F}$ of 0.8345.","sentences":["Complex video object segmentation serves as a fundamental task for a wide range of downstream applications such as video editing and automatic data annotation.","Here we present the 2nd place solution in the MOSE track of PVUW 2024.","To mitigate problems caused by tiny objects, similar objects and fast movements in MOSE.","We use instance segmentation to generate extra pretraining data from the valid and test set of MOSE.","The segmented instances are combined with objects extracted from COCO to augment the training data and enhance semantic representation of the baseline model.","Besides, motion blur is added during training to increase robustness against image blur induced by motion.","Finally, we apply test time augmentation (TTA) and memory strategy to the inference stage.","Our method ranked 2nd in the MOSE track of PVUW 2024, with a $\\mathcal{J}$ of 0.8007, a $\\mathcal{F}$ of 0.8683 and a $\\mathcal{J}$\\&$\\mathcal{F}$ of 0.8345."],"url":"http://arxiv.org/abs/2406.08192v1","category":"cs.CV"}
{"created":"2024-06-12 13:20:56","title":"CrowdEgress: A Multi-Agent Simulation Platform for Pedestrian Crowd","abstract":"This article introduces a simulation platform to study complex crowd behavior in social context. The agent-based model is extended based on the well-known social force model, and it mainly describes how agents interact with each other, and also with surrounding facilities such as walls, doors and exits. The simulation platform is compatible to FDS+Evac, and the input data in FDS+Evac could be imported into our simulation platform to create single-floor compartment geometry, and a flow solver is used to generate the roadmap towards exits. Most importantly, we plan to integrate advanced social and psychological theory into our simulation platform, especially investigating human behavior in emergency evacuation,such as pre-evacuation behavior, exit-selection activities, social group and herding effect and so forth.","sentences":["This article introduces a simulation platform to study complex crowd behavior in social context.","The agent-based model is extended based on the well-known social force model, and it mainly describes how agents interact with each other, and also with surrounding facilities such as walls, doors and exits.","The simulation platform is compatible to FDS+Evac, and the input data in FDS+Evac could be imported into our simulation platform to create single-floor compartment geometry, and a flow solver is used to generate the roadmap towards exits.","Most importantly, we plan to integrate advanced social and psychological theory into our simulation platform, especially investigating human behavior in emergency evacuation,such as pre-evacuation behavior, exit-selection activities, social group and herding effect and so forth."],"url":"http://arxiv.org/abs/2406.08190v1","category":"physics.soc-ph"}
{"created":"2024-06-12 13:18:58","title":"Learning-based Traversability Costmap for Autonomous Off-road Navigation","abstract":"Traversability estimation in off-road terrains is an essential procedure for autonomous navigation. However, creating reliable labels for complex interactions between the robot and the surface is still a challenging problem in learning-based costmap generation. To address this, we propose a method that predicts traversability costmaps by leveraging both visual and geometric information of the environment. To quantify the surface properties like roughness and bumpiness, we introduce a novel way of risk-aware labelling with proprioceptive information for network training. We validate our method in costmap prediction and navigation tasks for complex off-road scenarios. Our results demonstrate that our costmap prediction method excels in terms of average accuracy and MSE. The navigation results indicate that using our learned costmaps leads to safer and smoother driving, outperforming previous methods in terms of the highest success rate, lowest normalized trajectory length, lowest time cost, and highest mean stability across two scenarios.","sentences":["Traversability estimation in off-road terrains is an essential procedure for autonomous navigation.","However, creating reliable labels for complex interactions between the robot and the surface is still a challenging problem in learning-based costmap generation.","To address this, we propose a method that predicts traversability costmaps by leveraging both visual and geometric information of the environment.","To quantify the surface properties like roughness and bumpiness, we introduce a novel way of risk-aware labelling with proprioceptive information for network training.","We validate our method in costmap prediction and navigation tasks for complex off-road scenarios.","Our results demonstrate that our costmap prediction method excels in terms of average accuracy and MSE.","The navigation results indicate that using our learned costmaps leads to safer and smoother driving, outperforming previous methods in terms of the highest success rate, lowest normalized trajectory length, lowest time cost, and highest mean stability across two scenarios."],"url":"http://arxiv.org/abs/2406.08187v1","category":"cs.RO"}
{"created":"2024-06-12 13:14:50","title":"MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents","abstract":"Large language model (LLM)-based mobile agents are increasingly popular due to their capability to interact directly with mobile phone Graphic User Interfaces (GUIs) and their potential to autonomously manage daily tasks. Despite their promising prospects in both academic and industrial sectors, little research has focused on benchmarking the performance of existing mobile agents, due to the inexhaustible states of apps and the vague definition of feasible action sequences. To address this challenge, we propose an efficient and user-friendly benchmark, MobileAgentBench, designed to alleviate the burden of extensive manual testing. We initially define 100 tasks across 10 open-source apps, categorized by multiple levels of difficulty. Subsequently, we evaluate several existing mobile agents, including AppAgent and MobileAgent, to thoroughly and systematically compare their performance. All materials are accessible on our project webpage: https://MobileAgentBench.github.io, contributing to the advancement of both academic and industrial fields.","sentences":["Large language model (LLM)-based mobile agents are increasingly popular due to their capability to interact directly with mobile phone Graphic User Interfaces (GUIs) and their potential to autonomously manage daily tasks.","Despite their promising prospects in both academic and industrial sectors, little research has focused on benchmarking the performance of existing mobile agents, due to the inexhaustible states of apps and the vague definition of feasible action sequences.","To address this challenge, we propose an efficient and user-friendly benchmark, MobileAgentBench, designed to alleviate the burden of extensive manual testing.","We initially define 100 tasks across 10 open-source apps, categorized by multiple levels of difficulty.","Subsequently, we evaluate several existing mobile agents, including AppAgent and MobileAgent, to thoroughly and systematically compare their performance.","All materials are accessible on our project webpage: https://MobileAgentBench.github.io, contributing to the advancement of both academic and industrial fields."],"url":"http://arxiv.org/abs/2406.08184v1","category":"cs.AI"}
{"created":"2024-06-12 13:11:47","title":"Linear stability theory and molecular simulations of nanofilm dewetting with disjoining pressure, strong liquid-solid slip, and thermal fluctuations","abstract":"The dewetting of thin nanofilms is significantly impacted by thermal fluctuations, liquid-solid slip, and disjoining pressure, which can be described by lubrication equations augmented by appropriately scaled noise terms, known as stochastic lubrication equations. Here molecular dynamics simulations along with a newly proposed slip-generating method are adopted to study the instability of nanofilms with arbitrary slip. These simulations show that strong-slip dewetting is distinct from weak-slip dewetting by faster growth of perturbations and fewer droplets after dewetting, which can not be predicted by the existing stochastic lubrication equation. A new stochastic lubrication equation considering the strong slip boundary condition is thus derived using a long-wave approximation to the equations of fluctuating hydrodynamics. The linear stability analysis of this equation, i.e., surface spectrum, agrees well with molecular simulations. Interestingly, strong slip can break down the usual Stokes limits adopted in weak-slip dewetting and bring the inertia into effect. The evolution of the standard deviation of the film height $W^2(t)={\\overline{h^2}-{\\overline{h}}^2}$ at the initial stage of the strong-slip dewetting is found to be $W\\sim t^{1/4}$ in contrast to $W\\sim t^{1/8}$ for the weak-slip dewetting.","sentences":["The dewetting of thin nanofilms is significantly impacted by thermal fluctuations, liquid-solid slip, and disjoining pressure, which can be described by lubrication equations augmented by appropriately scaled noise terms, known as stochastic lubrication equations.","Here molecular dynamics simulations along with a newly proposed slip-generating method are adopted to study the instability of nanofilms with arbitrary slip.","These simulations show that strong-slip dewetting is distinct from weak-slip dewetting by faster growth of perturbations and fewer droplets after dewetting, which can not be predicted by the existing stochastic lubrication equation.","A new stochastic lubrication equation considering the strong slip boundary condition is thus derived using a long-wave approximation to the equations of fluctuating hydrodynamics.","The linear stability analysis of this equation, i.e., surface spectrum, agrees well with molecular simulations.","Interestingly, strong slip can break down the usual Stokes limits adopted in weak-slip dewetting and bring the inertia into effect.","The evolution of the standard deviation of the film height $W^2(t)={\\overline{h^2}-{\\overline{h}}^2}$ at the initial stage of the strong-slip dewetting is found to be $W\\sim t^{1/4}$ in contrast to $W\\sim t^{1/8}$ for the weak-slip dewetting."],"url":"http://arxiv.org/abs/2406.08179v1","category":"physics.flu-dyn"}
{"created":"2024-06-12 13:11:19","title":"Shape differentiation for Poincar\u00e9 maps of harmonic fields in toroidal domains","abstract":"In this article, we study Poincar\\'e maps of harmonic fields in toroidal domains using a shape variational approach. Given a bounded domain of $\\mathbb{R}^3$, we define its harmonic fields as the set of magnetic fields which are curl free and tangent to the boundary. For toroidal domains, this space is one dimensional, and one may thus single out a harmonic field by specifying a degree of freedom, such as the circulation along a toroidal loop. We are then interested in the Poincar\\'e maps of such fields restricted to the boundary, which produce diffeomorphisms of the circle. We begin by proving a general shape differentiability result of such Poincar\\'e maps in the smooth category, and obtain a general formula for the shape derivative. We then investigate two specific examples of interest; axisymmetric domains, and domains for which the harmonic field has a diophantine rotation number on the boundary. We prove that, in the first case, the shape derivative of the Poincar\\'e map is always identically zero, whereas in the second case, assuming an additional condition on the geometry of the domain, the shape derivative of the Poincar\\'e map may be any smooth function of the circle by choosing an appropriate perturbation of the domain.","sentences":["In this article, we study Poincar\\'e maps of harmonic fields in toroidal domains using a shape variational approach.","Given a bounded domain of $\\mathbb{R}^3$, we define its harmonic fields as the set of magnetic fields which are curl free and tangent to the boundary.","For toroidal domains, this space is one dimensional, and one may thus single out a harmonic field by specifying a degree of freedom, such as the circulation along a toroidal loop.","We are then interested in the Poincar\\'e maps of such fields restricted to the boundary, which produce diffeomorphisms of the circle.","We begin by proving a general shape differentiability result of such Poincar\\'e maps in the smooth category, and obtain a general formula for the shape derivative.","We then investigate two specific examples of interest; axisymmetric domains, and domains for which the harmonic field has a diophantine rotation number on the boundary.","We prove that, in the first case, the shape derivative of the Poincar\\'e map is always identically zero, whereas in the second case, assuming an additional condition on the geometry of the domain, the shape derivative of the Poincar\\'e map may be any smooth function of the circle by choosing an appropriate perturbation of the domain."],"url":"http://arxiv.org/abs/2406.08178v1","category":"math.OC"}
{"created":"2024-06-12 13:10:31","title":"One-Step Effective Diffusion Network for Real-World Image Super-Resolution","abstract":"The pre-trained text-to-image diffusion models have been increasingly employed to tackle the real-world image super-resolution (Real-ISR) problem due to their powerful generative image priors. Most of the existing methods start from random noise to reconstruct the high-quality (HQ) image under the guidance of the given low-quality (LQ) image. While promising results have been achieved, such Real- ISR methods require multiple diffusion steps to reproduce the HQ image, increasing the computational cost. Meanwhile, the random noise introduces uncertainty in the output, which is unfriendly to image restoration tasks. To address these issues, we propose a one-step effective diffusion network, namely OSEDiff, for the Real- ISR problem. We argue that the LQ image contains rich information to restore its HQ counterpart, and hence the given LQ image can be directly taken as the starting point for diffusion, eliminating the uncertainty introduced by random noise sampling. We finetune the pre-trained diffusion network with trainable layers to adapt it to complex image degradations. To ensure that the one-step diffusion model could yield HQ Real-ISR output, we apply variational score distillation in the latent space to conduct KL-divergence regularization. As a result, our OSEDiff model can efficiently and effectively generate HQ images in just one diffusion step. Our experiments demonstrate that OSEDiff achieves comparable or even better Real-ISR results, in terms of both objective metrics and subjective evaluations, than previous diffusion model based Real-ISR methods that require dozens or hundreds of steps. The source codes will be released at https://github.com/cswry/OSEDiff.","sentences":["The pre-trained text-to-image diffusion models have been increasingly employed to tackle the real-world image super-resolution (Real-ISR) problem due to their powerful generative image priors.","Most of the existing methods start from random noise to reconstruct the high-quality (HQ) image under the guidance of the given low-quality (LQ) image.","While promising results have been achieved, such Real- ISR methods require multiple diffusion steps to reproduce the HQ image, increasing the computational cost.","Meanwhile, the random noise introduces uncertainty in the output, which is unfriendly to image restoration tasks.","To address these issues, we propose a one-step effective diffusion network, namely OSEDiff, for the Real- ISR problem.","We argue that the LQ image contains rich information to restore its HQ counterpart, and hence the given LQ image can be directly taken as the starting point for diffusion, eliminating the uncertainty introduced by random noise sampling.","We finetune the pre-trained diffusion network with trainable layers to adapt it to complex image degradations.","To ensure that the one-step diffusion model could yield HQ Real-ISR output, we apply variational score distillation in the latent space to conduct KL-divergence regularization.","As a result, our OSEDiff model can efficiently and effectively generate HQ images in just one diffusion step.","Our experiments demonstrate that OSEDiff achieves comparable or even better Real-ISR results, in terms of both objective metrics and subjective evaluations, than previous diffusion model based Real-ISR methods that require dozens or hundreds of steps.","The source codes will be released at https://github.com/cswry/OSEDiff."],"url":"http://arxiv.org/abs/2406.08177v1","category":"eess.IV"}
{"created":"2024-06-12 13:08:28","title":"Certificates and Witnesses for Multi-Objective Queries in Markov Decision Processes","abstract":"Certifying verification algorithms not only return whether a given property holds or not, but also provide an accompanying independently checkable certificate and a corresponding witness. The certificate can be used to easily validate the correctness of the result and the witness provides useful diagnostic information, e.g. for debugging purposes. Thus, certificates and witnesses substantially increase the trustworthiness and understandability of the verification process. In this work, we consider certificates and witnesses for multi-objective reachability-invariant and mean-payoff queries in Markov decision processes, that is conjunctions or disjunctions either of reachability and invariant or mean-payoff predicates, both universally and existentially quantified. Thereby, we generalize previous works on certificates and witnesses for single reachability and invariant constraints. To this end, we turn known linear programming techniques into certifying algorithms and show that witnesses in the form of schedulers and subsystems can be obtained. As a proof-of-concept, we report on implementations of certifying verification algorithms and experimental results.","sentences":["Certifying verification algorithms not only return whether a given property holds or not, but also provide an accompanying independently checkable certificate and a corresponding witness.","The certificate can be used to easily validate the correctness of the result and the witness provides useful diagnostic information, e.g. for debugging purposes.","Thus, certificates and witnesses substantially increase the trustworthiness and understandability of the verification process.","In this work, we consider certificates and witnesses for multi-objective reachability-invariant and mean-payoff queries in Markov decision processes, that is conjunctions or disjunctions either of reachability and invariant or mean-payoff predicates, both universally and existentially quantified.","Thereby, we generalize previous works on certificates and witnesses for single reachability and invariant constraints.","To this end, we turn known linear programming techniques into certifying algorithms and show that witnesses in the form of schedulers and subsystems can be obtained.","As a proof-of-concept, we report on implementations of certifying verification algorithms and experimental results."],"url":"http://arxiv.org/abs/2406.08175v1","category":"cs.LO"}
{"created":"2024-06-12 13:04:48","title":"inlamemi: An R package for missing data imputation and measurement error modelling using INLA","abstract":"Measurement error and missing data in variables used in statistical models are common, and can at worst lead to serious biases in analyses if they are ignored. Yet, these problems are often not dealt with adequately, presumably in part because analysts lack simple enough tools to account for error and missingness. In this R package, we provide functions to aid fitting hierarchical Bayesian models that account for cases where either measurement error (classical or Berkson), missing data, or both are present in continuous covariates. Model fitting is done in a Bayesian framework using integrated nested Laplace approximations (INLA), an approach that is growing in popularity due to its combination of computational speed and accuracy. The {inlamemi} R package is suitable for data analysts who have little prior experience using the R package {R-INLA}, and aids in formulating suitable hierarchical models for a variety of scenarios in order to appropriately capture the processes that generate the measurement error and/or missingness. Numerous examples are given to help analysts identify scenarios similar to their own, and make the process of specifying a suitable model easier.","sentences":["Measurement error and missing data in variables used in statistical models are common, and can at worst lead to serious biases in analyses if they are ignored.","Yet, these problems are often not dealt with adequately, presumably in part because analysts lack simple enough tools to account for error and missingness.","In this R package, we provide functions to aid fitting hierarchical Bayesian models that account for cases where either measurement error (classical or Berkson), missing data, or both are present in continuous covariates.","Model fitting is done in a Bayesian framework using integrated nested Laplace approximations (INLA), an approach that is growing in popularity due to its combination of computational speed and accuracy.","The {inlamemi} R package is suitable for data analysts who have little prior experience using the R package {R-INLA}, and aids in formulating suitable hierarchical models for a variety of scenarios in order to appropriately capture the processes that generate the measurement error and/or missingness.","Numerous examples are given to help analysts identify scenarios similar to their own, and make the process of specifying a suitable model easier."],"url":"http://arxiv.org/abs/2406.08172v1","category":"stat.ME"}
{"created":"2024-06-12 13:04:06","title":"Continuous fake media detection: adapting deepfake detectors to new generative techniques","abstract":"Generative techniques continue to evolve at an impressively high rate, driven by the hype about these technologies. This rapid advancement severely limits the application of deepfake detectors, which, despite numerous efforts by the scientific community, struggle to achieve sufficiently robust performance against the ever-changing content. To address these limitations, in this paper, we propose an analysis of two continuous learning techniques on a Short and a Long sequence of fake media. Both sequences include a complex and heterogeneous range of deepfakes generated from GANs, computer graphics techniques, and unknown sources. Our study shows that continual learning could be important in mitigating the need for generalizability. In fact, we show that, although with some limitations, continual learning methods help to maintain good performance across the entire training sequence. For these techniques to work in a sufficiently robust way, however, it is necessary that the tasks in the sequence share similarities. In fact, according to our experiments, the order and similarity of the tasks can affect the performance of the models over time. To address this problem, we show that it is possible to group tasks based on their similarity. This small measure allows for a significant improvement even in longer sequences. This result suggests that continual techniques can be combined with the most promising detection methods, allowing them to catch up with the latest generative techniques. In addition to this, we propose an overview of how this learning approach can be integrated into a deepfake detection pipeline for continuous integration and continuous deployment (CI/CD). This allows you to keep track of different funds, such as social networks, new generative tools, or third-party datasets, and through the integration of continuous learning, allows constant maintenance of the detectors.","sentences":["Generative techniques continue to evolve at an impressively high rate, driven by the hype about these technologies.","This rapid advancement severely limits the application of deepfake detectors, which, despite numerous efforts by the scientific community, struggle to achieve sufficiently robust performance against the ever-changing content.","To address these limitations, in this paper, we propose an analysis of two continuous learning techniques on a Short and a Long sequence of fake media.","Both sequences include a complex and heterogeneous range of deepfakes generated from GANs, computer graphics techniques, and unknown sources.","Our study shows that continual learning could be important in mitigating the need for generalizability.","In fact, we show that, although with some limitations, continual learning methods help to maintain good performance across the entire training sequence.","For these techniques to work in a sufficiently robust way, however, it is necessary that the tasks in the sequence share similarities.","In fact, according to our experiments, the order and similarity of the tasks can affect the performance of the models over time.","To address this problem, we show that it is possible to group tasks based on their similarity.","This small measure allows for a significant improvement even in longer sequences.","This result suggests that continual techniques can be combined with the most promising detection methods, allowing them to catch up with the latest generative techniques.","In addition to this, we propose an overview of how this learning approach can be integrated into a deepfake detection pipeline for continuous integration and continuous deployment (CI/CD).","This allows you to keep track of different funds, such as social networks, new generative tools, or third-party datasets, and through the integration of continuous learning, allows constant maintenance of the detectors."],"url":"http://arxiv.org/abs/2406.08171v1","category":"cs.CV"}
{"created":"2024-06-12 13:03:38","title":"Can AI Understand Human Personality? -- Comparing Human Experts and AI Systems at Predicting Personality Correlations","abstract":"We test the abilities of specialised deep neural networks like PersonalityMap as well as general LLMs like GPT-4o and Claude 3 Opus in understanding human personality. Specifically, we compare their ability to predict correlations between personality items to the abilities of lay people and academic experts. We find that when compared with individual humans, all AI models make better predictions than the vast majority of lay people and academic experts. However, when selecting the median prediction for each item, we find a different pattern: Experts and PersonalityMap outperform LLMs and lay people on most measures. Our results suggest that while frontier LLMs' are better than most individual humans at predicting correlations between personality items, specialised models like PersonalityMap continue to match or exceed expert human performance even on some outcome measures where LLMs underperform. This provides evidence both in favour of the general capabilities of large language models and in favour of the continued place for specialised models trained and deployed for specific domains.","sentences":["We test the abilities of specialised deep neural networks like PersonalityMap as well as general LLMs like GPT-4o and Claude 3 Opus in understanding human personality.","Specifically, we compare their ability to predict correlations between personality items to the abilities of lay people and academic experts.","We find that when compared with individual humans, all AI models make better predictions than the vast majority of lay people and academic experts.","However, when selecting the median prediction for each item, we find a different pattern: Experts and PersonalityMap outperform LLMs and lay people on most measures.","Our results suggest that while frontier LLMs' are better than most individual humans at predicting correlations between personality items, specialised models like PersonalityMap continue to match or exceed expert human performance even on some outcome measures where LLMs underperform.","This provides evidence both in favour of the general capabilities of large language models and in favour of the continued place for specialised models trained and deployed for specific domains."],"url":"http://arxiv.org/abs/2406.08170v1","category":"cs.CY"}
{"created":"2024-06-12 12:58:43","title":"Feedback-Based Quantum Algorithm for Constrained Optimization Problems","abstract":"The feedback-based algorithm for quantum optimization \\\\(FALQON) has recently been proposed to solve quadratic unconstrained binary optimization problems. This paper efficiently generalizes FALQON to tackle quadratic constrained binary optimization (QCBO) problems. For this purpose, we introduce a new operator that encodes the problem's solution as its ground state. Using Lyapunov control theory, we design a quantum control system such that the state converges to the ground state of this operator. When applied to the QCBO problem, we show that our proposed algorithm saves computational resources by reducing the depth of the quantum circuit and can perform better than FALQON. The effectiveness of our proposed algorithm is further illustrated through numerical simulations.","sentences":["The feedback-based algorithm for quantum optimization \\\\(FALQON) has recently been proposed to solve quadratic unconstrained binary optimization problems.","This paper efficiently generalizes FALQON to tackle quadratic constrained binary optimization (QCBO) problems.","For this purpose, we introduce a new operator that encodes the problem's solution as its ground state.","Using Lyapunov control theory, we design a quantum control system such that the state converges to the ground state of this operator.","When applied to the QCBO problem, we show that our proposed algorithm saves computational resources by reducing the depth of the quantum circuit and can perform better than FALQON.","The effectiveness of our proposed algorithm is further illustrated through numerical simulations."],"url":"http://arxiv.org/abs/2406.08169v1","category":"quant-ph"}
{"created":"2024-06-12 12:54:27","title":"ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs","abstract":"Compositional Reasoning (CR) entails grasping the significance of attributes, relations, and word order. Recent Vision-Language Models (VLMs), comprising a visual encoder and a Large Language Model (LLM) decoder, have demonstrated remarkable proficiency in such reasoning tasks. This prompts a crucial question: have VLMs effectively tackled the CR challenge? We conjecture that existing CR benchmarks may not adequately push the boundaries of modern VLMs due to the reliance on an LLM-only negative text generation pipeline. Consequently, the negatives produced either appear as outliers from the natural language distribution learned by VLMs' LLM decoders or as improbable within the corresponding image context. To address these limitations, we introduce ConMe -- a compositional reasoning benchmark and a novel data generation pipeline leveraging VLMs to produce `hard CR Q&A'. Through a new concept of VLMs conversing with each other to collaboratively expose their weaknesses, our pipeline autonomously generates, evaluates, and selects challenging compositional reasoning questions, establishing a robust CR benchmark, also subsequently validated manually. Our benchmark provokes a noteworthy, up to 33%, decrease in CR performance compared to preceding benchmarks, reinstating the CR challenge even for state-of-the-art VLMs.","sentences":["Compositional Reasoning (CR) entails grasping the significance of attributes, relations, and word order.","Recent Vision-Language Models (VLMs), comprising a visual encoder and a Large Language Model (LLM) decoder, have demonstrated remarkable proficiency in such reasoning tasks.","This prompts a crucial question: have VLMs effectively tackled the CR challenge?","We conjecture that existing CR benchmarks may not adequately push the boundaries of modern VLMs due to the reliance on an LLM-only negative text generation pipeline.","Consequently, the negatives produced either appear as outliers from the natural language distribution learned by VLMs' LLM decoders or as improbable within the corresponding image context.","To address these limitations, we introduce ConMe -- a compositional reasoning benchmark and a novel data generation pipeline leveraging VLMs to produce `hard CR Q&A'.","Through a new concept of VLMs conversing with each other to collaboratively expose their weaknesses, our pipeline autonomously generates, evaluates, and selects challenging compositional reasoning questions, establishing a robust CR benchmark, also subsequently validated manually.","Our benchmark provokes a noteworthy, up to 33%, decrease in CR performance compared to preceding benchmarks, reinstating the CR challenge even for state-of-the-art VLMs."],"url":"http://arxiv.org/abs/2406.08164v1","category":"cs.CV"}
{"created":"2024-06-12 12:51:49","title":"Non-existence of low rank Ulrich bundles on Veronese varieties","abstract":"We show that Veronese varieties of dimension $n \\ge 4$ do not carry any Ulrich bundles of rank $r \\le 3$. In order to prove this, we prove that a Veronese embedding of a complete intersection of dimension $m \\ge 4$, which if $m=4$ is either $\\mathbb P^4$ or has degree $d \\ge 2$ and is very general and not of type $(2), (2,2)$, does not carry any Ulrich bundles of rank $r \\le 3$.","sentences":["We show that Veronese varieties of dimension $n \\ge 4$ do not carry any Ulrich bundles of rank $r \\le 3$.","In order to prove this, we prove that a Veronese embedding of a complete intersection of dimension $m \\ge 4$, which if $m=4$ is either $\\mathbb P^4$ or has degree $d \\ge 2$ and is very general and not of type $(2), (2,2)$, does not carry any Ulrich bundles of rank $r \\le 3$."],"url":"http://arxiv.org/abs/2406.08162v1","category":"math.AG"}
{"created":"2024-06-12 12:50:33","title":"Quadratic perturbations of the Schwarzschild black hole: The algebraically special sector","abstract":"We investigate quadratic algebraically special perturbations (ASPs) of the Schwarzschild black hole. Their dynamics are derived from the expansion up to second order in perturbation of the most general algebraically special twisting vacuum solution of general relativity. Following this strategy, we present analytical expressions for the axial-axial, polar-polar and polar-axial source terms entering in the dynamical equations. We show that these complicated inhomogeneous equations can be solved analytically and we present explicit expressions for the profiles of the quadratic ASPs. As expected, they exhibit exponential growth both at the past and future horizons even in the non-linear regime. We further use this result to analyze the quadratic zero modes and their interpretation in terms of quadratic corrections to mass and spin of the Schwarzschild black hole. The present work provides a direct extension beyond the linear regime of the original work by Couch and Newman.","sentences":["We investigate quadratic algebraically special perturbations (ASPs) of the Schwarzschild black hole.","Their dynamics are derived from the expansion up to second order in perturbation of the most general algebraically special twisting vacuum solution of general relativity.","Following this strategy, we present analytical expressions for the axial-axial, polar-polar and polar-axial source terms entering in the dynamical equations.","We show that these complicated inhomogeneous equations can be solved analytically and we present explicit expressions for the profiles of the quadratic ASPs.","As expected, they exhibit exponential growth both at the past and future horizons even in the non-linear regime.","We further use this result to analyze the quadratic zero modes and their interpretation in terms of quadratic corrections to mass and spin of the Schwarzschild black hole.","The present work provides a direct extension beyond the linear regime of the original work by Couch and Newman."],"url":"http://arxiv.org/abs/2406.08159v1","category":"gr-qc"}
{"created":"2024-06-12 12:44:48","title":"Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark","abstract":"Large Language Models~(LLMs) have become foundational in the realm of natural language processing, demonstrating performance improvements as model sizes increase. The Mixture-of-Experts~(MoE) approach offers a promising way to scale LLMs more efficiently by using fewer computational FLOPs through sparse activation. However, it suffers from significant memory overheads, necessitating model compression techniques. Post-training quantization, a popular method for model compression, proves less effective when directly applied to MoE models due to MoE's overlooked inherent sparsity. This paper explores several MoE structure-aware quantization heuristics, ranging from coarse to fine granularity, from MoE block to individual linear weight. Our investigations reveal critical principles: different MoE structures (i.e., blocks, experts, linear layers) require varying numbers of weight bits for effective and efficient quantization. Conclusions are supported by extensive benchmarking across two representative MoE models and six tasks. We further introduce novel enhancements to more accurately identify the most critical weights in MoE quantization that necessitate higher bit allocations, including the linear weight outlier scorer and MoE block scorer. Additionally, subsequent experiments validate our findings in the context of both weight and activation quantization.","sentences":["Large Language Models~(LLMs) have become foundational in the realm of natural language processing, demonstrating performance improvements as model sizes increase.","The Mixture-of-Experts~(MoE) approach offers a promising way to scale LLMs more efficiently by using fewer computational FLOPs through sparse activation.","However, it suffers from significant memory overheads, necessitating model compression techniques.","Post-training quantization, a popular method for model compression, proves less effective when directly applied to MoE models due to MoE's overlooked inherent sparsity.","This paper explores several MoE structure-aware quantization heuristics, ranging from coarse to fine granularity, from MoE block to individual linear weight.","Our investigations reveal critical principles: different MoE structures (i.e., blocks, experts, linear layers) require varying numbers of weight bits for effective and efficient quantization.","Conclusions are supported by extensive benchmarking across two representative MoE models and six tasks.","We further introduce novel enhancements to more accurately identify the most critical weights in MoE quantization that necessitate higher bit allocations, including the linear weight outlier scorer and MoE block scorer.","Additionally, subsequent experiments validate our findings in the context of both weight and activation quantization."],"url":"http://arxiv.org/abs/2406.08155v1","category":"cs.LG"}
{"created":"2024-06-12 12:39:57","title":"Distribution spaces associated with elliptic operators","abstract":"We study complex distribution spaces given over a bounded Lipschitz domain $\\Omega$ and associated with an elliptic differential operator $A$ with $C^{\\infty}$-coefficients on $\\overline{\\Omega}$. If $X$ and $Y$ are quasi-Banach distribution spaces over $\\Omega$, then the space $X(A,Y)$ under study consists of all distributions $u\\in X$ such that $Au\\in Y$ and is endowed with the graph quasi-norm. Assuming $X$ to be an arbitrary Besov space or Triebel--Lizorkin space over $\\Omega$, we find sufficient conditions for $Y$ under which the interpolation between the spaces $X(A,Y)$ preserves their structure, these spaces are separable, and the set $C^{\\infty}(\\overline{\\Omega})$ is dense in them. We then explicitly describe the spaces obtained by the real, complex, and $\\pm$ interpolation between the spaces under study. We apply these spaces to general elliptic problems with rough boundary data by proving the Fredholm property for bounded operators induced by these problems and defined on certain spaces $X(A,Y)$. Specifically, we establish the maximal regularity of solutions to some elliptic problems with Gaussian white noise in boundary conditions. Quasi-Banach distribution spaces are involved in the concept of $X(A,Y)$ for the first time. Our results are new even for inner product Sobolev spaces of integer-valued order.","sentences":["We study complex distribution spaces given over a bounded Lipschitz domain $\\Omega$ and associated with an elliptic differential operator $A$ with $C^{\\infty}$-coefficients on $\\overline{\\Omega}$. If $X$ and $Y$ are quasi-Banach distribution spaces over $\\Omega$, then the space $X(A,Y)$ under study consists of all distributions $u\\in X$ such that $Au\\in Y$ and is endowed with the graph quasi-norm.","Assuming $X$ to be an arbitrary Besov space or Triebel--Lizorkin space over $\\Omega$, we find sufficient conditions for $Y$ under which the interpolation between the spaces $X(A,Y)$ preserves their structure, these spaces are separable, and the set $C^{\\infty}(\\overline{\\Omega})$ is dense in them.","We then explicitly describe the spaces obtained by the real, complex, and $\\pm$ interpolation between the spaces under study.","We apply these spaces to general elliptic problems with rough boundary data by proving the Fredholm property for bounded operators induced by these problems and defined on certain spaces $X(A,Y)$. Specifically, we establish the maximal regularity of solutions to some elliptic problems with Gaussian white noise in boundary conditions.","Quasi-Banach distribution spaces are involved in the concept of $X(A,Y)$ for the first time.","Our results are new even for inner product Sobolev spaces of integer-valued order."],"url":"http://arxiv.org/abs/2406.08150v1","category":"math.FA"}
{"created":"2024-06-12 12:37:53","title":"Probing Implicit Bias in Semi-gradient Q-learning: Visualizing the Effective Loss Landscapes via the Fokker--Planck Equation","abstract":"Semi-gradient Q-learning is applied in many fields, but due to the absence of an explicit loss function, studying its dynamics and implicit bias in the parameter space is challenging. This paper introduces the Fokker--Planck equation and employs partial data obtained through sampling to construct and visualize the effective loss landscape within a two-dimensional parameter space. This visualization reveals how the global minima in the loss landscape can transform into saddle points in the effective loss landscape, as well as the implicit bias of the semi-gradient method. Additionally, we demonstrate that saddle points, originating from the global minima in loss landscape, still exist in the effective loss landscape under high-dimensional parameter spaces and neural network settings. This paper develop a novel approach for probing implicit bias in semi-gradient Q-learning.","sentences":["Semi-gradient Q-learning is applied in many fields, but due to the absence of an explicit loss function, studying its dynamics and implicit bias in the parameter space is challenging.","This paper introduces the Fokker--Planck equation and employs partial data obtained through sampling to construct and visualize the effective loss landscape within a two-dimensional parameter space.","This visualization reveals how the global minima in the loss landscape can transform into saddle points in the effective loss landscape, as well as the implicit bias of the semi-gradient method.","Additionally, we demonstrate that saddle points, originating from the global minima in loss landscape, still exist in the effective loss landscape under high-dimensional parameter spaces and neural network settings.","This paper develop a novel approach for probing implicit bias in semi-gradient Q-learning."],"url":"http://arxiv.org/abs/2406.08148v1","category":"cs.LG"}
{"created":"2024-06-12 12:34:13","title":"A computational materials science paradigm for a Course-based Undergraduate Research Experience (CURE)","abstract":"Course-based Undergraduate Research Experiences (CUREs) bring the excitement of research into the classroom to improve learning and the sense of belonging in the field. They can reach more students, earlier in their studies, than typical undergraduate research. Key aspects are: students learn and use research methods, give input into the project, generate new research data, and analyze it to draw conclusions that are not known beforehand. CUREs are common in other fields but have been rare in materials science and engineering. I propose a paradigm for computational material science CUREs, enabled by web-based simulation tools from nanoHUB.org that require minimal computational skills. After preparatory exercises, students each calculate part of a set of closely related materials, following a defined protocol to contribute to a novel class dataset which they analyze, and also calculate an additional property of their choice. This approach has been used successfully in several class projects.","sentences":["Course-based Undergraduate Research Experiences (CUREs) bring the excitement of research into the classroom to improve learning and the sense of belonging in the field.","They can reach more students, earlier in their studies, than typical undergraduate research.","Key aspects are: students learn and use research methods, give input into the project, generate new research data, and analyze it to draw conclusions that are not known beforehand.","CUREs are common in other fields but have been rare in materials science and engineering.","I propose a paradigm for computational material science CUREs, enabled by web-based simulation tools from nanoHUB.org that require minimal computational skills.","After preparatory exercises, students each calculate part of a set of closely related materials, following a defined protocol to contribute to a novel class dataset which they analyze, and also calculate an additional property of their choice.","This approach has been used successfully in several class projects."],"url":"http://arxiv.org/abs/2406.08142v1","category":"physics.ed-ph"}
{"created":"2024-06-12 12:25:04","title":"Making AI Intelligible: Philosophical Foundations","abstract":"Can humans and artificial intelligences share concepts and communicate? 'Making AI Intelligible' shows that philosophical work on the metaphysics of meaning can help answer these questions. Herman Cappelen and Josh Dever use the externalist tradition in philosophy to create models of how AIs and humans can understand each other. In doing so, they illustrate ways in which that philosophical tradition can be improved.   The questions addressed in the book are not only theoretically interesting, but the answers have pressing practical implications. Many important decisions about human life are now influenced by AI. In giving that power to AI, we presuppose that AIs can track features of the world that we care about (for example, creditworthiness, recidivism, cancer, and combatants). If AIs can share our concepts, that will go some way towards justifying this reliance on AI. This ground-breaking study offers insight into how to take some first steps towards achieving Interpretable AI.","sentences":["Can humans and artificial intelligences share concepts and communicate?","'Making AI Intelligible' shows that philosophical work on the metaphysics of meaning can help answer these questions.","Herman Cappelen and Josh Dever use the externalist tradition in philosophy to create models of how AIs and humans can understand each other.","In doing so, they illustrate ways in which that philosophical tradition can be improved.   ","The questions addressed in the book are not only theoretically interesting, but the answers have pressing practical implications.","Many important decisions about human life are now influenced by AI.","In giving that power to AI, we presuppose that AIs can track features of the world that we care about (for example, creditworthiness, recidivism, cancer, and combatants).","If AIs can share our concepts, that will go some way towards justifying this reliance on AI.","This ground-breaking study offers insight into how to take some first steps towards achieving Interpretable AI."],"url":"http://arxiv.org/abs/2406.08134v1","category":"cs.AI"}
{"created":"2024-06-12 12:22:18","title":"Relative nonhomogeneous Koszul duality for PROPs associated to nonaugmented operads","abstract":"The purpose of this paper is to show how Positselski's relative nonhomogeneous Koszul duality theory applies when studying the linear category underlying the PROP associated to a (non-augmented) operad of a certain form, in particular assuming that the reduced part of the operad is binary quadratic. In this case, the linear category has both a left augmentation and a right augmentation (corresponding to different units), using Positselski's terminology.   The general theory provides two associated linear differential graded (DG) categories; indeed, in this framework, one can work entirely within the DG realm, as opposed to the curved setting required for Positselski's general theory. Moreover, DG modules over DG categories are related by adjunctions.   When the reduced part of the operad is Koszul (working over a field of characteristic zero), the relative Koszul duality theory shows that there is a Koszul-type equivalence between the appropriate homotopy categories of DG modules. This gives a form of Koszul duality relationship between the above DG categories.   This is illustrated by the case of the operad encoding unital, commutative associative algebras, extending the classical Koszul duality between commutative associative algebras and Lie algebras. In this case, the associated linear category is the linearization of the category of finite sets and all maps. The relative nonhomogeneous Koszul duality theory relates its derived category to the respective homotopy categories of modules over two explicit linear DG categories.","sentences":["The purpose of this paper is to show how Positselski's relative nonhomogeneous Koszul duality theory applies when studying the linear category underlying the PROP associated to a (non-augmented) operad of a certain form, in particular assuming that the reduced part of the operad is binary quadratic.","In this case, the linear category has both a left augmentation and a right augmentation (corresponding to different units), using Positselski's terminology.   ","The general theory provides two associated linear differential graded (DG) categories; indeed, in this framework, one can work entirely within the DG realm, as opposed to the curved setting required for Positselski's general theory.","Moreover, DG modules over DG categories are related by adjunctions.   ","When the reduced part of the operad is Koszul (working over a field of characteristic zero), the relative Koszul duality theory shows that there is a Koszul-type equivalence between the appropriate homotopy categories of DG modules.","This gives a form of Koszul duality relationship between the above DG categories.   ","This is illustrated by the case of the operad encoding unital, commutative associative algebras, extending the classical Koszul duality between commutative associative algebras and Lie algebras.","In this case, the associated linear category is the linearization of the category of finite sets and all maps.","The relative nonhomogeneous Koszul duality theory relates its derived category to the respective homotopy categories of modules over two explicit linear DG categories."],"url":"http://arxiv.org/abs/2406.08132v1","category":"math.AT"}
{"created":"2024-06-12 12:09:36","title":"Coproduct Formula for Motivic Version of Yamamoto's Integral","abstract":"Goncharov proved an explicit formula for the coproduct in the Hopf algebra of motivic iterated integrals. Yamamoto introduced Yamamoto's integral which generalizes iterated integrals and gave a new integral expression for multiple zeta star values using Yamamoto's integral. In this paper, we consider the motivic version of Yamamoto's integral and generalize Goncharov's coproduct formula to those motivic integrals. As an example, we will compute the coproduct of a certain type of Schur multiple zeta values.","sentences":["Goncharov proved an explicit formula for the coproduct in the Hopf algebra of motivic iterated integrals.","Yamamoto introduced Yamamoto's integral which generalizes iterated integrals and gave a new integral expression for multiple zeta star values using Yamamoto's integral.","In this paper, we consider the motivic version of Yamamoto's integral and generalize Goncharov's coproduct formula to those motivic integrals.","As an example, we will compute the coproduct of a certain type of Schur multiple zeta values."],"url":"http://arxiv.org/abs/2406.08127v1","category":"math.NT"}
{"created":"2024-06-12 12:09:22","title":"Towards a unified description of isotopic fragment properties in spontaneous and fusion-induced fission within a 4D dynamical Langevin model","abstract":"Spontaneous fission of 252Cf and fusion-induced fission of 250Cf are investigated within a multi-dimensional Langevin model. The potential-energy surface is calculated in the macroscopic-microscopic LSD+Yukawa-folded approach using the four-dimensional Fourier-over-Spheroid shape parametrization. The dynamical evolution described by the Langevin equation is coupled to neutron evaporation, thereby allowing for the possibility of multi-chance fission. Charge equilibration and excitation-energy sharing between the fragments emerging at scission are evaluated, and their de-excitation is finally computed. The correlation between various observables, particularly the isotopic properties of the fragments, is discussed and compared with the experiment whenever available. The theoretical predictions are generally in good agreement with the data.","sentences":["Spontaneous fission of 252Cf and fusion-induced fission of 250Cf are investigated within a multi-dimensional Langevin model.","The potential-energy surface is calculated in the macroscopic-microscopic LSD+Yukawa-folded approach using the four-dimensional Fourier-over-Spheroid shape parametrization.","The dynamical evolution described by the Langevin equation is coupled to neutron evaporation, thereby allowing for the possibility of multi-chance fission.","Charge equilibration and excitation-energy sharing between the fragments emerging at scission are evaluated, and their de-excitation is finally computed.","The correlation between various observables, particularly the isotopic properties of the fragments, is discussed and compared with the experiment whenever available.","The theoretical predictions are generally in good agreement with the data."],"url":"http://arxiv.org/abs/2406.08126v1","category":"nucl-th"}
{"created":"2024-06-12 12:07:44","title":"Discrete Single-Parameter Optimal Auction Design","abstract":"We study the classic single-item auction setting of Myerson, but under the assumption that the buyers' values for the item are distributed over finite supports. Using strong LP duality and polyhedral theory, we rederive various key results regarding the revenue-maximizing auction, including the characterization through virtual welfare maximization and the optimality of deterministic mechanisms, as well as a novel, generic equivalence between dominant-strategy and Bayesian incentive compatibility.   Inspired by this, we abstract our approach to handle more general auction settings, where the feasibility space can be given by arbitrary convex constraints, and the objective is a convex combination of revenue and social welfare. We characterize the optimal auctions of such systems as generalized virtual welfare maximizers, by making use of their KKT conditions, and we present an analogue of Myerson's payment formula for general discrete single-parameter auction settings. Additionally, we prove that total unimodularity of the feasibility space is a sufficient condition to guarantee the optimality of auctions with integral allocation rules.   Finally, we demonstrate this KKT approach by applying it to a setting where bidders are interested in buying feasible flows on trees with capacity constraints, and provide a combinatorial description of the (randomized, in general) optimal auction.","sentences":["We study the classic single-item auction setting of Myerson, but under the assumption that the buyers' values for the item are distributed over finite supports.","Using strong LP duality and polyhedral theory, we rederive various key results regarding the revenue-maximizing auction, including the characterization through virtual welfare maximization and the optimality of deterministic mechanisms, as well as a novel, generic equivalence between dominant-strategy and Bayesian incentive compatibility.   ","Inspired by this, we abstract our approach to handle more general auction settings, where the feasibility space can be given by arbitrary convex constraints, and the objective is a convex combination of revenue and social welfare.","We characterize the optimal auctions of such systems as generalized virtual welfare maximizers, by making use of their KKT conditions, and we present an analogue of Myerson's payment formula for general discrete single-parameter auction settings.","Additionally, we prove that total unimodularity of the feasibility space is a sufficient condition to guarantee the optimality of auctions with integral allocation rules.   ","Finally, we demonstrate this KKT approach by applying it to a setting where bidders are interested in buying feasible flows on trees with capacity constraints, and provide a combinatorial description of the (randomized, in general) optimal auction."],"url":"http://arxiv.org/abs/2406.08125v1","category":"cs.GT"}
{"created":"2024-06-12 12:06:32","title":"Legend: Leveraging Representation Engineering to Annotate Safety Margin for Preference Datasets","abstract":"The success of the reward model in distinguishing between responses with subtle safety differences depends critically on the high-quality preference dataset, which should capture the fine-grained nuances of harmful and harmless responses. This motivates the need to develop a dataset involving preference margins, which accurately quantify how harmless one response is compared to another. In this paper, we take the first step to propose an effective and cost-efficient framework to promote the margin-enhanced preference dataset development. Our framework, Legend, Leverages representation engineering to annotate preference datasets. It constructs the specific direction within the LLM's embedding space that represents safety. By leveraging this safety direction, Legend can then leverage the semantic distances of paired responses along this direction to annotate margins automatically. We experimentally demonstrate our effectiveness in both reward modeling and harmless alignment for LLMs. Legend also stands out for its efficiency, requiring only the inference time rather than additional training. This efficiency allows for easier implementation and scalability, making Legend particularly valuable for practical applications in aligning LLMs with safe conversations.","sentences":["The success of the reward model in distinguishing between responses with subtle safety differences depends critically on the high-quality preference dataset, which should capture the fine-grained nuances of harmful and harmless responses.","This motivates the need to develop a dataset involving preference margins, which accurately quantify how harmless one response is compared to another.","In this paper, we take the first step to propose an effective and cost-efficient framework to promote the margin-enhanced preference dataset development.","Our framework, Legend, Leverages representation engineering to annotate preference datasets.","It constructs the specific direction within the LLM's embedding space that represents safety.","By leveraging this safety direction, Legend can then leverage the semantic distances of paired responses along this direction to annotate margins automatically.","We experimentally demonstrate our effectiveness in both reward modeling and harmless alignment for LLMs.","Legend also stands out for its efficiency, requiring only the inference time rather than additional training.","This efficiency allows for easier implementation and scalability, making Legend particularly valuable for practical applications in aligning LLMs with safe conversations."],"url":"http://arxiv.org/abs/2406.08124v1","category":"cs.CL"}
{"created":"2024-06-12 12:04:18","title":"Defect-related Anomalous Mobility of Small polarons in Oxides: the Case of Congruent Lithium Niobate","abstract":"Polarons play a major role in the description of optical, electrical and dielectrical properties of several ferroelectric oxides. The motion of those particles occur by elementary hops among the material lattice sites. In order to compute macroscopic transport parameters such as charge mobility, normal diffusion laws are generally assumed. In this paper we show that when defect states able to trap the polarons for long times are considered, significant deviations from the normal diffusion behaviour arise. As an example of this behavior, we consider here the case of lithium niobate (LN), a prototypical system, having interacting polaron types. Our analysis considers the case of a stoichiometric LN containing a certain concentration of small electron polarons hopping on regular Nb sites and compares it to the material in congruent composition, which is characterized by a large concentration of antisite defects. While in the first case the charge carriers are free polarons hopping on a regular Nb sublattice, in the second case a fraction of polarons is trapped on antisite defects. Thus a range of different hopping possibilities arises, depending on the type of starting and destination sites. We develop a formalism encompassing all these microscopic processes in the framework of a switching diffusion model which can be well approximated by a mobile-immobile transport model providing explicit expressions for the polaron mobility. Starting from the Marcus-Holstein model for the polaron hopping frequency we verify by means of a Monte Carlo approach the diffusion/mobility of the different polarons showing that, while free polarons obey the laws for normal diffusion as expected, bound polarons follow an anomalous diffusion behaviour and that in the case of the congruent crystal where mixed free and bound polaron transport is involved, our expressions indeed provide a satisfactory description.","sentences":["Polarons play a major role in the description of optical, electrical and dielectrical properties of several ferroelectric oxides.","The motion of those particles occur by elementary hops among the material lattice sites.","In order to compute macroscopic transport parameters such as charge mobility, normal diffusion laws are generally assumed.","In this paper we show that when defect states able to trap the polarons for long times are considered, significant deviations from the normal diffusion behaviour arise.","As an example of this behavior, we consider here the case of lithium niobate (LN), a prototypical system, having interacting polaron types.","Our analysis considers the case of a stoichiometric LN containing a certain concentration of small electron polarons hopping on regular Nb sites and compares it to the material in congruent composition, which is characterized by a large concentration of antisite defects.","While in the first case the charge carriers are free polarons hopping on a regular Nb sublattice, in the second case a fraction of polarons is trapped on antisite defects.","Thus a range of different hopping possibilities arises, depending on the type of starting and destination sites.","We develop a formalism encompassing all these microscopic processes in the framework of a switching diffusion model which can be well approximated by a mobile-immobile transport model providing explicit expressions for the polaron mobility.","Starting from the Marcus-Holstein model for the polaron hopping frequency we verify by means of a Monte Carlo approach the diffusion/mobility of the different polarons showing that, while free polarons obey the laws for normal diffusion as expected, bound polarons follow an anomalous diffusion behaviour and that in the case of the congruent crystal where mixed free and bound polaron transport is involved, our expressions indeed provide a satisfactory description."],"url":"http://arxiv.org/abs/2406.08123v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-12 11:53:34","title":"Non-maximal Anosov Representations from Surface Groups to $\\mathrm{SO}_0(2,3)$","abstract":"We prove the representation given by a stable $\\alpha_1$-cyclic parabolic $\\mathrm{SO}_0(2,3)$-Higgs bundle through the non-Abelian Hodge correspondence is $\\{\\alpha_2\\}$-almost dominated. This is a generalization of Filip's result on weight $3$ variation of Hodge structures and answers a question asked by Collier, Tholozan and Toulisse.","sentences":["We prove the representation given by a stable $\\alpha_1$-cyclic parabolic $\\mathrm{SO}_0(2,3)$-Higgs bundle through the non-Abelian Hodge correspondence is $\\{\\alpha_2\\}$-almost dominated.","This is a generalization of Filip's result on weight $3$ variation of Hodge structures and answers a question asked by Collier, Tholozan and Toulisse."],"url":"http://arxiv.org/abs/2406.08118v1","category":"math.DG"}
{"created":"2024-06-12 11:53:09","title":"Algorithmic methods of finite discrete structures. Isomorphism of Nonseparable Graphs","abstract":"In this monography, it is proposed to consider the concepts of spectra of edge cuts and edge cycles of a graph as a basic mathematical structure for solving the problem of graph isomorphism. An edge cut is defined by an edge and the vertices incident to it. In contrast to the generation of iterated edge graphs, we consider an iterated chain of qualicuts of the original graph, generated by edge cuts and determined by a recurrence relation. An edge cycle is defined by the set of isometric cycles of a graph. The monography examines the issues of constructing the spectrum of edge cuts Ws and the spectrum of edge cycles Tc of a graph G. It is shown that the formation of spectra is based on the incidence matrix of the graph. The independence of the construction of the graph structure from the numbering of vertices and edges is shown. The necessity and sufficiency of the spectra of edge cuts and the spectrum of edge cycles for determining the isomorphism of graph structures is shown. The relation between the internal structures of the graph and Whitney's theorem is considered.","sentences":["In this monography, it is proposed to consider the concepts of spectra of edge cuts and edge cycles of a graph as a basic mathematical structure for solving the problem of graph isomorphism.","An edge cut is defined by an edge and the vertices incident to it.","In contrast to the generation of iterated edge graphs, we consider an iterated chain of qualicuts of the original graph, generated by edge cuts and determined by a recurrence relation.","An edge cycle is defined by the set of isometric cycles of a graph.","The monography examines the issues of constructing the spectrum of edge cuts Ws and the spectrum of edge cycles Tc of a graph","G.","It is shown that the formation of spectra is based on the incidence matrix of the graph.","The independence of the construction of the graph structure from the numbering of vertices and edges is shown.","The necessity and sufficiency of the spectra of edge cuts and the spectrum of edge cycles for determining the isomorphism of graph structures is shown.","The relation between the internal structures of the graph and Whitney's theorem is considered."],"url":"http://arxiv.org/abs/2406.08117v1","category":"math.CO"}
{"created":"2024-06-12 11:52:35","title":"Supportiveness-based Knowledge Rewriting for Retrieval-augmented Language Modeling","abstract":"Retrieval-augmented language models (RALMs) have recently shown great potential in mitigating the limitations of implicit knowledge in LLMs, such as untimely updating of the latest expertise and unreliable retention of long-tail knowledge. However, since the external knowledge base, as well as the retriever, can not guarantee reliability, potentially leading to the knowledge retrieved not being helpful or even misleading for LLM generation. In this paper, we introduce Supportiveness-based Knowledge Rewriting (SKR), a robust and pluggable knowledge rewriter inherently optimized for LLM generation. Specifically, we introduce the novel concept of \"supportiveness\"--which represents how effectively a knowledge piece facilitates downstream tasks--by considering the perplexity impact of augmented knowledge on the response text of a white-box LLM. Based on knowledge supportiveness, we first design a training data curation strategy for our rewriter model, effectively identifying and filtering out poor or irrelevant rewrites (e.g., with low supportiveness scores) to improve data efficacy. We then introduce the direct preference optimization (DPO) algorithm to align the generated rewrites to optimal supportiveness, guiding the rewriter model to summarize augmented content that better improves the final response. Comprehensive evaluations across six popular knowledge-intensive tasks and four LLMs have demonstrated the effectiveness and superiority of SKR. With only 7B parameters, SKR has shown better knowledge rewriting capability over GPT-4, the current state-of-the-art general-purpose LLM.","sentences":["Retrieval-augmented language models (RALMs) have recently shown great potential in mitigating the limitations of implicit knowledge in LLMs, such as untimely updating of the latest expertise and unreliable retention of long-tail knowledge.","However, since the external knowledge base, as well as the retriever, can not guarantee reliability, potentially leading to the knowledge retrieved not being helpful or even misleading for LLM generation.","In this paper, we introduce Supportiveness-based Knowledge Rewriting (SKR), a robust and pluggable knowledge rewriter inherently optimized for LLM generation.","Specifically, we introduce the novel concept of \"supportiveness\"--which represents how effectively a knowledge piece facilitates downstream tasks--by considering the perplexity impact of augmented knowledge on the response text of a white-box LLM.","Based on knowledge supportiveness, we first design a training data curation strategy for our rewriter model, effectively identifying and filtering out poor or irrelevant rewrites (e.g., with low supportiveness scores) to improve data efficacy.","We then introduce the direct preference optimization (DPO) algorithm to align the generated rewrites to optimal supportiveness, guiding the rewriter model to summarize augmented content that better improves the final response.","Comprehensive evaluations across six popular knowledge-intensive tasks and four LLMs have demonstrated the effectiveness and superiority of SKR.","With only 7B parameters, SKR has shown better knowledge rewriting capability over GPT-4, the current state-of-the-art general-purpose LLM."],"url":"http://arxiv.org/abs/2406.08116v1","category":"cs.CL"}
{"created":"2024-06-12 11:51:44","title":"Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey","abstract":"With rapidly increasing distributed deep learning workloads in large-scale data centers, efficient distributed deep learning framework strategies for resource allocation and workload scheduling have become the key to high-performance deep learning. The large-scale environment with large volumes of datasets, models, and computational and communication resources raises various unique challenges for resource allocation and workload scheduling in distributed deep learning, such as scheduling complexity, resource and workload heterogeneity, and fault tolerance. To uncover these challenges and corresponding solutions, this survey reviews the literature, mainly from 2019 to 2024, on efficient resource allocation and workload scheduling strategies for large-scale distributed DL. We explore these strategies by focusing on various resource types, scheduling granularity levels, and performance goals during distributed training and inference processes. We highlight critical challenges for each topic and discuss key insights of existing technologies. To illustrate practical large-scale resource allocation and workload scheduling in real distributed deep learning scenarios, we use a case study of training large language models. This survey aims to encourage computer science, artificial intelligence, and communications researchers to understand recent advances and explore future research directions for efficient framework strategies for large-scale distributed deep learning.","sentences":["With rapidly increasing distributed deep learning workloads in large-scale data centers, efficient distributed deep learning framework strategies for resource allocation and workload scheduling have become the key to high-performance deep learning.","The large-scale environment with large volumes of datasets, models, and computational and communication resources raises various unique challenges for resource allocation and workload scheduling in distributed deep learning, such as scheduling complexity, resource and workload heterogeneity, and fault tolerance.","To uncover these challenges and corresponding solutions, this survey reviews the literature, mainly from 2019 to 2024, on efficient resource allocation and workload scheduling strategies for large-scale distributed DL.","We explore these strategies by focusing on various resource types, scheduling granularity levels, and performance goals during distributed training and inference processes.","We highlight critical challenges for each topic and discuss key insights of existing technologies.","To illustrate practical large-scale resource allocation and workload scheduling in real distributed deep learning scenarios, we use a case study of training large language models.","This survey aims to encourage computer science, artificial intelligence, and communications researchers to understand recent advances and explore future research directions for efficient framework strategies for large-scale distributed deep learning."],"url":"http://arxiv.org/abs/2406.08115v1","category":"cs.DC"}
{"created":"2024-06-12 11:47:23","title":"Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio","abstract":"With the proliferation of Large Language Model (LLM) based deepfake audio, there is an urgent need for effective detection methods. Previous deepfake audio generation methods typically involve a multi-step generation process, with the final step using a vocoder to predict the waveform from handcrafted features. However, LLM-based audio is directly generated from discrete neural codecs in an end-to-end generation process, skipping the final step of vocoder processing. This poses a significant challenge for current audio deepfake detection (ADD) models based on vocoder artifacts. To effectively detect LLM-based deepfake audio, we focus on the core of the generation process, the conversion from neural codec to waveform. We propose Codecfake dataset, which is generated by seven representative neural codec methods. Experiment results show that codec-trained ADD models exhibit a 41.406% reduction in average equal error rate compared to vocoder-trained ADD models on the Codecfake test set.","sentences":["With the proliferation of Large Language Model (LLM) based deepfake audio",", there is an urgent need for effective detection methods.","Previous deepfake audio generation methods typically involve a multi-step generation process, with the final step using a vocoder to predict the waveform from handcrafted features.","However, LLM-based audio is directly generated from discrete neural codecs in an end-to-end generation process, skipping the final step of vocoder processing.","This poses a significant challenge for current audio deepfake detection (ADD) models based on vocoder artifacts.","To effectively detect LLM-based deepfake audio, we focus on the core of the generation process, the conversion from neural codec to waveform.","We propose Codecfake dataset, which is generated by seven representative neural codec methods.","Experiment results show that codec-trained ADD models exhibit a 41.406% reduction in average equal error rate compared to vocoder-trained ADD models on the Codecfake test set."],"url":"http://arxiv.org/abs/2406.08112v1","category":"cs.SD"}
{"created":"2024-06-12 11:45:49","title":"Audio-conditioned phonemic and prosodic annotation for building text-to-speech models from unlabeled speech data","abstract":"This paper proposes an audio-conditioned phonemic and prosodic annotation model for building text-to-speech (TTS) datasets from unlabeled speech samples. For creating a TTS dataset that consists of label-speech paired data, the proposed annotation model leverages an automatic speech recognition (ASR) model to obtain phonemic and prosodic labels from unlabeled speech samples. By fine-tuning a large-scale pre-trained ASR model, we can construct the annotation model using a limited amount of label-speech paired data within an existing TTS dataset. To alleviate the shortage of label-speech paired data for training the annotation model, we generate pseudo label-speech paired data using text-only corpora and an auxiliary TTS model. This TTS model is also trained with the existing TTS dataset. Experimental results show that the TTS model trained with the dataset created by the proposed annotation method can synthesize speech as naturally as the one trained with a fully-labeled dataset.","sentences":["This paper proposes an audio-conditioned phonemic and prosodic annotation model for building text-to-speech (TTS) datasets from unlabeled speech samples.","For creating a TTS dataset that consists of label-speech paired data, the proposed annotation model leverages an automatic speech recognition (ASR) model to obtain phonemic and prosodic labels from unlabeled speech samples.","By fine-tuning a large-scale pre-trained ASR model, we can construct the annotation model using a limited amount of label-speech paired data within an existing TTS dataset.","To alleviate the shortage of label-speech paired data for training the annotation model, we generate pseudo label-speech paired data using text-only corpora and an auxiliary TTS model.","This TTS model is also trained with the existing TTS dataset.","Experimental results show that the TTS model trained with the dataset created by the proposed annotation method can synthesize speech as naturally as the one trained with a fully-labeled dataset."],"url":"http://arxiv.org/abs/2406.08111v1","category":"eess.AS"}
{"created":"2024-06-12 11:42:08","title":"Conference Proceedings of The European DAO Workshop 2024","abstract":"The European DAO Workshop 2024 held on July 4th/5th in Winterthur, Switzerland aims to explore the challenges and opportunities of Decentralized Autonomous Organizations (DAOs). Its goal is to foster innovation and knowledge transfer between academics and practitioners to advance DAOs as a new organizational structure. This collection of full papers delves into areas such as decentralized decision-making, business models, artificial intelligence, economics, and legal challenges for DAOs. This diverse compilation offers a multi-disciplinary examination of the rapidly growing phenomenon of DAOs that are based on blockchain technology.","sentences":["The European DAO Workshop 2024 held on July 4th/5th in Winterthur, Switzerland aims to explore the challenges and opportunities of Decentralized Autonomous Organizations (DAOs).","Its goal is to foster innovation and knowledge transfer between academics and practitioners to advance DAOs as a new organizational structure.","This collection of full papers delves into areas such as decentralized decision-making, business models, artificial intelligence, economics, and legal challenges for DAOs.","This diverse compilation offers a multi-disciplinary examination of the rapidly growing phenomenon of DAOs that are based on blockchain technology."],"url":"http://arxiv.org/abs/2406.08110v1","category":"cs.CY"}
{"created":"2024-06-12 11:40:45","title":"On the representation property for 1D general diffusion semimartingales","abstract":"A general diffusion semimartingale is a one-dimensional path-continuous semimartingale that is also a regular strong Markov process. We say that a continuous semimartingale has the representation property if all local martingales w.r.t. its canonical filtration have an integral representation w.r.t. its continuous local martingle part. The representation property is of fundamental interest in the field of mathematical finance, where it is strongly connected to market completeness. The main result from this paper shows that the representation property holds for a general diffusion semimartingale (that is not started in an absorbing boundary point) if and only if its scale function is (locally) absolutely continuous on the interior of the state space. As an application of our main theorem, we deduce that general diffusion semimartingales with such scale functions are extreme points in their semimartingale problem. This observation contributes to a solution of a problem posed by J. Jacod and M. Yor on the extremality of strong Markov solutions.","sentences":["A general diffusion semimartingale is a one-dimensional path-continuous semimartingale that is also a regular strong Markov process.","We say that a continuous semimartingale has the representation property if all local martingales w.r.t.","its canonical filtration have an integral representation w.r.t.","its continuous local martingle part.","The representation property is of fundamental interest in the field of mathematical finance, where it is strongly connected to market completeness.","The main result from this paper shows that the representation property holds for a general diffusion semimartingale (that is not started in an absorbing boundary point)","if and only if its scale function is (locally) absolutely continuous on the interior of the state space.","As an application of our main theorem, we deduce that general diffusion semimartingales with such scale functions are extreme points in their semimartingale problem.","This observation contributes to a solution of a problem posed by J. Jacod and M. Yor on the extremality of strong Markov solutions."],"url":"http://arxiv.org/abs/2406.08109v1","category":"math.PR"}
{"created":"2024-06-12 11:34:19","title":"Prediction of the Realisation of an Information Need: An EEG Study","abstract":"One of the foundational goals of Information Retrieval (IR) is to satisfy searchers' Information Needs (IN). Understanding how INs physically manifest has long been a complex and elusive process. However, recent studies utilising Electroencephalography (EEG) data have provided real-time insights into the neural processes associated with INs. Unfortunately, they have yet to demonstrate how this insight can practically benefit the search experience. As such, within this study, we explore the ability to predict the realisation of IN within EEG data across 14 subjects whilst partaking in a Question-Answering (Q/A) task. Furthermore, we investigate the combinations of EEG features that yield optimal predictive performance, as well as identify regions within the Q/A queries where a subject's realisation of IN is more pronounced. The findings from this work demonstrate that EEG data is sufficient for the real-time prediction of the realisation of an IN across all subjects with an accuracy of 73.5\\% (SD 2.6\\%) and on a per-subject basis with an accuracy of 90.1\\% (SD 22.1\\%). This work helps to close the gap by bridging theoretical neuroscientific advancements with tangible improvements in information retrieval practices, paving the way for real-time prediction of the realisation of IN.","sentences":["One of the foundational goals of Information Retrieval (IR) is to satisfy searchers' Information Needs (IN).","Understanding how INs physically manifest has long been a complex and elusive process.","However, recent studies utilising Electroencephalography (EEG) data have provided real-time insights into the neural processes associated with INs.","Unfortunately, they have yet to demonstrate how this insight can practically benefit the search experience.","As such, within this study, we explore the ability to predict the realisation of IN within EEG data across 14 subjects whilst partaking in a Question-Answering (Q/A) task.","Furthermore, we investigate the combinations of EEG features that yield optimal predictive performance, as well as identify regions within the Q/A queries where a subject's realisation of IN is more pronounced.","The findings from this work demonstrate that EEG data is sufficient for the real-time prediction of the realisation of an IN across all subjects with an accuracy of 73.5\\% (SD 2.6\\%) and on a per-subject basis with an accuracy of 90.1\\% (SD 22.1\\%).","This work helps to close the gap by bridging theoretical neuroscientific advancements with tangible improvements in information retrieval practices, paving the way for real-time prediction of the realisation of IN."],"url":"http://arxiv.org/abs/2406.08105v1","category":"cs.IR"}
{"created":"2024-06-12 11:31:18","title":"Adversarial Patch for 3D Local Feature Extractor","abstract":"Local feature extractors are the cornerstone of many computer vision tasks. However, their vulnerability to adversarial attacks can significantly compromise their effectiveness. This paper discusses approaches to attack sophisticated local feature extraction algorithms and models to achieve two distinct goals: (1) forcing a match between originally non-matching image regions, and (2) preventing a match between originally matching regions. At the end of the paper, we discuss the performance and drawbacks of different patch generation methods.","sentences":["Local feature extractors are the cornerstone of many computer vision tasks.","However, their vulnerability to adversarial attacks can significantly compromise their effectiveness.","This paper discusses approaches to attack sophisticated local feature extraction algorithms and models to achieve two distinct goals: (1) forcing a match between originally non-matching image regions, and (2) preventing a match between originally matching regions.","At the end of the paper, we discuss the performance and drawbacks of different patch generation methods."],"url":"http://arxiv.org/abs/2406.08102v1","category":"cs.CV"}
{"created":"2024-06-12 11:27:10","title":"CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems","abstract":"Conversational explainable artificial intelligence (ConvXAI) systems based on large language models (LLMs) have garnered significant interest from the research community in natural language processing (NLP) and human-computer interaction (HCI). Such systems can provide answers to user questions about explanations, have the potential to enhance users' comprehension and offer more information about the decision-making and generation processes of LLMs. Currently available ConvXAI systems are based on intent recognition rather than free chat. Thus, reliably grasping users' intentions in ConvXAI systems still presents a challenge, because there is a broad range of XAI methods to map requests onto and each of them can have multiple slots to take care of. In order to bridge this gap, we present CoXQL, the first dataset for user intent recognition in ConvXAI, covering 31 intents, seven of which require filling additional slots. Subsequently, we enhance an existing parsing approach by incorporating template validations, and conduct an evaluation of several LLMs on CoXQL using different parsing strategies. We conclude that the improved parsing approach (MP+) surpasses the performance of previous approaches. We also discover that intents with multiple slots remain highly challenging for LLMs.","sentences":["Conversational explainable artificial intelligence (ConvXAI) systems based on large language models (LLMs) have garnered significant interest from the research community in natural language processing (NLP) and human-computer interaction (HCI).","Such systems can provide answers to user questions about explanations, have the potential to enhance users' comprehension and offer more information about the decision-making and generation processes of LLMs.","Currently available ConvXAI systems are based on intent recognition rather than free chat.","Thus, reliably grasping users' intentions in ConvXAI systems still presents a challenge, because there is a broad range of XAI methods to map requests onto and each of them can have multiple slots to take care of.","In order to bridge this gap, we present CoXQL, the first dataset for user intent recognition in ConvXAI, covering 31 intents, seven of which require filling additional slots.","Subsequently, we enhance an existing parsing approach by incorporating template validations, and conduct an evaluation of several LLMs on CoXQL using different parsing strategies.","We conclude that the improved parsing approach (MP+) surpasses the performance of previous approaches.","We also discover that intents with multiple slots remain highly challenging for LLMs."],"url":"http://arxiv.org/abs/2406.08101v1","category":"cs.CL"}
{"created":"2024-06-12 11:27:03","title":"Multimodal Table Understanding","abstract":"Although great progress has been made by previous table understanding methods including recent approaches based on large language models (LLMs), they rely heavily on the premise that given tables must be converted into a certain text sequence (such as Markdown or HTML) to serve as model input. However, it is difficult to access such high-quality textual table representations in some real-world scenarios, and table images are much more accessible. Therefore, how to directly understand tables using intuitive visual information is a crucial and urgent challenge for developing more practical applications. In this paper, we propose a new problem, multimodal table understanding, where the model needs to generate correct responses to various table-related requests based on the given table image. To facilitate both the model training and evaluation, we construct a large-scale dataset named MMTab, which covers a wide spectrum of table images, instructions and tasks. On this basis, we develop Table-LLaVA, a generalist tabular multimodal large language model (MLLM), which significantly outperforms recent open-source MLLM baselines on 23 benchmarks under held-in and held-out settings. The code and data is available at this https://github.com/SpursGoZmy/Table-LLaVA","sentences":["Although great progress has been made by previous table understanding methods including recent approaches based on large language models (LLMs), they rely heavily on the premise that given tables must be converted into a certain text sequence (such as Markdown or HTML) to serve as model input.","However, it is difficult to access such high-quality textual table representations in some real-world scenarios, and table images are much more accessible.","Therefore, how to directly understand tables using intuitive visual information is a crucial and urgent challenge for developing more practical applications.","In this paper, we propose a new problem, multimodal table understanding, where the model needs to generate correct responses to various table-related requests based on the given table image.","To facilitate both the model training and evaluation, we construct a large-scale dataset named MMTab, which covers a wide spectrum of table images, instructions and tasks.","On this basis, we develop Table-LLaVA, a generalist tabular multimodal large language model (MLLM), which significantly outperforms recent open-source MLLM baselines on 23 benchmarks under held-in and held-out settings.","The code and data is available at this https://github.com/SpursGoZmy/Table-LLaVA"],"url":"http://arxiv.org/abs/2406.08100v1","category":"cs.CL"}
{"created":"2024-06-12 11:26:29","title":"Confidence Interval Estimation of Predictive Performance in the Context of AutoML","abstract":"Any supervised machine learning analysis is required to provide an estimate of the out-of-sample predictive performance. However, it is imperative to also provide a quantification of the uncertainty of this performance in the form of a confidence or credible interval (CI) and not just a point estimate. In an AutoML setting, estimating the CI is challenging due to the ``winner's curse\", i.e., the bias of estimation due to cross-validating several machine learning pipelines and selecting the winning one. In this work, we perform a comparative evaluation of 9 state-of-the-art methods and variants in CI estimation in an AutoML setting on a corpus of real and simulated datasets. The methods are compared in terms of inclusion percentage (does a 95\\% CI include the true performance at least 95\\% of the time), CI tightness (tighter CIs are preferable as being more informative), and execution time. The evaluation is the first one that covers most, if not all, such methods and extends previous work to imbalanced and small-sample tasks. In addition, we present a variant, called BBC-F, of an existing method (the Bootstrap Bias Correction, or BBC) that maintains the statistical properties of the BBC but is more computationally efficient. The results support that BBC-F and BBC dominate the other methods in all metrics measured.","sentences":["Any supervised machine learning analysis is required to provide an estimate of the out-of-sample predictive performance.","However, it is imperative to also provide a quantification of the uncertainty of this performance in the form of a confidence or credible interval (CI) and not just a point estimate.","In an AutoML setting, estimating the CI is challenging due to the ``winner's curse\", i.e., the bias of estimation due to cross-validating several machine learning pipelines and selecting the winning one.","In this work, we perform a comparative evaluation of 9 state-of-the-art methods and variants in CI estimation in an AutoML setting on a corpus of real and simulated datasets.","The methods are compared in terms of inclusion percentage (does a 95\\% CI include the true performance at least 95\\% of the time), CI tightness (tighter CIs are preferable as being more informative), and execution time.","The evaluation is the first one that covers most, if not all, such methods and extends previous work to imbalanced and small-sample tasks.","In addition, we present a variant, called BBC-F, of an existing method (the Bootstrap Bias Correction, or BBC) that maintains the statistical properties of the BBC but is more computationally efficient.","The results support that BBC-F and BBC dominate the other methods in all metrics measured."],"url":"http://arxiv.org/abs/2406.08099v1","category":"cs.LG"}
{"created":"2024-06-12 11:24:52","title":"Scalable Defect Detection via Traversal on Code Graph","abstract":"Detecting defects and vulnerabilities in the early stage has long been a challenge in software engineering. Static analysis, a technique that inspects code without execution, has emerged as a key strategy to address this challenge. Among recent advancements, the use of graph-based representations, particularly Code Property Graph (CPG), has gained traction due to its comprehensive depiction of code structure and semantics. Despite the progress, existing graph-based analysis tools still face performance and scalability issues. The main bottleneck lies in the size and complexity of CPG, which makes analyzing large codebases inefficient and memory-consuming. Also, query rules used by the current tools can be over-specific. Hence, we introduce QVoG, a graph-based static analysis platform for detecting defects and vulnerabilities. It employs a compressed CPG representation to maintain a reasonable graph size, thereby enhancing the overall query efficiency. Based on the CPG, it also offers a declarative query language to simplify the queries. Furthermore, it takes a step forward to integrate machine learning to enhance the generality of vulnerability detection. For projects consisting of 1,000,000+ lines of code, QVoG can complete analysis in approximately 15 minutes, as opposed to 19 minutes with CodeQL.","sentences":["Detecting defects and vulnerabilities in the early stage has long been a challenge in software engineering.","Static analysis, a technique that inspects code without execution, has emerged as a key strategy to address this challenge.","Among recent advancements, the use of graph-based representations, particularly Code Property Graph (CPG), has gained traction due to its comprehensive depiction of code structure and semantics.","Despite the progress, existing graph-based analysis tools still face performance and scalability issues.","The main bottleneck lies in the size and complexity of CPG, which makes analyzing large codebases inefficient and memory-consuming.","Also, query rules used by the current tools can be over-specific.","Hence, we introduce QVoG, a graph-based static analysis platform for detecting defects and vulnerabilities.","It employs a compressed CPG representation to maintain a reasonable graph size, thereby enhancing the overall query efficiency.","Based on the CPG, it also offers a declarative query language to simplify the queries.","Furthermore, it takes a step forward to integrate machine learning to enhance the generality of vulnerability detection.","For projects consisting of 1,000,000+ lines of code, QVoG can complete analysis in approximately 15 minutes, as opposed to 19 minutes with CodeQL."],"url":"http://arxiv.org/abs/2406.08098v1","category":"cs.SE"}
{"created":"2024-06-12 11:22:03","title":"Make Your Actor Talk: Generalizable and High-Fidelity Lip Sync with Motion and Appearance Disentanglement","abstract":"We aim to edit the lip movements in talking video according to the given speech while preserving the personal identity and visual details. The task can be decomposed into two sub-problems: (1) speech-driven lip motion generation and (2) visual appearance synthesis. Current solutions handle the two sub-problems within a single generative model, resulting in a challenging trade-off between lip-sync quality and visual details preservation. Instead, we propose to disentangle the motion and appearance, and then generate them one by one with a speech-to-motion diffusion model and a motion-conditioned appearance generation model. However, there still remain challenges in each stage, such as motion-aware identity preservation in (1) and visual details preservation in (2). Therefore, to preserve personal identity, we adopt landmarks to represent the motion, and further employ a landmark-based identity loss. To capture motion-agnostic visual details, we use separate encoders to encode the lip, non-lip appearance and motion, and then integrate them with a learned fusion module. We train MyTalk on a large-scale and diverse dataset. Experiments show that our method generalizes well to the unknown, even out-of-domain person, in terms of both lip sync and visual detail preservation. We encourage the readers to watch the videos on our project page (https://Ingrid789.github.io/MyTalk/).","sentences":["We aim to edit the lip movements in talking video according to the given speech while preserving the personal identity and visual details.","The task can be decomposed into two sub-problems: (1) speech-driven lip motion generation and (2) visual appearance synthesis.","Current solutions handle the two sub-problems within a single generative model, resulting in a challenging trade-off between lip-sync quality and visual details preservation.","Instead, we propose to disentangle the motion and appearance, and then generate them one by one with a speech-to-motion diffusion model and a motion-conditioned appearance generation model.","However, there still remain challenges in each stage, such as motion-aware identity preservation in (1) and visual details preservation in (2).","Therefore, to preserve personal identity, we adopt landmarks to represent the motion, and further employ a landmark-based identity loss.","To capture motion-agnostic visual details, we use separate encoders to encode the lip, non-lip appearance and motion, and then integrate them with a learned fusion module.","We train MyTalk on a large-scale and diverse dataset.","Experiments show that our method generalizes well to the unknown, even out-of-domain person, in terms of both lip sync and visual detail preservation.","We encourage the readers to watch the videos on our project page (https://Ingrid789.github.io/MyTalk/)."],"url":"http://arxiv.org/abs/2406.08096v1","category":"cs.CV"}
{"created":"2024-06-12 11:18:30","title":"Cosmological particle production in a quantum field simulator as a quantum mechanical scattering problem","abstract":"The production of quantum field excitations or particles in cosmological spacetimes is a hallmark prediction of curved quantum field theory. The generation of cosmological perturbations from quantum fluctuations in the early universe constitutes an important application. The problem can be quantum-simulated in terms of structure formation in an interacting Bose-Einstein condensate (BEC) with time-dependent s-wave scattering length. Here, we explore a mapping between cosmological particle production in general (D+1)-dimensional spacetimes and scattering problems described by the non-relativistic stationary Schr\\\"odinger equation in one dimension. Through this mapping, intuitive explanations for emergent spatial structures in both the BEC and the cosmological system can be obtained for a large class of analogue cosmological scenarios, ranging from power-law expansions to periodic modulations. The investigated cosmologies and their scattering analogues are tuned to be implemented in a (2+1)-dimensional quantum field simulator.","sentences":["The production of quantum field excitations or particles in cosmological spacetimes is a hallmark prediction of curved quantum field theory.","The generation of cosmological perturbations from quantum fluctuations in the early universe constitutes an important application.","The problem can be quantum-simulated in terms of structure formation in an interacting Bose-Einstein condensate (BEC) with time-dependent s-wave scattering length.","Here, we explore a mapping between cosmological particle production in general (D+1)-dimensional spacetimes and scattering problems described by the non-relativistic stationary Schr\\\"odinger equation in one dimension.","Through this mapping, intuitive explanations for emergent spatial structures in both the BEC and the cosmological system can be obtained for a large class of analogue cosmological scenarios, ranging from power-law expansions to periodic modulations.","The investigated cosmologies and their scattering analogues are tuned to be implemented in a (2+1)-dimensional quantum field simulator."],"url":"http://arxiv.org/abs/2406.08094v1","category":"gr-qc"}
{"created":"2024-06-12 11:17:11","title":"Learnable & Interpretable Model Combination in Dynamic Systems Modeling","abstract":"One of the core concepts in science, and something that happens intuitively in every-day dynamic systems modeling, is the combination of models or methods. Especially in dynamical systems modeling, often two or more structures are combined to obtain a more powerful or efficient architecture regarding a specific application (area). Further, even physical simulations are combined with machine learning architectures, to increase prediction accuracy or optimize the computational performance. In this work, we shortly discuss, which types of models are usually combined and propose a model interface that is capable of expressing a width variety of mixed algebraic, discrete and differential equation based models. Further, we examine different established, as well as new ways of combining these models from a system theoretical point of view and highlight two challenges - algebraic loops and local event affect functions in discontinuous models - that require a special approach. Finally, we propose a new wildcard topology, that is capable of describing the generic connection between two combined models in an easy to interpret fashion that can be learned as part of a gradient based optimization procedure. The contributions of this paper are highlighted at a proof of concept: Different connection topologies between two models are learned, interpreted and compared applying the proposed methodology and software implementation.","sentences":["One of the core concepts in science, and something that happens intuitively in every-day dynamic systems modeling, is the combination of models or methods.","Especially in dynamical systems modeling, often two or more structures are combined to obtain a more powerful or efficient architecture regarding a specific application (area).","Further, even physical simulations are combined with machine learning architectures, to increase prediction accuracy or optimize the computational performance.","In this work, we shortly discuss, which types of models are usually combined and propose a model interface that is capable of expressing a width variety of mixed algebraic, discrete and differential equation based models.","Further, we examine different established, as well as new ways of combining these models from a system theoretical point of view and highlight two challenges - algebraic loops and local event affect functions in discontinuous models - that require a special approach.","Finally, we propose a new wildcard topology, that is capable of describing the generic connection between two combined models in an easy to interpret fashion that can be learned as part of a gradient based optimization procedure.","The contributions of this paper are highlighted at a proof of concept: Different connection topologies between two models are learned, interpreted and compared applying the proposed methodology and software implementation."],"url":"http://arxiv.org/abs/2406.08093v1","category":"cs.LG"}
{"created":"2024-06-12 11:15:59","title":"From Sim-to-Real: Toward General Event-based Low-light Frame Interpolation with Per-scene Optimization","abstract":"Video Frame Interpolation (VFI) is important for video enhancement, frame rate up-conversion, and slow-motion generation. The introduction of event cameras, which capture per-pixel brightness changes asynchronously, has significantly enhanced VFI capabilities, particularly for high-speed, nonlinear motions. However, these event-based methods encounter challenges in low-light conditions, notably trailing artifacts and signal latency, which hinder their direct applicability and generalization. Addressing these issues, we propose a novel per-scene optimization strategy tailored for low-light conditions. This approach utilizes the internal statistics of a sequence to handle degraded event data under low-light conditions, improving the generalizability to different lighting and camera settings. To evaluate its robustness in low-light condition, we further introduce EVFI-LL, a unique RGB+Event dataset captured under low-light conditions. Our results demonstrate state-of-the-art performance in low-light environments. Both the dataset and the source code will be made publicly available upon publication. Project page: https://naturezhanghn.github.io/sim2real.","sentences":["Video Frame Interpolation (VFI) is important for video enhancement, frame rate up-conversion, and slow-motion generation.","The introduction of event cameras, which capture per-pixel brightness changes asynchronously, has significantly enhanced VFI capabilities, particularly for high-speed, nonlinear motions.","However, these event-based methods encounter challenges in low-light conditions, notably trailing artifacts and signal latency, which hinder their direct applicability and generalization.","Addressing these issues, we propose a novel per-scene optimization strategy tailored for low-light conditions.","This approach utilizes the internal statistics of a sequence to handle degraded event data under low-light conditions, improving the generalizability to different lighting and camera settings.","To evaluate its robustness in low-light condition, we further introduce EVFI-LL, a unique RGB+Event dataset captured under low-light conditions.","Our results demonstrate state-of-the-art performance in low-light environments.","Both the dataset and the source code will be made publicly available upon publication.","Project page: https://naturezhanghn.github.io/sim2real."],"url":"http://arxiv.org/abs/2406.08090v1","category":"cs.CV"}
{"created":"2024-06-12 11:09:05","title":"A Unified Pilot Design for Integrated Sensing and Communications","abstract":"This paper investigates a unified pilot signal design in an orthogonal frequency division modulation (OFDM)-based integrated sensing and communications (ISAC) system. The novel designed two-dimensional (2D) pilot signal is generated on the delay-Doppler (DD) plane for sensing, while its time-frequency (TF) plane transformation acts as the demodulation reference signal (DMRS) for the OFDM data. The well-designed pilot signal preserves orthogonality with the data in terms of resource occupancy in the TF plane and quasi-orthogonality in terms of codeword in the DD plane. Leveraging these nice properties, we are allowed to implement sensing detection in the DD plane using a simple 2D correlation, taking advantage of the favorable auto-correlation properties of the 2D pilot. In the communication part, the transformed pilot in the TF plane serves as a known DMRS for channel estimation and equalization. The 2D pilot design demonstrates good scalability and can adapt to different delay and Doppler resolution requirements without violating the OFDM data detection and can overcome the fractional Doppler with limited sensing resources. Experimental results show the effective sensing performance of the proposed pilot, with only a small fraction of power shared from the OFDM data,while maintaining satisfactory symbol detection performance in communication.","sentences":["This paper investigates a unified pilot signal design in an orthogonal frequency division modulation (OFDM)-based integrated sensing and communications (ISAC) system.","The novel designed two-dimensional (2D) pilot signal is generated on the delay-Doppler (DD) plane for sensing, while its time-frequency (TF) plane transformation acts as the demodulation reference signal (DMRS) for the OFDM data.","The well-designed pilot signal preserves orthogonality with the data in terms of resource occupancy in the TF plane and quasi-orthogonality in terms of codeword in the DD plane.","Leveraging these nice properties, we are allowed to implement sensing detection in the DD plane using a simple 2D correlation, taking advantage of the favorable auto-correlation properties of the 2D pilot.","In the communication part, the transformed pilot in the TF plane serves as a known DMRS for channel estimation and equalization.","The 2D pilot design demonstrates good scalability and can adapt to different delay and Doppler resolution requirements without violating the OFDM data detection and can overcome the fractional Doppler with limited sensing resources.","Experimental results show the effective sensing performance of the proposed pilot, with only a small fraction of power shared from the OFDM data,while maintaining satisfactory symbol detection performance in communication."],"url":"http://arxiv.org/abs/2406.08087v1","category":"eess.SP"}
{"created":"2024-06-12 11:05:42","title":"CLDTA: Contrastive Learning based on Diagonal Transformer Autoencoder for Cross-Dataset EEG Emotion Recognition","abstract":"Recent advances in non-invasive EEG technology have broadened its application in emotion recognition, yielding a multitude of related datasets. Yet, deep learning models struggle to generalize across these datasets due to variations in acquisition equipment and emotional stimulus materials. To address the pressing need for a universal model that fluidly accommodates diverse EEG dataset formats and bridges the gap between laboratory and real-world data, we introduce a novel deep learning framework: the Contrastive Learning based Diagonal Transformer Autoencoder (CLDTA), tailored for EEG-based emotion recognition. The CLDTA employs a diagonal masking strategy within its encoder to extracts full-channel EEG data's brain network knowledge, facilitating transferability to the datasets with fewer channels. And an information separation mechanism improves model interpretability by enabling straightforward visualization of brain networks. The CLDTA framework employs contrastive learning to distill subject-independent emotional representations and uses a calibration prediction process to enable rapid adaptation of the model to new subjects with minimal samples, achieving accurate emotion recognition. Our analysis across the SEED, SEED-IV, SEED-V, and DEAP datasets highlights CLDTA's consistent performance and proficiency in detecting both task-specific and general features of EEG signals related to emotions, underscoring its potential to revolutionize emotion recognition research.","sentences":["Recent advances in non-invasive EEG technology have broadened its application in emotion recognition, yielding a multitude of related datasets.","Yet, deep learning models struggle to generalize across these datasets due to variations in acquisition equipment and emotional stimulus materials.","To address the pressing need for a universal model that fluidly accommodates diverse EEG dataset formats and bridges the gap between laboratory and real-world data, we introduce a novel deep learning framework: the Contrastive Learning based Diagonal Transformer Autoencoder (CLDTA), tailored for EEG-based emotion recognition.","The CLDTA employs a diagonal masking strategy within its encoder to extracts full-channel EEG data's brain network knowledge, facilitating transferability to the datasets with fewer channels.","And an information separation mechanism improves model interpretability by enabling straightforward visualization of brain networks.","The CLDTA framework employs contrastive learning to distill subject-independent emotional representations and uses a calibration prediction process to enable rapid adaptation of the model to new subjects with minimal samples, achieving accurate emotion recognition.","Our analysis across the SEED, SEED-IV, SEED-V, and DEAP datasets highlights CLDTA's consistent performance and proficiency in detecting both task-specific and general features of EEG signals related to emotions, underscoring its potential to revolutionize emotion recognition research."],"url":"http://arxiv.org/abs/2406.08081v1","category":"eess.SP"}
{"created":"2024-06-12 11:04:11","title":"AustroTox: A Dataset for Target-Based Austrian German Offensive Language Detection","abstract":"Model interpretability in toxicity detection greatly profits from token-level annotations. However, currently such annotations are only available in English. We introduce a dataset annotated for offensive language detection sourced from a news forum, notable for its incorporation of the Austrian German dialect, comprising 4,562 user comments. In addition to binary offensiveness classification, we identify spans within each comment constituting vulgar language or representing targets of offensive statements. We evaluate fine-tuned language models as well as large language models in a zero- and few-shot fashion. The results indicate that while fine-tuned models excel in detecting linguistic peculiarities such as vulgar dialect, large language models demonstrate superior performance in detecting offensiveness in AustroTox. We publish the data and code.","sentences":["Model interpretability in toxicity detection greatly profits from token-level annotations.","However, currently such annotations are only available in English.","We introduce a dataset annotated for offensive language detection sourced from a news forum, notable for its incorporation of the Austrian German dialect, comprising 4,562 user comments.","In addition to binary offensiveness classification, we identify spans within each comment constituting vulgar language or representing targets of offensive statements.","We evaluate fine-tuned language models as well as large language models in a zero- and few-shot fashion.","The results indicate that while fine-tuned models excel in detecting linguistic peculiarities such as vulgar dialect, large language models demonstrate superior performance in detecting offensiveness in AustroTox.","We publish the data and code."],"url":"http://arxiv.org/abs/2406.08080v1","category":"cs.CL"}
{"created":"2024-06-12 11:02:15","title":"A$^{2}$-MAE: A spatial-temporal-spectral unified remote sensing pre-training method based on anchor-aware masked autoencoder","abstract":"Vast amounts of remote sensing (RS) data provide Earth observations across multiple dimensions, encompassing critical spatial, temporal, and spectral information which is essential for addressing global-scale challenges such as land use monitoring, disaster prevention, and environmental change mitigation. Despite various pre-training methods tailored to the characteristics of RS data, a key limitation persists: the inability to effectively integrate spatial, temporal, and spectral information within a single unified model. To unlock the potential of RS data, we construct a Spatial-Temporal-Spectral Structured Dataset (STSSD) characterized by the incorporation of multiple RS sources, diverse coverage, unified locations within image sets, and heterogeneity within images. Building upon this structured dataset, we propose an Anchor-Aware Masked AutoEncoder method (A$^{2}$-MAE), leveraging intrinsic complementary information from the different kinds of images and geo-information to reconstruct the masked patches during the pre-training phase. A$^{2}$-MAE integrates an anchor-aware masking strategy and a geographic encoding module to comprehensively exploit the properties of RS images. Specifically, the proposed anchor-aware masking strategy dynamically adapts the masking process based on the meta-information of a pre-selected anchor image, thereby facilitating the training on images captured by diverse types of RS sources within one model. Furthermore, we propose a geographic encoding method to leverage accurate spatial patterns, enhancing the model generalization capabilities for downstream applications that are generally location-related. Extensive experiments demonstrate our method achieves comprehensive improvements across various downstream tasks compared with existing RS pre-training methods, including image classification, semantic segmentation, and change detection tasks.","sentences":["Vast amounts of remote sensing (RS) data provide Earth observations across multiple dimensions, encompassing critical spatial, temporal, and spectral information which is essential for addressing global-scale challenges such as land use monitoring, disaster prevention, and environmental change mitigation.","Despite various pre-training methods tailored to the characteristics of RS data, a key limitation persists: the inability to effectively integrate spatial, temporal, and spectral information within a single unified model.","To unlock the potential of RS data, we construct a Spatial-Temporal-Spectral Structured Dataset (STSSD) characterized by the incorporation of multiple RS sources, diverse coverage, unified locations within image sets, and heterogeneity within images.","Building upon this structured dataset, we propose an Anchor-Aware Masked AutoEncoder method (A$^{2}$-MAE), leveraging intrinsic complementary information from the different kinds of images and geo-information to reconstruct the masked patches during the pre-training phase.","A$^{2}$-MAE integrates an anchor-aware masking strategy and a geographic encoding module to comprehensively exploit the properties of RS images.","Specifically, the proposed anchor-aware masking strategy dynamically adapts the masking process based on the meta-information of a pre-selected anchor image, thereby facilitating the training on images captured by diverse types of RS sources within one model.","Furthermore, we propose a geographic encoding method to leverage accurate spatial patterns, enhancing the model generalization capabilities for downstream applications that are generally location-related.","Extensive experiments demonstrate our method achieves comprehensive improvements across various downstream tasks compared with existing RS pre-training methods, including image classification, semantic segmentation, and change detection tasks."],"url":"http://arxiv.org/abs/2406.08079v1","category":"cs.CV"}
{"created":"2024-06-12 10:57:18","title":"Analyzing the effect of higher dimensions on the black hole silhouette, deflection angles, and PINN approximated quasinormal modes","abstract":"We investigate the impact of higher dimensions on the properties of Schwarzschild-Tangherlini black holes, focusing on the photonsphere, black hole shadow, deflection angles, and quasinormal modes (QNMs). We find that these properties diminish as the dimensionality ($n$) of the black hole increases. Analysis of the shadow radius measured by the Event Horizon Telescope suggests non-integer dimensions around $n\\lessgtr4$. We derive an analytic formula for the weak field deflection angle, highlighting the need for advanced sensitive detection devices to observe lensed images influenced by higher dimensions. Our study of QNMs using physics-informed neural networks and the WKB method reveals a convergence towards known relationships between QNM frequencies and photon-sphere orbit frequencies. Despite the energetic nature of perturbing fields in higher dimensions, their damping increases. This suggests a complex interplay between dimensionality and the dynamics of black hole phenomena.","sentences":["We investigate the impact of higher dimensions on the properties of Schwarzschild-Tangherlini black holes, focusing on the photonsphere, black hole shadow, deflection angles, and quasinormal modes (QNMs).","We find that these properties diminish as the dimensionality ($n$) of the black hole increases.","Analysis of the shadow radius measured by the Event Horizon Telescope suggests non-integer dimensions around $n\\lessgtr4$. We derive an analytic formula for the weak field deflection angle, highlighting the need for advanced sensitive detection devices to observe lensed images influenced by higher dimensions.","Our study of QNMs using physics-informed neural networks and the WKB method reveals a convergence towards known relationships between QNM frequencies and photon-sphere orbit frequencies.","Despite the energetic nature of perturbing fields in higher dimensions, their damping increases.","This suggests a complex interplay between dimensionality and the dynamics of black hole phenomena."],"url":"http://arxiv.org/abs/2406.08078v1","category":"gr-qc"}
{"created":"2024-06-12 10:51:00","title":"Balancing Molecular Information and Empirical Data in the Prediction of Physico-Chemical Properties","abstract":"Predicting the physico-chemical properties of pure substances and mixtures is a central task in thermodynamics. Established prediction methods range from fully physics-based ab-initio calculations, which are only feasible for very simple systems, over descriptor-based methods that use some information on the molecules to be modeled together with fitted model parameters (e.g., quantitative-structure-property relationship methods or classical group contribution methods), to representation-learning methods, which may, in extreme cases, completely ignore molecular descriptors and extrapolate only from existing data on the property to be modeled (e.g., matrix completion methods). In this work, we propose a general method for combining molecular descriptors with representation learning using the so-called expectation maximization algorithm from the probabilistic machine learning literature, which uses uncertainty estimates to trade off between the two approaches. The proposed hybrid model exploits chemical structure information using graph neural networks, but it automatically detects cases where structure-based predictions are unreliable, in which case it corrects them by representation-learning based predictions that can better specialize to unusual cases. The effectiveness of the proposed method is demonstrated using the prediction of activity coefficients in binary mixtures as an example. The results are compelling, as the method significantly improves predictive accuracy over the current state of the art, showcasing its potential to advance the prediction of physico-chemical properties in general.","sentences":["Predicting the physico-chemical properties of pure substances and mixtures is a central task in thermodynamics.","Established prediction methods range from fully physics-based ab-initio calculations, which are only feasible for very simple systems, over descriptor-based methods that use some information on the molecules to be modeled together with fitted model parameters (e.g., quantitative-structure-property relationship methods or classical group contribution methods), to representation-learning methods, which may, in extreme cases, completely ignore molecular descriptors and extrapolate only from existing data on the property to be modeled (e.g., matrix completion methods).","In this work, we propose a general method for combining molecular descriptors with representation learning using the so-called expectation maximization algorithm from the probabilistic machine learning literature, which uses uncertainty estimates to trade off between the two approaches.","The proposed hybrid model exploits chemical structure information using graph neural networks, but it automatically detects cases where structure-based predictions are unreliable, in which case it corrects them by representation-learning based predictions that can better specialize to unusual cases.","The effectiveness of the proposed method is demonstrated using the prediction of activity coefficients in binary mixtures as an example.","The results are compelling, as the method significantly improves predictive accuracy over the current state of the art, showcasing its potential to advance the prediction of physico-chemical properties in general."],"url":"http://arxiv.org/abs/2406.08075v1","category":"cs.LG"}
{"created":"2024-06-12 10:48:53","title":"A Concept-Based Explainability Framework for Large Multimodal Models","abstract":"Large multimodal models (LMMs) combine unimodal encoders and large language models (LLMs) to perform multimodal tasks. Despite recent advancements towards the interpretability of these models, understanding internal representations of LMMs remains largely a mystery. In this paper, we present a novel framework for the interpretation of LMMs. We propose a dictionary learning based approach, applied to the representation of tokens. The elements of the learned dictionary correspond to our proposed concepts. We show that these concepts are well semantically grounded in both vision and text. Thus we refer to these as \"multi-modal concepts\". We qualitatively and quantitatively evaluate the results of the learnt concepts. We show that the extracted multimodal concepts are useful to interpret representations of test samples. Finally, we evaluate the disentanglement between different concepts and the quality of grounding concepts visually and textually. We will publicly release our code.","sentences":["Large multimodal models (LMMs) combine unimodal encoders and large language models (LLMs) to perform multimodal tasks.","Despite recent advancements towards the interpretability of these models, understanding internal representations of LMMs remains largely a mystery.","In this paper, we present a novel framework for the interpretation of LMMs.","We propose a dictionary learning based approach, applied to the representation of tokens.","The elements of the learned dictionary correspond to our proposed concepts.","We show that these concepts are well semantically grounded in both vision and text.","Thus we refer to these as \"multi-modal concepts\".","We qualitatively and quantitatively evaluate the results of the learnt concepts.","We show that the extracted multimodal concepts are useful to interpret representations of test samples.","Finally, we evaluate the disentanglement between different concepts and the quality of grounding concepts visually and textually.","We will publicly release our code."],"url":"http://arxiv.org/abs/2406.08074v1","category":"cs.LG"}
{"created":"2024-06-12 10:48:24","title":"Bridging Resource Theory and Quantum Key Distribution: Geometric Analysis and Statistical Testing","abstract":"Discerning between quantum and classical correlations is of great importance. Bell polytopes are well established as a fundamental tool. In this paper, we extend this line of inquiry by applying resource theory within the context of Network scenarios, to a Quantum Key Distribution (QKD) protocol. To achieve this, we consider the causal structure $P3$ that can describe the protocol, and we aim to develop useful statistical tests to assess it.   More concretely, our objectives are twofold: firstly, to utilise the underlying causal structure of the QKD protocol to obtain a geometrical analysis of the resulting non-convex polytope, with a focus on the classical behaviours. Second, we devise a test within this framework to evaluate the distance between any two behaviours within the generated polytope. This approach offers a unique perspective, linking deviations from expected behaviour directly to the quality of the quantum resource or the residual nonclassicality in protocol execution.","sentences":["Discerning between quantum and classical correlations is of great importance.","Bell polytopes are well established as a fundamental tool.","In this paper, we extend this line of inquiry by applying resource theory within the context of Network scenarios, to a Quantum Key Distribution (QKD) protocol.","To achieve this, we consider the causal structure $P3$ that can describe the protocol, and we aim to develop useful statistical tests to assess it.   ","More concretely, our objectives are twofold: firstly, to utilise the underlying causal structure of the QKD protocol to obtain a geometrical analysis of the resulting non-convex polytope, with a focus on the classical behaviours.","Second, we devise a test within this framework to evaluate the distance between any two behaviours within the generated polytope.","This approach offers a unique perspective, linking deviations from expected behaviour directly to the quality of the quantum resource or the residual nonclassicality in protocol execution."],"url":"http://arxiv.org/abs/2406.08073v1","category":"quant-ph"}
{"created":"2024-06-12 10:40:10","title":"CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models","abstract":"Classifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation. Although effective, CFG has notable drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality outputs, frequently result in issues like mode collapse. Contrary to the widespread belief that these are inherent limitations of diffusion models, this paper reveals that the problems actually stem from the off-manifold phenomenon associated with CFG, rather than the diffusion models themselves. More specifically, inspired by the recent advancements of diffusion model-based inverse problem solvers (DIS), we reformulate text-guidance as an inverse problem with a text-conditioned score matching loss, and develop CFG++, a novel approach that tackles the off-manifold challenges inherent in traditional CFG. CFG++ features a surprisingly simple fix to CFG, yet it offers significant improvements, including better sample quality for text-to-image generation, invertibility, smaller guidance scales, reduced mode collapse, etc. Furthermore, CFG++ enables seamless interpolation between unconditional and conditional sampling at lower guidance scales, consistently outperforming traditional CFG at all scales. Experimental results confirm that our method significantly enhances performance in text-to-image generation, DDIM inversion, editing, and solving inverse problems, suggesting a wide-ranging impact and potential applications in various fields that utilize text guidance. Project Page: https://cfgpp-diffusion.github.io/.","sentences":["Classifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation.","Although effective, CFG has notable drawbacks.","For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality outputs, frequently result in issues like mode collapse.","Contrary to the widespread belief that these are inherent limitations of diffusion models, this paper reveals that the problems actually stem from the off-manifold phenomenon associated with CFG, rather than the diffusion models themselves.","More specifically, inspired by the recent advancements of diffusion model-based inverse problem solvers (DIS), we reformulate text-guidance as an inverse problem with a text-conditioned score matching loss, and develop CFG++, a novel approach that tackles the off-manifold challenges inherent in traditional CFG.","CFG++ features a surprisingly simple fix to CFG, yet it offers significant improvements, including better sample quality for text-to-image generation, invertibility, smaller guidance scales, reduced mode collapse, etc.","Furthermore, CFG++ enables seamless interpolation between unconditional and conditional sampling at lower guidance scales, consistently outperforming traditional CFG at all scales.","Experimental results confirm that our method significantly enhances performance in text-to-image generation, DDIM inversion, editing, and solving inverse problems, suggesting a wide-ranging impact and potential applications in various fields that utilize text guidance.","Project Page: https://cfgpp-diffusion.github.io/."],"url":"http://arxiv.org/abs/2406.08070v1","category":"cs.CV"}
{"created":"2024-06-12 10:39:31","title":"Explore-Go: Leveraging Exploration for Generalisation in Deep Reinforcement Learning","abstract":"One of the remaining challenges in reinforcement learning is to develop agents that can generalise to novel scenarios they might encounter once deployed. This challenge is often framed in a multi-task setting where agents train on a fixed set of tasks and have to generalise to new tasks. Recent work has shown that in this setting increased exploration during training can be leveraged to increase the generalisation performance of the agent. This makes sense when the states encountered during testing can actually be explored during training. In this paper, we provide intuition why exploration can also benefit generalisation to states that cannot be explicitly encountered during training. Additionally, we propose a novel method Explore-Go that exploits this intuition by increasing the number of states on which the agent trains. Explore-Go effectively increases the starting state distribution of the agent and as a result can be used in conjunction with most existing on-policy or off-policy reinforcement learning algorithms. We show empirically that our method can increase generalisation performance in an illustrative environment and on the Procgen benchmark.","sentences":["One of the remaining challenges in reinforcement learning is to develop agents that can generalise to novel scenarios they might encounter once deployed.","This challenge is often framed in a multi-task setting where agents train on a fixed set of tasks and have to generalise to new tasks.","Recent work has shown that in this setting increased exploration during training can be leveraged to increase the generalisation performance of the agent.","This makes sense when the states encountered during testing can actually be explored during training.","In this paper, we provide intuition why exploration can also benefit generalisation to states that cannot be explicitly encountered during training.","Additionally, we propose a novel method Explore-Go that exploits this intuition by increasing the number of states on which the agent trains.","Explore-Go effectively increases the starting state distribution of the agent and as a result can be used in conjunction with most existing on-policy or off-policy reinforcement learning algorithms.","We show empirically that our method can increase generalisation performance in an illustrative environment and on the Procgen benchmark."],"url":"http://arxiv.org/abs/2406.08069v1","category":"cs.LG"}
{"created":"2024-06-12 10:33:53","title":"Two-tone spectroscopy of high-frequency quantum circuits with a Josephson emitter","abstract":"We perform two-tone spectroscopy on quantum circuits, where high-frequency radiation is generated by a voltage-biased superconductor-normal-superconductor Josephson junction and detection is carried out by an ancillary microwave resonator. We implement this protocol on two different systems, a transmon qubit and a $\\lambda/4$ resonator. We demonstrate that this two-tone Josephson spectroscopy operates well into the millimeter-wave band, reaching frequencies larger than 80 GHz, and is well-suited for probing highly coherent quantum systems.","sentences":["We perform two-tone spectroscopy on quantum circuits, where high-frequency radiation is generated by a voltage-biased superconductor-normal-superconductor Josephson junction and detection is carried out by an ancillary microwave resonator.","We implement this protocol on two different systems, a transmon qubit and a $\\lambda/4$ resonator.","We demonstrate that this two-tone Josephson spectroscopy operates well into the millimeter-wave band, reaching frequencies larger than 80 GHz, and is well-suited for probing highly coherent quantum systems."],"url":"http://arxiv.org/abs/2406.08066v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-12 10:29:21","title":"Gate-based counterdiabatic driving with complexity guarantees","abstract":"We propose a general, fully gate-based quantum algorithm for counterdiabatic driving. The algorithm does not depend on heuristics as in previous variational methods, and exploits regularisation of the adiabatic gauge potential to suppress only the transitions from the eigenstate of interest. This allows for a rigorous quantum gate complexity upper bound in terms of the minimum gap $\\Delta$ around this target eigenstate. We find that the algorithm requires at most $\\tilde O(\\Delta^{-(3 + o(1))} \\epsilon^{-(1 + o(1))})$ quantum gates to achieve a target state fidelity of at least $1 - \\epsilon^2$, which is nearly equivalent to the gate complexity of gate-based adiabatic state preparation. This calls into question the perception of counterdiabatic driving as a general shortcut to adiabaticity.","sentences":["We propose a general, fully gate-based quantum algorithm for counterdiabatic driving.","The algorithm does not depend on heuristics as in previous variational methods, and exploits regularisation of the adiabatic gauge potential to suppress only the transitions from the eigenstate of interest.","This allows for a rigorous quantum gate complexity upper bound in terms of the minimum gap $\\Delta$ around this target eigenstate.","We find that the algorithm requires at most $\\tilde O(\\Delta^{-(3 + o(1))} \\epsilon^{-(1","+ o(1))})$ quantum gates to achieve a target state fidelity of at least $1 - \\epsilon^2$, which is nearly equivalent to the gate complexity of gate-based adiabatic state preparation.","This calls into question the perception of counterdiabatic driving as a general shortcut to adiabaticity."],"url":"http://arxiv.org/abs/2406.08064v1","category":"quant-ph"}
{"created":"2024-06-12 10:24:47","title":"On Constructions of Fractal Spaces Using Replacement and the Combinatorial Loewner Property","abstract":"The combinatorial Loewner property was introduced by Bourdon and Kleiner as a quasismmetrically invariant substitute for the Loewner property for general fractals and boundaries of hyperbolic groups. While the Loewner property is somewhat restrictive, the combinatorial Loewner property is very generic -- Bourdon and Kleiner showed that many familiar fractals and group boundaries satisfy it. If $X$ is quasisymmetric to a Loewner space, it has the combinatorial Loewner property. Kleiner conjectured in 2006 that the converse to this holds for self-similar fractals -- the hope being that this would lead to the existence of many exotic Loewner spaces. We disprove this conjecture and give the first examples of spaces which are self-similar and combinatorially Loewner and which are not quasisymmetric to Loewner spaces.   In the process we introduce a self-similar replacement rule, called a linear replacement rule, which is inspired by the work of Laakso. This produces a new rich class of fractal spaces, where closed form computations of potentials and their conformal dimensions are possible. These spaces exhibit a rich class of behaviors from analysis on fractals in regards to diffusions, Sobolev spaces, energy measures and conformal dimensions. These behaviors expand on the known examples of Cantor sets, gaskets, Vicsek sets, and the often too difficult carpet-like spaces. Especially the counter examples to Kleiner's conjecture that arise from this construction are interesting, since they open up the possibility to study the new realm of combinatorially Loewner spaces that are not quasisymmetric to Loewner spaces.","sentences":["The combinatorial Loewner property was introduced by Bourdon and Kleiner as a quasismmetrically invariant substitute for the Loewner property for general fractals and boundaries of hyperbolic groups.","While the Loewner property is somewhat restrictive, the combinatorial Loewner property is very generic -- Bourdon and Kleiner showed that many familiar fractals and group boundaries satisfy it.","If $X$ is quasisymmetric to a Loewner space, it has the combinatorial Loewner property.","Kleiner conjectured in 2006 that the converse to this holds for self-similar fractals -- the hope being that this would lead to the existence of many exotic Loewner spaces.","We disprove this conjecture and give the first examples of spaces which are self-similar and combinatorially Loewner and which are not quasisymmetric to Loewner spaces.   ","In the process we introduce a self-similar replacement rule, called a linear replacement rule, which is inspired by the work of Laakso.","This produces a new rich class of fractal spaces, where closed form computations of potentials and their conformal dimensions are possible.","These spaces exhibit a rich class of behaviors from analysis on fractals in regards to diffusions, Sobolev spaces, energy measures and conformal dimensions.","These behaviors expand on the known examples of Cantor sets, gaskets, Vicsek sets, and the often too difficult carpet-like spaces.","Especially the counter examples to Kleiner's conjecture that arise from this construction are interesting, since they open up the possibility to study the new realm of combinatorially Loewner spaces that are not quasisymmetric to Loewner spaces."],"url":"http://arxiv.org/abs/2406.08062v1","category":"math.MG"}
{"created":"2024-06-12 10:22:14","title":"Functional approach to the normality of mappings","abstract":"In the article a technique of the usage of $f$-continuous functions (on mappings) and their families is developed. A proof of the Urysohn's Lemma for mappings is presented and a variant of the Brouwer-Tietze-Urysohn Extension Theorem for mappings is proven. Characterizations of the normality properties of mappings are given and the notion of a perfect normality of a mapping is introduced. It seems to be the most optimal in this approach.","sentences":["In the article a technique of the usage of $f$-continuous functions (on mappings) and their families is developed.","A proof of the Urysohn's Lemma for mappings is presented and a variant of the Brouwer-Tietze-Urysohn Extension Theorem for mappings is proven.","Characterizations of the normality properties of mappings are given and the notion of a perfect normality of a mapping is introduced.","It seems to be the most optimal in this approach."],"url":"http://arxiv.org/abs/2406.08061v1","category":"math.GN"}
{"created":"2024-06-12 10:20:28","title":"MOCCA: Global properties of tidally filling and underfilling globular star clusters with multiple stellar populations","abstract":"We explore the evolution of various properties of multiple-population globular clusters (GCs) for a broad range of initial conditions. We simulated over 200 GC models using the MOCCA Monte Carlo code and find that present-day properties (core and half-light radii, ratio of the number of second-generation (SG) stars to the total number of stars, NSG/NTOT) of these models cover the observed values of these quantities for Milky Way GCs. Starting with a relatively small value of the SG fraction (NSG/NTOT ~ 0.25) and a SG system concentrated in the inner regions of the cluster, we find, in agreement with previous studies, that systems in which the first-generation (FG) is initially tidally filling or slightly tidally underfilling best reproduce the observed ratios of NSG/NTOT and have values of the core and half-light radii typical of those of many Galactic globular clusters. Models in which the FG is initially tidally underfilling retain values of NSG/NTOT close to their initial values. These simulations expand previous investigations and serve to further constrain the viable range of initial parameters and better understand their influence on present-day GC properties. The results of this investigation also provide the basis for our future survey aimed at building specific models to reproduce the observed trends (or lack thereof) between the properties of multiple stellar populations and other clusters properties.","sentences":["We explore the evolution of various properties of multiple-population globular clusters (GCs) for a broad range of initial conditions.","We simulated over 200 GC models using the MOCCA Monte Carlo code and find that present-day properties (core and half-light radii, ratio of the number of second-generation (SG) stars to the total number of stars, NSG/NTOT) of these models cover the observed values of these quantities for Milky Way GCs.","Starting with a relatively small value of the SG fraction (NSG/NTOT ~ 0.25) and a SG system concentrated in the inner regions of the cluster, we find, in agreement with previous studies, that systems in which the first-generation (FG) is initially tidally filling or slightly tidally underfilling best reproduce the observed ratios of NSG/NTOT and have values of the core and half-light radii typical of those of many Galactic globular clusters.","Models in which the FG is initially tidally underfilling retain values of NSG/NTOT close to their initial values.","These simulations expand previous investigations and serve to further constrain the viable range of initial parameters and better understand their influence on present-day GC properties.","The results of this investigation also provide the basis for our future survey aimed at building specific models to reproduce the observed trends (or lack thereof) between the properties of multiple stellar populations and other clusters properties."],"url":"http://arxiv.org/abs/2406.08059v1","category":"astro-ph.GA"}
{"created":"2024-06-12 10:12:53","title":"DCASE 2024 Task 4: Sound Event Detection with Heterogeneous Data and Missing Labels","abstract":"The Detection and Classification of Acoustic Scenes and Events Challenge Task 4 aims to advance sound event detection (SED) systems in domestic environments by leveraging training data with different supervision uncertainty. Participants are challenged in exploring how to best use training data from different domains and with varying annotation granularity (strong/weak temporal resolution, soft/hard labels), to obtain a robust SED system that can generalize across different scenarios. Crucially, annotation across available training datasets can be inconsistent and hence sound labels of one dataset may be present but not annotated in the other one and vice-versa. As such, systems will have to cope with potentially missing target labels during training. Moreover, as an additional novelty, systems will also be evaluated on labels with different granularity in order to assess their robustness for different applications. To lower the entry barrier for participants, we developed an updated baseline system with several caveats to address these aforementioned problems. Results with our baseline system indicate that this research direction is promising and is possible to obtain a stronger SED system by using diverse domain training data with missing labels compared to training a SED system for each domain separately.","sentences":["The Detection and Classification of Acoustic Scenes and Events Challenge Task 4 aims to advance sound event detection (SED) systems in domestic environments by leveraging training data with different supervision uncertainty.","Participants are challenged in exploring how to best use training data from different domains and with varying annotation granularity (strong/weak temporal resolution, soft/hard labels), to obtain a robust SED system that can generalize across different scenarios.","Crucially, annotation across available training datasets can be inconsistent and hence sound labels of one dataset may be present but not annotated in the other one and vice-versa.","As such, systems will have to cope with potentially missing target labels during training.","Moreover, as an additional novelty, systems will also be evaluated on labels with different granularity in order to assess their robustness for different applications.","To lower the entry barrier for participants, we developed an updated baseline system with several caveats to address these aforementioned problems.","Results with our baseline system indicate that this research direction is promising and is possible to obtain a stronger SED system by using diverse domain training data with missing labels compared to training a SED system for each domain separately."],"url":"http://arxiv.org/abs/2406.08056v1","category":"eess.AS"}
{"created":"2024-06-12 10:10:27","title":"A note for W^{1,p}(V) and W_0^{1,p}(V) on a locally finite graph","abstract":"In this paper, we investigate the Sobolev spaces W^{1,p}(V) and W_0^{1,p}(V) on a locally finite graph G=(V,E), which are fundamental tools when we apply the variational methods to partial differential equations on graphs. As a key contribution of this note, we show that in general, W^{1,p}(V) is not equivalent to W_0^{1,p}(V) on locally finite graphs, which is different from the situation on Euclidean space R^N.","sentences":["In this paper, we investigate the Sobolev spaces W^{1,p}(V) and W_0^{1,p}(V) on a locally finite graph G=(V,E), which are fundamental tools when we apply the variational methods to partial differential equations on graphs.","As a key contribution of this note, we show that in general, W^{1,p}(V) is not equivalent to W_0^{1,p}(V) on locally finite graphs, which is different from the situation on Euclidean space R^N."],"url":"http://arxiv.org/abs/2406.08053v1","category":"math.AP"}
{"created":"2024-06-12 10:07:40","title":"FakeSound: Deepfake General Audio Detection","abstract":"With the advancement of audio generation, generative models can produce highly realistic audios. However, the proliferation of deepfake general audio can pose negative consequences. Therefore, we propose a new task, deepfake general audio detection, which aims to identify whether audio content is manipulated and to locate deepfake regions. Leveraging an automated manipulation pipeline, a dataset named FakeSound for deepfake general audio detection is proposed, and samples can be viewed on website https://FakeSoundData.github.io. The average binary accuracy of humans on all test sets is consistently below 0.6, which indicates the difficulty humans face in discerning deepfake audio and affirms the efficacy of the FakeSound dataset. A deepfake detection model utilizing a general audio pre-trained model is proposed as a benchmark system. Experimental results demonstrate that the performance of the proposed model surpasses the state-of-the-art in deepfake speech detection and human testers.","sentences":["With the advancement of audio generation, generative models can produce highly realistic audios.","However, the proliferation of deepfake general audio can pose negative consequences.","Therefore, we propose a new task, deepfake general audio detection, which aims to identify whether audio content is manipulated and to locate deepfake regions.","Leveraging an automated manipulation pipeline, a dataset named FakeSound for deepfake general audio detection is proposed, and samples can be viewed on website https://FakeSoundData.github.io.","The average binary accuracy of humans on all test sets is consistently below 0.6, which indicates the difficulty humans face in discerning deepfake audio and affirms the efficacy of the FakeSound dataset.","A deepfake detection model utilizing a general audio pre-trained model is proposed as a benchmark system.","Experimental results demonstrate that the performance of the proposed model surpasses the state-of-the-art in deepfake speech detection and human testers."],"url":"http://arxiv.org/abs/2406.08052v1","category":"cs.SD"}
{"created":"2024-06-12 17:59:08","title":"Real3D: Scaling Up Large Reconstruction Models with Real-World Images","abstract":"The default strategy for training single-view Large Reconstruction Models (LRMs) follows the fully supervised route using large-scale datasets of synthetic 3D assets or multi-view captures. Although these resources simplify the training procedure, they are hard to scale up beyond the existing datasets and they are not necessarily representative of the real distribution of object shapes. To address these limitations, in this paper, we introduce Real3D, the first LRM system that can be trained using single-view real-world images. Real3D introduces a novel self-training framework that can benefit from both the existing synthetic data and diverse single-view real images. We propose two unsupervised losses that allow us to supervise LRMs at the pixel- and semantic-level, even for training examples without ground-truth 3D or novel views. To further improve performance and scale up the image data, we develop an automatic data curation approach to collect high-quality examples from in-the-wild images. Our experiments show that Real3D consistently outperforms prior work in four diverse evaluation settings that include real and synthetic data, as well as both in-domain and out-of-domain shapes. Code and model can be found here: https://hwjiang1510.github.io/Real3D/","sentences":["The default strategy for training single-view Large Reconstruction Models (LRMs) follows the fully supervised route using large-scale datasets of synthetic 3D assets or multi-view captures.","Although these resources simplify the training procedure, they are hard to scale up beyond the existing datasets and they are not necessarily representative of the real distribution of object shapes.","To address these limitations, in this paper, we introduce Real3D, the first LRM system that can be trained using single-view real-world images.","Real3D introduces a novel self-training framework that can benefit from both the existing synthetic data and diverse single-view real images.","We propose two unsupervised losses that allow us to supervise LRMs at the pixel- and semantic-level, even for training examples without ground-truth 3D or novel views.","To further improve performance and scale up the image data, we develop an automatic data curation approach to collect high-quality examples from in-the-wild images.","Our experiments show that Real3D consistently outperforms prior work in four diverse evaluation settings that include real and synthetic data, as well as both in-domain and out-of-domain shapes.","Code and model can be found here: https://hwjiang1510.github.io/Real3D/"],"url":"http://arxiv.org/abs/2406.08479v1","category":"cs.CV"}
{"created":"2024-06-12 17:15:25","title":"A Sticker is Worth a Thousand Words: Characterizing the Use of Stickers in WhatsApp Political Groups in Brazil","abstract":"With the increasing use of smartphones, instant messaging platforms turned into important communication tools. According to WhatsApp, more than 100 billion messages are sent each day on the app. Communication on these platforms has allowed individuals to express themselves in other types of media, rather than simple text, including audio, videos, images, and stickers. Particularly, stickers are a new multimedia format that emerged with messaging apps, promoting new forms of interactions among users, especially in the Brazilian context, transcending their role as a mere form of humor to become a key element in political strategy. In this regard, we investigate how stickers are being used, unveiling unique characteristics that these media bring to WhatsApp chats and the political use of this new media format. To achieve that, we collected a large sample of messages from WhatsApp public political discussion groups in Brazil and analyzed the sticker messages shared in this context","sentences":["With the increasing use of smartphones, instant messaging platforms turned into important communication tools.","According to WhatsApp, more than 100 billion messages are sent each day on the app.","Communication on these platforms has allowed individuals to express themselves in other types of media, rather than simple text, including audio, videos, images, and stickers.","Particularly, stickers are a new multimedia format that emerged with messaging apps, promoting new forms of interactions among users, especially in the Brazilian context, transcending their role as a mere form of humor to become a key element in political strategy.","In this regard, we investigate how stickers are being used, unveiling unique characteristics that these media bring to WhatsApp chats and the political use of this new media format.","To achieve that, we collected a large sample of messages from WhatsApp public political discussion groups in Brazil and analyzed the sticker messages shared in this context"],"url":"http://arxiv.org/abs/2406.08429v1","category":"cs.SI"}
{"created":"2024-06-12 15:08:15","title":"Measuring model variability using robust non-parametric testing","abstract":"Training a deep neural network often involves stochastic optimization, meaning each run will produce a different model. The seed used to initialize random elements of the optimization procedure heavily influences the quality of a trained model, which may be obscure from many commonly reported summary statistics, like accuracy. However, random seed is often not included in hyper-parameter optimization, perhaps because the relationship between seed and model quality is hard to describe. This work attempts to describe the relationship between deep net models trained with different random seeds and the behavior of the expected model. We adopt robust hypothesis testing to propose a novel summary statistic for network similarity, referred to as the $\\alpha$-trimming level. We use the $\\alpha$-trimming level to show that the empirical cumulative distribution function of an ensemble model created from a collection of trained models with different random seeds approximates the average of these functions as the number of models in the collection grows large. This insight provides guidance for how many random seeds should be sampled to ensure that an ensemble of these trained models is a reliable representative. We also show that the $\\alpha$-trimming level is more expressive than different performance metrics like validation accuracy, churn, or expected calibration error when taken alone and may help with random seed selection in a more principled fashion. We demonstrate the value of the proposed statistic in real experiments and illustrate the advantage of fine-tuning over random seed with an experiment in transfer learning.","sentences":["Training a deep neural network often involves stochastic optimization, meaning each run will produce a different model.","The seed used to initialize random elements of the optimization procedure heavily influences the quality of a trained model, which may be obscure from many commonly reported summary statistics, like accuracy.","However, random seed is often not included in hyper-parameter optimization, perhaps because the relationship between seed and model quality is hard to describe.","This work attempts to describe the relationship between deep net models trained with different random seeds and the behavior of the expected model.","We adopt robust hypothesis testing to propose a novel summary statistic for network similarity, referred to as the $\\alpha$-trimming level.","We use the $\\alpha$-trimming level to show that the empirical cumulative distribution function of an ensemble model created from a collection of trained models with different random seeds approximates the average of these functions as the number of models in the collection grows large.","This insight provides guidance for how many random seeds should be sampled to ensure that an ensemble of these trained models is a reliable representative.","We also show that the $\\alpha$-trimming level is more expressive than different performance metrics like validation accuracy, churn, or expected calibration error when taken alone and may help with random seed selection in a more principled fashion.","We demonstrate the value of the proposed statistic in real experiments and illustrate the advantage of fine-tuning over random seed with an experiment in transfer learning."],"url":"http://arxiv.org/abs/2406.08307v1","category":"stat.ML"}
{"created":"2024-06-12 14:56:42","title":"Collective Invasion: When does domain curvature matter?","abstract":"Real-world cellular invasion processes often take place in curved geometries. Such problems are frequently simplified in models to neglect the curved geometry in favour of computational simplicity, yet doing so risks inaccuracy in any model-based predictions. To quantify the conditions under which neglecting a curved geometry are justifiable, we examined solutions to the Fisher-Kolmogorov-Petrovsky-Piskunov (Fisher-KPP) model, a paradigm nonlinear reaction-diffusion equation typically used to model spatial invasion, on an annular geometry. Defining $\\epsilon$ as the ratio of the annulus thickness $\\delta$ and radius $r_0$ we derive, through an asymptotic expansion, the conditions under which it is appropriate to ignore the domain curvature, a result that generalises to other reaction-diffusion equations with constant diffusion coefficient. We further characterise the nature of the solutions through numerical simulation for different $r_0$ and $\\delta$. Thus, we quantify the size of the deviation from an analogous simulation on the rectangle, and how this deviation changes across the width of the annulus. Our results grant insight into when it is appropriate to neglect the domain curvature in studying travelling wave behaviour in reaction-diffusion equations.","sentences":["Real-world cellular invasion processes often take place in curved geometries.","Such problems are frequently simplified in models to neglect the curved geometry in favour of computational simplicity, yet doing so risks inaccuracy in any model-based predictions.","To quantify the conditions under which neglecting a curved geometry are justifiable, we examined solutions to the Fisher-Kolmogorov-Petrovsky-Piskunov (Fisher-KPP) model, a paradigm nonlinear reaction-diffusion equation typically used to model spatial invasion, on an annular geometry.","Defining $\\epsilon$ as the ratio of the annulus thickness $\\delta$ and radius $r_0$ we derive, through an asymptotic expansion, the conditions under which it is appropriate to ignore the domain curvature, a result that generalises to other reaction-diffusion equations with constant diffusion coefficient.","We further characterise the nature of the solutions through numerical simulation for different $r_0$ and $\\delta$.","Thus, we quantify the size of the deviation from an analogous simulation on the rectangle, and how this deviation changes across the width of the annulus.","Our results grant insight into when it is appropriate to neglect the domain curvature in studying travelling wave behaviour in reaction-diffusion equations."],"url":"http://arxiv.org/abs/2406.08291v1","category":"q-bio.CB"}
{"created":"2024-06-12 14:40:30","title":"Yellow hypergiant V509 Cas -- stable in the 'yellow void'","abstract":"The yellow hypergiant star V509 Cas is currently undergoing an extreme phase of evolution. Having experienced eruptive mass-loss outbursts in the 20th century, the star's effective temperature reached record high values in the early 2000s. However, since then, the star's behaviour has displayed an unprecedented level of stability. In spite of that, the star could be traversing through the 'yellow void' instability region. To describe the current evolutionary state of V509 Cas, we analysed its variability using photometric and spectroscopic data collected over recent years. By comparing our findings with historical records, we aim to determine whether the star's surface shows signs of stabilisation. Additionally, we investigate the variability of emission components in the wings of certain spectral lines to highlight the contribution of the circumstellar gaseous disc to this phenomenon. Our spectroscopic monitoring observations were carried out at Tartu Observatory over the course of seven years, supplemented by echelle spectra obtained at the Nordic Optical Telescope, as well as publicly available photometric data from Gaia, AAVSO, and AAVSO's Bright Star Monitor programme. We estimated the variability of effective temperature and radial velocity from the spectral time series and correlated it with the brightness variability of V509 Cas. The results indicate that the star's average brightness level has remained stable throughout the observed period, with an amplitude of variability ~0.1 mag. While the amplitude of short-term temperature fluctuations has decreased compared to the early 2000s, the variability of the radial velocity remains similar to historical values from the early 20th century. Moreover, we show how the variable radial velocity affects the emission components in some absorption lines (e.g. Sc II) and how that follows the hypothesis of a disc surrounding the star.","sentences":["The yellow hypergiant star V509 Cas is currently undergoing an extreme phase of evolution.","Having experienced eruptive mass-loss outbursts in the 20th century, the star's effective temperature reached record high values in the early 2000s.","However, since then, the star's behaviour has displayed an unprecedented level of stability.","In spite of that, the star could be traversing through the 'yellow void' instability region.","To describe the current evolutionary state of V509 Cas, we analysed its variability using photometric and spectroscopic data collected over recent years.","By comparing our findings with historical records, we aim to determine whether the star's surface shows signs of stabilisation.","Additionally, we investigate the variability of emission components in the wings of certain spectral lines to highlight the contribution of the circumstellar gaseous disc to this phenomenon.","Our spectroscopic monitoring observations were carried out at Tartu Observatory over the course of seven years, supplemented by echelle spectra obtained at the Nordic Optical Telescope, as well as publicly available photometric data from Gaia, AAVSO, and AAVSO's Bright Star Monitor programme.","We estimated the variability of effective temperature and radial velocity from the spectral time series and correlated it with the brightness variability of V509 Cas.","The results indicate that the star's average brightness level has remained stable throughout the observed period, with an amplitude of variability ~0.1 mag.","While the amplitude of short-term temperature fluctuations has decreased compared to the early 2000s, the variability of the radial velocity remains similar to historical values from the early 20th century.","Moreover, we show how the variable radial velocity affects the emission components in some absorption lines (e.g. Sc II) and how that follows the hypothesis of a disc surrounding the star."],"url":"http://arxiv.org/abs/2406.08276v1","category":"astro-ph.SR"}
{"created":"2024-06-12 14:38:34","title":"SonicID: User Identification on Smart Glasses with Acoustic Sensing","abstract":"Smart glasses have become more prevalent as they provide an increasing number of applications for users. They store various types of private information or can access it via connections established with other devices. Therefore, there is a growing need for user identification on smart glasses. In this paper, we introduce a low-power and minimally-obtrusive system called SonicID, designed to authenticate users on glasses. SonicID extracts unique biometric information from users by scanning their faces with ultrasonic waves and utilizes this information to distinguish between different users, powered by a customized binary classifier with the ResNet-18 architecture. SonicID can authenticate users within 0.12 seconds, with an energy consumption of 19.8 mAs per trial. A user study involving 24 participants confirms that SonicID achieves a true positive rate of 96.5%, a false positive rate of 4.1%, and a balanced accuracy of 96.2% using just 4 minutes of training data collected for each new user. This performance is relatively consistent across different remounting sessions and days. Given this promising performance, we further discuss the potential applications of SonicID and methods to improve its performance in the future.","sentences":["Smart glasses have become more prevalent as they provide an increasing number of applications for users.","They store various types of private information or can access it via connections established with other devices.","Therefore, there is a growing need for user identification on smart glasses.","In this paper, we introduce a low-power and minimally-obtrusive system called SonicID, designed to authenticate users on glasses.","SonicID extracts unique biometric information from users by scanning their faces with ultrasonic waves and utilizes this information to distinguish between different users, powered by a customized binary classifier with the ResNet-18 architecture.","SonicID can authenticate users within 0.12 seconds, with an energy consumption of 19.8 mAs per trial.","A user study involving 24 participants confirms that SonicID achieves a true positive rate of 96.5%, a false positive rate of 4.1%, and a balanced accuracy of 96.2% using just 4 minutes of training data collected for each new user.","This performance is relatively consistent across different remounting sessions and days.","Given this promising performance, we further discuss the potential applications of SonicID and methods to improve its performance in the future."],"url":"http://arxiv.org/abs/2406.08273v1","category":"cs.HC"}
{"created":"2024-06-12 13:54:25","title":"Observation of $\u03b7_{c}$(1S, 2S) and $\u03c7_{cJ}$ decays to 2$(\u03c0^{+}\u03c0^{-})\u03b7$ via $\u03c8$(3686) radiative transitions","abstract":"Based on $2.7 \\times 10^9~\\psi(3686)$ decays collected with the BESIII detector, the radiative decay $\\psi(3686)\\to\\gamma2(\\pi^{+}\\pi^{-})\\eta$ is investigated to measure properties of S- and P-wave charmonium states. The branching fraction of the decay $\\eta_{c}(1S) \\to 2(\\pi^{+}\\pi^{-})\\eta$, which is found to have a strong dependence on the interference pattern between $\\eta_c(1S)$ and non-$\\eta_c(1S)$ processes, is measured in both destructive and constructive interference scenarios for the first time. The mass and width of the $\\eta_{c}(1S)$ are measured to be $M=(2984.14 \\pm 0.13 \\pm 0.38)$ MeV/$c^{2}$ and $\\Gamma=(28.82 \\pm 0.11 \\pm 0.82)$ MeV, respectively. Clear signals for the decays of the $\\chi_{cJ}(J=0,1,2)$ and the $\\eta_{c}(2S)$ to $2(\\pi^{+}\\pi^{-})\\eta$ are also observed for the first time, and the corresponding branching fractions are measured. The ratio of the branching fractions between the $\\eta_{c}(2S)$ and $\\eta_{c}(1S)$ decays is significantly lower than the theoretical prediction, which might suggest different dynamics in their decays.","sentences":["Based on $2.7 \\times 10^9~\\psi(3686)$ decays collected with the BESIII detector, the radiative decay $\\psi(3686)\\to\\gamma2(\\pi^{+}\\pi^{-})\\eta$ is investigated to measure properties of S- and P-wave charmonium states.","The branching fraction of the decay $\\eta_{c}(1S) \\to 2(\\pi^{+}\\pi^{-})\\eta$, which is found to have a strong dependence on the interference pattern between $\\eta_c(1S)$ and non-$\\eta_c(1S)$ processes, is measured in both destructive and constructive interference scenarios for the first time.","The mass and width of the $\\eta_{c}(1S)$ are measured to be $M=(2984.14 \\pm 0.13 \\pm 0.38)$ MeV/$c^{2}$ and $\\Gamma=(28.82 \\pm 0.11 \\pm 0.82)$ MeV, respectively.","Clear signals for the decays of the $\\chi_{cJ}(J=0,1,2)$ and the $\\eta_{c}(2S)$ to $2(\\pi^{+}\\pi^{-})\\eta$ are also observed for the first time, and the corresponding branching fractions are measured.","The ratio of the branching fractions between the $\\eta_{c}(2S)$ and $\\eta_{c}(1S)$ decays is significantly lower than the theoretical prediction, which might suggest different dynamics in their decays."],"url":"http://arxiv.org/abs/2406.08225v1","category":"hep-ex"}
{"created":"2024-06-12 13:40:07","title":"Fluorescence enhancement of single V2 centers in a 4H-SiC cavity antenna","abstract":"Solid state quantum emitters are a prime candidate in distributed quantum technologies since they inherently provide a spin-photon interface. An ongoing challenge in the field, however, is the low photon extraction due to the high refractive index of typical host materials. This challenge can be overcome using photonic structures. Here, we report the integration of V2 centers in a cavity-based optical antenna. The structure consists of a silver-coated, 135 nm thin 4H-SiC membrane functioning as a planar cavity with a broadband resonance yielding a theoretical photon collection enhancement factor of 34. The planar geometry allows us to identify over 20 single V2 centers at room temperature with a mean (maximum) count rate enhancement factor of 9 (15). Moreover, we observe 10 V2 centers with a mean absorption linewidth below 80MHz at cryogenic temperatures. These results demonstrate a photon collection enhancement that is robust to the lateral emitter position.","sentences":["Solid state quantum emitters are a prime candidate in distributed quantum technologies since they inherently provide a spin-photon interface.","An ongoing challenge in the field, however, is the low photon extraction due to the high refractive index of typical host materials.","This challenge can be overcome using photonic structures.","Here, we report the integration of V2 centers in a cavity-based optical antenna.","The structure consists of a silver-coated, 135 nm thin 4H-SiC membrane functioning as a planar cavity with a broadband resonance yielding a theoretical photon collection enhancement factor of 34.","The planar geometry allows us to identify over 20 single V2 centers at room temperature with a mean (maximum) count rate enhancement factor of 9 (15).","Moreover, we observe 10 V2 centers with a mean absorption linewidth below 80MHz at cryogenic temperatures.","These results demonstrate a photon collection enhancement that is robust to the lateral emitter position."],"url":"http://arxiv.org/abs/2406.08208v1","category":"quant-ph"}
{"created":"2024-06-12 12:26:26","title":"The impact of deep learning aid on the workload and interpretation accuracy of radiologists on chest computed tomography: a cross-over reader study","abstract":"Interpretation of chest computed tomography (CT) is time-consuming. Previous studies have measured the time-saving effect of using a deep-learning-based aid (DLA) for CT interpretation. We evaluated the joint impact of a multi-pathology DLA on the time and accuracy of radiologists' reading.   40 radiologists were randomly split into three experimental arms: control (10), who interpret studies without assistance; informed group (10), who were briefed about DLA pathologies, but performed readings without it; and the experimental group (20), who interpreted half studies with DLA, and half without. Every arm used the same 200 CT studies retrospectively collected from BIMCV-COVID19 dataset; each radiologist provided readings for 20 CT studies. We compared interpretation time, and accuracy of participants diagnostic report with respect to 12 pathological findings.   Mean reading time per study was 15.6 minutes [SD 8.5] in the control arm, 13.2 minutes [SD 8.7] in the informed arm, 14.4 [SD 10.3] in the experimental arm without DLA, and 11.4 minutes [SD 7.8] in the experimental arm with DLA. Mean sensitivity and specificity were 41.5 [SD 30.4], 86.8 [SD 28.3] in the control arm; 53.5 [SD 22.7], 92.3 [SD 9.4] in the informed non-assisted arm; 63.2 [SD 16.4], 92.3 [SD 8.2] in the experimental arm without DLA; and 91.6 [SD 7.2], 89.9 [SD 6.0] in the experimental arm with DLA. DLA speed up interpretation time per study by 2.9 minutes (CI95 [1.7, 4.3], p<0.0005), increased sensitivity by 28.4 (CI95 [23.4, 33.4], p<0.0005), and decreased specificity by 2.4 (CI95 [0.6, 4.3], p=0.13).   Of 20 radiologists in the experimental arm, 16 have improved reading time and sensitivity, two improved their time with a marginal drop in sensitivity, and two participants improved sensitivity with increased time. Overall, DLA introduction decreased reading time by 20.6%.","sentences":["Interpretation of chest computed tomography (CT) is time-consuming.","Previous studies have measured the time-saving effect of using a deep-learning-based aid (DLA) for CT interpretation.","We evaluated the joint impact of a multi-pathology DLA on the time and accuracy of radiologists' reading.   ","40 radiologists were randomly split into three experimental arms: control (10), who interpret studies without assistance; informed group (10), who were briefed about DLA pathologies, but performed readings without it; and the experimental group (20), who interpreted half studies with DLA, and half without.","Every arm used the same 200 CT studies retrospectively collected from BIMCV-COVID19 dataset; each radiologist provided readings for 20 CT studies.","We compared interpretation time, and accuracy of participants diagnostic report with respect to 12 pathological findings.   ","Mean reading time per study was 15.6 minutes","[SD 8.5] in the control arm, 13.2 minutes [SD 8.7] in the informed arm, 14.4 [SD 10.3] in the experimental arm without DLA, and 11.4 minutes","[SD 7.8] in the experimental arm with DLA.","Mean sensitivity and specificity were 41.5","[SD 30.4], 86.8","[SD 28.3] in the control arm; 53.5 [SD 22.7], 92.3","[SD 9.4] in the informed non-assisted arm; 63.2 [SD 16.4], 92.3","[SD 8.2] in the experimental arm without DLA; and 91.6","[SD 7.2], 89.9 [SD 6.0] in the experimental arm with DLA.","DLA speed up interpretation time per study by 2.9 minutes (CI95 [1.7, 4.3], p<0.0005), increased sensitivity by 28.4 (CI95 [23.4, 33.4], p<0.0005), and decreased specificity by 2.4 (CI95 [0.6, 4.3], p=0.13).   ","Of 20 radiologists in the experimental arm, 16 have improved reading time and sensitivity, two improved their time with a marginal drop in sensitivity, and two participants improved sensitivity with increased time.","Overall, DLA introduction decreased reading time by 20.6%."],"url":"http://arxiv.org/abs/2406.08137v1","category":"eess.IV"}
{"created":"2024-06-12 12:02:15","title":"Fully Few-shot Class-incremental Audio Classification Using Expandable Dual-embedding Extractor","abstract":"It's assumed that training data is sufficient in base session of few-shot class-incremental audio classification. However, it's difficult to collect abundant samples for model training in base session in some practical scenarios due to the data scarcity of some classes. This paper explores a new problem of fully few-shot class-incremental audio classification with few training samples in all sessions. Moreover, we propose a method using expandable dual-embedding extractor to solve it. The proposed model consists of an embedding extractor and an expandable classifier. The embedding extractor consists of a pretrained Audio Spectrogram Transformer (AST) and a finetuned AST. The expandable classifier consists of prototypes and each prototype represents a class. Experiments are conducted on three datasets (LS-100, NSynth-100 and FSC-89). Results show that our method exceeds seven baseline ones in average accuracy with statistical significance. Code is at: https://github.com/YongjieSi/EDE.","sentences":["It's assumed that training data is sufficient in base session of few-shot class-incremental audio classification.","However, it's difficult to collect abundant samples for model training in base session in some practical scenarios due to the data scarcity of some classes.","This paper explores a new problem of fully few-shot class-incremental audio classification with few training samples in all sessions.","Moreover, we propose a method using expandable dual-embedding extractor to solve it.","The proposed model consists of an embedding extractor and an expandable classifier.","The embedding extractor consists of a pretrained Audio Spectrogram Transformer (AST) and a finetuned AST.","The expandable classifier consists of prototypes and each prototype represents a class.","Experiments are conducted on three datasets (LS-100, NSynth-100 and FSC-89).","Results show that our method exceeds seven baseline ones in average accuracy with statistical significance.","Code is at: https://github.com/YongjieSi/EDE."],"url":"http://arxiv.org/abs/2406.08122v1","category":"eess.AS"}
{"created":"2024-06-12 11:59:26","title":"Interlinking User Stories and GUI Prototyping: A Semi-Automatic LLM-based Approach","abstract":"Interactive systems are omnipresent today and the need to create graphical user interfaces (GUIs) is just as ubiquitous. For the elicitation and validation of requirements, GUI prototyping is a well-known and effective technique, typically employed after gathering initial user requirements represented in natural language (NL) (e.g., in the form of user stories). Unfortunately, GUI prototyping often requires extensive resources, resulting in a costly and time-consuming process. Despite various easy-to-use prototyping tools in practice, there is often a lack of adequate resources for developing GUI prototypes based on given user requirements. In this work, we present a novel Large Language Model (LLM)-based approach providing assistance for validating the implementation of functional NL-based requirements in a GUI prototype embedded in a prototyping tool. In particular, our approach aims to detect functional user stories that are not implemented in a GUI prototype and provides recommendations for suitable GUI components directly implementing the requirements. We collected requirements for existing GUIs in the form of user stories and evaluated our proposed validation and recommendation approach with this dataset. The obtained results are promising for user story validation and we demonstrate feasibility for the GUI component recommendations.","sentences":["Interactive systems are omnipresent today and the need to create graphical user interfaces (GUIs) is just as ubiquitous.","For the elicitation and validation of requirements, GUI prototyping is a well-known and effective technique, typically employed after gathering initial user requirements represented in natural language (NL) (e.g., in the form of user stories).","Unfortunately, GUI prototyping often requires extensive resources, resulting in a costly and time-consuming process.","Despite various easy-to-use prototyping tools in practice, there is often a lack of adequate resources for developing GUI prototypes based on given user requirements.","In this work, we present a novel Large Language Model (LLM)-based approach providing assistance for validating the implementation of functional NL-based requirements in a GUI prototype embedded in a prototyping tool.","In particular, our approach aims to detect functional user stories that are not implemented in a GUI prototype and provides recommendations for suitable GUI components directly implementing the requirements.","We collected requirements for existing GUIs in the form of user stories and evaluated our proposed validation and recommendation approach with this dataset.","The obtained results are promising for user story validation and we demonstrate feasibility for the GUI component recommendations."],"url":"http://arxiv.org/abs/2406.08120v1","category":"cs.SE"}
{"created":"2024-06-12 11:07:27","title":"Characterizing and Detecting Propaganda-Spreading Accounts on Telegram","abstract":"Information-based attacks on social media, such as disinformation campaigns and propaganda, are emerging cybersecurity threats. The security community has focused on countering these threats on social media platforms like X and Reddit. However, they also appear in instant-messaging social media platforms such as WhatsApp, Telegram, and Signal. In these platforms information-based attacks primarily happen in groups and channels, requiring manual moderation efforts by channel administrators. We collect, label, and analyze a large dataset of more than 17 million Telegram comments and messages. Our analysis uncovers two independent, coordinated networks that spread pro-Russian and pro-Ukrainian propaganda, garnering replies from real users. We propose a novel mechanism for detecting propaganda that capitalizes on the relationship between legitimate user messages and propaganda replies and is tailored to the information that Telegram makes available to moderators. Our method is faster, cheaper, and has a detection rate (97.6%) 11.6 percentage points higher than human moderators after seeing only one message from an account. It remains effective despite evolving propaganda.","sentences":["Information-based attacks on social media, such as disinformation campaigns and propaganda, are emerging cybersecurity threats.","The security community has focused on countering these threats on social media platforms like X and Reddit.","However, they also appear in instant-messaging social media platforms such as WhatsApp, Telegram, and Signal.","In these platforms information-based attacks primarily happen in groups and channels, requiring manual moderation efforts by channel administrators.","We collect, label, and analyze a large dataset of more than 17 million Telegram comments and messages.","Our analysis uncovers two independent, coordinated networks that spread pro-Russian and pro-Ukrainian propaganda, garnering replies from real users.","We propose a novel mechanism for detecting propaganda that capitalizes on the relationship between legitimate user messages and propaganda replies and is tailored to the information that Telegram makes available to moderators.","Our method is faster, cheaper, and has a detection rate (97.6%) 11.6 percentage points higher than human moderators after seeing only one message from an account.","It remains effective despite evolving propaganda."],"url":"http://arxiv.org/abs/2406.08084v1","category":"cs.SI"}
{"created":"2024-06-12 10:02:27","title":"Adversarial Evasion Attack Efficiency against Large Language Models","abstract":"Large Language Models (LLMs) are valuable for text classification, but their vulnerabilities must not be disregarded. They lack robustness against adversarial examples, so it is pertinent to understand the impacts of different types of perturbations, and assess if those attacks could be replicated by common users with a small amount of perturbations and a small number of queries to a deployed LLM. This work presents an analysis of the effectiveness, efficiency, and practicality of three different types of adversarial attacks against five different LLMs in a sentiment classification task. The obtained results demonstrated the very distinct impacts of the word-level and character-level attacks. The word attacks were more effective, but the character and more constrained attacks were more practical and required a reduced number of perturbations and queries. These differences need to be considered during the development of adversarial defense strategies to train more robust LLMs for intelligent text classification applications.","sentences":["Large Language Models (LLMs) are valuable for text classification, but their vulnerabilities must not be disregarded.","They lack robustness against adversarial examples, so it is pertinent to understand the impacts of different types of perturbations, and assess if those attacks could be replicated by common users with a small amount of perturbations and a small number of queries to a deployed LLM.","This work presents an analysis of the effectiveness, efficiency, and practicality of three different types of adversarial attacks against five different LLMs in a sentiment classification task.","The obtained results demonstrated the very distinct impacts of the word-level and character-level attacks.","The word attacks were more effective, but the character and more constrained attacks were more practical and required a reduced number of perturbations and queries.","These differences need to be considered during the development of adversarial defense strategies to train more robust LLMs for intelligent text classification applications."],"url":"http://arxiv.org/abs/2406.08050v1","category":"cs.CL"}
{"created":"2024-06-12 09:36:52","title":"LVBench: An Extreme Long Video Understanding Benchmark","abstract":"Recent progress in multimodal large language models has markedly enhanced the understanding of short videos (typically under one minute), and several evaluation datasets have emerged accordingly. However, these advancements fall short of meeting the demands of real-world applications such as embodied intelligence for long-term decision-making, in-depth movie reviews and discussions, and live sports commentary, all of which require comprehension of long videos spanning several hours. To address this gap, we introduce LVBench, a benchmark specifically designed for long video understanding. Our dataset comprises publicly sourced videos and encompasses a diverse set of tasks aimed at long video comprehension and information extraction. LVBench is designed to challenge multimodal models to demonstrate long-term memory and extended comprehension capabilities. Our extensive evaluations reveal that current multimodal models still underperform on these demanding long video understanding tasks. Through LVBench, we aim to spur the development of more advanced models capable of tackling the complexities of long video comprehension. Our data and code are publicly available at: https://lvbench.github.io.","sentences":["Recent progress in multimodal large language models has markedly enhanced the understanding of short videos (typically under one minute), and several evaluation datasets have emerged accordingly.","However, these advancements fall short of meeting the demands of real-world applications such as embodied intelligence for long-term decision-making, in-depth movie reviews and discussions, and live sports commentary, all of which require comprehension of long videos spanning several hours.","To address this gap, we introduce LVBench, a benchmark specifically designed for long video understanding.","Our dataset comprises publicly sourced videos and encompasses a diverse set of tasks aimed at long video comprehension and information extraction.","LVBench is designed to challenge multimodal models to demonstrate long-term memory and extended comprehension capabilities.","Our extensive evaluations reveal that current multimodal models still underperform on these demanding long video understanding tasks.","Through LVBench, we aim to spur the development of more advanced models capable of tackling the complexities of long video comprehension.","Our data and code are publicly available at: https://lvbench.github.io."],"url":"http://arxiv.org/abs/2406.08035v1","category":"cs.CV"}
{"created":"2024-06-12 09:31:52","title":"Deep Learning for Slum Mapping in Remote Sensing Images: A Meta-analysis and Review","abstract":"The major Sustainable Development Goals (SDG) 2030, set by the United Nations Development Program (UNDP), include sustainable cities and communities, no poverty, and reduced inequalities. However, millions of people live in slums or informal settlements with poor living conditions in many major cities around the world, especially in less developed countries. To emancipate these settlements and their inhabitants through government intervention, accurate data about slum location and extent is required. While ground survey data is the most reliable, such surveys are costly and time-consuming. An alternative is remotely sensed data obtained from very high-resolution (VHR) imagery. With the advancement of new technology, remote sensing based mapping of slums has emerged as a prominent research area. The parallel rise of Artificial Intelligence, especially Deep Learning has added a new dimension to this field as it allows automated analysis of satellite imagery to identify complex spatial patterns associated with slums. This article offers a detailed review and meta-analysis of research on slum mapping using remote sensing imagery from 2014 to 2024, with a special focus on deep learning approaches. Our analysis reveals a trend towards increasingly complex neural network architectures, with advancements in data preprocessing and model training techniques significantly enhancing slum identification accuracy. We have attempted to identify key methodologies that are effective across diverse geographic contexts. While acknowledging the transformative impact Convolutional Neural Networks (CNNs) in slum detection, our review underscores the absence of a universally optimal model, suggesting the need for context-specific adaptations. We also identify prevailing challenges in this field, such as data limitations and a lack of model explainability and suggest potential strategies for overcoming these.","sentences":["The major Sustainable Development Goals (SDG) 2030, set by the United Nations Development Program (UNDP), include sustainable cities and communities, no poverty, and reduced inequalities.","However, millions of people live in slums or informal settlements with poor living conditions in many major cities around the world, especially in less developed countries.","To emancipate these settlements and their inhabitants through government intervention, accurate data about slum location and extent is required.","While ground survey data is the most reliable, such surveys are costly and time-consuming.","An alternative is remotely sensed data obtained from very high-resolution (VHR) imagery.","With the advancement of new technology, remote sensing based mapping of slums has emerged as a prominent research area.","The parallel rise of Artificial Intelligence, especially Deep Learning has added a new dimension to this field as it allows automated analysis of satellite imagery to identify complex spatial patterns associated with slums.","This article offers a detailed review and meta-analysis of research on slum mapping using remote sensing imagery from 2014 to 2024, with a special focus on deep learning approaches.","Our analysis reveals a trend towards increasingly complex neural network architectures, with advancements in data preprocessing and model training techniques significantly enhancing slum identification accuracy.","We have attempted to identify key methodologies that are effective across diverse geographic contexts.","While acknowledging the transformative impact Convolutional Neural Networks (CNNs) in slum detection, our review underscores the absence of a universally optimal model, suggesting the need for context-specific adaptations.","We also identify prevailing challenges in this field, such as data limitations and a lack of model explainability and suggest potential strategies for overcoming these."],"url":"http://arxiv.org/abs/2406.08031v1","category":"cs.CV"}
{"created":"2024-06-12 09:29:59","title":"Metaverse Identity: Core Principles and Critical Challenges","abstract":"This paper explores the core principles that should guide the construction and governance of identity in the metaverse and identifies the critical challenges that need to be addressed. Drawing on multidisciplinary theories and perspectives, we propose two core principles for metaverse identity: \\emph{Equivalence and Alignment}, and \\emph{Fusion and Expansiveness}. The first principle contends that metaverse identities should be consistent with real-world identities in terms of norms and standards, which is crucial for establishing guidelines and safeguarding rights. The second principle emphasizes the necessity for seamless integration and boundless expansion of metaverse identities, transcending real-world limitations to accommodate diverse needs and foster inclusive participation. We argue that these two principles are vital for ensuring the accountability, inclusiveness, and consistency of identity in the metaverse. We also identify five critical challenges: Identity Interoperability, Legal Implications, Privacy and Identity Management, Deepfakes and Synthetic Identities, and Identity Fragmentation and Psychological Well-being. We discuss potential strategies to navigate these challenges. The paper concludes by underscoring the importance of a proactive and collaborative approach to shaping the future of metaverse identity. As the metaverse continues to evolve, it is imperative that we cultivate a thorough understanding of the principles and challenges surrounding identity in this uncharted territory and work collectively to build a metaverse that fosters responsible identity construction and expression.","sentences":["This paper explores the core principles that should guide the construction and governance of identity in the metaverse and identifies the critical challenges that need to be addressed.","Drawing on multidisciplinary theories and perspectives, we propose two core principles for metaverse identity: \\emph{Equivalence and Alignment}, and \\emph{Fusion and Expansiveness}.","The first principle contends that metaverse identities should be consistent with real-world identities in terms of norms and standards, which is crucial for establishing guidelines and safeguarding rights.","The second principle emphasizes the necessity for seamless integration and boundless expansion of metaverse identities, transcending real-world limitations to accommodate diverse needs and foster inclusive participation.","We argue that these two principles are vital for ensuring the accountability, inclusiveness, and consistency of identity in the metaverse.","We also identify five critical challenges: Identity Interoperability, Legal Implications, Privacy and Identity Management, Deepfakes and Synthetic Identities, and Identity Fragmentation and Psychological Well-being.","We discuss potential strategies to navigate these challenges.","The paper concludes by underscoring the importance of a proactive and collaborative approach to shaping the future of metaverse identity.","As the metaverse continues to evolve, it is imperative that we cultivate a thorough understanding of the principles and challenges surrounding identity in this uncharted territory and work collectively to build a metaverse that fosters responsible identity construction and expression."],"url":"http://arxiv.org/abs/2406.08029v1","category":"cs.CY"}
{"created":"2024-06-12 09:22:45","title":"Fewer Tokens and Fewer Videos: Extending Video Understanding Abilities in Large Vision-Language Models","abstract":"Amidst the advancements in image-based Large Vision-Language Models (image-LVLM), the transition to video-based models (video-LVLM) is hindered by the limited availability of quality video data. This paper addresses the challenge by leveraging the visual commonalities between images and videos to efficiently evolve image-LVLMs into video-LVLMs. We present a cost-effective video-LVLM that enhances model architecture, introduces innovative training strategies, and identifies the most effective types of video instruction data. Our innovative weighted token sampler significantly compresses the visual token numbers of each video frame, effectively cutting computational expenses. We also find that judiciously using just 10% of the video data, compared to prior video-LVLMs, yields impressive results during various training phases. Moreover, we delve into the influence of video instruction data in limited-resource settings, highlighting the significance of incorporating video training data that emphasizes temporal understanding to enhance model performance. The resulting Fewer Tokens and Fewer Videos LVLM (FTFV-LVLM) exhibits exceptional performance across video and image benchmarks, validating our model's design and training approaches.","sentences":["Amidst the advancements in image-based Large Vision-Language Models (image-LVLM), the transition to video-based models (video-LVLM) is hindered by the limited availability of quality video data.","This paper addresses the challenge by leveraging the visual commonalities between images and videos to efficiently evolve image-LVLMs into video-LVLMs.","We present a cost-effective video-LVLM that enhances model architecture, introduces innovative training strategies, and identifies the most effective types of video instruction data.","Our innovative weighted token sampler significantly compresses the visual token numbers of each video frame, effectively cutting computational expenses.","We also find that judiciously using just 10% of the video data, compared to prior video-LVLMs, yields impressive results during various training phases.","Moreover, we delve into the influence of video instruction data in limited-resource settings, highlighting the significance of incorporating video training data that emphasizes temporal understanding to enhance model performance.","The resulting Fewer Tokens and Fewer Videos LVLM (FTFV-LVLM) exhibits exceptional performance across video and image benchmarks, validating our model's design and training approaches."],"url":"http://arxiv.org/abs/2406.08024v1","category":"cs.CV"}
{"created":"2024-06-12 09:20:25","title":"SHACL2FOL: An FOL Toolkit for SHACL Decision Problems","abstract":"Recent studies on the Shapes Constraint Language (SHACL), a W3C specification for validating RDF graphs, rely on translating the language into first-order logic in order to provide formally-grounded solutions to the validation, containment and satisfiability decision problems. Continuing on this line of research, we introduce SHACL2FOL, the first automatic tool that (i) translates SHACL documents into FOL sentences and (ii) computes the answer to the two static analysis problems of satisfiability and containment; it also allow to test the validity of a graph with respect to a set of constraints. By integrating with existing theorem provers, such as E and Vampire, the tool computes the answer to the aforementioned decision problems and outputs the corresponding first-order logic theories in the standard TPTP format. We believe this tool can contribute to further theoretical studies of SHACL, by providing an automatic first-order logic interpretation of its semantics, while also benefiting SHACL practitioners, by supplying static analysis capabilities to help the creation and management of SHACL constraints.","sentences":["Recent studies on the Shapes Constraint Language (SHACL), a W3C specification for validating RDF graphs, rely on translating the language into first-order logic in order to provide formally-grounded solutions to the validation, containment and satisfiability decision problems.","Continuing on this line of research, we introduce SHACL2FOL, the first automatic tool that (i) translates SHACL documents into FOL sentences and (ii) computes the answer to the two static analysis problems of satisfiability and containment; it also allow to test the validity of a graph with respect to a set of constraints.","By integrating with existing theorem provers, such as E and Vampire, the tool computes the answer to the aforementioned decision problems and outputs the corresponding first-order logic theories in the standard TPTP format.","We believe this tool can contribute to further theoretical studies of SHACL, by providing an automatic first-order logic interpretation of its semantics, while also benefiting SHACL practitioners, by supplying static analysis capabilities to help the creation and management of SHACL constraints."],"url":"http://arxiv.org/abs/2406.08018v1","category":"cs.AI"}
{"created":"2024-06-12 08:59:33","title":"OpenObj: Open-Vocabulary Object-Level Neural Radiance Fields with Fine-Grained Understanding","abstract":"In recent years, there has been a surge of interest in open-vocabulary 3D scene reconstruction facilitated by visual language models (VLMs), which showcase remarkable capabilities in open-set retrieval. However, existing methods face some limitations: they either focus on learning point-wise features, resulting in blurry semantic understanding, or solely tackle object-level reconstruction, thereby overlooking the intricate details of the object's interior. To address these challenges, we introduce OpenObj, an innovative approach to build open-vocabulary object-level Neural Radiance Fields (NeRF) with fine-grained understanding. In essence, OpenObj establishes a robust framework for efficient and watertight scene modeling and comprehension at the object-level. Moreover, we incorporate part-level features into the neural fields, enabling a nuanced representation of object interiors. This approach captures object-level instances while maintaining a fine-grained understanding. The results on multiple datasets demonstrate that OpenObj achieves superior performance in zero-shot semantic segmentation and retrieval tasks. Additionally, OpenObj supports real-world robotics tasks at multiple scales, including global movement and local manipulation.","sentences":["In recent years, there has been a surge of interest in open-vocabulary 3D scene reconstruction facilitated by visual language models (VLMs), which showcase remarkable capabilities in open-set retrieval.","However, existing methods face some limitations: they either focus on learning point-wise features, resulting in blurry semantic understanding, or solely tackle object-level reconstruction, thereby overlooking the intricate details of the object's interior.","To address these challenges, we introduce OpenObj, an innovative approach to build open-vocabulary object-level Neural Radiance Fields (NeRF) with fine-grained understanding.","In essence, OpenObj establishes a robust framework for efficient and watertight scene modeling and comprehension at the object-level.","Moreover, we incorporate part-level features into the neural fields, enabling a nuanced representation of object interiors.","This approach captures object-level instances while maintaining a fine-grained understanding.","The results on multiple datasets demonstrate that OpenObj achieves superior performance in zero-shot semantic segmentation and retrieval tasks.","Additionally, OpenObj supports real-world robotics tasks at multiple scales, including global movement and local manipulation."],"url":"http://arxiv.org/abs/2406.08009v1","category":"cs.CV"}
{"created":"2024-06-12 08:48:06","title":"Efficient Adaptation in Mixed-Motive Environments via Hierarchical Opponent Modeling and Planning","abstract":"Despite the recent successes of multi-agent reinforcement learning (MARL) algorithms, efficiently adapting to co-players in mixed-motive environments remains a significant challenge. One feasible approach is to hierarchically model co-players' behavior based on inferring their characteristics. However, these methods often encounter difficulties in efficient reasoning and utilization of inferred information. To address these issues, we propose Hierarchical Opponent modeling and Planning (HOP), a novel multi-agent decision-making algorithm that enables few-shot adaptation to unseen policies in mixed-motive environments. HOP is hierarchically composed of two modules: an opponent modeling module that infers others' goals and learns corresponding goal-conditioned policies, and a planning module that employs Monte Carlo Tree Search (MCTS) to identify the best response. Our approach improves efficiency by updating beliefs about others' goals both across and within episodes and by using information from the opponent modeling module to guide planning. Experimental results demonstrate that in mixed-motive environments, HOP exhibits superior few-shot adaptation capabilities when interacting with various unseen agents, and excels in self-play scenarios. Furthermore, the emergence of social intelligence during our experiments underscores the potential of our approach in complex multi-agent environments.","sentences":["Despite the recent successes of multi-agent reinforcement learning (MARL) algorithms, efficiently adapting to co-players in mixed-motive environments remains a significant challenge.","One feasible approach is to hierarchically model co-players' behavior based on inferring their characteristics.","However, these methods often encounter difficulties in efficient reasoning and utilization of inferred information.","To address these issues, we propose Hierarchical Opponent modeling and Planning (HOP), a novel multi-agent decision-making algorithm that enables few-shot adaptation to unseen policies in mixed-motive environments.","HOP is hierarchically composed of two modules: an opponent modeling module that infers others' goals and learns corresponding goal-conditioned policies, and a planning module that employs Monte Carlo Tree Search (MCTS) to identify the best response.","Our approach improves efficiency by updating beliefs about others' goals both across and within episodes and by using information from the opponent modeling module to guide planning.","Experimental results demonstrate that in mixed-motive environments, HOP exhibits superior few-shot adaptation capabilities when interacting with various unseen agents, and excels in self-play scenarios.","Furthermore, the emergence of social intelligence during our experiments underscores the potential of our approach in complex multi-agent environments."],"url":"http://arxiv.org/abs/2406.08002v1","category":"cs.AI"}
{"created":"2024-06-12 08:26:30","title":"Blowfish: Topological and statistical signatures for quantifying ambiguity in semantic search","abstract":"This works reports evidence for the topological signatures of ambiguity in sentence embeddings that could be leveraged for ranking and/or explanation purposes in the context of vector search and Retrieval Augmented Generation (RAG) systems. We proposed a working definition of ambiguity and designed an experiment where we have broken down a proprietary dataset into collections of chunks of varying size - 3, 5, and 10 lines and used the different collections successively as queries and answers sets. It allowed us to test the signatures of ambiguity with removal of confounding factors. Our results show that proxy ambiguous queries (size 10 queries against size 3 documents) display different distributions of homologies 0 and 1 based features than proxy clear queries (size 5 queries against size 10 documents). We then discuss those results in terms increased manifold complexity and/or approximately discontinuous embedding submanifolds. Finally we propose a strategy to leverage those findings as a new scoring strategy of semantic similarities.","sentences":["This works reports evidence for the topological signatures of ambiguity in sentence embeddings that could be leveraged for ranking and/or explanation purposes in the context of vector search and Retrieval Augmented Generation (RAG) systems.","We proposed a working definition of ambiguity and designed an experiment where we have broken down a proprietary dataset into collections of chunks of varying size - 3, 5, and 10 lines and used the different collections successively as queries and answers sets.","It allowed us to test the signatures of ambiguity with removal of confounding factors.","Our results show that proxy ambiguous queries (size 10 queries against size 3 documents) display different distributions of homologies 0 and 1 based features than proxy clear queries (size 5 queries against size 10 documents).","We then discuss those results in terms increased manifold complexity and/or approximately discontinuous embedding submanifolds.","Finally we propose a strategy to leverage those findings as a new scoring strategy of semantic similarities."],"url":"http://arxiv.org/abs/2406.07990v1","category":"cs.LG"}
{"created":"2024-06-12 08:01:30","title":"Multivariate Log-based Anomaly Detection for Distributed Database","abstract":"Distributed databases are fundamental infrastructures of today's large-scale software systems such as cloud systems. Detecting anomalies in distributed databases is essential for maintaining software availability. Existing approaches, predominantly developed using Loghub-a comprehensive collection of log datasets from various systems-lack datasets specifically tailored to distributed databases, which exhibit unique anomalies. Additionally, there's a notable absence of datasets encompassing multi-anomaly, multi-node logs. Consequently, models built upon these datasets, primarily designed for standalone systems, are inadequate for distributed databases, and the prevalent method of deeming an entire cluster anomalous based on irregularities in a single node leads to a high false-positive rate. This paper addresses the unique anomalies and multivariate nature of logs in distributed databases. We expose the first open-sourced, comprehensive dataset with multivariate logs from distributed databases. Utilizing this dataset, we conduct an extensive study to identify multiple database anomalies and to assess the effectiveness of state-of-the-art anomaly detection using multivariate log data. Our findings reveal that relying solely on logs from a single node is insufficient for accurate anomaly detection on distributed database. Leveraging these insights, we propose MultiLog, an innovative multivariate log-based anomaly detection approach tailored for distributed databases. Our experiments, based on this novel dataset, demonstrate MultiLog's superiority, outperforming existing state-of-the-art methods by approximately 12%.","sentences":["Distributed databases are fundamental infrastructures of today's large-scale software systems such as cloud systems.","Detecting anomalies in distributed databases is essential for maintaining software availability.","Existing approaches, predominantly developed using Loghub-a comprehensive collection of log datasets from various systems-lack datasets specifically tailored to distributed databases, which exhibit unique anomalies.","Additionally, there's a notable absence of datasets encompassing multi-anomaly, multi-node logs.","Consequently, models built upon these datasets, primarily designed for standalone systems, are inadequate for distributed databases, and the prevalent method of deeming an entire cluster anomalous based on irregularities in a single node leads to a high false-positive rate.","This paper addresses the unique anomalies and multivariate nature of logs in distributed databases.","We expose the first open-sourced, comprehensive dataset with multivariate logs from distributed databases.","Utilizing this dataset, we conduct an extensive study to identify multiple database anomalies and to assess the effectiveness of state-of-the-art anomaly detection using multivariate log data.","Our findings reveal that relying solely on logs from a single node is insufficient for accurate anomaly detection on distributed database.","Leveraging these insights, we propose MultiLog, an innovative multivariate log-based anomaly detection approach tailored for distributed databases.","Our experiments, based on this novel dataset, demonstrate MultiLog's superiority, outperforming existing state-of-the-art methods by approximately 12%."],"url":"http://arxiv.org/abs/2406.07976v1","category":"cs.SE"}
{"created":"2024-06-12 08:00:37","title":"FINER: Far-Infrared Nebular Emission Receiver for the Large Millimeter Telescope","abstract":"Unveiling the emergence and prevalence of massive/bright galaxies during the epoch of reionization and beyond, within the first 600 million years of the Universe, stands as a pivotal pursuit in astronomy. Remarkable progress has been made by JWST in identifying an immense population of bright galaxies, which hints at exceptionally efficient galaxy assembly processes. However, the underlying physical mechanisms propelling their rapid growth remain unclear. With this in mind, millimeter and submillimeter-wave spectroscopic observations of redshifted far-infrared spectral lines, particularly the [O III] 88 micron and [C II] 158 micron lines, offers a crucial pathway to address this fundamental query.   To this end, we develop a dual-polarization sideband-separating superconductor-insulator-superconductor (SIS) mixer receiver, FINER, for the Large Millimeter Telescope (LMT) situated in Mexico. Harnessing advancements from ALMA's wideband sensitivity upgrade (WSU) technology, FINER covers radio frequencies spanning 120-360 GHz, delivering an instantaneous intermediate frequency (IF) of 3-21 GHz per sideband per polarization, which is followed by a set of 10.24 GHz-wide digital spectrometers. At 40% of ALMA's light-collecting area, the LMT's similar atmospheric transmittance and FINER's 5 times wider bandwidth compared to ALMA culminate in an unparalleled spectral scanning capability in the northern hemisphere, paving the way for finer spectral-resolution detection of distant galaxies.","sentences":["Unveiling the emergence and prevalence of massive/bright galaxies during the epoch of reionization and beyond, within the first 600 million years of the Universe, stands as a pivotal pursuit in astronomy.","Remarkable progress has been made by JWST in identifying an immense population of bright galaxies, which hints at exceptionally efficient galaxy assembly processes.","However, the underlying physical mechanisms propelling their rapid growth remain unclear.","With this in mind, millimeter and submillimeter-wave spectroscopic observations of redshifted far-infrared spectral lines, particularly the [O III] 88 micron and [C II] 158 micron lines, offers a crucial pathway to address this fundamental query.   ","To this end, we develop a dual-polarization sideband-separating superconductor-insulator-superconductor (SIS) mixer receiver, FINER, for the Large Millimeter Telescope (LMT) situated in Mexico.","Harnessing advancements from ALMA's wideband sensitivity upgrade (WSU) technology, FINER covers radio frequencies spanning 120-360 GHz, delivering an instantaneous intermediate frequency (IF) of 3-21 GHz per sideband per polarization, which is followed by a set of 10.24 GHz-wide digital spectrometers.","At 40% of ALMA's light-collecting area, the LMT's similar atmospheric transmittance and FINER's 5 times wider bandwidth compared to ALMA culminate in an unparalleled spectral scanning capability in the northern hemisphere, paving the way for finer spectral-resolution detection of distant galaxies."],"url":"http://arxiv.org/abs/2406.07975v1","category":"astro-ph.IM"}
{"created":"2024-06-12 08:00:10","title":"Development Status of Wideband Millimeter-Wave Receivers for LMT-FINER","abstract":"Spectroscopic observations of the far-infrared [O III] and [C II] lines present a pathway to explore the mechanisms of the emergence of massive galaxies in the epoch of reionization and beyond, which is one of the most fundamental questions in astronomy. To address this question, the Far-Infrared Nebular Emission Receiver (FINER) project is developing two wideband dual-polarization sideband-separating heterodyne receivers at 120--210 GHz and 210--360 GHz for the Large Millimeter Telescope (LMT) in Mexico. Compared with Atacama Large Millimeter/submillimeter Array (ALMA), LMT provides 40% of ALMA's light-collecting area and a similar atmospheric transmittance, but FINER plans to have an instantaneous intermediate frequency (IF) of 3--21 GHz per sideband per polarization which is five times wider than current ALMA's bandwidth. Therefore, FINER is going to offer cutting-edge spectral scanning capability in the next several years.   The project is currently in an active development phase. In this proceeding, the latest development status for FINER, including the optics, wideband waveguide components as well as low-noise superconductor-insulator-superconductor (SIS) mixers is reported.","sentences":["Spectroscopic observations of the far-infrared [O III] and [C II] lines present a pathway to explore the mechanisms of the emergence of massive galaxies in the epoch of reionization and beyond, which is one of the most fundamental questions in astronomy.","To address this question, the Far-Infrared Nebular Emission Receiver (FINER) project is developing two wideband dual-polarization sideband-separating heterodyne receivers at 120--210 GHz and 210--360 GHz for the Large Millimeter Telescope (LMT) in Mexico.","Compared with Atacama Large Millimeter/submillimeter Array (ALMA), LMT provides 40% of ALMA's light-collecting area and a similar atmospheric transmittance, but FINER plans to have an instantaneous intermediate frequency (IF) of 3--21 GHz per sideband per polarization which is five times wider than current ALMA's bandwidth.","Therefore, FINER is going to offer cutting-edge spectral scanning capability in the next several years.   ","The project is currently in an active development phase.","In this proceeding, the latest development status for FINER, including the optics, wideband waveguide components as well as low-noise superconductor-insulator-superconductor (SIS) mixers is reported."],"url":"http://arxiv.org/abs/2406.07974v1","category":"astro-ph.IM"}
{"created":"2024-06-12 07:55:32","title":"Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey","abstract":"With the rapid development of artificial intelligence, large language models (LLMs) have made remarkable progress in natural language processing. These models are trained on large amounts of data to demonstrate powerful language understanding and generation capabilities for various applications, from machine translation and chatbots to agents. However, LLMs have exposed a variety of privacy and security issues during their life cycle, which have become the focus of academic and industrial attention. Moreover, these risks LLMs face are pretty different from previous traditional language models. Since current surveys lack a clear taxonomy of unique threat models based on diverse scenarios, we highlight unique privacy and security issues based on five scenarios: pre-training, fine-tuning, RAG system, deploying, and LLM-based agent. Concerning the characteristics of each risk, this survey provides potential threats and countermeasures. The research on attack and defense situations LLMs face can provide feasible research directions, making more areas reap LLMs' benefits.","sentences":["With the rapid development of artificial intelligence, large language models (LLMs) have made remarkable progress in natural language processing.","These models are trained on large amounts of data to demonstrate powerful language understanding and generation capabilities for various applications, from machine translation and chatbots to agents.","However, LLMs have exposed a variety of privacy and security issues during their life cycle, which have become the focus of academic and industrial attention.","Moreover, these risks LLMs face are pretty different from previous traditional language models.","Since current surveys lack a clear taxonomy of unique threat models based on diverse scenarios, we highlight unique privacy and security issues based on five scenarios: pre-training, fine-tuning, RAG system, deploying, and LLM-based agent.","Concerning the characteristics of each risk, this survey provides potential threats and countermeasures.","The research on attack and defense situations LLMs face can provide feasible research directions, making more areas reap LLMs' benefits."],"url":"http://arxiv.org/abs/2406.07973v1","category":"cs.CR"}
{"created":"2024-06-12 07:52:17","title":"It Takes Two: On the Seamlessness between Reward and Policy Model in RLHF","abstract":"Reinforcement Learning from Human Feedback (RLHF) involves training policy models (PMs) and reward models (RMs) to align language models with human preferences. Instead of focusing solely on PMs and RMs independently, we propose to examine their interactions during fine-tuning, introducing the concept of seamlessness. Our study starts with observing the saturation phenomenon, where continual improvements in RM and PM do not translate into RLHF progress. Our analysis shows that RMs fail to assign proper scores to PM responses, resulting in a 35% mismatch rate with human preferences, highlighting a significant discrepancy between PM and RM. To measure seamlessness between PM and RM without human effort, we propose an automatic metric, SEAM. SEAM quantifies the discrepancies between PM and RM judgments induced by data samples. We validate the effectiveness of SEAM in data selection and model augmentation. Our experiments demonstrate that (1) using SEAM-filtered data for RL training improves RLHF performance by 4.5%, and (2) SEAM-guided model augmentation results in a 4% performance improvement over standard augmentation methods.","sentences":["Reinforcement Learning from Human Feedback (RLHF) involves training policy models (PMs) and reward models (RMs) to align language models with human preferences.","Instead of focusing solely on PMs and RMs independently, we propose to examine their interactions during fine-tuning, introducing the concept of seamlessness.","Our study starts with observing the saturation phenomenon, where continual improvements in RM and PM do not translate into RLHF progress.","Our analysis shows that RMs fail to assign proper scores to PM responses, resulting in a 35% mismatch rate with human preferences, highlighting a significant discrepancy between PM and RM.","To measure seamlessness between PM and RM without human effort, we propose an automatic metric, SEAM.","SEAM quantifies the discrepancies between PM and RM judgments induced by data samples.","We validate the effectiveness of SEAM in data selection and model augmentation.","Our experiments demonstrate that (1) using SEAM-filtered data for RL training improves RLHF performance by 4.5%, and (2) SEAM-guided model augmentation results in a 4% performance improvement over standard augmentation methods."],"url":"http://arxiv.org/abs/2406.07971v1","category":"cs.CL"}
{"created":"2024-06-12 07:42:12","title":"Political Leaning Inference through Plurinational Scenarios","abstract":"Social media users express their political preferences via interaction with other users, by spontaneous declarations or by participation in communities within the network. This makes a social network such as Twitter a valuable data source to study computational science approaches to political learning inference. In this work we focus on three diverse regions in Spain (Basque Country, Catalonia and Galicia) to explore various methods for multi-party categorization, required to analyze evolving and complex political landscapes, and compare it with binary left-right approaches. We use a two-step method involving unsupervised user representations obtained from the retweets and their subsequent use for political leaning detection. Comprehensive experimentation on a newly collected and curated dataset comprising labeled users and their interactions demonstrate the effectiveness of using Relational Embeddings as representation method for political ideology detection in both binary and multi-party frameworks, even with limited training data. Finally, data visualization illustrates the ability of the Relational Embeddings to capture intricate intra-group and inter-group political affinities.","sentences":["Social media users express their political preferences via interaction with other users, by spontaneous declarations or by participation in communities within the network.","This makes a social network such as Twitter a valuable data source to study computational science approaches to political learning inference.","In this work we focus on three diverse regions in Spain (Basque Country, Catalonia and Galicia) to explore various methods for multi-party categorization, required to analyze evolving and complex political landscapes, and compare it with binary left-right approaches.","We use a two-step method involving unsupervised user representations obtained from the retweets and their subsequent use for political leaning detection.","Comprehensive experimentation on a newly collected and curated dataset comprising labeled users and their interactions demonstrate the effectiveness of using Relational Embeddings as representation method for political ideology detection in both binary and multi-party frameworks, even with limited training data.","Finally, data visualization illustrates the ability of the Relational Embeddings to capture intricate intra-group and inter-group political affinities."],"url":"http://arxiv.org/abs/2406.07964v1","category":"cs.SI"}
{"created":"2024-06-12 07:41:44","title":"Toward a Method to Generate Capability Ontologies from Natural Language Descriptions","abstract":"To achieve a flexible and adaptable system, capability ontologies are increasingly leveraged to describe functions in a machine-interpretable way. However, modeling such complex ontological descriptions is still a manual and error-prone task that requires a significant amount of effort and ontology expertise. This contribution presents an innovative method to automate capability ontology modeling using Large Language Models (LLMs), which have proven to be well suited for such tasks. Our approach requires only a natural language description of a capability, which is then automatically inserted into a predefined prompt using a few-shot prompting technique. After prompting an LLM, the resulting capability ontology is automatically verified through various steps in a loop with the LLM to check the overall correctness of the capability ontology. First, a syntax check is performed, then a check for contradictions, and finally a check for hallucinations and missing ontology elements. Our method greatly reduces manual effort, as only the initial natural language description and a final human review and possible correction are necessary, thereby streamlining the capability ontology generation process.","sentences":["To achieve a flexible and adaptable system, capability ontologies are increasingly leveraged to describe functions in a machine-interpretable way.","However, modeling such complex ontological descriptions is still a manual and error-prone task that requires a significant amount of effort and ontology expertise.","This contribution presents an innovative method to automate capability ontology modeling using Large Language Models (LLMs), which have proven to be well suited for such tasks.","Our approach requires only a natural language description of a capability, which is then automatically inserted into a predefined prompt using a few-shot prompting technique.","After prompting an LLM, the resulting capability ontology is automatically verified through various steps in a loop with the LLM to check the overall correctness of the capability ontology.","First, a syntax check is performed, then a check for contradictions, and finally a check for hallucinations and missing ontology elements.","Our method greatly reduces manual effort, as only the initial natural language description and a final human review and possible correction are necessary, thereby streamlining the capability ontology generation process."],"url":"http://arxiv.org/abs/2406.07962v1","category":"cs.AI"}
{"created":"2024-06-12 07:41:00","title":"Accurate Explanation Model for Image Classifiers using Class Association Embedding","abstract":"Image classification is a primary task in data analysis where explainable models are crucially demanded in various applications. Although amounts of methods have been proposed to obtain explainable knowledge from the black-box classifiers, these approaches lack the efficiency of extracting global knowledge regarding the classification task, thus is vulnerable to local traps and often leads to poor accuracy. In this study, we propose a generative explanation model that combines the advantages of global and local knowledge for explaining image classifiers. We develop a representation learning method called class association embedding (CAE), which encodes each sample into a pair of separated class-associated and individual codes. Recombining the individual code of a given sample with altered class-associated code leads to a synthetic real-looking sample with preserved individual characters but modified class-associated features and possibly flipped class assignments. A building-block coherency feature extraction algorithm is proposed that efficiently separates class-associated features from individual ones. The extracted feature space forms a low-dimensional manifold that visualizes the classification decision patterns. Explanation on each individual sample can be then achieved in a counter-factual generation manner which continuously modifies the sample in one direction, by shifting its class-associated code along a guided path, until its classification outcome is changed. We compare our method with state-of-the-art ones on explaining image classification tasks in the form of saliency maps, demonstrating that our method achieves higher accuracies. The code is available at https://github.com/xrt11/XAI-CODE.","sentences":["Image classification is a primary task in data analysis where explainable models are crucially demanded in various applications.","Although amounts of methods have been proposed to obtain explainable knowledge from the black-box classifiers, these approaches lack the efficiency of extracting global knowledge regarding the classification task, thus is vulnerable to local traps and often leads to poor accuracy.","In this study, we propose a generative explanation model that combines the advantages of global and local knowledge for explaining image classifiers.","We develop a representation learning method called class association embedding (CAE), which encodes each sample into a pair of separated class-associated and individual codes.","Recombining the individual code of a given sample with altered class-associated code leads to a synthetic real-looking sample with preserved individual characters but modified class-associated features and possibly flipped class assignments.","A building-block coherency feature extraction algorithm is proposed that efficiently separates class-associated features from individual ones.","The extracted feature space forms a low-dimensional manifold that visualizes the classification decision patterns.","Explanation on each individual sample can be then achieved in a counter-factual generation manner which continuously modifies the sample in one direction, by shifting its class-associated code along a guided path, until its classification outcome is changed.","We compare our method with state-of-the-art ones on explaining image classification tasks in the form of saliency maps, demonstrating that our method achieves higher accuracies.","The code is available at https://github.com/xrt11/XAI-CODE."],"url":"http://arxiv.org/abs/2406.07961v1","category":"cs.CV"}
{"created":"2024-06-12 07:27:28","title":"Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition","abstract":"Large language model systems face important security risks from maliciously crafted messages that aim to overwrite the system's original instructions or leak private data. To study this problem, we organized a capture-the-flag competition at IEEE SaTML 2024, where the flag is a secret string in the LLM system prompt. The competition was organized in two phases. In the first phase, teams developed defenses to prevent the model from leaking the secret. During the second phase, teams were challenged to extract the secrets hidden for defenses proposed by the other teams. This report summarizes the main insights from the competition. Notably, we found that all defenses were bypassed at least once, highlighting the difficulty of designing a successful defense and the necessity for additional research to protect LLM systems. To foster future research in this direction, we compiled a dataset with over 137k multi-turn attack chats and open-sourced the platform.","sentences":["Large language model systems face important security risks from maliciously crafted messages that aim to overwrite the system's original instructions or leak private data.","To study this problem, we organized a capture-the-flag competition at IEEE SaTML 2024, where the flag is a secret string in the LLM system prompt.","The competition was organized in two phases.","In the first phase, teams developed defenses to prevent the model from leaking the secret.","During the second phase, teams were challenged to extract the secrets hidden for defenses proposed by the other teams.","This report summarizes the main insights from the competition.","Notably, we found that all defenses were bypassed at least once, highlighting the difficulty of designing a successful defense and the necessity for additional research to protect LLM systems.","To foster future research in this direction, we compiled a dataset with over 137k multi-turn attack chats and open-sourced the platform."],"url":"http://arxiv.org/abs/2406.07954v1","category":"cs.CR"}
{"created":"2024-06-12 07:13:11","title":"Ents: An Efficient Three-party Training Framework for Decision Trees by Communication Optimization","abstract":"Multi-party training frameworks for decision trees based on secure multi-party computation enable multiple parties to train high-performance models on distributed private data with privacy preservation. The training process essentially involves frequent dataset splitting according to the splitting criterion (e.g. Gini impurity). However, existing multi-party training frameworks for decision trees demonstrate communication inefficiency due to the following issues: (1) They suffer from huge communication overhead in securely splitting a dataset with continuous attributes. (2) They suffer from huge communication overhead due to performing almost all the computations on a large ring to accommodate the secure computations for the splitting criterion.   In this paper, we are motivated to present an efficient three-party training framework, namely Ents, for decision trees by communication optimization. For the first issue, we present a series of training protocols based on the secure radix sort protocols to efficiently and securely split a dataset with continuous attributes. For the second issue, we propose an efficient share conversion protocol to convert shares between a small ring and a large ring to reduce the communication overhead incurred by performing almost all the computations on a large ring. Experimental results from eight widely used datasets show that Ents outperforms state-of-the-art frameworks by $5.5\\times \\sim 9.3\\times$ in communication sizes and $3.9\\times \\sim 5.3\\times$ in communication rounds. In terms of training time, Ents yields an improvement of $3.5\\times \\sim 6.7\\times$. To demonstrate its practicality, Ents requires less than three hours to securely train a decision tree on a widely used real-world dataset (Skin Segmentation) with more than 245,000 samples in the WAN setting.","sentences":["Multi-party training frameworks for decision trees based on secure multi-party computation enable multiple parties to train high-performance models on distributed private data with privacy preservation.","The training process essentially involves frequent dataset splitting according to the splitting criterion (e.g. Gini impurity).","However, existing multi-party training frameworks for decision trees demonstrate communication inefficiency due to the following issues: (1) They suffer from huge communication overhead in securely splitting a dataset with continuous attributes.","(2) They suffer from huge communication overhead due to performing almost all the computations on a large ring to accommodate the secure computations for the splitting criterion.   ","In this paper, we are motivated to present an efficient three-party training framework, namely Ents, for decision trees by communication optimization.","For the first issue, we present a series of training protocols based on the secure radix sort protocols to efficiently and securely split a dataset with continuous attributes.","For the second issue, we propose an efficient share conversion protocol to convert shares between a small ring and a large ring to reduce the communication overhead incurred by performing almost all the computations on a large ring.","Experimental results from eight widely used datasets show that Ents outperforms state-of-the-art frameworks by $5.5\\times \\sim 9.3\\times$ in communication sizes and $3.9\\times \\sim 5.3\\times$ in communication rounds.","In terms of training time, Ents yields an improvement of $3.5\\times \\sim 6.7\\times$. To demonstrate its practicality, Ents requires less than three hours to securely train a decision tree on a widely used real-world dataset (Skin Segmentation) with more than 245,000 samples in the WAN setting."],"url":"http://arxiv.org/abs/2406.07948v1","category":"cs.CR"}
{"created":"2024-06-12 07:06:38","title":"DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis","abstract":"Testing is a major approach to ensuring the quality of deep learning (DL) libraries. Existing testing techniques commonly adopt differential testing to relieve the need for test oracle construction. However, these techniques are limited in finding implementations that offer the same functionality and generating diverse test inputs for differential testing. This paper introduces DLLens, a novel differential testing technique for DL library testing. Our insight is that APIs in different DL libraries are commonly designed to accomplish various computations for the same set of published DL algorithms. Although the mapping of these APIs is not often one-to-one, we observe that their computations can be mutually simulated after proper composition and adaptation. The use of these simulation counterparts facilitates differential testing for the detection of functional DL library bugs. Leveraging the insight, we propose DLLens as a novel mechanism that utilizes a large language model (LLM) to synthesize valid counterparts of DL library APIs. To generate diverse test inputs, DLLens incorporates a static analysis method aided by LLM to extract path constraints from all execution paths in each API and its counterpart's implementations. These path constraints are then used to guide the generation of diverse test inputs. We evaluate DLLens on two popular DL libraries, TensorFlow and PyTorch. Our evaluation shows that DLLens can synthesize counterparts for more than twice as many APIs found by state-of-the-art techniques on these libraries. Moreover, DLLens can extract 26.7% more constraints and detect 2.5 times as many bugs as state-of-the-art techniques. DLLens has successfully found 56 bugs in recent TensorFlow and PyTorch libraries. Among them, 41 are previously unknown, 39 of which have been confirmed by developers after reporting, and 19 of those confirmed bugs have been fixed by developers.","sentences":["Testing is a major approach to ensuring the quality of deep learning (DL) libraries.","Existing testing techniques commonly adopt differential testing to relieve the need for test oracle construction.","However, these techniques are limited in finding implementations that offer the same functionality and generating diverse test inputs for differential testing.","This paper introduces DLLens, a novel differential testing technique for DL library testing.","Our insight is that APIs in different DL libraries are commonly designed to accomplish various computations for the same set of published DL algorithms.","Although the mapping of these APIs is not often one-to-one, we observe that their computations can be mutually simulated after proper composition and adaptation.","The use of these simulation counterparts facilitates differential testing for the detection of functional DL library bugs.","Leveraging the insight, we propose DLLens as a novel mechanism that utilizes a large language model (LLM) to synthesize valid counterparts of DL library APIs.","To generate diverse test inputs, DLLens incorporates a static analysis method aided by LLM to extract path constraints from all execution paths in each API and its counterpart's implementations.","These path constraints are then used to guide the generation of diverse test inputs.","We evaluate DLLens on two popular DL libraries, TensorFlow and PyTorch.","Our evaluation shows that DLLens can synthesize counterparts for more than twice as many APIs found by state-of-the-art techniques on these libraries.","Moreover, DLLens can extract 26.7% more constraints and detect 2.5 times as many bugs as state-of-the-art techniques.","DLLens has successfully found 56 bugs in recent TensorFlow and PyTorch libraries.","Among them, 41 are previously unknown, 39 of which have been confirmed by developers after reporting, and 19 of those confirmed bugs have been fixed by developers."],"url":"http://arxiv.org/abs/2406.07944v1","category":"cs.SE"}
{"created":"2024-06-12 06:59:31","title":"Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable NLG Evaluation","abstract":"Human evaluation serves as the gold standard for assessing the quality of Natural Language Generation (NLG) systems. Nevertheless, the evaluation guideline, as a pivotal element ensuring reliable and reproducible human assessment, has received limited attention.Our investigation revealed that only 29.84% of recent papers involving human evaluation at top conferences release their evaluation guidelines, with vulnerabilities identified in 77.09% of these guidelines. Unreliable evaluation guidelines can yield inaccurate assessment outcomes, potentially impeding the advancement of NLG in the right direction. To address these challenges, we take an initial step towards reliable evaluation guidelines and propose the first human evaluation guideline dataset by collecting annotations of guidelines extracted from existing papers as well as generated via Large Language Models (LLMs). We then introduce a taxonomy of eight vulnerabilities and formulate a principle for composing evaluation guidelines. Furthermore, a method for detecting guideline vulnerabilities has been explored using LLMs, and we offer a set of recommendations to enhance reliability in human evaluation. The annotated human evaluation guideline dataset and code for the vulnerability detection method are publicly available online.","sentences":["Human evaluation serves as the gold standard for assessing the quality of Natural Language Generation (NLG) systems.","Nevertheless, the evaluation guideline, as a pivotal element ensuring reliable and reproducible human assessment, has received limited attention.","Our investigation revealed that only 29.84% of recent papers involving human evaluation at top conferences release their evaluation guidelines, with vulnerabilities identified in 77.09% of these guidelines.","Unreliable evaluation guidelines can yield inaccurate assessment outcomes, potentially impeding the advancement of NLG in the right direction.","To address these challenges, we take an initial step towards reliable evaluation guidelines and propose the first human evaluation guideline dataset by collecting annotations of guidelines extracted from existing papers as well as generated via Large Language Models (LLMs).","We then introduce a taxonomy of eight vulnerabilities and formulate a principle for composing evaluation guidelines.","Furthermore, a method for detecting guideline vulnerabilities has been explored using LLMs, and we offer a set of recommendations to enhance reliability in human evaluation.","The annotated human evaluation guideline dataset and code for the vulnerability detection method are publicly available online."],"url":"http://arxiv.org/abs/2406.07935v1","category":"cs.CL"}
{"created":"2024-06-12 06:56:20","title":"Large Language Model Unlearning via Embedding-Corrupted Prompts","abstract":"Large language models (LLMs) have advanced to encompass extensive knowledge across diverse domains. Yet controlling what a large language model should not know is important for ensuring alignment and thus safe use. However, accurately and efficiently unlearning knowledge from an LLM remains challenging due to the potential collateral damage caused by the fuzzy boundary between retention and forgetting, and the large computational requirements for optimization across state-of-the-art models with hundreds of billions of parameters. In this work, we present Embedding-COrrupted (ECO) Prompts, a lightweight unlearning framework for large language models to address both the challenges of knowledge entanglement and unlearning efficiency. Instead of relying on the LLM itself to unlearn, we enforce an unlearned state during inference by employing a prompt classifier to identify and safeguard prompts to forget. We learn corruptions added to prompt embeddings via zeroth order optimization toward the unlearning objective offline and corrupt prompts flagged by the classifier during inference. We find that these embedding-corrupted prompts not only lead to desirable outputs that satisfy the unlearning objective but also closely approximate the output from a model that has never been trained on the data intended for forgetting. Through extensive experiments on unlearning, we demonstrate the superiority of our method in achieving promising unlearning at nearly zero side effects in general domains and domains closely related to the unlearned ones. Additionally, we highlight the scalability of our method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the number of parameters increases.","sentences":["Large language models (LLMs) have advanced to encompass extensive knowledge across diverse domains.","Yet controlling what a large language model should not know is important for ensuring alignment and thus safe use.","However, accurately and efficiently unlearning knowledge from an LLM remains challenging due to the potential collateral damage caused by the fuzzy boundary between retention and forgetting, and the large computational requirements for optimization across state-of-the-art models with hundreds of billions of parameters.","In this work, we present Embedding-COrrupted (ECO) Prompts, a lightweight unlearning framework for large language models to address both the challenges of knowledge entanglement and unlearning efficiency.","Instead of relying on the LLM itself to unlearn, we enforce an unlearned state during inference by employing a prompt classifier to identify and safeguard prompts to forget.","We learn corruptions added to prompt embeddings via zeroth order optimization toward the unlearning objective offline and corrupt prompts flagged by the classifier during inference.","We find that these embedding-corrupted prompts not only lead to desirable outputs that satisfy the unlearning objective but also closely approximate the output from a model that has never been trained on the data intended for forgetting.","Through extensive experiments on unlearning, we demonstrate the superiority of our method in achieving promising unlearning at nearly zero side effects in general domains and domains closely related to the unlearned ones.","Additionally, we highlight the scalability of our method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the number of parameters increases."],"url":"http://arxiv.org/abs/2406.07933v1","category":"cs.CL"}
{"created":"2024-06-12 06:46:37","title":"A Generic Layer Pruning Method for Signal Modulation Recognition Deep Learning Models","abstract":"With the successful application of deep learning in communications systems, deep neural networks are becoming the preferred method for signal classification. Although these models yield impressive results, they often come with high computational complexity and large model sizes, which hinders their practical deployment in communication systems. To address this challenge, we propose a novel layer pruning method. Specifically, we decompose the model into several consecutive blocks, each containing consecutive layers with similar semantics. Then, we identify layers that need to be preserved within each block based on their contribution. Finally, we reassemble the pruned blocks and fine-tune the compact model. Extensive experiments on five datasets demonstrate the efficiency and effectiveness of our method over a variety of state-of-the-art baselines, including layer pruning and channel pruning methods.","sentences":["With the successful application of deep learning in communications systems, deep neural networks are becoming the preferred method for signal classification.","Although these models yield impressive results, they often come with high computational complexity and large model sizes, which hinders their practical deployment in communication systems.","To address this challenge, we propose a novel layer pruning method.","Specifically, we decompose the model into several consecutive blocks, each containing consecutive layers with similar semantics.","Then, we identify layers that need to be preserved within each block based on their contribution.","Finally, we reassemble the pruned blocks and fine-tune the compact model.","Extensive experiments on five datasets demonstrate the efficiency and effectiveness of our method over a variety of state-of-the-art baselines, including layer pruning and channel pruning methods."],"url":"http://arxiv.org/abs/2406.07929v1","category":"cs.LG"}
{"created":"2024-06-12 06:45:03","title":"Efficient Neural Common Neighbor for Temporal Graph Link Prediction","abstract":"Temporal graphs are ubiquitous in real-world scenarios, such as social network, trade and transportation. Predicting dynamic links between nodes in a temporal graph is of vital importance. Traditional methods usually leverage the temporal neighborhood of interaction history to generate node embeddings first and then aggregate the source and target node embeddings to predict the link. However, such methods focus on learning individual node representations, but overlook the pairwise representation learning nature of link prediction and fail to capture the important pairwise features of links such as common neighbors (CN). Motivated by the success of Neural Common Neighbor (NCN) for static graph link prediction, we propose TNCN, a temporal version of NCN for link prediction in temporal graphs. TNCN dynamically updates a temporal neighbor dictionary for each node, and utilizes multi-hop common neighbors between the source and target node to learn a more effective pairwise representation. We validate our model on five large-scale real-world datasets from the Temporal Graph Benchmark (TGB), and find that it achieves new state-of-the-art performance on three of them. Additionally, TNCN demonstrates excellent scalability on large datasets, outperforming popular GNN baselines by up to 6.4 times in speed. Our code is available at https: //github.com/GraphPKU/TNCN.","sentences":["Temporal graphs are ubiquitous in real-world scenarios, such as social network, trade and transportation.","Predicting dynamic links between nodes in a temporal graph is of vital importance.","Traditional methods usually leverage the temporal neighborhood of interaction history to generate node embeddings first and then aggregate the source and target node embeddings to predict the link.","However, such methods focus on learning individual node representations, but overlook the pairwise representation learning nature of link prediction and fail to capture the important pairwise features of links such as common neighbors (CN).","Motivated by the success of Neural Common Neighbor (NCN) for static graph link prediction, we propose TNCN, a temporal version of NCN for link prediction in temporal graphs.","TNCN dynamically updates a temporal neighbor dictionary for each node, and utilizes multi-hop common neighbors between the source and target node to learn a more effective pairwise representation.","We validate our model on five large-scale real-world datasets from the Temporal Graph Benchmark (TGB), and find that it achieves new state-of-the-art performance on three of them.","Additionally, TNCN demonstrates excellent scalability on large datasets, outperforming popular GNN baselines by up to 6.4 times in speed.","Our code is available at https: //github.com/GraphPKU/TNCN."],"url":"http://arxiv.org/abs/2406.07926v1","category":"cs.LG"}
{"created":"2024-06-12 06:44:40","title":"CTC-aligned Audio-Text Embedding for Streaming Open-vocabulary Keyword Spotting","abstract":"This paper introduces a novel approach for streaming openvocabulary keyword spotting (KWS) with text-based keyword enrollment. For every input frame, the proposed method finds the optimal alignment ending at the frame using connectionist temporal classification (CTC) and aggregates the frame-level acoustic embedding (AE) to obtain higher-level (i.e., character, word, or phrase) AE that aligns with the text embedding (TE) of the target keyword text. After that, we calculate the similarity of the aggregated AE and the TE. To the best of our knowledge, this is the first attempt to dynamically align the audio and the keyword text on-the-fly to attain the joint audio-text embedding for KWS. Despite operating in a streaming fashion, our approach achieves competitive performance on the LibriPhrase dataset compared to the non-streaming methods with a mere 155K model parameters and a decoding algorithm with time complexity O(U), where U is the length of the target keyword at inference time.","sentences":["This paper introduces a novel approach for streaming openvocabulary keyword spotting (KWS) with text-based keyword enrollment.","For every input frame, the proposed method finds the optimal alignment ending at the frame using connectionist temporal classification (CTC) and aggregates the frame-level acoustic embedding (AE) to obtain higher-level (i.e., character, word, or phrase) AE that aligns with the text embedding (TE) of the target keyword text.","After that, we calculate the similarity of the aggregated AE and the TE.","To the best of our knowledge, this is the first attempt to dynamically align the audio and the keyword text on-the-fly to attain the joint audio-text embedding for KWS.","Despite operating in a streaming fashion, our approach achieves competitive performance on the LibriPhrase dataset compared to the non-streaming methods with a mere 155K model parameters and a decoding algorithm with time complexity O(U), where U is the length of the target keyword at inference time."],"url":"http://arxiv.org/abs/2406.07923v1","category":"cs.SD"}
{"created":"2024-06-12 06:44:05","title":"Automated Information Extraction from Thyroid Operation Narrative: A Comparative Study of GPT-4 and Fine-tuned KoELECTRA","abstract":"In the rapidly evolving field of healthcare, the integration of artificial intelligence (AI) has become a pivotal component in the automation of clinical workflows, ushering in a new era of efficiency and accuracy. This study focuses on the transformative capabilities of the fine-tuned KoELECTRA model in comparison to the GPT-4 model, aiming to facilitate automated information extraction from thyroid operation narratives. The current research landscape is dominated by traditional methods heavily reliant on regular expressions, which often face challenges in processing free-style text formats containing critical details of operation records, including frozen biopsy reports. Addressing this, the study leverages advanced natural language processing (NLP) techniques to foster a paradigm shift towards more sophisticated data processing systems. Through this comparative study, we aspire to unveil a more streamlined, precise, and efficient approach to document processing in the healthcare domain, potentially revolutionizing the way medical data is handled and analyzed.","sentences":["In the rapidly evolving field of healthcare, the integration of artificial intelligence (AI) has become a pivotal component in the automation of clinical workflows, ushering in a new era of efficiency and accuracy.","This study focuses on the transformative capabilities of the fine-tuned KoELECTRA model in comparison to the GPT-4 model, aiming to facilitate automated information extraction from thyroid operation narratives.","The current research landscape is dominated by traditional methods heavily reliant on regular expressions, which often face challenges in processing free-style text formats containing critical details of operation records, including frozen biopsy reports.","Addressing this, the study leverages advanced natural language processing (NLP) techniques to foster a paradigm shift towards more sophisticated data processing systems.","Through this comparative study, we aspire to unveil a more streamlined, precise, and efficient approach to document processing in the healthcare domain, potentially revolutionizing the way medical data is handled and analyzed."],"url":"http://arxiv.org/abs/2406.07922v1","category":"cs.CL"}
{"created":"2024-06-12 06:41:47","title":"Near-Optimal Learning and Planning in Separated Latent MDPs","abstract":"We study computational and statistical aspects of learning Latent Markov Decision Processes (LMDPs). In this model, the learner interacts with an MDP drawn at the beginning of each epoch from an unknown mixture of MDPs. To sidestep known impossibility results, we consider several notions of separation of the constituent MDPs. The main thrust of this paper is in establishing a nearly-sharp *statistical threshold* for the horizon length necessary for efficient learning. On the computational side, we show that under a weaker assumption of separability under the optimal policy, there is a quasi-polynomial algorithm with time complexity scaling in terms of the statistical threshold. We further show a near-matching time complexity lower bound under the exponential time hypothesis.","sentences":["We study computational and statistical aspects of learning Latent Markov Decision Processes (LMDPs).","In this model, the learner interacts with an MDP drawn at the beginning of each epoch from an unknown mixture of MDPs.","To sidestep known impossibility results, we consider several notions of separation of the constituent MDPs.","The main thrust of this paper is in establishing a nearly-sharp *statistical threshold* for the horizon length necessary for efficient learning.","On the computational side, we show that under a weaker assumption of separability under the optimal policy, there is a quasi-polynomial algorithm with time complexity scaling in terms of the statistical threshold.","We further show a near-matching time complexity lower bound under the exponential time hypothesis."],"url":"http://arxiv.org/abs/2406.07920v1","category":"cs.LG"}
{"created":"2024-06-12 06:36:37","title":"Graph Transductive Defense: a Two-Stage Defense for Graph Membership Inference Attacks","abstract":"Graph neural networks (GNNs) have become instrumental in diverse real-world applications, offering powerful graph learning capabilities for tasks such as social networks and medical data analysis. Despite their successes, GNNs are vulnerable to adversarial attacks, including membership inference attacks (MIA), which threaten privacy by identifying whether a record was part of the model's training data. While existing research has explored MIA in GNNs under graph inductive learning settings, the more common and challenging graph transductive learning setting remains understudied in this context. This paper addresses this gap and proposes an effective two-stage defense, Graph Transductive Defense (GTD), tailored to graph transductive learning characteristics. The gist of our approach is a combination of a train-test alternate training schedule and flattening strategy, which successfully reduces the difference between the training and testing loss distributions. Extensive empirical results demonstrate the superior performance of our method (a decrease in attack AUROC by $9.42\\%$ and an increase in utility performance by $18.08\\%$ on average compared to LBP), highlighting its potential for seamless integration into various classification models with minimal overhead.","sentences":["Graph neural networks (GNNs) have become instrumental in diverse real-world applications, offering powerful graph learning capabilities for tasks such as social networks and medical data analysis.","Despite their successes, GNNs are vulnerable to adversarial attacks, including membership inference attacks (MIA), which threaten privacy by identifying whether a record was part of the model's training data.","While existing research has explored MIA in GNNs under graph inductive learning settings, the more common and challenging graph transductive learning setting remains understudied in this context.","This paper addresses this gap and proposes an effective two-stage defense, Graph Transductive Defense (GTD), tailored to graph transductive learning characteristics.","The gist of our approach is a combination of a train-test alternate training schedule and flattening strategy, which successfully reduces the difference between the training and testing loss distributions.","Extensive empirical results demonstrate the superior performance of our method (a decrease in attack AUROC by $9.42\\%$ and an increase in utility performance by $18.08\\%$ on average compared to LBP), highlighting its potential for seamless integration into various classification models with minimal overhead."],"url":"http://arxiv.org/abs/2406.07917v1","category":"cs.LG"}
{"created":"2024-06-12 06:22:51","title":"Ablation Based Counterfactuals","abstract":"Diffusion models are a class of generative models that generate high-quality samples, but at present it is difficult to characterize how they depend upon their training data. This difficulty raises scientific and regulatory questions, and is a consequence of the complexity of diffusion models and their sampling process. To analyze this dependence, we introduce Ablation Based Counterfactuals (ABC), a method of performing counterfactual analysis that relies on model ablation rather than model retraining. In our approach, we train independent components of a model on different but overlapping splits of a training set. These components are then combined into a single model, from which the causal influence of any training sample can be removed by ablating a combination of model components. We demonstrate how we can construct a model like this using an ensemble of diffusion models. We then use this model to study the limits of training data attribution by enumerating full counterfactual landscapes, and show that single source attributability diminishes with increasing training data size. Finally, we demonstrate the existence of unattributable samples.","sentences":["Diffusion models are a class of generative models that generate high-quality samples, but at present it is difficult to characterize how they depend upon their training data.","This difficulty raises scientific and regulatory questions, and is a consequence of the complexity of diffusion models and their sampling process.","To analyze this dependence, we introduce Ablation Based Counterfactuals (ABC), a method of performing counterfactual analysis that relies on model ablation rather than model retraining.","In our approach, we train independent components of a model on different but overlapping splits of a training set.","These components are then combined into a single model, from which the causal influence of any training sample can be removed by ablating a combination of model components.","We demonstrate how we can construct a model like this using an ensemble of diffusion models.","We then use this model to study the limits of training data attribution by enumerating full counterfactual landscapes, and show that single source attributability diminishes with increasing training data size.","Finally, we demonstrate the existence of unattributable samples."],"url":"http://arxiv.org/abs/2406.07908v1","category":"cs.LG"}
{"created":"2024-06-12 06:06:55","title":"Exploring Self-Supervised Multi-view Contrastive Learning for Speech Emotion Recognition with Limited Annotations","abstract":"Recent advancements in Deep and Self-Supervised Learning (SSL) have led to substantial improvements in Speech Emotion Recognition (SER) performance, reaching unprecedented levels. However, obtaining sufficient amounts of accurately labeled data for training or fine-tuning the models remains a costly and challenging task. In this paper, we propose a multi-view SSL pre-training technique that can be applied to various representations of speech, including the ones generated by large speech models, to improve SER performance in scenarios where annotations are limited. Our experiments, based on wav2vec 2.0, spectral and paralinguistic features, demonstrate that the proposed framework boosts the SER performance, by up to 10% in Unweighted Average Recall, in settings with extremely sparse data annotations.","sentences":["Recent advancements in Deep and Self-Supervised Learning (SSL) have led to substantial improvements in Speech Emotion Recognition (SER) performance, reaching unprecedented levels.","However, obtaining sufficient amounts of accurately labeled data for training or fine-tuning the models remains a costly and challenging task.","In this paper, we propose a multi-view SSL pre-training technique that can be applied to various representations of speech, including the ones generated by large speech models, to improve SER performance in scenarios where annotations are limited.","Our experiments, based on wav2vec 2.0, spectral and paralinguistic features, demonstrate that the proposed framework boosts the SER performance, by up to 10% in Unweighted Average Recall, in settings with extremely sparse data annotations."],"url":"http://arxiv.org/abs/2406.07900v1","category":"cs.CL"}
{"created":"2024-06-12 06:01:42","title":"When Do Skills Help Reinforcement Learning? A Theoretical Analysis of Temporal Abstractions","abstract":"Skills are temporal abstractions that are intended to improve reinforcement learning (RL) performance through hierarchical RL. Despite our intuition about the properties of an environment that make skills useful, a precise characterization has been absent. We provide the first such characterization, focusing on the utility of deterministic skills in deterministic sparse-reward environments with finite action spaces. We show theoretically and empirically that RL performance gain from skills is worse in environments where solutions to states are less compressible. Additional theoretical results suggest that skills benefit exploration more than they benefit learning from existing experience, and that using unexpressive skills such as macroactions may worsen RL performance. We hope our findings can guide research on automatic skill discovery and help RL practitioners better decide when and how to use skills.","sentences":["Skills are temporal abstractions that are intended to improve reinforcement learning (RL) performance through hierarchical RL.","Despite our intuition about the properties of an environment that make skills useful, a precise characterization has been absent.","We provide the first such characterization, focusing on the utility of deterministic skills in deterministic sparse-reward environments with finite action spaces.","We show theoretically and empirically that RL performance gain from skills is worse in environments where solutions to states are less compressible.","Additional theoretical results suggest that skills benefit exploration more than they benefit learning from existing experience, and that using unexpressive skills such as macroactions may worsen RL performance.","We hope our findings can guide research on automatic skill discovery and help RL practitioners better decide when and how to use skills."],"url":"http://arxiv.org/abs/2406.07897v1","category":"cs.LG"}
{"created":"2024-06-12 05:58:55","title":"100 Drivers, 2200 km: A Natural Dataset of Driving Style toward Human-centered Intelligent Driving Systems","abstract":"Effective driving style analysis is critical to developing human-centered intelligent driving systems that consider drivers' preferences. However, the approaches and conclusions of most related studies are diverse and inconsistent because no unified datasets tagged with driving styles exist as a reliable benchmark. The absence of explicit driving style labels makes verifying different approaches and algorithms difficult. This paper provides a new benchmark by constructing a natural dataset of Driving Style (100-DrivingStyle) tagged with the subjective evaluation of 100 drivers' driving styles. In this dataset, the subjective quantification of each driver's driving style is from themselves and an expert according to the Likert-scale questionnaire. The testing routes are selected to cover various driving scenarios, including highways, urban, highway ramps, and signalized traffic. The collected driving data consists of lateral and longitudinal manipulation information, including steering angle, steering speed, lateral acceleration, throttle position, throttle rate, brake pressure, etc. This dataset is the first to provide detailed manipulation data with driving-style tags, and we demonstrate its benchmark function using six classifiers. The 100-DrivingStyle dataset is available via https://github.com/chaopengzhang/100-DrivingStyle-Dataset","sentences":["Effective driving style analysis is critical to developing human-centered intelligent driving systems that consider drivers' preferences.","However, the approaches and conclusions of most related studies are diverse and inconsistent because no unified datasets tagged with driving styles exist as a reliable benchmark.","The absence of explicit driving style labels makes verifying different approaches and algorithms difficult.","This paper provides a new benchmark by constructing a natural dataset of Driving Style (100-DrivingStyle) tagged with the subjective evaluation of 100 drivers' driving styles.","In this dataset, the subjective quantification of each driver's driving style is from themselves and an expert according to the Likert-scale questionnaire.","The testing routes are selected to cover various driving scenarios, including highways, urban, highway ramps, and signalized traffic.","The collected driving data consists of lateral and longitudinal manipulation information, including steering angle, steering speed, lateral acceleration, throttle position, throttle rate, brake pressure, etc.","This dataset is the first to provide detailed manipulation data with driving-style tags, and we demonstrate its benchmark function using six classifiers.","The 100-DrivingStyle dataset is available via https://github.com/chaopengzhang/100-DrivingStyle-Dataset"],"url":"http://arxiv.org/abs/2406.07894v1","category":"cs.RO"}
{"created":"2024-06-12 05:49:53","title":"Finite Time Analysis of Temporal Difference Learning for Mean-Variance in a Discounted MDP","abstract":"Motivated by risk-sensitive reinforcement learning scenarios, we consider the problem of policy evaluation for variance in a discounted reward Markov decision process (MDP). For this problem, a temporal difference (TD) type learning algorithm with linear function approximation (LFA) exists in the literature, though only asymptotic guarantees are available for this algorithm. We derive finite sample bounds that hold (i) in the mean-squared sense; and (ii) with high probability, when tail iterate averaging is employed with/without regularization. Our bounds exhibit exponential decay for the initial error, while the overall bound is $O(1/t)$, where $t$ is the number of update iterations of the TD algorithm. Further, the bound for the regularized TD variant is for a universal step size. Our bounds open avenues for analysis of actor-critic algorithms for mean-variance optimization in a discounted MDP.","sentences":["Motivated by risk-sensitive reinforcement learning scenarios, we consider the problem of policy evaluation for variance in a discounted reward Markov decision process (MDP).","For this problem, a temporal difference (TD) type learning algorithm with linear function approximation (LFA) exists in the literature, though only asymptotic guarantees are available for this algorithm.","We derive finite sample bounds that hold (i) in the mean-squared sense; and (ii) with high probability, when tail iterate averaging is employed with/without regularization.","Our bounds exhibit exponential decay for the initial error, while the overall bound is $O(1/t)$, where $t$ is the number of update iterations of the TD algorithm.","Further, the bound for the regularized TD variant is for a universal step size.","Our bounds open avenues for analysis of actor-critic algorithms for mean-variance optimization in a discounted MDP."],"url":"http://arxiv.org/abs/2406.07892v1","category":"cs.LG"}
{"created":"2024-06-12 05:35:28","title":"Classification Modeling with RNN-Based, Random Forest, and XGBoost for Imbalanced Data: A Case of Early Crash Detection in ASEAN-5 Stock Markets","abstract":"This research aims to evaluate the performance of several Recurrent Neural Network (RNN) architectures including Simple RNN, Gated Recurrent Units (GRU), and Long Short-Term Memory (LSTM), compared to classic algorithms such as Random Forest and XGBoost in building classification models for early crash detection in ASEAN-5 stock markets. The study is examined using imbalanced data, which is common due to the rarity of market crashes. The study analyzes daily data from 2010 to 2023 across the major stock markets of the ASEAN-5 countries, including Indonesia, Malaysia, Singapore, Thailand, and Philippines. Market crash is identified as the target variable when the major stock price indices fall below the Value at Risk (VaR) thresholds of 5%, 2.5% and 1%. predictors involving technical indicators of major local and global markets as well as commodity markets. This study includes 213 predictors with their respective lags (5, 10, 15, 22, 50, 200) and uses a time step of 7, expanding the total number of predictors to 1491. The challenge of data imbalance is addressed with SMOTE-ENN. The results show that all RNN-Based architectures outperform Random Forest and XGBoost. Among the various RNN architectures, Simple RNN stands out as the most superior, mainly due to the data characteristics that are not overly complex and focus more on short-term information. This study enhances and extends the range of phenomena observed in previous studies by incorporating variables like different geographical zones and time periods, as well as methodological adjustments.","sentences":["This research aims to evaluate the performance of several Recurrent Neural Network (RNN) architectures including Simple RNN, Gated Recurrent Units (GRU), and Long Short-Term Memory (LSTM), compared to classic algorithms such as Random Forest and XGBoost in building classification models for early crash detection in ASEAN-5 stock markets.","The study is examined using imbalanced data, which is common due to the rarity of market crashes.","The study analyzes daily data from 2010 to 2023 across the major stock markets of the ASEAN-5 countries, including Indonesia, Malaysia, Singapore, Thailand, and Philippines.","Market crash is identified as the target variable when the major stock price indices fall below the Value at Risk (VaR) thresholds of 5%, 2.5% and 1%.","predictors involving technical indicators of major local and global markets as well as commodity markets.","This study includes 213 predictors with their respective lags (5, 10, 15, 22, 50, 200) and uses a time step of 7, expanding the total number of predictors to 1491.","The challenge of data imbalance is addressed with SMOTE-ENN.","The results show that all RNN-Based architectures outperform Random Forest and XGBoost.","Among the various RNN architectures, Simple RNN stands out as the most superior, mainly due to the data characteristics that are not overly complex and focus more on short-term information.","This study enhances and extends the range of phenomena observed in previous studies by incorporating variables like different geographical zones and time periods, as well as methodological adjustments."],"url":"http://arxiv.org/abs/2406.07888v1","category":"stat.AP"}
{"created":"2024-06-12 05:20:16","title":"Designing a Dashboard for Transparency and Control of Conversational AI","abstract":"Conversational LLMs function as black box systems, leaving users guessing about why they see the output they do. This lack of transparency is potentially problematic, especially given concerns around bias and truthfulness. To address this issue, we present an end-to-end prototype-connecting interpretability techniques with user experience design-that seeks to make chatbots more transparent. We begin by showing evidence that a prominent open-source LLM has a \"user model\": examining the internal state of the system, we can extract data related to a user's age, gender, educational level, and socioeconomic status. Next, we describe the design of a dashboard that accompanies the chatbot interface, displaying this user model in real time. The dashboard can also be used to control the user model and the system's behavior. Finally, we discuss a study in which users conversed with the instrumented system. Our results suggest that users appreciate seeing internal states, which helped them expose biased behavior and increased their sense of control. Participants also made valuable suggestions that point to future directions for both design and machine learning research. The project page and video demo of our TalkTuner system are available at https://bit.ly/talktuner-project-page","sentences":["Conversational LLMs function as black box systems, leaving users guessing about why they see the output they do.","This lack of transparency is potentially problematic, especially given concerns around bias and truthfulness.","To address this issue, we present an end-to-end prototype-connecting interpretability techniques with user experience design-that seeks to make chatbots more transparent.","We begin by showing evidence that a prominent open-source LLM has a \"user model\": examining the internal state of the system, we can extract data related to a user's age, gender, educational level, and socioeconomic status.","Next, we describe the design of a dashboard that accompanies the chatbot interface, displaying this user model in real time.","The dashboard can also be used to control the user model and the system's behavior.","Finally, we discuss a study in which users conversed with the instrumented system.","Our results suggest that users appreciate seeing internal states, which helped them expose biased behavior and increased their sense of control.","Participants also made valuable suggestions that point to future directions for both design and machine learning research.","The project page and video demo of our TalkTuner system are available at https://bit.ly/talktuner-project-page"],"url":"http://arxiv.org/abs/2406.07882v1","category":"cs.CL"}
{"created":"2024-06-12 05:19:55","title":"A Comprehensive Survey on Machine Learning Driven Material Defect Detection: Challenges, Solutions, and Future Prospects","abstract":"Material defects (MD) represent a primary challenge affecting product performance and giving rise to safety issues in related products. The rapid and accurate identification and localization of MD constitute crucial research endeavours in addressing contemporary challenges associated with MD. Although conventional non-destructive testing methods such as ultrasonic and X-ray approaches have mitigated issues related to low efficiency in manual inspections, they struggle to meet the diverse requirements of high precision, real-time speed, automation, and intelligence. In recent years, propelled by the swift advancement of machine learning (ML) technologies, particularly exemplified by deep learning, ML has swiftly emerged as the core technology and a prominent research direction for material defect detection (MDD). Through a comprehensive review of the latest literature, we systematically survey the ML techniques applied in MDD into five categories: unsupervised learning, supervised learning, semi-supervised learning, reinforcement learning, and generative learning. We provide a detailed analysis of the main principles and techniques used, together with the advantages and potential challenges associated with these techniques. Furthermore, the survey focuses on the techniques for defect detection in composite materials, which are important types of materials enjoying increasingly wide application in various industries such as aerospace, automotive, construction, and renewable energy. Finally, the survey explores potential future directions in MDD utilizing ML technologies. This comprehensive survey not only consolidates existing literature on ML-based MDD technologies but also serves as a foundational reference for future researchers and industrial practitioners, providing valuable insights and guidance in developing advanced and efficient MDD systems.","sentences":["Material defects (MD) represent a primary challenge affecting product performance and giving rise to safety issues in related products.","The rapid and accurate identification and localization of MD constitute crucial research endeavours in addressing contemporary challenges associated with MD.","Although conventional non-destructive testing methods such as ultrasonic and X-ray approaches have mitigated issues related to low efficiency in manual inspections, they struggle to meet the diverse requirements of high precision, real-time speed, automation, and intelligence.","In recent years, propelled by the swift advancement of machine learning (ML) technologies, particularly exemplified by deep learning, ML has swiftly emerged as the core technology and a prominent research direction for material defect detection (MDD).","Through a comprehensive review of the latest literature, we systematically survey the ML techniques applied in MDD into five categories: unsupervised learning, supervised learning, semi-supervised learning, reinforcement learning, and generative learning.","We provide a detailed analysis of the main principles and techniques used, together with the advantages and potential challenges associated with these techniques.","Furthermore, the survey focuses on the techniques for defect detection in composite materials, which are important types of materials enjoying increasingly wide application in various industries such as aerospace, automotive, construction, and renewable energy.","Finally, the survey explores potential future directions in MDD utilizing ML technologies.","This comprehensive survey not only consolidates existing literature on ML-based MDD technologies but also serves as a foundational reference for future researchers and industrial practitioners, providing valuable insights and guidance in developing advanced and efficient MDD systems."],"url":"http://arxiv.org/abs/2406.07880v1","category":"cs.CV"}
{"created":"2024-06-12 05:16:26","title":"KernelWarehouse: Rethinking the Design of Dynamic Convolution","abstract":"Dynamic convolution learns a linear mixture of n static kernels weighted with their input-dependent attentions, demonstrating superior performance than normal convolution. However, it increases the number of convolutional parameters by n times, and thus is not parameter efficient. This leads to no research progress that can allow researchers to explore the setting n>100 (an order of magnitude larger than the typical setting n<10) for pushing forward the performance boundary of dynamic convolution while enjoying parameter efficiency. To fill this gap, in this paper, we propose KernelWarehouse, a more general form of dynamic convolution, which redefines the basic concepts of ``kernels\", ``assembling kernels\" and ``attention function\" through the lens of exploiting convolutional parameter dependencies within the same layer and across neighboring layers of a ConvNet. We testify the effectiveness of KernelWarehouse on ImageNet and MS-COCO datasets using various ConvNet architectures. Intriguingly, KernelWarehouse is also applicable to Vision Transformers, and it can even reduce the model size of a backbone while improving the model accuracy. For instance, KernelWarehouse (n=4) achieves 5.61%|3.90%|4.38% absolute top-1 accuracy gain on the ResNet18|MobileNetV2|DeiT-Tiny backbone, and KernelWarehouse (n=1/4) with 65.10% model size reduction still achieves 2.29% gain on the ResNet18 backbone. The code and models are available at https://github.com/OSVAI/KernelWarehouse.","sentences":["Dynamic convolution learns a linear mixture of n static kernels weighted with their input-dependent attentions, demonstrating superior performance than normal convolution.","However, it increases the number of convolutional parameters by n times, and thus is not parameter efficient.","This leads to no research progress that can allow researchers to explore the setting n>100 (an order of magnitude larger than the typical setting n<10) for pushing forward the performance boundary of dynamic convolution while enjoying parameter efficiency.","To fill this gap, in this paper, we propose KernelWarehouse, a more general form of dynamic convolution, which redefines the basic concepts of ``kernels\", ``assembling kernels\" and ``attention function\" through the lens of exploiting convolutional parameter dependencies within the same layer and across neighboring layers of a ConvNet.","We testify the effectiveness of KernelWarehouse on ImageNet and MS-COCO datasets using various ConvNet architectures.","Intriguingly, KernelWarehouse is also applicable to Vision Transformers, and it can even reduce the model size of a backbone while improving the model accuracy.","For instance, KernelWarehouse (n=4) achieves 5.61%|3.90%|4.38% absolute top-1 accuracy gain on the ResNet18|MobileNetV2|DeiT-Tiny backbone, and KernelWarehouse (n=1/4) with 65.10% model size reduction still achieves 2.29% gain on the ResNet18 backbone.","The code and models are available at https://github.com/OSVAI/KernelWarehouse."],"url":"http://arxiv.org/abs/2406.07879v1","category":"cs.CV"}
{"created":"2024-06-12 05:15:42","title":"A Game Theoretic Analysis of the Three-Gambler Ruin Game","abstract":"We study the following game. Three players start with initial capitals of $s_{1},s_{2},s_{3}$ dollars; in each round player $P_{m}$ is selected with probability $\\frac{1}{3}$; then \\emph{he} selects player $P_{n}$ and they play a game in which $P_{m}$ wins from (resp. loses to) $P_{n}$ one dollar with probability $p_{mn}$ (resp. $p_{nm}=1-p_{mn}$). When a player loses all his capital he drops out; the game continues until a single player wins by collecting everybody's money.   This is a \"strategic\" version of the classical Gambler's Ruin game. It seems reasonable that a player may improve his winning probability by judicious selection of which opponent to engage in each round. We formulate the situation as a \\emph{stochastic game} and prove that it has at least one Nash equilibrium in deterministic stationary strategies.","sentences":["We study the following game.","Three players start with initial capitals of $s_{1},s_{2},s_{3}$ dollars; in each round player $P_{m}$ is selected with probability $\\frac{1}{3}$; then \\emph{he} selects player $P_{n}$","and they play a game in which $P_{m}$ wins from (resp.","loses to) $P_{n}$ one dollar with probability $p_{mn}$ (resp.","$p_{nm}=1-p_{mn}$).","When a player loses all his capital he drops out; the game continues until a single player wins by collecting everybody's money.   ","This is a \"strategic\" version of the classical Gambler's Ruin game.","It seems reasonable that a player may improve his winning probability by judicious selection of which opponent to engage in each round.","We formulate the situation as a \\emph{stochastic game} and prove that it has at least one Nash equilibrium in deterministic stationary strategies."],"url":"http://arxiv.org/abs/2406.07878v1","category":"cs.GT"}
{"created":"2024-06-12 05:12:10","title":"Hierarchical Reinforcement Learning for Swarm Confrontation with High Uncertainty","abstract":"In swarm robotics, confrontation including the pursuit-evasion game is a key scenario. High uncertainty caused by unknown opponents' strategies and dynamic obstacles complicates the action space into a hybrid decision process. Although the deep reinforcement learning method is significant for swarm confrontation since it can handle various sizes, as an end-to-end implementation, it cannot deal with the hybrid process. Here, we propose a novel hierarchical reinforcement learning approach consisting of a target allocation layer, a path planning layer, and the underlying dynamic interaction mechanism between the two layers, which indicates the quantified uncertainty. It decouples the hybrid process into discrete allocation and continuous planning layers, with a probabilistic ensemble model to quantify the uncertainty and regulate the interaction frequency adaptively. Furthermore, to overcome the unstable training process introduced by the two layers, we design an integration training method including pre-training and cross-training, which enhances the training efficiency and stability. Experiment results in both comparison and ablation studies validate the effectiveness and generalization performance of our proposed approach.","sentences":["In swarm robotics, confrontation including the pursuit-evasion game is a key scenario.","High uncertainty caused by unknown opponents' strategies and dynamic obstacles complicates the action space into a hybrid decision process.","Although the deep reinforcement learning method is significant for swarm confrontation since it can handle various sizes, as an end-to-end implementation, it cannot deal with the hybrid process.","Here, we propose a novel hierarchical reinforcement learning approach consisting of a target allocation layer, a path planning layer, and the underlying dynamic interaction mechanism between the two layers, which indicates the quantified uncertainty.","It decouples the hybrid process into discrete allocation and continuous planning layers, with a probabilistic ensemble model to quantify the uncertainty and regulate the interaction frequency adaptively.","Furthermore, to overcome the unstable training process introduced by the two layers, we design an integration training method including pre-training and cross-training, which enhances the training efficiency and stability.","Experiment results in both comparison and ablation studies validate the effectiveness and generalization performance of our proposed approach."],"url":"http://arxiv.org/abs/2406.07877v1","category":"cs.RO"}
{"created":"2024-06-12 05:09:41","title":"Small Scale Data-Free Knowledge Distillation","abstract":"Data-free knowledge distillation is able to utilize the knowledge learned by a large teacher network to augment the training of a smaller student network without accessing the original training data, avoiding privacy, security, and proprietary risks in real applications. In this line of research, existing methods typically follow an inversion-and-distillation paradigm in which a generative adversarial network on-the-fly trained with the guidance of the pre-trained teacher network is used to synthesize a large-scale sample set for knowledge distillation. In this paper, we reexamine this common data-free knowledge distillation paradigm, showing that there is considerable room to improve the overall training efficiency through a lens of ``small-scale inverted data for knowledge distillation\". In light of three empirical observations indicating the importance of how to balance class distributions in terms of synthetic sample diversity and difficulty during both data inversion and distillation processes, we propose Small Scale Data-free Knowledge Distillation SSD-KD. In formulation, SSD-KD introduces a modulating function to balance synthetic samples and a priority sampling function to select proper samples, facilitated by a dynamic replay buffer and a reinforcement learning strategy. As a result, SSD-KD can perform distillation training conditioned on an extremely small scale of synthetic samples (e.g., 10X less than the original training data scale), making the overall training efficiency one or two orders of magnitude faster than many mainstream methods while retaining superior or competitive model performance, as demonstrated on popular image classification and semantic segmentation benchmarks. The code is available at https://github.com/OSVAI/SSD-KD.","sentences":["Data-free knowledge distillation is able to utilize the knowledge learned by a large teacher network to augment the training of a smaller student network without accessing the original training data, avoiding privacy, security, and proprietary risks in real applications.","In this line of research, existing methods typically follow an inversion-and-distillation paradigm in which a generative adversarial network on-the-fly trained with the guidance of the pre-trained teacher network is used to synthesize a large-scale sample set for knowledge distillation.","In this paper, we reexamine this common data-free knowledge distillation paradigm, showing that there is considerable room to improve the overall training efficiency through a lens of ``small-scale inverted data for knowledge distillation\".","In light of three empirical observations indicating the importance of how to balance class distributions in terms of synthetic sample diversity and difficulty during both data inversion and distillation processes, we propose Small Scale Data-free Knowledge Distillation SSD-KD.","In formulation, SSD-KD introduces a modulating function to balance synthetic samples and a priority sampling function to select proper samples, facilitated by a dynamic replay buffer and a reinforcement learning strategy.","As a result, SSD-KD can perform distillation training conditioned on an extremely small scale of synthetic samples (e.g., 10X less than the original training data scale), making the overall training efficiency one or two orders of magnitude faster than many mainstream methods while retaining superior or competitive model performance, as demonstrated on popular image classification and semantic segmentation benchmarks.","The code is available at https://github.com/OSVAI/SSD-KD."],"url":"http://arxiv.org/abs/2406.07876v1","category":"cs.CV"}
{"created":"2024-06-12 05:08:51","title":"Carbon Market Simulation with Adaptive Mechanism Design","abstract":"A carbon market is a market-based tool that incentivizes economic agents to align individual profits with the global utility, i.e., reducing carbon emissions to tackle climate change.   \\textit{Cap and trade} stands as a critical principle based on allocating and trading carbon allowances (carbon emission credit), enabling economic agents to follow planned emissions and penalizing excess emissions.   A central authority is responsible for introducing and allocating those allowances in cap and trade.   However, the complexity of carbon market dynamics makes accurate simulation intractable, which in turn hinders the design of effective allocation strategies.   To address this, we propose an adaptive mechanism design framework, simulating the market using hierarchical, model-free multi-agent reinforcement learning (MARL).   Government agents allocate carbon credits, while enterprises engage in economic activities and carbon trading.   This framework illustrates agents' behavior comprehensively.   Numerical results show MARL enables government agents to balance productivity, equality, and carbon emissions.   Our project is available at \\url{https://github.com/xwanghan/Carbon-Simulator}.","sentences":["A carbon market is a market-based tool that incentivizes economic agents to align individual profits with the global utility, i.e., reducing carbon emissions to tackle climate change.   ","\\textit{Cap and trade} stands as a critical principle based on allocating and trading carbon allowances (carbon emission credit), enabling economic agents to follow planned emissions and penalizing excess emissions.   ","A central authority is responsible for introducing and allocating those allowances in cap and trade.   ","However, the complexity of carbon market dynamics makes accurate simulation intractable, which in turn hinders the design of effective allocation strategies.   ","To address this, we propose an adaptive mechanism design framework, simulating the market using hierarchical, model-free multi-agent reinforcement learning (MARL).   ","Government agents allocate carbon credits, while enterprises engage in economic activities and carbon trading.   ","This framework illustrates agents' behavior comprehensively.   ","Numerical results show MARL enables government agents to balance productivity, equality, and carbon emissions.   ","Our project is available at \\url{https://github.com/xwanghan/Carbon-Simulator}."],"url":"http://arxiv.org/abs/2406.07875v1","category":"cs.LG"}
{"created":"2024-06-12 04:52:40","title":"Unveiling the Power of Wavelets: A Wavelet-based Kolmogorov-Arnold Network for Hyperspectral Image Classification","abstract":"Hyperspectral image classification is a crucial but challenging task due to the high dimensionality and complex spatial-spectral correlations inherent in hyperspectral data. This paper employs Wavelet-based Kolmogorov-Arnold Network (wav-kan) architecture tailored for efficient modeling of these intricate dependencies. Inspired by the Kolmogorov-Arnold representation theorem, Wav-KAN incorporates wavelet functions as learnable activation functions, enabling non-linear mapping of the input spectral signatures. The wavelet-based activation allows Wav-KAN to effectively capture multi-scale spatial and spectral patterns through dilations and translations. Experimental evaluation on three benchmark hyperspectral datasets (Salinas, Pavia, Indian Pines) demonstrates the superior performance of Wav-KAN compared to traditional multilayer perceptrons (MLPs) and the recently proposed Spline-based KAN (Spline-KAN) model. In this work we are: (1) conducting more experiments on additional hyperspectral datasets (Pavia University, WHU-Hi, and Urban Hyperspectral Image) to further validate the generalizability of Wav-KAN; (2) developing a multiresolution Wav-KAN architecture to capture scale-invariant features; (3) analyzing the effect of dimensional reduction techniques on classification performance; (4) exploring optimization methods for tuning the hyperparameters of KAN models; and (5) comparing Wav-KAN with other state-of-the-art models in hyperspectral image classification.","sentences":["Hyperspectral image classification is a crucial but challenging task due to the high dimensionality and complex spatial-spectral correlations inherent in hyperspectral data.","This paper employs Wavelet-based Kolmogorov-Arnold Network (wav-kan) architecture tailored for efficient modeling of these intricate dependencies.","Inspired by the Kolmogorov-Arnold representation theorem, Wav-KAN incorporates wavelet functions as learnable activation functions, enabling non-linear mapping of the input spectral signatures.","The wavelet-based activation allows Wav-KAN to effectively capture multi-scale spatial and spectral patterns through dilations and translations.","Experimental evaluation on three benchmark hyperspectral datasets (Salinas, Pavia, Indian Pines) demonstrates the superior performance of Wav-KAN compared to traditional multilayer perceptrons (MLPs) and the recently proposed Spline-based KAN (Spline-KAN) model.","In this work we are: (1) conducting more experiments on additional hyperspectral datasets (Pavia University, WHU-Hi, and Urban Hyperspectral Image) to further validate the generalizability of Wav-KAN; (2) developing a multiresolution Wav-KAN architecture to capture scale-invariant features; (3) analyzing the effect of dimensional reduction techniques on classification performance; (4) exploring optimization methods for tuning the hyperparameters of KAN models; and (5) comparing Wav-KAN with other state-of-the-art models in hyperspectral image classification."],"url":"http://arxiv.org/abs/2406.07869v1","category":"cs.CV"}
{"created":"2024-06-12 04:48:36","title":"Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation","abstract":"In this paper, we introduce a novel Face-to-Face spoken dialogue model. It processes audio-visual speech from user input and generates audio-visual speech as the response, marking the initial step towards creating an avatar chatbot system without relying on intermediate text. To this end, we newly introduce MultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken dialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded based on the open domain dialogue dataset, TopicalChat. The MultiDialog contains parallel audio-visual recordings of conversation partners acting according to the given script with emotion annotations, which we expect to open up research opportunities in multimodal synthesis. Our Face-to-Face spoken dialogue model incorporates a textually pretrained large language model and adapts it into the audio-visual spoken dialogue domain by incorporating speech-text joint pretraining. Through extensive experiments, we validate the effectiveness of our model in facilitating a face-to-face conversation. Demo and data are available at https://multidialog.github.io and https://huggingface.co/datasets/IVLLab/MultiDialog, respectively.","sentences":["In this paper, we introduce a novel Face-to-Face spoken dialogue model.","It processes audio-visual speech from user input and generates audio-visual speech as the response, marking the initial step towards creating an avatar chatbot system without relying on intermediate text.","To this end, we newly introduce MultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken dialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded based on the open domain dialogue dataset, TopicalChat.","The MultiDialog contains parallel audio-visual recordings of conversation partners acting according to the given script with emotion annotations, which we expect to open up research opportunities in multimodal synthesis.","Our Face-to-Face spoken dialogue model incorporates a textually pretrained large language model and adapts it into the audio-visual spoken dialogue domain by incorporating speech-text joint pretraining.","Through extensive experiments, we validate the effectiveness of our model in facilitating a face-to-face conversation.","Demo and data are available at https://multidialog.github.io and https://huggingface.co/datasets/IVLLab/MultiDialog, respectively."],"url":"http://arxiv.org/abs/2406.07867v1","category":"cs.CV"}
{"created":"2024-06-12 04:45:33","title":"FaithFill: Faithful Inpainting for Object Completion Using a Single Reference Image","abstract":"We present FaithFill, a diffusion-based inpainting object completion approach for realistic generation of missing object parts. Typically, multiple reference images are needed to achieve such realistic generation, otherwise the generation would not faithfully preserve shape, texture, color, and background. In this work, we propose a pipeline that utilizes only a single input reference image -having varying lighting, background, object pose, and/or viewpoint. The singular reference image is used to generate multiple views of the object to be inpainted. We demonstrate that FaithFill produces faithful generation of the object's missing parts, together with background/scene preservation, from a single reference image. This is demonstrated through standard similarity metrics, human judgement, and GPT evaluation. Our results are presented on the DreamBooth dataset, and a novel proposed dataset.","sentences":["We present FaithFill, a diffusion-based inpainting object completion approach for realistic generation of missing object parts.","Typically, multiple reference images are needed to achieve such realistic generation, otherwise the generation would not faithfully preserve shape, texture, color, and background.","In this work, we propose a pipeline that utilizes only a single input reference image -having varying lighting, background, object pose, and/or viewpoint.","The singular reference image is used to generate multiple views of the object to be inpainted.","We demonstrate that FaithFill produces faithful generation of the object's missing parts, together with background/scene preservation, from a single reference image.","This is demonstrated through standard similarity metrics, human judgement, and GPT evaluation.","Our results are presented on the DreamBooth dataset, and a novel proposed dataset."],"url":"http://arxiv.org/abs/2406.07865v1","category":"cs.CV"}
{"created":"2024-06-12 04:30:40","title":"Self-Distillation Learning Based on Temporal-Spatial Consistency for Spiking Neural Networks","abstract":"Spiking neural networks (SNNs) have attracted considerable attention for their event-driven, low-power characteristics and high biological interpretability. Inspired by knowledge distillation (KD), recent research has improved the performance of the SNN model with a pre-trained teacher model. However, additional teacher models require significant computational resources, and it is tedious to manually define the appropriate teacher network architecture. In this paper, we explore cost-effective self-distillation learning of SNNs to circumvent these concerns. Without an explicit defined teacher, the SNN generates pseudo-labels and learns consistency during training. On the one hand, we extend the timestep of the SNN during training to create an implicit temporal ``teacher\" that guides the learning of the original ``student\", i.e., the temporal self-distillation. On the other hand, we guide the output of the weak classifier at the intermediate stage by the final output of the SNN, i.e., the spatial self-distillation. Our temporal-spatial self-distillation (TSSD) learning method does not introduce any inference overhead and has excellent generalization ability. Extensive experiments on the static image datasets CIFAR10/100 and ImageNet as well as the neuromorphic datasets CIFAR10-DVS and DVS-Gesture validate the superior performance of the TSSD method. This paper presents a novel manner of fusing SNNs with KD, providing insights into high-performance SNN learning methods.","sentences":["Spiking neural networks (SNNs) have attracted considerable attention for their event-driven, low-power characteristics and high biological interpretability.","Inspired by knowledge distillation (KD), recent research has improved the performance of the SNN model with a pre-trained teacher model.","However, additional teacher models require significant computational resources, and it is tedious to manually define the appropriate teacher network architecture.","In this paper, we explore cost-effective self-distillation learning of SNNs to circumvent these concerns.","Without an explicit defined teacher, the SNN generates pseudo-labels and learns consistency during training.","On the one hand, we extend the timestep of the SNN during training to create an implicit temporal ``teacher\" that guides the learning of the original ``student\", i.e., the temporal self-distillation.","On the other hand, we guide the output of the weak classifier at the intermediate stage by the final output of the SNN, i.e., the spatial self-distillation.","Our temporal-spatial self-distillation (TSSD) learning method does not introduce any inference overhead and has excellent generalization ability.","Extensive experiments on the static image datasets CIFAR10/100 and ImageNet as well as the neuromorphic datasets CIFAR10-DVS and DVS-Gesture validate the superior performance of the TSSD method.","This paper presents a novel manner of fusing SNNs with KD, providing insights into high-performance SNN learning methods."],"url":"http://arxiv.org/abs/2406.07862v1","category":"cs.LG"}
{"created":"2024-06-12 04:22:27","title":"BookSQL: A Large Scale Text-to-SQL Dataset for Accounting Domain","abstract":"Several large-scale datasets (e.g., WikiSQL, Spider) for developing natural language interfaces to databases have recently been proposed. These datasets cover a wide breadth of domains but fall short on some essential domains, such as finance and accounting. Given that accounting databases are used worldwide, particularly by non-technical people, there is an imminent need to develop models that could help extract information from accounting databases via natural language queries. In this resource paper, we aim to fill this gap by proposing a new large-scale Text-to-SQL dataset for the accounting and financial domain: BookSQL. The dataset consists of 100k natural language queries-SQL pairs, and accounting databases of 1 million records. We experiment with and analyze existing state-of-the-art models (including GPT-4) for the Text-to-SQL task on BookSQL. We find significant performance gaps, thus pointing towards developing more focused models for this domain.","sentences":["Several large-scale datasets (e.g., WikiSQL, Spider) for developing natural language interfaces to databases have recently been proposed.","These datasets cover a wide breadth of domains but fall short on some essential domains, such as finance and accounting.","Given that accounting databases are used worldwide, particularly by non-technical people, there is an imminent need to develop models that could help extract information from accounting databases via natural language queries.","In this resource paper, we aim to fill this gap by proposing a new large-scale Text-to-SQL dataset for the accounting and financial domain: BookSQL.","The dataset consists of 100k natural language queries-SQL pairs, and accounting databases of 1 million records.","We experiment with and analyze existing state-of-the-art models (including GPT-4) for the Text-to-SQL task on BookSQL.","We find significant performance gaps, thus pointing towards developing more focused models for this domain."],"url":"http://arxiv.org/abs/2406.07860v1","category":"cs.CL"}
{"created":"2024-06-12 03:38:45","title":"Dynamic Stochastic Decoding Strategy for Open-Domain Dialogue Generation","abstract":"Stochastic sampling strategies such as top-k and top-p have been widely used in dialogue generation task. However, as an open-domain chatting system, there will be two different conversation scenarios, i.e. chit-chat and knowledge-based question answering. In the former situation, responses diversity is essential due to the one-to-many nature in dialogue. The latter, on the other hand, requires less randomness given that stochastic decoding strategy entails the risk of generating incorrect information. As a result, an adaptive and flexible decoding strategy is needed to cope with these two scenarios simultaneously. To this end, we propose the dynamic decoding strategy (DDS), which can adjust the decoding space w.r.t. different contexts. In DDS, both sequence-level and token-level adaptive search can be achieved to adjust the decoding process in a unified framework. Besides, our adaptive algorithm can not only be used during model inference, but it can also be applied during the model training stage to further enhance the performance. Comprehensive experiments indicate that the proposed decoding strategy can consistently improve the performance of pre-trained dialogue models when coupled with four well-used stochastic decoding algorithms.","sentences":["Stochastic sampling strategies such as top-k and top-p have been widely used in dialogue generation task.","However, as an open-domain chatting system, there will be two different conversation scenarios, i.e. chit-chat and knowledge-based question answering.","In the former situation, responses diversity is essential due to the one-to-many nature in dialogue.","The latter, on the other hand, requires less randomness given that stochastic decoding strategy entails the risk of generating incorrect information.","As a result, an adaptive and flexible decoding strategy is needed to cope with these two scenarios simultaneously.","To this end, we propose the dynamic decoding strategy (DDS), which can adjust the decoding space w.r.t.","different contexts.","In DDS, both sequence-level and token-level adaptive search can be achieved to adjust the decoding process in a unified framework.","Besides, our adaptive algorithm can not only be used during model inference, but it can also be applied during the model training stage to further enhance the performance.","Comprehensive experiments indicate that the proposed decoding strategy can consistently improve the performance of pre-trained dialogue models when coupled with four well-used stochastic decoding algorithms."],"url":"http://arxiv.org/abs/2406.07850v1","category":"cs.CL"}
{"created":"2024-06-12 03:30:10","title":"Multi-agent Reinforcement Learning with Deep Networks for Diverse Q-Vectors","abstract":"Multi-agent reinforcement learning (MARL) has become a significant research topic due to its ability to facilitate learning in complex environments. In multi-agent tasks, the state-action value, commonly referred to as the Q-value, can vary among agents because of their individual rewards, resulting in a Q-vector. Determining an optimal policy is challenging, as it involves more than just maximizing a single Q-value. Various optimal policies, such as a Nash equilibrium, have been studied in this context. Algorithms like Nash Q-learning and Nash Actor-Critic have shown effectiveness in these scenarios. This paper extends this research by proposing a deep Q-networks (DQN) algorithm capable of learning various Q-vectors using Max, Nash, and Maximin strategies. The effectiveness of this approach is demonstrated in an environment where dual robotic arms collaborate to lift a pot.","sentences":["Multi-agent reinforcement learning (MARL) has become a significant research topic due to its ability to facilitate learning in complex environments.","In multi-agent tasks, the state-action value, commonly referred to as the Q-value, can vary among agents because of their individual rewards, resulting in a Q-vector.","Determining an optimal policy is challenging, as it involves more than just maximizing a single Q-value.","Various optimal policies, such as a Nash equilibrium, have been studied in this context.","Algorithms like Nash Q-learning and Nash Actor-Critic have shown effectiveness in these scenarios.","This paper extends this research by proposing a deep Q-networks (DQN) algorithm capable of learning various Q-vectors using Max, Nash, and Maximin strategies.","The effectiveness of this approach is demonstrated in an environment where dual robotic arms collaborate to lift a pot."],"url":"http://arxiv.org/abs/2406.07848v1","category":"cs.AI"}
{"created":"2024-06-12 03:10:27","title":"Scaling Manipulation Learning with Visual Kinematic Chain Prediction","abstract":"Learning general-purpose models from diverse datasets has achieved great success in machine learning. In robotics, however, existing methods in multi-task learning are typically constrained to a single robot and workspace, while recent work such as RT-X requires a non-trivial action normalization procedure to manually bridge the gap between different action spaces in diverse environments. In this paper, we propose the visual kinematics chain as a precise and universal representation of quasi-static actions for robot learning over diverse environments, which requires no manual adjustment since the visual kinematic chains can be automatically obtained from the robot's model and camera parameters. We propose the Visual Kinematics Transformer (VKT), a convolution-free architecture that supports an arbitrary number of camera viewpoints, and that is trained with a single objective of forecasting kinematic structures through optimal point-set matching. We demonstrate the superior performance of VKT over BC transformers as a general agent on Calvin, RLBench, Open-X, and real robot manipulation tasks. Video demonstrations can be found at https://mlzxy.github.io/visual-kinetic-chain.","sentences":["Learning general-purpose models from diverse datasets has achieved great success in machine learning.","In robotics, however, existing methods in multi-task learning are typically constrained to a single robot and workspace, while recent work such as RT-X requires a non-trivial action normalization procedure to manually bridge the gap between different action spaces in diverse environments.","In this paper, we propose the visual kinematics chain as a precise and universal representation of quasi-static actions for robot learning over diverse environments, which requires no manual adjustment since the visual kinematic chains can be automatically obtained from the robot's model and camera parameters.","We propose the Visual Kinematics Transformer (VKT), a convolution-free architecture that supports an arbitrary number of camera viewpoints, and that is trained with a single objective of forecasting kinematic structures through optimal point-set matching.","We demonstrate the superior performance of VKT over BC transformers as a general agent on Calvin, RLBench, Open-X, and real robot manipulation tasks.","Video demonstrations can be found at https://mlzxy.github.io/visual-kinetic-chain."],"url":"http://arxiv.org/abs/2406.07837v1","category":"cs.RO"}
{"created":"2024-06-12 03:03:51","title":"Research on Comprehensive Optimization Design of CNC Machine Tool Feed System Considering Subsystem Coupling Effects","abstract":"The feed system of CNC machine tools is critical in determining machining accuracy. Current design methods often utilize lightweight and structural optimization to enhance natural frequencies or restrict the load-to-inertia ratio to ensure dynamic performance. However, it is challenging to determine whether these methods can meet accuracy requirements during the design phase. This paper proposes using position and velocity errors as design criteria to meet accuracy requirements during the design phase. However, the feed system of CNC machine tools is influenced by various subsystems. Still, comprehensive research on the coupling effects of these subsystems and their integrated impact on dynamic performance is currently lacking. This paper addresses this issue by comprehensively studying the integrated effects of various subsystems on the dynamic performance of the feed system. It proposes comprehensive optimization design criteria for the dynamic design of the feed system of CNC machine tools. First, the influence of the control system is isolated using intelligent optimization algorithms. Then, the influence mechanisms of motor and mechanical structure parameters on the dynamic performance of the feed system under different motion processes are studied. Finally, based on the integrated effects of coupling between subsystems on feed system performance, a comprehensive optimization design criteria for the feed system of CNC machine tools considering the coupling effects of various subsystems is proposed. This paper provides a theoretical basis for the dynamic design of machine tool feed systems.","sentences":["The feed system of CNC machine tools is critical in determining machining accuracy.","Current design methods often utilize lightweight and structural optimization to enhance natural frequencies or restrict the load-to-inertia ratio to ensure dynamic performance.","However, it is challenging to determine whether these methods can meet accuracy requirements during the design phase.","This paper proposes using position and velocity errors as design criteria to meet accuracy requirements during the design phase.","However, the feed system of CNC machine tools is influenced by various subsystems.","Still, comprehensive research on the coupling effects of these subsystems and their integrated impact on dynamic performance is currently lacking.","This paper addresses this issue by comprehensively studying the integrated effects of various subsystems on the dynamic performance of the feed system.","It proposes comprehensive optimization design criteria for the dynamic design of the feed system of CNC machine tools.","First, the influence of the control system is isolated using intelligent optimization algorithms.","Then, the influence mechanisms of motor and mechanical structure parameters on the dynamic performance of the feed system under different motion processes are studied.","Finally, based on the integrated effects of coupling between subsystems on feed system performance, a comprehensive optimization design criteria for the feed system of CNC machine tools considering the coupling effects of various subsystems is proposed.","This paper provides a theoretical basis for the dynamic design of machine tool feed systems."],"url":"http://arxiv.org/abs/2406.07834v1","category":"eess.SY"}
{"created":"2024-06-12 03:02:54","title":"Sense Less, Generate More: Pre-training LiDAR Perception with Masked Autoencoders for Ultra-Efficient 3D Sensing","abstract":"In this work, we propose a disruptively frugal LiDAR perception dataflow that generates rather than senses parts of the environment that are either predictable based on the extensive training of the environment or have limited consequence to the overall prediction accuracy. Therefore, the proposed methodology trades off sensing energy with training data for low-power robotics and autonomous navigation to operate frugally with sensors, extending their lifetime on a single battery charge. Our proposed generative pre-training strategy for this purpose, called as radially masked autoencoding (R-MAE), can also be readily implemented in a typical LiDAR system by selectively activating and controlling the laser power for randomly generated angular regions during on-field operations. Our extensive evaluations show that pre-training with R-MAE enables focusing on the radial segments of the data, thereby capturing spatial relationships and distances between objects more effectively than conventional procedures. Therefore, the proposed methodology not only reduces sensing energy but also improves prediction accuracy. For example, our extensive evaluations on Waymo, nuScenes, and KITTI datasets show that the approach achieves over a 5% average precision improvement in detection tasks across datasets and over a 4% accuracy improvement in transferring domains from Waymo and nuScenes to KITTI. In 3D object detection, it enhances small object detection by up to 4.37% in AP at moderate difficulty levels in the KITTI dataset. Even with 90% radial masking, it surpasses baseline models by up to 5.59% in mAP/mAPH across all object classes in the Waymo dataset. Additionally, our method achieves up to 3.17% and 2.31% improvements in mAP and NDS, respectively, on the nuScenes dataset, demonstrating its effectiveness with both single and fused LiDAR-camera modalities. https://github.com/sinatayebati/Radial_MAE.","sentences":["In this work, we propose a disruptively frugal LiDAR perception dataflow that generates rather than senses parts of the environment that are either predictable based on the extensive training of the environment or have limited consequence to the overall prediction accuracy.","Therefore, the proposed methodology trades off sensing energy with training data for low-power robotics and autonomous navigation to operate frugally with sensors, extending their lifetime on a single battery charge.","Our proposed generative pre-training strategy for this purpose, called as radially masked autoencoding (R-MAE), can also be readily implemented in a typical LiDAR system by selectively activating and controlling the laser power for randomly generated angular regions during on-field operations.","Our extensive evaluations show that pre-training with R-MAE enables focusing on the radial segments of the data, thereby capturing spatial relationships and distances between objects more effectively than conventional procedures.","Therefore, the proposed methodology not only reduces sensing energy but also improves prediction accuracy.","For example, our extensive evaluations on Waymo, nuScenes, and KITTI datasets show that the approach achieves over a 5% average precision improvement in detection tasks across datasets and over a 4% accuracy improvement in transferring domains from Waymo and nuScenes to KITTI.","In 3D object detection, it enhances small object detection by up to 4.37% in AP at moderate difficulty levels in the KITTI dataset.","Even with 90% radial masking",", it surpasses baseline models by up to 5.59% in mAP/mAPH across all object classes in the Waymo dataset.","Additionally, our method achieves up to 3.17% and 2.31% improvements in mAP and NDS, respectively, on the nuScenes dataset, demonstrating its effectiveness with both single and fused LiDAR-camera modalities.","https://github.com/sinatayebati/Radial_MAE."],"url":"http://arxiv.org/abs/2406.07833v1","category":"cs.CV"}
{"created":"2024-06-12 02:47:54","title":"The Max-Min Formulation of Multi-Objective Reinforcement Learning: From Theory to a Model-Free Algorithm","abstract":"In this paper, we consider multi-objective reinforcement learning, which arises in many real-world problems with multiple optimization goals. We approach the problem with a max-min framework focusing on fairness among the multiple goals and develop a relevant theory and a practical model-free algorithm under the max-min framework. The developed theory provides a theoretical advance in multi-objective reinforcement learning, and the proposed algorithm demonstrates a notable performance improvement over existing baseline methods.","sentences":["In this paper, we consider multi-objective reinforcement learning, which arises in many real-world problems with multiple optimization goals.","We approach the problem with a max-min framework focusing on fairness among the multiple goals and develop a relevant theory and a practical model-free algorithm under the max-min framework.","The developed theory provides a theoretical advance in multi-objective reinforcement learning, and the proposed algorithm demonstrates a notable performance improvement over existing baseline methods."],"url":"http://arxiv.org/abs/2406.07826v1","category":"cs.LG"}
{"created":"2024-06-12 02:23:51","title":"Are Large Language Models Good Statisticians?","abstract":"Large Language Models (LLMs) have demonstrated impressive capabilities across a range of scientific tasks including mathematics, physics, and chemistry. Despite their successes, the effectiveness of LLMs in handling complex statistical tasks remains systematically under-explored. To bridge this gap, we introduce StatQA, a new benchmark designed for statistical analysis tasks. StatQA comprises 11,623 examples tailored to evaluate LLMs' proficiency in specialized statistical tasks and their applicability assessment capabilities, particularly for hypothesis testing methods. We systematically experiment with representative LLMs using various prompting strategies and show that even state-of-the-art models such as GPT-4o achieve a best performance of only 64.83%, indicating significant room for improvement. Notably, while open-source LLMs (e.g. LLaMA-3) show limited capability, those fine-tuned ones exhibit marked improvements, outperforming all in-context learning-based methods (e.g. GPT-4o). Moreover, our comparative human experiments highlight a striking contrast in error types between LLMs and humans: LLMs primarily make applicability errors, whereas humans mostly make statistical task confusion errors. This divergence highlights distinct areas of proficiency and deficiency, suggesting that combining LLM and human expertise could lead to complementary strengths, inviting further investigation into their collaborative potential.","sentences":["Large Language Models (LLMs) have demonstrated impressive capabilities across a range of scientific tasks including mathematics, physics, and chemistry.","Despite their successes, the effectiveness of LLMs in handling complex statistical tasks remains systematically under-explored.","To bridge this gap, we introduce StatQA, a new benchmark designed for statistical analysis tasks.","StatQA comprises 11,623 examples tailored to evaluate LLMs' proficiency in specialized statistical tasks and their applicability assessment capabilities, particularly for hypothesis testing methods.","We systematically experiment with representative LLMs using various prompting strategies and show that even state-of-the-art models such as GPT-4o achieve a best performance of only 64.83%, indicating significant room for improvement.","Notably, while open-source LLMs (e.g. LLaMA-3) show limited capability, those fine-tuned ones exhibit marked improvements, outperforming all in-context learning-based methods (e.g. GPT-4o).","Moreover, our comparative human experiments highlight a striking contrast in error types between LLMs and humans: LLMs primarily make applicability errors, whereas humans mostly make statistical task confusion errors.","This divergence highlights distinct areas of proficiency and deficiency, suggesting that combining LLM and human expertise could lead to complementary strengths, inviting further investigation into their collaborative potential."],"url":"http://arxiv.org/abs/2406.07815v1","category":"cs.CL"}
{"created":"2024-06-12 02:20:46","title":"Collective Constitutional AI: Aligning a Language Model with Public Input","abstract":"There is growing consensus that language model (LM) developers should not be the sole deciders of LM behavior, creating a need for methods that enable the broader public to collectively shape the behavior of LM systems that affect them. To address this need, we present Collective Constitutional AI (CCAI): a multi-stage process for sourcing and integrating public input into LMs-from identifying a target population to sourcing principles to training and evaluating a model. We demonstrate the real-world practicality of this approach by creating what is, to our knowledge, the first LM fine-tuned with collectively sourced public input and evaluating this model against a baseline model trained with established principles from a LM developer. Our quantitative evaluations demonstrate several benefits of our approach: the CCAI-trained model shows lower bias across nine social dimensions compared to the baseline model, while maintaining equivalent performance on language, math, and helpful-harmless evaluations. Qualitative comparisons of the models suggest that the models differ on the basis of their respective constitutions, e.g., when prompted with contentious topics, the CCAI-trained model tends to generate responses that reframe the matter positively instead of a refusal. These results demonstrate a promising, tractable pathway toward publicly informed development of language models.","sentences":["There is growing consensus that language model (LM) developers should not be the sole deciders of LM behavior, creating a need for methods that enable the broader public to collectively shape the behavior of LM systems that affect them.","To address this need, we present Collective Constitutional AI (CCAI): a multi-stage process for sourcing and integrating public input into LMs-from identifying a target population to sourcing principles to training and evaluating a model.","We demonstrate the real-world practicality of this approach by creating what is, to our knowledge, the first LM fine-tuned with collectively sourced public input and evaluating this model against a baseline model trained with established principles from a LM developer.","Our quantitative evaluations demonstrate several benefits of our approach: the CCAI-trained model shows lower bias across nine social dimensions compared to the baseline model, while maintaining equivalent performance on language, math, and helpful-harmless evaluations.","Qualitative comparisons of the models suggest that the models differ on the basis of their respective constitutions, e.g., when prompted with contentious topics, the CCAI-trained model tends to generate responses that reframe the matter positively instead of a refusal.","These results demonstrate a promising, tractable pathway toward publicly informed development of language models."],"url":"http://arxiv.org/abs/2406.07814v1","category":"cs.AI"}
{"created":"2024-06-12 02:08:45","title":"To be Continuous, or to be Discrete, Those are Bits of Questions","abstract":"Recently, binary representation has been proposed as a novel representation that lies between continuous and discrete representations. It exhibits considerable information-preserving capability when being used to replace continuous input vectors. In this paper, we investigate the feasibility of further introducing it to the output side, aiming to allow models to output binary labels instead. To preserve the structural information on the output side along with label information, we extend the previous contrastive hashing method as structured contrastive hashing. More specifically, we upgrade CKY from label-level to bit-level, define a new similarity function with span marginal probabilities, and introduce a novel contrastive loss function with a carefully designed instance selection strategy. Our model achieves competitive performance on various structured prediction tasks, and demonstrates that binary representation can be considered a novel representation that further bridges the gap between the continuous nature of deep learning and the discrete intrinsic property of natural languages.","sentences":["Recently, binary representation has been proposed as a novel representation that lies between continuous and discrete representations.","It exhibits considerable information-preserving capability when being used to replace continuous input vectors.","In this paper, we investigate the feasibility of further introducing it to the output side, aiming to allow models to output binary labels instead.","To preserve the structural information on the output side along with label information, we extend the previous contrastive hashing method as structured contrastive hashing.","More specifically, we upgrade CKY from label-level to bit-level, define a new similarity function with span marginal probabilities, and introduce a novel contrastive loss function with a carefully designed instance selection strategy.","Our model achieves competitive performance on various structured prediction tasks, and demonstrates that binary representation can be considered a novel representation that further bridges the gap between the continuous nature of deep learning and the discrete intrinsic property of natural languages."],"url":"http://arxiv.org/abs/2406.07812v1","category":"cs.CL"}
{"created":"2024-06-12 02:06:24","title":"Evolutionary Computation and Explainable AI: A Roadmap to Transparent Intelligent Systems","abstract":"AI methods are finding an increasing number of applications, but their often black-box nature has raised concerns about accountability and trust. The field of explainable artificial intelligence (XAI) has emerged in response to the need for human understanding of AI models. Evolutionary computation (EC), as a family of powerful optimization and learning tools, has significant potential to contribute to XAI. In this paper, we provide an introduction to XAI and review various techniques in current use for explaining machine learning (ML) models. We then focus on how EC can be used in XAI, and review some XAI approaches which incorporate EC techniques. Additionally, we discuss the application of XAI principles within EC itself, examining how these principles can shed some light on the behavior and outcomes of EC algorithms in general, on the (automatic) configuration of these algorithms, and on the underlying problem landscapes that these algorithms optimize. Finally, we discuss some open challenges in XAI and opportunities for future research in this field using EC. Our aim is to demonstrate that EC is well-suited for addressing current problems in explainability and to encourage further exploration of these methods to contribute to the development of more transparent and trustworthy ML models and EC algorithms.","sentences":["AI methods are finding an increasing number of applications, but their often black-box nature has raised concerns about accountability and trust.","The field of explainable artificial intelligence (XAI) has emerged in response to the need for human understanding of AI models.","Evolutionary computation (EC), as a family of powerful optimization and learning tools, has significant potential to contribute to XAI.","In this paper, we provide an introduction to XAI and review various techniques in current use for explaining machine learning (ML) models.","We then focus on how EC can be used in XAI, and review some XAI approaches which incorporate EC techniques.","Additionally, we discuss the application of XAI principles within EC itself, examining how these principles can shed some light on the behavior and outcomes of EC algorithms in general, on the (automatic) configuration of these algorithms, and on the underlying problem landscapes that these algorithms optimize.","Finally, we discuss some open challenges in XAI and opportunities for future research in this field using EC.","Our aim is to demonstrate that EC is well-suited for addressing current problems in explainability and to encourage further exploration of these methods to contribute to the development of more transparent and trustworthy ML models and EC algorithms."],"url":"http://arxiv.org/abs/2406.07811v1","category":"cs.NE"}
{"created":"2024-06-12 01:55:24","title":"Dynamic Energy-Saving Design for Double-Faced Active RIS Assisted Communications with Perfect/Imperfect CSI","abstract":"Although the emerging reconfigurable intelligent surface (RIS) paves a new way for next-generation wireless communications, it suffers from inherent flaws, i.e., double-fading attenuation effects and half-space coverage limitations. The state-of-the-art double-face active (DFA)-RIS architecture is proposed for significantly amplifying and transmitting incident signals in full-space. Despite the efficacy of DFA-RIS in mitigating the aforementioned flaws, its potential drawback is that the complex active hardware also incurs intolerable energy consumption. To overcome this drawback, in this paper we propose a novel dynamic energy-saving design for the DFA-RIS, called the sub-array based DFA-RIS architecture. This architecture divides the DFA-RIS into multiple sub-arrays, where the signal amplification function in each sub-array can be activated/deactivated dynamically and flexibly. Utilizing the above architecture, we develop the joint optimization scheme based on transmit beamforming, DFA-RIS configuration, and reflection amplifier (RA) operating pattern to maximize the energy efficiency (EE) of the DFA-RIS assisted multiuser MISO system considering the perfect/imperfect channel state information (CSI) case. Then, the penalty dual decomposition (PDD) based alternating optimization (AO) algorithm and the constrained stochastic majorization-minimization (CSMM) based AO algorithm address non-convex problems in the perfect/imperfect CSI case, respectively. Simulation results verified that our proposed sub-array based DFA-RIS architecture can benefit the EE of the system more than other RIS architectures.","sentences":["Although the emerging reconfigurable intelligent surface (RIS) paves a new way for next-generation wireless communications, it suffers from inherent flaws, i.e., double-fading attenuation effects and half-space coverage limitations.","The state-of-the-art double-face active (DFA)-RIS architecture is proposed for significantly amplifying and transmitting incident signals in full-space.","Despite the efficacy of DFA-RIS in mitigating the aforementioned flaws, its potential drawback is that the complex active hardware also incurs intolerable energy consumption.","To overcome this drawback, in this paper we propose a novel dynamic energy-saving design for the DFA-RIS, called the sub-array based DFA-RIS architecture.","This architecture divides the DFA-RIS into multiple sub-arrays, where the signal amplification function in each sub-array can be activated/deactivated dynamically and flexibly.","Utilizing the above architecture, we develop the joint optimization scheme based on transmit beamforming, DFA-RIS configuration, and reflection amplifier (RA) operating pattern to maximize the energy efficiency (EE) of the DFA-RIS assisted multiuser MISO system considering the perfect/imperfect channel state information (CSI) case.","Then, the penalty dual decomposition (PDD) based alternating optimization (AO) algorithm and the constrained stochastic majorization-minimization (CSMM) based AO algorithm address non-convex problems in the perfect/imperfect CSI case, respectively.","Simulation results verified that our proposed sub-array based DFA-RIS architecture can benefit the EE of the system more than other RIS architectures."],"url":"http://arxiv.org/abs/2406.07807v1","category":"cs.IT"}
{"created":"2024-06-12 01:40:29","title":"EmoSphere-TTS: Emotional Style and Intensity Modeling via Spherical Emotion Vector for Controllable Emotional Text-to-Speech","abstract":"Despite rapid advances in the field of emotional text-to-speech (TTS), recent studies primarily focus on mimicking the average style of a particular emotion. As a result, the ability to manipulate speech emotion remains constrained to several predefined labels, compromising the ability to reflect the nuanced variations of emotion. In this paper, we propose EmoSphere-TTS, which synthesizes expressive emotional speech by using a spherical emotion vector to control the emotional style and intensity of the synthetic speech. Without any human annotation, we use the arousal, valence, and dominance pseudo-labels to model the complex nature of emotion via a Cartesian-spherical transformation. Furthermore, we propose a dual conditional adversarial network to improve the quality of generated speech by reflecting the multi-aspect characteristics. The experimental results demonstrate the model ability to control emotional style and intensity with high-quality expressive speech.","sentences":["Despite rapid advances in the field of emotional text-to-speech (TTS), recent studies primarily focus on mimicking the average style of a particular emotion.","As a result, the ability to manipulate speech emotion remains constrained to several predefined labels, compromising the ability to reflect the nuanced variations of emotion.","In this paper, we propose EmoSphere-TTS, which synthesizes expressive emotional speech by using a spherical emotion vector to control the emotional style and intensity of the synthetic speech.","Without any human annotation, we use the arousal, valence, and dominance pseudo-labels to model the complex nature of emotion via a Cartesian-spherical transformation.","Furthermore, we propose a dual conditional adversarial network to improve the quality of generated speech by reflecting the multi-aspect characteristics.","The experimental results demonstrate the model ability to control emotional style and intensity with high-quality expressive speech."],"url":"http://arxiv.org/abs/2406.07803v1","category":"cs.SD"}
{"created":"2024-06-12 01:21:12","title":"Real-time Deformation Correction in Additively Printed Flexible Antenna Arrays","abstract":"Conformal phased arrays provide multiple degrees of freedom to the scan angle, which is typically limited by antenna aperture in rigid arrays. Silicon-based RF signal processing offers reliable, reconfigurable, multi-functional, and compact control for conformal phased arrays that can be used for on-the-move communication. While the lightweight, compactness, and shape-changing properties of the conformal phased arrays are attractive, these features result in dynamic deformation of the array during motion leading to significant dynamic beam pointing errors. We propose a silicon-based, compact, reconfigurable solution to self-correct these dynamic deformation-induced beam pointing errors. Furthermore, additive printing is leveraged to enhance the flexibility of the conformal phased arrays, as the printed conductive ink is more flexible than bulk copper and can be easily deposited on flexible sheets using different printing tools, providing an environmentally-friendly solution for large-scale production. The inks such as conventional silver inks are expensive and copper-based printable inks suffer from spontaneous metal oxidation that alters trace impedance and degrades beamforming performance. This work uses a low-cost molecular copper decomposition ink with reliable RF properties at different temperature and strain to print the proposed intelligent conformal phased array operating at 2.1 GHz. Proof-of-concept prototype $2\\times2$ array self-corrects the deformation induces beampointing error with an error $<1.25^\\circ$. The silicon based array processing part occupying only 2.56 mm$^2$ area and 78.5 mW power per tile.","sentences":["Conformal phased arrays provide multiple degrees of freedom to the scan angle, which is typically limited by antenna aperture in rigid arrays.","Silicon-based RF signal processing offers reliable, reconfigurable, multi-functional, and compact control for conformal phased arrays that can be used for on-the-move communication.","While the lightweight, compactness, and shape-changing properties of the conformal phased arrays are attractive, these features result in dynamic deformation of the array during motion leading to significant dynamic beam pointing errors.","We propose a silicon-based, compact, reconfigurable solution to self-correct these dynamic deformation-induced beam pointing errors.","Furthermore, additive printing is leveraged to enhance the flexibility of the conformal phased arrays, as the printed conductive ink is more flexible than bulk copper and can be easily deposited on flexible sheets using different printing tools, providing an environmentally-friendly solution for large-scale production.","The inks such as conventional silver inks are expensive and copper-based printable inks suffer from spontaneous metal oxidation that alters trace impedance and degrades beamforming performance.","This work uses a low-cost molecular copper decomposition ink with reliable RF properties at different temperature and strain to print the proposed intelligent conformal phased array operating at 2.1 GHz.","Proof-of-concept prototype $2\\times2$ array self-corrects the deformation induces beampointing error with an error $<1.25^\\circ$. The silicon based array processing part occupying only 2.56 mm$^2$ area and 78.5 mW power per tile."],"url":"http://arxiv.org/abs/2406.07797v1","category":"eess.SP"}
{"created":"2024-06-12 01:19:36","title":"Harnessing GenAI for Higher Education: A Study of a Retrieval Augmented Generation Chatbot's Impact on Human Learning","abstract":"The advent of generative artificial intelligence (GenAI) and large language models (LLMs) has opened new avenues for enhancing human learning. This study introduces Professor Leodar, a custom-built, Singlish-speaking Retrieval Augmented Generation (RAG) chatbot designed to enhance educational support for undergraduate engineering students. Deployed at Nanyang Technological University, Singapore, Professor Leodar offers a glimpse into the future of AI-assisted learning, offering personalized guidance, 24/7 availability, and contextually relevant information. Through a mixed-methods approach, we uncover the impact of Professor Leodar on student learning, engagement, and exam preparedness, with 97.1% of participants reporting positive experiences. These findings help define possible roles of AI in education and highlight the potential of custom GenAI chatbots. Our combination of chatbot development, in-class deployment and study of learning outcomes offers a benchmark for GenAI educational tools and serves as stepping stone for redefining the interplay between AI and human learning.","sentences":["The advent of generative artificial intelligence (GenAI) and large language models (LLMs) has opened new avenues for enhancing human learning.","This study introduces Professor Leodar, a custom-built, Singlish-speaking Retrieval Augmented Generation (RAG) chatbot designed to enhance educational support for undergraduate engineering students.","Deployed at Nanyang Technological University, Singapore, Professor Leodar offers a glimpse into the future of AI-assisted learning, offering personalized guidance, 24/7 availability, and contextually relevant information.","Through a mixed-methods approach, we uncover the impact of Professor Leodar on student learning, engagement, and exam preparedness, with 97.1% of participants reporting positive experiences.","These findings help define possible roles of AI in education and highlight the potential of custom GenAI chatbots.","Our combination of chatbot development, in-class deployment and study of learning outcomes offers a benchmark for GenAI educational tools and serves as stepping stone for redefining the interplay between AI and human learning."],"url":"http://arxiv.org/abs/2406.07796v1","category":"cs.HC"}
{"created":"2024-06-12 01:18:04","title":"IndirectRequests: Making Task-Oriented Dialogue Datasets More Natural by Synthetically Generating Indirect User Requests","abstract":"Existing benchmark corpora of task-oriented dialogue are collected either using a \"machines talking to machines\" approach or by giving template-based goal descriptions to crowdworkers. These methods, however, often produce utterances that are markedly different from natural human conversations in which people often convey their preferences in indirect ways, such as through small talk. We term such utterances as Indirect User Requests (IURs). Understanding such utterances demands considerable world knowledge and reasoning capabilities on the listener's part. Our study introduces an LLM-based pipeline to automatically generate realistic, high-quality IURs for a given domain, with the ultimate goal of supporting research in natural language understanding (NLU) and dialogue state tracking (DST) for task-oriented dialogue systems. Our findings show that while large LLMs such as GPT-3.5 and GPT-4 generate high-quality IURs, achieving similar quality with smaller models is more challenging. We release IndirectRequests, a dataset of IURs that advances beyond the initial Schema-Guided Dialog (SGD) dataset in that it provides a challenging testbed for testing the \"in the wild\" performance of NLU and DST models.","sentences":["Existing benchmark corpora of task-oriented dialogue are collected either using a \"machines talking to machines\" approach or by giving template-based goal descriptions to crowdworkers.","These methods, however, often produce utterances that are markedly different from natural human conversations in which people often convey their preferences in indirect ways, such as through small talk.","We term such utterances as Indirect User Requests (IURs).","Understanding such utterances demands considerable world knowledge and reasoning capabilities on the listener's part.","Our study introduces an LLM-based pipeline to automatically generate realistic, high-quality IURs for a given domain, with the ultimate goal of supporting research in natural language understanding (NLU) and dialogue state tracking (DST) for task-oriented dialogue systems.","Our findings show that while large LLMs such as GPT-3.5 and GPT-4 generate high-quality IURs, achieving similar quality with smaller models is more challenging.","We release IndirectRequests, a dataset of IURs that advances beyond the initial Schema-Guided Dialog (SGD) dataset in that it provides a challenging testbed for testing the \"in the wild\" performance of NLU and DST models."],"url":"http://arxiv.org/abs/2406.07794v1","category":"cs.CL"}
{"created":"2024-06-12 01:12:28","title":"Judging the Judges: A Systematic Investigation of Position Bias in Pairwise Comparative Assessments by LLMs","abstract":"LLM-as-a-Judge offers a promising alternative to human judges across various tasks, yet inherent biases, particularly position bias - a systematic preference for answers based on their position in the prompt - compromise its effectiveness. Our study investigates this issue by developing a framework to systematically study and quantify position bias using metrics such as repetitional consistency, positional consistency, and positional fairness. We conduct experiments with 9 judge models across 22 tasks from the MTBench and DevBench benchmarks and nearly 40 answer-generating models, generating approximately 80,000 evaluation instances. This comprehensive assessment reveals significant variations in bias across judges and tasks. Although GPT-4 often excels in positional consistency and fairness, some more cost-effective models perform comparably or even better in specific tasks, highlighting essential trade-offs between consistency, fairness, and cost. Our results also demonstrate high consistency of judgment across repetitions, confirming that position bias is not due to random variations. This research significantly contributes to the field by introducing new concepts for understanding position bias and providing a multi-dimensional framework for evaluation. These insights guide the selection of optimal judge models, enhance benchmark design, and lay the foundation for future research into effective debiasing strategies, ultimately enhancing the reliability of LLM evaluators.","sentences":["LLM-as-a-Judge offers a promising alternative to human judges across various tasks, yet inherent biases, particularly position bias - a systematic preference for answers based on their position in the prompt - compromise its effectiveness.","Our study investigates this issue by developing a framework to systematically study and quantify position bias using metrics such as repetitional consistency, positional consistency, and positional fairness.","We conduct experiments with 9 judge models across 22 tasks from the MTBench and DevBench benchmarks and nearly 40 answer-generating models, generating approximately 80,000 evaluation instances.","This comprehensive assessment reveals significant variations in bias across judges and tasks.","Although GPT-4 often excels in positional consistency and fairness, some more cost-effective models perform comparably or even better in specific tasks, highlighting essential trade-offs between consistency, fairness, and cost.","Our results also demonstrate high consistency of judgment across repetitions, confirming that position bias is not due to random variations.","This research significantly contributes to the field by introducing new concepts for understanding position bias and providing a multi-dimensional framework for evaluation.","These insights guide the selection of optimal judge models, enhance benchmark design, and lay the foundation for future research into effective debiasing strategies, ultimately enhancing the reliability of LLM evaluators."],"url":"http://arxiv.org/abs/2406.07791v1","category":"cs.CL"}
{"created":"2024-06-12 01:10:32","title":"Hierarchical Neural Networks, p-Adic PDEs, and Applications to Image Processing","abstract":"The first goal of this article is to introduce a new type of p-adic reaction-diffusion cellular neural network with delay. We study the stability of these networks and provide numerical simulations of their responses. The second goal is to provide a quick review of the state of the art of p-adic cellular neural networks and their applications to image processing.","sentences":["The first goal of this article is to introduce a new type of p-adic reaction-diffusion cellular neural network with delay.","We study the stability of these networks and provide numerical simulations of their responses.","The second goal is to provide a quick review of the state of the art of p-adic cellular neural networks and their applications to image processing."],"url":"http://arxiv.org/abs/2406.07790v1","category":"cs.NE"}
{"created":"2024-06-12 00:01:32","title":"On Trojans in Refined Language Models","abstract":"A Trojan in a language model can be inserted when the model is refined for a particular application such as determining the sentiment of product reviews. In this paper, we clarify and empirically explore variations of the data-poisoning threat model. We then empirically assess two simple defenses each for a different defense scenario. Finally, we provide a brief survey of related attacks and defenses.","sentences":["A Trojan in a language model can be inserted when the model is refined for a particular application such as determining the sentiment of product reviews.","In this paper, we clarify and empirically explore variations of the data-poisoning threat model.","We then empirically assess two simple defenses each for a different defense scenario.","Finally, we provide a brief survey of related attacks and defenses."],"url":"http://arxiv.org/abs/2406.07778v1","category":"cs.CR"}
{"created":"2024-06-11 23:29:48","title":"DualBind: A Dual-Loss Framework for Protein-Ligand Binding Affinity Prediction","abstract":"Accurate prediction of protein-ligand binding affinities is crucial for drug development. Recent advances in machine learning show promising results on this task. However, these methods typically rely heavily on labeled data, which can be scarce or unreliable, or they rely on assumptions like Boltzmann-distributed data that may not hold true in practice. Here, we present DualBind, a novel framework that integrates supervised mean squared error (MSE) with unsupervised denoising score matching (DSM) to accurately learn the binding energy function. DualBind not only addresses the limitations of DSM-only models by providing more accurate absolute affinity predictions but also improves generalizability and reduces reliance on labeled data compared to MSE-only models. Our experimental results demonstrate that DualBind excels in predicting binding affinities and can effectively utilize both labeled and unlabeled data to enhance performance.","sentences":["Accurate prediction of protein-ligand binding affinities is crucial for drug development.","Recent advances in machine learning show promising results on this task.","However, these methods typically rely heavily on labeled data, which can be scarce or unreliable, or they rely on assumptions like Boltzmann-distributed data that may not hold true in practice.","Here, we present DualBind, a novel framework that integrates supervised mean squared error (MSE) with unsupervised denoising score matching (DSM) to accurately learn the binding energy function.","DualBind not only addresses the limitations of DSM-only models by providing more accurate absolute affinity predictions but also improves generalizability and reduces reliance on labeled data compared to MSE-only models.","Our experimental results demonstrate that DualBind excels in predicting binding affinities and can effectively utilize both labeled and unlabeled data to enhance performance."],"url":"http://arxiv.org/abs/2406.07770v1","category":"cs.LG"}
{"created":"2024-06-11 23:10:43","title":"Using AI-Based Coding Assistants in Practice: State of Affairs, Perceptions, and Ways Forward","abstract":"The last several years saw the emergence of AI assistants for code -- multi-purpose AI-based helpers in software engineering. Their quick development makes it necessary to better understand how specifically developers are using them, why they are not using them in certain parts of their development workflow, and what needs to be improved.   In this work, we carried out a large-scale survey aimed at how AI assistants are used, focusing on specific software development activities and stages. We collected opinions of 481 programmers on five broad activities: (a) implementing new features, (b) writing tests, (c) bug triaging, (d) refactoring, and (e) writing natural-language artifacts, as well as their individual stages.   Our results show that usage of AI assistants varies depending on activity and stage. For instance, developers find writing tests and natural-language artifacts to be the least enjoyable activities and want to delegate them the most, currently using AI assistants to generate tests and test data, as well as generating comments and docstrings most of all. This can be a good focus for features aimed to help developers right now. As for why developers do not use assistants, in addition to general things like trust and company policies, there are fixable issues that can serve as a guide for further research, e.g., the lack of project-size context, and lack of awareness about assistants. We believe that our comprehensive and specific results are especially needed now to steer active research toward where users actually need AI assistants.","sentences":["The last several years saw the emergence of AI assistants for code -- multi-purpose AI-based helpers in software engineering.","Their quick development makes it necessary to better understand how specifically developers are using them, why they are not using them in certain parts of their development workflow, and what needs to be improved.   ","In this work, we carried out a large-scale survey aimed at how AI assistants are used, focusing on specific software development activities and stages.","We collected opinions of 481 programmers on five broad activities: (a) implementing new features, (b) writing tests, (c) bug triaging, (d) refactoring, and (e) writing natural-language artifacts, as well as their individual stages.   ","Our results show that usage of AI assistants varies depending on activity and stage.","For instance, developers find writing tests and natural-language artifacts to be the least enjoyable activities and want to delegate them the most, currently using AI assistants to generate tests and test data, as well as generating comments and docstrings most of all.","This can be a good focus for features aimed to help developers right now.","As for why developers do not use assistants, in addition to general things like trust and company policies, there are fixable issues that can serve as a guide for further research, e.g., the lack of project-size context, and lack of awareness about assistants.","We believe that our comprehensive and specific results are especially needed now to steer active research toward where users actually need AI assistants."],"url":"http://arxiv.org/abs/2406.07765v1","category":"cs.SE"}
{"created":"2024-06-11 22:31:56","title":"A Machine Learning Framework for Quantum Cascade Laser Design","abstract":"A multi-layer perceptron neural network was used to predict the laser transition figure of merit, a measure of the laser threshold gain, of over 900 million Quantum Cascade Laser designs using only layer thicknesses and the applied electric field as inputs. Designs were generated by randomly altering the layer thicknesses of an initial 10-layer design. Validating the predictions with our 1D Schr\\\"odinger solver, the predicted values show 5% to 15% error for structures where a laser transition could occur, and 35% to 70% error for structures where there was no laser transition. The algorithm allowed (i) for the identification of high figure of merit structures, (ii) recognition of which layers should be altered to maximize the figure of merit at a given electric field, and (iii) increased the original design figure of merit of 94.7 to 141.2 eV ps \\r{A}^2, a 1.5-fold improvement and significant for QC lasers. The computational time for laser design data collection is greatly reduced from 32 hours for 27000 designs using our 1D Schr\\\"odinger solver on a virtual machine, to 8 hours for 907 million designs using the machine learning algorithm on a laptop computer.","sentences":["A multi-layer perceptron neural network was used to predict the laser transition figure of merit, a measure of the laser threshold gain, of over 900 million Quantum Cascade Laser designs using only layer thicknesses and the applied electric field as inputs.","Designs were generated by randomly altering the layer thicknesses of an initial 10-layer design.","Validating the predictions with our 1D Schr\\\"odinger solver, the predicted values show 5% to 15% error for structures where a laser transition could occur, and 35% to 70% error for structures where there was no laser transition.","The algorithm allowed (i) for the identification of high figure of merit structures, (ii) recognition of which layers should be altered to maximize the figure of merit at a given electric field, and (iii) increased the original design figure of merit of 94.7 to 141.2 eV ps \\r{A}^2, a 1.5-fold improvement and significant for QC lasers.","The computational time for laser design data collection is greatly reduced from 32 hours for 27000 designs using our 1D Schr\\\"odinger solver on a virtual machine, to 8 hours for 907 million designs using the machine learning algorithm on a laptop computer."],"url":"http://arxiv.org/abs/2406.07755v1","category":"physics.optics"}
{"created":"2024-06-11 22:26:20","title":"The MuSe 2024 Multimodal Sentiment Analysis Challenge: Social Perception and Humor Recognition","abstract":"The Multimodal Sentiment Analysis Challenge (MuSe) 2024 addresses two contemporary multimodal affect and sentiment analysis problems: In the Social Perception Sub-Challenge (MuSe-Perception), participants will predict 16 different social attributes of individuals such as assertiveness, dominance, likability, and sincerity based on the provided audio-visual data. The Cross-Cultural Humor Detection Sub-Challenge (MuSe-Humor) dataset expands upon the Passau Spontaneous Football Coach Humor (Passau-SFCH) dataset, focusing on the detection of spontaneous humor in a cross-lingual and cross-cultural setting. The main objective of MuSe 2024 is to unite a broad audience from various research domains, including multimodal sentiment analysis, audio-visual affective computing, continuous signal processing, and natural language processing. By fostering collaboration and exchange among experts in these fields, the MuSe 2024 endeavors to advance the understanding and application of sentiment analysis and affective computing across multiple modalities. This baseline paper provides details on each sub-challenge and its corresponding dataset, extracted features from each data modality, and discusses challenge baselines. For our baseline system, we make use of a range of Transformers and expert-designed features and train Gated Recurrent Unit (GRU)-Recurrent Neural Network (RNN) models on them, resulting in a competitive baseline system. On the unseen test datasets of the respective sub-challenges, it achieves a mean Pearson's Correlation Coefficient ($\\rho$) of 0.3573 for MuSe-Perception and an Area Under the Curve (AUC) value of 0.8682 for MuSe-Humor.","sentences":["The Multimodal Sentiment Analysis Challenge (MuSe) 2024 addresses two contemporary multimodal affect and sentiment analysis problems: In the Social Perception Sub-Challenge (MuSe-Perception), participants will predict 16 different social attributes of individuals such as assertiveness, dominance, likability, and sincerity based on the provided audio-visual data.","The Cross-Cultural Humor Detection Sub-Challenge (MuSe-Humor) dataset expands upon the Passau Spontaneous Football Coach Humor (Passau-SFCH) dataset, focusing on the detection of spontaneous humor in a cross-lingual and cross-cultural setting.","The main objective of MuSe 2024 is to unite a broad audience from various research domains, including multimodal sentiment analysis, audio-visual affective computing, continuous signal processing, and natural language processing.","By fostering collaboration and exchange among experts in these fields, the MuSe 2024 endeavors to advance the understanding and application of sentiment analysis and affective computing across multiple modalities.","This baseline paper provides details on each sub-challenge and its corresponding dataset, extracted features from each data modality, and discusses challenge baselines.","For our baseline system, we make use of a range of Transformers and expert-designed features and train Gated Recurrent Unit (GRU)-Recurrent Neural Network (RNN) models on them, resulting in a competitive baseline system.","On the unseen test datasets of the respective sub-challenges, it achieves a mean Pearson's Correlation Coefficient ($\\rho$) of 0.3573 for MuSe-Perception and an Area Under the Curve (AUC) value of 0.8682 for MuSe-Humor."],"url":"http://arxiv.org/abs/2406.07753v1","category":"cs.AI"}
{"created":"2024-06-11 22:05:34","title":"Report on laser-induced fluorescence transitions relevant for the microelectronics industry and sustainability applications","abstract":"A wide variety of feed gases are used to generate low-temperature plasmas for the microelectronics and the sustainability applications. These plasmas often have a complex combination of reactive and non-reactive species which may have spatial and temporal variations in the density, the temperature and the energy. Accurate knowledge of these parameters and their variations is critically important for understanding and advancing these applications through validated and predictive modeling and design of relevant devices. Laser-induced fluorescence (LIF) provides both spatial and temporally resolved information about the plasma-produced radicals, ions, and metastables. However, the use of this powerful diagnostic tool requires the knowledge of optical transitions including excitation and fluorescence wavelengths which may not be available or scattered through a huge literature domain. In this manuscript, we collected, analyzed and compiled the available transitions for laser-induced fluorescence for more than 160 chemical species relevant to the microelectronics industry and the sustainability applications. A list of species with overlapping LIF excitation and fluorescence wavelengths have been identified. This summary is intended to serve as a data reference for LIF transitions and should be updated in the future.","sentences":["A wide variety of feed gases are used to generate low-temperature plasmas for the microelectronics and the sustainability applications.","These plasmas often have a complex combination of reactive and non-reactive species which may have spatial and temporal variations in the density, the temperature and the energy.","Accurate knowledge of these parameters and their variations is critically important for understanding and advancing these applications through validated and predictive modeling and design of relevant devices.","Laser-induced fluorescence (LIF) provides both spatial and temporally resolved information about the plasma-produced radicals, ions, and metastables.","However, the use of this powerful diagnostic tool requires the knowledge of optical transitions including excitation and fluorescence wavelengths which may not be available or scattered through a huge literature domain.","In this manuscript, we collected, analyzed and compiled the available transitions for laser-induced fluorescence for more than 160 chemical species relevant to the microelectronics industry and the sustainability applications.","A list of species with overlapping LIF excitation and fluorescence wavelengths have been identified.","This summary is intended to serve as a data reference for LIF transitions and should be updated in the future."],"url":"http://arxiv.org/abs/2406.07747v1","category":"physics.plasm-ph"}
{"created":"2024-06-11 21:48:20","title":"On the Application of Egocentric Computer Vision to Industrial Scenarios","abstract":"Egocentric vision aims to capture and analyse the world from the first-person perspective. We explore the possibilities for egocentric wearable devices to improve and enhance industrial use cases w.r.t. data collection, annotation, labelling and downstream applications. This would contribute to easier data collection and allow users to provide additional context. We envision that this approach could serve as a supplement to the traditional industrial Machine Vision workflow. Code, Dataset and related resources will be available at: https://github.com/Vivek9Chavan/EgoVis24","sentences":["Egocentric vision aims to capture and analyse the world from the first-person perspective.","We explore the possibilities for egocentric wearable devices to improve and enhance industrial use cases w.r.t. data collection, annotation, labelling and downstream applications.","This would contribute to easier data collection and allow users to provide additional context.","We envision that this approach could serve as a supplement to the traditional industrial Machine Vision workflow.","Code, Dataset and related resources will be available at: https://github.com/Vivek9Chavan/EgoVis24"],"url":"http://arxiv.org/abs/2406.07738v1","category":"cs.CV"}
{"created":"2024-06-11 21:46:19","title":"The Future of Software Engineering in an AI-Driven World","abstract":"A paradigm shift is underway in Software Engineering, with AI systems such as LLMs gaining increasing importance for improving software development productivity. This trend is anticipated to persist. In the next five years, we will likely see an increasing symbiotic partnership between human developers and AI. The Software Engineering research community cannot afford to overlook this trend; we must address the key research challenges posed by the integration of AI into the software development process. In this paper, we present our vision of the future of software development in an AI-Driven world and explore the key challenges that our research community should address to realize this vision.","sentences":["A paradigm shift is underway in Software Engineering, with AI systems such as LLMs gaining increasing importance for improving software development productivity.","This trend is anticipated to persist.","In the next five years, we will likely see an increasing symbiotic partnership between human developers and AI.","The Software Engineering research community cannot afford to overlook this trend; we must address the key research challenges posed by the integration of AI into the software development process.","In this paper, we present our vision of the future of software development in an AI-Driven world and explore the key challenges that our research community should address to realize this vision."],"url":"http://arxiv.org/abs/2406.07737v1","category":"cs.SE"}
{"created":"2024-06-11 21:12:34","title":"Efficient Parallel Multi-Hop Reasoning: A Scalable Approach for Knowledge Graph Analysis","abstract":"Multi-hop reasoning (MHR) is a process in artificial intelligence and natural language processing where a system needs to make multiple inferential steps to arrive at a conclusion or answer. In the context of knowledge graphs or databases, it involves traversing multiple linked entities and relationships to understand complex queries or perform tasks requiring a deeper understanding. Multi-hop reasoning is a critical function in various applications, including question answering, knowledge base completion, and link prediction. It has garnered significant interest in artificial intelligence, machine learning, and graph analytics.   This paper focuses on optimizing MHR for time efficiency on large-scale graphs, diverging from the traditional emphasis on accuracy which is an orthogonal goal. We introduce a novel parallel algorithm that harnesses domain-specific learned embeddings to efficiently identify the top K paths between vertices in a knowledge graph to find the best answers to a three-hop query. Our contributions are: (1) We present a new parallel algorithm to enhance MHR performance, scalability and efficiency. (2) We demonstrate the algorithm's superior performance on leading-edge Intel and AMD architectures through empirical results.   We showcase the algorithm's practicality through a case study on identifying academic affiliations of potential Turing Award laureates in Deep Learning, highlighting its capability to handle intricate entity relationships. This demonstrates the potential of our approach to enabling high-performance MHR, useful to navigate the growing complexity of modern knowledge graphs.","sentences":["Multi-hop reasoning (MHR) is a process in artificial intelligence and natural language processing where a system needs to make multiple inferential steps to arrive at a conclusion or answer.","In the context of knowledge graphs or databases, it involves traversing multiple linked entities and relationships to understand complex queries or perform tasks requiring a deeper understanding.","Multi-hop reasoning is a critical function in various applications, including question answering, knowledge base completion, and link prediction.","It has garnered significant interest in artificial intelligence, machine learning, and graph analytics.   ","This paper focuses on optimizing MHR for time efficiency on large-scale graphs, diverging from the traditional emphasis on accuracy which is an orthogonal goal.","We introduce a novel parallel algorithm that harnesses domain-specific learned embeddings to efficiently identify the top K paths between vertices in a knowledge graph to find the best answers to a three-hop query.","Our contributions are: (1) We present a new parallel algorithm to enhance MHR performance, scalability and efficiency.","(2) We demonstrate the algorithm's superior performance on leading-edge Intel and AMD architectures through empirical results.   ","We showcase the algorithm's practicality through a case study on identifying academic affiliations of potential Turing Award laureates in Deep Learning, highlighting its capability to handle intricate entity relationships.","This demonstrates the potential of our approach to enabling high-performance MHR, useful to navigate the growing complexity of modern knowledge graphs."],"url":"http://arxiv.org/abs/2406.07727v1","category":"cs.AI"}
{"created":"2024-06-11 21:03:09","title":"On the Effects of Non-metricity in an Averaged Universe","abstract":"In the covariant averaging scheme of macroscopic gravity, the process of averaging breaks the metricity of geometry. We reinterpret the back-reaction within macroscopic gravity in terms of the non-metricity of averaged geometry. This interpretation extends the effect of back-reaction beyond mere dynamics to kinematics of geodesic bundles. With a 1+3 decomposition of the spacetime, we analyse how geometric flows are modified by deriving the Raychaudhuri and Sachs equations. We also present the modified forms of Gauss and Codazzi equations. Finally, we derive an expression for the angular diameter distance in Friedmann Lema\\^itre Robertson Walker universe and show that non-metricity modifies it only through the Hubble parameter. Thus, we caution against overestimating the influence of back-reaction on the distances.","sentences":["In the covariant averaging scheme of macroscopic gravity, the process of averaging breaks the metricity of geometry.","We reinterpret the back-reaction within macroscopic gravity in terms of the non-metricity of averaged geometry.","This interpretation extends the effect of back-reaction beyond mere dynamics to kinematics of geodesic bundles.","With a 1+3 decomposition of the spacetime, we analyse how geometric flows are modified by deriving the Raychaudhuri and Sachs equations.","We also present the modified forms of Gauss and Codazzi equations.","Finally, we derive an expression for the angular diameter distance in Friedmann Lema\\^itre Robertson Walker universe and show that non-metricity modifies it only through the Hubble parameter.","Thus, we caution against overestimating the influence of back-reaction on the distances."],"url":"http://arxiv.org/abs/2406.07722v1","category":"gr-qc"}
{"created":"2024-06-11 20:51:50","title":"Coin-Flipping In The Brain: Statistical Learning with Neuronal Assemblies","abstract":"How intelligence arises from the brain is a central problem in science. A crucial aspect of intelligence is dealing with uncertainty -- developing good predictions about one's environment, and converting these predictions into decisions. The brain itself seems to be noisy at many levels, from chemical processes which drive development and neuronal activity to trial variability of responses to stimuli. One hypothesis is that the noise inherent to the brain's mechanisms is used to sample from a model of the world and generate predictions. To test this hypothesis, we study the emergence of statistical learning in NEMO, a biologically plausible computational model of the brain based on stylized neurons and synapses, plasticity, and inhibition, and giving rise to assemblies -- a group of neurons whose coordinated firing is tantamount to recalling a location, concept, memory, or other primitive item of cognition. We show in theory and simulation that connections between assemblies record statistics, and ambient noise can be harnessed to make probabilistic choices between assemblies. This allows NEMO to create internal models such as Markov chains entirely from the presentation of sequences of stimuli. Our results provide a foundation for biologically plausible probabilistic computation, and add theoretical support to the hypothesis that noise is a useful component of the brain's mechanism for cognition.","sentences":["How intelligence arises from the brain is a central problem in science.","A crucial aspect of intelligence is dealing with uncertainty -- developing good predictions about one's environment, and converting these predictions into decisions.","The brain itself seems to be noisy at many levels, from chemical processes which drive development and neuronal activity to trial variability of responses to stimuli.","One hypothesis is that the noise inherent to the brain's mechanisms is used to sample from a model of the world and generate predictions.","To test this hypothesis, we study the emergence of statistical learning in NEMO, a biologically plausible computational model of the brain based on stylized neurons and synapses, plasticity, and inhibition, and giving rise to assemblies -- a group of neurons whose coordinated firing is tantamount to recalling a location, concept, memory, or other primitive item of cognition.","We show in theory and simulation that connections between assemblies record statistics, and ambient noise can be harnessed to make probabilistic choices between assemblies.","This allows NEMO to create internal models such as Markov chains entirely from the presentation of sequences of stimuli.","Our results provide a foundation for biologically plausible probabilistic computation, and add theoretical support to the hypothesis that noise is a useful component of the brain's mechanism for cognition."],"url":"http://arxiv.org/abs/2406.07715v1","category":"q-bio.NC"}
{"created":"2024-06-11 20:48:28","title":"LLAMAFUZZ: Large Language Model Enhanced Greybox Fuzzing","abstract":"Greybox fuzzing has achieved success in revealing bugs and vulnerabilities in programs. However, randomized mutation strategies have limited the fuzzer's performance on structured data. Specialized fuzzers can handle complex structured data, but require additional efforts in grammar and suffer from low throughput.   In this paper, we explore the potential of utilizing the Large Language Model to enhance greybox fuzzing for structured data. We utilize the pre-trained knowledge of LLM about data conversion and format to generate new valid inputs. We further fine-tuned it with paired mutation seeds to learn structured format and mutation strategies effectively. Our LLM-based fuzzer, LLAMAFUZZ, integrates the power of LLM to understand and mutate structured data to fuzzing. We conduct experiments on the standard bug-based benchmark Magma and a wide variety of real-world programs. LLAMAFUZZ outperforms our top competitor by 41 bugs on average. We also identified 47 unique bugs across all trials. Moreover, LLAMAFUZZ demonstrated consistent performance on both bug trigger and bug reached. Compared to AFL++, LLAMAFUZZ achieved 27.19% more branches in real-world program sets on average. We also demonstrate a case study to explain how LLMs enhance the fuzzing process in terms of code coverage.","sentences":["Greybox fuzzing has achieved success in revealing bugs and vulnerabilities in programs.","However, randomized mutation strategies have limited the fuzzer's performance on structured data.","Specialized fuzzers can handle complex structured data, but require additional efforts in grammar and suffer from low throughput.   ","In this paper, we explore the potential of utilizing the Large Language Model to enhance greybox fuzzing for structured data.","We utilize the pre-trained knowledge of LLM about data conversion and format to generate new valid inputs.","We further fine-tuned it with paired mutation seeds to learn structured format and mutation strategies effectively.","Our LLM-based fuzzer, LLAMAFUZZ, integrates the power of LLM to understand and mutate structured data to fuzzing.","We conduct experiments on the standard bug-based benchmark Magma and a wide variety of real-world programs.","LLAMAFUZZ outperforms our top competitor by 41 bugs on average.","We also identified 47 unique bugs across all trials.","Moreover, LLAMAFUZZ demonstrated consistent performance on both bug trigger and bug reached.","Compared to AFL++, LLAMAFUZZ achieved 27.19% more branches in real-world program sets on average.","We also demonstrate a case study to explain how LLMs enhance the fuzzing process in terms of code coverage."],"url":"http://arxiv.org/abs/2406.07714v1","category":"cs.CR"}
{"created":"2024-06-11 20:26:41","title":"CUPID: Contextual Understanding of Prompt-conditioned Image Distributions","abstract":"We present CUPID: a visualization method for the contextual understanding of prompt-conditioned image distributions. CUPID targets the visual analysis of distributions produced by modern text-to-image generative models, wherein a user can specify a scene via natural language, and the model generates a set of images, each intended to satisfy the user's description. CUPID is designed to help understand the resulting distribution, using contextual cues to facilitate analysis: objects mentioned in the prompt, novel, synthesized objects not explicitly mentioned, and their potential relationships. Central to CUPID is a novel method for visualizing high-dimensional distributions, wherein contextualized embeddings of objects, those found within images, are mapped to a low-dimensional space via density-based embeddings. We show how such embeddings allows one to discover salient styles of objects within a distribution, as well as identify anomalous, or rare, object styles. Moreover, we introduce conditional density embeddings, whereby conditioning on a given object allows one to compare object dependencies within the distribution. We employ CUPID for analyzing image distributions produced by large-scale diffusion models, where our experimental results offer insights on language misunderstanding from such models and biases in object composition, while also providing an interface for discovery of typical, or rare, synthesized scenes.","sentences":["We present CUPID: a visualization method for the contextual understanding of prompt-conditioned image distributions.","CUPID targets the visual analysis of distributions produced by modern text-to-image generative models, wherein a user can specify a scene via natural language, and the model generates a set of images, each intended to satisfy the user's description.","CUPID is designed to help understand the resulting distribution, using contextual cues to facilitate analysis: objects mentioned in the prompt, novel, synthesized objects not explicitly mentioned, and their potential relationships.","Central to CUPID is a novel method for visualizing high-dimensional distributions, wherein contextualized embeddings of objects, those found within images, are mapped to a low-dimensional space via density-based embeddings.","We show how such embeddings allows one to discover salient styles of objects within a distribution, as well as identify anomalous, or rare, object styles.","Moreover, we introduce conditional density embeddings, whereby conditioning on a given object allows one to compare object dependencies within the distribution.","We employ CUPID for analyzing image distributions produced by large-scale diffusion models, where our experimental results offer insights on language misunderstanding from such models and biases in object composition, while also providing an interface for discovery of typical, or rare, synthesized scenes."],"url":"http://arxiv.org/abs/2406.07699v1","category":"cs.CV"}
{"created":"2024-06-11 20:21:36","title":"Sustainable self-supervised learning for speech representations","abstract":"Sustainable artificial intelligence focuses on data, hardware, and algorithms to make machine learning models more environmentally responsible. In particular, machine learning models for speech representations are computationally expensive, generating environmental concerns because of their high energy consumption. Thus, we propose a sustainable self-supervised model to learn speech representation, combining optimizations in neural layers and training to reduce computing costs. The proposed model improves over a resource-efficient baseline, reducing both memory usage and computing cost estimations. It pretrains using a single GPU in less than a day. On top of that, it improves the error rate performance of the baseline in downstream task evaluations. When comparing it to large speech representation approaches, there is an order of magnitude reduction in memory usage, while computing cost reductions represent almost three orders of magnitude improvement.","sentences":["Sustainable artificial intelligence focuses on data, hardware, and algorithms to make machine learning models more environmentally responsible.","In particular, machine learning models for speech representations are computationally expensive, generating environmental concerns because of their high energy consumption.","Thus, we propose a sustainable self-supervised model to learn speech representation, combining optimizations in neural layers and training to reduce computing costs.","The proposed model improves over a resource-efficient baseline, reducing both memory usage and computing cost estimations.","It pretrains using a single GPU in less than a day.","On top of that, it improves the error rate performance of the baseline in downstream task evaluations.","When comparing it to large speech representation approaches, there is an order of magnitude reduction in memory usage, while computing cost reductions represent almost three orders of magnitude improvement."],"url":"http://arxiv.org/abs/2406.07696v1","category":"cs.CL"}
{"created":"2024-06-11 20:14:22","title":"A Labelled Dataset for Sentiment Analysis of Videos on YouTube, TikTok, and Other Sources about the 2024 Outbreak of Measles","abstract":"The work of this paper presents a dataset that contains the data of 4011 videos about the ongoing outbreak of measles published on 264 websites on the internet between January 1, 2024, and May 31, 2024. The dataset is available at https://dx.doi.org/10.21227/40s8-xf63. These websites primarily include YouTube and TikTok, which account for 48.6% and 15.2% of the videos, respectively. The remainder of the websites include Instagram and Facebook as well as the websites of various global and local news organizations. For each of these videos, the URL of the video, title of the post, description of the post, and the date of publication of the video are presented as separate attributes in the dataset. After developing this dataset, sentiment analysis (using VADER), subjectivity analysis (using TextBlob), and fine-grain sentiment analysis (using DistilRoBERTa-base) of the video titles and video descriptions were performed. This included classifying each video title and video description into (i) one of the sentiment classes i.e. positive, negative, or neutral, (ii) one of the subjectivity classes i.e. highly opinionated, neutral opinionated, or least opinionated, and (iii) one of the fine-grain sentiment classes i.e. fear, surprise, joy, sadness, anger, disgust, or neutral. These results are presented as separate attributes in the dataset for the training and testing of machine learning algorithms for performing sentiment analysis or subjectivity analysis in this field as well as for other applications. Finally, this paper also presents a list of open research questions that may be investigated using this dataset.","sentences":["The work of this paper presents a dataset that contains the data of 4011 videos about the ongoing outbreak of measles published on 264 websites on the internet between January 1, 2024, and May 31, 2024.","The dataset is available at https://dx.doi.org/10.21227/40s8-xf63.","These websites primarily include YouTube and TikTok, which account for 48.6% and 15.2% of the videos, respectively.","The remainder of the websites include Instagram and Facebook as well as the websites of various global and local news organizations.","For each of these videos, the URL of the video, title of the post, description of the post, and the date of publication of the video are presented as separate attributes in the dataset.","After developing this dataset, sentiment analysis (using VADER), subjectivity analysis (using TextBlob), and fine-grain sentiment analysis (using DistilRoBERTa-base) of the video titles and video descriptions were performed.","This included classifying each video title and video description into (i) one of the sentiment classes i.e. positive, negative, or neutral, (ii) one of the subjectivity classes i.e. highly opinionated, neutral opinionated, or least opinionated, and (iii) one of the fine-grain sentiment classes i.e. fear, surprise, joy, sadness, anger, disgust, or neutral.","These results are presented as separate attributes in the dataset for the training and testing of machine learning algorithms for performing sentiment analysis or subjectivity analysis in this field as well as for other applications.","Finally, this paper also presents a list of open research questions that may be investigated using this dataset."],"url":"http://arxiv.org/abs/2406.07693v1","category":"cs.CY"}
{"created":"2024-06-11 20:10:16","title":"AI Radiologist: Revolutionizing Liver Tissue Segmentation with Convolutional Neural Networks and a Clinician-Friendly GUI","abstract":"Artificial Intelligence (AI) is a pervasive research topic, permeating various sectors and applications. In this study, we harness the power of AI, specifically convolutional neural networks (ConvNets), for segmenting liver tissues. It also focuses on developing a user-friendly graphical user interface (GUI) tool, \"AI Radiologist\", enabling clinicians to effectively delineate different liver tissues (parenchyma, tumors, and vessels), thereby saving lives. This endeavor bridges the gap between academic research and practical, industrial applications. The GUI is a single-page application and is designed using the PyQt5 Python framework. The offline-available AI Radiologist resorts to three ConvNet models trained to segment all liver tissues. With respect to the Dice metric, the best liver ConvNet scores 98.16%, the best tumor ConvNet scores 65.95%, and the best vessel ConvNet scores 51.94%. It outputs 2D slices of the liver, tumors, and vessels, along with 3D interpolations in .obj and .mtl formats, which can be visualized/printed using any 3D-compatible software. Thus, the AI Radiologist offers a convenient tool for clinicians to perform liver tissue segmentation and 3D interpolation employing state-of-the-art models for tissues segmentation. With the provided capacity to select the volumes and pre-trained models, the clinicians can leave the rest to the AI Radiologist.","sentences":["Artificial Intelligence (AI) is a pervasive research topic, permeating various sectors and applications.","In this study, we harness the power of AI, specifically convolutional neural networks (ConvNets), for segmenting liver tissues.","It also focuses on developing a user-friendly graphical user interface (GUI) tool, \"AI Radiologist\", enabling clinicians to effectively delineate different liver tissues (parenchyma, tumors, and vessels), thereby saving lives.","This endeavor bridges the gap between academic research and practical, industrial applications.","The GUI is a single-page application and is designed using the PyQt5 Python framework.","The offline-available AI Radiologist resorts to three ConvNet models trained to segment all liver tissues.","With respect to the Dice metric, the best liver ConvNet scores 98.16%, the best tumor ConvNet scores 65.95%, and the best vessel ConvNet scores 51.94%.","It outputs 2D slices of the liver, tumors, and vessels, along with 3D interpolations in .obj","and .mtl","formats, which can be visualized/printed using any 3D-compatible software.","Thus, the AI Radiologist offers a convenient tool for clinicians to perform liver tissue segmentation and 3D interpolation employing state-of-the-art models for tissues segmentation.","With the provided capacity to select the volumes and pre-trained models, the clinicians can leave the rest to the AI Radiologist."],"url":"http://arxiv.org/abs/2406.07688v1","category":"cs.CV"}
{"created":"2024-06-11 20:05:15","title":"Out-Of-Context Prompting Boosts Fairness and Robustness in Large Language Model Predictions","abstract":"Frontier Large Language Models (LLMs) are increasingly being deployed for high-stakes decision-making. On the other hand, these models are still consistently making predictions that contradict users' or society's expectations, e.g., hallucinating, or discriminating. Thus, it is important that we develop test-time strategies to improve their trustworthiness. Inspired by prior work, we leverage causality as a tool to formally encode two aspects of trustworthiness in LLMs: fairness and robustness. Under this perspective, existing test-time solutions explicitly instructing the model to be fair or robust implicitly depend on the LLM's causal reasoning capabilities. In this work, we explore the opposite approach. Instead of explicitly asking the LLM for trustworthiness, we design prompts to encode the underlying causal inference algorithm that will, by construction, result in more trustworthy predictions. Concretely, we propose out-of-context prompting as a test-time solution to encourage fairness and robustness in LLMs. Out-of-context prompting leverages the user's prior knowledge of the task's causal model to apply (random) counterfactual transformations and improve the model's trustworthiness. Empirically, we show that out-of-context prompting consistently improves the fairness and robustness of frontier LLMs across five different benchmark datasets without requiring additional data, finetuning or pre-training.","sentences":["Frontier Large Language Models (LLMs) are increasingly being deployed for high-stakes decision-making.","On the other hand, these models are still consistently making predictions that contradict users' or society's expectations, e.g., hallucinating, or discriminating.","Thus, it is important that we develop test-time strategies to improve their trustworthiness.","Inspired by prior work, we leverage causality as a tool to formally encode two aspects of trustworthiness in LLMs: fairness and robustness.","Under this perspective, existing test-time solutions explicitly instructing the model to be fair or robust implicitly depend on the LLM's causal reasoning capabilities.","In this work, we explore the opposite approach.","Instead of explicitly asking the LLM for trustworthiness, we design prompts to encode the underlying causal inference algorithm that will, by construction, result in more trustworthy predictions.","Concretely, we propose out-of-context prompting as a test-time solution to encourage fairness and robustness in LLMs.","Out-of-context prompting leverages the user's prior knowledge of the task's causal model to apply (random) counterfactual transformations and improve the model's trustworthiness.","Empirically, we show that out-of-context prompting consistently improves the fairness and robustness of frontier LLMs across five different benchmark datasets without requiring additional data, finetuning or pre-training."],"url":"http://arxiv.org/abs/2406.07685v1","category":"cs.CL"}
{"created":"2024-06-11 20:04:09","title":"Impact of AI-tooling on the Engineering Workspace","abstract":"To understand the impacts of AI-driven coding tools on engineers' workflow and work environment, we utilize the Jellyfish platform to analyze indicators of change. Key indicators are derived from Allocations, Coding Fraction vs. PR Fraction, Lifecycle Phases, Cycle Time, Jira ticket size, PR pickup time, PR comments, PR comment count, interactions, and coding languages. Significant changes were observed in coding time fractions among Copilot users, with an average decrease of 3% with individual decreases as large as 15%. Ticket sizes decreased by an average of 16% across four companies, accompanied by an 8% decrease in cycle times, whereas the control group showed no change. Additionally, the PR process evolved with Copilot usage, featuring longer and more comprehensive comments, despite the weekly number of PRs reviewed remaining constant. Not all hypothesized changes were observed across all participating companies. However, some companies experienced a decrease in PR pickup times by up to 33%, indicating reduced workflow bottlenecks, and one company experienced a shift of up to 17% of effort from maintenance and support work towards product growth initiatives. This study is the first to utilize data from more than one company and goes beyond simple productivity and satisfaction measures, considering real-world engineering settings instead. By doing so, we highlight that some companies seem to benefit more than others from the use of Copilot and that changes can be subtle when investigating aggregates rather than specific aspects of engineering work and workflows - something that will be further investigated in the future.","sentences":["To understand the impacts of AI-driven coding tools on engineers' workflow and work environment, we utilize the Jellyfish platform to analyze indicators of change.","Key indicators are derived from Allocations, Coding Fraction vs. PR Fraction, Lifecycle Phases, Cycle Time, Jira ticket size, PR pickup time, PR comments, PR comment count, interactions, and coding languages.","Significant changes were observed in coding time fractions among Copilot users, with an average decrease of 3% with individual decreases as large as 15%.","Ticket sizes decreased by an average of 16% across four companies, accompanied by an 8% decrease in cycle times, whereas the control group showed no change.","Additionally, the PR process evolved with Copilot usage, featuring longer and more comprehensive comments, despite the weekly number of PRs reviewed remaining constant.","Not all hypothesized changes were observed across all participating companies.","However, some companies experienced a decrease in PR pickup times by up to 33%, indicating reduced workflow bottlenecks, and one company experienced a shift of up to 17% of effort from maintenance and support work towards product growth initiatives.","This study is the first to utilize data from more than one company and goes beyond simple productivity and satisfaction measures, considering real-world engineering settings instead.","By doing so, we highlight that some companies seem to benefit more than others from the use of Copilot and that changes can be subtle when investigating aggregates rather than specific aspects of engineering work and workflows - something that will be further investigated in the future."],"url":"http://arxiv.org/abs/2406.07683v1","category":"cs.SE"}
{"created":"2024-06-11 19:57:00","title":"Watching Swarm Dynamics from Above: A Framework for Advanced Object Tracking in Drone Videos","abstract":"Easily accessible sensors, like drones with diverse onboard sensors, have greatly expanded studying animal behavior in natural environments. Yet, analyzing vast, unlabeled video data, often spanning hours, remains a challenge for machine learning, especially in computer vision. Existing approaches often analyze only a few frames. Our focus is on long-term animal behavior analysis. To address this challenge, we utilize classical probabilistic methods for state estimation, such as particle filtering. By incorporating recent advancements in semantic object segmentation, we enable continuous tracking of rapidly evolving object formations, even in scenarios with limited data availability. Particle filters offer a provably optimal algorithmic structure for recursively adding new incoming information. We propose a novel approach for tracking schools of fish in the open ocean from drone videos. Our framework not only performs classical object tracking in 2D, instead it tracks the position and spatial expansion of the fish school in world coordinates by fusing video data and the drone's on board sensor information (GPS and IMU). The presented framework for the first time allows researchers to study collective behavior of fish schools in its natural social and environmental context in a non-invasive and scalable way.","sentences":["Easily accessible sensors, like drones with diverse onboard sensors, have greatly expanded studying animal behavior in natural environments.","Yet, analyzing vast, unlabeled video data, often spanning hours, remains a challenge for machine learning, especially in computer vision.","Existing approaches often analyze only a few frames.","Our focus is on long-term animal behavior analysis.","To address this challenge, we utilize classical probabilistic methods for state estimation, such as particle filtering.","By incorporating recent advancements in semantic object segmentation, we enable continuous tracking of rapidly evolving object formations, even in scenarios with limited data availability.","Particle filters offer a provably optimal algorithmic structure for recursively adding new incoming information.","We propose a novel approach for tracking schools of fish in the open ocean from drone videos.","Our framework not only performs classical object tracking in 2D, instead it tracks the position and spatial expansion of the fish school in world coordinates by fusing video data and the drone's on board sensor information (GPS and IMU).","The presented framework for the first time allows researchers to study collective behavior of fish schools in its natural social and environmental context in a non-invasive and scalable way."],"url":"http://arxiv.org/abs/2406.07680v1","category":"cs.CV"}
{"created":"2024-06-11 19:50:50","title":"FastAST: Accelerating Audio Spectrogram Transformer via Token Merging and Cross-Model Knowledge Distillation","abstract":"Audio classification models, particularly the Audio Spectrogram Transformer (AST), play a crucial role in efficient audio analysis. However, optimizing their efficiency without compromising accuracy remains a challenge. In this paper, we introduce FastAST, a framework that integrates Token Merging (ToMe) into the AST framework. FastAST enhances inference speed without requiring extensive retraining by merging similar tokens in audio spectrograms. Furthermore, during training, FastAST brings about significant speed improvements. The experiments indicate that FastAST can increase audio classification throughput with minimal impact on accuracy. To mitigate the accuracy impact, we integrate Cross-Model Knowledge Distillation (CMKD) into the FastAST framework. Integrating ToMe and CMKD into AST results in improved accuracy compared to AST while maintaining faster inference speeds. FastAST represents a step towards real-time, resource-efficient audio analysis.","sentences":["Audio classification models, particularly the Audio Spectrogram Transformer (AST), play a crucial role in efficient audio analysis.","However, optimizing their efficiency without compromising accuracy remains a challenge.","In this paper, we introduce FastAST, a framework that integrates Token Merging (ToMe) into the AST framework.","FastAST enhances inference speed without requiring extensive retraining by merging similar tokens in audio spectrograms.","Furthermore, during training, FastAST brings about significant speed improvements.","The experiments indicate that FastAST can increase audio classification throughput with minimal impact on accuracy.","To mitigate the accuracy impact, we integrate Cross-Model Knowledge Distillation (CMKD) into the FastAST framework.","Integrating ToMe and CMKD into AST results in improved accuracy compared to AST while maintaining faster inference speeds.","FastAST represents a step towards real-time, resource-efficient audio analysis."],"url":"http://arxiv.org/abs/2406.07676v1","category":"cs.SD"}
{"created":"2024-06-11 19:08:32","title":"Progress Towards Decoding Visual Imagery via fNIRS","abstract":"We demonstrate the possibility of reconstructing images from fNIRS brain activity and start building a prototype to match the required specs. By training an image reconstruction model on downsampled fMRI data, we discovered that cm-scale spatial resolution is sufficient for image generation. We obtained 71% retrieval accuracy with 1-cm resolution, compared to 93% on the full-resolution fMRI, and 20% with 2-cm resolution. With simulations and high-density tomography, we found that time-domain fNIRS can achieve 1-cm resolution, compared to 2-cm resolution for continuous-wave fNIRS. Lastly, we share designs for a prototype time-domain fNIRS device, consisting of a laser driver, a single photon detector, and a time-to-digital converter system.","sentences":["We demonstrate the possibility of reconstructing images from fNIRS brain activity and start building a prototype to match the required specs.","By training an image reconstruction model on downsampled fMRI data, we discovered that cm-scale spatial resolution is sufficient for image generation.","We obtained 71% retrieval accuracy with 1-cm resolution, compared to 93% on the full-resolution fMRI, and 20% with 2-cm resolution.","With simulations and high-density tomography, we found that time-domain fNIRS can achieve 1-cm resolution, compared to 2-cm resolution for continuous-wave fNIRS.","Lastly, we share designs for a prototype time-domain fNIRS device, consisting of a laser driver, a single photon detector, and a time-to-digital converter system."],"url":"http://arxiv.org/abs/2406.07662v1","category":"eess.IV"}
{"created":"2024-06-11 18:55:04","title":"OPTune: Efficient Online Preference Tuning","abstract":"Reinforcement learning with human feedback~(RLHF) is critical for aligning Large Language Models (LLMs) with human preference. Compared to the widely studied offline version of RLHF, \\emph{e.g.} direct preference optimization (DPO), recent works have shown that the online variants achieve even better alignment. However, online alignment requires on-the-fly generation of new training data, which is costly, hard to parallelize, and suffers from varying quality and utility. In this paper, we propose a more efficient data exploration strategy for online preference tuning (OPTune), which does not rely on human-curated or pre-collected teacher responses but dynamically samples informative responses for on-policy preference alignment. During data generation, OPTune only selects prompts whose (re)generated responses can potentially provide more informative and higher-quality training signals than the existing responses. In the training objective, OPTune reweights each generated response (pair) by its utility in improving the alignment so that learning can be focused on the most helpful samples. Throughout our evaluations, OPTune'd LLMs maintain the instruction-following benefits provided by standard preference tuning whilst enjoying 1.27-1.56x faster training speed due to the efficient data exploration strategy.","sentences":["Reinforcement learning with human feedback~(RLHF) is critical for aligning Large Language Models (LLMs) with human preference.","Compared to the widely studied offline version of RLHF, \\emph{e.g.} direct preference optimization (DPO), recent works have shown that the online variants achieve even better alignment.","However, online alignment requires on-the-fly generation of new training data, which is costly, hard to parallelize, and suffers from varying quality and utility.","In this paper, we propose a more efficient data exploration strategy for online preference tuning (OPTune), which does not rely on human-curated or pre-collected teacher responses but dynamically samples informative responses for on-policy preference alignment.","During data generation, OPTune only selects prompts whose (re)generated responses can potentially provide more informative and higher-quality training signals than the existing responses.","In the training objective, OPTune reweights each generated response (pair) by its utility in improving the alignment so that learning can be focused on the most helpful samples.","Throughout our evaluations, OPTune'd LLMs maintain the instruction-following benefits provided by standard preference tuning whilst enjoying 1.27-1.56x faster training speed due to the efficient data exploration strategy."],"url":"http://arxiv.org/abs/2406.07657v1","category":"cs.LG"}
{"created":"2024-06-11 18:22:59","title":"Pre-training Feature Guided Diffusion Model for Speech Enhancement","abstract":"Speech enhancement significantly improves the clarity and intelligibility of speech in noisy environments, improving communication and listening experiences. In this paper, we introduce a novel pretraining feature-guided diffusion model tailored for efficient speech enhancement, addressing the limitations of existing discriminative and generative models. By integrating spectral features into a variational autoencoder (VAE) and leveraging pre-trained features for guidance during the reverse process, coupled with the utilization of the deterministic discrete integration method (DDIM) to streamline sampling steps, our model improves efficiency and speech enhancement quality. Demonstrating state-of-the-art results on two public datasets with different SNRs, our model outshines other baselines in efficiency and robustness. The proposed method not only optimizes performance but also enhances practical deployment capabilities, without increasing computational demands.","sentences":["Speech enhancement significantly improves the clarity and intelligibility of speech in noisy environments, improving communication and listening experiences.","In this paper, we introduce a novel pretraining feature-guided diffusion model tailored for efficient speech enhancement, addressing the limitations of existing discriminative and generative models.","By integrating spectral features into a variational autoencoder (VAE) and leveraging pre-trained features for guidance during the reverse process, coupled with the utilization of the deterministic discrete integration method (DDIM) to streamline sampling steps, our model improves efficiency and speech enhancement quality.","Demonstrating state-of-the-art results on two public datasets with different SNRs, our model outshines other baselines in efficiency and robustness.","The proposed method not only optimizes performance but also enhances practical deployment capabilities, without increasing computational demands."],"url":"http://arxiv.org/abs/2406.07646v1","category":"cs.SD"}
{"created":"2024-06-11 18:13:46","title":"When is an Embedding Model More Promising than Another?","abstract":"Embedders play a central role in machine learning, projecting any object into numerical representations that can, in turn, be leveraged to perform various downstream tasks. The evaluation of embedding models typically depends on domain-specific empirical approaches utilizing downstream tasks, primarily because of the lack of a standardized framework for comparison. However, acquiring adequately large and representative datasets for conducting these assessments is not always viable and can prove to be prohibitively expensive and time-consuming. In this paper, we present a unified approach to evaluate embedders. First, we establish theoretical foundations for comparing embedding models, drawing upon the concepts of sufficiency and informativeness. We then leverage these concepts to devise a tractable comparison criterion (information sufficiency), leading to a task-agnostic and self-supervised ranking procedure. We demonstrate experimentally that our approach aligns closely with the capability of embedding models to facilitate various downstream tasks in both natural language processing and molecular biology. This effectively offers practitioners a valuable tool for prioritizing model trials.","sentences":["Embedders play a central role in machine learning, projecting any object into numerical representations that can, in turn, be leveraged to perform various downstream tasks.","The evaluation of embedding models typically depends on domain-specific empirical approaches utilizing downstream tasks, primarily because of the lack of a standardized framework for comparison.","However, acquiring adequately large and representative datasets for conducting these assessments is not always viable and can prove to be prohibitively expensive and time-consuming.","In this paper, we present a unified approach to evaluate embedders.","First, we establish theoretical foundations for comparing embedding models, drawing upon the concepts of sufficiency and informativeness.","We then leverage these concepts to devise a tractable comparison criterion (information sufficiency), leading to a task-agnostic and self-supervised ranking procedure.","We demonstrate experimentally that our approach aligns closely with the capability of embedding models to facilitate various downstream tasks in both natural language processing and molecular biology.","This effectively offers practitioners a valuable tool for prioritizing model trials."],"url":"http://arxiv.org/abs/2406.07640v1","category":"cs.LG"}
{"created":"2024-06-11 18:03:07","title":"A gamma-ray flare from TXS 1508+572: characterizing the jet of a $z=4.31$ blazar in the early Universe","abstract":"Blazars can be detected from very large distances due to their high luminosity. However, the detection of $\\gamma$-ray emission of blazars beyond $z=3$ has only been confirmed for a small number of sources. Such observations probe the growth of supermassive black holes close to the peak of star formation in the history of galaxy evolution. As a result from a continuous monitoring of a sample of 80 $z>3$ blazars with Fermi-LAT, we present the first detection of a $\\gamma$-ray flare from the $z=4.31$ blazar TXS 1508+572. This source showed high $\\gamma$-ray activity from February to August 2022, reaching a peak luminosity comparable to the most luminous flares ever detected with Fermi -LAT. We conducted a multiwavelength observing campaign involving XMM-Newton, Swift, the Effelsberg 100-m radio telescope and the Very Long Baseline Array. In addition, we make use of the monitoring programs by the Zwicky Transient Facility and NEOWISE at optical and infrared wavelengths, respectively. We find that the source is particularly variable in the infrared band on daily time scales. The spectral energy distribution collected during our campaign is well described by a one-zone leptonic model, with the $\\gamma$-ray flare originating from an increase of external Compton emission as a result of a fresh injection of accelerated electrons.","sentences":["Blazars can be detected from very large distances due to their high luminosity.","However, the detection of $\\gamma$-ray emission of blazars beyond $z=3$ has only been confirmed for a small number of sources.","Such observations probe the growth of supermassive black holes close to the peak of star formation in the history of galaxy evolution.","As a result from a continuous monitoring of a sample of 80 $z>3$ blazars with Fermi-LAT, we present the first detection of a $\\gamma$-ray flare from the $z=4.31$ blazar TXS 1508+572.","This source showed high $\\gamma$-ray activity from February to August 2022, reaching a peak luminosity comparable to the most luminous flares ever detected with Fermi -LAT.","We conducted a multiwavelength observing campaign involving XMM-Newton, Swift, the Effelsberg 100-m radio telescope and the Very Long Baseline Array.","In addition, we make use of the monitoring programs by the Zwicky Transient Facility and NEOWISE at optical and infrared wavelengths, respectively.","We find that the source is particularly variable in the infrared band on daily time scales.","The spectral energy distribution collected during our campaign is well described by a one-zone leptonic model, with the $\\gamma$-ray flare originating from an increase of external Compton emission as a result of a fresh injection of accelerated electrons."],"url":"http://arxiv.org/abs/2406.07635v1","category":"astro-ph.HE"}
{"created":"2024-06-11 17:53:06","title":"IceCube Search for Neutrino Emission from X-ray Bright Seyfert Galaxies","abstract":"The recent IceCube detection of TeV neutrino emission from the nearby active galaxy NGC 1068 suggests that active galactic nuclei (AGN) could make a sizable contribution to the diffuse flux of astrophysical neutrinos. The absence of TeV $\\gamma$-rays from NGC 1068 indicates neutrino production in the vicinity of the supermassive black hole, where the high radiation density leads to $\\gamma$-ray attenuation. Therefore, any potential neutrino emission from similar sources is not expected to correlate with high-energy $\\gamma$-rays. Disk-corona models predict neutrino emission from Seyfert galaxies to correlate with keV X-rays, as they are tracers of coronal activity. Using through-going track events from the Northern Sky recorded by IceCube between 2011 and 2021, we report results from a search for individual and aggregated neutrino signals from 27 additional Seyfert galaxies that are contained in the BAT AGN Spectroscopic Survey (BASS). Besides the generic single power-law, we evaluate the spectra predicted by the disk-corona model. Assuming all sources to be intrinsically similar to NGC 1068, our findings constrain the collective neutrino emission from X-ray bright Seyfert galaxies in the Northern Hemisphere, but, at the same time, show excesses of neutrinos that could be associated with the objects NGC 4151 and CGCG 420-015. These excesses result in a 2.7$\\sigma$ significance with respect to background expectations.","sentences":["The recent IceCube detection of TeV neutrino emission from the nearby active galaxy NGC 1068 suggests that active galactic nuclei (AGN) could make a sizable contribution to the diffuse flux of astrophysical neutrinos.","The absence of TeV $\\gamma$-rays from NGC 1068 indicates neutrino production in the vicinity of the supermassive black hole, where the high radiation density leads to $\\gamma$-ray attenuation.","Therefore, any potential neutrino emission from similar sources is not expected to correlate with high-energy $\\gamma$-rays.","Disk-corona models predict neutrino emission from Seyfert galaxies to correlate with keV X-rays, as they are tracers of coronal activity.","Using through-going track events from the Northern Sky recorded by IceCube between 2011 and 2021, we report results from a search for individual and aggregated neutrino signals from 27 additional Seyfert galaxies that are contained in the BAT AGN Spectroscopic Survey (BASS).","Besides the generic single power-law, we evaluate the spectra predicted by the disk-corona model.","Assuming all sources to be intrinsically similar to NGC 1068, our findings constrain the collective neutrino emission from X-ray bright Seyfert galaxies in the Northern Hemisphere, but, at the same time, show excesses of neutrinos that could be associated with the objects NGC 4151 and CGCG 420-015.","These excesses result in a 2.7$\\sigma$ significance with respect to background expectations."],"url":"http://arxiv.org/abs/2406.07601v1","category":"astro-ph.HE"}
{"created":"2024-06-12 17:59:05","title":"Improving LLMs for Recommendation with Out-Of-Vocabulary Tokens","abstract":"Characterizing users and items through vector representations is crucial for various tasks in recommender systems. Recent approaches attempt to apply Large Language Models (LLMs) in recommendation through a question and answer format, where real users and items (e.g., Item No.2024) are represented with in-vocabulary tokens (e.g., \"item\", \"20\", \"24\"). However, since LLMs are typically pretrained on natural language tasks, these in-vocabulary tokens lack the expressive power for distinctive users and items, thereby weakening the recommendation ability even after fine-tuning on recommendation tasks. In this paper, we explore how to effectively tokenize users and items in LLM-based recommender systems. We emphasize the role of out-of-vocabulary (OOV) tokens in addition to the in-vocabulary ones and claim the memorization of OOV tokens that capture correlations of users/items as well as diversity of OOV tokens. By clustering the learned representations from historical user-item interactions, we make the representations of user/item combinations share the same OOV tokens if they have similar properties. Furthermore, integrating these OOV tokens into the LLM's vocabulary allows for better distinction between users and items and enhanced capture of user-item relationships during fine-tuning on downstream tasks. Our proposed framework outperforms existing state-of-the-art methods across various downstream recommendation tasks.","sentences":["Characterizing users and items through vector representations is crucial for various tasks in recommender systems.","Recent approaches attempt to apply Large Language Models (LLMs) in recommendation through a question and answer format, where real users and items (e.g., Item No.2024) are represented with in-vocabulary tokens (e.g., \"item\", \"20\", \"24\").","However, since LLMs are typically pretrained on natural language tasks, these in-vocabulary tokens lack the expressive power for distinctive users and items, thereby weakening the recommendation ability even after fine-tuning on recommendation tasks.","In this paper, we explore how to effectively tokenize users and items in LLM-based recommender systems.","We emphasize the role of out-of-vocabulary (OOV) tokens in addition to the in-vocabulary ones and claim the memorization of OOV tokens that capture correlations of users/items as well as diversity of OOV tokens.","By clustering the learned representations from historical user-item interactions, we make the representations of user/item combinations share the same OOV tokens if they have similar properties.","Furthermore, integrating these OOV tokens into the LLM's vocabulary allows for better distinction between users and items and enhanced capture of user-item relationships during fine-tuning on downstream tasks.","Our proposed framework outperforms existing state-of-the-art methods across various downstream recommendation tasks."],"url":"http://arxiv.org/abs/2406.08477v1","category":"cs.IR"}
{"created":"2024-06-12 17:51:18","title":"Critical Lambda-adic modular forms and bi-ordinary complexes","abstract":"We produce a flat $\\Lambda$-module of $\\Lambda$-adic critical slope overconvergent modular forms, producing a Hida-type theory that interpolates such forms over $p$-adically varying integer weights. This provides a Hida-theoretic explanation for an observation of Coleman that the rank of such forms is locally constant in the weight. The key to the interpolation is to use Coleman's presentation of de Rham cohomology in terms of overconvergent forms to link critical slope overconvergent modular forms with the ordinary part of 1st coherent cohomology of modular curves interpolated by Boxer--Pilloni's higher Hida theory. We also set up a Galois deformation theory designed to conform to these critical overconvergent forms and prove \"$R = T$\" for it. As applications, we (1) produce a \"formal\" $\\Lambda$-adic interpolation of the ordinary part of de Rham cohomology, (2) produce a bi-ordinary complex whose Hecke algebra $T$ is a natural candidate for an \"$R = T$\" theorem where $R$ is a deformation ring for 2-dimensional $p$-adic representations of ${\\rm Gal}(\\bar{\\bf Q}/{\\bf Q})$ that become reducible and decomposable upon restriction to a decomposition group at $p$, (3) produce a degree-shifting Hecke action on the cohomology of the bi-ordinary complex, and (4) specialize this degree-shifting action to weight 1 and apply the critical \"$R = T$\" theorem to find, under a supplemental assumption, an action of a Stark unit on the part of weight 1 coherent cohomology over ${\\bf Z}_p$ that is isotypic for an ordinary eigenform with complex multiplication.","sentences":["We produce a flat $\\Lambda$-module of $\\Lambda$-adic critical slope overconvergent modular forms, producing a Hida-type theory that interpolates such forms over $p$-adically varying integer weights.","This provides a Hida-theoretic explanation for an observation of Coleman that the rank of such forms is locally constant in the weight.","The key to the interpolation is to use Coleman's presentation of de Rham cohomology in terms of overconvergent forms to link critical slope overconvergent modular forms with the ordinary part of 1st coherent cohomology of modular curves interpolated by Boxer--Pilloni's higher Hida theory.","We also set up a Galois deformation theory designed to conform to these critical overconvergent forms and prove \"$R = T$\" for it.","As applications, we (1) produce a \"formal\" $\\Lambda$-adic interpolation of the ordinary part of de Rham cohomology, (2) produce a bi-ordinary complex whose Hecke algebra $T$ is a natural candidate for an \"$R = T$\" theorem where $R$ is a deformation ring for 2-dimensional $p$-adic representations of ${\\rm Gal}(\\bar{\\bf Q}/{\\bf Q})$ that become reducible and decomposable upon restriction to a decomposition group at $p$, (3) produce a degree-shifting Hecke action on the cohomology of the bi-ordinary complex, and (4) specialize this degree-shifting action to weight 1 and apply the critical \"$R = T$\" theorem to find, under a supplemental assumption, an action of a Stark unit on the part of weight 1 coherent cohomology over ${\\bf Z}_p$ that is isotypic for an ordinary eigenform with complex multiplication."],"url":"http://arxiv.org/abs/2406.08460v1","category":"math.NT"}
{"created":"2024-06-12 17:45:04","title":"ORES-Inspect: A technology probe for machine learning audits on enwiki","abstract":"Auditing the machine learning (ML) models used on Wikipedia is important for ensuring that vandalism-detection processes remain fair and effective. However, conducting audits is challenging because stakeholders have diverse priorities and assembling evidence for a model's [in]efficacy is technically complex. We designed an interface to enable editors to learn about and audit the performance of the ORES edit quality model. ORES-Inspect is an open-source web tool and a provocative technology probe for researching how editors think about auditing the many ML models used on Wikipedia. We describe the design of ORES-Inspect and our plans for further research with this system.","sentences":["Auditing the machine learning (ML) models used on Wikipedia is important for ensuring that vandalism-detection processes remain fair and effective.","However, conducting audits is challenging because stakeholders have diverse priorities and assembling evidence for a model's [in]efficacy is technically complex.","We designed an interface to enable editors to learn about and audit the performance of the ORES edit quality model.","ORES-Inspect is an open-source web tool and a provocative technology probe for researching how editors think about auditing the many ML models used on Wikipedia.","We describe the design of ORES-Inspect and our plans for further research with this system."],"url":"http://arxiv.org/abs/2406.08453v1","category":"cs.HC"}
{"created":"2024-06-12 17:34:38","title":"PixMamba: Leveraging State Space Models in a Dual-Level Architecture for Underwater Image Enhancement","abstract":"Underwater Image Enhancement (UIE) is critical for marine research and exploration but hindered by complex color distortions and severe blurring. Recent deep learning-based methods have achieved remarkable results, yet these methods struggle with high computational costs and insufficient global modeling, resulting in locally under- or over- adjusted regions. We present PixMamba, a novel architecture, designed to overcome these challenges by leveraging State Space Models (SSMs) for efficient global dependency modeling. Unlike convolutional neural networks (CNNs) with limited receptive fields and transformer networks with high computational costs, PixMamba efficiently captures global contextual information while maintaining computational efficiency. Our dual-level strategy features the patch-level Efficient Mamba Net (EMNet) for reconstructing enhanced image feature and the pixel-level PixMamba Net (PixNet) to ensure fine-grained feature capturing and global consistency of enhanced image that were previously difficult to obtain. PixMamba achieves state-of-the-art performance across various underwater image datasets and delivers visually superior results. Code is available at: https://github.com/weitunglin/pixmamba.","sentences":["Underwater Image Enhancement (UIE) is critical for marine research and exploration but hindered by complex color distortions and severe blurring.","Recent deep learning-based methods have achieved remarkable results, yet these methods struggle with high computational costs and insufficient global modeling, resulting in locally under- or over- adjusted regions.","We present PixMamba, a novel architecture, designed to overcome these challenges by leveraging State Space Models (SSMs) for efficient global dependency modeling.","Unlike convolutional neural networks (CNNs) with limited receptive fields and transformer networks with high computational costs, PixMamba efficiently captures global contextual information while maintaining computational efficiency.","Our dual-level strategy features the patch-level Efficient Mamba Net (EMNet) for reconstructing enhanced image feature and the pixel-level PixMamba Net (PixNet) to ensure fine-grained feature capturing and global consistency of enhanced image that were previously difficult to obtain.","PixMamba achieves state-of-the-art performance across various underwater image datasets and delivers visually superior results.","Code is available at: https://github.com/weitunglin/pixmamba."],"url":"http://arxiv.org/abs/2406.08444v1","category":"cs.CV"}
{"created":"2024-06-12 17:27:54","title":"A green solvent system for precursor phase-engineered sequential deposition of stable formamidinium lead triiodide for perovskite solar cells","abstract":"Perovskite solar cells (PSCs) offer an efficient, inexpensive alternative to current photovoltaic technologies, with the potential for manufacture via high-throughput coating methods. However, challenges for commercial-scale solution-processing of metal-halide perovskites include the use of harmful solvents, the expense of maintaining controlled atmospheric conditions, and the inherent instabilities of PSCs under operation. Here, we address these challenges by introducing a high volatility, low toxicity, biorenewable solvent system to fabricate a range of 2D perovskites, which highly effective precursor phases for subsequent transformation to alpha-formamidinium lead triiodide (FAPbI3), fully processed under ambient conditions. PSCs utilising our FAPbI3 reproducibly show remarkable stability under illumination and elevated temperature (ISOS-L-2) and \"damp heat\" (ISOS-D-3) stressing, surpassing other state-of-the-art perovskite compositions. We determine that this enhancement is a consequence of the 2D precursor phase crystallisation route, which simultaneously avoids retention of residual low-volatility solvents (such as DMF and DMSO) and reduces the rate of degradation of FA+ in the material. Our findings highlight both the critical role of the initial crystallisation process in determining the operational stability of perovskite materials, and that neat FA+-based perovskites can be competitively stable despite the inherent metastability of the alpha-phase.","sentences":["Perovskite solar cells (PSCs) offer an efficient, inexpensive alternative to current photovoltaic technologies, with the potential for manufacture via high-throughput coating methods.","However, challenges for commercial-scale solution-processing of metal-halide perovskites include the use of harmful solvents, the expense of maintaining controlled atmospheric conditions, and the inherent instabilities of PSCs under operation.","Here, we address these challenges by introducing a high volatility, low toxicity, biorenewable solvent system to fabricate a range of 2D perovskites, which highly effective precursor phases for subsequent transformation to alpha-formamidinium lead triiodide (FAPbI3), fully processed under ambient conditions.","PSCs utilising our FAPbI3 reproducibly show remarkable stability under illumination and elevated temperature (ISOS-L-2) and \"damp heat\" (ISOS-D-3) stressing, surpassing other state-of-the-art perovskite compositions.","We determine that this enhancement is a consequence of the 2D precursor phase crystallisation route, which simultaneously avoids retention of residual low-volatility solvents (such as DMF and DMSO) and reduces the rate of degradation of FA+ in the material.","Our findings highlight both the critical role of the initial crystallisation process in determining the operational stability of perovskite materials, and that neat FA+-based perovskites can be competitively stable despite the inherent metastability of the alpha-phase."],"url":"http://arxiv.org/abs/2406.08441v1","category":"physics.app-ph"}
{"created":"2024-06-12 17:24:41","title":"Coherent Optical Modems for Full-Wavefield Lidar","abstract":"The advent of the digital age has driven the development of coherent optical modems -- devices that modulate the amplitude and phase of light in multiple polarization states. These modems transmit data through fiber optic cables that are thousands of kilometers in length at data rates exceeding one terabit per second. This remarkable technology is made possible through near-THz-rate programmable control and sensing of the full optical wavefield. While coherent optical modems form the backbone of telecommunications networks around the world, their extraordinary capabilities also provide unique opportunities for imaging. Here, we introduce full-wavefield lidar: a new imaging modality that repurposes off-the-shelf coherent optical modems to simultaneously measure distance, axial velocity, and polarization. We demonstrate this modality by combining a 74 GHz-bandwidth coherent optical modem with free-space coupling optics and scanning mirrors. We develop a time-resolved image formation model for this system and formulate a maximum-likelihood reconstruction algorithm to recover depth, velocity, and polarization information at each scene point from the modem's raw transmitted and received symbols. Compared to existing lidars, full-wavefield lidar promises improved mm-scale ranging accuracy from brief, microsecond exposure times, reliable velocimetry, and robustness to intererence from ambient light or other lidar signals.","sentences":["The advent of the digital age has driven the development of coherent optical modems -- devices that modulate the amplitude and phase of light in multiple polarization states.","These modems transmit data through fiber optic cables that are thousands of kilometers in length at data rates exceeding one terabit per second.","This remarkable technology is made possible through near-THz-rate programmable control and sensing of the full optical wavefield.","While coherent optical modems form the backbone of telecommunications networks around the world, their extraordinary capabilities also provide unique opportunities for imaging.","Here, we introduce full-wavefield lidar: a new imaging modality that repurposes off-the-shelf coherent optical modems to simultaneously measure distance, axial velocity, and polarization.","We demonstrate this modality by combining a 74 GHz-bandwidth coherent optical modem with free-space coupling optics and scanning mirrors.","We develop a time-resolved image formation model for this system and formulate a maximum-likelihood reconstruction algorithm to recover depth, velocity, and polarization information at each scene point from the modem's raw transmitted and received symbols.","Compared to existing lidars, full-wavefield lidar promises improved mm-scale ranging accuracy from brief, microsecond exposure times, reliable velocimetry, and robustness to intererence from ambient light or other lidar signals."],"url":"http://arxiv.org/abs/2406.08439v1","category":"cs.CV"}
{"created":"2024-06-12 17:19:52","title":"Dynamical control in a prethermalized molecular ultracold plasma: Local dissipation drives global relaxation","abstract":"Prethermalization occurs as an important phase in the dynamics of many-body systems when strong coupling drives a quasi-equilibrium in a subspace separated from the thermodynamic equilibrium by the restriction of a gap in energy or other conserved quantity. Here, we report the signature of an enduring prethermal regime of arrested relaxation in the molecular ultracold plasma that forms following the avalanche of a state-selected Rydberg gas of nitric oxide. Electron collisions mix orbital angular momentum, scattering Rydberg molecules to states of very high-$\\ell$. Spontaneous predissociation purifies this non-penetrating character, creating an extraordinary gap between the plasma states of $n \\approx \\ell$, with measured $n>200$ and penetrating states of $\\ell = 0, ~1$ and 2. Evolution to a statistically equilibrated state of N and O atoms cannot occur without Rydberg electron penetration, and this gap blocks relaxation for a millisecond or more. Evolving through the critical phase, electrons that balance the NO$^+$ charge behave as though localized in the prethermal phase and play an ineffective role in bridging this gap. However, the application of a weak radiofrequency (RF) field promotes a dramatic degree of relaxation owing to electron collisions. On an entirely different scale, exciting a quantum-state transition in an exceedingly small fraction of the molecules in the prethermalized ensemble acts with even greater effect to drive the entire system toward equilibrium. We ascribe this to dissipative character added to a small fraction of the states in the prethermally localized ensemble. Using the Lindblad master equation, we illustrate qualitatively similar dynamics for a toy model of an open quantum system that consists of a localized set of spins on which dissipation acts locally at a single site.","sentences":["Prethermalization occurs as an important phase in the dynamics of many-body systems when strong coupling drives a quasi-equilibrium in a subspace separated from the thermodynamic equilibrium by the restriction of a gap in energy or other conserved quantity.","Here, we report the signature of an enduring prethermal regime of arrested relaxation in the molecular ultracold plasma that forms following the avalanche of a state-selected Rydberg gas of nitric oxide.","Electron collisions mix orbital angular momentum, scattering Rydberg molecules to states of very high-$\\ell$. Spontaneous predissociation purifies this non-penetrating character, creating an extraordinary gap between the plasma states of $n \\approx \\ell$, with measured $n>200$ and penetrating states of $\\ell = 0, ~1$ and 2.","Evolution to a statistically equilibrated state of N and O atoms cannot occur without Rydberg electron penetration, and this gap blocks relaxation for a millisecond or more.","Evolving through the critical phase, electrons that balance the NO$^+$ charge behave as though localized in the prethermal phase and play an ineffective role in bridging this gap.","However, the application of a weak radiofrequency (RF) field promotes a dramatic degree of relaxation owing to electron collisions.","On an entirely different scale, exciting a quantum-state transition in an exceedingly small fraction of the molecules in the prethermalized ensemble acts with even greater effect to drive the entire system toward equilibrium.","We ascribe this to dissipative character added to a small fraction of the states in the prethermally localized ensemble.","Using the Lindblad master equation, we illustrate qualitatively similar dynamics for a toy model of an open quantum system that consists of a localized set of spins on which dissipation acts locally at a single site."],"url":"http://arxiv.org/abs/2406.08433v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-12 17:04:36","title":"Designing Child-Centered Content Exposure and Moderation","abstract":"Research on children's online experience and computer interaction often overlooks the relationship children have with hidden algorithms that control the content they encounter. Furthermore, it is not only about how children interact with targeted content but also how their development and agency are largely affected by these. By engaging with the body of literature at the intersection of i) human-centered design approaches, ii) exclusion and discrimination in A.I., iii) privacy, transparency, and accountability, and iv) children's online citizenship, this article dives into the question of \"How can we approach the design of a child-centered moderation process to (1) include aspects that families value for their children and (2) provide explanations for content appropriateness and removal so that we can scale (according to systems and human needs) the moderation process assisted by A.I.?\".   This article contributes a sociotechnical highlight of core challenges and opportunities of designing child-centered content control tools. The article concludes by grounding and characterizing design considerations for a child-centered, family-guided moderation system. We hope this work serves as a stepping stone for designers and researchers pursuing children's safety online with an eye on hidden agents controlling children's online experiences and, by extension, the values and opportunities children are exposed to.","sentences":["Research on children's online experience and computer interaction often overlooks the relationship children have with hidden algorithms that control the content they encounter.","Furthermore, it is not only about how children interact with targeted content but also how their development and agency are largely affected by these.","By engaging with the body of literature at the intersection of i) human-centered design approaches, ii) exclusion and discrimination in A.I., iii) privacy, transparency, and accountability, and iv) children's online citizenship, this article dives into the question of \"How can we approach the design of a child-centered moderation process to (1) include aspects that families value for their children and (2) provide explanations for content appropriateness and removal so that we can scale (according to systems and human needs)","the moderation process assisted by A.I.?\".   ","This article contributes a sociotechnical highlight of core challenges and opportunities of designing child-centered content control tools.","The article concludes by grounding and characterizing design considerations for a child-centered, family-guided moderation system.","We hope this work serves as a stepping stone for designers and researchers pursuing children's safety online with an eye on hidden agents controlling children's online experiences and, by extension, the values and opportunities children are exposed to."],"url":"http://arxiv.org/abs/2406.08420v1","category":"cs.HC"}
{"created":"2024-06-12 17:00:31","title":"Stability of a Two-Phase Stokes Problem with Surface Tension","abstract":"In this work, we study the well-posedness of a system of partial differential equations that model the dynamics of a two-dimensional Stokes bubble immersed in two-dimensional ambient Stokes fluid of the same viscosity that extends to infinity under the effect of surface tension. We assume that the two fluids are immiscible and incompressible and that there is no interfacial jump in the fluid velocity. For this PDE system, a circular fluid bubble is a steady-state solution. Given an initial contour for the fluid bubble which is sufficiently close to a circle, we show that there exists a unique, global-in-time solution. This unique solution decays to a circle exponentially fast, which means that circular fluid bubbles are stable steady-state solutions. We also obtain a result concerning the regularity of the unique solution, that although the initial perturbation around a circular contour is assumed to be of low regularity, any later perturbation becomes real analytic, hence smooth.","sentences":["In this work, we study the well-posedness of a system of partial differential equations that model the dynamics of a two-dimensional Stokes bubble immersed in two-dimensional ambient Stokes fluid of the same viscosity that extends to infinity under the effect of surface tension.","We assume that the two fluids are immiscible and incompressible and that there is no interfacial jump in the fluid velocity.","For this PDE system, a circular fluid bubble is a steady-state solution.","Given an initial contour for the fluid bubble which is sufficiently close to a circle, we show that there exists a unique, global-in-time solution.","This unique solution decays to a circle exponentially fast, which means that circular fluid bubbles are stable steady-state solutions.","We also obtain a result concerning the regularity of the unique solution, that although the initial perturbation around a circular contour is assumed to be of low regularity, any later perturbation becomes real analytic, hence smooth."],"url":"http://arxiv.org/abs/2406.08417v1","category":"math.AP"}
{"created":"2024-06-12 16:59:24","title":"TokSing: Singing Voice Synthesis based on Discrete Tokens","abstract":"Recent advancements in speech synthesis witness significant benefits by leveraging discrete tokens extracted from self-supervised learning (SSL) models. Discrete tokens offer higher storage efficiency and greater operability in intermediate representations compared to traditional continuous Mel spectrograms. However, when it comes to singing voice synthesis(SVS), achieving higher levels of melody expression poses a great challenge for utilizing discrete tokens. In this paper, we introduce TokSing, a discrete-based SVS system equipped with a token formulator that offers flexible token blendings. We observe a melody degradation during discretization, prompting us to integrate a melody signal with the discrete token and incorporate a specially-designed melody enhancement strategy in the musical encoder. Extensive experiments demonstrate that our TokSing achieves better performance against the Mel spectrogram baselines while offering advantages in intermediate representation space cost and convergence speed.","sentences":["Recent advancements in speech synthesis witness significant benefits by leveraging discrete tokens extracted from self-supervised learning (SSL) models.","Discrete tokens offer higher storage efficiency and greater operability in intermediate representations compared to traditional continuous Mel spectrograms.","However, when it comes to singing voice synthesis(SVS), achieving higher levels of melody expression poses a great challenge for utilizing discrete tokens.","In this paper, we introduce TokSing, a discrete-based SVS system equipped with a token formulator that offers flexible token blendings.","We observe a melody degradation during discretization, prompting us to integrate a melody signal with the discrete token and incorporate a specially-designed melody enhancement strategy in the musical encoder.","Extensive experiments demonstrate that our TokSing achieves better performance against the Mel spectrogram baselines while offering advantages in intermediate representation space cost and convergence speed."],"url":"http://arxiv.org/abs/2406.08416v1","category":"cs.SD"}
{"created":"2024-06-12 16:56:24","title":"Spectral properties of dynamical tensor powers, and tensor factorizations of simple Lebesgue spectrum","abstract":"For every $n>0$ there is a unitary operator $U$ such that the unitary operator with simple Lebesgue spectrum is isomorphic to the tensor product $U\\otimes U^2\\otimes\\dots\\otimes U^{2^n}.$ There is an ergodic automorphism $T$ with its symmetric tensor power $T^{\\odot n}$ of simple spectrum, and $T^{\\odot(n+1)}$ of absolutely continuous spectrum.","sentences":["For every $n>0$ there is a unitary operator $U$ such that the unitary operator with simple Lebesgue spectrum is isomorphic to the tensor product $U\\otimes U^2\\otimes\\dots\\otimes U^{2^n}.$","There is an ergodic automorphism $T$ with its symmetric tensor power $T^{\\odot n}$ of simple spectrum, and $T^{\\odot(n+1)}$ of absolutely continuous spectrum."],"url":"http://arxiv.org/abs/2406.08409v1","category":"math.DS"}
{"created":"2024-06-12 16:50:12","title":"Nystr\u00f6m Kernel Stein Discrepancy","abstract":"Kernel methods underpin many of the most successful approaches in data science and statistics, and they allow representing probability measures as elements of a reproducing kernel Hilbert space without loss of information. Recently, the kernel Stein discrepancy (KSD), which combines Stein's method with kernel techniques, gained considerable attention. Through the Stein operator, KSD allows the construction of powerful goodness-of-fit tests where it is sufficient to know the target distribution up to a multiplicative constant. However, the typical U- and V-statistic-based KSD estimators suffer from a quadratic runtime complexity, which hinders their application in large-scale settings. In this work, we propose a Nystr\\\"om-based KSD acceleration -- with runtime $\\mathcal O\\!\\left(mn+m^3\\right)$ for $n$ samples and $m\\ll n$ Nystr\\\"om points -- , show its $\\sqrt{n}$-consistency under the null with a classical sub-Gaussian assumption, and demonstrate its applicability for goodness-of-fit testing on a suite of benchmarks.","sentences":["Kernel methods underpin many of the most successful approaches in data science and statistics, and they allow representing probability measures as elements of a reproducing kernel Hilbert space without loss of information.","Recently, the kernel Stein discrepancy (KSD), which combines Stein's method with kernel techniques, gained considerable attention.","Through the Stein operator, KSD allows the construction of powerful goodness-of-fit tests where it is sufficient to know the target distribution up to a multiplicative constant.","However, the typical U- and V-statistic-based KSD estimators suffer from a quadratic runtime complexity, which hinders their application in large-scale settings.","In this work, we propose a Nystr\\\"om-based KSD acceleration -- with runtime $\\mathcal O\\!\\left(mn+m^3\\right)$ for $n$ samples and $m\\ll n$ Nystr\\\"om points -- , show its $\\sqrt{n}$-consistency under the null with a classical sub-Gaussian assumption, and demonstrate its applicability for goodness-of-fit testing on a suite of benchmarks."],"url":"http://arxiv.org/abs/2406.08401v1","category":"stat.ML"}
{"created":"2024-06-12 16:40:27","title":"Coordinated Trading Strategies for Battery Storage in Reserve and Spot Markets","abstract":"Quantity and price risks are key uncertainties market participants face in electricity markets with increased volatility, for instance, due to high shares of renewables. From day ahead until real-time, there is a large variation in the best available information, leading to price changes that flexible assets, such as battery storage, can exploit economically. This study contributes to understanding how coordinated bidding strategies can enhance multi-market trading and large-scale energy storage integration. Our findings shed light on the complexities arising from interdependencies and the high-dimensional nature of the problem. We show how stochastic dual dynamic programming is a suitable solution technique for such an environment. We include the three markets of the frequency containment reserve, day-ahead, and intraday in stochastic modelling and develop a multi-stage stochastic program. Prices are represented in a multidimensional Markov Chain, following the scheduling of the markets and allowing for time-dependent randomness. Using the example of a battery storage in the German energy sector, we provide valuable insights into the technical aspects of our method and the economic feasibility of battery storage operation. We find that capacity reservation in the frequency containment reserve dominates over the battery's cycling in spot markets at the given resolution on prices in 2022. In an adjusted price environment, we find that coordination can yield an additional value of up to 12.5%.","sentences":["Quantity and price risks are key uncertainties market participants face in electricity markets with increased volatility, for instance, due to high shares of renewables.","From day ahead until real-time, there is a large variation in the best available information, leading to price changes that flexible assets, such as battery storage, can exploit economically.","This study contributes to understanding how coordinated bidding strategies can enhance multi-market trading and large-scale energy storage integration.","Our findings shed light on the complexities arising from interdependencies and the high-dimensional nature of the problem.","We show how stochastic dual dynamic programming is a suitable solution technique for such an environment.","We include the three markets of the frequency containment reserve, day-ahead, and intraday in stochastic modelling and develop a multi-stage stochastic program.","Prices are represented in a multidimensional Markov Chain, following the scheduling of the markets and allowing for time-dependent randomness.","Using the example of a battery storage in the German energy sector, we provide valuable insights into the technical aspects of our method and the economic feasibility of battery storage operation.","We find that capacity reservation in the frequency containment reserve dominates over the battery's cycling in spot markets at the given resolution on prices in 2022.","In an adjusted price environment, we find that coordination can yield an additional value of up to 12.5%."],"url":"http://arxiv.org/abs/2406.08390v1","category":"stat.ME"}
{"created":"2024-06-12 16:39:00","title":"The Slope Problem in Discrete Iteration","abstract":"The slope problem in holomorphic dynamics in the unit disk goes back to Wolff in 1929. However, there have been several contributions to this problem in the last decade. In this article the problem is revisited, comparing the discrete and continuous cases. Some advances are derived in the discrete parabolic case of zero hyperbolic step, showing that the set of slopes has to be a closed interval which is independent of the initial point. The continuous setting is used to show that any such interval is a possible example. In addition, the set of slopes of a family of parabolic function is discussed, leading to examples of functions with some regularity whose set of slopes is non-trivial.","sentences":["The slope problem in holomorphic dynamics in the unit disk goes back to Wolff in 1929.","However, there have been several contributions to this problem in the last decade.","In this article the problem is revisited, comparing the discrete and continuous cases.","Some advances are derived in the discrete parabolic case of zero hyperbolic step, showing that the set of slopes has to be a closed interval which is independent of the initial point.","The continuous setting is used to show that any such interval is a possible example.","In addition, the set of slopes of a family of parabolic function is discussed, leading to examples of functions with some regularity whose set of slopes is non-trivial."],"url":"http://arxiv.org/abs/2406.08389v1","category":"math.CV"}
{"created":"2024-06-12 16:35:03","title":"Exploring Geometrical Properties of Chaotic Systems Through an Analysis of the Rulkov Neuron Maps","abstract":"While extensive research has been conducted on chaos emerging from a dynamical system's temporal dynamics, the research documented in this paper examines extreme sensitivity to initial conditions in discrete-time dynamical systems from a geometrical perspective. The heart of this paper focuses on two simple neuron maps developed by Nikolai F. Rulkov in the early 2000s and the complex geometrical structures that emerge from them. Beginning with a conversational introduction to the geometry of chaos, this paper integrates mathematics, physics, neurobiology, computational modeling, and electrochemistry to present original research that provides a novel perspective on how types of geometrical sensitivity to initial conditions appear in discrete-time neuron systems.   This paper was developed in the Thomas Jefferson High School for Science and Technology Quantum Lab as part of a senior research project.","sentences":["While extensive research has been conducted on chaos emerging from a dynamical system's temporal dynamics, the research documented in this paper examines extreme sensitivity to initial conditions in discrete-time dynamical systems from a geometrical perspective.","The heart of this paper focuses on two simple neuron maps developed by Nikolai F. Rulkov in the early 2000s and the complex geometrical structures that emerge from them.","Beginning with a conversational introduction to the geometry of chaos, this paper integrates mathematics, physics, neurobiology, computational modeling, and electrochemistry to present original research that provides a novel perspective on how types of geometrical sensitivity to initial conditions appear in discrete-time neuron systems.   ","This paper was developed in the Thomas Jefferson High School for Science and Technology Quantum Lab as part of a senior research project."],"url":"http://arxiv.org/abs/2406.08385v1","category":"nlin.CD"}
{"created":"2024-06-12 16:34:12","title":"On existence of weak solutions to a Baer-Nunziato type system","abstract":"In this paper, a dissipative version of a compressible one velocity Baer--Nunziato type system for a mixture of two compressible heat conducting gases is considered. The complete existence proof for weak solutions to this system was addressed as an open problem in \\cite[Section 5]{KNAC}. The purpose of this paper is to prove the global in time existence of weak solutions to the one velocity Baer--Nunziato type system for arbitrary large initial data. The goal is achieved in three steps. Firstly, the given system is transformed into a new one which possesses the \"Navier-Stokes-Fourier\" structure. Secondly, the new system is solved by an adaptation of the Feireisl--Lions approach for solving the compressible Navier--Stokes equations. Eventually, the existence of a weak solution to the original one velocity Baer--Nunziato system using the almost uniqueness property of renormalized solutions to pure transport equations.","sentences":["In this paper, a dissipative version of a compressible one velocity Baer--Nunziato type system for a mixture of two compressible heat conducting gases is considered.","The complete existence proof for weak solutions to this system was addressed as an open problem in \\cite[Section 5]{KNAC}.","The purpose of this paper is to prove the global in time existence of weak solutions to the one velocity Baer--Nunziato type system for arbitrary large initial data.","The goal is achieved in three steps.","Firstly, the given system is transformed into a new one which possesses the \"Navier-Stokes-Fourier\" structure.","Secondly, the new system is solved by an adaptation of the Feireisl--Lions approach for solving the compressible Navier--Stokes equations.","Eventually, the existence of a weak solution to the original one velocity Baer--Nunziato system using the almost uniqueness property of renormalized solutions to pure transport equations."],"url":"http://arxiv.org/abs/2406.08383v1","category":"math.AP"}
{"created":"2024-06-12 16:31:06","title":"LaneCPP: Continuous 3D Lane Detection using Physical Priors","abstract":"Monocular 3D lane detection has become a fundamental problem in the context of autonomous driving, which comprises the tasks of finding the road surface and locating lane markings. One major challenge lies in a flexible but robust line representation capable of modeling complex lane structures, while still avoiding unpredictable behavior. While previous methods rely on fully data-driven approaches, we instead introduce a novel approach LaneCPP that uses a continuous 3D lane detection model leveraging physical prior knowledge about the lane structure and road geometry. While our sophisticated lane model is capable of modeling complex road structures, it also shows robust behavior since physical constraints are incorporated by means of a regularization scheme that can be analytically applied to our parametric representation. Moreover, we incorporate prior knowledge about the road geometry into the 3D feature space by modeling geometry-aware spatial features, guiding the network to learn an internal road surface representation. In our experiments, we show the benefits of our contributions and prove the meaningfulness of using priors to make 3D lane detection more robust. The results show that LaneCPP achieves state-of-the-art performance in terms of F-Score and geometric errors.","sentences":["Monocular 3D lane detection has become a fundamental problem in the context of autonomous driving, which comprises the tasks of finding the road surface and locating lane markings.","One major challenge lies in a flexible but robust line representation capable of modeling complex lane structures, while still avoiding unpredictable behavior.","While previous methods rely on fully data-driven approaches, we instead introduce a novel approach LaneCPP that uses a continuous 3D lane detection model leveraging physical prior knowledge about the lane structure and road geometry.","While our sophisticated lane model is capable of modeling complex road structures, it also shows robust behavior since physical constraints are incorporated by means of a regularization scheme that can be analytically applied to our parametric representation.","Moreover, we incorporate prior knowledge about the road geometry into the 3D feature space by modeling geometry-aware spatial features, guiding the network to learn an internal road surface representation.","In our experiments, we show the benefits of our contributions and prove the meaningfulness of using priors to make 3D lane detection more robust.","The results show that LaneCPP achieves state-of-the-art performance in terms of F-Score and geometric errors."],"url":"http://arxiv.org/abs/2406.08381v1","category":"cs.CV"}
{"created":"2024-06-12 16:30:58","title":"Towards Unsupervised Speech Recognition Without Pronunciation Models","abstract":"Recent advancements in supervised automatic speech recognition (ASR) have achieved remarkable performance, largely due to the growing availability of large transcribed speech corpora. However, most languages lack sufficient paired speech and text data to effectively train these systems. In this article, we tackle the challenge of developing ASR systems without paired speech and text corpora by proposing the removal of reliance on a phoneme lexicon. We explore a new research direction: word-level unsupervised ASR. Using a curated speech corpus containing only high-frequency English words, our system achieves a word error rate of nearly 20% without parallel transcripts or oracle word boundaries. Furthermore, we experimentally demonstrate that an unsupervised speech recognizer can emerge from joint speech-to-speech and text-to-text masked token-infilling. This innovative model surpasses the performance of previous unsupervised ASR models trained with direct distribution matching.","sentences":["Recent advancements in supervised automatic speech recognition (ASR) have achieved remarkable performance, largely due to the growing availability of large transcribed speech corpora.","However, most languages lack sufficient paired speech and text data to effectively train these systems.","In this article, we tackle the challenge of developing ASR systems without paired speech and text corpora by proposing the removal of reliance on a phoneme lexicon.","We explore a new research direction: word-level unsupervised ASR.","Using a curated speech corpus containing only high-frequency English words, our system achieves a word error rate of nearly 20% without parallel transcripts or oracle word boundaries.","Furthermore, we experimentally demonstrate that an unsupervised speech recognizer can emerge from joint speech-to-speech and text-to-text masked token-infilling.","This innovative model surpasses the performance of previous unsupervised ASR models trained with direct distribution matching."],"url":"http://arxiv.org/abs/2406.08380v1","category":"cs.CL"}
{"created":"2024-06-12 16:26:56","title":"DDR: Exploiting Deep Degradation Response as Flexible Image Descriptor","abstract":"Image deep features extracted by pre-trained networks are known to contain rich and informative representations. In this paper, we present Deep Degradation Response (DDR), a method to quantify changes in image deep features under varying degradation conditions. Specifically, our approach facilitates flexible and adaptive degradation, enabling the controlled synthesis of image degradation through text-driven prompts. Extensive evaluations demonstrate the versatility of DDR as an image descriptor, with strong correlations observed with key image attributes such as complexity, colorfulness, sharpness, and overall quality. Moreover, we demonstrate the efficacy of DDR across a spectrum of applications. It excels as a blind image quality assessment metric, outperforming existing methodologies across multiple datasets. Additionally, DDR serves as an effective unsupervised learning objective in image restoration tasks, yielding notable advancements in image deblurring and single-image super-resolution. Our code will be made available.","sentences":["Image deep features extracted by pre-trained networks are known to contain rich and informative representations.","In this paper, we present Deep Degradation Response (DDR), a method to quantify changes in image deep features under varying degradation conditions.","Specifically, our approach facilitates flexible and adaptive degradation, enabling the controlled synthesis of image degradation through text-driven prompts.","Extensive evaluations demonstrate the versatility of DDR as an image descriptor, with strong correlations observed with key image attributes such as complexity, colorfulness, sharpness, and overall quality.","Moreover, we demonstrate the efficacy of DDR across a spectrum of applications.","It excels as a blind image quality assessment metric, outperforming existing methodologies across multiple datasets.","Additionally, DDR serves as an effective unsupervised learning objective in image restoration tasks, yielding notable advancements in image deblurring and single-image super-resolution.","Our code will be made available."],"url":"http://arxiv.org/abs/2406.08377v1","category":"cs.CV"}
{"created":"2024-06-12 16:22:52","title":"A Parameterized Nonlinear Magnetic Equivalent Circuit for Design and Fast Analysis of Radial Flux Magnetic Gears","abstract":"Magnetic gears offer advantages over mechanical gears, including contactless power transfer, but require robust analysis tools for optimization and commercialization. This study proposes a rapid and accurate 2D nonlinear magnetic equivalent circuit (MEC) model for radial flux magnetic gears (RFMG). The model, featuring a parameterized gear geometry and adjustable flux tube distribution, accommodates nonlinear effects like magnetic saturation while maintaining quick simulation times. Comparison with a nonlinear finite element analysis (FEA) model demonstrates the MEC's accuracy in torque and flux density predictions across diverse designs. Additionally, a parametric optimization study of 140,000 designs confirms the MEC's high accuracy, achieving close agreement with FEA torque predictions, with simulations running up to 100 times faster. Finally, the MEC shows good agreement with 2D FEA for a prototype RFMG.","sentences":["Magnetic gears offer advantages over mechanical gears, including contactless power transfer, but require robust analysis tools for optimization and commercialization.","This study proposes a rapid and accurate 2D nonlinear magnetic equivalent circuit (MEC) model for radial flux magnetic gears (RFMG).","The model, featuring a parameterized gear geometry and adjustable flux tube distribution, accommodates nonlinear effects like magnetic saturation while maintaining quick simulation times.","Comparison with a nonlinear finite element analysis (FEA) model demonstrates the MEC's accuracy in torque and flux density predictions across diverse designs.","Additionally, a parametric optimization study of 140,000 designs confirms the MEC's high accuracy, achieving close agreement with FEA torque predictions, with simulations running up to 100 times faster.","Finally, the MEC shows good agreement with 2D FEA for a prototype RFMG."],"url":"http://arxiv.org/abs/2406.08375v1","category":"eess.SY"}
{"created":"2024-06-12 16:14:45","title":"Hierarchical Bayesian Emulation of the Expected Net Present Value Utility Function via a Multi-Model Ensemble Member Decomposition","abstract":"Computer models are widely used to study complex real world physical systems. However, there are major limitations to their direct use including: their complex structure; large numbers of inputs and outputs; and long evaluation times. Bayesian emulators are an effective means of addressing these challenges providing fast and efficient statistical approximation for computer model outputs. It is commonly assumed that computer models behave like a ``black-box'' function with no knowledge of the output prior to its evaluation. This ensures that emulators are generalisable but potentially limits their accuracy compared with exploiting such knowledge of constrained or structured output behaviour. We assume a ``grey-box'' computer model and establish a hierarchical emulation framework encompassing structured emulators which exploit known constrained and structured behaviour of constituent computer model outputs. This achieves greater physical interpretability and more accurate emulator predictions. This research is motivated by and applied to the commercially important TNO OLYMPUS Well Control Optimisation Challenge from the petroleum industry. We re-express this as a decision support under uncertainty problem. First, we reduce the computational expense of the analysis by identifying a representative subset of models using an efficient multi-model ensemble subsampling technique. Next we apply our hierarchical emulation methodology to the expected Net Present Value utility function with well control decision parameters as inputs.","sentences":["Computer models are widely used to study complex real world physical systems.","However, there are major limitations to their direct use including: their complex structure; large numbers of inputs and outputs; and long evaluation times.","Bayesian emulators are an effective means of addressing these challenges providing fast and efficient statistical approximation for computer model outputs.","It is commonly assumed that computer models behave like a ``black-box'' function with no knowledge of the output prior to its evaluation.","This ensures that emulators are generalisable but potentially limits their accuracy compared with exploiting such knowledge of constrained or structured output behaviour.","We assume a ``grey-box'' computer model and establish a hierarchical emulation framework encompassing structured emulators which exploit known constrained and structured behaviour of constituent computer model outputs.","This achieves greater physical interpretability and more accurate emulator predictions.","This research is motivated by and applied to the commercially important TNO OLYMPUS","Well Control Optimisation Challenge from the petroleum industry.","We re-express this as a decision support under uncertainty problem.","First, we reduce the computational expense of the analysis by identifying a representative subset of models using an efficient multi-model ensemble subsampling technique.","Next we apply our hierarchical emulation methodology to the expected Net Present Value utility function with well control decision parameters as inputs."],"url":"http://arxiv.org/abs/2406.08367v1","category":"stat.ME"}
{"created":"2024-06-12 16:07:56","title":"Sequence Complexity and Monomer Rigidity Control the Morphologies and Aging Dynamics of Protein Aggregates","abstract":"Protein aggregates exhibit diverse morphology, exemplified by amyloid fibrils, gel-like structures, and liquid-like condensates. Differences in the morphologies in identical proteins play important functional roles in several diseases. Simulations using a minimal model show that such structures are encoded in the sequence complexity and bending rigidity of the monomers. The low-complexity flexible sequences form liquid droplets, whose relaxation dynamics are ergodic. In contrast, rigid low and high-complexity sequences, which form ordered nematic fibril-like structures and amorphous aggregates, exhibit heterogenous, non-ergodic dynamics. The relaxation times under these conditions increase as the waiting time increases, which is a signature of aging. The implications of our findings for aging in intrinsically disordered proteins and repeat RNA sequences are outlined.","sentences":["Protein aggregates exhibit diverse morphology, exemplified by amyloid fibrils, gel-like structures, and liquid-like condensates.","Differences in the morphologies in identical proteins play important functional roles in several diseases.","Simulations using a minimal model show that such structures are encoded in the sequence complexity and bending rigidity of the monomers.","The low-complexity flexible sequences form liquid droplets, whose relaxation dynamics are ergodic.","In contrast, rigid low and high-complexity sequences, which form ordered nematic fibril-like structures and amorphous aggregates, exhibit heterogenous, non-ergodic dynamics.","The relaxation times under these conditions increase as the waiting time increases, which is a signature of aging.","The implications of our findings for aging in intrinsically disordered proteins and repeat RNA sequences are outlined."],"url":"http://arxiv.org/abs/2406.08362v1","category":"physics.bio-ph"}
{"created":"2024-06-12 16:05:32","title":"Dissipation bounds precision of current response to kinetic perturbations","abstract":"The precision of currents in Markov networks is bounded by dissipation via the so-called thermodynamic uncertainty relation (TUR). In our work, we demonstrate a similar inequality that bounds the precision of the static current response to perturbations of kinetic barriers. Perturbations of such type, which affect only the system kinetics but not the thermodynamic forces, are highly important in biochemistry and nanoelectronics. We prove that our inequality cannot be derived from the standard TUR. Instead, it implies the standard TUR and provides an even tighter bound for dissipation. We also provide a procedure for obtaining the optimal response precision for a given model.","sentences":["The precision of currents in Markov networks is bounded by dissipation via the so-called thermodynamic uncertainty relation (TUR).","In our work, we demonstrate a similar inequality that bounds the precision of the static current response to perturbations of kinetic barriers.","Perturbations of such type, which affect only the system kinetics but not the thermodynamic forces, are highly important in biochemistry and nanoelectronics.","We prove that our inequality cannot be derived from the standard TUR.","Instead, it implies the standard TUR and provides an even tighter bound for dissipation.","We also provide a procedure for obtaining the optimal response precision for a given model."],"url":"http://arxiv.org/abs/2406.08361v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-12 16:01:19","title":"$\\mathbb{Z}_2$ gauge field and topological chirality from Umklapp scattering in twisted graphite","abstract":"Spinless systems exhibit unique topological characteristics compared to spinful ones, stemming from their distinct algebra. Without chiral interactions typically linked to spin, an intriguing yet unexplored interplay between topological and structural chirality may be anticipated. Here we show examples of spinless topological chirality solely from structural chirality in two types of twisted graphite. In a 3D helical structure, we find a chiral Weyl semimetal phase where bulk topology and chiral surface states are both determined by the screw direction. And in a 3D periodic structure formed with alternating twisting angle signs, a higher-order Dirac semimetal with chiral hinge states is discovered. Underlying these novel topological states is the Umklapp scattering that captures the chirality of the twisted interfaces, leading effectively to a sign-flipped chiral interlayer hopping, thereby introducing $\\mathbb{Z}_2$ lattice gauge field that alters the symmetry algebra. Our findings point to a new pathway for engineering topological chirality.","sentences":["Spinless systems exhibit unique topological characteristics compared to spinful ones, stemming from their distinct algebra.","Without chiral interactions typically linked to spin, an intriguing yet unexplored interplay between topological and structural chirality may be anticipated.","Here we show examples of spinless topological chirality solely from structural chirality in two types of twisted graphite.","In a 3D helical structure, we find a chiral Weyl semimetal phase where bulk topology and chiral surface states are both determined by the screw direction.","And in a 3D periodic structure formed with alternating twisting angle signs, a higher-order Dirac semimetal with chiral hinge states is discovered.","Underlying these novel topological states is the Umklapp scattering that captures the chirality of the twisted interfaces, leading effectively to a sign-flipped chiral interlayer hopping, thereby introducing $\\mathbb{Z}_2$ lattice gauge field that alters the symmetry algebra.","Our findings point to a new pathway for engineering topological chirality."],"url":"http://arxiv.org/abs/2406.08355v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-12 15:59:25","title":"Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques","abstract":"Text data is commonly utilized as a primary input to enhance Speech Emotion Recognition (SER) performance and reliability. However, the reliance on human-transcribed text in most studies impedes the development of practical SER systems, creating a gap between in-lab research and real-world scenarios where Automatic Speech Recognition (ASR) serves as the text source. Hence, this study benchmarks SER performance using ASR transcripts with varying Word Error Rates (WERs) on well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our evaluation includes text-only and bimodal SER with diverse fusion techniques, aiming for a comprehensive analysis that uncovers novel findings and challenges faced by current SER research. Additionally, we propose a unified ASR error-robust framework integrating ASR error correction and modality-gated fusion, achieving lower WER and higher SER results compared to the best-performing ASR transcript. This research is expected to provide insights into SER with ASR assistance, especially for real-world applications.","sentences":["Text data is commonly utilized as a primary input to enhance Speech Emotion Recognition (SER) performance and reliability.","However, the reliance on human-transcribed text in most studies impedes the development of practical SER systems, creating a gap between in-lab research and real-world scenarios where Automatic Speech Recognition (ASR) serves as the text source.","Hence, this study benchmarks SER performance using ASR transcripts with varying Word Error Rates (WERs) on well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast.","Our evaluation includes text-only and bimodal SER with diverse fusion techniques, aiming for a comprehensive analysis that uncovers novel findings and challenges faced by current SER research.","Additionally, we propose a unified ASR error-robust framework integrating ASR error correction and modality-gated fusion, achieving lower WER and higher SER results compared to the best-performing ASR transcript.","This research is expected to provide insights into SER with ASR assistance, especially for real-world applications."],"url":"http://arxiv.org/abs/2406.08353v1","category":"eess.AS"}
{"created":"2024-06-12 15:58:58","title":"Doppler-Robust Maximum Likelihood Parametric Channel Estimation for Multiuser MIMO-OFDM","abstract":"The high directionality and intense Doppler effects of millimeter wave (mmWave) and sub-terahertz (subTHz) channels demand accurate localization of the users and a new paradigm of channel estimation. For orthogonal frequency division multiplexing (OFDM) waveforms, estimating the geometric parameters of the radio channel can make these systems more Doppler-resistant and also enhance sensing and positioning performance. In this paper, we derive a multiuser, multiple-input multiple-output (MIMO), maximum likelihood, parametric channel estimation algorithm for uplink sensing, which is capable of accurately estimating the parameters of each multipath that composes each user's channel under severe Doppler shift conditions. The presented method is one of the only Doppler-robust currently available algorithms that does not rely on line search.","sentences":["The high directionality and intense Doppler effects of millimeter wave (mmWave) and sub-terahertz (subTHz) channels demand accurate localization of the users and a new paradigm of channel estimation.","For orthogonal frequency division multiplexing (OFDM) waveforms, estimating the geometric parameters of the radio channel can make these systems more Doppler-resistant and also enhance sensing and positioning performance.","In this paper, we derive a multiuser, multiple-input multiple-output (MIMO), maximum likelihood, parametric channel estimation algorithm for uplink sensing, which is capable of accurately estimating the parameters of each multipath that composes each user's channel under severe Doppler shift conditions.","The presented method is one of the only Doppler-robust currently available algorithms that does not rely on line search."],"url":"http://arxiv.org/abs/2406.08352v1","category":"eess.SP"}
{"created":"2024-06-12 15:56:06","title":"Rigorous Safety Analysis and Design of ADAS and ADS: Implications on Tools","abstract":"Currently, a major concern is the insufficient level of safety offered by commercial automated vehicles and/or services such self-driving vehicles, self-driving trucks, and robotaxis. Unfortunately, stakeholders do not agree on definitions and characterizations of what is meant by safety of automated vehicles including how to measure it and how to design for it. This paper sheds some light into the answers to important questions about the safety of automated vehicles. In addition, we identify rigor as a significant missing requirement in the current literature, we also provide a discussion of rigor in the design, development, and commercialization of automated vehicles. Furthermore, we discuss software tool requirements at the organizational level to support a rigorous approach for the analysis, design, and commercialization of automated vehicles. An ALM tool, EwQIMS, is introduced emphasizing its rigorous features of its functional safety module that implements much of the ISO 26262 standard.","sentences":["Currently, a major concern is the insufficient level of safety offered by commercial automated vehicles and/or services such self-driving vehicles, self-driving trucks, and robotaxis.","Unfortunately, stakeholders do not agree on definitions and characterizations of what is meant by safety of automated vehicles including how to measure it and how to design for it.","This paper sheds some light into the answers to important questions about the safety of automated vehicles.","In addition, we identify rigor as a significant missing requirement in the current literature, we also provide a discussion of rigor in the design, development, and commercialization of automated vehicles.","Furthermore, we discuss software tool requirements at the organizational level to support a rigorous approach for the analysis, design, and commercialization of automated vehicles.","An ALM tool, EwQIMS, is introduced emphasizing its rigorous features of its functional safety module that implements much of the ISO 26262 standard."],"url":"http://arxiv.org/abs/2406.08350v1","category":"eess.SY"}
{"created":"2024-06-12 15:47:10","title":"Microstructures in a two-dimensional frustrated spin system: Scaling regimes and a discrete-to-continuum limit","abstract":"We study pattern formation within the $J_1$-$J_3$ - spin model on a two-dimensional square lattice in the case of incompatible (ferromagnetic) boundary conditions on the spin field. We derive the discrete-to-continuum $\\Gamma$-limit at the helimagnetic/ferromagnetic transition point, which turns out to be characterized by a singularly perturbed multiwell energy functional on gradient fields. Furthermore, we study the scaling law of the discrete minimal energy. The constructions used in the upper bound include besides rather uniform or complex branching-type patterns also structures with vortices. Our results show in particular that in certain parameter regimes the formation of vortices is energetically favorable.","sentences":["We study pattern formation within the $J_1$-$J_3$ - spin model on a two-dimensional square lattice in the case of incompatible (ferromagnetic) boundary conditions on the spin field.","We derive the discrete-to-continuum $\\Gamma$-limit at the helimagnetic/ferromagnetic transition point, which turns out to be characterized by a singularly perturbed multiwell energy functional on gradient fields.","Furthermore, we study the scaling law of the discrete minimal energy.","The constructions used in the upper bound include besides rather uniform or complex branching-type patterns also structures with vortices.","Our results show in particular that in certain parameter regimes the formation of vortices is energetically favorable."],"url":"http://arxiv.org/abs/2406.08339v1","category":"math.AP"}
{"created":"2024-06-12 15:39:35","title":"Review of Autonomous Mobile Robots for the Warehouse Environment","abstract":"Autonomous mobile robots (AMRs) have been a rapidly expanding research topic for the past decade. Unlike their counterpart, the automated guided vehicle (AGV), AMRs can make decisions and do not need any previously installed infrastructure to navigate. Recent technological developments in hardware and software have made them more feasible, especially in warehouse environments. Traditionally, most wasted warehouse expenses come from the logistics of moving material from one point to another, and is exhaustive for humans to continuously walk those distances while carrying a load. Here, AMRs can help by working with humans to cut down the time and effort of these repetitive tasks, improving performance and reducing the fatigue of their human collaborators. This literature review covers the recent developments in AMR technology including hardware, robotic control, and system control. This paper also discusses examples of current AMR producers, their robots, and the software that is used to control them. We conclude with future research topics and where we see AMRs developing in the warehouse environment.","sentences":["Autonomous mobile robots (AMRs) have been a rapidly expanding research topic for the past decade.","Unlike their counterpart, the automated guided vehicle (AGV), AMRs can make decisions and do not need any previously installed infrastructure to navigate.","Recent technological developments in hardware and software have made them more feasible, especially in warehouse environments.","Traditionally, most wasted warehouse expenses come from the logistics of moving material from one point to another, and is exhaustive for humans to continuously walk those distances while carrying a load.","Here, AMRs can help by working with humans to cut down the time and effort of these repetitive tasks, improving performance and reducing the fatigue of their human collaborators.","This literature review covers the recent developments in AMR technology including hardware, robotic control, and system control.","This paper also discusses examples of current AMR producers, their robots, and the software that is used to control them.","We conclude with future research topics and where we see AMRs developing in the warehouse environment."],"url":"http://arxiv.org/abs/2406.08333v1","category":"cs.RO"}
{"created":"2024-06-12 15:27:52","title":"On the application of components manufactured with stereolithographic 3D printing in high vacuum systems","abstract":"We report on a method for using stereolithographic (SLA) additive manufacturing to rapidly and cheaply prototype components for use in high-vacuum environments. We demonstrate the primary vacuum contaminant from freshly printed SLA plastics is water with no evidence of polymers out-gassing from the material and thus the vacuum performance can be controlled with simple treatments which do not involve surface sealing. An unbaked vacuum system containing SLA printed parts achieved 1.9e-8 mbar base pressure whilst retaining structural integrity and manufacturing accuracy. Preliminary results indicate that our method can be extended to achieve ultrahigh-vacuum compatibility by baking at higher temperatures. We further report on the effect of atmospheric exposure to components and present evidence to suggest that re-wetting occurs exclusively in the component skin layer, by showing that the bulk mass changes of the material is irreversible on the timescale investigated (< 2 weeks).","sentences":["We report on a method for using stereolithographic (SLA) additive manufacturing to rapidly and cheaply prototype components for use in high-vacuum environments.","We demonstrate the primary vacuum contaminant from freshly printed SLA plastics is water with no evidence of polymers out-gassing from the material and thus the vacuum performance can be controlled with simple treatments which do not involve surface sealing.","An unbaked vacuum system containing SLA printed parts achieved 1.9e-8 mbar base pressure whilst retaining structural integrity and manufacturing accuracy.","Preliminary results indicate that our method can be extended to achieve ultrahigh-vacuum compatibility by baking at higher temperatures.","We further report on the effect of atmospheric exposure to components and present evidence to suggest that re-wetting occurs exclusively in the component skin layer, by showing that the bulk mass changes of the material is irreversible on the timescale investigated (< 2 weeks)."],"url":"http://arxiv.org/abs/2406.08326v1","category":"physics.app-ph"}
{"created":"2024-06-12 15:25:03","title":"On the existence of solutions for a class of systems of integro-differential equations with the logarithmic Laplacian and drift","abstract":"In this article, we consider a system of integro-differential equations in L^2(R, R^N), which contains the logarithmic Laplacian in the presence of transport terms. The linear operators associated with the system satisfy the Fredholm property. By virtue of a fixed point technique, we demonstrate the existence of solutions. We emphasize that the discussion is more complicated than that of the scalar situation as there are more cumbersome technicalities to overcome.","sentences":["In this article, we consider a system of integro-differential equations in L^2(R, R^N), which contains the logarithmic Laplacian in the presence of transport terms.","The linear operators associated with the system satisfy the Fredholm property.","By virtue of a fixed point technique, we demonstrate the existence of solutions.","We emphasize that the discussion is more complicated than that of the scalar situation as there are more cumbersome technicalities to overcome."],"url":"http://arxiv.org/abs/2406.08325v1","category":"math.AP"}
{"created":"2024-06-12 15:21:48","title":"Geometric representations of braid and Yang-Baxter gates","abstract":"Brick-wall circuits composed of the Yang-Baxter gates are integrable. It becomes an important tool to study the quantum many-body system out of equilibrium. To put the Yang-Baxter gate on the quantum computer, it has to be decomposed into the native gates of quantum computers. It is favorable to apply the least number of native two-qubit gates to construct the Yang-Baxter gate. We study the geometric representations of all X-type braid gates and their corresponding Yang-Baxter gates via the Yang-Baxterization. We find that the braid and Yang-Baxter gates can only exist on certain edges and faces of the two-qubit tetrahedron. We identify the parameters by which the braid and Yang-Baxter gates are the Clifford gate, the matchgate, and the dual-unitary gate. The geometric representations provide the optimal decompositions of the braid and Yang-Baxter gates in terms of other two-qubit gates. We also find that the entangling powers of the Yang-Baxter gates are determined by the spectral parameters. Our results provide the necessary conditions to construct the braid and Yang-Baxter gates on quantum computers.","sentences":["Brick-wall circuits composed of the Yang-Baxter gates are integrable.","It becomes an important tool to study the quantum many-body system out of equilibrium.","To put the Yang-Baxter gate on the quantum computer, it has to be decomposed into the native gates of quantum computers.","It is favorable to apply the least number of native two-qubit gates to construct the Yang-Baxter gate.","We study the geometric representations of all X-type braid gates and their corresponding Yang-Baxter gates via the Yang-Baxterization.","We find that the braid and Yang-Baxter gates can only exist on certain edges and faces of the two-qubit tetrahedron.","We identify the parameters by which the braid and Yang-Baxter gates are the Clifford gate, the matchgate, and the dual-unitary gate.","The geometric representations provide the optimal decompositions of the braid and Yang-Baxter gates in terms of other two-qubit gates.","We also find that the entangling powers of the Yang-Baxter gates are determined by the spectral parameters.","Our results provide the necessary conditions to construct the braid and Yang-Baxter gates on quantum computers."],"url":"http://arxiv.org/abs/2406.08320v1","category":"quant-ph"}
{"created":"2024-06-12 15:19:25","title":"Invariant multiscale neural networks for data-scarce scientific applications","abstract":"Success of machine learning (ML) in the modern world is largely determined by abundance of data. However at many industrial and scientific problems, amount of data is limited. Application of ML methods to data-scarce scientific problems can be made more effective via several routes, one of them is equivariant neural networks possessing knowledge of symmetries. Here we suggest that combination of symmetry-aware invariant architectures and stacks of dilated convolutions is a very effective and easy to implement receipt allowing sizable improvements in accuracy over standard approaches. We apply it to representative physical problems from different realms: prediction of bandgaps of photonic crystals, and network approximations of magnetic ground states. The suggested invariant multiscale architectures increase expressibility of networks, which allow them to perform better in all considered cases.","sentences":["Success of machine learning (ML) in the modern world is largely determined by abundance of data.","However at many industrial and scientific problems, amount of data is limited.","Application of ML methods to data-scarce scientific problems can be made more effective via several routes, one of them is equivariant neural networks possessing knowledge of symmetries.","Here we suggest that combination of symmetry-aware invariant architectures and stacks of dilated convolutions is a very effective and easy to implement receipt allowing sizable improvements in accuracy over standard approaches.","We apply it to representative physical problems from different realms: prediction of bandgaps of photonic crystals, and network approximations of magnetic ground states.","The suggested invariant multiscale architectures increase expressibility of networks, which allow them to perform better in all considered cases."],"url":"http://arxiv.org/abs/2406.08318v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-12 15:16:53","title":"Pathways to hyperchaos in a three-dimensional quadratic map","abstract":"This paper deals with various routes to hyperchaos with all three positive Lyapunov exponents in a three-dimensional quadratic map. The map under consideration displays strong hyperchaoticity in the sense that in a wider range of parameter space the system showcase three positive Lyapunov exponents. It is shown that the saddle periodic orbits eventually become repellers at this hyperchaotic regime. By computing the distance of the repellers to the attractors as a function of parameters, it is shown that the hyperchaotic attractors absorb the repelling periodic orbits. First we discuss a route from stable fixed point undergoing period-doubling bifurcations to chaos and then hyperchaos, and role of saddle periodic orbits. We then illustrate a route from doubling bifurcation of quasiperiodic closed invariant curves to hyperchaotic attractors. Finally, presence of weak hyperchaotic flow like attractors are discussed.","sentences":["This paper deals with various routes to hyperchaos with all three positive Lyapunov exponents in a three-dimensional quadratic map.","The map under consideration displays strong hyperchaoticity in the sense that in a wider range of parameter space the system showcase three positive Lyapunov exponents.","It is shown that the saddle periodic orbits eventually become repellers at this hyperchaotic regime.","By computing the distance of the repellers to the attractors as a function of parameters, it is shown that the hyperchaotic attractors absorb the repelling periodic orbits.","First we discuss a route from stable fixed point undergoing period-doubling bifurcations to chaos and then hyperchaos, and role of saddle periodic orbits.","We then illustrate a route from doubling bifurcation of quasiperiodic closed invariant curves to hyperchaotic attractors.","Finally, presence of weak hyperchaotic flow like attractors are discussed."],"url":"http://arxiv.org/abs/2406.08317v1","category":"nlin.CD"}
{"created":"2024-06-12 15:15:17","title":"GPU-accelerated Auxiliary-field quantum Monte Carlo with multi-Slater determinant trial states","abstract":"The accuracy of phaseless auxiliary-field quantum Monte Carlo (ph-AFQMC) can be systematically improved with better trial states. Using multi-Slater determinant trial states, ph-AFQMC has the potential to faithfully treat strongly correlated systems, while balancing the static and dynamical correlations on an equal footing. This preprint presents an implementation and application of graphics processing unit-accelerated ph-AFQMC, for multi-Slater determinant trial wavefunctions (GPU-accelerated MSD-AFQMC), to enable efficient simulation of large-scale, strongly correlated systems. This approach allows for nearly-exact computation of ground state energies in multi-reference systems. Our GPU-accelerated MSD-AFQMC is implemented in the open-source code \\texttt{ipie}, a Python-based AFQMC package [\\textit{J. Chem. Theory Comput.}, 2022, 19(1): 109-121]. We benchmark the performance of the GPU code on transition-metal clusters like [Cu$_2$O$_2$]$^{2+}$ and [Fe$_2$S$_2$(SCH$_3$)]$^{2-}$. The GPU code achieves at least sixfold speedup in both cases, comparing the timings of a single A100 GPU to that of a 32-CPU node. For [Fe$_2$S$_2$(SCH$_3$)]$^{2-}$, we demonstrate that our GPU MSD-AFQMC can recover the dynamical correlation necessary for chemical accuracy with an MSD trial, despite the large number of determinants required ($>10^5$). Our work significantly enhances the efficiency of MSD-AFQMC calculations for large, strongly correlated molecules by utilizing GPUs, offering a promising path for exploring the electronic structure of transition metal complexes.","sentences":["The accuracy of phaseless auxiliary-field quantum Monte Carlo (ph-AFQMC) can be systematically improved with better trial states.","Using multi-Slater determinant trial states, ph-AFQMC has the potential to faithfully treat strongly correlated systems, while balancing the static and dynamical correlations on an equal footing.","This preprint presents an implementation and application of graphics processing unit-accelerated ph-AFQMC, for multi-Slater determinant trial wavefunctions (GPU-accelerated MSD-AFQMC), to enable efficient simulation of large-scale, strongly correlated systems.","This approach allows for nearly-exact computation of ground state energies in multi-reference systems.","Our GPU-accelerated MSD-AFQMC is implemented in the open-source code \\texttt{ipie}, a Python-based AFQMC package [\\textit{J. Chem.","Theory Comput.}, 2022, 19(1): 109-121].","We benchmark the performance of the GPU code on transition-metal clusters like [Cu$_2$O$_2$]$^{2+}$ and","[Fe$_2$S$_2$(SCH$_3$)]$^{2-}$. The GPU code achieves at least sixfold speedup in both cases, comparing the timings of a single A100 GPU to that of a 32-CPU node.","For [Fe$_2$S$_2$(SCH$_3$)]$^{2-}$, we demonstrate that our GPU MSD-AFQMC can recover the dynamical correlation necessary for chemical accuracy with an MSD trial, despite the large number of determinants required ($>10^5$).","Our work significantly enhances the efficiency of MSD-AFQMC calculations for large, strongly correlated molecules by utilizing GPUs, offering a promising path for exploring the electronic structure of transition metal complexes."],"url":"http://arxiv.org/abs/2406.08314v1","category":"physics.chem-ph"}
{"created":"2024-06-12 15:13:01","title":"On the nature of the radio calibrator and gamma-ray emitting NLS1 galaxy 3C 286 and its multiwavelength variability","abstract":"The quasar 3C 286, a well-known calibrator source in radio astronomy, was found to exhibit exceptional multiwavelength properties. Its rich and complex optical emission-line spectrum revealed its narrow-line Seyfert 1 (NLS1) nature. Given its strong radio emission, this makes 3C 286 one of the radio-loudest NLS1 galaxies known to date. 3C 286 is also one of very few known compact steep-spectrum (CSS) sources detected in the gamma-ray regime. Observations in the X-ray regime, rarely carried out so far, revealed evidence for variability, raising the question if driven by the accretion disk or jet. 3C 286 is also well known for its damped Lyman alpha system from an intervening absorber at z = 0.692, triggering a search for the corresponding X-ray absorption along the line-of-sight. Here, we present new observations in the radio, X-ray, optical and UV band. The nature of the X-ray variability is addressed. Spectral evidence suggests that it is primarily driven by the accretion disk (not the jet), and the X-ray spectrum is well fit by a powerlaw plus soft excess model. The radio flux density and polarization remain constant at the Effelsberg telescope resolution, reconfirming the use of 3C 286 as radio calibrator. The amount of reddening/absorption along the line-of-sight {\\em{intrinsic}} to 3C 286 is rigorously assessed. None is found, validating the derivation of a high Eddington ratio (L/L-Edd ~ 1) and of the very high radio-loudness index of 3C 286. Based on the first deep Chandra image of 3C 286, tentative evidence for hard X-ray emission from the SW radio lobe is reported. A large variety of models for the gamma-ray emission of 3C 286 is briefly discussed.","sentences":["The quasar 3C 286, a well-known calibrator source in radio astronomy, was found to exhibit exceptional multiwavelength properties.","Its rich and complex optical emission-line spectrum revealed its narrow-line Seyfert 1 (NLS1) nature.","Given its strong radio emission, this makes 3C 286 one of the radio-loudest NLS1 galaxies known to date.","3C 286 is also one of very few known compact steep-spectrum (CSS) sources detected in the gamma-ray regime.","Observations in the X-ray regime, rarely carried out so far, revealed evidence for variability, raising the question if driven by the accretion disk or jet.","3C 286 is also well known for its damped Lyman alpha system from an intervening absorber at z = 0.692, triggering a search for the corresponding X-ray absorption along the line-of-sight.","Here, we present new observations in the radio, X-ray, optical and UV band.","The nature of the X-ray variability is addressed.","Spectral evidence suggests that it is primarily driven by the accretion disk (not the jet), and the X-ray spectrum is well fit by a powerlaw plus soft excess model.","The radio flux density and polarization remain constant at the Effelsberg telescope resolution, reconfirming the use of 3C 286 as radio calibrator.","The amount of reddening/absorption along the line-of-sight {\\em{intrinsic}} to 3C 286 is rigorously assessed.","None is found, validating the derivation of a high Eddington ratio (L/L-Edd ~ 1) and of the very high radio-loudness index of 3C 286.","Based on the first deep Chandra image of 3C 286, tentative evidence for hard X-ray emission from the SW radio lobe is reported.","A large variety of models for the gamma-ray emission of 3C 286 is briefly discussed."],"url":"http://arxiv.org/abs/2406.08312v1","category":"astro-ph.HE"}
{"created":"2024-06-12 15:03:24","title":"NIRPS first light and early science: breaking the 1 m/s RV precision barrier at infrared wavelengths","abstract":"The Near-InfraRed Planet Searcher or NIRPS is a precision radial velocity spectrograph developed through collaborative efforts among laboratories in Switzerland, Canada, Brazil, France, Portugal and Spain. NIRPS extends to the 0.98-1.8 $\\mu$m domain of the pioneering HARPS instrument at the La Silla 3.6-m telescope in Chile and it has achieved unparalleled precision, measuring stellar radial velocities in the infrared with accuracy better than 1 m/s. NIRPS can be used either stand-alone or simultaneously with HARPS. Commissioned in late 2022 and early 2023, NIRPS embarked on a 5-year Guaranteed Time Observation (GTO) program in April 2023, spanning 720 observing nights. This program focuses on planetary systems around M dwarfs, encompassing both the immediate solar vicinity and transit follow-ups, alongside transit and emission spectroscopy observations. We highlight NIRPS's current performances and the insights gained during its deployment at the telescope. The lessons learned and successes achieved contribute to the ongoing advancement of precision radial velocity measurements and high spectral fidelity, further solidifying NIRPS' role in the forefront of the field of exoplanets.","sentences":["The Near-InfraRed Planet Searcher or NIRPS is a precision radial velocity spectrograph developed through collaborative efforts among laboratories in Switzerland, Canada, Brazil, France, Portugal and Spain.","NIRPS extends to the 0.98-1.8 $\\mu$m domain of the pioneering HARPS instrument at the La Silla 3.6-m telescope in Chile and it has achieved unparalleled precision, measuring stellar radial velocities in the infrared with accuracy better than 1 m/s. NIRPS can be used either stand-alone or simultaneously with HARPS.","Commissioned in late 2022 and early 2023, NIRPS embarked on a 5-year Guaranteed Time Observation (GTO) program in April 2023, spanning 720 observing nights.","This program focuses on planetary systems around M dwarfs, encompassing both the immediate solar vicinity and transit follow-ups, alongside transit and emission spectroscopy observations.","We highlight NIRPS's current performances and the insights gained during its deployment at the telescope.","The lessons learned and successes achieved contribute to the ongoing advancement of precision radial velocity measurements and high spectral fidelity, further solidifying NIRPS' role in the forefront of the field of exoplanets."],"url":"http://arxiv.org/abs/2406.08304v1","category":"astro-ph.IM"}
{"created":"2024-06-12 15:03:01","title":"Familiar biological, chemical and physical events credibly evolve the Standard Genetic Code","abstract":"The genetic code is profoundly shaped by an origin in ancient RNA-mediated interactions, needing an extended development to reach the Standard Genetic Code (SGC). That development can serially use RNA specificities, a ribonucleopeptide transition (RNPT), finally code escape and diaspora. An index of evolutionary plausibility based on least selection takes simultaneous account of speed and accuracy of evolution, identifying favored evolutions. Combining RNA world specificities allowed convergence of early coding to SGC assignments. Secondly, this was sufficient to launch a post-RNA-world RNPT. The RNPT allowed biosynthesis of complex amino acids, depending heavily on late code fusions between coexisting independent codes. Thirdly, escape from fluctuating, but highly-evolved codes of the RNPT applied a near-ideal selection for fastest-evolving and most accurate/useful genetic codes. Concurrently, a code and its microbial carrier suited to a free-living existence necessarily evolved. The established unity of life on Earth likely traces to SGC ascendancy during escape from the RNPT, and code diaspora.","sentences":["The genetic code is profoundly shaped by an origin in ancient RNA-mediated interactions, needing an extended development to reach the Standard Genetic Code (SGC).","That development can serially use RNA specificities, a ribonucleopeptide transition (RNPT), finally code escape and diaspora.","An index of evolutionary plausibility based on least selection takes simultaneous account of speed and accuracy of evolution, identifying favored evolutions.","Combining RNA world specificities allowed convergence of early coding to SGC assignments.","Secondly, this was sufficient to launch a post-RNA-world RNPT.","The RNPT allowed biosynthesis of complex amino acids, depending heavily on late code fusions between coexisting independent codes.","Thirdly, escape from fluctuating, but highly-evolved codes of the RNPT applied a near-ideal selection for fastest-evolving and most accurate/useful genetic codes.","Concurrently, a code and its microbial carrier suited to a free-living existence necessarily evolved.","The established unity of life on Earth likely traces to SGC ascendancy during escape from the RNPT, and code diaspora."],"url":"http://arxiv.org/abs/2406.08302v1","category":"q-bio.PE"}
{"created":"2024-06-12 15:00:05","title":"Dynamical evolution of social network polarization and its impact on the propagation of a virus","abstract":"The COVID-19 pandemic that emerged in 2020 has highlighted the complex interplay between vaccine hesitancy and societal polarization. In this study, we analyse the dynamical polarization within a social network as well as the network properties before and after a vaccine was made available. Our results show that as the network evolves from a less structured state to one with more clustered communities. Then using an agent-based modeling approach, we simulate the propagation of a virus in a polarized society by assigning vaccines to pro-vaccine individuals and none to the anti-vaccine individuals. We compare this propagation to the case where the same number of vaccines is distributed homogeneously across the population. In polarized networks, we observe a significantly more widespread diffusion of the virus, highlighting the importance of considering polarization for epidemic forecasting.","sentences":["The COVID-19 pandemic that emerged in 2020 has highlighted the complex interplay between vaccine hesitancy and societal polarization.","In this study, we analyse the dynamical polarization within a social network as well as the network properties before and after a vaccine was made available.","Our results show that as the network evolves from a less structured state to one with more clustered communities.","Then using an agent-based modeling approach, we simulate the propagation of a virus in a polarized society by assigning vaccines to pro-vaccine individuals and none to the anti-vaccine individuals.","We compare this propagation to the case where the same number of vaccines is distributed homogeneously across the population.","In polarized networks, we observe a significantly more widespread diffusion of the virus, highlighting the importance of considering polarization for epidemic forecasting."],"url":"http://arxiv.org/abs/2406.08299v1","category":"cs.SI"}
{"created":"2024-06-12 14:58:25","title":"Cooling of nuclear spin systems in semiconductor structures due to dynamic polarization by strongly localized electrons","abstract":"We present a theoretical analysis of cooling the spins of lattice nuclei in semiconductor structures by dynamic polarization with optically oriented electrons. Our consideration is not restricted to short spin correlation times of electrons. Therefore, it applies not only to electrons weakly bound to shallow donors, but also to electrons localized in quantum dots or deep impurity centers. The main result of the theory is that, in order to polarize nuclear spins efficiently with strongly localized electrons, one should apply external magnetic fields by far exceeding the local field of nuclear spin-spin interaction.","sentences":["We present a theoretical analysis of cooling the spins of lattice nuclei in semiconductor structures by dynamic polarization with optically oriented electrons.","Our consideration is not restricted to short spin correlation times of electrons.","Therefore, it applies not only to electrons weakly bound to shallow donors, but also to electrons localized in quantum dots or deep impurity centers.","The main result of the theory is that, in order to polarize nuclear spins efficiently with strongly localized electrons, one should apply external magnetic fields by far exceeding the local field of nuclear spin-spin interaction."],"url":"http://arxiv.org/abs/2406.08296v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-12 14:57:37","title":"Vessel Re-identification and Activity Detection in Thermal Domain for Maritime Surveillance","abstract":"Maritime surveillance is vital to mitigate illegal activities such as drug smuggling, illegal fishing, and human trafficking. Vision-based maritime surveillance is challenging mainly due to visibility issues at night, which results in failures in re-identifying vessels and detecting suspicious activities. In this paper, we introduce a thermal, vision-based approach for maritime surveillance with object tracking, vessel re-identification, and suspicious activity detection capabilities. For vessel re-identification, we propose a novel viewpoint-independent algorithm which compares features of the sides of the vessel separately (separate side-spaces) leveraging shape information in the absence of color features. We propose techniques to adapt tracking and activity detection algorithms for the thermal domain and train them using a thermal dataset we created. This dataset will be the first publicly available benchmark dataset for thermal maritime surveillance. Our system is capable of re-identifying vessels with an 81.8% Top1 score and identifying suspicious activities with a 72.4\\% frame mAP score; a new benchmark for each task in the thermal domain.","sentences":["Maritime surveillance is vital to mitigate illegal activities such as drug smuggling, illegal fishing, and human trafficking.","Vision-based maritime surveillance is challenging mainly due to visibility issues at night, which results in failures in re-identifying vessels and detecting suspicious activities.","In this paper, we introduce a thermal, vision-based approach for maritime surveillance with object tracking, vessel re-identification, and suspicious activity detection capabilities.","For vessel re-identification, we propose a novel viewpoint-independent algorithm which compares features of the sides of the vessel separately (separate side-spaces) leveraging shape information in the absence of color features.","We propose techniques to adapt tracking and activity detection algorithms for the thermal domain and train them using a thermal dataset we created.","This dataset will be the first publicly available benchmark dataset for thermal maritime surveillance.","Our system is capable of re-identifying vessels with an 81.8% Top1 score and identifying suspicious activities with a 72.4\\% frame mAP score; a new benchmark for each task in the thermal domain."],"url":"http://arxiv.org/abs/2406.08294v1","category":"cs.CV"}
{"created":"2024-06-12 14:47:27","title":"Conformal Load Prediction with Transductive Graph Autoencoders","abstract":"Predicting edge weights on graphs has various applications, from transportation systems to social networks. This paper describes a Graph Neural Network (GNN) approach for edge weight prediction with guaranteed coverage. We leverage conformal prediction to calibrate the GNN outputs and produce valid prediction intervals. We handle data heteroscedasticity through error reweighting and Conformalized Quantile Regression (CQR). We compare the performance of our method against baseline techniques on real-world transportation datasets. Our approach has better coverage and efficiency than all baselines and showcases robustness and adaptability.","sentences":["Predicting edge weights on graphs has various applications, from transportation systems to social networks.","This paper describes a Graph Neural Network (GNN) approach for edge weight prediction with guaranteed coverage.","We leverage conformal prediction to calibrate the GNN outputs and produce valid prediction intervals.","We handle data heteroscedasticity through error reweighting and Conformalized Quantile Regression (CQR).","We compare the performance of our method against baseline techniques on real-world transportation datasets.","Our approach has better coverage and efficiency than all baselines and showcases robustness and adaptability."],"url":"http://arxiv.org/abs/2406.08281v1","category":"cs.LG"}
{"created":"2024-06-12 14:46:34","title":"Minimal amenable subshift with full mean dimension","abstract":"Let $G$ be an infinite countable amenable group and $P$ a polyhedron with topological dimension $dim(P)<\\infty$. We construct a minimal subshift $(X,G)$ such that its mean topological dimension is equal to $dim(P)$. This result answers the question of D. Dou in \\cite{DD}, moreover, it is also an extension of the work of L. Jin and Y. Qiao \\cite{JQ} for $\\mathbb{Z}$-action.","sentences":["Let $G$ be an infinite countable amenable group and $P$ a polyhedron with topological dimension $dim(P)<\\infty$. We construct a minimal subshift $(X,G)$ such that its mean topological dimension is equal to $dim(P)$. This result answers the question of D. Dou in \\cite{DD}, moreover, it is also an extension of the work of L. Jin and Y. Qiao \\cite{JQ} for $\\mathbb{Z}$-action."],"url":"http://arxiv.org/abs/2406.08280v1","category":"math.DS"}
{"created":"2024-06-12 14:39:38","title":"The Camera and Readout for the Trinity Demonstrator and the EUSO-SPB2 Cherenkov Telescope","abstract":"We developed a modular silicon photomultiplier camera to detect Earth-skimming PeV to EeV tau neutrinos with the imaging atmospheric Cherenkov technique. We built two cameras, a 256-pixel camera with S14161-6050HS SiPMs for the Trinity Demonstrator located on Frisco Peak, Utah, and a 512-pixel camera with S14521-6050AN SiPMs for the EUSO-SPB2 Cherenkov Telescope. The front-end electronics are based on the eMUSIC ASIC, and the camera signals are sampled and digitized with the 100MS/s and 12-bit AGET system. Both cameras are liquid-cooled. We detail the camera concept and the results from characterizing the SiPMs, bench testing, and calibrating the two cameras.","sentences":["We developed a modular silicon photomultiplier camera to detect Earth-skimming PeV to EeV tau neutrinos with the imaging atmospheric Cherenkov technique.","We built two cameras, a 256-pixel camera with S14161-6050HS SiPMs for the Trinity Demonstrator located on Frisco Peak, Utah, and a 512-pixel camera with S14521-6050AN SiPMs for the EUSO-SPB2 Cherenkov Telescope.","The front-end electronics are based on the eMUSIC ASIC, and the camera signals are sampled and digitized with the 100MS/s and 12-bit AGET system.","Both cameras are liquid-cooled.","We detail the camera concept and the results from characterizing the SiPMs, bench testing, and calibrating the two cameras."],"url":"http://arxiv.org/abs/2406.08274v1","category":"astro-ph.IM"}
{"created":"2024-06-12 14:35:14","title":"Multi-Static ISAC based on Network-Assisted Full-Duplex Cell-Free Networks: Performance Analysis and Duplex Mode Optimization","abstract":"Multi-static integrated sensing and communication (ISAC) technology, which can achieve a wider coverage range and avoid self-interference, is an important trend for the future development of ISAC. Existing multi-static ISAC designs are unable to support the asymmetric uplink (UL)/downlink (DL) communication requirements in the scenario while simultaneously achieving optimal sensing performance. This paper proposes a design for multi-static ISAC based on network-assisted full-duplex (NAFD) cell-free networks can well solve the above problems. Under this design, closed-form expressions for the individual comunication rate and localization error rate are derived under imperfect channel state information, which are respectively utilized to assess the communication and sensing performances. Then, we propose a deep Q-network-based accesss point (AP) duplex mode optimization algorithm to obtain the trade-off between communication and sensing from the UL and DL perspectives of the APs. Simulation results demonstrate that the NAFD-based ISAC system proposed in this paper can achieve significantly better communication performance than other ISAC systems while ensuring minimal impact on sensing performance. Then, we validate the accuracy of the derived closed-form expressions. Furthermore, the proposed optimization algorithm achieves performance comparable to that of the exhaustion method with low complexity.","sentences":["Multi-static integrated sensing and communication (ISAC) technology, which can achieve a wider coverage range and avoid self-interference, is an important trend for the future development of ISAC.","Existing multi-static ISAC designs are unable to support the asymmetric uplink (UL)/downlink (DL) communication requirements in the scenario while simultaneously achieving optimal sensing performance.","This paper proposes a design for multi-static ISAC based on network-assisted full-duplex (NAFD) cell-free networks can well solve the above problems.","Under this design, closed-form expressions for the individual comunication rate and localization error rate are derived under imperfect channel state information, which are respectively utilized to assess the communication and sensing performances.","Then, we propose a deep Q-network-based accesss point (AP) duplex mode optimization algorithm to obtain the trade-off between communication and sensing from the UL and DL perspectives of the APs.","Simulation results demonstrate that the NAFD-based ISAC system proposed in this paper can achieve significantly better communication performance than other ISAC systems while ensuring minimal impact on sensing performance.","Then, we validate the accuracy of the derived closed-form expressions.","Furthermore, the proposed optimization algorithm achieves performance comparable to that of the exhaustion method with low complexity."],"url":"http://arxiv.org/abs/2406.08268v1","category":"eess.SY"}
{"created":"2024-06-12 14:34:26","title":"Emergent spinon-holon Feshbach resonance in a doped Majumdar-Ghosh model","abstract":"Experimental and numerical spectroscopy have revealed rich physics in antiferromagnets, in particular in frustrated and doped systems. The Majumdar-Ghosh (MG) model has an analytically known spin-disordered ground state of dimerized singlets as a result of magnetic frustration. Here we study the single-hole angle-resolved photoemission spectrum (ARPES) of a doped MG model, where we introduce a spin-hole interaction that is experimentally accessible with ultracold molecules. We report a bound spinon-holon ground state and clear signatures of a spinon-holon molecule state and polarons in the ARPES spectrum at different magnetizations. Moreover, we find signatures of an emergent Feshbach resonance with tunable interactions associated with the unbinding of the spinon and the holon. Our results provide new insights into the physics of dopants in frustrated $t$-$J$ models and establish the latter as a new platform for studies of emergent few-body phenomena.","sentences":["Experimental and numerical spectroscopy have revealed rich physics in antiferromagnets, in particular in frustrated and doped systems.","The Majumdar-Ghosh (MG) model has an analytically known spin-disordered ground state of dimerized singlets as a result of magnetic frustration.","Here we study the single-hole angle-resolved photoemission spectrum (ARPES) of a doped MG model, where we introduce a spin-hole interaction that is experimentally accessible with ultracold molecules.","We report a bound spinon-holon ground state and clear signatures of a spinon-holon molecule state and polarons in the ARPES spectrum at different magnetizations.","Moreover, we find signatures of an emergent Feshbach resonance with tunable interactions associated with the unbinding of the spinon and the holon.","Our results provide new insights into the physics of dopants in frustrated $t$-$J$ models and establish the latter as a new platform for studies of emergent few-body phenomena."],"url":"http://arxiv.org/abs/2406.08264v1","category":"cond-mat.str-el"}
{"created":"2024-06-12 14:29:32","title":"A Tannakian framework for prismatic $F$-crystals","abstract":"We develop the Tannakian theory of (analytic) prismatic $F$-crystals on a smooth formal scheme $\\mathfrak{X}$ over the ring of integers of a discretely valued field with perfect residue field. Our main result gives an equivalence between the $\\mathcal{G}$-objects of prismatic $F$-crystals on $\\mathfrak{X}$ and $\\mathcal{G}$-objects on a newly-defined category of $\\mathbb{Z}_p$-local systems on $\\mathfrak{X}_\\eta$: those of prismatically good reduction. Additionally, we develop a shtuka realization functor for (analytic) prismatic $F$-crystals on $p$-adic (formal) schemes and show it satisfies several compatibilities with previous work on the Tannakian theory of shtukas over such objects.","sentences":["We develop the Tannakian theory of (analytic) prismatic $F$-crystals on a smooth formal scheme $\\mathfrak{X}$ over the ring of integers of a discretely valued field with perfect residue field.","Our main result gives an equivalence between the $\\mathcal{G}$-objects of prismatic $F$-crystals on $\\mathfrak{X}$ and $\\mathcal{G}$-objects on a newly-defined category of $\\mathbb{Z}_p$-local systems on $\\mathfrak{X}_\\eta$: those of prismatically good reduction.","Additionally, we develop a shtuka realization functor for (analytic) prismatic $F$-crystals on $p$-adic (formal) schemes and show it satisfies several compatibilities with previous work on the Tannakian theory of shtukas over such objects."],"url":"http://arxiv.org/abs/2406.08259v1","category":"math.NT"}
{"created":"2024-06-12 14:29:19","title":"Chiral edge transport along domain walls in magnetic topological insulator nanoribbons","abstract":"Quantum anomalous Hall insulators are topologically characterized by non-zero integer Chern numbers, the sign of which depends on the direction of the exchange field that breaks time-reversal symmetry. This feature allows the manipulation of the conducting chiral edge states present at the interface of two magnetic domains with opposite magnetization and opposite Chern numbers. Motivated by this broad understanding, the present study investigates the quantum transport properties of a magnetized $Bi_2Se_3$ topological insulator nanoribbon with a domain wall oriented either parallel or perpendicular to the transport direction. Employing an atomistic tight-binding model and a non-equilibrium Green's function formalism, we calculate the quantum conductance and explore the nature of the edge states. We elucidate the conditions leading to exact conductance quantization and identify the origin of deviations from this behavior. Our analysis shows that although the conductance is quantized in the presence of the horizontal domain wall, the quantization is absent in the perpendicular domain wall case. Furthermore, the investigation of the spin character of the edge modes confirms that the conductance in the horizontal domain wall configuration is spin polarized. This finding underscores the potential of our system as a simple three dimensional spin-filter device.","sentences":["Quantum anomalous Hall insulators are topologically characterized by non-zero integer Chern numbers, the sign of which depends on the direction of the exchange field that breaks time-reversal symmetry.","This feature allows the manipulation of the conducting chiral edge states present at the interface of two magnetic domains with opposite magnetization and opposite Chern numbers.","Motivated by this broad understanding, the present study investigates the quantum transport properties of a magnetized $Bi_2Se_3$ topological insulator nanoribbon with a domain wall oriented either parallel or perpendicular to the transport direction.","Employing an atomistic tight-binding model and a non-equilibrium Green's function formalism, we calculate the quantum conductance and explore the nature of the edge states.","We elucidate the conditions leading to exact conductance quantization and identify the origin of deviations from this behavior.","Our analysis shows that although the conductance is quantized in the presence of the horizontal domain wall, the quantization is absent in the perpendicular domain wall case.","Furthermore, the investigation of the spin character of the edge modes confirms that the conductance in the horizontal domain wall configuration is spin polarized.","This finding underscores the potential of our system as a simple three dimensional spin-filter device."],"url":"http://arxiv.org/abs/2406.08258v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-12 14:29:03","title":"The need for accuracy and smoothness in numerical simulations","abstract":"We consider the problem of estimating the error when solving a system of differential algebraic equations. Richardson extrapolation is a classical technique that can be used to judge when computational errors are irrelevant and estimate the discretization error. We have simulated molecular dynamics with constraints using the GROMACS library and found that the output is not always amenable to Richardson extrapolation. We derive and illustrate Richardson extrapolation using a variety of numerical experiments. We identify two necessary conditions that are not always satisfied by the GROMACS library.","sentences":["We consider the problem of estimating the error when solving a system of differential algebraic equations.","Richardson extrapolation is a classical technique that can be used to judge when computational errors are irrelevant and estimate the discretization error.","We have simulated molecular dynamics with constraints using the GROMACS library and found that the output is not always amenable to Richardson extrapolation.","We derive and illustrate Richardson extrapolation using a variety of numerical experiments.","We identify two necessary conditions that are not always satisfied by the GROMACS library."],"url":"http://arxiv.org/abs/2406.08257v1","category":"math.NA"}
{"created":"2024-06-12 14:28:25","title":"M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation","abstract":"Document translation poses a challenge for Neural Machine Translation (NMT) systems. Most document-level NMT systems rely on meticulously curated sentence-level parallel data, assuming flawless extraction of text from documents along with their precise reading order. These systems also tend to disregard additional visual cues such as the document layout, deeming it irrelevant. However, real-world documents often possess intricate text layouts that defy these assumptions. Extracting information from Optical Character Recognition (OCR) or heuristic rules can result in errors, and the layout (e.g., paragraphs, headers) may convey relationships between distant sections of text. This complexity is particularly evident in widely used PDF documents, which represent information visually. This paper addresses this gap by introducing M3T, a novel benchmark dataset tailored to evaluate NMT systems on the comprehensive task of translating semi-structured documents. This dataset aims to bridge the evaluation gap in document-level NMT systems, acknowledging the challenges posed by rich text layouts in real-world applications.","sentences":["Document translation poses a challenge for Neural Machine Translation (NMT) systems.","Most document-level NMT systems rely on meticulously curated sentence-level parallel data, assuming flawless extraction of text from documents along with their precise reading order.","These systems also tend to disregard additional visual cues such as the document layout, deeming it irrelevant.","However, real-world documents often possess intricate text layouts that defy these assumptions.","Extracting information from Optical Character Recognition (OCR) or heuristic rules can result in errors, and the layout (e.g., paragraphs, headers) may convey relationships between distant sections of text.","This complexity is particularly evident in widely used PDF documents, which represent information visually.","This paper addresses this gap by introducing M3T, a novel benchmark dataset tailored to evaluate NMT systems on the comprehensive task of translating semi-structured documents.","This dataset aims to bridge the evaluation gap in document-level NMT systems, acknowledging the challenges posed by rich text layouts in real-world applications."],"url":"http://arxiv.org/abs/2406.08255v1","category":"cs.CL"}
{"created":"2024-06-12 14:26:57","title":"The Very High Energy Afterglow of Structured Jets: GW 170817 and Prospects for Future Detections","abstract":"We present a complete numerical model of the afterglow of a laterally-structured relativistic ejecta from radio to very high energy (VHE). This includes a self-consistent calculation of the synchrotron radiation, with its maximum frequency, and of Synchrotron Self-Compton (SSC) scatterings taking into account the Klein-Nishina regime. The attenuation due to pair production is also included. This model is computationally-efficient, allowing for multi-wavelength data fitting. As a validation test, the radiative model is used to fit the broad-band spectrum of GRB 190114C at 90 s up to the TeV range. The full model is then used to fit the afterglow of GW 170817 and predict its VHE emission. We find that the SSC flux at the peak was much dimmer than the upper limit from H.E.S.S. observations. However, we show that either a smaller viewing angle or a larger external density would make similar off-axis events detectable in the future at VHE, even above 100 Mpc with the sensitivity of the CTA. Large external densities are expected in the case of fast mergers, but the existence of a formation channel for such binary neutron stars is still uncertain. We highlight that VHE afterglow detections would help probing efficiently such systems.","sentences":["We present a complete numerical model of the afterglow of a laterally-structured relativistic ejecta from radio to very high energy (VHE).","This includes a self-consistent calculation of the synchrotron radiation, with its maximum frequency, and of Synchrotron Self-Compton (SSC) scatterings taking into account the Klein-Nishina regime.","The attenuation due to pair production is also included.","This model is computationally-efficient, allowing for multi-wavelength data fitting.","As a validation test, the radiative model is used to fit the broad-band spectrum of GRB 190114C at 90 s up to the TeV range.","The full model is then used to fit the afterglow of GW 170817 and predict its VHE emission.","We find that the SSC flux at the peak was much dimmer than the upper limit from H.E.S.S. observations.","However, we show that either a smaller viewing angle or a larger external density would make similar off-axis events detectable in the future at VHE, even above 100 Mpc with the sensitivity of the CTA.","Large external densities are expected in the case of fast mergers, but the existence of a formation channel for such binary neutron stars is still uncertain.","We highlight that VHE afterglow detections would help probing efficiently such systems."],"url":"http://arxiv.org/abs/2406.08254v1","category":"astro-ph.HE"}
{"created":"2024-06-12 14:23:53","title":"Sharding SMR with Optimal-size Shards for Highly Scalable Blockchains","abstract":"Sharding can enhance blockchain scalability by dividing nodes into multiple shards to handle transactions in parallel. However, the size-security dilemma where a shard must be large enough to ensure its security constrains the overall number of shards, rendering blockchain sharding low parallelism and poor scalability.   This paper presents Arete, an optimally scalable blockchain sharding architecture designed to resolve the dilemma based on a key observation: higher (Byzantine) fault-resilient shards allow the creation of more secure shards. The main idea of Arete, therefore, is to improve the security resilience/threshold of shards by sharding the blockchain's State Machine Replication (SMR) process itself. First, Arete decouples the three steps in SMR, leading to a single ordering shard performing the ordering task and multiple processing shards performing the dispersing and execution tasks. This frees processing shards from running consensus, allowing up to half compromised nodes per processing shard. Second, Arete considers safety and liveness against Byzantine failures separately to improve the safety threshold further while tolerating temporary liveness violations in a controlled manner. Apart from the creation of more optimal-size shards, such a deconstructed SMR scheme also empowers us to devise a novel certify-order-execute model to fully parallelize transaction handling, thereby significantly improving the performance of sharded blockchain systems. We implement Arete and evaluate it on a geo-distributed AWS environment. Our results demonstrate that Arete outperforms the state-of-the-art sharding protocol in terms of transaction throughput and cross-shard confirmation latency without compromising on intra-shard confirmation latency.","sentences":["Sharding can enhance blockchain scalability by dividing nodes into multiple shards to handle transactions in parallel.","However, the size-security dilemma where a shard must be large enough to ensure its security constrains the overall number of shards, rendering blockchain sharding low parallelism and poor scalability.   ","This paper presents Arete, an optimally scalable blockchain sharding architecture designed to resolve the dilemma based on a key observation: higher (Byzantine) fault-resilient shards allow the creation of more secure shards.","The main idea of Arete, therefore, is to improve the security resilience/threshold of shards by sharding the blockchain's State Machine Replication (SMR) process itself.","First, Arete decouples the three steps in SMR, leading to a single ordering shard performing the ordering task and multiple processing shards performing the dispersing and execution tasks.","This frees processing shards from running consensus, allowing up to half compromised nodes per processing shard.","Second, Arete considers safety and liveness against Byzantine failures separately to improve the safety threshold further while tolerating temporary liveness violations in a controlled manner.","Apart from the creation of more optimal-size shards, such a deconstructed SMR scheme also empowers us to devise a novel certify-order-execute model to fully parallelize transaction handling, thereby significantly improving the performance of sharded blockchain systems.","We implement Arete and evaluate it on a geo-distributed AWS environment.","Our results demonstrate that Arete outperforms the state-of-the-art sharding protocol in terms of transaction throughput and cross-shard confirmation latency without compromising on intra-shard confirmation latency."],"url":"http://arxiv.org/abs/2406.08252v1","category":"cs.CR"}
{"created":"2024-06-12 14:16:37","title":"Traffic Signal Cycle Control with Centralized Critic and Decentralized Actors under Varying Intervention Frequencies","abstract":"Traffic congestion in urban areas is a significant problem, leading to prolonged travel times, reduced efficiency, and increased environmental concerns. Effective traffic signal control (TSC) is a key strategy for reducing congestion. Unlike most TSC systems that rely on high-frequency control, this study introduces an innovative joint phase traffic signal cycle control method that operates effectively with varying control intervals. Our method features an adjust all phases action design, enabling simultaneous phase changes within the signal cycle, which fosters both immediate stability and sustained TSC effectiveness, especially at lower frequencies. The approach also integrates decentralized actors to handle the complexity of the action space, with a centralized critic to ensure coordinated phase adjusting. Extensive testing on both synthetic and real-world data across different intersection types and signal setups shows that our method significantly outperforms other popular techniques, particularly at high control intervals. Case studies of policies derived from traffic data further illustrate the robustness and reliability of our proposed method.","sentences":["Traffic congestion in urban areas is a significant problem, leading to prolonged travel times, reduced efficiency, and increased environmental concerns.","Effective traffic signal control (TSC) is a key strategy for reducing congestion.","Unlike most TSC systems that rely on high-frequency control, this study introduces an innovative joint phase traffic signal cycle control method that operates effectively with varying control intervals.","Our method features an adjust all phases action design, enabling simultaneous phase changes within the signal cycle, which fosters both immediate stability and sustained TSC effectiveness, especially at lower frequencies.","The approach also integrates decentralized actors to handle the complexity of the action space, with a centralized critic to ensure coordinated phase adjusting.","Extensive testing on both synthetic and real-world data across different intersection types and signal setups shows that our method significantly outperforms other popular techniques, particularly at high control intervals.","Case studies of policies derived from traffic data further illustrate the robustness and reliability of our proposed method."],"url":"http://arxiv.org/abs/2406.08248v1","category":"eess.SY"}
{"created":"2024-06-12 13:59:31","title":"GPT4Rec: Graph Prompt Tuning for Streaming Recommendation","abstract":"In the realm of personalized recommender systems, the challenge of adapting to evolving user preferences and the continuous influx of new users and items is paramount. Conventional models, typically reliant on a static training-test approach, struggle to keep pace with these dynamic demands. Streaming recommendation, particularly through continual graph learning, has emerged as a novel solution. However, existing methods in this area either rely on historical data replay, which is increasingly impractical due to stringent data privacy regulations; or are inability to effectively address the over-stability issue; or depend on model-isolation and expansion strategies. To tackle these difficulties, we present GPT4Rec, a Graph Prompt Tuning method for streaming Recommendation. Given the evolving user-item interaction graph, GPT4Rec first disentangles the graph patterns into multiple views. After isolating specific interaction patterns and relationships in different views, GPT4Rec utilizes lightweight graph prompts to efficiently guide the model across varying interaction patterns within the user-item graph. Firstly, node-level prompts are employed to instruct the model to adapt to changes in the attributes or properties of individual nodes within the graph. Secondly, structure-level prompts guide the model in adapting to broader patterns of connectivity and relationships within the graph. Finally, view-level prompts are innovatively designed to facilitate the aggregation of information from multiple disentangled views. These prompt designs allow GPT4Rec to synthesize a comprehensive understanding of the graph, ensuring that all vital aspects of the user-item interactions are considered and effectively integrated. Experiments on four diverse real-world datasets demonstrate the effectiveness and efficiency of our proposal.","sentences":["In the realm of personalized recommender systems, the challenge of adapting to evolving user preferences and the continuous influx of new users and items is paramount.","Conventional models, typically reliant on a static training-test approach, struggle to keep pace with these dynamic demands.","Streaming recommendation, particularly through continual graph learning, has emerged as a novel solution.","However, existing methods in this area either rely on historical data replay, which is increasingly impractical due to stringent data privacy regulations; or are inability to effectively address the over-stability issue; or depend on model-isolation and expansion strategies.","To tackle these difficulties, we present GPT4Rec, a Graph Prompt Tuning method for streaming Recommendation.","Given the evolving user-item interaction graph, GPT4Rec first disentangles the graph patterns into multiple views.","After isolating specific interaction patterns and relationships in different views, GPT4Rec utilizes lightweight graph prompts to efficiently guide the model across varying interaction patterns within the user-item graph.","Firstly, node-level prompts are employed to instruct the model to adapt to changes in the attributes or properties of individual nodes within the graph.","Secondly, structure-level prompts guide the model in adapting to broader patterns of connectivity and relationships within the graph.","Finally, view-level prompts are innovatively designed to facilitate the aggregation of information from multiple disentangled views.","These prompt designs allow GPT4Rec to synthesize a comprehensive understanding of the graph, ensuring that all vital aspects of the user-item interactions are considered and effectively integrated.","Experiments on four diverse real-world datasets demonstrate the effectiveness and efficiency of our proposal."],"url":"http://arxiv.org/abs/2406.08229v1","category":"cs.IR"}
{"created":"2024-06-12 13:50:38","title":"Impact of environmental interaction on bias induced circular current in a ring nanojunction","abstract":"The specific role of environmental interaction on bias driven circular current in a ring nanojunction is explored, for the first time to the best of our concern, within a tight-binding framework based on wave-guide theory. The environmental interaction is implemented through disorder in backbone sites where these sites are directly coupled to parent lattice sites of the ring via single bonds. In absence of backbone disorder circular current becomes zero for a lengthwise symmetric nanojunction, while it increases with disorder which is quite unusual, and after reaching a maximum it eventually drops to zero in the limit of high disorder. The effects of ring-electrode interface configuration, ring-backbone coupling, different types of backbone disorder and system temperature are critically investigated to make the present analysis comprehensive. All the studied results are valid for a broad range of physical parameters, giving us confidence that the outcomes of this theoretical work can be verified in a laboratory.","sentences":["The specific role of environmental interaction on bias driven circular current in a ring nanojunction is explored, for the first time to the best of our concern, within a tight-binding framework based on wave-guide theory.","The environmental interaction is implemented through disorder in backbone sites where these sites are directly coupled to parent lattice sites of the ring via single bonds.","In absence of backbone disorder circular current becomes zero for a lengthwise symmetric nanojunction, while it increases with disorder which is quite unusual, and after reaching a maximum it eventually drops to zero in the limit of high disorder.","The effects of ring-electrode interface configuration, ring-backbone coupling, different types of backbone disorder and system temperature are critically investigated to make the present analysis comprehensive.","All the studied results are valid for a broad range of physical parameters, giving us confidence that the outcomes of this theoretical work can be verified in a laboratory."],"url":"http://arxiv.org/abs/2406.08219v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-12 13:39:44","title":"Transformer-based Model for ASR N-Best Rescoring and Rewriting","abstract":"Voice assistants increasingly use on-device Automatic Speech Recognition (ASR) to ensure speed and privacy. However, due to resource constraints on the device, queries pertaining to complex information domains often require further processing by a search engine. For such applications, we propose a novel Transformer based model capable of rescoring and rewriting, by exploring full context of the N-best hypotheses in parallel. We also propose a new discriminative sequence training objective that can work well for both rescore and rewrite tasks. We show that our Rescore+Rewrite model outperforms the Rescore-only baseline, and achieves up to an average 8.6% relative Word Error Rate (WER) reduction over the ASR system by itself.","sentences":["Voice assistants increasingly use on-device Automatic Speech Recognition (ASR) to ensure speed and privacy.","However, due to resource constraints on the device, queries pertaining to complex information domains often require further processing by a search engine.","For such applications, we propose a novel Transformer based model capable of rescoring and rewriting, by exploring full context of the N-best hypotheses in parallel.","We also propose a new discriminative sequence training objective that can work well for both rescore and rewrite tasks.","We show that our Rescore+Rewrite model outperforms the Rescore-only baseline, and achieves up to an average 8.6% relative Word Error Rate (WER) reduction over the ASR system by itself."],"url":"http://arxiv.org/abs/2406.08207v1","category":"eess.AS"}
{"created":"2024-06-12 13:39:32","title":"Sources of Gain: Decomposing Performance in Conditional Average Dose Response Estimation","abstract":"Estimating conditional average dose responses (CADR) is an important but challenging problem. Estimators must correctly model the potentially complex relationships between covariates, interventions, doses, and outcomes. In recent years, the machine learning community has shown great interest in developing tailored CADR estimators that target specific challenges. Their performance is typically evaluated against other methods on (semi-) synthetic benchmark datasets. Our paper analyses this practice and shows that using popular benchmark datasets without further analysis is insufficient to judge model performance. Established benchmarks entail multiple challenges, whose impacts must be disentangled. Therefore, we propose a novel decomposition scheme that allows the evaluation of the impact of five distinct components contributing to CADR estimator performance. We apply this scheme to eight popular CADR estimators on four widely-used benchmark datasets, running nearly 1,500 individual experiments. Our results reveal that most established benchmarks are challenging for reasons different from their creators' claims. Notably, confounding, the key challenge tackled by most estimators, is not an issue in any of the considered datasets. We discuss the major implications of our findings and present directions for future research.","sentences":["Estimating conditional average dose responses (CADR) is an important but challenging problem.","Estimators must correctly model the potentially complex relationships between covariates, interventions, doses, and outcomes.","In recent years, the machine learning community has shown great interest in developing tailored CADR estimators that target specific challenges.","Their performance is typically evaluated against other methods on (semi-) synthetic benchmark datasets.","Our paper analyses this practice and shows that using popular benchmark datasets without further analysis is insufficient to judge model performance.","Established benchmarks entail multiple challenges, whose impacts must be disentangled.","Therefore, we propose a novel decomposition scheme that allows the evaluation of the impact of five distinct components contributing to CADR estimator performance.","We apply this scheme to eight popular CADR estimators on four widely-used benchmark datasets, running nearly 1,500 individual experiments.","Our results reveal that most established benchmarks are challenging for reasons different from their creators' claims.","Notably, confounding, the key challenge tackled by most estimators, is not an issue in any of the considered datasets.","We discuss the major implications of our findings and present directions for future research."],"url":"http://arxiv.org/abs/2406.08206v1","category":"cs.LG"}
{"created":"2024-06-12 13:29:36","title":"FreeV: Free Lunch For Vocoders Through Pseudo Inversed Mel Filter","abstract":"Vocoders reconstruct speech waveforms from acoustic features and play a pivotal role in modern TTS systems. Frequent-domain GAN vocoders like Vocos and APNet2 have recently seen rapid advancements, outperforming time-domain models in inference speed while achieving comparable audio quality. However, these frequency-domain vocoders suffer from large parameter sizes, thus introducing extra memory burden. Inspired by PriorGrad and SpecGrad, we employ pseudo-inverse to estimate the amplitude spectrum as the initialization roughly. This simple initialization significantly mitigates the parameter demand for vocoder. Based on APNet2 and our streamlined Amplitude prediction branch, we propose our FreeV, compared with its counterpart APNet2, our FreeV achieves 1.8 times inference speed improvement with nearly half parameters. Meanwhile, our FreeV outperforms APNet2 in resynthesis quality, marking a step forward in pursuing real-time, high-fidelity speech synthesis. Code and checkpoints is available at: https://github.com/BakerBunker/FreeV","sentences":["Vocoders reconstruct speech waveforms from acoustic features and play a pivotal role in modern TTS systems.","Frequent-domain GAN vocoders like Vocos and APNet2 have recently seen rapid advancements, outperforming time-domain models in inference speed while achieving comparable audio quality.","However, these frequency-domain vocoders suffer from large parameter sizes, thus introducing extra memory burden.","Inspired by PriorGrad and SpecGrad, we employ pseudo-inverse to estimate the amplitude spectrum as the initialization roughly.","This simple initialization significantly mitigates the parameter demand for vocoder.","Based on APNet2 and our streamlined Amplitude prediction branch, we propose our FreeV, compared with its counterpart APNet2, our FreeV achieves 1.8 times inference speed improvement with nearly half parameters.","Meanwhile, our FreeV outperforms APNet2 in resynthesis quality, marking a step forward in pursuing real-time, high-fidelity speech synthesis.","Code and checkpoints is available at: https://github.com/BakerBunker/FreeV"],"url":"http://arxiv.org/abs/2406.08196v1","category":"cs.SD"}
{"created":"2024-06-12 13:20:39","title":"Inverse scattering transform for the defocusing-defocusing coupled Hirota equations with non-zero boundary conditions: double-pole solutions","abstract":"The inverse scattering transform for the defocusing-defocusing coupled Hirota equations with non-zero boundary conditions at infinity is thoroughly discussed. We delve into the analytical properties of the Jost eigenfunctions and scrutinize the characteristics of the scattering coefficients. To enhance our investigation of the fundamental eigenfunctions, we have derived additional auxiliary eigenfunctions with the help of the adjoint problem. Two symmetry conditions are studied to constrain the behavior of the eigenfunctions and scattering coefficients. Utilizing these symmetries, we precisely delineate the discrete spectrum and establish the associated symmetries of the scattering data. By framing the inverse problem within the context of the Riemann-Hilbert problem, we develop suitable jump conditions to express the eigenfunctions. Consequently, we deduce the pure soliton solutions from the defocusing-defocusing coupled Hirota equations, and the double-poles solutions are provided explicitly for the first time in this work.","sentences":["The inverse scattering transform for the defocusing-defocusing coupled Hirota equations with non-zero boundary conditions at infinity is thoroughly discussed.","We delve into the analytical properties of the Jost eigenfunctions and scrutinize the characteristics of the scattering coefficients.","To enhance our investigation of the fundamental eigenfunctions, we have derived additional auxiliary eigenfunctions with the help of the adjoint problem.","Two symmetry conditions are studied to constrain the behavior of the eigenfunctions and scattering coefficients.","Utilizing these symmetries, we precisely delineate the discrete spectrum and establish the associated symmetries of the scattering data.","By framing the inverse problem within the context of the Riemann-Hilbert problem, we develop suitable jump conditions to express the eigenfunctions.","Consequently, we deduce the pure soliton solutions from the defocusing-defocusing coupled Hirota equations, and the double-poles solutions are provided explicitly for the first time in this work."],"url":"http://arxiv.org/abs/2406.08189v1","category":"nlin.SI"}
{"created":"2024-06-12 13:17:05","title":"Hiperwalk: Simulation of Quantum Walks with Heterogeneous High-Performance Computing","abstract":"The Hiperwalk package is designed to facilitate the simulation of quantum walks using heterogeneous high-performance computing, taking advantage of the parallel processing power of diverse processors such as CPUs, GPUs, and acceleration cards. This package enables the simulation of both the continuous-time and discrete-time quantum walk models, effectively modeling the behavior of quantum systems on large graphs. Hiperwalk features a user-friendly Python package frontend with comprehensive documentation, as well as a high-performance C-based inner core that leverages parallel computing for efficient linear algebra calculations. This versatile tool empowers researchers to better understand quantum walk behavior, optimize implementation, and explore a wide range of potential applications, including spatial search algorithms.","sentences":["The Hiperwalk package is designed to facilitate the simulation of quantum walks using heterogeneous high-performance computing, taking advantage of the parallel processing power of diverse processors such as CPUs, GPUs, and acceleration cards.","This package enables the simulation of both the continuous-time and discrete-time quantum walk models, effectively modeling the behavior of quantum systems on large graphs.","Hiperwalk features a user-friendly Python package frontend with comprehensive documentation, as well as a high-performance C-based inner core that leverages parallel computing for efficient linear algebra calculations.","This versatile tool empowers researchers to better understand quantum walk behavior, optimize implementation, and explore a wide range of potential applications, including spatial search algorithms."],"url":"http://arxiv.org/abs/2406.08186v1","category":"quant-ph"}
{"created":"2024-06-12 13:12:54","title":"Stochastic Process-based Method for Degree-Degree Correlation of Evolving Networks","abstract":"Existing studies on the degree correlation of evolving networks typically rely on differential equations and statistical analysis, resulting in only approximate solutions due to inherent randomness. To address this limitation, we propose an improved Markov chain method for modeling degree correlation in evolving networks. By redesigning the network evolution rules to reflect actual network dynamics more accurately, we achieve a topological structure that closely matches real-world network evolution. Our method models the degree correlation evolution process for both directed and undirected networks and provides theoretical results that are verified through simulations. This work offers the first theoretical solution for the steady-state degree correlation in evolving network models and is applicable to more complex evolution mechanisms and networks with directional attributes. Additionally, it supports the study of dynamic characteristic control based on network structure at any given time, offering a new tool for researchers in the field.","sentences":["Existing studies on the degree correlation of evolving networks typically rely on differential equations and statistical analysis, resulting in only approximate solutions due to inherent randomness.","To address this limitation, we propose an improved Markov chain method for modeling degree correlation in evolving networks.","By redesigning the network evolution rules to reflect actual network dynamics more accurately, we achieve a topological structure that closely matches real-world network evolution.","Our method models the degree correlation evolution process for both directed and undirected networks and provides theoretical results that are verified through simulations.","This work offers the first theoretical solution for the steady-state degree correlation in evolving network models and is applicable to more complex evolution mechanisms and networks with directional attributes.","Additionally, it supports the study of dynamic characteristic control based on network structure at any given time, offering a new tool for researchers in the field."],"url":"http://arxiv.org/abs/2406.08180v1","category":"stat.CO"}
{"created":"2024-06-12 13:06:40","title":"A computationally efficient procedure for combining ecological datasets by means of sequential consensus inference","abstract":"Combining data has become an indispensable tool for managing the current diversity and abundance of data. But, as data complexity and data volume swell, the computational demands of previously proposed models for combining data escalate proportionally, posing a significant challenge to practical implementation. This study presents a sequential consensus Bayesian inference procedure that allows for a flexible definition of models, aiming to emulate the versatility of integrated models while significantly reducing their computational cost. The method is based on updating the distribution of the fixed effects and hyperparameters from their marginal posterior distribution throughout a sequential inference procedure, and performing a consensus on the random effects after the sequential inference is completed. The applicability, together with its strengths and limitations, is outlined in the methodological description of the procedure. The sequential consensus method is presented in two distinct algorithms. The first algorithm performs a sequential updating and consensus from the stored values of the marginal or joint posterior distribution of the random effects. The second algorithm performs an extra step, addressing the deficiencies that may arise when the model partition does not share the whole latent field. The performance of the procedure is shown by three different examples -- one simulated and two with real data -- intending to expose its strengths and limitations.","sentences":["Combining data has become an indispensable tool for managing the current diversity and abundance of data.","But, as data complexity and data volume swell, the computational demands of previously proposed models for combining data escalate proportionally, posing a significant challenge to practical implementation.","This study presents a sequential consensus Bayesian inference procedure that allows for a flexible definition of models, aiming to emulate the versatility of integrated models while significantly reducing their computational cost.","The method is based on updating the distribution of the fixed effects and hyperparameters from their marginal posterior distribution throughout a sequential inference procedure, and performing a consensus on the random effects after the sequential inference is completed.","The applicability, together with its strengths and limitations, is outlined in the methodological description of the procedure.","The sequential consensus method is presented in two distinct algorithms.","The first algorithm performs a sequential updating and consensus from the stored values of the marginal or joint posterior distribution of the random effects.","The second algorithm performs an extra step, addressing the deficiencies that may arise when the model partition does not share the whole latent field.","The performance of the procedure is shown by three different examples -- one simulated and two with real data -- intending to expose its strengths and limitations."],"url":"http://arxiv.org/abs/2406.08174v1","category":"stat.ME"}
{"created":"2024-06-12 12:58:02","title":"Global Tests for Smoothed Functions in Mean Field Variational Additive Models","abstract":"Variational regression methods are an increasingly popular tool for their efficient estimation of complex. Given the mixed model representation of penalized effects, additive regression models with smoothed effects and scalar-on-function regression models can be fit relatively efficiently in a variational framework. However, inferential procedures for smoothed and functional effects in such a context is limited. We demonstrate that by using the Mean Field Variational Bayesian (MFVB) approximation to the additive model and the subsequent Coordinate Ascent Variational Inference (CAVI) algorithm, we can obtain a form of the estimated effects required of a Frequentist test for semiparametric curves. We establish MFVB approximations and CAVI algorithms for both Gaussian and binary additive models with an arbitrary number of smoothed and functional effects. We then derive a global testing framework for smoothed and functional effects. Our empirical study demonstrates that the test maintains good Frequentist properties in the variational framework and can be used to directly test results from a converged, MFVB approximation and CAVI algorithm. We illustrate the applicability of this approach in a wide range of data illustrations.","sentences":["Variational regression methods are an increasingly popular tool for their efficient estimation of complex.","Given the mixed model representation of penalized effects, additive regression models with smoothed effects and scalar-on-function regression models can be fit relatively efficiently in a variational framework.","However, inferential procedures for smoothed and functional effects in such a context is limited.","We demonstrate that by using the Mean Field Variational Bayesian (MFVB) approximation to the additive model and the subsequent Coordinate Ascent Variational Inference (CAVI) algorithm, we can obtain a form of the estimated effects required of a Frequentist test for semiparametric curves.","We establish MFVB approximations and CAVI algorithms for both Gaussian and binary additive models with an arbitrary number of smoothed and functional effects.","We then derive a global testing framework for smoothed and functional effects.","Our empirical study demonstrates that the test maintains good Frequentist properties in the variational framework and can be used to directly test results from a converged, MFVB approximation and CAVI algorithm.","We illustrate the applicability of this approach in a wide range of data illustrations."],"url":"http://arxiv.org/abs/2406.08168v1","category":"stat.ME"}
{"created":"2024-06-12 12:52:09","title":"A conceptual predator-prey model with super-long transients","abstract":"Drawing on the understanding of the logistic map, we propose a simple predator-prey model where predators and prey adapt to each other, leading to the co-evolution of the system. The special dynamics observed in periodic windows contribute to the coexistence of multiple time scales, adding to the complexity of the system. Typical dynamics in ecosystems, such as the persistence and coexistence of population cycles and chaotic behaviors, the emergence of super-long transients, regime shifts, and the quantifying of resilience, are encapsulated within this single model. The simplicity of our model allows for detailed analysis, including linear analysis, reinforcing its potential as a conceptual tool for understanding ecosystems deeply. Additionally, our results suggest that longer lifetimes in ecosystems might come at the expense of reduced populations due to limited resources.","sentences":["Drawing on the understanding of the logistic map, we propose a simple predator-prey model where predators and prey adapt to each other, leading to the co-evolution of the system.","The special dynamics observed in periodic windows contribute to the coexistence of multiple time scales, adding to the complexity of the system.","Typical dynamics in ecosystems, such as the persistence and coexistence of population cycles and chaotic behaviors, the emergence of super-long transients, regime shifts, and the quantifying of resilience, are encapsulated within this single model.","The simplicity of our model allows for detailed analysis, including linear analysis, reinforcing its potential as a conceptual tool for understanding ecosystems deeply.","Additionally, our results suggest that longer lifetimes in ecosystems might come at the expense of reduced populations due to limited resources."],"url":"http://arxiv.org/abs/2406.08163v1","category":"q-bio.PE"}
{"created":"2024-06-12 12:45:30","title":"Scaling behavior of the localization length for TE waves at critical incidence on short-range correlated stratified random media","abstract":"We theoretically investigate the scaling behavior of the localization length for $s$-polarized electromagnetic waves incident at a critical angle on stratified random media with short-range correlated disorder. By employing the invariant embedding method, extended to waves in correlated random media, and utilizing the Shapiro-Loginov formula of differentiation, we accurately compute the localization length $\\xi$ of $s$ waves incident obliquely on stratified random media that exhibit short-range correlated dichotomous randomness in the dielectric permittivity. The random component of the permittivity is characterized by the disorder strength parameter $\\sigma^2$ and the disorder correlation length $l_c$. Away from the critical angle, $\\xi$ depends on these parameters independently. However, precisely at the critical angle, we discover that for waves with wavenumber $k$, $k\\xi$ depends on the single parameter $kl_c\\sigma^2$, satisfying a universal equation $k\\xi\\approx 1.3717\\left(kl_c\\sigma^2\\right)^{-1/3}$ across the entire range of parameter values. Additionally, we find that $\\xi$ scales as ${\\lambda}^{4/3}$ for the entire range of the wavelength $\\lambda$, regardless of the values of $\\sigma^2$ and $l_c$. We demonstrate that under sufficiently strong disorder, the scaling behavior of the localization length for all other incident angles converges to that for the critical incidence.","sentences":["We theoretically investigate the scaling behavior of the localization length for $s$-polarized electromagnetic waves incident at a critical angle on stratified random media with short-range correlated disorder.","By employing the invariant embedding method, extended to waves in correlated random media, and utilizing the Shapiro-Loginov formula of differentiation, we accurately compute the localization length $\\xi$ of $s$ waves incident obliquely on stratified random media that exhibit short-range correlated dichotomous randomness in the dielectric permittivity.","The random component of the permittivity is characterized by the disorder strength parameter $\\sigma^2$ and the disorder correlation length $l_c$. Away from the critical angle, $\\xi$ depends on these parameters independently.","However, precisely at the critical angle, we discover that for waves with wavenumber $k$, $k\\xi$ depends on the single parameter $kl_c\\sigma^2$, satisfying a universal equation $k\\xi\\approx 1.3717\\left(kl_c\\sigma^2\\right)^{-1/3}$ across the entire range of parameter values.","Additionally, we find that $\\xi$ scales as ${\\lambda}^{4/3}$ for the entire range of the wavelength $\\lambda$, regardless of the values of $\\sigma^2$ and $l_c$. We demonstrate that under sufficiently strong disorder, the scaling behavior of the localization length for all other incident angles converges to that for the critical incidence."],"url":"http://arxiv.org/abs/2406.08156v1","category":"physics.optics"}
{"created":"2024-06-12 12:43:44","title":"Revisiting N$_2$ with Neural-Network-Supported CI","abstract":"We apply a recently proposed computational protocol for a neural-network-supported configuration interaction (NN CI) calculation to the paradigmatic N$_2$ molecule. By comparison of correlation energy, binding energy, and the full dissociation curve to experimental and full CI benchmarks, we demonstrate the applicability and robustness of our approach for the first time in the context of molecular systems, and offer thereby a new complementary tool in the family of machine-learning-based computation methods. The main advantage of the method lies in the efficiency of the neural-network-selected many-body basis set. Specifically, we approximate full CI results obtained on bases of $\\approx 10^{10}$ Slater Determinants with only $\\approx10^{5}$ determinants with good accuracy. The high efficiency of the NN CI approach underlines its potential for broader applications such as structural optimizations and even computation of spectroscopic observables in systems for which computational resources are a limiting factor.","sentences":["We apply a recently proposed computational protocol for a neural-network-supported configuration interaction (NN CI) calculation to the paradigmatic N$_2$ molecule.","By comparison of correlation energy, binding energy, and the full dissociation curve to experimental and full CI benchmarks, we demonstrate the applicability and robustness of our approach for the first time in the context of molecular systems, and offer thereby a new complementary tool in the family of machine-learning-based computation methods.","The main advantage of the method lies in the efficiency of the neural-network-selected many-body basis set.","Specifically, we approximate full CI results obtained on bases of $\\approx 10^{10}$ Slater Determinants with only $\\approx10^{5}$ determinants with good accuracy.","The high efficiency of the NN CI approach underlines its potential for broader applications such as structural optimizations and even computation of spectroscopic observables in systems for which computational resources are a limiting factor."],"url":"http://arxiv.org/abs/2406.08154v1","category":"physics.chem-ph"}
{"created":"2024-06-12 12:42:07","title":"Optimal control of quantum system in fermion fields: Pontryagin-type maximum principle(I)","abstract":"In this paper, the Pontryagin-type maximum principle for optimal control of quantum stochastic systems in fermion fields is obtained. These systems have gained significant prominence in numerous quantum applications ranging from physical chemistry to multi-dimensional nuclear magnetic resonance experiments. Furthermore, we establish the existence and uniqueness of solutions to backward quantum stochastic differential equations driven by fermion Brownian motion. The application of noncommutative martingale inequalities and the martingale representation theorem enables this achievement.","sentences":["In this paper, the Pontryagin-type maximum principle for optimal control of quantum stochastic systems in fermion fields is obtained.","These systems have gained significant prominence in numerous quantum applications ranging from physical chemistry to multi-dimensional nuclear magnetic resonance experiments.","Furthermore, we establish the existence and uniqueness of solutions to backward quantum stochastic differential equations driven by fermion Brownian motion.","The application of noncommutative martingale inequalities and the martingale representation theorem enables this achievement."],"url":"http://arxiv.org/abs/2406.08153v1","category":"math.OC"}
{"created":"2024-06-12 12:38:33","title":"Universal Scale Laws for Colors and Patterns in Imagery","abstract":"Distribution of colors and patterns in images is observed through cascades that adjust spatial resolution and dynamics. Cascades of colors reveal the emergent universal property that Fully Colored Images (FCIs) of natural scenes adhere to the debated continuous linear log-scale law (slope $-2.00 \\pm 0.01$) (L1). Cascades of discrete $2 \\times 2$ patterns are derived from pixel squares reductions onto the seven unlabeled rotation-free textures (0000, 0001, 0011, 0012, 0101, 0102, 0123). They exhibit an unparalleled universal entropy maximum of $1.74 \\pm 0.013$ at some dynamics regardless of spatial scale (L2). Patterns also adhere to the Integral Fluctuation Theorem ($1.00 \\pm 0.01$) (L3), pivotal in studies of chaotic systems. Images with fewer colors exhibit quadratic shift and bias from L1 and L3 but adhere to L2. Randomized Hilbert fractals FCIs better match the laws than basic-to-AI-based simulations. Those results are of interest in Neural Networks, out of equilibrium physics and spectral imagery.","sentences":["Distribution of colors and patterns in images is observed through cascades that adjust spatial resolution and dynamics.","Cascades of colors reveal the emergent universal property that Fully Colored Images (FCIs) of natural scenes adhere to the debated continuous linear log-scale law (slope $-2.00 \\pm 0.01$) (L1).","Cascades of discrete $2 \\times 2$ patterns are derived from pixel squares reductions onto the seven unlabeled rotation-free textures (0000, 0001, 0011, 0012, 0101, 0102, 0123).","They exhibit an unparalleled universal entropy maximum of $1.74 \\pm 0.013$ at some dynamics regardless of spatial scale (L2).","Patterns also adhere to the Integral Fluctuation Theorem ($1.00 \\pm 0.01$) (L3), pivotal in studies of chaotic systems.","Images with fewer colors exhibit quadratic shift and bias from L1 and L3 but adhere to L2.","Randomized Hilbert fractals FCIs better match the laws than basic-to-AI-based simulations.","Those results are of interest in Neural Networks, out of equilibrium physics and spectral imagery."],"url":"http://arxiv.org/abs/2406.08149v1","category":"cs.CV"}
{"created":"2024-06-12 12:35:53","title":"Interfacial Dynamics and Catalytic Behavior of Single Ni Atom Site","abstract":"Single-atom catalysts (SACs) have garnered significant interest due to their ability to reduce metal particles to the atomic scale, enabling finely tunable local environments and enhanced catalytic properties in terms of reactivity and selectivity. Despite this potential, their application has largely been confined to small-molecule transformations as metal-catalyzed reaction. In this study, we present a diverse single-atom nickel (Ni) catalyst established via a nanoporous carbon (NPC) supported practice. This catalyst represents a breakthrough by achieving the bond formation between carbon and nitrogen and interfacial dynamics in the SAC. The present first principle-based density functional simulations establish the reaction dynamics and catalytic behaviour of such SAC. This dynamic nature comprises an exclusive nitrogen intercalated site showing excellent base effects. This base quickly tunes the interfacial atmosphere, enabling dynamic movement of adatoms into the NPC species, significantly changing the reaction path in Ni SACs due to superior steric effects. The research demonstrates that SACs can extend the capabilities of catalytic systems to include a wider range of complex reactions, offering substantial promise for the development of new, efficient synthetic methods for creating value-added molecular products.","sentences":["Single-atom catalysts (SACs) have garnered significant interest due to their ability to reduce metal particles to the atomic scale, enabling finely tunable local environments and enhanced catalytic properties in terms of reactivity and selectivity.","Despite this potential, their application has largely been confined to small-molecule transformations as metal-catalyzed reaction.","In this study, we present a diverse single-atom nickel (Ni) catalyst established via a nanoporous carbon (NPC) supported practice.","This catalyst represents a breakthrough by achieving the bond formation between carbon and nitrogen and interfacial dynamics in the SAC.","The present first principle-based density functional simulations establish the reaction dynamics and catalytic behaviour of such SAC.","This dynamic nature comprises an exclusive nitrogen intercalated site showing excellent base effects.","This base quickly tunes the interfacial atmosphere, enabling dynamic movement of adatoms into the NPC species, significantly changing the reaction path in Ni SACs due to superior steric effects.","The research demonstrates that SACs can extend the capabilities of catalytic systems to include a wider range of complex reactions, offering substantial promise for the development of new, efficient synthetic methods for creating value-added molecular products."],"url":"http://arxiv.org/abs/2406.08146v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-12 12:35:33","title":"Realizations of free actions via their fixed point algebras","abstract":"Let $G$ be a compact group, let $\\mathcal{B}$ be a unital C$^*$-algebra, and let $(\\mathcal{A},G,\\alpha)$ be a free C$^*$-dynamical system, in the sense of Ellwood, with fixed point algebra $\\mathcal{B}$. We prove that $(\\mathcal{A},G,\\alpha)$ can be realized as the invariants of an equivariant coaction of $G$ on a corner of $\\mathcal{B} \\otimes \\mathcal{K}(\\mathfrak{H})$ for a certain Hilbert space $\\mathfrak{H}$ that arises from the freeness of the action. This extends a result by Wassermann for free C$^*$-dynamical systems with trivial fixed point algebras. As an application, we show that any faithful \\Star-representation of $\\mathcal{B}$ on a Hilbert space $\\mathfrak{H}_{\\mathcal{B}}$ gives rise to a faithful covariant representation of $(\\mathcal{A},G,\\alpha)$ on some truncation of $\\mathfrak{H}_{\\mathcal{B}} \\otimes \\mathfrak{H}$.","sentences":["Let $G$ be a compact group, let $\\mathcal{B}$ be a unital C$^*$-algebra, and let $(\\mathcal{A},G,\\alpha)$ be a free C$^*$-dynamical system, in the sense of Ellwood, with fixed point algebra $\\mathcal{B}$. We prove that $(\\mathcal{A},G,\\alpha)$ can be realized as the invariants of an equivariant coaction of $G$ on a corner of $\\mathcal{B} \\otimes \\mathcal{K}(\\mathfrak{H})$ for a certain Hilbert space $\\mathfrak{H}$ that arises from the freeness of the action.","This extends a result by Wassermann for free C$^*$-dynamical systems with trivial fixed point algebras.","As an application, we show that any faithful \\Star-representation of $\\mathcal{B}$ on a Hilbert space $\\mathfrak{H}_{\\mathcal{B}}$ gives rise to a faithful covariant representation of $(\\mathcal{A},G,\\alpha)$ on some truncation of $\\mathfrak{H}_{\\mathcal{B}} \\otimes \\mathfrak{H}$."],"url":"http://arxiv.org/abs/2406.08145v1","category":"math.OA"}
{"created":"2024-06-12 12:28:13","title":"Limit cycles of piecewise smooth differential systems of the type nonlinear center and saddle","abstract":"Piecewise linear differential systems separated by two parallel straight lines of the type of center-center-Hamiltonian saddle and the center-Hamiltonian saddle-Hamiltonian saddle can have at most one limit cycle and there are systems in these classes having one limit cycle. In this paper, we study the limit cycles of a piecewise smooth differential system separated by two parallel straight lines formed by nonlinear centers and a Hamiltonian saddle.","sentences":["Piecewise linear differential systems separated by two parallel straight lines of the type of center-center-Hamiltonian saddle and the center-Hamiltonian saddle-Hamiltonian saddle can have at most one limit cycle and there are systems in these classes having one limit cycle.","In this paper, we study the limit cycles of a piecewise smooth differential system separated by two parallel straight lines formed by nonlinear centers and a Hamiltonian saddle."],"url":"http://arxiv.org/abs/2406.08138v1","category":"math.DS"}
{"created":"2024-06-12 12:26:10","title":"$\u03c9$-regular Expression Synthesis from Transition-Based B\u00fcchi Automata","abstract":"A popular method for modelling reactive systems is to use $\\omega$-regular languages. These languages can be represented as nondeterministic B\\\"uchi automata (NBAs) or $\\omega$-regular expressions. Existing methods synthesise expressions from state-based NBAs. Synthesis from transition-based NBAs is traditionally done by transforming transition-based NBAs into state-based NBAs. This transformation, however, can increase the complexity of the synthesised expressions. This paper proposes a novel method for directly synthesising $\\omega$-regular expressions from transition-based NBAs. We prove that the method is sound and complete. Our empirical results show that the $\\omega$-regular expressions synthesised from transition-based NBAs are more compact than those synthesised from state-based NBAs. This is particularly the case for NBAs computed from obligation, reactivity, safety and recurrence-type LTL formulas, reporting in the latter case an average reduction of over 50%. We also show that our method successfully synthesises $\\omega$-regular expressions from more LTL formulas when using a transition-based instead of a state-based NBA.","sentences":["A popular method for modelling reactive systems is to use $\\omega$-regular languages.","These languages can be represented as nondeterministic B\\\"uchi automata (NBAs) or $\\omega$-regular expressions.","Existing methods synthesise expressions from state-based NBAs.","Synthesis from transition-based NBAs is traditionally done by transforming transition-based NBAs into state-based NBAs.","This transformation, however, can increase the complexity of the synthesised expressions.","This paper proposes a novel method for directly synthesising $\\omega$-regular expressions from transition-based NBAs.","We prove that the method is sound and complete.","Our empirical results show that the $\\omega$-regular expressions synthesised from transition-based NBAs are more compact than those synthesised from state-based NBAs.","This is particularly the case for NBAs computed from obligation, reactivity, safety and recurrence-type LTL formulas, reporting in the latter case an average reduction of over 50%.","We also show that our method successfully synthesises $\\omega$-regular expressions from more LTL formulas when using a transition-based instead of a state-based NBA."],"url":"http://arxiv.org/abs/2406.08136v1","category":"cs.FL"}
{"created":"2024-06-12 12:26:08","title":"Design, modeling, and characteristics of ringshaped robot actuated by functional fluid","abstract":"The controlled actuation of hydraulic and pneumatic actuators has unveiled fresh and thrilling opportunities for designing mobile robots with adaptable structures. Previously reported rolling robots, which were powered by fluidic systems, often relied on complex principles, cumbersome pump and valve systems, and intricate control strategies, limiting their applicability in other fields. In this investigation, we employed a distinct category of functional fluid identified as Electrohydrodynamic (EHD) fluid, serving as the pivotal element within the ring-shaped actuator. A short stream of functional fluid is placed within a fluidic channel and is then actuated by applying a direct current voltage aiming at shifting the center of mass of the robot and finally pushed the actuator to roll. We designed a ring-shaped fluidic robot, manufactured it using digital machining methods, and evaluated the robot's characteristics. Furthermore, we developed static and dynamic models to analyze the oscillation and rolling motion of the ring-shaped robots using the Lagrange method. This study is anticipated to contribute to the expansion of current research on EHD flexible actuators, enabling the realization of complex robotic systems.","sentences":["The controlled actuation of hydraulic and pneumatic actuators has unveiled fresh and thrilling opportunities for designing mobile robots with adaptable structures.","Previously reported rolling robots, which were powered by fluidic systems, often relied on complex principles, cumbersome pump and valve systems, and intricate control strategies, limiting their applicability in other fields.","In this investigation, we employed a distinct category of functional fluid identified as Electrohydrodynamic (EHD) fluid, serving as the pivotal element within the ring-shaped actuator.","A short stream of functional fluid is placed within a fluidic channel and is then actuated by applying a direct current voltage aiming at shifting the center of mass of the robot and finally pushed the actuator to roll.","We designed a ring-shaped fluidic robot, manufactured it using digital machining methods, and evaluated the robot's characteristics.","Furthermore, we developed static and dynamic models to analyze the oscillation and rolling motion of the ring-shaped robots using the Lagrange method.","This study is anticipated to contribute to the expansion of current research on EHD flexible actuators, enabling the realization of complex robotic systems."],"url":"http://arxiv.org/abs/2406.08135v1","category":"cs.RO"}
{"created":"2024-06-12 12:19:49","title":"dx2-y2-wave Bose Metal induced by the next-nearest-neighbor hopping t'","abstract":"Superconductivity arises when electrons form Cooper pairs with phase coherence. In contrast, a lack of phase coherence in Cooper pairs can lead to an uncondensed metallic ground state known as the Bose metal state. In this study, we investigate an attractively interacting fermionic system with nearest-neighbor (NN) hopping (t) and next-nearest-neighbor (NNN) hopping (t') anisotropy between two species of spins in a two-dimensional (2D) lattice. Utilizing the constrained path quantum Monte Carlo (CPQMC) method, we demonstrate the existence of a dx2-y2-wave Cooper pair Bose metal (CPBM) phase with t'/t > 0.7. The CPBM phase exhibits a dome-like structure in the phase diagram of filling n~0.65, with the maximal region around an optimal t'/t ~ 0.2, suggesting that an appropriate value of t' facilitates the formation of the Bose metal. Furthermore, we find that a Bose metal formed by fermions with a closed Fermi surface confirms that the crucial condition for this exotic phenomenon is primarily the anisotropy of the Fermi surface, rather than its topology. Our finding of the dx2-y2-wave CPBM demonstrates the same pairing symmetry as the pseudogap behavior in cuprates, and its experimental realization in ultracold atom systems is also feasible.","sentences":["Superconductivity arises when electrons form Cooper pairs with phase coherence.","In contrast, a lack of phase coherence in Cooper pairs can lead to an uncondensed metallic ground state known as the Bose metal state.","In this study, we investigate an attractively interacting fermionic system with nearest-neighbor (NN) hopping (t) and next-nearest-neighbor (NNN) hopping (t') anisotropy between two species of spins in a two-dimensional (2D) lattice.","Utilizing the constrained path quantum Monte Carlo (CPQMC) method, we demonstrate the existence of a dx2-y2-wave Cooper pair Bose metal (CPBM) phase with t'/t >","0.7.","The CPBM phase exhibits a dome-like structure in the phase diagram of filling n~0.65, with the maximal region around an optimal t'/t ~ 0.2, suggesting that an appropriate value of t' facilitates the formation of the Bose metal.","Furthermore, we find that a Bose metal formed by fermions with a closed Fermi surface confirms that the crucial condition for this exotic phenomenon is primarily the anisotropy of the Fermi surface, rather than its topology.","Our finding of the dx2-y2-wave CPBM demonstrates the same pairing symmetry as the pseudogap behavior in cuprates, and its experimental realization in ultracold atom systems is also feasible."],"url":"http://arxiv.org/abs/2406.08131v1","category":"cond-mat.str-el"}
{"created":"2024-06-12 12:12:38","title":"Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences","abstract":"To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.","sentences":["To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences.","Recent studies have shown the priorities by combining them as one.","However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data.","Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs.","To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner.","This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.","Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2406.08128v1","category":"cs.LG"}
{"created":"2024-06-12 11:56:13","title":"Low-Complexity Acoustic Scene Classification Using Parallel Attention-Convolution Network","abstract":"This work is an improved system that we submitted to task 1 of DCASE2023 challenge. We propose a method of low-complexity acoustic scene classification by a parallel attention-convolution network which consists of four modules, including pre-processing, fusion, global and local contextual information extraction. The proposed network is computationally efficient to capture global and local contextual information from each audio clip. In addition, we integrate other techniques into our method, such as knowledge distillation, data augmentation, and adaptive residual normalization. When evaluated on the official dataset of DCASE2023 challenge, our method obtains the highest accuracy of 56.10% with parameter number of 5.21 kilo and multiply-accumulate operations of 1.44 million. It exceeds the top two systems of DCASE2023 challenge in accuracy and complexity, and obtains state-of-the-art result. Code is at: https://github.com/Jessytan/Low-complexity-ASC.","sentences":["This work is an improved system that we submitted to task 1 of DCASE2023 challenge.","We propose a method of low-complexity acoustic scene classification by a parallel attention-convolution network which consists of four modules, including pre-processing, fusion, global and local contextual information extraction.","The proposed network is computationally efficient to capture global and local contextual information from each audio clip.","In addition, we integrate other techniques into our method, such as knowledge distillation, data augmentation, and adaptive residual normalization.","When evaluated on the official dataset of DCASE2023 challenge, our method obtains the highest accuracy of 56.10% with parameter number of 5.21 kilo and multiply-accumulate operations of 1.44 million.","It exceeds the top two systems of DCASE2023 challenge in accuracy and complexity, and obtains state-of-the-art result.","Code is at: https://github.com/Jessytan/Low-complexity-ASC."],"url":"http://arxiv.org/abs/2406.08119v1","category":"eess.AS"}
{"created":"2024-06-12 11:50:51","title":"Valeo4Cast: A Modular Approach to End-to-End Forecasting","abstract":"Motion forecasting is crucial in autonomous driving systems to anticipate the future trajectories of surrounding agents such as pedestrians, vehicles, and traffic signals. In end-to-end forecasting, the model must jointly detect from sensor data (cameras or LiDARs) the position and past trajectories of the different elements of the scene and predict their future location. We depart from the current trend of tackling this task via end-to-end training from perception to forecasting and we use a modular approach instead. Following a recent study, we individually build and train detection, tracking, and forecasting modules. We then only use consecutive finetuning steps to integrate the modules better and alleviate compounding errors. Our study reveals that this simple yet effective approach significantly improves performance on the end-to-end forecasting benchmark. Consequently, our solution ranks first in the Argoverse 2 end-to-end Forecasting Challenge held at CVPR 2024 Workshop on Autonomous Driving (WAD), with 63.82 mAPf. We surpass forecasting results by +17.1 points over last year's winner and by +13.3 points over this year's runner-up. This remarkable performance in forecasting can be explained by our modular paradigm, which integrates finetuning strategies and significantly outperforms the end-to-end-trained counterparts.","sentences":["Motion forecasting is crucial in autonomous driving systems to anticipate the future trajectories of surrounding agents such as pedestrians, vehicles, and traffic signals.","In end-to-end forecasting, the model must jointly detect from sensor data (cameras or LiDARs) the position and past trajectories of the different elements of the scene and predict their future location.","We depart from the current trend of tackling this task via end-to-end training from perception to forecasting and we use a modular approach instead.","Following a recent study, we individually build and train detection, tracking, and forecasting modules.","We then only use consecutive finetuning steps to integrate the modules better and alleviate compounding errors.","Our study reveals that this simple yet effective approach significantly improves performance on the end-to-end forecasting benchmark.","Consequently, our solution ranks first in the Argoverse 2 end-to-end Forecasting Challenge held at CVPR 2024 Workshop on Autonomous Driving (WAD), with 63.82 mAPf.","We surpass forecasting results by +17.1 points over last year's winner and by +13.3 points over this year's runner-up.","This remarkable performance in forecasting can be explained by our modular paradigm, which integrates finetuning strategies and significantly outperforms the end-to-end-trained counterparts."],"url":"http://arxiv.org/abs/2406.08113v1","category":"cs.CV"}
{"created":"2024-06-12 11:40:14","title":"Simulation of the Dissipative Dynamics of Strongly Interacting NV Centers with Tensor Networks","abstract":"NV centers in diamond are a promising platform for highly sensitive quantum sensors for magnetic fields and other physical quantities. The quest for high sensitivity combined with high spatial resolution leads naturally to dense ensembles of NV centers, and hence to strong, long-range interactions between them. Hence, simulating strongly interacting NVs becomes essential. However, obtaining the exact dynamics for a many-spin system is a challenging task due to the exponential scaling of the Hilbert space dimension, a problem that is exacerbated when the system is modelled as an open quantum system. In this work, we employ the Matrix Product Density Operator (MPDO) method to represent the many-body mixed state and to simulate the dynamics of an ensemble of NVs in the presence of strong long-range couplings due to dipole-dipole forces. We benchmark different time-evolution algorithms in terms of numerical accuracy and stability. Subsequently, we simulate the dynamics in the strong interaction regime, with and without dissipation.","sentences":["NV centers in diamond are a promising platform for highly sensitive quantum sensors for magnetic fields and other physical quantities.","The quest for high sensitivity combined with high spatial resolution leads naturally to dense ensembles of NV centers, and hence to strong, long-range interactions between them.","Hence, simulating strongly interacting NVs becomes essential.","However, obtaining the exact dynamics for a many-spin system is a challenging task due to the exponential scaling of the Hilbert space dimension, a problem that is exacerbated when the system is modelled as an open quantum system.","In this work, we employ the Matrix Product Density Operator (MPDO) method to represent the many-body mixed state and to simulate the dynamics of an ensemble of NVs in the presence of strong long-range couplings due to dipole-dipole forces.","We benchmark different time-evolution algorithms in terms of numerical accuracy and stability.","Subsequently, we simulate the dynamics in the strong interaction regime, with and without dissipation."],"url":"http://arxiv.org/abs/2406.08108v1","category":"quant-ph"}
{"created":"2024-06-12 11:38:13","title":"Counterfactual-based Root Cause Analysis for Dynamical Systems","abstract":"Identifying the underlying reason for a failing dynamic process or otherwise anomalous observation is a fundamental challenge, yet has numerous industrial applications. Identifying the failure-causing sub-system using causal inference, one can ask the question: \"Would the observed failure also occur, if we had replaced the behaviour of a sub-system at a certain point in time with its normal behaviour?\" To this end, a formal description of behaviour of the full system is needed in which such counterfactual questions can be answered. However, existing causal methods for root cause identification are typically limited to static settings and focusing on additive external influences causing failures rather than structural influences. In this paper, we address these problems by modelling the dynamic causal system using a Residual Neural Network and deriving corresponding counterfactual distributions over trajectories. We show quantitatively that more root causes are identified when an intervention is performed on the structural equation and the external influence, compared to an intervention on the external influence only. By employing an efficient approximation to a corresponding Shapley value, we also obtain a ranking between the different subsystems at different points in time being responsible for an observed failure, which is applicable in settings with large number of variables. We illustrate the effectiveness of the proposed method on a benchmark dynamic system as well as on a real world river dataset.","sentences":["Identifying the underlying reason for a failing dynamic process or otherwise anomalous observation is a fundamental challenge, yet has numerous industrial applications.","Identifying the failure-causing sub-system using causal inference, one can ask the question: \"Would the observed failure also occur, if we had replaced the behaviour of a sub-system at a certain point in time with its normal behaviour?\"","To this end, a formal description of behaviour of the full system is needed in which such counterfactual questions can be answered.","However, existing causal methods for root cause identification are typically limited to static settings and focusing on additive external influences causing failures rather than structural influences.","In this paper, we address these problems by modelling the dynamic causal system using a Residual Neural Network and deriving corresponding counterfactual distributions over trajectories.","We show quantitatively that more root causes are identified when an intervention is performed on the structural equation and the external influence, compared to an intervention on the external influence only.","By employing an efficient approximation to a corresponding Shapley value, we also obtain a ranking between the different subsystems at different points in time being responsible for an observed failure, which is applicable in settings with large number of variables.","We illustrate the effectiveness of the proposed method on a benchmark dynamic system as well as on a real world river dataset."],"url":"http://arxiv.org/abs/2406.08106v1","category":"cs.LG"}
{"created":"2024-06-12 11:32:52","title":"Resource Leveling: Complexity of a UET two-processor scheduling variant and related problems","abstract":"This paper mainly focuses on a resource leveling variant of a two-processor scheduling problem. The latter problem is to schedule a set of dependent UET jobs on two identical processors with minimum makespan. It is known to be polynomial-time solvable.   In the variant we consider, the resource constraint on processors is relaxed and the objective is no longer to minimize makespan. Instead, a deadline is imposed on the makespan and the objective is to minimize the total resource use exceeding a threshold resource level of two. This resource leveling criterion is known as the total overload cost. Sophisticated matching arguments allow us to provide a polynomial algorithm computing the optimal solution as a function of the makespan deadline. It extends a solving method from the literature for the two-processor scheduling problem.   Moreover, the complexity of related resource leveling problems sharing the same objective is studied. These results lead to polynomial or pseudo-polynomial algorithms or NP-hardness proofs, allowing for an interesting comparison with classical machine scheduling problems.","sentences":["This paper mainly focuses on a resource leveling variant of a two-processor scheduling problem.","The latter problem is to schedule a set of dependent UET jobs on two identical processors with minimum makespan.","It is known to be polynomial-time solvable.   ","In the variant we consider, the resource constraint on processors is relaxed and the objective is no longer to minimize makespan.","Instead, a deadline is imposed on the makespan and the objective is to minimize the total resource use exceeding a threshold resource level of two.","This resource leveling criterion is known as the total overload cost.","Sophisticated matching arguments allow us to provide a polynomial algorithm computing the optimal solution as a function of the makespan deadline.","It extends a solving method from the literature for the two-processor scheduling problem.   ","Moreover, the complexity of related resource leveling problems sharing the same objective is studied.","These results lead to polynomial or pseudo-polynomial algorithms or NP-hardness proofs, allowing for an interesting comparison with classical machine scheduling problems."],"url":"http://arxiv.org/abs/2406.08104v1","category":"cs.CC"}
{"created":"2024-06-12 11:08:57","title":"Classical simulability of constant-depth linear-optical circuits with noise","abstract":"Noise is one of the main obstacles to realizing quantum devices that achieve a quantum computational advantage. A possible approach to minimize the noise effect is to employ shallow-depth quantum circuits since noise typically accumulates as circuit depth grows. In this work, we investigate the complexity of shallow-depth linear-optical circuits under the effects of photon loss and partial distinguishability. By establishing a correspondence between a linear-optical circuit and a bipartite graph, we show that the effects of photon loss and partial distinguishability are equivalent to removing the corresponding vertices. Using this correspondence and percolation theory, we prove that for constant-depth linear-optical circuits with single photons, there is a threshold of loss (noise) rate above which the linear-optical systems can be decomposed into smaller systems with high probability, which enables us to simulate the systems efficiently. Consequently, our result implies that even in shallow-depth circuits where noise is not accumulated enough, its effect may be sufficiently significant to make them efficiently simulable using classical algorithms due to its entanglement structure constituted by shallow-depth circuits.","sentences":["Noise is one of the main obstacles to realizing quantum devices that achieve a quantum computational advantage.","A possible approach to minimize the noise effect is to employ shallow-depth quantum circuits since noise typically accumulates as circuit depth grows.","In this work, we investigate the complexity of shallow-depth linear-optical circuits under the effects of photon loss and partial distinguishability.","By establishing a correspondence between a linear-optical circuit and a bipartite graph, we show that the effects of photon loss and partial distinguishability are equivalent to removing the corresponding vertices.","Using this correspondence and percolation theory, we prove that for constant-depth linear-optical circuits with single photons, there is a threshold of loss (noise) rate above which the linear-optical systems can be decomposed into smaller systems with high probability, which enables us to simulate the systems efficiently.","Consequently, our result implies that even in shallow-depth circuits where noise is not accumulated enough, its effect may be sufficiently significant to make them efficiently simulable using classical algorithms due to its entanglement structure constituted by shallow-depth circuits."],"url":"http://arxiv.org/abs/2406.08086v1","category":"quant-ph"}
{"created":"2024-06-12 11:06:23","title":"Bridging Simulation and Measurements through Ray-Launching Analysis: A Study in a Complex Urban Scenario Environment","abstract":"With the rapid increase in mobile subscribers, there is a drive towards achieving higher data rates, prompting the use of higher frequencies in future wireless communication technologies. Wave propagation channel modeling for these frequencies must be considered in conjunction with measurement results. This paper presents a ray-launching (RL)-based simulation in a complex urban scenario characterized by an undulating terrain with a high density of trees. The simulation results tend to closely match the reported measurements when more details are considered. This underscores the benefits of using the RL method, which provides detailed space-time and angle-delay results.","sentences":["With the rapid increase in mobile subscribers, there is a drive towards achieving higher data rates, prompting the use of higher frequencies in future wireless communication technologies.","Wave propagation channel modeling for these frequencies must be considered in conjunction with measurement results.","This paper presents a ray-launching (RL)-based simulation in a complex urban scenario characterized by an undulating terrain with a high density of trees.","The simulation results tend to closely match the reported measurements when more details are considered.","This underscores the benefits of using the RL method, which provides detailed space-time and angle-delay results."],"url":"http://arxiv.org/abs/2406.08082v1","category":"eess.SP"}
{"created":"2024-06-12 10:55:47","title":"Uses of Active and Passive Learning in Stateful Fuzzing","abstract":"This paper explores the use of active and passive learning, i.e.\\ active and passive techniques to infer state machine models of systems, for fuzzing. Fuzzing has become a very popular and successful technique to improve the robustness of software over the past decade, but stateful systems are still difficult to fuzz. Passive and active techniques can help in a variety of ways: to compare and benchmark different fuzzers, to discover differences between various implementations of the same protocol, and to improve fuzzers.","sentences":["This paper explores the use of active and passive learning, i.e.\\ active and passive techniques to infer state machine models of systems, for fuzzing.","Fuzzing has become a very popular and successful technique to improve the robustness of software over the past decade, but stateful systems are still difficult to fuzz.","Passive and active techniques can help in a variety of ways: to compare and benchmark different fuzzers, to discover differences between various implementations of the same protocol, and to improve fuzzers."],"url":"http://arxiv.org/abs/2406.08077v1","category":"cs.SE"}
{"created":"2024-06-12 10:51:29","title":"VECL-TTS: Voice identity and Emotional style controllable Cross-Lingual Text-to-Speech","abstract":"Despite the significant advancements in Text-to-Speech (TTS) systems, their full utilization in automatic dubbing remains limited. This task necessitates the extraction of voice identity and emotional style from a reference speech in a source language and subsequently transferring them to a target language using cross-lingual TTS techniques. While previous approaches have mainly concentrated on controlling voice identity within the cross-lingual TTS framework, there has been limited work on incorporating emotion and voice identity together. To this end, we introduce an end-to-end Voice Identity and Emotional Style Controllable Cross-Lingual (VECL) TTS system using multilingual speakers and an emotion embedding network. Moreover, we introduce content and style consistency losses to enhance the quality of synthesized speech further. The proposed system achieved an average relative improvement of 8.83\\% compared to the state-of-the-art (SOTA) methods on a database comprising English and three Indian languages (Hindi, Telugu, and Marathi).","sentences":["Despite the significant advancements in Text-to-Speech (TTS) systems, their full utilization in automatic dubbing remains limited.","This task necessitates the extraction of voice identity and emotional style from a reference speech in a source language and subsequently transferring them to a target language using cross-lingual TTS techniques.","While previous approaches have mainly concentrated on controlling voice identity within the cross-lingual TTS framework, there has been limited work on incorporating emotion and voice identity together.","To this end, we introduce an end-to-end Voice Identity and Emotional Style Controllable Cross-Lingual (VECL) TTS system using multilingual speakers and an emotion embedding network.","Moreover, we introduce content and style consistency losses to enhance the quality of synthesized speech further.","The proposed system achieved an average relative improvement of 8.83\\% compared to the state-of-the-art (SOTA) methods on a database comprising English and three Indian languages (Hindi, Telugu, and Marathi)."],"url":"http://arxiv.org/abs/2406.08076v1","category":"eess.AS"}
{"created":"2024-06-12 10:45:20","title":"LQR control for a system describing the interaction between a floating solid and the surrounding fluid","abstract":"This paper studies an infinite time horizon LQR optimal control problem for a system describing, within a linear approximation, the vertical oscillations of a floating solid, coupled to the motion of the free boundary fluid on which it floats. The fluid flow is described by a viscous version of the linearized Saint-Venant equations (shallow water regime). The major difficulty we are facing is that the domain occupied by the fluid is unbounded so that the system is not exponentially stable. This fact firstly raises challenges in proving the wellposedness, requiring the combined use of analytic semigroup theory and of an interpolation technique. The main contribution of the paper is that we show that, in spite of the lack of exponential stabilizability, we can define a wellposed LQR problem for which a Riccati based approach to design feedback controls can be implemented.","sentences":["This paper studies an infinite time horizon LQR optimal control problem for a system describing, within a linear approximation, the vertical oscillations of a floating solid, coupled to the motion of the free boundary fluid on which it floats.","The fluid flow is described by a viscous version of the linearized Saint-Venant equations (shallow water regime).","The major difficulty we are facing is that the domain occupied by the fluid is unbounded so that the system is not exponentially stable.","This fact firstly raises challenges in proving the wellposedness, requiring the combined use of analytic semigroup theory and of an interpolation technique.","The main contribution of the paper is that we show that, in spite of the lack of exponential stabilizability, we can define a wellposed LQR problem for which a Riccati based approach to design feedback controls can be implemented."],"url":"http://arxiv.org/abs/2406.08072v1","category":"math.OC"}
{"created":"2024-06-12 10:29:26","title":"Gauge-invariant renormalization of four-quark operators in lattice QCD","abstract":"We study the renormalization of four-quark operators in one-loop perturbation theory. We employ a coordinate-space Gauge-Invariant Renormalization Scheme (GIRS), which can be advantageous compared to other schemes, especially in nonperturbative lattice investigations. From our perturbative calculations, we extract the conversion factors between GIRS and the modified Minimal Subtraction scheme ($\\overline{\\rm MS}$) at the next-to-leading order. A formidable issue in the study of the four-quark operators is that operators with different Dirac matrices mix among themselves upon renormalization. We focus on both parity-conserving and parity-violating four-quark operators, which change flavor numbers by two units ($\\Delta F = 2$). The extraction of the conversion factors entails the calculation of two-point Green's functions involving products of two four-quark operators, as well as three-point Green's functions with one four-quark and two bilinear operators. The significance of our results lies in their potential to refine our understanding of QCD phenomena, offering insights into the precision of Cabibbo-Kobayashi-Maskawa (CKM) matrix elements and shedding light on the nonperturbative treatment of complex mixing patterns associated with four-quark operators.","sentences":["We study the renormalization of four-quark operators in one-loop perturbation theory.","We employ a coordinate-space Gauge-Invariant Renormalization Scheme (GIRS), which can be advantageous compared to other schemes, especially in nonperturbative lattice investigations.","From our perturbative calculations, we extract the conversion factors between GIRS and the modified Minimal Subtraction scheme ($\\overline{\\rm MS}$) at the next-to-leading order.","A formidable issue in the study of the four-quark operators is that operators with different Dirac matrices mix among themselves upon renormalization.","We focus on both parity-conserving and parity-violating four-quark operators, which change flavor numbers by two units ($\\Delta F = 2$).","The extraction of the conversion factors entails the calculation of two-point Green's functions involving products of two four-quark operators, as well as three-point Green's functions with one four-quark and two bilinear operators.","The significance of our results lies in their potential to refine our understanding of QCD phenomena, offering insights into the precision of Cabibbo-Kobayashi-Maskawa (CKM) matrix elements and shedding light on the nonperturbative treatment of complex mixing patterns associated with four-quark operators."],"url":"http://arxiv.org/abs/2406.08065v1","category":"hep-lat"}
{"created":"2024-06-12 10:20:47","title":"Iterative method for real-time Hybrid testing: application to a cantilever beam with two interface degrees of freedom","abstract":"In this paper, an iterative method for real-time hybrid testing (RTHT) is proposed. The method seeks to iteratively balance the interface conditions between the physical and numerical substructures by controlling the periodic demand of the actuators. It is then suitable for RTHT of structures undergoing a periodic response, e.g. structures excited at resonance. We demonstrate the capabilities of the method on a cantilever beam in bending motion with two degrees of freedom at the interface, which we use as a prototype for future testing of aircraft wings. We show that a number of challenges arise in these settings, such as the difficulty in measuring interface forces while controlling a continuous structure and the instability of the hybrid test for small time delays. Classical RTHT strategies could produce inaccurate or unstable outcomes, whereas the proposed method is able to attain very good interface synchronisation in a wide range of tested scenarios.","sentences":["In this paper, an iterative method for real-time hybrid testing (RTHT) is proposed.","The method seeks to iteratively balance the interface conditions between the physical and numerical substructures by controlling the periodic demand of the actuators.","It is then suitable for RTHT of structures undergoing a periodic response, e.g. structures excited at resonance.","We demonstrate the capabilities of the method on a cantilever beam in bending motion with two degrees of freedom at the interface, which we use as a prototype for future testing of aircraft wings.","We show that a number of challenges arise in these settings, such as the difficulty in measuring interface forces while controlling a continuous structure and the instability of the hybrid test for small time delays.","Classical RTHT strategies could produce inaccurate or unstable outcomes, whereas the proposed method is able to attain very good interface synchronisation in a wide range of tested scenarios."],"url":"http://arxiv.org/abs/2406.08060v1","category":"eess.SY"}
{"created":"2024-06-12 10:17:10","title":"Bright electrically contacted circular Bragg grating resonators with deterministically integrated quantum dots","abstract":"Cavity-enhanced emission of electrically controlled semiconductor quantum dots is essential in developing bright quantum devices for real-world quantum photonic applications. Combining the circular Bragg grating (CBG) approach with a PIN-diode structure, we propose and implement an innovative concept for ridge-based electrically-contacted CBG resonators. Through fine-tuning of device parameters in numerical simulations and deterministic nanoprocessing, we produced electrically controlled single quantum dot CBG resonators with excellent electro-optical emission properties. These include multiple wavelength-tunable emission lines and a photon extraction efficiency (PEE) of up to (30.4$\\pm$3.4)%, where refined numerical optimization based on experimental findings suggests a substantial improvement, promising PEE >50%. Additionally, the developed quantum light sources yield single-photon purity reaching (98.8$\\pm$0.2)% [post-selected: (99.5$\\pm$0.3)%] and a photon indistinguishability of (25.8$\\pm$2.1)% [post-selected: (92.8$\\pm$4.8)%]. Our results pave the way for high-performance quantum devices with combined cavity enhancement and deterministic charge-environment controls, advancing the development of photonic quantum information systems such as complex quantum repeater networks.","sentences":["Cavity-enhanced emission of electrically controlled semiconductor quantum dots is essential in developing bright quantum devices for real-world quantum photonic applications.","Combining the circular Bragg grating (CBG) approach with a PIN-diode structure, we propose and implement an innovative concept for ridge-based electrically-contacted CBG resonators.","Through fine-tuning of device parameters in numerical simulations and deterministic nanoprocessing, we produced electrically controlled single quantum dot CBG resonators with excellent electro-optical emission properties.","These include multiple wavelength-tunable emission lines and a photon extraction efficiency (PEE) of up to (30.4$\\pm$3.4)%, where refined numerical optimization based on experimental findings suggests a substantial improvement, promising PEE >50%.","Additionally, the developed quantum light sources yield single-photon purity reaching (98.8$\\pm$0.2)% [post-selected: (99.5$\\pm$0.3)%] and a photon indistinguishability of (25.8$\\pm$2.1)% [post-selected: (92.8$\\pm$4.8)%].","Our results pave the way for high-performance quantum devices with combined cavity enhancement and deterministic charge-environment controls, advancing the development of photonic quantum information systems such as complex quantum repeater networks."],"url":"http://arxiv.org/abs/2406.08057v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-12 10:12:38","title":"Quantum harvester enables energy transfer without randomness transfer or dissipation","abstract":"We consider a foundational question in energy harvesting: given a partly random energy source, is it possible to extract the energy without also transferring randomness or accepting another thermodynamical cost? We answer this in the positive, describing scenarios and protocols where in principle energy is extracted from a field with randomness but without any randomness being transferred, and without energy dissipation. Such protocols fundamentally outperform existing methods of rectification which dissipate power, or feedback demon-like protocols which transfer randomness to the feedback system. The protocols exploit the possibility of the harvesting system taking several trajectories that lead to the same final state at a given time. We explain why these protocols do not violate basic physical principles. A key example involves the experimentally well-established phenomenon of Rabi oscillations between energy levels, exploiting the multitude of rotation axes in the state space that take the lower energy state to the excited state. The quantum system is deterministically excited to the highest energy level after interacting with the source for a fixed amount of time, irrespective of the random initial phase of the external potential.","sentences":["We consider a foundational question in energy harvesting: given a partly random energy source, is it possible to extract the energy without also transferring randomness or accepting another thermodynamical cost?","We answer this in the positive, describing scenarios and protocols where in principle energy is extracted from a field with randomness but without any randomness being transferred, and without energy dissipation.","Such protocols fundamentally outperform existing methods of rectification which dissipate power, or feedback demon-like protocols which transfer randomness to the feedback system.","The protocols exploit the possibility of the harvesting system taking several trajectories that lead to the same final state at a given time.","We explain why these protocols do not violate basic physical principles.","A key example involves the experimentally well-established phenomenon of Rabi oscillations between energy levels, exploiting the multitude of rotation axes in the state space that take the lower energy state to the excited state.","The quantum system is deterministically excited to the highest energy level after interacting with the source for a fixed amount of time, irrespective of the random initial phase of the external potential."],"url":"http://arxiv.org/abs/2406.08054v1","category":"quant-ph"}
{"created":"2024-06-12 10:05:15","title":"ONNXim: A Fast, Cycle-level Multi-core NPU Simulator","abstract":"As DNNs are widely adopted in various application domains while demanding increasingly higher compute and memory requirements, designing efficient and performant NPUs (Neural Processing Units) is becoming more important. However, existing architectural NPU simulators lack support for high-speed simulation, multi-core modeling, multi-tenant scenarios, detailed DRAM/NoC modeling, and/or different deep learning frameworks. To address these limitations, this work proposes ONNXim, a fast cycle-level simulator for multi-core NPUs in DNN serving systems. It takes DNN models represented in the ONNX graph format generated from various deep learning frameworks for ease of simulation. In addition, based on the observation that typical NPU cores process tensor tiles from on-chip scratchpad memory with deterministic compute latency, we forgo a detailed modeling for the computation while still preserving simulation accuracy. ONNXim also preserves dependencies between compute and tile DMAs. Meanwhile, the DRAM and NoC are modeled in cycle-level to properly model contention among multiple cores that can execute different DNN models for multi-tenancy. Consequently, ONNXim is significantly faster than existing simulators (e.g., by up to 384x over Accel-sim) and enables various case studies, such as multi-tenant NPUs, that were previously impractical due to slow speed and/or lack of functionalities. ONNXim is publicly available at https://github.com/PSAL-POSTECH/ONNXim.","sentences":["As DNNs are widely adopted in various application domains while demanding increasingly higher compute and memory requirements, designing efficient and performant NPUs (Neural Processing Units) is becoming more important.","However, existing architectural NPU simulators lack support for high-speed simulation, multi-core modeling, multi-tenant scenarios, detailed DRAM/NoC modeling, and/or different deep learning frameworks.","To address these limitations, this work proposes ONNXim, a fast cycle-level simulator for multi-core NPUs in DNN serving systems.","It takes DNN models represented in the ONNX graph format generated from various deep learning frameworks for ease of simulation.","In addition, based on the observation that typical NPU cores process tensor tiles from on-chip scratchpad memory with deterministic compute latency, we forgo a detailed modeling for the computation while still preserving simulation accuracy.","ONNXim also preserves dependencies between compute and tile DMAs.","Meanwhile, the DRAM and NoC are modeled in cycle-level to properly model contention among multiple cores that can execute different DNN models for multi-tenancy.","Consequently, ONNXim is significantly faster than existing simulators (e.g., by up to 384x over Accel-sim) and enables various case studies, such as multi-tenant NPUs, that were previously impractical due to slow speed and/or lack of functionalities.","ONNXim is publicly available at https://github.com/PSAL-POSTECH/ONNXim."],"url":"http://arxiv.org/abs/2406.08051v1","category":"cs.AR"}
{"created":"2024-06-12 09:53:09","title":"Hofstadter spectrum in a semiconductor moir\u00e9 lattice","abstract":"Recently, the Hofstadter spectrum of a twisted $\\mathrm{WSe_2/MoSe_2}$ heterobilayer has been observed in experiment [C. R. Kometter, et al. Nat.Phys.19, 1861 (2023)], but the origin of Hofstadter states remains unclear. Here, we present a comprehensive theoretical interpretation of the observed Hofstadter states by calculating its accurate Hofstadter spectrum. We point out that the valley Zeeman effect, a unique feature of the transition metal dichalcogenide (TMD) materials, plays a crucial role in determining the shape of the Hofstadter spectrum, due to the narrow bandwidth of the moir\\'e bands. This is distinct from the graphene-based moir\\'e systems. We further predict that the Hofstadter spectrum of the moir\\'e flat band, which was not observed in experiment, can be observed in the same system with a larger twist angle $2^\\circ\\lesssim\\theta \\lesssim 3^\\circ$. Our theory paves the way for further studies of the interplay between the Hofstadter states and correlated insulting states in such moir\\'e lattice systems.","sentences":["Recently, the Hofstadter spectrum of a twisted $\\mathrm{WSe_2/MoSe_2}$ heterobilayer has been observed in experiment [C. R. Kometter, et al.","Nat.Phys.19, 1861 (2023)], but the origin of Hofstadter states remains unclear.","Here, we present a comprehensive theoretical interpretation of the observed Hofstadter states by calculating its accurate Hofstadter spectrum.","We point out that the valley Zeeman effect, a unique feature of the transition metal dichalcogenide (TMD) materials, plays a crucial role in determining the shape of the Hofstadter spectrum, due to the narrow bandwidth of the moir\\'e bands.","This is distinct from the graphene-based moir\\'e systems.","We further predict that the Hofstadter spectrum of the moir\\'e flat band, which was not observed in experiment, can be observed in the same system with a larger twist angle $2^\\circ\\lesssim\\theta \\lesssim 3^\\circ$. Our theory paves the way for further studies of the interplay between the Hofstadter states and correlated insulting states in such moir\\'e lattice systems."],"url":"http://arxiv.org/abs/2406.08044v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-12 09:48:24","title":"Emergence of non-uniform strain induced exciton species in homo- and heterobilayer transition metal dichalcogenides","abstract":"Full control of excitons in 2D materials is an important step to exploit them for applications. Straintronics is one method that can be used to effectively control the movement of excitons. Unfortunately, the effects of non-uniform strain in 2D materials are not yet well understood theoretically, although these strain fields can be present in experiments in the form of wrinkles, bubbles, and folds, or even explicitly applied to 2D materials through pre-patterned surfaces. The effects of these non-uniform strain fields on multilayers are even less studied due to the sheer size of these systems. In the present investigation, we study wrinkles that form in homo- and heterobilayers of 2D transition metal dichalcogenides using density functional theory. We show that the non-uniform strain leads to the formation of interlayer excitons in homobilayers of $ \\mathrm WSe_2 $ and to exciton localization in heterobilayers of $ \\mathrm WSe_2$-$ \\mathrm MoSe_2$. Our results also reveal that the spin angular momentum is changed due to the mixing of in- and out-of-plane states which can explain the brightening of the formerly dark excitonic states under strain. Our results will pave the way towards a full understanding of the strain-control of excitons in 2D materials.","sentences":["Full control of excitons in 2D materials is an important step to exploit them for applications.","Straintronics is one method that can be used to effectively control the movement of excitons.","Unfortunately, the effects of non-uniform strain in 2D materials are not yet well understood theoretically, although these strain fields can be present in experiments in the form of wrinkles, bubbles, and folds, or even explicitly applied to 2D materials through pre-patterned surfaces.","The effects of these non-uniform strain fields on multilayers are even less studied due to the sheer size of these systems.","In the present investigation, we study wrinkles that form in homo- and heterobilayers of 2D transition metal dichalcogenides using density functional theory.","We show that the non-uniform strain leads to the formation of interlayer excitons in homobilayers of $ \\mathrm WSe_2 $ and to exciton localization in heterobilayers of $ \\mathrm WSe_2$-$ \\mathrm MoSe_2$.","Our results also reveal that the spin angular momentum is changed due to the mixing of in- and out-of-plane states which can explain the brightening of the formerly dark excitonic states under strain.","Our results will pave the way towards a full understanding of the strain-control of excitons in 2D materials."],"url":"http://arxiv.org/abs/2406.08040v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-12 09:39:32","title":"Interference Analysis for Coexistence of UAVs and Civil Aircrafts Based on Automatic Dependent Surveillance-Broadcast","abstract":"Due to the advantages of high mobility and easy deployment, unmanned aerial vehicles (UAVs) are widely applied in both military and civilian fields. In order to strengthen the flight surveillance of UAVs and guarantee the airspace safety, UAVs can be equipped with the automatic dependent surveillance-broadcast (ADS-B) system, which periodically sends flight information to other aircrafts and ground stations (GSs). However, due to the limited resource of channel capacity, UAVs equipped with ADS-B results in the interference between UAVs and civil aircrafts (CAs), which further impacts the accuracy of received information at GSs. In detail, the channel capacity is mainly affected by the density of aircrafts and the transmitting power of ADS-B. Hence, based on the three-dimensional poisson point process, this work leverages the stochastic geometry theory to build a model of the coexistence of UAVs and CAs and analyze the interference performance of ADS-B monitoring system. From simulation results, we reveal the effects of transmitting power, density, threshold and pathloss on the performance of the ADS-B monitoring system. Besides, we provide the suggested transmitting power and density for the safe coexistence of UAVs and CAs.","sentences":["Due to the advantages of high mobility and easy deployment, unmanned aerial vehicles (UAVs) are widely applied in both military and civilian fields.","In order to strengthen the flight surveillance of UAVs and guarantee the airspace safety, UAVs can be equipped with the automatic dependent surveillance-broadcast (ADS-B) system, which periodically sends flight information to other aircrafts and ground stations (GSs).","However, due to the limited resource of channel capacity, UAVs equipped with ADS-B results in the interference between UAVs and civil aircrafts (CAs), which further impacts the accuracy of received information at GSs.","In detail, the channel capacity is mainly affected by the density of aircrafts and the transmitting power of ADS-B. Hence, based on the three-dimensional poisson point process, this work leverages the stochastic geometry theory to build a model of the coexistence of UAVs and CAs and analyze the interference performance of ADS-B monitoring system.","From simulation results, we reveal the effects of transmitting power, density, threshold and pathloss on the performance of the ADS-B monitoring system.","Besides, we provide the suggested transmitting power and density for the safe coexistence of UAVs and CAs."],"url":"http://arxiv.org/abs/2406.08038v1","category":"eess.SP"}
{"created":"2024-06-12 09:37:15","title":"A Census of Sun's Ancestors and their Contributions to the Solar System Chemical Composition","abstract":"In this work we compute the rates and numbers of different types of stars and phenomena (SNe, novae, white dwarfs, merging neutron stars, black holes) that contributed to the chemical composition of the Solar System. Stars die and restore the newly formed elements into the interstellar gas. This process is called \"chemical evolution\". In particular, we analyse the death rates of stars of all masses, dying either quiescently or explosively. These rates and total star numbers are computed in the context of a revised version of the two-infall model for the chemical evolution of the Milky Way, which reproduces fairly well the observed abundance patterns of several chemical species, as well as the global solar metallicity. We compute also the total number of stars ever born and still alive as well as the number of stars born up to the formation of the Solar System and with a mass and metallicity like the Sun. This latter number will account for all the possible existing Solar Systems which can host life in the solar vicinity. Among all the stars (from 0.8 to 100 M$_{\\odot}$) born and died from the beginning up to the Solar System formation epoch, which contributed to its chemical composition, 93.00$\\%$ are represented by stars dying as single white dwarfs (without interacting significantly with a companion star) and originating in the mass range 0.8-8 M$_{\\odot}$, while 5.24$\\%$ are neutron stars and 0.73$\\%$ are black holes, both originating from SNe core-collapse (M>8 M$_{\\odot}$); 0.64$\\%$ are Type Ia SNe and 0.40$\\%$ are nova systems, both originating from the same mass range as the white dwarfs. The number of stars similar to the Sun born from the beginning up to the Solar System formation, with metallicity in the range 12+log(Fe/H)= 7.50 $\\pm$ 0.04 dex is 3.1732$\\cdot$ 10$^{7}$, and in particular our Sun is the 2.6092$\\cdot$ 10$^7$-th star of this kind, born in the solar vicinity.","sentences":["In this work we compute the rates and numbers of different types of stars and phenomena (SNe, novae, white dwarfs, merging neutron stars, black holes) that contributed to the chemical composition of the Solar System.","Stars die and restore the newly formed elements into the interstellar gas.","This process is called \"chemical evolution\".","In particular, we analyse the death rates of stars of all masses, dying either quiescently or explosively.","These rates and total star numbers are computed in the context of a revised version of the two-infall model for the chemical evolution of the Milky Way, which reproduces fairly well the observed abundance patterns of several chemical species, as well as the global solar metallicity.","We compute also the total number of stars ever born and still alive as well as the number of stars born up to the formation of the Solar System and with a mass and metallicity like the Sun.","This latter number will account for all the possible existing Solar Systems which can host life in the solar vicinity.","Among all the stars (from 0.8 to 100 M$_{\\odot}$) born and died from the beginning up to the Solar System formation epoch, which contributed to its chemical composition, 93.00$\\%$ are represented by stars dying as single white dwarfs (without interacting significantly with a companion star) and originating in the mass range 0.8-8 M$_{\\odot}$, while 5.24$\\%$ are neutron stars and 0.73$\\%$ are black holes, both originating from SNe core-collapse (M>8 M$_{\\odot}$); 0.64$\\%$ are Type Ia SNe and 0.40$\\%$ are nova systems, both originating from the same mass range as the white dwarfs.","The number of stars similar to the Sun born from the beginning up to the Solar System formation, with metallicity in the range 12+log(Fe/H)= 7.50 $\\pm$ 0.04 dex is 3.1732$\\cdot$ 10$^{7}$, and in particular our Sun is the 2.6092$\\cdot$ 10$^7$-th star of this kind, born in the solar vicinity."],"url":"http://arxiv.org/abs/2406.08036v1","category":"astro-ph.SR"}
{"created":"2024-06-12 09:36:20","title":"Strong and Weak Random Walks on Signed Networks","abstract":"Random walks play an important role in probing the structure of complex networks. On traditional networks, they can be used to extract community structure, understand node centrality, perform link prediction, or capture the similarity between nodes. On signed networks, where the edge weights can be either positive or negative, it is non-trivial to design a random walk which can be used to extract information about the signed structure of the network, in particular the ability to partition the graph into communities with positive edges inside and negative edges in between. Prior works on signed network random walks focus on the case where there are only two such communities (strong balance), which is rarely the case in empirical networks. In this paper, we propose a signed network random walk which can capture the structure of a network with more than two such communities (weak balance). The walk results in a similarity matrix which can be used to cluster the nodes into antagonistic communities. We compare the characteristics of the so-called strong and weak random walks, in terms of walk length and stationarity. We show through a series of experiments on synthetic and empirical networks that the similarity matrix based on weak walks can be used for both unsupervised and semi-supervised clustering, outperforming the same similarity matrix based on strong walks when the graph has more than two communities, or exhibits asymmetry in the density of links. These results suggest that other random-walk based algorithms for signed networks could be improved simply by running them with weak walks instead of strong walks.","sentences":["Random walks play an important role in probing the structure of complex networks.","On traditional networks, they can be used to extract community structure, understand node centrality, perform link prediction, or capture the similarity between nodes.","On signed networks, where the edge weights can be either positive or negative, it is non-trivial to design a random walk which can be used to extract information about the signed structure of the network, in particular the ability to partition the graph into communities with positive edges inside and negative edges in between.","Prior works on signed network random walks focus on the case where there are only two such communities (strong balance), which is rarely the case in empirical networks.","In this paper, we propose a signed network random walk which can capture the structure of a network with more than two such communities (weak balance).","The walk results in a similarity matrix which can be used to cluster the nodes into antagonistic communities.","We compare the characteristics of the so-called strong and weak random walks, in terms of walk length and stationarity.","We show through a series of experiments on synthetic and empirical networks that the similarity matrix based on weak walks can be used for both unsupervised and semi-supervised clustering, outperforming the same similarity matrix based on strong walks when the graph has more than two communities, or exhibits asymmetry in the density of links.","These results suggest that other random-walk based algorithms for signed networks could be improved simply by running them with weak walks instead of strong walks."],"url":"http://arxiv.org/abs/2406.08034v1","category":"physics.soc-ph"}
{"created":"2024-06-12 09:31:03","title":"Fault detection in propulsion motors in the presence of concept drift","abstract":"Machine learning and statistical methods can be used to enhance monitoring and fault prediction in marine systems. These methods rely on a dataset with records of historical system behaviour, potentially containing periods of both fault-free and faulty operation. An unexpected change in the underlying system, called a concept drift, may impact the performance of these methods, triggering the need for model retraining or other adaptations. In this article, we present an approach for detecting overheating in stator windings of marine propulsion motors that is able to successfully operate during concept drift without the need for full model retraining. Two distinct approaches are presented and tested. All models are trained and verified using a dataset from operational propulsion motors, with known, sudden concept drifts.","sentences":["Machine learning and statistical methods can be used to enhance monitoring and fault prediction in marine systems.","These methods rely on a dataset with records of historical system behaviour, potentially containing periods of both fault-free and faulty operation.","An unexpected change in the underlying system, called a concept drift, may impact the performance of these methods, triggering the need for model retraining or other adaptations.","In this article, we present an approach for detecting overheating in stator windings of marine propulsion motors that is able to successfully operate during concept drift without the need for full model retraining.","Two distinct approaches are presented and tested.","All models are trained and verified using a dataset from operational propulsion motors, with known, sudden concept drifts."],"url":"http://arxiv.org/abs/2406.08030v1","category":"stat.AP"}
{"created":"2024-06-12 09:22:19","title":"Null hypothesis Bayes factor estimates can be biased in (some) common factorial designs: A simulation study","abstract":"Bayes factor null hypothesis tests provide a viable alternative to frequentist measures of evidence quantification. Bayes factors for realistic interesting models cannot be calculated exactly, but have to be estimated, which involves approximations to complex integrals. Crucially, the accuracy of these estimates, i.e., whether an estimated Bayes factor corresponds to the true Bayes factor, is unknown, and may depend on data, prior, and likelihood. We have recently developed a novel statistical procedure, namely simulation-based calibration (SBC) for Bayes factors, to test for a given analysis, whether the computed Bayes factors are accurate. Here, we use SBC for Bayes factors to test for some common cognitive designs, whether Bayes factors are estimated accurately. We use the bridgesampling/brms packages as well as the BayesFactor package in R. We find that Bayes factor estimates are accurate and exhibit only little bias in Latin square designs with (a) random effects for subjects only and (b) for crossed random effects for subjects and items, but a single fixed-factor. However, Bayes factor estimates turn out biased and liberal in a 2x2 design with crossed random effects for subjects and items. These results suggest that researchers should test for their individual analysis, whether Bayes factor estimates are accurate. Moreover, future research is needed to determine the boundary conditions under which Bayes factor estimates are accurate or biased, as well as software development to improve estimation accuracy.","sentences":["Bayes factor null hypothesis tests provide a viable alternative to frequentist measures of evidence quantification.","Bayes factors for realistic interesting models cannot be calculated exactly, but have to be estimated, which involves approximations to complex integrals.","Crucially, the accuracy of these estimates, i.e., whether an estimated Bayes factor corresponds to the true Bayes factor, is unknown, and may depend on data, prior, and likelihood.","We have recently developed a novel statistical procedure, namely simulation-based calibration (SBC) for Bayes factors, to test for a given analysis, whether the computed Bayes factors are accurate.","Here, we use SBC for Bayes factors to test for some common cognitive designs, whether Bayes factors are estimated accurately.","We use the bridgesampling/brms packages as well as the BayesFactor package in R. We find that Bayes factor estimates are accurate and exhibit only little bias in Latin square designs with (a) random effects for subjects only and (b) for crossed random effects for subjects and items, but a single fixed-factor.","However, Bayes factor estimates turn out biased and liberal in a 2x2 design with crossed random effects for subjects and items.","These results suggest that researchers should test for their individual analysis, whether Bayes factor estimates are accurate.","Moreover, future research is needed to determine the boundary conditions under which Bayes factor estimates are accurate or biased, as well as software development to improve estimation accuracy."],"url":"http://arxiv.org/abs/2406.08022v1","category":"stat.ME"}
{"created":"2024-06-12 09:09:51","title":"Robust intensification of global ocean Eddy Kinetic Energy from three decades of satellite altimetry observations","abstract":"Ocean mesoscale variability, a key component of the climate system, influences ocean circulation and heat, gas, carbon and nutrient distribution. Trends on Eddy Kinetic Energy (EKE), a metric measuring its intensity, are investigated using two products constructed from 30 years of altimetric observations. Statistically significant positive trends in globally-averaged EKE reveal a strengthening of 1-3% per decade. Regions of intense mesoscale activity become more energetic than other areas. Robust positive EKE trends are observed in the Kuroshio Extension and the Gulf Stream, with remarkable EKE increases of ~50% and ~20%, respectively, over the last decade. Our study opens a new question into how the observed Gulf Stream strengthening impacts the AMOC, and challenges existing climate models emphasizing the necessity for improved small-scale ocean process representation.","sentences":["Ocean mesoscale variability, a key component of the climate system, influences ocean circulation and heat, gas, carbon and nutrient distribution.","Trends on Eddy Kinetic Energy (EKE), a metric measuring its intensity, are investigated using two products constructed from 30 years of altimetric observations.","Statistically significant positive trends in globally-averaged EKE reveal a strengthening of 1-3% per decade.","Regions of intense mesoscale activity become more energetic than other areas.","Robust positive EKE trends are observed in the Kuroshio Extension and the Gulf Stream, with remarkable EKE increases of ~50% and ~20%, respectively, over the last decade.","Our study opens a new question into how the observed Gulf Stream strengthening impacts the AMOC, and challenges existing climate models emphasizing the necessity for improved small-scale ocean process representation."],"url":"http://arxiv.org/abs/2406.08014v1","category":"physics.ao-ph"}
{"created":"2024-06-12 09:04:45","title":"Deep reinforcement learning with positional context for intraday trading","abstract":"Deep reinforcement learning (DRL) is a well-suited approach to financial decision-making, where an agent makes decisions based on its trading strategy developed from market observations. Existing DRL intraday trading strategies mainly use price-based features to construct the state space. They neglect the contextual information related to the position of the strategy, which is an important aspect given the sequential nature of intraday trading. In this study, we propose a novel DRL model for intraday trading that introduces positional features encapsulating the contextual information into its sparse state space. The model is evaluated over an extended period of almost a decade and across various assets including commodities and foreign exchange securities, taking transaction costs into account. The results show a notable performance in terms of profitability and risk-adjusted metrics. The feature importance results show that each feature incorporating contextual information contributes to the overall performance of the model. Additionally, through an exploration of the agent's intraday trading activity, we unveil patterns that substantiate the effectiveness of our proposed model.","sentences":["Deep reinforcement learning (DRL) is a well-suited approach to financial decision-making, where an agent makes decisions based on its trading strategy developed from market observations.","Existing DRL intraday trading strategies mainly use price-based features to construct the state space.","They neglect the contextual information related to the position of the strategy, which is an important aspect given the sequential nature of intraday trading.","In this study, we propose a novel DRL model for intraday trading that introduces positional features encapsulating the contextual information into its sparse state space.","The model is evaluated over an extended period of almost a decade and across various assets including commodities and foreign exchange securities, taking transaction costs into account.","The results show a notable performance in terms of profitability and risk-adjusted metrics.","The feature importance results show that each feature incorporating contextual information contributes to the overall performance of the model.","Additionally, through an exploration of the agent's intraday trading activity, we unveil patterns that substantiate the effectiveness of our proposed model."],"url":"http://arxiv.org/abs/2406.08013v1","category":"q-fin.TR"}
{"created":"2024-06-12 09:00:49","title":"A Self-boosted Framework for Calibrated Ranking","abstract":"Scale-calibrated ranking systems are ubiquitous in real-world applications nowadays, which pursue accurate ranking quality and calibrated probabilistic predictions simultaneously. For instance, in the advertising ranking system, the predicted click-through rate (CTR) is utilized for ranking and required to be calibrated for the downstream cost-per-click ads bidding. Recently, multi-objective based methods have been wildly adopted as a standard approach for Calibrated Ranking, which incorporates the combination of two loss functions: a pointwise loss that focuses on calibrated absolute values and a ranking loss that emphasizes relative orderings. However, when applied to industrial online applications, existing multi-objective CR approaches still suffer from two crucial limitations. First, previous methods need to aggregate the full candidate list within a single mini-batch to compute the ranking loss. Such aggregation strategy violates extensive data shuffling which has long been proven beneficial for preventing overfitting, and thus degrades the training effectiveness. Second, existing multi-objective methods apply the two inherently conflicting loss functions on a single probabilistic prediction, which results in a sub-optimal trade-off between calibration and ranking. To tackle the two limitations, we propose a Self-Boosted framework for Calibrated Ranking (SBCR).","sentences":["Scale-calibrated ranking systems are ubiquitous in real-world applications nowadays, which pursue accurate ranking quality and calibrated probabilistic predictions simultaneously.","For instance, in the advertising ranking system, the predicted click-through rate (CTR) is utilized for ranking and required to be calibrated for the downstream cost-per-click ads bidding.","Recently, multi-objective based methods have been wildly adopted as a standard approach for Calibrated Ranking, which incorporates the combination of two loss functions: a pointwise loss that focuses on calibrated absolute values and a ranking loss that emphasizes relative orderings.","However, when applied to industrial online applications, existing multi-objective CR approaches still suffer from two crucial limitations.","First, previous methods need to aggregate the full candidate list within a single mini-batch to compute the ranking loss.","Such aggregation strategy violates extensive data shuffling which has long been proven beneficial for preventing overfitting, and thus degrades the training effectiveness.","Second, existing multi-objective methods apply the two inherently conflicting loss functions on a single probabilistic prediction, which results in a sub-optimal trade-off between calibration and ranking.","To tackle the two limitations, we propose a Self-Boosted framework for Calibrated Ranking (SBCR)."],"url":"http://arxiv.org/abs/2406.08010v1","category":"cs.IR"}
{"created":"2024-06-12 08:57:45","title":"Quantum Reductive Perturbation Method for Photon Propagations in a Cold Atomic Gas","abstract":"We develop a quantum reductive perturbation method (RPM), a generalization of classical RPM widely used in nonlinear wave theory, to derive a simplified model (i.e. quantum nonlinear Schrodinger equation) from fully quantum Heisenberg-Langevin-Maxwell equations describingphoton propagations in a coherent cold atomic gas. The result is used to discuss two-photon bound states and optical solitons in the gas. Though a specific system is considered, the quantum RPM established here is very general and can be applied to other complex quantum nonlinear problems.","sentences":["We develop a quantum reductive perturbation method (RPM), a generalization of classical RPM widely used in nonlinear wave theory, to derive a simplified model (i.e. quantum nonlinear Schrodinger equation) from fully quantum Heisenberg-Langevin-Maxwell equations describingphoton propagations in a coherent cold atomic gas.","The result is used to discuss two-photon bound states and optical solitons in the gas.","Though a specific system is considered, the quantum RPM established here is very general and can be applied to other complex quantum nonlinear problems."],"url":"http://arxiv.org/abs/2406.08008v1","category":"quant-ph"}
{"created":"2024-06-12 08:49:40","title":"Metasensor: a proposal for sensor evolution in robotics","abstract":"Sensors play a fundamental role in achieving the complex behaviors typically found in biological organisms. However, their potential role in the design of artificial agents is often overlooked. This often results in the design of robots that are poorly adapted to the environment, compared to their biological counterparts. This paper proposes a formalization of a novel architectural component, called a metasensor, which enables a process of sensor evolution reminiscent of what occurs in living organisms. Even in online scenarios, the metasensor layer searches for the optimal interpretation of its input signals and then feeds them to the robotic agent to accomplish the assigned task.","sentences":["Sensors play a fundamental role in achieving the complex behaviors typically found in biological organisms.","However, their potential role in the design of artificial agents is often overlooked.","This often results in the design of robots that are poorly adapted to the environment, compared to their biological counterparts.","This paper proposes a formalization of a novel architectural component, called a metasensor, which enables a process of sensor evolution reminiscent of what occurs in living organisms.","Even in online scenarios, the metasensor layer searches for the optimal interpretation of its input signals and then feeds them to the robotic agent to accomplish the assigned task."],"url":"http://arxiv.org/abs/2406.08005v1","category":"cs.RO"}
{"created":"2024-06-12 08:48:52","title":"Neural Data-Enabled Predictive Control","abstract":"Data-enabled predictive control (DeePC) for linear systems utilizes data matrices of recorded trajectories to directly predict new system trajectories, which is very appealing for real-life applications. In this paper we leverage the universal approximation properties of neural networks (NNs) to develop neural DeePC algorithms for nonlinear systems. Firstly, we point out that the outputs of the last hidden layer of a deep NN implicitly construct a basis in a so-called neural (feature) space, while the output linear layer performs affine interpolation in the neural space. As such, we can train off-line a deep NN using large data sets of trajectories to learn the neural basis and compute on-line a suitable affine interpolation using DeePC. Secondly, methods for guaranteeing consistency of neural DeePC and for reducing computational complexity are developed. Several neural DeePC formulations are illustrated on a nonlinear pendulum example.","sentences":["Data-enabled predictive control (DeePC) for linear systems utilizes data matrices of recorded trajectories to directly predict new system trajectories, which is very appealing for real-life applications.","In this paper we leverage the universal approximation properties of neural networks (NNs) to develop neural DeePC algorithms for nonlinear systems.","Firstly, we point out that the outputs of the last hidden layer of a deep NN implicitly construct a basis in a so-called neural (feature) space, while the output linear layer performs affine interpolation in the neural space.","As such, we can train off-line a deep NN using large data sets of trajectories to learn the neural basis and compute on-line a suitable affine interpolation using DeePC.","Secondly, methods for guaranteeing consistency of neural DeePC and for reducing computational complexity are developed.","Several neural DeePC formulations are illustrated on a nonlinear pendulum example."],"url":"http://arxiv.org/abs/2406.08003v1","category":"math.OC"}
{"created":"2024-06-12 08:45:22","title":"Scaling properties of (2+1) directed polymers in the low temperature limit","abstract":"In terms of the replica method we consider the low temperature limit of (2+1) directed polymers in a random potential. The proposed approach allows to compute the scaling exponent $\\theta$ of the free energy fluctuations as well as the left tail of its probability distribution function. It is argued that $\\theta = 1/4$ which is slightly different from the zero-temperature numerical value which is close to $0.241$.","sentences":["In terms of the replica method we consider the low temperature limit of (2+1) directed polymers in a random potential.","The proposed approach allows to compute the scaling exponent $\\theta$ of the free energy fluctuations as well as the left tail of its probability distribution function.","It is argued that $\\theta = 1/4$ which is slightly different from the zero-temperature numerical value which is close to $0.241$."],"url":"http://arxiv.org/abs/2406.07998v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-12 08:42:23","title":"Semantic-Aware Resource Allocation Based on Deep Reinforcement Learning for 5G-V2X HetNets","abstract":"This letter proposes a semantic-aware resource allocation (SARA) framework with flexible duty cycle (DC) coexistence mechanism (SARADC) for 5G-V2X Heterogeneous Network (HetNets) based on deep reinforcement learning (DRL) proximal policy optimization (PPO). Specifically, we investigate V2X networks within a two-tiered HetNets structure. In response to the needs of high-speed vehicular networking in urban environments, we design a semantic communication system and introduce two resource allocation metrics: high-speed semantic transmission rate (HSR) and semantic spectrum efficiency (HSSE). Our main goal is to maximize HSSE. Additionally, we address the coexistence of vehicular users and WiFi users in 5G New Radio Unlicensed (NR-U) networks. To tackle this complex challenge, we propose a novel approach that jointly optimizes flexible DC coexistence mechanism and the allocation of resources and base stations (BSs). Unlike traditional bit transmission methods, our approach integrates the semantic communication paradigm into the communication system. Experimental results demonstrate that our proposed solution outperforms traditional bit transmission methods with traditional DC coexistence mechanism in terms of HSSE and semantic throughput (ST) for both vehicular and WiFi users.","sentences":["This letter proposes a semantic-aware resource allocation (SARA) framework with flexible duty cycle (DC) coexistence mechanism (SARADC) for 5G-V2X Heterogeneous Network (HetNets) based on deep reinforcement learning (DRL) proximal policy optimization (PPO).","Specifically, we investigate V2X networks within a two-tiered HetNets structure.","In response to the needs of high-speed vehicular networking in urban environments, we design a semantic communication system and introduce two resource allocation metrics: high-speed semantic transmission rate (HSR) and semantic spectrum efficiency (HSSE).","Our main goal is to maximize HSSE.","Additionally, we address the coexistence of vehicular users and WiFi users in 5G New Radio Unlicensed (NR-U) networks.","To tackle this complex challenge, we propose a novel approach that jointly optimizes flexible DC coexistence mechanism and the allocation of resources and base stations (BSs).","Unlike traditional bit transmission methods, our approach integrates the semantic communication paradigm into the communication system.","Experimental results demonstrate that our proposed solution outperforms traditional bit transmission methods with traditional DC coexistence mechanism in terms of HSSE and semantic throughput (ST) for both vehicular and WiFi users."],"url":"http://arxiv.org/abs/2406.07996v1","category":"cs.NI"}
{"created":"2024-06-12 08:38:47","title":"How social reinforcement learning can lead to metastable polarisation and the voter model","abstract":"Previous explanations for the persistence of polarization of opinions have typically included modelling assumptions that predispose the possibility of polarization (e.g.\\ repulsive interactions). An exception is recent research showing that polarization is stable when agents form their opinions using reinforcement learning.   We show that the polarization observed in this model is not stable, but exhibits consensus asymptotically with probability one. By constructing a link between the reinforcement learning model and the voter model, we argue that the observed polarization is metastable. Finally, we show that a slight modification in the learning process of the agents changes the model from being non-ergodic to being ergodic.   Our results show that reinforcement learning may be a powerful method for modelling polarization in opinion dynamics, but that the tools appropriate for analysing such models crucially depend on the properties of the resulting systems. Properties which are determined by the details of the learning process.","sentences":["Previous explanations for the persistence of polarization of opinions have typically included modelling assumptions that predispose the possibility of polarization (e.g.\\ repulsive interactions).","An exception is recent research showing that polarization is stable when agents form their opinions using reinforcement learning.   ","We show that the polarization observed in this model is not stable, but exhibits consensus asymptotically with probability one.","By constructing a link between the reinforcement learning model and the voter model, we argue that the observed polarization is metastable.","Finally, we show that a slight modification in the learning process of the agents changes the model from being non-ergodic to being ergodic.   ","Our results show that reinforcement learning may be a powerful method for modelling polarization in opinion dynamics, but that the tools appropriate for analysing such models crucially depend on the properties of the resulting systems.","Properties which are determined by the details of the learning process."],"url":"http://arxiv.org/abs/2406.07993v1","category":"physics.soc-ph"}
{"created":"2024-06-12 08:34:53","title":"A Federated Online Restless Bandit Framework for Cooperative Resource Allocation","abstract":"Restless multi-armed bandits (RMABs) have been widely utilized to address resource allocation problems with Markov reward processes (MRPs). Existing works often assume that the dynamics of MRPs are known prior, which makes the RMAB problem solvable from an optimization perspective. Nevertheless, an efficient learning-based solution for RMABs with unknown system dynamics remains an open problem. In this paper, we study the cooperative resource allocation problem with unknown system dynamics of MRPs. This problem can be modeled as a multi-agent online RMAB problem, where multiple agents collaboratively learn the system dynamics while maximizing their accumulated rewards. We devise a federated online RMAB framework to mitigate the communication overhead and data privacy issue by adopting the federated learning paradigm. Based on this framework, we put forth a Federated Thompson Sampling-enabled Whittle Index (FedTSWI) algorithm to solve this multi-agent online RMAB problem. The FedTSWI algorithm enjoys a high communication and computation efficiency, and a privacy guarantee. Moreover, we derive a regret upper bound for the FedTSWI algorithm. Finally, we demonstrate the effectiveness of the proposed algorithm on the case of online multi-user multi-channel access. Numerical results show that the proposed algorithm achieves a fast convergence rate of $\\mathcal{O}(\\sqrt{T\\log(T)})$ and better performance compared with baselines. More importantly, its sample complexity decreases with the number of agents.","sentences":["Restless multi-armed bandits (RMABs) have been widely utilized to address resource allocation problems with Markov reward processes (MRPs).","Existing works often assume that the dynamics of MRPs are known prior, which makes the RMAB problem solvable from an optimization perspective.","Nevertheless, an efficient learning-based solution for RMABs with unknown system dynamics remains an open problem.","In this paper, we study the cooperative resource allocation problem with unknown system dynamics of MRPs.","This problem can be modeled as a multi-agent online RMAB problem, where multiple agents collaboratively learn the system dynamics while maximizing their accumulated rewards.","We devise a federated online RMAB framework to mitigate the communication overhead and data privacy issue by adopting the federated learning paradigm.","Based on this framework, we put forth a Federated Thompson Sampling-enabled Whittle Index (FedTSWI) algorithm to solve this multi-agent online RMAB problem.","The FedTSWI algorithm enjoys a high communication and computation efficiency, and a privacy guarantee.","Moreover, we derive a regret upper bound for the FedTSWI algorithm.","Finally, we demonstrate the effectiveness of the proposed algorithm on the case of online multi-user multi-channel access.","Numerical results show that the proposed algorithm achieves a fast convergence rate of $\\mathcal{O}(\\sqrt{T\\log(T)})$ and better performance compared with baselines.","More importantly, its sample complexity decreases with the number of agents."],"url":"http://arxiv.org/abs/2406.07992v1","category":"cs.LG"}
{"created":"2024-06-12 08:26:22","title":"Near-Field Wideband Beam Training Based on Distance-Dependent Beam Split","abstract":"Near-field beam training is essential for acquiring channel state information in 6G extremely large-scale multiple input multiple output (XL-MIMO) systems. To achieve low-overhead beam training, existing method has been proposed to leverage the near-field beam split effect, which deploys true-time-delay arrays to simultaneously search multiple angles of the entire angular range in a distance ring with a single pilot. However, the method still requires exhaustive search in the distance domain, which limits its efficiency. To address the problem, we propose a distance-dependent beam-split-based beam training method to further reduce the training overheads. Specifically, we first reveal the new phenomenon of distance-dependent beam split, where by manipulating the configurations of time-delay and phase-shift, beams at different frequencies can simultaneously scan the angular domain in multiple distance rings. Leveraging the phenomenon, we propose a near-field beam training method where both different angles and distances can simultaneously be searched in one time slot. Thus, a few pilots are capable of covering the whole angle-distance space for wideband XL-MIMO. Theoretical analysis and numerical simulations are also displayed to verify the superiority of the proposed method on beamforming gain and training overhead.","sentences":["Near-field beam training is essential for acquiring channel state information in 6G extremely large-scale multiple input multiple output (XL-MIMO) systems.","To achieve low-overhead beam training, existing method has been proposed to leverage the near-field beam split effect, which deploys true-time-delay arrays to simultaneously search multiple angles of the entire angular range in a distance ring with a single pilot.","However, the method still requires exhaustive search in the distance domain, which limits its efficiency.","To address the problem, we propose a distance-dependent beam-split-based beam training method to further reduce the training overheads.","Specifically, we first reveal the new phenomenon of distance-dependent beam split, where by manipulating the configurations of time-delay and phase-shift, beams at different frequencies can simultaneously scan the angular domain in multiple distance rings.","Leveraging the phenomenon, we propose a near-field beam training method where both different angles and distances can simultaneously be searched in one time slot.","Thus, a few pilots are capable of covering the whole angle-distance space for wideband XL-MIMO.","Theoretical analysis and numerical simulations are also displayed to verify the superiority of the proposed method on beamforming gain and training overhead."],"url":"http://arxiv.org/abs/2406.07989v1","category":"cs.IT"}
{"created":"2024-06-12 08:07:21","title":"Quantitative analysis and its applications for Keller-Segel type systems","abstract":"In this paper, we utilize the De Giorgi iteration to quantitatively analyze the upper bound of solutions for Keller-Segel type systems. The refined upper bound estimate presented here has broad applications in determining large time behaviours of weak solutions and improving the regularity for models involving the $p$-Laplace operator. To demonstrate the applicability of our findings, we investigate the asymptotic stability of a chemotaxis model with nonlinear signal production and a chemotaxis-Navier-Stokes model with a logistic source. Additionally, within the context of $p$-Laplacian diffusion, we establish H\\\"{o}lder continuity for a chemotaxis-haptotaxis model and a chemotaxis-Stokes model.","sentences":["In this paper, we utilize the De Giorgi iteration to quantitatively analyze the upper bound of solutions for Keller-Segel type systems.","The refined upper bound estimate presented here has broad applications in determining large time behaviours of weak solutions and improving the regularity for models involving the $p$-Laplace operator.","To demonstrate the applicability of our findings, we investigate the asymptotic stability of a chemotaxis model with nonlinear signal production and a chemotaxis-Navier-Stokes model with a logistic source.","Additionally, within the context of $p$-Laplacian diffusion, we establish H\\\"{o}lder continuity for a chemotaxis-haptotaxis model and a chemotaxis-Stokes model."],"url":"http://arxiv.org/abs/2406.07982v1","category":"math.AP"}
{"created":"2024-06-12 08:06:46","title":"Towards Accelerating Real-Time Path Tracing with Foveated Framework","abstract":"Path tracing is one of the most widespread rendering techniques for high-end graphics fidelity. However, the slow convergence time and presence of intensive noises make it infeasible for numerous real-time applications where physically corrected photorealistic effects are salient. Additionally, the increased demand for pixel density, geometric complexity, advanced material, and multiple lights hinder the algorithm from attaining an interactive frame rate for real-time applications. To address these issues, we developed a framework to accelerate path tracing through foveated rendering, a robust technique that leverages human vision. Our dynamic foveated path-tracing framework integrates fixation data and selectively lowers the rendering resolution towards the periphery. The framework is built on NVIDIA's OptiX 7.5 API with CUDA 12.1, serving as the base of future foveated path tracing research. Through comprehensive experimentation, we demonstrated the effectiveness of our framework in this paper. Depending on the scene complexity, our solution can significantly enhance rendering performance up to a factor of 25 without any notable visual differences. We further evaluated the framework using a structured error map algorithm with variable sample numbers and foveated area size.","sentences":["Path tracing is one of the most widespread rendering techniques for high-end graphics fidelity.","However, the slow convergence time and presence of intensive noises make it infeasible for numerous real-time applications where physically corrected photorealistic effects are salient.","Additionally, the increased demand for pixel density, geometric complexity, advanced material, and multiple lights hinder the algorithm from attaining an interactive frame rate for real-time applications.","To address these issues, we developed a framework to accelerate path tracing through foveated rendering, a robust technique that leverages human vision.","Our dynamic foveated path-tracing framework integrates fixation data and selectively lowers the rendering resolution towards the periphery.","The framework is built on NVIDIA's OptiX 7.5 API with CUDA 12.1, serving as the base of future foveated path tracing research.","Through comprehensive experimentation, we demonstrated the effectiveness of our framework in this paper.","Depending on the scene complexity, our solution can significantly enhance rendering performance up to a factor of 25 without any notable visual differences.","We further evaluated the framework using a structured error map algorithm with variable sample numbers and foveated area size."],"url":"http://arxiv.org/abs/2406.07981v1","category":"cs.GR"}
{"created":"2024-06-12 08:05:45","title":"Heuristic Learning with Graph Neural Networks: A Unified Framework for Link Prediction","abstract":"Link prediction is a fundamental task in graph learning, inherently shaped by the topology of the graph. While traditional heuristics are grounded in graph topology, they encounter challenges in generalizing across diverse graphs. Recent research efforts have aimed to leverage the potential of heuristics, yet a unified formulation accommodating both local and global heuristics remains undiscovered. Drawing insights from the fact that both local and global heuristics can be represented by adjacency matrix multiplications, we propose a unified matrix formulation to accommodate and generalize various heuristics. We further propose the Heuristic Learning Graph Neural Network (HL-GNN) to efficiently implement the formulation. HL-GNN adopts intra-layer propagation and inter-layer connections, allowing it to reach a depth of around 20 layers with lower time complexity than GCN. HL-GNN is proven to be more expressive than heuristics and conventional GNNs, and it can adaptively trade-off between node features and topological information. Extensive experiments on the Planetoid, Amazon, and OGB datasets underscore the effectiveness and efficiency of HL-GNN. It outperforms existing methods by a large margin in prediction performance. Additionally, HL-GNN is several orders of magnitude faster than heuristic-inspired methods while requiring only a few trainable parameters. The case study further demonstrates that the generalized heuristics and learned weights are highly interpretable.","sentences":["Link prediction is a fundamental task in graph learning, inherently shaped by the topology of the graph.","While traditional heuristics are grounded in graph topology, they encounter challenges in generalizing across diverse graphs.","Recent research efforts have aimed to leverage the potential of heuristics, yet a unified formulation accommodating both local and global heuristics remains undiscovered.","Drawing insights from the fact that both local and global heuristics can be represented by adjacency matrix multiplications, we propose a unified matrix formulation to accommodate and generalize various heuristics.","We further propose the Heuristic Learning Graph Neural Network (HL-GNN) to efficiently implement the formulation.","HL-GNN adopts intra-layer propagation and inter-layer connections, allowing it to reach a depth of around 20 layers with lower time complexity than GCN.","HL-GNN is proven to be more expressive than heuristics and conventional GNNs, and it can adaptively trade-off between node features and topological information.","Extensive experiments on the Planetoid, Amazon, and OGB datasets underscore the effectiveness and efficiency of HL-GNN.","It outperforms existing methods by a large margin in prediction performance.","Additionally, HL-GNN is several orders of magnitude faster than heuristic-inspired methods while requiring only a few trainable parameters.","The case study further demonstrates that the generalized heuristics and learned weights are highly interpretable."],"url":"http://arxiv.org/abs/2406.07979v1","category":"cs.LG"}
{"created":"2024-06-12 08:05:37","title":"A 10.24-GHz-wide digital spectrometer array system for LMT-FINER: system design and laboratory performance verification","abstract":"For efficient spectroscopic redshift identification of early galaxies in the northern hemisphere, we aim to combine the Large Millimeter Telescope (LMT) with a wide-band heterodyne receiver, FINER, which will cover radio frequencies of 120--360 GHz and offer a 3--21 GHz intermediate frequency (IF) per sideband and polarization. To take full advantage of such wide IFs, we present a novel 10.24-GHz-wide digital spectrometer, DRS4 (Elecs Industry Co., Ltd.). It incorporates 20.48 Gsps samplers with an FPGA-based digital signal processing module. To mitigate the noise contamination from the image sideband, it is equipped with a digital sideband separation function to improve the sideband rejection up to 25 dB. Laboratory performance evaluations show that it exhibits an Allan time of at least ~100 s and a total power dynamic range of at least 7 dB. These results demonstrate its capability of instantaneously wide-band spectroscopy toward high-redshift galaxies with position-switching observations.","sentences":["For efficient spectroscopic redshift identification of early galaxies in the northern hemisphere, we aim to combine the Large Millimeter Telescope (LMT) with a wide-band heterodyne receiver, FINER, which will cover radio frequencies of 120--360 GHz and offer a 3--21 GHz intermediate frequency (IF) per sideband and polarization.","To take full advantage of such wide IFs, we present a novel 10.24-GHz-wide digital spectrometer, DRS4 (Elecs Industry Co., Ltd.).","It incorporates 20.48 Gsps samplers with an FPGA-based digital signal processing module.","To mitigate the noise contamination from the image sideband, it is equipped with a digital sideband separation function to improve the sideband rejection up to 25 dB. Laboratory performance evaluations show that it exhibits an Allan time of at least ~100 s and a total power dynamic range of at least 7 dB. These results demonstrate its capability of instantaneously wide-band spectroscopy toward high-redshift galaxies with position-switching observations."],"url":"http://arxiv.org/abs/2406.07978v1","category":"astro-ph.IM"}
{"created":"2024-06-12 07:44:36","title":"Better than Random: Reliable NLG Human Evaluation with Constrained Active Sampling","abstract":"Human evaluation is viewed as a reliable evaluation method for NLG which is expensive and time-consuming. To save labor and costs, researchers usually perform human evaluation on a small subset of data sampled from the whole dataset in practice. However, different selection subsets will lead to different rankings of the systems. To give a more correct inter-system ranking and make the gold standard human evaluation more reliable, we propose a Constrained Active Sampling Framework (CASF) for reliable human judgment. CASF operates through a Learner, a Systematic Sampler and a Constrained Controller to select representative samples for getting a more correct inter-system ranking.Experiment results on 137 real NLG evaluation setups with 44 human evaluation metrics across 16 datasets and 5 NLG tasks demonstrate CASF receives 93.18% top-ranked system recognition accuracy and ranks first or ranks second on 90.91% of the human metrics with 0.83 overall inter-system ranking Kendall correlation.Code and data are publicly available online.","sentences":["Human evaluation is viewed as a reliable evaluation method for NLG which is expensive and time-consuming.","To save labor and costs, researchers usually perform human evaluation on a small subset of data sampled from the whole dataset in practice.","However, different selection subsets will lead to different rankings of the systems.","To give a more correct inter-system ranking and make the gold standard human evaluation more reliable, we propose a Constrained Active Sampling Framework (CASF) for reliable human judgment.","CASF operates through a Learner, a Systematic Sampler and a Constrained Controller to select representative samples for getting a more correct inter-system ranking.","Experiment results on 137 real NLG evaluation setups with 44 human evaluation metrics across 16 datasets and 5 NLG tasks demonstrate CASF receives 93.18% top-ranked system recognition accuracy and ranks first or ranks second on 90.91% of the human metrics with 0.83 overall inter-system ranking Kendall correlation.","Code and data are publicly available online."],"url":"http://arxiv.org/abs/2406.07967v1","category":"cs.CL"}
{"created":"2024-06-12 07:44:22","title":"Real-world Image Dehazing with Coherence-based Label Generator and Cooperative Unfolding Network","abstract":"Real-world Image Dehazing (RID) aims to alleviate haze-induced degradation in real-world settings. This task remains challenging due to the complexities in accurately modeling real haze distributions and the scarcity of paired real-world data. To address these challenges, we first introduce a cooperative unfolding network that jointly models atmospheric scattering and image scenes, effectively integrating physical knowledge into deep networks to restore haze-contaminated details. Additionally, we propose the first RID-oriented iterative mean-teacher framework, termed the Coherence-based Label Generator, to generate high-quality pseudo labels for network training. Specifically, we provide an optimal label pool to store the best pseudo-labels during network training, leveraging both global and local coherence to select high-quality candidates and assign weights to prioritize haze-free regions. We verify the effectiveness of our method, with experiments demonstrating that it achieves state-of-the-art performance on RID tasks. Code will be available at \\url{https://github.com/cnyvfang/CORUN-Colabator}.","sentences":["Real-world Image Dehazing (RID) aims to alleviate haze-induced degradation in real-world settings.","This task remains challenging due to the complexities in accurately modeling real haze distributions and the scarcity of paired real-world data.","To address these challenges, we first introduce a cooperative unfolding network that jointly models atmospheric scattering and image scenes, effectively integrating physical knowledge into deep networks to restore haze-contaminated details.","Additionally, we propose the first RID-oriented iterative mean-teacher framework, termed the Coherence-based Label Generator, to generate high-quality pseudo labels for network training.","Specifically, we provide an optimal label pool to store the best pseudo-labels during network training, leveraging both global and local coherence to select high-quality candidates and assign weights to prioritize haze-free regions.","We verify the effectiveness of our method, with experiments demonstrating that it achieves state-of-the-art performance on RID tasks.","Code will be available at \\url{https://github.com/cnyvfang/CORUN-Colabator}."],"url":"http://arxiv.org/abs/2406.07966v1","category":"cs.CV"}
{"created":"2024-06-12 07:42:55","title":"Compressive Beam Alignment for Indoor Millimeter-Wave Systems","abstract":"The dynamic nature of indoor environments poses unique challenges for next-generation millimeter-wave (mmwave) connectivity. These challenges arise from blockages due to mobile obstacles, mm-wave signal scattering caused by indoor surfaces, and imperfections in phased antenna arrays. Consequently, traditional compressed sensing (CS) techniques for beam alignment become ineffective in practice under such settings. This paper proposes a novel beam alignment technique suited for mm-wave systems operating in indoor environments. The proposed technique exploits the energy compaction property of the discrete cosine transform to compressively sense and identify the strongest cluster locations in the transform domain for robust beamforming. Experimental results at 60 GHz demonstrate successful beam alignment with limited measurements even in the presence of partial blockages during the beam training phase.","sentences":["The dynamic nature of indoor environments poses unique challenges for next-generation millimeter-wave (mmwave) connectivity.","These challenges arise from blockages due to mobile obstacles, mm-wave signal scattering caused by indoor surfaces, and imperfections in phased antenna arrays.","Consequently, traditional compressed sensing (CS) techniques for beam alignment become ineffective in practice under such settings.","This paper proposes a novel beam alignment technique suited for mm-wave systems operating in indoor environments.","The proposed technique exploits the energy compaction property of the discrete cosine transform to compressively sense and identify the strongest cluster locations in the transform domain for robust beamforming.","Experimental results at 60 GHz demonstrate successful beam alignment with limited measurements even in the presence of partial blockages during the beam training phase."],"url":"http://arxiv.org/abs/2406.07965v1","category":"eess.SP"}
{"created":"2024-06-12 07:41:56","title":"High-temperature threshold of damage of SiC by swift heavy ions","abstract":"At ambient conditions, SiC is known to be resistant to irradiation with swift heavy ions (SHI) decelerating in the electronic stopping regime. However, there is no experimental data on the SiC irradiation at elevated temperatures. To investigate this problem, we evaluate the stability of SiC to SHI impacts at high temperatures up to 2200 K. We apply the combination of the Monte-Carlo code TREKIS-3, describing excitation of the electronic and atomic systems using temperature-dependent scattering cross-sections, with molecular-dynamic modeling of the lattice response to the excitation. We demonstrate that increasing irradiation temperature increases the energy transferred to the atomic lattice from the excited electronic system. This material heating leads to formation of a stable nanometric damaged core along the trajectory of 710 MeV Bi ion when the irradiation temperature overcomes the threshold of ~1800 K. In this case, a chain of nanometric voids along the ion trajectory forms due to the mass transport from the track core by edge dislocations. Voids of larger sizes appear at higher irradiation temperatures. At lower irradiation temperatures, the damaged regions recrystallize completely within ~100 ps after the ion passage.","sentences":["At ambient conditions, SiC is known to be resistant to irradiation with swift heavy ions (SHI) decelerating in the electronic stopping regime.","However, there is no experimental data on the SiC irradiation at elevated temperatures.","To investigate this problem, we evaluate the stability of SiC to SHI impacts at high temperatures up to 2200 K.","We apply the combination of the Monte-Carlo code TREKIS-3, describing excitation of the electronic and atomic systems using temperature-dependent scattering cross-sections, with molecular-dynamic modeling of the lattice response to the excitation.","We demonstrate that increasing irradiation temperature increases the energy transferred to the atomic lattice from the excited electronic system.","This material heating leads to formation of a stable nanometric damaged core along the trajectory of 710 MeV Bi ion when the irradiation temperature overcomes the threshold of ~1800","K. In this case, a chain of nanometric voids along the ion trajectory forms due to the mass transport from the track core by edge dislocations.","Voids of larger sizes appear at higher irradiation temperatures.","At lower irradiation temperatures, the damaged regions recrystallize completely within ~100 ps after the ion passage."],"url":"http://arxiv.org/abs/2406.07963v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-12 07:12:42","title":"Inverse scattering problem for third order differential operators on the whole axis","abstract":"Inverse scattering problem for an operator, which is a sum of the operator of the third derivative and of an operator of multiplication by a real function, is solved. The main closed system of equations of inverse problem is obtained. This system contains scattering coefficients and bound state elements as independent parameters. Form of the simplest reflectionless potential (analogous to soliton) is found.","sentences":["Inverse scattering problem for an operator, which is a sum of the operator of the third derivative and of an operator of multiplication by a real function, is solved.","The main closed system of equations of inverse problem is obtained.","This system contains scattering coefficients and bound state elements as independent parameters.","Form of the simplest reflectionless potential (analogous to soliton) is found."],"url":"http://arxiv.org/abs/2406.07947v1","category":"math.CA"}
{"created":"2024-06-12 06:59:51","title":"Characterizing Unsafe Code Encapsulation In Real-world Rust Systems","abstract":"Interior unsafe is an essential design paradigm advocated by the Rust community in system software development. However, there is little official guidance or few best practices regarding how to encapsulate unsafe code and achieve interior unsafe. The problem is critical because the Rust compiler is incapable of verifying the soundness of a safe function containing unsafe code. Falsely declaring an interior unsafe function as safe may undermine the fundamental memory-safety guarantee of Rust. To address this issue, this paper studies how interior unsafe is achieved in practice, aiming to identify best practices to guide Rust code design concerning unsafe code encapsulation. Specifically, we propose a novel unsafety isolation graph to model the essential usage and encapsulation of unsafe code. Based on the graph, we further propose four major isolation types and nine structural patterns to split a graph into several small self-contained subgraphs. These subgraphs can serve as useful audit units for examining the soundness of unsafe code encapsulation. We applied our approach to four real-world Rust projects. The experimental results demonstrate that our method is effective in characterizing their encapsulation code. Additionally, we identified two common issues in these projects that could complicate soundness verification or incur unsoundness issues.","sentences":["Interior unsafe is an essential design paradigm advocated by the Rust community in system software development.","However, there is little official guidance or few best practices regarding how to encapsulate unsafe code and achieve interior unsafe.","The problem is critical because the Rust compiler is incapable of verifying the soundness of a safe function containing unsafe code.","Falsely declaring an interior unsafe function as safe may undermine the fundamental memory-safety guarantee of Rust.","To address this issue, this paper studies how interior unsafe is achieved in practice, aiming to identify best practices to guide Rust code design concerning unsafe code encapsulation.","Specifically, we propose a novel unsafety isolation graph to model the essential usage and encapsulation of unsafe code.","Based on the graph, we further propose four major isolation types and nine structural patterns to split a graph into several small self-contained subgraphs.","These subgraphs can serve as useful audit units for examining the soundness of unsafe code encapsulation.","We applied our approach to four real-world Rust projects.","The experimental results demonstrate that our method is effective in characterizing their encapsulation code.","Additionally, we identified two common issues in these projects that could complicate soundness verification or incur unsoundness issues."],"url":"http://arxiv.org/abs/2406.07936v1","category":"cs.SE"}
{"created":"2024-06-12 17:59:42","title":"On Evaluating Adversarial Robustness of Volumetric Medical Segmentation Models","abstract":"Volumetric medical segmentation models have achieved significant success on organ and tumor-based segmentation tasks in recent years. However, their vulnerability to adversarial attacks remains largely unexplored, raising serious concerns regarding the real-world deployment of tools employing such models in the healthcare sector. This underscores the importance of investigating the robustness of existing models. In this context, our work aims to empirically examine the adversarial robustness across current volumetric segmentation architectures, encompassing Convolutional, Transformer, and Mamba-based models. We extend this investigation across four volumetric segmentation datasets, evaluating robustness under both white box and black box adversarial attacks. Overall, we observe that while both pixel and frequency-based attacks perform reasonably well under white box setting, the latter performs significantly better under transfer-based black box attacks. Across our experiments, we observe transformer-based models show higher robustness than convolution-based models with Mamba-based models being the most vulnerable. Additionally, we show that large-scale training of volumetric segmentation models improves the model's robustness against adversarial attacks. The code and pretrained models will be made available at https://github.com/HashmatShadab/Robustness-of-Volumetric-Medical-Segmentation-Models.","sentences":["Volumetric medical segmentation models have achieved significant success on organ and tumor-based segmentation tasks in recent years.","However, their vulnerability to adversarial attacks remains largely unexplored, raising serious concerns regarding the real-world deployment of tools employing such models in the healthcare sector.","This underscores the importance of investigating the robustness of existing models.","In this context, our work aims to empirically examine the adversarial robustness across current volumetric segmentation architectures, encompassing Convolutional, Transformer, and Mamba-based models.","We extend this investigation across four volumetric segmentation datasets, evaluating robustness under both white box and black box adversarial attacks.","Overall, we observe that while both pixel and frequency-based attacks perform reasonably well under white box setting, the latter performs significantly better under transfer-based black box attacks.","Across our experiments, we observe transformer-based models show higher robustness than convolution-based models with Mamba-based models being the most vulnerable.","Additionally, we show that large-scale training of volumetric segmentation models improves the model's robustness against adversarial attacks.","The code and pretrained models will be made available at https://github.com/HashmatShadab/Robustness-of-Volumetric-Medical-Segmentation-Models."],"url":"http://arxiv.org/abs/2406.08486v1","category":"eess.IV"}
{"created":"2024-06-12 17:42:08","title":"Existence of nonnegative energy-dissipating solutions to a class of stochastic thin-film equations under weak slippage: Part I -- positive solutions","abstract":"For mobility exponents $n \\in (2,3)$, we prove existence of strictly positive solutions to stochastic thin-film equations with singular effective interface potential and Stratonovich-type lower-order terms. With the perspective of using these solutions in Part II to construct surface-tension-energy dissipating solutions to stochastic thin-film equations with compactly supported initial data, for which finite speed of propagation is shown in future work, we establish decay estimates on the sum of surface-tension energy and effective interface potential -- without relying on further functionals involving initial data. Besides an extension of earlier techniques used in the case $n=2$ and a refinement of oscillation estimates for discrete solutions, the main analytical novelty of this paper is a discretization method which shows nonnegativity for a finite-element counterpart of the integral $\\int_{\\mathcal O} (u^{n-2}u_{x})_x u_{xx} dx$ under periodic boundary conditions in the parameter regime $n \\in (2,3)$. This nonnegativity property serves to control It\\^o-correction terms in the estimate for the decay of the surface-tension energy. This way, it is the key to obtain the desired decay estimates for the sum of surface-tension energy and effective interface potential which permit to establish the singular limit of vanishing effective interface potential in Part II.","sentences":["For mobility exponents $n \\in (2,3)$, we prove existence of strictly positive solutions to stochastic thin-film equations with singular effective interface potential and Stratonovich-type lower-order terms.","With the perspective of using these solutions in Part II to construct surface-tension-energy dissipating solutions to stochastic thin-film equations with compactly supported initial data, for which finite speed of propagation is shown in future work, we establish decay estimates on the sum of surface-tension energy and effective interface potential -- without relying on further functionals involving initial data.","Besides an extension of earlier techniques used in the case $n=2$ and a refinement of oscillation estimates for discrete solutions, the main analytical novelty of this paper is a discretization method which shows nonnegativity for a finite-element counterpart of the integral $\\int_{\\mathcal O} (u^{n-2}u_{x})_x u_{xx} dx$ under periodic boundary conditions in the parameter regime $n \\in (2,3)$. This nonnegativity property serves to control It\\^o-correction terms in the estimate for the decay of the surface-tension energy.","This way, it is the key to obtain the desired decay estimates for the sum of surface-tension energy and effective interface potential which permit to establish the singular limit of vanishing effective interface potential in Part II."],"url":"http://arxiv.org/abs/2406.08449v1","category":"math.AP"}
{"created":"2024-06-12 17:14:35","title":"Existence of nonnegative energy-dissipating solutions to a class of stochastic thin-film equations under weak slippage: Part II -- compactly supported initial data","abstract":"We prove existence of martingale solutions to a class of stochastic thin-film equations for mobility exponents $n \\in (2,3)$ and compactly supported initial data. With the perspective to study free-boundary problems related to stochastic thin-film equations in future work, we start from the surface-tension driven stochastic thin-film equation with Stratonovich noise and exploit the regime of coefficients in front of the Stratonovich correction term (which is of porous-media-type) for which energy dissipation can be established. By Bernis inequalities, third order spatial derivatives of appropriate powers of the solution are controlled. Analytically, we rely on approximation with $\\mathbb{P}$-almost surely strictly positive solutions and compactness methods based on energy-entropy estimates as well as martingale identification techniques.","sentences":["We prove existence of martingale solutions to a class of stochastic thin-film equations for mobility exponents $n \\in (2,3)$ and compactly supported initial data.","With the perspective to study free-boundary problems related to stochastic thin-film equations in future work, we start from the surface-tension driven stochastic thin-film equation with Stratonovich noise and exploit the regime of coefficients in front of the Stratonovich correction term (which is of porous-media-type) for which energy dissipation can be established.","By Bernis inequalities, third order spatial derivatives of appropriate powers of the solution are controlled.","Analytically, we rely on approximation with $\\mathbb{P}$-almost surely strictly positive solutions and compactness methods based on energy-entropy estimates as well as martingale identification techniques."],"url":"http://arxiv.org/abs/2406.08427v1","category":"math.AP"}
{"created":"2024-06-12 17:07:45","title":"Ultrasensitive single-ion electrometry in a magnetic field gradient","abstract":"Hyperfine energy levels in trapped ions offer long-lived spin states. In addition, the motion of these charged particles couples strongly to external electric field perturbations. These characteristics make trapped ions attractive platforms for the quantum sensing of electric fields. However, the spin states do not exhibit a strong intrinsic coupling to electric fields. This limits the achievable sensitivities. Here, we amplify the coupling between electric field perturbations and the spin states by using a static magnetic field gradient. Displacements of the trapped ion resulting from the forces experienced by an applied external electric field perturbation are thereby mapped to an instantaneous change in the energy level splitting of the internal spin states. This gradient mediated coupling of the electric field to the spin enables the use of a range of well-established magnetometry protocols for electrometry. Using our quantum sensor, we demonstrate AC sensitivities of $\\mathrm{S^{AC}_{min}=960(10)\\times 10^{-6}~V m^{-1}Hz^{-\\frac{1}{2}}}$ at a signal frequency of $\\omega_{\\epsilon}/2\\pi=5.82~\\mathrm{Hz}$, and DC sensitivities of $\\mathrm{S^{DC}_{min}=1.97(3)\\times 10^{-3} ~V m^{-1}Hz^{-\\frac{1}{2}}}$ with a Hahn-echo type sensing sequence. We also employ a rotating frame relaxometry technique, with which our quantum sensor can be utilised as an electric field noise spectrum analyser. We measure electric field signals down to a noise floor of $\\mathrm{S_{E}(\\omega)=6.2(5)\\times 10^{-12}~V^2 m^{-2}Hz^{-1}}$ at a frequency of $\\mathrm{30.0(3)~kHz}$. We therefore demonstrate unprecedented electric field sensitivities for the measurement of both DC signals and AC signals across a frequency range of sub-Hz to $\\sim\\mathrm{500~kHz}$. Finally, we describe a set of hardware modifications that are capable of achieving a further improvement in sensitivity by up to six orders of magnitude.","sentences":["Hyperfine energy levels in trapped ions offer long-lived spin states.","In addition, the motion of these charged particles couples strongly to external electric field perturbations.","These characteristics make trapped ions attractive platforms for the quantum sensing of electric fields.","However, the spin states do not exhibit a strong intrinsic coupling to electric fields.","This limits the achievable sensitivities.","Here, we amplify the coupling between electric field perturbations and the spin states by using a static magnetic field gradient.","Displacements of the trapped ion resulting from the forces experienced by an applied external electric field perturbation are thereby mapped to an instantaneous change in the energy level splitting of the internal spin states.","This gradient mediated coupling of the electric field to the spin enables the use of a range of well-established magnetometry protocols for electrometry.","Using our quantum sensor, we demonstrate AC sensitivities of $\\mathrm{S^{AC}_{min}=960(10)\\times 10^{-6}~V m^{-1}Hz^{-\\frac{1}{2}}}$ at a signal frequency of $\\omega_{\\epsilon}/2\\pi=5.82~\\mathrm{Hz}$, and DC sensitivities of $\\mathrm{S^{DC}_{min}=1.97(3)\\times 10^{-3} ~V m^{-1}Hz^{-\\frac{1}{2}}}$ with a Hahn-echo type sensing sequence.","We also employ a rotating frame relaxometry technique, with which our quantum sensor can be utilised as an electric field noise spectrum analyser.","We measure electric field signals down to a noise floor of $\\mathrm{S_{E}(\\omega)=6.2(5)\\times 10^{-12}~V^2 m^{-2}Hz^{-1}}$ at a frequency of $\\mathrm{30.0(3)~kHz}$. We therefore demonstrate unprecedented electric field sensitivities for the measurement of both DC signals and AC signals across a frequency range of sub-Hz to $\\sim\\mathrm{500~kHz}$. Finally, we describe a set of hardware modifications that are capable of achieving a further improvement in sensitivity by up to six orders of magnitude."],"url":"http://arxiv.org/abs/2406.08424v1","category":"quant-ph"}
{"created":"2024-06-12 16:58:41","title":"Discovering Preference Optimization Algorithms with and for Large Language Models","abstract":"Offline preference optimization is a key method for enhancing and controlling the quality of Large Language Model (LLM) outputs. Typically, preference optimization is approached as an offline supervised learning task using manually-crafted convex loss functions. While these methods are based on theoretical insights, they are inherently constrained by human creativity, so the large search space of possible loss functions remains under explored. We address this by performing LLM-driven objective discovery to automatically discover new state-of-the-art preference optimization algorithms without (expert) human intervention. Specifically, we iteratively prompt an LLM to propose and implement new preference optimization loss functions based on previously-evaluated performance metrics. This process leads to the discovery of previously-unknown and performant preference optimization algorithms. The best performing of these we call Discovered Preference Optimization (DiscoPOP), a novel algorithm that adaptively blends logistic and exponential losses. Experiments demonstrate the state-of-the-art performance of DiscoPOP and its successful transfer to held-out tasks.","sentences":["Offline preference optimization is a key method for enhancing and controlling the quality of Large Language Model (LLM) outputs.","Typically, preference optimization is approached as an offline supervised learning task using manually-crafted convex loss functions.","While these methods are based on theoretical insights, they are inherently constrained by human creativity, so the large search space of possible loss functions remains under explored.","We address this by performing LLM-driven objective discovery to automatically discover new state-of-the-art preference optimization algorithms without (expert) human intervention.","Specifically, we iteratively prompt an LLM to propose and implement new preference optimization loss functions based on previously-evaluated performance metrics.","This process leads to the discovery of previously-unknown and performant preference optimization algorithms.","The best performing of these we call Discovered Preference Optimization (DiscoPOP), a novel algorithm that adaptively blends logistic and exponential losses.","Experiments demonstrate the state-of-the-art performance of DiscoPOP and its successful transfer to held-out tasks."],"url":"http://arxiv.org/abs/2406.08414v1","category":"cs.LG"}
{"created":"2024-06-12 16:56:14","title":"Discovery of a new N-emitter in the epoch of reionization","abstract":"We report the discovery of a compact star-forming galaxy at $z=9.380$ in the GOODS-North field (named GN-z9p4) which shows numerous strong UV-optical emission lines and a single UV line, NIV] 1486. This makes GN-z9p4 the third-highest redshift N-emitter known to date. We determine the nebular abundances of H, C, N, O and Ne, size, and other physical properties of this object, and compare them to those of the other N-emitters known so far and to other star-forming galaxies. Using the direct method we find a metallicity 12+log(O/H)$=7.37 \\pm 0.15$, one of the lowest among the N-emitters. The N/O abundance ratio is highly super-solar, and C/O and Ne/O normal compared to other galaxies at low metallicity. We show that the compactness of GN-z9p4 (with effective radius $118\\pm16$ pc at 2 micron) and other N-emitters translates into very high stellar mass and SFR surface densities, which could be a criterium to identify other N-emitters. Future studies and larger samples are needed to understand these recently discovered, rare, and enigmatic objects.","sentences":["We report the discovery of a compact star-forming galaxy at $z=9.380$ in the GOODS-North field (named GN-z9p4) which shows numerous strong UV-optical emission lines and a single UV line, NIV] 1486.","This makes GN-z9p4 the third-highest redshift N-emitter known to date.","We determine the nebular abundances of H, C, N, O and Ne, size, and other physical properties of this object, and compare them to those of the other N-emitters known so far and to other star-forming galaxies.","Using the direct method we find a metallicity 12+log(O/H)$=7.37 \\pm 0.15$, one of the lowest among the N-emitters.","The N/O abundance ratio is highly super-solar, and C/O and Ne/O normal compared to other galaxies at low metallicity.","We show that the compactness of GN-z9p4 (with effective radius $118\\pm16$ pc at 2 micron) and other N-emitters translates into very high stellar mass and SFR surface densities, which could be a criterium to identify other N-emitters.","Future studies and larger samples are needed to understand these recently discovered, rare, and enigmatic objects."],"url":"http://arxiv.org/abs/2406.08408v1","category":"astro-ph.GA"}
{"created":"2024-06-12 16:53:51","title":"RRLS : Robust Reinforcement Learning Suite","abstract":"Robust reinforcement learning is the problem of learning control policies that provide optimal worst-case performance against a span of adversarial environments. It is a crucial ingredient for deploying algorithms in real-world scenarios with prevalent environmental uncertainties and has been a long-standing object of attention in the community, without a standardized set of benchmarks. This contribution endeavors to fill this gap. We introduce the Robust Reinforcement Learning Suite (RRLS), a benchmark suite based on Mujoco environments. RRLS provides six continuous control tasks with two types of uncertainty sets for training and evaluation. Our benchmark aims to standardize robust reinforcement learning tasks, facilitating reproducible and comparable experiments, in particular those from recent state-of-the-art contributions, for which we demonstrate the use of RRLS. It is also designed to be easily expandable to new environments. The source code is available at \\href{https://github.com/SuReLI/RRLS}{https://github.com/SuReLI/RRLS}.","sentences":["Robust reinforcement learning is the problem of learning control policies that provide optimal worst-case performance against a span of adversarial environments.","It is a crucial ingredient for deploying algorithms in real-world scenarios with prevalent environmental uncertainties and has been a long-standing object of attention in the community, without a standardized set of benchmarks.","This contribution endeavors to fill this gap.","We introduce the Robust Reinforcement Learning Suite (RRLS), a benchmark suite based on Mujoco environments.","RRLS provides six continuous control tasks with two types of uncertainty sets for training and evaluation.","Our benchmark aims to standardize robust reinforcement learning tasks, facilitating reproducible and comparable experiments, in particular those from recent state-of-the-art contributions, for which we demonstrate the use of RRLS.","It is also designed to be easily expandable to new environments.","The source code is available at \\href{https://github.com/SuReLI/RRLS}{https://github.com/SuReLI/RRLS}."],"url":"http://arxiv.org/abs/2406.08406v1","category":"cs.LG"}
{"created":"2024-06-12 16:52:34","title":"Optical microscope with nanometer longitudinal resolution based on a Linnik interferometer","abstract":"A microscope based on the Linnik interferometer was designed, built, and tested. Two methods were used for interference pattern measurement: phase-shifting and polarized single-shot methods. The former uses a low coherence light emitting diode as a light source, providing 10 nm resolution in the Z direction and diffraction-limited resolution in the X and Y directions. The second method is insensitive to vibrations and enables observation of moving objects. The simplicity and low cost of this instrument make it valuable for a variety of applications.","sentences":["A microscope based on the Linnik interferometer was designed, built, and tested.","Two methods were used for interference pattern measurement: phase-shifting and polarized single-shot methods.","The former uses a low coherence light emitting diode as a light source, providing 10 nm resolution in the Z direction and diffraction-limited resolution in the X and Y directions.","The second method is insensitive to vibrations and enables observation of moving objects.","The simplicity and low cost of this instrument make it valuable for a variety of applications."],"url":"http://arxiv.org/abs/2406.08403v1","category":"physics.optics"}
{"created":"2024-06-12 16:45:09","title":"Time-Constrained Robust MDPs","abstract":"Robust reinforcement learning is essential for deploying reinforcement learning algorithms in real-world scenarios where environmental uncertainty predominates. Traditional robust reinforcement learning often depends on rectangularity assumptions, where adverse probability measures of outcome states are assumed to be independent across different states and actions. This assumption, rarely fulfilled in practice, leads to overly conservative policies. To address this problem, we introduce a new time-constrained robust MDP (TC-RMDP) formulation that considers multifactorial, correlated, and time-dependent disturbances, thus more accurately reflecting real-world dynamics. This formulation goes beyond the conventional rectangularity paradigm, offering new perspectives and expanding the analytical framework for robust RL. We propose three distinct algorithms, each using varying levels of environmental information, and evaluate them extensively on continuous control benchmarks. Our results demonstrate that these algorithms yield an efficient tradeoff between performance and robustness, outperforming traditional deep robust RL methods in time-constrained environments while preserving robustness in classical benchmarks. This study revisits the prevailing assumptions in robust RL and opens new avenues for developing more practical and realistic RL applications.","sentences":["Robust reinforcement learning is essential for deploying reinforcement learning algorithms in real-world scenarios where environmental uncertainty predominates.","Traditional robust reinforcement learning often depends on rectangularity assumptions, where adverse probability measures of outcome states are assumed to be independent across different states and actions.","This assumption, rarely fulfilled in practice, leads to overly conservative policies.","To address this problem, we introduce a new time-constrained robust MDP (TC-RMDP) formulation that considers multifactorial, correlated, and time-dependent disturbances, thus more accurately reflecting real-world dynamics.","This formulation goes beyond the conventional rectangularity paradigm, offering new perspectives and expanding the analytical framework for robust RL.","We propose three distinct algorithms, each using varying levels of environmental information, and evaluate them extensively on continuous control benchmarks.","Our results demonstrate that these algorithms yield an efficient tradeoff between performance and robustness, outperforming traditional deep robust RL methods in time-constrained environments while preserving robustness in classical benchmarks.","This study revisits the prevailing assumptions in robust RL and opens new avenues for developing more practical and realistic RL applications."],"url":"http://arxiv.org/abs/2406.08395v1","category":"cs.LG"}
{"created":"2024-06-12 16:14:44","title":"Highest Probability Density Conformal Regions","abstract":"We propose a new method for finding the highest predictive density set or region using signed conformal inference. The proposed method is computationally efficient, while also carrying conformal coverage guarantees. We prove that under, mild regularity conditions, the conformal prediction set is asymptotically close to its oracle counterpart. The efficacy of the method is illustrated through simulations and real applications.","sentences":["We propose a new method for finding the highest predictive density set or region using signed conformal inference.","The proposed method is computationally efficient, while also carrying conformal coverage guarantees.","We prove that under, mild regularity conditions, the conformal prediction set is asymptotically close to its oracle counterpart.","The efficacy of the method is illustrated through simulations and real applications."],"url":"http://arxiv.org/abs/2406.08366v1","category":"stat.ME"}
{"created":"2024-06-12 16:13:37","title":"Weak coupling limit of KPZ with rougher than white noise","abstract":"We consider the KPZ equation in $1$ spatial dimension with noise that is rougher than white by an exponent $\\gamma>1/4$. Under a weak coupling limit, formally removing the nonlinearity from the equation, we show using regularity structures that the renormalised solutions converge to a Gaussian limit that is different from the solution of the linear part of the equation. The regime of this effect has a nontrivial overlap with the subcritical regime $\\gamma<1/2$.","sentences":["We consider the KPZ equation in $1$ spatial dimension with noise that is rougher than white by an exponent $\\gamma>1/4$. Under a weak coupling limit, formally removing the nonlinearity from the equation, we show using regularity structures that the renormalised solutions converge to a Gaussian limit that is different from the solution of the linear part of the equation.","The regime of this effect has a nontrivial overlap with the subcritical regime $\\gamma<1/2$."],"url":"http://arxiv.org/abs/2406.08364v1","category":"math.PR"}
{"created":"2024-06-12 15:30:46","title":"Bessel potentials and Green functions on pseudo-Euclidean spaces","abstract":"We review properties of Bessel potentials, that is, inverse Fourier transforms of (regularizations of) $\\frac{1}{(m^2+p^2)^{\\frac{\\mu}{2}}}$ on a pseudoEuclidean space with signature $(q,d-q)$. We are mostly interested in the Lorentzian signature $(1,d-1)$, and the case $\\mu=2$, related to the Klein-Gordon equation $(-\\Box+m^2)f=0$. We analyze properties of various ``two-point functions'', which play an important role in Quantum Field Theory, such as the retarded/advanced propagators or Feynman/antiFeynman propagators. We consistently use hypergeometric functions instead of Bessel functions, which makes most formulas much more transparent. We pay attention to distributional properties of various Bessel potentials. We include in our analysis the ``tachyonic case'', corresponding to the ``wrong'' sign in the Klein-Gordon equation.","sentences":["We review properties of Bessel potentials, that is, inverse Fourier transforms of (regularizations of) $\\frac{1}{(m^2+p^2)^{\\frac{\\mu}{2}}}$ on a pseudoEuclidean space with signature $(q,d-q)$.","We are mostly interested in the Lorentzian signature $(1,d-1)$, and the case $\\mu=2$, related to the Klein-Gordon equation $(-\\Box+m^2)f=0$. We analyze properties of various ``two-point functions'', which play an important role in Quantum Field Theory, such as the retarded/advanced propagators or Feynman/antiFeynman propagators.","We consistently use hypergeometric functions instead of Bessel functions, which makes most formulas much more transparent.","We pay attention to distributional properties of various Bessel potentials.","We include in our analysis the ``tachyonic case'', corresponding to the ``wrong'' sign in the Klein-Gordon equation."],"url":"http://arxiv.org/abs/2406.08327v1","category":"math-ph"}
{"created":"2024-06-12 15:09:03","title":"FSH: 3D Representation via Fibonacci Spherical Harmonics","abstract":"Spherical harmonics are a favorable technique for 3D representation, employing a frequency-based approach through the spherical harmonic transform (SHT). Typically, SHT is performed using equiangular sampling grids. However, these grids are non-uniform on spherical surfaces and exhibit local anisotropy, a common limitation in existing spherical harmonic decomposition methods. This paper proposes a 3D representation method using Fibonacci Spherical Harmonics (FSH). We introduce a spherical Fibonacci grid (SFG), which is more uniform than equiangular grids for SHT in the frequency domain. Our method employs analytical weights for SHT on SFG, effectively assigning sampling errors to spherical harmonic degrees higher than the recovered band-limited function. This provides a novel solution for spherical harmonic transformation on non-equiangular grids. The key advantages of our FSH method include: 1) With the same number of sampling points, SFG captures more features without bias compared to equiangular grids; 2) The root mean square error of 32-degree spherical harmonic coefficients is reduced by approximately 34.6\\% for SFG compared to equiangular grids; and 3) FSH offers more stable frequency domain representations, especially for rotating functions. FSH enhances the stability of frequency domain representations under rotational transformations. Its application in 3D shape reconstruction and 3D shape classification results in more accurate and robust representations.","sentences":["Spherical harmonics are a favorable technique for 3D representation, employing a frequency-based approach through the spherical harmonic transform (SHT).","Typically, SHT is performed using equiangular sampling grids.","However, these grids are non-uniform on spherical surfaces and exhibit local anisotropy, a common limitation in existing spherical harmonic decomposition methods.","This paper proposes a 3D representation method using Fibonacci Spherical Harmonics (FSH).","We introduce a spherical Fibonacci grid (SFG), which is more uniform than equiangular grids for SHT in the frequency domain.","Our method employs analytical weights for SHT on SFG, effectively assigning sampling errors to spherical harmonic degrees higher than the recovered band-limited function.","This provides a novel solution for spherical harmonic transformation on non-equiangular grids.","The key advantages of our FSH method include: 1) With the same number of sampling points, SFG captures more features without bias compared to equiangular grids; 2) The root mean square error of 32-degree spherical harmonic coefficients is reduced by approximately 34.6\\% for SFG compared to equiangular grids; and 3) FSH offers more stable frequency domain representations, especially for rotating functions.","FSH enhances the stability of frequency domain representations under rotational transformations.","Its application in 3D shape reconstruction and 3D shape classification results in more accurate and robust representations."],"url":"http://arxiv.org/abs/2406.08308v1","category":"cs.GR"}
{"created":"2024-06-12 14:50:40","title":"A New Class Biorthogonal Spline Wavelet for Image Edge Detection","abstract":"Spline wavelets have shown favorable characteristics for localizing in both time and frequency. In this paper, we propose a new biorthogonal cubic special spline wavelet (BCSSW), based on the Cohen-Daubechies-Feauveau wavelet construction method and the cubic special spline algorithm. BCSSW has better properties in compact support, symmetry, and frequency domain characteristics. However, current mainstream detection operators usually ignore the uncertain representation of regional pixels and global structures. To solve these problems, we propose a structural uncertainty-aware and multi-structure operator fusion detection algorithm (EDBSW) based on a new BCSSW spline wavelet. By constructing a spline wavelet that efficiently handles edge effects, we utilize structural uncertainty-aware modulus maxima to detect highly uncertain edge samples. The proposed wavelet detection operator utilizes the multi-structure morphological operator and fusion reconstruction strategy to effectively address anti-noise processing and edge information of different frequencies. Numerous experiments have demonstrated its excellent performance in reducing noise and capturing edge structure details.","sentences":["Spline wavelets have shown favorable characteristics for localizing in both time and frequency.","In this paper, we propose a new biorthogonal cubic special spline wavelet (BCSSW), based on the Cohen-Daubechies-Feauveau wavelet construction method and the cubic special spline algorithm.","BCSSW has better properties in compact support, symmetry, and frequency domain characteristics.","However, current mainstream detection operators usually ignore the uncertain representation of regional pixels and global structures.","To solve these problems, we propose a structural uncertainty-aware and multi-structure operator fusion detection algorithm (EDBSW) based on a new BCSSW spline wavelet.","By constructing a spline wavelet that efficiently handles edge effects, we utilize structural uncertainty-aware modulus maxima to detect highly uncertain edge samples.","The proposed wavelet detection operator utilizes the multi-structure morphological operator and fusion reconstruction strategy to effectively address anti-noise processing and edge information of different frequencies.","Numerous experiments have demonstrated its excellent performance in reducing noise and capturing edge structure details."],"url":"http://arxiv.org/abs/2406.08285v1","category":"cs.CV"}
{"created":"2024-06-12 14:34:33","title":"Protected gap closing and reopening in topological-insulator Josephson junctions","abstract":"In the seminal proposal by Fu and Kane, the superconducting proximity effect was predicted to transform the surface state of a topological insulator (TI) into a topological superconductor, forming a nonchiral 1D Majorana state within a linear Josephson junction on the TI surface. The hallmark of this 1D Majorana state is a robust gap closing as a function of the superconducting phase difference $\\varphi$ across the junction, which alternates in and out of the topological phase. These topological phase-transitions occur at $\\varphi = (2n+1)\\pi$ with integer $n$, leading to a $4\\pi$-periodicity of the ground state. While the $4\\pi$-periodicity has been indirectly inferred in the AC Josephson effect, the direct observation of the 1D Majorana state in a TI Josephson junction has remained contentious. Here, we report the direct observation of topological phase-transitions in a TI Josephson junction, where the local density of states is probed via tunnel contacts and $\\varphi$ is controlled by a flux loop. The observed transitions are independent of the chemical potential, reinforcing their topological origin. Under an applied perpendicular magnetic field, Josephson vortices form, making $\\varphi$ position-dependent. In this case, the gap closing occurs locally at the Josephson vortex cores where $\\varphi = (2n+1)\\pi$, which we also observe. Our findings provide direct confirmation of the Fu-Kane proposal and robust evidence for the emergence of topological superconductivity in a TI Josephson junction.","sentences":["In the seminal proposal by Fu and Kane, the superconducting proximity effect was predicted to transform the surface state of a topological insulator (TI) into a topological superconductor, forming a nonchiral 1D Majorana state within a linear Josephson junction on the TI surface.","The hallmark of this 1D Majorana state is a robust gap closing as a function of the superconducting phase difference $\\varphi$ across the junction, which alternates in and out of the topological phase.","These topological phase-transitions occur at $\\varphi = (2n+1)\\pi$ with integer $n$, leading to a $4\\pi$-periodicity of the ground state.","While the $4\\pi$-periodicity has been indirectly inferred in the AC Josephson effect, the direct observation of the 1D Majorana state in a TI Josephson junction has remained contentious.","Here, we report the direct observation of topological phase-transitions in a TI Josephson junction, where the local density of states is probed via tunnel contacts and $\\varphi$ is controlled by a flux loop.","The observed transitions are independent of the chemical potential, reinforcing their topological origin.","Under an applied perpendicular magnetic field, Josephson vortices form, making $\\varphi$ position-dependent.","In this case, the gap closing occurs locally at the Josephson vortex cores where $\\varphi = (2n+1)\\pi$, which we also observe.","Our findings provide direct confirmation of the Fu-Kane proposal and robust evidence for the emergence of topological superconductivity in a TI Josephson junction."],"url":"http://arxiv.org/abs/2406.08265v1","category":"cond-mat.supr-con"}
{"created":"2024-06-12 14:13:06","title":"Measurement and assignment of J = 5 to 9 rotational energy levels in the 9070-9370 cm$^{-1}$ range of methane using optical frequency comb double-resonance spectroscopy","abstract":"We use optical-optical double-resonance (OODR) spectroscopy with a continuous wave (CW) pump and a cavity-enhanced frequency comb probe to measure high rotational energy levels of methane in the upper part of the triacontad polyad (P6). A high-power CW optical parametric oscillator, tunable around 3000 cm$^{-1}$, is consecutively locked to the P(7, A$_2$), Q(7, A$_2$), R(7, A$_2$), and Q(6, F$_2$) transitions in the ${\\nu}$$_3$ band, and a comb covering the 5800-6100 cm$^{-1}$ range probes sub-Doppler ladder-type transitions from the pumped levels with J' = 6 to 8, respectively. We report 118 probe transitions in the 3${\\nu}$$_3$ $\\leftarrow$ ${\\nu}$$_3$ spectral range with uncertainties down to 300 kHz (1 x 10$^{-5}$ cm$^{-1}$), reaching 84 unique final states in the 9070-9370 cm$^{-1}$ range with rotational quantum numbers J between 5 and 9. We assign these states using combination differences and by comparison to theoretical predictions from a new ab initio-based effective Hamiltonian and dipole moment operator. This is the first line-by-line experimental verification of theoretical predictions for these hot-band transitions, and we find a better agreement of transition wavenumbers with the new calculations compared to the TheoReTS/HITEMP and ExoMol databases. We also compare the relative intensities and find an overall good agreement with all three sets of predictions. Finally, we report the wavenumbers of 27 transitions in the 2${\\nu}$$_3$ spectral range, observed as V-type transitions from the ground state, and compare them to the new Hamiltonian, HITRAN2020, ExoMol and the WKMLC line lists.","sentences":["We use optical-optical double-resonance (OODR) spectroscopy with a continuous wave (CW) pump and a cavity-enhanced frequency comb probe to measure high rotational energy levels of methane in the upper part of the triacontad polyad (P6).","A high-power CW optical parametric oscillator, tunable around 3000 cm$^{-1}$, is consecutively locked to the P(7, A$_2$), Q(7, A$_2$), R(7, A$_2$), and Q(6, F$_2$) transitions in the ${\\nu}$$_3$ band, and a comb covering the 5800-6100 cm$^{-1}$ range probes sub-Doppler ladder-type transitions from the pumped levels with J' = 6 to 8, respectively.","We report 118 probe transitions in the 3${\\nu}$$_3$ $\\leftarrow$ ${\\nu}$$_3$ spectral range with uncertainties down to 300 kHz (1 x 10$^{-5}$ cm$^{-1}$), reaching 84 unique final states in the 9070-9370 cm$^{-1}$ range with rotational quantum numbers J between 5 and 9.","We assign these states using combination differences and by comparison to theoretical predictions from a new ab initio-based effective Hamiltonian and dipole moment operator.","This is the first line-by-line experimental verification of theoretical predictions for these hot-band transitions, and we find a better agreement of transition wavenumbers with the new calculations compared to the TheoReTS/HITEMP and ExoMol databases.","We also compare the relative intensities and find an overall good agreement with all three sets of predictions.","Finally, we report the wavenumbers of 27 transitions in the 2${\\nu}$$_3$ spectral range, observed as V-type transitions from the ground state, and compare them to the new Hamiltonian, HITRAN2020, ExoMol and the WKMLC line lists."],"url":"http://arxiv.org/abs/2406.08244v1","category":"physics.chem-ph"}
{"created":"2024-06-12 14:01:12","title":"MaIL: Improving Imitation Learning with Mamba","abstract":"This work introduces Mamba Imitation Learning (MaIL), a novel imitation learning (IL) architecture that offers a computationally efficient alternative to state-of-the-art (SoTA) Transformer policies. Transformer-based policies have achieved remarkable results due to their ability in handling human-recorded data with inherently non-Markovian behavior. However, their high performance comes with the drawback of large models that complicate effective training. While state space models (SSMs) have been known for their efficiency, they were not able to match the performance of Transformers. Mamba significantly improves the performance of SSMs and rivals against Transformers, positioning it as an appealing alternative for IL policies. MaIL leverages Mamba as a backbone and introduces a formalism that allows using Mamba in the encoder-decoder structure. This formalism makes it a versatile architecture that can be used as a standalone policy or as part of a more advanced architecture, such as a diffuser in the diffusion process. Extensive evaluations on the LIBERO IL benchmark and three real robot experiments show that MaIL: i) outperforms Transformers in all LIBERO tasks, ii) achieves good performance even with small datasets, iii) is able to effectively process multi-modal sensory inputs, iv) is more robust to input noise compared to Transformers.","sentences":["This work introduces Mamba Imitation Learning (MaIL), a novel imitation learning (IL) architecture that offers a computationally efficient alternative to state-of-the-art (SoTA) Transformer policies.","Transformer-based policies have achieved remarkable results due to their ability in handling human-recorded data with inherently non-Markovian behavior.","However, their high performance comes with the drawback of large models that complicate effective training.","While state space models (SSMs) have been known for their efficiency, they were not able to match the performance of Transformers.","Mamba significantly improves the performance of SSMs and rivals against Transformers, positioning it as an appealing alternative for IL policies.","MaIL leverages Mamba as a backbone and introduces a formalism that allows using Mamba in the encoder-decoder structure.","This formalism makes it a versatile architecture that can be used as a standalone policy or as part of a more advanced architecture, such as a diffuser in the diffusion process.","Extensive evaluations on the LIBERO IL benchmark and three real robot experiments show that MaIL: i) outperforms Transformers in all LIBERO tasks, ii) achieves good performance even with small datasets, iii) is able to effectively process multi-modal sensory inputs, iv) is more robust to input noise compared to Transformers."],"url":"http://arxiv.org/abs/2406.08234v1","category":"cs.LG"}
{"created":"2024-06-12 13:09:59","title":"Category-level Neural Field for Reconstruction of Partially Observed Objects in Indoor Environment","abstract":"Neural implicit representation has attracted attention in 3D reconstruction through various success cases. For further applications such as scene understanding or editing, several works have shown progress towards object compositional reconstruction. Despite their superior performance in observed regions, their performance is still limited in reconstructing objects that are partially observed. To better treat this problem, we introduce category-level neural fields that learn meaningful common 3D information among objects belonging to the same category present in the scene. Our key idea is to subcategorize objects based on their observed shape for better training of the category-level model. Then we take advantage of the neural field to conduct the challenging task of registering partially observed objects by selecting and aligning against representative objects selected by ray-based uncertainty. Experiments on both simulation and real-world datasets demonstrate that our method improves the reconstruction of unobserved parts for several categories.","sentences":["Neural implicit representation has attracted attention in 3D reconstruction through various success cases.","For further applications such as scene understanding or editing, several works have shown progress towards object compositional reconstruction.","Despite their superior performance in observed regions, their performance is still limited in reconstructing objects that are partially observed.","To better treat this problem, we introduce category-level neural fields that learn meaningful common 3D information among objects belonging to the same category present in the scene.","Our key idea is to subcategorize objects based on their observed shape for better training of the category-level model.","Then we take advantage of the neural field to conduct the challenging task of registering partially observed objects by selecting and aligning against representative objects selected by ray-based uncertainty.","Experiments on both simulation and real-world datasets demonstrate that our method improves the reconstruction of unobserved parts for several categories."],"url":"http://arxiv.org/abs/2406.08176v1","category":"cs.CV"}
{"created":"2024-06-12 12:55:25","title":"Double pion photoproduction off nucleons in covariant chiral perturbation theory","abstract":"The double pion photoproduction off nucleons near threshold is analyzed in a covariant baryon chiral perturbation theory up to next to leading order, where the $\\Delta(1232)$, $N^*(1400)$ and $\\rho(770)$ resonances are included as explicit degrees of freedom. For the process $\\gamma p \\to \\pi^+ \\pi^0 n$, the chiral results of total cross sections, invariant-mass distributions and beam-helicity asymmetry are in good agreement with the experimental data within uncertainties. For the process $\\gamma p \\to \\pi^0 \\pi^0 p$, the prediction of total cross section deviates from the existing experimental data. Once the final-state interaction of $\\pi \\pi$ in the isoscalar S-wave channel is taken into account, a good description of the cross section is achieved. The effect of the Roper resonance always turns out be negligible, and hence can be thrown away in future study of this process.","sentences":["The double pion photoproduction off nucleons near threshold is analyzed in a covariant baryon chiral perturbation theory up to next to leading order, where the $\\Delta(1232)$, $N^*(1400)$ and $\\rho(770)$ resonances are included as explicit degrees of freedom.","For the process $\\gamma p \\to \\pi^+ \\pi^0 n$, the chiral results of total cross sections, invariant-mass distributions and beam-helicity asymmetry are in good agreement with the experimental data within uncertainties.","For the process $\\gamma p \\to \\pi^0 \\pi^0 p$, the prediction of total cross section deviates from the existing experimental data.","Once the final-state interaction of $\\pi \\pi$ in the isoscalar S-wave channel is taken into account, a good description of the cross section is achieved.","The effect of the Roper resonance always turns out be negligible, and hence can be thrown away in future study of this process."],"url":"http://arxiv.org/abs/2406.08165v1","category":"hep-ph"}
{"created":"2024-06-12 12:47:17","title":"Superconducting diode effect under time reversal symmetry","abstract":"In noncentrosymmetric superconductors, superconducting and normal conductions can interchange based on the current flow direction. This effect is termed a superconducting diode effect (SDE), which is a focal point of recent research. The broken inversion and time reversal symmetry is believed to be the requirements of SDE but their intrinsic role has remained elusive. Here, we report strain-controlled SDEs in a layered trigonal superconductor, PbTaSe2. The SDE was found exclusively in a strained device with its absence in an unstrained device, despite that it is allowed in unstrained trigonal structure. Moreover, the zero-field or magnetic field-even (magnetic field-odd) SDE is observed when the strain and current are along armchair (zigzag) direction The results unambiguously demonstrate the intrinsic SDE under time-reversal symmetry and the critical role of strain-induced electric polarization.","sentences":["In noncentrosymmetric superconductors, superconducting and normal conductions can interchange based on the current flow direction.","This effect is termed a superconducting diode effect (SDE), which is a focal point of recent research.","The broken inversion and time reversal symmetry is believed to be the requirements of SDE but their intrinsic role has remained elusive.","Here, we report strain-controlled SDEs in a layered trigonal superconductor, PbTaSe2.","The SDE was found exclusively in a strained device with its absence in an unstrained device, despite that it is allowed in unstrained trigonal structure.","Moreover, the zero-field or magnetic field-even (magnetic field-odd) SDE is observed when the strain and current are along armchair (zigzag) direction The results unambiguously demonstrate the intrinsic SDE under time-reversal symmetry and the critical role of strain-induced electric polarization."],"url":"http://arxiv.org/abs/2406.08157v1","category":"cond-mat.supr-con"}
{"created":"2024-06-12 12:34:44","title":"Constraining the axial-vector X17 interpretation with ${}^{12}$C data","abstract":"Recent findings of an unexpected, narrow resonance in the $e^+e^-$ decay spectra of excited states of $^8$Be, $^4$He and $^{12}$C by the ATOMKI collaboration have received considerable experimental and theoretical attention, whereby a new, 17-MeV vector-like or axial-vector-like boson termed X17 was conjectured as an explanation of the anomaly. Further analysis of all existing constraints disfavors a vector X17 scenario. For a similar analysis of the axial-vector scenario, a calculation of the reduced matrix element of a spin-dipole operator between the excited nuclear state ${}^{12}$C(17.23) and the carbon ground state is required. In the present work, we compute the aforementioned reduced matrix element under the assumption that the state ${}^{12}$C(17.23) is well represented by the $2s_{1/2}1p^{-1}_{3/2}$ particle-hole shell-model excitation of the ground state, as supported by experimental data. Within such a framework, our results indicate that, like the vector scenario, the axial-vector interpretation of X17 shows strong tensions with the other existing constraints on the nucleon coupling of a conjectured X17.","sentences":["Recent findings of an unexpected, narrow resonance in the $e^+e^-$ decay spectra of excited states of $^8$Be, $^4$He and $^{12}$C by the ATOMKI collaboration have received considerable experimental and theoretical attention, whereby a new, 17-MeV vector-like or axial-vector-like boson termed X17 was conjectured as an explanation of the anomaly.","Further analysis of all existing constraints disfavors a vector X17 scenario.","For a similar analysis of the axial-vector scenario, a calculation of the reduced matrix element of a spin-dipole operator between the excited nuclear state ${}^{12}$C(17.23) and the carbon ground state is required.","In the present work, we compute the aforementioned reduced matrix element under the assumption that the state ${}^{12}$C(17.23) is well represented by the $2s_{1/2}1p^{-1}_{3/2}$ particle-hole shell-model excitation of the ground state, as supported by experimental data.","Within such a framework, our results indicate that, like the vector scenario, the axial-vector interpretation of X17 shows strong tensions with the other existing constraints on the nucleon coupling of a conjectured X17."],"url":"http://arxiv.org/abs/2406.08143v1","category":"hep-ph"}
{"created":"2024-06-12 11:51:31","title":"Massive 1D Dirac Line, Solitons and Reversible Manipulation on the Surface of a Prototype Obstructed Atomic Insulator, Silicon","abstract":"Topologically trivial insulators can be classified into atomic insulators (AIs) and obstructed atomic insulators (OAIs) depending on whether the Wannier charge centers are localized or not at spatial positions occupied by atoms. An OAI can possess unusual properties such as surface states along certain crystalline surfaces, which advantageously appear in materials with much larger bulk energy gap than topological insulators, making them more attractive for potential applications. In this work, we show that a well-known crystal, silicon (Si) is a model OAI, which naturally explains some of Si's unusual properties such as its famous (111) surface states. On this surface, using angle resolved photoemission spectroscopy (ARPES), we reveal sharp quasi-1D massive Dirac line dispersions; we also observe, using scanning tunneling microscopy/spectroscopy (STM/STS), topological solitons at the interface of the two atomic chains. Remarkably, we show that the different chain domains can be reversibly switched at the nanometer scale, suggesting the application potential in ultra-high density storage devices.","sentences":["Topologically trivial insulators can be classified into atomic insulators (AIs) and obstructed atomic insulators (OAIs) depending on whether the Wannier charge centers are localized or not at spatial positions occupied by atoms.","An OAI can possess unusual properties such as surface states along certain crystalline surfaces, which advantageously appear in materials with much larger bulk energy gap than topological insulators, making them more attractive for potential applications.","In this work, we show that a well-known crystal, silicon (Si) is a model OAI, which naturally explains some of Si's unusual properties such as its famous (111) surface states.","On this surface, using angle resolved photoemission spectroscopy (ARPES), we reveal sharp quasi-1D massive Dirac line dispersions; we also observe, using scanning tunneling microscopy/spectroscopy (STM/STS), topological solitons at the interface of the two atomic chains.","Remarkably, we show that the different chain domains can be reversibly switched at the nanometer scale, suggesting the application potential in ultra-high density storage devices."],"url":"http://arxiv.org/abs/2406.08114v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-12 11:20:45","title":"Substochastic operators in symmetric spaces","abstract":"First, we solve a crucial problem under which conditions increasing uniform K-monotonicity is equivalent to lower locally uniform K-monotonicity. Next, we investigate properties of substochastic operators on $L^1+L^\\infty$ with applications. Namely, we show that a countable infinite combination of substochastic operators is also substochastic. Using K-monotonicity properties, we prove several theorems devoted to the convergence of the sequence of substochastic operators in the norm of a symmetric space E under addition assumption on E. In our final discussion we focus on compactness of admissible operators for arbitrary Banach couples.","sentences":["First, we solve a crucial problem under which conditions increasing uniform K-monotonicity is equivalent to lower locally uniform K-monotonicity.","Next, we investigate properties of substochastic operators on $L^1+L^\\infty$ with applications.","Namely, we show that a countable infinite combination of substochastic operators is also substochastic.","Using K-monotonicity properties, we prove several theorems devoted to the convergence of the sequence of substochastic operators in the norm of a symmetric space E under addition assumption on E.","In our final discussion we focus on compactness of admissible operators for arbitrary Banach couples."],"url":"http://arxiv.org/abs/2406.08095v1","category":"math.FA"}
{"created":"2024-06-12 09:58:42","title":"A Robust Pipeline for Classification and Detection of Bleeding Frames in Wireless Capsule Endoscopy using Swin Transformer and RT-DETR","abstract":"In this paper, we present our approach to the Auto WCEBleedGen Challenge V2 2024. Our solution combines the Swin Transformer for the initial classification of bleeding frames and RT-DETR for further detection of bleeding in Wireless Capsule Endoscopy (WCE), enhanced by a series of image preprocessing steps. These steps include converting images to Lab colour space, applying Contrast Limited Adaptive Histogram Equalization (CLAHE) for better contrast, and using Gaussian blur to suppress artefacts. The Swin Transformer utilizes a tiered architecture with shifted windows to efficiently manage self-attention calculations, focusing on local windows while enabling cross-window interactions. RT-DETR features an efficient hybrid encoder for fast processing of multi-scale features and an uncertainty-minimal query selection for enhanced accuracy. The class activation maps by Ablation-CAM are plausible to the model's decisions. On the validation set, this approach achieves a classification accuracy of 98.5% (best among the other state-of-the-art models) compared to 91.7% without any pre-processing and an $\\text{AP}_{50}$ of 66.7% compared to 65.0% with state-of-the-art YOLOv8. On the test set, this approach achieves a classification accuracy and F1 score of 87.0% and 89.0% respectively.","sentences":["In this paper, we present our approach to the Auto WCEBleedGen Challenge V2 2024.","Our solution combines the Swin Transformer for the initial classification of bleeding frames and RT-DETR for further detection of bleeding in Wireless Capsule Endoscopy (WCE), enhanced by a series of image preprocessing steps.","These steps include converting images to Lab colour space, applying Contrast Limited Adaptive Histogram Equalization (CLAHE) for better contrast, and using Gaussian blur to suppress artefacts.","The Swin Transformer utilizes a tiered architecture with shifted windows to efficiently manage self-attention calculations, focusing on local windows while enabling cross-window interactions.","RT-DETR features an efficient hybrid encoder for fast processing of multi-scale features and an uncertainty-minimal query selection for enhanced accuracy.","The class activation maps by Ablation-CAM are plausible to the model's decisions.","On the validation set, this approach achieves a classification accuracy of 98.5% (best among the other state-of-the-art models) compared to 91.7% without any pre-processing and an $\\text{AP}_{50}$ of 66.7% compared to 65.0% with state-of-the-art YOLOv8.","On the test set, this approach achieves a classification accuracy and F1 score of 87.0% and 89.0% respectively."],"url":"http://arxiv.org/abs/2406.08046v1","category":"cs.CV"}
{"created":"2024-06-12 09:41:12","title":"Beyond the Mean: Differentially Private Prototypes for Private Transfer Learning","abstract":"Machine learning (ML) models have been shown to leak private information from their training datasets. Differential Privacy (DP), typically implemented through the differential private stochastic gradient descent algorithm (DP-SGD), has become the standard solution to bound leakage from the models. Despite recent improvements, DP-SGD-based approaches for private learning still usually struggle in the high privacy ($\\varepsilon\\le1)$ and low data regimes, and when the private training datasets are imbalanced. To overcome these limitations, we propose Differentially Private Prototype Learning (DPPL) as a new paradigm for private transfer learning. DPPL leverages publicly pre-trained encoders to extract features from private data and generates DP prototypes that represent each private class in the embedding space and can be publicly released for inference. Since our DP prototypes can be obtained from only a few private training data points and without iterative noise addition, they offer high-utility predictions and strong privacy guarantees even under the notion of pure DP. We additionally show that privacy-utility trade-offs can be further improved when leveraging the public data beyond pre-training of the encoder: in particular, we can privately sample our DP prototypes from the publicly available data points used to train the encoder. Our experimental evaluation with four state-of-the-art encoders, four vision datasets, and under different data and imbalancedness regimes demonstrate DPPL's high performance under strong privacy guarantees in challenging private learning setups.","sentences":["Machine learning (ML) models have been shown to leak private information from their training datasets.","Differential Privacy (DP), typically implemented through the differential private stochastic gradient descent algorithm (DP-SGD), has become the standard solution to bound leakage from the models.","Despite recent improvements, DP-SGD-based approaches for private learning still usually struggle in the high privacy ($\\varepsilon\\le1)$ and low data regimes, and when the private training datasets are imbalanced.","To overcome these limitations, we propose Differentially Private Prototype Learning (DPPL) as a new paradigm for private transfer learning.","DPPL leverages publicly pre-trained encoders to extract features from private data and generates DP prototypes that represent each private class in the embedding space and can be publicly released for inference.","Since our DP prototypes can be obtained from only a few private training data points and without iterative noise addition, they offer high-utility predictions and strong privacy guarantees even under the notion of pure DP.","We additionally show that privacy-utility trade-offs can be further improved when leveraging the public data beyond pre-training of the encoder: in particular, we can privately sample our DP prototypes from the publicly available data points used to train the encoder.","Our experimental evaluation with four state-of-the-art encoders, four vision datasets, and under different data and imbalancedness regimes demonstrate DPPL's high performance under strong privacy guarantees in challenging private learning setups."],"url":"http://arxiv.org/abs/2406.08039v1","category":"cs.LG"}
{"created":"2024-06-12 09:39:18","title":"Adaptively Bypassing Vision Transformer Blocks for Efficient Visual Tracking","abstract":"Empowered by transformer-based models, visual tracking has advanced significantly. However, the slow speed of current trackers limits their applicability on devices with constrained computational resources. To address this challenge, we introduce ABTrack, an adaptive computation framework that adaptively bypassing transformer blocks for efficient visual tracking. The rationale behind ABTrack is rooted in the observation that semantic features or relations do not uniformly impact the tracking task across all abstraction levels. Instead, this impact varies based on the characteristics of the target and the scene it occupies. Consequently, disregarding insignificant semantic features or relations at certain abstraction levels may not significantly affect the tracking accuracy. We propose a Bypass Decision Module (BDM) to determine if a transformer block should be bypassed, which adaptively simplifies the architecture of ViTs and thus speeds up the inference process. To counteract the time cost incurred by the BDMs and further enhance the efficiency of ViTs, we innovatively adapt a pruning technique to reduce the dimension of the latent representation of tokens in each transformer block. Extensive experiments on multiple tracking benchmarks validate the effectiveness and generality of the proposed method and show that it achieves state-of-the-art performance. Code is released at: \\href{https://github.com/1HykhqV3rU/ABTrack}","sentences":["Empowered by transformer-based models, visual tracking has advanced significantly.","However, the slow speed of current trackers limits their applicability on devices with constrained computational resources.","To address this challenge, we introduce ABTrack, an adaptive computation framework that adaptively bypassing transformer blocks for efficient visual tracking.","The rationale behind ABTrack is rooted in the observation that semantic features or relations do not uniformly impact the tracking task across all abstraction levels.","Instead, this impact varies based on the characteristics of the target and the scene it occupies.","Consequently, disregarding insignificant semantic features or relations at certain abstraction levels may not significantly affect the tracking accuracy.","We propose a Bypass Decision Module (BDM) to determine if a transformer block should be bypassed, which adaptively simplifies the architecture of ViTs and thus speeds up the inference process.","To counteract the time cost incurred by the BDMs and further enhance the efficiency of ViTs, we innovatively adapt a pruning technique to reduce the dimension of the latent representation of tokens in each transformer block.","Extensive experiments on multiple tracking benchmarks validate the effectiveness and generality of the proposed method and show that it achieves state-of-the-art performance.","Code is released at: \\href{https://github.com/1HykhqV3rU/ABTrack}"],"url":"http://arxiv.org/abs/2406.08037v1","category":"cs.CV"}
{"created":"2024-06-12 09:24:53","title":"Photoproduction in general-purpose event generators","abstract":"We present a comparison of three different general-purpose Monte Carlo event generators, Herwig, Pythia, and Sherpa, with respect to the simulation of photoproduction. We outline the default inputs, implementation differences and compare the results at different stages of the event generation. We find that, despite a similar starting point, the final cross sections do have some differences related to different non-perturbative inputs. We compare the simulations with experimental data for jet production in LEP and HERA and find that all generators provide a decent desription of the data within the considered uncertainties. We also present predictions for the upcoming EIC for jet observables and event shapes and conclude that accurate simulations will require further phenomenological advances.","sentences":["We present a comparison of three different general-purpose Monte Carlo event generators, Herwig, Pythia, and Sherpa, with respect to the simulation of photoproduction.","We outline the default inputs, implementation differences and compare the results at different stages of the event generation.","We find that, despite a similar starting point, the final cross sections do have some differences related to different non-perturbative inputs.","We compare the simulations with experimental data for jet production in LEP and HERA and find that all generators provide a decent desription of the data within the considered uncertainties.","We also present predictions for the upcoming EIC for jet observables and event shapes and conclude that accurate simulations will require further phenomenological advances."],"url":"http://arxiv.org/abs/2406.08026v1","category":"hep-ph"}
{"created":"2024-06-12 09:21:28","title":"Generalizable Disaster Damage Assessment via Change Detection with Vision Foundation Model","abstract":"The increasing frequency and intensity of natural disasters demand more sophisticated approaches for rapid and precise damage assessment. To tackle this issue, researchers have developed various methods on disaster benchmark datasets from satellite imagery to aid in detecting disaster damage. However, the diverse nature of geographical landscapes and disasters makes it challenging to apply existing methods to regions unseen during training. We present DAVI (Disaster Assessment with VIsion foundation model), which overcomes domain disparities and detects structural damage (e.g., building) without requiring ground-truth labels of the target region. DAVI integrates task-specific knowledge from a model trained on source regions with an image segmentation foundation model to generate pseudo labels of possible damage in the target region. It then employs a two-stage refinement process, targeting both the pixel and overall image, to more accurately pinpoint changes in disaster-struck areas based on before-and-after images. Comprehensive evaluations demonstrate that DAVI achieves exceptional performance across diverse terrains (e.g., USA and Mexico) and disaster types (e.g., wildfires, hurricanes, and earthquakes). This confirms its robustness in assessing disaster impact without dependence on ground-truth labels.","sentences":["The increasing frequency and intensity of natural disasters demand more sophisticated approaches for rapid and precise damage assessment.","To tackle this issue, researchers have developed various methods on disaster benchmark datasets from satellite imagery to aid in detecting disaster damage.","However, the diverse nature of geographical landscapes and disasters makes it challenging to apply existing methods to regions unseen during training.","We present DAVI (Disaster Assessment with VIsion foundation model), which overcomes domain disparities and detects structural damage (e.g., building) without requiring ground-truth labels of the target region.","DAVI integrates task-specific knowledge from a model trained on source regions with an image segmentation foundation model to generate pseudo labels of possible damage in the target region.","It then employs a two-stage refinement process, targeting both the pixel and overall image, to more accurately pinpoint changes in disaster-struck areas based on before-and-after images.","Comprehensive evaluations demonstrate that DAVI achieves exceptional performance across diverse terrains (e.g., USA and Mexico) and disaster types (e.g., wildfires, hurricanes, and earthquakes).","This confirms its robustness in assessing disaster impact without dependence on ground-truth labels."],"url":"http://arxiv.org/abs/2406.08020v1","category":"cs.CV"}
{"created":"2024-06-12 09:21:13","title":"Assessing Extreme Risk using Stochastic Simulation of Extremes","abstract":"Risk management is particularly concerned with extreme events, but analysing these events is often hindered by the scarcity of data, especially in a multivariate context. This data scarcity complicates risk management efforts. Various tools can assess the risk posed by extreme events, even under extraordinary circumstances. This paper studies the evaluation of univariate risk for a given risk factor using metrics that account for its asymptotic dependence on other risk factors. Data availability is crucial, particularly for extreme events where it is often limited by the nature of the phenomenon itself, making estimation challenging. To address this issue, two non-parametric simulation algorithms based on multivariate extreme theory are developed. These algorithms aim to extend a sample of extremes jointly and conditionally for asymptotically dependent variables using stochastic simulation and multivariate Generalised Pareto Distributions. The approach is illustrated with numerical analyses of both simulated and real data to assess the accuracy of extreme risk metric estimations.","sentences":["Risk management is particularly concerned with extreme events, but analysing these events is often hindered by the scarcity of data, especially in a multivariate context.","This data scarcity complicates risk management efforts.","Various tools can assess the risk posed by extreme events, even under extraordinary circumstances.","This paper studies the evaluation of univariate risk for a given risk factor using metrics that account for its asymptotic dependence on other risk factors.","Data availability is crucial, particularly for extreme events where it is often limited by the nature of the phenomenon itself, making estimation challenging.","To address this issue, two non-parametric simulation algorithms based on multivariate extreme theory are developed.","These algorithms aim to extend a sample of extremes jointly and conditionally for asymptotically dependent variables using stochastic simulation and multivariate Generalised Pareto Distributions.","The approach is illustrated with numerical analyses of both simulated and real data to assess the accuracy of extreme risk metric estimations."],"url":"http://arxiv.org/abs/2406.08019v1","category":"stat.ME"}
{"created":"2024-06-12 08:54:17","title":"Enhancing phase sensitivity in Mach-Zehnder interferometer with various detection schemes using SU(1,1) coherent states","abstract":"Improving interferometric phase sensitivity is crucial for high-precision measurements in rapidly developing quantum technologies. The Mach-Zehnder interferometer (MZI) is a versatile tool for analyzing this phenomenon. By splitting and recombining a light beam using beam splitters, MZIs allow for precise phase sensitivity analysis using tools like the quantum Cram\\'er-Rao bound (QCRB) and the quantum Fisher information (QFI). This paper analyzes the phase sensitivity of a MZI in various scenarios using different detection schemes and input states. We compare the single- and two-parameter quantum estimation and their associated QCRB for three phase-shift situations: in both arms, only in the upper arm (asymmetric), and in both arms symmetrically. We then investigate the phase sensitivity under three detection schemes: difference intensity, single-mode intensity, and balanced homodyne. Additionally, we explore the use of Perelomov and Barut-Girardello coherent states, two types of SU(1,1) coherent states, in all scenarios. Notably, we demonstrate that under optimal conditions, all detection schemes can achieve the QCRB by utilizing entangled SU(1,1) coherent states as input states.","sentences":["Improving interferometric phase sensitivity is crucial for high-precision measurements in rapidly developing quantum technologies.","The Mach-Zehnder interferometer (MZI) is a versatile tool for analyzing this phenomenon.","By splitting and recombining a light beam using beam splitters, MZIs allow for precise phase sensitivity analysis using tools like the quantum Cram\\'er-Rao bound (QCRB) and the quantum Fisher information (QFI).","This paper analyzes the phase sensitivity of a MZI in various scenarios using different detection schemes and input states.","We compare the single- and two-parameter quantum estimation and their associated QCRB for three phase-shift situations: in both arms, only in the upper arm (asymmetric), and in both arms symmetrically.","We then investigate the phase sensitivity under three detection schemes: difference intensity, single-mode intensity, and balanced homodyne.","Additionally, we explore the use of Perelomov and Barut-Girardello coherent states, two types of SU(1,1) coherent states, in all scenarios.","Notably, we demonstrate that under optimal conditions, all detection schemes can achieve the QCRB by utilizing entangled SU(1,1) coherent states as input states."],"url":"http://arxiv.org/abs/2406.08007v1","category":"quant-ph"}
{"created":"2024-06-12 08:53:09","title":"A new type of $f(\\mathcal{T})$ gravity from Barrow entropy","abstract":"In this work, two originally separate adjustments for the Friedmann equations are concurrently considered. Firstly, the fractal structure of the black hole horizon region is imposed by the Barrow entropy. The second adjustment is the $f(\\mathcal{T})$ gravity, which is based on a teleparallel framework generalization of the Einstein-Hilbert action, where $\\mathcal{T}$ is the scalar torsion. This can be considered as a Barrow entropy modification of the $f(\\mathcal{T})$ gravity thus yielding a new model. Gravity thermodynamics hypothesis principles are used to integrate these two models under a unified framework. We derive the modified Friedmann equation and note the corrections obtained. To understand the implications of such dual modifications, an application is analyzed and a particular $f(\\mathcal{T})$ toy model is chosen for the purpose. The equation of the state parameter of the resulting model, the dimensionless density parameters of matter and dark energy, and the deceleration parameter are explored to check the viability of the new model. A discussion of the dynamic evolution of the universe follows from these results. It is seen that the results comply with the observations. The newly developed model is promising and demands further study.","sentences":["In this work, two originally separate adjustments for the Friedmann equations are concurrently considered.","Firstly, the fractal structure of the black hole horizon region is imposed by the Barrow entropy.","The second adjustment is the $f(\\mathcal{T})$ gravity, which is based on a teleparallel framework generalization of the Einstein-Hilbert action, where $\\mathcal{T}$ is the scalar torsion.","This can be considered as a Barrow entropy modification of the $f(\\mathcal{T})$ gravity thus yielding a new model.","Gravity thermodynamics hypothesis principles are used to integrate these two models under a unified framework.","We derive the modified Friedmann equation and note the corrections obtained.","To understand the implications of such dual modifications, an application is analyzed and a particular $f(\\mathcal{T})$ toy model is chosen for the purpose.","The equation of the state parameter of the resulting model, the dimensionless density parameters of matter and dark energy, and the deceleration parameter are explored to check the viability of the new model.","A discussion of the dynamic evolution of the universe follows from these results.","It is seen that the results comply with the observations.","The newly developed model is promising and demands further study."],"url":"http://arxiv.org/abs/2406.08006v1","category":"gr-qc"}
{"created":"2024-06-12 08:46:48","title":"Diffuse fraction as a tool for exploring the sensitivity of parametric clear-sky models to changing aerosol conditions","abstract":"Aerosols' impact on the performance of a clear-sky solar irradiance model is often evaluated from the perspective of the overall accuracy of estimates. This study assesses the aerosol role in clear-sky solar irradiance modelling from a totally different perspective, namely the ability of a model to accurately separate global solar irradiance into its fundamental direct and diffuse components. In an innovative approach, the analysis is focused on the sensitivity of parametric solar irradiance models to changes in aerosol conditions. A new measure, the aerosol influence quantifier, is introduced for assessing the correlation strength between the relative variation in clear-sky diffuse fraction and the relative variation in atmospheric aerosol load. The effectiveness of the aerosol influence quantifier is explored as a tool for assessing the sensitivity of three parametric clear-sky models to variations in input aerosol data. A rather surprising result is reported, e.g. if high quality aerosol data are not available, one could potentially maximize the estimate's accuracy by using a parametric clear-sky solar irradiance model with low sensitivity to aerosol variations. In such cases, it is shown that the aerosol influence quantifier can be used as a tool for identifying an adequate model for input data with a given uncertainty.","sentences":["Aerosols' impact on the performance of a clear-sky solar irradiance model is often evaluated from the perspective of the overall accuracy of estimates.","This study assesses the aerosol role in clear-sky solar irradiance modelling from a totally different perspective, namely the ability of a model to accurately separate global solar irradiance into its fundamental direct and diffuse components.","In an innovative approach, the analysis is focused on the sensitivity of parametric solar irradiance models to changes in aerosol conditions.","A new measure, the aerosol influence quantifier, is introduced for assessing the correlation strength between the relative variation in clear-sky diffuse fraction and the relative variation in atmospheric aerosol load.","The effectiveness of the aerosol influence quantifier is explored as a tool for assessing the sensitivity of three parametric clear-sky models to variations in input aerosol data.","A rather surprising result is reported, e.g. if high quality aerosol data are not available, one could potentially maximize the estimate's accuracy by using a parametric clear-sky solar irradiance model with low sensitivity to aerosol variations.","In such cases, it is shown that the aerosol influence quantifier can be used as a tool for identifying an adequate model for input data with a given uncertainty."],"url":"http://arxiv.org/abs/2406.08000v1","category":"physics.ao-ph"}
{"created":"2024-06-12 08:18:45","title":"A general solution for the response of materials under radiation and tilted magnetic field: semi-classical regime","abstract":"The Berry curvature dipole is well-known to cause Hall conductivity. This study expands on previous results to demonstrate how two- and three-dimensional materials react under a tilted magnetic field in the linear and nonlinear regimes. We show how the Hall effect has a quantum origin by deriving the general form of intrinsic and extrinsic currents in materials under a tilted magnetic field. Our focus is on determining the linear and nonlinear response of two-dimensional materials. We also demonstrate that as the result of the perpendicular component of the magnetic field a classic-quantum current can occur in two-dimensional materials and topological crystalline insulators in second harmonic generation and ratchet responses. The findings of this research may provide insight into the transport characteristics of materials in the semi-classical regime and initiate a new chapter in linear and nonlinear Hall effects.","sentences":["The Berry curvature dipole is well-known to cause Hall conductivity.","This study expands on previous results to demonstrate how two- and three-dimensional materials react under a tilted magnetic field in the linear and nonlinear regimes.","We show how the Hall effect has a quantum origin by deriving the general form of intrinsic and extrinsic currents in materials under a tilted magnetic field.","Our focus is on determining the linear and nonlinear response of two-dimensional materials.","We also demonstrate that as the result of the perpendicular component of the magnetic field a classic-quantum current can occur in two-dimensional materials and topological crystalline insulators in second harmonic generation and ratchet responses.","The findings of this research may provide insight into the transport characteristics of materials in the semi-classical regime and initiate a new chapter in linear and nonlinear Hall effects."],"url":"http://arxiv.org/abs/2406.07987v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-12 08:11:43","title":"Normalized solutions to a class of $(2, q)$-Laplacian equations in the strongly sublinear regime","abstract":"In this paper, we consider the existence and multiplicity of normalized solutions for the following $(2, q)$-Laplacian equation \\begin{equation}\\label{Equation} \\left\\{\\begin{aligned} &-\\Delta u-\\Delta_q u+\\lambda u=g(u),\\quad x \\in \\mathbb{R}^N, &\\|u\\|_2^2 =c^2, \\end{aligned}\\right. \\tag{$\\mathscr E_\\lambda$} \\end{equation} where $1<q<N$, $\\Delta_q=\\operatorname{div}\\left(|\\nabla u|^{q-2} \\nabla u\\right)$ is the $q$-Laplacian operator, $\\lambda$ is a Lagrange multiplier and $c>0$ is a constant. The nonlinearity $g:\\mathbb{R}\\rightarrow \\mathbb{R}$ is continuous and the behaviour of $g$ at the origin is allowed to be strongly sublinear, i.e., $\\lim \\limits_{s \\rightarrow 0} g(s) / s=-\\infty$, which includes the logarithmic nonlinearity $$g(s)= s \\log s^2.$$ We consider a family of approximating problems that can be set in $H^1\\left(\\mathbb{R}^N\\right)\\cap D^{1, q}\\left(\\mathbb{R}^N\\right)$ and the corresponding least-energy solutions. Then, we prove that such a family of solutions converges to a least-energy solution to the original problem. Additionally, under certain assumptions about $g$ that allow us to work in a suitable subspace of $H^1\\left(\\mathbb{R}^N\\right)\\cap D^{1, q}\\left(\\mathbb{R}^N\\right)$, we prove the existence of infinitely many solutions of the above $(2, q)$-Laplacian equation.","sentences":["In this paper, we consider the existence and multiplicity of normalized solutions for the following $(2, q)$-Laplacian equation \\begin{equation}\\label{Equation} \\left\\{\\begin{aligned} &-\\Delta u-\\Delta_q u+\\lambda","u=g(u),\\quad x \\in \\mathbb{R}^N, &\\|u\\|_2^2","=c^2, \\end{aligned}\\right.","\\tag{$\\mathscr E_\\lambda$} \\end{equation} where $1<q<N$, $\\Delta_q=\\operatorname{div}\\left(|\\nabla u|^{q-2} \\nabla u\\right)$ is the $q$-Laplacian operator, $\\lambda$ is a Lagrange multiplier and $c>0$ is a constant.","The nonlinearity $g:\\mathbb{R}\\rightarrow \\mathbb{R}$ is continuous and the behaviour of $g$ at the origin is allowed to be strongly sublinear, i.e., $\\lim \\limits_{s \\rightarrow 0} g(s) / s=-\\infty$, which includes the logarithmic nonlinearity $$g(s)= s \\log s^2.$$ We consider a family of approximating problems that can be set in $H^1\\left(\\mathbb{R}^N\\right)\\cap D^{1, q}\\left(\\mathbb{R}^N\\right)$ and the corresponding least-energy solutions.","Then, we prove that such a family of solutions converges to a least-energy solution to the original problem.","Additionally, under certain assumptions about $g$ that allow us to work in a suitable subspace of $H^1\\left(\\mathbb{R}^N\\right)\\cap D^{1, q}\\left(\\mathbb{R}^N\\right)$, we prove the existence of infinitely many solutions of the above $(2, q)$-Laplacian equation."],"url":"http://arxiv.org/abs/2406.07985v1","category":"math.AP"}
{"created":"2024-06-12 08:10:05","title":"On the density patch problem for the 2-D inhomogeneous Navier-Stokes equations","abstract":"In this paper, we first construct a class of global strong solutions for the 2-D inhomogeneous Navier-Stokes equations under very general assumption that the initial density is only bounded and the initial velocity is in $H^1(\\mathbb{R}^2)$. With suitable assumptions on the initial density, which includes the case of density patch and vacuum bubbles, we prove that Lions' s weak solution is the same as the strong solution with the same initial data. In particular, this gives a complete resolution of the density patch problem proposed by Lions: {\\it for the density patch data $\\rho_0=1_{D}$ with a smooth bounded domain $D\\subset\\mathbb{R}^2$, the regularity of $D$ is preserved by the time evolution of Lions's weak solution.}","sentences":["In this paper, we first construct a class of global strong solutions for the 2-D inhomogeneous Navier-Stokes equations under very general assumption that the initial density is only bounded and the initial velocity is in $H^1(\\mathbb{R}^2)$. With suitable assumptions on the initial density, which includes the case of density patch and vacuum bubbles, we prove that Lions' s weak solution is the same as the strong solution with the same initial data.","In particular, this gives a complete resolution of the density patch problem proposed by Lions: {\\it for the density patch data $\\rho_0=1_{D}$ with a smooth bounded domain $D\\subset\\mathbb{R}^2$, the regularity of $D$ is preserved by the time evolution of Lions's weak solution.}"],"url":"http://arxiv.org/abs/2406.07984v1","category":"math.AP"}
{"created":"2024-06-12 08:09:29","title":"Meta-Learning Neural Procedural Biases","abstract":"The goal of few-shot learning is to generalize and achieve high performance on new unseen learning tasks, where each task has only a limited number of examples available. Gradient-based meta-learning attempts to address this challenging task by learning how to learn new tasks by embedding inductive biases informed by prior learning experiences into the components of the learning algorithm. In this work, we build upon prior research and propose Neural Procedural Bias Meta-Learning (NPBML), a novel framework designed to meta-learn task-adaptive procedural biases. Our approach aims to consolidate recent advancements in meta-learned initializations, optimizers, and loss functions by learning them simultaneously and making them adapt to each individual task to maximize the strength of the learned inductive biases. This imbues each learning task with a unique set of procedural biases which is specifically designed and selected to attain strong learning performance in only a few gradient steps. The experimental results show that by meta-learning the procedural biases of a neural network, we can induce strong inductive biases towards a distribution of learning tasks, enabling robust learning performance across many well-established few-shot learning benchmarks.","sentences":["The goal of few-shot learning is to generalize and achieve high performance on new unseen learning tasks, where each task has only a limited number of examples available.","Gradient-based meta-learning attempts to address this challenging task by learning how to learn new tasks by embedding inductive biases informed by prior learning experiences into the components of the learning algorithm.","In this work, we build upon prior research and propose Neural Procedural Bias Meta-Learning (NPBML), a novel framework designed to meta-learn task-adaptive procedural biases.","Our approach aims to consolidate recent advancements in meta-learned initializations, optimizers, and loss functions by learning them simultaneously and making them adapt to each individual task to maximize the strength of the learned inductive biases.","This imbues each learning task with a unique set of procedural biases which is specifically designed and selected to attain strong learning performance in only a few gradient steps.","The experimental results show that by meta-learning the procedural biases of a neural network, we can induce strong inductive biases towards a distribution of learning tasks, enabling robust learning performance across many well-established few-shot learning benchmarks."],"url":"http://arxiv.org/abs/2406.07983v1","category":"cs.LG"}
{"created":"2024-06-12 07:36:45","title":"Multicriteria Adjustable Robustness","abstract":"Multicriteria adjustable robust optimization (MARO) problems arise in a wide variety of practical settings, for example, in the design of a building's energy supply. However, no general approaches, neither for the characterization of solutions to this problem class, nor potential solution methods, are available in the literature so far. We give different definitions for efficient solutions to MARO problems and look at three computational concepts to deal with the problems. These computational concepts can also be understood as additional solution definitions. We assess the advantages and disadvantages of the different computational approaches and analyze their connections to our initial definitions of MARO-efficiency. We observe that an $\\varepsilon$-constraint inspired first-scalarize-then-robustify computational approach is beneficial because it provides an efficient set that is easy to understand for decision makers and provides tight bounds on the worst-case evaluation for a particular efficient solution. In contrast, a weighted sum first-scalarize-then-robustify approach keeps the problem structure more simple but is only beneficial if the desired trade-off between objectives is already known because the efficient set might look ambiguous. Further, we demonstrate that a first-robustify procedure only gives bad bounds and can be too optimistic as well as too pessimistic.","sentences":["Multicriteria adjustable robust optimization (MARO) problems arise in a wide variety of practical settings, for example, in the design of a building's energy supply.","However, no general approaches, neither for the characterization of solutions to this problem class, nor potential solution methods, are available in the literature so far.","We give different definitions for efficient solutions to MARO problems and look at three computational concepts to deal with the problems.","These computational concepts can also be understood as additional solution definitions.","We assess the advantages and disadvantages of the different computational approaches and analyze their connections to our initial definitions of MARO-efficiency.","We observe that an $\\varepsilon$-constraint inspired first-scalarize-then-robustify computational approach is beneficial because it provides an efficient set that is easy to understand for decision makers and provides tight bounds on the worst-case evaluation for a particular efficient solution.","In contrast, a weighted sum first-scalarize-then-robustify approach keeps the problem structure more simple but is only beneficial if the desired trade-off between objectives is already known because the efficient set might look ambiguous.","Further, we demonstrate that a first-robustify procedure only gives bad bounds and can be too optimistic as well as too pessimistic."],"url":"http://arxiv.org/abs/2406.07959v1","category":"math.OC"}
{"created":"2024-06-12 07:28:28","title":"How Interpretable Are Interpretable Graph Neural Networks?","abstract":"Interpretable graph neural networks (XGNNs ) are widely adopted in various scientific applications involving graph-structured data. Existing XGNNs predominantly adopt the attention-based mechanism to learn edge or node importance for extracting and making predictions with the interpretable subgraph. However, the representational properties and limitations of these methods remain inadequately explored. In this work, we present a theoretical framework that formulates interpretable subgraph learning with the multilinear extension of the subgraph distribution, coined as subgraph multilinear extension (SubMT). Extracting the desired interpretable subgraph requires an accurate approximation of SubMT, yet we find that the existing XGNNs can have a huge gap in fitting SubMT. Consequently, the SubMT approximation failure will lead to the degenerated interpretability of the extracted subgraphs. To mitigate the issue, we design a new XGNN architecture called Graph Multilinear neT (GMT), which is provably more powerful in approximating SubMT. We empirically validate our theoretical findings on a number of graph classification benchmarks. The results demonstrate that GMT outperforms the state-of-the-art up to 10% in terms of both interpretability and generalizability across 12 regular and geometric graph benchmarks.","sentences":["Interpretable graph neural networks (XGNNs ) are widely adopted in various scientific applications involving graph-structured data.","Existing XGNNs predominantly adopt the attention-based mechanism to learn edge or node importance for extracting and making predictions with the interpretable subgraph.","However, the representational properties and limitations of these methods remain inadequately explored.","In this work, we present a theoretical framework that formulates interpretable subgraph learning with the multilinear extension of the subgraph distribution, coined as subgraph multilinear extension (SubMT).","Extracting the desired interpretable subgraph requires an accurate approximation of SubMT, yet we find that the existing XGNNs can have a huge gap in fitting SubMT.","Consequently, the SubMT approximation failure will lead to the degenerated interpretability of the extracted subgraphs.","To mitigate the issue, we design a new XGNN architecture called Graph Multilinear neT (GMT), which is provably more powerful in approximating SubMT.","We empirically validate our theoretical findings on a number of graph classification benchmarks.","The results demonstrate that GMT outperforms the state-of-the-art up to 10% in terms of both interpretability and generalizability across 12 regular and geometric graph benchmarks."],"url":"http://arxiv.org/abs/2406.07955v1","category":"cs.LG"}
{"created":"2024-06-12 07:17:13","title":"Reduced Basis method for finite volume simulations of parabolic PDEs applied to porous media flows","abstract":"Numerical simulations are a highly valuable tool to evaluate the impact of the uncertainties of various modelparameters, and to optimize e.g. injection-production scenarios in the context of underground storage (of CO2typically). Finite volume approximations of Darcy's parabolic model for flows in porous media are typically runmany times, for many values of parameters like permeability and porosity, at costly computational efforts.We study the relevance of reduced basis methods as a way to lower the overall simulation cost of finite volumeapproximations to Darcy's parabolic model for flows in porous media for different values of the parameters suchas permeability. In the context of underground gas storage (of CO2 typically) in saline aquifers, our aim isto evaluate quickly, for many parameter values, the flux along some interior boundaries near the well injectionarea-regarded as a quantity of interest-. To this end, we construct reduced bases by a standard POD-Greedyalgorithm. Our POD-Greedy algorithm uses a new goal-oriented error estimator designed from a discrete space-time energy norm independent of the parameter. We provide some numerical experiments that validate theefficiency of the proposed estimator.","sentences":["Numerical simulations are a highly valuable tool to evaluate the impact of the uncertainties of various modelparameters, and to optimize e.g. injection-production scenarios in the context of underground storage (of CO2typically).","Finite volume approximations of Darcy's parabolic model for flows in porous media are typically runmany times, for many values of parameters like permeability and porosity, at costly computational efforts.","We study the relevance of reduced basis methods as a way to lower the overall simulation cost of finite volumeapproximations to Darcy's parabolic model for flows in porous media for different values of the parameters suchas permeability.","In the context of underground gas storage (of CO2 typically) in saline aquifers, our aim isto evaluate quickly, for many parameter values, the flux along some interior boundaries near the well injectionarea-regarded as a quantity of interest-.","To this end, we construct reduced bases by a standard POD-Greedyalgorithm.","Our POD-Greedy algorithm uses a new goal-oriented error estimator designed from a discrete space-time energy norm independent of the parameter.","We provide some numerical experiments that validate theefficiency of the proposed estimator."],"url":"http://arxiv.org/abs/2406.07950v1","category":"math.NA"}
{"created":"2024-06-12 07:06:59","title":"Making peace with random phases: Ab initio conical intersection dynamics in random gauges","abstract":"Ab initio modeling of conical intersection dynamics is crucial for various photochemical, photophysical, and biological processes. However, adiabatic electronic states obtained from electronic structure computations involve random phases, or more generally, random gauge fixings, which hampers the modeling of nonadiabatic molecular dynamics. Here we develop a random-gauge local diabatic representation that allows an exact modeling of conical intersection dynamics directly using the adiabatic electronic states with phases randomly assigned during the electronic structure computations. Its utility is demonstrated by an exact ab initio modeling of the two-dimensional Shin-Metiu model with and without an external magnetic field. Our results provide a simple approach to integrating the electronic structure computations into non-adiabatic quantum dynamics, thus paving the way for ab initio modeling of conical intersection dynamics.","sentences":["Ab initio modeling of conical intersection dynamics is crucial for various photochemical, photophysical, and biological processes.","However, adiabatic electronic states obtained from electronic structure computations involve random phases, or more generally, random gauge fixings, which hampers the modeling of nonadiabatic molecular dynamics.","Here we develop a random-gauge local diabatic representation that allows an exact modeling of conical intersection dynamics directly using the adiabatic electronic states with phases randomly assigned during the electronic structure computations.","Its utility is demonstrated by an exact ab initio modeling of the two-dimensional Shin-Metiu model with and without an external magnetic field.","Our results provide a simple approach to integrating the electronic structure computations into non-adiabatic quantum dynamics, thus paving the way for ab initio modeling of conical intersection dynamics."],"url":"http://arxiv.org/abs/2406.07945v1","category":"physics.chem-ph"}
{"created":"2024-06-12 07:02:59","title":"Simple yet Sharp Sensitivity Analysis for Any Contrast Under Unmeasured Confounding","abstract":"We extend our previous work on sensitivity analysis for the risk ratio and difference contrasts under unmeasured confounding to any contrast. We prove that the bounds produced are still arbitrarily sharp, i.e. practically attainable. We illustrate the usability of the bounds with real data.","sentences":["We extend our previous work on sensitivity analysis for the risk ratio and difference contrasts under unmeasured confounding to any contrast.","We prove that the bounds produced are still arbitrarily sharp, i.e. practically attainable.","We illustrate the usability of the bounds with real data."],"url":"http://arxiv.org/abs/2406.07940v1","category":"stat.ME"}
{"created":"2024-06-12 06:54:12","title":"Prediction of the Dark Fermion Mass using Multicritical-Point Principle","abstract":"This paper proposes a method to determine the effective potential using the multicritical-point principle (MPP) under the additional scalar field. The MPP is applied to the model in which a singlet dark fermion and a singlet real scalar field are added to the Standard Model (SM) to predict the dark fermion mass. As a result, the dark fermion mass is predicted to be about 866-937 GeV.","sentences":["This paper proposes a method to determine the effective potential using the multicritical-point principle (MPP) under the additional scalar field.","The MPP is applied to the model in which a singlet dark fermion and a singlet real scalar field are added to the Standard Model (SM) to predict the dark fermion mass.","As a result, the dark fermion mass is predicted to be about 866-937 GeV."],"url":"http://arxiv.org/abs/2406.07931v1","category":"hep-ph"}
{"created":"2024-06-12 06:44:59","title":"FDLoRA: Personalized Federated Learning of Large Language Model via Dual LoRA Tuning","abstract":"Large language models (LLMs) have emerged as important components across various fields, yet their training requires substantial computation resources and abundant labeled data. It poses a challenge to robustly training LLMs for individual users (clients). To tackle this challenge, the intuitive idea is to introduce federated learning (FL), which can collaboratively train models on distributed private data. However, existing methods suffer from the challenges of data heterogeneity, system heterogeneity, and model size, resulting in suboptimal performance and high costs. In this work, we proposed a variant of personalized federated learning (PFL) framework, namely FDLoRA, which allows the client to be a single device or a cluster and adopts low-rank adaptation (LoRA) tuning. FDLoRA sets dual LoRA modules on each client to capture personalized and global knowledge, respectively, and only the global LoRA module uploads parameters to the central server to aggregate cross-client knowledge. Finally, an adaptive fusion approach is employed to combine the parameters of the dual LoRAs. This enables FDLoRA to make effective use of private data distributed across different clients, thereby improving performance on the client without incurring high communication and computing costs. We conducted extensive experiments in two practice scenarios. The results demonstrate that FDLoRA outperforms six baselines in terms of performance, stability, robustness, computation cost, and communication cost.","sentences":["Large language models (LLMs) have emerged as important components across various fields, yet their training requires substantial computation resources and abundant labeled data.","It poses a challenge to robustly training LLMs for individual users (clients).","To tackle this challenge, the intuitive idea is to introduce federated learning (FL), which can collaboratively train models on distributed private data.","However, existing methods suffer from the challenges of data heterogeneity, system heterogeneity, and model size, resulting in suboptimal performance and high costs.","In this work, we proposed a variant of personalized federated learning (PFL) framework, namely FDLoRA, which allows the client to be a single device or a cluster and adopts low-rank adaptation (LoRA) tuning.","FDLoRA sets dual LoRA modules on each client to capture personalized and global knowledge, respectively, and only the global LoRA module uploads parameters to the central server to aggregate cross-client knowledge.","Finally, an adaptive fusion approach is employed to combine the parameters of the dual LoRAs.","This enables FDLoRA to make effective use of private data distributed across different clients, thereby improving performance on the client without incurring high communication and computing costs.","We conducted extensive experiments in two practice scenarios.","The results demonstrate that FDLoRA outperforms six baselines in terms of performance, stability, robustness, computation cost, and communication cost."],"url":"http://arxiv.org/abs/2406.07925v1","category":"cs.DC"}
{"created":"2024-06-12 06:42:39","title":"A Two-Stage Online Algorithm for EV Charging Station Energy Management and Carbon Trading","abstract":"The increasing electric vehicle (EV) adoption challenges the energy management of charging stations (CSs) due to the large number of EVs and the underlying uncertainties. Moreover, the carbon footprint of CSs is growing significantly due to the rising charging power demand. This makes it important for CSs to properly manage their energy usage and ensure their carbon footprint stay within their carbon emission quotas. This paper proposes a two-stage online algorithm for this purpose, considering the different time scales of energy management and carbon trading. In the first stage, the CS characterizes the real-time aggregate EV power flexibility, in terms of upper and lower bounds on the total charging power, by a Lyapunov optimization-based online algorithm. In the second stage, the CS co-optimizes energy management and carbon trading, with EV charging power chosen within the aggregate flexibility region provided by the first stage. A generalized battery model is proposed to capture the dynamic carbon footprint changes and carbon trading. A virtual carbon queue is designed to develop an online algorithm for the second stage, which can ensure the carbon footprint of CS be within its carbon emission quota and its total operation cost is nearly offline optimal. Case studies validate the effectiveness and advantages of the proposed algorithm.","sentences":["The increasing electric vehicle (EV) adoption challenges the energy management of charging stations (CSs) due to the large number of EVs and the underlying uncertainties.","Moreover, the carbon footprint of CSs is growing significantly due to the rising charging power demand.","This makes it important for CSs to properly manage their energy usage and ensure their carbon footprint stay within their carbon emission quotas.","This paper proposes a two-stage online algorithm for this purpose, considering the different time scales of energy management and carbon trading.","In the first stage, the CS characterizes the real-time aggregate EV power flexibility, in terms of upper and lower bounds on the total charging power, by a Lyapunov optimization-based online algorithm.","In the second stage, the CS co-optimizes energy management and carbon trading, with EV charging power chosen within the aggregate flexibility region provided by the first stage.","A generalized battery model is proposed to capture the dynamic carbon footprint changes and carbon trading.","A virtual carbon queue is designed to develop an online algorithm for the second stage, which can ensure the carbon footprint of CS be within its carbon emission quota and its total operation cost is nearly offline optimal.","Case studies validate the effectiveness and advantages of the proposed algorithm."],"url":"http://arxiv.org/abs/2406.07921v1","category":"math.OC"}
{"created":"2024-06-12 06:40:19","title":"Micro-expression recognition based on depth map to point cloud","abstract":"Micro-expressions are nonverbal facial expressions that reveal the covert emotions of individuals, making the micro-expression recognition task receive widespread attention. However, the micro-expression recognition task is challenging due to the subtle facial motion and brevity in duration. Many 2D image-based methods have been developed in recent years to recognize MEs effectively, but, these approaches are restricted by facial texture information and are susceptible to environmental factors, such as lighting. Conversely, depth information can effectively represent motion information related to facial structure changes and is not affected by lighting. Motion information derived from facial structures can describe motion features that pixel textures cannot delineate. We proposed a network for micro-expression recognition based on facial depth information, and our experiments have demonstrated the crucial role of depth maps in the micro-expression recognition task. Initially, we transform the depth map into a point cloud and obtain the motion information for each point by aligning the initiating frame with the apex frame and performing a differential operation. Subsequently, we adjusted all point cloud motion feature input dimensions and used them as inputs for multiple point cloud networks to assess the efficacy of this representation. PointNet++ was chosen as the ultimate outcome for micro-expression recognition due to its superior performance. Our experiments show that our proposed method significantly outperforms the existing deep learning methods, including the baseline, on the $CAS(ME)^3$ dataset, which includes depth information.","sentences":["Micro-expressions are nonverbal facial expressions that reveal the covert emotions of individuals, making the micro-expression recognition task receive widespread attention.","However, the micro-expression recognition task is challenging due to the subtle facial motion and brevity in duration.","Many 2D image-based methods have been developed in recent years to recognize MEs effectively, but, these approaches are restricted by facial texture information and are susceptible to environmental factors, such as lighting.","Conversely, depth information can effectively represent motion information related to facial structure changes and is not affected by lighting.","Motion information derived from facial structures can describe motion features that pixel textures cannot delineate.","We proposed a network for micro-expression recognition based on facial depth information, and our experiments have demonstrated the crucial role of depth maps in the micro-expression recognition task.","Initially, we transform the depth map into a point cloud and obtain the motion information for each point by aligning the initiating frame with the apex frame and performing a differential operation.","Subsequently, we adjusted all point cloud motion feature input dimensions and used them as inputs for multiple point cloud networks to assess the efficacy of this representation.","PointNet++ was chosen as the ultimate outcome for micro-expression recognition due to its superior performance.","Our experiments show that our proposed method significantly outperforms the existing deep learning methods, including the baseline, on the $CAS(ME)^3$ dataset, which includes depth information."],"url":"http://arxiv.org/abs/2406.07918v1","category":"eess.IV"}
{"created":"2024-06-12 06:35:49","title":"Revisiting strong-coupling determinations from $e^+ e^-$ event shapes","abstract":"We reassess the theoretical uncertainties of strong-coupling determinations from a global fit to the $e^+ e^-$ thrust data, focusing in particular on the scheme dependence associated with the applied renormalon-cancellation prescription and on the choice of scale parameters that are used to estimate higher-order perturbative corrections in all sectors of the calculation.","sentences":["We reassess the theoretical uncertainties of strong-coupling determinations from a global fit to the $e^+ e^-$ thrust data, focusing in particular on the scheme dependence associated with the applied renormalon-cancellation prescription and on the choice of scale parameters that are used to estimate higher-order perturbative corrections in all sectors of the calculation."],"url":"http://arxiv.org/abs/2406.07916v1","category":"hep-ph"}
{"created":"2024-06-12 06:29:40","title":"Proposal for realizing and probing topological crystalline insulators in optical lattices","abstract":"We develop a lattice model which exhibits topological transitions from $Z_2$ topological insulators to mirror symmetry-protected topological crystalline insulators by introducing additional spin-orbit coupling terms. The topological phase is characterized by the mirror winding number, defined within the mirror symmetry invariant subspace, which ensures the protection of gapless edge states and zero-energy corner states under specific boundary conditions. Additionally, we propose a feasible scheme using ultracold atoms confined in a stacked hexagonal optical lattice with Raman fields to realize the two-dimensional topological crystalline insulators. Detection of the mirror winding number in these systems can be achieved by implementing a simple quench sequence and observing the evolution of the time-of-flight patterns.","sentences":["We develop a lattice model which exhibits topological transitions from $Z_2$ topological insulators to mirror symmetry-protected topological crystalline insulators by introducing additional spin-orbit coupling terms.","The topological phase is characterized by the mirror winding number, defined within the mirror symmetry invariant subspace, which ensures the protection of gapless edge states and zero-energy corner states under specific boundary conditions.","Additionally, we propose a feasible scheme using ultracold atoms confined in a stacked hexagonal optical lattice with Raman fields to realize the two-dimensional topological crystalline insulators.","Detection of the mirror winding number in these systems can be achieved by implementing a simple quench sequence and observing the evolution of the time-of-flight patterns."],"url":"http://arxiv.org/abs/2406.07911v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-12 06:27:28","title":"Demonstration of Safe Electromagnetic Radiation Emitted by 5G Active Antenna Systems","abstract":"The careful planning and safe deployment of 5G technologies will bring enormous benefits to society and the economy. Higher frequency, beamforming, and small-cells are key technologies that will provide unmatched throughput and seamless connectivity to 5G users. Superficial knowledge of these technologies has raised concerns among the general public about the harmful effects of radiation. Several standardization bodies are active to put limits on the emissions which are based on a defined set of radiation measurement methodologies. However, due to the peculiarity of 5G such as dynamicity of the beams, network densification, Time Division Duplexing mode of operation, etc, using existing EMF measurement methods may provide inaccurate results. In this context, we discuss our experimental studies aimed towards the measurement of radiation caused by beam-based transmissions from a 5G base station equipped with an Active Antenna System(AAS). We elaborate on the shortcomings of current measurement methodologies and address several open questions. Next, we demonstrate that using user-specific downlink beamforming, not only better performance is achieved compared to non-beamformed downlink, but also the radiation in the vicinity of the intended user is significantly decreased. Further, we show that under weak reception conditions, an uplink transmission can cause significantly high radiation in the vicinity of the user equipment. We believe that our work will help in clearing several misleading concepts about the 5G EMF radiation effects. We conclude the work by providing guidelines to improve the methodology of EMF measurement by considering the spatiotemporal dynamicity of the 5G transmission.","sentences":["The careful planning and safe deployment of 5G technologies will bring enormous benefits to society and the economy.","Higher frequency, beamforming, and small-cells are key technologies that will provide unmatched throughput and seamless connectivity to 5G users.","Superficial knowledge of these technologies has raised concerns among the general public about the harmful effects of radiation.","Several standardization bodies are active to put limits on the emissions which are based on a defined set of radiation measurement methodologies.","However, due to the peculiarity of 5G such as dynamicity of the beams, network densification, Time Division Duplexing mode of operation, etc, using existing EMF measurement methods may provide inaccurate results.","In this context, we discuss our experimental studies aimed towards the measurement of radiation caused by beam-based transmissions from a 5G base station equipped with an Active Antenna System(AAS).","We elaborate on the shortcomings of current measurement methodologies and address several open questions.","Next, we demonstrate that using user-specific downlink beamforming, not only better performance is achieved compared to non-beamformed downlink, but also the radiation in the vicinity of the intended user is significantly decreased.","Further, we show that under weak reception conditions, an uplink transmission can cause significantly high radiation in the vicinity of the user equipment.","We believe that our work will help in clearing several misleading concepts about the 5G EMF radiation effects.","We conclude the work by providing guidelines to improve the methodology of EMF measurement by considering the spatiotemporal dynamicity of the 5G transmission."],"url":"http://arxiv.org/abs/2406.07910v1","category":"cs.ET"}
{"created":"2024-06-12 06:05:41","title":"Content Provider Contributions to Capacity Expansion of a Neutral ISP: Effect of Private Option","abstract":"Increasing content consumption by users and the expectation of a better Internet experience requires Internet service providers (ISPs) to expand the capacity of the access network continually. The ISPs have been demanding the participation of the content providers (CPs) in sharing the cost of upgrading the infrastructure. From CPs' perspective, investing in the ISP infrastructure, termed as \\emph{public investment}, seems rational as it will boost their profit. However, the CPs can alternatively invest in making content delivery more efficient, termed as \\emph{private investment}, as it also boosts their profit. Thus, in this work, we investigate this trade-off between public and private investment of the CPs for a net-neutral ISP. Specifically, we consider centralized decision and non-cooperative forms of interaction between CPs and an ISP and determine the optimum public and private investments of the CPs for each model. In the non-cooperative interaction, we find that at most one CP contributes to the public infrastructure, whereas all invest in their private infrastructure.","sentences":["Increasing content consumption by users and the expectation of a better Internet experience requires Internet service providers (ISPs) to expand the capacity of the access network continually.","The ISPs have been demanding the participation of the content providers (CPs) in sharing the cost of upgrading the infrastructure.","From CPs' perspective, investing in the ISP infrastructure, termed as \\emph{public investment}, seems rational as it will boost their profit.","However, the CPs can alternatively invest in making content delivery more efficient, termed as \\emph{private investment}, as it also boosts their profit.","Thus, in this work, we investigate this trade-off between public and private investment of the CPs for a net-neutral ISP.","Specifically, we consider centralized decision and non-cooperative forms of interaction between CPs and an ISP and determine the optimum public and private investments of the CPs for each model.","In the non-cooperative interaction, we find that at most one CP contributes to the public infrastructure, whereas all invest in their private infrastructure."],"url":"http://arxiv.org/abs/2406.07898v1","category":"cs.NI"}
{"created":"2024-06-12 05:49:50","title":"McCormick envelopes in mixed-integer PDE-constrained optimization","abstract":"McCormick envelopes are a standard tool for deriving convex relaxations of optimization problems that involve polynomial terms. McCormick envelopes provide lower bounds, for example, in branch-and-bound procedures for mixed-integer nonlinear programs but have not gained much attention in infinite-dimensional optimization so far. This lack of attention may be due to the distributed nature of infinite-dimensional problems, which on the one hand leads to infinitely many constraints (generally state constraints that may be difficult to handle) that are added to the problem under consideration and on the other hand renders bound-tightening procedures that successively improve these convex envelopes computationally intractable.   We take on the challenge and analyze McCormick envelopes for a model problem class that is governed by a semilinear PDE involving a bilinearity. We approximate the McCormick envelopes by averaging them over the cells of a discretization of the computational domain. This process yields convex problems that underestimate the original problem up to an a priori error estimate that depends on the mesh size of the discretization. These approximate McCormick envelopes can be improved by means of an optimization-based bound-tightening procedure and can be shown to $\\Gamma$-converge to a limit problem with a pointwise formulation of the McCormick envelopes.   We provide a computational example, for which we certify all of our imposed assumptions. The results point to both the potential of the methodology and the gaps in the research that need to be closed.   Our methodology provides a framework first for obtaining pointwise underestimators for nonconvexities and second for approximating them with finitely many linear inequalities in a superordinate infinite-dimensional setting.","sentences":["McCormick envelopes are a standard tool for deriving convex relaxations of optimization problems that involve polynomial terms.","McCormick envelopes provide lower bounds, for example, in branch-and-bound procedures for mixed-integer nonlinear programs but have not gained much attention in infinite-dimensional optimization so far.","This lack of attention may be due to the distributed nature of infinite-dimensional problems, which on the one hand leads to infinitely many constraints (generally state constraints that may be difficult to handle) that are added to the problem under consideration and on the other hand renders bound-tightening procedures that successively improve these convex envelopes computationally intractable.   ","We take on the challenge and analyze McCormick envelopes for a model problem class that is governed by a semilinear PDE involving a bilinearity.","We approximate the McCormick envelopes by averaging them over the cells of a discretization of the computational domain.","This process yields convex problems that underestimate the original problem up to an a priori error estimate that depends on the mesh size of the discretization.","These approximate McCormick envelopes can be improved by means of an optimization-based bound-tightening procedure and can be shown to $\\Gamma$-converge to a limit problem with a pointwise formulation of the McCormick envelopes.   ","We provide a computational example, for which we certify all of our imposed assumptions.","The results point to both the potential of the methodology and the gaps in the research that need to be closed.   ","Our methodology provides a framework first for obtaining pointwise underestimators for nonconvexities and second for approximating them with finitely many linear inequalities in a superordinate infinite-dimensional setting."],"url":"http://arxiv.org/abs/2406.07891v1","category":"math.OC"}
{"created":"2024-06-12 05:25:15","title":"An Empirical Study of Mamba-based Language Models","abstract":"Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.","sentences":["Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache.","Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative.","In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers.","To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens.","We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid).","Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets.","Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning.","In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time.","To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences.","On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average.","To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project."],"url":"http://arxiv.org/abs/2406.07887v1","category":"cs.LG"}
{"created":"2024-06-12 05:23:08","title":"Reinforcement Learning to Disentangle Multiqubit Quantum States from Partial Observations","abstract":"Using partial knowledge of a quantum state to control multiqubit entanglement is a largely unexplored paradigm in the emerging field of quantum interactive dynamics with the potential to address outstanding challenges in quantum state preparation and compression, quantum control, and quantum complexity. We present a deep reinforcement learning (RL) approach to constructing short disentangling circuits for arbitrary 4-, 5-, and 6-qubit states using an actor-critic algorithm. With access to only two-qubit reduced density matrices, our agent decides which pairs of qubits to apply two-qubit gates on; requiring only local information makes it directly applicable on modern NISQ devices. Utilizing a permutation-equivariant transformer architecture, the agent can autonomously identify qubit permutations within the state, and adjusts the disentangling protocol accordingly. Once trained, it provides circuits from different initial states without further optimization. We demonstrate the agent's ability to identify and exploit the entanglement structure of multiqubit states. For 4-, 5-, and 6-qubit Haar-random states, the agent learns to construct disentangling circuits that exhibit strong correlations both between consecutive gates and among the qubits involved. Through extensive benchmarking, we show the efficacy of the RL approach to find disentangling protocols with minimal gate resources. We explore the resilience of our trained agents to noise, highlighting their potential for real-world quantum computing applications. Analyzing optimal disentangling protocols, we report a general circuit to prepare an arbitrary 4-qubit state using at most 5 two-qubit (10 CNOT) gates.","sentences":["Using partial knowledge of a quantum state to control multiqubit entanglement is a largely unexplored paradigm in the emerging field of quantum interactive dynamics with the potential to address outstanding challenges in quantum state preparation and compression, quantum control, and quantum complexity.","We present a deep reinforcement learning (RL) approach to constructing short disentangling circuits for arbitrary 4-, 5-, and 6-qubit states using an actor-critic algorithm.","With access to only two-qubit reduced density matrices, our agent decides which pairs of qubits to apply two-qubit gates on; requiring only local information makes it directly applicable on modern NISQ devices.","Utilizing a permutation-equivariant transformer architecture, the agent can autonomously identify qubit permutations within the state, and adjusts the disentangling protocol accordingly.","Once trained, it provides circuits from different initial states without further optimization.","We demonstrate the agent's ability to identify and exploit the entanglement structure of multiqubit states.","For 4-, 5-, and 6-qubit Haar-random states, the agent learns to construct disentangling circuits that exhibit strong correlations both between consecutive gates and among the qubits involved.","Through extensive benchmarking, we show the efficacy of the RL approach to find disentangling protocols with minimal gate resources.","We explore the resilience of our trained agents to noise, highlighting their potential for real-world quantum computing applications.","Analyzing optimal disentangling protocols, we report a general circuit to prepare an arbitrary 4-qubit state using at most 5 two-qubit (10 CNOT) gates."],"url":"http://arxiv.org/abs/2406.07884v1","category":"quant-ph"}
{"created":"2024-06-12 04:49:11","title":"Bridging multiple worlds: multi-marginal optimal transport for causal partial-identification problem","abstract":"Under the prevalent potential outcome model in causal inference, each unit is associated with multiple potential outcomes but at most one of which is observed, leading to many causal quantities being only partially identified. The inherent missing data issue echoes the multi-marginal optimal transport (MOT) problem, where marginal distributions are known, but how the marginals couple to form the joint distribution is unavailable. In this paper, we cast the causal partial identification problem in the framework of MOT with $K$ margins and $d$-dimensional outcomes and obtain the exact partial identified set. In order to estimate the partial identified set via MOT, statistically, we establish a convergence rate of the plug-in MOT estimator for general quadratic objective functions and prove it is minimax optimal for a quadratic objective function stemming from the variance minimization problem with arbitrary $K$ and $d \\le 4$. Numerically, we demonstrate the efficacy of our method over several real-world datasets where our proposal consistently outperforms the baseline by a significant margin (over 70%). In addition, we provide efficient off-the-shelf implementations of MOT with general objective functions.","sentences":["Under the prevalent potential outcome model in causal inference, each unit is associated with multiple potential outcomes but at most one of which is observed, leading to many causal quantities being only partially identified.","The inherent missing data issue echoes the multi-marginal optimal transport (MOT) problem, where marginal distributions are known, but how the marginals couple to form the joint distribution is unavailable.","In this paper, we cast the causal partial identification problem in the framework of MOT with $K$ margins and $d$-dimensional outcomes and obtain the exact partial identified set.","In order to estimate the partial identified set via MOT, statistically, we establish a convergence rate of the plug-in MOT estimator for general quadratic objective functions and prove it is minimax optimal for a quadratic objective function stemming from the variance minimization problem with arbitrary $K$ and $d \\le 4$.","Numerically, we demonstrate the efficacy of our method over several real-world datasets where our proposal consistently outperforms the baseline by a significant margin (over 70%).","In addition, we provide efficient off-the-shelf implementations of MOT with general objective functions."],"url":"http://arxiv.org/abs/2406.07868v1","category":"stat.ME"}
{"created":"2024-06-12 04:46:23","title":"Asymptotically Optimal Regret for Black-Box Predict-then-Optimize","abstract":"We consider the predict-then-optimize paradigm for decision-making in which a practitioner (1) trains a supervised learning model on historical data of decisions, contexts, and rewards, and then (2) uses the resulting model to make future binary decisions for new contexts by finding the decision that maximizes the model's predicted reward. This approach is common in industry. Past analysis assumes that rewards are observed for all actions for all historical contexts, which is possible only in problems with special structure. Motivated by problems from ads targeting and recommender systems, we study new black-box predict-then-optimize problems that lack this special structure and where we only observe the reward from the action taken. We present a novel loss function, which we call Empirical Soft Regret (ESR), designed to significantly improve reward when used in training compared to classical accuracy-based metrics like mean-squared error. This loss function targets the regret achieved when taking a suboptimal decision; because the regret is generally not differentiable, we propose a differentiable \"soft\" regret term that allows the use of neural networks and other flexible machine learning models dependent on gradient-based training. In the particular case of paired data, we show theoretically that optimizing our loss function yields asymptotically optimal regret within the class of supervised learning models. We also show our approach significantly outperforms state-of-the-art algorithms on real-world decision-making problems in news recommendation and personalized healthcare compared to benchmark methods from contextual bandits and conditional average treatment effect estimation.","sentences":["We consider the predict-then-optimize paradigm for decision-making in which a practitioner (1) trains a supervised learning model on historical data of decisions, contexts, and rewards, and then (2) uses the resulting model to make future binary decisions for new contexts by finding the decision that maximizes the model's predicted reward.","This approach is common in industry.","Past analysis assumes that rewards are observed for all actions for all historical contexts, which is possible only in problems with special structure.","Motivated by problems from ads targeting and recommender systems, we study new black-box predict-then-optimize problems that lack this special structure and where we only observe the reward from the action taken.","We present a novel loss function, which we call Empirical Soft Regret (ESR), designed to significantly improve reward when used in training compared to classical accuracy-based metrics like mean-squared error.","This loss function targets the regret achieved when taking a suboptimal decision; because the regret is generally not differentiable, we propose a differentiable \"soft\" regret term that allows the use of neural networks and other flexible machine learning models dependent on gradient-based training.","In the particular case of paired data, we show theoretically that optimizing our loss function yields asymptotically optimal regret within the class of supervised learning models.","We also show our approach significantly outperforms state-of-the-art algorithms on real-world decision-making problems in news recommendation and personalized healthcare compared to benchmark methods from contextual bandits and conditional average treatment effect estimation."],"url":"http://arxiv.org/abs/2406.07866v1","category":"cs.LG"}
{"created":"2024-06-12 04:27:39","title":"A Measurement of CO(3-2) Line Emission from eBOSS Galaxies at $z\\sim 0.5$ using Planck Data","abstract":"Line intensity mapping (LIM) is a novel observational technique in astrophysics that utilizes the integrated emission from multiple atomic and molecular transition lines from galaxies to probe the complex physics of galaxy formation and evolution, as well as the large-scale structure of the universe. Modeling multiple line luminosities of galaxies with varying masses or their host halo masses poses significant uncertainty due to the lack of observational data across a wide redshift range and the intricate nature of astrophysical processes, making them challenging to model analytically or in simulations. While future experiments aim to measure multiple line intensities up to $z\\sim 8$ across a wide volume using tomographic methods, we leverage publicly available datasets from the CMB experiment Planck and the galaxy survey eBOSS to constrain the CO(3-2) emission from galaxies. We correlate galaxies from eBOSS data onto the full-sky CO(2-1) map produced by Planck and report the first measurement of the average CO(3-2) intensity, $I_{CO} = 45.7 \\pm 14.2\\, \\mathrm{Jy/sr}$ at $z\\sim 0.5$ with $3.2\\sigma$ confidence. Our findings demonstrate that stacking methods are already viable with existing observations from CMB experiments and galaxy surveys, and are complementary to traditional LIM experiments.","sentences":["Line intensity mapping (LIM) is a novel observational technique in astrophysics that utilizes the integrated emission from multiple atomic and molecular transition lines from galaxies to probe the complex physics of galaxy formation and evolution, as well as the large-scale structure of the universe.","Modeling multiple line luminosities of galaxies with varying masses or their host halo masses poses significant uncertainty due to the lack of observational data across a wide redshift range and the intricate nature of astrophysical processes, making them challenging to model analytically or in simulations.","While future experiments aim to measure multiple line intensities up to $z\\sim 8$ across a wide volume using tomographic methods, we leverage publicly available datasets from the CMB experiment Planck and the galaxy survey eBOSS to constrain the CO(3-2) emission from galaxies.","We correlate galaxies from eBOSS data onto the full-sky CO(2-1) map produced by Planck and report the first measurement of the average CO(3-2) intensity, $I_{CO} = 45.7 \\pm 14.2\\, \\mathrm{Jy/sr}$ at $z\\sim 0.5$ with $3.2\\sigma$ confidence.","Our findings demonstrate that stacking methods are already viable with existing observations from CMB experiments and galaxy surveys, and are complementary to traditional LIM experiments."],"url":"http://arxiv.org/abs/2406.07861v1","category":"astro-ph.GA"}
{"created":"2024-06-12 04:22:23","title":"Relations between monotone complexity measures based on decision tree complexity","abstract":"In a recent result, Knop, Lovett, McGuire and Yuan (STOC 2021) proved the log-rank conjecture for communication complexity, up to log n factor, for any Boolean function composed with AND function as the inner gadget. One of the main tools in this result was the relationship between monotone analogues of well-studied Boolean complexity measures like block sensitivity and certificate complexity. The relationship between the standard measures has been a long line of research, with a landmark result by Huang (Annals of Mathematics 2019), finally showing that sensitivity is polynomially related to all other standard measures. In this article, we study the monotone analogues of standard measures like block sensitivity (mbs(f)), certificate complexity (MCC(f)) and fractional block sensitivity (fmbs(f)); and study the relationship between these measures given their connection with AND-decision tree and sparsity of a Boolean function. We show the following results: 1) Given a Boolean function $f : \\{0, 1\\}^{n} \\rightarrow \\{0, 1\\}$, the ratio $fmbs(f^l )/mbs(f^l )$ is bounded by a function of n (and not l). A similar result was known for the corresponding standard measures (Tal, ITCS 2013). This result allows us to extend any upper bound by a well behaved measure on monotone block sensitivity to monotone fractional block sensitivity. 2) The question of the best possible upper bound on monotone block sensitivity by the logarithm of sparsity is equivalent to the natural question of best upper bound by degree on sensitivity. One side of this relationship was used in the proof by Knop, Lovett, McGuire and Yuan (STOC 2021). 3) For two natural classes of functions, symmetric and monotone, hitting set complexity (MCC) is equal to monotone sensitivity.","sentences":["In a recent result, Knop, Lovett, McGuire and Yuan (STOC 2021) proved the log-rank conjecture for communication complexity, up to log n factor, for any Boolean function composed with AND function as the inner gadget.","One of the main tools in this result was the relationship between monotone analogues of well-studied Boolean complexity measures like block sensitivity and certificate complexity.","The relationship between the standard measures has been a long line of research, with a landmark result by Huang (Annals of Mathematics 2019), finally showing that sensitivity is polynomially related to all other standard measures.","In this article, we study the monotone analogues of standard measures like block sensitivity (mbs(f)), certificate complexity (MCC(f)) and fractional block sensitivity (fmbs(f)); and study the relationship between these measures given their connection with AND-decision tree and sparsity of a Boolean function.","We show the following results: 1) Given a Boolean function $f : \\{0, 1\\}^{n} \\rightarrow \\{0, 1\\}$, the ratio $fmbs(f^l )/mbs(f^l )$ is bounded by a function of n (and not l).","A similar result was known for the corresponding standard measures (Tal, ITCS 2013).","This result allows us to extend any upper bound by a well behaved measure on monotone block sensitivity to monotone fractional block sensitivity.","2) The question of the best possible upper bound on monotone block sensitivity by the logarithm of sparsity is equivalent to the natural question of best upper bound by degree on sensitivity.","One side of this relationship was used in the proof by Knop, Lovett, McGuire and Yuan (STOC 2021).","3) For two natural classes of functions, symmetric and monotone, hitting set complexity (MCC) is equal to monotone sensitivity."],"url":"http://arxiv.org/abs/2406.07859v1","category":"cs.CC"}
{"created":"2024-06-12 04:06:56","title":"Zero-Shot Fake Video Detection by Audio-Visual Consistency","abstract":"Recent studies have advocated the detection of fake videos as a one-class detection task, predicated on the hypothesis that the consistency between audio and visual modalities of genuine data is more significant than that of fake data. This methodology, which solely relies on genuine audio-visual data while negating the need for forged counterparts, is thus delineated as a `zero-shot' detection paradigm. This paper introduces a novel zero-shot detection approach anchored in content consistency across audio and video. By employing pre-trained ASR and VSR models, we recognize the audio and video content sequences, respectively. Then, the edit distance between the two sequences is computed to assess whether the claimed video is genuine. Experimental results indicate that, compared to two mainstream approaches based on semantic consistency and temporal consistency, our approach achieves superior generalizability across various deepfake techniques and demonstrates strong robustness against audio-visual perturbations. Finally, state-of-the-art performance gains can be achieved by simply integrating the decision scores of these three systems.","sentences":["Recent studies have advocated the detection of fake videos as a one-class detection task, predicated on the hypothesis that the consistency between audio and visual modalities of genuine data is more significant than that of fake data.","This methodology, which solely relies on genuine audio-visual data while negating the need for forged counterparts, is thus delineated as a `zero-shot' detection paradigm.","This paper introduces a novel zero-shot detection approach anchored in content consistency across audio and video.","By employing pre-trained ASR and VSR models, we recognize the audio and video content sequences, respectively.","Then, the edit distance between the two sequences is computed to assess whether the claimed video is genuine.","Experimental results indicate that, compared to two mainstream approaches based on semantic consistency and temporal consistency, our approach achieves superior generalizability across various deepfake techniques and demonstrates strong robustness against audio-visual perturbations.","Finally, state-of-the-art performance gains can be achieved by simply integrating the decision scores of these three systems."],"url":"http://arxiv.org/abs/2406.07854v1","category":"cs.SD"}
{"created":"2024-06-12 03:27:31","title":"Output-sensitive Conjunctive Query Evaluation","abstract":"Join evaluation is one of the most fundamental operations performed by database systems and arguably the most well-studied problem in the Database community. A staggering number of join algorithms have been developed, and commercial database engines use finely tuned join heuristics that take into account many factors including the selectivity of predicates, memory, IO, etc. However, most of the results have catered to either full join queries or non-full join queries but with degree constraints (such as PK-FK relationships) that make joins \\emph{easier} to evaluate. Further, most of the algorithms are also not output-sensitive.   In this paper, we present a novel, output-sensitive algorithm for the evaluation of acyclic Conjunctive Queries (CQs) that contain arbitrary free variables. Our result is based on a novel generalization of the Yannakakis algorithm and shows that it is possible to improve the running time guarantee of the Yannakakis algorithm by a polynomial factor. Importantly, our algorithmic improvement does not depend on the use of fast matrix multiplication, as a recently proposed algorithm does. The upper bound is complemented with matching lower bounds conditioned on two variants of the $k$-clique conjecture. The application of our algorithm recovers known prior results and improves on known state-of-the-art results for common queries such as paths and stars.","sentences":["Join evaluation is one of the most fundamental operations performed by database systems and arguably the most well-studied problem in the Database community.","A staggering number of join algorithms have been developed, and commercial database engines use finely tuned join heuristics that take into account many factors including the selectivity of predicates, memory, IO, etc.","However, most of the results have catered to either full join queries or non-full join queries but with degree constraints (such as PK-FK relationships) that make joins \\emph{easier} to evaluate.","Further, most of the algorithms are also not output-sensitive.   ","In this paper, we present a novel, output-sensitive algorithm for the evaluation of acyclic Conjunctive Queries (CQs) that contain arbitrary free variables.","Our result is based on a novel generalization of the Yannakakis algorithm and shows that it is possible to improve the running time guarantee of the Yannakakis algorithm by a polynomial factor.","Importantly, our algorithmic improvement does not depend on the use of fast matrix multiplication, as a recently proposed algorithm does.","The upper bound is complemented with matching lower bounds conditioned on two variants of the $k$-clique conjecture.","The application of our algorithm recovers known prior results and improves on known state-of-the-art results for common queries such as paths and stars."],"url":"http://arxiv.org/abs/2406.07847v1","category":"cs.DB"}
{"created":"2024-06-12 03:25:18","title":"DualVC 3: Leveraging Language Model Generated Pseudo Context for End-to-end Low Latency Streaming Voice Conversion","abstract":"Streaming voice conversion has become increasingly popular for its potential in real-time applications. The recently proposed DualVC 2 has achieved robust and high-quality streaming voice conversion with a latency of about 180ms. Nonetheless, the recognition-synthesis framework hinders end-to-end optimization, and the instability of automatic speech recognition (ASR) model with short chunks makes it challenging to further reduce latency. To address these issues, we propose an end-to-end model, DualVC 3. With speaker-independent semantic tokens to guide the training of the content encoder, the dependency on ASR is removed and the model can operate under extremely small chunks, with cascading errors eliminated. A language model is trained on the content encoder output to produce pseudo context by iteratively predicting future frames, providing more contextual information for the decoder to improve conversion quality. Experimental results demonstrate that DualVC 3 achieves comparable performance to DualVC 2 in subjective and objective metrics, with a latency of only 50 ms.","sentences":["Streaming voice conversion has become increasingly popular for its potential in real-time applications.","The recently proposed DualVC 2 has achieved robust and high-quality streaming voice conversion with a latency of about 180ms.","Nonetheless, the recognition-synthesis framework hinders end-to-end optimization, and the instability of automatic speech recognition (ASR) model with short chunks makes it challenging to further reduce latency.","To address these issues, we propose an end-to-end model, DualVC 3.","With speaker-independent semantic tokens to guide the training of the content encoder, the dependency on ASR is removed and the model can operate under extremely small chunks, with cascading errors eliminated.","A language model is trained on the content encoder output to produce pseudo context by iteratively predicting future frames, providing more contextual information for the decoder to improve conversion quality.","Experimental results demonstrate that DualVC 3 achieves comparable performance to DualVC 2 in subjective and objective metrics, with a latency of only 50 ms."],"url":"http://arxiv.org/abs/2406.07846v1","category":"eess.AS"}
{"created":"2024-06-12 03:16:45","title":"Labeling Comic Mischief Content in Online Videos with a Multimodal Hierarchical-Cross-Attention Model","abstract":"We address the challenge of detecting questionable content in online media, specifically the subcategory of comic mischief. This type of content combines elements such as violence, adult content, or sarcasm with humor, making it difficult to detect. Employing a multimodal approach is vital to capture the subtle details inherent in comic mischief content. To tackle this problem, we propose a novel end-to-end multimodal system for the task of comic mischief detection. As part of this contribution, we release a novel dataset for the targeted task consisting of three modalities: video, text (video captions and subtitles), and audio. We also design a HIerarchical Cross-attention model with CAPtions (HICCAP) to capture the intricate relationships among these modalities. The results show that the proposed approach makes a significant improvement over robust baselines and state-of-the-art models for comic mischief detection and its type classification. This emphasizes the potential of our system to empower users, to make informed decisions about the online content they choose to see. In addition, we conduct experiments on the UCF101, HMDB51, and XD-Violence datasets, comparing our model against other state-of-the-art approaches showcasing the outstanding performance of our proposed model in various scenarios.","sentences":["We address the challenge of detecting questionable content in online media, specifically the subcategory of comic mischief.","This type of content combines elements such as violence, adult content, or sarcasm with humor, making it difficult to detect.","Employing a multimodal approach is vital to capture the subtle details inherent in comic mischief content.","To tackle this problem, we propose a novel end-to-end multimodal system for the task of comic mischief detection.","As part of this contribution, we release a novel dataset for the targeted task consisting of three modalities: video, text (video captions and subtitles), and audio.","We also design a HIerarchical Cross-attention model with CAPtions (HICCAP) to capture the intricate relationships among these modalities.","The results show that the proposed approach makes a significant improvement over robust baselines and state-of-the-art models for comic mischief detection and its type classification.","This emphasizes the potential of our system to empower users, to make informed decisions about the online content they choose to see.","In addition, we conduct experiments on the UCF101, HMDB51, and XD-Violence datasets, comparing our model against other state-of-the-art approaches showcasing the outstanding performance of our proposed model in various scenarios."],"url":"http://arxiv.org/abs/2406.07841v1","category":"cs.CV"}
{"created":"2024-06-12 03:15:15","title":"SynthForge: Synthesizing High-Quality Face Dataset with Controllable 3D Generative Models","abstract":"Recent advancements in generative models have unlocked the capabilities to render photo-realistic data in a controllable fashion. Trained on the real data, these generative models are capable of producing realistic samples with minimal to no domain gap, as compared to the traditional graphics rendering. However, using the data generated using such models for training downstream tasks remains under-explored, mainly due to the lack of 3D consistent annotations. Moreover, controllable generative models are learned from massive data and their latent space is often too vast to obtain meaningful sample distributions for downstream task with limited generation. To overcome these challenges, we extract 3D consistent annotations from an existing controllable generative model, making the data useful for downstream tasks. Our experiments show competitive performance against state-of-the-art models using only generated synthetic data, demonstrating potential for solving downstream tasks. Project page: https://synth-forge.github.io","sentences":["Recent advancements in generative models have unlocked the capabilities to render photo-realistic data in a controllable fashion.","Trained on the real data, these generative models are capable of producing realistic samples with minimal to no domain gap, as compared to the traditional graphics rendering.","However, using the data generated using such models for training downstream tasks remains under-explored, mainly due to the lack of 3D consistent annotations.","Moreover, controllable generative models are learned from massive data and their latent space is often too vast to obtain meaningful sample distributions for downstream task with limited generation.","To overcome these challenges, we extract 3D consistent annotations from an existing controllable generative model, making the data useful for downstream tasks.","Our experiments show competitive performance against state-of-the-art models using only generated synthetic data, demonstrating potential for solving downstream tasks.","Project page: https://synth-forge.github.io"],"url":"http://arxiv.org/abs/2406.07840v1","category":"cs.CV"}
{"created":"2024-06-12 02:39:46","title":"Are Objective Explanatory Evaluation metrics Trustworthy? An Adversarial Analysis","abstract":"Explainable AI (XAI) has revolutionized the field of deep learning by empowering users to have more trust in neural network models. The field of XAI allows users to probe the inner workings of these algorithms to elucidate their decision-making processes. The rise in popularity of XAI has led to the advent of different strategies to produce explanations, all of which only occasionally agree. Thus several objective evaluation metrics have been devised to decide which of these modules give the best explanation for specific scenarios. The goal of the paper is twofold: (i) we employ the notions of necessity and sufficiency from causal literature to come up with a novel explanatory technique called SHifted Adversaries using Pixel Elimination(SHAPE) which satisfies all the theoretical and mathematical criteria of being a valid explanation, (ii) we show that SHAPE is, infact, an adversarial explanation that fools causal metrics that are employed to measure the robustness and reliability of popular importance based visual XAI methods. Our analysis shows that SHAPE outperforms popular explanatory techniques like GradCAM and GradCAM++ in these tests and is comparable to RISE, raising questions about the sanity of these metrics and the need for human involvement for an overall better evaluation.","sentences":["Explainable AI (XAI) has revolutionized the field of deep learning by empowering users to have more trust in neural network models.","The field of XAI allows users to probe the inner workings of these algorithms to elucidate their decision-making processes.","The rise in popularity of XAI has led to the advent of different strategies to produce explanations, all of which only occasionally agree.","Thus several objective evaluation metrics have been devised to decide which of these modules give the best explanation for specific scenarios.","The goal of the paper is twofold: (i) we employ the notions of necessity and sufficiency from causal literature to come up with a novel explanatory technique called SHifted Adversaries using Pixel Elimination(SHAPE) which satisfies all the theoretical and mathematical criteria of being a valid explanation, (ii) we show that SHAPE is, infact, an adversarial explanation that fools causal metrics that are employed to measure the robustness and reliability of popular importance based visual XAI methods.","Our analysis shows that SHAPE outperforms popular explanatory techniques like GradCAM and GradCAM++ in these tests and is comparable to RISE, raising questions about the sanity of these metrics and the need for human involvement for an overall better evaluation."],"url":"http://arxiv.org/abs/2406.07820v1","category":"cs.CV"}
{"created":"2024-06-12 02:26:53","title":"Single MoS2-flake as a high TCR non-cryogenic bolometer","abstract":"Temperature coefficient of resistance (TCR) of a bolometer can be tuned by modifying the thermal conductance of an absorbing materials since they sense radiations via the temperature change in the absorber. However, the thermal conductance of the absorber can be reduced by engineering the appropriate thermal isolation, which can be an ultimate solution towards making a highly sensitive thermal detector. Here, we have developed an atomically thin 2D bolometer detector made up of a mechanically transferred suspended multilayer-MoS2 flake, eliminating the use of challenging thin-film fabrication process. The strength of our detector lies on the two factors: its large surface-to-volume window to absorb the radiations; the suspended configuration which prevents the heat dissipation through the substrate and therefore reduces the thermal conductance. The bolometric response of the detector is tested in both modes, via the photoresponse and the thermal response. The prototype is found to exhibit a very high TCR ~ -9.5%/K with the least achievable thermal noise-equivalent power (NEP) ~ 0.61 pWHz-1/2, in ambient conditions at 328 K.","sentences":["Temperature coefficient of resistance (TCR) of a bolometer can be tuned by modifying the thermal conductance of an absorbing materials since they sense radiations via the temperature change in the absorber.","However, the thermal conductance of the absorber can be reduced by engineering the appropriate thermal isolation, which can be an ultimate solution towards making a highly sensitive thermal detector.","Here, we have developed an atomically thin 2D bolometer detector made up of a mechanically transferred suspended multilayer-MoS2 flake, eliminating the use of challenging thin-film fabrication process.","The strength of our detector lies on the two factors: its large surface-to-volume window to absorb the radiations; the suspended configuration which prevents the heat dissipation through the substrate and therefore reduces the thermal conductance.","The bolometric response of the detector is tested in both modes, via the photoresponse and the thermal response.","The prototype is found to exhibit a very high TCR ~","-9.5%/K with the least achievable thermal noise-equivalent power (NEP) ~ 0.61 pWHz-1/2, in ambient conditions at 328 K."],"url":"http://arxiv.org/abs/2406.07818v1","category":"physics.app-ph"}
{"created":"2024-06-12 02:09:05","title":"Evaluating the Impact of Sequence Combinations on Breast Tumor Segmentation in Multiparametric MRI","abstract":"Multiparametric magnetic resonance imaging (mpMRI) is a key tool for assessing breast cancer progression. Although deep learning has been applied to automate tumor segmentation in breast MRI, the effect of sequence combinations in mpMRI remains under-investigated. This study explores the impact of different combinations of T2-weighted (T2w), dynamic contrast-enhanced MRI (DCE-MRI) and diffusion-weighted imaging (DWI) with apparent diffusion coefficient (ADC) map on breast tumor segmentation using nnU-Net. Evaluated on a multicenter mpMRI dataset, the nnU-Net model using DCE sequences achieved a Dice similarity coefficient (DSC) of 0.69 $\\pm$ 0.18 for functional tumor volume (FTV) segmentation. For whole tumor mask (WTM) segmentation, adding the predicted FTV to DWI and ADC map improved the DSC from 0.57 $\\pm$ 0.24 to 0.60 $\\pm$ 0.21. Adding T2w did not yield significant improvement, which still requires further investigation under a more standardized imaging protocol. This study serves as a foundation for future work on predicting breast cancer treatment response using mpMRI.","sentences":["Multiparametric magnetic resonance imaging (mpMRI) is a key tool for assessing breast cancer progression.","Although deep learning has been applied to automate tumor segmentation in breast MRI, the effect of sequence combinations in mpMRI remains under-investigated.","This study explores the impact of different combinations of T2-weighted (T2w), dynamic contrast-enhanced MRI (DCE-MRI) and diffusion-weighted imaging (DWI) with apparent diffusion coefficient (ADC) map on breast tumor segmentation using nnU-Net.","Evaluated on a multicenter mpMRI dataset, the nnU-Net model using DCE sequences achieved a Dice similarity coefficient (DSC) of 0.69 $\\pm$ 0.18 for functional tumor volume (FTV) segmentation.","For whole tumor mask (WTM) segmentation, adding the predicted FTV to DWI and ADC map improved the DSC from 0.57 $\\pm$ 0.24 to 0.60 $\\pm$ 0.21.","Adding T2w did not yield significant improvement, which still requires further investigation under a more standardized imaging protocol.","This study serves as a foundation for future work on predicting breast cancer treatment response using mpMRI."],"url":"http://arxiv.org/abs/2406.07813v1","category":"eess.IV"}
{"created":"2024-06-12 02:03:57","title":"Did Harold Zuercher Have Time-Separable Preferences?","abstract":"This paper proposes an empirical model of dynamic discrete choice to allow for non-separable time preferences, generalizing the well-known Rust (1987) model. Under weak conditions, we show the existence of value functions and hence well-defined optimal choices. We construct a contraction mapping of the value function and propose an estimation method similar to Rust's nested fixed point algorithm. Finally, we apply the framework to the bus engine replacement data. We improve the fit of the data with our general model and reject the null hypothesis that Harold Zuercher has separable time preferences. Misspecifying an agent's preference as time-separable when it is not leads to biased inferences about structure parameters (such as the agent's risk attitudes) and misleading policy recommendations.","sentences":["This paper proposes an empirical model of dynamic discrete choice to allow for non-separable time preferences, generalizing the well-known Rust (1987) model.","Under weak conditions, we show the existence of value functions and hence well-defined optimal choices.","We construct a contraction mapping of the value function and propose an estimation method similar to Rust's nested fixed point algorithm.","Finally, we apply the framework to the bus engine replacement data.","We improve the fit of the data with our general model and reject the null hypothesis that Harold Zuercher has separable time preferences.","Misspecifying an agent's preference as time-separable when it is not leads to biased inferences about structure parameters (such as the agent's risk attitudes) and misleading policy recommendations."],"url":"http://arxiv.org/abs/2406.07809v1","category":"econ.EM"}
{"created":"2024-06-12 01:46:33","title":"Wiser than the Wisest of Crowds: The Asch Effect Revisited under Friedkin-Johnsen Opinion Dynamics","abstract":"In 1907, Sir Francis Galton independently asked 787 villagers to estimate the weight of an ox. Although none of them guessed the exact weight, the average estimate was remarkably accurate. This phenomenon is known as wisdom of crowds. In a clever experiment, Asch employed actors to demonstrate the human tendency to conform to others' opinions. The question we ask is: what would Sir Francis Galton have observed if Asch had interfered by employing actors? Would the wisdom of crowds become even wiser or not? The problem becomes intriguing when considering the inter-connectedness of the villagers, which is the central theme of this work. We examine a scenario where $n$ agents are interconnected and influence each other. The average of their opinions provides an estimator of a certain quality for some unknown quantity. How can one improve or reduce the quality of the original estimator in terms of the MSE by utilizing Asch's strategy of hiring a few stooges?   We present a new formulation of this problem, assuming that nodes adjust their opinions according to the Friedkin-Johnsen opinion dynamics. We demonstrate that selecting $k$ stooges for maximizing and minimizing the MSE is NP-hard. We also demonstrate that our formulation is closely related to maximizing or minimizing polarization and show NP-hardness. We propose an efficient greedy heuristic that scales to large networks and test our algorithm on synthetic and real-world datasets. Although MSE and polarization objectives differ, we find in practice that maximizing polarization often yields solutions that are nearly optimal for minimizing the wisdom of crowds in terms of MSE. Our analysis of real-world data reveals that even a small number of stooges can significantly influence the conversation on the war in Ukraine, resulting in a relative increase of the MSE of 207.80% (maximization) or a decrease of 50.62% (minimization).","sentences":["In 1907, Sir Francis Galton independently asked 787 villagers to estimate the weight of an ox.","Although none of them guessed the exact weight, the average estimate was remarkably accurate.","This phenomenon is known as wisdom of crowds.","In a clever experiment, Asch employed actors to demonstrate the human tendency to conform to others' opinions.","The question we ask is: what would Sir Francis Galton have observed if Asch had interfered by employing actors?","Would the wisdom of crowds become even wiser or not?","The problem becomes intriguing when considering the inter-connectedness of the villagers, which is the central theme of this work.","We examine a scenario where $n$ agents are interconnected and influence each other.","The average of their opinions provides an estimator of a certain quality for some unknown quantity.","How can one improve or reduce the quality of the original estimator in terms of the MSE by utilizing Asch's strategy of hiring a few stooges?   ","We present a new formulation of this problem, assuming that nodes adjust their opinions according to the Friedkin-Johnsen opinion dynamics.","We demonstrate that selecting $k$ stooges for maximizing and minimizing the MSE is NP-hard.","We also demonstrate that our formulation is closely related to maximizing or minimizing polarization and show NP-hardness.","We propose an efficient greedy heuristic that scales to large networks and test our algorithm on synthetic and real-world datasets.","Although MSE and polarization objectives differ, we find in practice that maximizing polarization often yields solutions that are nearly optimal for minimizing the wisdom of crowds in terms of MSE.","Our analysis of real-world data reveals that even a small number of stooges can significantly influence the conversation on the war in Ukraine, resulting in a relative increase of the MSE of 207.80% (maximization) or a decrease of 50.62% (minimization)."],"url":"http://arxiv.org/abs/2406.07805v1","category":"cs.SI"}
{"created":"2024-06-12 01:45:37","title":"The maximum likelihood type estimator of SDEs with fractional Brownian motion under small noise asymptotics in the rough case","abstract":"We study the problem of parametric estimation for continuously observed stochastic differential equation driven by fractional Brownian motion. Under some assumptions on drift and diffusion coefficients, we construct maximum likelihood estimator and establish its the asymptotic normality and moment convergence of the drift parameter when a small dispersion coefficient vanishes.","sentences":["We study the problem of parametric estimation for continuously observed stochastic differential equation driven by fractional Brownian motion.","Under some assumptions on drift and diffusion coefficients, we construct maximum likelihood estimator and establish its the asymptotic normality and moment convergence of the drift parameter when a small dispersion coefficient vanishes."],"url":"http://arxiv.org/abs/2406.07804v1","category":"math.ST"}
{"created":"2024-06-12 01:23:49","title":"Counting independent sets in structured graphs","abstract":"Counting independent sets in graphs and hypergraphs under a variety of restrictions is a classical question with a long history. It is the subject of the celebrated container method which found numerous spectacular applications over the years. We consider the question of how many independent sets we can have in a graph under structural restrictions. We show that any $n$-vertex graph with independence number $\\alpha$ without $bK_a$ as an induced subgraph has at most $n^{O(1)} \\cdot \\alpha^{O(\\alpha)}$ independent sets. This substantially improves the trivial upper bound of $n^{\\alpha},$ whenever $\\alpha \\le n^{o(1)}$ and gives a characterization of graphs forbidding of which allows for such an improvement. It is also in general tight up to a constant in the exponent since there exist triangle-free graphs with $\\alpha^{\\Omega(\\alpha)}$ independent sets. We also prove that if one in addition assumes the ground graph is chi-bounded one can improve the bound to $n^{O(1)} \\cdot 2^{O(\\alpha)}$ which is tight up to a constant factor in the exponent.","sentences":["Counting independent sets in graphs and hypergraphs under a variety of restrictions is a classical question with a long history.","It is the subject of the celebrated container method which found numerous spectacular applications over the years.","We consider the question of how many independent sets we can have in a graph under structural restrictions.","We show that any $n$-vertex graph with independence number $\\alpha$ without $bK_a$ as an induced subgraph has at most $n^{O(1)}","\\cdot \\alpha^{O(\\alpha)}$ independent sets.","This substantially improves the trivial upper bound of $n^{\\alpha},$ whenever $\\alpha \\le n^{o(1)}$ and gives a characterization of graphs forbidding of which allows for such an improvement.","It is also in general tight up to a constant in the exponent since there exist triangle-free graphs with $\\alpha^{\\Omega(\\alpha)}$ independent sets.","We also prove that if one in addition assumes the ground graph is chi-bounded one can improve the bound to $n^{O(1)} \\cdot 2^{O(\\alpha)}$ which is tight up to a constant factor in the exponent."],"url":"http://arxiv.org/abs/2406.07799v1","category":"math.CO"}
{"created":"2024-06-12 01:17:38","title":"Data-driven confidence bound for structural response using segmented least squares: a mixed-integer programming approach","abstract":"As one of data-driven approaches to computational mechanics in elasticity, this paper presents a method finding a bound for structural response, taking uncertainty in a material data set into account. For construction of an uncertainty set, we adopt the segmented least squares so that a data set that is not fitted well by the linear regression model can be dealt with. Since the obtained uncertainty set is nonconvex, the optimization problem solved for the uncertainty analysis is nonconvex. We recast this optimization problem as a mixed-integer programming problem to find a global optimal solution. This global optimality, together with a fundamental property of the order statistics, guarantees that the obtained bound for the structural response is conservative, in the sense that, at least a specified confidence level, probability that the structural response is in this bound is no smaller than a specified target value. We present numerical examples for three different types of skeletal structures.","sentences":["As one of data-driven approaches to computational mechanics in elasticity, this paper presents a method finding a bound for structural response, taking uncertainty in a material data set into account.","For construction of an uncertainty set, we adopt the segmented least squares so that a data set that is not fitted well by the linear regression model can be dealt with.","Since the obtained uncertainty set is nonconvex, the optimization problem solved for the uncertainty analysis is nonconvex.","We recast this optimization problem as a mixed-integer programming problem to find a global optimal solution.","This global optimality, together with a fundamental property of the order statistics, guarantees that the obtained bound for the structural response is conservative, in the sense that, at least a specified confidence level, probability that the structural response is in this bound is no smaller than a specified target value.","We present numerical examples for three different types of skeletal structures."],"url":"http://arxiv.org/abs/2406.07793v1","category":"math.OC"}
{"created":"2024-06-12 01:12:53","title":"Hierarchical Patch Diffusion Models for High-Resolution Video Generation","abstract":"Diffusion models have demonstrated remarkable performance in image and video synthesis. However, scaling them to high-resolution inputs is challenging and requires restructuring the diffusion pipeline into multiple independent components, limiting scalability and complicating downstream applications. This makes it very efficient during training and unlocks end-to-end optimization on high-resolution videos. We improve PDMs in two principled ways. First, to enforce consistency between patches, we develop deep context fusion -- an architectural technique that propagates the context information from low-scale to high-scale patches in a hierarchical manner. Second, to accelerate training and inference, we propose adaptive computation, which allocates more network capacity and computation towards coarse image details. The resulting model sets a new state-of-the-art FVD score of 66.32 and Inception Score of 87.68 in class-conditional video generation on UCF-101 $256^2$, surpassing recent methods by more than 100%. Then, we show that it can be rapidly fine-tuned from a base $36\\times 64$ low-resolution generator for high-resolution $64 \\times 288 \\times 512$ text-to-video synthesis. To the best of our knowledge, our model is the first diffusion-based architecture which is trained on such high resolutions entirely end-to-end. Project webpage: https://snap-research.github.io/hpdm.","sentences":["Diffusion models have demonstrated remarkable performance in image and video synthesis.","However, scaling them to high-resolution inputs is challenging and requires restructuring the diffusion pipeline into multiple independent components, limiting scalability and complicating downstream applications.","This makes it very efficient during training and unlocks end-to-end optimization on high-resolution videos.","We improve PDMs in two principled ways.","First, to enforce consistency between patches, we develop deep context fusion -- an architectural technique that propagates the context information from low-scale to high-scale patches in a hierarchical manner.","Second, to accelerate training and inference, we propose adaptive computation, which allocates more network capacity and computation towards coarse image details.","The resulting model sets a new state-of-the-art FVD score of 66.32 and Inception Score of 87.68 in class-conditional video generation on UCF-101 $256^2$, surpassing recent methods by more than 100%.","Then, we show that it can be rapidly fine-tuned from a base $36\\times 64$ low-resolution generator for high-resolution $64 \\times 288 \\times 512$ text-to-video synthesis.","To the best of our knowledge, our model is the first diffusion-based architecture which is trained on such high resolutions entirely end-to-end.","Project webpage: https://snap-research.github.io/hpdm."],"url":"http://arxiv.org/abs/2406.07792v1","category":"cs.CV"}
{"created":"2024-06-12 00:43:14","title":"A Diagnostic Tool for Functional Causal Discovery","abstract":"Causal discovery methods aim to determine the causal direction between variables using observational data. Functional causal discovery methods, such as those based on the Linear Non-Gaussian Acyclic Model (LiNGAM), rely on structural and distributional assumptions to infer the causal direction. However, approaches for assessing causal discovery methods' performance as a function of sample size or the impact of assumption violations, inevitable in real-world scenarios, are lacking. To address this need, we propose Causal Direction Detection Rate (CDDR) diagnostic that evaluates whether and to what extent the interaction between assumption violations and sample size affects the ability to identify the hypothesized causal direction. Given a bivariate dataset of size N on a pair of variables, X and Y, CDDR diagnostic is the plotted comparison of the probability of each causal discovery outcome (e.g. X causes Y, Y causes X, or inconclusive) as a function of sample size less than N. We fully develop CDDR diagnostic in a bivariate case and demonstrate its use for two methods, LiNGAM and our new test-based causal discovery approach. We find CDDR diagnostic for the test-based approach to be more informative since it uses a richer set of causal discovery outcomes. Under certain assumptions, we prove that the probability estimates of detecting each possible causal discovery outcome are consistent and asymptotically normal. Through simulations, we study CDDR diagnostic's behavior when linearity and non-Gaussianity assumptions are violated. Additionally, we illustrate CDDR diagnostic on four real datasets, including three for which the causal direction is known.","sentences":["Causal discovery methods aim to determine the causal direction between variables using observational data.","Functional causal discovery methods, such as those based on the Linear Non-Gaussian Acyclic Model (LiNGAM), rely on structural and distributional assumptions to infer the causal direction.","However, approaches for assessing causal discovery methods' performance as a function of sample size or the impact of assumption violations, inevitable in real-world scenarios, are lacking.","To address this need, we propose Causal Direction Detection Rate (CDDR) diagnostic that evaluates whether and to what extent the interaction between assumption violations and sample size affects the ability to identify the hypothesized causal direction.","Given a bivariate dataset of size N on a pair of variables, X and Y, CDDR diagnostic is the plotted comparison of the probability of each causal discovery outcome (e.g. X causes Y, Y causes X, or inconclusive) as a function of sample size less than N. We fully develop CDDR diagnostic in a bivariate case and demonstrate its use for two methods, LiNGAM and our new test-based causal discovery approach.","We find CDDR diagnostic for the test-based approach to be more informative since it uses a richer set of causal discovery outcomes.","Under certain assumptions, we prove that the probability estimates of detecting each possible causal discovery outcome are consistent and asymptotically normal.","Through simulations, we study CDDR diagnostic's behavior when linearity and non-Gaussianity assumptions are violated.","Additionally, we illustrate CDDR diagnostic on four real datasets, including three for which the causal direction is known."],"url":"http://arxiv.org/abs/2406.07787v1","category":"stat.ME"}
{"created":"2024-06-11 23:54:42","title":"Unifying Interpretability and Explainability for Alzheimer's Disease Progression Prediction","abstract":"Reinforcement learning (RL) has recently shown promise in predicting Alzheimer's disease (AD) progression due to its unique ability to model domain knowledge. However, it is not clear which RL algorithms are well-suited for this task. Furthermore, these methods are not inherently explainable, limiting their applicability in real-world clinical scenarios. Our work addresses these two important questions. Using a causal, interpretable model of AD, we first compare the performance of four contemporary RL algorithms in predicting brain cognition over 10 years using only baseline (year 0) data. We then apply SHAP (SHapley Additive exPlanations) to explain the decisions made by each algorithm in the model. Our approach combines interpretability with explainability to provide insights into the key factors influencing AD progression, offering both global and individual, patient-level analysis. Our findings show that only one of the RL methods is able to satisfactorily model disease progression, but the post-hoc explanations indicate that all methods fail to properly capture the importance of amyloid accumulation, one of the pathological hallmarks of Alzheimer's disease. Our work aims to merge predictive accuracy with transparency, assisting clinicians and researchers in enhancing disease progression modeling for informed healthcare decisions. Code is available at https://github.com/rfali/xrlad.","sentences":["Reinforcement learning (RL) has recently shown promise in predicting Alzheimer's disease (AD) progression due to its unique ability to model domain knowledge.","However, it is not clear which RL algorithms are well-suited for this task.","Furthermore, these methods are not inherently explainable, limiting their applicability in real-world clinical scenarios.","Our work addresses these two important questions.","Using a causal, interpretable model of AD, we first compare the performance of four contemporary RL algorithms in predicting brain cognition over 10 years using only baseline (year 0) data.","We then apply SHAP (SHapley Additive exPlanations) to explain the decisions made by each algorithm in the model.","Our approach combines interpretability with explainability to provide insights into the key factors influencing AD progression, offering both global and individual, patient-level analysis.","Our findings show that only one of the RL methods is able to satisfactorily model disease progression, but the post-hoc explanations indicate that all methods fail to properly capture the importance of amyloid accumulation, one of the pathological hallmarks of Alzheimer's disease.","Our work aims to merge predictive accuracy with transparency, assisting clinicians and researchers in enhancing disease progression modeling for informed healthcare decisions.","Code is available at https://github.com/rfali/xrlad."],"url":"http://arxiv.org/abs/2406.07777v1","category":"cs.LG"}
{"created":"2024-06-11 23:45:47","title":"Beurling and Model subspaces invariant under a universal operator","abstract":"In this article, we characterize the Beurling and Model subspaces of the Hardy-Hilbert space $H^2(\\mathbb{D})$ invariant under the composition operator $C_{\\phi_a}f=f\\circ\\phi_a$, where $\\phi_a(z) = az + 1 - a$ for $a \\in (0,1)$ is an affine self-map of the open unit disk $\\mathbb{D}$. These operators have universal translates (in the sense of Rota) and have attracted attention recently due to their connection with the Invariant Subspace Problem (ISP) and the classical Ces\\`aro operator.","sentences":["In this article, we characterize the Beurling and Model subspaces of the Hardy-Hilbert space $H^2(\\mathbb{D})$ invariant under the composition operator $C_{\\phi_a}f=f\\circ\\phi_a$, where $\\phi_a(z) = az + 1 - a$ for $a \\in (0,1)$ is an affine self-map of the open unit disk $\\mathbb{D}$. These operators have universal translates (in the sense of Rota) and have attracted attention recently due to their connection with the Invariant Subspace Problem (ISP) and the classical Ces\\`aro operator."],"url":"http://arxiv.org/abs/2406.07774v1","category":"math.FA"}
{"created":"2024-06-11 23:31:46","title":"Photonic lantern wavefront reconstruction in a multi-wavefront sensor single-conjugate adaptive optics system","abstract":"Exoplanet direct imaging using adaptive optics (AO) is often limited by non-common path aberrations (NCPAs) and aberrations that are invisible to traditional pupil-plane wavefront sensors (WFSs). This can be remedied by focal-plane (FP) WFSs that characterize aberrations directly from a final science image. Photonic lanterns (PLs) can act as low-order FPWFSs with the ability to direct some light to downstream science instruments. Using a PL on the SEAL (Santa Cruz Extreme AO Laboratory) high-contrast imaging testbed, we demonstrate (1) linear ranges and (2) closed-loop control. Additionally, we simulate the use of the PL in a multi-wavefront sensor AO system, in which multiple WFSs feed back to the same common-path deformable mirror. Building on previous multi-WFS AO demonstrations on SEAL, we simulate a modulated pyramid WFS to sense aberrations of high spatial order and large amplitude, and the PL to sense low order aberrations including NCPAs. We assess adaptive optics performance in this setting using three different PL wavefront reconstruction algorithms. We also provide a new method to experimentally identify the propagation matrix of a PL, making advanced model-based algorithms practical. This work demonstrates the role of photonic technologies and multi-stage wavefront sensing in the context of extreme AO and high contrast imaging.","sentences":["Exoplanet direct imaging using adaptive optics (AO) is often limited by non-common path aberrations (NCPAs) and aberrations that are invisible to traditional pupil-plane wavefront sensors (WFSs).","This can be remedied by focal-plane (FP) WFSs that characterize aberrations directly from a final science image.","Photonic lanterns (PLs) can act as low-order FPWFSs with the ability to direct some light to downstream science instruments.","Using a PL on the SEAL (Santa Cruz Extreme AO Laboratory) high-contrast imaging testbed, we demonstrate (1) linear ranges and (2) closed-loop control.","Additionally, we simulate the use of the PL in a multi-wavefront sensor AO system, in which multiple WFSs feed back to the same common-path deformable mirror.","Building on previous multi-WFS AO demonstrations on SEAL, we simulate a modulated pyramid WFS to sense aberrations of high spatial order and large amplitude, and the PL to sense low order aberrations including NCPAs.","We assess adaptive optics performance in this setting using three different PL wavefront reconstruction algorithms.","We also provide a new method to experimentally identify the propagation matrix of a PL, making advanced model-based algorithms practical.","This work demonstrates the role of photonic technologies and multi-stage wavefront sensing in the context of extreme AO and high contrast imaging."],"url":"http://arxiv.org/abs/2406.07771v1","category":"astro-ph.IM"}
{"created":"2024-06-11 23:23:54","title":"Personalized Product Assortment with Real-time 3D Perception and Bayesian Payoff Estimation","abstract":"Product assortment selection is a critical challenge facing physical retailers. Effectively aligning inventory with the preferences of shoppers can increase sales and decrease out-of-stocks. However, in real-world settings the problem is challenging due to the combinatorial explosion of product assortment possibilities. Consumer preferences are typically heterogeneous across space and time, making inventory-preference alignment challenging. Additionally, existing strategies rely on syndicated data, which tends to be aggregated, low resolution, and suffer from high latency. To solve these challenges we introduce a real-time recommendation system, which we call \\ours. Our system utilizes recent advances in 3D computer vision for perception and automatic, fine grained sales estimation. These perceptual components run on the edge of the network and facilitate real-time reward signals. Additionally, we develop a Bayesian payoff model to account for noisy estimates from 3D LIDAR data. We rely on spatial clustering to allow the system to adapt to heterogeneous consumer preferences, and a graph-based candidate generation algorithm to address the combinatorial search problem. We test our system in real-world stores across two, 6-8 week A/B tests with beverage products and demonstrate a 35% and 27\\% increase in sales respectively. Finally, we monitor the deployed system for a period of 28 weeks with an observational study and show a 9.4\\% increase in sales.","sentences":["Product assortment selection is a critical challenge facing physical retailers.","Effectively aligning inventory with the preferences of shoppers can increase sales and decrease out-of-stocks.","However, in real-world settings the problem is challenging due to the combinatorial explosion of product assortment possibilities.","Consumer preferences are typically heterogeneous across space and time, making inventory-preference alignment challenging.","Additionally, existing strategies rely on syndicated data, which tends to be aggregated, low resolution, and suffer from high latency.","To solve these challenges we introduce a real-time recommendation system, which we call \\ours.","Our system utilizes recent advances in 3D computer vision for perception and automatic, fine grained sales estimation.","These perceptual components run on the edge of the network and facilitate real-time reward signals.","Additionally, we develop a Bayesian payoff model to account for noisy estimates from 3D LIDAR data.","We rely on spatial clustering to allow the system to adapt to heterogeneous consumer preferences, and a graph-based candidate generation algorithm to address the combinatorial search problem.","We test our system in real-world stores across two, 6-8 week A/B tests with beverage products and demonstrate a 35% and 27\\% increase in sales respectively.","Finally, we monitor the deployed system for a period of 28 weeks with an observational study and show a 9.4\\% increase in sales."],"url":"http://arxiv.org/abs/2406.07769v1","category":"cs.LG"}
{"created":"2024-06-11 23:22:07","title":"Selective Undercut of Undoped Optical Membranes for Spin-Active Color Centers in 4H-SiC","abstract":"Silicon carbide (SiC) is a semiconductor used in quantum information processing, microelectromechanical systems, photonics, power electronics, and harsh environment sensors. However, its high temperature stability, high breakdown voltage, wide bandgap, and high mechanical strength are accompanied by a chemical inertness which makes complex micromachining difficult. Photoelectrochemical etching is a simple, rapid means of wet processing SiC, including the use of dopant selective etch stops that take advantage of mature SiC homoepitaxy. However, dopant selective photoelectrochemical etching typically relies on highly doped material, which poses challenges for device applications such as quantum defects and photonics that benefit from low doping to produce robust emitter properties and high optical transparency. In this work, we develop a new, selective photoelectrochemical etching process that relies not on high doping but on the electrical depletion of a fabricated diode structure, allowing the selective etching of an n-doped substrate wafer versus an undoped epitaxial ($N_a=1(10)^{14}cm^{-3}$) device layer. We characterize the photo-response and photoelectrochemical etching behavior of the diode under bias and use those insights to suspend large ($>100\\mu m^2$) undoped membranes of SiC. We further characterize the compatibility of membranes with quantum emitters, performing comparative spin spectroscopy between undoped and highly doped membrane structures, finding the use of undoped material improves ensemble spin lifetime by $>3x$. This work enables the fabrication of high-purity suspended thin films suitable for scalable photonics, mechanics, and quantum technologies in SiC.","sentences":["Silicon carbide (SiC) is a semiconductor used in quantum information processing, microelectromechanical systems, photonics, power electronics, and harsh environment sensors.","However, its high temperature stability, high breakdown voltage, wide bandgap, and high mechanical strength are accompanied by a chemical inertness which makes complex micromachining difficult.","Photoelectrochemical etching is a simple, rapid means of wet processing SiC, including the use of dopant selective etch stops that take advantage of mature SiC homoepitaxy.","However, dopant selective photoelectrochemical etching typically relies on highly doped material, which poses challenges for device applications such as quantum defects and photonics that benefit from low doping to produce robust emitter properties and high optical transparency.","In this work, we develop a new, selective photoelectrochemical etching process that relies not on high doping but on the electrical depletion of a fabricated diode structure, allowing the selective etching of an n-doped substrate wafer versus an undoped epitaxial ($N_a=1(10)^{14}cm^{-3}$) device layer.","We characterize the photo-response and photoelectrochemical etching behavior of the diode under bias and use those insights to suspend large ($>100\\mu m^2$) undoped membranes of SiC.","We further characterize the compatibility of membranes with quantum emitters, performing comparative spin spectroscopy between undoped and highly doped membrane structures, finding the use of undoped material improves ensemble spin lifetime by $>3x$. This work enables the fabrication of high-purity suspended thin films suitable for scalable photonics, mechanics, and quantum technologies in SiC."],"url":"http://arxiv.org/abs/2406.07768v1","category":"physics.app-ph"}
{"created":"2024-06-11 23:16:46","title":"Conformalized Teleoperation: Confidently Mapping Human Inputs to High-Dimensional Robot Actions","abstract":"Assistive robotic arms often have more degrees-of-freedom than a human teleoperator can control with a low-dimensional input, like a joystick. To overcome this challenge, existing approaches use data-driven methods to learn a mapping from low-dimensional human inputs to high-dimensional robot actions. However, determining if such a black-box mapping can confidently infer a user's intended high-dimensional action from low-dimensional inputs remains an open problem. Our key idea is to adapt the assistive map at training time to additionally estimate high-dimensional action quantiles, and then calibrate these quantiles via rigorous uncertainty quantification methods. Specifically, we leverage adaptive conformal prediction which adjusts the intervals over time, reducing the uncertainty bounds when the mapping is performant and increasing the bounds when the mapping consistently mis-predicts. Furthermore, we propose an uncertainty-interval-based mechanism for detecting high-uncertainty user inputs and robot states. We evaluate the efficacy of our proposed approach in a 2D assistive navigation task and two 7DOF Kinova Jaco tasks involving assistive cup grasping and goal reaching. Our findings demonstrate that conformalized assistive teleoperation manages to detect (but not differentiate between) high uncertainty induced by diverse preferences and induced by low-precision trajectories in the mapping's training dataset. On the whole, we see this work as a key step towards enabling robots to quantify their own uncertainty and proactively seek intervention when needed.","sentences":["Assistive robotic arms often have more degrees-of-freedom than a human teleoperator can control with a low-dimensional input, like a joystick.","To overcome this challenge, existing approaches use data-driven methods to learn a mapping from low-dimensional human inputs to high-dimensional robot actions.","However, determining if such a black-box mapping can confidently infer a user's intended high-dimensional action from low-dimensional inputs remains an open problem.","Our key idea is to adapt the assistive map at training time to additionally estimate high-dimensional action quantiles, and then calibrate these quantiles via rigorous uncertainty quantification methods.","Specifically, we leverage adaptive conformal prediction which adjusts the intervals over time, reducing the uncertainty bounds when the mapping is performant and increasing the bounds when the mapping consistently mis-predicts.","Furthermore, we propose an uncertainty-interval-based mechanism for detecting high-uncertainty user inputs and robot states.","We evaluate the efficacy of our proposed approach in a 2D assistive navigation task and two 7DOF Kinova Jaco tasks involving assistive cup grasping and goal reaching.","Our findings demonstrate that conformalized assistive teleoperation manages to detect (but not differentiate between) high uncertainty induced by diverse preferences and induced by low-precision trajectories in the mapping's training dataset.","On the whole, we see this work as a key step towards enabling robots to quantify their own uncertainty and proactively seek intervention when needed."],"url":"http://arxiv.org/abs/2406.07767v1","category":"cs.RO"}
{"created":"2024-06-11 23:15:17","title":"An Integrated Supply Chain Network Design for Advanced Air Mobility Aircraft Manufacturing Using Stochastic Optimization","abstract":"Electric vertical takeoff and landing (eVTOL) aircraft manufacturers await numerous pre-orders for eVTOLs and expect demand for such advanced air mobility (AAM) aircraft to rise dramatically soon. However, eVTOL manufacturers (EMs) cannot commence mass production of commercial eVTOLs due to a lack of supply chain planning for eVTOL manufacturing. The eVTOL supply chain differs from traditional ones due to stringent quality standards and limited suppliers for eVTOL parts, shortages in skilled labor and machinery, and contract renegotiations with major aerospace suppliers. The emerging AAM aircraft market introduces uncertainties in supplier pricing and capacities, eVTOL manufacturing costs, and eVTOL demand, further compounding the supply chain planning challenges for EMs. Despite this critical need, no study has been conducted to develop a comprehensive supply chain planning model for EMs. To address this research gap, we propose a stochastic optimization model for integrated supply chain planning of EMs while maximizing their operating profits under the abovementioned uncertainties. We conduct various numerical cases to analyze the impact of 1) endogenous eVTOL demand influenced by the quality of eVTOLs, 2) supply chain disruptions caused by geopolitical conflicts and resource scarcity, and 3) high-volume eVTOL demand similar to that experienced by automotive manufacturers, on EM supply chain planning. The results indicate that our proposed model is adaptable in all cases and outperforms established benchmark stochastic models. The findings suggest that EMs can commence mass eVTOL production with our model, enabling them to make optimal decisions and profits even under potential disruptions.","sentences":["Electric vertical takeoff and landing (eVTOL) aircraft manufacturers await numerous pre-orders for eVTOLs and expect demand for such advanced air mobility (AAM) aircraft to rise dramatically soon.","However, eVTOL manufacturers (EMs) cannot commence mass production of commercial eVTOLs due to a lack of supply chain planning for eVTOL manufacturing.","The eVTOL supply chain differs from traditional ones due to stringent quality standards and limited suppliers for eVTOL parts, shortages in skilled labor and machinery, and contract renegotiations with major aerospace suppliers.","The emerging AAM aircraft market introduces uncertainties in supplier pricing and capacities, eVTOL manufacturing costs, and eVTOL demand, further compounding the supply chain planning challenges for EMs.","Despite this critical need, no study has been conducted to develop a comprehensive supply chain planning model for EMs.","To address this research gap, we propose a stochastic optimization model for integrated supply chain planning of EMs while maximizing their operating profits under the abovementioned uncertainties.","We conduct various numerical cases to analyze the impact of 1) endogenous eVTOL demand influenced by the quality of eVTOLs, 2) supply chain disruptions caused by geopolitical conflicts and resource scarcity, and 3) high-volume eVTOL demand similar to that experienced by automotive manufacturers, on EM supply chain planning.","The results indicate that our proposed model is adaptable in all cases and outperforms established benchmark stochastic models.","The findings suggest that EMs can commence mass eVTOL production with our model, enabling them to make optimal decisions and profits even under potential disruptions."],"url":"http://arxiv.org/abs/2406.07766v1","category":"math.OC"}
{"created":"2024-06-11 23:00:47","title":"Shear Viscosity of an $N$-Component Gas Mixture using the Chapman--Enskog Method under Anisotropic Scatterings","abstract":"The analytical Chapman--Enskog formula for calculating the shear viscosity $\\eta$ of a relativistic ideal gas, such as a massless quark-gluon plasma, has consistently demonstrated good agreement with the numerical results obtained using the Green--Kubo relation under both isotropic and anisotropic two-body scatterings. However, past analyses of massless, multicomponent quark-gluon plasma have focused on an effective single-component ``gluon gas.\" The Chapman--Enskog formula for multicomponent mixtures with nonzero yet adjustable masses was previously developed for simpler cases of isotropic scatterings. This study aims to obtain the Chapman--Enskog shear viscosity formula for a massless, multicomponent mixture under general anisotropic scatterings. Since the shear viscosity depends on a linearized collision kernel, an approximation formula for the linearized collision kernel is derived under elastic and anisotropic $l+k\\rightarrow l+k$ scatterings. This derived approximation agrees very well with the isotropic two-body kernels provided in previous works for both like and different species. Furthermore, for multicomponent mixtures beyond two species types, an alternative expansion method of the $N$-component Chapman--Enskog viscosity is presented. This is applied to a two-component ``binary\" mixture and compared with the conventional formula for binary viscosity. The agreement between the two, for interacting and noninteracting binary mixtures, varies from moderate to well.","sentences":["The analytical Chapman--Enskog formula for calculating the shear viscosity $\\eta$ of a relativistic ideal gas, such as a massless quark-gluon plasma, has consistently demonstrated good agreement with the numerical results obtained using the Green--Kubo relation under both isotropic and anisotropic two-body scatterings.","However, past analyses of massless, multicomponent quark-gluon plasma have focused on an effective single-component ``gluon gas.\"","The Chapman--Enskog formula for multicomponent mixtures with nonzero yet adjustable masses was previously developed for simpler cases of isotropic scatterings.","This study aims to obtain the Chapman--Enskog shear viscosity formula for a massless, multicomponent mixture under general anisotropic scatterings.","Since the shear viscosity depends on a linearized collision kernel, an approximation formula for the linearized collision kernel is derived under elastic and anisotropic $l+k\\rightarrow l+k$ scatterings.","This derived approximation agrees very well with the isotropic two-body kernels provided in previous works for both like and different species.","Furthermore, for multicomponent mixtures beyond two species types, an alternative expansion method of the $N$-component Chapman--Enskog viscosity is presented.","This is applied to a two-component ``binary\" mixture and compared with the conventional formula for binary viscosity.","The agreement between the two, for interacting and noninteracting binary mixtures, varies from moderate to well."],"url":"http://arxiv.org/abs/2406.07764v1","category":"nucl-th"}
{"created":"2024-06-11 22:52:45","title":"Deep Learning of Structural Morphology Imaged by Scanning X-ray Diffraction Microscopy","abstract":"Scanning X-ray nanodiffraction microscopy is a powerful technique for spatially resolving nanoscale structural morphologies by diffraction contrast. One of the critical challenges in experimental nanodiffraction data analysis is posed by the convergence angle of nanoscale focusing optics which creates simultaneous dependency of the far-field scattering data on three independent components of the local strain tensor - corresponding to dilation and two potential rigid body rotations of the unit cell. All three components are in principle resolvable through a spatially mapped sample tilt series however traditional data analysis is computationally expensive and prone to artifacts. In this study, we implement NanobeamNN, a convolutional neural network specifically tailored to the analysis of scanning probe X-ray microscopy data. NanobeamNN learns lattice strain and rotation angles from simulated diffraction of a focused X-ray nanobeam by an epitaxial thin film and can directly make reasonable predictions on experimental data without the need for additional fine-tuning. We demonstrate that this approach represents a significant advancement in computational speed over conventional methods, as well as a potential improvement in accuracy over the current standard.","sentences":["Scanning X-ray nanodiffraction microscopy is a powerful technique for spatially resolving nanoscale structural morphologies by diffraction contrast.","One of the critical challenges in experimental nanodiffraction data analysis is posed by the convergence angle of nanoscale focusing optics which creates simultaneous dependency of the far-field scattering data on three independent components of the local strain tensor - corresponding to dilation and two potential rigid body rotations of the unit cell.","All three components are in principle resolvable through a spatially mapped sample tilt series however traditional data analysis is computationally expensive and prone to artifacts.","In this study, we implement NanobeamNN, a convolutional neural network specifically tailored to the analysis of scanning probe X-ray microscopy data.","NanobeamNN learns lattice strain and rotation angles from simulated diffraction of a focused X-ray nanobeam by an epitaxial thin film and can directly make reasonable predictions on experimental data without the need for additional fine-tuning.","We demonstrate that this approach represents a significant advancement in computational speed over conventional methods, as well as a potential improvement in accuracy over the current standard."],"url":"http://arxiv.org/abs/2406.07761v1","category":"physics.app-ph"}
{"created":"2024-06-11 22:38:31","title":"High-Precision Surrogate Modeling for Uncertainty Quantification in Complex Slurry Flows","abstract":"Slurry transportation via pipelines is essential for global industries, offering efficiency and environmental benefits. Specifically, the precise calibration of physical parameters for transporting raw phosphate material to fertilizer plants is crucial to minimize energy losses and ensure secure operations. Computational fluid dynamics (CFD) is commonly employed to understand solid concentration, velocity distributions, and flow pressure along the pipeline. However, numerical solutions for slurry flows often entail uncertainties from initial and boundary conditions, emphasizing the need for quantification. This study addresses the challenge by proposing a framework that combines proper orthogonal decomposition and polynomial chaos expansions to quantify uncertainties in two-dimensional phosphate slurry flow simulations. The use of surrogate modeling methods, like polynomial chaos expansion, proves effective in reducing computational costs associated with direct stochastic simulations, especially for complex flows with high spatial variability, as observed in phosphate slurries. Numerical results demonstrate the accuracy of the non-intrusive reduction method in reproducing mean and variance distributions. Moreover, the uncertainty quantification analysis shows that the reduced-order model significantly reduces computational costs compared to the full-order model.","sentences":["Slurry transportation via pipelines is essential for global industries, offering efficiency and environmental benefits.","Specifically, the precise calibration of physical parameters for transporting raw phosphate material to fertilizer plants is crucial to minimize energy losses and ensure secure operations.","Computational fluid dynamics (CFD) is commonly employed to understand solid concentration, velocity distributions, and flow pressure along the pipeline.","However, numerical solutions for slurry flows often entail uncertainties from initial and boundary conditions, emphasizing the need for quantification.","This study addresses the challenge by proposing a framework that combines proper orthogonal decomposition and polynomial chaos expansions to quantify uncertainties in two-dimensional phosphate slurry flow simulations.","The use of surrogate modeling methods, like polynomial chaos expansion, proves effective in reducing computational costs associated with direct stochastic simulations, especially for complex flows with high spatial variability, as observed in phosphate slurries.","Numerical results demonstrate the accuracy of the non-intrusive reduction method in reproducing mean and variance distributions.","Moreover, the uncertainty quantification analysis shows that the reduced-order model significantly reduces computational costs compared to the full-order model."],"url":"http://arxiv.org/abs/2406.07758v1","category":"physics.flu-dyn"}
{"created":"2024-06-11 21:55:20","title":"Back to the Color: Learning Depth to Specific Color Transformation for Unsupervised Depth Estimation","abstract":"Virtual engines have the capability to generate dense depth maps for various synthetic scenes, making them invaluable for training depth estimation models. However, synthetic colors often exhibit significant discrepancies compared to real-world colors, thereby posing challenges for depth estimation in real-world scenes, particularly in complex and uncertain environments encountered in unsupervised monocular depth estimation tasks. To address this issue, we propose Back2Color, a framework that predicts realistic colors from depth utilizing a model trained on real-world data, thus facilitating the transformation of synthetic colors into real-world counterparts. Additionally, by employing the Syn-Real CutMix method for joint training with both real-world unsupervised and synthetic supervised depth samples, we achieve improved performance in monocular depth estimation for real-world scenes. Moreover, to comprehensively address the impact of non-rigid motions on depth estimation, we propose an auto-learning uncertainty temporal-spatial fusion method (Auto-UTSF), which integrates the benefits of unsupervised learning in both temporal and spatial dimensions. Furthermore, we design a depth estimation network (VADepth) based on the Vision Attention Network. Our Back2Color framework demonstrates state-of-the-art performance, as evidenced by improvements in performance metrics and the production of fine-grained details in our predictions, particularly on challenging datasets such as Cityscapes for unsupervised depth estimation.","sentences":["Virtual engines have the capability to generate dense depth maps for various synthetic scenes, making them invaluable for training depth estimation models.","However, synthetic colors often exhibit significant discrepancies compared to real-world colors, thereby posing challenges for depth estimation in real-world scenes, particularly in complex and uncertain environments encountered in unsupervised monocular depth estimation tasks.","To address this issue, we propose Back2Color, a framework that predicts realistic colors from depth utilizing a model trained on real-world data, thus facilitating the transformation of synthetic colors into real-world counterparts.","Additionally, by employing the Syn-Real CutMix method for joint training with both real-world unsupervised and synthetic supervised depth samples, we achieve improved performance in monocular depth estimation for real-world scenes.","Moreover, to comprehensively address the impact of non-rigid motions on depth estimation, we propose an auto-learning uncertainty temporal-spatial fusion method (Auto-UTSF), which integrates the benefits of unsupervised learning in both temporal and spatial dimensions.","Furthermore, we design a depth estimation network (VADepth) based on the Vision Attention Network.","Our Back2Color framework demonstrates state-of-the-art performance, as evidenced by improvements in performance metrics and the production of fine-grained details in our predictions, particularly on challenging datasets such as Cityscapes for unsupervised depth estimation."],"url":"http://arxiv.org/abs/2406.07741v1","category":"cs.CV"}
{"created":"2024-06-11 21:46:03","title":"MultiPragEval: Multilingual Pragmatic Evaluation of Large Language Models","abstract":"As the capabilities of LLMs expand, it becomes increasingly important to evaluate them beyond basic knowledge assessment, focusing on higher-level language understanding. This study introduces MultiPragEval, a robust test suite designed for the multilingual pragmatic evaluation of LLMs across English, German, Korean, and Chinese. Comprising 1200 question units categorized according to Grice's Cooperative Principle and its four conversational maxims, MultiPragEval enables an in-depth assessment of LLMs' contextual awareness and their ability to infer implied meanings. Our findings demonstrate that Claude3-Opus significantly outperforms other models in all tested languages, establishing a state-of-the-art in the field. Among open-source models, Solar-10.7B and Qwen1.5-14B emerge as strong competitors. This study not only leads the way in the multilingual evaluation of LLMs in pragmatic inference but also provides valuable insights into the nuanced capabilities necessary for advanced language comprehension in AI systems.","sentences":["As the capabilities of LLMs expand, it becomes increasingly important to evaluate them beyond basic knowledge assessment, focusing on higher-level language understanding.","This study introduces MultiPragEval, a robust test suite designed for the multilingual pragmatic evaluation of LLMs across English, German, Korean, and Chinese.","Comprising 1200 question units categorized according to Grice's Cooperative Principle and its four conversational maxims, MultiPragEval enables an in-depth assessment of LLMs' contextual awareness and their ability to infer implied meanings.","Our findings demonstrate that Claude3-Opus significantly outperforms other models in all tested languages, establishing a state-of-the-art in the field.","Among open-source models, Solar-10.7B and Qwen1.5-14B emerge as strong competitors.","This study not only leads the way in the multilingual evaluation of LLMs in pragmatic inference but also provides valuable insights into the nuanced capabilities necessary for advanced language comprehension in AI systems."],"url":"http://arxiv.org/abs/2406.07736v1","category":"cs.CL"}
{"created":"2024-06-11 21:44:49","title":"REAL Sampling: Boosting Factuality and Diversity of Open-Ended Generation via Asymptotic Entropy","abstract":"Decoding methods for large language models (LLMs) usually struggle with the tradeoff between ensuring factuality and maintaining diversity. For example, a higher p threshold in the nucleus (top-p) sampling increases the diversity but decreases the factuality, and vice versa. In this paper, we propose REAL (Residual Entropy from Asymptotic Line) sampling, a decoding method that achieves improved factuality and diversity over nucleus sampling by predicting an adaptive threshold of $p$. Specifically, REAL sampling predicts the step-wise likelihood of an LLM to hallucinate, and lowers the p threshold when an LLM is likely to hallucinate. Otherwise, REAL sampling increases the p threshold to boost the diversity. To predict the step-wise hallucination likelihood without supervision, we construct a Token-level Hallucination Forecasting (THF) model to predict the asymptotic entropy (i.e., inherent uncertainty) of the next token by extrapolating the next-token entropies from a series of LLMs with different sizes. If a LLM's entropy is higher than the asymptotic entropy (i.e., the LLM is more uncertain than it should be), the THF model predicts a high hallucination hazard, which leads to a lower p threshold in REAL sampling. In the FactualityPrompts benchmark, we demonstrate that REAL sampling based on a 70M THF model can substantially improve the factuality and diversity of 7B LLMs simultaneously, judged by both retrieval-based metrics and human evaluation. After combined with contrastive decoding, REAL sampling outperforms 9 sampling methods, and generates texts that are more factual than the greedy sampling and more diverse than the nucleus sampling with $p=0.5$. Furthermore, the predicted asymptotic entropy is also a useful unsupervised signal for hallucination detection tasks.","sentences":["Decoding methods for large language models (LLMs) usually struggle with the tradeoff between ensuring factuality and maintaining diversity.","For example, a higher p threshold in the nucleus (top-p) sampling increases the diversity but decreases the factuality, and vice versa.","In this paper, we propose REAL (Residual Entropy from Asymptotic Line) sampling, a decoding method that achieves improved factuality and diversity over nucleus sampling by predicting an adaptive threshold of $p$. Specifically, REAL sampling predicts the step-wise likelihood of an LLM to hallucinate, and lowers the p threshold when an LLM is likely to hallucinate.","Otherwise, REAL sampling increases the p threshold to boost the diversity.","To predict the step-wise hallucination likelihood without supervision, we construct a Token-level Hallucination Forecasting (THF) model to predict the asymptotic entropy (i.e., inherent uncertainty) of the next token by extrapolating the next-token entropies from a series of LLMs with different sizes.","If a LLM's entropy is higher than the asymptotic entropy (i.e., the LLM is more uncertain than it should be), the THF model predicts a high hallucination hazard, which leads to a lower p threshold in REAL sampling.","In the FactualityPrompts benchmark, we demonstrate that REAL sampling based on a 70M THF model can substantially improve the factuality and diversity of 7B LLMs simultaneously, judged by both retrieval-based metrics and human evaluation.","After combined with contrastive decoding, REAL sampling outperforms 9 sampling methods, and generates texts that are more factual than the greedy sampling and more diverse than the nucleus sampling with $p=0.5$. Furthermore, the predicted asymptotic entropy is also a useful unsupervised signal for hallucination detection tasks."],"url":"http://arxiv.org/abs/2406.07735v1","category":"cs.CL"}
{"created":"2024-06-11 21:40:55","title":"Gravitational Wormholes","abstract":"Spacetime wormholes are evidently an essential component of the construction of a time machine. Within the context of general relativity, such objects require, for their formation, exotic matter -- matter that violates at least one of the standard energy conditions. Here, we explore the possibility that higher-curvature gravity theories might permit the construction of a wormhole without any matter at all. In particular, we consider the simplest form of a generalized quasi topological theory in four spacetime dimensions, known as Einsteinian Cubic Gravity. This theory has a number of promising features that make it an interesting phenomenological competitor to general relativity, including having non-hairy generalizations of the Schwarzschild black hole and linearized equations of second order around maximally symmetric backgrounds. By matching series solutions near the horizon and at large distances, we find evidence that strong asymptotically AdS wormhole solutions can be constructed, with strong curvature effects ensuring that the wormhole throat can exist.","sentences":["Spacetime wormholes are evidently an essential component of the construction of a time machine.","Within the context of general relativity, such objects require, for their formation, exotic matter -- matter that violates at least one of the standard energy conditions.","Here, we explore the possibility that higher-curvature gravity theories might permit the construction of a wormhole without any matter at all.","In particular, we consider the simplest form of a generalized quasi topological theory in four spacetime dimensions, known as Einsteinian Cubic Gravity.","This theory has a number of promising features that make it an interesting phenomenological competitor to general relativity, including having non-hairy generalizations of the Schwarzschild black hole and linearized equations of second order around maximally symmetric backgrounds.","By matching series solutions near the horizon and at large distances, we find evidence that strong asymptotically AdS wormhole solutions can be constructed, with strong curvature effects ensuring that the wormhole throat can exist."],"url":"http://arxiv.org/abs/2406.07734v1","category":"gr-qc"}
{"created":"2024-06-11 21:30:53","title":"Experimenting with D-Wave Quantum Annealers on Prime Factorization problems","abstract":"This paper builds on top of a paper we have published very recently, in which we have proposed a novel approach to prime factorization (PF) by quantum annealing, where 8,219,999=32,749x251 was the highest prime product we were able to factorize -- which, to the best of our knowledge is the largest number which was ever factorized by means of a quantum device. The series of annealing experiments which led us to these results, however, did not follow a straight-line path; rather, they involved a convoluted trial-and-error process, full of failed or partially-failed attempts and backtracks, which only in the end drove us to find the successful annealing strategies. In this paper, we delve into the reasoning behind our experimental decisions and provide an account of some of the attempts we have taken before conceiving the final strategies that allowed us to achieve the results. This involves also a bunch of ideas, techniques, and strategies we investigated which, although turned out to be inferior wrt. those we adopted in the end, may instead provide insights to a more-specialized audience of D-Wave users and practitioners. In particular, we show the following insights: ($i$) different initialization techniques affect performances, among which flux biases are effective when targeting locally-structured embeddings; ($ii$) chain strengths have a lower impact in locally-structured embeddings compared to problem relying on global embeddings; ($iii$) there is a trade-off between broken chain and excited CFAs, suggesting an incremental annealing offset remedy approach based on the modules instead of single qubits. Thus, by sharing the details of our experiences, we aim to provide insights into the evolving landscape of quantum annealing, and help people access and effectively use D-Wave quantum annealers.","sentences":["This paper builds on top of a paper we have published very recently, in which we have proposed a novel approach to prime factorization (PF) by quantum annealing, where 8,219,999=32,749x251 was the highest prime product we were able to factorize -- which, to the best of our knowledge is the largest number which was ever factorized by means of a quantum device.","The series of annealing experiments which led us to these results, however, did not follow a straight-line path; rather, they involved a convoluted trial-and-error process, full of failed or partially-failed attempts and backtracks, which only in the end drove us to find the successful annealing strategies.","In this paper, we delve into the reasoning behind our experimental decisions and provide an account of some of the attempts we have taken before conceiving the final strategies that allowed us to achieve the results.","This involves also a bunch of ideas, techniques, and strategies we investigated which, although turned out to be inferior wrt.","those we adopted in the end, may instead provide insights to a more-specialized audience of D-Wave users and practitioners.","In particular, we show the following insights: ($i$) different initialization techniques affect performances, among which flux biases are effective when targeting locally-structured embeddings; ($ii$) chain strengths have a lower impact in locally-structured embeddings compared to problem relying on global embeddings; ($iii$) there is a trade-off between broken chain and excited CFAs, suggesting an incremental annealing offset remedy approach based on the modules instead of single qubits.","Thus, by sharing the details of our experiences, we aim to provide insights into the evolving landscape of quantum annealing, and help people access and effectively use D-Wave quantum annealers."],"url":"http://arxiv.org/abs/2406.07732v1","category":"quant-ph"}
{"created":"2024-06-11 21:25:14","title":"\"It answers questions that I didn't know I had\": Ph.D. Students' Evaluation of an Information Sharing Knowledge Graph","abstract":"Interdisciplinary PhD programs can be challenging as the vital information needed by students may not be readily available, it is scattered across university's websites, while tacit knowledge can be obtained only by interacting with people. Hence, there is a need to develop a knowledge management model to create, query, and maintain a knowledge repository for interdisciplinary students. We propose a knowledge graph containing information on critical categories and their relationships, extracted from multiple sources, essential for interdisciplinary PhD students. This study evaluates the usability of a participatory designed knowledge graph intended to facilitate information exchange and decision-making. The usability findings demonstrate that interaction with this knowledge graph benefits PhD students by notably reducing uncertainty and academic stress, particularly among newcomers. Knowledge graph supported them in decision making, especially when choosing collaborators in an interdisciplinary setting. Key helpful features are related to exploring student faculty networks, milestones tracking, rapid access to aggregated data, and insights into crowdsourced fellow students' activities. The knowledge graph provides a solution to meet the personalized needs of doctoral researchers and has the potential to improve the information discovery and decision-making process substantially. It also includes the tacit knowledge exchange support missing from most current approaches, which is critical for this population and establishing interdisciplinary collaborations. This approach can be applied to other interdisciplinary programs and domains globally.","sentences":["Interdisciplinary PhD programs can be challenging as the vital information needed by students may not be readily available, it is scattered across university's websites, while tacit knowledge can be obtained only by interacting with people.","Hence, there is a need to develop a knowledge management model to create, query, and maintain a knowledge repository for interdisciplinary students.","We propose a knowledge graph containing information on critical categories and their relationships, extracted from multiple sources, essential for interdisciplinary PhD students.","This study evaluates the usability of a participatory designed knowledge graph intended to facilitate information exchange and decision-making.","The usability findings demonstrate that interaction with this knowledge graph benefits PhD students by notably reducing uncertainty and academic stress, particularly among newcomers.","Knowledge graph supported them in decision making, especially when choosing collaborators in an interdisciplinary setting.","Key helpful features are related to exploring student faculty networks, milestones tracking, rapid access to aggregated data, and insights into crowdsourced fellow students' activities.","The knowledge graph provides a solution to meet the personalized needs of doctoral researchers and has the potential to improve the information discovery and decision-making process substantially.","It also includes the tacit knowledge exchange support missing from most current approaches, which is critical for this population and establishing interdisciplinary collaborations.","This approach can be applied to other interdisciplinary programs and domains globally."],"url":"http://arxiv.org/abs/2406.07730v1","category":"cs.HC"}
{"created":"2024-06-11 21:09:45","title":"A Concise Mathematical Description of Active Inference in Discrete Time","abstract":"In this paper we present a concise mathematical description of active inference in discrete time. The main part of the paper serves as a general introduction to the topic, including an example illustrating the theory on action selection. In the appendix the more subtle mathematical details are discussed. This part is aimed at readers who have already studied the active inference literature but struggle to make sense of the mathematical details and derivations. Throughout the whole manuscript, special attention has been paid to adopting notation that is both precise and in line with standard mathematical texts. All equations and derivations are linked to specific equation numbers in other popular text on the topic. Furthermore, Python code is provided that implements the action selection mechanism described in this paper and is compatible with pymdp environments.","sentences":["In this paper we present a concise mathematical description of active inference in discrete time.","The main part of the paper serves as a general introduction to the topic, including an example illustrating the theory on action selection.","In the appendix the more subtle mathematical details are discussed.","This part is aimed at readers who have already studied the active inference literature but struggle to make sense of the mathematical details and derivations.","Throughout the whole manuscript, special attention has been paid to adopting notation that is both precise and in line with standard mathematical texts.","All equations and derivations are linked to specific equation numbers in other popular text on the topic.","Furthermore, Python code is provided that implements the action selection mechanism described in this paper and is compatible with pymdp environments."],"url":"http://arxiv.org/abs/2406.07726v1","category":"cs.LG"}
{"created":"2024-06-11 20:58:19","title":"A Set Cover Mapping Heuristic for Demand-Robust Fleet Size Vehicle Routing Problem with Time Windows and Compatibility Constraints","abstract":"We study the demand-robust fleet size vehicle routing problem with time windows and compatibility constraints. Unlike traditional robust optimization, which considers uncertainty in the data, demand-robust optimization considers uncertainty in which constraints must be satisfied. This paper is the first to solve a practical demand-robust optimization problem at large scale. We present an MILP formulation and also propose a heuristic that maps the problem to set cover in polynomial time. We show that under modest assumptions the relative difference in time complexity from a standard branch-and-bound algorithm to the proposed heuristic scales exponentially with the size of the problem. We evaluate our heuristic using a simulation case study on the Solomon benchmark instances for a variety of practical problem sizes, and compare with Gurobi. The empirical approximation ratio remains below 2.0.","sentences":["We study the demand-robust fleet size vehicle routing problem with time windows and compatibility constraints.","Unlike traditional robust optimization, which considers uncertainty in the data, demand-robust optimization considers uncertainty in which constraints must be satisfied.","This paper is the first to solve a practical demand-robust optimization problem at large scale.","We present an MILP formulation and also propose a heuristic that maps the problem to set cover in polynomial time.","We show that under modest assumptions the relative difference in time complexity from a standard branch-and-bound algorithm to the proposed heuristic scales exponentially with the size of the problem.","We evaluate our heuristic using a simulation case study on the Solomon benchmark instances for a variety of practical problem sizes, and compare with Gurobi.","The empirical approximation ratio remains below 2.0."],"url":"http://arxiv.org/abs/2406.07719v1","category":"eess.SY"}
{"created":"2024-06-11 20:46:32","title":"Loss Gradient Gaussian Width based Generalization and Optimization Guarantees","abstract":"Generalization and optimization guarantees on the population loss in machine learning often rely on uniform convergence based analysis, typically based on the Rademacher complexity of the predictors. The rich representation power of modern models has led to concerns about this approach. In this paper, we present generalization and optimization guarantees in terms of the complexity of the gradients, as measured by the Loss Gradient Gaussian Width (LGGW). First, we introduce generalization guarantees directly in terms of the LGGW under a flexible gradient domination condition, which we demonstrate to hold empirically for deep models. Second, we show that sample reuse in finite sum (stochastic) optimization does not make the empirical gradient deviate from the population gradient as long as the LGGW is small. Third, focusing on deep networks, we present results showing how to bound their LGGW under mild assumptions. In particular, we show that their LGGW can be bounded (a) by the $L_2$-norm of the loss Hessian eigenvalues, which has been empirically shown to be $\\tilde{O}(1)$ for commonly used deep models; and (b) in terms of the Gaussian width of the featurizer, i.e., the output of the last-but-one layer. To our knowledge, our generalization and optimization guarantees in terms of LGGW are the first results of its kind, avoid the pitfalls of predictor Rademacher complexity based analysis, and hold considerable promise towards quantitatively tight bounds for deep models.","sentences":["Generalization and optimization guarantees on the population loss in machine learning often rely on uniform convergence based analysis, typically based on the Rademacher complexity of the predictors.","The rich representation power of modern models has led to concerns about this approach.","In this paper, we present generalization and optimization guarantees in terms of the complexity of the gradients, as measured by the Loss Gradient Gaussian Width (LGGW).","First, we introduce generalization guarantees directly in terms of the LGGW under a flexible gradient domination condition, which we demonstrate to hold empirically for deep models.","Second, we show that sample reuse in finite sum (stochastic) optimization does not make the empirical gradient deviate from the population gradient as long as the LGGW is small.","Third, focusing on deep networks, we present results showing how to bound their LGGW under mild assumptions.","In particular, we show that their LGGW can be bounded (a) by the $L_2$-norm of the loss","Hessian eigenvalues, which has been empirically shown to be $\\tilde{O}(1)$ for commonly used deep models; and (b) in terms of the Gaussian width of the featurizer, i.e., the output of the last-but-one layer.","To our knowledge, our generalization and optimization guarantees in terms of LGGW are the first results of its kind, avoid the pitfalls of predictor Rademacher complexity based analysis, and hold considerable promise towards quantitatively tight bounds for deep models."],"url":"http://arxiv.org/abs/2406.07712v1","category":"cs.LG"}
{"created":"2024-06-11 20:45:40","title":"Vehicle Speed Detection System Utilizing YOLOv8: Enhancing Road Safety and Traffic Management for Metropolitan Areas","abstract":"In order to ensure traffic safety through a reduction in fatalities and accidents, vehicle speed detection is essential. Relentless driving practices are discouraged by the enforcement of speed restrictions, which are made possible by accurate monitoring of vehicle speeds. Road accidents remain one of the leading causes of death in Bangladesh. The Bangladesh Passenger Welfare Association stated in 2023 that 7,902 individuals lost their lives in traffic accidents during the course of the year. Efficient vehicle speed detection is essential to maintaining traffic safety. Reliable speed detection can also help gather important traffic data, which makes it easier to optimize traffic flow and provide safer road infrastructure. The YOLOv8 model can recognize and track cars in videos with greater speed and accuracy when trained under close supervision. By providing insights into the application of supervised learning in object identification for vehicle speed estimation and concentrating on the particular traffic conditions and safety concerns in Bangladesh, this work represents a noteworthy contribution to the area. The MAE was 3.5 and RMSE was 4.22 between the predicted speed of our model and the actual speed or the ground truth measured by the speedometer Promising increased efficiency and wider applicability in a variety of traffic conditions, the suggested solution offers a financially viable substitute for conventional approaches.","sentences":["In order to ensure traffic safety through a reduction in fatalities and accidents, vehicle speed detection is essential.","Relentless driving practices are discouraged by the enforcement of speed restrictions, which are made possible by accurate monitoring of vehicle speeds.","Road accidents remain one of the leading causes of death in Bangladesh.","The Bangladesh Passenger Welfare Association stated in 2023 that 7,902 individuals lost their lives in traffic accidents during the course of the year.","Efficient vehicle speed detection is essential to maintaining traffic safety.","Reliable speed detection can also help gather important traffic data, which makes it easier to optimize traffic flow and provide safer road infrastructure.","The YOLOv8 model can recognize and track cars in videos with greater speed and accuracy when trained under close supervision.","By providing insights into the application of supervised learning in object identification for vehicle speed estimation and concentrating on the particular traffic conditions and safety concerns in Bangladesh, this work represents a noteworthy contribution to the area.","The MAE was 3.5 and RMSE was 4.22 between the predicted speed of our model and the actual speed or the ground truth measured by the speedometer Promising increased efficiency and wider applicability in a variety of traffic conditions, the suggested solution offers a financially viable substitute for conventional approaches."],"url":"http://arxiv.org/abs/2406.07710v1","category":"cs.CV"}
{"created":"2024-06-11 20:38:41","title":"A Deep Learning Approach to Detect Complete Safety Equipment For Construction Workers Based On YOLOv7","abstract":"In the construction sector, ensuring worker safety is of the utmost significance. In this study, a deep learning-based technique is presented for identifying safety gear worn by construction workers, such as helmets, goggles, jackets, gloves, and footwears. The recommended approach uses the YOLO v7 (You Only Look Once) object detection algorithm to precisely locate these safety items. The dataset utilized in this work consists of labeled images split into training, testing and validation sets. Each image has bounding box labels that indicate where the safety equipment is located within the image. The model is trained to identify and categorize the safety equipment based on the labeled dataset through an iterative training approach. We used custom dataset to train this model. Our trained model performed admirably well, with good precision, recall, and F1-score for safety equipment recognition. Also, the model's evaluation produced encouraging results, with a mAP@0.5 score of 87.7\\%. The model performs effectively, making it possible to quickly identify safety equipment violations on building sites. A thorough evaluation of the outcomes reveals the model's advantages and points up potential areas for development. By offering an automatic and trustworthy method for safety equipment detection, this research makes a contribution to the fields of computer vision and workplace safety. The proposed deep learning-based approach will increase safety compliance and reduce the risk of accidents in the construction industry","sentences":["In the construction sector, ensuring worker safety is of the utmost significance.","In this study, a deep learning-based technique is presented for identifying safety gear worn by construction workers, such as helmets, goggles, jackets, gloves, and footwears.","The recommended approach uses the YOLO v7 (You Only Look Once) object detection algorithm to precisely locate these safety items.","The dataset utilized in this work consists of labeled images split into training, testing and validation sets.","Each image has bounding box labels that indicate where the safety equipment is located within the image.","The model is trained to identify and categorize the safety equipment based on the labeled dataset through an iterative training approach.","We used custom dataset to train this model.","Our trained model performed admirably well, with good precision, recall, and F1-score for safety equipment recognition.","Also, the model's evaluation produced encouraging results, with a mAP@0.5 score of 87.7\\%.","The model performs effectively, making it possible to quickly identify safety equipment violations on building sites.","A thorough evaluation of the outcomes reveals the model's advantages and points up potential areas for development.","By offering an automatic and trustworthy method for safety equipment detection, this research makes a contribution to the fields of computer vision and workplace safety.","The proposed deep learning-based approach will increase safety compliance and reduce the risk of accidents in the construction industry"],"url":"http://arxiv.org/abs/2406.07707v1","category":"cs.CV"}
{"created":"2024-06-11 20:29:25","title":"Graphical Perception of Saliency-based Model Explanations","abstract":"In recent years, considerable work has been devoted to explaining predictive, deep learning-based models, and in turn how to evaluate explanations. An important class of evaluation methods are ones that are human-centered, which typically require the communication of explanations through visualizations. And while visualization plays a critical role in perceiving and understanding model explanations, how visualization design impacts human perception of explanations remains poorly understood. In this work, we study the graphical perception of model explanations, specifically, saliency-based explanations for visual recognition models. We propose an experimental design to investigate how human perception is influenced by visualization design, wherein we study the task of alignment assessment, or whether a saliency map aligns with an object in an image. Our findings show that factors related to visualization design decisions, the type of alignment, and qualities of the saliency map all play important roles in how humans perceive saliency-based visual explanations.","sentences":["In recent years, considerable work has been devoted to explaining predictive, deep learning-based models, and in turn how to evaluate explanations.","An important class of evaluation methods are ones that are human-centered, which typically require the communication of explanations through visualizations.","And while visualization plays a critical role in perceiving and understanding model explanations, how visualization design impacts human perception of explanations remains poorly understood.","In this work, we study the graphical perception of model explanations, specifically, saliency-based explanations for visual recognition models.","We propose an experimental design to investigate how human perception is influenced by visualization design, wherein we study the task of alignment assessment, or whether a saliency map aligns with an object in an image.","Our findings show that factors related to visualization design decisions, the type of alignment, and qualities of the saliency map all play important roles in how humans perceive saliency-based visual explanations."],"url":"http://arxiv.org/abs/2406.07702v1","category":"cs.CV"}
{"created":"2024-06-12 17:59:49","title":"Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models","abstract":"Seeing clearly with high resolution is a foundation of Large Multimodal Models (LMMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the most important tokens accounting for the user question will be further selected by a similarity-based selector. Our empirical results demonstrate a `less is more' pattern, where \\textit{utilizing fewer but more informative local image tokens leads to improved performance}. Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed LMM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data.","sentences":["Seeing clearly with high resolution is a foundation of Large Multimodal Models (LMMs), which has been proven to be vital for visual perception and reasoning.","Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former.","This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context.","In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy.","Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks.","With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the most important tokens accounting for the user question will be further selected by a similarity-based selector.","Our empirical results demonstrate a `less is more' pattern, where \\textit{utilizing fewer but more informative local image tokens leads to improved performance}.","Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results.","We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects.","Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer.","The proposed method, termed LMM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data."],"url":"http://arxiv.org/abs/2406.08487v1","category":"cs.CV"}
{"created":"2024-06-12 17:51:47","title":"Bridging the Gap: Unravelling Local Government Data Sharing Barriers in Estonia and Beyond","abstract":"Estonia's digital government success has received global acclaim, yet its Open Government Data (OGD) initiatives, especially at the local level, encounter persistent challenges. Despite significant progress of national OGD initiative in OGD rankings, local governments lag in OGD provision. This study aims to examine barriers hindering municipalities from openly sharing OGD. Employing a qualitative approach through interviews with Estonian municipalities and drawing on the OGD-adapted Innovation Resistance Theory model, the study sheds light on barriers impeding OGD sharing. Practical recommendations are proposed to bridge the gap between national policies and local implementation, including enhancing awareness, improving data governance frameworks, and fostering collaboration be-tween local and national authorities. By addressing overlooked weaknesses in the Estonian open data ecosystem and providing actionable recommendations, this research contributes to a more resilient and sustainable open data ecosystem. Additionally, by validating the OGD-adapted Innovation Resistance Theory model and proposing a revised version tailored for local government contexts, the study advances theoretical frameworks for understanding data sharing resistance. Ultimately, this study serves as a call to action for policymakers and practitioners to prioritize local OGD initiatives.","sentences":["Estonia's digital government success has received global acclaim, yet its Open Government Data (OGD) initiatives, especially at the local level, encounter persistent challenges.","Despite significant progress of national OGD initiative in OGD rankings, local governments lag in OGD provision.","This study aims to examine barriers hindering municipalities from openly sharing OGD.","Employing a qualitative approach through interviews with Estonian municipalities and drawing on the OGD-adapted Innovation Resistance Theory model, the study sheds light on barriers impeding OGD sharing.","Practical recommendations are proposed to bridge the gap between national policies and local implementation, including enhancing awareness, improving data governance frameworks, and fostering collaboration be-tween local and national authorities.","By addressing overlooked weaknesses in the Estonian open data ecosystem and providing actionable recommendations, this research contributes to a more resilient and sustainable open data ecosystem.","Additionally, by validating the OGD-adapted Innovation Resistance Theory model and proposing a revised version tailored for local government contexts, the study advances theoretical frameworks for understanding data sharing resistance.","Ultimately, this study serves as a call to action for policymakers and practitioners to prioritize local OGD initiatives."],"url":"http://arxiv.org/abs/2406.08461v1","category":"cs.CY"}
{"created":"2024-06-12 15:49:41","title":"Doing Battle with the Sun: Lessons From LEO and Operating a Satellite Constellation in the Elevated Atmospheric Drag Environment of Solar Cycle 25","abstract":"Capella Space, which designs, builds, and operates a constellation of Synthetic Aperture Radar (SAR) Earth-imaging small satellites, faced new challenges with the onset of Solar Cycle 25. By mid-2022, it had become clear that solar activity levels were far exceeding the 2019 prediction published by the National Atmospheric and Oceanic Administration's (NOAA) Space Weather Prediction Center (SWPC). This resulted in the atmospheric density of low Earth orbit (LEO) increasing 2-3x higher than predicted. While this raises difficulties for all satellite operators, Capella's satellites are especially sensitive to aerodynamic drag due to the high surface area of their large deployable radar reflectors. This unpredicted increase in drag threatened premature deorbit and reentry of some of Capella's fleet of spacecraft.   This paper explores Capella's strategic response to this problem at all layers of the satellite lifecycle, examining the engineering challenges and insights gained from adapting an operational constellation to rapidly changing space weather conditions. A key development was the implementation of \"low drag mode\", which increased on-orbit satellite lifetime by 24% and decreased accumulated momentum from aerodynamic torques by 20-30%. The paper shares operational tradeoffs and lessons from the development, deployment, and validation of this flight mode, offering valuable insights for satellite operators facing similar challenges in LEO's current elevated drag environment.","sentences":["Capella Space, which designs, builds, and operates a constellation of Synthetic Aperture Radar (SAR) Earth-imaging small satellites, faced new challenges with the onset of Solar Cycle 25.","By mid-2022, it had become clear that solar activity levels were far exceeding the 2019 prediction published by the National Atmospheric and Oceanic Administration's (NOAA) Space Weather Prediction Center (SWPC).","This resulted in the atmospheric density of low Earth orbit (LEO) increasing 2-3x higher than predicted.","While this raises difficulties for all satellite operators, Capella's satellites are especially sensitive to aerodynamic drag due to the high surface area of their large deployable radar reflectors.","This unpredicted increase in drag threatened premature deorbit and reentry of some of Capella's fleet of spacecraft.   ","This paper explores Capella's strategic response to this problem at all layers of the satellite lifecycle, examining the engineering challenges and insights gained from adapting an operational constellation to rapidly changing space weather conditions.","A key development was the implementation of \"low drag mode\", which increased on-orbit satellite lifetime by 24% and decreased accumulated momentum from aerodynamic torques by 20-30%.","The paper shares operational tradeoffs and lessons from the development, deployment, and validation of this flight mode, offering valuable insights for satellite operators facing similar challenges in LEO's current elevated drag environment."],"url":"http://arxiv.org/abs/2406.08342v1","category":"physics.space-ph"}
{"created":"2024-06-12 10:36:27","title":"Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey","abstract":"Compared to traditional sentiment analysis, which only considers text, multimodal sentiment analysis needs to consider emotional signals from multimodal sources simultaneously and is therefore more consistent with the way how humans process sentiment in real-world scenarios. It involves processing emotional information from various sources such as natural language, images, videos, audio, physiological signals, etc. However, although other modalities also contain diverse emotional cues, natural language usually contains richer contextual information and therefore always occupies a crucial position in multimodal sentiment analysis. The emergence of ChatGPT has opened up immense potential for applying large language models (LLMs) to text-centric multimodal tasks. However, it is still unclear how existing LLMs can adapt better to text-centric multimodal sentiment analysis tasks. This survey aims to (1) present a comprehensive review of recent research in text-centric multimodal sentiment analysis tasks, (2) examine the potential of LLMs for text-centric multimodal sentiment analysis, outlining their approaches, advantages, and limitations, (3) summarize the application scenarios of LLM-based multimodal sentiment analysis technology, and (4) explore the challenges and potential research directions for multimodal sentiment analysis in the future.","sentences":["Compared to traditional sentiment analysis, which only considers text, multimodal sentiment analysis needs to consider emotional signals from multimodal sources simultaneously and is therefore more consistent with the way how humans process sentiment in real-world scenarios.","It involves processing emotional information from various sources such as natural language, images, videos, audio, physiological signals, etc.","However, although other modalities also contain diverse emotional cues, natural language usually contains richer contextual information and therefore always occupies a crucial position in multimodal sentiment analysis.","The emergence of ChatGPT has opened up immense potential for applying large language models (LLMs) to text-centric multimodal tasks.","However, it is still unclear how existing LLMs can adapt better to text-centric multimodal sentiment analysis tasks.","This survey aims to (1) present a comprehensive review of recent research in text-centric multimodal sentiment analysis tasks, (2) examine the potential of LLMs for text-centric multimodal sentiment analysis, outlining their approaches, advantages, and limitations, (3) summarize the application scenarios of LLM-based multimodal sentiment analysis technology, and (4) explore the challenges and potential research directions for multimodal sentiment analysis in the future."],"url":"http://arxiv.org/abs/2406.08068v1","category":"cs.CL"}
{"created":"2024-06-12 09:02:09","title":"Growth of VO2-ZnS Thin Film Cavity for Adaptive Thermal Emission","abstract":"Low-weight, passive, thermal-adaptive radiation technologies are needed to maintain an operable temperature for spacecraft while they experience various energy fluxes. In this study, we used a thin-film coating with the Fabry-Perot (FP) effect to enhance emissivity contrast ({\\Delta}{\\epsilon}) between VO2 phase-change states. This coating utilizes a novel hybrid material architecture that combines VO2 with a mid- and long-wave infrared transparent chalcogenide, zinc sulfide (ZnS), as a cavity spacer layer. We simulated the design parameter space to obtain a theoretical maximum {\\Delta}{\\epsilon} of 0.63 and grew prototype devices. Using X-ray diffraction, Raman spectroscopy, and Fourier Transform Infrared (FTIR) Spectroscopy, we determined that an intermediate buffer layer of TiO2 is necessary to execute the crystalline growth of monoclinic VO2 on ZnS. Through temperature-dependent FTIR spectroscopy measurements, our fabricated devices demonstrated FP-cavity enhanced adaptive thermal emittance.","sentences":["Low-weight, passive, thermal-adaptive radiation technologies are needed to maintain an operable temperature for spacecraft while they experience various energy fluxes.","In this study, we used a thin-film coating with the Fabry-Perot (FP) effect to enhance emissivity contrast ({\\Delta}{\\epsilon}) between VO2 phase-change states.","This coating utilizes a novel hybrid material architecture that combines VO2 with a mid- and long-wave infrared transparent chalcogenide, zinc sulfide (ZnS), as a cavity spacer layer.","We simulated the design parameter space to obtain a theoretical maximum {\\Delta}{\\epsilon} of 0.63 and grew prototype devices.","Using X-ray diffraction, Raman spectroscopy, and Fourier Transform Infrared (FTIR) Spectroscopy, we determined that an intermediate buffer layer of TiO2 is necessary to execute the crystalline growth of monoclinic VO2 on ZnS.","Through temperature-dependent FTIR spectroscopy measurements, our fabricated devices demonstrated FP-cavity enhanced adaptive thermal emittance."],"url":"http://arxiv.org/abs/2406.08011v1","category":"physics.optics"}
{"created":"2024-06-12 08:06:31","title":"Reinforcement Learning for High-Level Strategic Control in Tower Defense Games","abstract":"In strategy games, one of the most important aspects of game design is maintaining a sense of challenge for players. Many mobile titles feature quick gameplay loops that allow players to progress steadily, requiring an abundance of levels and puzzles to prevent them from reaching the end too quickly. As with any content creation, testing and validation are essential to ensure engaging gameplay mechanics, enjoyable game assets, and playable levels. In this paper, we propose an automated approach that can be leveraged for gameplay testing and validation that combines traditional scripted methods with reinforcement learning, reaping the benefits of both approaches while adapting to new situations similarly to how a human player would. We test our solution on a popular tower defense game, Plants vs. Zombies. The results show that combining a learned approach, such as reinforcement learning, with a scripted AI produces a higher-performing and more robust agent than using only heuristic AI, achieving a 57.12% success rate compared to 47.95% in a set of 40 levels. Moreover, the results demonstrate the difficulty of training a general agent for this type of puzzle-like game.","sentences":["In strategy games, one of the most important aspects of game design is maintaining a sense of challenge for players.","Many mobile titles feature quick gameplay loops that allow players to progress steadily, requiring an abundance of levels and puzzles to prevent them from reaching the end too quickly.","As with any content creation, testing and validation are essential to ensure engaging gameplay mechanics, enjoyable game assets, and playable levels.","In this paper, we propose an automated approach that can be leveraged for gameplay testing and validation that combines traditional scripted methods with reinforcement learning, reaping the benefits of both approaches while adapting to new situations similarly to how a human player would.","We test our solution on a popular tower defense game, Plants vs. Zombies.","The results show that combining a learned approach, such as reinforcement learning, with a scripted AI produces a higher-performing and more robust agent than using only heuristic AI, achieving a 57.12% success rate compared to 47.95% in a set of 40 levels.","Moreover, the results demonstrate the difficulty of training a general agent for this type of puzzle-like game."],"url":"http://arxiv.org/abs/2406.07980v1","category":"cs.LG"}
{"created":"2024-06-12 07:13:31","title":"Multi-Teacher Multi-Objective Meta-Learning for Zero-Shot Hyperspectral Band Selection","abstract":"Band selection plays a crucial role in hyperspectral image classification by removing redundant and noisy bands and retaining discriminative ones. However, most existing deep learning-based methods are aimed at dealing with a specific band selection dataset, and need to retrain parameters for new datasets, which significantly limits their generalizability.To address this issue, a novel multi-teacher multi-objective meta-learning network (M$^3$BS) is proposed for zero-shot hyperspectral band selection. In M$^3$BS, a generalizable graph convolution network (GCN) is constructed to generate dataset-agnostic base, and extract compatible meta-knowledge from multiple band selection tasks. To enhance the ability of meta-knowledge extraction, multiple band selection teachers are introduced to provide diverse high-quality experiences.strategy Finally, subsequent classification tasks are attached and jointly optimized with multi-teacher band selection tasks through multi-objective meta-learning in an end-to-end trainable way. Multi-objective meta-learning guarantees to coordinate diverse optimization objectives automatically and adapt to various datasets simultaneously. Once the optimization is accomplished, the acquired meta-knowledge can be directly transferred to unseen datasets without any retraining or fine-tuning. Experimental results demonstrate the effectiveness and efficiency of our proposed method on par with state-of-the-art baselines for zero-shot hyperspectral band selection.","sentences":["Band selection plays a crucial role in hyperspectral image classification by removing redundant and noisy bands and retaining discriminative ones.","However, most existing deep learning-based methods are aimed at dealing with a specific band selection dataset, and need to retrain parameters for new datasets, which significantly limits their generalizability.","To address this issue, a novel multi-teacher multi-objective meta-learning network (M$^3$BS) is proposed for zero-shot hyperspectral band selection.","In M$^3$BS, a generalizable graph convolution network (GCN) is constructed to generate dataset-agnostic base, and extract compatible meta-knowledge from multiple band selection tasks.","To enhance the ability of meta-knowledge extraction, multiple band selection teachers are introduced to provide diverse high-quality experiences.strategy Finally, subsequent classification tasks are attached and jointly optimized with multi-teacher band selection tasks through multi-objective meta-learning in an end-to-end trainable way.","Multi-objective meta-learning guarantees to coordinate diverse optimization objectives automatically and adapt to various datasets simultaneously.","Once the optimization is accomplished, the acquired meta-knowledge can be directly transferred to unseen datasets without any retraining or fine-tuning.","Experimental results demonstrate the effectiveness and efficiency of our proposed method on par with state-of-the-art baselines for zero-shot hyperspectral band selection."],"url":"http://arxiv.org/abs/2406.07949v1","category":"cs.CV"}
{"created":"2024-06-12 07:03:43","title":"Global-in-time energy stability: a powerful analysis tool for the gradient flow problem without maximum principle or Lipschitz assumption","abstract":"Before proving (unconditional) energy stability for gradient flows, most existing studies either require a strong Lipschitz condition regarding the non-linearity or certain $L^{\\infty}$ bounds on the numerical solutions (the maximum principle). However, proving energy stability without such premises is a very challenging task. In this paper, we aim to develop a novel analytical tool, namely global-in-time energy stability, to demonstrate energy dissipation without assuming any strong Lipschitz condition or $L^{\\infty}$ boundedness. The fourth-order-in-space Swift-Hohenberg equation is used to elucidate the theoretical results in detail. We also propose a temporal second-order accurate scheme for efficiently solving such a strongly stiff equation. Furthermore, we present the corresponding optimal $L^2$ error estimate and provide several numerical simulations to demonstrate the dynamics.","sentences":["Before proving (unconditional) energy stability for gradient flows, most existing studies either require a strong Lipschitz condition regarding the non-linearity or certain $L^{\\infty}$ bounds on the numerical solutions (the maximum principle).","However, proving energy stability without such premises is a very challenging task.","In this paper, we aim to develop a novel analytical tool, namely global-in-time energy stability, to demonstrate energy dissipation without assuming any strong Lipschitz condition or $L^{\\infty}$ boundedness.","The fourth-order-in-space Swift-Hohenberg equation is used to elucidate the theoretical results in detail.","We also propose a temporal second-order accurate scheme for efficiently solving such a strongly stiff equation.","Furthermore, we present the corresponding optimal $L^2$ error estimate and provide several numerical simulations to demonstrate the dynamics."],"url":"http://arxiv.org/abs/2406.07941v1","category":"math.NA"}
{"created":"2024-06-12 06:34:21","title":"Can Large Language Models Understand Spatial Audio?","abstract":"This paper explores enabling large language models (LLMs) to understand spatial information from multichannel audio, a skill currently lacking in auditory LLMs. By leveraging LLMs' advanced cognitive and inferential abilities, the aim is to enhance understanding of 3D environments via audio. We study 3 spatial audio tasks: sound source localization (SSL), far-field speech recognition (FSR), and localisation-informed speech extraction (LSE), achieving notable progress in each task. For SSL, our approach achieves an MAE of $2.70^{\\circ}$ on the Spatial LibriSpeech dataset, substantially surpassing the prior benchmark of about $6.60^{\\circ}$. Moreover, our model can employ spatial cues to improve FSR accuracy and execute LSE by selectively attending to sounds originating from a specified direction via text prompts, even amidst overlapping speech. These findings highlight the potential of adapting LLMs to grasp physical audio concepts, paving the way for LLM-based agents in 3D environments.","sentences":["This paper explores enabling large language models (LLMs) to understand spatial information from multichannel audio, a skill currently lacking in auditory LLMs.","By leveraging LLMs' advanced cognitive and inferential abilities, the aim is to enhance understanding of 3D environments via audio.","We study 3 spatial audio tasks: sound source localization (SSL), far-field speech recognition (FSR), and localisation-informed speech extraction (LSE), achieving notable progress in each task.","For SSL, our approach achieves an MAE of $2.70^{\\circ}$ on the Spatial LibriSpeech dataset, substantially surpassing the prior benchmark of about $6.60^{\\circ}$. Moreover, our model can employ spatial cues to improve FSR accuracy and execute LSE by selectively attending to sounds originating from a specified direction via text prompts, even amidst overlapping speech.","These findings highlight the potential of adapting LLMs to grasp physical audio concepts, paving the way for LLM-based agents in 3D environments."],"url":"http://arxiv.org/abs/2406.07914v1","category":"cs.SD"}
{"created":"2024-06-12 06:33:54","title":"DeTriever: Decoder-representation-based Retriever for Improving NL2SQL In-Context Learning","abstract":"While in-context Learning (ICL) has proven to be an effective technique to improve the performance of Large Language Models (LLMs) in a variety of complex tasks, notably in translating natural language questions into Structured Query Language (NL2SQL), the question of how to select the most beneficial demonstration examples remains an open research problem. While prior works often adapted off-the-shelf encoders to retrieve examples dynamically, an inherent discrepancy exists in the representational capacities between the external retrievers and the LLMs. Further, optimizing the selection of examples is a non-trivial task, since there are no straightforward methods to assess the relative benefits of examples without performing pairwise inference. To address these shortcomings, we propose DeTriever, a novel demonstration retrieval framework that learns a weighted combination of LLM hidden states, where rich semantic information is encoded. To train the model, we propose a proxy score that estimates the relative benefits of examples based on the similarities between output queries. Experiments on two popular NL2SQL benchmarks demonstrate that our method significantly outperforms the state-of-the-art baselines on one-shot NL2SQL tasks.","sentences":["While in-context Learning (ICL) has proven to be an effective technique to improve the performance of Large Language Models (LLMs) in a variety of complex tasks, notably in translating natural language questions into Structured Query Language (NL2SQL), the question of how to select the most beneficial demonstration examples remains an open research problem.","While prior works often adapted off-the-shelf encoders to retrieve examples dynamically, an inherent discrepancy exists in the representational capacities between the external retrievers and the LLMs.","Further, optimizing the selection of examples is a non-trivial task, since there are no straightforward methods to assess the relative benefits of examples without performing pairwise inference.","To address these shortcomings, we propose DeTriever, a novel demonstration retrieval framework that learns a weighted combination of LLM hidden states, where rich semantic information is encoded.","To train the model, we propose a proxy score that estimates the relative benefits of examples based on the similarities between output queries.","Experiments on two popular NL2SQL benchmarks demonstrate that our method significantly outperforms the state-of-the-art baselines on one-shot NL2SQL tasks."],"url":"http://arxiv.org/abs/2406.07913v1","category":"cs.CL"}
{"created":"2024-06-12 06:12:04","title":"Grounding Multimodal Large Language Models in Actions","abstract":"Multimodal Large Language Models (MLLMs) have demonstrated a wide range of capabilities across many domains, including Embodied AI. In this work, we study how to best ground a MLLM into different embodiments and their associated action spaces, with the goal of leveraging the multimodal world knowledge of the MLLM. We first generalize a number of methods through a unified architecture and the lens of action space adaptors. For continuous actions, we show that a learned tokenization allows for sufficient modeling precision, yielding the best performance on downstream tasks. For discrete actions, we demonstrate that semantically aligning these actions with the native output token space of the MLLM leads to the strongest performance. We arrive at these lessons via a thorough study of seven action space adapters on five different environments, encompassing over 114 embodied tasks.","sentences":["Multimodal Large Language Models (MLLMs) have demonstrated a wide range of capabilities across many domains, including Embodied AI.","In this work, we study how to best ground a MLLM into different embodiments and their associated action spaces, with the goal of leveraging the multimodal world knowledge of the MLLM.","We first generalize a number of methods through a unified architecture and the lens of action space adaptors.","For continuous actions, we show that a learned tokenization allows for sufficient modeling precision, yielding the best performance on downstream tasks.","For discrete actions, we demonstrate that semantically aligning these actions with the native output token space of the MLLM leads to the strongest performance.","We arrive at these lessons via a thorough study of seven action space adapters on five different environments, encompassing over 114 embodied tasks."],"url":"http://arxiv.org/abs/2406.07904v1","category":"cs.LG"}
{"created":"2024-06-12 04:14:24","title":"Toward Enhanced Reinforcement Learning-Based Resource Management via Digital Twin: Opportunities, Applications, and Challenges","abstract":"This article presents a digital twin (DT)-enhanced reinforcement learning (RL) framework aimed at optimizing performance and reliability in network resource management, since the traditional RL methods face several unified challenges when applied to physical networks, including limited exploration efficiency, slow convergence, poor long-term performance, and safety concerns during the exploration phase. To deal with the above challenges, a comprehensive DT-based framework is proposed to enhance the convergence speed and performance for unified RL-based resource management. The proposed framework provides safe action exploration, more accurate estimates of long-term returns, faster training convergence, higher convergence performance, and real-time adaptation to varying network conditions. Then, two case studies on ultra-reliable and low-latency communication (URLLC) services and multiple unmanned aerial vehicles (UAV) network are presented, demonstrating improvements of the proposed framework in performance, convergence speed, and training cost reduction both on traditional RL and neural network based Deep RL (DRL). Finally, the article identifies and explores some of the research challenges and open issues in this rapidly evolving field.","sentences":["This article presents a digital twin (DT)-enhanced reinforcement learning (RL) framework aimed at optimizing performance and reliability in network resource management, since the traditional RL methods face several unified challenges when applied to physical networks, including limited exploration efficiency, slow convergence, poor long-term performance, and safety concerns during the exploration phase.","To deal with the above challenges, a comprehensive DT-based framework is proposed to enhance the convergence speed and performance for unified RL-based resource management.","The proposed framework provides safe action exploration, more accurate estimates of long-term returns, faster training convergence, higher convergence performance, and real-time adaptation to varying network conditions.","Then, two case studies on ultra-reliable and low-latency communication (URLLC) services and multiple unmanned aerial vehicles (UAV) network are presented, demonstrating improvements of the proposed framework in performance, convergence speed, and training cost reduction both on traditional RL and neural network based Deep RL (DRL).","Finally, the article identifies and explores some of the research challenges and open issues in this rapidly evolving field."],"url":"http://arxiv.org/abs/2406.07857v1","category":"eess.SY"}
{"created":"2024-06-12 04:12:21","title":"Learning dynamical behaviors in physical systems","abstract":"Physical learning is an emerging paradigm in science and engineering whereby (meta)materials acquire desired macroscopic behaviors by exposure to examples. So far, it has been applied to static properties such as elastic moduli and self-assembled structures encoded in minima of an energy landscape. Here, we extend this paradigm to dynamic functionalities, such as motion and shape change, that are instead encoded in limit cycles or pathways of a dynamical system. We identify the two ingredients needed to learn time-dependent behaviors irrespective of experimental platforms: (i) learning rules with time delays and (ii) exposure to examples that break time-reversal symmetry during training. After providing a hands-on demonstration of these requirements using programmable LEGO toys, we turn to realistic particle-based simulations where the training rules are not programmed on a computer. Instead, we elucidate how they emerge from physico-chemical processes involving the causal propagation of fields, like in recent experiments on moving oil droplets with chemotactic signalling. Our trainable particles can self-assemble into structures that move or change shape on demand, either by retrieving the dynamic behavior previously seen during training, or by learning on the fly. This rich phenomenology is captured by a modified Hopfield spin model amenable to analytical treatment. The principles illustrated here provide a step towards von Neumann's dream of engineering synthetic living systems that adapt to the environment.","sentences":["Physical learning is an emerging paradigm in science and engineering whereby (meta)materials acquire desired macroscopic behaviors by exposure to examples.","So far, it has been applied to static properties such as elastic moduli and self-assembled structures encoded in minima of an energy landscape.","Here, we extend this paradigm to dynamic functionalities, such as motion and shape change, that are instead encoded in limit cycles or pathways of a dynamical system.","We identify the two ingredients needed to learn time-dependent behaviors irrespective of experimental platforms: (i) learning rules with time delays and (ii) exposure to examples that break time-reversal symmetry during training.","After providing a hands-on demonstration of these requirements using programmable LEGO toys, we turn to realistic particle-based simulations where the training rules are not programmed on a computer.","Instead, we elucidate how they emerge from physico-chemical processes involving the causal propagation of fields, like in recent experiments on moving oil droplets with chemotactic signalling.","Our trainable particles can self-assemble into structures that move or change shape on demand, either by retrieving the dynamic behavior previously seen during training, or by learning on the fly.","This rich phenomenology is captured by a modified Hopfield spin model amenable to analytical treatment.","The principles illustrated here provide a step towards von Neumann's dream of engineering synthetic living systems that adapt to the environment."],"url":"http://arxiv.org/abs/2406.07856v1","category":"cond-mat.soft"}
{"created":"2024-06-12 03:17:57","title":"Dual-Pipeline with Low-Rank Adaptation for New Language Integration in Multilingual ASR","abstract":"This paper addresses challenges in integrating new languages into a pre-trained multilingual automatic speech recognition (mASR) system, particularly in scenarios where training data for existing languages is limited or unavailable. The proposed method employs a dual-pipeline with low-rank adaptation (LoRA). It maintains two data flow pipelines-one for existing languages and another for new languages. The primary pipeline follows the standard flow through the pre-trained parameters of mASR, while the secondary pipeline additionally utilizes language-specific parameters represented by LoRA and a separate output decoder module. Importantly, the proposed approach minimizes the performance degradation of existing languages and enables a language-agnostic operation mode, facilitated by a decoder selection strategy. We validate the effectiveness of the proposed method by extending the pre-trained Whisper model to 19 new languages from the FLEURS dataset","sentences":["This paper addresses challenges in integrating new languages into a pre-trained multilingual automatic speech recognition (mASR) system, particularly in scenarios where training data for existing languages is limited or unavailable.","The proposed method employs a dual-pipeline with low-rank adaptation (LoRA).","It maintains two data flow pipelines-one for existing languages and another for new languages.","The primary pipeline follows the standard flow through the pre-trained parameters of mASR, while the secondary pipeline additionally utilizes language-specific parameters represented by LoRA and a separate output decoder module.","Importantly, the proposed approach minimizes the performance degradation of existing languages and enables a language-agnostic operation mode, facilitated by a decoder selection strategy.","We validate the effectiveness of the proposed method by extending the pre-trained Whisper model to 19 new languages from the FLEURS dataset"],"url":"http://arxiv.org/abs/2406.07842v1","category":"eess.AS"}
{"created":"2024-06-12 03:01:38","title":"SE/BN Adapter: Parametric Efficient Domain Adaptation for Speaker Recognition","abstract":"Deploying a well-optimized pre-trained speaker recognition model in a new domain often leads to a significant decline in performance. While fine-tuning is a commonly employed solution, it demands ample adaptation data and suffers from parameter inefficiency, rendering it impractical for real-world applications with limited data available for model adaptation. Drawing inspiration from the success of adapters in self-supervised pre-trained models, this paper introduces a SE/BN adapter to address this challenge. By freezing the core speaker encoder and adjusting the feature maps' weights and activation distributions, we introduce a novel adapter utilizing trainable squeeze-and-excitation (SE) blocks and batch normalization (BN) layers, termed SE/BN adapter. Our experiments, conducted using VoxCeleb for pre-training and 4 genres from CN-Celeb for adaptation, demonstrate that the SE/BN adapter offers significant performance improvement over the baseline and competes with the vanilla fine-tuning approach by tuning just 1% of the parameters.","sentences":["Deploying a well-optimized pre-trained speaker recognition model in a new domain often leads to a significant decline in performance.","While fine-tuning is a commonly employed solution, it demands ample adaptation data and suffers from parameter inefficiency, rendering it impractical for real-world applications with limited data available for model adaptation.","Drawing inspiration from the success of adapters in self-supervised pre-trained models, this paper introduces a SE/BN adapter to address this challenge.","By freezing the core speaker encoder and adjusting the feature maps' weights and activation distributions, we introduce a novel adapter utilizing trainable squeeze-and-excitation (SE) blocks and batch normalization (BN) layers, termed SE/BN adapter.","Our experiments, conducted using VoxCeleb for pre-training and 4 genres from CN-Celeb for adaptation, demonstrate that the SE/BN adapter offers significant performance improvement over the baseline and competes with the vanilla fine-tuning approach by tuning just 1% of the parameters."],"url":"http://arxiv.org/abs/2406.07832v1","category":"cs.SD"}
{"created":"2024-06-11 22:04:59","title":"Fully Adaptive Regret-Guaranteed Algorithm for Control of Linear Quadratic Systems","abstract":"The first algorithm for the Linear Quadratic (LQ) control problem with an unknown system model, featuring a regret of $\\mathcal{O}(\\sqrt{T})$, was introduced by Abbasi-Yadkori and Szepesv\\'ari (2011). Recognizing the computational complexity of this algorithm, subsequent efforts (see Cohen et al. (2019), Mania et al. (2019), Faradonbeh et al. (2020a), and Kargin et al.(2022)) have been dedicated to proposing algorithms that are computationally tractable while preserving this order of regret. Although successful, the existing works in the literature lack a fully adaptive exploration-exploitation trade-off adjustment and require a user-defined value, which can lead to overall regret bound growth with some factors. In this work, noticing this gap, we propose the first fully adaptive algorithm that controls the number of policy updates (i.e., tunes the exploration-exploitation trade-off) and optimizes the upper-bound of regret adaptively. Our proposed algorithm builds on the SDP-based approach of Cohen et al. (2019) and relaxes its need for a horizon-dependant warm-up phase by appropriately tuning the regularization parameter and adding an adaptive input perturbation. We further show that through careful exploration-exploitation trade-off adjustment there is no need to commit to the widely-used notion of strong sequential stability, which is restrictive and can introduce complexities in initialization.","sentences":["The first algorithm for the Linear Quadratic (LQ) control problem with an unknown system model, featuring a regret of $\\mathcal{O}(\\sqrt{T})$, was introduced by Abbasi-Yadkori and Szepesv\\'ari (2011).","Recognizing the computational complexity of this algorithm, subsequent efforts (see Cohen et al. (2019), Mania et al. (2019), Faradonbeh et al. (2020a), and Kargin et al.(2022)) have been dedicated to proposing algorithms that are computationally tractable while preserving this order of regret.","Although successful, the existing works in the literature lack a fully adaptive exploration-exploitation trade-off adjustment and require a user-defined value, which can lead to overall regret bound growth with some factors.","In this work, noticing this gap, we propose the first fully adaptive algorithm that controls the number of policy updates (i.e., tunes the exploration-exploitation trade-off) and optimizes the upper-bound of regret adaptively.","Our proposed algorithm builds on the SDP-based approach of Cohen et al. (2019) and relaxes its need for a horizon-dependant warm-up phase by appropriately tuning the regularization parameter and adding an adaptive input perturbation.","We further show that through careful exploration-exploitation trade-off adjustment there is no need to commit to the widely-used notion of strong sequential stability, which is restrictive and can introduce complexities in initialization."],"url":"http://arxiv.org/abs/2406.07746v1","category":"stat.ML"}
{"created":"2024-06-11 21:26:20","title":"Reconfigurable, Multifunctional Origami Electronic Membranes for Mechanical and Environmental Sensing","abstract":"This work introduces a concept of origami electronic membranes that leverages the design and fabrication of flexible electronics and the mechanical behavior of engineering origami to achieve unique multifunctional, shape-reconfigurable, and adaptive membranes for mechanical and environmental sensing in benign and harsh conditions. This paper presents the materials, design, and fabrication methods for realizing six origami electronic membranes capable of reconfiguring planar or three-dimensional shapes based on the modified flasher, Kresling, Miura-ori, circular, letter, and Tachi-Miura origami patterns. These origami-based, thin-film flexible electronics can obtain both expansion and folding of their shapes, as well as transformation between different geometries. The origami electronic membranes can achieve mechanical and environmental sensing functions such as measuring motions, mechanical strains, temperatures, UV light, and humidity. The results reported here demonstrate the promise of combining engineering origami with flexible electronics to advance the state-of-the-art in multifunctional foldable and deployable electronics and systems.","sentences":["This work introduces a concept of origami electronic membranes that leverages the design and fabrication of flexible electronics and the mechanical behavior of engineering origami to achieve unique multifunctional, shape-reconfigurable, and adaptive membranes for mechanical and environmental sensing in benign and harsh conditions.","This paper presents the materials, design, and fabrication methods for realizing six origami electronic membranes capable of reconfiguring planar or three-dimensional shapes based on the modified flasher, Kresling, Miura-ori, circular, letter, and Tachi-Miura origami patterns.","These origami-based, thin-film flexible electronics can obtain both expansion and folding of their shapes, as well as transformation between different geometries.","The origami electronic membranes can achieve mechanical and environmental sensing functions such as measuring motions, mechanical strains, temperatures, UV light, and humidity.","The results reported here demonstrate the promise of combining engineering origami with flexible electronics to advance the state-of-the-art in multifunctional foldable and deployable electronics and systems."],"url":"http://arxiv.org/abs/2406.07731v1","category":"physics.app-ph"}
{"created":"2024-06-11 20:05:58","title":"AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation","abstract":"Recent Diffusion Transformers (DiTs) have shown impressive capabilities in generating high-quality single-modality content, including images, videos, and audio. However, it is still under-explored whether the transformer-based diffuser can efficiently denoise the Gaussian noises towards superb multimodal content creation. To bridge this gap, we introduce AV-DiT, a novel and efficient audio-visual diffusion transformer designed to generate high-quality, realistic videos with both visual and audio tracks. To minimize model complexity and computational costs, AV-DiT utilizes a shared DiT backbone pre-trained on image-only data, with only lightweight, newly inserted adapters being trainable. This shared backbone facilitates both audio and video generation. Specifically, the video branch incorporates a trainable temporal attention layer into a frozen pre-trained DiT block for temporal consistency. Additionally, a small number of trainable parameters adapt the image-based DiT block for audio generation. An extra shared DiT block, equipped with lightweight parameters, facilitates feature interaction between audio and visual modalities, ensuring alignment. Extensive experiments on the AIST++ and Landscape datasets demonstrate that AV-DiT achieves state-of-the-art performance in joint audio-visual generation with significantly fewer tunable parameters. Furthermore, our results highlight that a single shared image generative backbone with modality-specific adaptations is sufficient for constructing a joint audio-video generator. Our source code and pre-trained models will be released.","sentences":["Recent Diffusion Transformers (DiTs) have shown impressive capabilities in generating high-quality single-modality content, including images, videos, and audio.","However, it is still under-explored whether the transformer-based diffuser can efficiently denoise the Gaussian noises towards superb multimodal content creation.","To bridge this gap, we introduce AV-DiT, a novel and efficient audio-visual diffusion transformer designed to generate high-quality, realistic videos with both visual and audio tracks.","To minimize model complexity and computational costs, AV-DiT utilizes a shared DiT backbone pre-trained on image-only data, with only lightweight, newly inserted adapters being trainable.","This shared backbone facilitates both audio and video generation.","Specifically, the video branch incorporates a trainable temporal attention layer into a frozen pre-trained DiT block for temporal consistency.","Additionally, a small number of trainable parameters adapt the image-based DiT block for audio generation.","An extra shared DiT block, equipped with lightweight parameters, facilitates feature interaction between audio and visual modalities, ensuring alignment.","Extensive experiments on the AIST++ and Landscape datasets demonstrate that AV-DiT achieves state-of-the-art performance in joint audio-visual generation with significantly fewer tunable parameters.","Furthermore, our results highlight that a single shared image generative backbone with modality-specific adaptations is sufficient for constructing a joint audio-video generator.","Our source code and pre-trained models will be released."],"url":"http://arxiv.org/abs/2406.07686v1","category":"cs.CV"}
{"created":"2024-06-11 19:21:46","title":"PLT-D3: A High-fidelity Dynamic Driving Simulation Dataset for Stereo Depth and Scene Flow","abstract":"Autonomous driving has experienced remarkable progress, bolstered by innovations in computational hardware and sophisticated deep learning methodologies. The foundation of these advancements rests on the availability and quality of datasets, which are crucial for the development and refinement of dependable and versatile autonomous driving algorithms. While numerous datasets have been developed to support the evolution of autonomous driving perception technologies, few offer the diversity required to thoroughly test and enhance system robustness under varied weather conditions. Many public datasets lack the comprehensive coverage of challenging weather scenarios and detailed, high-resolution data, which are critical for training and validating advanced autonomous-driving perception models. In this paper, we introduce PLT-D3; a Dynamic-weather Driving Dataset, designed specifically to enhance autonomous driving systems' adaptability to diverse weather conditions. PLT-D3 provides high-fidelity stereo depth and scene flow ground truth data generated using Unreal Engine 5. In particular, this dataset includes synchronized high-resolution stereo image sequences that replicate a wide array of dynamic weather scenarios including rain, snow, fog, and diverse lighting conditions, offering an unprecedented level of realism in simulation-based testing. The primary aim of PLT-D3 is to address the scarcity of comprehensive training and testing resources that can simulate real-world weather variations. Benchmarks have been established for several critical autonomous driving tasks using PLT-D3, such as depth estimation, optical flow and scene-flow to measure and enhance the performance of state-of-the-art models.","sentences":["Autonomous driving has experienced remarkable progress, bolstered by innovations in computational hardware and sophisticated deep learning methodologies.","The foundation of these advancements rests on the availability and quality of datasets, which are crucial for the development and refinement of dependable and versatile autonomous driving algorithms.","While numerous datasets have been developed to support the evolution of autonomous driving perception technologies, few offer the diversity required to thoroughly test and enhance system robustness under varied weather conditions.","Many public datasets lack the comprehensive coverage of challenging weather scenarios and detailed, high-resolution data, which are critical for training and validating advanced autonomous-driving perception models.","In this paper, we introduce PLT-D3; a Dynamic-weather Driving Dataset, designed specifically to enhance autonomous driving systems' adaptability to diverse weather conditions.","PLT-D3 provides high-fidelity stereo depth and scene flow ground truth data generated using Unreal Engine 5.","In particular, this dataset includes synchronized high-resolution stereo image sequences that replicate a wide array of dynamic weather scenarios including rain, snow, fog, and diverse lighting conditions, offering an unprecedented level of realism in simulation-based testing.","The primary aim of PLT-D3 is to address the scarcity of comprehensive training and testing resources that can simulate real-world weather variations.","Benchmarks have been established for several critical autonomous driving tasks using PLT-D3, such as depth estimation, optical flow and scene-flow to measure and enhance the performance of state-of-the-art models."],"url":"http://arxiv.org/abs/2406.07667v1","category":"cs.CV"}
{"created":"2024-06-11 18:00:04","title":"Cooperative Sensing with Impurities in a Two-Dimensional Subwavelength Array","abstract":"We propose a versatile quantum sensing protocol based on two dissipatively coupled distant atoms embedded as impurities in a two-dimensional sub-wavelength atomic array. The array acts as a waveguide for the emitter light, creating cooperative enhancement that allows for more efficient population transfer. By monitoring the population of one of the impurity atoms, it is possible to detect frequency shifts in the emitters' resonance frequencies. We analytically estimate achievable sensitivities as well as the dependence on various system parameters. The proposed protocol is robust against various environmental factors and perturbations, which enhances its applicability in real-world scenarios.","sentences":["We propose a versatile quantum sensing protocol based on two dissipatively coupled distant atoms embedded as impurities in a two-dimensional sub-wavelength atomic array.","The array acts as a waveguide for the emitter light, creating cooperative enhancement that allows for more efficient population transfer.","By monitoring the population of one of the impurity atoms, it is possible to detect frequency shifts in the emitters' resonance frequencies.","We analytically estimate achievable sensitivities as well as the dependence on various system parameters.","The proposed protocol is robust against various environmental factors and perturbations, which enhances its applicability in real-world scenarios."],"url":"http://arxiv.org/abs/2406.07619v1","category":"quant-ph"}
{"created":"2024-06-11 18:00:03","title":"Breakdown of the quantum distinction of regular and chaotic classical dynamics in dissipative systems","abstract":"The Grobe-Haake-Sommers (GHS) conjecture generalizes the Bohigas-Giannoni-Schmit conjecture to dissipative systems, connecting classically chaotic systems with quantum spectra that exhibit level repulsion as predicted by Ginibre ensembles. Here, we show that the GHS conjecture does not hold for the open Dicke model, which is a spin-boson model of experimental interest. Surprisingly, where the open quantum model shows Ginibre level statistics, we do not always find evidence of chaotic structures in the classical limit. This result challenges the universality of the GHS conjecture and raises the question of what is the source of spectral correlations in open quantum systems.","sentences":["The Grobe-Haake-Sommers (GHS) conjecture generalizes the Bohigas-Giannoni-Schmit conjecture to dissipative systems, connecting classically chaotic systems with quantum spectra that exhibit level repulsion as predicted by Ginibre ensembles.","Here, we show that the GHS conjecture does not hold for the open Dicke model, which is a spin-boson model of experimental interest.","Surprisingly, where the open quantum model shows Ginibre level statistics, we do not always find evidence of chaotic structures in the classical limit.","This result challenges the universality of the GHS conjecture and raises the question of what is the source of spectral correlations in open quantum systems."],"url":"http://arxiv.org/abs/2406.07616v1","category":"quant-ph"}
{"created":"2024-06-11 13:36:12","title":"Modeling Sustainable Resource Management using Active Inference","abstract":"Active inference helps us simulate adaptive behavior and decision-making in biological and artificial agents. Building on our previous work exploring the relationship between active inference, well-being, resilience, and sustainability, we present a computational model of an agent learning sustainable resource management strategies in both static and dynamic environments. The agent's behavior emerges from optimizing its own well-being, represented by prior preferences, subject to beliefs about environmental dynamics. In a static environment, the agent learns to consistently consume resources to satisfy its needs. In a dynamic environment where resources deplete and replenish based on the agent's actions, the agent adapts its behavior to balance immediate needs with long-term resource availability. This demonstrates how active inference can give rise to sustainable and resilient behaviors in the face of changing environmental conditions. We discuss the implications of our model, its limitations, and suggest future directions for integrating more complex agent-environment interactions. Our work highlights active inference's potential for understanding and shaping sustainable behaviors.","sentences":["Active inference helps us simulate adaptive behavior and decision-making in biological and artificial agents.","Building on our previous work exploring the relationship between active inference, well-being, resilience, and sustainability, we present a computational model of an agent learning sustainable resource management strategies in both static and dynamic environments.","The agent's behavior emerges from optimizing its own well-being, represented by prior preferences, subject to beliefs about environmental dynamics.","In a static environment, the agent learns to consistently consume resources to satisfy its needs.","In a dynamic environment where resources deplete and replenish based on the agent's actions, the agent adapts its behavior to balance immediate needs with long-term resource availability.","This demonstrates how active inference can give rise to sustainable and resilient behaviors in the face of changing environmental conditions.","We discuss the implications of our model, its limitations, and suggest future directions for integrating more complex agent-environment interactions.","Our work highlights active inference's potential for understanding and shaping sustainable behaviors."],"url":"http://arxiv.org/abs/2406.07593v1","category":"cs.AI"}
{"created":"2024-06-11 10:46:41","title":"StreamPrompt: Learnable Prompt-guided Data Selection for Efficient Stream Learning","abstract":"Stream Learning (SL) requires models to rapidly adapt to continuous data streams, setting it apart from traditional Continual Learning (CL). Recent SL methods emphasize efficiency by selecting data subsets for training, but they often struggle due to their reliance on static, rule-based selection algorithms that cannot effectively adapt to the changing importance of data. In this work, we introduce StreamPrompt, a method that enhances data selection through dynamic, learnable prompts. These dynamic prompts serve two purposes beyond guiding model inference: 1) optimizing data selection, and 2) guiding updates to the rehearsal buffer. This approach addresses the challenges of adaptability and computational efficiency in processing continuous data streams. Moreover, StreamPrompt introduces Prompt Attunement,a mechanism that enhances the efficiency of prompt learning. By leveraging attention layers from vision transformers and softly combining their outputs with a gate unit, Prompt Attunementrefines prompts with minimal computational resources. Comprehensive evaluations demonstrate StreamPrompts superior performance over state-of-the-art, with significant improvements in accuracy and reductions in training time. These results underscore the efficacy and efficiency of StreamPrompt, establishing its potential as a scalable and effective solution for the evolving demands of SL. Our code is available at https://github.com/intellistream/Efficient-Stream-Learning.","sentences":["Stream Learning (SL) requires models to rapidly adapt to continuous data streams, setting it apart from traditional Continual Learning (CL).","Recent SL methods emphasize efficiency by selecting data subsets for training, but they often struggle due to their reliance on static, rule-based selection algorithms that cannot effectively adapt to the changing importance of data.","In this work, we introduce StreamPrompt, a method that enhances data selection through dynamic, learnable prompts.","These dynamic prompts serve two purposes beyond guiding model inference: 1) optimizing data selection, and 2) guiding updates to the rehearsal buffer.","This approach addresses the challenges of adaptability and computational efficiency in processing continuous data streams.","Moreover, StreamPrompt introduces Prompt Attunement,a mechanism that enhances the efficiency of prompt learning.","By leveraging attention layers from vision transformers and softly combining their outputs with a gate unit, Prompt Attunementrefines prompts with minimal computational resources.","Comprehensive evaluations demonstrate StreamPrompts superior performance over state-of-the-art, with significant improvements in accuracy and reductions in training time.","These results underscore the efficacy and efficiency of StreamPrompt, establishing its potential as a scalable and effective solution for the evolving demands of SL.","Our code is available at https://github.com/intellistream/Efficient-Stream-Learning."],"url":"http://arxiv.org/abs/2406.07590v1","category":"cs.LG"}
{"created":"2024-06-10 21:22:08","title":"SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature","abstract":"We present SciRIFF (Scientific Resource for Instruction-Following and Finetuning), a dataset of 137K instruction-following demonstrations for 54 tasks covering five essential scientific literature understanding capabilities: information extraction, summarization, question answering, claim verification, and classification. SciRIFF demonstrations are notable for their long input contexts, detailed task specifications, and complex structured outputs. While instruction-following resources are available in specific domains such as clinical medicine and chemistry, SciRIFF is the first dataset focused on extracting and synthesizing information from research literature across a wide range of scientific fields. To demonstrate the utility of SciRIFF, we develop a sample-efficient strategy to adapt a general instruction-following model for science by performing additional finetuning on a mix of general-domain and SciRIFF demonstrations. In evaluations on nine held-out scientific tasks, our model -- called SciTulu -- improves over a strong LLM baseline by 28.1% and 6.5% at the 7B and 70B scales respectively, while maintaining general instruction-following performance within 2% of the baseline. We are optimistic that SciRIFF will facilitate the development and evaluation of LLMs to help researchers navigate the ever-growing body of scientific literature. We release our dataset, model checkpoints, and data processing and evaluation code to enable further research.","sentences":["We present SciRIFF (Scientific Resource for Instruction-Following and Finetuning), a dataset of 137K instruction-following demonstrations for 54 tasks covering five essential scientific literature understanding capabilities: information extraction, summarization, question answering, claim verification, and classification.","SciRIFF demonstrations are notable for their long input contexts, detailed task specifications, and complex structured outputs.","While instruction-following resources are available in specific domains such as clinical medicine and chemistry, SciRIFF is the first dataset focused on extracting and synthesizing information from research literature across a wide range of scientific fields.","To demonstrate the utility of SciRIFF, we develop a sample-efficient strategy to adapt a general instruction-following model for science by performing additional finetuning on a mix of general-domain and SciRIFF demonstrations.","In evaluations on nine held-out scientific tasks, our model -- called SciTulu -- improves over a strong LLM baseline by 28.1% and 6.5% at the 7B and 70B scales respectively, while maintaining general instruction-following performance within 2% of the baseline.","We are optimistic that SciRIFF will facilitate the development and evaluation of LLMs to help researchers navigate the ever-growing body of scientific literature.","We release our dataset, model checkpoints, and data processing and evaluation code to enable further research."],"url":"http://arxiv.org/abs/2406.07835v1","category":"cs.CL"}
{"created":"2024-06-10 17:59:55","title":"GaussianCity: Generative Gaussian Splatting for Unbounded 3D City Generation","abstract":"3D city generation with NeRF-based methods shows promising generation results but is computationally inefficient. Recently 3D Gaussian Splatting (3D-GS) has emerged as a highly efficient alternative for object-level 3D generation. However, adapting 3D-GS from finite-scale 3D objects and humans to infinite-scale 3D cities is non-trivial. Unbounded 3D city generation entails significant storage overhead (out-of-memory issues), arising from the need to expand points to billions, often demanding hundreds of Gigabytes of VRAM for a city scene spanning 10km^2. In this paper, we propose GaussianCity, a generative Gaussian Splatting framework dedicated to efficiently synthesizing unbounded 3D cities with a single feed-forward pass. Our key insights are two-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a highly compact intermediate representation, ensuring that the growth in VRAM usage for unbounded scenes remains constant, thus enabling unbounded city generation. 2) Spatial-aware Gaussian Attribute Decoder: We present spatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which leverages Point Serializer to integrate the structural and contextual characteristics of BEV points. Extensive experiments demonstrate that GaussianCity achieves state-of-the-art results in both drone-view and street-view 3D city generation. Notably, compared to CityDreamer, GaussianCity exhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18 FPS).","sentences":["3D city generation with NeRF-based methods shows promising generation results but is computationally inefficient.","Recently 3D Gaussian Splatting (3D-GS) has emerged as a highly efficient alternative for object-level 3D generation.","However, adapting 3D-GS from finite-scale 3D objects and humans to infinite-scale 3D cities is non-trivial.","Unbounded 3D city generation entails significant storage overhead (out-of-memory issues), arising from the need to expand points to billions, often demanding hundreds of Gigabytes of VRAM for a city scene spanning 10km^2.","In this paper, we propose GaussianCity, a generative Gaussian Splatting framework dedicated to efficiently synthesizing unbounded 3D cities with a single feed-forward pass.","Our key insights are two-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a highly compact intermediate representation, ensuring that the growth in VRAM usage for unbounded scenes remains constant, thus enabling unbounded city generation.","2) Spatial-aware Gaussian Attribute Decoder:","We present spatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which leverages Point Serializer to integrate the structural and contextual characteristics of BEV points.","Extensive experiments demonstrate that GaussianCity achieves state-of-the-art results in both drone-view and street-view 3D city generation.","Notably, compared to CityDreamer, GaussianCity exhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18 FPS)."],"url":"http://arxiv.org/abs/2406.06526v1","category":"cs.CV"}
{"created":"2024-06-10 17:59:46","title":"NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing","abstract":"We propose a video editing framework, NaRCan, which integrates a hybrid deformation field and diffusion prior to generate high-quality natural canonical images to represent the input video. Our approach utilizes homography to model global motion and employs multi-layer perceptrons (MLPs) to capture local residual deformations, enhancing the model's ability to handle complex video dynamics. By introducing a diffusion prior from the early stages of training, our model ensures that the generated images retain a high-quality natural appearance, making the produced canonical images suitable for various downstream tasks in video editing, a capability not achieved by current canonical-based methods. Furthermore, we incorporate low-rank adaptation (LoRA) fine-tuning and introduce a noise and diffusion prior update scheduling technique that accelerates the training process by 14 times. Extensive experimental results show that our method outperforms existing approaches in various video editing tasks and produces coherent and high-quality edited video sequences. See our project page for video results at https://koi953215.github.io/NaRCan_page/.","sentences":["We propose a video editing framework, NaRCan, which integrates a hybrid deformation field and diffusion prior to generate high-quality natural canonical images to represent the input video.","Our approach utilizes homography to model global motion and employs multi-layer perceptrons (MLPs) to capture local residual deformations, enhancing the model's ability to handle complex video dynamics.","By introducing a diffusion prior from the early stages of training, our model ensures that the generated images retain a high-quality natural appearance, making the produced canonical images suitable for various downstream tasks in video editing, a capability not achieved by current canonical-based methods.","Furthermore, we incorporate low-rank adaptation (LoRA) fine-tuning and introduce a noise and diffusion prior update scheduling technique that accelerates the training process by 14 times.","Extensive experimental results show that our method outperforms existing approaches in various video editing tasks and produces coherent and high-quality edited video sequences.","See our project page for video results at https://koi953215.github.io/NaRCan_page/."],"url":"http://arxiv.org/abs/2406.06523v1","category":"cs.CV"}
{"created":"2024-06-10 17:58:02","title":"Data Augmentation for Multivariate Time Series Classification: An Experimental Study","abstract":"Our study investigates the impact of data augmentation on the performance of multivariate time series models, focusing on datasets from the UCR archive. Despite the limited size of these datasets, we achieved classification accuracy improvements in 10 out of 13 datasets using the Rocket and InceptionTime models. This highlights the essential role of sufficient data in training effective models, paralleling the advancements seen in computer vision. Our work delves into adapting and applying existing methods in innovative ways to the domain of multivariate time series classification. Our comprehensive exploration of these techniques sets a new standard for addressing data scarcity in time series analysis, emphasizing that diverse augmentation strategies are crucial for unlocking the potential of both traditional and deep learning models. Moreover, by meticulously analyzing and applying a variety of augmentation techniques, we demonstrate that strategic data enrichment can enhance model accuracy. This not only establishes a benchmark for future research in time series analysis but also underscores the importance of adopting varied augmentation approaches to improve model performance in the face of limited data availability.","sentences":["Our study investigates the impact of data augmentation on the performance of multivariate time series models, focusing on datasets from the UCR archive.","Despite the limited size of these datasets, we achieved classification accuracy improvements in 10 out of 13 datasets using the Rocket and InceptionTime models.","This highlights the essential role of sufficient data in training effective models, paralleling the advancements seen in computer vision.","Our work delves into adapting and applying existing methods in innovative ways to the domain of multivariate time series classification.","Our comprehensive exploration of these techniques sets a new standard for addressing data scarcity in time series analysis, emphasizing that diverse augmentation strategies are crucial for unlocking the potential of both traditional and deep learning models.","Moreover, by meticulously analyzing and applying a variety of augmentation techniques, we demonstrate that strategic data enrichment can enhance model accuracy.","This not only establishes a benchmark for future research in time series analysis but also underscores the importance of adopting varied augmentation approaches to improve model performance in the face of limited data availability."],"url":"http://arxiv.org/abs/2406.06518v1","category":"cs.LG"}
{"created":"2024-06-10 17:55:43","title":"Distribution-Free Predictive Inference under Unknown Temporal Drift","abstract":"Distribution-free prediction sets play a pivotal role in uncertainty quantification for complex statistical models. Their validity hinges on reliable calibration data, which may not be readily available as real-world environments often undergo unknown changes over time. In this paper, we propose a strategy for choosing an adaptive window and use the data therein to construct prediction sets. The window is selected by optimizing an estimated bias-variance tradeoff. We provide sharp coverage guarantees for our method, showing its adaptivity to the underlying temporal drift. We also illustrate its efficacy through numerical experiments on synthetic and real data.","sentences":["Distribution-free prediction sets play a pivotal role in uncertainty quantification for complex statistical models.","Their validity hinges on reliable calibration data, which may not be readily available as real-world environments often undergo unknown changes over time.","In this paper, we propose a strategy for choosing an adaptive window and use the data therein to construct prediction sets.","The window is selected by optimizing an estimated bias-variance tradeoff.","We provide sharp coverage guarantees for our method, showing its adaptivity to the underlying temporal drift.","We also illustrate its efficacy through numerical experiments on synthetic and real data."],"url":"http://arxiv.org/abs/2406.06516v1","category":"stat.ME"}
{"created":"2024-06-10 17:53:01","title":"Merlin: A Vision Language Foundation Model for 3D Computed Tomography","abstract":"Over 85 million computed tomography (CT) scans are performed annually in the US, of which approximately one quarter focus on the abdomen. Given the current radiologist shortage, there is a large impetus to use artificial intelligence to alleviate the burden of interpreting these complex imaging studies. Prior state-of-the-art approaches for automated medical image interpretation leverage vision language models (VLMs). However, current medical VLMs are generally limited to 2D images and short reports, and do not leverage electronic health record (EHR) data for supervision. We introduce Merlin - a 3D VLM that we train using paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes (1.8+ million codes), and radiology reports (6+ million tokens). We evaluate Merlin on 6 task types and 752 individual tasks. The non-adapted (off-the-shelf) tasks include zero-shot findings classification (31 findings), phenotype classification (692 phenotypes), and zero-shot cross-modal retrieval (image to findings and image to impressions), while model adapted tasks include 5-year disease prediction (6 diseases), radiology report generation, and 3D semantic segmentation (20 organs). We perform internal validation on a test set of 5,137 CTs, and external validation on 7,000 clinical CTs and on two public CT datasets (VerSe, TotalSegmentator). Beyond these clinically-relevant evaluations, we assess the efficacy of various network architectures and training strategies to depict that Merlin has favorable performance to existing task-specific baselines. We derive data scaling laws to empirically assess training data needs for requisite downstream task performance. Furthermore, unlike conventional VLMs that require hundreds of GPUs for training, we perform all training on a single GPU.","sentences":["Over 85 million computed tomography (CT) scans are performed annually in the US, of which approximately one quarter focus on the abdomen.","Given the current radiologist shortage, there is a large impetus to use artificial intelligence to alleviate the burden of interpreting these complex imaging studies.","Prior state-of-the-art approaches for automated medical image interpretation leverage vision language models (VLMs).","However, current medical VLMs are generally limited to 2D images and short reports, and do not leverage electronic health record (EHR) data for supervision.","We introduce Merlin - a 3D VLM that we train using paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes (1.8+ million codes), and radiology reports (6+ million tokens).","We evaluate Merlin on 6 task types and 752 individual tasks.","The non-adapted (off-the-shelf) tasks include zero-shot findings classification (31 findings), phenotype classification (692 phenotypes), and zero-shot cross-modal retrieval (image to findings and image to impressions), while model adapted tasks include 5-year disease prediction (6 diseases), radiology report generation, and 3D semantic segmentation (20 organs).","We perform internal validation on a test set of 5,137 CTs, and external validation on 7,000 clinical CTs and on two public CT datasets (VerSe, TotalSegmentator).","Beyond these clinically-relevant evaluations, we assess the efficacy of various network architectures and training strategies to depict that Merlin has favorable performance to existing task-specific baselines.","We derive data scaling laws to empirically assess training data needs for requisite downstream task performance.","Furthermore, unlike conventional VLMs that require hundreds of GPUs for training, we perform all training on a single GPU."],"url":"http://arxiv.org/abs/2406.06512v1","category":"cs.CV"}
{"created":"2024-06-10 17:34:44","title":"Adaptive Opponent Policy Detection in Multi-Agent MDPs: Real-Time Strategy Switch Identification Using Running Error Estimation","abstract":"In Multi-agent Reinforcement Learning (MARL), accurately perceiving opponents' strategies is essential for both cooperative and adversarial contexts, particularly within dynamic environments. While Proximal Policy Optimization (PPO) and related algorithms such as Actor-Critic with Experience Replay (ACER), Trust Region Policy Optimization (TRPO), and Deep Deterministic Policy Gradient (DDPG) perform well in single-agent, stationary environments, they suffer from high variance in MARL due to non-stationary and hidden policies of opponents, leading to diminished reward performance. Additionally, existing methods in MARL face significant challenges, including the need for inter-agent communication, reliance on explicit reward information, high computational demands, and sampling inefficiencies. These issues render them less effective in continuous environments where opponents may abruptly change their policies without prior notice. Against this background, we present OPS-DeMo (Online Policy Switch-Detection Model), an online algorithm that employs dynamic error decay to detect changes in opponents' policies. OPS-DeMo continuously updates its beliefs using an Assumed Opponent Policy (AOP) Bank and selects corresponding responses from a pre-trained Response Policy Bank. Each response policy is trained against consistently strategizing opponents, reducing training uncertainty and enabling the effective use of algorithms like PPO in multi-agent environments. Comparative assessments show that our approach outperforms PPO-trained models in dynamic scenarios like the Predator-Prey setting, providing greater robustness to sudden policy shifts and enabling more informed decision-making through precise opponent policy insights.","sentences":["In Multi-agent Reinforcement Learning (MARL), accurately perceiving opponents' strategies is essential for both cooperative and adversarial contexts, particularly within dynamic environments.","While Proximal Policy Optimization (PPO) and related algorithms such as Actor-Critic with Experience Replay (ACER), Trust Region Policy Optimization (TRPO), and Deep Deterministic Policy Gradient (DDPG) perform well in single-agent, stationary environments, they suffer from high variance in MARL due to non-stationary and hidden policies of opponents, leading to diminished reward performance.","Additionally, existing methods in MARL face significant challenges, including the need for inter-agent communication, reliance on explicit reward information, high computational demands, and sampling inefficiencies.","These issues render them less effective in continuous environments where opponents may abruptly change their policies without prior notice.","Against this background, we present OPS-DeMo (Online Policy Switch-Detection Model), an online algorithm that employs dynamic error decay to detect changes in opponents' policies.","OPS-DeMo continuously updates its beliefs using an Assumed Opponent Policy (AOP) Bank and selects corresponding responses from a pre-trained Response Policy Bank.","Each response policy is trained against consistently strategizing opponents, reducing training uncertainty and enabling the effective use of algorithms like PPO in multi-agent environments.","Comparative assessments show that our approach outperforms PPO-trained models in dynamic scenarios like the Predator-Prey setting, providing greater robustness to sudden policy shifts and enabling more informed decision-making through precise opponent policy insights."],"url":"http://arxiv.org/abs/2406.06500v1","category":"cs.AI"}
{"created":"2024-06-10 17:31:07","title":"Boosting Robustness in Preference-Based Reinforcement Learning with Dynamic Sparsity","abstract":"For autonomous agents to successfully integrate into human-centered environments, agents should be able to learn from and adapt to humans in their native settings. Preference-based reinforcement learning (PbRL) is a promising approach that learns reward functions from human preferences. This enables RL agents to adapt their behavior based on human desires. However, humans live in a world full of diverse information, most of which is not relevant to completing a particular task. It becomes essential that agents learn to focus on the subset of task-relevant environment features. Unfortunately, prior work has largely ignored this aspect; primarily focusing on improving PbRL algorithms in standard RL environments that are carefully constructed to contain only task-relevant features. This can result in algorithms that may not effectively transfer to a more noisy real-world setting. To that end, this work proposes R2N (Robust-to-Noise), the first PbRL algorithm that leverages principles of dynamic sparse training to learn robust reward models that can focus on task-relevant features. We study the effectiveness of R2N in the Extremely Noisy Environment setting, an RL problem setting where up to 95% of the state features are irrelevant distractions. In experiments with a simulated teacher, we demonstrate that R2N can adapt the sparse connectivity of its neural networks to focus on task-relevant features, enabling R2N to significantly outperform several state-of-the-art PbRL algorithms in multiple locomotion and control environments.","sentences":["For autonomous agents to successfully integrate into human-centered environments, agents should be able to learn from and adapt to humans in their native settings.","Preference-based reinforcement learning (PbRL) is a promising approach that learns reward functions from human preferences.","This enables RL agents to adapt their behavior based on human desires.","However, humans live in a world full of diverse information, most of which is not relevant to completing a particular task.","It becomes essential that agents learn to focus on the subset of task-relevant environment features.","Unfortunately, prior work has largely ignored this aspect; primarily focusing on improving PbRL algorithms in standard RL environments that are carefully constructed to contain only task-relevant features.","This can result in algorithms that may not effectively transfer to a more noisy real-world setting.","To that end, this work proposes R2N (Robust-to-Noise), the first PbRL algorithm that leverages principles of dynamic sparse training to learn robust reward models that can focus on task-relevant features.","We study the effectiveness of R2N in the Extremely Noisy Environment setting, an RL problem setting where up to 95% of the state features are irrelevant distractions.","In experiments with a simulated teacher, we demonstrate that R2N can adapt the sparse connectivity of its neural networks to focus on task-relevant features, enabling R2N to significantly outperform several state-of-the-art PbRL algorithms in multiple locomotion and control environments."],"url":"http://arxiv.org/abs/2406.06495v1","category":"cs.LG"}
{"created":"2024-06-10 17:02:08","title":"AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction","abstract":"Text-guided video prediction (TVP) involves predicting the motion of future frames from the initial frame according to an instruction, which has wide applications in virtual reality, robotics, and content creation. Previous TVP methods make significant breakthroughs by adapting Stable Diffusion for this task. However, they struggle with frame consistency and temporal stability primarily due to the limited scale of video datasets. We observe that pretrained Image2Video diffusion models possess good priors for video dynamics but they lack textual control. Hence, transferring Image2Video models to leverage their video dynamic priors while injecting instruction control to generate controllable videos is both a meaningful and challenging task. To achieve this, we introduce the Multi-Modal Large Language Model (MLLM) to predict future video states based on initial frames and text instructions. More specifically, we design a dual query transformer (DQFormer) architecture, which integrates the instructions and frames into the conditional embeddings for future frame prediction. Additionally, we develop Long-Short Term Temporal Adapters and Spatial Adapters that can quickly transfer general video diffusion models to specific scenarios with minimal training costs. Experimental results show that our method significantly outperforms state-of-the-art techniques on four datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and UCF-101. Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and SSv2 respectively, demonstrating its effectiveness in various domains. More examples can be found at our website https://chenhsing.github.io/AID.","sentences":["Text-guided video prediction (TVP) involves predicting the motion of future frames from the initial frame according to an instruction, which has wide applications in virtual reality, robotics, and content creation.","Previous TVP methods make significant breakthroughs by adapting Stable Diffusion for this task.","However, they struggle with frame consistency and temporal stability primarily due to the limited scale of video datasets.","We observe that pretrained Image2Video diffusion models possess good priors for video dynamics but they lack textual control.","Hence, transferring Image2Video models to leverage their video dynamic priors while injecting instruction control to generate controllable videos is both a meaningful and challenging task.","To achieve this, we introduce the Multi-Modal Large Language Model (MLLM) to predict future video states based on initial frames and text instructions.","More specifically, we design a dual query transformer (DQFormer) architecture, which integrates the instructions and frames into the conditional embeddings for future frame prediction.","Additionally, we develop Long-Short Term Temporal Adapters and Spatial Adapters that can quickly transfer general video diffusion models to specific scenarios with minimal training costs.","Experimental results show that our method significantly outperforms state-of-the-art techniques on four datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and UCF-101.","Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and SSv2 respectively, demonstrating its effectiveness in various domains.","More examples can be found at our website https://chenhsing.github.io/AID."],"url":"http://arxiv.org/abs/2406.06465v1","category":"cs.CV"}
{"created":"2024-06-10 16:24:35","title":"DISCO: An End-to-End Bandit Framework for Personalised Discount Allocation","abstract":"Personalised discount codes provide a powerful mechanism for managing customer relationships and operational spend in e-commerce. Bandits are well suited for this product area, given the partial information nature of the problem, as well as the need for adaptation to the changing business environment. Here, we introduce DISCO, an end-to-end contextual bandit framework for personalised discount code allocation at ASOS. DISCO adapts the traditional Thompson Sampling algorithm by integrating it within an integer program, thereby allowing for operational cost control. Because bandit learning is often worse with high dimensional actions, we focused on building low dimensional action and context representations that were nonetheless capable of good accuracy. Additionally, we sought to build a model that preserved the relationship between price and sales, in which customers increasing their purchasing in response to lower prices (\"negative price elasticity\"). These aims were achieved by using radial basis functions to represent the continuous (i.e. infinite armed) action space, in combination with context embeddings extracted from a neural network. These feature representations were used within a Thompson Sampling framework to facilitate exploration, and further integrated with an integer program to allocate discount codes across ASOS's customer base. These modelling decisions result in a reward model that (a) enables pooled learning across similar actions, (b) is highly accurate, including in extrapolation, and (c) preserves the expected negative price elasticity. Through offline analysis, we show that DISCO is able to effectively enact exploration and improves its performance over time, despite the global constraint. Finally, we subjected DISCO to a rigorous online A/B test, and find that it achieves a significant improvement of >1% in average basket value, relative to the legacy systems.","sentences":["Personalised discount codes provide a powerful mechanism for managing customer relationships and operational spend in e-commerce.","Bandits are well suited for this product area, given the partial information nature of the problem, as well as the need for adaptation to the changing business environment.","Here, we introduce DISCO, an end-to-end contextual bandit framework for personalised discount code allocation at ASOS.","DISCO adapts the traditional Thompson Sampling algorithm by integrating it within an integer program, thereby allowing for operational cost control.","Because bandit learning is often worse with high dimensional actions, we focused on building low dimensional action and context representations that were nonetheless capable of good accuracy.","Additionally, we sought to build a model that preserved the relationship between price and sales, in which customers increasing their purchasing in response to lower prices (\"negative price elasticity\").","These aims were achieved by using radial basis functions to represent the continuous (i.e. infinite armed) action space, in combination with context embeddings extracted from a neural network.","These feature representations were used within a Thompson Sampling framework to facilitate exploration, and further integrated with an integer program to allocate discount codes across ASOS's customer base.","These modelling decisions result in a reward model that (a) enables pooled learning across similar actions, (b) is highly accurate, including in extrapolation, and (c) preserves the expected negative price elasticity.","Through offline analysis, we show that DISCO is able to effectively enact exploration and improves its performance over time, despite the global constraint.","Finally, we subjected DISCO to a rigorous online A/B test, and find that it achieves a significant improvement of >1% in average basket value, relative to the legacy systems."],"url":"http://arxiv.org/abs/2406.06433v2","category":"cs.LG"}
{"created":"2024-06-10 16:15:14","title":"Biomarker-Guided Adaptive Enrichment Design with Threshold Detection for Clinical Trials with Time-to-Event Outcome","abstract":"Biomarker-guided designs are increasingly used to evaluate personalized treatments based on patients' biomarker status in Phase II and III clinical trials. With adaptive enrichment, these designs can improve the efficiency of evaluating the treatment effect in biomarker-positive patients by increasing their proportion in the randomized trial. While time-to-event outcomes are often used as the primary endpoint to measure treatment effects for a new therapy in severe diseases like cancer and cardiovascular diseases, there is limited research on biomarker-guided adaptive enrichment trials in this context. Such trials almost always adopt hazard ratio methods for statistical measurement of treatment effects. In contrast, restricted mean survival time (RMST) has gained popularity for analyzing time-to-event outcomes because it offers more straightforward interpretations of treatment effects and does not require the proportional hazard assumption. This paper proposes a two-stage biomarker-guided adaptive RMST design with threshold detection and patient enrichment. We develop sophisticated methods for identifying the optimal biomarker threshold, treatment effect estimators in the biomarker-positive subgroup, and approaches for type I error rate, power analysis, and sample size calculation. We present a numerical example of re-designing an oncology trial. An extensive simulation study is conducted to evaluate the performance of the proposed design.","sentences":["Biomarker-guided designs are increasingly used to evaluate personalized treatments based on patients' biomarker status in Phase II and III clinical trials.","With adaptive enrichment, these designs can improve the efficiency of evaluating the treatment effect in biomarker-positive patients by increasing their proportion in the randomized trial.","While time-to-event outcomes are often used as the primary endpoint to measure treatment effects for a new therapy in severe diseases like cancer and cardiovascular diseases, there is limited research on biomarker-guided adaptive enrichment trials in this context.","Such trials almost always adopt hazard ratio methods for statistical measurement of treatment effects.","In contrast, restricted mean survival time (RMST) has gained popularity for analyzing time-to-event outcomes because it offers more straightforward interpretations of treatment effects and does not require the proportional hazard assumption.","This paper proposes a two-stage biomarker-guided adaptive RMST design with threshold detection and patient enrichment.","We develop sophisticated methods for identifying the optimal biomarker threshold, treatment effect estimators in the biomarker-positive subgroup, and approaches for type I error rate, power analysis, and sample size calculation.","We present a numerical example of re-designing an oncology trial.","An extensive simulation study is conducted to evaluate the performance of the proposed design."],"url":"http://arxiv.org/abs/2406.06426v1","category":"stat.ME"}
{"created":"2024-06-10 16:02:48","title":"Differentially Private Best-Arm Identification","abstract":"Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies. Motivated by the data privacy concerns invoked by these applications, we study the problem of BAI with fixed confidence in both the local and central models, i.e. $\\epsilon$-local and $\\epsilon$-global Differential Privacy (DP). First, to quantify the cost of privacy, we derive lower bounds on the sample complexity of any $\\delta$-correct BAI algorithm satisfying $\\epsilon$-global DP or $\\epsilon$-local DP. Our lower bounds suggest the existence of two privacy regimes. In the high-privacy regime, the hardness depends on a coupled effect of privacy and novel information-theoretic quantities involving the Total Variation. In the low-privacy regime, the lower bounds reduce to the non-private lower bounds. We propose $\\epsilon$-local DP and $\\epsilon$-global DP variants of a Top Two algorithm, namely CTB-TT and AdaP-TT*, respectively. For $\\epsilon$-local DP, CTB-TT is asymptotically optimal by plugging in a private estimator of the means based on Randomised Response. For $\\epsilon$-global DP, our private estimator of the mean runs in arm-dependent adaptive episodes and adds Laplace noise to ensure a good privacy-utility trade-off. By adapting the transportation costs, the expected sample complexity of AdaP-TT* reaches the asymptotic lower bound up to multiplicative constants.","sentences":["Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies.","Motivated by the data privacy concerns invoked by these applications, we study the problem of BAI with fixed confidence in both the local and central models, i.e. $\\epsilon$-local and $\\epsilon$-global Differential Privacy (DP).","First, to quantify the cost of privacy, we derive lower bounds on the sample complexity of any $\\delta$-correct BAI algorithm satisfying $\\epsilon$-global DP or $\\epsilon$-local DP.","Our lower bounds suggest the existence of two privacy regimes.","In the high-privacy regime, the hardness depends on a coupled effect of privacy and novel information-theoretic quantities involving the Total Variation.","In the low-privacy regime, the lower bounds reduce to the non-private lower bounds.","We propose $\\epsilon$-local DP and $\\epsilon$-global DP variants of a Top Two algorithm, namely CTB-TT and AdaP-TT*, respectively.","For $\\epsilon$-local DP, CTB-TT is asymptotically optimal by plugging in a private estimator of the means based on Randomised Response.","For $\\epsilon$-global DP, our private estimator of the mean runs in arm-dependent adaptive episodes and adds Laplace noise to ensure a good privacy-utility trade-off.","By adapting the transportation costs, the expected sample complexity of AdaP-TT* reaches the asymptotic lower bound up to multiplicative constants."],"url":"http://arxiv.org/abs/2406.06408v1","category":"stat.ML"}
{"created":"2024-06-10 15:57:33","title":"A LoRa-based Energy-efficient Sensing System for Urban Data Collection","abstract":"Nowadays, cities provide much more than shopping opportunities or working spaces. Individual locations such as parks and squares are used as meeting points and local recreation areas by many people. To ensure that they remain attractive in the future, the design of such squares must be regularly adapted to the needs of the public. These utilization trends can be derived using public data collection. The more diverse and rich the data sets are, the easier it is to optimize public space design through data analysis. Traditional data collection methods such as questionnaires, observations, or videos are either labor intensive or cannot guarantee to preserve the individual's privacy. This work presents a privacy-preserving, low-power, and low-cost smart sensing system that is capable of anonymously collecting data about public space utilization by analyzing the occupancy distribution of public seating. To support future urban planning the sensor nodes are capable of monitoring environmental noise, chair utilization, and their position, temperature, and humidity and provide them over a city-wide Long Range Wide Area Network (LoRaWAN). The final sensing system's robust operation is proven in a trial run at two public squares in a city with 16 sensor nodes over a duration of two months. By consuming 33.65 mWh per day with all subsystems enabled, including sitting detection based on a continuous acceleration measurement operating on a robust and simple threshold algorithm, the custom-designed sensor node achieves continuous monitoring during the 2-month trial run. The evaluation of the experimental results clearly shows how the two locations are used, which confirms the practicability of the proposed solution. All data collected during the field trial is publicly available as open data.","sentences":["Nowadays, cities provide much more than shopping opportunities or working spaces.","Individual locations such as parks and squares are used as meeting points and local recreation areas by many people.","To ensure that they remain attractive in the future, the design of such squares must be regularly adapted to the needs of the public.","These utilization trends can be derived using public data collection.","The more diverse and rich the data sets are, the easier it is to optimize public space design through data analysis.","Traditional data collection methods such as questionnaires, observations, or videos are either labor intensive or cannot guarantee to preserve the individual's privacy.","This work presents a privacy-preserving, low-power, and low-cost smart sensing system that is capable of anonymously collecting data about public space utilization by analyzing the occupancy distribution of public seating.","To support future urban planning the sensor nodes are capable of monitoring environmental noise, chair utilization, and their position, temperature, and humidity and provide them over a city-wide Long Range Wide Area Network (LoRaWAN).","The final sensing system's robust operation is proven in a trial run at two public squares in a city with 16 sensor nodes over a duration of two months.","By consuming 33.65 mWh per day with all subsystems enabled, including sitting detection based on a continuous acceleration measurement operating on a robust and simple threshold algorithm, the custom-designed sensor node achieves continuous monitoring during the 2-month trial run.","The evaluation of the experimental results clearly shows how the two locations are used, which confirms the practicability of the proposed solution.","All data collected during the field trial is publicly available as open data."],"url":"http://arxiv.org/abs/2406.06404v1","category":"eess.SY"}
{"created":"2024-06-10 15:52:49","title":"Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue","abstract":"We study the limitations of Large Language Models (LLMs) for the task of response generation in human-machine dialogue. Several techniques have been proposed in the literature for different dialogue types (e.g., Open-Domain). However, the evaluations of these techniques have been limited in terms of base LLMs, dialogue types and evaluation metrics. In this work, we extensively analyze different LLM adaptation techniques when applied to different dialogue types. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue types Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering. We evaluate the performance of in-context learning and fine-tuning techniques across datasets selected for each dialogue type. We assess the impact of incorporating external knowledge to ground the generation in both scenarios of Retrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent evaluation and explainability criteria for automatic metrics and human evaluation protocols. Our analysis shows that there is no universal best-technique for adapting large language models as the efficacy of each technique depends on both the base LLM and the specific type of dialogue. Last but not least, the assessment of the best adaptation technique should include human evaluation to avoid false expectations and outcomes derived from automatic metrics.","sentences":["We study the limitations of Large Language Models (LLMs) for the task of response generation in human-machine dialogue.","Several techniques have been proposed in the literature for different dialogue types (e.g., Open-Domain).","However, the evaluations of these techniques have been limited in terms of base LLMs, dialogue types and evaluation metrics.","In this work, we extensively analyze different LLM adaptation techniques when applied to different dialogue types.","We have selected two base LLMs, Llama-2 and Mistral, and four dialogue types Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.","We evaluate the performance of in-context learning and fine-tuning techniques across datasets selected for each dialogue type.","We assess the impact of incorporating external knowledge to ground the generation in both scenarios of Retrieval-Augmented Generation (RAG) and gold knowledge.","We adopt consistent evaluation and explainability criteria for automatic metrics and human evaluation protocols.","Our analysis shows that there is no universal best-technique for adapting large language models as the efficacy of each technique depends on both the base LLM and the specific type of dialogue.","Last but not least, the assessment of the best adaptation technique should include human evaluation to avoid false expectations and outcomes derived from automatic metrics."],"url":"http://arxiv.org/abs/2406.06399v1","category":"cs.CL"}
{"created":"2024-06-10 15:52:27","title":"Universality of AdaGrad Stepsizes for Stochastic Optimization: Inexact Oracle, Acceleration and Variance Reduction","abstract":"We present adaptive gradient methods (both basic and accelerated) for solving convex composite optimization problems in which the main part is approximately smooth (a.k.a. $(\\delta, L)$-smooth) and can be accessed only via a (potentially biased) stochastic gradient oracle. This setting covers many interesting examples including H\\\"older smooth problems and various inexact computations of the stochastic gradient. Our methods use AdaGrad stepsizes and are adaptive in the sense that they do not require knowing any problem-dependent constants except an estimate of the diameter of the feasible set but nevertheless achieve the best possible convergence rates as if they knew the corresponding constants. We demonstrate that AdaGrad stepsizes work in a variety of situations by proving, in a unified manner, three types of new results. First, we establish efficiency guarantees for our methods in the classical setting where the oracle's variance is uniformly bounded. We then show that, under more refined assumptions on the variance, the same methods without any modifications enjoy implicit variance reduction properties allowing us to express their complexity estimates in terms of the variance only at the minimizer. Finally, we show how to incorporate explicit SVRG-type variance reduction into our methods and obtain even faster algorithms. In all three cases, we present both basic and accelerated algorithms achieving state-of-the-art complexity bounds. As a direct corollary of our results, we obtain universal stochastic gradient methods for H\\\"older smooth problems which can be used in all situations.","sentences":["We present adaptive gradient methods (both basic and accelerated) for solving convex composite optimization problems in which the main part is approximately smooth (a.k.a. $(\\delta, L)$-smooth) and can be accessed only via a (potentially biased) stochastic gradient oracle.","This setting covers many interesting examples including H\\\"older smooth problems and various inexact computations of the stochastic gradient.","Our methods use AdaGrad stepsizes and are adaptive in the sense that they do not require knowing any problem-dependent constants except an estimate of the diameter of the feasible set but nevertheless achieve the best possible convergence rates as if they knew the corresponding constants.","We demonstrate that AdaGrad stepsizes work in a variety of situations by proving, in a unified manner, three types of new results.","First, we establish efficiency guarantees for our methods in the classical setting where the oracle's variance is uniformly bounded.","We then show that, under more refined assumptions on the variance, the same methods without any modifications enjoy implicit variance reduction properties allowing us to express their complexity estimates in terms of the variance only at the minimizer.","Finally, we show how to incorporate explicit SVRG-type variance reduction into our methods and obtain even faster algorithms.","In all three cases, we present both basic and accelerated algorithms achieving state-of-the-art complexity bounds.","As a direct corollary of our results, we obtain universal stochastic gradient methods for H\\\"older smooth problems which can be used in all situations."],"url":"http://arxiv.org/abs/2406.06398v1","category":"math.OC"}
{"created":"2024-06-10 15:46:25","title":"Towards Lifelong Learning of Large Language Models: A Survey","abstract":"As the applications of large language models (LLMs) expand across diverse fields, the ability of these models to adapt to ongoing changes in data, tasks, and user preferences becomes crucial. Traditional training methods, relying on static datasets, are increasingly inadequate for coping with the dynamic nature of real-world information. Lifelong learning, also known as continual or incremental learning, addresses this challenge by enabling LLMs to learn continuously and adaptively over their operational lifetime, integrating new knowledge while retaining previously learned information and preventing catastrophic forgetting. This survey delves into the sophisticated landscape of lifelong learning, categorizing strategies into two primary groups: Internal Knowledge and External Knowledge. Internal Knowledge includes continual pretraining and continual finetuning, each enhancing the adaptability of LLMs in various scenarios. External Knowledge encompasses retrieval-based and tool-based lifelong learning, leveraging external data sources and computational tools to extend the model's capabilities without modifying core parameters. The key contributions of our survey are: (1) Introducing a novel taxonomy categorizing the extensive literature of lifelong learning into 12 scenarios; (2) Identifying common techniques across all lifelong learning scenarios and classifying existing literature into various technique groups within each scenario; (3) Highlighting emerging techniques such as model expansion and data selection, which were less explored in the pre-LLM era. Through a detailed examination of these groups and their respective categories, this survey aims to enhance the adaptability, reliability, and overall performance of LLMs in real-world applications.","sentences":["As the applications of large language models (LLMs) expand across diverse fields, the ability of these models to adapt to ongoing changes in data, tasks, and user preferences becomes crucial.","Traditional training methods, relying on static datasets, are increasingly inadequate for coping with the dynamic nature of real-world information.","Lifelong learning, also known as continual or incremental learning, addresses this challenge by enabling LLMs to learn continuously and adaptively over their operational lifetime, integrating new knowledge while retaining previously learned information and preventing catastrophic forgetting.","This survey delves into the sophisticated landscape of lifelong learning, categorizing strategies into two primary groups: Internal Knowledge and External Knowledge.","Internal Knowledge includes continual pretraining and continual finetuning, each enhancing the adaptability of LLMs in various scenarios.","External Knowledge encompasses retrieval-based and tool-based lifelong learning, leveraging external data sources and computational tools to extend the model's capabilities without modifying core parameters.","The key contributions of our survey are: (1) Introducing a novel taxonomy categorizing the extensive literature of lifelong learning into 12 scenarios; (2) Identifying common techniques across all lifelong learning scenarios and classifying existing literature into various technique groups within each scenario; (3) Highlighting emerging techniques such as model expansion and data selection, which were less explored in the pre-LLM era.","Through a detailed examination of these groups and their respective categories, this survey aims to enhance the adaptability, reliability, and overall performance of LLMs in real-world applications."],"url":"http://arxiv.org/abs/2406.06391v1","category":"cs.LG"}
{"created":"2024-06-10 15:44:41","title":"FPN-IAIA-BL: A Multi-Scale Interpretable Deep Learning Model for Classification of Mass Margins in Digital Mammography","abstract":"Digital mammography is essential to breast cancer detection, and deep learning offers promising tools for faster and more accurate mammogram analysis. In radiology and other high-stakes environments, uninterpretable (\"black box\") deep learning models are unsuitable and there is a call in these fields to make interpretable models. Recent work in interpretable computer vision provides transparency to these formerly black boxes by utilizing prototypes for case-based explanations, achieving high accuracy in applications including mammography. However, these models struggle with precise feature localization, reasoning on large portions of an image when only a small part is relevant. This paper addresses this gap by proposing a novel multi-scale interpretable deep learning model for mammographic mass margin classification. Our contribution not only offers an interpretable model with reasoning aligned with radiologist practices, but also provides a general architecture for computer vision with user-configurable prototypes from coarse- to fine-grained prototypes.","sentences":["Digital mammography is essential to breast cancer detection, and deep learning offers promising tools for faster and more accurate mammogram analysis.","In radiology and other high-stakes environments, uninterpretable (\"black box\") deep learning models are unsuitable and there is a call in these fields to make interpretable models.","Recent work in interpretable computer vision provides transparency to these formerly black boxes by utilizing prototypes for case-based explanations, achieving high accuracy in applications including mammography.","However, these models struggle with precise feature localization, reasoning on large portions of an image when only a small part is relevant.","This paper addresses this gap by proposing a novel multi-scale interpretable deep learning model for mammographic mass margin classification.","Our contribution not only offers an interpretable model with reasoning aligned with radiologist practices, but also provides a general architecture for computer vision with user-configurable prototypes from coarse- to fine-grained prototypes."],"url":"http://arxiv.org/abs/2406.06386v1","category":"cs.CV"}
{"created":"2024-06-10 15:44:22","title":"Low-Rank Quantization-Aware Training for LLMs","abstract":"Large language models (LLMs) are omnipresent, however their practical deployment is challenging due to their ever increasing computational and memory demands. Quantization is one of the most effective ways to make them more compute and memory efficient. Quantization-aware training (QAT) methods, generally produce the best quantized performance, however it comes at the cost of potentially long training time and excessive memory usage, making it impractical when applying for LLMs. Inspired by parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a lightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several components to save memory without sacrificing predictive performance: (a) low-rank auxiliary weights that are aware of the quantization grid; (b) a downcasting operator using fixed-point or double-packed integers and (c) checkpointing. Unlike most related work, our method (i) is inference-efficient, leading to no additional overhead compared to traditional PTQ; (ii) can be seen as a general extended pretraining framework, meaning that the resulting model can still be utilized for any downstream task afterwards; (iii) can be applied across a wide range of quantization settings, such as different choices quantization granularity, activation quantization, and seamlessly combined with many PTQ techniques. We apply LR-QAT to the LLaMA-2/3 and Mistral model families and validate its effectiveness on several downstream tasks. Our method outperforms common post-training quantization (PTQ) approaches and reaches the same model performance as full-model QAT at the fraction of its memory usage. Specifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of memory.","sentences":["Large language models (LLMs) are omnipresent, however their practical deployment is challenging due to their ever increasing computational and memory demands.","Quantization is one of the most effective ways to make them more compute and memory efficient.","Quantization-aware training (QAT) methods, generally produce the best quantized performance, however it comes at the cost of potentially long training time and excessive memory usage, making it impractical when applying for LLMs.","Inspired by parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a lightweight and memory-efficient QAT algorithm for LLMs.","LR-QAT employs several components to save memory without sacrificing predictive performance: (a) low-rank auxiliary weights that are aware of the quantization grid; (b) a downcasting operator using fixed-point or double-packed integers and (c) checkpointing.","Unlike most related work, our method (i) is inference-efficient, leading to no additional overhead compared to traditional PTQ; (ii) can be seen as a general extended pretraining framework, meaning that the resulting model can still be utilized for any downstream task afterwards; (iii) can be applied across a wide range of quantization settings, such as different choices quantization granularity, activation quantization, and seamlessly combined with many PTQ techniques.","We apply LR-QAT to the LLaMA-2/3 and Mistral model families and validate its effectiveness on several downstream tasks.","Our method outperforms common post-training quantization (PTQ) approaches and reaches the same model performance as full-model QAT at the fraction of its memory usage.","Specifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of memory."],"url":"http://arxiv.org/abs/2406.06385v1","category":"cs.LG"}
{"created":"2024-06-10 15:13:57","title":"A quantitative investigation for deployment of mobile collaborative robots in high-value manufacturing","abstract":"Component inspection is often the bottleneck in high-value manufacturing, driving industries like aerospace toward automated inspection technologies. Current systems often employ fixed arm robots, but they lack the flexibility in adapting to new components or orientations Advanced mobile robotic platforms with updated sensor technologies and algorithms have improved localization and path planning capabilities, making them ideal for bringing inspection processes directly to parts. However, mobile platforms introduce challenges in localization and maneuverability, leading to potential errors. Their positional uncertainty is higher than fixed systems due to the lack of a fixed calibrated location, posing challenges for position-sensitive inspection sensors. Therefore, it's essential to assess the positional accuracy and repeatability of mobile manipulator platforms. The KUKA KMR iiwa was chosen for its collaborative features, robust build, and scalability within the KUKA product range. The accuracy and repeatability of the mobile platform were evaluated through a series of tests to evaluate the performance of its integrated feature mapping, the effect of various speeds on positional accuracy, and the efficiency of the omnidirectional wheels for a range of translation orientations. Experimental evaluation revealed that enabling feature mapping substantially improves the KUKA KMR iiwa's performance, with accuracy gains and error reductions exceeding 90%. Repeatability errors were under 7 mm with mapping activated and around 2.5 mm in practical scenarios, demonstrating that mobile manipulators, incorporating both the manipulator and platform, can fulfil the precise requirements of industries with high precision needs. Providing a highly diverse alternative to traditional fixed-base industrial manipulators.","sentences":["Component inspection is often the bottleneck in high-value manufacturing, driving industries like aerospace toward automated inspection technologies.","Current systems often employ fixed arm robots, but they lack the flexibility in adapting to new components or orientations Advanced mobile robotic platforms with updated sensor technologies and algorithms have improved localization and path planning capabilities, making them ideal for bringing inspection processes directly to parts.","However, mobile platforms introduce challenges in localization and maneuverability, leading to potential errors.","Their positional uncertainty is higher than fixed systems due to the lack of a fixed calibrated location, posing challenges for position-sensitive inspection sensors.","Therefore, it's essential to assess the positional accuracy and repeatability of mobile manipulator platforms.","The KUKA KMR iiwa was chosen for its collaborative features, robust build, and scalability within the KUKA product range.","The accuracy and repeatability of the mobile platform were evaluated through a series of tests to evaluate the performance of its integrated feature mapping, the effect of various speeds on positional accuracy, and the efficiency of the omnidirectional wheels for a range of translation orientations.","Experimental evaluation revealed that enabling feature mapping substantially improves the KUKA KMR iiwa's performance, with accuracy gains and error reductions exceeding 90%.","Repeatability errors were under 7 mm with mapping activated and around 2.5 mm in practical scenarios, demonstrating that mobile manipulators, incorporating both the manipulator and platform, can fulfil the precise requirements of industries with high precision needs.","Providing a highly diverse alternative to traditional fixed-base industrial manipulators."],"url":"http://arxiv.org/abs/2406.06353v1","category":"cs.RO"}
{"created":"2024-06-10 15:13:51","title":"Latent Directions: A Simple Pathway to Bias Mitigation in Generative AI","abstract":"Mitigating biases in generative AI and, particularly in text-to-image models, is of high importance given their growing implications in society. The biased datasets used for training pose challenges in ensuring the responsible development of these models, and mitigation through hard prompting or embedding alteration, are the most common present solutions. Our work introduces a novel approach to achieve diverse and inclusive synthetic images by learning a direction in the latent space and solely modifying the initial Gaussian noise provided for the diffusion process. Maintaining a neutral prompt and untouched embeddings, this approach successfully adapts to diverse debiasing scenarios, such as geographical biases. Moreover, our work proves it is possible to linearly combine these learned latent directions to introduce new mitigations, and if desired, integrate it with text embedding adjustments. Furthermore, text-to-image models lack transparency for assessing bias in outputs, unless visually inspected. Thus, we provide a tool to empower developers to select their desired concepts to mitigate. The project page with code is available online.","sentences":["Mitigating biases in generative AI and, particularly in text-to-image models, is of high importance given their growing implications in society.","The biased datasets used for training pose challenges in ensuring the responsible development of these models, and mitigation through hard prompting or embedding alteration, are the most common present solutions.","Our work introduces a novel approach to achieve diverse and inclusive synthetic images by learning a direction in the latent space and solely modifying the initial Gaussian noise provided for the diffusion process.","Maintaining a neutral prompt and untouched embeddings, this approach successfully adapts to diverse debiasing scenarios, such as geographical biases.","Moreover, our work proves it is possible to linearly combine these learned latent directions to introduce new mitigations, and if desired, integrate it with text embedding adjustments.","Furthermore, text-to-image models lack transparency for assessing bias in outputs, unless visually inspected.","Thus, we provide a tool to empower developers to select their desired concepts to mitigate.","The project page with code is available online."],"url":"http://arxiv.org/abs/2406.06352v1","category":"cs.CV"}
{"created":"2024-06-10 14:46:07","title":"A Parameter-efficient Language Extension Framework for Multilingual ASR","abstract":"Covering all languages with a multilingual speech recognition model (MASR) is very difficult. Performing language extension on top of an existing MASR is a desirable choice. In this study, the MASR continual learning problem is probabilistically decomposed into language identity prediction (LP) and cross-lingual adaptation (XLA) sub-problems. Based on this, we propose an architecture-based framework for language extension that can fundamentally solve catastrophic forgetting, debudded as PELE. PELE is designed to be parameter-efficient, incrementally incorporating an add-on module to adapt to a new language. Specifically, different parameter-efficient fine-tuning (PEFT) modules and their variants are explored as potential candidates to perform XLA. Experiments are carried out on 5 new languages with a wide range of low-resourced data sizes. The best-performing PEFT candidate can achieve satisfactory performance across all languages and demonstrates superiority in three of five languages over the continual joint learning setting. Notably, PEFT methods focusing on weight parameters or input features are revealed to be limited in performance, showing significantly inferior extension capabilities compared to inserting a lightweight module in between layers such as an Adapter.","sentences":["Covering all languages with a multilingual speech recognition model (MASR) is very difficult.","Performing language extension on top of an existing MASR is a desirable choice.","In this study, the MASR continual learning problem is probabilistically decomposed into language identity prediction (LP) and cross-lingual adaptation (XLA) sub-problems.","Based on this, we propose an architecture-based framework for language extension that can fundamentally solve catastrophic forgetting, debudded as PELE.","PELE is designed to be parameter-efficient, incrementally incorporating an add-on module to adapt to a new language.","Specifically, different parameter-efficient fine-tuning (PEFT) modules and their variants are explored as potential candidates to perform XLA.","Experiments are carried out on 5 new languages with a wide range of low-resourced data sizes.","The best-performing PEFT candidate can achieve satisfactory performance across all languages and demonstrates superiority in three of five languages over the continual joint learning setting.","Notably, PEFT methods focusing on weight parameters or input features are revealed to be limited in performance, showing significantly inferior extension capabilities compared to inserting a lightweight module in between layers such as an Adapter."],"url":"http://arxiv.org/abs/2406.06329v1","category":"cs.CL"}
{"created":"2024-06-10 14:33:59","title":"Should my Blockchain Learn to Drive? A Study of Hyperledger Fabric","abstract":"Similar to other transaction processing frameworks, blockchain systems need to be dynamically reconfigured to adapt to varying workloads and changes in network conditions. However, achieving optimal reconfiguration is particularly challenging due to the complexity of the blockchain stack, which has diverse configurable parameters. This paper explores the concept of self-driving blockchains, which have the potential to predict workload changes and reconfigure themselves for optimal performance without human intervention. We compare and contrast our discussions with existing research on databases and highlight aspects unique to blockchains. We identify specific parameters and components in Hyperledger Fabric, a popular permissioned blockchain system, that are suitable for autonomous adaptation and offer potential solutions for the challenges involved. Further, we implement three demonstrative locally autonomous systems, each targeting a different layer of the blockchain stack, and conduct experiments to understand the feasibility of our findings. Our experiments indicate up to 11% improvement in success throughput and a 30% decrease in latency, making this a significant step towards implementing a fully autonomous blockchain system in the future.","sentences":["Similar to other transaction processing frameworks, blockchain systems need to be dynamically reconfigured to adapt to varying workloads and changes in network conditions.","However, achieving optimal reconfiguration is particularly challenging due to the complexity of the blockchain stack, which has diverse configurable parameters.","This paper explores the concept of self-driving blockchains, which have the potential to predict workload changes and reconfigure themselves for optimal performance without human intervention.","We compare and contrast our discussions with existing research on databases and highlight aspects unique to blockchains.","We identify specific parameters and components in Hyperledger Fabric, a popular permissioned blockchain system, that are suitable for autonomous adaptation and offer potential solutions for the challenges involved.","Further, we implement three demonstrative locally autonomous systems, each targeting a different layer of the blockchain stack, and conduct experiments to understand the feasibility of our findings.","Our experiments indicate up to 11% improvement in success throughput and a 30% decrease in latency, making this a significant step towards implementing a fully autonomous blockchain system in the future."],"url":"http://arxiv.org/abs/2406.06318v1","category":"cs.DC"}
{"created":"2024-06-10 14:11:27","title":"Identifying Bottlenecks of NISQ-friendly HHL algorithms","abstract":"Quantum computing promises enabling solving large problem instances, e.g. large linear equation systems with HHL algorithm, once the hardware stack matures. For the foreseeable future quantum computing will remain in the so-called NISQ era, in which the algorithms need to account for the flaws of the hardware such as noise. In this work, we perform an empirical study to test scaling properties and directly related noise resilience of the the most resources-intense component of the HHL algorithm, namely QPE and its NISQ-adaptation Iterative QPE. We explore the effectiveness of noise mitigation techniques for these algorithms and investigate whether we can keep the gate number low by enforcing sparsity constraints on the input or using circuit optimization techniques provided by Qiskit package. Our results indicate that currently available noise mitigation techniques, such as Qiskit readout and Mthree readout packages, are insufficient for enabling results recovery even in the small instances tested here. Moreover, our results indicate that the scaling of these algorithms with increase in precision seems to be the most substantial obstacle. These insights allowed us to deduce an approximate bottleneck for algorithms that consider a similar time evolution as QPE. Such observations provide evidence of weaknesses of such algorithms on NISQ devices and help us formulate meaningful future research directions.","sentences":["Quantum computing promises enabling solving large problem instances, e.g. large linear equation systems with HHL algorithm, once the hardware stack matures.","For the foreseeable future quantum computing will remain in the so-called NISQ era, in which the algorithms need to account for the flaws of the hardware such as noise.","In this work, we perform an empirical study to test scaling properties and directly related noise resilience of the the most resources-intense component of the HHL algorithm, namely QPE and its NISQ-adaptation Iterative QPE.","We explore the effectiveness of noise mitigation techniques for these algorithms and investigate whether we can keep the gate number low by enforcing sparsity constraints on the input or using circuit optimization techniques provided by Qiskit package.","Our results indicate that currently available noise mitigation techniques, such as Qiskit readout and Mthree readout packages, are insufficient for enabling results recovery even in the small instances tested here.","Moreover, our results indicate that the scaling of these algorithms with increase in precision seems to be the most substantial obstacle.","These insights allowed us to deduce an approximate bottleneck for algorithms that consider a similar time evolution as QPE.","Such observations provide evidence of weaknesses of such algorithms on NISQ devices and help us formulate meaningful future research directions."],"url":"http://arxiv.org/abs/2406.06288v1","category":"quant-ph"}
{"created":"2024-06-10 14:01:21","title":"PowerInfer-2: Fast Large Language Model Inference on a Smartphone","abstract":"This paper introduces PowerInfer-2, a framework designed for high-speed inference of Large Language Models (LLMs) on smartphones, particularly effective for models whose sizes exceed the device's memory capacity. The key insight of PowerInfer-2 is to utilize the heterogeneous computation, memory, and I/O resources in smartphones by decomposing traditional matrix computations into fine-grained neuron cluster computations. Specifically, PowerInfer-2 features a polymorphic neuron engine that adapts computational strategies for various stages of LLM inference. Additionally, it introduces segmented neuron caching and fine-grained neuron-cluster-level pipelining, which effectively minimize and conceal the overhead caused by I/O operations. The implementation and evaluation of PowerInfer-2 demonstrate its capability to support a wide array of LLM models on two smartphones, achieving up to a 29.2x speed increase compared with state-of-the-art frameworks. Notably, PowerInfer-2 is the first system to serve the TurboSparse-Mixtral-47B model with a generation rate of 11.68 tokens per second on a smartphone. For models that fit entirely within the memory, PowerInfer-2 can achieve approximately a 40% reduction in memory usage while maintaining inference speeds comparable to llama.cpp and MLC-LLM. For more details, including a demonstration video, please visit the project site at www.powerinfer.ai/v2.","sentences":["This paper introduces PowerInfer-2, a framework designed for high-speed inference of Large Language Models (LLMs) on smartphones, particularly effective for models whose sizes exceed the device's memory capacity.","The key insight of PowerInfer-2 is to utilize the heterogeneous computation, memory, and I/O resources in smartphones by decomposing traditional matrix computations into fine-grained neuron cluster computations.","Specifically, PowerInfer-2 features a polymorphic neuron engine that adapts computational strategies for various stages of LLM inference.","Additionally, it introduces segmented neuron caching and fine-grained neuron-cluster-level pipelining, which effectively minimize and conceal the overhead caused by I/O operations.","The implementation and evaluation of PowerInfer-2 demonstrate its capability to support a wide array of LLM models on two smartphones, achieving up to a 29.2x speed increase compared with state-of-the-art frameworks.","Notably, PowerInfer-2 is the first system to serve the TurboSparse-Mixtral-47B model with a generation rate of 11.68 tokens per second on a smartphone.","For models that fit entirely within the memory, PowerInfer-2 can achieve approximately a 40% reduction in memory usage while maintaining inference speeds comparable to llama.cpp and MLC-LLM.","For more details, including a demonstration video, please visit the project site at www.powerinfer.ai/v2."],"url":"http://arxiv.org/abs/2406.06282v2","category":"cs.LG"}
{"created":"2024-06-10 13:59:28","title":"Optimal sensing policy with interference-model uncertainty","abstract":"Assume that an interferer behaves according to a parametric model but one does not know the value of the model parameters. Sensing enables to improve the model knowledge and therefore perform a better link adaptation. However, we consider a half-duplex scenario where, at each time slot, the communication system should decide between sensing and communication. We thus propose to investigate the optimal policy to maximize the expected sum rate given a finite-time communication. % the following question therefore arises: At a given time slot, should one sense or communicate? We first show that this problem can be modelled in the Markov decision process (MDP) framework. We then demonstrate that the optimal open-loop and closed-loop policies can be found significantly faster than the standard backward-induction algorithm.","sentences":["Assume that an interferer behaves according to a parametric model but one does not know the value of the model parameters.","Sensing enables to improve the model knowledge and therefore perform a better link adaptation.","However, we consider a half-duplex scenario where, at each time slot, the communication system should decide between sensing and communication.","We thus propose to investigate the optimal policy to maximize the expected sum rate given a finite-time communication.","% the following question therefore arises: At a given time slot, should one sense or communicate?","We first show that this problem can be modelled in the Markov decision process (MDP) framework.","We then demonstrate that the optimal open-loop and closed-loop policies can be found significantly faster than the standard backward-induction algorithm."],"url":"http://arxiv.org/abs/2406.06280v2","category":"cs.IT"}
{"created":"2024-06-10 13:58:46","title":"Multi-Prompting Decoder Helps Better Language Understanding","abstract":"Recent Pre-trained Language Models (PLMs) usually only provide users with the inference APIs, namely the emerging Model-as-a-Service (MaaS) setting. To adapt MaaS PLMs to downstream tasks without accessing their parameters and gradients, some existing methods focus on the output-side adaptation of PLMs, viewing the PLM as an encoder and then optimizing a task-specific decoder for decoding the output hidden states and class scores of the PLM. Despite the effectiveness of these methods, they only use a single prompt to query PLMs for decoding, leading to a heavy reliance on the quality of the adopted prompt. In this paper, we propose a simple yet effective Multi-Prompting Decoder (MPD) framework for MaaS adaptation. The core idea is to query PLMs with multiple different prompts for each sample, thereby obtaining multiple output hidden states and class scores for subsequent decoding. Such multi-prompting decoding paradigm can simultaneously mitigate reliance on the quality of a single prompt, alleviate the issue of data scarcity under the few-shot setting, and provide richer knowledge extracted from PLMs. Specifically, we propose two decoding strategies: multi-prompting decoding with optimal transport for hidden states and calibrated decoding for class scores. Extensive experiments demonstrate that our method achieves new state-of-the-art results on multiple natural language understanding datasets under the few-shot setting.","sentences":["Recent Pre-trained Language Models (PLMs) usually only provide users with the inference APIs, namely the emerging Model-as-a-Service (MaaS) setting.","To adapt MaaS PLMs to downstream tasks without accessing their parameters and gradients, some existing methods focus on the output-side adaptation of PLMs, viewing the PLM as an encoder and then optimizing a task-specific decoder for decoding the output hidden states and class scores of the PLM.","Despite the effectiveness of these methods, they only use a single prompt to query PLMs for decoding, leading to a heavy reliance on the quality of the adopted prompt.","In this paper, we propose a simple yet effective Multi-Prompting Decoder (MPD) framework for MaaS adaptation.","The core idea is to query PLMs with multiple different prompts for each sample, thereby obtaining multiple output hidden states and class scores for subsequent decoding.","Such multi-prompting decoding paradigm can simultaneously mitigate reliance on the quality of a single prompt, alleviate the issue of data scarcity under the few-shot setting, and provide richer knowledge extracted from PLMs.","Specifically, we propose two decoding strategies: multi-prompting decoding with optimal transport for hidden states and calibrated decoding for class scores.","Extensive experiments demonstrate that our method achieves new state-of-the-art results on multiple natural language understanding datasets under the few-shot setting."],"url":"http://arxiv.org/abs/2406.06279v1","category":"cs.CL"}
{"created":"2024-06-10 13:53:31","title":"Global-in-time energy stability analysis for the exponential time differencing Runge-Kutta scheme for the phase field crystal equation","abstract":"The global-in-time energy estimate is derived for the second-order accurate exponential time differencing Runge-Kutta (ETDRK2) numerical scheme to the phase field crystal (PFC) equation, a sixth-order parabolic equation modeling crystal evolution. To recover the value of stabilization constant, some local-in-time convergence analysis has been reported, and the energy stability becomes available over a fixed final time. In this work, we develop a global-in-time energy estimate for the ETDRK2 numerical scheme to the PFC equation by showing the energy dissipation property for any final time. An a priori assumption at the previous time step, combined with a single-step $H^2$ estimate of the numerical solution, is the key point in the analysis. Such an $H^2$ estimate recovers the maximum norm bound of the numerical solution at the next time step, and then the value of the stabilization parameter can be theoretically justified. This justification ensures the energy dissipation at the next time step, so that the mathematical induction can be effectively applied, by then the global-in-time energy estimate is accomplished. This paper represents the first effort to theoretically establish a global-in-time energy stability analysis for a second-order stabilized numerical scheme in terms of the original free energy functional. The presented methodology is expected to be available for many other Runge-Kutta numerical schemes to the gradient flow equations.","sentences":["The global-in-time energy estimate is derived for the second-order accurate exponential time differencing Runge-Kutta (ETDRK2) numerical scheme to the phase field crystal (PFC) equation, a sixth-order parabolic equation modeling crystal evolution.","To recover the value of stabilization constant, some local-in-time convergence analysis has been reported, and the energy stability becomes available over a fixed final time.","In this work, we develop a global-in-time energy estimate for the ETDRK2 numerical scheme to the PFC equation by showing the energy dissipation property for any final time.","An a priori assumption at the previous time step, combined with a single-step $H^2$ estimate of the numerical solution, is the key point in the analysis.","Such an $H^2$ estimate recovers the maximum norm bound of the numerical solution at the next time step, and then the value of the stabilization parameter can be theoretically justified.","This justification ensures the energy dissipation at the next time step, so that the mathematical induction can be effectively applied, by then the global-in-time energy estimate is accomplished.","This paper represents the first effort to theoretically establish a global-in-time energy stability analysis for a second-order stabilized numerical scheme in terms of the original free energy functional.","The presented methodology is expected to be available for many other Runge-Kutta numerical schemes to the gradient flow equations."],"url":"http://arxiv.org/abs/2406.06272v1","category":"math.NA"}
{"created":"2024-06-10 13:41:10","title":"Tuning-Free Visual Customization via View Iterative Self-Attention Control","abstract":"Fine-Tuning Diffusion Models enable a wide range of personalized generation and editing applications on diverse visual modalities. While Low-Rank Adaptation (LoRA) accelerates the fine-tuning process, it still requires multiple reference images and time-consuming training, which constrains its scalability for large-scale and real-time applications. In this paper, we propose \\textit{View Iterative Self-Attention Control (VisCtrl)} to tackle this challenge. Specifically, VisCtrl is a training-free method that injects the appearance and structure of a user-specified subject into another subject in the target image, unlike previous approaches that require fine-tuning the model. Initially, we obtain the initial noise for both the reference and target images through DDIM inversion. Then, during the denoising phase, features from the reference image are injected into the target image via the self-attention mechanism. Notably, by iteratively performing this feature injection process, we ensure that the reference image features are gradually integrated into the target image. This approach results in consistent and harmonious editing with only one reference image in a few denoising steps. Moreover, benefiting from our plug-and-play architecture design and the proposed Feature Gradual Sampling strategy for multi-view editing, our method can be easily extended to edit in complex visual domains. Extensive experiments show the efficacy of VisCtrl across a spectrum of tasks, including personalized editing of images, videos, and 3D scenes.","sentences":["Fine-Tuning Diffusion Models enable a wide range of personalized generation and editing applications on diverse visual modalities.","While Low-Rank Adaptation (LoRA) accelerates the fine-tuning process, it still requires multiple reference images and time-consuming training, which constrains its scalability for large-scale and real-time applications.","In this paper, we propose \\textit{View Iterative Self-Attention Control (VisCtrl)} to tackle this challenge.","Specifically, VisCtrl is a training-free method that injects the appearance and structure of a user-specified subject into another subject in the target image, unlike previous approaches that require fine-tuning the model.","Initially, we obtain the initial noise for both the reference and target images through DDIM inversion.","Then, during the denoising phase, features from the reference image are injected into the target image via the self-attention mechanism.","Notably, by iteratively performing this feature injection process, we ensure that the reference image features are gradually integrated into the target image.","This approach results in consistent and harmonious editing with only one reference image in a few denoising steps.","Moreover, benefiting from our plug-and-play architecture design and the proposed Feature Gradual Sampling strategy for multi-view editing, our method can be easily extended to edit in complex visual domains.","Extensive experiments show the efficacy of VisCtrl across a spectrum of tasks, including personalized editing of images, videos, and 3D scenes."],"url":"http://arxiv.org/abs/2406.06258v2","category":"cs.CV"}
{"created":"2024-06-10 13:34:43","title":"Stabilized Adaptive Steering for 3D Sonar Microphone Arrays with IMU Sensor Fusion","abstract":"This paper presents a novel software-based approach to stabilizing the acoustic images for in-air 3D sonars. Due to uneven terrain, traditional static beamforming techniques can be misaligned, causing inaccurate measurements and imaging artifacts. Furthermore, mechanical stabilization can be more costly and prone to failure. We propose using an adaptive conventional beamforming approach by fusing it with real-time IMU data to adjust the sonar array's steering matrix dynamically based on the elevation tilt angle caused by the uneven ground. Additionally, we propose gaining compensation to offset emission energy loss due to the transducer's directivity pattern and validate our approach through various experiments, which show significant improvements in temporal consistency in the acoustic images. We implemented a GPU-accelerated software system that operates in real-time with an average execution time of 210ms, meeting autonomous navigation requirements.","sentences":["This paper presents a novel software-based approach to stabilizing the acoustic images for in-air 3D sonars.","Due to uneven terrain, traditional static beamforming techniques can be misaligned, causing inaccurate measurements and imaging artifacts.","Furthermore, mechanical stabilization can be more costly and prone to failure.","We propose using an adaptive conventional beamforming approach by fusing it with real-time IMU data to adjust the sonar array's steering matrix dynamically based on the elevation tilt angle caused by the uneven ground.","Additionally, we propose gaining compensation to offset emission energy loss due to the transducer's directivity pattern and validate our approach through various experiments, which show significant improvements in temporal consistency in the acoustic images.","We implemented a GPU-accelerated software system that operates in real-time with an average execution time of 210ms, meeting autonomous navigation requirements."],"url":"http://arxiv.org/abs/2406.06255v1","category":"cs.RO"}
{"created":"2024-06-10 13:34:23","title":"Understanding Students' Acceptance of ChatGPT as a Translation Tool: A UTAUT Model Analysis","abstract":"The potential of ChatGPT to transform the education landscape is drawing increasing attention. With its translation-related capabilities being tested and examined, ChatGPT presents both opportunities and challenges for translation training. The effective integration of ChatGPT into translation training necessitates an understanding of students' reactions to and acceptance of ChatGPT-assisted translation. Against this backdrop, this study draws on the Unified Theory of Acceptance and Use of Technology (UTAUT) to examine the potential determinants of students' adoption of ChatGPT for translation and investigates the moderating effects of use experience and translation training on those relationships. An online survey targeting university students in Hong Kong collected 308 valid responses, including 148 from translation students and 160 from non-translation students. Respondents were divided into two groups based on their ChatGPT use experience. Data were analyzed using structural equation modeling. A multigroup analysis revealed different structural relationships between the influencing factors of students' intention to use ChatGPT across groups. Notably, less-experienced users' behavioral intention to use ChatGPT for translation was more strongly correlated with social influence compared with experienced users. Non-translation students' use intention was more strongly driven by facilitating conditions compared to translation majors. These results are discussed with the different primary purposes of translation and non-translation students' translation practices. The findings of this study contribute to the growing body of research on AI-powered translation training and provide insights for the ongoing adaptation of translation training programs.","sentences":["The potential of ChatGPT to transform the education landscape is drawing increasing attention.","With its translation-related capabilities being tested and examined, ChatGPT presents both opportunities and challenges for translation training.","The effective integration of ChatGPT into translation training necessitates an understanding of students' reactions to and acceptance of ChatGPT-assisted translation.","Against this backdrop, this study draws on the Unified Theory of Acceptance and Use of Technology (UTAUT) to examine the potential determinants of students' adoption of ChatGPT for translation and investigates the moderating effects of use experience and translation training on those relationships.","An online survey targeting university students in Hong Kong collected 308 valid responses, including 148 from translation students and 160 from non-translation students.","Respondents were divided into two groups based on their ChatGPT use experience.","Data were analyzed using structural equation modeling.","A multigroup analysis revealed different structural relationships between the influencing factors of students' intention to use ChatGPT across groups.","Notably, less-experienced users' behavioral intention to use ChatGPT for translation was more strongly correlated with social influence compared with experienced users.","Non-translation students' use intention was more strongly driven by facilitating conditions compared to translation majors.","These results are discussed with the different primary purposes of translation and non-translation students' translation practices.","The findings of this study contribute to the growing body of research on AI-powered translation training and provide insights for the ongoing adaptation of translation training programs."],"url":"http://arxiv.org/abs/2406.06254v1","category":"cs.HC"}
{"created":"2024-06-10 13:31:18","title":"Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning","abstract":"As the scale of generative models continues to grow, efficient reuse and adaptation of pre-trained models have become crucial considerations. In this work, we propose Voicebox Adapter, a novel approach that integrates fine-grained conditions into a pre-trained Voicebox speech generation model using a cross-attention module. To ensure a smooth integration of newly added modules with pre-trained ones, we explore various efficient fine-tuning approaches. Our experiment shows that the LoRA with bias-tuning configuration yields the best performance, enhancing controllability without compromising speech quality. Across three fine-grained conditional generation tasks, we demonstrate the effectiveness and resource efficiency of Voicebox Adapter. Follow-up experiments further highlight the robustness of Voicebox Adapter across diverse data setups.","sentences":["As the scale of generative models continues to grow, efficient reuse and adaptation of pre-trained models have become crucial considerations.","In this work, we propose Voicebox Adapter, a novel approach that integrates fine-grained conditions into a pre-trained Voicebox speech generation model using a cross-attention module.","To ensure a smooth integration of newly added modules with pre-trained ones, we explore various efficient fine-tuning approaches.","Our experiment shows that the LoRA with bias-tuning configuration yields the best performance, enhancing controllability without compromising speech quality.","Across three fine-grained conditional generation tasks, we demonstrate the effectiveness and resource efficiency of Voicebox Adapter.","Follow-up experiments further highlight the robustness of Voicebox Adapter across diverse data setups."],"url":"http://arxiv.org/abs/2406.06251v1","category":"eess.AS"}
{"created":"2024-06-10 13:24:18","title":"Image Compression with Isotropic and Anisotropic Shepard Inpainting","abstract":"Inpainting-based codecs store sparse selected pixel data and decode by reconstructing the discarded image parts by inpainting. Successful codecs (coders and decoders) traditionally use inpainting operators that solve partial differential equations. This requires some numerical expertise if efficient implementations are necessary. Our goal is to investigate variants of Shepard inpainting as simple alternatives for inpainting-based compression. They can be implemented efficiently when we localise their weighting function. To turn them into viable codecs, we have to introduce novel extensions of classical Shepard interpolation that adapt successful ideas from previous codecs: Anisotropy allows direction-dependent inpainting, which improves reconstruction quality. Additionally, we incorporate data selection by subdivision as an efficient way to tailor the stored information to the image structure. On the encoding side, we introduce the novel concept of joint inpainting and prediction for isotropic Shepard codecs, where storage cost can be reduced based on intermediate inpainting results. In an ablation study, we show the usefulness of these individual contributions and demonstrate that they offer synergies which elevate the performance of Shepard inpainting to surprising levels. Our resulting approaches offer a more favourable trade-off between simplicity and quality than traditional inpainting-based codecs. Experiments show that they can outperform JPEG and JPEG2000 at high compression ratios.","sentences":["Inpainting-based codecs store sparse selected pixel data and decode by reconstructing the discarded image parts by inpainting.","Successful codecs (coders and decoders) traditionally use inpainting operators that solve partial differential equations.","This requires some numerical expertise if efficient implementations are necessary.","Our goal is to investigate variants of Shepard inpainting as simple alternatives for inpainting-based compression.","They can be implemented efficiently when we localise their weighting function.","To turn them into viable codecs, we have to introduce novel extensions of classical Shepard interpolation that adapt successful ideas from previous codecs: Anisotropy allows direction-dependent inpainting, which improves reconstruction quality.","Additionally, we incorporate data selection by subdivision as an efficient way to tailor the stored information to the image structure.","On the encoding side, we introduce the novel concept of joint inpainting and prediction for isotropic Shepard codecs, where storage cost can be reduced based on intermediate inpainting results.","In an ablation study, we show the usefulness of these individual contributions and demonstrate that they offer synergies which elevate the performance of Shepard inpainting to surprising levels.","Our resulting approaches offer a more favourable trade-off between simplicity and quality than traditional inpainting-based codecs.","Experiments show that they can outperform JPEG and JPEG2000 at high compression ratios."],"url":"http://arxiv.org/abs/2406.06247v1","category":"eess.IV"}
{"created":"2024-06-10 13:23:00","title":"Data-Efficient Learning with Neural Programs","abstract":"Many computational tasks can be naturally expressed as a composition of a DNN followed by a program written in a traditional programming language or an API call to an LLM. We call such composites \"neural programs\" and focus on the problem of learning the DNN parameters when the training data consist of end-to-end input-output labels for the composite. When the program is written in a differentiable logic programming language, techniques from neurosymbolic learning are applicable, but in general, the learning for neural programs requires estimating the gradients of black-box components. We present an algorithm for learning neural programs, called ISED, that only relies on input-output samples of black-box components. For evaluation, we introduce new benchmarks that involve calls to modern LLMs such as GPT-4 and also consider benchmarks from the neurosymolic learning literature. Our evaluation shows that for the latter benchmarks, ISED has comparable performance to state-of-the-art neurosymbolic frameworks. For the former, we use adaptations of prior work on gradient approximations of black-box components as a baseline, and show that ISED achieves comparable accuracy but in a more data- and sample-efficient manner.","sentences":["Many computational tasks can be naturally expressed as a composition of a DNN followed by a program written in a traditional programming language or an API call to an LLM.","We call such composites \"neural programs\" and focus on the problem of learning the DNN parameters when the training data consist of end-to-end input-output labels for the composite.","When the program is written in a differentiable logic programming language, techniques from neurosymbolic learning are applicable, but in general, the learning for neural programs requires estimating the gradients of black-box components.","We present an algorithm for learning neural programs, called ISED, that only relies on input-output samples of black-box components.","For evaluation, we introduce new benchmarks that involve calls to modern LLMs such as GPT-4 and also consider benchmarks from the neurosymolic learning literature.","Our evaluation shows that for the latter benchmarks, ISED has comparable performance to state-of-the-art neurosymbolic frameworks.","For the former, we use adaptations of prior work on gradient approximations of black-box components as a baseline, and show that ISED achieves comparable accuracy but in a more data- and sample-efficient manner."],"url":"http://arxiv.org/abs/2406.06246v1","category":"cs.LG"}
{"created":"2024-06-10 13:20:46","title":"Lower eigenvalue bounds with hybrid high-order methods","abstract":"This paper proposes hybrid high-order eigensolvers for the computation of guaranteed lower eigenvalue bounds. These bounds display higher order convergence rates and are accessible to adaptive mesh-refining algorithms. The involved constants arise from local embeddings and are available for all polynomial degrees. Applications include the linear elasticity and Steklov eigenvalue problem.","sentences":["This paper proposes hybrid high-order eigensolvers for the computation of guaranteed lower eigenvalue bounds.","These bounds display higher order convergence rates and are accessible to adaptive mesh-refining algorithms.","The involved constants arise from local embeddings and are available for all polynomial degrees.","Applications include the linear elasticity and Steklov eigenvalue problem."],"url":"http://arxiv.org/abs/2406.06244v2","category":"math.NA"}
{"created":"2024-06-10 13:08:31","title":"I-MPN: Inductive Message Passing Network for Effective and Efficient Human-in-the-Loop Annotation of Mobile Eye Tracking Data","abstract":"Understanding human visual processing in dynamic environments is essential for psychology and human-centered interaction design. Mobile eye-tracking systems, combining egocentric video and gaze signals, offer valuable insights. However, manual analysis of these recordings is time-intensive. In this work, we present a novel human-centered learning algorithm designed for automated object recognition within mobile eye-tracking settings. Our approach seamlessly integrates an object detector with an inductive message-passing network technique (I-MPN), harnessing node features such as node profile information and positions. This integration enables our algorithm to learn embedding functions capable of generalizing to new object angle views, thereby facilitating rapid adaptation and efficient reasoning in dynamic contexts as users navigate through their environment. Through experiments conducted on three distinct video sequences, our \\textit{interactive-based method} showcases significant performance improvements over fixed training/testing algorithms, even when trained on considerably smaller annotated samples collected through user feedback. Furthermore, we showcase exceptional efficiency in data annotation processes, surpassing approaches that use complete object detectors, combine detectors with convolutional networks, or employ interactive video segmentation.","sentences":["Understanding human visual processing in dynamic environments is essential for psychology and human-centered interaction design.","Mobile eye-tracking systems, combining egocentric video and gaze signals, offer valuable insights.","However, manual analysis of these recordings is time-intensive.","In this work, we present a novel human-centered learning algorithm designed for automated object recognition within mobile eye-tracking settings.","Our approach seamlessly integrates an object detector with an inductive message-passing network technique (I-MPN), harnessing node features such as node profile information and positions.","This integration enables our algorithm to learn embedding functions capable of generalizing to new object angle views, thereby facilitating rapid adaptation and efficient reasoning in dynamic contexts as users navigate through their environment.","Through experiments conducted on three distinct video sequences, our \\textit{interactive-based method} showcases significant performance improvements over fixed training/testing algorithms, even when trained on considerably smaller annotated samples collected through user feedback.","Furthermore, we showcase exceptional efficiency in data annotation processes, surpassing approaches that use complete object detectors, combine detectors with convolutional networks, or employ interactive video segmentation."],"url":"http://arxiv.org/abs/2406.06239v1","category":"cs.CV"}
{"created":"2024-06-10 13:06:13","title":"Adaptive combinations of tail-risk forecasts","abstract":"In order to meet the increasingly stringent global standards of banking management and regulation, several methods have been proposed in the literature for forecasting tail risk measures such as the Value-at-Risk (VaR) and Expected Shortfall (ES). However, regardless of the approach used, there are several sources of uncertainty, including model specifications, data-related issues and the estimation procedure, which can significantly affect the accuracy of VaR and ES measures. Aiming to mitigate the influence of these sources of uncertainty and improve the predictive performance of individual models, we propose novel forecast combination strategies based on the Model Confidence Set (MCS). In particular, consistent joint VaR and ES loss functions within the MCS framework are used to adaptively combine forecasts generated by a wide range of parametric, semi-parametric, and non-parametric models. Our results reveal that the proposed combined predictors provide a suitable alternative for forecasting risk measures, passing the usual backtests, entering the set of superior models of the MCS, and usually exhibiting lower standard deviations than other model specifications.","sentences":["In order to meet the increasingly stringent global standards of banking management and regulation, several methods have been proposed in the literature for forecasting tail risk measures such as the Value-at-Risk (VaR) and Expected Shortfall (ES).","However, regardless of the approach used, there are several sources of uncertainty, including model specifications, data-related issues and the estimation procedure, which can significantly affect the accuracy of VaR and ES measures.","Aiming to mitigate the influence of these sources of uncertainty and improve the predictive performance of individual models, we propose novel forecast combination strategies based on the Model Confidence Set (MCS).","In particular, consistent joint VaR and ES loss functions within the MCS framework are used to adaptively combine forecasts generated by a wide range of parametric, semi-parametric, and non-parametric models.","Our results reveal that the proposed combined predictors provide a suitable alternative for forecasting risk measures, passing the usual backtests, entering the set of superior models of the MCS, and usually exhibiting lower standard deviations than other model specifications."],"url":"http://arxiv.org/abs/2406.06235v1","category":"q-fin.RM"}
{"created":"2024-06-10 12:47:49","title":"Siren -- Advancing Cybersecurity through Deception and Adaptive Analysis","abstract":"Siren represents a pioneering research effort aimed at fortifying cybersecurity through strategic integration of deception, machine learning, and proactive threat analysis. Drawing inspiration from mythical sirens, this project employs sophisticated methods to lure potential threats into controlled environments. The system features a dynamic machine learning model for real-time analysis and classification, ensuring continuous adaptability to emerging cyber threats. The architectural framework includes a link monitoring proxy, a purpose-built machine learning model for dynamic link analysis, and a honeypot enriched with simulated user interactions to intensify threat engagement. Data protection within the honeypot is fortified with probabilistic encryption. Additionally, the incorporation of simulated user activity extends the system's capacity to capture and learn from potential attackers even after user disengagement. Siren introduces a paradigm shift in cybersecurity, transforming traditional defense mechanisms into proactive systems that actively engage and learn from potential adversaries. The research strives to enhance user protection while yielding valuable insights for ongoing refinement in response to the evolving landscape of cybersecurity threats.","sentences":["Siren represents a pioneering research effort aimed at fortifying cybersecurity through strategic integration of deception, machine learning, and proactive threat analysis.","Drawing inspiration from mythical sirens, this project employs sophisticated methods to lure potential threats into controlled environments.","The system features a dynamic machine learning model for real-time analysis and classification, ensuring continuous adaptability to emerging cyber threats.","The architectural framework includes a link monitoring proxy, a purpose-built machine learning model for dynamic link analysis, and a honeypot enriched with simulated user interactions to intensify threat engagement.","Data protection within the honeypot is fortified with probabilistic encryption.","Additionally, the incorporation of simulated user activity extends the system's capacity to capture and learn from potential attackers even after user disengagement.","Siren introduces a paradigm shift in cybersecurity, transforming traditional defense mechanisms into proactive systems that actively engage and learn from potential adversaries.","The research strives to enhance user protection while yielding valuable insights for ongoing refinement in response to the evolving landscape of cybersecurity threats."],"url":"http://arxiv.org/abs/2406.06225v1","category":"cs.CR"}
{"created":"2024-06-10 12:14:05","title":"Lurking in the shadows: Unveiling Stealthy Backdoor Attacks against Personalized Federated Learning","abstract":"Federated Learning (FL) is a collaborative machine learning technique where multiple clients work together with a central server to train a global model without sharing their private data. However, the distribution shift across non-IID datasets of clients poses a challenge to this one-model-fits-all method hindering the ability of the global model to effectively adapt to each client's unique local data. To echo this challenge, personalized FL (PFL) is designed to allow each client to create personalized local models tailored to their private data. While extensive research has scrutinized backdoor risks in FL, it has remained underexplored in PFL applications. In this study, we delve deep into the vulnerabilities of PFL to backdoor attacks. Our analysis showcases a tale of two cities. On the one hand, the personalization process in PFL can dilute the backdoor poisoning effects injected into the personalized local models. Furthermore, PFL systems can also deploy both server-end and client-end defense mechanisms to strengthen the barrier against backdoor attacks. On the other hand, our study shows that PFL fortified with these defense methods may offer a false sense of security. We propose \\textit{PFedBA}, a stealthy and effective backdoor attack strategy applicable to PFL systems. \\textit{PFedBA} ingeniously aligns the backdoor learning task with the main learning task of PFL by optimizing the trigger generation process. Our comprehensive experiments demonstrate the effectiveness of \\textit{PFedBA} in seamlessly embedding triggers into personalized local models. \\textit{PFedBA} yields outstanding attack performance across 10 state-of-the-art PFL algorithms, defeating the existing 6 defense mechanisms. Our study sheds light on the subtle yet potent backdoor threats to PFL systems, urging the community to bolster defenses against emerging backdoor challenges.","sentences":["Federated Learning (FL) is a collaborative machine learning technique where multiple clients work together with a central server to train a global model without sharing their private data.","However, the distribution shift across non-IID datasets of clients poses a challenge to this one-model-fits-all method hindering the ability of the global model to effectively adapt to each client's unique local data.","To echo this challenge, personalized FL (PFL) is designed to allow each client to create personalized local models tailored to their private data.","While extensive research has scrutinized backdoor risks in FL, it has remained underexplored in PFL applications.","In this study, we delve deep into the vulnerabilities of PFL to backdoor attacks.","Our analysis showcases a tale of two cities.","On the one hand, the personalization process in PFL can dilute the backdoor poisoning effects injected into the personalized local models.","Furthermore, PFL systems can also deploy both server-end and client-end defense mechanisms to strengthen the barrier against backdoor attacks.","On the other hand, our study shows that PFL fortified with these defense methods may offer a false sense of security.","We propose \\textit{PFedBA}, a stealthy and effective backdoor attack strategy applicable to PFL systems.","\\textit{PFedBA} ingeniously aligns the backdoor learning task with the main learning task of PFL by optimizing the trigger generation process.","Our comprehensive experiments demonstrate the effectiveness of \\textit{PFedBA} in seamlessly embedding triggers into personalized local models.","\\textit{PFedBA} yields outstanding attack performance across 10 state-of-the-art PFL algorithms, defeating the existing 6 defense mechanisms.","Our study sheds light on the subtle yet potent backdoor threats to PFL systems, urging the community to bolster defenses against emerging backdoor challenges."],"url":"http://arxiv.org/abs/2406.06207v1","category":"cs.LG"}
{"created":"2024-06-10 11:50:38","title":"Learning effective Hamiltonians for adaptive time-evolution quantum algorithms","abstract":"Digital quantum simulation of many-body dynamics relies on Trotterization to decompose the target time evolution into elementary quantum gates operating at a fixed equidistant time discretization. Recent advances have outlined protocols enabling more efficient adaptive Trotter protocols, which have been shown to exhibit a controlled error in the dynamics of local observables and correlation functions. However, it has remained open to which extent the errors on the actual generator of the dynamics, i.e., the target many-body Hamiltonian, remain controlled. Here, we propose to use quantum Hamiltonian learning to numerically obtain the effective Hamiltonian and apply it on the recently introduced ADA-Trotter algorithm as a concrete demonstration. Our key observation is that deviations from the target generator remain bounded on all simulation times. This result suggests that the ADA-Trotter not only generates reliable digital quantum simulation of local dynamics, but also controllably approximates the global quantum state of the target system. Our proposal is sufficiently general and readily applicable to other adaptive time-evolution algorithms.","sentences":["Digital quantum simulation of many-body dynamics relies on Trotterization to decompose the target time evolution into elementary quantum gates operating at a fixed equidistant time discretization.","Recent advances have outlined protocols enabling more efficient adaptive Trotter protocols, which have been shown to exhibit a controlled error in the dynamics of local observables and correlation functions.","However, it has remained open to which extent the errors on the actual generator of the dynamics, i.e., the target many-body Hamiltonian, remain controlled.","Here, we propose to use quantum Hamiltonian learning to numerically obtain the effective Hamiltonian and apply it on the recently introduced ADA-Trotter algorithm as a concrete demonstration.","Our key observation is that deviations from the target generator remain bounded on all simulation times.","This result suggests that the ADA-Trotter not only generates reliable digital quantum simulation of local dynamics, but also controllably approximates the global quantum state of the target system.","Our proposal is sufficiently general and readily applicable to other adaptive time-evolution algorithms."],"url":"http://arxiv.org/abs/2406.06198v1","category":"quant-ph"}
{"created":"2024-06-10 11:09:11","title":"Non-equilibrium cotunneling in interacting Josephson junctions","abstract":"We investigate non-equilibrium transport through interacting superconducting nanojunctions using a Liouville space approach. The formalism allows us to study finite gap effects, and to account for both quasiparticle and Cooper pair tunneling. With focus on the weak tunneling limit, we study the stationary dc and ac current up to second order (cotunneling) in the hybridization energy. We identify the characteristic virtual processes that yield the Andreev and Josephson current and obtain the dependence on the gate and bias voltage for the dc current, the critical current and the phase-dependent dissipative current. In particular, the critical current is characterized by regions in the stability diagram in which its sign changes from positive to negative, resulting in a multitude of 0-\\pi transitions. The latter signal the interplay between strong interactions and tunneling at finite bias.","sentences":["We investigate non-equilibrium transport through interacting superconducting nanojunctions using a Liouville space approach.","The formalism allows us to study finite gap effects, and to account for both quasiparticle and Cooper pair tunneling.","With focus on the weak tunneling limit, we study the stationary dc and ac current up to second order (cotunneling) in the hybridization energy.","We identify the characteristic virtual processes that yield the Andreev and Josephson current and obtain the dependence on the gate and bias voltage for the dc current, the critical current and the phase-dependent dissipative current.","In particular, the critical current is characterized by regions in the stability diagram in which its sign changes from positive to negative, resulting in a multitude of 0-\\pi transitions.","The latter signal the interplay between strong interactions and tunneling at finite bias."],"url":"http://arxiv.org/abs/2406.06170v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-12 17:59:32","title":"A suite of classical Cepheids tied to the binary cluster Berkeley 58 \\& NGC 7790","abstract":"The classical Cepheids CE Cas A, CE Cas B, CF Cas, and CG Cas are likely members of the binary open cluster comprising NGC 7790 and Berkeley 58. The clusters are of comparable age and in close proximity, as deduced from differentially dereddened $UuB_PBVGR_P$ photometry, and Cepheid period-age relations. Gaia DR3 astrometric and spectroscopic solutions for the clusters are likewise consistent. Conversely, the seemingly adjacent open cluster NGC 7788 is substantially younger and less distant.","sentences":["The classical Cepheids CE Cas A, CE Cas B, CF Cas, and CG Cas are likely members of the binary open cluster comprising NGC 7790 and Berkeley 58.","The clusters are of comparable age and in close proximity, as deduced from differentially dereddened $UuB_PBVGR_P$ photometry, and Cepheid period-age relations.","Gaia DR3 astrometric and spectroscopic solutions for the clusters are likewise consistent.","Conversely, the seemingly adjacent open cluster NGC 7788 is substantially younger and less distant."],"url":"http://arxiv.org/abs/2406.08483v1","category":"astro-ph.SR"}
{"created":"2024-06-12 17:51:53","title":"Self-supervised Learning of Neural Implicit Feature Fields for Camera Pose Refinement","abstract":"Visual localization techniques rely upon some underlying scene representation to localize against. These representations can be explicit such as 3D SFM map or implicit, such as a neural network that learns to encode the scene. The former requires sparse feature extractors and matchers to build the scene representation. The latter might lack geometric grounding not capturing the 3D structure of the scene well enough. This paper proposes to jointly learn the scene representation along with a 3D dense feature field and a 2D feature extractor whose outputs are embedded in the same metric space. Through a contrastive framework we align this volumetric field with the image-based extractor and regularize the latter with a ranking loss from learned surface information. We learn the underlying geometry of the scene with an implicit field through volumetric rendering and design our feature field to leverage intermediate geometric information encoded in the implicit field. The resulting features are discriminative and robust to viewpoint change while maintaining rich encoded information. Visual localization is then achieved by aligning the image-based features and the rendered volumetric features. We show the effectiveness of our approach on real-world scenes, demonstrating that our approach outperforms prior and concurrent work on leveraging implicit scene representations for localization.","sentences":["Visual localization techniques rely upon some underlying scene representation to localize against.","These representations can be explicit such as 3D SFM map or implicit, such as a neural network that learns to encode the scene.","The former requires sparse feature extractors and matchers to build the scene representation.","The latter might lack geometric grounding not capturing the 3D structure of the scene well enough.","This paper proposes to jointly learn the scene representation along with a 3D dense feature field and a 2D feature extractor whose outputs are embedded in the same metric space.","Through a contrastive framework we align this volumetric field with the image-based extractor and regularize the latter with a ranking loss from learned surface information.","We learn the underlying geometry of the scene with an implicit field through volumetric rendering and design our feature field to leverage intermediate geometric information encoded in the implicit field.","The resulting features are discriminative and robust to viewpoint change while maintaining rich encoded information.","Visual localization is then achieved by aligning the image-based features and the rendered volumetric features.","We show the effectiveness of our approach on real-world scenes, demonstrating that our approach outperforms prior and concurrent work on leveraging implicit scene representations for localization."],"url":"http://arxiv.org/abs/2406.08463v1","category":"cs.CV"}
{"created":"2024-06-12 17:50:47","title":"Consistent Theories for the DESI dark energy fit","abstract":"We search for physically consistent realizations of evolving dark energy suggested by the cosmological fit of DESI, Planck and Supernovae data. First we note that any lagrangian description of the standard Chevallier-Linder-Polarski (CLP) parametrization for the dark energy equation of state $w_{\\rm eff}$, allows for the addition of a cosmological constant. We perform the cosmological fit finding new regions of parameter space that however continue to favour dark energy with $w_{\\rm eff}<-1$ at early times that are challenging to realize in theoretically consistent theories. Next, in the spirit of effective field theories, we consider the effect of higher order terms in the Taylor expansion of the equation of state around the present epoch. We find that non-linear corrections of the equation of state are weakly constrained, thus opening the way to scenarios that differ from CLP at early times, possibly with $w_{\\rm eff}>-1$ at all times. We present indeed scenarios where evolving dark energy can be realized through quintessence models. We introduce in particular the ramp model where dark energy coincides with CLP at late times and approximates to a cosmological constant at early times. The latter model provides a much better fit than $\\Lambda$CDM, and only slightly worse than $w_0w_a$CDM, but with the big advantage of being described by a simple and theoretically consistent lagrangian of a canonical quintessence model.","sentences":["We search for physically consistent realizations of evolving dark energy suggested by the cosmological fit of DESI, Planck and Supernovae data.","First we note that any lagrangian description of the standard Chevallier-Linder-Polarski (CLP) parametrization for the dark energy equation of state $w_{\\rm eff}$, allows for the addition of a cosmological constant.","We perform the cosmological fit finding new regions of parameter space that however continue to favour dark energy with $w_{\\rm eff}<-1$ at early times that are challenging to realize in theoretically consistent theories.","Next, in the spirit of effective field theories, we consider the effect of higher order terms in the Taylor expansion of the equation of state around the present epoch.","We find that non-linear corrections of the equation of state are weakly constrained, thus opening the way to scenarios that differ from CLP at early times, possibly with $w_{\\rm eff}>-1$ at all times.","We present indeed scenarios where evolving dark energy can be realized through quintessence models.","We introduce in particular the ramp model where dark energy coincides with CLP at late times and approximates to a cosmological constant at early times.","The latter model provides a much better fit than $\\Lambda$CDM, and only slightly worse than $w_0w_a$CDM, but with the big advantage of being described by a simple and theoretically consistent lagrangian of a canonical quintessence model."],"url":"http://arxiv.org/abs/2406.08459v1","category":"astro-ph.CO"}
{"created":"2024-06-12 17:22:39","title":"Visualization of atomic structures on faceted and non-flat surfaces by difference-of-gaussians approach","abstract":"Detailed analysis of scanning probe microscopy (SPM) data acquired for tilted and non-flat surfaces is usually complicated due to presence of large number of surface areas tilted by large/variable angles relative to the scanning plane. As a consequence, standard methods of elimination of global/local slope by a plane subtraction or numerical differentiation seems to be ineffective. We demonstrate that simple difference-of-gaussians procedure provides us output data corresponding to a projection of considered surface onto scanning plane without undesirable modifications of contrast. This method allows us to minimize small-scale noise, finite slopes of SPM image along both fast and slow scanning directions and eliminate surface rippling. This method can be applied both for fast 'on-the-fly' visualization of experimental data and more detailed analysis, including high precision determination of lattice parameters and angles between translations vectors in different surface domains. In order to estimate geometrical distortions related to the difference-of-gaussians approach, we compare this method with direct image rotation in 3D space.","sentences":["Detailed analysis of scanning probe microscopy (SPM) data acquired for tilted and non-flat surfaces is usually complicated due to presence of large number of surface areas tilted by large/variable angles relative to the scanning plane.","As a consequence, standard methods of elimination of global/local slope by a plane subtraction or numerical differentiation seems to be ineffective.","We demonstrate that simple difference-of-gaussians procedure provides us output data corresponding to a projection of considered surface onto scanning plane without undesirable modifications of contrast.","This method allows us to minimize small-scale noise, finite slopes of SPM image along both fast and slow scanning directions and eliminate surface rippling.","This method can be applied both for fast 'on-the-fly' visualization of experimental data and more detailed analysis, including high precision determination of lattice parameters and angles between translations vectors in different surface domains.","In order to estimate geometrical distortions related to the difference-of-gaussians approach, we compare this method with direct image rotation in 3D space."],"url":"http://arxiv.org/abs/2406.08436v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-12 16:48:23","title":"Improved lower bound for the radius of analyticity for the modified KdV equation","abstract":"We investigate the initial value problem (IVP) associated to the modified Korteweg-de Vries equation (mKdV) in the defocusing scenario: \\begin{equation*} \\left\\{\\begin{array}{l} \\partial_t u+ \\partial_x^3u-u^2\\partial_x(u) = 0, \\quad x,t\\in\\mathbb{R}, \\\\ u(x,0) = u_0(x), \\end{array}\\right. \\end{equation*} where $u$ is a real valued function and the initial data $u_0$ is analytic on $\\mathbb{R}$ and has uniform radius of analyticity $\\sigma_0$ in the spatial variable.   It is well-known that the solution $u$ preserves its analyticity with the same radius $\\sigma_0$ for at least some time span $0<T_0\\le 1$. This local result was obtained in [Nonlinear Differ. Equ. Appl. (2024), 31--68] by proving a trilinear estimate in the Gevrey spaces $G^{\\sigma, s}$, $s\\geq \\frac14$. Global in time behaviour of the solution and algebraic lower bound of the evolution of the radius of analyticity was also studied in authors' earlier works in [Nonlinear Differ. Equ. Appl. (2024), 31--68] and [J. Evol. Equ. 24 No. 42 (2024)] by constructing almost conserved quantities in the classical Gevrey space with $H^1$ and $H^2$ levels of Sobolev regularities. The present study aims to construct a new almost conservation law in the Gevrey space defined with a weight function $\\cosh(\\sigma|\\xi|)$ and use it demonstrate that the local solution $u$ extends globally in time, and the radius of spatial analyticity is bounded from below by $c T^{-\\frac{1}{2}}$, for any time $T\\geq T_0$. The outcome of this paper represents an improvement on the one achieved by the authors' previous work in [J. Evol. Equ. 24 No. 42 (2024)].","sentences":["We investigate the initial value problem (IVP) associated to the modified Korteweg-de Vries equation (mKdV) in the defocusing scenario: \\begin{equation*} \\left\\{\\begin{array}{l} \\partial_t u+ \\partial_x^3u-u^2\\partial_x(u) = 0, \\quad x,t\\in\\mathbb{R}, \\\\ u(x,0) = u_0(x), \\end{array}\\right.","\\end{equation*} where $u$ is a real valued function and the initial data $u_0$ is analytic on $\\mathbb{R}$ and has uniform radius of analyticity $\\sigma_0$ in the spatial variable.   ","It is well-known that the solution $u$ preserves its analyticity with the same radius $\\sigma_0$ for at least some time span $0<T_0\\le 1$.","This local result was obtained in [Nonlinear Differ.","Equ.","Appl.","(2024), 31--68] by proving a trilinear estimate in the Gevrey spaces $G^{\\sigma, s}$, $s\\geq","\\frac14$. Global in time behaviour of the solution and algebraic lower bound of the evolution of the radius of analyticity was also studied in authors' earlier works in [Nonlinear Differ.","Equ.","Appl.","(2024), 31--68] and [J. Evol.","Equ. 24 No. 42 (2024)]","by constructing almost conserved quantities in the classical Gevrey space with $H^1$ and $H^2$ levels of Sobolev regularities.","The present study aims to construct a new almost conservation law in the Gevrey space defined with a weight function $\\cosh(\\sigma|\\xi|)$ and use it demonstrate that the local solution $u$ extends globally in time, and the radius of spatial analyticity is bounded from below by $c T^{-\\frac{1}{2}}$, for any time $T\\geq T_0$.","The outcome of this paper represents an improvement on the one achieved by the authors' previous work in [J. Evol.","Equ. 24 No. 42 (2024)]."],"url":"http://arxiv.org/abs/2406.08400v1","category":"math.AP"}
{"created":"2024-06-12 16:37:45","title":"Resetting by rescaling: exact results for a diffusing particle in one-dimension","abstract":"In this paper, we study a simple model of a diffusive particle on a line, undergoing a stochastic resetting with rate $r$, via rescaling its current position by a factor $a$, which can be either positive or negative. For $|a|<1$, the position distribution becomes stationary at long times and we compute this limiting distribution exactly for all $|a|<1$. This symmetric distribution has a Gaussian shape near its peak at $x=0$, but decays exponentially for large $|x|$. We also studied the mean first-passage time (MFPT) $T(0)$ to a target located at a distance $L$ from the initial position (the origin) of the particle. As a function of the initial position $x$, the MFPT $T(x)$ satisfies a nonlocal second order differential equation and we have solved it explicitly for $0 \\leq a < 1$. For $-1<a\\leq 0$, we also solved it analytically but up to a constant factor $\\kappa$ whose value can be determined independently from numerical simulations. Our results show that, for all $-1<a<1$, the MFPT $T(0)$ (starting from the origin) shows a minimum at $r=r^*(a)$. However, the optimised MFPT $T_{\\rm opt}(a)$ turns out to be a monotonically increasing function of $a$ for $-1<a<1$. This demonstrates that, compared to the standard resetting to the origin ($a=0$), while the positive rescaling is not beneficial for the search of a target, the negative rescaling is. Thus resetting via rescaling followed by a reflection around the origin expedites the search of a target in one dimension.","sentences":["In this paper, we study a simple model of a diffusive particle on a line, undergoing a stochastic resetting with rate $r$, via rescaling its current position by a factor $a$, which can be either positive or negative.","For $|a|<1$, the position distribution becomes stationary at long times and we compute this limiting distribution exactly for all $|a|<1$. This symmetric distribution has a Gaussian shape near its peak at $x=0$, but decays exponentially for large $|x|$. We also studied the mean first-passage time (MFPT) $T(0)$ to a target located at a distance $L$ from the initial position (the origin) of the particle.","As a function of the initial position $x$, the MFPT $T(x)$ satisfies a nonlocal second order differential equation and we have solved it explicitly for $0 \\leq a < 1$. For $-1<a\\leq 0$, we also solved it analytically but up to a constant factor $\\kappa$ whose value can be determined independently from numerical simulations.","Our results show that, for all $-1<a<1$, the MFPT $T(0)$ (starting from the origin) shows a minimum at $r=r^*(a)$. However, the optimised MFPT $T_{\\rm opt}(a)$ turns out to be a monotonically increasing function of $a$ for $-1<a<1$.","This demonstrates that, compared to the standard resetting to the origin ($a=0$), while the positive rescaling is not beneficial for the search of a target, the negative rescaling is.","Thus resetting via rescaling followed by a reflection around the origin expedites the search of a target in one dimension."],"url":"http://arxiv.org/abs/2406.08387v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-12 16:27:02","title":"The Birational Geometry of Ceva's Theorem","abstract":"In this article we study Ceva's theorem and its higher-dimensional extensions from the perspective of algebraic and projective geometry. First, we situate the theorem within the study of algebraic surfaces by relating it to the defining equation of a del Pezzo surface of degree six inside the product of three projective lines. Second, by interpreting (higher-dimensional analogues of) Ceva's theorem in terms of projections from projective spaces, we recast these results as matrix completion problems. We use these ideas to offer proofs of some higher-dimensional analogues of Ceva's theorem. This article is written with a nonspecialist audience in mind and we hope that some useful context is provided in the form of remarks in the sections on surfaces for students of algebraic geometry.","sentences":["In this article we study Ceva's theorem and its higher-dimensional extensions from the perspective of algebraic and projective geometry.","First, we situate the theorem within the study of algebraic surfaces by relating it to the defining equation of a del Pezzo surface of degree six inside the product of three projective lines.","Second, by interpreting (higher-dimensional analogues of) Ceva's theorem in terms of projections from projective spaces, we recast these results as matrix completion problems.","We use these ideas to offer proofs of some higher-dimensional analogues of Ceva's theorem.","This article is written with a nonspecialist audience in mind and we hope that some useful context is provided in the form of remarks in the sections on surfaces for students of algebraic geometry."],"url":"http://arxiv.org/abs/2406.08378v1","category":"math.AG"}
{"created":"2024-06-12 15:42:21","title":"CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction","abstract":"Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness.","sentences":["Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech.","It still suffers from low speaker similarity and poor prosody naturalness.","In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness.","Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs.","Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness."],"url":"http://arxiv.org/abs/2406.08336v1","category":"cs.SD"}
{"created":"2024-06-12 15:06:29","title":"2-dimensional Ricci limit spaces","abstract":"In this note, we will show that if a measured Gromov-Hausdorff limit space of a sequence of Riemannian manifolds with lower Ricci curvature bound has dense 2-regular set, then it is homeomorphic to a 2-dimensional manifold in an open full measure set. This result gives a positive answer to an open problem in [Naber, Open problem 3.4] in dimension 2 and for dimension larger than 2 there are counterexamples by [HNW, Zhou].","sentences":["In this note, we will show that if a measured Gromov-Hausdorff limit space of a sequence of Riemannian manifolds with lower Ricci curvature bound has dense 2-regular set, then it is homeomorphic to a 2-dimensional manifold in an open full measure set.","This result gives a positive answer to an open problem in [Naber, Open problem 3.4] in dimension 2 and for dimension larger than 2 there are counterexamples by [HNW, Zhou]."],"url":"http://arxiv.org/abs/2406.08306v1","category":"math.DG"}
{"created":"2024-06-12 14:49:44","title":"Constant scalar curvature K\u00e4hler metrics and semistable vector bundles","abstract":"We give a necessary and sufficient condition for the projectivisation of a slope semistable vector bundle to admit constant scalar curvature K\\\"ahler (cscK) metrics in adiabatic classes, when the base admits a constant scalar curvature metric. More precisely, we introduce a stability condition on vector bundles, which we call adiabatic slope stability, which is a weaker version of K-stability and involves only test configurations arising from subsheaves of the bundle. We prove that, for a simple vector bundle with locally free graded object, adiabatic slope stability is equivalent to the existence of cscK metrics on the projectivisation, which solves a problem that has been open since work of Ross--Thomas. In particular, this shows that the existence of cscK metrics is equivalent to K-stability in this setting. We provide a numerical criterion for the Donaldson-Futaki invariant associated to said test configurations in terms of Chern classes of the vector bundle. This criterion is computable in practice and we present an explicit example satisfying our assumptions which is coming from a vector bundle that does not admit a Hermite-Einstein metric.","sentences":["We give a necessary and sufficient condition for the projectivisation of a slope semistable vector bundle to admit constant scalar curvature K\\\"ahler (cscK) metrics in adiabatic classes, when the base admits a constant scalar curvature metric.","More precisely, we introduce a stability condition on vector bundles, which we call adiabatic slope stability, which is a weaker version of K-stability and involves only test configurations arising from subsheaves of the bundle.","We prove that, for a simple vector bundle with locally free graded object, adiabatic slope stability is equivalent to the existence of cscK metrics on the projectivisation, which solves a problem that has been open since work of Ross--Thomas.","In particular, this shows that the existence of cscK metrics is equivalent to K-stability in this setting.","We provide a numerical criterion for the Donaldson-Futaki invariant associated to said test configurations in terms of Chern classes of the vector bundle.","This criterion is computable in practice and we present an explicit example satisfying our assumptions which is coming from a vector bundle that does not admit a Hermite-Einstein metric."],"url":"http://arxiv.org/abs/2406.08284v1","category":"math.DG"}
{"created":"2024-06-12 14:34:41","title":"Refining Self-Supervised Learnt Speech Representation using Brain Activations","abstract":"It was shown in literature that speech representations extracted by self-supervised pre-trained models exhibit similarities with brain activations of human for speech perception and fine-tuning speech representation models on downstream tasks can further improve the similarity. However, it still remains unclear if this similarity can be used to optimize the pre-trained speech models. In this work, we therefore propose to use the brain activations recorded by fMRI to refine the often-used wav2vec2.0 model by aligning model representations toward human neural responses. Experimental results on SUPERB reveal that this operation is beneficial for several downstream tasks, e.g., speaker verification, automatic speech recognition, intent classification.One can then consider the proposed method as a new alternative to improve self-supervised speech models.","sentences":["It was shown in literature that speech representations extracted by self-supervised pre-trained models exhibit similarities with brain activations of human for speech perception and fine-tuning speech representation models on downstream tasks can further improve the similarity.","However, it still remains unclear if this similarity can be used to optimize the pre-trained speech models.","In this work, we therefore propose to use the brain activations recorded by fMRI to refine the often-used wav2vec2.0 model by aligning model representations toward human neural responses.","Experimental results on SUPERB reveal that this operation is beneficial for several downstream tasks, e.g., speaker verification, automatic speech recognition, intent classification.","One can then consider the proposed method as a new alternative to improve self-supervised speech models."],"url":"http://arxiv.org/abs/2406.08266v1","category":"eess.AS"}
{"created":"2024-06-12 13:21:29","title":"Deep learning of optical spectra of semiconductors and insulators","abstract":"Despite its potential, optical spectrum prediction for crystalline materials has been largely overlooked in machine learning for materials science. Here we present a proof of concept by creating a ab initio database of 9,915 dielectric tensors of semiconductors and insulators calculated in the independent particle approximation, and subsequently training graph attention neural networks to predict the dielectric function and refractive index. Our study shows that accurate prediction of optical spectra is possible using only the crystal structure and a database of about 10^4 materials.","sentences":["Despite its potential, optical spectrum prediction for crystalline materials has been largely overlooked in machine learning for materials science.","Here we present a proof of concept by creating a ab initio database of 9,915 dielectric tensors of semiconductors and insulators calculated in the independent particle approximation, and subsequently training graph attention neural networks to predict the dielectric function and refractive index.","Our study shows that accurate prediction of optical spectra is possible using only the crystal structure and a database of about 10^4 materials."],"url":"http://arxiv.org/abs/2406.08191v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-12 13:19:42","title":"Attention-Based Learning for Fluid State Interpolation and Editing in a Time-Continuous Framework","abstract":"In this work, we introduce FluidsFormer: a transformer-based approach for fluid interpolation within a continuous-time framework. By combining the capabilities of PITT and a residual neural network (RNN), we analytically predict the physical properties of the fluid state. This enables us to interpolate substep frames between simulated keyframes, enhancing the temporal smoothness and sharpness of animations. We demonstrate promising results for smoke interpolation and conduct initial experiments on liquids.","sentences":["In this work, we introduce FluidsFormer: a transformer-based approach for fluid interpolation within a continuous-time framework.","By combining the capabilities of PITT and a residual neural network (RNN), we analytically predict the physical properties of the fluid state.","This enables us to interpolate substep frames between simulated keyframes, enhancing the temporal smoothness and sharpness of animations.","We demonstrate promising results for smoke interpolation and conduct initial experiments on liquids."],"url":"http://arxiv.org/abs/2406.08188v1","category":"cs.LG"}
{"created":"2024-06-12 13:16:01","title":"Non-stationary Gaussian random fields on hypersurfaces: Sampling and strong error analysis","abstract":"A flexible model for non-stationary Gaussian random fields on hypersurfaces is introduced. The class of random fields on curves and surfaces is characterized by a power spectral density of a second order elliptic differential operator. Sampling is done by a Galerkin--Chebyshev approximation based on the surface finite element method and Chebyshev polynomials. Strong error bounds are shown with convergence rates depending on the smoothness of the approximated random field.","sentences":["A flexible model for non-stationary Gaussian random fields on hypersurfaces is introduced.","The class of random fields on curves and surfaces is characterized by a power spectral density of a second order elliptic differential operator.","Sampling is done by a Galerkin--Chebyshev approximation based on the surface finite element method and Chebyshev polynomials.","Strong error bounds are shown with convergence rates depending on the smoothness of the approximated random field."],"url":"http://arxiv.org/abs/2406.08185v1","category":"math.NA"}
{"created":"2024-06-12 12:37:14","title":"A New Linear Programming Approach and a New Backtracking Strategy for Multiple-Gradient Descent in Multi-Objective Optimization","abstract":"In this work, the author presents a novel method for finding descent directions shared by two or more differentiable functions defined on the same unconstrained domain space. Then, the author illustrates an alternative Multiple-Gradient Descent procedure for Multi-Objective Optimization problems that is based on this new method. In particular, the proposed method consists in finding the shared descent direction solving a relatively cheap Linear Programming (LP) problem, where the LP's objective function and the constraints are defined by the gradients of the objective functions of the Multi-Objective Optimization problem. More precisely, the formulation of the LP problem is such that, if a shared descent direction does not exist for the objective functions, but a non-ascent direction for all the objectives does, the LP problem returns the latter. Moreover, the author defines a new backtracking strategy for Multiple-Gradient Descent methods such that, if the proposed LP is used for computing the direction, the ability to reach and/or explore the Pareto set and the Pareto front is improved. A theoretical analysis of the properties of the new methods is performed, and tests on classic Multi-Objective Optimization problems are proposed to assess their goodness.","sentences":["In this work, the author presents a novel method for finding descent directions shared by two or more differentiable functions defined on the same unconstrained domain space.","Then, the author illustrates an alternative Multiple-Gradient Descent procedure for Multi-Objective Optimization problems that is based on this new method.","In particular, the proposed method consists in finding the shared descent direction solving a relatively cheap Linear Programming (LP) problem, where the LP's objective function and the constraints are defined by the gradients of the objective functions of the Multi-Objective Optimization problem.","More precisely, the formulation of the LP problem is such that, if a shared descent direction does not exist for the objective functions, but a non-ascent direction for all the objectives does, the LP problem returns the latter.","Moreover, the author defines a new backtracking strategy for Multiple-Gradient Descent methods such that, if the proposed LP is used for computing the direction, the ability to reach and/or explore the Pareto set and the Pareto front is improved.","A theoretical analysis of the properties of the new methods is performed, and tests on classic Multi-Objective Optimization problems are proposed to assess their goodness."],"url":"http://arxiv.org/abs/2406.08147v1","category":"math.OC"}
{"created":"2024-06-12 11:31:43","title":"Null controllability for stochastic parabolic equations with Robin boundary conditions","abstract":"We establish the null controllability of forward and backward linear stochastic parabolic equations with linear Robin (or Fourier) boundary conditions. These equations incorporate zero and first order terms with bounded coefficients. To prove our null controllability results, a key tool will be the derivation of two new global Carleman estimates for the weak solutions of the corresponding adjoint equations in negative Sobolev space. These Carleman estimates are established using a duality method.","sentences":["We establish the null controllability of forward and backward linear stochastic parabolic equations with linear Robin (or Fourier) boundary conditions.","These equations incorporate zero and first order terms with bounded coefficients.","To prove our null controllability results, a key tool will be the derivation of two new global Carleman estimates for the weak solutions of the corresponding adjoint equations in negative Sobolev space.","These Carleman estimates are established using a duality method."],"url":"http://arxiv.org/abs/2406.08103v1","category":"math.AP"}
{"created":"2024-06-12 11:22:27","title":"Inductive Global and Local Manifold Approximation and Projection","abstract":"Nonlinear dimensional reduction with the manifold assumption, often called manifold learning, has proven its usefulness in a wide range of high-dimensional data analysis. The significant impact of t-SNE and UMAP has catalyzed intense research interest, seeking further innovations toward visualizing not only the local but also the global structure information of the data. Moreover, there have been consistent efforts toward generalizable dimensional reduction that handles unseen data. In this paper, we first propose GLoMAP, a novel manifold learning method for dimensional reduction and high-dimensional data visualization. GLoMAP preserves locally and globally meaningful distance estimates and displays a progression from global to local formation during the course of optimization. Furthermore, we extend GLoMAP to its inductive version, iGLoMAP, which utilizes a deep neural network to map data to its lower-dimensional representation. This allows iGLoMAP to provide lower-dimensional embeddings for unseen points without needing to re-train the algorithm. iGLoMAP is also well-suited for mini-batch learning, enabling large-scale, accelerated gradient calculations. We have successfully applied both GLoMAP and iGLoMAP to the simulated and real-data settings, with competitive experiments against the state-of-the-art methods.","sentences":["Nonlinear dimensional reduction with the manifold assumption, often called manifold learning, has proven its usefulness in a wide range of high-dimensional data analysis.","The significant impact of t-SNE and UMAP has catalyzed intense research interest, seeking further innovations toward visualizing not only the local but also the global structure information of the data.","Moreover, there have been consistent efforts toward generalizable dimensional reduction that handles unseen data.","In this paper, we first propose GLoMAP, a novel manifold learning method for dimensional reduction and high-dimensional data visualization.","GLoMAP preserves locally and globally meaningful distance estimates and displays a progression from global to local formation during the course of optimization.","Furthermore, we extend GLoMAP to its inductive version, iGLoMAP, which utilizes a deep neural network to map data to its lower-dimensional representation.","This allows iGLoMAP to provide lower-dimensional embeddings for unseen points without needing to re-train the algorithm.","iGLoMAP is also well-suited for mini-batch learning, enabling large-scale, accelerated gradient calculations.","We have successfully applied both GLoMAP and iGLoMAP to the simulated and real-data settings, with competitive experiments against the state-of-the-art methods."],"url":"http://arxiv.org/abs/2406.08097v1","category":"cs.LG"}
{"created":"2024-06-12 11:16:30","title":"Languages Transferred Within the Encoder: On Representation Transfer in Zero-Shot Multilingual Translation","abstract":"Understanding representation transfer in multilingual neural machine translation can reveal the representational issue causing the zero-shot translation deficiency. In this work, we introduce the identity pair, a sentence translated into itself, to address the lack of the base measure in multilingual investigations, as the identity pair represents the optimal state of representation among any language transfers. In our analysis, we demonstrate that the encoder transfers the source language to the representational subspace of the target language instead of the language-agnostic state. Thus, the zero-shot translation deficiency arises because representations are entangled with other languages and are not transferred effectively to the target language. Based on our findings, we propose two methods: 1) low-rank language-specific embedding at the encoder, and 2) language-specific contrastive learning of the representation at the decoder. The experimental results on Europarl-15, TED-19, and OPUS-100 datasets show that our methods substantially enhance the performance of zero-shot translations by improving language transfer capacity, thereby providing practical evidence to support our conclusions.","sentences":["Understanding representation transfer in multilingual neural machine translation can reveal the representational issue causing the zero-shot translation deficiency.","In this work, we introduce the identity pair, a sentence translated into itself, to address the lack of the base measure in multilingual investigations, as the identity pair represents the optimal state of representation among any language transfers.","In our analysis, we demonstrate that the encoder transfers the source language to the representational subspace of the target language instead of the language-agnostic state.","Thus, the zero-shot translation deficiency arises because representations are entangled with other languages and are not transferred effectively to the target language.","Based on our findings, we propose two methods: 1) low-rank language-specific embedding at the encoder, and 2) language-specific contrastive learning of the representation at the decoder.","The experimental results on Europarl-15, TED-19, and OPUS-100 datasets show that our methods substantially enhance the performance of zero-shot translations by improving language transfer capacity, thereby providing practical evidence to support our conclusions."],"url":"http://arxiv.org/abs/2406.08092v1","category":"cs.CL"}
{"created":"2024-06-12 11:11:38","title":"Vector valued piecewise continuous almost automorphic functions and some consequences","abstract":"In the present work, for $\\mathbb{X}$ a Banach space, the notion of piecewise continuous $\\mathbb{Z}$-almost automorphic functions with values in finite dimensional spaces is extended to piecewise continuous $\\mathbb{Z}$-almost automorphic functions with values in $\\mathbb{X}$. Several properties of this class of functions are provided, in particular it is shown that if $\\mathbb{X}$ is a Banach algebra, then this class of functions constitute also a Banach algebra; furthermore, using the theory of $\\mathbb{Z}$-almost automorphic functions, a new characterization of compact almost automorphic functions is given. As consequences, with the help of $\\mathbb{Z}$-almost automorphic functions, it is presented a simple proof of the characterization of almost automorphic sequences by compact almost automorphic functions; the method permits us to give explicit examples of compact almost automorphic functions which are not almost periodic. Also, using the theory developed here, it is shown that almost automorphic solutions of differential equations with piecewise constant argument are in fact compact almost automorphic. Finally, it is proved that the classical solution of the $1D$ heat equation with continuous $\\mathbb{Z}$-almost automorphic source is also continuous $\\mathbb{Z}$-almost automorphic; furthermore, we comment applications to the existence and uniqueness of the asymptotically continuous $\\mathbb{Z}$-almost automorphic mild solution to abstract integro-differential equations with nonlocal initial conditions.","sentences":["In the present work, for $\\mathbb{X}$ a Banach space, the notion of piecewise continuous $\\mathbb{Z}$-almost automorphic functions with values in finite dimensional spaces is extended to piecewise continuous $\\mathbb{Z}$-almost automorphic functions with values in $\\mathbb{X}$. Several properties of this class of functions are provided, in particular it is shown that if $\\mathbb{X}$ is a Banach algebra, then this class of functions constitute also a Banach algebra; furthermore, using the theory of $\\mathbb{Z}$-almost automorphic functions, a new characterization of compact almost automorphic functions is given.","As consequences, with the help of $\\mathbb{Z}$-almost automorphic functions, it is presented a simple proof of the characterization of almost automorphic sequences by compact almost automorphic functions; the method permits us to give explicit examples of compact almost automorphic functions which are not almost periodic.","Also, using the theory developed here, it is shown that almost automorphic solutions of differential equations with piecewise constant argument are in fact compact almost automorphic.","Finally, it is proved that the classical solution of the $1D$ heat equation with continuous $\\mathbb{Z}$-almost automorphic source is also continuous $\\mathbb{Z}$-almost automorphic; furthermore, we comment applications to the existence and uniqueness of the asymptotically continuous $\\mathbb{Z}$-almost automorphic mild solution to abstract integro-differential equations with nonlocal initial conditions."],"url":"http://arxiv.org/abs/2406.08088v1","category":"math.FA"}
{"created":"2024-06-12 09:34:42","title":"A geometric application of Lagrange multipliers: extremal compatible linear connections","abstract":"The L\\'evi-Civita connection of a Riemannian manifold is a metric (compatible) linear connection, uniquely determined by its vanishing torsion. It is extremal in the sense that it has minimal torsion at each point. We can extend this idea to more general spaces with more general (not necessarily quadratic) indicatrix hypersurfaces in the tangent spaces. Here, the existence of compatible linear connections on the base manifold is not guaranteed anymore, which needs to be addressed along with the intrinsic characterization of the extremal one. The first step is to provide the Riemann metrizability of the compatible linear connections. This Riemannian environment establishes a one-to-one correspondence between linear connections and their torsion tensors, also giving a way of measuring the length of the latter. The second step is to solve a hybrid conditional extremum problem at each point of the base manifold, all of whose constraint equations (compatibility equations) involve functions defined on the indicatrix hypersurface. The objective function to be minimized is a quadratic squared norm function defined on the finite dimensional fiber (vector space) of the torsion tensor bundle. We express the solution by using the method of Lagrange multipliers on function spaces point by point, we present a necessary and sufficient condition of the solvability and the solution is also given in terms of intrinsic quantities affecting the uniform size of the linear isometry groups of the indicatrices. This completes the description of differential geometric spaces admitting compatible linear connections on the base manifold, called generalized Berwald spaces, in Finsler geometry.","sentences":["The L\\'evi-Civita connection of a Riemannian manifold is a metric (compatible) linear connection, uniquely determined by its vanishing torsion.","It is extremal in the sense that it has minimal torsion at each point.","We can extend this idea to more general spaces with more general (not necessarily quadratic) indicatrix hypersurfaces in the tangent spaces.","Here, the existence of compatible linear connections on the base manifold is not guaranteed anymore, which needs to be addressed along with the intrinsic characterization of the extremal one.","The first step is to provide the Riemann metrizability of the compatible linear connections.","This Riemannian environment establishes a one-to-one correspondence between linear connections and their torsion tensors, also giving a way of measuring the length of the latter.","The second step is to solve a hybrid conditional extremum problem at each point of the base manifold, all of whose constraint equations (compatibility equations) involve functions defined on the indicatrix hypersurface.","The objective function to be minimized is a quadratic squared norm function defined on the finite dimensional fiber (vector space) of the torsion tensor bundle.","We express the solution by using the method of Lagrange multipliers on function spaces point by point, we present a necessary and sufficient condition of the solvability and the solution is also given in terms of intrinsic quantities affecting the uniform size of the linear isometry groups of the indicatrices.","This completes the description of differential geometric spaces admitting compatible linear connections on the base manifold, called generalized Berwald spaces, in Finsler geometry."],"url":"http://arxiv.org/abs/2406.08033v1","category":"math.DG"}
{"created":"2024-06-12 08:45:42","title":"Stochastic Maximum Principle for optimal advertising models with delay and non-convex control space","abstract":"In this paper we study optimal advertising problems that models the introduction of a new product into the market in the presence of carryover effects of the advertisement and with memory effects in the level of goodwill. In particular, we let the dynamics of the product goodwill to depend on the past, and also on past advertising efforts. We treat the problem by means of the stochastic Pontryagin maximum principle, that here is considered for a class of problems where in the state equation either the state or the control depend on the past. Moreover the control acts on the martingale term and the space of controls can be chosen to be non-convex. The maximum principle is thus formulated using a first-order adjoint Backward Stochastic Differential Equations (BSDEs), which can be explicitly computed due to the specific characteristics of the model, and a second-order adjoint relation.","sentences":["In this paper we study optimal advertising problems that models the introduction of a new product into the market in the presence of carryover effects of the advertisement and with memory effects in the level of goodwill.","In particular, we let the dynamics of the product goodwill to depend on the past, and also on past advertising efforts.","We treat the problem by means of the stochastic Pontryagin maximum principle, that here is considered for a class of problems where in the state equation either the state or the control depend on the past.","Moreover the control acts on the martingale term and the space of controls can be chosen to be non-convex.","The maximum principle is thus formulated using a first-order adjoint Backward Stochastic Differential Equations (BSDEs), which can be explicitly computed due to the specific characteristics of the model, and a second-order adjoint relation."],"url":"http://arxiv.org/abs/2406.07999v1","category":"math.OC"}
{"created":"2024-06-12 08:45:04","title":"Stabilizability of parabolic equations by switching controls based on point actuators","abstract":"It is shown that a switching control involving a finite number of Dirac delta actuators is able to steer the state of a general class of nonautonomous parabolic equations to zero as time increases to infinity. The strategy is based on a recent feedback stabilizability result, which utilizes control forces given by linear combinations of appropriately located Dirac delta distribution actuators. Then, the existence of a stabilizing switching control with no more than one actuator active at each time instant is established. For the implementation in practice, the stabilization problem is formulated as an infinite horizon optimal control problem, with cardinality-type control constraints enforcing the switching property. Subsequently, this problem is tackled using a receding horizon framework. Its suboptimality and stabilizabilizing properties are analyzed. Numerical simulations validate the approach, illustrating its stabilizing and switching properties.","sentences":["It is shown that a switching control involving a finite number of Dirac delta actuators is able to steer the state of a general class of nonautonomous parabolic equations to zero as time increases to infinity.","The strategy is based on a recent feedback stabilizability result, which utilizes control forces given by linear combinations of appropriately located Dirac delta distribution actuators.","Then, the existence of a stabilizing switching control with no more than one actuator active at each time instant is established.","For the implementation in practice, the stabilization problem is formulated as an infinite horizon optimal control problem, with cardinality-type control constraints enforcing the switching property.","Subsequently, this problem is tackled using a receding horizon framework.","Its suboptimality and stabilizabilizing properties are analyzed.","Numerical simulations validate the approach, illustrating its stabilizing and switching properties."],"url":"http://arxiv.org/abs/2406.07997v1","category":"math.OC"}
{"created":"2024-06-12 07:24:19","title":"DPSW-Sketch: A Differentially Private Sketch Framework for Frequency Estimation over Sliding Windows (Technical Report)","abstract":"The sliding window model of computation captures scenarios in which data are continually arriving in the form of a stream, and only the most recent $w$ items are used for analysis. In this setting, an algorithm needs to accurately track some desired statistics over the sliding window using a small space. When data streams contain sensitive information about individuals, the algorithm is also urgently needed to provide a provable guarantee of privacy. In this paper, we focus on the two fundamental problems of privately (1) estimating the frequency of an arbitrary item and (2) identifying the most frequent items (i.e., \\emph{heavy hitters}), in the sliding window model. We propose \\textsc{DPSW-Sketch}, a sliding window framework based on the count-min sketch that not only satisfies differential privacy over the stream but also approximates the results for frequency and heavy-hitter queries within bounded errors in sublinear time and space w.r.t.~$w$. Extensive experiments on five real-world and synthetic datasets show that \\textsc{DPSW-Sketch} provides significantly better utility-privacy trade-offs than state-of-the-art methods.","sentences":["The sliding window model of computation captures scenarios in which data are continually arriving in the form of a stream, and only the most recent $w$ items are used for analysis.","In this setting, an algorithm needs to accurately track some desired statistics over the sliding window using a small space.","When data streams contain sensitive information about individuals, the algorithm is also urgently needed to provide a provable guarantee of privacy.","In this paper, we focus on the two fundamental problems of privately (1) estimating the frequency of an arbitrary item and (2) identifying the most frequent items (i.e., \\emph{heavy hitters}), in the sliding window model.","We propose \\textsc{DPSW-Sketch}, a sliding window framework based on the count-min sketch that not only satisfies differential privacy over the stream but also approximates the results for frequency and heavy-hitter queries within bounded errors in sublinear time and space w.r.t.~$w$. Extensive experiments on five real-world and synthetic datasets show that \\textsc{DPSW-Sketch} provides significantly better utility-privacy trade-offs than state-of-the-art methods."],"url":"http://arxiv.org/abs/2406.07953v1","category":"cs.CR"}
{"created":"2024-06-12 07:00:49","title":"On Annotation-free Optimization of Video Coding for Machines","abstract":"Today, image and video data is not only viewed by humans, but also automatically analyzed by computer vision algorithms. However, current coding standards are optimized for human perception. Emerging from this, research on video coding for machines tries to develop coding methods designed for machines as information sink. Since many of these algorithms are based on neural networks, most proposals for video coding for machines build upon neural compression. So far, optimizing the compression by applying the task loss of the analysis network, for which ground truth data is needed, is achieving the best coding performance. But ground truth data is difficult to obtain and thus an optimization without ground truth is preferred. In this paper, we present an annotation-free optimization strategy for video coding for machines. We measure the distortion by calculating the task loss of the analysis network. Therefore, the predictions on the compressed image are compared with the predictions on the original image, instead of the ground truth data. Our results show that this strategy can even outperform training with ground truth data with rate savings of up to 7.5 %. By using the non-annotated training data, the rate gains can be further increased up to 8.2 %.","sentences":["Today, image and video data is not only viewed by humans, but also automatically analyzed by computer vision algorithms.","However, current coding standards are optimized for human perception.","Emerging from this, research on video coding for machines tries to develop coding methods designed for machines as information sink.","Since many of these algorithms are based on neural networks, most proposals for video coding for machines build upon neural compression.","So far, optimizing the compression by applying the task loss of the analysis network, for which ground truth data is needed, is achieving the best coding performance.","But ground truth data is difficult to obtain and thus an optimization without ground truth is preferred.","In this paper, we present an annotation-free optimization strategy for video coding for machines.","We measure the distortion by calculating the task loss of the analysis network.","Therefore, the predictions on the compressed image are compared with the predictions on the original image, instead of the ground truth data.","Our results show that this strategy can even outperform training with ground truth data with rate savings of up to 7.5 %.","By using the non-annotated training data, the rate gains can be further increased up to 8.2 %."],"url":"http://arxiv.org/abs/2406.07938v1","category":"eess.IV"}
{"created":"2024-06-12 06:45:45","title":"ExoSpikeNet: A Light Curve Analysis Based Spiking Neural Network for Exoplanet Detection","abstract":"Exoplanets are celestial bodies orbiting stars beyond our Solar System. Although historically they posed detection challenges, Kepler's data has revolutionized our understanding. By analyzing flux values from the Kepler Mission, we investigate the intricate patterns in starlight that may indicate the presence of exoplanets. This study investigates a novel approach for exoplanet classification using Spiking Neural Networks (SNNs) applied to data obtained from the NASA Kepler mission. SNNs offer a unique advantage by mimicking the spiking behavior of neurons in the brain, allowing for more nuanced and biologically inspired processing of temporal data. Experimental results demonstrate the efficacy of the proposed SNN architecture, excelling in various performance metrics such as accuracy, F1 score, precision, and recall.","sentences":["Exoplanets are celestial bodies orbiting stars beyond our Solar System.","Although historically they posed detection challenges, Kepler's data has revolutionized our understanding.","By analyzing flux values from the Kepler Mission, we investigate the intricate patterns in starlight that may indicate the presence of exoplanets.","This study investigates a novel approach for exoplanet classification using Spiking Neural Networks (SNNs) applied to data obtained from the NASA Kepler mission.","SNNs offer a unique advantage by mimicking the spiking behavior of neurons in the brain, allowing for more nuanced and biologically inspired processing of temporal data.","Experimental results demonstrate the efficacy of the proposed SNN architecture, excelling in various performance metrics such as accuracy, F1 score, precision, and recall."],"url":"http://arxiv.org/abs/2406.07927v1","category":"astro-ph.IM"}
{"created":"2024-06-12 06:44:57","title":"The Galerkin method for a regularised combined field integral equation without a dual basis function","abstract":"We propose discretisation of a regularised combined field integral equation (regularised CFIE) only with the Rao-Wilton-Glisson (RWG) basis function. The CFIE is a formulation of integral equations, which avoids the so-called ficticious frequencies of integral equations. The most typical CFIE, which is a linear combination of the electric field integral equation (EFIE) and magnetic field integral equation (MFIE), is known to be ill-conditioned and requires many iterations when solved with iteration methods such as the generalised minimum residual (GMRES) method. The regularised CFIE is another formulation of the CFIE to solve this problem by applying a regularising operator to the part of the EFIE. In several previous studies the regularising operator is determined based on the Calderon preconditioning. This regularising operator however takes much more computatonal time than the standard CFIE since discretising the EFIE with the Calderon preconditioner requires the dual basis function. In this article we propose a formulation of the regularised CFIE, which can be discretised with the Galerkin method without the dual basis function.","sentences":["We propose discretisation of a regularised combined field integral equation (regularised CFIE) only with the Rao-Wilton-Glisson (RWG) basis function.","The CFIE is a formulation of integral equations, which avoids the so-called ficticious frequencies of integral equations.","The most typical CFIE, which is a linear combination of the electric field integral equation (EFIE) and magnetic field integral equation (MFIE), is known to be ill-conditioned and requires many iterations when solved with iteration methods such as the generalised minimum residual (GMRES) method.","The regularised CFIE is another formulation of the CFIE to solve this problem by applying a regularising operator to the part of the EFIE.","In several previous studies the regularising operator is determined based on the Calderon preconditioning.","This regularising operator however takes much more computatonal time than the standard CFIE since discretising the EFIE with the Calderon preconditioner requires the dual basis function.","In this article we propose a formulation of the regularised CFIE, which can be discretised with the Galerkin method without the dual basis function."],"url":"http://arxiv.org/abs/2406.07924v1","category":"math.NA"}
{"created":"2024-06-12 06:31:42","title":"A quark core-gluon model for heavy hybrid baryons","abstract":"Besides the ordinary hadrons, QCD allows the existence of states in which excitations of the gluonic field can play the role of valence particles, either alone in a glueball, or coupled to quarks in a hybrid. So, hybrid baryons, made of three quarks and a gluon, can a priori exist. Till now, there is no experimental evidence for such exotic hadrons but experimental efforts are being made to search for them at CEBAF Large Acceptance Spectrometer. In this work, a hybrid baryon is considered as a two-body system composed of a color octet three-quark core and a gluon, interacting via a QCD-inspired interaction. A semirelativistic potential model is built in which the dominant interaction is a potential simulating the flux tube confinement, and the Casimir scaling is assumed to link interactions between triplet and octet color sources. This picture is similar to the quark-diquark description for baryons. It is chosen in order to take properly into account the helicity of the gluon. Only $cccg$ and $bbbg$ states are considered because the strong mass asymmetry between the quark core and the gluon is expected to favor the formation of the core. As the results for heavy hybrid baryons seem relevant, we consider this paper as a proof of concept which can be extended for the study of light hybrid baryons.","sentences":["Besides the ordinary hadrons, QCD allows the existence of states in which excitations of the gluonic field can play the role of valence particles, either alone in a glueball, or coupled to quarks in a hybrid.","So, hybrid baryons, made of three quarks and a gluon, can a priori exist.","Till now, there is no experimental evidence for such exotic hadrons but experimental efforts are being made to search for them at CEBAF Large Acceptance Spectrometer.","In this work, a hybrid baryon is considered as a two-body system composed of a color octet three-quark core and a gluon, interacting via a QCD-inspired interaction.","A semirelativistic potential model is built in which the dominant interaction is a potential simulating the flux tube confinement, and the Casimir scaling is assumed to link interactions between triplet and octet color sources.","This picture is similar to the quark-diquark description for baryons.","It is chosen in order to take properly into account the helicity of the gluon.","Only $cccg$ and $bbbg$ states are considered because the strong mass asymmetry between the quark core and the gluon is expected to favor the formation of the core.","As the results for heavy hybrid baryons seem relevant, we consider this paper as a proof of concept which can be extended for the study of light hybrid baryons."],"url":"http://arxiv.org/abs/2406.07912v1","category":"hep-ph"}
{"created":"2024-06-12 06:17:41","title":"Hybrid Rendering for Dynamic Scenes","abstract":"Despite significant advances in algorithms and hardware, global illumination continues to be a challenge in the real-time domain. Time constraints often force developers to either compromise on the quality of global illumination or disregard it altogether. We take advantage of a common setup in modern games: having a set of a level, which is a static scene with dynamic characters and lighting. We introduce a novel method for efficiently and accurately rendering global illumination in dynamic scenes. Our hybrid technique leverages precomputation and neural networks to capture the light transport of a static scene. Then, we introduce a method to compute the difference between the current scene and the static scene, which we already precomputed. By handling the bulk of the light transport through precomputation, our method only requires the rendering of a minimal difference, reducing the noise and increasing the quality.","sentences":["Despite significant advances in algorithms and hardware, global illumination continues to be a challenge in the real-time domain.","Time constraints often force developers to either compromise on the quality of global illumination or disregard it altogether.","We take advantage of a common setup in modern games: having a set of a level, which is a static scene with dynamic characters and lighting.","We introduce a novel method for efficiently and accurately rendering global illumination in dynamic scenes.","Our hybrid technique leverages precomputation and neural networks to capture the light transport of a static scene.","Then, we introduce a method to compute the difference between the current scene and the static scene, which we already precomputed.","By handling the bulk of the light transport through precomputation, our method only requires the rendering of a minimal difference, reducing the noise and increasing the quality."],"url":"http://arxiv.org/abs/2406.07906v1","category":"cs.GR"}
{"created":"2024-06-12 05:55:45","title":"Parameter Estimation in Quantum Metrology Technique for Time Series Prediction","abstract":"The paper investigates the techniques of quantum computation in metrological predictions, with a particular emphasis on enhancing prediction potential through variational parameter estimation. The applicability of quantum simulations and quantum metrology techniques for modelling complex physical systems and achieving high-resolution measurements are proposed. The impacts of various parameter distributions and learning rates on predictive accuracy are investigated. Modelling the time evolution of physical systems Hamiltonian simulation and the product formula procedure are adopted. The time block method is analyzed in order to reduce simulation errors, while the Schatten-infinite norm is used to evaluate the simulation precision. Methodology requires estimation of optimized parameters by minimizing loss functions and resource needs. For this purpose, the mathematical formulations of Cramer Rao Bound and Fischer Information are indispensable requirements. The impact of learning rates on regulating the loss function for various parameter values. Using parameterized quantum circuits, the article outlines a four-step procedure for extracting information. This method involves the preparation of input states, the evolution of parameterized quantum states, the measurement of outputs, and the estimation of parameters based on multiple measurements. The study analyses variational unitary circuits with optimized parameter estimation for more precise predictions. The findings shed light on the effects of normal parameter distributions and learning rates on attaining the most optimal state and comparison with classical Long Short Term Memory (LSTM) predictions, providing valuable insights for the development of more appropriate approaches in quantum computing.","sentences":["The paper investigates the techniques of quantum computation in metrological predictions, with a particular emphasis on enhancing prediction potential through variational parameter estimation.","The applicability of quantum simulations and quantum metrology techniques for modelling complex physical systems and achieving high-resolution measurements are proposed.","The impacts of various parameter distributions and learning rates on predictive accuracy are investigated.","Modelling the time evolution of physical systems Hamiltonian simulation and the product formula procedure are adopted.","The time block method is analyzed in order to reduce simulation errors, while the Schatten-infinite norm is used to evaluate the simulation precision.","Methodology requires estimation of optimized parameters by minimizing loss functions and resource needs.","For this purpose, the mathematical formulations of Cramer Rao Bound and Fischer Information are indispensable requirements.","The impact of learning rates on regulating the loss function for various parameter values.","Using parameterized quantum circuits, the article outlines a four-step procedure for extracting information.","This method involves the preparation of input states, the evolution of parameterized quantum states, the measurement of outputs, and the estimation of parameters based on multiple measurements.","The study analyses variational unitary circuits with optimized parameter estimation for more precise predictions.","The findings shed light on the effects of normal parameter distributions and learning rates on attaining the most optimal state and comparison with classical Long Short Term Memory (LSTM) predictions, providing valuable insights for the development of more appropriate approaches in quantum computing."],"url":"http://arxiv.org/abs/2406.07893v1","category":"quant-ph"}
{"created":"2024-06-12 05:36:45","title":"Nonparametric estimation of linear multiplier for processes driven by a bifractional Brownian motion","abstract":"We study the problem of nonparametric estimation of the linear multiplier function $\\theta(t)$ for processes satisfying stochastic differential equations of the type $$dX_t=\\theta(t)X_tdt+\\epsilon dW_t^{H,K}, X_0=x_0,0\\leq t \\leq T$$ where $\\{W_t^{H,K}, t \\geq 0\\}$ is a bifractional Brownian motion with known parameters $H\\in (0,1), K\\in (0,1]$ and $HK\\in (\\frac{1}{2},1).$ We investigate the asymptotic behaviour of the estimator of the unknown function $\\theta(t)$ as $\\epsilon \\rightarrow 0.$","sentences":["We study the problem of nonparametric estimation of the linear multiplier function $\\theta(t)$ for processes satisfying stochastic differential equations of the type $$dX_t=\\theta(t)X_tdt+\\epsilon dW_t^{H,K}, X_0=x_0,0\\leq t \\leq T$$ where $\\{W_t^{H,K}, t \\geq 0\\}$ is a bifractional Brownian motion with known parameters $H\\in (0,1), K\\in (0,1]$ and $HK\\in (\\frac{1}{2},1).$ We investigate the asymptotic behaviour of the estimator of the unknown function $\\theta(t)$ as $\\epsilon \\rightarrow 0.$"],"url":"http://arxiv.org/abs/2406.07889v1","category":"math.ST"}
{"created":"2024-06-12 05:24:53","title":"GENIU: A Restricted Data Access Unlearning for Imbalanced Data","abstract":"With the increasing emphasis on data privacy, the significance of machine unlearning has grown substantially. Class unlearning, which involves enabling a trained model to forget data belonging to a specific class learned before, is important as classification tasks account for the majority of today's machine learning as a service (MLaaS). Retraining the model on the original data, excluding the data to be forgotten (a.k.a forgetting data), is a common approach to class unlearning. However, the availability of original data during the unlearning phase is not always guaranteed, leading to the exploration of class unlearning with restricted data access. While current unlearning methods with restricted data access usually generate proxy sample via the trained neural network classifier, they typically focus on training and forgetting balanced data. However, the imbalanced original data can cause trouble for these proxies and unlearning, particularly when the forgetting data consists predominantly of the majority class. To address this issue, we propose the GENerative Imbalanced Unlearning (GENIU) framework. GENIU utilizes a Variational Autoencoder (VAE) to concurrently train a proxy generator alongside the original model. These generated proxies accurately represent each class and are leveraged in the unlearning phase, eliminating the reliance on the original training data. To further mitigate the performance degradation resulting from forgetting the majority class, we introduce an in-batch tuning strategy that works with the generated proxies. GENIU is the first practical framework for class unlearning in imbalanced data settings and restricted data access, ensuring the preservation of essential information for future unlearning. Experimental results confirm the superiority of GENIU over existing methods, establishing its effectiveness in empirical scenarios.","sentences":["With the increasing emphasis on data privacy, the significance of machine unlearning has grown substantially.","Class unlearning, which involves enabling a trained model to forget data belonging to a specific class learned before, is important as classification tasks account for the majority of today's machine learning as a service (MLaaS).","Retraining the model on the original data, excluding the data to be forgotten (a.k.a forgetting data), is a common approach to class unlearning.","However, the availability of original data during the unlearning phase is not always guaranteed, leading to the exploration of class unlearning with restricted data access.","While current unlearning methods with restricted data access usually generate proxy sample via the trained neural network classifier, they typically focus on training and forgetting balanced data.","However, the imbalanced original data can cause trouble for these proxies and unlearning, particularly when the forgetting data consists predominantly of the majority class.","To address this issue, we propose the GENerative Imbalanced Unlearning (GENIU) framework.","GENIU utilizes a Variational Autoencoder (VAE) to concurrently train a proxy generator alongside the original model.","These generated proxies accurately represent each class and are leveraged in the unlearning phase, eliminating the reliance on the original training data.","To further mitigate the performance degradation resulting from forgetting the majority class, we introduce an in-batch tuning strategy that works with the generated proxies.","GENIU is the first practical framework for class unlearning in imbalanced data settings and restricted data access, ensuring the preservation of essential information for future unlearning.","Experimental results confirm the superiority of GENIU over existing methods, establishing its effectiveness in empirical scenarios."],"url":"http://arxiv.org/abs/2406.07885v1","category":"cs.LG"}
{"created":"2024-06-12 05:20:40","title":"Continuity estimates for the gradient of solutions to the Monge-Amp\u00e8re equation with nature boundary conditions","abstract":"We study the first derivative estimates for solutions to Monge-Amp\\`ere equations in terms of modulus of continuity. As a result, we establish the optimal global log-Lipschitz continuity for the gradient of solutions to the Monge-Amp\\`ere equation with natural boundary conditions.","sentences":["We study the first derivative estimates for solutions to Monge-Amp\\`ere equations in terms of modulus of continuity.","As a result, we establish the optimal global log-Lipschitz continuity for the gradient of solutions to the Monge-Amp\\`ere equation with natural boundary conditions."],"url":"http://arxiv.org/abs/2406.07883v1","category":"math.AP"}
{"created":"2024-06-12 05:20:01","title":"McKean-Vlasov Forward-backward doubly stochastic differential equations and applications to stochastic control","abstract":"This paper investigates first the existence and uniqueness of solutions for McKean-Vlasov forward-backward doubly stochastic differential equations (MV-FBDSDEs) in infinite-dimensional real separable Hilbert spaces. These equations combine the features of forward-backward doubly stochastic differential equations with the mean-field approach, allowing the coefficients to depend on the solution distribution. We establish the existence and uniqueness of solutions for MV-FBDSDEs using the method of continuation and provide an example and a counterexample to illustrate our findings. Moreover, we extend the practical applicability of our results by employing them within the context of the stochastic maximum principle for a control problem governed by MV-FBDSDEs. This study contributes to the field of stochastic control problems and presents the first analysis of MV-FBDSDEs in infinite-dimensional spaces.","sentences":["This paper investigates first the existence and uniqueness of solutions for McKean-Vlasov forward-backward doubly stochastic differential equations (MV-FBDSDEs) in infinite-dimensional real separable Hilbert spaces.","These equations combine the features of forward-backward doubly stochastic differential equations with the mean-field approach, allowing the coefficients to depend on the solution distribution.","We establish the existence and uniqueness of solutions for MV-FBDSDEs using the method of continuation and provide an example and a counterexample to illustrate our findings.","Moreover, we extend the practical applicability of our results by employing them within the context of the stochastic maximum principle for a control problem governed by MV-FBDSDEs.","This study contributes to the field of stochastic control problems and presents the first analysis of MV-FBDSDEs in infinite-dimensional spaces."],"url":"http://arxiv.org/abs/2406.07881v1","category":"math.PR"}
{"created":"2024-06-12 05:02:16","title":"Robust 3D Face Alignment with Multi-Path Neural Architecture Search","abstract":"3D face alignment is a very challenging and fundamental problem in computer vision. Existing deep learning-based methods manually design different networks to regress either parameters of a 3D face model or 3D positions of face vertices. However, designing such networks relies on expert knowledge, and these methods often struggle to produce consistent results across various face poses. To address this limitation, we employ Neural Architecture Search (NAS) to automatically discover the optimal architecture for 3D face alignment. We propose a novel Multi-path One-shot Neural Architecture Search (MONAS) framework that leverages multi-scale features and contextual information to enhance face alignment across various poses. The MONAS comprises two key algorithms: Multi-path Networks Unbiased Sampling Based Training and Simulated Annealing based Multi-path One-shot Search. Experimental results on three popular benchmarks demonstrate the superior performance of the MONAS for both sparse alignment and dense alignment.","sentences":["3D face alignment is a very challenging and fundamental problem in computer vision.","Existing deep learning-based methods manually design different networks to regress either parameters of a 3D face model or 3D positions of face vertices.","However, designing such networks relies on expert knowledge, and these methods often struggle to produce consistent results across various face poses.","To address this limitation, we employ Neural Architecture Search (NAS) to automatically discover the optimal architecture for 3D face alignment.","We propose a novel Multi-path One-shot Neural Architecture Search (MONAS) framework that leverages multi-scale features and contextual information to enhance face alignment across various poses.","The MONAS comprises two key algorithms: Multi-path Networks Unbiased Sampling Based Training and Simulated Annealing based Multi-path One-shot Search.","Experimental results on three popular benchmarks demonstrate the superior performance of the MONAS for both sparse alignment and dense alignment."],"url":"http://arxiv.org/abs/2406.07873v1","category":"cs.CV"}
{"created":"2024-06-12 04:53:46","title":"Event-Triggered Optimal Tracking Control for Strict-Feedback Nonlinear Systems With Non-Affine Nonlinear Faults","abstract":"This article studies the control ideas of the optimal backstepping technique, proposing an event-triggered optimal tracking control scheme for a class of strict-feedback nonlinear systems with non-affine and nonlinear faults. A simplified identifier-critic-actor framework is employed in the reinforcement learning algorithm to achieve optimal control. The identifier estimates the unknown dynamic functions, the critic evaluates the system performance, and the actor implements control actions, enabling modeling and control of anonymous systems for achieving optimal control performance. In this paper, a simplified reinforcement learning algorithm is designed by deriving update rules from the negative gradient of a simple positive function related to the Hamilton-Jacobi-Bellman equation, and it also releases the stringent persistent excitation condition. Then, a fault-tolerant control method is developed by applying filtered signals for controller design. Additionally, to address communication resource reduction, an event-triggered mechanism is employed for designing the actual controller. Finally, the proposed scheme's feasibility is validated through theoretical analysis and simulation.","sentences":["This article studies the control ideas of the optimal backstepping technique, proposing an event-triggered optimal tracking control scheme for a class of strict-feedback nonlinear systems with non-affine and nonlinear faults.","A simplified identifier-critic-actor framework is employed in the reinforcement learning algorithm to achieve optimal control.","The identifier estimates the unknown dynamic functions, the critic evaluates the system performance, and the actor implements control actions, enabling modeling and control of anonymous systems for achieving optimal control performance.","In this paper, a simplified reinforcement learning algorithm is designed by deriving update rules from the negative gradient of a simple positive function related to the Hamilton-Jacobi-Bellman equation, and it also releases the stringent persistent excitation condition.","Then, a fault-tolerant control method is developed by applying filtered signals for controller design.","Additionally, to address communication resource reduction, an event-triggered mechanism is employed for designing the actual controller.","Finally, the proposed scheme's feasibility is validated through theoretical analysis and simulation."],"url":"http://arxiv.org/abs/2406.07870v1","category":"math.OC"}
{"created":"2024-06-12 04:09:44","title":"VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment","abstract":"With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler.","sentences":["With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis.","However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition.","In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression.","To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes.","Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output.","Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth.","In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference.","This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia.","Audio samples will be available at: https://aka.ms/valler."],"url":"http://arxiv.org/abs/2406.07855v1","category":"cs.CL"}
{"created":"2024-06-12 03:21:06","title":"Incremental Learning and Self-Attention Mechanisms Improve Neural System Identification","abstract":"Convolutional neural networks (CNNs) have been shown to be the state-of-the-art approach for modeling the transfer functions of visual cortical neurons. Cortical neurons in the primary visual cortex are are sensitive to contextual information mediated by extensive horizontal and feedback connections. Standard CNNs can integrate global spatial image information to model such contextual modulation via two mechanisms: successive rounds of convolutions and a fully connected readout layer. In this paper, we find that non-local networks or self-attention (SA) mechanisms, theoretically related to context-dependent flexible gating mechanisms observed in the primary visual cortex, improve neural response predictions over parameter-matched CNNs in two key metrics: tuning curve correlation and tuning peak. We factorize networks to determine the relative contribution of each context mechanism. This reveals that information in the local receptive field is most important for modeling the overall tuning curve, but surround information is critically necessary for characterizing the tuning peak. We find that self-attention can replace subsequent spatial-integration convolutions when learned in an incremental manner, and is further enhanced in the presence of a fully connected readout layer, suggesting that the two context mechanisms are complementary. Finally, we find that learning a receptive-field-centric model with self-attention, before incrementally learning a fully connected readout, yields a more biologically realistic model in terms of center-surround contributions.","sentences":["Convolutional neural networks (CNNs) have been shown to be the state-of-the-art approach for modeling the transfer functions of visual cortical neurons.","Cortical neurons in the primary visual cortex are are sensitive to contextual information mediated by extensive horizontal and feedback connections.","Standard CNNs can integrate global spatial image information to model such contextual modulation via two mechanisms: successive rounds of convolutions and a fully connected readout layer.","In this paper, we find that non-local networks or self-attention (SA) mechanisms, theoretically related to context-dependent flexible gating mechanisms observed in the primary visual cortex, improve neural response predictions over parameter-matched CNNs in two key metrics: tuning curve correlation and tuning peak.","We factorize networks to determine the relative contribution of each context mechanism.","This reveals that information in the local receptive field is most important for modeling the overall tuning curve, but surround information is critically necessary for characterizing the tuning peak.","We find that self-attention can replace subsequent spatial-integration convolutions when learned in an incremental manner, and is further enhanced in the presence of a fully connected readout layer, suggesting that the two context mechanisms are complementary.","Finally, we find that learning a receptive-field-centric model with self-attention, before incrementally learning a fully connected readout, yields a more biologically realistic model in terms of center-surround contributions."],"url":"http://arxiv.org/abs/2406.07843v1","category":"cs.CV"}
{"created":"2024-06-12 02:50:43","title":"Quantum Hamilton-Jacobi Theory, Spectral Path Integrals and Exact-WKB","abstract":"We propose a new way to perform path integrals in quantum mechanics by using a quantum version of Hamilton-Jacobi theory. In classical mechanics, Hamilton-Jacobi theory is a powerful formalism, however, its utility is not explored in quantum theory beyond the correspondence principle. The canonical transformation enables one to set the new Hamiltonian to constant or zero, but keeps the information about solution in Hamilton's characteristic function. To benefit from this in quantum theory, one must work with a formulation in which classical Hamiltonian is used. This uniquely points to phase space path integral. However, the main variable in HJ-formalism is energy, not time. Thus, we are led to consider Fourier transform of path integral, spectral path integral, $\\tilde Z(E)$. This admits a representation in terms of a quantum Hamilton's characteristic functions for perturbative and non-perturbative periodic orbits, generalizing Gutzwiller's sum. This results in a path integral derivation of exact quantization conditions, complementary to the exact WKB analysis of differential equations. We apply these to generic $\\mathbb Z_2$ symmetric multi-well potential problems and point out some new instanton effects, e.g., the level splitting is generically a multi-instanton effect, unlike double-well.","sentences":["We propose a new way to perform path integrals in quantum mechanics by using a quantum version of Hamilton-Jacobi theory.","In classical mechanics, Hamilton-Jacobi theory is a powerful formalism, however, its utility is not explored in quantum theory beyond the correspondence principle.","The canonical transformation enables one to set the new Hamiltonian to constant or zero, but keeps the information about solution in Hamilton's characteristic function.","To benefit from this in quantum theory, one must work with a formulation in which classical Hamiltonian is used.","This uniquely points to phase space path integral.","However, the main variable in HJ-formalism is energy, not time.","Thus, we are led to consider Fourier transform of path integral, spectral path integral, $\\tilde Z(E)$.","This admits a representation in terms of a quantum Hamilton's characteristic functions for perturbative and non-perturbative periodic orbits, generalizing Gutzwiller's sum.","This results in a path integral derivation of exact quantization conditions, complementary to the exact WKB analysis of differential equations.","We apply these to generic $\\mathbb Z_2$ symmetric multi-well potential problems and point out some new instanton effects, e.g., the level splitting is generically a multi-instanton effect, unlike double-well."],"url":"http://arxiv.org/abs/2406.07829v1","category":"hep-th"}
{"created":"2024-06-12 02:48:52","title":"Spatial Annealing Smoothing for Efficient Few-shot Neural Rendering","abstract":"Neural Radiance Fields (NeRF) with hybrid representations have shown impressive capabilities in reconstructing scenes for view synthesis, delivering high efficiency. Nonetheless, their performance significantly drops with sparse view inputs, due to the issue of overfitting. While various regularization strategies have been devised to address these challenges, they often depend on inefficient assumptions or are not compatible with hybrid models. There is a clear need for a method that maintains efficiency and improves resilience to sparse views within a hybrid framework. In this paper, we introduce an accurate and efficient few-shot neural rendering method named Spatial Annealing smoothing regularized NeRF (SANeRF), which is specifically designed for a pre-filtering-driven hybrid representation architecture. We implement an exponential reduction of the sample space size from an initially large value. This methodology is crucial for stabilizing the early stages of the training phase and significantly contributes to the enhancement of the subsequent process of detail refinement. Our extensive experiments reveal that, by adding merely one line of code, SANeRF delivers superior rendering quality and much faster reconstruction speed compared to current few-shot NeRF methods. Notably, SANeRF outperforms FreeNeRF by 0.3 dB in PSNR on the Blender dataset, while achieving 700x faster reconstruction speed.","sentences":["Neural Radiance Fields (NeRF) with hybrid representations have shown impressive capabilities in reconstructing scenes for view synthesis, delivering high efficiency.","Nonetheless, their performance significantly drops with sparse view inputs, due to the issue of overfitting.","While various regularization strategies have been devised to address these challenges, they often depend on inefficient assumptions or are not compatible with hybrid models.","There is a clear need for a method that maintains efficiency and improves resilience to sparse views within a hybrid framework.","In this paper, we introduce an accurate and efficient few-shot neural rendering method named Spatial Annealing smoothing regularized NeRF (SANeRF), which is specifically designed for a pre-filtering-driven hybrid representation architecture.","We implement an exponential reduction of the sample space size from an initially large value.","This methodology is crucial for stabilizing the early stages of the training phase and significantly contributes to the enhancement of the subsequent process of detail refinement.","Our extensive experiments reveal that, by adding merely one line of code, SANeRF delivers superior rendering quality and much faster reconstruction speed compared to current few-shot NeRF methods.","Notably, SANeRF outperforms FreeNeRF by 0.3 dB in PSNR on the Blender dataset, while achieving 700x faster reconstruction speed."],"url":"http://arxiv.org/abs/2406.07828v1","category":"cs.CV"}
{"created":"2024-06-12 02:03:05","title":"Coexistence of ferroelectric and ferrielectric phases in ultrathin antiferroelectric PbZrO3 thin films","abstract":"Whereas ferroelectricity may vanish in ultra-thin ferroelectric films, it is expected to emerge in ultra-thin anti-ferroelectric films, sparking people's interest in using antiferroelectric materials as an alternative to ferroelectric ones for high-density data storage applications. Lead Zirconate (PbZrO3) is considered the prototype material for antiferroelectricity, and indeed previous studies indicated that nanoscale PbZrO3 films exhibit ferroelectricity. The understanding of such phenomena from the microstructure aspect is crucial but still lacking. In this study, we fabricated a PbZrO3 film with thicknesses varying from 5 nm to 80 nm. Using Piezoresponse Force Microscopy, we discovered the film displayed a transition from antiferroelectric behaviour in the thicker areas to ferroelectric behaviour in the thinner ones, with a critical thickness between 10 and 15 nm. In this critical thickness range, a 12 nm PZO thin film was chosen for further study using aberration-corrected scanning transmission electron microscopy. The investigation showed that the film comprises both ferroelectric and ferrielectric phases. The ferroelectric phase is characterized by polarisation along the pseudocubic [011] projection direction. The positions of Pb, Zr, and O were determined using the integrated differential phase contrast method. This allowed us to ascertain that the ferroelectric PbZrO3 unit cell is half the size of that in the antiferroelectric phase on the ab plane. The observed unit cell is different from the electric field-induced ferroelectric rhombohedral phases. Additionally, we identified a ferrielectric phase with a unique up-up-zero-zero dipole configuration. The finding is crucial for understanding the performance of ultrathin antiferroelectric thin films and the subsequent design and development of antiferroelectric devices.","sentences":["Whereas ferroelectricity may vanish in ultra-thin ferroelectric films, it is expected to emerge in ultra-thin anti-ferroelectric films, sparking people's interest in using antiferroelectric materials as an alternative to ferroelectric ones for high-density data storage applications.","Lead Zirconate (PbZrO3) is considered the prototype material for antiferroelectricity, and indeed previous studies indicated that nanoscale PbZrO3 films exhibit ferroelectricity.","The understanding of such phenomena from the microstructure aspect is crucial but still lacking.","In this study, we fabricated a PbZrO3 film with thicknesses varying from 5 nm to 80 nm.","Using Piezoresponse Force Microscopy, we discovered the film displayed a transition from antiferroelectric behaviour in the thicker areas to ferroelectric behaviour in the thinner ones, with a critical thickness between 10 and 15 nm.","In this critical thickness range, a 12 nm PZO thin film was chosen for further study using aberration-corrected scanning transmission electron microscopy.","The investigation showed that the film comprises both ferroelectric and ferrielectric phases.","The ferroelectric phase is characterized by polarisation along the pseudocubic [011] projection direction.","The positions of Pb, Zr, and O were determined using the integrated differential phase contrast method.","This allowed us to ascertain that the ferroelectric PbZrO3 unit cell is half the size of that in the antiferroelectric phase on the ab plane.","The observed unit cell is different from the electric field-induced ferroelectric rhombohedral phases.","Additionally, we identified a ferrielectric phase with a unique up-up-zero-zero dipole configuration.","The finding is crucial for understanding the performance of ultrathin antiferroelectric thin films and the subsequent design and development of antiferroelectric devices."],"url":"http://arxiv.org/abs/2406.07808v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-12 00:55:52","title":"A posteriori error estimates for the exponential midpoint method for linear and semilinear parabolic equations","abstract":"In this paper, the a posteriori error estimates of the exponential midpoint method for time discretization are studied for linear and semilinear parabolic equations. Using the exponential midpoint approximation defined by a continuous and piecewise linear interpolation of nodal values yields the suboptimal order estimates. Based on the property of the entire function, we introduce a continuous and piecewise quadratic time reconstruction of the exponential midpoint method to derive the optimal order estimates, and the error bounds are solely dependent on the discretization parameters, the data of the problem and the approximation of the entire function. Several numerical examples are implemented to illustrate the theoretical results.","sentences":["In this paper, the a posteriori error estimates of the exponential midpoint method for time discretization are studied for linear and semilinear parabolic equations.","Using the exponential midpoint approximation defined by a continuous and piecewise linear interpolation of nodal values yields the suboptimal order estimates.","Based on the property of the entire function, we introduce a continuous and piecewise quadratic time reconstruction of the exponential midpoint method to derive the optimal order estimates, and the error bounds are solely dependent on the discretization parameters, the data of the problem and the approximation of the entire function.","Several numerical examples are implemented to illustrate the theoretical results."],"url":"http://arxiv.org/abs/2406.07789v1","category":"math.NA"}
{"created":"2024-06-12 00:42:43","title":"Field Test of Quantum Key Distribution with High Key Creation Efficiency","abstract":"Quantumkey distribution (QKD) promises unconditional security for communication. However, the random choices of the measurement basis in QKD usually result in low key creation efficiency. This drawback is overcome in the differential-phase-shift QKD, provided that each photon can be prepared in a large number of time bins with a proper waveform. In this work we develop a miniature 1550-nm single-photon source to generate narrowband single photon in 50 time bins with a nearly optimal waveform for achieving unity key creation efficiency. By utilizing these single photons in the field test, we demonstrate the differential-phase-shift QKD with a key creation efficiency of 97%. Our work shows that the practical QKD can benefit from the narrowband single photons with controllable waveforms.","sentences":["Quantumkey distribution (QKD) promises unconditional security for communication.","However, the random choices of the measurement basis in QKD usually result in low key creation efficiency.","This drawback is overcome in the differential-phase-shift QKD, provided that each photon can be prepared in a large number of time bins with a proper waveform.","In this work we develop a miniature 1550-nm single-photon source to generate narrowband single photon in 50 time bins with a nearly optimal waveform for achieving unity key creation efficiency.","By utilizing these single photons in the field test, we demonstrate the differential-phase-shift QKD with a key creation efficiency of 97%.","Our work shows that the practical QKD can benefit from the narrowband single photons with controllable waveforms."],"url":"http://arxiv.org/abs/2406.07786v1","category":"quant-ph"}
{"created":"2024-06-12 00:41:25","title":"From Variance to Veracity: Unbundling and Mitigating Gradient Variance in Differentiable Bundle Adjustment Layers","abstract":"Various pose estimation and tracking problems in robotics can be decomposed into a correspondence estimation problem (often computed using a deep network) followed by a weighted least squares optimization problem to solve for the poses. Recent work has shown that coupling the two problems by iteratively refining one conditioned on the other's output yields SOTA results across domains. However, training these models has proved challenging, requiring a litany of tricks to stabilize and speed up training. In this work, we take the visual odometry problem as an example and identify three plausible causes: (1) flow loss interference, (2) linearization errors in the bundle adjustment (BA) layer, and (3) dependence of weight gradients on the BA residual. We show how these issues result in noisy and higher variance gradients, potentially leading to a slow down in training and instabilities. We then propose a simple, yet effective solution to reduce the gradient variance by using the weights predicted by the network in the inner optimization loop to weight the correspondence objective in the training problem. This helps the training objective `focus' on the more important points, thereby reducing the variance and mitigating the influence of outliers. We show that the resulting method leads to faster training and can be more flexibly trained in varying training setups without sacrificing performance. In particular we show $2$--$2.5\\times$ training speedups over a baseline visual odometry model we modify.","sentences":["Various pose estimation and tracking problems in robotics can be decomposed into a correspondence estimation problem (often computed using a deep network) followed by a weighted least squares optimization problem to solve for the poses.","Recent work has shown that coupling the two problems by iteratively refining one conditioned on the other's output yields SOTA results across domains.","However, training these models has proved challenging, requiring a litany of tricks to stabilize and speed up training.","In this work, we take the visual odometry problem as an example and identify three plausible causes: (1) flow loss interference, (2) linearization errors in the bundle adjustment (BA) layer, and (3) dependence of weight gradients on the BA residual.","We show how these issues result in noisy and higher variance gradients, potentially leading to a slow down in training and instabilities.","We then propose a simple, yet effective solution to reduce the gradient variance by using the weights predicted by the network in the inner optimization loop to weight the correspondence objective in the training problem.","This helps the training objective `focus' on the more important points, thereby reducing the variance and mitigating the influence of outliers.","We show that the resulting method leads to faster training and can be more flexibly trained in varying training setups without sacrificing performance.","In particular we show $2$--$2.5\\times$ training speedups over a baseline visual odometry model we modify."],"url":"http://arxiv.org/abs/2406.07785v1","category":"cs.CV"}
{"created":"2024-06-12 00:23:48","title":"TRANSP integrated modeling code for interpretive and predictive analysis of tokamak plasmas","abstract":"This paper provides a comprehensive review of the TRANSP code, a sophisticated tool for interpretive and predictive analysis of tokamak plasmas, detailing its major capabilities and features. It describes the equations for particle, power, and momentum balance analysis, as well as the poloidal field diffusion equations. The paper outlines the spatial and time grids used in TRANSP and details the equilibrium assumptions and solvers. Various models for heating and current drive, including updates to the NUBEAM model, are discussed. The handling of large-scale events such as sawtooth crashes and pellet injections is examined, along with the predictive capabilities for advancing plasma profiles. The integration of TRANSP with the ITER Integrated Modeling and Analysis Suite (IMAS) is highlighted, demonstrating enhanced data access and analysis capabilities. Additionally, the paper discusses best practices and continuous integration techniques to enhance TRANSP's robustness. The suite of TRANSP tools, designed for efficient data analysis and simulation, further supports the optimization of tokamak operations and coupling with other tokamak codes. Continuous development and support ensure that TRANSP remains a major code for the analysis of experimental data for controlled thermonuclear fusion, maintaining its critical role in supporting the optimization of tokamak operations and advancing fusion research.","sentences":["This paper provides a comprehensive review of the TRANSP code, a sophisticated tool for interpretive and predictive analysis of tokamak plasmas, detailing its major capabilities and features.","It describes the equations for particle, power, and momentum balance analysis, as well as the poloidal field diffusion equations.","The paper outlines the spatial and time grids used in TRANSP and details the equilibrium assumptions and solvers.","Various models for heating and current drive, including updates to the NUBEAM model, are discussed.","The handling of large-scale events such as sawtooth crashes and pellet injections is examined, along with the predictive capabilities for advancing plasma profiles.","The integration of TRANSP with the ITER Integrated Modeling and Analysis Suite (IMAS) is highlighted, demonstrating enhanced data access and analysis capabilities.","Additionally, the paper discusses best practices and continuous integration techniques to enhance TRANSP's robustness.","The suite of TRANSP tools, designed for efficient data analysis and simulation, further supports the optimization of tokamak operations and coupling with other tokamak codes.","Continuous development and support ensure that TRANSP remains a major code for the analysis of experimental data for controlled thermonuclear fusion, maintaining its critical role in supporting the optimization of tokamak operations and advancing fusion research."],"url":"http://arxiv.org/abs/2406.07781v1","category":"physics.plasm-ph"}
{"created":"2024-06-11 23:51:47","title":"The $L^1$-$L^\\infty$-geometry of Teichm\u00fcller space -- Second order infinitesimal structures --","abstract":"The $L^1$-$L^\\infty$ geometry is the Finsler geometry of the Teichm\\\"uller space by the Teichm\\\"uller metric and the $L^1$-norm function of holomorphic quadratic differentials. In this paper, aiming to develop the $L^1$-$L^\\infty$-geometry and the differential geometry on the Teichm\\\"uller space, we formulate the second order infinitesimal structures (the infinitesimal structures on the (co)tangent bundles) over the Teichm\\\"uller space. We will give model spaces of the second order infinitesimal spaces.   By applying our formulation, we give affirmative answers to two folklore. We first show that the map from the space of holomorphic quadratic differentials to the tangent bundle defined by Teichm\\\"uller Beltrami differentials is a real-analytic diffeomorphism on every stratum in the space of holomorphic quadratic differentials. Second, we show that the Teichm\\\"uller metric is real-analytic on the image of each stratum. We also observe a new duality between the Teichm\\\"uller metric and the $L^1$-norm function at the infinitesimal level.","sentences":["The $L^1$-$L^\\infty$ geometry is the Finsler geometry of the Teichm\\\"uller space by the Teichm\\\"uller metric and the $L^1$-norm function of holomorphic quadratic differentials.","In this paper, aiming to develop the $L^1$-$L^\\infty$-geometry and the differential geometry on the Teichm\\\"uller space, we formulate the second order infinitesimal structures (the infinitesimal structures on the (co)tangent bundles) over the Teichm\\\"uller space.","We will give model spaces of the second order infinitesimal spaces.   ","By applying our formulation, we give affirmative answers to two folklore.","We first show that the map from the space of holomorphic quadratic differentials to the tangent bundle defined by Teichm\\\"uller Beltrami differentials is a real-analytic diffeomorphism on every stratum in the space of holomorphic quadratic differentials.","Second, we show that the Teichm\\\"uller metric is real-analytic on the image of each stratum.","We also observe a new duality between the Teichm\\\"uller metric and the $L^1$-norm function at the infinitesimal level."],"url":"http://arxiv.org/abs/2406.07776v1","category":"math.CV"}
{"created":"2024-06-11 23:51:06","title":"Self-attention-based non-linear basis transformations for compact latent space modelling of dynamic optical fibre transmission matrices","abstract":"Multimode optical fibres are hair-thin strands of glass that efficiently transport light. They promise next-generation medical endoscopes that provide unprecedented sub-cellular image resolution deep inside the body. However, confining light to such fibres means that images are inherently scrambled in transit. Conventionally, this scrambling has been compensated by pre-calibrating how a specific fibre scrambles light and solving a stationary linear matrix equation that represents a physical model of the fibre. However, as the technology develops towards real-world deployment, the unscrambling process must account for dynamic changes in the matrix representing the fibre's effect on light, due to factors such as movement and temperature shifts, and non-linearities resulting from the inaccessibility of the fibre tip when inside the body. Such complex, dynamic and nonlinear behaviour is well-suited to approximation by neural networks, but most leading image reconstruction networks rely on convolutional layers, which assume strong correlations between adjacent pixels, a strong inductive bias that is inappropriate for fibre matrices which may be expressed in a range of arbitrary coordinate representations with long-range correlations. We introduce a new concept that uses self-attention layers to dynamically transform the coordinate representations of varying fibre matrices to a basis that admits compact, low-dimensional representations suitable for further processing. We demonstrate the effectiveness of this approach on diverse fibre matrix datasets. We show our models significantly improve the sparsity of fibre bases in their transformed bases with a participation ratio, p, as a measure of sparsity, of between 0.01 and 0.11. Further, we show that these transformed representations admit reconstruction of the original matrices with < 10% reconstruction error, demonstrating the invertibility.","sentences":["Multimode optical fibres are hair-thin strands of glass that efficiently transport light.","They promise next-generation medical endoscopes that provide unprecedented sub-cellular image resolution deep inside the body.","However, confining light to such fibres means that images are inherently scrambled in transit.","Conventionally, this scrambling has been compensated by pre-calibrating how a specific fibre scrambles light and solving a stationary linear matrix equation that represents a physical model of the fibre.","However, as the technology develops towards real-world deployment, the unscrambling process must account for dynamic changes in the matrix representing the fibre's effect on light, due to factors such as movement and temperature shifts, and non-linearities resulting from the inaccessibility of the fibre tip when inside the body.","Such complex, dynamic and nonlinear behaviour is well-suited to approximation by neural networks, but most leading image reconstruction networks rely on convolutional layers, which assume strong correlations between adjacent pixels, a strong inductive bias that is inappropriate for fibre matrices which may be expressed in a range of arbitrary coordinate representations with long-range correlations.","We introduce a new concept that uses self-attention layers to dynamically transform the coordinate representations of varying fibre matrices to a basis that admits compact, low-dimensional representations suitable for further processing.","We demonstrate the effectiveness of this approach on diverse fibre matrix datasets.","We show our models significantly improve the sparsity of fibre bases in their transformed bases with a participation ratio, p, as a measure of sparsity, of between 0.01 and 0.11.","Further, we show that these transformed representations admit reconstruction of the original matrices with < 10% reconstruction error, demonstrating the invertibility."],"url":"http://arxiv.org/abs/2406.07775v1","category":"cs.LG"}
{"created":"2024-06-11 22:07:20","title":"Conformable Derivative Approach to Granular Gases","abstract":"Proper modeling of complex systems requires innovative mathematical tools. In this sense, we sought to use deformed or fractal derivatives for studying the dynamics of systems, particularly those, such as granular gases, in which the description of the dynamics can be done by using the stretched exponential probability densities. In this contribution we draw up three results of this application of mathematical tools. The first result shows that when we use constraints with finite momentum and the principle of maximum entropy, the Kohlrausch--Williams--Watts function, known as stretched exponential, emerges naturally and in a simpler way, when compared to results in the literature. Next, we obtain generalized expressions for the Langevin equation, as well as its solutions for three different deformed derivatives, including those connected with nonaddictive statistical mechanics. The Haff's-like law for granular gases are obtained. Next, we calculate the partition function $Z$ for a granular gas system by building up the probability density in terms of the stretched exponential function. From this partition function, we determine the internal energy of the system as well as the specific heat, both dependent on temperature. The consistency with classical approach of kinetic theory for ideal gases was verified.","sentences":["Proper modeling of complex systems requires innovative mathematical tools.","In this sense, we sought to use deformed or fractal derivatives for studying the dynamics of systems, particularly those, such as granular gases, in which the description of the dynamics can be done by using the stretched exponential probability densities.","In this contribution we draw up three results of this application of mathematical tools.","The first result shows that when we use constraints with finite momentum and the principle of maximum entropy, the Kohlrausch--Williams--Watts function, known as stretched exponential, emerges naturally and in a simpler way, when compared to results in the literature.","Next, we obtain generalized expressions for the Langevin equation, as well as its solutions for three different deformed derivatives, including those connected with nonaddictive statistical mechanics.","The Haff's-like law for granular gases are obtained.","Next, we calculate the partition function $Z$ for a granular gas system by building up the probability density in terms of the stretched exponential function.","From this partition function, we determine the internal energy of the system as well as the specific heat, both dependent on temperature.","The consistency with classical approach of kinetic theory for ideal gases was verified."],"url":"http://arxiv.org/abs/2406.07748v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-11 22:01:50","title":"Bergman spaces and reproducing kernel for the biquaternionic Vekua equation","abstract":"We analyze the main properties of the Bergman spaces of weak $L_p$- solutions for a biquaternionic Vekua equation of the form \\[ \\mathbf{D}w(x)-\\mathbf{Q}_Aw(x)=0 \\] on bounded domains of $\\mathbb{R}^3$, where the operator $\\mathbf{Q}_A$ involves quaternionic conjugation and multiplications, both left and right, by essentially bounded functions. Properties such as completeness, separability, and reflexivity are shown. It is demonstrated that the solutions belonging to the Bergman spaces are locally H\\\"older continuous and that the evaluation maps are bounded in the $L_p$-norm. Consequently, for the case $p=2$, we obtain a reproducing integral kernel and an explicit formula for the orthogonal projection onto the Bergman space. For $1<p<\\infty$, the explicit form for the annihilator of the Bergman space in the dual $L_{p'}$ is presented, along with an orthogonal decomposition for $L_2$.","sentences":["We analyze the main properties of the Bergman spaces of weak $L_p$- solutions for a biquaternionic Vekua equation of the form \\[ \\mathbf{D}w(x)-\\mathbf{Q}_Aw(x)=0 \\] on bounded domains of $\\mathbb{R}^3$, where the operator $\\mathbf{Q}_A$ involves quaternionic conjugation and multiplications, both left and right, by essentially bounded functions.","Properties such as completeness, separability, and reflexivity are shown.","It is demonstrated that the solutions belonging to the Bergman spaces are locally H\\\"older continuous and that the evaluation maps are bounded in the $L_p$-norm.","Consequently, for the case $p=2$, we obtain a reproducing integral kernel and an explicit formula for the orthogonal projection onto the Bergman space.","For $1<p<\\infty$, the explicit form for the annihilator of the Bergman space in the dual $L_{p'}$ is presented, along with an orthogonal decomposition for $L_2$."],"url":"http://arxiv.org/abs/2406.07744v1","category":"math.AP"}
{"created":"2024-06-11 21:54:43","title":"Domain Walls and Hubble Constant Tension","abstract":"We present the idea that replacing the cosmological constant $\\Lambda$ in the $\\Lambda$CDM model by a distribution of walls, with very low tension compared to what one would expect from new physics, could help explaining the tension in the Hubble constant fits in the Standard Cosmological Model. Using parameters from our model for dark matter as macroscopic pearls, we can get a promising order of magnitude for the correction to the Hubble constant estimated from observations of the cosmic microwave background. Our model is on the borderline of failing by predicting too large extra fluctuations as a function of direction in the cosmological microwave background radiation. However, imagining the bubbles in the voids to have come from more somewhat smaller big bubbles also occurring outside the big voids may help. We estimate that, in order to have big volumes of the new vacuum in intergalactic space, a very high temperature is needed and that such regions would be likely to get cooled, freeze and shrink down to the degenerate form of dark matter if hitting some ordinary matter, as is likely in the denser parts of the Universe. We also review our model for dark matter, and develop the understanding of the stopping of the dark matter particles in the shielding of say the DAMA-LIBRA underground experiment and the counting rate this experiment observes. We manage to obtain a consistent fit with a mass $M = 2 \\cdot 10^{-18}$kg = $10^9$GeV and radius $R = 10^{-10}$m for the dark matter pearls, corresponding to a tension in the domain wall of $S= (8 {\\rm MeV})^3$.","sentences":["We present the idea that replacing the cosmological constant $\\Lambda$ in the $\\Lambda$CDM model by a distribution of walls, with very low tension compared to what one would expect from new physics, could help explaining the tension in the Hubble constant fits in the Standard Cosmological Model.","Using parameters from our model for dark matter as macroscopic pearls, we can get a promising order of magnitude for the correction to the Hubble constant estimated from observations of the cosmic microwave background.","Our model is on the borderline of failing by predicting too large extra fluctuations as a function of direction in the cosmological microwave background radiation.","However, imagining the bubbles in the voids to have come from more somewhat smaller big bubbles also occurring outside the big voids may help.","We estimate that, in order to have big volumes of the new vacuum in intergalactic space, a very high temperature is needed and that such regions would be likely to get cooled, freeze and shrink down to the degenerate form of dark matter if hitting some ordinary matter, as is likely in the denser parts of the Universe.","We also review our model for dark matter, and develop the understanding of the stopping of the dark matter particles in the shielding of say the DAMA-LIBRA underground experiment and the counting rate this experiment observes.","We manage to obtain a consistent fit with a mass $M = 2 \\cdot 10^{-18}$kg = $10^9$GeV and radius $R = 10^{-10}$m for the dark matter pearls, corresponding to a tension in the domain wall of $S= (8 {\\rm MeV})^3$."],"url":"http://arxiv.org/abs/2406.07740v1","category":"astro-ph.CO"}
{"created":"2024-06-11 20:52:42","title":"Unleashing the Power of Transfer Learning Model for Sophisticated Insect Detection: Revolutionizing Insect Classification","abstract":"The purpose of the Insect Detection System for Crop and Plant Health is to keep an eye out for and identify insect infestations in farming areas. By utilizing cutting-edge technology like computer vision and machine learning, the system seeks to identify hazardous insects early and accurately. This would enable prompt response to save crops and maintain optimal plant health. The Method of this study includes Data Acquisition, Preprocessing, Data splitting, Model Implementation and Model evaluation. Different models like MobileNetV2, ResNet152V2, Xecption, Custom CNN was used in this study. In order to categorize insect photos, a Convolutional Neural Network (CNN) based on the ResNet152V2 architecture is constructed and evaluated in this work. Achieving 99% training accuracy and 97% testing accuracy, ResNet152V2 demonstrates superior performance among four implemented models. The results highlight its potential for real-world applications in insect classification and entomology studies, emphasizing efficiency and accuracy. To ensure food security and sustain agricultural output globally, finding insects is crucial. Cutting-edge technology, such as ResNet152V2 models, greatly influence automating and improving the accuracy of insect identification. Efficient insect detection not only minimizes crop losses but also enhances agricultural productivity, contributing to sustainable food production. This underscores the pivotal role of technology in addressing challenges related to global food security.","sentences":["The purpose of the Insect Detection System for Crop and Plant Health is to keep an eye out for and identify insect infestations in farming areas.","By utilizing cutting-edge technology like computer vision and machine learning, the system seeks to identify hazardous insects early and accurately.","This would enable prompt response to save crops and maintain optimal plant health.","The Method of this study includes Data Acquisition, Preprocessing, Data splitting, Model Implementation and Model evaluation.","Different models like MobileNetV2, ResNet152V2, Xecption, Custom CNN was used in this study.","In order to categorize insect photos, a Convolutional Neural Network (CNN) based on the ResNet152V2 architecture is constructed and evaluated in this work.","Achieving 99% training accuracy and 97% testing accuracy, ResNet152V2 demonstrates superior performance among four implemented models.","The results highlight its potential for real-world applications in insect classification and entomology studies, emphasizing efficiency and accuracy.","To ensure food security and sustain agricultural output globally, finding insects is crucial.","Cutting-edge technology, such as ResNet152V2 models, greatly influence automating and improving the accuracy of insect identification.","Efficient insect detection not only minimizes crop losses but also enhances agricultural productivity, contributing to sustainable food production.","This underscores the pivotal role of technology in addressing challenges related to global food security."],"url":"http://arxiv.org/abs/2406.07716v1","category":"cs.CV"}
{"created":"2024-06-11 20:33:55","title":"Identification of cross-frequency interactions in compressible cavity flow using harmonic resolvent analysis","abstract":"The resolvent analysis reveals the worst-case disturbances and the most amplified response in a fluid flow that can develop around a stationary base state. The recent work by Padovan et al.(2020) extended the classical resolvent analysis to the harmonic resolvent analysis framework by incorporating the time-varying nature of the base flow. The harmonic resolvent analysis can capture the triadic interactions between perturbations at two different frequencies through a base flow at a particular frequency. The singular values of the harmonic resolvent operator act as a gain between the spatio-temporal forcing and the response provided by the singular vectors. In the current study, we formulate the harmonic resolvent analysis framework for compressible flows based on the linearized Navier-Stokes equation (i.e., operator-based formulation). We validate our approach by applying the technique to the low-mach number flow past an airfoil. We further illustrate the application of this method to compressible cavity flows at Mach numbers of 0.6 and 0.8 with a length-to-depth ratio of $2$. For the cavity flow at Mach number of 0.6, the harmonic resolvent analysis reveals that the nonlinear cross-frequency interactions dominate the amplification of perturbations at frequencies that are harmonics of the leading Rossiter mode in the nonlinear flow. The findings demonstrate a physically consistent representation of an energy transfer from slow-evolving modes toward fast-evolving modes in the flow through cross-frequency interactions. For the cavity flow at Mach number of 0.8, the analysis also sheds light on the nature of cross-frequency interaction in a cavity flow with two coexisting resonances.","sentences":["The resolvent analysis reveals the worst-case disturbances and the most amplified response in a fluid flow that can develop around a stationary base state.","The recent work by Padovan et al.(2020) extended the classical resolvent analysis to the harmonic resolvent analysis framework by incorporating the time-varying nature of the base flow.","The harmonic resolvent analysis can capture the triadic interactions between perturbations at two different frequencies through a base flow at a particular frequency.","The singular values of the harmonic resolvent operator act as a gain between the spatio-temporal forcing and the response provided by the singular vectors.","In the current study, we formulate the harmonic resolvent analysis framework for compressible flows based on the linearized Navier-Stokes equation (i.e., operator-based formulation).","We validate our approach by applying the technique to the low-mach number flow past an airfoil.","We further illustrate the application of this method to compressible cavity flows at Mach numbers of 0.6 and 0.8 with a length-to-depth ratio of $2$. For the cavity flow at Mach number of 0.6, the harmonic resolvent analysis reveals that the nonlinear cross-frequency interactions dominate the amplification of perturbations at frequencies that are harmonics of the leading Rossiter mode in the nonlinear flow.","The findings demonstrate a physically consistent representation of an energy transfer from slow-evolving modes toward fast-evolving modes in the flow through cross-frequency interactions.","For the cavity flow at Mach number of 0.8, the analysis also sheds light on the nature of cross-frequency interaction in a cavity flow with two coexisting resonances."],"url":"http://arxiv.org/abs/2406.07705v1","category":"physics.flu-dyn"}
{"created":"2024-06-11 20:26:26","title":"Label Smoothing Improves Machine Unlearning","abstract":"The objective of machine unlearning (MU) is to eliminate previously learned data from a model. However, it is challenging to strike a balance between computation cost and performance when using existing MU techniques. Taking inspiration from the influence of label smoothing on model confidence and differential privacy, we propose a simple gradient-based MU approach that uses an inverse process of label smoothing. This work introduces UGradSL, a simple, plug-and-play MU approach that uses smoothed labels. We provide theoretical analyses demonstrating why properly introducing label smoothing improves MU performance. We conducted extensive experiments on six datasets of various sizes and different modalities, demonstrating the effectiveness and robustness of our proposed method. The consistent improvement in MU performance is only at a marginal cost of additional computations. For instance, UGradSL improves over the gradient ascent MU baseline by 66% unlearning accuracy without sacrificing unlearning efficiency.","sentences":["The objective of machine unlearning (MU) is to eliminate previously learned data from a model.","However, it is challenging to strike a balance between computation cost and performance when using existing MU techniques.","Taking inspiration from the influence of label smoothing on model confidence and differential privacy, we propose a simple gradient-based MU approach that uses an inverse process of label smoothing.","This work introduces UGradSL, a simple, plug-and-play MU approach that uses smoothed labels.","We provide theoretical analyses demonstrating why properly introducing label smoothing improves MU performance.","We conducted extensive experiments on six datasets of various sizes and different modalities, demonstrating the effectiveness and robustness of our proposed method.","The consistent improvement in MU performance is only at a marginal cost of additional computations.","For instance, UGradSL improves over the gradient ascent MU baseline by 66% unlearning accuracy without sacrificing unlearning efficiency."],"url":"http://arxiv.org/abs/2406.07698v1","category":"cs.LG"}
{"created":"2024-06-11 20:07:22","title":"Adversarial Machine Unlearning","abstract":"This paper focuses on the challenge of machine unlearning, aiming to remove the influence of specific training data on machine learning models. Traditionally, the development of unlearning algorithms runs parallel with that of membership inference attacks (MIA), a type of privacy threat to determine whether a data instance was used for training. However, the two strands are intimately connected: one can view machine unlearning through the lens of MIA success with respect to removed data. Recognizing this connection, we propose a game-theoretic framework that integrates MIAs into the design of unlearning algorithms. Specifically, we model the unlearning problem as a Stackelberg game in which an unlearner strives to unlearn specific training data from a model, while an auditor employs MIAs to detect the traces of the ostensibly removed data. Adopting this adversarial perspective allows the utilization of new attack advancements, facilitating the design of unlearning algorithms. Our framework stands out in two ways. First, it takes an adversarial approach and proactively incorporates the attacks into the design of unlearning algorithms. Secondly, it uses implicit differentiation to obtain the gradients that limit the attacker's success, thus benefiting the process of unlearning. We present empirical results to demonstrate the effectiveness of the proposed approach for machine unlearning.","sentences":["This paper focuses on the challenge of machine unlearning, aiming to remove the influence of specific training data on machine learning models.","Traditionally, the development of unlearning algorithms runs parallel with that of membership inference attacks (MIA), a type of privacy threat to determine whether a data instance was used for training.","However, the two strands are intimately connected: one can view machine unlearning through the lens of MIA success with respect to removed data.","Recognizing this connection, we propose a game-theoretic framework that integrates MIAs into the design of unlearning algorithms.","Specifically, we model the unlearning problem as a Stackelberg game in which an unlearner strives to unlearn specific training data from a model, while an auditor employs MIAs to detect the traces of the ostensibly removed data.","Adopting this adversarial perspective allows the utilization of new attack advancements, facilitating the design of unlearning algorithms.","Our framework stands out in two ways.","First, it takes an adversarial approach and proactively incorporates the attacks into the design of unlearning algorithms.","Secondly, it uses implicit differentiation to obtain the gradients that limit the attacker's success, thus benefiting the process of unlearning.","We present empirical results to demonstrate the effectiveness of the proposed approach for machine unlearning."],"url":"http://arxiv.org/abs/2406.07687v1","category":"cs.LG"}
{"created":"2024-06-11 20:04:36","title":"Scalable Optimal Motion Planning for Multi-Agent Systems by Cosserat Theory of Rods","abstract":"We address the motion planning problem for large multi-agent systems, utilizing Cosserat rod theory to model the dynamic behavior of vehicle formations. The problem is formulated as an optimal control problem over partial differential equations (PDEs) that describe the system as a continuum. This approach ensures scalability with respect to the number of vehicles, as the problem's complexity remains unaffected by the size of the formation. The numerical discretization of the governing equations and problem's constraints is achieved through Bernstein surface polynomials, facilitating the conversion of the optimal control problem into a nonlinear programming (NLP) problem. This NLP problem is subsequently solved using off-the-shelf optimization software. We present several properties and algorithms related to Bernstein surface polynomials to support the selection of this methodology. Numerical demonstrations underscore the efficacy of this mathematical framework.","sentences":["We address the motion planning problem for large multi-agent systems, utilizing Cosserat rod theory to model the dynamic behavior of vehicle formations.","The problem is formulated as an optimal control problem over partial differential equations (PDEs) that describe the system as a continuum.","This approach ensures scalability with respect to the number of vehicles, as the problem's complexity remains unaffected by the size of the formation.","The numerical discretization of the governing equations and problem's constraints is achieved through Bernstein surface polynomials, facilitating the conversion of the optimal control problem into a nonlinear programming (NLP) problem.","This NLP problem is subsequently solved using off-the-shelf optimization software.","We present several properties and algorithms related to Bernstein surface polynomials to support the selection of this methodology.","Numerical demonstrations underscore the efficacy of this mathematical framework."],"url":"http://arxiv.org/abs/2406.07684v1","category":"math.OC"}
{"created":"2024-06-11 19:50:35","title":"Structure of a factor ring $R/P$ in terms of differential identities involving a new kind of involution","abstract":"Let $R$ be a ring and $P$ a prime ideal of $R.$ In this paper, we establish some commutativity criteria for the factor ring $R/P$ in terms of derivations of $R$ satisfying some algebraic identities involving a new kind of involution in relation with $P.$","sentences":["Let $R$ be a ring and $P$ a prime ideal of $R.$ In this paper, we establish some commutativity criteria for the factor ring $R/P$ in terms of derivations of $R$ satisfying some algebraic identities involving a new kind of involution in relation with $P.$"],"url":"http://arxiv.org/abs/2406.07675v1","category":"math.RA"}
{"created":"2024-06-11 19:25:41","title":"Finite Energy Geodesic Rays in Big Cohomology Classes","abstract":"For a big class represented by $\\theta$, we show that the metric space $(\\mathcal{E}^{p}(X,\\theta),d_{p})$ for $p \\geq 1$ is Buseman convex. This allows us to construct a chordal metric $d_{p}^{c}$ on the space of geodesic rays in $\\mathcal{E}^{p}(X,\\theta)$. We also prove that the space of finite $p$-energy geodesic rays with the chordal metric $d_{p}^{c}$ is a complete geodesic metric space.   With the help of the metric $d_{p}$, we find a characterization of geodesic rays lying in $\\mathcal{E}^{p}(X,\\theta)$ in terms of the corresponding test curves via the Ross-Witt Nystr\\\"om correspondence. This result is new even in the K\\\"ahler setting.","sentences":["For a big class represented by $\\theta$, we show that the metric space $(\\mathcal{E}^{p}(X,\\theta),d_{p})$ for $p \\geq 1$ is Buseman convex.","This allows us to construct a chordal metric $d_{p}^{c}$ on the space of geodesic rays in $\\mathcal{E}^{p}(X,\\theta)$. We also prove that the space of finite $p$-energy geodesic rays with the chordal metric $d_{p}^{c}$ is a complete geodesic metric space.   ","With the help of the metric $d_{p}$, we find a characterization of geodesic rays lying in $\\mathcal{E}^{p}(X,\\theta)$ in terms of the corresponding test curves via the Ross-Witt Nystr\\\"om correspondence.","This result is new even in the K\\\"ahler setting."],"url":"http://arxiv.org/abs/2406.07669v1","category":"math.DG"}
{"created":"2024-06-11 19:05:02","title":"Universal Differentiability Sets in Laakso Space","abstract":"We show that there exists a family of mutually singular doubling measures on Laakso space with respect to which real-valued Lipschitz functions are almost everywhere differentiable. This implies that there exists a measure zero universal differentiability set in Laakso space. Additionally, we show that each of the measures constructed supports a Poincar\\'e inequality.","sentences":["We show that there exists a family of mutually singular doubling measures on Laakso space with respect to which real-valued Lipschitz functions are almost everywhere differentiable.","This implies that there exists a measure zero universal differentiability set in Laakso space.","Additionally, we show that each of the measures constructed supports a Poincar\\'e inequality."],"url":"http://arxiv.org/abs/2406.07660v1","category":"math.FA"}
{"created":"2024-06-11 18:51:11","title":"Classical electrodynamics as a tool to examine optical effects of chiral dielectrics and media with magnetic conductivity","abstract":"We discuss how classical electromagnetic techniques are useful to describe optical effects in conventional and chiral dielectric systems endowed with optical activity. Starting from the Maxwell equations and constitutive relations of the medium, we obtain the wave equation (for the electric field) that yields the refractive indices and the corresponding propagating modes that define the electromagnetic wave polarization. This procedure is employed for bi-isotropic and bi-anisotropic dielectrics, allowing us to determine birefringence, a manifestation of the space or time inversion breaking, typical of the chiral media. Such a method can also be applied for axion electrodynamics and a dielectric supporting an isotropic chiral magnetic current, similar to the Chiral Magnetic Effect (CME). For a diagonal magnetic conductivity, two distinct refractive indices are found, associated with left circularly polarized (LCP) and right circularly polarized (RCP) waves, and yielding circular birefringence (evaluated in terms of the optical rotatory power). Additionally, the scenario of a bi-isotropic medium endowed with isotropic magnetic current is examined, where a rotatory power marked by sign reversal may work as an optical signature of this particular system.","sentences":["We discuss how classical electromagnetic techniques are useful to describe optical effects in conventional and chiral dielectric systems endowed with optical activity.","Starting from the Maxwell equations and constitutive relations of the medium, we obtain the wave equation (for the electric field) that yields the refractive indices and the corresponding propagating modes that define the electromagnetic wave polarization.","This procedure is employed for bi-isotropic and bi-anisotropic dielectrics, allowing us to determine birefringence, a manifestation of the space or time inversion breaking, typical of the chiral media.","Such a method can also be applied for axion electrodynamics and a dielectric supporting an isotropic chiral magnetic current, similar to the Chiral Magnetic Effect (CME).","For a diagonal magnetic conductivity, two distinct refractive indices are found, associated with left circularly polarized (LCP) and right circularly polarized (RCP) waves, and yielding circular birefringence (evaluated in terms of the optical rotatory power).","Additionally, the scenario of a bi-isotropic medium endowed with isotropic magnetic current is examined, where a rotatory power marked by sign reversal may work as an optical signature of this particular system."],"url":"http://arxiv.org/abs/2406.07654v1","category":"physics.optics"}
{"created":"2024-06-11 18:20:39","title":"SSNVC: Single Stream Neural Video Compression with Implicit Temporal Information","abstract":"Recently, Neural Video Compression (NVC) techniques have achieved remarkable performance, even surpassing the best traditional lossy video codec. However, most existing NVC methods heavily rely on transmitting Motion Vector (MV) to generate accurate contextual features, which has the following drawbacks. (1) Compressing and transmitting MV requires specialized MV encoder and decoder, which makes modules redundant. (2) Due to the existence of MV Encoder-Decoder, the training strategy is complex. In this paper, we present a noval Single Stream NVC framework (SSNVC), which removes complex MV Encoder-Decoder structure and uses a one-stage training strategy. SSNVC implicitly use temporal information by adding previous entropy model feature to current entropy model and using previous two frame to generate predicted motion information at the decoder side. Besides, we enhance the frame generator to generate higher quality reconstructed frame. Experiments demonstrate that SSNVC can achieve state-of-the-art performance on multiple benchmarks, and can greatly simplify compression process as well as training process.","sentences":["Recently, Neural Video Compression (NVC) techniques have achieved remarkable performance, even surpassing the best traditional lossy video codec.","However, most existing NVC methods heavily rely on transmitting Motion Vector (MV) to generate accurate contextual features, which has the following drawbacks.","(1) Compressing and transmitting MV requires specialized MV encoder and decoder, which makes modules redundant.","(2) Due to the existence of MV Encoder-Decoder, the training strategy is complex.","In this paper, we present a noval Single Stream NVC framework (SSNVC), which removes complex MV Encoder-Decoder structure and uses a one-stage training strategy.","SSNVC implicitly use temporal information by adding previous entropy model feature to current entropy model and using previous two frame to generate predicted motion information at the decoder side.","Besides, we enhance the frame generator to generate higher quality reconstructed frame.","Experiments demonstrate that SSNVC can achieve state-of-the-art performance on multiple benchmarks, and can greatly simplify compression process as well as training process."],"url":"http://arxiv.org/abs/2406.07645v1","category":"cs.CV"}
{"created":"2024-06-11 18:00:27","title":"Investigating the Interplay Between Aging and Rejuvenation in Spin Glasses","abstract":"Aging in a single crystal spin glass ($\\mathrm{Cu}_{0.92}\\mathrm{Mn}_{0.08}$) has been measured using ac susceptibility techniques over a temperature range of $0.3 - 0.8 \\, T_g$. In these studies, traditional aging experiments (or ``quench'' aging protocols) are compared to aging curves constructed from finite-cooling-rate curves. By comparing the growth rates of spin glass order between the two types of aging curves, it is determined that quantitative comparisons between protocols which are taken by quenching and protocols using a finite cooling rate are not possible without a deeper understanding of the interplay between aging and rejuvenation. We then demonstrate that the data presented indicate that rejuvenation, rather than cumulative aging, is the cause for the discrepancies between the two growth rates.","sentences":["Aging in a single crystal spin glass ($\\mathrm{Cu}_{0.92}\\mathrm{Mn}_{0.08}$) has been measured using ac susceptibility techniques over a temperature range of $0.3 - 0.8 \\, T_g$. In these studies, traditional aging experiments (or ``quench'' aging protocols) are compared to aging curves constructed from finite-cooling-rate curves.","By comparing the growth rates of spin glass order between the two types of aging curves, it is determined that quantitative comparisons between protocols which are taken by quenching and protocols using a finite cooling rate are not possible without a deeper understanding of the interplay between aging and rejuvenation.","We then demonstrate that the data presented indicate that rejuvenation, rather than cumulative aging, is the cause for the discrepancies between the two growth rates."],"url":"http://arxiv.org/abs/2406.07628v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-11 18:00:13","title":"Semi-Analytical Fokker Planck Models for Nuclear Star Clusters","abstract":"We study the dynamics of nuclear star clusters, the dense stellar environments surrounding massive black holes in the centers of galaxies. We consider angular momentum diffusion due to two-body scatterings among stellar objects and energy advection due to gravitational wave emission upon interaction with the central massive black hole. Such dynamics is described by a two-dimensional Fokker-Planck equation in energy-angular momentum space. Focusing on the transition between the diffusion-dominated region and the advection-dominated one, we utilize self-similarity to obtain a full solution for the Fokker-Planck equation. This solution provides the density and flux of the stellar objects in nuclear star clusters. This improves the rate estimates for extreme mass-ratio inspirals, and has interesting implications for a new class of galactic center transients called quasi-periodic eruptions.","sentences":["We study the dynamics of nuclear star clusters, the dense stellar environments surrounding massive black holes in the centers of galaxies.","We consider angular momentum diffusion due to two-body scatterings among stellar objects and energy advection due to gravitational wave emission upon interaction with the central massive black hole.","Such dynamics is described by a two-dimensional Fokker-Planck equation in energy-angular momentum space.","Focusing on the transition between the diffusion-dominated region and the advection-dominated one, we utilize self-similarity to obtain a full solution for the Fokker-Planck equation.","This solution provides the density and flux of the stellar objects in nuclear star clusters.","This improves the rate estimates for extreme mass-ratio inspirals, and has interesting implications for a new class of galactic center transients called quasi-periodic eruptions."],"url":"http://arxiv.org/abs/2406.07627v1","category":"astro-ph.HE"}
{"created":"2024-06-11 18:00:12","title":"Tailoring Bound State Geometry in High-Dimensional Non-Hermitian Systems","abstract":"It is generally believed that the non-Hermitian effect (NHSE), due to its non-reciprocal nature, creates barriers for the appearance of impurity bound states. In this paper, we find that in two and higher dimensions, the presence of geometry-dependent skin effect eliminates this barrier such that even an infinitesimal impurity potential can confine bound states in this type of non-Hermitian systems. By examining bound states around Bloch saddle points, we find that non-Hermiticity can disrupt the isotropy of bound states, resulting in concave dumbbell-shaped bound states. Our work reveals a geometry transition of bound state between concavity and convexity in high-dimensional non-Hermitian systems.","sentences":["It is generally believed that the non-Hermitian effect (NHSE), due to its non-reciprocal nature, creates barriers for the appearance of impurity bound states.","In this paper, we find that in two and higher dimensions, the presence of geometry-dependent skin effect eliminates this barrier such that even an infinitesimal impurity potential can confine bound states in this type of non-Hermitian systems.","By examining bound states around Bloch saddle points, we find that non-Hermiticity can disrupt the isotropy of bound states, resulting in concave dumbbell-shaped bound states.","Our work reveals a geometry transition of bound state between concavity and convexity in high-dimensional non-Hermitian systems."],"url":"http://arxiv.org/abs/2406.07626v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-11 18:00:00","title":"Soft zero for cylindrical gravitational waves","abstract":"The graviton S-matrix has a famous soft pole. We show that the S-matrix for cylindrical gravitational waves has a soft zero. The soft pole for ordinary gravitons comes from a Ward identity for supertranslation symmetry at asymptotic infinity. We show that the soft zero for cylindrical gravitational waves comes from a Ward identity for Geroch symmetry at asymptotic infinity. Because it is a zero rather than a pole, there is no memory effect. Overall, this soft zero is a manifestation of Geroch symmetry and of the extraordinary simplicity of cylindrical gravitational waves.","sentences":["The graviton S-matrix has a famous soft pole.","We show that the S-matrix for cylindrical gravitational waves has a soft zero.","The soft pole for ordinary gravitons comes from a Ward identity for supertranslation symmetry at asymptotic infinity.","We show that the soft zero for cylindrical gravitational waves comes from a Ward identity for Geroch symmetry at asymptotic infinity.","Because it is a zero rather than a pole, there is no memory effect.","Overall, this soft zero is a manifestation of Geroch symmetry and of the extraordinary simplicity of cylindrical gravitational waves."],"url":"http://arxiv.org/abs/2406.07604v1","category":"hep-th"}
{"created":"2024-06-11 17:29:16","title":"Unique continuation for area minimizing currents","abstract":"The main goal of this work is to prove an instance of the unique continuation principle for area minimizing integral currents. More precisely, consider an $m$-dimensional area minimizing integral current and an $m$-dimensional minimal surface, both contained in $\\mathbb{R}^{n+m}$ with $n\\geq 1$. We show that if, in an integral sense, the current has infinite order of contact with the minimal surface at a point, then the current and the minimal surface coincide in a neighborhood of that point.","sentences":["The main goal of this work is to prove an instance of the unique continuation principle for area minimizing integral currents.","More precisely, consider an $m$-dimensional area minimizing integral current and an $m$-dimensional minimal surface, both contained in $\\mathbb{R}^{n+m}$ with $n\\geq 1$.","We show that if, in an integral sense, the current has infinite order of contact with the minimal surface at a point, then the current and the minimal surface coincide in a neighborhood of that point."],"url":"http://arxiv.org/abs/2406.07600v1","category":"math.DG"}
{"created":"2024-06-12 17:59:21","title":"Enhancing End-to-End Autonomous Driving with Latent World Model","abstract":"End-to-end autonomous driving has garnered widespread attention. Current end-to-end approaches largely rely on the supervision from perception tasks such as detection, tracking, and map segmentation to aid in learning scene representations. However, these methods require extensive annotations, hindering the data scalability. To address this challenge, we propose a novel self-supervised method to enhance end-to-end driving without the need for costly labels. Specifically, our framework \\textbf{LAW} uses a LAtent World model to predict future latent features based on the predicted ego actions and the latent feature of the current frame. The predicted latent features are supervised by the actually observed features in the future. This supervision jointly optimizes the latent feature learning and action prediction, which greatly enhances the driving performance. As a result, our approach achieves state-of-the-art performance in both open-loop and closed-loop benchmarks without costly annotations.","sentences":["End-to-end autonomous driving has garnered widespread attention.","Current end-to-end approaches largely rely on the supervision from perception tasks such as detection, tracking, and map segmentation to aid in learning scene representations.","However, these methods require extensive annotations, hindering the data scalability.","To address this challenge, we propose a novel self-supervised method to enhance end-to-end driving without the need for costly labels.","Specifically, our framework \\textbf{LAW} uses a LAtent World model to predict future latent features based on the predicted ego actions and the latent feature of the current frame.","The predicted latent features are supervised by the actually observed features in the future.","This supervision jointly optimizes the latent feature learning and action prediction, which greatly enhances the driving performance.","As a result, our approach achieves state-of-the-art performance in both open-loop and closed-loop benchmarks without costly annotations."],"url":"http://arxiv.org/abs/2406.08481v1","category":"cs.CV"}
{"created":"2024-06-12 17:53:28","title":"Nonconvex Federated Learning on Compact Smooth Submanifolds With Heterogeneous Data","abstract":"Many machine learning tasks, such as principal component analysis and low-rank matrix completion, give rise to manifold optimization problems. Although there is a large body of work studying the design and analysis of algorithms for manifold optimization in the centralized setting, there are currently very few works addressing the federated setting. In this paper, we consider nonconvex federated learning over a compact smooth submanifold in the setting of heterogeneous client data. We propose an algorithm that leverages stochastic Riemannian gradients and a manifold projection operator to improve computational efficiency, uses local updates to improve communication efficiency, and avoids client drift. Theoretically, we show that our proposed algorithm converges sub-linearly to a neighborhood of a first-order optimal solution by using a novel analysis that jointly exploits the manifold structure and properties of the loss functions. Numerical experiments demonstrate that our algorithm has significantly smaller computational and communication overhead than existing methods.","sentences":["Many machine learning tasks, such as principal component analysis and low-rank matrix completion, give rise to manifold optimization problems.","Although there is a large body of work studying the design and analysis of algorithms for manifold optimization in the centralized setting, there are currently very few works addressing the federated setting.","In this paper, we consider nonconvex federated learning over a compact smooth submanifold in the setting of heterogeneous client data.","We propose an algorithm that leverages stochastic Riemannian gradients and a manifold projection operator to improve computational efficiency, uses local updates to improve communication efficiency, and avoids client drift.","Theoretically, we show that our proposed algorithm converges sub-linearly to a neighborhood of a first-order optimal solution by using a novel analysis that jointly exploits the manifold structure and properties of the loss functions.","Numerical experiments demonstrate that our algorithm has significantly smaller computational and communication overhead than existing methods."],"url":"http://arxiv.org/abs/2406.08465v1","category":"cs.LG"}
{"created":"2024-06-12 17:49:26","title":"ConceptHash: Interpretable Fine-Grained Hashing via Concept Discovery","abstract":"Existing fine-grained hashing methods typically lack code interpretability as they compute hash code bits holistically using both global and local features. To address this limitation, we propose ConceptHash, a novel method that achieves sub-code level interpretability. In ConceptHash, each sub-code corresponds to a human-understandable concept, such as an object part, and these concepts are automatically discovered without human annotations. Specifically, we leverage a Vision Transformer architecture and introduce concept tokens as visual prompts, along with image patch tokens as model inputs. Each concept is then mapped to a specific sub-code at the model output, providing natural sub-code interpretability. To capture subtle visual differences among highly similar sub-categories (e.g., bird species), we incorporate language guidance to ensure that the learned hash codes are distinguishable within fine-grained object classes while maintaining semantic alignment. This approach allows us to develop hash codes that exhibit similarity within families of species while remaining distinct from species in other families. Extensive experiments on four fine-grained image retrieval benchmarks demonstrate that ConceptHash outperforms previous methods by a significant margin, offering unique sub-code interpretability as an additional benefit. Code at: https://github.com/kamwoh/concepthash.","sentences":["Existing fine-grained hashing methods typically lack code interpretability as they compute hash code bits holistically using both global and local features.","To address this limitation, we propose ConceptHash, a novel method that achieves sub-code level interpretability.","In ConceptHash, each sub-code corresponds to a human-understandable concept, such as an object part, and these concepts are automatically discovered without human annotations.","Specifically, we leverage a Vision Transformer architecture and introduce concept tokens as visual prompts, along with image patch tokens as model inputs.","Each concept is then mapped to a specific sub-code at the model output, providing natural sub-code interpretability.","To capture subtle visual differences among highly similar sub-categories (e.g., bird species), we incorporate language guidance to ensure that the learned hash codes are distinguishable within fine-grained object classes while maintaining semantic alignment.","This approach allows us to develop hash codes that exhibit similarity within families of species while remaining distinct from species in other families.","Extensive experiments on four fine-grained image retrieval benchmarks demonstrate that ConceptHash outperforms previous methods by a significant margin, offering unique sub-code interpretability as an additional benefit.","Code at: https://github.com/kamwoh/concepthash."],"url":"http://arxiv.org/abs/2406.08457v1","category":"cs.CV"}
{"created":"2024-06-12 17:43:45","title":"Detection of Open Cluster Members Inside and Beyond Tidal Radius by Machine Learning Methods Based on Gaia DR3","abstract":"In our previous work, we introduced a method that combines two unsupervised algorithms: DBSCAN and GMM. We applied this method to 12 open clusters based on Gaia EDR3 data, demonstrating its effectiveness in identifying reliable cluster members within the tidal radius. However, for studying cluster morphology, we need a method capable of detecting members both inside and outside the tidal radius. By incorporating a supervised algorithm into our approach, we successfully identified members beyond the tidal radius. In our current work, we initially applied DBSCAN and GMM to identify reliable members of cluster stars. Subsequently, we trained the Random Forest algorithm using DBSCAN and GMM-selected data. Leveraging the random forest, we can identify cluster members outside the tidal radius and observe cluster morphology across a wide field of view. Our method was then applied to 15 open clusters based on Gaia DR3, which exhibit a wide range of metallicity, distances, members, and ages. Additionally, we calculated the tidal radius for each of the 15 clusters using the King profile and detected stars both inside and outside this radius. Finally, we investigated mass segregation and luminosity distribution within the clusters. Overall, our approach significantly improved the estimation of the tidal radius and detection of mass segregation compared to previous work. We found that in Collinder 463, low-mass stars do not segregate in comparison to high-mass and middle-mass stars. Additionally, we detected a peak of luminosity in the clusters, some of which were located far from the center, beyond the tidal radius.","sentences":["In our previous work, we introduced a method that combines two unsupervised algorithms: DBSCAN and GMM.","We applied this method to 12 open clusters based on Gaia EDR3 data, demonstrating its effectiveness in identifying reliable cluster members within the tidal radius.","However, for studying cluster morphology, we need a method capable of detecting members both inside and outside the tidal radius.","By incorporating a supervised algorithm into our approach, we successfully identified members beyond the tidal radius.","In our current work, we initially applied DBSCAN and GMM to identify reliable members of cluster stars.","Subsequently, we trained the Random Forest algorithm using DBSCAN and GMM-selected data.","Leveraging the random forest, we can identify cluster members outside the tidal radius and observe cluster morphology across a wide field of view.","Our method was then applied to 15 open clusters based on Gaia DR3, which exhibit a wide range of metallicity, distances, members, and ages.","Additionally, we calculated the tidal radius for each of the 15 clusters using the King profile and detected stars both inside and outside this radius.","Finally, we investigated mass segregation and luminosity distribution within the clusters.","Overall, our approach significantly improved the estimation of the tidal radius and detection of mass segregation compared to previous work.","We found that in Collinder 463, low-mass stars do not segregate in comparison to high-mass and middle-mass stars.","Additionally, we detected a peak of luminosity in the clusters, some of which were located far from the center, beyond the tidal radius."],"url":"http://arxiv.org/abs/2406.08450v1","category":"astro-ph.GA"}
{"created":"2024-06-12 17:31:36","title":"Transformation-Dependent Adversarial Attacks","abstract":"We introduce transformation-dependent adversarial attacks, a new class of threats where a single additive perturbation can trigger diverse, controllable mis-predictions by systematically transforming the input (e.g., scaling, blurring, compression). Unlike traditional attacks with static effects, our perturbations embed metamorphic properties to enable different adversarial attacks as a function of the transformation parameters. We demonstrate the transformation-dependent vulnerability across models (e.g., convolutional networks and vision transformers) and vision tasks (e.g., image classification and object detection). Our proposed geometric and photometric transformations enable a range of targeted errors from one crafted input (e.g., higher than 90% attack success rate for classifiers). We analyze effects of model architecture and type/variety of transformations on attack effectiveness. This work forces a paradigm shift by redefining adversarial inputs as dynamic, controllable threats. We highlight the need for robust defenses against such multifaceted, chameleon-like perturbations that current techniques are ill-prepared for.","sentences":["We introduce transformation-dependent adversarial attacks, a new class of threats where a single additive perturbation can trigger diverse, controllable mis-predictions by systematically transforming the input (e.g., scaling, blurring, compression).","Unlike traditional attacks with static effects, our perturbations embed metamorphic properties to enable different adversarial attacks as a function of the transformation parameters.","We demonstrate the transformation-dependent vulnerability across models (e.g., convolutional networks and vision transformers) and vision tasks (e.g., image classification and object detection).","Our proposed geometric and photometric transformations enable a range of targeted errors from one crafted input (e.g., higher than 90% attack success rate for classifiers).","We analyze effects of model architecture and type/variety of transformations on attack effectiveness.","This work forces a paradigm shift by redefining adversarial inputs as dynamic, controllable threats.","We highlight the need for robust defenses against such multifaceted, chameleon-like perturbations that current techniques are ill-prepared for."],"url":"http://arxiv.org/abs/2406.08443v1","category":"cs.CV"}
{"created":"2024-06-12 16:51:54","title":"Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models","abstract":"Large audio-language models (LALMs) enhance traditional large language models by integrating audio perception capabilities, allowing them to tackle audio-related tasks. Previous research has primarily focused on assessing the performance of LALMs across various tasks, yet overlooking their reliability, particularly concerning issues like object hallucination. In our study, we introduce methods to assess the extent of object hallucination of publicly available LALMs. Our findings reveal that LALMs are comparable to specialized audio captioning models in their understanding of audio content, but struggle to answer discriminative questions, specifically those requiring the identification of the presence of particular object sounds within an audio clip. This limitation highlights a critical weakness in current LALMs: their inadequate understanding of discriminative queries. Moreover, we explore the potential of prompt engineering to enhance LALMs' performance on discriminative questions.","sentences":["Large audio-language models (LALMs) enhance traditional large language models by integrating audio perception capabilities, allowing them to tackle audio-related tasks.","Previous research has primarily focused on assessing the performance of LALMs across various tasks, yet overlooking their reliability, particularly concerning issues like object hallucination.","In our study, we introduce methods to assess the extent of object hallucination of publicly available LALMs.","Our findings reveal that LALMs are comparable to specialized audio captioning models in their understanding of audio content, but struggle to answer discriminative questions, specifically those requiring the identification of the presence of particular object sounds within an audio clip.","This limitation highlights a critical weakness in current LALMs: their inadequate understanding of discriminative queries.","Moreover, we explore the potential of prompt engineering to enhance LALMs' performance on discriminative questions."],"url":"http://arxiv.org/abs/2406.08402v1","category":"eess.AS"}
{"created":"2024-06-12 16:43:55","title":"SCDNet: Self-supervised Learning Feature-based Speaker Change Detection","abstract":"Speaker Change Detection (SCD) is to identify boundaries among speakers in a conversation. Motivated by the success of fine-tuning wav2vec 2.0 models for the SCD task, a further investigation of self-supervised learning (SSL) features for SCD is conducted in this work. Specifically, an SCD model, named SCDNet, is proposed. With this model, various state-of-the-art SSL models, including Hubert, wav2vec 2.0, and WavLm are investigated. To discern the most potent layer of SSL models for SCD, a learnable weighting method is employed to analyze the effectiveness of intermediate representations. Additionally, a fine-tuning-based approach is also implemented to further compare the characteristics of SSL models in the SCD task. Furthermore, a contrastive learning method is proposed to mitigate the overfitting tendencies in the training of both the fine-tuning-based method and SCDNet. Experiments showcase the superiority of WavLm in the SCD task and also demonstrate the good design of SCDNet.","sentences":["Speaker Change Detection (SCD) is to identify boundaries among speakers in a conversation.","Motivated by the success of fine-tuning wav2vec 2.0 models for the SCD task, a further investigation of self-supervised learning (SSL) features for SCD is conducted in this work.","Specifically, an SCD model, named SCDNet, is proposed.","With this model, various state-of-the-art SSL models, including Hubert, wav2vec 2.0, and WavLm are investigated.","To discern the most potent layer of SSL models for SCD, a learnable weighting method is employed to analyze the effectiveness of intermediate representations.","Additionally, a fine-tuning-based approach is also implemented to further compare the characteristics of SSL models in the SCD task.","Furthermore, a contrastive learning method is proposed to mitigate the overfitting tendencies in the training of both the fine-tuning-based method and SCDNet.","Experiments showcase the superiority of WavLm in the SCD task and also demonstrate the good design of SCDNet."],"url":"http://arxiv.org/abs/2406.08393v1","category":"eess.AS"}
{"created":"2024-06-12 16:16:29","title":"Teaching Literature Reviewing for Software Engineering Research","abstract":"The goal of this chapter is to support teachers in holistically introducing graduate students to literature reviews, with a particular focus on secondary research. It provides an overview of the overall literature review process and the different types of literature review before diving into guidelines for selecting and conducting different types of literature review. The chapter also provides recommendations for evaluating the quality of existing literature reviews and concludes with a summary of our learning goals and how the chapter supports teachers in addressing them.","sentences":["The goal of this chapter is to support teachers in holistically introducing graduate students to literature reviews, with a particular focus on secondary research.","It provides an overview of the overall literature review process and the different types of literature review before diving into guidelines for selecting and conducting different types of literature review.","The chapter also provides recommendations for evaluating the quality of existing literature reviews and concludes with a summary of our learning goals and how the chapter supports teachers in addressing them."],"url":"http://arxiv.org/abs/2406.08369v1","category":"cs.SE"}
{"created":"2024-06-12 15:33:41","title":"Multimodal Representation Loss Between Timed Text and Audio for Regularized Speech Separation","abstract":"Recent studies highlight the potential of textual modalities in conditioning the speech separation model's inference process. However, regularization-based methods remain underexplored despite their advantages of not requiring auxiliary text data during the test time. To address this gap, we introduce a timed text-based regularization (TTR) method that uses language model-derived semantics to improve speech separation models. Our approach involves two steps. We begin with two pretrained audio and language models, WavLM and BERT, respectively. Then, a Transformer-based audio summarizer is learned to align the audio and word embeddings and to minimize their gap. The summarizer Transformer, incorporated as a regularizer, promotes the separated sources' alignment with the semantics from the timed text. Experimental results show that the proposed TTR method consistently improves the various objective metrics of the separation results over the unregularized baselines.","sentences":["Recent studies highlight the potential of textual modalities in conditioning the speech separation model's inference process.","However, regularization-based methods remain underexplored despite their advantages of not requiring auxiliary text data during the test time.","To address this gap, we introduce a timed text-based regularization (TTR) method that uses language model-derived semantics to improve speech separation models.","Our approach involves two steps.","We begin with two pretrained audio and language models, WavLM and BERT, respectively.","Then, a Transformer-based audio summarizer is learned to align the audio and word embeddings and to minimize their gap.","The summarizer Transformer, incorporated as a regularizer, promotes the separated sources' alignment with the semantics from the timed text.","Experimental results show that the proposed TTR method consistently improves the various objective metrics of the separation results over the unregularized baselines."],"url":"http://arxiv.org/abs/2406.08328v1","category":"eess.AS"}
{"created":"2024-06-12 15:00:16","title":"From Chaos to Clarity: 3DGS in the Dark","abstract":"Novel view synthesis from raw images provides superior high dynamic range (HDR) information compared to reconstructions from low dynamic range RGB images. However, the inherent noise in unprocessed raw images compromises the accuracy of 3D scene representation. Our study reveals that 3D Gaussian Splatting (3DGS) is particularly susceptible to this noise, leading to numerous elongated Gaussian shapes that overfit the noise, thereby significantly degrading reconstruction quality and reducing inference speed, especially in scenarios with limited views. To address these issues, we introduce a novel self-supervised learning framework designed to reconstruct HDR 3DGS from a limited number of noisy raw images. This framework enhances 3DGS by integrating a noise extractor and employing a noise-robust reconstruction loss that leverages a noise distribution prior. Experimental results show that our method outperforms LDR/HDR 3DGS and previous state-of-the-art (SOTA) self-supervised and supervised pre-trained models in both reconstruction quality and inference speed on the RawNeRF dataset across a broad range of training views. Code can be found in \\url{https://lizhihao6.github.io/Raw3DGS}.","sentences":["Novel view synthesis from raw images provides superior high dynamic range (HDR) information compared to reconstructions from low dynamic range RGB images.","However, the inherent noise in unprocessed raw images compromises the accuracy of 3D scene representation.","Our study reveals that 3D Gaussian Splatting (3DGS) is particularly susceptible to this noise, leading to numerous elongated Gaussian shapes that overfit the noise, thereby significantly degrading reconstruction quality and reducing inference speed, especially in scenarios with limited views.","To address these issues, we introduce a novel self-supervised learning framework designed to reconstruct HDR 3DGS from a limited number of noisy raw images.","This framework enhances 3DGS by integrating a noise extractor and employing a noise-robust reconstruction loss that leverages a noise distribution prior.","Experimental results show that our method outperforms LDR/HDR 3DGS and previous state-of-the-art (SOTA) self-supervised and supervised pre-trained models in both reconstruction quality and inference speed on the RawNeRF dataset across a broad range of training views.","Code can be found in \\url{https://lizhihao6.github.io/Raw3DGS}."],"url":"http://arxiv.org/abs/2406.08300v1","category":"eess.IV"}
{"created":"2024-06-12 13:49:38","title":"Figuratively Speaking: Authorship Attribution via Multi-Task Figurative Language Modeling","abstract":"The identification of Figurative Language (FL) features in text is crucial for various Natural Language Processing (NLP) tasks, where understanding of the author's intended meaning and its nuances is key for successful communication. At the same time, the use of a specific blend of various FL forms most accurately reflects a writer's style, rather than the use of any single construct, such as just metaphors or irony. Thus, we postulate that FL features could play an important role in Authorship Attribution (AA) tasks. We believe that our is the first computational study of AA based on FL use. Accordingly, we propose a Multi-task Figurative Language Model (MFLM) that learns to detect multiple FL features in text at once. We demonstrate, through detailed evaluation across multiple test sets, that the our model tends to perform equally or outperform specialized binary models in FL detection. Subsequently, we evaluate the predictive capability of joint FL features towards the AA task on three datasets, observing improved AA performance through the integration of MFLM embeddings.","sentences":["The identification of Figurative Language (FL) features in text is crucial for various Natural Language Processing (NLP) tasks, where understanding of the author's intended meaning and its nuances is key for successful communication.","At the same time, the use of a specific blend of various FL forms most accurately reflects a writer's style, rather than the use of any single construct, such as just metaphors or irony.","Thus, we postulate that FL features could play an important role in Authorship Attribution (AA) tasks.","We believe that our is the first computational study of AA based on FL use.","Accordingly, we propose a Multi-task Figurative Language Model (MFLM) that learns to detect multiple FL features in text at once.","We demonstrate, through detailed evaluation across multiple test sets, that the our model tends to perform equally or outperform specialized binary models in FL detection.","Subsequently, we evaluate the predictive capability of joint FL features towards the AA task on three datasets, observing improved AA performance through the integration of MFLM embeddings."],"url":"http://arxiv.org/abs/2406.08218v1","category":"cs.CL"}
{"created":"2024-06-12 13:45:47","title":"Runtime Freezing: Dynamic Class Loss for Multi-Organ 3D Segmentation","abstract":"Segmentation has become a crucial pre-processing step to many refined downstream tasks, and particularly so in the medical domain. Even with recent improvements in segmentation models, many segmentation tasks remain difficult. When multiple organs are segmented simultaneously, difficulties are due not only to the limited availability of labelled data, but also to class imbalance. In this work we propose dynamic class-based loss strategies to mitigate the effects of highly imbalanced training data. We show how our approach improves segmentation performance on a challenging Multi-Class 3D Abdominal Organ dataset.","sentences":["Segmentation has become a crucial pre-processing step to many refined downstream tasks, and particularly so in the medical domain.","Even with recent improvements in segmentation models, many segmentation tasks remain difficult.","When multiple organs are segmented simultaneously, difficulties are due not only to the limited availability of labelled data, but also to class imbalance.","In this work we propose dynamic class-based loss strategies to mitigate the effects of highly imbalanced training data.","We show how our approach improves segmentation performance on a challenging Multi-Class 3D Abdominal Organ dataset."],"url":"http://arxiv.org/abs/2406.08217v1","category":"cs.CV"}
{"created":"2024-06-12 13:40:47","title":"Forward-Euler time-discretization for Wasserstein gradient flows can be wrong","abstract":"In this note, we examine the forward-Euler discretization for simulating Wasserstein gradient flows. We provide two counter-examples showcasing the failure of this discretization even for a simple case where the energy functional is defined as the KL divergence against some nicely structured probability densities. A simple explanation of this failure is also discussed.","sentences":["In this note, we examine the forward-Euler discretization for simulating Wasserstein gradient flows.","We provide two counter-examples showcasing the failure of this discretization even for a simple case where the energy functional is defined as the KL divergence against some nicely structured probability densities.","A simple explanation of this failure is also discussed."],"url":"http://arxiv.org/abs/2406.08209v1","category":"stat.ML"}
{"created":"2024-06-12 13:38:48","title":"What do we know about Hugging Face? A systematic literature review and quantitative validation of qualitative claims","abstract":"Background: Collaborative Software Package Registries (SPRs) are an integral part of the software supply chain. Much engineering work synthesizes SPR package into applications. Prior research has examined SPRs for traditional software, such as NPM (JavaScript) and PyPI (Python). Pre-Trained Model (PTM) Registries are an emerging class of SPR of increasing importance, because they support the deep learning supply chain.   Aims: Recent empirical research has examined PTM registries in ways such as vulnerabilities, reuse processes, and evolution. However, no existing research synthesizes them to provide a systematic understanding of the current knowledge. Some of the existing research includes qualitative claims lacking quantitative analysis. Our research fills these gaps by providing a knowledge synthesis and quantitative analyses.   Methods: We first conduct a systematic literature review (SLR). We then observe that some of the claims are qualitative. We identify quantifiable metrics associated with those claims, and measure in order to substantiate these claims.   Results: From our SLR, we identify 12 claims about PTM reuse on the HuggingFace platform, 4 of which lack quantitative validation. We successfully test 3 of these claims through a quantitative analysis, and directly compare one with traditional software. Our findings corroborate qualitative claims with quantitative measurements. Our findings are: (1) PTMs have a much higher turnover rate than traditional software, indicating a dynamic and rapidly evolving reuse environment within the PTM ecosystem; and (2) There is a strong correlation between documentation quality and PTM popularity.   Conclusions: We confirm qualitative research claims with concrete metrics, supporting prior qualitative and case study research. Our measures show further dynamics of PTM reuse, inspiring research infrastructure and new measures.","sentences":["Background: Collaborative Software Package Registries (SPRs) are an integral part of the software supply chain.","Much engineering work synthesizes SPR package into applications.","Prior research has examined SPRs for traditional software, such as NPM (JavaScript) and PyPI (Python).","Pre-Trained Model (PTM) Registries are an emerging class of SPR of increasing importance, because they support the deep learning supply chain.   ","Aims:","Recent empirical research has examined PTM registries in ways such as vulnerabilities, reuse processes, and evolution.","However, no existing research synthesizes them to provide a systematic understanding of the current knowledge.","Some of the existing research includes qualitative claims lacking quantitative analysis.","Our research fills these gaps by providing a knowledge synthesis and quantitative analyses.   ","Methods: We first conduct a systematic literature review (SLR).","We then observe that some of the claims are qualitative.","We identify quantifiable metrics associated with those claims, and measure in order to substantiate these claims.   ","Results: From our SLR, we identify 12 claims about PTM reuse on the HuggingFace platform, 4 of which lack quantitative validation.","We successfully test 3 of these claims through a quantitative analysis, and directly compare one with traditional software.","Our findings corroborate qualitative claims with quantitative measurements.","Our findings are: (1) PTMs have a much higher turnover rate than traditional software, indicating a dynamic and rapidly evolving reuse environment within the PTM ecosystem; and (2) There is a strong correlation between documentation quality and PTM popularity.   ","Conclusions: We confirm qualitative research claims with concrete metrics, supporting prior qualitative and case study research.","Our measures show further dynamics of PTM reuse, inspiring research infrastructure and new measures."],"url":"http://arxiv.org/abs/2406.08205v1","category":"cs.SE"}
{"created":"2024-06-12 13:14:19","title":"Underneath the Numbers: Quantitative and Qualitative Gender Fairness in LLMs for Depression Prediction","abstract":"Recent studies show bias in many machine learning models for depression detection, but bias in LLMs for this task remains unexplored. This work presents the first attempt to investigate the degree of gender bias present in existing LLMs (ChatGPT, LLaMA 2, and Bard) using both quantitative and qualitative approaches. From our quantitative evaluation, we found that ChatGPT performs the best across various performance metrics and LLaMA 2 outperforms other LLMs in terms of group fairness metrics. As qualitative fairness evaluation remains an open research question we propose several strategies (e.g., word count, thematic analysis) to investigate whether and how a qualitative evaluation can provide valuable insights for bias analysis beyond what is possible with quantitative evaluation. We found that ChatGPT consistently provides a more comprehensive, well-reasoned explanation for its prediction compared to LLaMA 2. We have also identified several themes adopted by LLMs to qualitatively evaluate gender fairness. We hope our results can be used as a stepping stone towards future attempts at improving qualitative evaluation of fairness for LLMs especially for high-stakes tasks such as depression detection.","sentences":["Recent studies show bias in many machine learning models for depression detection, but bias in LLMs for this task remains unexplored.","This work presents the first attempt to investigate the degree of gender bias present in existing LLMs (ChatGPT, LLaMA 2, and Bard) using both quantitative and qualitative approaches.","From our quantitative evaluation, we found that ChatGPT performs the best across various performance metrics and LLaMA 2 outperforms other LLMs in terms of group fairness metrics.","As qualitative fairness evaluation remains an open research question we propose several strategies (e.g., word count, thematic analysis) to investigate whether and how a qualitative evaluation can provide valuable insights for bias analysis beyond what is possible with quantitative evaluation.","We found that ChatGPT consistently provides a more comprehensive, well-reasoned explanation for its prediction compared to LLaMA 2.","We have also identified several themes adopted by LLMs to qualitatively evaluate gender fairness.","We hope our results can be used as a stepping stone towards future attempts at improving qualitative evaluation of fairness for LLMs especially for high-stakes tasks such as depression detection."],"url":"http://arxiv.org/abs/2406.08183v1","category":"cs.CL"}
{"created":"2024-06-12 13:05:27","title":"Semi-Supervised Spoken Language Glossification","abstract":"Spoken language glossification (SLG) aims to translate the spoken language text into the sign language gloss, i.e., a written record of sign language. In this work, we present a framework named $S$emi-$S$upervised $S$poken $L$anguage $G$lossification ($S^3$LG) for SLG. To tackle the bottleneck of limited parallel data in SLG, our $S^3$LG incorporates large-scale monolingual spoken language text into SLG training. The proposed framework follows the self-training structure that iteratively annotates and learns from pseudo labels. Considering the lexical similarity and syntactic difference between sign language and spoken language, our $S^3$LG adopts both the rule-based heuristic and model-based approach for auto-annotation. During training, we randomly mix these complementary synthetic datasets and mark their differences with a special token. As the synthetic data may be less quality, the $S^3$LG further leverages consistency regularization to reduce the negative impact of noise in the synthetic data. Extensive experiments are conducted on public benchmarks to demonstrate the effectiveness of the $S^3$LG. Our code is available at \\url{https://github.com/yaohj11/S3LG}.","sentences":["Spoken language glossification (SLG) aims to translate the spoken language text into the sign language gloss, i.e., a written record of sign language.","In this work, we present a framework named $S$emi-$S$upervised $S$poken $L$anguage $G$lossification ($S^3$LG) for SLG.","To tackle the bottleneck of limited parallel data in SLG, our $S^3$LG incorporates large-scale monolingual spoken language text into SLG training.","The proposed framework follows the self-training structure that iteratively annotates and learns from pseudo labels.","Considering the lexical similarity and syntactic difference between sign language and spoken language, our $S^3$LG adopts both the rule-based heuristic and model-based approach for auto-annotation.","During training, we randomly mix these complementary synthetic datasets and mark their differences with a special token.","As the synthetic data may be less quality, the $S^3$LG further leverages consistency regularization to reduce the negative impact of noise in the synthetic data.","Extensive experiments are conducted on public benchmarks to demonstrate the effectiveness of the $S^3$LG.","Our code is available at \\url{https://github.com/yaohj11/S3LG}."],"url":"http://arxiv.org/abs/2406.08173v1","category":"cs.CL"}
{"created":"2024-06-12 12:51:20","title":"Chemistry3D: Robotic Interaction Benchmark for Chemistry Experiments","abstract":"The advent of simulation engines has revolutionized learning and operational efficiency for robots, offering cost-effective and swift pipelines. However, the lack of a universal simulation platform tailored for chemical scenarios impedes progress in robotic manipulation and visualization of reaction processes. Addressing this void, we present Chemistry3D, an innovative toolkit that integrates extensive chemical and robotic knowledge. Chemistry3D not only enables robots to perform chemical experiments but also provides real-time visualization of temperature, color, and pH changes during reactions. Built on the NVIDIA Omniverse platform, Chemistry3D offers interfaces for robot operation, visual inspection, and liquid flow control, facilitating the simulation of special objects such as liquids and transparent entities. Leveraging this toolkit, we have devised RL tasks, object detection, and robot operation scenarios. Additionally, to discern disparities between the rendering engine and the real world, we conducted transparent object detection experiments using Sim2Real, validating the toolkit's exceptional simulation performance. The source code is available at https://github.com/huangyan28/Chemistry3D, and a related tutorial can be found at https://www.omni-chemistry.com.","sentences":["The advent of simulation engines has revolutionized learning and operational efficiency for robots, offering cost-effective and swift pipelines.","However, the lack of a universal simulation platform tailored for chemical scenarios impedes progress in robotic manipulation and visualization of reaction processes.","Addressing this void, we present Chemistry3D, an innovative toolkit that integrates extensive chemical and robotic knowledge.","Chemistry3D","not only enables robots to perform chemical experiments but also provides real-time visualization of temperature, color, and pH changes during reactions.","Built on the NVIDIA Omniverse platform, Chemistry3D offers interfaces for robot operation, visual inspection, and liquid flow control, facilitating the simulation of special objects such as liquids and transparent entities.","Leveraging this toolkit, we have devised RL tasks, object detection, and robot operation scenarios.","Additionally, to discern disparities between the rendering engine and the real world, we conducted transparent object detection experiments using Sim2Real, validating the toolkit's exceptional simulation performance.","The source code is available at https://github.com/huangyan28/Chemistry3D, and a related tutorial can be found at https://www.omni-chemistry.com."],"url":"http://arxiv.org/abs/2406.08160v1","category":"cs.RO"}
{"created":"2024-06-12 10:41:14","title":"US College Net Price Prediction Comparing ML Regression Models","abstract":"This paper will illustrate the usage of Machine Learning algorithms on US College Scorecard datasets. For this paper, we will use our knowledge, research, and development of a predictive model to compare the results of all the models and predict the public and private net prices. This paper focuses on analyzing US College Scorecard data from data published on government websites.   Our goal is to use four machine learning regression models to develop a predictive model to forecast the equitable net cost for every college, encompassing both public institutions and private, whether for-profit or nonprofit.","sentences":["This paper will illustrate the usage of Machine Learning algorithms on US College Scorecard datasets.","For this paper, we will use our knowledge, research, and development of a predictive model to compare the results of all the models and predict the public and private net prices.","This paper focuses on analyzing US College Scorecard data from data published on government websites.   ","Our goal is to use four machine learning regression models to develop a predictive model to forecast the equitable net cost for every college, encompassing both public institutions and private, whether for-profit or nonprofit."],"url":"http://arxiv.org/abs/2406.08071v1","category":"cs.CY"}
{"created":"2024-06-12 10:26:52","title":"MWIRSTD: A MWIR Small Target Detection Dataset","abstract":"This paper presents a novel mid-wave infrared (MWIR) small target detection dataset (MWIRSTD) comprising 14 video sequences containing approximately 1053 images with annotated targets of three distinct classes of small objects. Captured using cooled MWIR imagers, the dataset offers a unique opportunity for researchers to develop and evaluate state-of-the-art methods for small object detection in realistic MWIR scenes. Unlike existing datasets, which primarily consist of uncooled thermal images or synthetic data with targets superimposed onto the background or vice versa, MWIRSTD provides authentic MWIR data with diverse targets and environments. Extensive experiments on various traditional methods and deep learning-based techniques for small target detection are performed on the proposed dataset, providing valuable insights into their efficacy. The dataset and code are available at https://github.com/avinres/MWIRSTD.","sentences":["This paper presents a novel mid-wave infrared (MWIR) small target detection dataset (MWIRSTD) comprising 14 video sequences containing approximately 1053 images with annotated targets of three distinct classes of small objects.","Captured using cooled MWIR imagers, the dataset offers a unique opportunity for researchers to develop and evaluate state-of-the-art methods for small object detection in realistic MWIR scenes.","Unlike existing datasets, which primarily consist of uncooled thermal images or synthetic data with targets superimposed onto the background or vice versa, MWIRSTD provides authentic MWIR data with diverse targets and environments.","Extensive experiments on various traditional methods and deep learning-based techniques for small target detection are performed on the proposed dataset, providing valuable insights into their efficacy.","The dataset and code are available at https://github.com/avinres/MWIRSTD."],"url":"http://arxiv.org/abs/2406.08063v1","category":"cs.CV"}
{"created":"2024-06-12 10:12:52","title":"Learning Job Title Representation from Job Description Aggregation Network","abstract":"Learning job title representation is a vital process for developing automatic human resource tools. To do so, existing methods primarily rely on learning the title representation through skills extracted from the job description, neglecting the rich and diverse content within. Thus, we propose an alternative framework for learning job titles through their respective job description (JD) and utilize a Job Description Aggregator component to handle the lengthy description and bidirectional contrastive loss to account for the bidirectional relationship between the job title and its description. We evaluated the performance of our method on both in-domain and out-of-domain settings, achieving a superior performance over the skill-based approach.","sentences":["Learning job title representation is a vital process for developing automatic human resource tools.","To do so, existing methods primarily rely on learning the title representation through skills extracted from the job description, neglecting the rich and diverse content within.","Thus, we propose an alternative framework for learning job titles through their respective job description (JD) and utilize a Job Description Aggregator component to handle the lengthy description and bidirectional contrastive loss to account for the bidirectional relationship between the job title and its description.","We evaluated the performance of our method on both in-domain and out-of-domain settings, achieving a superior performance over the skill-based approach."],"url":"http://arxiv.org/abs/2406.08055v1","category":"cs.CL"}
{"created":"2024-06-12 09:53:14","title":"A novel approach to graph distinction through GENEOs and permutants","abstract":"The theory of Group Equivariant Non-Expansive Operators (GENEOs) was initially developed in Topological Data Analysis for the geometric approximation of data observers, including their invariances and symmetries. This paper departs from that line of research and explores the use of GENEOs for distinguishing $r$-regular graphs up to isomorphisms. In doing so, we aim to test the capabilities and flexibility of these operators. Our experiments show that GENEOs offer a good compromise between efficiency and computational cost in comparing $r$-regular graphs, while their actions on data are easily interpretable. This supports the idea that GENEOs could be a general-purpose approach to discriminative problems in Machine Learning when some structural information about data and observers is explicitly given.","sentences":["The theory of Group Equivariant Non-Expansive Operators (GENEOs) was initially developed in Topological Data Analysis for the geometric approximation of data observers, including their invariances and symmetries.","This paper departs from that line of research and explores the use of GENEOs for distinguishing $r$-regular graphs up to isomorphisms.","In doing so, we aim to test the capabilities and flexibility of these operators.","Our experiments show that GENEOs offer a good compromise between efficiency and computational cost in comparing $r$-regular graphs, while their actions on data are easily interpretable.","This supports the idea that GENEOs could be a general-purpose approach to discriminative problems in Machine Learning when some structural information about data and observers is explicitly given."],"url":"http://arxiv.org/abs/2406.08045v1","category":"cs.LG"}
{"created":"2024-06-12 09:51:29","title":"Efficient Network Traffic Feature Sets for IoT Intrusion Detection","abstract":"The use of Machine Learning (ML) models in cybersecurity solutions requires high-quality data that is stripped of redundant, missing, and noisy information. By selecting the most relevant features, data integrity and model efficiency can be significantly improved. This work evaluates the feature sets provided by a combination of different feature selection methods, namely Information Gain, Chi-Squared Test, Recursive Feature Elimination, Mean Absolute Deviation, and Dispersion Ratio, in multiple IoT network datasets. The influence of the smaller feature sets on both the classification performance and the training time of ML models is compared, with the aim of increasing the computational efficiency of IoT intrusion detection. Overall, the most impactful features of each dataset were identified, and the ML models obtained higher computational efficiency while preserving a good generalization, showing little to no difference between the sets.","sentences":["The use of Machine Learning (ML) models in cybersecurity solutions requires high-quality data that is stripped of redundant, missing, and noisy information.","By selecting the most relevant features, data integrity and model efficiency can be significantly improved.","This work evaluates the feature sets provided by a combination of different feature selection methods, namely Information Gain, Chi-Squared Test, Recursive Feature Elimination, Mean Absolute Deviation, and Dispersion Ratio, in multiple IoT network datasets.","The influence of the smaller feature sets on both the classification performance and the training time of ML models is compared, with the aim of increasing the computational efficiency of IoT intrusion detection.","Overall, the most impactful features of each dataset were identified, and the ML models obtained higher computational efficiency while preserving a good generalization, showing little to no difference between the sets."],"url":"http://arxiv.org/abs/2406.08042v1","category":"cs.CR"}
{"created":"2024-06-12 09:50:41","title":"HARd to Beat: The Overlooked Impact of Rolling Windows in the Era of Machine Learning","abstract":"We investigate the predictive abilities of the heterogeneous autoregressive (HAR) model compared to machine learning (ML) techniques across an unprecedented dataset of 1,455 stocks. Our analysis focuses on the role of fitting schemes, particularly the training window and re-estimation frequency, in determining the HAR model's performance. Despite extensive hyperparameter tuning, ML models fail to surpass the linear benchmark set by HAR when utilizing a refined fitting approach for the latter. Moreover, the simplicity of HAR allows for an interpretable model with drastically lower computational costs. We assess performance using QLIKE, MSE, and realized utility metrics, finding that HAR consistently outperforms its ML counterparts when both rely solely on realized volatility and VIX as predictors. Our results underscore the importance of a correctly specified fitting scheme. They suggest that properly fitted HAR models provide superior forecasting accuracy, establishing robust guidelines for their practical application and use as a benchmark. This study not only reaffirms the efficacy of the HAR model but also provides a critical perspective on the practical limitations of ML approaches in realized volatility forecasting.","sentences":["We investigate the predictive abilities of the heterogeneous autoregressive (HAR) model compared to machine learning (ML) techniques across an unprecedented dataset of 1,455 stocks.","Our analysis focuses on the role of fitting schemes, particularly the training window and re-estimation frequency, in determining the HAR model's performance.","Despite extensive hyperparameter tuning, ML models fail to surpass the linear benchmark set by HAR when utilizing a refined fitting approach for the latter.","Moreover, the simplicity of HAR allows for an interpretable model with drastically lower computational costs.","We assess performance using QLIKE, MSE, and realized utility metrics, finding that HAR consistently outperforms its ML counterparts when both rely solely on realized volatility and VIX as predictors.","Our results underscore the importance of a correctly specified fitting scheme.","They suggest that properly fitted HAR models provide superior forecasting accuracy, establishing robust guidelines for their practical application and use as a benchmark.","This study not only reaffirms the efficacy of the HAR model but also provides a critical perspective on the practical limitations of ML approaches in realized volatility forecasting."],"url":"http://arxiv.org/abs/2406.08041v1","category":"q-fin.ST"}
{"created":"2024-06-12 08:47:44","title":"Asymptotic Unbiased Sample Sampling to Speed Up Sharpness-Aware Minimization","abstract":"Sharpness-Aware Minimization (SAM) has emerged as a promising approach for effectively reducing the generalization error. However, SAM incurs twice the computational cost compared to base optimizer (e.g., SGD). We propose Asymptotic Unbiased Sampling with respect to iterations to accelerate SAM (AUSAM), which maintains the model's generalization capacity while significantly enhancing computational efficiency. Concretely, we probabilistically sample a subset of data points beneficial for SAM optimization based on a theoretically guaranteed criterion, i.e., the Gradient Norm of each Sample (GNS). We further approximate the GNS by the difference in loss values before and after perturbation in SAM. As a plug-and-play, architecture-agnostic method, our approach consistently accelerates SAM across a range of tasks and networks, i.e., classification, human pose estimation and network quantization. On CIFAR10/100 and Tiny-ImageNet, AUSAM achieves results comparable to SAM while providing a speedup of over 70%. Compared to recent dynamic data pruning methods, AUSAM is better suited for SAM and excels in maintaining performance. Additionally, AUSAM accelerates optimization in human pose estimation and model quantization without sacrificing performance, demonstrating its broad practicality.","sentences":["Sharpness-Aware Minimization (SAM) has emerged as a promising approach for effectively reducing the generalization error.","However, SAM incurs twice the computational cost compared to base optimizer (e.g., SGD).","We propose Asymptotic Unbiased Sampling with respect to iterations to accelerate SAM (AUSAM), which maintains the model's generalization capacity while significantly enhancing computational efficiency.","Concretely, we probabilistically sample a subset of data points beneficial for SAM optimization based on a theoretically guaranteed criterion, i.e., the Gradient Norm of each Sample (GNS).","We further approximate the GNS by the difference in loss values before and after perturbation in SAM.","As a plug-and-play, architecture-agnostic method, our approach consistently accelerates SAM across a range of tasks and networks, i.e., classification, human pose estimation and network quantization.","On CIFAR10/100 and Tiny-ImageNet, AUSAM achieves results comparable to SAM while providing a speedup of over 70%.","Compared to recent dynamic data pruning methods, AUSAM is better suited for SAM and excels in maintaining performance.","Additionally, AUSAM accelerates optimization in human pose estimation and model quantization without sacrificing performance, demonstrating its broad practicality."],"url":"http://arxiv.org/abs/2406.08001v1","category":"cs.CV"}
{"created":"2024-06-12 08:30:16","title":"Interpetable Target-Feature Aggregation for Multi-Task Learning based on Bias-Variance Analysis","abstract":"Multi-task learning (MTL) is a powerful machine learning paradigm designed to leverage shared knowledge across tasks to improve generalization and performance. Previous works have proposed approaches to MTL that can be divided into feature learning, focused on the identification of a common feature representation, and task clustering, where similar tasks are grouped together. In this paper, we propose an MTL approach at the intersection between task clustering and feature transformation based on a two-phase iterative aggregation of targets and features. First, we propose a bias-variance analysis for regression models with additive Gaussian noise, where we provide a general expression of the asymptotic bias and variance of a task, considering a linear regression trained on aggregated input features and an aggregated target. Then, we exploit this analysis to provide a two-phase MTL algorithm (NonLinCTFA). Firstly, this method partitions the tasks into clusters and aggregates each obtained group of targets with their mean. Then, for each aggregated task, it aggregates subsets of features with their mean in a dimensionality reduction fashion. In both phases, a key aspect is to preserve the interpretability of the reduced targets and features through the aggregation with the mean, which is further motivated by applications to Earth science. Finally, we validate the algorithms on synthetic data, showing the effect of different parameters and real-world datasets, exploring the validity of the proposed methodology on classical datasets, recent baselines, and Earth science applications.","sentences":["Multi-task learning (MTL) is a powerful machine learning paradigm designed to leverage shared knowledge across tasks to improve generalization and performance.","Previous works have proposed approaches to MTL that can be divided into feature learning, focused on the identification of a common feature representation, and task clustering, where similar tasks are grouped together.","In this paper, we propose an MTL approach at the intersection between task clustering and feature transformation based on a two-phase iterative aggregation of targets and features.","First, we propose a bias-variance analysis for regression models with additive Gaussian noise, where we provide a general expression of the asymptotic bias and variance of a task, considering a linear regression trained on aggregated input features and an aggregated target.","Then, we exploit this analysis to provide a two-phase MTL algorithm (NonLinCTFA).","Firstly, this method partitions the tasks into clusters and aggregates each obtained group of targets with their mean.","Then, for each aggregated task, it aggregates subsets of features with their mean in a dimensionality reduction fashion.","In both phases, a key aspect is to preserve the interpretability of the reduced targets and features through the aggregation with the mean, which is further motivated by applications to Earth science.","Finally, we validate the algorithms on synthetic data, showing the effect of different parameters and real-world datasets, exploring the validity of the proposed methodology on classical datasets, recent baselines, and Earth science applications."],"url":"http://arxiv.org/abs/2406.07991v1","category":"cs.LG"}
{"created":"2024-06-12 08:17:06","title":"SimSAM: Simple Siamese Representations Based Semantic Affinity Matrix for Unsupervised Image Segmentation","abstract":"Recent developments in self-supervised learning (SSL) have made it possible to learn data representations without the need for annotations. Inspired by the non-contrastive SSL approach (SimSiam), we introduce a novel framework SIMSAM to compute the Semantic Affinity Matrix, which is significant for unsupervised image segmentation. Given an image, SIMSAM first extracts features using pre-trained DINO-ViT, then projects the features to predict the correlations of dense features in a non-contrastive way. We show applications of the Semantic Affinity Matrix in object segmentation and semantic segmentation tasks. Our code is available at https://github.com/chandagrover/SimSAM.","sentences":["Recent developments in self-supervised learning (SSL) have made it possible to learn data representations without the need for annotations.","Inspired by the non-contrastive SSL approach (SimSiam), we introduce a novel framework SIMSAM to compute the Semantic Affinity Matrix, which is significant for unsupervised image segmentation.","Given an image, SIMSAM first extracts features using pre-trained DINO-ViT, then projects the features to predict the correlations of dense features in a non-contrastive way.","We show applications of the Semantic Affinity Matrix in object segmentation and semantic segmentation tasks.","Our code is available at https://github.com/chandagrover/SimSAM."],"url":"http://arxiv.org/abs/2406.07986v1","category":"cs.CV"}
{"created":"2024-06-12 07:49:36","title":"Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation","abstract":"The quality of output from large language models (LLMs), particularly in machine translation (MT), is closely tied to the quality of in-context examples (ICEs) provided along with the query, i.e., the text to translate. The effectiveness of these ICEs is influenced by various factors, such as the domain of the source text, the order in which the ICEs are presented, the number of these examples, and the prompt templates used. Naturally, selecting the most impactful ICEs depends on understanding how these affect the resulting translation quality, which ultimately relies on translation references or human judgment. This paper presents a novel methodology for in-context learning (ICL) that relies on a search algorithm guided by domain-specific quality estimation (QE). Leveraging the XGLM model, our methodology estimates the resulting translation quality without the need for translation references, selecting effective ICEs for MT to maximize translation quality. Our results demonstrate significant improvements over existing ICL methods and higher translation performance compared to fine-tuning a pre-trained language model (PLM), specifically mBART-50.","sentences":["The quality of output from large language models (LLMs), particularly in machine translation (MT), is closely tied to the quality of in-context examples (ICEs) provided along with the query, i.e., the text to translate.","The effectiveness of these ICEs is influenced by various factors, such as the domain of the source text, the order in which the ICEs are presented, the number of these examples, and the prompt templates used.","Naturally, selecting the most impactful ICEs depends on understanding how these affect the resulting translation quality, which ultimately relies on translation references or human judgment.","This paper presents a novel methodology for in-context learning (ICL) that relies on a search algorithm guided by domain-specific quality estimation (QE).","Leveraging the XGLM model, our methodology estimates the resulting translation quality without the need for translation references, selecting effective ICEs for MT to maximize translation quality.","Our results demonstrate significant improvements over existing ICL methods and higher translation performance compared to fine-tuning a pre-trained language model (PLM), specifically mBART-50."],"url":"http://arxiv.org/abs/2406.07970v1","category":"cs.CL"}
{"created":"2024-06-12 07:49:21","title":"LibriTTS-P: A Corpus with Speaking Style and Speaker Identity Prompts for Text-to-Speech and Style Captioning","abstract":"We introduce LibriTTS-P, a new corpus based on LibriTTS-R that includes utterance-level descriptions (i.e., prompts) of speaking style and speaker-level prompts of speaker characteristics. We employ a hybrid approach to construct prompt annotations: (1) manual annotations that capture human perceptions of speaker characteristics and (2) synthetic annotations on speaking style. Compared to existing English prompt datasets, our corpus provides more diverse prompt annotations for all speakers of LibriTTS-R. Experimental results for prompt-based controllable TTS demonstrate that the TTS model trained with LibriTTS-P achieves higher naturalness than the model using the conventional dataset. Furthermore, the results for style captioning tasks show that the model utilizing LibriTTS-P generates 2.5 times more accurate words than the model using a conventional dataset. Our corpus, LibriTTS-P, is available at https://github.com/line/LibriTTS-P.","sentences":["We introduce LibriTTS-P, a new corpus based on LibriTTS-R that includes utterance-level descriptions (i.e., prompts) of speaking style and speaker-level prompts of speaker characteristics.","We employ a hybrid approach to construct prompt annotations: (1) manual annotations that capture human perceptions of speaker characteristics and (2) synthetic annotations on speaking style.","Compared to existing English prompt datasets, our corpus provides more diverse prompt annotations for all speakers of LibriTTS-R. Experimental results for prompt-based controllable TTS demonstrate that the TTS model trained with LibriTTS-P achieves higher naturalness than the model using the conventional dataset.","Furthermore, the results for style captioning tasks show that the model utilizing LibriTTS-P generates 2.5 times more accurate words than the model using a conventional dataset.","Our corpus, LibriTTS-P, is available at https://github.com/line/LibriTTS-P."],"url":"http://arxiv.org/abs/2406.07969v1","category":"eess.AS"}
{"created":"2024-06-12 07:22:05","title":"Spatial-Frequency Dual Progressive Attention Network For Medical Image Segmentation","abstract":"In medical images, various types of lesions often manifest significant differences in their shape and texture. Accurate medical image segmentation demands deep learning models with robust capabilities in multi-scale and boundary feature learning. However, previous networks still have limitations in addressing the above issues. Firstly, previous networks simultaneously fuse multi-level features or employ deep supervision to enhance multi-scale learning. However, this may lead to feature redundancy and excessive computational overhead, which is not conducive to network training and clinical deployment. Secondly, the majority of medical image segmentation networks exclusively learn features in the spatial domain, disregarding the abundant global information in the frequency domain. This results in a bias towards low-frequency components, neglecting crucial high-frequency information. To address these problems, we introduce SF-UNet, a spatial-frequency dual-domain attention network. It comprises two main components: the Multi-scale Progressive Channel Attention (MPCA) block, which progressively extract multi-scale features across adjacent encoder layers, and the lightweight Frequency-Spatial Attention (FSA) block, with only 0.05M parameters, enabling concurrent learning of texture and boundary features from both spatial and frequency domains. We validate the effectiveness of the proposed SF-UNet on three public datasets. Experimental results show that compared to previous state-of-the-art (SOTA) medical image segmentation networks, SF-UNet achieves the best performance, and achieves up to 9.4\\% and 10.78\\% improvement in DSC and IOU. Codes will be released at https://github.com/nkicsl/SF-UNet.","sentences":["In medical images, various types of lesions often manifest significant differences in their shape and texture.","Accurate medical image segmentation demands deep learning models with robust capabilities in multi-scale and boundary feature learning.","However, previous networks still have limitations in addressing the above issues.","Firstly, previous networks simultaneously fuse multi-level features or employ deep supervision to enhance multi-scale learning.","However, this may lead to feature redundancy and excessive computational overhead, which is not conducive to network training and clinical deployment.","Secondly, the majority of medical image segmentation networks exclusively learn features in the spatial domain, disregarding the abundant global information in the frequency domain.","This results in a bias towards low-frequency components, neglecting crucial high-frequency information.","To address these problems, we introduce SF-UNet, a spatial-frequency dual-domain attention network.","It comprises two main components: the Multi-scale Progressive Channel Attention (MPCA) block, which progressively extract multi-scale features across adjacent encoder layers, and the lightweight Frequency-Spatial Attention (FSA) block, with only 0.05M parameters, enabling concurrent learning of texture and boundary features from both spatial and frequency domains.","We validate the effectiveness of the proposed SF-UNet on three public datasets.","Experimental results show that compared to previous state-of-the-art (SOTA) medical image segmentation networks, SF-UNet achieves the best performance, and achieves up to 9.4\\% and 10.78\\% improvement in DSC and IOU.","Codes will be released at https://github.com/nkicsl/SF-UNet."],"url":"http://arxiv.org/abs/2406.07952v1","category":"eess.IV"}
{"created":"2024-06-12 06:55:35","title":"Counteracting Duration Bias in Video Recommendation via Counterfactual Watch Time","abstract":"In video recommendation, an ongoing effort is to satisfy users' personalized information needs by leveraging their logged watch time. However, watch time prediction suffers from duration bias, hindering its ability to reflect users' interests accurately. Existing label-correction approaches attempt to uncover user interests through grouping and normalizing observed watch time according to video duration. Although effective to some extent, we found that these approaches regard completely played records (i.e., a user watches the entire video) as equally high interest, which deviates from what we observed on real datasets: users have varied explicit feedback proportion when completely playing videos. In this paper, we introduce the counterfactual watch time(CWT), the potential watch time a user would spend on the video if its duration is sufficiently long. Analysis shows that the duration bias is caused by the truncation of CWT due to the video duration limitation, which usually occurs on those completely played records. Besides, a Counterfactual Watch Model (CWM) is proposed, revealing that CWT equals the time users get the maximum benefit from video recommender systems. Moreover, a cost-based transform function is defined to transform the CWT into the estimation of user interest, and the model can be learned by optimizing a counterfactual likelihood function defined over observed user watch times. Extensive experiments on three real video recommendation datasets and online A/B testing demonstrated that CWM effectively enhanced video recommendation accuracy and counteracted the duration bias.","sentences":["In video recommendation, an ongoing effort is to satisfy users' personalized information needs by leveraging their logged watch time.","However, watch time prediction suffers from duration bias, hindering its ability to reflect users' interests accurately.","Existing label-correction approaches attempt to uncover user interests through grouping and normalizing observed watch time according to video duration.","Although effective to some extent, we found that these approaches regard completely played records (i.e., a user watches the entire video) as equally high interest, which deviates from what we observed on real datasets: users have varied explicit feedback proportion when completely playing videos.","In this paper, we introduce the counterfactual watch time(CWT), the potential watch time a user would spend on the video if its duration is sufficiently long.","Analysis shows that the duration bias is caused by the truncation of CWT due to the video duration limitation, which usually occurs on those completely played records.","Besides, a Counterfactual Watch Model (CWM) is proposed, revealing that CWT equals the time users get the maximum benefit from video recommender systems.","Moreover, a cost-based transform function is defined to transform the CWT into the estimation of user interest, and the model can be learned by optimizing a counterfactual likelihood function defined over observed user watch times.","Extensive experiments on three real video recommendation datasets and online A/B testing demonstrated that CWM effectively enhanced video recommendation accuracy and counteracted the duration bias."],"url":"http://arxiv.org/abs/2406.07932v1","category":"cs.IR"}
{"created":"2024-06-12 06:46:00","title":"Undergraduate Robotics Education with General Instructors using a Student-Centered Personalized Learning Framework","abstract":"Recent advancements in robotics, including applications like self-driving cars, unmanned systems, and medical robots, have had a significant impact on the job market. On one hand, big robotics companies offer training programs based on the job requirements. However, these training programs may not be as beneficial as general robotics programs offered by universities or community colleges. On the other hand, community colleges and universities face challenges with required resources, especially qualified instructors, to offer students advanced robotics education. Furthermore, the diverse backgrounds of undergraduate students present additional challenges. Some students bring extensive industry experiences, while others are newcomers to the field. To address these challenges, we propose a student-centered personalized learning framework for robotics. This framework allows a general instructor to teach undergraduate-level robotics courses by breaking down course topics into smaller components with well-defined topic dependencies, structured as a graph. This modular approach enables students to choose their learning path, catering to their unique preferences and pace. Moreover, our framework's flexibility allows for easy customization of teaching materials to meet the specific needs of host institutions. In addition to teaching materials, a frequently-asked-questions document would be prepared for a general instructor. If students' robotics questions cannot be answered by the instructor, the answers to these questions may be included in this document. For questions not covered in this document, we can gather and address them through collaboration with the robotics community and course content creators. Our user study results demonstrate the promise of this method in delivering undergraduate-level robotics education tailored to individual learning outcomes and preferences.","sentences":["Recent advancements in robotics, including applications like self-driving cars, unmanned systems, and medical robots, have had a significant impact on the job market.","On one hand, big robotics companies offer training programs based on the job requirements.","However, these training programs may not be as beneficial as general robotics programs offered by universities or community colleges.","On the other hand, community colleges and universities face challenges with required resources, especially qualified instructors, to offer students advanced robotics education.","Furthermore, the diverse backgrounds of undergraduate students present additional challenges.","Some students bring extensive industry experiences, while others are newcomers to the field.","To address these challenges, we propose a student-centered personalized learning framework for robotics.","This framework allows a general instructor to teach undergraduate-level robotics courses by breaking down course topics into smaller components with well-defined topic dependencies, structured as a graph.","This modular approach enables students to choose their learning path, catering to their unique preferences and pace.","Moreover, our framework's flexibility allows for easy customization of teaching materials to meet the specific needs of host institutions.","In addition to teaching materials, a frequently-asked-questions document would be prepared for a general instructor.","If students' robotics questions cannot be answered by the instructor, the answers to these questions may be included in this document.","For questions not covered in this document, we can gather and address them through collaboration with the robotics community and course content creators.","Our user study results demonstrate the promise of this method in delivering undergraduate-level robotics education tailored to individual learning outcomes and preferences."],"url":"http://arxiv.org/abs/2406.07928v1","category":"cs.RO"}
{"created":"2024-06-12 06:35:19","title":"Aggregation Design for Personalized Federated Multi-Modal Learning over Wireless Networks","abstract":"Federated Multi-Modal Learning (FMML) is an emerging field that integrates information from different modalities in federated learning to improve the learning performance. In this letter, we develop a parameter scheduling scheme to improve personalized performance and communication efficiency in personalized FMML, considering the non-independent and nonidentically distributed (non-IID) data along with the modality heterogeneity. Specifically, a learning-based approach is utilized to obtain the aggregation coefficients for parameters of different modalities on distinct devices. Based on the aggregation coefficients and channel state, a subset of parameters is scheduled to be uploaded to a server for each modality. Experimental results show that the proposed algorithm can effectively improve the personalized performance of FMML.","sentences":["Federated Multi-Modal Learning (FMML) is an emerging field that integrates information from different modalities in federated learning to improve the learning performance.","In this letter, we develop a parameter scheduling scheme to improve personalized performance and communication efficiency in personalized FMML, considering the non-independent and nonidentically distributed (non-IID) data along with the modality heterogeneity.","Specifically, a learning-based approach is utilized to obtain the aggregation coefficients for parameters of different modalities on distinct devices.","Based on the aggregation coefficients and channel state, a subset of parameters is scheduled to be uploaded to a server for each modality.","Experimental results show that the proposed algorithm can effectively improve the personalized performance of FMML."],"url":"http://arxiv.org/abs/2406.07915v1","category":"cs.IT"}
{"created":"2024-06-12 06:22:52","title":"Guiding Frame-Level CTC Alignments Using Self-knowledge Distillation","abstract":"Transformer encoder with connectionist temporal classification (CTC) framework is widely used for automatic speech recognition (ASR). However, knowledge distillation (KD) for ASR displays a problem of disagreement between teacher-student models in frame-level alignment which ultimately hinders it from improving the student model's performance. In order to resolve this problem, this paper introduces a self-knowledge distillation (SKD) method that guides the frame-level alignment during the training time. In contrast to the conventional method using separate teacher and student models, this study introduces a simple and effective method sharing encoder layers and applying the sub-model as the student model. Overall, our approach is effective in improving both the resource efficiency as well as performance. We also conducted an experimental analysis of the spike timings to illustrate that the proposed method improves performance by reducing the alignment disagreement.","sentences":["Transformer encoder with connectionist temporal classification (CTC) framework is widely used for automatic speech recognition (ASR).","However, knowledge distillation (KD) for ASR displays a problem of disagreement between teacher-student models in frame-level alignment which ultimately hinders it from improving the student model's performance.","In order to resolve this problem, this paper introduces a self-knowledge distillation (SKD) method that guides the frame-level alignment during the training time.","In contrast to the conventional method using separate teacher and student models, this study introduces a simple and effective method sharing encoder layers and applying the sub-model as the student model.","Overall, our approach is effective in improving both the resource efficiency as well as performance.","We also conducted an experimental analysis of the spike timings to illustrate that the proposed method improves performance by reducing the alignment disagreement."],"url":"http://arxiv.org/abs/2406.07909v1","category":"eess.AS"}
{"created":"2024-06-12 06:00:00","title":"Emotional Conversation: Empowering Talking Faces with Cohesive Expression, Gaze and Pose Generation","abstract":"Vivid talking face generation holds immense potential applications across diverse multimedia domains, such as film and game production. While existing methods accurately synchronize lip movements with input audio, they typically ignore crucial alignments between emotion and facial cues, which include expression, gaze, and head pose. These alignments are indispensable for synthesizing realistic videos. To address these issues, we propose a two-stage audio-driven talking face generation framework that employs 3D facial landmarks as intermediate variables. This framework achieves collaborative alignment of expression, gaze, and pose with emotions through self-supervised learning. Specifically, we decompose this task into two key steps, namely speech-to-landmarks synthesis and landmarks-to-face generation. The first step focuses on simultaneously synthesizing emotionally aligned facial cues, including normalized landmarks that represent expressions, gaze, and head pose. These cues are subsequently reassembled into relocated facial landmarks. In the second step, these relocated landmarks are mapped to latent key points using self-supervised learning and then input into a pretrained model to create high-quality face images. Extensive experiments on the MEAD dataset demonstrate that our model significantly advances the state-of-the-art performance in both visual quality and emotional alignment.","sentences":["Vivid talking face generation holds immense potential applications across diverse multimedia domains, such as film and game production.","While existing methods accurately synchronize lip movements with input audio, they typically ignore crucial alignments between emotion and facial cues, which include expression, gaze, and head pose.","These alignments are indispensable for synthesizing realistic videos.","To address these issues, we propose a two-stage audio-driven talking face generation framework that employs 3D facial landmarks as intermediate variables.","This framework achieves collaborative alignment of expression, gaze, and pose with emotions through self-supervised learning.","Specifically, we decompose this task into two key steps, namely speech-to-landmarks synthesis and landmarks-to-face generation.","The first step focuses on simultaneously synthesizing emotionally aligned facial cues, including normalized landmarks that represent expressions, gaze, and head pose.","These cues are subsequently reassembled into relocated facial landmarks.","In the second step, these relocated landmarks are mapped to latent key points using self-supervised learning and then input into a pretrained model to create high-quality face images.","Extensive experiments on the MEAD dataset demonstrate that our model significantly advances the state-of-the-art performance in both visual quality and emotional alignment."],"url":"http://arxiv.org/abs/2406.07895v1","category":"cs.CV"}
{"created":"2024-06-12 05:41:01","title":"Exploring Speech Foundation Models for Speaker Diarization in Child-Adult Dyadic Interactions","abstract":"Speech foundation models, trained on vast datasets, have opened unique opportunities in addressing challenging low-resource speech understanding, such as child speech. In this work, we explore the capabilities of speech foundation models on child-adult speaker diarization. We show that exemplary foundation models can achieve 39.5% and 62.3% relative reductions in Diarization Error Rate and Speaker Confusion Rate, respectively, compared to previous speaker diarization methods. In addition, we benchmark and evaluate the speaker diarization results of the speech foundation models with varying the input audio window size, speaker demographics, and training data ratio. Our results highlight promising pathways for understanding and adopting speech foundation models to facilitate child speech understanding.","sentences":["Speech foundation models, trained on vast datasets, have opened unique opportunities in addressing challenging low-resource speech understanding, such as child speech.","In this work, we explore the capabilities of speech foundation models on child-adult speaker diarization.","We show that exemplary foundation models can achieve 39.5% and 62.3% relative reductions in Diarization Error Rate and Speaker Confusion Rate, respectively, compared to previous speaker diarization methods.","In addition, we benchmark and evaluate the speaker diarization results of the speech foundation models with varying the input audio window size, speaker demographics, and training data ratio.","Our results highlight promising pathways for understanding and adopting speech foundation models to facilitate child speech understanding."],"url":"http://arxiv.org/abs/2406.07890v1","category":"eess.AS"}
{"created":"2024-06-12 05:24:58","title":"Label-aware Hard Negative Sampling Strategies with Momentum Contrastive Learning for Implicit Hate Speech Detection","abstract":"Detecting implicit hate speech that is not directly hateful remains a challenge. Recent research has attempted to detect implicit hate speech by applying contrastive learning to pre-trained language models such as BERT and RoBERTa, but the proposed models still do not have a significant advantage over cross-entropy loss-based learning. We found that contrastive learning based on randomly sampled batch data does not encourage the model to learn hard negative samples. In this work, we propose Label-aware Hard Negative sampling strategies (LAHN) that encourage the model to learn detailed features from hard negative samples, instead of naive negative samples in random batch, using momentum-integrated contrastive learning. LAHN outperforms the existing models for implicit hate speech detection both in- and cross-datasets. The code is available at https://github.com/Hanyang-HCC-Lab/LAHN","sentences":["Detecting implicit hate speech that is not directly hateful remains a challenge.","Recent research has attempted to detect implicit hate speech by applying contrastive learning to pre-trained language models such as BERT and RoBERTa, but the proposed models still do not have a significant advantage over cross-entropy loss-based learning.","We found that contrastive learning based on randomly sampled batch data does not encourage the model to learn hard negative samples.","In this work, we propose Label-aware Hard Negative sampling strategies (LAHN) that encourage the model to learn detailed features from hard negative samples, instead of naive negative samples in random batch, using momentum-integrated contrastive learning.","LAHN outperforms the existing models for implicit hate speech detection both in- and cross-datasets.","The code is available at https://github.com/Hanyang-HCC-Lab/LAHN"],"url":"http://arxiv.org/abs/2406.07886v1","category":"cs.CL"}
{"created":"2024-06-12 17:16:51","title":"Idealized Social Dynamics In Bayesian Space of Assessments","abstract":"The purpose of this paper is to present an idealized hypotheses-drawn model describing dynamics of variability in business and other social areas. A new construct is introduced called the Bayesian space of assessments to consider changes in positions of individuals and intrinsically interrelated entities called social bodies. The bodies spatial coordinates are their assessments in any required for a specific purpose professional and or ethical dimensions. A concept of market power introduced originally in entrepreneurship and technology commercialization (Michael Danov, Brock Smith, and Ronald Mitchell, 2003) is extended to become a driving force applicable to other social processes. The model describes interrelations between driving forces acting on social bodies and changes in positions of these bodies in the Bayesian space of assessments. Implications are discussed for some business and political oscillations.","sentences":["The purpose of this paper is to present an idealized hypotheses-drawn model describing dynamics of variability in business and other social areas.","A new construct is introduced called the Bayesian space of assessments to consider changes in positions of individuals and intrinsically interrelated entities called social bodies.","The bodies spatial coordinates are their assessments in any required for a specific purpose professional and or ethical dimensions.","A concept of market power introduced originally in entrepreneurship and technology commercialization (Michael Danov, Brock Smith, and Ronald Mitchell, 2003) is extended to become a driving force applicable to other social processes.","The model describes interrelations between driving forces acting on social bodies and changes in positions of these bodies in the Bayesian space of assessments.","Implications are discussed for some business and political oscillations."],"url":"http://arxiv.org/abs/2406.08432v1","category":"math.OC"}
{"created":"2024-06-12 17:16:02","title":"Testing Quantum and Simulated Annealers on the Drone Delivery Packing Problem","abstract":"Using drones to perform human-related tasks can play a key role in various fields, such as defense, disaster response, agriculture, healthcare, and many others. The drone delivery packing problem (DDPP) arises in the context of logistics in response to an increasing demand in the delivery process along with the necessity of lowering human intervention. The DDPP is usually formulated as a combinatorial optimization problem, aiming to minimize drone usage with specific battery constraints while ensuring timely consistent deliveries with fixed locations and energy budget. In this work, we propose two alternative formulations of the DDPP as a quadratic unconstrained binary optimization (QUBO) problem, in order to test the performance of classical and quantum annealing (QA) approaches. We perform extensive experiments showing the advantages as well as the limitations of quantum annealers for this optimization problem, as compared to simulated annealing (SA) and classical state-of-the-art commercial tools for global optimization.","sentences":["Using drones to perform human-related tasks can play a key role in various fields, such as defense, disaster response, agriculture, healthcare, and many others.","The drone delivery packing problem (DDPP) arises in the context of logistics in response to an increasing demand in the delivery process along with the necessity of lowering human intervention.","The DDPP is usually formulated as a combinatorial optimization problem, aiming to minimize drone usage with specific battery constraints while ensuring timely consistent deliveries with fixed locations and energy budget.","In this work, we propose two alternative formulations of the DDPP as a quadratic unconstrained binary optimization (QUBO) problem, in order to test the performance of classical and quantum annealing (QA) approaches.","We perform extensive experiments showing the advantages as well as the limitations of quantum annealers for this optimization problem, as compared to simulated annealing (SA) and classical state-of-the-art commercial tools for global optimization."],"url":"http://arxiv.org/abs/2406.08430v1","category":"math.CO"}
{"created":"2024-06-12 16:57:56","title":"Experimental Quantum Advantage in the Odd-Cycle Game","abstract":"We report the first experimental demonstration of the odd-cycle game. We entangle two ions separated by ~2 m and the players use them to win the odd-cycle game significantly more often than the best classical strategy allows. The experiment implements the optimal quantum strategy, is free of the detection loophole (the detection efficiency is >~99.999 %), and achieves 97.8(3) % of the theoretical limit to the quantum winning probability. It provides a nonlocal content of 0.54(2) -- the largest value for physically separate devices, free of the detection loophole, ever observed.","sentences":["We report the first experimental demonstration of the odd-cycle game.","We entangle two ions separated by ~2 m and the players use them to win the odd-cycle game significantly more often than the best classical strategy allows.","The experiment implements the optimal quantum strategy, is free of the detection loophole (the detection efficiency is >~99.999 %), and achieves 97.8(3) % of the theoretical limit to the quantum winning probability.","It provides a nonlocal content of 0.54(2) -- the largest value for physically separate devices, free of the detection loophole, ever observed."],"url":"http://arxiv.org/abs/2406.08412v1","category":"quant-ph"}
{"created":"2024-06-12 16:29:45","title":"Eyes Wide Unshut: Unsupervised Mistake Detection in Egocentric Video by Detecting Unpredictable Gaze","abstract":"In this paper, we address the challenge of unsupervised mistake detection in egocentric video through the analysis of gaze signals, a critical component for advancing user assistance in smart glasses. Traditional supervised methods, reliant on manually labeled mistakes, suffer from domain-dependence and scalability issues. This research introduces an unsupervised method for detecting mistakes in videos of human activities, overcoming the challenges of domain-specific requirements and the necessity for annotated data. By analyzing unusual gaze patterns that signal user disorientation during tasks, we propose a gaze completion model that forecasts eye gaze trajectories from incomplete inputs. The difference between the anticipated and observed gaze paths acts as an indicator for identifying errors. Our method is validated on the EPIC-Tent dataset, showing its superiority compared to current one-class supervised and unsupervised techniques.","sentences":["In this paper, we address the challenge of unsupervised mistake detection in egocentric video through the analysis of gaze signals, a critical component for advancing user assistance in smart glasses.","Traditional supervised methods, reliant on manually labeled mistakes, suffer from domain-dependence and scalability issues.","This research introduces an unsupervised method for detecting mistakes in videos of human activities, overcoming the challenges of domain-specific requirements and the necessity for annotated data.","By analyzing unusual gaze patterns that signal user disorientation during tasks, we propose a gaze completion model that forecasts eye gaze trajectories from incomplete inputs.","The difference between the anticipated and observed gaze paths acts as an indicator for identifying errors.","Our method is validated on the EPIC-Tent dataset, showing its superiority compared to current one-class supervised and unsupervised techniques."],"url":"http://arxiv.org/abs/2406.08379v1","category":"cs.CV"}
{"created":"2024-06-12 14:28:41","title":"Towards a sub-kelvin cryogenic Fabry-Perot silicon cavity","abstract":"We report on the development of a sub-kelvin, single-crystal silicon Fabry-Perot cavity. Operating such a cavity below 1~K should reduce the thermal noise limit of the cavity, and by this way address the current limitations of ultrastable lasers. To further decrease mechanical losses, mirrors with silicon substrates and crystalline coatings are optically contacted to the spacer, resulting in a room-temperature finesse of 220,000. To operate our cavity at sub-kelvin temperatures, we use a dilution refrigerator able to reach temperatures down to 10 mK. We have designed a mechanical mount to house our cavity in such a cryostat, with optimized heat transfer that will decrease the cooldown time for temperatures below 1~K. The estimated thermal noise is projected to be $\\sim 7{\\times}10^{-19}$ at 100~mK. However, silicon cavities with crystalline mirror coatings at cryogenic temperatures have shown birefringence correlated frequency fluctuations as well as unknown additional noise mechanisms \\cite{yu2023, kedar2023}. We have measured a room-temperature TEM$_{00}$ birefringent mode splitting of about 250 kHz. Understanding and measuring these noise mechanisms will be a key to attaining fractional frequency stabilities beyond state-of-the-art.","sentences":["We report on the development of a sub-kelvin, single-crystal silicon Fabry-Perot cavity.","Operating such a cavity below 1~K should reduce the thermal noise limit of the cavity, and by this way address the current limitations of ultrastable lasers.","To further decrease mechanical losses, mirrors with silicon substrates and crystalline coatings are optically contacted to the spacer, resulting in a room-temperature finesse of 220,000.","To operate our cavity at sub-kelvin temperatures, we use a dilution refrigerator able to reach temperatures down to 10 mK. We have designed a mechanical mount to house our cavity in such a cryostat, with optimized heat transfer that will decrease the cooldown time for temperatures below 1~K. The estimated thermal noise is projected to be $\\sim 7{\\times}10^{-19}$ at 100~mK. However, silicon cavities with crystalline mirror coatings at cryogenic temperatures have shown birefringence correlated frequency fluctuations as well as unknown additional noise mechanisms \\cite{yu2023, kedar2023}.","We have measured a room-temperature TEM$_{00}$ birefringent mode splitting of about 250 kHz.","Understanding and measuring these noise mechanisms will be a key to attaining fractional frequency stabilities beyond state-of-the-art."],"url":"http://arxiv.org/abs/2406.08256v1","category":"physics.ins-det"}
{"created":"2024-06-12 14:06:35","title":"Mode-based estimation of the center of symmetry","abstract":"In the mean-median-mode triad of univariate centrality measures, the mode has been overlooked for estimating the center of symmetry in continuous and unimodal settings. This paper expands on the connection between kernel mode estimators and M-estimators for location, bridging the gap between the nonparametrics and robust statistics communities. The variance of modal estimators is studied in terms of a bandwidth parameter, establishing conditions for an optimal solution that outperforms the household sample mean. A purely nonparametric approach is adopted, modeling heavy-tailedness through regular variation. The results lead to an estimator proposal that includes a novel one-parameter family of kernels with compact support, offering extra robustness and efficiency. The effectiveness and versatility of the new method are demonstrated in a real-world case study and a thorough simulation study, comparing favorably to traditional and more competitive alternatives. Several myths about the mode are clarified along the way, reopening the quest for flexible and efficient nonparametric estimators.","sentences":["In the mean-median-mode triad of univariate centrality measures, the mode has been overlooked for estimating the center of symmetry in continuous and unimodal settings.","This paper expands on the connection between kernel mode estimators and M-estimators for location, bridging the gap between the nonparametrics and robust statistics communities.","The variance of modal estimators is studied in terms of a bandwidth parameter, establishing conditions for an optimal solution that outperforms the household sample mean.","A purely nonparametric approach is adopted, modeling heavy-tailedness through regular variation.","The results lead to an estimator proposal that includes a novel one-parameter family of kernels with compact support, offering extra robustness and efficiency.","The effectiveness and versatility of the new method are demonstrated in a real-world case study and a thorough simulation study, comparing favorably to traditional and more competitive alternatives.","Several myths about the mode are clarified along the way, reopening the quest for flexible and efficient nonparametric estimators."],"url":"http://arxiv.org/abs/2406.08241v1","category":"stat.ME"}
{"created":"2024-06-12 12:28:58","title":"Functional voxel hierarchy and afferent capacity revealed mental state transition on dynamic correlation resting-state fMRI","abstract":"Voxel hierarchy on dynamic brain graphs is produced by k core percolation on functional dynamic amplitude correlation of resting-state fMRI. Directed graphs and their afferent/efferent capacities are produced by Markov modeling of the universal cover of undirected graphs simultaneously with the calculation of volume entropy. Positive and unsigned negative brain graphs were analyzed separately on sliding-window representation to underpin the visualization and quantitation of mental dynamic states with their transitions. Voxel hierarchy animation maps of positive graphs revealed abrupt changes in coreness k and kmaxcore, which we called mental state transitions. Afferent voxel capacities of the positive graphs also revealed transient modules composed of dominating voxels/independent components and their exchanges representing mental state transitions. Animation and quantification plots of voxel hierarchy and afferent capacity corroborated each other in underpinning mental state transitions and afferent module exchange on the positive directed functional connectivity graphs. We propose the use of spatiotemporal trajectories of voxels on positive dynamic graphs to construct hierarchical structures by k core percolation and quantified in- and out-flows of information of voxels by volume entropy/directed graphs to subserve diverse resting mental state transitions on resting-state fMRI graphs in normal human individuals.","sentences":["Voxel hierarchy on dynamic brain graphs is produced by k core percolation on functional dynamic amplitude correlation of resting-state fMRI.","Directed graphs and their afferent/efferent capacities are produced by Markov modeling of the universal cover of undirected graphs simultaneously with the calculation of volume entropy.","Positive and unsigned negative brain graphs were analyzed separately on sliding-window representation to underpin the visualization and quantitation of mental dynamic states with their transitions.","Voxel hierarchy animation maps of positive graphs revealed abrupt changes in coreness k and kmaxcore, which we called mental state transitions.","Afferent voxel capacities of the positive graphs also revealed transient modules composed of dominating voxels/independent components and their exchanges representing mental state transitions.","Animation and quantification plots of voxel hierarchy and afferent capacity corroborated each other in underpinning mental state transitions and afferent module exchange on the positive directed functional connectivity graphs.","We propose the use of spatiotemporal trajectories of voxels on positive dynamic graphs to construct hierarchical structures by k core percolation and quantified in- and out-flows of information of voxels by volume entropy/directed graphs to subserve diverse resting mental state transitions on resting-state fMRI graphs in normal human individuals."],"url":"http://arxiv.org/abs/2406.08140v1","category":"q-bio.NC"}
{"created":"2024-06-12 12:15:49","title":"Upper bounds on the highest phonon frequency and superconducting temperature from fundamental physical constants","abstract":"Fundamental physical constants govern key effects in high-energy particle physics and astrophysics, including the stability of particles, nuclear reactions, formation and evolution of stars, synthesis of heavy nuclei and emergence of stable molecular structures. Here, we show that fundamental constants also set an upper bound for the frequency of phonons in condensed matter phases, or how rapidly an atom can vibrate. This bound is in agreement with \\textit{ab initio} simulations of atomic hydrogen and high-temperature hydride superconductors, and implies an upper limit to the superconducting transition temperature $T_c$ in condensed matter. Fundamental constants set this limit to the order of 10$^2-10^3$ K. This range is consistent with our calculations of $T_c$ from optimal Eliashberg functions. As a corollary, we observe that the very existence of high-temperature superconductivity research is due to the observed values of fundamental constants. We finally discuss how fundamental constants affect the observability of other condensed matter phenomena including phase transitions.","sentences":["Fundamental physical constants govern key effects in high-energy particle physics and astrophysics, including the stability of particles, nuclear reactions, formation and evolution of stars, synthesis of heavy nuclei and emergence of stable molecular structures.","Here, we show that fundamental constants also set an upper bound for the frequency of phonons in condensed matter phases, or how rapidly an atom can vibrate.","This bound is in agreement with \\textit{ab initio} simulations of atomic hydrogen and high-temperature hydride superconductors, and implies an upper limit to the superconducting transition temperature $T_c$ in condensed matter.","Fundamental constants set this limit to the order of 10$^2-10^3$ K. This range is consistent with our calculations of $T_c$ from optimal Eliashberg functions.","As a corollary, we observe that the very existence of high-temperature superconductivity research is due to the observed values of fundamental constants.","We finally discuss how fundamental constants affect the observability of other condensed matter phenomena including phase transitions."],"url":"http://arxiv.org/abs/2406.08129v1","category":"cond-mat.supr-con"}
{"created":"2024-06-12 11:39:50","title":"Analytical evaluation of ground state gradients in quantum electrodynamics coupled cluster theory","abstract":"Analytical gradients of potential energy surfaces play a central role in quantum chemistry, allowing for molecular geometry optimizations and molecular dynamics simulations. In strong coupling conditions, potential energy surfaces can account for strong interactions between matter and the quantized electromagnetic field. In this paper, we derive expressions for the ground state analytical gradients in quantum electrodynamics coupled cluster theory. We also present a Cholesky-based implementation for the coupled cluster singles and doubles model. We report timings to show the performance of the implementation and present optimized geometries to highlight cavity-induced molecular orientation effects in strong coupling conditions.","sentences":["Analytical gradients of potential energy surfaces play a central role in quantum chemistry, allowing for molecular geometry optimizations and molecular dynamics simulations.","In strong coupling conditions, potential energy surfaces can account for strong interactions between matter and the quantized electromagnetic field.","In this paper, we derive expressions for the ground state analytical gradients in quantum electrodynamics coupled cluster theory.","We also present a Cholesky-based implementation for the coupled cluster singles and doubles model.","We report timings to show the performance of the implementation and present optimized geometries to highlight cavity-induced molecular orientation effects in strong coupling conditions."],"url":"http://arxiv.org/abs/2406.08107v1","category":"physics.chem-ph"}
{"created":"2024-06-12 06:06:17","title":"Josephson Parametric Amplifier based Quantum Noise Limited Amplifier Development for Axion Search Experiments in CAPP","abstract":"This paper provides a comprehensive overview of the development of flux-driven Josephson Parametric Amplifiers (JPAs) as Quantum Noise Limited Amplifier for axion search experiments conducted at the Center for Axion and Precision Physics Research (CAPP) of the Institute for Basic Science. It focuses on the characterization, and optimization of JPAs, which are crucial for achieving the highest sensitivity in axion particle detection. We discuss various characterization techniques, methods for improving bandwidth, and the attainment of ultra-low noise temperatures. JPAs have emerged as indispensable tools in CAPPs axion search endeavors, playing a significant role in advancing our understanding of fundamental physics and unraveling the mysteries of the universe.","sentences":["This paper provides a comprehensive overview of the development of flux-driven Josephson Parametric Amplifiers (JPAs) as Quantum Noise Limited Amplifier for axion search experiments conducted at the Center for Axion and Precision Physics Research (CAPP) of the Institute for Basic Science.","It focuses on the characterization, and optimization of JPAs, which are crucial for achieving the highest sensitivity in axion particle detection.","We discuss various characterization techniques, methods for improving bandwidth, and the attainment of ultra-low noise temperatures.","JPAs have emerged as indispensable tools in CAPPs axion search endeavors, playing a significant role in advancing our understanding of fundamental physics and unraveling the mysteries of the universe."],"url":"http://arxiv.org/abs/2406.07899v1","category":"hep-ex"}
{"created":"2024-06-12 04:20:00","title":"Random Combinatorial Billiards and Stoned Exclusion Processes","abstract":"We introduce and study several random combinatorial billiard trajectories. Such a system, which depends on a fixed parameter $p\\in(0,1)$, models a beam of light that travels in a Euclidean space, occasionally randomly reflecting off of a hyperplane in the Coxeter arrangement of an affine Weyl group with some probability that depends on the side of the hyperplane that it hits. In one case, we (essentially) recover Lam's reduced random walk in the limit as $p$ tends to $0$. The investigation of our random billiard trajectories relies on an analysis of new finite Markov chains that we call stoned exclusion processes. These processes have remarkable stationary distributions determined by well-studied polynomials such as ASEP polynomials, inhomogeneous TASEP polynomials, and open boundary ASEP polynomials; in many cases, it was previously not known how to construct Markov chains with these stationary distributions. Using multiline queues, we analyze correlations in the stoned multispecies TASEP, allowing us to determine limit directions for reduced random billiard trajectories and limit shapes for new random growth processes for $n$-core partitions. Our perspective coming from combinatorial billiards naturally leads us to formulate a new variant of the ASEP on $\\mathbb{Z}$ called the scan ASEP, which we deem interesting in its own right.","sentences":["We introduce and study several random combinatorial billiard trajectories.","Such a system, which depends on a fixed parameter $p\\in(0,1)$, models a beam of light that travels in a Euclidean space, occasionally randomly reflecting off of a hyperplane in the Coxeter arrangement of an affine Weyl group with some probability that depends on the side of the hyperplane that it hits.","In one case, we (essentially) recover Lam's reduced random walk in the limit as $p$ tends to $0$. The investigation of our random billiard trajectories relies on an analysis of new finite Markov chains that we call stoned exclusion processes.","These processes have remarkable stationary distributions determined by well-studied polynomials such as ASEP polynomials, inhomogeneous TASEP polynomials, and open boundary ASEP polynomials; in many cases, it was previously not known how to construct Markov chains with these stationary distributions.","Using multiline queues, we analyze correlations in the stoned multispecies TASEP, allowing us to determine limit directions for reduced random billiard trajectories and limit shapes for new random growth processes for $n$-core partitions.","Our perspective coming from combinatorial billiards naturally leads us to formulate a new variant of the ASEP on $\\mathbb{Z}$ called the scan ASEP, which we deem interesting in its own right."],"url":"http://arxiv.org/abs/2406.07858v1","category":"math.PR"}
{"created":"2024-06-12 03:39:16","title":"A Labeled Array Distance Metric for Measuring Image Segmentation Quality","abstract":"This work introduces two new distance metrics for comparing labeled arrays, which are common outputs of image segmentation algorithms. Each pixel in an image is assigned a label, with binary segmentation providing only two labels ('foreground' and 'background'). These can be represented by a simple binary matrix and compared using pixel differences. However, many segmentation algorithms output multiple regions in a labeled array. We propose two distance metrics, named LAD and MADLAD, that calculate the distance between two labeled images. By doing so, the accuracy of different image segmentation algorithms can be evaluated by measuring their outputs against a 'ground truth' labeling. Both proposed metrics, operating with a complexity of $O(N)$ for images with $N$ pixels, are designed to quickly identify similar labeled arrays, even when different labeling methods are used. Comparisons are made between images labeled manually and those labeled by segmentation algorithms. This evaluation is crucial when searching through a space of segmentation algorithms and their hyperparameters via a genetic algorithm to identify the optimal solution for automated segmentation, which is the goal in our lab, SEE-Insight. By measuring the distance from the ground truth, these metrics help determine which algorithm provides the most accurate segmentation.","sentences":["This work introduces two new distance metrics for comparing labeled arrays, which are common outputs of image segmentation algorithms.","Each pixel in an image is assigned a label, with binary segmentation providing only two labels ('foreground' and 'background').","These can be represented by a simple binary matrix and compared using pixel differences.","However, many segmentation algorithms output multiple regions in a labeled array.","We propose two distance metrics, named LAD and MADLAD, that calculate the distance between two labeled images.","By doing so, the accuracy of different image segmentation algorithms can be evaluated by measuring their outputs against a 'ground truth' labeling.","Both proposed metrics, operating with a complexity of $O(N)$ for images with $N$ pixels, are designed to quickly identify similar labeled arrays, even when different labeling methods are used.","Comparisons are made between images labeled manually and those labeled by segmentation algorithms.","This evaluation is crucial when searching through a space of segmentation algorithms and their hyperparameters via a genetic algorithm to identify the optimal solution for automated segmentation, which is the goal in our lab, SEE-Insight.","By measuring the distance from the ground truth, these metrics help determine which algorithm provides the most accurate segmentation."],"url":"http://arxiv.org/abs/2406.07851v1","category":"cs.CV"}
{"created":"2024-06-12 03:21:34","title":"Understanding and Mitigating Compositional Issues in Text-to-Image Generative Models","abstract":"Recent text-to-image diffusion-based generative models have the stunning ability to generate highly detailed and photo-realistic images and achieve state-of-the-art low FID scores on challenging image generation benchmarks. However, one of the primary failure modes of these text-to-image generative models is in composing attributes, objects, and their associated relationships accurately into an image. In our paper, we investigate this compositionality-based failure mode and highlight that imperfect text conditioning with CLIP text-encoder is one of the primary reasons behind the inability of these models to generate high-fidelity compositional scenes. In particular, we show that (i) there exists an optimal text-embedding space that can generate highly coherent compositional scenes which shows that the output space of the CLIP text-encoder is sub-optimal, and (ii) we observe that the final token embeddings in CLIP are erroneous as they often include attention contributions from unrelated tokens in compositional prompts. Our main finding shows that the best compositional improvements can be achieved (without harming the model's FID scores) by fine-tuning {\\it only} a simple linear projection on CLIP's representation space in Stable-Diffusion variants using a small set of compositional image-text pairs. This result demonstrates that the sub-optimality of the CLIP's output space is a major error source. We also show that re-weighting the erroneous attention contributions in CLIP can also lead to improved compositional performances, however these improvements are often less significant than those achieved by solely learning a linear projection head, highlighting erroneous attentions to be only a minor error source.","sentences":["Recent text-to-image diffusion-based generative models have the stunning ability to generate highly detailed and photo-realistic images and achieve state-of-the-art low FID scores on challenging image generation benchmarks.","However, one of the primary failure modes of these text-to-image generative models is in composing attributes, objects, and their associated relationships accurately into an image.","In our paper, we investigate this compositionality-based failure mode and highlight that imperfect text conditioning with CLIP text-encoder is one of the primary reasons behind the inability of these models to generate high-fidelity compositional scenes.","In particular, we show that (i) there exists an optimal text-embedding space that can generate highly coherent compositional scenes which shows that the output space of the CLIP text-encoder is sub-optimal, and (ii) we observe that the final token embeddings in CLIP are erroneous as they often include attention contributions from unrelated tokens in compositional prompts.","Our main finding shows that the best compositional improvements can be achieved (without harming the model's FID scores) by fine-tuning {\\it only} a simple linear projection on CLIP's representation space in Stable-Diffusion variants using a small set of compositional image-text pairs.","This result demonstrates that the sub-optimality of the CLIP's output space is a major error source.","We also show that re-weighting the erroneous attention contributions in CLIP can also lead to improved compositional performances, however these improvements are often less significant than those achieved by solely learning a linear projection head, highlighting erroneous attentions to be only a minor error source."],"url":"http://arxiv.org/abs/2406.07844v1","category":"cs.CV"}
{"created":"2024-06-12 02:57:41","title":"ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models","abstract":"The impressive performance of Large Language Models (LLMs) across various natural language processing tasks comes at the cost of vast computational resources and storage requirements. One-shot pruning techniques offer a way to alleviate these burdens by removing redundant weights without the need for retraining. Yet, the massive scale of LLMs often forces current pruning approaches to rely on heuristics instead of optimization-based techniques, potentially resulting in suboptimal compression. In this paper, we introduce ALPS, an optimization-based framework that tackles the pruning problem using the operator splitting technique and a preconditioned conjugate gradient-based post-processing step. Our approach incorporates novel techniques to accelerate and theoretically guarantee convergence while leveraging vectorization and GPU parallelism for efficiency. ALPS substantially outperforms state-of-the-art methods in terms of the pruning objective and perplexity reduction, particularly for highly sparse models. On the OPT-30B model with 70% sparsity, ALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a 19% improvement in zero-shot benchmark performance compared to existing methods.","sentences":["The impressive performance of Large Language Models (LLMs) across various natural language processing tasks comes at the cost of vast computational resources and storage requirements.","One-shot pruning techniques offer a way to alleviate these burdens by removing redundant weights without the need for retraining.","Yet, the massive scale of LLMs often forces current pruning approaches to rely on heuristics instead of optimization-based techniques, potentially resulting in suboptimal compression.","In this paper, we introduce ALPS, an optimization-based framework that tackles the pruning problem using the operator splitting technique and a preconditioned conjugate gradient-based post-processing step.","Our approach incorporates novel techniques to accelerate and theoretically guarantee convergence while leveraging vectorization and GPU parallelism for efficiency.","ALPS substantially outperforms state-of-the-art methods in terms of the pruning objective and perplexity reduction, particularly for highly sparse models.","On the OPT-30B model with 70% sparsity, ALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a 19% improvement in zero-shot benchmark performance compared to existing methods."],"url":"http://arxiv.org/abs/2406.07831v1","category":"cs.LG"}
{"created":"2024-06-12 02:52:32","title":"Simplified and Flexible Coils for Stellarators using Single-Stage Optimization","abstract":"Single-stage optimization, also known as combined plasma-coil algorithms or direct coil optimization, has recently emerged as a possible method to expedite the design of stellarator devices by including, in a single step, confinement, stability, and engineering constraints. In this work, we show how such frameworks allow us to find new designs in a streamlined manner, yielding a broad range of new configurations. Examples are shown for stellarators with a small number of coils and quasisymmetric stellarators with only one to three coils per half field period, with external trim coils, helical coils, and a single set of coils generating both a quasi-axisymmetric and a quasi-helical equilibrium.","sentences":["Single-stage optimization, also known as combined plasma-coil algorithms or direct coil optimization, has recently emerged as a possible method to expedite the design of stellarator devices by including, in a single step, confinement, stability, and engineering constraints.","In this work, we show how such frameworks allow us to find new designs in a streamlined manner, yielding a broad range of new configurations.","Examples are shown for stellarators with a small number of coils and quasisymmetric stellarators with only one to three coils per half field period, with external trim coils, helical coils, and a single set of coils generating both a quasi-axisymmetric and a quasi-helical equilibrium."],"url":"http://arxiv.org/abs/2406.07830v1","category":"physics.plasm-ph"}
{"created":"2024-06-12 02:47:40","title":"Shape-Constrained Distributional Optimization via Importance-Weighted Sample Average Approximation","abstract":"Shape-constrained optimization arises in a wide range of problems including distributionally robust optimization (DRO) that has surging popularity in recent years. In the DRO literature, these problems are usually solved via reduction into moment-constrained problems using the Choquet representation. While powerful, such an approach could face tractability challenges arising from the geometries and the compatibility between the shape and the objective function and moment constraints. In this paper, we propose an alternative methodology to solve shape-constrained optimization problems by integrating sample average approximation with importance sampling, the latter used to convert the distributional optimization into an optimization problem over the likelihood ratio with respect to a sampling distribution. We demonstrate how our approach, which relies on finite-dimensional linear programs, can handle a range of shape-constrained problems beyond the reach of previous Choquet-based reformulations, and entails vanishing and quantifiable optimality gaps. Moreover, our theoretical analyses based on strong duality and empirical processes reveal the critical role of shape constraints in guaranteeing desirable consistency and convergence rates.","sentences":["Shape-constrained optimization arises in a wide range of problems including distributionally robust optimization (DRO) that has surging popularity in recent years.","In the DRO literature, these problems are usually solved via reduction into moment-constrained problems using the Choquet representation.","While powerful, such an approach could face tractability challenges arising from the geometries and the compatibility between the shape and the objective function and moment constraints.","In this paper, we propose an alternative methodology to solve shape-constrained optimization problems by integrating sample average approximation with importance sampling, the latter used to convert the distributional optimization into an optimization problem over the likelihood ratio with respect to a sampling distribution.","We demonstrate how our approach, which relies on finite-dimensional linear programs, can handle a range of shape-constrained problems beyond the reach of previous Choquet-based reformulations, and entails vanishing and quantifiable optimality gaps.","Moreover, our theoretical analyses based on strong duality and empirical processes reveal the critical role of shape constraints in guaranteeing desirable consistency and convergence rates."],"url":"http://arxiv.org/abs/2406.07825v1","category":"math.OC"}
{"created":"2024-06-12 01:35:46","title":"PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models","abstract":"Recently, there have been attempts to integrate various speech processing tasks into a unified model. However, few previous works directly demonstrated that joint optimization of diverse tasks in multitask speech models has positive influence on the performance of individual tasks. In this paper we present a multitask speech model -- PolySpeech, which supports speech recognition, speech synthesis, and two speech classification tasks. PolySpeech takes multi-modal language model as its core structure and uses semantic representations as speech inputs. We introduce semantic speech embedding tokenization and speech reconstruction methods to PolySpeech, enabling efficient generation of high-quality speech for any given speaker. PolySpeech shows competitiveness across various tasks compared to single-task models. In our experiments, multitask optimization achieves performance comparable to single-task optimization and is especially beneficial for specific tasks.","sentences":["Recently, there have been attempts to integrate various speech processing tasks into a unified model.","However, few previous works directly demonstrated that joint optimization of diverse tasks in multitask speech models has positive influence on the performance of individual tasks.","In this paper we present a multitask speech model -- PolySpeech, which supports speech recognition, speech synthesis, and two speech classification tasks.","PolySpeech takes multi-modal language model as its core structure and uses semantic representations as speech inputs.","We introduce semantic speech embedding tokenization and speech reconstruction methods to PolySpeech, enabling efficient generation of high-quality speech for any given speaker.","PolySpeech shows competitiveness across various tasks compared to single-task models.","In our experiments, multitask optimization achieves performance comparable to single-task optimization and is especially beneficial for specific tasks."],"url":"http://arxiv.org/abs/2406.07801v1","category":"cs.CL"}
{"created":"2024-06-11 22:37:41","title":"Approximating Optimum Online for Capacitated Resource Allocation","abstract":"We study online capacitated resource allocation, a natural generalization of online stochastic max-weight bipartite matching. This problem is motivated by ride-sharing and Internet advertising applications, where online arrivals may have the capacity to serve multiple offline users.   Our main result is a polynomial-time online algorithm which is $(1/2 + \\kappa)$-approximate to the optimal online algorithm for $\\kappa = 0.0115$. This can be contrasted to the (tight) $1/2$-competitive algorithms to the optimum offline benchmark from the prophet inequality literature. Optimum online is a recently popular benchmark for online Bayesian problems which can use unbounded computation, but not \"prophetic\" knowledge of future inputs.   Our algorithm (which also works for the case of stochastic rewards) rounds a generalized LP relaxation from the unit-capacity case via a two-proposal algorithm, as in previous works in the online matching literature. A key technical challenge in deriving our guarantee is bounding the positive correlation among users introduced when rounding our LP relaxation online. Unlike in the case of unit capacities, this positive correlation is unavoidable for guarantees beyond $1/2$. Conceptually, our results show that the study of optimum online as a benchmark can reveal problem-specific insights that are irrelevant to competitive analysis.","sentences":["We study online capacitated resource allocation, a natural generalization of online stochastic max-weight bipartite matching.","This problem is motivated by ride-sharing and Internet advertising applications, where online arrivals may have the capacity to serve multiple offline users.   ","Our main result is a polynomial-time online algorithm which is $(1/2 + \\kappa)$-approximate to the optimal online algorithm for $\\kappa = 0.0115$. This can be contrasted to the (tight) $1/2$-competitive algorithms to the optimum offline benchmark from the prophet inequality literature.","Optimum online is a recently popular benchmark for online Bayesian problems which can use unbounded computation, but not \"prophetic\" knowledge of future inputs.   ","Our algorithm (which also works for the case of stochastic rewards) rounds a generalized LP relaxation from the unit-capacity case via a two-proposal algorithm, as in previous works in the online matching literature.","A key technical challenge in deriving our guarantee is bounding the positive correlation among users introduced when rounding our LP relaxation online.","Unlike in the case of unit capacities, this positive correlation is unavoidable for guarantees beyond $1/2$. Conceptually, our results show that the study of optimum online as a benchmark can reveal problem-specific insights that are irrelevant to competitive analysis."],"url":"http://arxiv.org/abs/2406.07757v1","category":"cs.DS"}
{"created":"2024-06-11 22:22:13","title":"A square root algorithm faster than Newton's method for multiprecision numbers, using floating-point arithmetic","abstract":"In this paper, an optimized version of classical Bombelli's algorithm for computing integer square roots is presented. In particular, floating-point arithmetic is used to compute the initial guess of each digit of the root, following similar ideas to those used in \"The Art of Computer Programming\" Vol. 2, p. 4.3.1 for division. A program with an implementation of the algorithm in Java is also presented, and its running time is compared with that of the algorithm provided by the Java standard library, which uses the Newton's method. From tests, the algorithm presented here turns out to be much faster.","sentences":["In this paper, an optimized version of classical Bombelli's algorithm for computing integer square roots is presented.","In particular, floating-point arithmetic is used to compute the initial guess of each digit of the root, following similar ideas to those used in \"The Art of Computer Programming\" Vol. 2, p. 4.3.1 for division.","A program with an implementation of the algorithm in Java is also presented, and its running time is compared with that of the algorithm provided by the Java standard library, which uses the Newton's method.","From tests, the algorithm presented here turns out to be much faster."],"url":"http://arxiv.org/abs/2406.07751v1","category":"cs.MS"}
{"created":"2024-06-11 21:08:37","title":"Nitsche stabilized Virtual element approximations for a Brinkman problem with mixed boundary conditions","abstract":"In this paper, we formulate, analyse and implement the discrete formulation of the Brinkman problem with mixed boundary conditions, including slip boundary condition, using the Nitsche's technique for virtual element methods. The divergence conforming virtual element spaces for the velocity function and piecewise polynomials for pressure are approached for the discrete scheme. We derive a robust stability analysis of the Nitsche stabilized discrete scheme for this model problem. We establish an optimal and vigorous a priori error estimates of the discrete scheme with constants independent of the viscosity. Moreover, a set of numerical tests demonstrates the robustness with respect to the physical parameters and verifies the derived convergence results.","sentences":["In this paper, we formulate, analyse and implement the discrete formulation of the Brinkman problem with mixed boundary conditions, including slip boundary condition, using the Nitsche's technique for virtual element methods.","The divergence conforming virtual element spaces for the velocity function and piecewise polynomials for pressure are approached for the discrete scheme.","We derive a robust stability analysis of the Nitsche stabilized discrete scheme for this model problem.","We establish an optimal and vigorous a priori error estimates of the discrete scheme with constants independent of the viscosity.","Moreover, a set of numerical tests demonstrates the robustness with respect to the physical parameters and verifies the derived convergence results."],"url":"http://arxiv.org/abs/2406.07724v1","category":"math.NA"}
{"created":"2024-06-11 21:04:21","title":"Ultra-fast Oxygen Conduction in Sill\u00e9n Oxychlorides","abstract":"Oxygen ion conductors are crucial for enhancing the efficiency of various clean energy technologies, including fuel cells, batteries, electrolyzers, membranes, sensors, and more. In this study, LaBi2O4Cl is identified as an ultra-fast oxygen conductor from the MBi2O4X (M=rare-earth element, X=halogen element) family, discovered by a structure-similarity analysis of >60k oxygen-containing compounds. Ab initio studies reveal that LaBi2O4Cl has an ultra-low migration barrier of 0.1 eV for oxygen vacancy, significantly lower than 0.6-0.8 eV for interstitial oxygen. Frenkel pairs are the dominant defects in intrinsic LaBi2O4Cl, facilitating notable oxygen diffusion primarily through vacancies at higher temperatures. LaBi2O4Cl with extrinsic oxygen vacancies (2.8%) exhibits a conductivity of 0.3 S/cm at 25{\\deg}C, maintains a 0.1 eV diffusion barrier up to 1100{\\deg}C, and transitions from extrinsic to mixed extrinsic and intrinsic behavior as the Frenkel pair concentration increases at higher temperatures. Experimental results on synthesized LaBi2O4Cl and Sr-doped LaBi2O4Cl demonstrate comparable or higher oxygen conductivity than YSZ and LSGM below 400 {\\deg}C, with lower activation energies. Further experimental optimization of LaBi2O4Cl, including aliovalent doping and microstructure refinement, could significantly enhance its performance and efficiency, facilitating fast oxygen conduction approaching room temperature.","sentences":["Oxygen ion conductors are crucial for enhancing the efficiency of various clean energy technologies, including fuel cells, batteries, electrolyzers, membranes, sensors, and more.","In this study, LaBi2O4Cl is identified as an ultra-fast oxygen conductor from the MBi2O4X (M=rare-earth element, X=halogen element) family, discovered by a structure-similarity analysis of >60k oxygen-containing compounds.","Ab initio studies reveal that LaBi2O4Cl has an ultra-low migration barrier of 0.1 eV for oxygen vacancy, significantly lower than 0.6-0.8 eV for interstitial oxygen.","Frenkel pairs are the dominant defects in intrinsic LaBi2O4Cl, facilitating notable oxygen diffusion primarily through vacancies at higher temperatures.","LaBi2O4Cl with extrinsic oxygen vacancies (2.8%) exhibits a conductivity of 0.3 S/cm at 25{\\deg}C, maintains a 0.1 eV diffusion barrier up to 1100{\\deg}C, and transitions from extrinsic to mixed extrinsic and intrinsic behavior as the Frenkel pair concentration increases at higher temperatures.","Experimental results on synthesized LaBi2O4Cl and Sr-doped LaBi2O4Cl demonstrate comparable or higher oxygen conductivity than YSZ and LSGM below 400 {\\deg}C, with lower activation energies.","Further experimental optimization of LaBi2O4Cl, including aliovalent doping and microstructure refinement, could significantly enhance its performance and efficiency, facilitating fast oxygen conduction approaching room temperature."],"url":"http://arxiv.org/abs/2406.07723v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-11 20:46:29","title":"Multi-objective optimization for multi-agent injection strategies in subsurface CO$_2$ storage","abstract":"We propose a novel framework for optimizing injection strategies in large-scale CO$_2$ storage combining multi-agent models with multi-objective optimization, and reservoir simulation. We investigate whether agents should form coalitions for collaboration to maximize the outcome of their storage activities. In multi-agent systems, it is typically assumed that the optimal strategy for any given coalition structure is already known, and it remains to identify which coalition structure is optimal according to some predefined criterion. For any coalition structure in this work, the optimal CO$_2$ injection strategy is not a priori known, and needs to be found by a combination of reservoir simulation and a multi-objective optimization problem. The multi-objective optimization problems all come with the numerical challenges of repeated evaluations of complex-physics models. We use versatile evolutionary algorithms to solve the multi-objective optimization problems, where the solution is a set of values, e.g., a Pareto front. The Pareto fronts are first computed using the so-called weighted sum method that transforms the multi-objective optimization problem into a set of single-objective optimization problems. Results based on two different Pareto front selection criteria are presented. Then a truly multi-objective optimization method is used to obtain the Pareto fronts, and compared to the previous weighted sum method. We demonstrate the proposed framework on the Bjarmeland formation, a pressure-limited prospective storage site in the Barents Sea. The problem is constrained by the maximum sustainable pressure buildup and a supply of CO$_2$ that can vary over time. In addition to identifying the optimal coalitions, the methodology shows how distinct suboptimal coalitions perform in comparison to the optimum.","sentences":["We propose a novel framework for optimizing injection strategies in large-scale CO$_2$ storage combining multi-agent models with multi-objective optimization, and reservoir simulation.","We investigate whether agents should form coalitions for collaboration to maximize the outcome of their storage activities.","In multi-agent systems, it is typically assumed that the optimal strategy for any given coalition structure is already known, and it remains to identify which coalition structure is optimal according to some predefined criterion.","For any coalition structure in this work, the optimal CO$_2$ injection strategy is not a priori known, and needs to be found by a combination of reservoir simulation and a multi-objective optimization problem.","The multi-objective optimization problems all come with the numerical challenges of repeated evaluations of complex-physics models.","We use versatile evolutionary algorithms to solve the multi-objective optimization problems, where the solution is a set of values, e.g., a Pareto front.","The Pareto fronts are first computed using the so-called weighted sum method that transforms the multi-objective optimization problem into a set of single-objective optimization problems.","Results based on two different Pareto front selection criteria are presented.","Then a truly multi-objective optimization method is used to obtain the Pareto fronts, and compared to the previous weighted sum method.","We demonstrate the proposed framework on the Bjarmeland formation, a pressure-limited prospective storage site in the Barents Sea.","The problem is constrained by the maximum sustainable pressure buildup and a supply of CO$_2$ that can vary over time.","In addition to identifying the optimal coalitions, the methodology shows how distinct suboptimal coalitions perform in comparison to the optimum."],"url":"http://arxiv.org/abs/2406.07711v1","category":"math.NA"}
{"created":"2024-06-11 20:44:04","title":"Diagnosing and fixing common problems in Bayesian optimization for molecule design","abstract":"Bayesian optimization (BO) is a principled approach to molecular design tasks. In this paper we explain three pitfalls of BO which can cause poor empirical performance: an incorrect prior width, over-smoothing, and inadequate acquisition function maximization. We show that with these issues addressed, even a basic BO setup is able to achieve the highest overall performance on the PMO benchmark for molecule design (Gao et al, 2022). These results suggest that BO may benefit from more attention in the machine learning for molecules community.","sentences":["Bayesian optimization (BO) is a principled approach to molecular design tasks.","In this paper we explain three pitfalls of BO which can cause poor empirical performance: an incorrect prior width, over-smoothing, and inadequate acquisition function maximization.","We show that with these issues addressed, even a basic BO setup is able to achieve the highest overall performance on the PMO benchmark for molecule design (Gao et al, 2022).","These results suggest that BO may benefit from more attention in the machine learning for molecules community."],"url":"http://arxiv.org/abs/2406.07709v1","category":"cs.LG"}
{"created":"2024-06-11 20:25:16","title":"Angular distribution of electron emission from ultrafast nanotip sources","abstract":"We investigate the angular distribution of ultrafast laser-induced electron emission from a tungsten nanotip in the multiphoton regime. A theoretical model allows precise determination of the relative contribution of different electron emission mechanisms, revealing connections between emission mechanism and the angular distribution of emitted electrons. We infer a continuous map of the work function across the surface of the tip, which in combination with the model can be used to predict values including the number of electrons per pulse and the angular divergence of the resulting beam as a function of laser power and tip voltage for (310)-oriented tungsten nanotips. The model is straightforward to implement and can be used to optimize the performance of instruments using ultrafast nanotip electron sources.","sentences":["We investigate the angular distribution of ultrafast laser-induced electron emission from a tungsten nanotip in the multiphoton regime.","A theoretical model allows precise determination of the relative contribution of different electron emission mechanisms, revealing connections between emission mechanism and the angular distribution of emitted electrons.","We infer a continuous map of the work function across the surface of the tip, which in combination with the model can be used to predict values including the number of electrons per pulse and the angular divergence of the resulting beam as a function of laser power and tip voltage for (310)-oriented tungsten nanotips.","The model is straightforward to implement and can be used to optimize the performance of instruments using ultrafast nanotip electron sources."],"url":"http://arxiv.org/abs/2406.07697v1","category":"physics.app-ph"}
{"created":"2024-06-11 19:59:05","title":"Optimized QUBO formulation methods for quantum computing","abstract":"Several combinatorial optimization problems can be solved with NISQ devices once that a corresponding quadratic unconstrained binary optimization (QUBO) form is derived. The aim of this work is to drastically reduce the variables needed for these QUBO reformulations in order to unlock the possibility to efficiently obtain optimal solutions for a class of optimization problems with NISQ devices. This is achieved by introducing novel tools that allow an efficient use of slack variables, even for problems with non-linear constraints, without the need to approximate the starting problem. We divide our new techniques in two independent parts, called the iterative quadratic polynomial and the master-satellite methods. Hence, we show how to apply our techniques in case of an NP-hard optimization problem inspired by a real-world financial scenario called Max-Profit Balance Settlement. We follow by submitting several instances of this problem to two D-wave quantum annealers, comparing the performances of our novel approach with the standard methods used in these scenarios. Moreover, this study allows to appreciate several performance differences between the D-wave Advantage and Advantage2 quantum annealers.","sentences":["Several combinatorial optimization problems can be solved with NISQ devices once that a corresponding quadratic unconstrained binary optimization (QUBO) form is derived.","The aim of this work is to drastically reduce the variables needed for these QUBO reformulations in order to unlock the possibility to efficiently obtain optimal solutions for a class of optimization problems with NISQ devices.","This is achieved by introducing novel tools that allow an efficient use of slack variables, even for problems with non-linear constraints, without the need to approximate the starting problem.","We divide our new techniques in two independent parts, called the iterative quadratic polynomial and the master-satellite methods.","Hence, we show how to apply our techniques in case of an NP-hard optimization problem inspired by a real-world financial scenario called Max-Profit Balance Settlement.","We follow by submitting several instances of this problem to two D-wave quantum annealers, comparing the performances of our novel approach with the standard methods used in these scenarios.","Moreover, this study allows to appreciate several performance differences between the D-wave Advantage and Advantage2 quantum annealers."],"url":"http://arxiv.org/abs/2406.07681v1","category":"quant-ph"}
{"created":"2024-06-11 19:46:26","title":"Generalized Zeno effect and entanglement dynamics induced by fermion counting","abstract":"We study a one-dimensional lattice system of free fermions subjected to a generalized measurement process: the system exchanges particles with its environment, but each fermion leaving or entering the system is counted. In contrast to the freezing of dynamics due to frequent measurements of lattice-site occupation numbers, a high rate of fermion counts induces fast fluctuations in the state of the system. Still, through numerical simulations of quantum trajectories and an analytical approach based on replica Keldysh field theory, we find that instantaneous correlations and entanglement properties of free fermions subjected to fermion counting and local occupation measurements are strikingly similar. We explain this similarity through a generalized Zeno effect induced by fermion counting and a universal long-wavelength description in terms of an $\\mathrm{SU}(R)$ nonlinear sigma model. Further, for both types of measurement processes, we present strong evidence against the existence of a critical phase with logarithmic entanglement and conformal invariance at finite measurement rates. Instead, we identify a well-defined and finite critical range of length scales on which signatures of conformal invariance are observable. While area-law entanglement is established beyond a scale that is exponentially large in the measurement rate, the upper boundary of the critical range is only algebraically large and thus numerically accessible.","sentences":["We study a one-dimensional lattice system of free fermions subjected to a generalized measurement process: the system exchanges particles with its environment, but each fermion leaving or entering the system is counted.","In contrast to the freezing of dynamics due to frequent measurements of lattice-site occupation numbers, a high rate of fermion counts induces fast fluctuations in the state of the system.","Still, through numerical simulations of quantum trajectories and an analytical approach based on replica Keldysh field theory, we find that instantaneous correlations and entanglement properties of free fermions subjected to fermion counting and local occupation measurements are strikingly similar.","We explain this similarity through a generalized Zeno effect induced by fermion counting and a universal long-wavelength description in terms of an $\\mathrm{SU}(R)$ nonlinear sigma model.","Further, for both types of measurement processes, we present strong evidence against the existence of a critical phase with logarithmic entanglement and conformal invariance at finite measurement rates.","Instead, we identify a well-defined and finite critical range of length scales on which signatures of conformal invariance are observable.","While area-law entanglement is established beyond a scale that is exponentially large in the measurement rate, the upper boundary of the critical range is only algebraically large and thus numerically accessible."],"url":"http://arxiv.org/abs/2406.07673v1","category":"quant-ph"}
{"created":"2024-06-11 19:20:45","title":"A Unified Framework for Integer Programming Formulation of Graph Matching Problems","abstract":"Graph theory has been a powerful tool in solving difficult and complex problems arising in all disciplines. In particular, graph matching is a classical problem in pattern analysis with enormous applications. Many graph problems have been formulated as a mathematical program and then solved using exact, heuristic, and/or approximated-guaranteed procedures. On the other hand, graph theory has been a powerful tool in visualizing and understanding complex mathematical programming problems, especially integer programs. Formulating a graph problem as a natural integer program (IP) is often a challenging task. However, an IP formulation of the problem has many advantages. Several researchers have noted the need for natural IP formulation of graph theoretic problems. The present study aims to provide a unified framework for IP formulation of graph-matching problems. Although there are many surveys on graph matching problems, none is concerned with IP formulation. This paper is the first to provide a comprehensive IP formulation for such problems. The framework includes a variety of graph optimization problems in the literature. While these problems have been studied by different research communities, however, the framework presented here helps to bring efforts from different disciplines to tackle such diverse and complex problems. We hope the present study can significantly help to simplify some of the difficult problems arising in practice, especially in pattern analysis.","sentences":["Graph theory has been a powerful tool in solving difficult and complex problems arising in all disciplines.","In particular, graph matching is a classical problem in pattern analysis with enormous applications.","Many graph problems have been formulated as a mathematical program and then solved using exact, heuristic, and/or approximated-guaranteed procedures.","On the other hand, graph theory has been a powerful tool in visualizing and understanding complex mathematical programming problems, especially integer programs.","Formulating a graph problem as a natural integer program (IP) is often a challenging task.","However, an IP formulation of the problem has many advantages.","Several researchers have noted the need for natural IP formulation of graph theoretic problems.","The present study aims to provide a unified framework for IP formulation of graph-matching problems.","Although there are many surveys on graph matching problems, none is concerned with IP formulation.","This paper is the first to provide a comprehensive IP formulation for such problems.","The framework includes a variety of graph optimization problems in the literature.","While these problems have been studied by different research communities, however, the framework presented here helps to bring efforts from different disciplines to tackle such diverse and complex problems.","We hope the present study can significantly help to simplify some of the difficult problems arising in practice, especially in pattern analysis."],"url":"http://arxiv.org/abs/2406.07666v1","category":"cs.DS"}
{"created":"2024-06-11 18:46:27","title":"Recovery of resource through sequential noisy measurements","abstract":"Noisy unsharp measurements incorporated in quantum information protocols may hinder performance, reducing the quantum advantage. However, we show that, unlike projective measurements which completely destroy quantum correlations between nodes in quantum networks, sequential applications of noisy measurements can mitigate the adverse impact of noise in the measurement device on quantum information processing tasks. We demonstrate this in the case of concentrating entanglement on chosen nodes in quantum networks via noisy measurements performed by assisting qubits. In the case of networks with a cluster of three or higher number of qubits, we exhibit that sequentially performing optimal unsharp measurements on the assisting qubits yields localizable entanglement between two nodes akin to that obtained by optimal projective measurements on the same assisting qubits. Furthermore, we find that the proposed approach using consecutive noisy measurements can potentially be used to prepare desired states that are resource for specific quantum schemes. We also argue that assisting qubits have greater control over the qubits on which entanglement is concentrated via unsharp measurements, in contrast to sharp measurement-based protocols, which may have implications for secure quantum communication.","sentences":["Noisy unsharp measurements incorporated in quantum information protocols may hinder performance, reducing the quantum advantage.","However, we show that, unlike projective measurements which completely destroy quantum correlations between nodes in quantum networks, sequential applications of noisy measurements can mitigate the adverse impact of noise in the measurement device on quantum information processing tasks.","We demonstrate this in the case of concentrating entanglement on chosen nodes in quantum networks via noisy measurements performed by assisting qubits.","In the case of networks with a cluster of three or higher number of qubits, we exhibit that sequentially performing optimal unsharp measurements on the assisting qubits yields localizable entanglement between two nodes akin to that obtained by optimal projective measurements on the same assisting qubits.","Furthermore, we find that the proposed approach using consecutive noisy measurements can potentially be used to prepare desired states that are resource for specific quantum schemes.","We also argue that assisting qubits have greater control over the qubits on which entanglement is concentrated via unsharp measurements, in contrast to sharp measurement-based protocols, which may have implications for secure quantum communication."],"url":"http://arxiv.org/abs/2406.07652v1","category":"quant-ph"}
{"created":"2024-06-11 18:18:46","title":"Regularizing Numerical Extremals Along Singular Arcs: A Lie-Theoretic Approach","abstract":"Numerical ``direct'' approaches to time-optimal control often fail to find solutions that are singular in the sense of the Pontryagin Maximum Principle, performing better when searching for saturated (bang-bang) solutions. In previous work by one of the authors, singular solutions were shown to exist for the time-optimal control problem for fully actuated mechanical systems under hard torque constraints. Explicit formulas, based on a Lie theoretic analysis of the problem, were given for singular segments of trajectories, but the global structure of solutions remains unknown. In this work, we review the aforementioned framework, and show how to effectively combine these formulas with the use of general-purpose optimal control software packages. By using the explicit formula given by the theory in the intervals where the numerical solution enters a singular arc, we not only obtain an algebraic expression for the control in that interval but we are also able to remove artifacts present in the numerical solution. In this way, the best features of numerical algorithms and theory complement each other and provide a better picture of the global optimal structure. We illustrate the technique on a two degree of freedom robotic arm example, using two distinct optimal control numerical software packages running on different programming languages.","sentences":["Numerical ``direct'' approaches to time-optimal control often fail to find solutions that are singular in the sense of the Pontryagin Maximum Principle, performing better when searching for saturated (bang-bang) solutions.","In previous work by one of the authors, singular solutions were shown to exist for the time-optimal control problem for fully actuated mechanical systems under hard torque constraints.","Explicit formulas, based on a Lie theoretic analysis of the problem, were given for singular segments of trajectories, but the global structure of solutions remains unknown.","In this work, we review the aforementioned framework, and show how to effectively combine these formulas with the use of general-purpose optimal control software packages.","By using the explicit formula given by the theory in the intervals where the numerical solution enters a singular arc, we not only obtain an algebraic expression for the control in that interval but we are also able to remove artifacts present in the numerical solution.","In this way, the best features of numerical algorithms and theory complement each other and provide a better picture of the global optimal structure.","We illustrate the technique on a two degree of freedom robotic arm example, using two distinct optimal control numerical software packages running on different programming languages."],"url":"http://arxiv.org/abs/2406.07644v1","category":"eess.SY"}
{"created":"2024-06-11 18:00:46","title":"Bipartite Matching in Massive Graphs: A Tight Analysis of EDCS","abstract":"Maximum matching is one of the most fundamental combinatorial optimization problems with applications in various contexts such as balanced clustering, data mining, resource allocation, and online advertisement. In many of these applications, the input graph is massive. The sheer size of these inputs makes it impossible to store the whole graph in the memory of a single machine and process it there. Graph sparsification has been an extremely powerful tool to alleviate this problem. In this paper, we study a highly successful and versatile sparsifier for the matching problem: the *edge-degree constrained subgraph (EDCS)* introduced first by Bernstein and Stein [ICALP'15].   The EDCS has a parameter $\\beta \\geq 2$ which controls the density of the sparsifier. It has been shown through various proofs in the literature that by picking a subgraph with $O(n\\beta)$ edges, the EDCS includes a matching of size at least $2/3-O(1/\\beta)$ times the maximum matching size. As such, by increasing $\\beta$ the approximation ratio of EDCS gets closer and closer to $2/3$.   In this paper, we propose a new approach for analyzing the approximation ratio of EDCS. Our analysis is *tight* for any value of $\\beta$. Namely, we pinpoint the precise approximation ratio of EDCS for any sparsity parameter $\\beta$. Our analysis reveals that one does not necessarily need to increase $\\beta$ to improve approximation, as suggested by previous analysis. In particular, the best choice turns out to be $\\beta = 6$, which achieves an approximation ratio of $.677$! This is arguably surprising as it is even better than $2/3 \\sim .666$, the bound that was widely believed to be the limit for EDCS.","sentences":["Maximum matching is one of the most fundamental combinatorial optimization problems with applications in various contexts such as balanced clustering, data mining, resource allocation, and online advertisement.","In many of these applications, the input graph is massive.","The sheer size of these inputs makes it impossible to store the whole graph in the memory of a single machine and process it there.","Graph sparsification has been an extremely powerful tool to alleviate this problem.","In this paper, we study a highly successful and versatile sparsifier for the matching problem: the *edge-degree constrained subgraph (EDCS)* introduced first by Bernstein and Stein [ICALP'15].   ","The EDCS has a parameter $\\beta \\geq 2$ which controls the density of the sparsifier.","It has been shown through various proofs in the literature that by picking a subgraph with $O(n\\beta)$ edges, the EDCS includes a matching of size at least $2/3-O(1/\\beta)$ times the maximum matching size.","As such, by increasing $\\beta$ the approximation ratio of EDCS gets closer and closer to $2/3$.   In this paper, we propose a new approach for analyzing the approximation ratio of EDCS.","Our analysis is *tight* for any value of $\\beta$. Namely, we pinpoint the precise approximation ratio of EDCS for any sparsity parameter $\\beta$. Our analysis reveals that one does not necessarily need to increase $\\beta$ to improve approximation, as suggested by previous analysis.","In particular, the best choice turns out to be $\\beta = 6$, which achieves an approximation ratio of $.677$!","This is arguably surprising as it is even better than $2/3 \\sim .666$, the bound that was widely believed to be the limit for EDCS."],"url":"http://arxiv.org/abs/2406.07630v1","category":"cs.DS"}
{"created":"2024-06-11 18:00:01","title":"Readout Error Mitigation for Mid-Circuit Measurements and Feedforward","abstract":"Current-day quantum computing platforms are subject to readout errors, in which faulty measurement outcomes are reported by the device. On circuits with mid-circuit measurements and feedforward, readout noise can cause incorrect conditional quantum operations to be applied on a per-shot basis. Standard readout error mitigation methods for terminal measurements which act in post-processing do not suffice in this context. Here we present a general method for readout error mitigation for expectation values on circuits containing an arbitrary number of layers of mid-circuit measurements and feedforward, at zero circuit depth and two-qubit gate count cost. The protocol uses a form of gate twirling for symmetrization of the error channels and probabilistic bit-flips in feedforward data to average over an ensemble of quantum trajectories. The mitigated estimator is unbiased and has a sampling overhead of ${\\sim} 1 / (1 - 2 r)^m$ for $m$ total measurements and characteristic readout error rate $r$ per measurement. We demonstrate the effectiveness of our method, obtaining up to a ${\\sim} 60\\%$ reduction in error on superconducting quantum processors for several examples of feedforward circuits of practical interest, including dynamic qubit resets, shallow-depth GHZ state preparation, and multi-stage quantum state teleportation.","sentences":["Current-day quantum computing platforms are subject to readout errors, in which faulty measurement outcomes are reported by the device.","On circuits with mid-circuit measurements and feedforward, readout noise can cause incorrect conditional quantum operations to be applied on a per-shot basis.","Standard readout error mitigation methods for terminal measurements which act in post-processing do not suffice in this context.","Here we present a general method for readout error mitigation for expectation values on circuits containing an arbitrary number of layers of mid-circuit measurements and feedforward, at zero circuit depth and two-qubit gate count cost.","The protocol uses a form of gate twirling for symmetrization of the error channels and probabilistic bit-flips in feedforward data to average over an ensemble of quantum trajectories.","The mitigated estimator is unbiased and has a sampling overhead of ${\\sim} 1 / (1 - 2 r)^m$ for $m$ total measurements and characteristic readout error rate $r$ per measurement.","We demonstrate the effectiveness of our method, obtaining up to a ${\\sim} 60\\%$ reduction in error on superconducting quantum processors for several examples of feedforward circuits of practical interest, including dynamic qubit resets, shallow-depth GHZ state preparation, and multi-stage quantum state teleportation."],"url":"http://arxiv.org/abs/2406.07611v1","category":"quant-ph"}
{"created":"2024-06-12 17:45:28","title":"Towards Musically Informed Evaluation of Piano Transcription Models","abstract":"Automatic piano transcription models are typically evaluated using simple frame- or note-wise information retrieval (IR) metrics. Such benchmark metrics do not provide insights into the transcription quality of specific musical aspects such as articulation, dynamics, or rhythmic precision of the output, which are essential in the context of expressive performance analysis. Furthermore, in recent years, MAESTRO has become the de-facto training and evaluation dataset for such models. However, inference performance has been observed to deteriorate substantially when applied on out-of-distribution data, thereby questioning the suitability and reliability of transcribed outputs from such models for specific MIR tasks. In this work, we investigate the performance of three state-of-the-art piano transcription models in two experiments. In the first one, we propose a variety of musically informed evaluation metrics which, in contrast to the IR metrics, offer more detailed insight into the musical quality of the transcriptions. In the second experiment, we compare inference performance on real-world and perturbed audio recordings, and highlight musical dimensions which our metrics can help explain. Our experimental results highlight the weaknesses of existing piano transcription metrics and contribute to a more musically sound error analysis of transcription outputs.","sentences":["Automatic piano transcription models are typically evaluated using simple frame- or note-wise information retrieval (IR) metrics.","Such benchmark metrics do not provide insights into the transcription quality of specific musical aspects such as articulation, dynamics, or rhythmic precision of the output, which are essential in the context of expressive performance analysis.","Furthermore, in recent years, MAESTRO has become the de-facto training and evaluation dataset for such models.","However, inference performance has been observed to deteriorate substantially when applied on out-of-distribution data, thereby questioning the suitability and reliability of transcribed outputs from such models for specific MIR tasks.","In this work, we investigate the performance of three state-of-the-art piano transcription models in two experiments.","In the first one, we propose a variety of musically informed evaluation metrics which, in contrast to the IR metrics, offer more detailed insight into the musical quality of the transcriptions.","In the second experiment, we compare inference performance on real-world and perturbed audio recordings, and highlight musical dimensions which our metrics can help explain.","Our experimental results highlight the weaknesses of existing piano transcription metrics and contribute to a more musically sound error analysis of transcription outputs."],"url":"http://arxiv.org/abs/2406.08454v1","category":"cs.SD"}
{"created":"2024-06-12 16:59:00","title":"Tailoring wetting properties of organic hole-transport interlayers for slot-die coated perovskite solar modules","abstract":"The use of self-assembled monolayers (SAMs) with anchoring groups was considered as an effective approach for interface engineering in perovskite solar cells with metal oxide charge transporting layers. However, the coating of SAM layers in PSMs by means of a slot-die is a challenging process due to the low viscosity of the solutions and the low wettability of the films. In this study, we integrate a triphenylamine-based polymer, pTPA-TDP, blended with SAM based on 5-[4-[4-(diphenylamino) phenyl] thiophene-2-carboxylic acid (TPATC), to address the challenges of uniform slot-die coating and interface passivation in large-area modules. We fabricated p-i-n oriented PSMs on 50x50 mm2 substrates (12-sub-cells) with NiO hole transport layer (HTL) and organic interlayers for surface modification. Wetting angle mapping demonstrated that ununiform regions of the slot-die coated SAM have hydrophobicity with contact angle values up to 90{\\deg}, causing fluctuations in absorber thickness and the presence of macro-defects at buried interfaces. The incorporation of the blended interlayer to NiO/perovskite junction homogenized the surface wettability (contact angle=40{\\deg}) and mitigated lattice strain in the absorber. This enabled the effective use of SAM properties on a large-area surface, improving energy level alignment and enhancing the power conversion efficiency (PCE) of the modules from 13.98% to 15.83% and stability (ISOS-L-2, T80 period) from 500-1000 hours to 1630 hours. Investigation of PSMs upon cooling till -5 {\\deg}C showed that the PCE increased by +0.19%/{\\deg}C for samples with NiO HTL, while using SAM and blended interlayers raised the coefficient to ~0.40%/{\\deg}C due to changes in activation energy and trap contributions to device performance across a wide temperature range.","sentences":["The use of self-assembled monolayers (SAMs) with anchoring groups was considered as an effective approach for interface engineering in perovskite solar cells with metal oxide charge transporting layers.","However, the coating of SAM layers in PSMs by means of a slot-die is a challenging process due to the low viscosity of the solutions and the low wettability of the films.","In this study, we integrate a triphenylamine-based polymer, pTPA-TDP, blended with SAM based on 5-[4-[4-(diphenylamino) phenyl] thiophene-2-carboxylic acid (TPATC), to address the challenges of uniform slot-die coating and interface passivation in large-area modules.","We fabricated p-i-n oriented PSMs on 50x50 mm2 substrates (12-sub-cells) with NiO hole transport layer (HTL) and organic interlayers for surface modification.","Wetting angle mapping demonstrated that ununiform regions of the slot-die coated SAM have hydrophobicity with contact angle values up to 90{\\deg}, causing fluctuations in absorber thickness and the presence of macro-defects at buried interfaces.","The incorporation of the blended interlayer to NiO/perovskite junction homogenized the surface wettability (contact angle=40{\\deg}) and mitigated lattice strain in the absorber.","This enabled the effective use of SAM properties on a large-area surface, improving energy level alignment and enhancing the power conversion efficiency (PCE) of the modules from 13.98% to 15.83% and stability (ISOS-L-2, T80 period) from 500-1000 hours to 1630 hours.","Investigation of PSMs upon cooling till -5 {\\deg}C showed that the PCE increased by +0.19%/{\\deg}C for samples with NiO HTL, while using SAM and blended interlayers raised the coefficient to ~0.40%/{\\deg}C due to changes in activation energy and trap contributions to device performance across a wide temperature range."],"url":"http://arxiv.org/abs/2406.08415v1","category":"physics.app-ph"}
{"created":"2024-06-12 16:53:28","title":"On the asymptotic density of states in solvable models of strings","abstract":"We present a closed formula for the asymptotic density of states for a class of solvable superstring models on curved backgrounds. The result accounts for the effects of the curvature of the target space in a concise way.","sentences":["We present a closed formula for the asymptotic density of states for a class of solvable superstring models on curved backgrounds.","The result accounts for the effects of the curvature of the target space in a concise way."],"url":"http://arxiv.org/abs/2406.08405v1","category":"hep-th"}
{"created":"2024-06-12 16:18:27","title":"An Untargeted Search for Radio-Emitting Tidal Disruption Events in the VAST Pilot Survey","abstract":"We present a systematic search for tidal disruption events (TDEs) using radio data from the Variables and Slow Transients (VAST) Pilot Survey conducted using the Australian Square Kilometre Array Pathfinder (ASKAP). Historically, TDEs have been identified using observations at X-ray, optical, and ultraviolet wavelengths. After discovery, a few dozen TDEs have been shown to have radio counterparts through follow-up observations. With systematic time-domain radio surveys becoming available, we can now identify new TDEs in the radio regime. A population of radio-discovered TDEs has the potential to provide several key insights including an independent constraint on their volumetric rate. We conducted a search to select variable radio sources with a single prominent radio flare and a position consistent within 2$\\sigma$ of the nucleus of a known galaxy. While TDEs were the primary target of our search, sources identified in this search may also be consistent with active galactic nuclei exhibiting unusual flux density changes at the timescales probed, uncharacteristically bright supernovae, or a population of gamma-ray bursts. We identify a sample of 12 radio-bright candidate TDEs. The timescales and luminosities range from ~6 to 230 days and ~10$^{38}$ to 10$^{41}$ erg s$^{-1}$, consistent with models of radio emission from TDEs that launch relativistic jets. After calculating the detection efficiency of our search using a Monte Carlo simulation of TDEs, and assuming all 12 sources are jetted TDEs, we derive a volumetric rate for jetted TDEs of 0.80$^{+0.31}_{-0.23}$ Gpc$^{-3}$ yr$^{-1}$, consistent with previous empirically estimated rates.","sentences":["We present a systematic search for tidal disruption events (TDEs) using radio data from the Variables and Slow Transients (VAST) Pilot Survey conducted using the Australian Square Kilometre Array Pathfinder (ASKAP).","Historically, TDEs have been identified using observations at X-ray, optical, and ultraviolet wavelengths.","After discovery, a few dozen TDEs have been shown to have radio counterparts through follow-up observations.","With systematic time-domain radio surveys becoming available, we can now identify new TDEs in the radio regime.","A population of radio-discovered TDEs has the potential to provide several key insights including an independent constraint on their volumetric rate.","We conducted a search to select variable radio sources with a single prominent radio flare and a position consistent within 2$\\sigma$ of the nucleus of a known galaxy.","While TDEs were the primary target of our search, sources identified in this search may also be consistent with active galactic nuclei exhibiting unusual flux density changes at the timescales probed, uncharacteristically bright supernovae, or a population of gamma-ray bursts.","We identify a sample of 12 radio-bright candidate TDEs.","The timescales and luminosities range from ~6 to 230 days and ~10$^{38}$ to 10$^{41}$ erg s$^{-1}$, consistent with models of radio emission from TDEs that launch relativistic jets.","After calculating the detection efficiency of our search using a Monte Carlo simulation of TDEs, and assuming all 12 sources are jetted TDEs, we derive a volumetric rate for jetted TDEs of 0.80$^{+0.31}_{-0.23}$ Gpc$^{-3}$ yr$^{-1}$, consistent with previous empirically estimated rates."],"url":"http://arxiv.org/abs/2406.08371v1","category":"astro-ph.HE"}
{"created":"2024-06-12 16:16:06","title":"Kink-Helium Interactions in Tungsten: Opposing Effects of Assisted Nucleation and Hindered Migration","abstract":"Point defects such as interstitial atoms are known to be attracted to screw dislocations. Understanding these interaction mechanisms is key to predicting the plasticity of real materials. Using a potential derived from ab initio calculations of helium in tungsten, we find that the binding of a helium atom to dislocation kinks is significantly more favorable than to a straight dislocation. This preference directly translates to a reduction in the kink pair nucleation energy, provided that the mechanism proceeds through a helium-stabilised kink. It is lowered from 1.64 eV (in the pure metal) to 0.51 eV when helium binds to the right kink. However, binding also has a competing effect whereby the kinks are unable to migrate. This hindering aspect becomes more relevant as the helium concentration increases.","sentences":["Point defects such as interstitial atoms are known to be attracted to screw dislocations.","Understanding these interaction mechanisms is key to predicting the plasticity of real materials.","Using a potential derived from ab initio calculations of helium in tungsten, we find that the binding of a helium atom to dislocation kinks is significantly more favorable than to a straight dislocation.","This preference directly translates to a reduction in the kink pair nucleation energy, provided that the mechanism proceeds through a helium-stabilised kink.","It is lowered from 1.64 eV (in the pure metal) to 0.51 eV when helium binds to the right kink.","However, binding also has a competing effect whereby the kinks are unable to migrate.","This hindering aspect becomes more relevant as the helium concentration increases."],"url":"http://arxiv.org/abs/2406.08368v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-12 16:03:35","title":"Reactor Antineutrino Directionality Measurement with the PROSPECT-I Detector","abstract":"The PROSPECT-I detector has several features that enable measurement of the direction of a compact neutrino source. In this paper, a detailed report on the directional measurements made on electron antineutrinos emitted from the High Flux Isotope Reactor is presented. With an estimated true neutrino (reactor to detector) direction of $\\phi = 40.8\\unicode{xB0} \\pm 0.7\\unicode{xB0}$ and $\\theta = 98.6\\unicode{xB0} \\pm 0.4\\unicode{xB0}$, the PROSPECT-I detector is able to reconstruct an average neutrino direction of $\\phi = 39.4\\unicode{xB0} \\pm 2.9\\unicode{xB0}$ and $\\theta = 97.6\\unicode{xB0} \\pm 1.6\\unicode{xB0}$. This measurement is made with approximately 48000 Inverse Beta Decay signal events and is the most precise directional reconstruction of reactor antineutrinos to date.","sentences":["The PROSPECT-I detector has several features that enable measurement of the direction of a compact neutrino source.","In this paper, a detailed report on the directional measurements made on electron antineutrinos emitted from the High Flux Isotope Reactor is presented.","With an estimated true neutrino (reactor to detector) direction of $\\phi = 40.8\\unicode{xB0} \\pm 0.7\\unicode{xB0}$ and $\\theta = 98.6\\unicode{xB0} \\pm 0.4\\unicode{xB0}$, the PROSPECT-I detector is able to reconstruct an average neutrino direction of $\\phi = 39.4\\unicode{xB0} \\pm 2.9\\unicode{xB0}$ and $\\theta = 97.6\\unicode{xB0} \\pm 1.6\\unicode{xB0}$.","This measurement is made with approximately 48000 Inverse Beta Decay signal events and is the most precise directional reconstruction of reactor antineutrinos to date."],"url":"http://arxiv.org/abs/2406.08359v1","category":"nucl-ex"}
{"created":"2024-06-12 15:55:40","title":"The atmospheric composition of the ultra-hot Jupiter WASP-178 b observed with ESPRESSO","abstract":"We search for atmospheric constituents for the UHJ WASP-178 b with two ESPRESSO transits using the narrow-band and cross-correlation techniques, focusing on the detections of NaI, H$\\alpha$, H$\\beta$, H$\\gamma$, MgI, FeI and FeII. Additionally, we show parallel photometry used to obtain updated and precise stellar, planetary and orbital parameters. We report the resolved line detections of NaI (5.5 and 5.4 $\\sigma$), H$\\alpha$ (13 $\\sigma$), H$\\beta$ (7.1 $\\sigma$), and tentatively MgI (4.6 $\\sigma$). In cross-correlation, we confirm the MgI detection (7.8 and 5.8 $\\sigma$) and additionally report the detections of FeI (12 and 10 $\\sigma$) and FeII (11 and 8.4 $\\sigma$), on both nights separately. The detection of MgI remains tentative, however, due to the differing results between both nights, as well as compared with the narrow-band derived properties. None of our resolved spectral lines probing the mid- to upper atmosphere show significant shifts relative to the planetary rest frame, however H$\\alpha$ and H$\\beta$ exhibit line broadenings of 39.6 $\\pm$ 2.1 km/s and 27.6 $\\pm$ 4.6 km/s, respectively, indicating the onset of possible escape. WASP-178 b differs from similar UHJ with its lack of strong atmospheric dynamics in the upper atmosphere, however the broadening seen for FeI (15.66 $\\pm$ 0.58 km/s) and FeII (11.32 $\\pm$ 0.52 km/s) could indicate the presence of winds in the mid-atmosphere. Future studies on the impact of the flux variability caused by the host star activity might shed more light on the subject. Previous work indicated the presence of SiO cloud-precursors in the atmosphere of WASP-178 b and a lack of MgI and FeII. However, our results suggest that a scenario where the planetary atmosphere is dominated by MgI and FeII is more likely. In light of our results, we encourage future observations to further elucidate these atmospheric properties.","sentences":["We search for atmospheric constituents for the UHJ WASP-178 b with two ESPRESSO transits using the narrow-band and cross-correlation techniques, focusing on the detections of NaI, H$\\alpha$, H$\\beta$, H$\\gamma$, MgI, FeI and FeII.","Additionally, we show parallel photometry used to obtain updated and precise stellar, planetary and orbital parameters.","We report the resolved line detections of NaI (5.5 and 5.4 $\\sigma$), H$\\alpha$ (13 $\\sigma$), H$\\beta$ (7.1 $\\sigma$), and tentatively MgI (4.6 $\\sigma$).","In cross-correlation, we confirm the MgI detection (7.8 and 5.8 $\\sigma$) and additionally report the detections of FeI (12 and 10 $\\sigma$) and FeII (11 and 8.4 $\\sigma$), on both nights separately.","The detection of MgI remains tentative, however, due to the differing results between both nights, as well as compared with the narrow-band derived properties.","None of our resolved spectral lines probing the mid- to upper atmosphere show significant shifts relative to the planetary rest frame, however H$\\alpha$ and H$\\beta$ exhibit line broadenings of 39.6 $\\pm$ 2.1 km/s and 27.6 $\\pm$ 4.6 km/s, respectively, indicating the onset of possible escape.","WASP-178 b differs from similar UHJ with its lack of strong atmospheric dynamics in the upper atmosphere, however the broadening seen for FeI (15.66 $\\pm$ 0.58 km/s) and FeII (11.32 $\\pm$ 0.52 km/s) could indicate the presence of winds in the mid-atmosphere.","Future studies on the impact of the flux variability caused by the host star activity might shed more light on the subject.","Previous work indicated the presence of SiO cloud-precursors in the atmosphere of WASP-178 b and a lack of MgI and FeII.","However, our results suggest that a scenario where the planetary atmosphere is dominated by MgI and FeII is more likely.","In light of our results, we encourage future observations to further elucidate these atmospheric properties."],"url":"http://arxiv.org/abs/2406.08348v1","category":"astro-ph.EP"}
{"created":"2024-06-12 15:02:54","title":"Jet modification via $\u03c0^0$-hadron correlations in Au$+$Au collisions at $\\sqrt{s_{_{NN}}}=200$ GeV","abstract":"High-momentum two-particle correlations are a useful tool for studying jet-quenching effects in the quark-gluon plasma. Angular correlations between neutral-pion triggers and charged hadrons with transverse momenta in the range 4--12~GeV/$c$ and 0.5--7~GeV/$c$, respectively, have been measured by the PHENIX experiment in 2014 for Au$+$Au collisions at $\\sqrt{s_{_{NN}}}=200$~GeV. Suppression is observed in the yield of high-momentum jet fragments opposite the trigger particle, which indicates jet suppression stemming from in-medium partonic energy loss, while enhancement is observed for low-momentum particles. The ratio and differences between the yield in Au$+$Au collisions and $p$$+$$p$ collisions, $I_{AA}$ and $\\Delta_{AA}$, as a function of the trigger-hadron azimuthal separation, $\\Delta\\phi$, are measured for the first time at the Relativistic Heavy Ion Collider. These results better quantify how the yield of low-$p_T$ associated hadrons is enhanced at wide angle, which is crucial for studying energy loss as well as medium-response effects.","sentences":["High-momentum two-particle correlations are a useful tool for studying jet-quenching effects in the quark-gluon plasma.","Angular correlations between neutral-pion triggers and charged hadrons with transverse momenta in the range 4--12~GeV/$c$ and 0.5--7~GeV/$c$, respectively, have been measured by the PHENIX experiment in 2014 for Au$+$Au collisions at $\\sqrt{s_{_{NN}}}=200$~GeV. Suppression is observed in the yield of high-momentum jet fragments opposite the trigger particle, which indicates jet suppression stemming from in-medium partonic energy loss, while enhancement is observed for low-momentum particles.","The ratio and differences between the yield in Au$+$Au collisions and $p$$+$$p$ collisions, $I_{AA}$ and $\\Delta_{AA}$, as a function of the trigger-hadron azimuthal separation, $\\Delta\\phi$, are measured for the first time at the Relativistic Heavy Ion Collider.","These results better quantify how the yield of low-$p_T$ associated hadrons is enhanced at wide angle, which is crucial for studying energy loss as well as medium-response effects."],"url":"http://arxiv.org/abs/2406.08301v1","category":"nucl-ex"}
{"created":"2024-06-12 14:58:36","title":"Improving subgroup analysis using methods to extend inferences to specific target populations","abstract":"Subgroup analyses are common in epidemiologic and clinical research. Unfortunately, restriction to subgroup members to test for heterogeneity can yield imprecise effect estimates. If the true effect differs between members and non-members due to different distributions of other measured effect measure modifiers (EMMs), leveraging data from non-members can improve the precision of subgroup effect estimates. We obtained data from the PRIME RCT of panitumumab in patients with metastatic colon and rectal cancer from Project Datasphere(TM) to demonstrate this method. We weighted non-Hispanic White patients to resemble Hispanic patients in measured potential EMMs (e.g., age, KRAS distribution, sex), combined Hispanic and weighted non-Hispanic White patients in one data set, and estimated 1-year differences in progression-free survival (PFS). We obtained percentile-based 95% confidence limits for this 1-year difference in PFS from 2,000 bootstraps. To show when the method is less helpful, we also reweighted male patients to resemble female patients and mutant-type KRAS (no treatment benefit) patients to resemble wild-type KRAS (treatment benefit) patients. The PRIME RCT included 795 non-Hispanic White and 42 Hispanic patients with complete data on EMMs. While the Hispanic-only analysis estimated a one-year PFS change of -17% (95% C.I. -45%, 8.8%) with panitumumab, the combined weighted estimate was more precise (-8.7%, 95% CI -22%, 5.3%) while differing from the full population estimate (1.0%, 95% CI: -5.9%, 7.5%). When targeting wild-type KRAS patients the combined weighted estimate incorrectly suggested no benefit (one-year PFS change: 0.9%, 95% CI: -6.0%, 7.2%). Methods to extend inferences from study populations to specific targets can improve the precision of estimates of subgroup effect estimates when their assumptions are met. Violations of those assumptions can lead to bias, however.","sentences":["Subgroup analyses are common in epidemiologic and clinical research.","Unfortunately, restriction to subgroup members to test for heterogeneity can yield imprecise effect estimates.","If the true effect differs between members and non-members due to different distributions of other measured effect measure modifiers (EMMs), leveraging data from non-members can improve the precision of subgroup effect estimates.","We obtained data from the PRIME RCT of panitumumab in patients with metastatic colon and rectal cancer from Project Datasphere(TM) to demonstrate this method.","We weighted non-Hispanic White patients to resemble Hispanic patients in measured potential EMMs (e.g., age, KRAS distribution, sex), combined Hispanic and weighted non-Hispanic White patients in one data set, and estimated 1-year differences in progression-free survival (PFS).","We obtained percentile-based 95% confidence limits for this 1-year difference in PFS from 2,000 bootstraps.","To show when the method is less helpful, we also reweighted male patients to resemble female patients and mutant-type KRAS (no treatment benefit) patients to resemble wild-type KRAS (treatment benefit) patients.","The PRIME RCT included 795 non-Hispanic White and 42 Hispanic patients with complete data on EMMs.","While the Hispanic-only analysis estimated a one-year PFS change of -17% (95% C.I. -45%, 8.8%) with panitumumab, the combined weighted estimate was more precise (-8.7%, 95% CI -22%, 5.3%) while differing from the full population estimate (1.0%, 95% CI: -5.9%, 7.5%).","When targeting wild-type KRAS patients the combined weighted estimate incorrectly suggested no benefit (one-year PFS change: 0.9%, 95% CI: -6.0%, 7.2%).","Methods to extend inferences from study populations to specific targets can improve the precision of estimates of subgroup effect estimates when their assumptions are met.","Violations of those assumptions can lead to bias, however."],"url":"http://arxiv.org/abs/2406.08297v1","category":"stat.AP"}
{"created":"2024-06-12 14:58:08","title":"Suppressing Counter-Rotating Errors for Fast Single-Qubit Gates with Fluxonium","abstract":"Qubit decoherence unavoidably degrades the fidelity of quantum logic gates. Accordingly, realizing gates that are as fast as possible is a guiding principle for qubit control, necessitating protocols for mitigating error channels that become significant as gate time is decreased. One such error channel arises from the counter-rotating component of strong, linearly polarized drives. This error channel is particularly important when gate times approach the qubit Larmor period and represents the dominant source of infidelity for sufficiently fast single-qubit gates with low-frequency qubits such as fluxonium. In this work, we develop and demonstrate two complementary protocols for mitigating this error channel. The first protocol realizes circularly polarized driving in circuit quantum electrodynamics (QED) through simultaneous charge and flux control. The second protocol -- commensurate pulses -- leverages the coherent and periodic nature of counter-rotating fields to regularize their contributions to gates, enabling single-qubit gate fidelities reliably exceeding $99.997\\%$. This protocol is platform independent and requires no additional calibration overhead. This work establishes straightforward strategies for mitigating counter-rotating effects from strong drives in circuit QED and other platforms, which we expect to be helpful in the effort to realize high-fidelity control for fault-tolerant quantum computing.","sentences":["Qubit decoherence unavoidably degrades the fidelity of quantum logic gates.","Accordingly, realizing gates that are as fast as possible is a guiding principle for qubit control, necessitating protocols for mitigating error channels that become significant as gate time is decreased.","One such error channel arises from the counter-rotating component of strong, linearly polarized drives.","This error channel is particularly important when gate times approach the qubit Larmor period and represents the dominant source of infidelity for sufficiently fast single-qubit gates with low-frequency qubits such as fluxonium.","In this work, we develop and demonstrate two complementary protocols for mitigating this error channel.","The first protocol realizes circularly polarized driving in circuit quantum electrodynamics (QED) through simultaneous charge and flux control.","The second protocol -- commensurate pulses -- leverages the coherent and periodic nature of counter-rotating fields to regularize their contributions to gates, enabling single-qubit gate fidelities reliably exceeding $99.997\\%$.","This protocol is platform independent and requires no additional calibration overhead.","This work establishes straightforward strategies for mitigating counter-rotating effects from strong drives in circuit QED and other platforms, which we expect to be helpful in the effort to realize high-fidelity control for fault-tolerant quantum computing."],"url":"http://arxiv.org/abs/2406.08295v1","category":"quant-ph"}
{"created":"2024-06-12 14:41:07","title":"Sudakov double logs in single-inclusive hadron production in DIS at small $x$ from the Color Glass Condensate formalism","abstract":"We investigate the high $Q^2$ (photon virtuality) limit of single-inclusive hadron production in DIS (SIDIS) at small $x$, using the color glass condensate formalism at next-to-leading order. We focus on the $\\Lambda_{QCD}^2 \\ll \\mathbf{p}_h^2 \\ll Q^2$ kinematic regime where $\\mathbf{p}_h$ is the produced hadron transverse momentum, and extract the Sudakov double logarithms. We further argue that compatibility between the CGC calculation and TMD factorization at one-loop order can only be achieved if the small-x evolution is kinematically constrained.","sentences":["We investigate the high $Q^2$ (photon virtuality) limit of single-inclusive hadron production in DIS (SIDIS) at small $x$, using the color glass condensate formalism at next-to-leading order.","We focus on the $\\Lambda_{QCD}^2 \\ll \\mathbf{p}_h^2 \\ll Q^2$ kinematic regime where $\\mathbf{p}_h$ is the produced hadron transverse momentum, and extract the Sudakov double logarithms.","We further argue that compatibility between the CGC calculation and TMD factorization at one-loop order can only be achieved if the small-x evolution is kinematically constrained."],"url":"http://arxiv.org/abs/2406.08277v1","category":"hep-ph"}
{"created":"2024-06-12 14:01:13","title":"Vibrational Branching Ratios for Laser-Cooling of Nonlinear Strontium-Containing Molecules","abstract":"The vibrational branching ratios from the lowest excited electronic state for $\\textrm{SrOCH}_3$, $\\textrm{SrNH}_2$, and $\\textrm{SrSH}$ are measured at the $< 0.1\\%$ level. Spectra are obtained by driving the $\\tilde{X} - \\tilde{A}$ transitions and dispersing the fluorescence on a grating spectrometer. We also perform $\\textit{ab initio}$ calculations for the energies of vibrational levels relevant for laser cooling, as well as branching ratios to support the interpretations of all molecular spectra. Symmetry group analysis is applied in conjunction with our data to study rotational closure in these molecules. These analyses indicate favorable prospects for laser cooling $\\textrm{SrNH}_2$ and other similar alkaline-earth(-like) amides for future beyond the Standard Model physics searches using polyatomic molecules with long-lived parity doublets.","sentences":["The vibrational branching ratios from the lowest excited electronic state for $\\textrm{SrOCH}_3$, $\\textrm{SrNH}_2$, and $\\textrm{SrSH}$ are measured at the $< 0.1\\%$ level.","Spectra are obtained by driving the $\\tilde{X} - \\tilde{A}$ transitions and dispersing the fluorescence on a grating spectrometer.","We also perform $\\textit{ab initio}$ calculations for the energies of vibrational levels relevant for laser cooling, as well as branching ratios to support the interpretations of all molecular spectra.","Symmetry group analysis is applied in conjunction with our data to study rotational closure in these molecules.","These analyses indicate favorable prospects for laser cooling $\\textrm{SrNH}_2$ and other similar alkaline-earth(-like) amides for future beyond the Standard Model physics searches using polyatomic molecules with long-lived parity doublets."],"url":"http://arxiv.org/abs/2406.08235v1","category":"physics.atom-ph"}
{"created":"2024-06-12 13:43:30","title":"The morphology and kinematics of the galaxy group AM 1054-325: A MUSE perspective","abstract":"Galaxy interactions in groups can lead to intense starbursts and the activation of active galactic nuclei (AGNs). The stripped gas from the outer disk can lead to star-forming clumps along the tidal tails or sometimes tidal dwarf galaxies. We investigate the impact of interaction on various galaxy properties, including morphology, star formation rates, and chemical composition in the galaxy group AM\\,1054-325 using Multi Unit Spectroscopic Explorer (MUSE) data. We conduct a comprehensive spatially and spectrally resolved investigation of the star formation rate, star formation histories, metallicity, and AGN activity. The galaxy subgroup AM\\,1054-325A shows multiple star-forming clumps in H$\\alpha$ emission along the western tidal tail, which are formed due to tidal stripping. These clumps also have higher metallicities. AM\\,1054-325B is quenched and shows disturbed gas kinematics and the signature of gas accretion in the H$\\alpha$ map. The specific star formation along the tidal tail is higher, contributing to the galaxy's overall stellar mass growth.","sentences":["Galaxy interactions in groups can lead to intense starbursts and the activation of active galactic nuclei (AGNs).","The stripped gas from the outer disk can lead to star-forming clumps along the tidal tails or sometimes tidal dwarf galaxies.","We investigate the impact of interaction on various galaxy properties, including morphology, star formation rates, and chemical composition in the galaxy group AM\\,1054-325 using Multi Unit Spectroscopic Explorer (MUSE) data.","We conduct a comprehensive spatially and spectrally resolved investigation of the star formation rate, star formation histories, metallicity, and AGN activity.","The galaxy subgroup AM\\,1054-325A shows multiple star-forming clumps in H$\\alpha$ emission along the western tidal tail, which are formed due to tidal stripping.","These clumps also have higher metallicities.","AM\\,1054-325B is quenched and shows disturbed gas kinematics and the signature of gas accretion in the H$\\alpha$ map.","The specific star formation along the tidal tail is higher, contributing to the galaxy's overall stellar mass growth."],"url":"http://arxiv.org/abs/2406.08213v1","category":"astro-ph.GA"}
{"created":"2024-06-12 13:35:07","title":"HTIM: Hybrid Text-Interaction Modeling for Broadening Political Leaning Inference in Social Media","abstract":"Political leaning can be defined as the inclination of an individual towards certain political orientations that align with their personal beliefs. Political leaning inference has traditionally been framed as a binary classification problem, namely, to distinguish between left vs. right or conservative vs liberal. Furthermore, although some recent work considers political leaning inference in a multi-party multi-region framework, their study is limited to the application of social interaction data. In order to address these shortcomings, in this study we propose Hybrid Text-Interaction Modeling (HTIM), a framework that enables hybrid modeling fusioning text and interactions from Social Media to accurately identify the political leaning of users in a multi-party multi-region framework. Access to textual and interaction-based data not only allows us to compare these data sources but also avoids reliance on specific data types. We show that, while state-of-the-art text-based representations on their own are not able to improve over interaction-based representations, a combination of text-based and interaction-based modeling using HTIM considerably improves the performance across the three regions, an improvement that is more prominent when we focus on the most challenging cases involving users who are less engaged in politics.","sentences":["Political leaning can be defined as the inclination of an individual towards certain political orientations that align with their personal beliefs.","Political leaning inference has traditionally been framed as a binary classification problem, namely, to distinguish between left vs. right or conservative vs liberal.","Furthermore, although some recent work considers political leaning inference in a multi-party multi-region framework, their study is limited to the application of social interaction data.","In order to address these shortcomings, in this study we propose Hybrid Text-Interaction Modeling (HTIM), a framework that enables hybrid modeling fusioning text and interactions from Social Media to accurately identify the political leaning of users in a multi-party multi-region framework.","Access to textual and interaction-based data not only allows us to compare these data sources but also avoids reliance on specific data types.","We show that, while state-of-the-art text-based representations on their own are not able to improve over interaction-based representations, a combination of text-based and interaction-based modeling using HTIM considerably improves the performance across the three regions, an improvement that is more prominent when we focus on the most challenging cases involving users who are less engaged in politics."],"url":"http://arxiv.org/abs/2406.08201v1","category":"cs.SI"}
{"created":"2024-06-12 13:31:02","title":"Why does the Milky Way have a metallicity floor?","abstract":"The prevalence of light element enhancement in the most metal-poor stars is potentially an indication that the Milky Way has a metallicity floor for star formation around $\\sim$10$^{-3.5}$ Z$_{\\odot}$. We propose that this metallicity floor has its origins in metal-enriched star formation in the minihalos present during the Galaxy's initial formation. To arrive at this conclusion, we analyze a cosmological radiation hydrodynamics simulation that follows the concurrent evolution of multiple Population III star-forming minihalos. The main driver for the central gas within minihalos is the steady increase in hydrostatic pressure as the halos grow. We incorporate this insight into a hybrid one-zone model that switches between pressure-confined and modified free-fall modes to evolve the gas density with time according to the ratio of the free-fall and sound-crossing timescales. This model is able to accurately reproduce the density and chemo-thermal evolution of the gas in each of the simulated minihalos up to the point of runaway collapse. We then use this model to investigate how the gas responds to the absence of H$_{2}$. Without metals, the central gas becomes increasingly stable against collapse as it grows to the atomic cooling limit. When metals are present in the halo at a level of $\\sim$10$^{-3.7}$ Z$_{\\odot}$, however, the gas is able to achieve gravitational instability while still in the minihalo regime. Thus, we conclude that the Galaxy's metallicity floor is set by the balance within minihalos of gas-phase metal cooling and the radiation background associated with its early formation environment.","sentences":["The prevalence of light element enhancement in the most metal-poor stars is potentially an indication that the Milky Way has a metallicity floor for star formation around $\\sim$10$^{-3.5}$ Z$_{\\odot}$. We propose that this metallicity floor has its origins in metal-enriched star formation in the minihalos present during the Galaxy's initial formation.","To arrive at this conclusion, we analyze a cosmological radiation hydrodynamics simulation that follows the concurrent evolution of multiple Population III star-forming minihalos.","The main driver for the central gas within minihalos is the steady increase in hydrostatic pressure as the halos grow.","We incorporate this insight into a hybrid one-zone model that switches between pressure-confined and modified free-fall modes to evolve the gas density with time according to the ratio of the free-fall and sound-crossing timescales.","This model is able to accurately reproduce the density and chemo-thermal evolution of the gas in each of the simulated minihalos up to the point of runaway collapse.","We then use this model to investigate how the gas responds to the absence of H$_{2}$. Without metals, the central gas becomes increasingly stable against collapse as it grows to the atomic cooling limit.","When metals are present in the halo at a level of $\\sim$10$^{-3.7}$ Z$_{\\odot}$, however, the gas is able to achieve gravitational instability while still in the minihalo regime.","Thus, we conclude that the Galaxy's metallicity floor is set by the balance within minihalos of gas-phase metal cooling and the radiation background associated with its early formation environment."],"url":"http://arxiv.org/abs/2406.08199v1","category":"astro-ph.GA"}
{"created":"2024-06-12 13:27:35","title":"Simulated Coronal Mass Ejections on a young Solar-Type Star and the Associated Instantaneous Angular Momentum Loss","abstract":"Coronal mass ejections (CMEs) on stars can change the stars' magnetic field configurations and mass loss rates during the eruption and propagation and therefore, may affect the stars' rotation properties on long time-scales. The dynamics of stellar CMEs and their influence on the stellar angular momentum loss rate are not yet well understood. In order to start investigating these CME-related aspects on other stars, we conducted a series of magnetohydrodynamic simulations of CMEs on a solar-type star of moderate activity levels. The propagation and evolution of the CMEs were traced in the three-dimensional outputs and the temporal evolution of their dynamic properties (such as masses, velocities, and kinetic energies) were determined. The simulated stellar CMEs are more massive and energetic than their solar analog, which is a result of the stronger magnetic field on the surface of the simulated star than that of the Sun. The simulated CMEs display masses ranging from ~10^16 g to ~10^18 g and kinetic energies from ~10^31 erg to ~10^33 erg. We also investigated the instantaneous influence of the CMEs to the star's angular momentum loss rate. Our results suggest that angular momentum can either be added to or be removed from the star during the evolution of CME events. We found a positive correlation between the amplitude of the angular momentum loss rate variation and the CME's kinetic energy as well as mass, suggesting that more energetic/massive CMEs have higher possibility to add angular momentum to the star.","sentences":["Coronal mass ejections (CMEs) on stars can change the stars' magnetic field configurations and mass loss rates during the eruption and propagation and therefore, may affect the stars' rotation properties on long time-scales.","The dynamics of stellar CMEs and their influence on the stellar angular momentum loss rate are not yet well understood.","In order to start investigating these CME-related aspects on other stars, we conducted a series of magnetohydrodynamic simulations of CMEs on a solar-type star of moderate activity levels.","The propagation and evolution of the CMEs were traced in the three-dimensional outputs and the temporal evolution of their dynamic properties (such as masses, velocities, and kinetic energies) were determined.","The simulated stellar CMEs are more massive and energetic than their solar analog, which is a result of the stronger magnetic field on the surface of the simulated star than that of the Sun.","The simulated CMEs display masses ranging from ~10^16 g to ~10^18 g and kinetic energies from ~10^31 erg to ~10^33 erg.","We also investigated the instantaneous influence of the CMEs to the star's angular momentum loss rate.","Our results suggest that angular momentum can either be added to or be removed from the star during the evolution of CME events.","We found a positive correlation between the amplitude of the angular momentum loss rate variation and the CME's kinetic energy as well as mass, suggesting that more energetic/massive CMEs have higher possibility to add angular momentum to the star."],"url":"http://arxiv.org/abs/2406.08194v1","category":"astro-ph.SR"}
{"created":"2024-06-12 13:13:24","title":"Heavy-to-light form factors to three loops","abstract":"We compute three-loop corrections of $\\mathcal{O}(\\alpha_{s}^3)$ to form factors with one massive and one massless quark coupling to an external vector, axialvector, scalar, pseudoscalar, or tensor current. We obtain analytic results for the color-planar contributions, for the contributions of light-quark loops, and the contributions with two heavy-quark loops. For the computation of the remaining master integrals we use the \"expand and match\" approach which leads to semi-analytic results for the form factors. We implement our results in a {\\tt Mathematica} and a {\\tt Fortran} code which allows for fast and precise numerical evaluations in the physically relevant phase space. The form factors are used to compute the hard matching coefficients in Soft-Collinear Effective Theory for all currents. The tensor coefficients at light-like momentum transfer are used to extract the hard function in $\\bar B \\to X_s \\gamma$ to three loops.","sentences":["We compute three-loop corrections of $\\mathcal{O}(\\alpha_{s}^3)$ to form factors with one massive and one massless quark coupling to an external vector, axialvector, scalar, pseudoscalar, or tensor current.","We obtain analytic results for the color-planar contributions, for the contributions of light-quark loops, and the contributions with two heavy-quark loops.","For the computation of the remaining master integrals we use the \"expand and match\" approach which leads to semi-analytic results for the form factors.","We implement our results in a {\\tt Mathematica} and a {\\tt Fortran} code which allows for fast and precise numerical evaluations in the physically relevant phase space.","The form factors are used to compute the hard matching coefficients in Soft-Collinear Effective Theory for all currents.","The tensor coefficients at light-like momentum transfer are used to extract the hard function in $\\bar B \\to X_s \\gamma$ to three loops."],"url":"http://arxiv.org/abs/2406.08182v1","category":"hep-ph"}
{"created":"2024-06-12 13:13:14","title":"Systematic analysis of the form factors of $B_c\\rightarrow\u03b7_c$, $J/\u03c8$ and corresponding weak decays","abstract":"The form factors of $B_c$ meson decaying to charmonium $\\eta_c$ and $J/\\psi$ are analyzed in the framework of three-point QCD sum rules. In these analyses, the contributions of the vacuum condensate terms $\\langle g_{s}^{2}G^{2}\\rangle$ and $\\langle f^{3}G^{3}\\rangle$ are considered. In addition, the decay widths and branch ratios of several decay channels are obtained by using the calculated form factors. These decay processes include the nonleptonic decay processes of $B_c^- \\to \\eta_c \\pi^-$, $\\eta_c K^-$, $\\eta_c \\rho^-$, $\\eta_c K^{*-}$, $B_c^- \\to J/\\psi \\pi^-$, $J/\\psi K^-$, $J/\\psi \\rho^-$, $J/\\psi K^{*-}$, and the semileptonic decay channels of $B_c^- \\to \\eta_c \\mathcal{l} \\bar{\\nu}$, $B_c^- \\to J/\\psi \\mathcal{l} \\bar{\\nu}$. These results about the form factors and decay properties of $B_c$ meson provide useful information to us to study the heavy-quark dynamics and find new physics(NP) beyond Standard Model(SM).","sentences":["The form factors of $B_c$ meson decaying to charmonium $\\eta_c$ and $J/\\psi$ are analyzed in the framework of three-point QCD sum rules.","In these analyses, the contributions of the vacuum condensate terms $\\langle g_{s}^{2}G^{2}\\rangle$ and $\\langle f^{3}G^{3}\\rangle$ are considered.","In addition, the decay widths and branch ratios of several decay channels are obtained by using the calculated form factors.","These decay processes include the nonleptonic decay processes of $B_c^- \\to \\eta_c \\pi^-$, $\\eta_c K^-$, $\\eta_c \\rho^-$, $\\eta_c K^{*-}$, $B_c^- \\to J/\\psi \\pi^-$, $J/\\psi K^-$, $J/\\psi \\rho^-$, $J/\\psi K^{*-}$, and the semileptonic decay channels of $B_c^- \\to \\eta_c \\mathcal{l} \\bar{\\nu}$, $B_c^- \\to J/\\psi \\mathcal{l} \\bar{\\nu}$. These results about the form factors and decay properties of $B_c$ meson provide useful information to us to study the heavy-quark dynamics and find new physics(NP) beyond Standard Model(SM)."],"url":"http://arxiv.org/abs/2406.08181v1","category":"hep-ph"}
{"created":"2024-06-12 12:56:19","title":"Optical Investigations of Coherence and Relaxation Dynamics of a Thulium-doped Yttrium Gallium Garnet Crystal at sub-Kelvin Temperatures for Optical Quantum Memory","abstract":"Rare-earth ion-doped crystals are of great interest for quantum memories, a central component in future quantum repeaters. To assess the promise of 1$\\%$ Tm$^{3+}$-doped yttrium gallium garnet (Tm:YGG), we report measurements of optical coherence and energy-level lifetimes of its $^3$H$_6$ $\\leftrightarrow$ $^3$H$_4$ transition at a temperature of around 500 mK and various magnetic fields. Using spectral hole burning, we find hyperfine ground-level (Zeeman level) lifetimes of several minutes at magnetic fields of less than 1000 G. We also measure coherence time exceeding one millisecond using two-pulse photon echoes. Three-pulse photon echo and spectral hole burning measurements reveal that due to spectral diffusion, the effective coherence time reduces to a few $\\mu$s over a timescale of around two hundred seconds. Finally, temporal and frequency-multiplexed storage of optical pulses using the atomic frequency comb protocol is demonstrated. Our results suggest Tm:YGG to be promising for multiplexed photonic quantum memory for quantum repeaters.","sentences":["Rare-earth ion-doped crystals are of great interest for quantum memories, a central component in future quantum repeaters.","To assess the promise of 1$\\%$ Tm$^{3+}$-doped yttrium gallium garnet (Tm:YGG), we report measurements of optical coherence and energy-level lifetimes of its $^3$H$_6$ $\\leftrightarrow$ $^3$H$_4$ transition at a temperature of around 500 mK and various magnetic fields.","Using spectral hole burning, we find hyperfine ground-level (Zeeman level) lifetimes of several minutes at magnetic fields of less than 1000 G.","We also measure coherence time exceeding one millisecond using two-pulse photon echoes.","Three-pulse photon echo and spectral hole burning measurements reveal that due to spectral diffusion, the effective coherence time reduces to a few $\\mu$s over a timescale of around two hundred seconds.","Finally, temporal and frequency-multiplexed storage of optical pulses using the atomic frequency comb protocol is demonstrated.","Our results suggest Tm:YGG to be promising for multiplexed photonic quantum memory for quantum repeaters."],"url":"http://arxiv.org/abs/2406.08167v1","category":"quant-ph"}
{"created":"2024-06-12 12:55:43","title":"Exploring the ultra-hot Jupiter WASP-178b. Constraints on atmospheric chemistry and dynamics from a joint retrieval of VLT/CRIRES$^+$ and space photometric data","abstract":"Despite recent progress in the spectroscopic characterization of individual exoplanets, the atmospheres of key ultra-hot Jupiters (UHJs) still lack comprehensive investigations. These include WASP-178b, one of the most irradiated UHJs known to date. We observed the dayside emission signal of this planet with CRIRES$^+$ in the spectral K-band. By applying the cross-correlation technique and a Bayesian retrieval framework to the high-resolution spectra, we identified the emission signature of $^{12}$CO (S/N = 8.9) and H$_2$O (S/N = 4.9), and a strong atmospheric thermal inversion. A joint retrieval with space-based secondary eclipse measurements from TESS and CHEOPS allows us to refine our results on the thermal profile and thus to constrain the atmospheric chemistry, yielding a solar to super-solar metallicity (1.4$\\pm$1.6 dex) and a solar C/O ratio (0.6$\\pm$0.2). We infer a significant excess of spectral line broadening and identify a slight Doppler-shift between the $^{12}$CO and H$_2$O signals. These findings provide strong evidence for a super-rotating atmospheric flow pattern and suggest the possible existence of chemical inhomogeneities across the planetary dayside hemisphere. In addition, the inclusion of photometric data in our retrieval allows us to account for stellar light reflected by the planetary atmosphere, resulting in an upper limit on the geometric albedo (0.23). The successful characterization of WASP-178b's atmosphere through a joint analysis of CRIRES$^+$, TESS, and CHEOPS observations highlights the potential of combined studies with space- and ground-based instruments and represents a promising avenue for advancing our understanding of exoplanet atmospheres.","sentences":["Despite recent progress in the spectroscopic characterization of individual exoplanets, the atmospheres of key ultra-hot Jupiters (UHJs) still lack comprehensive investigations.","These include WASP-178b, one of the most irradiated UHJs known to date.","We observed the dayside emission signal of this planet with CRIRES$^+$ in the spectral K-band.","By applying the cross-correlation technique and a Bayesian retrieval framework to the high-resolution spectra, we identified the emission signature of $^{12}$CO (S/N = 8.9) and H$_2$O (S/N = 4.9), and a strong atmospheric thermal inversion.","A joint retrieval with space-based secondary eclipse measurements from TESS and CHEOPS allows us to refine our results on the thermal profile and thus to constrain the atmospheric chemistry, yielding a solar to super-solar metallicity (1.4$\\pm$1.6 dex) and a solar C/O ratio (0.6$\\pm$0.2).","We infer a significant excess of spectral line broadening and identify a slight Doppler-shift between the $^{12}$CO and H$_2$O signals.","These findings provide strong evidence for a super-rotating atmospheric flow pattern and suggest the possible existence of chemical inhomogeneities across the planetary dayside hemisphere.","In addition, the inclusion of photometric data in our retrieval allows us to account for stellar light reflected by the planetary atmosphere, resulting in an upper limit on the geometric albedo (0.23).","The successful characterization of WASP-178b's atmosphere through a joint analysis of CRIRES$^+$, TESS, and CHEOPS observations highlights the potential of combined studies with space- and ground-based instruments and represents a promising avenue for advancing our understanding of exoplanet atmospheres."],"url":"http://arxiv.org/abs/2406.08166v1","category":"astro-ph.EP"}
{"created":"2024-06-12 12:40:17","title":"Unveiling potential neutron halos in intermediate-mass nuclei: an \\textit{ab initio} study","abstract":"Halos epitomize the fascinating interplay between weak binding, shell evolution, and deformation effects, especially in nuclei near the drip line. In this Letter, we apply the state-of-the-art \\textit{ab initio} valence-space in-medium similarity renormalization group approach to predict potential candidates for one- and two-neutron halo in the intermediate-mass region. Notably, we use spectroscopic factors (SF) and two-nucleon amplitudes (TNA) as criteria for suggesting one- and two-neutron halo candidates, respectively. This approach is not only theoretically sound but also amenable to experimental validation. Our research focuses on Mg, Al, Si, P, and S neutron-drip-line nuclei, offering systematic predictions of neutron halo candidates in terms of separation energies, SF (TNA), and average occupation. The calculation suggests the ground states of $^{40,42,44,46}$Al, $^{41,43,45,47}$Si, $^{46,48}$P, and $^{47,49}$S are promising candidates for one-neutron halos, while $^{40,42,44,46}$Mg, $^{45,47}$Al, $^{46,48}$Si, $^{49}$P, and $^{50}$S may harbor two-neutron halos. In addition, the relative mean-square neutron radius between halo nuclei and \\textit{inner core} is calculated for suggested potential neutron halos. Finally, the relations of halo formations and shell evolution are discussed.","sentences":["Halos epitomize the fascinating interplay between weak binding, shell evolution, and deformation effects, especially in nuclei near the drip line.","In this Letter, we apply the state-of-the-art \\textit{ab initio} valence-space in-medium similarity renormalization group approach to predict potential candidates for one- and two-neutron halo in the intermediate-mass region.","Notably, we use spectroscopic factors (SF) and two-nucleon amplitudes (TNA) as criteria for suggesting one-","and two-neutron halo candidates, respectively.","This approach is not only theoretically sound but also amenable to experimental validation.","Our research focuses on Mg, Al, Si, P, and S neutron-drip-line nuclei, offering systematic predictions of neutron halo candidates in terms of separation energies, SF (TNA), and average occupation.","The calculation suggests the ground states of $^{40,42,44,46}$Al, $^{41,43,45,47}$Si, $^{46,48}$P, and $^{47,49}$S are promising candidates for one-neutron halos, while $^{40,42,44,46}$Mg, $^{45,47}$Al, $^{46,48}$Si, $^{49}$P, and $^{50}$S may harbor two-neutron halos.","In addition, the relative mean-square neutron radius between halo nuclei and \\textit{inner core} is calculated for suggested potential neutron halos.","Finally, the relations of halo formations and shell evolution are discussed."],"url":"http://arxiv.org/abs/2406.08151v1","category":"nucl-th"}
{"created":"2024-06-12 12:34:45","title":"Design,fabrication and characterization of 8x9 n-type silicon pad array for sampling calorimetry","abstract":"This paper reports the development and testing of n-type silicon pad array detectors targeted for the Forward Calorimeter (FoCal) detector, which is an upgrade of the ALICE detector at CERN, scheduled for data taking in Run~4~(2029-2034). The FoCal detector includes hadronic and electromagnetic calorimeters, with the latter made of tungsten absorber layers and granular silicon pad arrays read out using the High Granularity Calorimeter Readout Chip~(HGCROC). This paper covers the Technology Computer-Aided Design (TCAD) simulations, the fabrication process, current versus voltage (IV) and capacitance versus voltage (CV) measurements, test results with a blue LED and $^{90}$Sr beta source, and neutron radiation hardness tests. IV measurements for the detector showed that 90\\% of the pads had leakage current below 10~nA at full depletion voltage. Simulations predicted a breakdown voltage of 1000~V and practical tests confirmed stable operation up to 500~V without breakdown. CV measurements in the data and the simulations gave a full depletion voltage of around 50~V at a capacitance of 35~pF. LED tests verified that all detector pads responded correctly. Additionally, the 1$\\times$1 cm$^2$ pads were also tested with the neutron radiations at a fluence of $5\\times10^{13}$ 1~MeV~n$_{eq}$/cm$^2$.","sentences":["This paper reports the development and testing of n-type silicon pad array detectors targeted for the Forward Calorimeter (FoCal) detector, which is an upgrade of the ALICE detector at CERN, scheduled for data taking in Run~4~(2029-2034).","The FoCal detector includes hadronic and electromagnetic calorimeters, with the latter made of tungsten absorber layers and granular silicon pad arrays read out using the High Granularity Calorimeter Readout Chip~(HGCROC).","This paper covers the Technology Computer-Aided Design (TCAD) simulations, the fabrication process, current versus voltage (IV) and capacitance versus voltage (CV) measurements, test results with a blue LED and $^{90}$Sr beta source, and neutron radiation hardness tests.","IV measurements for the detector showed that 90\\% of the pads had leakage current below 10~nA at full depletion voltage.","Simulations predicted a breakdown voltage of 1000~V and practical tests confirmed stable operation up to 500~V without breakdown.","CV measurements in the data and the simulations gave a full depletion voltage of around 50~V at a capacitance of 35~pF. LED tests verified that all detector pads responded correctly.","Additionally, the 1$\\times$1 cm$^2$ pads were also tested with the neutron radiations at a fluence of $5\\times10^{13}$ 1~MeV~n$_{eq}$/cm$^2$."],"url":"http://arxiv.org/abs/2406.08144v1","category":"physics.ins-det"}
{"created":"2024-06-12 12:33:49","title":"The Sun's Magnetic Power Spectra over Two Solar Cycles. \\uppercase\\expandafter{\\romannumeral2}. Cycle Dependence of Active Region, Magnetic Network, and Their Relation","abstract":"The multi-scaled solar magnetic field consists of two major components: active regions (ARs) and magnetic network. Unraveling the cycle-dependent properties and interrelations of these components is crucial for understanding the evolution of the solar magnetic field. In this study, we investigate these components using magnetic power spectra derived from high-resolution and continuous synoptic magnetograms since cycle 23 onwards. Our results show that the size of the magnetic network ranges from 26 Mm to 41 Mm without dependence on the solar cycle. The power of the network field ($P_{NW}$) accounts for approximately 20\\% of the total power during any phase of solar cycles. In contrast to the AR power ($P_{AR}$), $P_{NW}$ displays a weaker cycle dependence, as described by the relationship $P_{NW}$ $\\approx$ 0.6* $P_{AR}$ + 40. The power-law index between AR sizes and magnetic network sizes presents a strong anti-correlation with the activity level. Additionally, our study indicates that in the absence of sunspots on the solar disc, the magnetic power spectra remain time-independent, consistently exhibiting similarity in both shape and power. This study introduces a new method to investigate the properties of the magnetic network and provides magnetic power spectra for high-resolution simulations of the solar magnetic field at the surface at various phases of solar cycles.","sentences":["The multi-scaled solar magnetic field consists of two major components: active regions (ARs) and magnetic network.","Unraveling the cycle-dependent properties and interrelations of these components is crucial for understanding the evolution of the solar magnetic field.","In this study, we investigate these components using magnetic power spectra derived from high-resolution and continuous synoptic magnetograms since cycle 23 onwards.","Our results show that the size of the magnetic network ranges from 26 Mm to 41 Mm without dependence on the solar cycle.","The power of the network field ($P_{NW}$) accounts for approximately 20\\% of the total power during any phase of solar cycles.","In contrast to the AR power ($P_{AR}$), $P_{NW}$ displays a weaker cycle dependence, as described by the relationship $P_{NW}$ $\\approx$ 0.6* $P_{AR}$ + 40.","The power-law index between AR sizes and magnetic network sizes presents a strong anti-correlation with the activity level.","Additionally, our study indicates that in the absence of sunspots on the solar disc, the magnetic power spectra remain time-independent, consistently exhibiting similarity in both shape and power.","This study introduces a new method to investigate the properties of the magnetic network and provides magnetic power spectra for high-resolution simulations of the solar magnetic field at the surface at various phases of solar cycles."],"url":"http://arxiv.org/abs/2406.08141v1","category":"astro-ph.SR"}
{"created":"2024-06-12 12:23:28","title":"Temperature and composition disturbances in the southern auroral region of Jupiter revealed by JWST/MIRI","abstract":"Jupiters south polar region was observed by JWST Mid Infrared Instrument in December 2022. We used the Medium Resolution Spectrometer mode to provide new information about Jupiters South Polar stratosphere. The southern auroral region was visible and influenced the atmosphere in several ways. 1: In the interior of the southern auroral oval, we retrieved peak temperatures at two distinct pressure levels near 0.01 and 1 mbar, with warmer temperatures with respect to non auroral regions of 12 pm 2 K and 37 pm 4 K respectively. A cold polar vortex is centered at 65S at 10 mbar. 2: We found that the homopause is elevated to 590+25-118 km above the 1-bar pressure level inside the auroral oval compared to 460+60-50 km at neighboring latitudes and with an upper altitude of 350 km in regions not affected by auroral precipitation. 3: The retrieved abundance of C2H2 shows an increase within the auroral oval, and it exhibits high abundances throughout the polar region. The retrieved abundance of C2H6 increases towards the pole, without being localized in the auroral oval, in contrast with previous analysis. We determined that the warming at 0.01 mbar and the elevated homopause might be caused by the flux of charged particles depositing their energy in the South Polar Region. The 1 mbar hotspot may arise from adiabatic heating resulting from auroral driven downwelling. The cold region at 10 mbar may be caused by radiative cooling by stratospheric aerosols. The differences in spatial distribution seem to indicate that the hydrocarbons analyzed are affected differently by auroral precipitation.","sentences":["Jupiters south polar region was observed by JWST Mid Infrared Instrument in December 2022.","We used the Medium Resolution Spectrometer mode to provide new information about Jupiters South Polar stratosphere.","The southern auroral region was visible and influenced the atmosphere in several ways.","1: In the interior of the southern auroral oval, we retrieved peak temperatures at two distinct pressure levels near 0.01 and 1 mbar, with warmer temperatures with respect to non auroral regions of 12 pm 2 K and 37 pm 4 K respectively.","A cold polar vortex is centered at 65S at 10 mbar.","2: We found that the homopause is elevated to 590+25-118 km above the 1-bar pressure level inside the auroral oval compared to 460+60-50 km at neighboring latitudes and with an upper altitude of 350 km in regions not affected by auroral precipitation.","3: The retrieved abundance of C2H2 shows an increase within the auroral oval, and it exhibits high abundances throughout the polar region.","The retrieved abundance of C2H6 increases towards the pole, without being localized in the auroral oval, in contrast with previous analysis.","We determined that the warming at 0.01 mbar and the elevated homopause might be caused by the flux of charged particles depositing their energy in the South Polar Region.","The 1 mbar hotspot may arise from adiabatic heating resulting from auroral driven downwelling.","The cold region at 10 mbar may be caused by radiative cooling by stratospheric aerosols.","The differences in spatial distribution seem to indicate that the hydrocarbons analyzed are affected differently by auroral precipitation."],"url":"http://arxiv.org/abs/2406.08133v1","category":"astro-ph.EP"}
{"created":"2024-06-12 11:07:55","title":"Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams","abstract":"Benefiting from the advancements in large language models and cross-modal alignment, existing multi-modal video understanding methods have achieved prominent performance in offline scenario. However, online video streams, as one of the most common media forms in the real world, have seldom received attention. Compared to offline videos, the 'dynamic' nature of online video streams poses challenges for the direct application of existing models and introduces new problems, such as the storage of extremely long-term information, interaction between continuous visual content and 'asynchronous' user questions. Therefore, in this paper we present Flash-VStream, a video-language model that simulates the memory mechanism of human. Our model is able to process extremely long video streams in real-time and respond to user queries simultaneously. Compared to existing models, Flash-VStream achieves significant reductions in inference latency and VRAM consumption, which is intimately related to performing understanding of online streaming video. In addition, given that existing video understanding benchmarks predominantly concentrate on offline scenario, we propose VStream-QA, a novel question answering benchmark specifically designed for online video streaming understanding. Comparisons with popular existing methods on the proposed benchmark demonstrate the superiority of our method for such challenging setting. To verify the generalizability of our approach, we further evaluate it on existing video understanding benchmarks and achieves state-of-the-art performance in offline scenarios as well. All code, models, and datasets are available at the https://invinciblewyq.github.io/vstream-page/","sentences":["Benefiting from the advancements in large language models and cross-modal alignment, existing multi-modal video understanding methods have achieved prominent performance in offline scenario.","However, online video streams, as one of the most common media forms in the real world, have seldom received attention.","Compared to offline videos, the 'dynamic' nature of online video streams poses challenges for the direct application of existing models and introduces new problems, such as the storage of extremely long-term information, interaction between continuous visual content and 'asynchronous' user questions.","Therefore, in this paper we present Flash-VStream, a video-language model that simulates the memory mechanism of human.","Our model is able to process extremely long video streams in real-time and respond to user queries simultaneously.","Compared to existing models, Flash-VStream achieves significant reductions in inference latency and VRAM consumption, which is intimately related to performing understanding of online streaming video.","In addition, given that existing video understanding benchmarks predominantly concentrate on offline scenario, we propose VStream-QA, a novel question answering benchmark specifically designed for online video streaming understanding.","Comparisons with popular existing methods on the proposed benchmark demonstrate the superiority of our method for such challenging setting.","To verify the generalizability of our approach, we further evaluate it on existing video understanding benchmarks and achieves state-of-the-art performance in offline scenarios as well.","All code, models, and datasets are available at the https://invinciblewyq.github.io/vstream-page/"],"url":"http://arxiv.org/abs/2406.08085v1","category":"cs.CV"}
{"created":"2024-06-12 10:35:11","title":"Synchronous and Asynchronous Updates of Active Ising Spins in One Dimension","abstract":"How do update rules affect the dynamical and steady state properties of a flock? In this study, we have explored the active Ising spins (s = +-1) in one dimension, where spin updates its orientation according to the Metropolis algorithm (based on the neighbors) via two different update rules. (i) Parallel, and (ii) Random-sequential. We explore the effect of Parallel and Random-sequential updates on the dynamical properties of flocks in one dimension. Due to the inherent asynchronous nature of the Random-sequential update, the directional switching of the flock is increased compared to the Parallel one. The nature of phase transition is affected by the difference in the updating mechanism: discontinuous for Parallel and continuous for Random-sequential updates.","sentences":["How do update rules affect the dynamical and steady state properties of a flock?","In this study, we have explored the active Ising spins (s = +-1) in one dimension, where spin updates its orientation according to the Metropolis algorithm (based on the neighbors) via two different update rules.","(i) Parallel, and (ii) Random-sequential.","We explore the effect of Parallel and Random-sequential updates on the dynamical properties of flocks in one dimension.","Due to the inherent asynchronous nature of the Random-sequential update, the directional switching of the flock is increased compared to the Parallel one.","The nature of phase transition is affected by the difference in the updating mechanism: discontinuous for Parallel and continuous for Random-sequential updates."],"url":"http://arxiv.org/abs/2406.08067v1","category":"cond-mat.soft"}
{"created":"2024-06-12 10:00:23","title":"Dispersion Interaction Between Thin Conducting Cylinders","abstract":"The ground state and excited state resonance dipole-dipole interaction energy between two elongated conducting molecules are explored. We review the current status for ground state interactions. This interaction is found to be of a much longer range than in the case when the molecules are pointlike and nonconducting. These are well known results found earlier by Davies, Ninham, and Richmond, and later, using a different formalism, by Rubio and co-workers. We show how the theory can be extended to excited state interactions. A characteristic property following from our calculation is that the interaction energy dependence with separation ($R$) goes like $f(R)/R^2$ both for resonance and for the van der Waals case in the long range limit. In some limits $f(R)$ has a logarithmic dependency and in others it takes constant values. We predict an unusual slow decay rate for the energy transfer between conducting molecules.","sentences":["The ground state and excited state resonance dipole-dipole interaction energy between two elongated conducting molecules are explored.","We review the current status for ground state interactions.","This interaction is found to be of a much longer range than in the case when the molecules are pointlike and nonconducting.","These are well known results found earlier by Davies, Ninham, and Richmond, and later, using a different formalism, by Rubio and co-workers.","We show how the theory can be extended to excited state interactions.","A characteristic property following from our calculation is that the interaction energy dependence with separation ($R$) goes like $f(R)/R^2$ both for resonance and for the van der Waals case in the long range limit.","In some limits $f(R)$ has a logarithmic dependency and in others it takes constant values.","We predict an unusual slow decay rate for the energy transfer between conducting molecules."],"url":"http://arxiv.org/abs/2406.08047v1","category":"physics.chem-ph"}
{"created":"2024-06-12 09:25:29","title":"Real-time, chirped-pulse heterodyne detection at room-temperature with 100GHz 3dB-bandwidth mid-infrared quantum-well photodetectors","abstract":"Thanks to intrinsically short electronic relaxation on the ps time scale, III-V semiconductor unipolar devices are ideal candidates for ultrahigh-speed operation at mid-infrared frequencies. In this work, antenna-coupled, GaAs-based multi quantum-well photodetectors operating in the 10-11um range are demonstrated, with a responsivity of 0.3A/W and a 3dB-cutoff bandwidth of 100GHz at room-temperature. The frequency response is measured up to 220GHz: beyond 100GHz we find a roll-off dominated by the 2.5 ps-long recombination time of the photo-excited electrons. The potential of the detectors is illustrated by setting up an experiment where the time dependent emission frequency of a quantum cascade laser operated in pulsed mode is measured electronically and in real-time, over a frequency range >60GHz. By exploiting broadband electronics, and thanks to its high signal-to-noise ratio, this technique allows the acquisition, in a single-shot, of frequency-calibrated, mid-infrared molecular spectra spanning up to 100GHz and beyond, which is particularly attractive for fast, active remote sensing applications in fields such as environmental or combustion monitoring.","sentences":["Thanks to intrinsically short electronic relaxation on the ps time scale, III-V semiconductor unipolar devices are ideal candidates for ultrahigh-speed operation at mid-infrared frequencies.","In this work, antenna-coupled, GaAs-based multi quantum-well photodetectors operating in the 10-11um range are demonstrated, with a responsivity of 0.3A/W and a 3dB-cutoff bandwidth of 100GHz at room-temperature.","The frequency response is measured up to 220GHz: beyond 100GHz we find a roll-off dominated by the 2.5 ps-long recombination time of the photo-excited electrons.","The potential of the detectors is illustrated by setting up an experiment where the time dependent emission frequency of a quantum cascade laser operated in pulsed mode is measured electronically and in real-time, over a frequency range >60GHz.","By exploiting broadband electronics, and thanks to its high signal-to-noise ratio, this technique allows the acquisition, in a single-shot, of frequency-calibrated, mid-infrared molecular spectra spanning up to 100GHz and beyond, which is particularly attractive for fast, active remote sensing applications in fields such as environmental or combustion monitoring."],"url":"http://arxiv.org/abs/2406.08027v1","category":"physics.ins-det"}
{"created":"2024-06-12 09:22:41","title":"A fractional approach to strain-gradient plasticity: beyond core-radius of discrete dislocations","abstract":"We derive a strain-gradient theory for plasticity as the $\\Gamma$-limit of discrete dislocation fractional energies, without the introduction of a core-radius. By using the finite horizon fractional gradient introduced by Bellido, Cueto, and Mora-Corral of 2023, we consider a nonlocal model of semi-discrete dislocations, in which the stored elastic energy is computed via the fractional gradient of order $1-\\alpha$. As $\\alpha$ goes to $0$, we show that suitably rescaled energies $\\Gamma$-converge to the macroscopic strain-gradient model of Garroni, Leoni, and Ponsiglione of 2010.","sentences":["We derive a strain-gradient theory for plasticity as the $\\Gamma$-limit of discrete dislocation fractional energies, without the introduction of a core-radius.","By using the finite horizon fractional gradient introduced by Bellido, Cueto, and Mora-Corral of 2023, we consider a nonlocal model of semi-discrete dislocations, in which the stored elastic energy is computed via the fractional gradient of order $1-\\alpha$. As $\\alpha$ goes to $0$, we show that suitably rescaled energies $\\Gamma$-converge to the macroscopic strain-gradient model of Garroni, Leoni, and Ponsiglione of 2010."],"url":"http://arxiv.org/abs/2406.08023v1","category":"math.AP"}
{"created":"2024-06-12 09:11:49","title":"Studying $\u03c0^+\u03c0^-$ photoproduction beyond Pomeron exchange","abstract":"Forward photoproduction of $\\pi^+\\pi^-$ pairs with invariant mass of the order of $m_\\rho\\sim 770$ MeV is traditionally understood to be produced via Pomeron exchange. Based on a detailed analysis of the CLAS photoproduction data, it is shown that the dynamics of two-pion photoproduction for $|t|\\gtrsim 0.5$ GeV$^2$ cannot be explained by Pomeron exchange alone. This motivates the development of a new theoretical model of two-pion photoproduction which incorporates both two-pion and pion-nucleon resonant contributions. After fitting free parameters, the model provides an excellent description of the low moments of the angular distribution measured at CLAS, and enables an assessment of the relative contributions of particular production mechanisms and an interpretation of the various features of the data in terms of these mechanisms.","sentences":["Forward photoproduction of $\\pi^+\\pi^-$ pairs with invariant mass of the order of $m_\\rho\\sim 770$ MeV is traditionally understood to be produced via Pomeron exchange.","Based on a detailed analysis of the CLAS photoproduction data, it is shown that the dynamics of two-pion photoproduction for $|t|\\gtrsim 0.5$ GeV$^2$ cannot be explained by Pomeron exchange alone.","This motivates the development of a new theoretical model of two-pion photoproduction which incorporates both two-pion and pion-nucleon resonant contributions.","After fitting free parameters, the model provides an excellent description of the low moments of the angular distribution measured at CLAS, and enables an assessment of the relative contributions of particular production mechanisms and an interpretation of the various features of the data in terms of these mechanisms."],"url":"http://arxiv.org/abs/2406.08016v1","category":"hep-ph"}
{"created":"2024-06-12 09:10:02","title":"Highly agile flat swimming robot","abstract":"Exploring bodies of water on their surface allows robots to efficiently communicate and harvest energy from the sun. On the water surface, however, robots often face highly unstructured environments, cluttered with plant matter, animals, and debris. We report a fast (5.1 cm/s translation and 195 {\\deg}/s rotation), centimeter-scale swimming robot with high maneuverability and autonomous untethered operation. Locomotion is enabled by a pair of soft, millimeter-thin, undulating pectoral fins, in which traveling waves are electrically excited to generate propulsion. The robots navigate through narrow spaces, through grassy plants, and push objects weighing over 16x their body weight. Such robots can allow distributed environmental monitoring as well as continuous measurement of plant and water parameters for aqua-farming.","sentences":["Exploring bodies of water on their surface allows robots to efficiently communicate and harvest energy from the sun.","On the water surface, however, robots often face highly unstructured environments, cluttered with plant matter, animals, and debris.","We report a fast (5.1 cm/s translation and 195 {\\deg}/s rotation), centimeter-scale swimming robot with high maneuverability and autonomous untethered operation.","Locomotion is enabled by a pair of soft, millimeter-thin, undulating pectoral fins, in which traveling waves are electrically excited to generate propulsion.","The robots navigate through narrow spaces, through grassy plants, and push objects weighing over 16x their body weight.","Such robots can allow distributed environmental monitoring as well as continuous measurement of plant and water parameters for aqua-farming."],"url":"http://arxiv.org/abs/2406.08015v1","category":"cs.RO"}
{"created":"2024-06-12 09:04:12","title":"Interaction of an outflow with surrounding gaseous clouds as the origin of the late-time radio flares in TDEs","abstract":"Close encounter between a star and a supermassive black hole (SMBH) results in the tidal disruption of the star, known as a tidal disruption event (TDE). Recently, a few TDEs, e.g., ASASSN-15oi and AT2018hyz, have shown late-time (hundreds of days after their UV/optical peaks) radio flares with radio luminosities of $10^{38\\sim39}$ erg/s. The super-Eddington fallback or accretion in a TDE may generate a mass outflow. Here we investigate a scenario that the late-time radio flares come from the interaction of the outflow with the circum-nuclear gaseous clouds, in addition to the slow-evolving emission component due to the outflow-diffuse medium interaction. We calculate the associated radio temporal and spectral signatures and find that they reproduce well the observations. The outflows have the inferred velocity of 0.2$\\sim0.8$ c, the total mass of $10^{-3}\\sim10^{-1}$ $\\mathrm{M_{\\odot}}$ and the ejection duration of a month to a year. The distances of the clouds to the SMBH are $0.1\\sim1$ pc. This scenario has advantages in explaining the long delay, sharpness of the rise and the multiplicity of the late radio flares. Future observations may build up a much larger sample of late-time radio flares and enable their use as a probe of the TDE physics and the host circumnuclear environment.","sentences":["Close encounter between a star and a supermassive black hole (SMBH) results in the tidal disruption of the star, known as a tidal disruption event (TDE).","Recently, a few TDEs, e.g., ASASSN-15oi and AT2018hyz, have shown late-time (hundreds of days after their UV/optical peaks) radio flares with radio luminosities of $10^{38\\sim39}$ erg/s. The super-Eddington fallback or accretion in a TDE may generate a mass outflow.","Here we investigate a scenario that the late-time radio flares come from the interaction of the outflow with the circum-nuclear gaseous clouds, in addition to the slow-evolving emission component due to the outflow-diffuse medium interaction.","We calculate the associated radio temporal and spectral signatures and find that they reproduce well the observations.","The outflows have the inferred velocity of 0.2$\\sim0.8$ c, the total mass of $10^{-3}\\sim10^{-1}$ $\\mathrm{M_{\\odot}}$ and the ejection duration of a month to a year.","The distances of the clouds to the SMBH are $0.1\\sim1$ pc.","This scenario has advantages in explaining the long delay, sharpness of the rise and the multiplicity of the late radio flares.","Future observations may build up a much larger sample of late-time radio flares and enable their use as a probe of the TDE physics and the host circumnuclear environment."],"url":"http://arxiv.org/abs/2406.08012v1","category":"astro-ph.HE"}
{"created":"2024-06-12 08:22:59","title":"Holographic Superfluid Ring with a Weak Link","abstract":"We explore the generation of topological defects in the course of a dynamical phase transition in a ring with a weak link, i.e., a SSS Josephson junction, from the AdS/CFT correspondence. By setting different parameters of the junction (width, steepness, depth) and the final temperature of the quench, the configurations of the charge density and condensate of the order parameters of the dual field theory are presented. Meanwhile, we observe that in the final equilibrium state, variations in parameters of the junctions only affect the configurations of the charge density and condensate of the order parameters, without altering their values outside the junction. However, variations in the final temperature will directly affect the values of the charge density and condensate of the order parameters outside of the junction. Moreover, in the final equilibrium state, we propose an analytic relation between the gauge-invariant velocity in the two superconducting states in the SSS Josephson junction, which agrees well with the numerical results.","sentences":["We explore the generation of topological defects in the course of a dynamical phase transition in a ring with a weak link, i.e., a SSS Josephson junction, from the AdS/CFT correspondence.","By setting different parameters of the junction (width, steepness, depth) and the final temperature of the quench, the configurations of the charge density and condensate of the order parameters of the dual field theory are presented.","Meanwhile, we observe that in the final equilibrium state, variations in parameters of the junctions only affect the configurations of the charge density and condensate of the order parameters, without altering their values outside the junction.","However, variations in the final temperature will directly affect the values of the charge density and condensate of the order parameters outside of the junction.","Moreover, in the final equilibrium state, we propose an analytic relation between the gauge-invariant velocity in the two superconducting states in the SSS Josephson junction, which agrees well with the numerical results."],"url":"http://arxiv.org/abs/2406.07988v1","category":"hep-th"}
{"created":"2024-06-12 07:37:18","title":"Charge ordered phases in the hole-doped triangular Mott insulator 4Hb-TaS2","abstract":"4Hb-TaS2 has been proposed to possess unconventional superconductivity with broken time reveral symmetry due to distinctive layered structure, featuring a heterojunction between a 2D triangular Mott insulator and a charge density wave metal. However, since a frustrated spin state in the correlated insulating layer is susceptible to charge ordering with carrier doping, it is required to investigate the charge distribution driven by inter-layer charge transfer to understand its superconductivity. Here, we use scanning tunneling microscopy and spectroscopy (STM/S) to investigate the charge ordered phases of 1T-TaS2 layers within 4Hb-TaS2, explicitly focusing on the non-half-filled regime. Our STS results show an energy gap which exhibits an out-of-phase relation with the charge density. We ascribe the competition between on-site and nonlocal Coulomb repulsion as the driving force for the charge-ordered insulating phase of a doped triangular Mott insulator. In addition, we discuss the role of the insulating layer in the enhanced superconductivity of 4Hb-TaS2.","sentences":["4Hb-TaS2 has been proposed to possess unconventional superconductivity with broken time reveral symmetry due to distinctive layered structure, featuring a heterojunction between a 2D triangular Mott insulator and a charge density wave metal.","However, since a frustrated spin state in the correlated insulating layer is susceptible to charge ordering with carrier doping, it is required to investigate the charge distribution driven by inter-layer charge transfer to understand its superconductivity.","Here, we use scanning tunneling microscopy and spectroscopy (STM/S) to investigate the charge ordered phases of 1T-TaS2 layers within 4Hb-TaS2, explicitly focusing on the non-half-filled regime.","Our STS results show an energy gap which exhibits an out-of-phase relation with the charge density.","We ascribe the competition between on-site and nonlocal Coulomb repulsion as the driving force for the charge-ordered insulating phase of a doped triangular Mott insulator.","In addition, we discuss the role of the insulating layer in the enhanced superconductivity of 4Hb-TaS2."],"url":"http://arxiv.org/abs/2406.07960v1","category":"cond-mat.str-el"}
{"created":"2024-06-12 07:35:44","title":"Weak interaction axial form factors of the octet baryons in nuclear medium","abstract":"We study the axial-vector and the induced pseudoscalar form factors associated with the weak transitions between the octet baryon members in nuclear medium, using a covariant constituent quark model. We extend previous calculations of the axial transition form factors from the vacuum (free space) to the nuclear medium (symmetric nuclear matter). The extension of the model to the nuclear medium takes into account the modifications of the properties of hadrons in the medium (masses and coupling constants), as determined by the quark-meson coupling model. The axial-vector ($G_A$) and the induced pseudoscalar ($G_P$) form factors are evaluated for different values of the nuclear density $\\rho$ in terms of the square transfer momentum $q^2= -Q^2$. We conclude that in general the $G_A$ and $G_P$ form factors are reduced in the nuclear medium. The reduction is stronger for light baryons and high densities. The medium modifications are milder for the heavier octet baryons, particularly at large $Q^2$. The calculations presented here can be used to estimate the cross sections of neutrino and antineutrino scattering with nucleus, and neutrino and antineutrino scattering with hyperons bound to a nucleus, as well as those in the cores of compact stars.","sentences":["We study the axial-vector and the induced pseudoscalar form factors associated with the weak transitions between the octet baryon members in nuclear medium, using a covariant constituent quark model.","We extend previous calculations of the axial transition form factors from the vacuum (free space) to the nuclear medium (symmetric nuclear matter).","The extension of the model to the nuclear medium takes into account the modifications of the properties of hadrons in the medium (masses and coupling constants), as determined by the quark-meson coupling model.","The axial-vector ($G_A$) and the induced pseudoscalar ($G_P$) form factors are evaluated for different values of the nuclear density $\\rho$ in terms of the square transfer momentum $q^2= -Q^2$.","We conclude that in general the $G_A$ and $G_P$ form factors are reduced in the nuclear medium.","The reduction is stronger for light baryons and high densities.","The medium modifications are milder for the heavier octet baryons, particularly at large $Q^2$.","The calculations presented here can be used to estimate the cross sections of neutrino and antineutrino scattering with nucleus, and neutrino and antineutrino scattering with hyperons bound to a nucleus, as well as those in the cores of compact stars."],"url":"http://arxiv.org/abs/2406.07958v1","category":"hep-ph"}
{"created":"2024-06-12 07:32:10","title":"Light rings and causality for nonsingular ultracompact objects sourced by nonlinear electrodynamics","abstract":"We study observational signatures of nonsingular ultracompact objects regularized by nonlinear electrodynamics. The phenomenon of birefringence causes photons of different polarizations to propagate with respect to two distinct metrics, which manifests itself in the appearance of additional light rings surrounding the ultracompact object. We analyze the observational consequences of this result and illustrate our findings based on three regular black hole models commonly considered in the literature. We find that, for certain ranges of the minimal length scale parameter, nonsingular horizonless ultracompact objects may possess an odd number of light rings, contrary to what is expected based on topological arguments premised on matter satisfying the null energy condition. We comment on the interrelation of this result with respect to the necessity of violating the null energy condition for the formation of nonsingular ultracompact objects via gravitational collapse and discuss the viability of nonlinear electrodynamics as an effective description of their properties. In addition, we compare the phase velocities of polarized light rays propagating in nonsingular geometries sourced by nonlinear electrodynamics to the corresponding phase velocity in the Schwarzschild spacetime and demonstrate that regularizing the singularity by means of a theory that does not adhere to the Maxwell weak-field limit may lead to the emergence of acausal regions.","sentences":["We study observational signatures of nonsingular ultracompact objects regularized by nonlinear electrodynamics.","The phenomenon of birefringence causes photons of different polarizations to propagate with respect to two distinct metrics, which manifests itself in the appearance of additional light rings surrounding the ultracompact object.","We analyze the observational consequences of this result and illustrate our findings based on three regular black hole models commonly considered in the literature.","We find that, for certain ranges of the minimal length scale parameter, nonsingular horizonless ultracompact objects may possess an odd number of light rings, contrary to what is expected based on topological arguments premised on matter satisfying the null energy condition.","We comment on the interrelation of this result with respect to the necessity of violating the null energy condition for the formation of nonsingular ultracompact objects via gravitational collapse and discuss the viability of nonlinear electrodynamics as an effective description of their properties.","In addition, we compare the phase velocities of polarized light rays propagating in nonsingular geometries sourced by nonlinear electrodynamics to the corresponding phase velocity in the Schwarzschild spacetime and demonstrate that regularizing the singularity by means of a theory that does not adhere to the Maxwell weak-field limit may lead to the emergence of acausal regions."],"url":"http://arxiv.org/abs/2406.07957v1","category":"gr-qc"}
{"created":"2024-06-12 07:31:25","title":"Ab initio calculations with a new local chiral N3LO nucleon-nucleon force","abstract":"Ab initio calculations have achieved remarkable success in nuclear structure studies. Numerous works highlight the pivotal role of three-body forces in nuclear ab initio calculations. Concurrently, efforts have been made to replicate these calculations using only realistic nucleon-nucleon (NN) interactions. A novel local chiral next-to-next-to-next-to-leading order (N3LO) NN interaction, distinct due to its weaker tensor force, has recently been established. This paper applies this local NN interaction in ab initio frameworks to calculate the low-lying spectra of p-shell light nuclei, particularly 10B, ground-state energies and shell evolution in oxygen isotopes. Results are compared with calculations utilizing nonlocal chiral N3LO NN and chiral NN +3N interactions. The ab initio calculations with the local N N potential accurately describe the spectra of p-shell nuclei, notably the 10B. Additionally, the neutron drip line for oxygen isotopes, with 24O as the drip line nucleus, is accurately reproduced in ab initio calculations with the local NN interaction. Calculations with the local NN interaction also reproduce the subshell closure at N = 14 and 16, albeit with a stronger shell gap compared to experimental data. However, the calculated charge radii based on the local NN interaction are underestimated compared with experimental data, which is similar to results from the nonlocal NN interaction. Consequently, the present ab initio calculations further indicate significant spin-orbit splitting effects with the new local NN potential, suggesting that 3N forces remain an important consideration.","sentences":["Ab initio calculations have achieved remarkable success in nuclear structure studies.","Numerous works highlight the pivotal role of three-body forces in nuclear ab initio calculations.","Concurrently, efforts have been made to replicate these calculations using only realistic nucleon-nucleon (NN) interactions.","A novel local chiral next-to-next-to-next-to-leading order (N3LO) NN interaction, distinct due to its weaker tensor force, has recently been established.","This paper applies this local NN interaction in ab initio frameworks to calculate the low-lying spectra of p-shell light nuclei, particularly 10B, ground-state energies and shell evolution in oxygen isotopes.","Results are compared with calculations utilizing nonlocal chiral N3LO NN and chiral NN +3N interactions.","The ab initio calculations with the local N N potential accurately describe the spectra of p-shell nuclei, notably the 10B.","Additionally, the neutron drip line for oxygen isotopes, with 24O as the drip line nucleus, is accurately reproduced in ab initio calculations with the local NN interaction.","Calculations with the local NN interaction also reproduce the subshell closure at N = 14 and 16, albeit with a stronger shell gap compared to experimental data.","However, the calculated charge radii based on the local NN interaction are underestimated compared with experimental data, which is similar to results from the nonlocal NN interaction.","Consequently, the present ab initio calculations further indicate significant spin-orbit splitting effects with the new local NN potential, suggesting that 3N forces remain an important consideration."],"url":"http://arxiv.org/abs/2406.07956v1","category":"nucl-th"}
{"created":"2024-06-12 07:11:24","title":"Elevator: Self-* and Persistent Hub Sampling Service in Unstructured Peer-to-Peer Networks","abstract":"We present Elevator, a novel algorithm for hub samplingin peer-to-peer networks, enabling the construction of overlays with atopology between a random graph and a star network, and networksthat have both hubs and are resilient to failures. Our approach emergesfrom principles of preferential attachment, forming hubs spontaneously,offering an innovative solution for decentralized networks that can benefituse cases requiring a network with both low diameter and resilience tofailures.","sentences":["We present Elevator, a novel algorithm for hub samplingin peer-to-peer networks, enabling the construction of overlays with atopology between a random graph and a star network, and networksthat have both hubs and are resilient to failures.","Our approach emergesfrom principles of preferential attachment, forming hubs spontaneously,offering an innovative solution for decentralized networks that can benefituse cases requiring a network with both low diameter and resilience tofailures."],"url":"http://arxiv.org/abs/2406.07946v1","category":"cs.DC"}
{"created":"2024-06-12 07:04:16","title":"Supertranslation ambiguity in post-Minkowskian expansion","abstract":"The supertranslation ambiguity of angular momentum is a long-standing problem in general relativity. In the context of post-Minkowskian expansion, the supertranslation ambiguity arises the puzzle of the angular momentum loss to start at $\\cO(G^2)$ or at $\\cO(G^3)$ in gravitational scattering. In this paper, we propose a generic prescription to fix the supertranslation ambiguity at the linear order in post-Minkowskian expansion which will uniquely determine the angular momentum loss.","sentences":["The supertranslation ambiguity of angular momentum is a long-standing problem in general relativity.","In the context of post-Minkowskian expansion, the supertranslation ambiguity arises the puzzle of the angular momentum loss to start at $\\cO(G^2)$ or at $\\cO(G^3)$ in gravitational scattering.","In this paper, we propose a generic prescription to fix the supertranslation ambiguity at the linear order in post-Minkowskian expansion which will uniquely determine the angular momentum loss."],"url":"http://arxiv.org/abs/2406.07943v1","category":"gr-qc"}
{"created":"2024-06-12 07:01:57","title":"Representation-protected topology of spin-singlet $s$-wave superconductors","abstract":"We show that spin-singlet $s$-wave multi-band superconductors have a topological phase protected by rotation symmetry and time-reversal symmetry without spin-orbit coupling in two and three dimensions. This topological phase, an example of a representation-protected topological phase, has a $\\mathbb{Z}_2$ topological index and is stable as long as the bands at the Fermi energy are formed by a doublet of orbital states with finite angular momenta. In the limit of weak superconducting pair potential, the $\\mathbb{Z}_2$ index gives a Fermi-surface formula and is related to the winding number of three-dimensional strong topological superconductors of class CI. We present a model of a topological $s_{\\pm}$-wave superconductor that has gapless surface states with a quadratic dispersion and suggest a connection with iron-based superconductors.","sentences":["We show that spin-singlet $s$-wave multi-band superconductors have a topological phase protected by rotation symmetry and time-reversal symmetry without spin-orbit coupling in two and three dimensions.","This topological phase, an example of a representation-protected topological phase, has a $\\mathbb{Z}_2$ topological index and is stable as long as the bands at the Fermi energy are formed by a doublet of orbital states with finite angular momenta.","In the limit of weak superconducting pair potential, the $\\mathbb{Z}_2$ index gives a Fermi-surface formula and is related to the winding number of three-dimensional strong topological superconductors of class CI.","We present a model of a topological $s_{\\pm}$-wave superconductor that has gapless surface states with a quadratic dispersion and suggest a connection with iron-based superconductors."],"url":"http://arxiv.org/abs/2406.07939v1","category":"cond-mat.supr-con"}
{"created":"2024-06-12 06:11:47","title":"GAPses: Versatile smart glasses for comfortable and fully-dry acquisition and parallel ultra-low-power processing of EEG and EOG","abstract":"Recent advancements in head-mounted wearable technology are revolutionizing the field of biopotential measurement, but the integration of these technologies into practical, user-friendly devices remains challenging due to issues with design intrusiveness, comfort, and data privacy. To address these challenges, this paper presents GAPSES, a novel smart glasses platform designed for unobtrusive, comfortable, and secure acquisition and processing of electroencephalography (EEG) and electrooculography (EOG) signals. We introduce a direct electrode-electronics interface with custom fully dry soft electrodes to enhance comfort for long wear. An integrated parallel ultra-low-power RISC-V processor (GAP9, Greenwaves Technologies) processes data at the edge, thereby eliminating the need for continuous data streaming through a wireless link, enhancing privacy, and increasing system reliability in adverse channel conditions. We demonstrate the broad applicability of the designed prototype through validation in a number of EEG-based interaction tasks, including alpha waves, steady-state visual evoked potential analysis, and motor movement classification. Furthermore, we demonstrate an EEG-based biometric subject recognition task, where we reach a sensitivity and specificity of 98.87% and 99.86% respectively, with only 8 EEG channels and an energy consumption per inference on the edge as low as 121 uJ. Moreover, in an EOG-based eye movement classification task, we reach an accuracy of 96.68% on 11 classes, resulting in an information transfer rate of 94.78 bit/min, which can be further increased to 161.43 bit/min by reducing the accuracy to 81.43%. The deployed implementation has an energy consumption of 24 uJ per inference and a total system power of only 16.28 mW, allowing for continuous operation of more than 12 h with a small 75 mAh battery.","sentences":["Recent advancements in head-mounted wearable technology are revolutionizing the field of biopotential measurement, but the integration of these technologies into practical, user-friendly devices remains challenging due to issues with design intrusiveness, comfort, and data privacy.","To address these challenges, this paper presents GAPSES, a novel smart glasses platform designed for unobtrusive, comfortable, and secure acquisition and processing of electroencephalography (EEG) and electrooculography (EOG) signals.","We introduce a direct electrode-electronics interface with custom fully dry soft electrodes to enhance comfort for long wear.","An integrated parallel ultra-low-power RISC-V processor (GAP9, Greenwaves Technologies) processes data at the edge, thereby eliminating the need for continuous data streaming through a wireless link, enhancing privacy, and increasing system reliability in adverse channel conditions.","We demonstrate the broad applicability of the designed prototype through validation in a number of EEG-based interaction tasks, including alpha waves, steady-state visual evoked potential analysis, and motor movement classification.","Furthermore, we demonstrate an EEG-based biometric subject recognition task, where we reach a sensitivity and specificity of 98.87% and 99.86% respectively, with only 8 EEG channels and an energy consumption per inference on the edge as low as 121 uJ. Moreover, in an EOG-based eye movement classification task, we reach an accuracy of 96.68% on 11 classes, resulting in an information transfer rate of 94.78 bit/min, which can be further increased to 161.43 bit/min by reducing the accuracy to 81.43%.","The deployed implementation has an energy consumption of 24 uJ per inference and a total system power of only 16.28 mW, allowing for continuous operation of more than 12 h with a small 75 mAh battery."],"url":"http://arxiv.org/abs/2406.07903v1","category":"eess.SP"}
{"created":"2024-06-12 06:09:51","title":"The resolved star formation law in NGC 7469 from JWST, ALMA and VLA","abstract":"We investigate the star formation process within the central 3.3 kpc region of the nearby luminous infrared Seyfert NGC 7469, probing scales ranging from 88 to 330 pc. We combine JWST/MIRI imaging with the F770W filter, with CO(2-1) and the underlying 1.3 mm dust continuum data from ALMA, along with VLA radio continuum observations at 22 GHz. NGC 7469 hosts a starburst ring which dominates the overall star formation activity. We estimate a global star formation rate SFR $\\sim 11.5$ $\\rm M_{\\odot}~yr^{-1}$ from the radio at 22 GHz, and a cold molecular gas mass M(H2) $\\sim$ 6.4 $\\times$ $\\rm 10^9 M_{\\odot}$ from the CO(2-1) emission. We find that the 1.3 mm map shows a morphology remarkably similar to those traced by the 22 GHz and the 7.7 $\\rm \\mu m$ polycyclic aromatic hydrocarbon (PAH) emission observed with JWST. The three tracers reproduce the morphology of the starburst ring with good agreement. We further investigate the correlations between the PAHs, the star formation rate and the cold molecular gas. We find a stronger correlation of the PAHs with the star formation than with the CO, with steeper correlations within the starburst ring ($n > 2$) than in the outer region ($n < 1$). We derive the correlation between the star formation rate and the cold molecular gas mass surface densities, the Kennicutt-Schmidt star formation law. Comparisons with other galaxy populations, including starburst galaxies and active galactic nuclei, highlighted that NGC 7469 exhibits an intermediate behavior to the Kennicutt-Schmidt relations found for these galaxy populations.","sentences":["We investigate the star formation process within the central 3.3 kpc region of the nearby luminous infrared Seyfert NGC 7469, probing scales ranging from 88 to 330 pc.","We combine JWST/MIRI imaging with the F770W filter, with CO(2-1) and the underlying 1.3 mm dust continuum data from ALMA, along with VLA radio continuum observations at 22 GHz.","NGC 7469 hosts a starburst ring which dominates the overall star formation activity.","We estimate a global star formation rate SFR $\\sim 11.5$ $\\rm M_{\\odot}~yr^{-1}$ from the radio at 22 GHz, and a cold molecular gas mass M(H2) $\\sim$ 6.4 $\\times$ $\\rm 10^9 M_{\\odot}$ from the CO(2-1) emission.","We find that the 1.3 mm map shows a morphology remarkably similar to those traced by the 22 GHz and the 7.7 $\\rm \\mu m$ polycyclic aromatic hydrocarbon (PAH) emission observed with JWST.","The three tracers reproduce the morphology of the starburst ring with good agreement.","We further investigate the correlations between the PAHs, the star formation rate and the cold molecular gas.","We find a stronger correlation of the PAHs with the star formation than with the CO, with steeper correlations within the starburst ring ($n > 2$) than in the outer region ($n < 1$).","We derive the correlation between the star formation rate and the cold molecular gas mass surface densities, the Kennicutt-Schmidt star formation law.","Comparisons with other galaxy populations, including starburst galaxies and active galactic nuclei, highlighted that NGC 7469 exhibits an intermediate behavior to the Kennicutt-Schmidt relations found for these galaxy populations."],"url":"http://arxiv.org/abs/2406.07901v1","category":"astro-ph.GA"}
{"created":"2024-06-12 04:40:38","title":"The Size-luminosity Relation of the AGN Torus Determined from the Comparison between Optical and Mid-infrared Variability","abstract":"We investigate the optical variability of low-redshift ($0.15< z\\leq0.4$) active galactic nuclei using the multi-epoch data from the Zwicky Transient Facility. We find that a damped random walk model well describes the ensemble structure function in the $g$ band. Consistent with previous studies, more luminous active galactic nuclei tend to have a steeper structure function at a timescale less than the break timescale and smaller variability amplitude. By comparing the structure functions in the optical with the mid-infrared obtained from the Wide-field Infrared Survey Explorer, we derive the size of the dusty torus using a toy model for the geometry of the torus. The size of the torus positively correlates with the luminosity of the active nucleus, following a relation that agrees well with previous studies based on reverberation mapping. This result demonstrates that the structure function method can be used as a powerful and highly efficient tool to examine the size of the torus.","sentences":["We investigate the optical variability of low-redshift ($0.15< z\\leq0.4$) active galactic nuclei using the multi-epoch data from the Zwicky Transient Facility.","We find that a damped random walk model well describes the ensemble structure function in the $g$ band.","Consistent with previous studies, more luminous active galactic nuclei tend to have a steeper structure function at a timescale less than the break timescale and smaller variability amplitude.","By comparing the structure functions in the optical with the mid-infrared obtained from the Wide-field Infrared Survey Explorer, we derive the size of the dusty torus using a toy model for the geometry of the torus.","The size of the torus positively correlates with the luminosity of the active nucleus, following a relation that agrees well with previous studies based on reverberation mapping.","This result demonstrates that the structure function method can be used as a powerful and highly efficient tool to examine the size of the torus."],"url":"http://arxiv.org/abs/2406.07863v1","category":"astro-ph.GA"}
