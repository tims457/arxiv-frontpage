{"created":"2024-03-06 18:59:02","title":"Backtracing: Retrieving the Cause of the Query","abstract":"Many online content portals allow users to ask questions to supplement their understanding (e.g., of lectures). While information retrieval (IR) systems may provide answers for such user queries, they do not directly assist content creators -- such as lecturers who want to improve their content -- identify segments that _caused_ a user to ask those questions. We introduce the task of backtracing, in which systems retrieve the text segment that most likely caused a user query. We formalize three real-world domains for which backtracing is important in improving content delivery and communication: understanding the cause of (a) student confusion in the Lecture domain, (b) reader curiosity in the News Article domain, and (c) user emotion in the Conversation domain. We evaluate the zero-shot performance of popular information retrieval methods and language modeling methods, including bi-encoder, re-ranking and likelihood-based methods and ChatGPT. While traditional IR systems retrieve semantically relevant information (e.g., details on \"projection matrices\" for a query \"does projecting multiple times still lead to the same point?\"), they often miss the causally relevant context (e.g., the lecturer states \"projecting twice gets me the same answer as one projection\"). Our results show that there is room for improvement on backtracing and it requires new retrieval approaches. We hope our benchmark serves to improve future retrieval systems for backtracing, spawning systems that refine content generation and identify linguistic triggers influencing user queries. Our code and data are open-sourced: https://github.com/rosewang2008/backtracing.","sentences":["Many online content portals allow users to ask questions to supplement their understanding (e.g., of lectures).","While information retrieval (IR) systems may provide answers for such user queries, they do not directly assist content creators -- such as lecturers who want to improve their content -- identify segments that _caused_ a user to ask those questions.","We introduce the task of backtracing, in which systems retrieve the text segment that most likely caused a user query.","We formalize three real-world domains for which backtracing is important in improving content delivery and communication: understanding the cause of (a) student confusion in the Lecture domain, (b) reader curiosity in the News Article domain, and (c) user emotion in the Conversation domain.","We evaluate the zero-shot performance of popular information retrieval methods and language modeling methods, including bi-encoder, re-ranking and likelihood-based methods and ChatGPT.","While traditional IR systems retrieve semantically relevant information (e.g., details on \"projection matrices\" for a query \"does projecting multiple times still lead to the same point?\"), they often miss the causally relevant context (e.g., the lecturer states \"projecting twice gets me the same answer as one projection\").","Our results show that there is room for improvement on backtracing and it requires new retrieval approaches.","We hope our benchmark serves to improve future retrieval systems for backtracing, spawning systems that refine content generation and identify linguistic triggers influencing user queries.","Our code and data are open-sourced: https://github.com/rosewang2008/backtracing."],"url":"http://arxiv.org/abs/2403.03956v1","category":"cs.IR"}
{"created":"2024-03-06 18:59:00","title":"Understanding Stabilizer Codes Under Local Decoherence Through a General Statistical Mechanics Mapping","abstract":"We consider the problem of a generic stabilizer Hamiltonian under local, incoherent Pauli errors. Using two different approaches -- (i) Haah's polynomial formalism arXiv:1204.1063 and (ii) the homological perspective on CSS codes -- we construct a mapping from the $n$th moment of the decohered ground state density matrix to a classical statistical mechanics model. We demonstrate that various measures of information capacity -- (i) quantum relative entropy, (ii) coherent information, and (iii) entanglement negativity -- map to thermodynamic quantities in the statistical mechanics model and can be used to characterize the decoding phase transition. As examples, we analyze the 3D toric code and X-cube model, deriving bounds on their optimal decoding thresholds and gaining insight into their information properties under decoherence. Additionally, we demonstrate that the SM mapping acts an an \"ungauging\" map; the classical models that describe a given code under decoherence also can be gauged to obtain the same code. Finally, we comment on correlated errors and non-CSS stabilizer codes.","sentences":["We consider the problem of a generic stabilizer Hamiltonian under local, incoherent Pauli errors.","Using two different approaches -- (i) Haah's polynomial formalism arXiv:1204.1063 and (ii) the homological perspective on CSS codes -- we construct a mapping from the $n$th moment of the decohered ground state density matrix to a classical statistical mechanics model.","We demonstrate that various measures of information capacity -- (i) quantum relative entropy, (ii) coherent information, and (iii) entanglement negativity -- map to thermodynamic quantities in the statistical mechanics model and can be used to characterize the decoding phase transition.","As examples, we analyze the 3D toric code and X-cube model, deriving bounds on their optimal decoding thresholds and gaining insight into their information properties under decoherence.","Additionally, we demonstrate that the SM mapping acts an an \"ungauging\" map; the classical models that describe a given code under decoherence also can be gauged to obtain the same code.","Finally, we comment on correlated errors and non-CSS stabilizer codes."],"url":"http://arxiv.org/abs/2403.03955v1","category":"quant-ph"}
{"created":"2024-03-06 18:58:49","title":"3D Diffusion Policy","abstract":"Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 55.3% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in diverse aspects, including space, viewpoint, appearance, and instance. Interestingly, in real robot experiments, DP3 rarely violates safety requirements, in contrast to baseline methods which frequently do, necessitating human intervention. Our extensive evaluation highlights the critical importance of 3D representations in real-world robot learning. Videos, code, and data are available on https://3d-diffusion-policy.github.io .","sentences":["Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations.","To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models.","The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder.","In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 55.3% relative improvement.","In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in diverse aspects, including space, viewpoint, appearance, and instance.","Interestingly, in real robot experiments, DP3 rarely violates safety requirements, in contrast to baseline methods which frequently do, necessitating human intervention.","Our extensive evaluation highlights the critical importance of 3D representations in real-world robot learning.","Videos, code, and data are available on https://3d-diffusion-policy.github.io ."],"url":"http://arxiv.org/abs/2403.03954v1","category":"cs.RO"}
{"created":"2024-03-06 18:56:36","title":"Bridging Language and Items for Retrieval and Recommendation","abstract":"This paper introduces BLaIR, a series of pretrained sentence embedding models specialized for recommendation scenarios. BLaIR is trained to learn correlations between item metadata and potential natural language context, which is useful for retrieving and recommending items. To pretrain BLaIR, we collect Amazon Reviews 2023, a new dataset comprising over 570 million reviews and 48 million items from 33 categories, significantly expanding beyond the scope of previous versions. We evaluate the generalization ability of BLaIR across multiple domains and tasks, including a new task named complex product search, referring to retrieving relevant items given long, complex natural language contexts. Leveraging large language models like ChatGPT, we correspondingly construct a semi-synthetic evaluation set, Amazon-C4. Empirical results on the new task, as well as conventional retrieval and recommendation tasks, demonstrate that BLaIR exhibit strong text and item representation capacity. Our datasets, code, and checkpoints are available at: https://github.com/hyp1231/AmazonReviews2023.","sentences":["This paper introduces BLaIR, a series of pretrained sentence embedding models specialized for recommendation scenarios.","BLaIR is trained to learn correlations between item metadata and potential natural language context, which is useful for retrieving and recommending items.","To pretrain BLaIR, we collect Amazon Reviews 2023, a new dataset comprising over 570 million reviews and 48 million items from 33 categories, significantly expanding beyond the scope of previous versions.","We evaluate the generalization ability of BLaIR across multiple domains and tasks, including a new task named complex product search, referring to retrieving relevant items given long, complex natural language contexts.","Leveraging large language models like ChatGPT, we correspondingly construct a semi-synthetic evaluation set, Amazon-C4.","Empirical results on the new task, as well as conventional retrieval and recommendation tasks, demonstrate that BLaIR exhibit strong text and item representation capacity.","Our datasets, code, and checkpoints are available at: https://github.com/hyp1231/AmazonReviews2023."],"url":"http://arxiv.org/abs/2403.03952v1","category":"cs.IR"}
{"created":"2024-03-06 18:55:47","title":"Stop Regressing: Training Value Functions via Classification for Scalable Deep RL","abstract":"Value functions are a central component of deep reinforcement learning (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains. These include: single-task RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving state-of-the-art results on these domains. Through careful analysis, we show that the benefits of categorical cross-entropy primarily stem from its ability to mitigate issues inherent to value-based RL, such as noisy targets and non-stationarity. Overall, we argue that a simple shift to training value functions with categorical cross-entropy can yield substantial improvements in the scalability of deep RL at little-to-no cost.","sentences":["Value functions are a central component of deep reinforcement learning (RL).","These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values.","However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging.","This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks.","Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions.","We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains.","These include: single-task RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving state-of-the-art results on these domains.","Through careful analysis, we show that the benefits of categorical cross-entropy primarily stem from its ability to mitigate issues inherent to value-based RL, such as noisy targets and non-stationarity.","Overall, we argue that a simple shift to training value functions with categorical cross-entropy can yield substantial improvements in the scalability of deep RL at little-to-no cost."],"url":"http://arxiv.org/abs/2403.03950v1","category":"cs.LG"}
{"created":"2024-03-06 18:55:36","title":"Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation","abstract":"Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in \"digital twin\" simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel \"inverse distillation\" procedure for bringing real-world demonstrations into simulated environments for efficient fine-tuning, with minimal human intervention and engineering required. We evaluate RialTo across a variety of robotic manipulation problems in the real world, such as robustly stacking dishes on a rack, placing books on a shelf, and six other tasks. RialTo increases (over 67%) in policy robustness without requiring extensive human data collection. Project website and videos at https://real-to-sim-to-real.github.io/RialTo/","sentences":["Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors.","Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection.","To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in \"digital twin\" simulation environments constructed on the fly from small amounts of real-world data.","To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments.","We also introduce a novel \"inverse distillation\" procedure for bringing real-world demonstrations into simulated environments for efficient fine-tuning, with minimal human intervention and engineering required.","We evaluate RialTo","across a variety of robotic manipulation problems in the real world, such as robustly stacking dishes on a rack, placing books on a shelf, and six other tasks.","RialTo increases (over 67%) in policy robustness without requiring extensive human data collection.","Project website and videos at https://real-to-sim-to-real.github.io/RialTo/"],"url":"http://arxiv.org/abs/2403.03949v1","category":"cs.RO"}
{"created":"2024-03-06 18:54:13","title":"Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset","abstract":"Automatically estimating the performance difficulty of a music piece represents a key process in music education to create tailored curricula according to the individual needs of the students. Given its relevance, the Music Information Retrieval (MIR) field depicts some proof-of-concept works addressing this task that mainly focuses on high-level music abstractions such as machine-readable scores or music sheet images. In this regard, the potential of directly analyzing audio recordings has been generally neglected, which prevents students from exploring diverse music pieces that may not have a formal symbolic-level transcription. This work pioneers in the automatic estimation of performance difficulty of music pieces on audio recordings with two precise contributions: (i) the first audio-based difficulty estimation dataset -- namely, Piano Syllabus (PSyllabus) dataset -- featuring 7,901 piano pieces across 11 difficulty levels from 1,233 composers; and (ii) a recognition framework capable of managing different input representations -- both unimodal and multimodal manners -- directly derived from audio to perform the difficulty estimation task. The comprehensive experimentation comprising different pre-training schemes, input modalities, and multi-task scenarios prove the validity of the proposal and establishes PSyllabus as a reference dataset for audio-based difficulty estimation in the MIR field. The dataset as well as the developed code and trained models are publicly shared to promote further research in the field.","sentences":["Automatically estimating the performance difficulty of a music piece represents a key process in music education to create tailored curricula according to the individual needs of the students.","Given its relevance, the Music Information Retrieval (MIR) field depicts some proof-of-concept works addressing this task that mainly focuses on high-level music abstractions such as machine-readable scores or music sheet images.","In this regard, the potential of directly analyzing audio recordings has been generally neglected, which prevents students from exploring diverse music pieces that may not have a formal symbolic-level transcription.","This work pioneers in the automatic estimation of performance difficulty of music pieces on audio recordings with two precise contributions: (i) the first audio-based difficulty estimation dataset -- namely, Piano Syllabus (PSyllabus) dataset -- featuring 7,901 piano pieces across 11 difficulty levels from 1,233 composers; and (ii) a recognition framework capable of managing different input representations -- both unimodal and multimodal manners -- directly derived from audio to perform the difficulty estimation task.","The comprehensive experimentation comprising different pre-training schemes, input modalities, and multi-task scenarios prove the validity of the proposal and establishes PSyllabus as a reference dataset for audio-based difficulty estimation in the MIR field.","The dataset as well as the developed code and trained models are publicly shared to promote further research in the field."],"url":"http://arxiv.org/abs/2403.03947v1","category":"cs.SD"}
{"created":"2024-03-06 18:51:19","title":"Separate and Detailed Treatment of Absolute Signal and Noise Enables NMR Under Adverse Circumstances","abstract":"When deploying a spectrometer in an adverse environment, such as during a typical ODNP experiment or experiments that require low-volume low-field measurements, a clear and modern protocol for characterizing and quantifying the absolute signal and noise levels proves essential. This paper provides such a protocol. It also highlights the clarity and insight that comes from (1) discussing NMR signal intensities in (conserved) units of square root instantaneous power that are derived from a theory and notation developed initially for ESR spectroscopy; as well as (2) characterizing the spectral distribution of the noise.   Crucially, the strategy introduced here applies not only to ODNP measurements, but to all low-field NMR. Low-field NMR offers immense flexibility: it enables integration with other instrumentation and deploys in practical applications not accessible to higher-field instrumentation. More generally, the protocol introduced here should apply to a wide range of instruments, and should prove especially useful in cases subject to design constraints that requires integration with multiple other modules that are not dedicated to NMR but that control other forms of spectroscopy or other crucial aspects of the measurement. However, in the specific case of ODNP, this protocol demonstrates that the absolute signal and noise level can be estimated from the clarified theory presented here, and uses that theory to identify the inefficient distribution of fields in the hairpin loop probe as the main remaining bottleneck for the improvement of low-field low-volume ODNP SNR.","sentences":["When deploying a spectrometer in an adverse environment, such as during a typical ODNP experiment or experiments that require low-volume low-field measurements, a clear and modern protocol for characterizing and quantifying the absolute signal and noise levels proves essential.","This paper provides such a protocol.","It also highlights the clarity and insight that comes from (1) discussing NMR signal intensities in (conserved) units of square root instantaneous power that are derived from a theory and notation developed initially for ESR spectroscopy; as well as (2) characterizing the spectral distribution of the noise.   ","Crucially, the strategy introduced here applies not only to ODNP measurements, but to all low-field NMR.","Low-field NMR offers immense flexibility: it enables integration with other instrumentation and deploys in practical applications not accessible to higher-field instrumentation.","More generally, the protocol introduced here should apply to a wide range of instruments, and should prove especially useful in cases subject to design constraints that requires integration with multiple other modules that are not dedicated to NMR but that control other forms of spectroscopy or other crucial aspects of the measurement.","However, in the specific case of ODNP, this protocol demonstrates that the absolute signal and noise level can be estimated from the clarified theory presented here, and uses that theory to identify the inefficient distribution of fields in the hairpin loop probe as the main remaining bottleneck for the improvement of low-field low-volume ODNP SNR."],"url":"http://arxiv.org/abs/2403.03943v1","category":"physics.chem-ph"}
{"created":"2024-03-06 18:50:14","title":"The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models","abstract":"Prior work has found that pretrained language models (LMs) fine-tuned with different random seeds can achieve similar in-domain performance but generalize differently on tests of syntactic generalization. In this work, we show that, even within a single model, we can find multiple subnetworks that perform similarly in-domain, but generalize vastly differently. To better understand these phenomena, we investigate if they can be understood in terms of \"competing subnetworks\": the model initially represents a variety of distinct algorithms, corresponding to different subnetworks, and generalization occurs when it ultimately converges to one. This explanation has been used to account for generalization in simple algorithmic tasks. Instead of finding competing subnetworks, we find that all subnetworks -- whether they generalize or not -- share a set of attention heads, which we refer to as the heuristic core. Further analysis suggests that these attention heads emerge early in training and compute shallow, non-generalizing features. The model learns to generalize by incorporating additional attention heads, which depend on the outputs of the \"heuristic\" heads to compute higher-level features. Overall, our results offer a more detailed picture of the mechanisms for syntactic generalization in pretrained LMs.","sentences":["Prior work has found that pretrained language models (LMs) fine-tuned with different random seeds can achieve similar in-domain performance but generalize differently on tests of syntactic generalization.","In this work, we show that, even within a single model, we can find multiple subnetworks that perform similarly in-domain, but generalize vastly differently.","To better understand these phenomena, we investigate if they can be understood in terms of \"competing subnetworks\": the model initially represents a variety of distinct algorithms, corresponding to different subnetworks, and generalization occurs when it ultimately converges to one.","This explanation has been used to account for generalization in simple algorithmic tasks.","Instead of finding competing subnetworks, we find that all subnetworks -- whether they generalize or not -- share a set of attention heads, which we refer to as the heuristic core.","Further analysis suggests that these attention heads emerge early in training and compute shallow, non-generalizing features.","The model learns to generalize by incorporating additional attention heads, which depend on the outputs of the \"heuristic\" heads to compute higher-level features.","Overall, our results offer a more detailed picture of the mechanisms for syntactic generalization in pretrained LMs."],"url":"http://arxiv.org/abs/2403.03942v1","category":"cs.CL"}
{"created":"2024-03-06 18:49:45","title":"Scalar curvature deformations with non-compact boundaries","abstract":"We develop a general deformation principle for families of Riemannian metrics on smooth manifolds with possibly non-compact boundary, preserving lower scalar curvature bounds. The principle is used in order to strengthen boundary conditions, from mean convex to totally geodesic or doubling. The deformation principle preserves further geometric properties such as completeness and a given quasi-isometry type. As an application, we prove non-existence results for Riemannian metrics with (uniformly) positive scalar curvature and mean convex boundary.","sentences":["We develop a general deformation principle for families of Riemannian metrics on smooth manifolds with possibly non-compact boundary, preserving lower scalar curvature bounds.","The principle is used in order to strengthen boundary conditions, from mean convex to totally geodesic or doubling.","The deformation principle preserves further geometric properties such as completeness and a given quasi-isometry type.","As an application, we prove non-existence results for Riemannian metrics with (uniformly) positive scalar curvature and mean convex boundary."],"url":"http://arxiv.org/abs/2403.03941v1","category":"math.DG"}
{"created":"2024-03-06 18:48:56","title":"Subgroups arising from connected components in the Morse boundary","abstract":"We study connected components of the Morse boundary and their stabilisers. We introduce the notion of point-convergence and show that if the set of non-singleton connected components of the Morse boundary of a finitely generated group $G$ is point-convergent, then every non-singleton connected component is the (relative) Morse boundary of its stabiliser. The above property only depends on the topology of the Morse boundary and hence is invariant under quasi-isometry. This shows that the topology of the Morse boundary not only carries algebraic information but can be used to detect certain subgroups which in some sense are invariant under quasi-isometry.","sentences":["We study connected components of the Morse boundary and their stabilisers.","We introduce the notion of point-convergence and show that if the set of non-singleton connected components of the Morse boundary of a finitely generated group $G$ is point-convergent, then every non-singleton connected component is the (relative) Morse boundary of its stabiliser.","The above property only depends on the topology of the Morse boundary and hence is invariant under quasi-isometry.","This shows that the topology of the Morse boundary not only carries algebraic information but can be used to detect certain subgroups which in some sense are invariant under quasi-isometry."],"url":"http://arxiv.org/abs/2403.03939v1","category":"math.GR"}
{"created":"2024-03-06 18:47:32","title":"GUIDE: Guidance-based Incremental Learning with Diffusion Models","abstract":"We introduce GUIDE, a novel continual learning approach that directs diffusion models to rehearse samples at risk of being forgotten. Existing generative strategies combat catastrophic forgetting by randomly sampling rehearsal examples from a generative model. Such an approach contradicts buffer-based approaches where sampling strategy plays an important role. We propose to bridge this gap by integrating diffusion models with classifier guidance techniques to produce rehearsal examples specifically targeting information forgotten by a continuously trained model. This approach enables the generation of samples from preceding task distributions, which are more likely to be misclassified in the context of recently encountered classes. Our experimental results show that GUIDE significantly reduces catastrophic forgetting, outperforming conventional random sampling approaches and surpassing recent state-of-the-art methods in continual learning with generative replay.","sentences":["We introduce GUIDE, a novel continual learning approach that directs diffusion models to rehearse samples at risk of being forgotten.","Existing generative strategies combat catastrophic forgetting by randomly sampling rehearsal examples from a generative model.","Such an approach contradicts buffer-based approaches where sampling strategy plays an important role.","We propose to bridge this gap by integrating diffusion models with classifier guidance techniques to produce rehearsal examples specifically targeting information forgotten by a continuously trained model.","This approach enables the generation of samples from preceding task distributions, which are more likely to be misclassified in the context of recently encountered classes.","Our experimental results show that GUIDE significantly reduces catastrophic forgetting, outperforming conventional random sampling approaches and surpassing recent state-of-the-art methods in continual learning with generative replay."],"url":"http://arxiv.org/abs/2403.03938v1","category":"cs.LG"}
{"created":"2024-03-06 18:43:14","title":"Demographic Dynamics and Artificial Intelligence: Challenges and Opportunities in Europe and Africa for 2050","abstract":"This paper explores the complex relationship between demographics and artificial intelligence (AI) advances in Europe and Africa, projecting into the year 2050. The advancement of AI technologies has occurred at diverse rates, with Africa lagging behind Europe. Moreover, the imminent economic consequences of demographic shifts require a more careful examination of immigration patterns, with Africa emerging as a viable labor pool for European countries. However, within these dynamics, questions are raised about the differences in AI proficiency between African immigrants and Europeans by 2050. This paper examines demographic trends and AI developments to unravel insights into the multifaceted challenges and opportunities that lie ahead in the realms of technology, the economy, and society as we look ahead to 2050.","sentences":["This paper explores the complex relationship between demographics and artificial intelligence (AI) advances in Europe and Africa, projecting into the year 2050.","The advancement of AI technologies has occurred at diverse rates, with Africa lagging behind Europe.","Moreover, the imminent economic consequences of demographic shifts require a more careful examination of immigration patterns, with Africa emerging as a viable labor pool for European countries.","However, within these dynamics, questions are raised about the differences in AI proficiency between African immigrants and Europeans by 2050.","This paper examines demographic trends and AI developments to unravel insights into the multifaceted challenges and opportunities that lie ahead in the realms of technology, the economy, and society as we look ahead to 2050."],"url":"http://arxiv.org/abs/2403.03935v1","category":"cs.CY"}
{"created":"2024-03-06 18:42:06","title":"A Categorical Treatment of Open Linear Systems","abstract":"An open stochastic system \\`a la Willems is a system affected two qualitatively different kinds of uncertainty -- one is probabilistic fluctuation, and the other one is nondeterminism caused by lack of information. We give a formalization of open stochastic systems in the language of category theory. A new construction, which we term copartiality, is needed to model the propagating lack of information (which corresponds to varying sigma-algebras).   As a concrete example, we discuss extended Gaussian distributions, which combine Gaussian probability with nondeterminism and correspond precisely to Willems' notion of Gaussian linear systems. We describe them both as measure-theoretic and abstract categorical entities, which enables us to rigorously describe a variety of phenomena like noisy physical laws and uninformative priors in Bayesian statistics. The category of extended Gaussian maps can be seen as a mutual generalization of Gaussian probability and linear relations, which connects the literature on categorical probability with ideas from control theory like signal-flow diagrams.","sentences":["An open stochastic system \\`a la Willems is a system affected two qualitatively different kinds of uncertainty -- one is probabilistic fluctuation, and the other one is nondeterminism caused by lack of information.","We give a formalization of open stochastic systems in the language of category theory.","A new construction, which we term copartiality, is needed to model the propagating lack of information (which corresponds to varying sigma-algebras).   ","As a concrete example, we discuss extended Gaussian distributions, which combine Gaussian probability with nondeterminism and correspond precisely to Willems' notion of Gaussian linear systems.","We describe them both as measure-theoretic and abstract categorical entities, which enables us to rigorously describe a variety of phenomena like noisy physical laws and uninformative priors in Bayesian statistics.","The category of extended Gaussian maps can be seen as a mutual generalization of Gaussian probability and linear relations, which connects the literature on categorical probability with ideas from control theory like signal-flow diagrams."],"url":"http://arxiv.org/abs/2403.03934v1","category":"cs.LO"}
{"created":"2024-03-06 18:41:01","title":"Physical viability of traversable Finslerian wormholes with traceless fluid under conformal symmetry","abstract":"The current study explores the novel potential of traversable wormhole solutions within the framework of Finsler geometry, incorporating conformal symmetry alongside traceless fluid dynamics. Using the Conformal Killing vector approach, we have discussed the wormholes based on traceless fluid within the intriguing framework of Finsler geometry. The field equations and the associated conformal factor are obtained specifically under the condition of conformal motion in Finsler geometry. Furthermore, we have successfully derived and examined the shape function, considering a range of values for the Finslerian parameter $\\lambda$. Our investigation extends to fundamental physical characteristics such as proper radial distance, active mass function, and total gravitational energy, aiming to understand their influence on the traversability of the wormhole. The observation of energy condition violations provides evidence for the exotic matter's presence near the throat, reinforcing the assertion of the Finslerian wormhole's traversability.","sentences":["The current study explores the novel potential of traversable wormhole solutions within the framework of Finsler geometry, incorporating conformal symmetry alongside traceless fluid dynamics.","Using the Conformal Killing vector approach, we have discussed the wormholes based on traceless fluid within the intriguing framework of Finsler geometry.","The field equations and the associated conformal factor are obtained specifically under the condition of conformal motion in Finsler geometry.","Furthermore, we have successfully derived and examined the shape function, considering a range of values for the Finslerian parameter $\\lambda$.","Our investigation extends to fundamental physical characteristics such as proper radial distance, active mass function, and total gravitational energy, aiming to understand their influence on the traversability of the wormhole.","The observation of energy condition violations provides evidence for the exotic matter's presence near the throat, reinforcing the assertion of the Finslerian wormhole's traversability."],"url":"http://arxiv.org/abs/2403.03931v1","category":"gr-qc"}
{"created":"2024-03-06 18:39:41","title":"Extreme Precipitation Nowcasting using Transformer-based Generative Models","abstract":"This paper presents an innovative approach to extreme precipitation nowcasting by employing Transformer-based generative models, namely NowcastingGPT with Extreme Value Loss (EVL) regularization. Leveraging a comprehensive dataset from the Royal Netherlands Meteorological Institute (KNMI), our study focuses on predicting short-term precipitation with high accuracy. We introduce a novel method for computing EVL without assuming fixed extreme representations, addressing the limitations of current models in capturing extreme weather events. We present both qualitative and quantitative analyses, demonstrating the superior performance of the proposed NowcastingGPT-EVL in generating accurate precipitation forecasts, especially when dealing with extreme precipitation events. The code is available at \\url{https://github.com/Cmeo97/NowcastingGPT}.","sentences":["This paper presents an innovative approach to extreme precipitation nowcasting by employing Transformer-based generative models, namely NowcastingGPT with Extreme Value Loss (EVL) regularization.","Leveraging a comprehensive dataset from the Royal Netherlands Meteorological Institute (KNMI), our study focuses on predicting short-term precipitation with high accuracy.","We introduce a novel method for computing EVL without assuming fixed extreme representations, addressing the limitations of current models in capturing extreme weather events.","We present both qualitative and quantitative analyses, demonstrating the superior performance of the proposed NowcastingGPT-EVL in generating accurate precipitation forecasts, especially when dealing with extreme precipitation events.","The code is available at \\url{https://github.com/Cmeo97/NowcastingGPT}."],"url":"http://arxiv.org/abs/2403.03929v1","category":"cs.LG"}
{"created":"2024-03-06 18:37:06","title":"Consciousness qua Mortal Computation","abstract":"Computational functionalism posits that consciousness is a computation. Here we show, perhaps surprisingly, that it cannot be a Turing computation. Rather, computational functionalism implies that consciousness is a novel type of computation that has recently been proposed by Geoffrey Hinton, called mortal computation.","sentences":["Computational functionalism posits that consciousness is a computation.","Here we show, perhaps surprisingly, that it cannot be a Turing computation.","Rather, computational functionalism implies that consciousness is a novel type of computation that has recently been proposed by Geoffrey Hinton, called mortal computation."],"url":"http://arxiv.org/abs/2403.03925v1","category":"q-bio.NC"}
{"created":"2024-03-06 18:30:59","title":"Collision Cascade-Driven Evolution of Vacancy Defects in Ni-Based Concentrated Solid-Solution Alloys","abstract":"Concentrated solid--solution alloys (CSAs) in single--phase form have recently garnered considerable attention owing to their potential for exceptional irradiation resistance. This computational study delves into the intricate interplay of alloying elements on the generation, recombination, and evolution of irradiation-induced defects. Molecular dynamics simulations were conducted for collision cascades at room temperature, spanning a range of primary knock-on atom energies from 1 to 10 keV. The investigation encompasses a series of model crystals, progressing from pure Ni to binary CSAs such as NiFe$_{20}$, NiFe, NiCr$_{20}$, and culminating in the more intricate NiFeCr$_{20}$ CSA. We observe that materials rich in chromium actively facilitate dislocation emissions and induce the nucleation of stacking fault tetrahedra in the proximity of nanovoids, owing to Shockley partial interactions. This result is validated by molecular static simulations, which calculate the surface, vacancy, and defect formation energies. Among various shapes considered, the spherical void proves to be the most stable, followed by the truncated octahedron and octahedron shapes. On the other hand, the tetrahedron cubic shape is identified as the most unstable, and stacking fault tetrahedra exhibit the highest formation energy. Notably, among the materials studied, NiCr$_{20}$ and NiFeCr$_{20}$ CSAs stood out as the sole alloys capable of manifesting this mechanism, mainly observed at high impact energies.","sentences":["Concentrated solid--solution alloys (CSAs) in single--phase form have recently garnered considerable attention owing to their potential for exceptional irradiation resistance.","This computational study delves into the intricate interplay of alloying elements on the generation, recombination, and evolution of irradiation-induced defects.","Molecular dynamics simulations were conducted for collision cascades at room temperature, spanning a range of primary knock-on atom energies from 1 to 10 keV.","The investigation encompasses a series of model crystals, progressing from pure Ni to binary CSAs such as NiFe$_{20}$, NiFe, NiCr$_{20}$, and culminating in the more intricate NiFeCr$_{20}$ CSA.","We observe that materials rich in chromium actively facilitate dislocation emissions and induce the nucleation of stacking fault tetrahedra in the proximity of nanovoids, owing to Shockley partial interactions.","This result is validated by molecular static simulations, which calculate the surface, vacancy, and defect formation energies.","Among various shapes considered, the spherical void proves to be the most stable, followed by the truncated octahedron and octahedron shapes.","On the other hand, the tetrahedron cubic shape is identified as the most unstable, and stacking fault tetrahedra exhibit the highest formation energy.","Notably, among the materials studied, NiCr$_{20}$ and NiFeCr$_{20}$ CSAs stood out as the sole alloys capable of manifesting this mechanism, mainly observed at high impact energies."],"url":"http://arxiv.org/abs/2403.03922v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-06 18:29:18","title":"Enhancing Instructional Quality: Leveraging Computer-Assisted Textual Analysis to Generate In-Depth Insights from Educational Artifacts","abstract":"This paper explores the transformative potential of computer-assisted textual analysis in enhancing instructional quality through in-depth insights from educational artifacts. We integrate Richard Elmore's Instructional Core Framework to examine how artificial intelligence (AI) and machine learning (ML) methods, particularly natural language processing (NLP), can analyze educational content, teacher discourse, and student responses to foster instructional improvement. Through a comprehensive review and case studies within the Instructional Core Framework, we identify key areas where AI/ML integration offers significant advantages, including teacher coaching, student support, and content development. We unveil patterns that indicate AI/ML not only streamlines administrative tasks but also introduces novel pathways for personalized learning, providing actionable feedback for educators and contributing to a richer understanding of instructional dynamics. This paper emphasizes the importance of aligning AI/ML technologies with pedagogical goals to realize their full potential in educational settings, advocating for a balanced approach that considers ethical considerations, data quality, and the integration of human expertise.","sentences":["This paper explores the transformative potential of computer-assisted textual analysis in enhancing instructional quality through in-depth insights from educational artifacts.","We integrate Richard Elmore's Instructional Core Framework to examine how artificial intelligence (AI) and machine learning (ML) methods, particularly natural language processing (NLP), can analyze educational content, teacher discourse, and student responses to foster instructional improvement.","Through a comprehensive review and case studies within the Instructional Core Framework, we identify key areas where AI/ML integration offers significant advantages, including teacher coaching, student support, and content development.","We unveil patterns that indicate AI/ML not only streamlines administrative tasks but also introduces novel pathways for personalized learning, providing actionable feedback for educators and contributing to a richer understanding of instructional dynamics.","This paper emphasizes the importance of aligning AI/ML technologies with pedagogical goals to realize their full potential in educational settings, advocating for a balanced approach that considers ethical considerations, data quality, and the integration of human expertise."],"url":"http://arxiv.org/abs/2403.03920v1","category":"cs.AI"}
{"created":"2024-03-06 18:29:17","title":"Multi-parameter quantum estimation of single- and two-mode pure Gaussian states","abstract":"We discuss the ultimate precision bounds on the multiparameter estimation of single- and two-mode pure Gaussian states. By leveraging on previous approaches that focused on the estimation of a complex displacement only, we derive the Holevo Cram\\'er-Rao bound (HCRB) for both displacement and squeezing parameter characterizing single and two-mode squeezed states. In the single-mode scenario, we obtain an analytical bound and find that it degrades monotonically as the squeezing increases. Furthermore, we prove that heterodyne detection is nearly optimal in the large squeezing limit, but in general the optimal measurement must include non-Gaussian resources. On the other hand, in the two-mode setting, the HCRB improves as the squeezing parameter grows and we show that it can be attained using double-homodyne detection.","sentences":["We discuss the ultimate precision bounds on the multiparameter estimation of single- and two-mode pure Gaussian states.","By leveraging on previous approaches that focused on the estimation of a complex displacement only, we derive the Holevo Cram\\'er-Rao bound (HCRB) for both displacement and squeezing parameter characterizing single and two-mode squeezed states.","In the single-mode scenario, we obtain an analytical bound and find that it degrades monotonically as the squeezing increases.","Furthermore, we prove that heterodyne detection is nearly optimal in the large squeezing limit, but in general the optimal measurement must include non-Gaussian resources.","On the other hand, in the two-mode setting, the HCRB improves as the squeezing parameter grows and we show that it can be attained using double-homodyne detection."],"url":"http://arxiv.org/abs/2403.03919v1","category":"quant-ph"}
{"created":"2024-03-06 18:22:01","title":"Multipolar opinion evolution in biased networks","abstract":"Motivated by empirical research on bias and opinion formation, we introduce a novel multidimensional nonlinear opinion dynamical model where agents have individual biases, which are fixed, as well as opinions, which evolve. The dimensions are coupled through a normalization step, which is also the source of the nonlinearity, so that the state describes an agent's relative opinion of various options. This can capture, for example, an individual's relative trust in different media. In special cases including where biases are uniform across agents our model achieves consensus, but in general, behaviors are richer and capture multipolar opinion distributions. We examine general fixed points of the system, as well as special cases such as zero biases toward certain options or partitioned decision sets. Lastly, we demonstrate that our model exhibits polarization when biases are spatially correlated across the network, while, as empirical research suggests, a mixed community can mediate biases.","sentences":["Motivated by empirical research on bias and opinion formation, we introduce a novel multidimensional nonlinear opinion dynamical model where agents have individual biases, which are fixed, as well as opinions, which evolve.","The dimensions are coupled through a normalization step, which is also the source of the nonlinearity, so that the state describes an agent's relative opinion of various options.","This can capture, for example, an individual's relative trust in different media.","In special cases including where biases are uniform across agents our model achieves consensus, but in general, behaviors are richer and capture multipolar opinion distributions.","We examine general fixed points of the system, as well as special cases such as zero biases toward certain options or partitioned decision sets.","Lastly, we demonstrate that our model exhibits polarization when biases are spatially correlated across the network, while, as empirical research suggests, a mixed community can mediate biases."],"url":"http://arxiv.org/abs/2403.03913v1","category":"eess.SY"}
{"created":"2024-03-06 18:15:49","title":"Digamma function and general Fischer series in the theory of Kempner sums","abstract":"The infinite sum of reciprocals of the integers which are missing specified digits (from a given base) is computed with the help of the digamma function as a series involving the zeta values at integers and certain quantities which are rational in the base. This generalizes the work of Fischer (1993) who had handled the original \"no $9$ in base $10$\" case.","sentences":["The infinite sum of reciprocals of the integers which are missing specified digits (from a given base) is computed with the help of the digamma function as a series involving the zeta values at integers and certain quantities which are rational in the base.","This generalizes the work of Fischer (1993) who had handled the original \"no $9$ in base $10$\" case."],"url":"http://arxiv.org/abs/2403.03912v1","category":"math.NT"}
{"created":"2024-03-06 18:07:06","title":"Topological spaces satisfying a closed graph theorem","abstract":"We discuss topological versions of the closed graph theorem, where continuity is inferred from near continuity in tandem with suitable conditions on source or target spaces. We seek internal characterizations of spaces satisfying a closed graph theorem, and we compare closed graph and open mapping spaces.","sentences":["We discuss topological versions of the closed graph theorem, where continuity is inferred from near continuity in tandem with suitable conditions on source or target spaces.","We seek internal characterizations of spaces satisfying a closed graph theorem, and we compare closed graph and open mapping spaces."],"url":"http://arxiv.org/abs/2403.03904v1","category":"math.GN"}
{"created":"2024-03-06 18:02:53","title":"A notion of $s$-fractional mass for $1$-currents in higher codimension","abstract":"In this paper we propose a notion of $s$-fractional mass for $1$-currents in $\\R^d$. Such a notion generalizes the notion of $s$-fractional perimeters for sets in the plane to higher codimension one-dimensional singularities. Remarkably, the limit as $s\\to 1$ of the $s$-fractional mass gives back the classical notion of length for regular enough curves in $\\R^d$.   We prove a lower semi-continuity and compactness result for sequences of $1$-currents with uniformly bounded fractional mass and support. Moreover, we prove the density of weighted polygonal, closed and compact oriented curves in the class of divergence-free 1-currents with compact support and finite fractional mass.   Finally, we discuss some possible applications of our notion of fractional mass to build up purely geometrical approaches to the variational modeling of dislocation lines in crystals and to vortex filaments in superconductivity.","sentences":["In this paper we propose a notion of $s$-fractional mass for $1$-currents in $\\R^d$. Such a notion generalizes the notion of $s$-fractional perimeters for sets in the plane to higher codimension one-dimensional singularities.","Remarkably, the limit as $s\\to 1$ of the $s$-fractional mass gives back the classical notion of length for regular enough curves in $\\R^d$.   We prove a lower semi-continuity and compactness result for sequences of $1$-currents with uniformly bounded fractional mass and support.","Moreover, we prove the density of weighted polygonal, closed and compact oriented curves in the class of divergence-free 1-currents with compact support and finite fractional mass.   ","Finally, we discuss some possible applications of our notion of fractional mass to build up purely geometrical approaches to the variational modeling of dislocation lines in crystals and to vortex filaments in superconductivity."],"url":"http://arxiv.org/abs/2403.03901v1","category":"math.FA"}
{"created":"2024-03-06 17:58:59","title":"Magnetoconvection in a Long Vertical Enclosure With Walls of Finite Electrical Conductivity","abstract":"Magnetoconvection in a tall vertical box with vertical hot and cold walls, and an imposed steady uniform magnetic field perpendicular to the temperature gradient, is analyzed numerically. The geometry and the values of the non-dimensional parameters - the Prandtl number of 0.025, the Rayleigh number of $7.5 \\times 10^5$, and the Hartmann number between 0 and 798 - match those of an earlier experiment. A parametric study of the effect of wall electric conductivity, across a wide range of conductance ratio values, on flow properties is performed. Two configurations of electric boundary conditions are explored. In one configuration, all walls have finite electric conductivity, while in the other, only the walls with constant temperature are electrically conducting. The flows are analyzed using their integral properties and distributions of velocity, temperature, and electric currents. It is found that, in general, the convection flow is suppressed by the magnetic field. However, this effect is strongly modified by the wall's electric conductivity and is markedly different for the two wall configurations. The associated changes in flow structure, rate of heat transfer, and flow's kinetic energy are revealed. It is also shown that the assumption of quasi-two-dimensionality may not be valid under some conditions, even at high Hartmann numbers.","sentences":["Magnetoconvection in a tall vertical box with vertical hot and cold walls, and an imposed steady uniform magnetic field perpendicular to the temperature gradient, is analyzed numerically.","The geometry and the values of the non-dimensional parameters - the Prandtl number of 0.025, the Rayleigh number of $7.5 \\times 10^5$, and the Hartmann number between 0 and 798 - match those of an earlier experiment.","A parametric study of the effect of wall electric conductivity, across a wide range of conductance ratio values, on flow properties is performed.","Two configurations of electric boundary conditions are explored.","In one configuration, all walls have finite electric conductivity, while in the other, only the walls with constant temperature are electrically conducting.","The flows are analyzed using their integral properties and distributions of velocity, temperature, and electric currents.","It is found that, in general, the convection flow is suppressed by the magnetic field.","However, this effect is strongly modified by the wall's electric conductivity and is markedly different for the two wall configurations.","The associated changes in flow structure, rate of heat transfer, and flow's kinetic energy are revealed.","It is also shown that the assumption of quasi-two-dimensionality may not be valid under some conditions, even at high Hartmann numbers."],"url":"http://arxiv.org/abs/2403.03899v1","category":"physics.flu-dyn"}
{"created":"2024-03-06 17:57:03","title":"Fuzzing BusyBox: Leveraging LLM and Crash Reuse for Embedded Bug Unearthing","abstract":"BusyBox, an open-source software bundling over 300 essential Linux commands into a single executable, is ubiquitous in Linux-based embedded devices. Vulnerabilities in BusyBox can have far-reaching consequences, affecting a wide array of devices. This research, driven by the extensive use of BusyBox, delved into its analysis. The study revealed the prevalence of older BusyBox versions in real-world embedded products, prompting us to conduct fuzz testing on BusyBox. Fuzzing, a pivotal software testing method, aims to induce crashes that are subsequently scrutinized to uncover vulnerabilities. Within this study, we introduce two techniques to fortify software testing. The first technique enhances fuzzing by leveraging Large Language Models (LLM) to generate target-specific initial seeds. Our study showed a substantial increase in crashes when using LLM-generated initial seeds, highlighting the potential of LLM to efficiently tackle the typically labor-intensive task of generating target-specific initial seeds. The second technique involves repurposing previously acquired crash data from similar fuzzed targets before initiating fuzzing on a new target. This approach streamlines the time-consuming fuzz testing process by providing crash data directly to the new target before commencing fuzzing. We successfully identified crashes in the latest BusyBox target without conducting traditional fuzzing, emphasizing the effectiveness of LLM and crash reuse techniques in enhancing software testing and improving vulnerability detection in embedded systems. Additionally, manual triaging was performed to identify the nature of crashes in the latest BusyBox.","sentences":["BusyBox, an open-source software bundling over 300 essential Linux commands into a single executable, is ubiquitous in Linux-based embedded devices.","Vulnerabilities in BusyBox can have far-reaching consequences, affecting a wide array of devices.","This research, driven by the extensive use of BusyBox, delved into its analysis.","The study revealed the prevalence of older BusyBox versions in real-world embedded products, prompting us to conduct fuzz testing on BusyBox.","Fuzzing, a pivotal software testing method, aims to induce crashes that are subsequently scrutinized to uncover vulnerabilities.","Within this study, we introduce two techniques to fortify software testing.","The first technique enhances fuzzing by leveraging Large Language Models (LLM) to generate target-specific initial seeds.","Our study showed a substantial increase in crashes when using LLM-generated initial seeds, highlighting the potential of LLM to efficiently tackle the typically labor-intensive task of generating target-specific initial seeds.","The second technique involves repurposing previously acquired crash data from similar fuzzed targets before initiating fuzzing on a new target.","This approach streamlines the time-consuming fuzz testing process by providing crash data directly to the new target before commencing fuzzing.","We successfully identified crashes in the latest BusyBox target without conducting traditional fuzzing, emphasizing the effectiveness of LLM and crash reuse techniques in enhancing software testing and improving vulnerability detection in embedded systems.","Additionally, manual triaging was performed to identify the nature of crashes in the latest BusyBox."],"url":"http://arxiv.org/abs/2403.03897v1","category":"cs.SE"}
{"created":"2024-03-06 17:54:50","title":"DART: Implicit Doppler Tomography for Radar Novel View Synthesis","abstract":"Simulation is an invaluable tool for radio-frequency system designers that enables rapid prototyping of various algorithms for imaging, target detection, classification, and tracking. However, simulating realistic radar scans is a challenging task that requires an accurate model of the scene, radio frequency material properties, and a corresponding radar synthesis function. Rather than specifying these models explicitly, we propose DART - Doppler Aided Radar Tomography, a Neural Radiance Field-inspired method which uses radar-specific physics to create a reflectance and transmittance-based rendering pipeline for range-Doppler images. We then evaluate DART by constructing a custom data collection platform and collecting a novel radar dataset together with accurate position and instantaneous velocity measurements from lidar-based localization. In comparison to state-of-the-art baselines, DART synthesizes superior radar range-Doppler images from novel views across all datasets and additionally can be used to generate high quality tomographic images.","sentences":["Simulation is an invaluable tool for radio-frequency system designers that enables rapid prototyping of various algorithms for imaging, target detection, classification, and tracking.","However, simulating realistic radar scans is a challenging task that requires an accurate model of the scene, radio frequency material properties, and a corresponding radar synthesis function.","Rather than specifying these models explicitly, we propose DART - Doppler Aided Radar Tomography, a Neural Radiance Field-inspired method which uses radar-specific physics to create a reflectance and transmittance-based rendering pipeline for range-Doppler images.","We then evaluate DART by constructing a custom data collection platform and collecting a novel radar dataset together with accurate position and instantaneous velocity measurements from lidar-based localization.","In comparison to state-of-the-art baselines, DART synthesizes superior radar range-Doppler images from novel views across all datasets and additionally can be used to generate high quality tomographic images."],"url":"http://arxiv.org/abs/2403.03896v1","category":"cs.CV"}
{"created":"2024-03-06 17:52:08","title":"IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators","abstract":"Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.   To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled with respective intermediate representations. Next, starting from various base Code-LMs (ranging in size from 1.1B to 7.3B parameters), we carry out continued causal language modelling training on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2) align the IR constructs with respective constructs of various programming languages. Our resulting models, dubbed IRCoder, display sizeable and consistent gains across a wide variety of code generation tasks and metrics, including prompt robustness, multilingual code completion, code understanding, and instruction following.","sentences":["Code understanding and generation have fast become some of the most popular applications of language models (LMs).","Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts.","In particular, most mainstream Code-LMs have been pre-trained on source code files alone.","In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.   ","To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled with respective intermediate representations.","Next, starting from various base Code-LMs (ranging in size from 1.1B to 7.3B parameters), we carry out continued causal language modelling training on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2) align the IR constructs with respective constructs of various programming languages.","Our resulting models, dubbed IRCoder, display sizeable and consistent gains across a wide variety of code generation tasks and metrics, including prompt robustness, multilingual code completion, code understanding, and instruction following."],"url":"http://arxiv.org/abs/2403.03894v1","category":"cs.AI"}
{"created":"2024-03-06 17:51:43","title":"From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models","abstract":"To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it's crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic families and levels of resource availability, ranging from high to mid-resource languages. Through comprehensive experiments, we provide insights into the complexities of multilingual toxicity mitigation, offering valuable insights and paving the way for future research in this increasingly important field. Code and data are available at https://github.com/for-ai/goodtriever.","sentences":["To date, toxicity mitigation in language models has almost entirely been focused on single-language settings.","As language models embrace multilingual capabilities, it's crucial our safety measures keep pace.","Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages.","In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques.","We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios.","This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation.","We also explore how model size and data quantity affect the success of these mitigation efforts.","Covering nine languages, our study represents a broad array of linguistic families and levels of resource availability, ranging from high to mid-resource languages.","Through comprehensive experiments, we provide insights into the complexities of multilingual toxicity mitigation, offering valuable insights and paving the way for future research in this increasingly important field.","Code and data are available at https://github.com/for-ai/goodtriever."],"url":"http://arxiv.org/abs/2403.03893v1","category":"cs.CL"}
{"created":"2024-03-06 17:50:26","title":"Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation","abstract":"This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical agent for multi-task robotic manipulation. HDP factorises a manipulation policy into a hierarchical structure: a high-level task-planning agent which predicts a distant next-best end-effector pose (NBP), and a low-level goal-conditioned diffusion policy which generates optimal motion trajectories. The factorised policy representation allows HDP to tackle both long-horizon task planning while generating fine-grained low-level actions. To generate context-aware motion trajectories while satisfying robot kinematics constraints, we present a novel kinematics-aware goal-conditioned control agent, Robot Kinematics Diffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the end-effector pose and joint position trajectories, and distill the accurate but kinematics-unaware end-effector pose diffuser to the kinematics-aware but less accurate joint position diffuser via differentiable kinematics. Empirically, we show that HDP achieves a significantly higher success rate than the state-of-the-art methods in both simulation and real-world.","sentences":["This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical agent for multi-task robotic manipulation.","HDP factorises a manipulation policy into a hierarchical structure: a high-level task-planning agent which predicts a distant next-best end-effector pose (NBP), and a low-level goal-conditioned diffusion policy which generates optimal motion trajectories.","The factorised policy representation allows HDP to tackle both long-horizon task planning while generating fine-grained low-level actions.","To generate context-aware motion trajectories while satisfying robot kinematics constraints, we present a novel kinematics-aware goal-conditioned control agent, Robot Kinematics Diffuser (RK-Diffuser).","Specifically, RK-Diffuser learns to generate both the end-effector pose and joint position trajectories, and distill the accurate but kinematics-unaware end-effector pose diffuser to the kinematics-aware but less accurate joint position diffuser via differentiable kinematics.","Empirically, we show that HDP achieves a significantly higher success rate than the state-of-the-art methods in both simulation and real-world."],"url":"http://arxiv.org/abs/2403.03890v1","category":"cs.RO"}
{"created":"2024-03-06 17:48:06","title":"FaaF: Facts as a Function for the evaluation of RAG systems","abstract":"Factual recall from a reference source is crucial for evaluating the performance of Retrieval Augmented Generation (RAG) systems, as it directly probes into the quality of both retrieval and generation. However, it still remains a challenge to perform this evaluation reliably and efficiently. Recent work has focused on fact verification via prompting language model (LM) evaluators, however we demonstrate that these methods are unreliable in the presence of incomplete or inaccurate information. We introduce Facts as a Function (FaaF), a new approach to fact verification that utilizes the function calling abilities of LMs and a framework for RAG factual recall evaluation. FaaF substantially improves the ability of LMs to identify unsupported facts in text with incomplete information whilst improving efficiency and lowering cost by several times, compared to prompt-based approaches.","sentences":["Factual recall from a reference source is crucial for evaluating the performance of Retrieval Augmented Generation (RAG) systems, as it directly probes into the quality of both retrieval and generation.","However, it still remains a challenge to perform this evaluation reliably and efficiently.","Recent work has focused on fact verification via prompting language model (LM) evaluators, however we demonstrate that these methods are unreliable in the presence of incomplete or inaccurate information.","We introduce Facts as a Function (FaaF), a new approach to fact verification that utilizes the function calling abilities of LMs and a framework for RAG factual recall evaluation.","FaaF substantially improves the ability of LMs to identify unsupported facts in text with incomplete information whilst improving efficiency and lowering cost by several times, compared to prompt-based approaches."],"url":"http://arxiv.org/abs/2403.03888v1","category":"cs.CL"}
{"created":"2024-03-06 17:46:02","title":"Homeomorphism groups of telescoping 2-manifolds are strongly distorted","abstract":"Building on the work of Mann and Rafi, we introduce an expanded definition of a telescoping 2-manifold and proceed to study the homeomorphism group of a telescoping 2-manifold. Our main result shows that it is strongly distorted. We then give a simple description of its commutator subgroup, which is index one, two, or four depending on the topology of the manifold. Moreover, we show its commutator subgroup is uniformly perfect with commutator width at most two, and we give a family of uniform normal generators for its commutator subgroup. As a consequence of the latter result, we show that for an (orientable) telescoping 2-manifold, every (orientation-preserving) homeomorphism can be expressed as a product of at most 17 conjugate involutions. Finally, we provide analogous statements for mapping class groups.","sentences":["Building on the work of Mann and Rafi, we introduce an expanded definition of a telescoping 2-manifold and proceed to study the homeomorphism group of a telescoping 2-manifold.","Our main result shows that it is strongly distorted.","We then give a simple description of its commutator subgroup, which is index one, two, or four depending on the topology of the manifold.","Moreover, we show its commutator subgroup is uniformly perfect with commutator width at most two, and we give a family of uniform normal generators for its commutator subgroup.","As a consequence of the latter result, we show that for an (orientable) telescoping 2-manifold, every (orientation-preserving) homeomorphism can be expressed as a product of at most 17 conjugate involutions.","Finally, we provide analogous statements for mapping class groups."],"url":"http://arxiv.org/abs/2403.03887v1","category":"math.GT"}
{"created":"2024-03-06 17:45:50","title":"A Virtual Element method for non-Newtonian fluid flows","abstract":"In this paper, we design and analyze a Virtual Element discretization for the steady motion of non-Newtonian, incompressible fluids. A specific stabilization, tailored to mimic the monotonicity and boundedness properties of the continuous operator, is introduced and theoretically investigated. The proposed method has several appealing features, including the exact enforcement of the divergence free condition and the possibility of making use of fully general polygonal meshes. A complete well-posedness and convergence analysis of the proposed method is presented under mild assumptions on the non-linear laws, encompassing common examples such as the Carreau--Yasuda model. Numerical experiments validating the theoretical bounds as well as demonstrating the practical capabilities of the proposed formulation are presented.","sentences":["In this paper, we design and analyze a Virtual Element discretization for the steady motion of non-Newtonian, incompressible fluids.","A specific stabilization, tailored to mimic the monotonicity and boundedness properties of the continuous operator, is introduced and theoretically investigated.","The proposed method has several appealing features, including the exact enforcement of the divergence free condition and the possibility of making use of fully general polygonal meshes.","A complete well-posedness and convergence analysis of the proposed method is presented under mild assumptions on the non-linear laws, encompassing common examples such as the Carreau--Yasuda model.","Numerical experiments validating the theoretical bounds as well as demonstrating the practical capabilities of the proposed formulation are presented."],"url":"http://arxiv.org/abs/2403.03886v1","category":"math.NA"}
{"created":"2024-03-06 17:44:10","title":"Generalization of Cycle Decompositions of Even Dimensional Hypercubes on $d$-Dimensional Toruses","abstract":"We consider cycle decompositions of even, $2an$-dimensional hypercubes $Q_{2an},$ where $a \\geq 3$ is odd and $n \\geq 1.$ Prior work done by Axenovich, Offner, and Tompkins focused on obtaining the existence of cycle decompositions for even-dimensional hypercubes using long cycles of a given form, leaving out cycles of shorter lengths and, in fact, cycles of even longer lengths than those obtained there, such as $C_{7 \\cdot 2^{11}}$ in the case of $Q_{14}.$ In this paper, we provide two novel methods for explicitly constructing cycle decompositions of virtually all possible cycle lengths, using cycles of a given form, on Cartesian products of cycles up to those known by the work of Axenovich, Offner, and Tompkins. In particular, we show that we can explicitly obtain cycle decompositions of even dimensional hypercubes $Q_{2an}$ for all lengths mentioned above while on the same Cartesian product of cycles. With this, the current understanding of cycle decompositions of even dimensional hypercubes is furthered constructively and is featured with some interesting consequences for when $a$ is a positive, even integer. Additionally, progress is made towards obtaining cycle decompositions using the longest admissible cycle lengths with the incorporation of a more explicit starting point from which such decompositions of $Q_{2an}$ can be studied further.","sentences":["We consider cycle decompositions of even, $2an$-dimensional hypercubes $Q_{2an},$ where $a \\geq 3$ is odd and $n \\geq 1.$","Prior work done by Axenovich, Offner, and Tompkins focused on obtaining the existence of cycle decompositions for even-dimensional hypercubes using long cycles of a given form, leaving out cycles of shorter lengths and, in fact, cycles of even longer lengths than those obtained there, such as $C_{7 \\cdot 2^{11}}$ in the case of $Q_{14}.$ In this paper, we provide two novel methods for explicitly constructing cycle decompositions of virtually all possible cycle lengths, using cycles of a given form, on Cartesian products of cycles up to those known by the work of Axenovich, Offner, and Tompkins.","In particular, we show that we can explicitly obtain cycle decompositions of even dimensional hypercubes $Q_{2an}$ for all lengths mentioned above while on the same Cartesian product of cycles.","With this, the current understanding of cycle decompositions of even dimensional hypercubes is furthered constructively and is featured with some interesting consequences for when $a$ is a positive, even integer.","Additionally, progress is made towards obtaining cycle decompositions using the longest admissible cycle lengths with the incorporation of a more explicit starting point from which such decompositions of $Q_{2an}$ can be studied further."],"url":"http://arxiv.org/abs/2403.03885v1","category":"math.CO"}
{"created":"2024-03-06 17:43:05","title":"Towards a Schauder theory for fractional viscous Hamilton--Jacobi equations","abstract":"We survey some results on Lipschitz and Schauder regularity estimates for viscous Hamilton--Jacobi equations with subcritical L\\'evy diffusions. The Schauder estimates, along with existence of smooth solutions, are obtained with the help of a Duhamel formula and $L^1$ bounds on the spatial derivatives of the heat kernel. Our results cover very general nonlocal and mixed local-nonlocal diffusions, including strongly anisotropic, nonsymmetric, mixed order, and spectrally one-sided models.","sentences":["We survey some results on Lipschitz and Schauder regularity estimates for viscous Hamilton--Jacobi equations with subcritical L\\'evy diffusions.","The Schauder estimates, along with existence of smooth solutions, are obtained with the help of a Duhamel formula and $L^1$ bounds on the spatial derivatives of the heat kernel.","Our results cover very general nonlocal and mixed local-nonlocal diffusions, including strongly anisotropic, nonsymmetric, mixed order, and spectrally one-sided models."],"url":"http://arxiv.org/abs/2403.03884v1","category":"math.AP"}
{"created":"2024-03-06 17:42:16","title":"SaulLM-7B: A pioneering Large Language Model for Law","abstract":"In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is released under the CC-BY-SA-4.0 License.","sentences":["In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain.","With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation.","Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens.","SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents.","Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks.","SaulLM-7B is released under the CC-BY-SA-4.0 License."],"url":"http://arxiv.org/abs/2403.03883v1","category":"cs.CL"}
{"created":"2024-03-06 17:41:41","title":"Latent Dataset Distillation with Diffusion Models","abstract":"The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both challenges. LD3M incorporates a novel diffusion process tailored for dataset distillation, which improves the gradient norms for learning synthetic images. By adjusting the number of diffusion steps, LD3M also offers a straightforward way of controlling the trade-off between speed and accuracy. We evaluate our approach in several ImageNet subsets and for high-resolution images (128x128 and 256x256). As a result, LD3M consistently outperforms state-of-the-art distillation techniques by up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively.","sentences":["The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets.","However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model.","In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged.","One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets.","However, the final accuracy is lower if the employed model architecture differs from the model used during distillation.","Another challenge is the generation of high-resolution images, e.g., 128x128 and higher.","In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both challenges.","LD3M incorporates a novel diffusion process tailored for dataset distillation, which improves the gradient norms for learning synthetic images.","By adjusting the number of diffusion steps, LD3M also offers a straightforward way of controlling the trade-off between speed and accuracy.","We evaluate our approach in several ImageNet subsets and for high-resolution images (128x128 and 256x256).","As a result, LD3M consistently outperforms state-of-the-art distillation techniques by up to 4.8 p.p. and 4.2 p.p.","for 1 and 10 images per class, respectively."],"url":"http://arxiv.org/abs/2403.03881v1","category":"cs.CV"}
{"created":"2024-03-06 17:38:33","title":"Redefining cystoscopy with ai: bladder cancer diagnosis using an efficient hybrid cnn-transformer model","abstract":"Bladder cancer ranks within the top 10 most diagnosed cancers worldwide and is among the most expensive cancers to treat due to the high recurrence rates which require lifetime follow-ups. The primary tool for diagnosis is cystoscopy, which heavily relies on doctors' expertise and interpretation. Therefore, annually, numerous cases are either undiagnosed or misdiagnosed and treated as urinary infections. To address this, we suggest a deep learning approach for bladder cancer detection and segmentation which combines CNNs with a lightweight positional-encoding-free transformer and dual attention gates that fuse self and spatial attention for feature enhancement. The architecture suggested in this paper is efficient making it suitable for medical scenarios that require real time inference. Experiments have proven that this model addresses the critical need for a balance between computational efficiency and diagnostic accuracy in cystoscopic imaging as despite its small size it rivals large models in performance.","sentences":["Bladder cancer ranks within the top 10 most diagnosed cancers worldwide and is among the most expensive cancers to treat due to the high recurrence rates which require lifetime follow-ups.","The primary tool for diagnosis is cystoscopy, which heavily relies on doctors' expertise and interpretation.","Therefore, annually, numerous cases are either undiagnosed or misdiagnosed and treated as urinary infections.","To address this, we suggest a deep learning approach for bladder cancer detection and segmentation which combines CNNs with a lightweight positional-encoding-free transformer and dual attention gates that fuse self and spatial attention for feature enhancement.","The architecture suggested in this paper is efficient making it suitable for medical scenarios that require real time inference.","Experiments have proven that this model addresses the critical need for a balance between computational efficiency and diagnostic accuracy in cystoscopic imaging as despite its small size it rivals large models in performance."],"url":"http://arxiv.org/abs/2403.03879v1","category":"cs.CV"}
{"created":"2024-03-06 17:35:27","title":"Impoverished Language Technology: The Lack of (Social) Class in NLP","abstract":"Since Labov's (1964) foundational work on the social stratification of language, linguistics has dedicated concerted efforts towards understanding the relationships between socio-demographic factors and language production and perception. Despite the large body of evidence identifying significant relationships between socio-demographic factors and language production, relatively few of these factors have been investigated in the context of NLP technology. While age and gender are well covered, Labov's initial target, socio-economic class, is largely absent. We survey the existing Natural Language Processing (NLP) literature and find that only 20 papers even mention socio-economic status. However, the majority of those papers do not engage with class beyond collecting information of annotator-demographics. Given this research lacuna, we provide a definition of class that can be operationalised by NLP researchers, and argue for including socio-economic class in future language technologies.","sentences":["Since Labov's (1964) foundational work on the social stratification of language, linguistics has dedicated concerted efforts towards understanding the relationships between socio-demographic factors and language production and perception.","Despite the large body of evidence identifying significant relationships between socio-demographic factors and language production, relatively few of these factors have been investigated in the context of NLP technology.","While age and gender are well covered, Labov's initial target, socio-economic class, is largely absent.","We survey the existing Natural Language Processing (NLP) literature and find that only 20 papers even mention socio-economic status.","However, the majority of those papers do not engage with class beyond collecting information of annotator-demographics.","Given this research lacuna, we provide a definition of class that can be operationalised by NLP researchers, and argue for including socio-economic class in future language technologies."],"url":"http://arxiv.org/abs/2403.03874v1","category":"cs.CL"}
{"created":"2024-03-06 17:23:28","title":"Learning to Decode Collaboratively with Multiple Language Models","abstract":"We propose a method to teach multiple large language models (LLM) to collaborate by interleaving their generations at the token level. We model the decision of which LLM generates the next token as a latent variable. By optimizing the marginal likelihood of a training set under our latent variable model, the base LLM automatically learns when to generate itself and when to call on one of the ``assistant'' language models to generate, all without direct supervision. Token-level collaboration during decoding allows for a fusion of each model's expertise in a manner tailored to the specific task at hand. Our collaborative decoding is especially useful in cross-domain settings where a generalist base LLM learns to invoke domain expert models. On instruction-following, domain-specific QA, and reasoning tasks, we show that the performance of the joint system exceeds that of the individual models. Through qualitative analysis of the learned latent decisions, we show models trained with our method exhibit several interesting collaboration patterns, e.g., template-filling. Our code is available at https://github.com/clinicalml/co-llm.","sentences":["We propose a method to teach multiple large language models (LLM) to collaborate by interleaving their generations at the token level.","We model the decision of which LLM generates the next token as a latent variable.","By optimizing the marginal likelihood of a training set under our latent variable model, the base LLM automatically learns when to generate itself and when to call on one of the ``assistant'' language models to generate, all without direct supervision.","Token-level collaboration during decoding allows for a fusion of each model's expertise in a manner tailored to the specific task at hand.","Our collaborative decoding is especially useful in cross-domain settings where a generalist base LLM learns to invoke domain expert models.","On instruction-following, domain-specific QA, and reasoning tasks, we show that the performance of the joint system exceeds that of the individual models.","Through qualitative analysis of the learned latent decisions, we show models trained with our method exhibit several interesting collaboration patterns, e.g., template-filling.","Our code is available at https://github.com/clinicalml/co-llm."],"url":"http://arxiv.org/abs/2403.03870v1","category":"cs.CL"}
{"created":"2024-03-06 17:23:28","title":"Decoupled Vertical Federated Learning for Practical Training on Vertically Partitioned Data","abstract":"Vertical Federated Learning (VFL) is an emergent distributed machine learning paradigm wherein owners of disjoint features of a common set of entities collaborate to learn a global model without sharing data. In VFL, a host client owns data labels for each entity and learns a final representation based on intermediate local representations from all guest clients. Therefore, the host is a single point of failure and label feedback can be used by malicious guest clients to infer private features. Requiring all participants to remain active and trustworthy throughout the entire training process is generally impractical and altogether infeasible outside of controlled environments. We propose Decoupled VFL (DVFL), a blockwise learning approach to VFL. By training each model on its own objective, DVFL allows for decentralized aggregation and isolation between feature learning and label supervision. With these properties, DVFL is fault tolerant and secure. We implement DVFL to train split neural networks and show that model performance is comparable to VFL on a variety of classification datasets.","sentences":["Vertical Federated Learning (VFL) is an emergent distributed machine learning paradigm wherein owners of disjoint features of a common set of entities collaborate to learn a global model without sharing data.","In VFL, a host client owns data labels for each entity and learns a final representation based on intermediate local representations from all guest clients.","Therefore, the host is a single point of failure and label feedback can be used by malicious guest clients to infer private features.","Requiring all participants to remain active and trustworthy throughout the entire training process is generally impractical and altogether infeasible outside of controlled environments.","We propose Decoupled VFL (DVFL), a blockwise learning approach to VFL.","By training each model on its own objective, DVFL allows for decentralized aggregation and isolation between feature learning and label supervision.","With these properties, DVFL is fault tolerant and secure.","We implement DVFL to train split neural networks and show that model performance is comparable to VFL on a variety of classification datasets."],"url":"http://arxiv.org/abs/2403.03871v1","category":"cs.LG"}
{"created":"2024-03-06 17:18:24","title":"Confidence on the Focal: Conformal Prediction with Selection-Conditional Coverage","abstract":"Conformal prediction builds marginally valid prediction intervals which cover the unknown outcome of a randomly drawn new test point with a prescribed probability. In practice, a common scenario is that, after seeing the test unit(s), practitioners decide which test unit(s) to focus on in a data-driven manner, and wish to quantify the uncertainty for the focal unit(s). In such cases, marginally valid prediction intervals for these focal units can be misleading due to selection bias. This paper presents a general framework for constructing a prediction set with finite-sample exact coverage conditional on the unit being selected. Its general form works for arbitrary selection rules, and generalizes Mondrian Conformal Prediction to multiple test units and non-equivariant classifiers. We then work out computationally efficient implementation of our framework for a number of realistic selection rules, including top-K selection, optimization-based selection, selection based on conformal p-values, and selection based on properties of preliminary conformal prediction sets. The performance of our methods is demonstrated via applications in drug discovery and health risk prediction.","sentences":["Conformal prediction builds marginally valid prediction intervals which cover the unknown outcome of a randomly drawn new test point with a prescribed probability.","In practice, a common scenario is that, after seeing the test unit(s), practitioners decide which test unit(s) to focus on in a data-driven manner, and wish to quantify the uncertainty for the focal unit(s).","In such cases, marginally valid prediction intervals for these focal units can be misleading due to selection bias.","This paper presents a general framework for constructing a prediction set with finite-sample exact coverage conditional on the unit being selected.","Its general form works for arbitrary selection rules, and generalizes Mondrian Conformal Prediction to multiple test units and non-equivariant classifiers.","We then work out computationally efficient implementation of our framework for a number of realistic selection rules, including top-K selection, optimization-based selection, selection based on conformal p-values, and selection based on properties of preliminary conformal prediction sets.","The performance of our methods is demonstrated via applications in drug discovery and health risk prediction."],"url":"http://arxiv.org/abs/2403.03868v1","category":"stat.ME"}
{"created":"2024-03-06 17:16:44","title":"KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions","abstract":"Large language models (LLMs) adapted to follow user instructions are now widely deployed as conversational agents. In this work, we examine one increasingly common instruction-following task: providing writing assistance to compose a long-form answer. To evaluate the capabilities of current LLMs on this task, we construct KIWI, a dataset of knowledge-intensive writing instructions in the scientific domain. Given a research question, an initial model-generated answer and a set of relevant papers, an expert annotator iteratively issues instructions for the model to revise and improve its answer. We collect 1,260 interaction turns from 234 interaction sessions with three state-of-the-art LLMs. Each turn includes a user instruction, a model response, and a human evaluation of the model response. Through a detailed analysis of the collected responses, we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits. Further, we find that models struggle to judge whether their outputs successfully followed user instructions, with accuracy at least 10 points short of human agreement. Our findings indicate that KIWI will be a valuable resource to measure progress and improve LLMs' instruction-following capabilities for knowledge intensive writing tasks.","sentences":["Large language models (LLMs) adapted to follow user instructions are now widely deployed as conversational agents.","In this work, we examine one increasingly common instruction-following task: providing writing assistance to compose a long-form answer.","To evaluate the capabilities of current LLMs on this task, we construct KIWI, a dataset of knowledge-intensive writing instructions in the scientific domain.","Given a research question, an initial model-generated answer and a set of relevant papers, an expert annotator iteratively issues instructions for the model to revise and improve its answer.","We collect 1,260 interaction turns from 234 interaction sessions with three state-of-the-art LLMs.","Each turn includes a user instruction, a model response, and a human evaluation of the model response.","Through a detailed analysis of the collected responses, we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits.","Further, we find that models struggle to judge whether their outputs successfully followed user instructions, with accuracy at least 10 points short of human agreement.","Our findings indicate that KIWI will be a valuable resource to measure progress and improve LLMs' instruction-following capabilities for knowledge intensive writing tasks."],"url":"http://arxiv.org/abs/2403.03866v1","category":"cs.CL"}
{"created":"2024-03-06 17:15:04","title":"Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning","abstract":"This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models in solving algorithmic puzzles that necessitate both visual understanding, language understanding, and complex algorithmic reasoning. We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic, combinatorics, graph theory, optimization, search, etc., aiming to evaluate the gap between visual data interpretation and algorithmic problem-solving skills. The dataset is generated automatically from code authored by humans. All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations. It ensures that our dataset can be scaled up arbitrarily in terms of reasoning complexity and dataset size. Our investigation reveals that large language models (LLMs) such as GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We find that their performance is near random in a multi-choice question-answering setup for a significant number of puzzles. The findings emphasize the challenges of integrating visual, language, and algorithmic knowledge for solving complex reasoning problems.","sentences":["This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering.","We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models in solving algorithmic puzzles that necessitate both visual understanding, language understanding, and complex algorithmic reasoning.","We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic, combinatorics, graph theory, optimization, search, etc., aiming to evaluate the gap between visual data interpretation and algorithmic problem-solving skills.","The dataset is generated automatically from code authored by humans.","All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations.","It ensures that our dataset can be scaled up arbitrarily in terms of reasoning complexity and dataset size.","Our investigation reveals that large language models (LLMs) such as GPT4V and Gemini exhibit limited performance in puzzle-solving tasks.","We find that their performance is near random in a multi-choice question-answering setup for a significant number of puzzles.","The findings emphasize the challenges of integrating visual, language, and algorithmic knowledge for solving complex reasoning problems."],"url":"http://arxiv.org/abs/2403.03864v1","category":"cs.CV"}
{"created":"2024-03-06 17:13:24","title":"X-Shot: A Unified System to Handle Frequent, Few-shot and Zero-shot Learning Simultaneously in Classification","abstract":"In recent years, few-shot and zero-shot learning, which learn to predict labels with limited annotated instances, have garnered significant attention. Traditional approaches often treat frequent-shot (freq-shot; labels with abundant instances), few-shot, and zero-shot learning as distinct challenges, optimizing systems for just one of these scenarios. Yet, in real-world settings, label occurrences vary greatly. Some of them might appear thousands of times, while others might only appear sporadically or not at all. For practical deployment, it is crucial that a system can adapt to any label occurrence. We introduce a novel classification challenge: X-shot, reflecting a real-world context where freq-shot, few-shot, and zero-shot labels co-occur without predefined limits. Here, X can span from 0 to positive infinity. The crux of X-shot centers on open-domain generalization and devising a system versatile enough to manage various label scenarios. To solve X-shot, we propose BinBin (Binary INference Based on INstruction following) that leverages the Indirect Supervision from a large collection of NLP tasks via instruction following, bolstered by Weak Supervision provided by large language models. BinBin surpasses previous state-of-the-art techniques on three benchmark datasets across multiple domains. To our knowledge, this is the first work addressing X-shot learning, where X remains variable.","sentences":["In recent years, few-shot and zero-shot learning, which learn to predict labels with limited annotated instances, have garnered significant attention.","Traditional approaches often treat frequent-shot (freq-shot; labels with abundant instances), few-shot, and zero-shot learning as distinct challenges, optimizing systems for just one of these scenarios.","Yet, in real-world settings, label occurrences vary greatly.","Some of them might appear thousands of times, while others might only appear sporadically or not at all.","For practical deployment, it is crucial that a system can adapt to any label occurrence.","We introduce a novel classification challenge: X-shot, reflecting a real-world context where freq-shot, few-shot, and zero-shot labels co-occur without predefined limits.","Here, X can span from 0 to positive infinity.","The crux of X-shot centers on open-domain generalization and devising a system versatile enough to manage various label scenarios.","To solve X-shot, we propose BinBin (Binary INference Based on INstruction following) that leverages the Indirect Supervision from a large collection of NLP tasks via instruction following, bolstered by Weak Supervision provided by large language models.","BinBin surpasses previous state-of-the-art techniques on three benchmark datasets across multiple domains.","To our knowledge, this is the first work addressing X-shot learning, where X remains variable."],"url":"http://arxiv.org/abs/2403.03863v1","category":"cs.CL"}
{"created":"2024-03-06 17:06:11","title":"Public-data Assisted Private Stochastic Optimization: Power and Limitations","abstract":"We study the limits and capability of public-data assisted differentially private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data. For complete/labeled public data, we show that any $(\\epsilon,\\delta)$-PA-DP has excess risk $\\tilde{\\Omega}\\big(\\min\\big\\{\\frac{1}{\\sqrt{n_{\\text{pub}}}},\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon} \\big\\} \\big)$, where $d$ is the dimension, ${n_{\\text{pub}}}$ is the number of public samples, ${n_{\\text{priv}}}$ is the number of private samples, and $n={n_{\\text{pub}}}+{n_{\\text{priv}}}$. These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form. Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal. We also study PA-DP supervised learning with \\textit{unlabeled} public samples. In contrast to our previous result, we here show novel methods for leveraging public data in private supervised learning. For generalized linear models (GLM) with unlabeled public data, we show an efficient algorithm which, given $\\tilde{O}({n_{\\text{priv}}}\\epsilon)$ unlabeled public samples, achieves the dimension independent rate $\\tilde{O}\\big(\\frac{1}{\\sqrt{{n_{\\text{priv}}}}} + \\frac{1}{\\sqrt{{n_{\\text{priv}}}\\epsilon}}\\big)$. We develop new lower bounds for this setting which shows that this rate cannot be improved with more public samples, and any fewer public samples leads to a worse rate. Finally, we provide extensions of this result to general hypothesis classes with finite fat-shattering dimension with applications to neural networks and non-Euclidean geometries.","sentences":["We study the limits and capability of public-data assisted differentially private (PA-DP) algorithms.","Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data.","For complete/labeled public data, we show that any $(\\epsilon,\\delta)$-PA-DP has excess risk $\\tilde{\\Omega}\\big(\\min\\big\\{\\frac{1}{\\sqrt{n_{\\text{pub}}}},\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon} \\big\\} \\big)$, where $d$ is the dimension, ${n_{\\text{pub}}}$ is the number of public samples, ${n_{\\text{priv}}}$ is the number of private samples, and $n={n_{\\text{pub}}}+{n_{\\text{priv}}}$. These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form.","Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal.","We also study PA-DP supervised learning with \\textit{unlabeled} public samples.","In contrast to our previous result, we here show novel methods for leveraging public data in private supervised learning.","For generalized linear models (GLM) with unlabeled public data, we show an efficient algorithm which, given $\\tilde{O}({n_{\\text{priv}}}\\epsilon)$ unlabeled public samples, achieves the dimension independent rate $\\tilde{O}\\big(\\frac{1}{\\sqrt{{n_{\\text{priv}}}}} + \\frac{1}{\\sqrt{{n_{\\text{priv}}}\\epsilon}}\\big)$. We develop new lower bounds for this setting which shows that this rate cannot be improved with more public samples, and any fewer public samples leads to a worse rate.","Finally, we provide extensions of this result to general hypothesis classes with finite fat-shattering dimension with applications to neural networks and non-Euclidean geometries."],"url":"http://arxiv.org/abs/2403.03856v1","category":"cs.LG"}
{"created":"2024-03-06 17:02:39","title":"Accelerating Convergence of Score-Based Diffusion Models, Provably","abstract":"Score-based diffusion models, while achieving remarkable empirical performance, often suffer from low sampling speed, due to extensive function evaluations needed during the sampling phase. Despite a flurry of recent activities towards speeding up diffusion generative modeling in practice, theoretical underpinnings for acceleration techniques remain severely limited. In this paper, we design novel training-free algorithms to accelerate popular deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers. Our accelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the number of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our accelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the rate $O(1/\\sqrt{T})$ for the DDPM sampler. The design of our algorithms leverages insights from higher-order approximation, and shares similar intuitions as popular high-order ODE solvers like the DPM-Solver-2. Our theory accommodates $\\ell_2$-accurate score estimates, and does not require log-concavity or smoothness on the target distribution.","sentences":["Score-based diffusion models, while achieving remarkable empirical performance, often suffer from low sampling speed, due to extensive function evaluations needed during the sampling phase.","Despite a flurry of recent activities towards speeding up diffusion generative modeling in practice, theoretical underpinnings for acceleration techniques remain severely limited.","In this paper, we design novel training-free algorithms to accelerate popular deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers.","Our accelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the number of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our accelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the rate $O(1/\\sqrt{T})$ for the DDPM sampler.","The design of our algorithms leverages insights from higher-order approximation, and shares similar intuitions as popular high-order ODE solvers like the DPM-Solver-2.","Our theory accommodates $\\ell_2$-accurate score estimates, and does not require log-concavity or smoothness on the target distribution."],"url":"http://arxiv.org/abs/2403.03852v1","category":"cs.LG"}
{"created":"2024-03-06 16:49:33","title":"MedMamba: Vision Mamba for Medical Image Classification","abstract":"Medical image classification is a very fundamental and crucial task in the field of computer vision. These years, CNN-based and Transformer-based models are widely used in classifying various medical images. Unfortunately, The limitation of CNNs in long-range modeling capabilities prevent them from effectively extracting fine-grained features in medical images , while Transformers are hampered by their quadratic computational complexity. Recent research has shown that the state space model (SSM) represented by Mamba can efficiently model long-range interactions while maintaining linear computational complexity. Inspired by this, we propose Vision Mamba for medical image classification (MedMamba). More specifically, we introduce a novel Conv-SSM module, which combines the local feature extraction ability of convolutional layers with the ability of SSM to capture long-range dependency. To demonstrate the potential of MedMamba, we conduct extensive experiments using three publicly available medical datasets with different imaging techniques (i.e., Kvasir (endoscopic images), FETAL_PLANES_DB (ultrasound images) and Covid19-Pneumonia-Normal Chest X-Ray (X-ray images)) and two private datasets built by ourselves. Experimental results show that the proposed MedMamba performs well in detecting lesions in various medical images. To the best of our knowledge, this is the first Vision Mamba tailored for medical image classification. The purpose of this work is to establish a new baseline for medical image classification tasks and provide valuable insights for the future development of more efficient and effective SSM-based artificial intelligence algorithms and application systems in the medical. Source code has been available at https://github.com/YubiaoYue/MedMamba.","sentences":["Medical image classification is a very fundamental and crucial task in the field of computer vision.","These years, CNN-based and Transformer-based models are widely used in classifying various medical images.","Unfortunately, The limitation of CNNs in long-range modeling capabilities prevent them from effectively extracting fine-grained features in medical images , while Transformers are hampered by their quadratic computational complexity.","Recent research has shown that the state space model (SSM) represented by Mamba can efficiently model long-range interactions while maintaining linear computational complexity.","Inspired by this, we propose Vision Mamba for medical image classification (MedMamba).","More specifically, we introduce a novel Conv-SSM module, which combines the local feature extraction ability of convolutional layers with the ability of SSM to capture long-range dependency.","To demonstrate the potential of MedMamba, we conduct extensive experiments using three publicly available medical datasets with different imaging techniques (i.e., Kvasir (endoscopic images), FETAL_PLANES_DB (ultrasound images) and Covid19-Pneumonia-Normal Chest X-Ray (X-ray images)) and two private datasets built by ourselves.","Experimental results show that the proposed MedMamba performs well in detecting lesions in various medical images.","To the best of our knowledge, this is the first Vision Mamba tailored for medical image classification.","The purpose of this work is to establish a new baseline for medical image classification tasks and provide valuable insights for the future development of more efficient and effective SSM-based artificial intelligence algorithms and application systems in the medical.","Source code has been available at https://github.com/YubiaoYue/MedMamba."],"url":"http://arxiv.org/abs/2403.03849v1","category":"eess.IV"}
{"created":"2024-03-06 16:49:08","title":"Dexterous Legged Locomotion in Confined 3D Spaces with Reinforcement Learning","abstract":"Recent advances of locomotion controllers utilizing deep reinforcement learning (RL) have yielded impressive results in terms of achieving rapid and robust locomotion across challenging terrain, such as rugged rocks, non-rigid ground, and slippery surfaces. However, while these controllers primarily address challenges underneath the robot, relatively little research has investigated legged mobility through confined 3D spaces, such as narrow tunnels or irregular voids, which impose all-around constraints. The cyclic gait patterns resulted from existing RL-based methods to learn parameterized locomotion skills characterized by motion parameters, such as velocity and body height, may not be adequate to navigate robots through challenging confined 3D spaces, requiring both agile 3D obstacle avoidance and robust legged locomotion. Instead, we propose to learn locomotion skills end-to-end from goal-oriented navigation in confined 3D spaces. To address the inefficiency of tracking distant navigation goals, we introduce a hierarchical locomotion controller that combines a classical planner tasked with planning waypoints to reach a faraway global goal location, and an RL-based policy trained to follow these waypoints by generating low-level motion commands. This approach allows the policy to explore its own locomotion skills within the entire solution space and facilitates smooth transitions between local goals, enabling long-term navigation towards distant goals. In simulation, our hierarchical approach succeeds at navigating through demanding confined 3D environments, outperforming both pure end-to-end learning approaches and parameterized locomotion skills. We further demonstrate the successful real-world deployment of our simulation-trained controller on a real robot.","sentences":["Recent advances of locomotion controllers utilizing deep reinforcement learning (RL) have yielded impressive results in terms of achieving rapid and robust locomotion across challenging terrain, such as rugged rocks, non-rigid ground, and slippery surfaces.","However, while these controllers primarily address challenges underneath the robot, relatively little research has investigated legged mobility through confined 3D spaces, such as narrow tunnels or irregular voids, which impose all-around constraints.","The cyclic gait patterns resulted from existing RL-based methods to learn parameterized locomotion skills characterized by motion parameters, such as velocity and body height, may not be adequate to navigate robots through challenging confined 3D spaces, requiring both agile 3D obstacle avoidance and robust legged locomotion.","Instead, we propose to learn locomotion skills end-to-end from goal-oriented navigation in confined 3D spaces.","To address the inefficiency of tracking distant navigation goals, we introduce a hierarchical locomotion controller that combines a classical planner tasked with planning waypoints to reach a faraway global goal location, and an RL-based policy trained to follow these waypoints by generating low-level motion commands.","This approach allows the policy to explore its own locomotion skills within the entire solution space and facilitates smooth transitions between local goals, enabling long-term navigation towards distant goals.","In simulation, our hierarchical approach succeeds at navigating through demanding confined 3D environments, outperforming both pure end-to-end learning approaches and parameterized locomotion skills.","We further demonstrate the successful real-world deployment of our simulation-trained controller on a real robot."],"url":"http://arxiv.org/abs/2403.03848v1","category":"cs.RO"}
{"created":"2024-03-06 16:41:26","title":"Electromagnetic inverse wave scattering in anisotropic media via reduced order modeling","abstract":"The inverse wave scattering problem seeks to estimate a heterogeneous, inaccessible medium, modeled by unknown variable coefficients in wave equations, from transient recordings of waves generated by probing signals. It is a widely studied inverse problem with important applications, that is typically formulated as a nonlinear least squares data fit optimization. For typical measurement setups and band-limited probing signals, the least squares objective function has spurious local minima far and near the true solution, so Newton-type optimization methods fail. We introduce a different approach, for electromagnetic inverse wave scattering in lossless, anisotropic media. Our reduced order model (ROM) is an algebraic, discrete time dynamical system derived from Maxwell's equations with four important properties: (1) It is data driven, without knowledge of the medium. (2) The data to ROM mapping is nonlinear and yet the ROM can be obtained in a non-iterative fashion. (3) It has a special algebraic structure that captures the causal Wave propagation. (4) The ROM interpolates the data on a uniform time grid. We show how to obtain from the ROM an estimate of the wave field at inaccessible points inside the unknown medium. The use of this wave is twofold: First, it defines a computationally inexpensive imaging function designed to estimate the support of reflective structures in the medium, modeled by jump discontinuities of the matrix valued dielectric permittivity. Second, it gives an objective function for quantitative estimation of the dielectric permittivity, that has better behavior than the least squares data fitting objective function. The methodology introduced in this paper applies to Maxwell's equations in three dimensions. To avoid high computational costs, we limit the study to a cylindrical domain filled with an orthotropic medium, so the problem becomes two dimensional.","sentences":["The inverse wave scattering problem seeks to estimate a heterogeneous, inaccessible medium, modeled by unknown variable coefficients in wave equations, from transient recordings of waves generated by probing signals.","It is a widely studied inverse problem with important applications, that is typically formulated as a nonlinear least squares data fit optimization.","For typical measurement setups and band-limited probing signals, the least squares objective function has spurious local minima far and near the true solution, so Newton-type optimization methods fail.","We introduce a different approach, for electromagnetic inverse wave scattering in lossless, anisotropic media.","Our reduced order model (ROM) is an algebraic, discrete time dynamical system derived from Maxwell's equations with four important properties: (1) It is data driven, without knowledge of the medium.","(2) The data to ROM mapping is nonlinear and yet the ROM can be obtained in a non-iterative fashion.","(3) It has a special algebraic structure that captures the causal Wave propagation.","(4) The ROM interpolates the data on a uniform time grid.","We show how to obtain from the ROM an estimate of the wave field at inaccessible points inside the unknown medium.","The use of this wave is twofold:","First, it defines a computationally inexpensive imaging function designed to estimate the support of reflective structures in the medium, modeled by jump discontinuities of the matrix valued dielectric permittivity.","Second, it gives an objective function for quantitative estimation of the dielectric permittivity, that has better behavior than the least squares data fitting objective function.","The methodology introduced in this paper applies to Maxwell's equations in three dimensions.","To avoid high computational costs, we limit the study to a cylindrical domain filled with an orthotropic medium, so the problem becomes two dimensional."],"url":"http://arxiv.org/abs/2403.03844v1","category":"math.NA"}
{"created":"2024-03-06 16:34:41","title":"Loop Corrections in Bispectrum in USR Inflation with PBHs Formation","abstract":"We calculate the one-loop corrections in bispectrum of CMB scale perturbations induced from the small scale modes undergoing an intermediate phase of USR inflation in scenarios employed for PBHs formation. Using the formalism of effective field theory of inflation we calculate the cubic and quartic Hamiltonians and perform the in-in analysis for a subset of Feynman diagrams comprising both the cubic and the quartic exchange vertices. We show the one-loop corrections in bispectrum has the local shape with $f_{NL}$ having the same structure as the one-loop correction in power spectrum in their dependence on the duration of the USR phase and the sharpness of the transition to the final attractor phase. It is shown that in the models with a sharp transition the induced loop corrections in bispectrum can quickly violate the observational bounds on $f_{NL}$.","sentences":["We calculate the one-loop corrections in bispectrum of CMB scale perturbations induced from the small scale modes undergoing an intermediate phase of USR inflation in scenarios employed for PBHs formation.","Using the formalism of effective field theory of inflation we calculate the cubic and quartic Hamiltonians and perform the in-in analysis for a subset of Feynman diagrams comprising both the cubic and the quartic exchange vertices.","We show the one-loop corrections in bispectrum has the local shape with $f_{NL}$ having the same structure as the one-loop correction in power spectrum in their dependence on the duration of the USR phase and the sharpness of the transition to the final attractor phase.","It is shown that in the models with a sharp transition the induced loop corrections in bispectrum can quickly violate the observational bounds on $f_{NL}$."],"url":"http://arxiv.org/abs/2403.03841v1","category":"astro-ph.CO"}
{"created":"2024-03-06 16:31:56","title":"Feature Selection as Deep Sequential Generative Learning","abstract":"Feature selection aims to identify the most pattern-discriminative feature subset. In prior literature, filter (e.g., backward elimination) and embedded (e.g., Lasso) methods have hyperparameters (e.g., top-K, score thresholding) and tie to specific models, thus, hard to generalize; wrapper methods search a feature subset in a huge discrete space and is computationally costly. To transform the way of feature selection, we regard a selected feature subset as a selection decision token sequence and reformulate feature selection as a deep sequential generative learning task that distills feature knowledge and generates decision sequences. Our method includes three steps: (1) We develop a deep variational transformer model over a joint of sequential reconstruction, variational, and performance evaluator losses. Our model can distill feature selection knowledge and learn a continuous embedding space to map feature selection decision sequences into embedding vectors associated with utility scores. (2) We leverage the trained feature subset utility evaluator as a gradient provider to guide the identification of the optimal feature subset embedding;(3) We decode the optimal feature subset embedding to autoregressively generate the best feature selection decision sequence with autostop. Extensive experimental results show this generative perspective is effective and generic, without large discrete search space and expert-specific hyperparameters.","sentences":["Feature selection aims to identify the most pattern-discriminative feature subset.","In prior literature, filter (e.g., backward elimination) and embedded (e.g., Lasso) methods have hyperparameters (e.g., top-K, score thresholding) and tie to specific models, thus, hard to generalize; wrapper methods search a feature subset in a huge discrete space and is computationally costly.","To transform the way of feature selection, we regard a selected feature subset as a selection decision token sequence and reformulate feature selection as a deep sequential generative learning task that distills feature knowledge and generates decision sequences.","Our method includes three steps: (1) We develop a deep variational transformer model over a joint of sequential reconstruction, variational, and performance evaluator losses.","Our model can distill feature selection knowledge and learn a continuous embedding space to map feature selection decision sequences into embedding vectors associated with utility scores.","(2) We leverage the trained feature subset utility evaluator as a gradient provider to guide the identification of the optimal feature subset embedding;(3)","We decode the optimal feature subset embedding to autoregressively generate the best feature selection decision sequence with autostop.","Extensive experimental results show this generative perspective is effective and generic, without large discrete search space and expert-specific hyperparameters."],"url":"http://arxiv.org/abs/2403.03838v1","category":"cs.LG"}
{"created":"2024-03-06 16:26:40","title":"Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning","abstract":"Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure. Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects. However, a broader evaluation of Cobweb as a model of human categorization remains lacking. The current study addresses this gap. It establishes Cobweb's alignment with classical human category learning effects. It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model. These findings set the stage for future research on Cobweb as a comprehensive model of human category learning.","sentences":["Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure.","Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects.","However, a broader evaluation of Cobweb as a model of human categorization remains lacking.","The current study addresses this gap.","It establishes Cobweb's alignment with classical human category learning effects.","It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model.","These findings set the stage for future research on Cobweb as a comprehensive model of human category learning."],"url":"http://arxiv.org/abs/2403.03835v1","category":"cs.LG"}
{"created":"2024-03-06 16:23:25","title":"Superfluid dark stars","abstract":"We present a superfluid dark star model consisting of relativistic dark bosons with two-body self-interaction. The obtained masses, radii, and tidal deformability depend in a simple way on the boson mass and interaction strength. We report first results on binary mergers: the distinctive amplitude and frequency of the emitted gravitational waves are well within reach of terrestrial interferometers.","sentences":["We present a superfluid dark star model consisting of relativistic dark bosons with two-body self-interaction.","The obtained masses, radii, and tidal deformability depend in a simple way on the boson mass and interaction strength.","We report first results on binary mergers: the distinctive amplitude and frequency of the emitted gravitational waves are well within reach of terrestrial interferometers."],"url":"http://arxiv.org/abs/2403.03833v1","category":"astro-ph.CO"}
{"created":"2024-03-06 16:22:49","title":"Your device may know you better than you know yourself -- continuous authentication on novel dataset using machine learning","abstract":"This research aims to further understanding in the field of continuous authentication using behavioral biometrics. We are contributing a novel dataset that encompasses the gesture data of 15 users playing Minecraft with a Samsung Tablet, each for a duration of 15 minutes. Utilizing this dataset, we employed machine learning (ML) binary classifiers, being Random Forest (RF), K-Nearest Neighbors (KNN), and Support Vector Classifier (SVC), to determine the authenticity of specific user actions. Our most robust model was SVC, which achieved an average accuracy of approximately 90%, demonstrating that touch dynamics can effectively distinguish users. However, further studies are needed to make it viable option for authentication systems","sentences":["This research aims to further understanding in the field of continuous authentication using behavioral biometrics.","We are contributing a novel dataset that encompasses the gesture data of 15 users playing Minecraft with a Samsung Tablet, each for a duration of 15 minutes.","Utilizing this dataset, we employed machine learning (ML) binary classifiers, being Random Forest (RF), K-Nearest Neighbors (KNN), and Support Vector Classifier (SVC), to determine the authenticity of specific user actions.","Our most robust model was SVC, which achieved an average accuracy of approximately 90%, demonstrating that touch dynamics can effectively distinguish users.","However, further studies are needed to make it viable option for authentication systems"],"url":"http://arxiv.org/abs/2403.03832v1","category":"cs.AI"}
{"created":"2024-03-06 16:18:02","title":"From Clicks to Security: Investigating Continuous Authentication via Mouse Dynamics","abstract":"In the realm of computer security, the importance of efficient and reliable user authentication methods has become increasingly critical. This paper examines the potential of mouse movement dynamics as a consistent metric for continuous authentication. By analyzing user mouse movement patterns in two contrasting gaming scenarios, \"Team Fortress\" and Poly Bridge we investigate the distinctive behavioral patterns inherent in high-intensity and low-intensity UI interactions. The study extends beyond conventional methodologies by employing a range of machine learning models. These models are carefully selected to assess their effectiveness in capturing and interpreting the subtleties of user behavior as reflected in their mouse movements. This multifaceted approach allows for a more nuanced and comprehensive understanding of user interaction patterns. Our findings reveal that mouse movement dynamics can serve as a reliable indicator for continuous user authentication. The diverse machine learning models employed in this study demonstrate competent performance in user verification, marking an improvement over previous methods used in this field. This research contributes to the ongoing efforts to enhance computer security and highlights the potential of leveraging user behavior, specifically mouse dynamics, in developing robust authentication systems.","sentences":["In the realm of computer security, the importance of efficient and reliable user authentication methods has become increasingly critical.","This paper examines the potential of mouse movement dynamics as a consistent metric for continuous authentication.","By analyzing user mouse movement patterns in two contrasting gaming scenarios, \"Team Fortress\" and Poly Bridge we investigate the distinctive behavioral patterns inherent in high-intensity and low-intensity UI interactions.","The study extends beyond conventional methodologies by employing a range of machine learning models.","These models are carefully selected to assess their effectiveness in capturing and interpreting the subtleties of user behavior as reflected in their mouse movements.","This multifaceted approach allows for a more nuanced and comprehensive understanding of user interaction patterns.","Our findings reveal that mouse movement dynamics can serve as a reliable indicator for continuous user authentication.","The diverse machine learning models employed in this study demonstrate competent performance in user verification, marking an improvement over previous methods used in this field.","This research contributes to the ongoing efforts to enhance computer security and highlights the potential of leveraging user behavior, specifically mouse dynamics, in developing robust authentication systems."],"url":"http://arxiv.org/abs/2403.03828v1","category":"cs.AI"}
{"created":"2024-03-06 16:17:34","title":"Linear and nonlinear system identification under $\\ell_1$- and group-Lasso regularization via L-BFGS-B","abstract":"In this paper, we propose an approach for identifying linear and nonlinear discrete-time state-space models, possibly under $\\ell_1$- and group-Lasso regularization, based on the L-BFGS-B algorithm. For the identification of linear models, we show that, compared to classical linear subspace methods, the approach often provides better results, is much more general in terms of the loss and regularization terms used, and is also more stable from a numerical point of view. The proposed method not only enriches the existing set of linear system identification tools but can be also applied to identifying a very broad class of parametric nonlinear state-space models, including recurrent neural networks. We illustrate the approach on synthetic and experimental datasets and apply it to solve the challenging industrial robot benchmark for nonlinear multi-input/multi-output system identification proposed by Weigand et al. (2022). A Python implementation of the proposed identification method is available in the package \\texttt{jax-sysid}, available at \\url{https://github.com/bemporad/jax-sysid}.","sentences":["In this paper, we propose an approach for identifying linear and nonlinear discrete-time state-space models, possibly under $\\ell_1$- and group-Lasso regularization, based on the L-BFGS-B algorithm.","For the identification of linear models, we show that, compared to classical linear subspace methods, the approach often provides better results, is much more general in terms of the loss and regularization terms used, and is also more stable from a numerical point of view.","The proposed method not only enriches the existing set of linear system identification tools but can be also applied to identifying a very broad class of parametric nonlinear state-space models, including recurrent neural networks.","We illustrate the approach on synthetic and experimental datasets and apply it to solve the challenging industrial robot benchmark for nonlinear multi-input/multi-output system identification proposed by Weigand et al. (2022).","A Python implementation of the proposed identification method is available in the package \\texttt{jax-sysid}, available at \\url{https://github.com/bemporad/jax-sysid}."],"url":"http://arxiv.org/abs/2403.03827v1","category":"eess.SY"}
{"created":"2024-03-06 16:10:38","title":"A likelihood framework for cryogenic scintillating calorimeters used in the CRESST dark matter search","abstract":"Cryogenic scintillating calorimeters are ultrasensitive particle detectors for rare event searches, particularly for the search for dark matter and the measurement of neutrino properties. These detectors are made from scintillating target crystals generating two signals for each particle interaction. The phonon (heat) signal precisely measures the deposited energy independent of the type of interacting particle. The scintillation light signal yields particle discrimination on an event-by-event basis. This paper presents a likelihood framework modeling backgrounds and a potential dark matter signal in the two-dimensional plane spanned by phonon and scintillation light energies. We apply the framework to data from CaWO$_4$-based detectors operated in the CRESST dark matter search. For the first time, a single likelihood framework is used in CRESST to model the data and extract results on dark matter in one step by using a profile likelihood ratio test. Our framework simultaneously fits (neutron) calibration data and physics (background) data and allows combining data from multiple detectors. Although tailored to CaWO$_4$-targets and the CRESST experiment, the framework can easily be expanded to other materials and experiments using scintillating cryogenic calorimeters for dark matter search and neutrino physics.","sentences":["Cryogenic scintillating calorimeters are ultrasensitive particle detectors for rare event searches, particularly for the search for dark matter and the measurement of neutrino properties.","These detectors are made from scintillating target crystals generating two signals for each particle interaction.","The phonon (heat) signal precisely measures the deposited energy independent of the type of interacting particle.","The scintillation light signal yields particle discrimination on an event-by-event basis.","This paper presents a likelihood framework modeling backgrounds and a potential dark matter signal in the two-dimensional plane spanned by phonon and scintillation light energies.","We apply the framework to data from CaWO$_4$-based detectors operated in the CRESST dark matter search.","For the first time, a single likelihood framework is used in CRESST to model the data and extract results on dark matter in one step by using a profile likelihood ratio test.","Our framework simultaneously fits (neutron) calibration data and physics (background) data and allows combining data from multiple detectors.","Although tailored to CaWO$_4$-targets and the CRESST experiment, the framework can easily be expanded to other materials and experiments using scintillating cryogenic calorimeters for dark matter search and neutrino physics."],"url":"http://arxiv.org/abs/2403.03824v1","category":"astro-ph.CO"}
{"created":"2024-03-06 16:10:01","title":"A Modular Approach for Multimodal Summarization of TV Shows","abstract":"In this paper we address the task of summarizing television shows, which touches key areas in AI research: complex reasoning, multiple modalities, and long narratives. We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods. Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode. We also present a new metric, PREFS (\\textbf{P}recision and \\textbf{R}ecall \\textbf{E}valuation of Summary \\textbf{F}act\\textbf{s}), to measure both precision and recall of generated summaries, which we decompose into atomic facts. Tested on the recently released SummScreen3D dataset Papalampidi and Lapata (2023), our method produces higher quality summaries than comparison models, as measured with ROUGE and our new fact-based metric.","sentences":["In this paper we address the task of summarizing television shows, which touches key areas in AI research: complex reasoning, multiple modalities, and long narratives.","We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods.","Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode.","We also present a new metric, PREFS (\\textbf{P}recision and \\textbf{R}ecall \\textbf{E}valuation of Summary \\textbf{F}act\\textbf{s}), to measure both precision and recall of generated summaries, which we decompose into atomic facts.","Tested on the recently released SummScreen3D dataset Papalampidi and Lapata (2023), our method produces higher quality summaries than comparison models, as measured with ROUGE and our new fact-based metric."],"url":"http://arxiv.org/abs/2403.03823v1","category":"cs.CL"}
{"created":"2024-03-06 16:06:40","title":"Identifying Black Holes Through Space Telescopes and Deep Learning","abstract":"The EHT has captured a series of images of black holes, demonstrating the possibility of detecting them through a radio interferometer. These images could provide valuable information about the gravitational environment near the event horizon. However, accurate detection and parameter estimation for candidate black holes are necessary. This paper explores the potential for identifying black holes in the ultraviolet band using space telescopes. Firstly, a data pipeline is established for generating simulated observations. Next, we present an ensemble neural network model for detecting and estimating the parameters of black holes with different angular sizes. The model can achieve a mean average precision value [0, 0.5] of 0.9176 when reaching the imaging FWHM ($\\theta_c$) of the telescope and maintains its detection ability until $0.33\\theta_c$. These results indicate that our methodology enables super-resolution recognition. The model accurately estimates the size of the accretion disk, the inclination angle, the positional angle, and the temperature. This provides a specific scheme for automatically detecting captured images using neural networks. Moreover, the validity of the model is assessed by analyzing the shadow of M87*, which indicates that the model can discriminate the black hole from background noise and other celestial objects with a confidence level of 0.639. Our work demonstrates the feasibility of detecting black holes in the UV band and provides a new method for the accurate and real-time detection of candidate black holes and further parameter estimation.","sentences":["The EHT has captured a series of images of black holes, demonstrating the possibility of detecting them through a radio interferometer.","These images could provide valuable information about the gravitational environment near the event horizon.","However, accurate detection and parameter estimation for candidate black holes are necessary.","This paper explores the potential for identifying black holes in the ultraviolet band using space telescopes.","Firstly, a data pipeline is established for generating simulated observations.","Next, we present an ensemble neural network model for detecting and estimating the parameters of black holes with different angular sizes.","The model can achieve a mean average precision value [0, 0.5] of 0.9176 when reaching the imaging FWHM ($\\theta_c$) of the telescope and maintains its detection ability until $0.33\\theta_c$. These results indicate that our methodology enables super-resolution recognition.","The model accurately estimates the size of the accretion disk, the inclination angle, the positional angle, and the temperature.","This provides a specific scheme for automatically detecting captured images using neural networks.","Moreover, the validity of the model is assessed by analyzing the shadow of M87*, which indicates that the model can discriminate the black hole from background noise and other celestial objects with a confidence level of 0.639.","Our work demonstrates the feasibility of detecting black holes in the UV band and provides a new method for the accurate and real-time detection of candidate black holes and further parameter estimation."],"url":"http://arxiv.org/abs/2403.03821v1","category":"astro-ph.IM"}
{"created":"2024-03-06 16:06:34","title":"Continuous and deterministic all-photonic cluster state of indistinguishable photons","abstract":"Cluster states are key resources for measurement-based quantum information processing. Photonic cluster and graph states, in particular, play indispensable roles in quantum network and quantum metrology. We demonstrate a semiconductor quantum dot based device in which the confined hole spin acts as a needle in a quantum knitting machine producing continuously and deterministically at sub-Gigahertz repetition rate single indistinguishable photons which are all polarization entangled to each other and to the spin in a one dimensional cluster state. By projecting two nonadjacent photons onto circular polarization bases we disentangle the spin from the photons emitted in between, thus continuously and deterministically preparing all-photonic cluster states for the first time. We use polarization tomography on four sequentially detected photons to demonstrate and to directly quantify the robustness of the cluster's entanglement and the determinism in its photon generation.","sentences":["Cluster states are key resources for measurement-based quantum information processing.","Photonic cluster and graph states, in particular, play indispensable roles in quantum network and quantum metrology.","We demonstrate a semiconductor quantum dot based device in which the confined hole spin acts as a needle in a quantum knitting machine producing continuously and deterministically at sub-Gigahertz repetition rate single indistinguishable photons which are all polarization entangled to each other and to the spin in a one dimensional cluster state.","By projecting two nonadjacent photons onto circular polarization bases we disentangle the spin from the photons emitted in between, thus continuously and deterministically preparing all-photonic cluster states for the first time.","We use polarization tomography on four sequentially detected photons to demonstrate and to directly quantify the robustness of the cluster's entanglement and the determinism in its photon generation."],"url":"http://arxiv.org/abs/2403.03820v1","category":"quant-ph"}
{"created":"2024-03-06 16:01:44","title":"Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ","abstract":"Large language models (LLMs) need to serve everyone, including a global majority of non-English speakers. However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent research shows that, despite limits in their intended use, people prompt LLMs in many different languages. Therefore, in this paper, we investigate the basic multilingual capabilities of state-of-the-art open LLMs beyond their intended use. For this purpose, we introduce MultiQ, a new silver standard benchmark for basic open-ended question answering with 27.4k test questions across a typologically diverse set of 137 languages. With MultiQ, we evaluate language fidelity, i.e.\\ whether models respond in the prompted language, and question answering accuracy. All LLMs we test respond faithfully and/or accurately for at least some languages beyond their intended use. Most models are more accurate when they respond faithfully. However, differences across models are large, and there is a long tail of languages where models are neither accurate nor faithful. We explore differences in tokenization as a potential explanation for our findings, identifying possible correlations that warrant further investigation.","sentences":["Large language models (LLMs) need to serve everyone, including a global majority of non-English speakers.","However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen).","Recent research shows that, despite limits in their intended use, people prompt LLMs in many different languages.","Therefore, in this paper, we investigate the basic multilingual capabilities of state-of-the-art open LLMs beyond their intended use.","For this purpose, we introduce MultiQ, a new silver standard benchmark for basic open-ended question answering with 27.4k test questions across a typologically diverse set of 137 languages.","With MultiQ, we evaluate language fidelity, i.e.\\ whether models respond in the prompted language, and question answering accuracy.","All LLMs we test respond faithfully and/or accurately for at least some languages beyond their intended use.","Most models are more accurate when they respond faithfully.","However, differences across models are large, and there is a long tail of languages where models are neither accurate nor faithful.","We explore differences in tokenization as a potential explanation for our findings, identifying possible correlations that warrant further investigation."],"url":"http://arxiv.org/abs/2403.03814v1","category":"cs.CL"}
{"created":"2024-03-06 16:01:43","title":"Synthesis and Structural Analysis of Multilayered Graphene via Microwave Atmospheric Pressure Plasma","abstract":"This study reports the successful synthesis of multilayered graphene sheets via microwave atmospheric pressure plasma. This innovative approach streamlines and expedites graphene production and other carbon nanostructures, eliminating the need for catalysts, solvents, or complex processing conditions. Ethanol is directly injected into a microwave-generated argon plasma plume, leading to the formation of graphene. Raman spectroscopy revealed characteristic peaks (2D, G, and D bands) confirming graphene composition, with defects indicated by the D band. X-ray diffraction analysis supported these findings, indicating a broad peak at 25 degree corresponding to the (002) plane, affirming a multi-layered graphene structure. Scanning electron microscopy exhibited crumpled, randomly oriented graphene sheets, albeit with uneven structures suggesting impurity incorporation. The presence of defects was quantified through the intensity ratio of the D to G band (ID/IG) in Raman spectroscopy, revealing a value of 0.80, signifying the presence of defects in the synthesized graphene. The 2D to G band intensity ratio (I2D/IG) suggested the existence of 7-10 graphene layers, highlighting the need for further optimization for enhanced graphene quality and purity.","sentences":["This study reports the successful synthesis of multilayered graphene sheets via microwave atmospheric pressure plasma.","This innovative approach streamlines and expedites graphene production and other carbon nanostructures, eliminating the need for catalysts, solvents, or complex processing conditions.","Ethanol is directly injected into a microwave-generated argon plasma plume, leading to the formation of graphene.","Raman spectroscopy revealed characteristic peaks (2D, G, and D bands) confirming graphene composition, with defects indicated by the D band.","X-ray diffraction analysis supported these findings, indicating a broad peak at 25 degree corresponding to the (002) plane, affirming a multi-layered graphene structure.","Scanning electron microscopy exhibited crumpled, randomly oriented graphene sheets, albeit with uneven structures suggesting impurity incorporation.","The presence of defects was quantified through the intensity ratio of the D to G band (ID/IG) in Raman spectroscopy, revealing a value of 0.80, signifying the presence of defects in the synthesized graphene.","The 2D to G band intensity ratio (I2D/IG) suggested the existence of 7-10 graphene layers, highlighting the need for further optimization for enhanced graphene quality and purity."],"url":"http://arxiv.org/abs/2403.03813v1","category":"physics.plasm-ph"}
{"created":"2024-03-06 16:00:50","title":"ProbSAINT: Probabilistic Tabular Regression for Used Car Pricing","abstract":"Used car pricing is a critical aspect of the automotive industry, influenced by many economic factors and market dynamics. With the recent surge in online marketplaces and increased demand for used cars, accurate pricing would benefit both buyers and sellers by ensuring fair transactions. However, the transition towards automated pricing algorithms using machine learning necessitates the comprehension of model uncertainties, specifically the ability to flag predictions that the model is unsure about. Although recent literature proposes the use of boosting algorithms or nearest neighbor-based approaches for swift and precise price predictions, encapsulating model uncertainties with such algorithms presents a complex challenge. We introduce ProbSAINT, a model that offers a principled approach for uncertainty quantification of its price predictions, along with accurate point predictions that are comparable to state-of-the-art boosting techniques. Furthermore, acknowledging that the business prefers pricing used cars based on the number of days the vehicle was listed for sale, we show how ProbSAINT can be used as a dynamic forecasting model for predicting price probabilities for different expected offer duration. Our experiments further indicate that ProbSAINT is especially accurate on instances where it is highly certain. This proves the applicability of its probabilistic predictions in real-world scenarios where trustworthiness is crucial.","sentences":["Used car pricing is a critical aspect of the automotive industry, influenced by many economic factors and market dynamics.","With the recent surge in online marketplaces and increased demand for used cars, accurate pricing would benefit both buyers and sellers by ensuring fair transactions.","However, the transition towards automated pricing algorithms using machine learning necessitates the comprehension of model uncertainties, specifically the ability to flag predictions that the model is unsure about.","Although recent literature proposes the use of boosting algorithms or nearest neighbor-based approaches for swift and precise price predictions, encapsulating model uncertainties with such algorithms presents a complex challenge.","We introduce ProbSAINT, a model that offers a principled approach for uncertainty quantification of its price predictions, along with accurate point predictions that are comparable to state-of-the-art boosting techniques.","Furthermore, acknowledging that the business prefers pricing used cars based on the number of days the vehicle was listed for sale, we show how ProbSAINT can be used as a dynamic forecasting model for predicting price probabilities for different expected offer duration.","Our experiments further indicate that ProbSAINT is especially accurate on instances where it is highly certain.","This proves the applicability of its probabilistic predictions in real-world scenarios where trustworthiness is crucial."],"url":"http://arxiv.org/abs/2403.03812v1","category":"cs.LG"}
{"created":"2024-03-06 15:59:58","title":"Variatonal Beyesian Learning based Joint Localization and Channel Estimation with Distance-dependent Noise","abstract":"In the Industrial Internet of Things (IIoTs) and Ocean of Things (OoTs), the advent of massive intelligent services has imposed stringent requirements on both communication and localization, particularly emphasizing precise localization and channel information. This paper focuses on the challenge of jointly optimizing localization and communication in IoT networks. Departing from the conventional independent noise model used in localization and channel estimation problems, we consider a more realistic model incorporating distance-dependent noise variance, as revealed in recent theoretical analyses and experimental results. The distance-dependent noise introduces unknown noise power and a complex noise model, resulting in an exceptionally challenging non-convex and nonlinear optimization problem. In this study, we address a joint localization and channel estimation problem encompassing distance-dependent noise, unknown channel parameters, and uncertainties in sensor node locations. To surmount the intractable nonlinear and non-convex objective function inherent in the problem, we introduce a variational Bayesian learning-based framework. This framework enables the joint optimization of localization and channel parameters by leveraging an effective approximation to the true posterior distribution. Furthermore, the proposed joint learning algorithm provides an iterative closed-form solution and exhibits superior performance in terms of computational complexity compared to existing algorithms. Computer simulation results demonstrate that the proposed algorithm approaches the performance of the Bayesian Cramer-Rao bound (BCRB), achieves localization performance comparable to the ML-GMP algorithm, and outperforms the other two comparison algorithms.","sentences":["In the Industrial Internet of Things (IIoTs) and Ocean of Things (OoTs), the advent of massive intelligent services has imposed stringent requirements on both communication and localization, particularly emphasizing precise localization and channel information.","This paper focuses on the challenge of jointly optimizing localization and communication in IoT networks.","Departing from the conventional independent noise model used in localization and channel estimation problems, we consider a more realistic model incorporating distance-dependent noise variance, as revealed in recent theoretical analyses and experimental results.","The distance-dependent noise introduces unknown noise power and a complex noise model, resulting in an exceptionally challenging non-convex and nonlinear optimization problem.","In this study, we address a joint localization and channel estimation problem encompassing distance-dependent noise, unknown channel parameters, and uncertainties in sensor node locations.","To surmount the intractable nonlinear and non-convex objective function inherent in the problem, we introduce a variational Bayesian learning-based framework.","This framework enables the joint optimization of localization and channel parameters by leveraging an effective approximation to the true posterior distribution.","Furthermore, the proposed joint learning algorithm provides an iterative closed-form solution and exhibits superior performance in terms of computational complexity compared to existing algorithms.","Computer simulation results demonstrate that the proposed algorithm approaches the performance of the Bayesian Cramer-Rao bound (BCRB), achieves localization performance comparable to the ML-GMP algorithm, and outperforms the other two comparison algorithms."],"url":"http://arxiv.org/abs/2403.03809v1","category":"eess.SP"}
{"created":"2024-03-06 15:59:39","title":"Confidence-Aware Decision-Making and Control for Tool Selection","abstract":"Self-reflecting about our performance (e.g., how confident we are) before doing a task is essential for decision making, such as selecting the most suitable tool or choosing the best route to drive. While this form of awareness -- thinking about our performance or metacognitive performance -- is well-known in humans, robots still lack this cognitive ability. This reflective monitoring can enhance their embodied decision power, robustness and safety. Here, we take a step in this direction by introducing a mathematical framework that allows robots to use their control self-confidence to make better-informed decisions. We derive a mathematical closed-form expression for control confidence for dynamic systems (i.e., the posterior inverse covariance of the control action). This control confidence seamlessly integrates within an objective function for decision making, that balances the: i) performance for task completion, ii) control effort, and iii) self-confidence. To evaluate our theoretical account, we framed the decision-making within the tool selection problem, where the agent has to select the best robot arm for a particular control task. The statistical analysis of the numerical simulations with randomized 2DOF arms shows that using control confidence during tool selection improves both real task performance, and the reliability of the tool for performance under unmodelled perturbations (e.g., external forces). Furthermore, our results indicate that control confidence is an early indicator of performance and thus, it can be used as a heuristic for making decisions when computation power is restricted or decision-making is intractable. Overall, we show the advantages of using confidence-aware decision-making and control scheme for dynamic systems.","sentences":["Self-reflecting about our performance (e.g., how confident we are) before doing a task is essential for decision making, such as selecting the most suitable tool or choosing the best route to drive.","While this form of awareness -- thinking about our performance or metacognitive performance -- is well-known in humans, robots still lack this cognitive ability.","This reflective monitoring can enhance their embodied decision power, robustness and safety.","Here, we take a step in this direction by introducing a mathematical framework that allows robots to use their control self-confidence to make better-informed decisions.","We derive a mathematical closed-form expression for control confidence for dynamic systems (i.e., the posterior inverse covariance of the control action).","This control confidence seamlessly integrates within an objective function for decision making, that balances the: i) performance for task completion, ii) control effort, and iii) self-confidence.","To evaluate our theoretical account, we framed the decision-making within the tool selection problem, where the agent has to select the best robot arm for a particular control task.","The statistical analysis of the numerical simulations with randomized 2DOF arms shows that using control confidence during tool selection improves both real task performance, and the reliability of the tool for performance under unmodelled perturbations (e.g., external forces).","Furthermore, our results indicate that control confidence is an early indicator of performance and thus, it can be used as a heuristic for making decisions when computation power is restricted or decision-making is intractable.","Overall, we show the advantages of using confidence-aware decision-making and control scheme for dynamic systems."],"url":"http://arxiv.org/abs/2403.03808v1","category":"cs.RO"}
{"created":"2024-03-06 15:51:40","title":"Cosmological measurements from the CMB and BAO are insensitive to the tail probability in the assumed likelihood","abstract":"When fitting cosmological models to data, a Bayesian framework is commonly used, requiring assumptions on the form of the likelihood and model prior. In light of current tensions between different data, it is interesting to investigate the robustness of cosmological measurements to statistical assumptions about the likelihood distribution from which the data was drawn. We consider the impact of changes to the likelihood caused by uncertainties due to the finite number of mock catalogs used to estimate the covariance matrix, leading to the replacement of the standard Gaussian likelihood with a multivariate $t$-distribution. These changes to the likelihood have a negligible impact on recent cosmic microwave background (CMB) lensing and baryon acoustic oscillation (BAO) measurements. We then extend our analysis to perform a sensitivity test on the Gaussian likelihoods typically adopted, considering how increasing the size of the tails of the likelihood (again using a $t$-distribution) affects cosmological inferences. For an open $\\Lambda$CDM model constrained by BAO alone, we find that increasing the weight in the tails shifts and broadens the resulting posterior on the parameters, with a $\\sim$0.2-0.4$\\sigma$ effect on $\\Omega_\\Lambda$ and $\\Omega_{\\rm k}$. In contrast, the CMB temperature and polarization constraints in $\\Lambda$CDM showed less than 0.03$\\sigma$ changes in the parameters, except for $\\{\\tau$, ln($10^{10}A_{\\rm s})$, $\\sigma_8$, $S_8$, $\\sigma_8\\Omega_{\\rm m}^{0.25}$, $z_{\\rm re}$, $10^9A_{\\rm s}e^{-2\\tau}\\}$ which shifted by around 0.1-0.2$\\sigma$. If we use solely $\\ell < 30$ data, the amplitude $A_{\\rm s} e^{-2\\tau}$ varies in the posterior mean by 0.7$\\sigma$ and the error bars increase by 6%. We conclude, at least for current-generation CMB and BAO measurements, that uncertainties in the shape and tails of the likelihood do not contribute to current tensions.","sentences":["When fitting cosmological models to data, a Bayesian framework is commonly used, requiring assumptions on the form of the likelihood and model prior.","In light of current tensions between different data, it is interesting to investigate the robustness of cosmological measurements to statistical assumptions about the likelihood distribution from which the data was drawn.","We consider the impact of changes to the likelihood caused by uncertainties due to the finite number of mock catalogs used to estimate the covariance matrix, leading to the replacement of the standard Gaussian likelihood with a multivariate $t$-distribution.","These changes to the likelihood have a negligible impact on recent cosmic microwave background (CMB) lensing and baryon acoustic oscillation (BAO) measurements.","We then extend our analysis to perform a sensitivity test on the Gaussian likelihoods typically adopted, considering how increasing the size of the tails of the likelihood (again using a $t$-distribution) affects cosmological inferences.","For an open $\\Lambda$CDM model constrained by BAO alone, we find that increasing the weight in the tails shifts and broadens the resulting posterior on the parameters, with a $\\sim$0.2-0.4$\\sigma$ effect on $\\Omega_\\Lambda$ and $\\Omega_{\\rm k}$. In contrast, the CMB temperature and polarization constraints in $\\Lambda$CDM showed less than 0.03$\\sigma$ changes in the parameters, except for $\\{\\tau$, ln($10^{10}A_{\\rm s})$, $\\sigma_8$, $S_8$, $\\sigma_8\\Omega_{\\rm m}^{0.25}$, $z_{\\rm re}$, $10^9A_{\\rm s}e^{-2\\tau}\\}$ which shifted by around 0.1-0.2$\\sigma$. If we use solely $\\ell < 30$ data, the amplitude $A_{\\rm s} e^{-2\\tau}$ varies in the posterior mean by 0.7$\\sigma$ and the error bars increase by 6%.","We conclude, at least for current-generation CMB and BAO measurements, that uncertainties in the shape and tails of the likelihood do not contribute to current tensions."],"url":"http://arxiv.org/abs/2403.03799v1","category":"astro-ph.CO"}
{"created":"2024-03-06 15:40:30","title":"Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks","abstract":"We introduce a new family of prompt injection attacks, termed Neural Exec. Unlike known attacks that rely on handcrafted strings (e.g., \"Ignore previous instructions and...\"), we show that it is possible to conceptualize the creation of execution triggers as a differentiable search problem and use learning-based methods to autonomously generate them.   Our results demonstrate that a motivated adversary can forge triggers that are not only drastically more effective than current handcrafted ones but also exhibit inherent flexibility in shape, properties, and functionality. In this direction, we show that an attacker can design and generate Neural Execs capable of persisting through multi-stage preprocessing pipelines, such as in the case of Retrieval-Augmented Generation (RAG)-based applications. More critically, our findings show that attackers can produce triggers that deviate markedly in form and shape from any known attack, sidestepping existing blacklist-based detection and sanitation approaches.","sentences":["We introduce a new family of prompt injection attacks, termed Neural Exec.","Unlike known attacks that rely on handcrafted strings (e.g., \"Ignore previous instructions and...\"), we show that it is possible to conceptualize the creation of execution triggers as a differentiable search problem and use learning-based methods to autonomously generate them.   ","Our results demonstrate that a motivated adversary can forge triggers that are not only drastically more effective than current handcrafted ones but also exhibit inherent flexibility in shape, properties, and functionality.","In this direction, we show that an attacker can design and generate Neural Execs capable of persisting through multi-stage preprocessing pipelines, such as in the case of Retrieval-Augmented Generation (RAG)-based applications.","More critically, our findings show that attackers can produce triggers that deviate markedly in form and shape from any known attack, sidestepping existing blacklist-based detection and sanitation approaches."],"url":"http://arxiv.org/abs/2403.03792v1","category":"cs.CR"}
{"created":"2024-03-06 15:37:22","title":"KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs","abstract":"Treatment effect estimation (TEE) is the task of determining the impact of various treatments on patient outcomes. Current TEE methods fall short due to reliance on limited labeled data and challenges posed by sparse and high-dimensional observational patient data. To address the challenges, we introduce a novel pre-training and fine-tuning framework, KG-TREAT, which synergizes large-scale observational patient data with biomedical knowledge graphs (KGs) to enhance TEE. Unlike previous approaches, KG-TREAT constructs dual-focus KGs and integrates a deep bi-level attention synergy method for in-depth information fusion, enabling distinct encoding of treatment-covariate and outcome-covariate relationships. KG-TREAT also incorporates two pre-training tasks to ensure a thorough grounding and contextualization of patient data and KGs. Evaluation on four downstream TEE tasks shows KG-TREAT's superiority over existing methods, with an average improvement of 7% in Area under the ROC Curve (AUC) and 9% in Influence Function-based Precision of Estimating Heterogeneous Effects (IF-PEHE). The effectiveness of our estimated treatment effects is further affirmed by alignment with established randomized clinical trial findings.","sentences":["Treatment effect estimation (TEE) is the task of determining the impact of various treatments on patient outcomes.","Current TEE methods fall short due to reliance on limited labeled data and challenges posed by sparse and high-dimensional observational patient data.","To address the challenges, we introduce a novel pre-training and fine-tuning framework, KG-TREAT, which synergizes large-scale observational patient data with biomedical knowledge graphs (KGs) to enhance TEE.","Unlike previous approaches, KG-TREAT constructs dual-focus KGs and integrates a deep bi-level attention synergy method for in-depth information fusion, enabling distinct encoding of treatment-covariate and outcome-covariate relationships.","KG-TREAT also incorporates two pre-training tasks to ensure a thorough grounding and contextualization of patient data and KGs.","Evaluation on four downstream TEE tasks shows KG-TREAT's superiority over existing methods, with an average improvement of 7% in Area under the ROC Curve (AUC) and 9% in Influence Function-based Precision of Estimating Heterogeneous Effects (IF-PEHE).","The effectiveness of our estimated treatment effects is further affirmed by alignment with established randomized clinical trial findings."],"url":"http://arxiv.org/abs/2403.03791v1","category":"cs.LG"}
{"created":"2024-03-06 15:35:53","title":"Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery","abstract":"Ship detection needs to identify ship locations from remote sensing (RS) scenes. However, due to different imaging payloads, various appearances of ships, and complicated background interference from the bird's eye view, it is difficult to set up a unified paradigm for achieving multi-source ship detection. Therefore, in this article, considering that the large language models (LLMs) emerge the powerful generalization ability, a novel unified visual-language model called Popeye is proposed for multi-source ship detection from RS imagery. First, to bridge the interpretation gap between multi-source images for ship detection, a novel image-instruction-answer way is designed to integrate the various ship detection ways (e.g., horizontal bounding box (HBB), oriented bounding box (OBB)) into a unified labeling paradigm. Then, in view of this, a cross-modal image interpretation method is developed for the proposed Popeye to enhance interactive comprehension ability between visual and language content, which can be easily migrated into any multi-source ship detection task. Subsequently, owing to objective domain differences, a knowledge adaption mechanism is designed to adapt the pre-trained visual-language knowledge from the nature scene into the RS domain for multi-source ship detection. In addition, the segment anything model (SAM) is also seamlessly integrated into the proposed Popeye to achieve pixel-level ship segmentation without additional training costs. Finally, extensive experiments are conducted on the newly constructed instruction dataset named MMShip, and the results indicate that the proposed Popeye outperforms current specialist, open-vocabulary, and other visual-language models for zero-shot multi-source ship detection.","sentences":["Ship detection needs to identify ship locations from remote sensing (RS) scenes.","However, due to different imaging payloads, various appearances of ships, and complicated background interference from the bird's eye view, it is difficult to set up a unified paradigm for achieving multi-source ship detection.","Therefore, in this article, considering that the large language models (LLMs) emerge the powerful generalization ability, a novel unified visual-language model called Popeye is proposed for multi-source ship detection from RS imagery.","First, to bridge the interpretation gap between multi-source images for ship detection, a novel image-instruction-answer way is designed to integrate the various ship detection ways (e.g., horizontal bounding box (HBB), oriented bounding box (OBB)) into a unified labeling paradigm.","Then, in view of this, a cross-modal image interpretation method is developed for the proposed Popeye to enhance interactive comprehension ability between visual and language content, which can be easily migrated into any multi-source ship detection task.","Subsequently, owing to objective domain differences, a knowledge adaption mechanism is designed to adapt the pre-trained visual-language knowledge from the nature scene into the RS domain for multi-source ship detection.","In addition, the segment anything model (SAM) is also seamlessly integrated into the proposed Popeye to achieve pixel-level ship segmentation without additional training costs.","Finally, extensive experiments are conducted on the newly constructed instruction dataset named MMShip, and the results indicate that the proposed Popeye outperforms current specialist, open-vocabulary, and other visual-language models for zero-shot multi-source ship detection."],"url":"http://arxiv.org/abs/2403.03790v1","category":"cs.CV"}
{"created":"2024-03-06 15:34:23","title":"Recovering orthogonality from quasi-nature of Spectral transformations","abstract":"In this contribution, quasi-orthogonality of polynomials generated by Geronimus and Uvarov transformations is analyzed. An attempt is made to discuss the recovery of the source orthogonal polynomial from the quasi-Geronimus and quasi-Uvarov polynomials of order one. Moreover, the discussion on the difference equation satisfied by quasi-Geronimus and quasi-Uvarov polynomials is presented. Furthermore, the orthogonality of quasi-Geronimus and quasi-Uvarov polynomials is achieved through the reduction of the degree of coefficients in the difference equation. During this procedure, alternative representations of the parameters responsible for achieving orthogonality are derived. One of these representations involves the Stieltjes transform of the measure. Finally, the recurrence coefficients ensuring the existence of a measure that makes the quasi-Geronimus Laguerre polynomial of order one an orthogonal polynomial are calculated.","sentences":["In this contribution, quasi-orthogonality of polynomials generated by Geronimus and Uvarov transformations is analyzed.","An attempt is made to discuss the recovery of the source orthogonal polynomial from the quasi-Geronimus and quasi-Uvarov polynomials of order one.","Moreover, the discussion on the difference equation satisfied by quasi-Geronimus and quasi-Uvarov polynomials is presented.","Furthermore, the orthogonality of quasi-Geronimus and quasi-Uvarov polynomials is achieved through the reduction of the degree of coefficients in the difference equation.","During this procedure, alternative representations of the parameters responsible for achieving orthogonality are derived.","One of these representations involves the Stieltjes transform of the measure.","Finally, the recurrence coefficients ensuring the existence of a measure that makes the quasi-Geronimus Laguerre polynomial of order one an orthogonal polynomial are calculated."],"url":"http://arxiv.org/abs/2403.03789v1","category":"math.CA"}
{"created":"2024-03-06 15:33:03","title":"A simple prediction of the non-linear matter power spectrum in Brans-Dicke gravity from linear theory","abstract":"Brans-Dicke (BD) was one of the first proposed scalar-tensor theories of gravity, and effectively turns the gravitational constant of General Relativity (GR) time-dependent. Constraints on the BD parameter $\\omega$ serve as a benchmark for testing GR, which is recovered in the limit $\\omega \\rightarrow \\infty$. Current small-scale astrophysical constraints $\\omega \\gtrsim 10^5$ are much tighter than large-scale cosmological constraints $\\omega \\gtrsim 10^3$, but these decouple if the true theory of gravity features screening. On the largest cosmological scales BD approximates the most general second order scalar-tensor (Horndeski) theory, so constraints here have wider implications. These will improve with upcoming large-scale structure and CMB surveys. To constrain BD with weak gravitational lensing, one needs its non-linear matter power spectrum $P_{\\rm BD}$. By comparing the boost $B = P_{\\rm BD}/P_{\\rm GR}$ from linear theory and non-linear $N$-body simulations, we show that the non-linear boost can simply be predicted from linear theory if the ${\\rm BD}$ and ${\\rm GR}$ universes are parametrized in a way that makes their early cosmological evolution and quasi-linear power today similar. In particular, they need the same $H_0 / \\sqrt{\\smash[b]{G_{\\rm eff}(a=0)}}$ and $\\sigma_8$, where $G_{\\rm eff}$ are their (effective) gravitational strengths. Our prediction is $1\\%$ accurate for $\\omega \\geq 100$, $z \\leq 3$ and $k \\leq 1\\,h/\\rm{Mpc}$, and $2\\%$ further up to $k \\leq 5\\,h/\\rm{Mpc}$. It also holds for $G_{\\rm BD}$ that do not match Newton's constant today, so one can study GR with different gravitational constants $G_{\\rm GR}$ by sending $\\omega \\rightarrow \\infty$. We provide a code that computes $B$ with the linear Einstein-Boltzmann solver hi_class and multiplies it by the non-linear $P_{\\rm GR}$ from EuclidEmulator2 to predict $P_{\\rm BD}$.","sentences":["Brans-Dicke (BD) was one of the first proposed scalar-tensor theories of gravity, and effectively turns the gravitational constant of General Relativity (GR) time-dependent.","Constraints on the BD parameter $\\omega$ serve as a benchmark for testing GR, which is recovered in the limit $\\omega \\rightarrow \\infty$. Current small-scale astrophysical constraints $\\omega \\gtrsim 10^5$ are much tighter than large-scale cosmological constraints $\\omega \\gtrsim 10^3$, but these decouple if the true theory of gravity features screening.","On the largest cosmological scales BD approximates the most general second order scalar-tensor (Horndeski) theory, so constraints here have wider implications.","These will improve with upcoming large-scale structure and CMB surveys.","To constrain BD with weak gravitational lensing, one needs its non-linear matter power spectrum $P_{\\rm BD}$. By comparing the boost $B = P_{\\rm BD}/P_{\\rm GR}$ from linear theory and non-linear","$N$-body simulations, we show that the non-linear boost can simply be predicted from linear theory if the ${\\rm BD}$ and ${\\rm GR}$ universes are parametrized in a way that makes their early cosmological evolution and quasi-linear power today similar.","In particular, they need the same $H_0 / \\sqrt{\\smash[b]{G_{\\rm eff}(a=0)}}$ and $\\sigma_8$, where $G_{\\rm eff}$ are their (effective) gravitational strengths.","Our prediction is $1\\%$ accurate for $\\omega \\geq 100$, $z \\leq 3$ and $k \\leq 1\\,h/\\rm{Mpc}$, and $2\\%$ further up to $k \\leq 5\\,h/\\rm{Mpc}$. It also holds for $G_{\\rm BD}$ that do not match Newton's constant today, so one can study GR with different gravitational constants $G_{\\rm GR}$ by sending $\\omega \\rightarrow \\infty$. We provide a code that computes $B$ with the linear Einstein-Boltzmann solver hi_class and multiplies it by the non-linear $P_{\\rm GR}$ from EuclidEmulator2 to predict $P_{\\rm BD}$."],"url":"http://arxiv.org/abs/2403.03786v1","category":"astro-ph.CO"}
{"created":"2024-03-06 15:23:26","title":"Neural Architecture Search using Particle Swarm and Ant Colony Optimization","abstract":"Neural network models have a number of hyperparameters that must be chosen along with their architecture. This can be a heavy burden on a novice user, choosing which architecture and what values to assign to parameters. In most cases, default hyperparameters and architectures are used. Significant improvements to model accuracy can be achieved through the evaluation of multiple architectures. A process known as Neural Architecture Search (NAS) may be applied to automatically evaluate a large number of such architectures. A system integrating open source tools for Neural Architecture Search (OpenNAS), in the classification of images, has been developed as part of this research. OpenNAS takes any dataset of grayscale, or RBG images, and generates Convolutional Neural Network (CNN) architectures based on a range of metaheuristics using either an AutoKeras, a transfer learning or a Swarm Intelligence (SI) approach. Particle Swarm Optimization (PSO) and Ant Colony Optimization (ACO) are used as the SI algorithms. Furthermore, models developed through such metaheuristics may be combined using stacking ensembles. In the context of this paper, we focus on training and optimizing CNNs using the Swarm Intelligence (SI) components of OpenNAS. Two major types of SI algorithms, namely PSO and ACO, are compared to see which is more effective in generating higher model accuracies. It is shown, with our experimental design, that the PSO algorithm performs better than ACO. The performance improvement of PSO is most notable with a more complex dataset. As a baseline, the performance of fine-tuned pre-trained models is also evaluated.","sentences":["Neural network models have a number of hyperparameters that must be chosen along with their architecture.","This can be a heavy burden on a novice user, choosing which architecture and what values to assign to parameters.","In most cases, default hyperparameters and architectures are used.","Significant improvements to model accuracy can be achieved through the evaluation of multiple architectures.","A process known as Neural Architecture Search (NAS) may be applied to automatically evaluate a large number of such architectures.","A system integrating open source tools for Neural Architecture Search (OpenNAS), in the classification of images, has been developed as part of this research.","OpenNAS takes any dataset of grayscale, or RBG images, and generates Convolutional Neural Network (CNN) architectures based on a range of metaheuristics using either an AutoKeras, a transfer learning or a Swarm Intelligence (SI) approach.","Particle Swarm Optimization (PSO) and Ant Colony Optimization (ACO) are used as the SI algorithms.","Furthermore, models developed through such metaheuristics may be combined using stacking ensembles.","In the context of this paper, we focus on training and optimizing CNNs using the Swarm Intelligence (SI) components of OpenNAS.","Two major types of SI algorithms, namely PSO and ACO, are compared to see which is more effective in generating higher model accuracies.","It is shown, with our experimental design, that the PSO algorithm performs better than ACO.","The performance improvement of PSO is most notable with a more complex dataset.","As a baseline, the performance of fine-tuned pre-trained models is also evaluated."],"url":"http://arxiv.org/abs/2403.03781v1","category":"cs.NE"}
{"created":"2024-03-06 15:15:42","title":"ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport","abstract":"We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our method, called Expectile-Regularised Neural Optimal Transport (ENOT). ENOT outperforms previous state-of-the-art approaches on the Wasserstein-2 benchmark tasks by a large margin (up to a 3-fold improvement in quality and up to a 10-fold improvement in runtime).","sentences":["We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials.","The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction.","We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials.","Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning.","We formally justify the efficiency of our method, called Expectile-Regularised Neural Optimal Transport (ENOT).","ENOT outperforms previous state-of-the-art approaches on the Wasserstein-2 benchmark tasks by a large margin (up to a 3-fold improvement in quality and up to a 10-fold improvement in runtime)."],"url":"http://arxiv.org/abs/2403.03777v1","category":"cs.LG"}
{"created":"2024-03-06 15:08:21","title":"Photonic-electronic spiking neuron with multi-modal and multi-wavelength excitatory and inhibitory operation for high-speed neuromorphic sensing and computing","abstract":"We report a multi-modal spiking neuron that allows optical and electronic input and control, and wavelength-multiplexing operation, for use in novel high-speed neuromorphic sensing and computing functionalities. The photonic-electronic neuron is built with a micro-scale, nanostructure resonant tunnelling diode (RTD) with photodetection (PD) capability. Leveraging the advantageous intrinsic properties of this RTD-PD system, namely highly nonlinear characteristics, photo-sensitivity, light-induced I-V curve shift, and the ability to deliver excitable responses under electrical and optical inputs, we successfully achieve flexible neuromorphic spike activation and inhibition regimes through photonic-electrical control. We also demonstrate the ability of this RTD-PD spiking sensing-processing neuron to operate under the simultaneous arrival of multiple wavelength-multiplexed optical signals, due to its large photodetection spectral window (covering the 1310 and 1550 nm telecom wavelength bands). Our results highlight the potential of RTD photonic-electronic neurons to reproduce multiple key excitatory and inhibitory spiking regimes, at high speed (ns-rate spiking responses, with faster sub-ns regimes theoretically predicted) and low energy (requiring only ~10 mV and ~150 microW, electrical and optical input amplitudes, respectively), similar in nature to those commonly found in the biological neurons of the visual system and the brain. This work offers a highly promising approach for the realisation of high-speed, energy-efficient photonic-electronic spiking neurons and spiking neural networks, enabling multi-modal and multi-wavelength operation for sensing and information processing tasks. This work therefore paves the way for innovative high-speed, photonic-electronic, and spike-based neuromorphic sensing and computing systems and artificial intelligence hardware.","sentences":["We report a multi-modal spiking neuron that allows optical and electronic input and control, and wavelength-multiplexing operation, for use in novel high-speed neuromorphic sensing and computing functionalities.","The photonic-electronic neuron is built with a micro-scale, nanostructure resonant tunnelling diode (RTD) with photodetection (PD) capability.","Leveraging the advantageous intrinsic properties of this RTD-PD system, namely highly nonlinear characteristics, photo-sensitivity, light-induced I-V curve shift, and the ability to deliver excitable responses under electrical and optical inputs, we successfully achieve flexible neuromorphic spike activation and inhibition regimes through photonic-electrical control.","We also demonstrate the ability of this RTD-PD spiking sensing-processing neuron to operate under the simultaneous arrival of multiple wavelength-multiplexed optical signals, due to its large photodetection spectral window (covering the 1310 and 1550 nm telecom wavelength bands).","Our results highlight the potential of RTD photonic-electronic neurons to reproduce multiple key excitatory and inhibitory spiking regimes, at high speed (ns-rate spiking responses, with faster sub-ns regimes theoretically predicted) and low energy (requiring only ~10 mV and ~150 microW, electrical and optical input amplitudes, respectively), similar in nature to those commonly found in the biological neurons of the visual system and the brain.","This work offers a highly promising approach for the realisation of high-speed, energy-efficient photonic-electronic spiking neurons and spiking neural networks, enabling multi-modal and multi-wavelength operation for sensing and information processing tasks.","This work therefore paves the way for innovative high-speed, photonic-electronic, and spike-based neuromorphic sensing and computing systems and artificial intelligence hardware."],"url":"http://arxiv.org/abs/2403.03775v1","category":"physics.optics"}
{"created":"2024-03-06 15:06:30","title":"Multimessenger signals from compact axion star mergers","abstract":"Axion dark matter can form stable, self-gravitating, and coherent configurations known as axion stars, which are rendered unstable above a critical mass by the Chern-Simons coupling to electromagnetism. We study, using numerical relativity, the merger and subsequent decay of compact axion stars. We show that two sub-critical stars can merge, and form a more massive, excited and critical star, which survives for a finite period before rapidly decaying via electromagnetic radiation. We find a rich multimessenger signal, composed of gravitational waves, electromagnetic radiation, and axion radiation. The gravitational wave signal is broken into two parts: a weak and broad signal from the merger, followed by a much stronger signal of almost fixed frequency from the decay. The electromagnetic radiation follows only the gravitational waves from the decay, while the axion signal is continuous throughout the process. We briefly discuss the detectability of such a signal.","sentences":["Axion dark matter can form stable, self-gravitating, and coherent configurations known as axion stars, which are rendered unstable above a critical mass by the Chern-Simons coupling to electromagnetism.","We study, using numerical relativity, the merger and subsequent decay of compact axion stars.","We show that two sub-critical stars can merge, and form a more massive, excited and critical star, which survives for a finite period before rapidly decaying via electromagnetic radiation.","We find a rich multimessenger signal, composed of gravitational waves, electromagnetic radiation, and axion radiation.","The gravitational wave signal is broken into two parts: a weak and broad signal from the merger, followed by a much stronger signal of almost fixed frequency from the decay.","The electromagnetic radiation follows only the gravitational waves from the decay, while the axion signal is continuous throughout the process.","We briefly discuss the detectability of such a signal."],"url":"http://arxiv.org/abs/2403.03774v1","category":"astro-ph.CO"}
{"created":"2024-03-06 15:06:16","title":"Verified Training for Counterfactual Explanation Robustness under Data Shift","abstract":"Counterfactual explanations (CEs) enhance the interpretability of machine learning models by describing what changes to an input are necessary to change its prediction to a desired class. These explanations are commonly used to guide users' actions, e.g., by describing how a user whose loan application was denied can be approved for a loan in the future. Existing approaches generate CEs by focusing on a single, fixed model, and do not provide any formal guarantees on the CEs' future validity. When models are updated periodically to account for data shift, if the generated CEs are not robust to the shifts, users' actions may no longer have the desired impacts on their predictions. This paper introduces VeriTraCER, an approach that jointly trains a classifier and an explainer to explicitly consider the robustness of the generated CEs to small model shifts. VeriTraCER optimizes over a carefully designed loss function that ensures the verifiable robustness of CEs to local model updates, thus providing deterministic guarantees to CE validity. Our empirical evaluation demonstrates that VeriTraCER generates CEs that (1) are verifiably robust to small model updates and (2) display competitive robustness to state-of-the-art approaches in handling empirical model updates including random initialization, leave-one-out, and distribution shifts.","sentences":["Counterfactual explanations (CEs) enhance the interpretability of machine learning models by describing what changes to an input are necessary to change its prediction to a desired class.","These explanations are commonly used to guide users' actions, e.g., by describing how a user whose loan application was denied can be approved for a loan in the future.","Existing approaches generate CEs by focusing on a single, fixed model, and do not provide any formal guarantees on the CEs' future validity.","When models are updated periodically to account for data shift, if the generated CEs are not robust to the shifts, users' actions may no longer have the desired impacts on their predictions.","This paper introduces VeriTraCER, an approach that jointly trains a classifier and an explainer to explicitly consider the robustness of the generated CEs to small model shifts.","VeriTraCER optimizes over a carefully designed loss function that ensures the verifiable robustness of CEs to local model updates, thus providing deterministic guarantees to CE validity.","Our empirical evaluation demonstrates that VeriTraCER generates CEs that (1) are verifiably robust to small model updates and (2) display competitive robustness to state-of-the-art approaches in handling empirical model updates including random initialization, leave-one-out, and distribution shifts."],"url":"http://arxiv.org/abs/2403.03773v1","category":"cs.LG"}
{"created":"2024-03-06 15:04:41","title":"An assessment of $\\mathbf\u03a5$-states above $\\mathbf{B\\bar B}$-threshold using a constituent-quark-model based meson-meson coupled-channels framework","abstract":"The $\\Upsilon(10753)$ state has been recently observed by the Belle and Belle~II collaborations with enough global significance to motivate an assessment of the high-energy spectrum usually predicted by any reasonable \\emph{na\\\"ive} quark model. In the framework of a constituent quark model which satisfactorily describes a wide range of properties of conventional hadrons containing heavy quarks, the quark-antiquark and meson-meson degrees of freedom have been incorporated with the goal of elucidating the influence of open-bottom meson-meson thresholds into the $\\Upsilon$ states whose masses are within the energy range of the $\\Upsilon(10753)$'s mass. It is well known that such effects could be relevant enough as to generate dynamically new states and thus provide a plausible explanation of the nature of the $\\Upsilon(10753)$ state. In particular, we have performed a coupled-channels calculation in which the bare states $\\Upsilon(4S)$, $\\Upsilon(3D)$, $\\Upsilon(5S)$ and $\\Upsilon(4D)$ are considered together with the threshold channels $B\\bar{B}$, $B\\bar{B}^\\ast$, $B^\\ast \\bar{B}^\\ast$, $B_s\\bar{B}_s$, $B_s\\bar{B}_s^\\ast$ and $B_s^\\ast \\bar{B}_s^\\ast$. Among the results we have described, the following conclusions are of particular interest: (i) a richer complex spectrum is gained when thresholds are present and bare bound states are sufficiently non-relativistic; (ii) those poles obtained in the complex energy plane do not have to appear as simple peaks in the relevant cross sections; and (iii) the $\\Upsilon(10750)$ candidate is interpreted as a dressed hadronic resonance whose structure is an equally mixture of a conventional $b\\bar b$ state and $B^\\ast \\bar B^\\ast$ molecule.","sentences":["The $\\Upsilon(10753)$ state has been recently observed by the Belle and Belle~II collaborations with enough global significance to motivate an assessment of the high-energy spectrum usually predicted by any reasonable \\emph{na\\\"ive} quark model.","In the framework of a constituent quark model which satisfactorily describes a wide range of properties of conventional hadrons containing heavy quarks, the quark-antiquark and meson-meson degrees of freedom have been incorporated with the goal of elucidating the influence of open-bottom meson-meson thresholds into the $\\Upsilon$ states whose masses are within the energy range of the $\\Upsilon(10753)$'s mass.","It is well known that such effects could be relevant enough as to generate dynamically new states and thus provide a plausible explanation of the nature of the $\\Upsilon(10753)$ state.","In particular, we have performed a coupled-channels calculation in which the bare states $\\Upsilon(4S)$, $\\Upsilon(3D)$, $\\Upsilon(5S)$ and $\\Upsilon(4D)$ are considered together with the threshold channels $B\\bar{B}$, $B\\bar{B}^\\ast$, $B^\\ast \\bar{B}^\\ast$, $B_s\\bar{B}_s$, $B_s\\bar{B}_s^\\ast$ and $B_s^\\ast \\bar{B}_s^\\ast$. Among the results we have described, the following conclusions are of particular interest: (i) a richer complex spectrum is gained when thresholds are present and bare bound states are sufficiently non-relativistic; (ii) those poles obtained in the complex energy plane do not have to appear as simple peaks in the relevant cross sections; and (iii) the $\\Upsilon(10750)$ candidate is interpreted as a dressed hadronic resonance whose structure is an equally mixture of a conventional $b\\bar b$ state and $B^\\ast \\bar B^\\ast$ molecule."],"url":"http://arxiv.org/abs/2403.03770v1","category":"hep-ph"}
{"created":"2024-03-06 15:03:09","title":"DeepCRE: Revolutionizing Drug R&D with Cutting-Edge Computational Models","abstract":"The field of pharmaceutical development and therapeutic application both face substantial challenges. Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development. Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis. Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\\% in patient-level CRE, and a 5-fold increase in indication-level CRE. Furthermore, DeepCRE has identified six drug candidates that show significantly greater effectiveness than a comparator set of two approved drug in 5/8 colorectal cancer (CRC) organoids. This highlights DeepCRE's ability to identify a collection of drug candidates with superior therapeutic effects, underscoring its potential to revolutionize the field of therapeutic development.","sentences":["The field of pharmaceutical development and therapeutic application both face substantial challenges.","Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails.","One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development.","Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis.","Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development.","DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\\% in patient-level CRE, and a 5-fold increase in indication-level CRE.","Furthermore, DeepCRE has identified six drug candidates that show significantly greater effectiveness than a comparator set of two approved drug in 5/8 colorectal cancer (CRC) organoids.","This highlights DeepCRE's ability to identify a collection of drug candidates with superior therapeutic effects, underscoring its potential to revolutionize the field of therapeutic development."],"url":"http://arxiv.org/abs/2403.03768v1","category":"cs.AI"}
{"created":"2024-03-06 15:03:04","title":"Predicting the Temperature Dependence of Surfactant CMCs Using Graph Neural Networks","abstract":"The critical micelle concentration (CMC) of surfactant molecules is an essential property for surfactant applications in industry. Recently, classical QSPR and Graph Neural Networks (GNNs), a deep learning technique, have been successfully applied to predict the CMC of surfactants at room temperature. However, these models have not yet considered the temperature dependency of the CMC, which is highly relevant for practical applications. We herein develop a GNN model for temperature-dependent CMC prediction of surfactants. We collect about 1400 data points from public sources for all surfactant classes, i.e., ionic, nonionic, and zwitterionic, at multiple temperatures. We test the predictive quality of the model for following scenarios: i) when CMC data for surfactants are present in the training of the model in at least one different temperature, and ii) CMC data for surfactants are not present in the training, i.e., generalizing to unseen surfactants. In both test scenarios, our model exhibits a high predictive performance of R$^2 \\geq $ 0.94 on test data. We also find that the model performance varies by surfactant class. Finally, we evaluate the model for sugar-based surfactants with complex molecular structures, as these represent a more sustainable alternative to synthetic surfactants and are therefore of great interest for future applications in the personal and home care industries.","sentences":["The critical micelle concentration (CMC) of surfactant molecules is an essential property for surfactant applications in industry.","Recently, classical QSPR and Graph Neural Networks (GNNs), a deep learning technique, have been successfully applied to predict the CMC of surfactants at room temperature.","However, these models have not yet considered the temperature dependency of the CMC, which is highly relevant for practical applications.","We herein develop a GNN model for temperature-dependent CMC prediction of surfactants.","We collect about 1400 data points from public sources for all surfactant classes, i.e., ionic, nonionic, and zwitterionic, at multiple temperatures.","We test the predictive quality of the model for following scenarios: i) when CMC data for surfactants are present in the training of the model in at least one different temperature, and ii) CMC data for surfactants are not present in the training, i.e., generalizing to unseen surfactants.","In both test scenarios, our model exhibits a high predictive performance of R$^2 \\geq $ 0.94 on test data.","We also find that the model performance varies by surfactant class.","Finally, we evaluate the model for sugar-based surfactants with complex molecular structures, as these represent a more sustainable alternative to synthetic surfactants and are therefore of great interest for future applications in the personal and home care industries."],"url":"http://arxiv.org/abs/2403.03767v1","category":"physics.chem-ph"}
{"created":"2024-03-06 15:00:00","title":"Nonlinear Landau fan diagram and aperiodic magnetic oscillations in three-dimensional systems","abstract":"Quantum oscillations offer a powerful probe for the geometry and topology of the Fermi surface in metals. Onsager's semiclassical quantization relation governs these periodic oscillations in 1/B, leading to a linear Landau fan diagram. However, higher-order magnetic susceptibility-induced corrections give rise to a generalized Onsager's relation, manifesting in experiments as a nonlinear Landau fan diagram and aperiodic quantum oscillations. Here, we explore the generalized Onsager's relation to three-dimensional (3D) systems to capture the B-induced corrections in the free energy and the Fermi surface. We unravel the manifestation of these corrections in the nonlinear Landau fan diagrams and aperiodic quantum oscillations by deriving the B-dependent oscillation frequency and the generalized Lifshitz-Kosevich equation, respectively. Our theory explains the necessary conditions to observe these fascinating effects and predicts the magnetic field dependence of the cyclotron mass. As a concrete example, we elucidate these effects in a 3D spin-orbit coupled system and extract zero-field magnetic response functions from analytically obtained Landau levels. Our comprehensive study deepens and advances our understanding of aperiodic quantum oscillations.","sentences":["Quantum oscillations offer a powerful probe for the geometry and topology of the Fermi surface in metals.","Onsager's semiclassical quantization relation governs these periodic oscillations in 1/B, leading to a linear Landau fan diagram.","However, higher-order magnetic susceptibility-induced corrections give rise to a generalized Onsager's relation, manifesting in experiments as a nonlinear Landau fan diagram and aperiodic quantum oscillations.","Here, we explore the generalized Onsager's relation to three-dimensional (3D) systems to capture the B-induced corrections in the free energy and the Fermi surface.","We unravel the manifestation of these corrections in the nonlinear Landau fan diagrams and aperiodic quantum oscillations by deriving the B-dependent oscillation frequency and the generalized Lifshitz-Kosevich equation, respectively.","Our theory explains the necessary conditions to observe these fascinating effects and predicts the magnetic field dependence of the cyclotron mass.","As a concrete example, we elucidate these effects in a 3D spin-orbit coupled system and extract zero-field magnetic response functions from analytically obtained Landau levels.","Our comprehensive study deepens and advances our understanding of aperiodic quantum oscillations."],"url":"http://arxiv.org/abs/2403.03765v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-06 14:57:09","title":"Room Impulse Response Estimation using Optimal Transport: Simulation-Informed Inference","abstract":"The ability to accurately estimate room impulse responses (RIRs) is integral to many applications of spatial audio processing. Regrettably, estimating the RIR using ambient signals, such as speech or music, remains a challenging problem due to, e.g., low signal-to-noise ratios, finite sample lengths, and poor spectral excitation. Commonly, in order to improve the conditioning of the estimation problem, priors are placed on the amplitudes of the RIR. Although serving as a regularizer, this type of prior is generally not useful when only approximate knowledge of the delay structure is available, which, for example, is the case when the prior is a simulated RIR from an approximation of the room geometry. In this work, we target the delay structure itself, constructing a prior based on the concept of optimal transport. As illustrated using both simulated and measured data, the resulting method is able to beneficially incorporate information even from simple simulation models, displaying considerable robustness to perturbations in the assumed room dimensions and its temperature.","sentences":["The ability to accurately estimate room impulse responses (RIRs) is integral to many applications of spatial audio processing.","Regrettably, estimating the RIR using ambient signals, such as speech or music, remains a challenging problem due to, e.g., low signal-to-noise ratios, finite sample lengths, and poor spectral excitation.","Commonly, in order to improve the conditioning of the estimation problem, priors are placed on the amplitudes of the RIR.","Although serving as a regularizer, this type of prior is generally not useful when only approximate knowledge of the delay structure is available, which, for example, is the case when the prior is a simulated RIR from an approximation of the room geometry.","In this work, we target the delay structure itself, constructing a prior based on the concept of optimal transport.","As illustrated using both simulated and measured data, the resulting method is able to beneficially incorporate information even from simple simulation models, displaying considerable robustness to perturbations in the assumed room dimensions and its temperature."],"url":"http://arxiv.org/abs/2403.03762v1","category":"eess.SP"}
{"created":"2024-03-06 14:53:24","title":"Parameterized quantum comb and simpler circuits for reversing unknown qubit-unitary operations","abstract":"Quantum comb is an essential tool for characterizing complex quantum protocols in quantum information processing. In this work, we introduce PQComb, a framework leveraging parameterized quantum circuits to explore the capabilities of quantum combs for general quantum process transformation tasks and beyond. By optimizing PQComb for time-reversal simulations of unknown unitary evolutions, we develop a simpler protocol for unknown qubit unitary inversion that reduces the ancilla qubit overhead from 6 to 3 compared to the existing method in [Yoshida, Soeda, Murao, PRL 131, 120602, 2023]. This demonstrates the utility of quantum comb structures and showcases PQComb's potential for solving complex quantum tasks. Our results pave the way for broader PQComb applications in quantum computing and quantum information, emphasizing its versatility for tackling diverse problems in quantum machine learning.","sentences":["Quantum comb is an essential tool for characterizing complex quantum protocols in quantum information processing.","In this work, we introduce PQComb, a framework leveraging parameterized quantum circuits to explore the capabilities of quantum combs for general quantum process transformation tasks and beyond.","By optimizing PQComb for time-reversal simulations of unknown unitary evolutions, we develop a simpler protocol for unknown qubit unitary inversion that reduces the ancilla qubit overhead from 6 to 3 compared to the existing method in [Yoshida, Soeda, Murao, PRL 131, 120602, 2023].","This demonstrates the utility of quantum comb structures and showcases PQComb's potential for solving complex quantum tasks.","Our results pave the way for broader PQComb applications in quantum computing and quantum information, emphasizing its versatility for tackling diverse problems in quantum machine learning."],"url":"http://arxiv.org/abs/2403.03761v1","category":"quant-ph"}
{"created":"2024-03-06 14:52:43","title":"A new pairwise boost quantum number from celestial states","abstract":"Infrared effects in the scattering of particles in gravity and electrodynamics entail an exchange of relativistic angular momentum between pairs of particles and the gauge field. Due to this exchange particles can carry an asymptotically non-vanishing \"pairwise\" boost-like angular momentum proportional to the product of their couplings to the field. At the quantum level this asymptotic angular momentum suggests the existence of a new quantum number carried by multi-particle states. We argue that such quantum number is related to a modification of the action of the generators of Lorentz transformations on multi-particle states. We derive such a modification using a group-theoretic argument based on the little group of the conformal primary basis for asymptotic states. The corresponding representation is an extension of the ordinary multi-particle Fock representation of the Poincar\\'e group. The new multi-particle states belonging to such representation no longer factorize into tensor products of one-particle states. Viewed from a gravitational point of view, our results provide evidence for a universal breakdown of the description of multi-particle sates in terms of Fock space due to infrared back-reaction.","sentences":["Infrared effects in the scattering of particles in gravity and electrodynamics entail an exchange of relativistic angular momentum between pairs of particles and the gauge field.","Due to this exchange particles can carry an asymptotically non-vanishing \"pairwise\" boost-like angular momentum proportional to the product of their couplings to the field.","At the quantum level this asymptotic angular momentum suggests the existence of a new quantum number carried by multi-particle states.","We argue that such quantum number is related to a modification of the action of the generators of Lorentz transformations on multi-particle states.","We derive such a modification using a group-theoretic argument based on the little group of the conformal primary basis for asymptotic states.","The corresponding representation is an extension of the ordinary multi-particle Fock representation of the Poincar\\'e group.","The new multi-particle states belonging to such representation no longer factorize into tensor products of one-particle states.","Viewed from a gravitational point of view, our results provide evidence for a universal breakdown of the description of multi-particle sates in terms of Fock space due to infrared back-reaction."],"url":"http://arxiv.org/abs/2403.03760v1","category":"hep-th"}
{"created":"2024-03-06 14:48:13","title":"Coherent feedback for quantum expander in gravitational wave observatories","abstract":"The observation of gravitational waves from binary neutron star mergers offers insights into properties of extreme nuclear matter. However, their high-frequency signals in the kHz range are often masked by quantum noise of the laser light used. Here, we propose the \"quantum expander with coherent feedback\", a new detector design that features an additional optical cavity in the detector output and an internal squeeze operation. This approach allows to boost the sensitivity at high frequencies, at the same time providing a compact and tunable design for signal extraction. It allows to tailor the sensitivity of the detector to the specific signal frequency range. We demonstrate that our design allows to improve the sensitivity of the high-frequency detector concept NEMO (neutron star extreme matter observatory), increasing the detection rates by around 15%. Our approach promises new level of flexibility in designing the detectors aiming at high-frequency signals.","sentences":["The observation of gravitational waves from binary neutron star mergers offers insights into properties of extreme nuclear matter.","However, their high-frequency signals in the kHz range are often masked by quantum noise of the laser light used.","Here, we propose the \"quantum expander with coherent feedback\", a new detector design that features an additional optical cavity in the detector output and an internal squeeze operation.","This approach allows to boost the sensitivity at high frequencies, at the same time providing a compact and tunable design for signal extraction.","It allows to tailor the sensitivity of the detector to the specific signal frequency range.","We demonstrate that our design allows to improve the sensitivity of the high-frequency detector concept NEMO (neutron star extreme matter observatory), increasing the detection rates by around 15%.","Our approach promises new level of flexibility in designing the detectors aiming at high-frequency signals."],"url":"http://arxiv.org/abs/2403.03758v1","category":"physics.ins-det"}
{"created":"2024-03-06 14:40:02","title":"The solenoidal Virasoro algebra and its simple weight modules","abstract":"Let $A_n=\\mathbb{C}[t_i^{\\pm1},~1\\leq i\\leq n]$ be the algebra of Laurent polynomials in $n$-variables.   Let $\\mu=(\\mu_1,\\ldots,\\mu_n)$ be a generic vector in $\\mathbb{C}^n$ and $\\Gamma_{\\mu}=\\{\\mu\\cdot\\alpha,\\alpha\\in \\mathbb{Z}^n\\}$ where   $\\mu\\cdot\\alpha=\\displaystyle\\sum_{i=1}^n\\mu_i\\alpha_i$ for $\\alpha=(\\alpha_1,\\ldots,\\alpha_n)\\in \\mathbb{Z}^n$. Denote by $d_\\mu$ the vector field:   $$d_\\mu=\\displaystyle\\sum_{i=1}^n\\mu_it_i\\frac{d}{dt_i}.$$ In \\cite{BiFu}, Y. Billig and V. Futorny introduce the solenoidal Lie algebra $\\mathbf{W}(n)_{\\mu}:=A_nd_\\mu$, where the Lie structure is given by the commutators of vector fields.   In the first part of this paper, we study the universal central extension of $\\mathbf{W}(n)_{\\mu}$. We obtain a rank $n$ Virasoro algebra called the solenoidal Virasoro algebra $\\mathbf{Vir}(n)_\\mu$.   In the second part, we recall in the case of $\\mathbf{Vir}(n)_\\mu$, the well know Harich-Chandra modules for generalized Virasoro algebra studied in \\cite{Su,Su1,LuZhao}.   In the third part, we construct irreducible highest and lowest $\\mathbf{Vir}(n)_\\mu$-modules using triangular decomposition given by lexicographic order on $\\mathbb{Z}^{n}$. We prove that these modules are weight modules which have infinite dimensional weight spaces.","sentences":["Let $A_n=\\mathbb{C}[t_i^{\\pm1},~1\\leq i\\leq n]$ be the algebra of Laurent polynomials in $n$-variables.   ","Let $\\mu=(\\mu_1,\\ldots,\\mu_n)$ be a generic vector in $\\mathbb{C}^n$ and $\\Gamma_{\\mu}=\\{\\mu\\cdot\\alpha,\\alpha\\in \\mathbb{Z}^n\\}$ where   $\\mu\\cdot\\alpha=\\displaystyle\\sum_{i=1}^n\\mu_i\\alpha_i$ for $\\alpha=(\\alpha_1,\\ldots,\\alpha_n)\\in \\mathbb{Z}^n$.","Denote by $d_\\mu$ the vector field:   $$d_\\mu=\\displaystyle\\sum_{i=1}^n\\mu_it_i\\frac{d}{dt_i}.$$ In \\cite{BiFu}, Y. Billig and V. Futorny introduce the solenoidal Lie algebra $\\mathbf{W}(n)_{\\mu}:=A_nd_\\mu$, where the Lie structure is given by the commutators of vector fields.   ","In the first part of this paper, we study the universal central extension of $\\mathbf{W}(n)_{\\mu}$. We obtain a rank $n$ Virasoro algebra called the solenoidal Virasoro algebra $\\mathbf{Vir}(n)_\\mu$.   In the second part, we recall in the case of $\\mathbf{Vir}(n)_\\mu$, the well know Harich-Chandra modules for generalized Virasoro algebra studied in \\cite{Su,Su1,LuZhao}.   ","In the third part, we construct irreducible highest and lowest $\\mathbf{Vir}(n)_\\mu$-modules using triangular decomposition given by lexicographic order on $\\mathbb{Z}^{n}$. We prove that these modules are weight modules which have infinite dimensional weight spaces."],"url":"http://arxiv.org/abs/2403.03753v1","category":"math.RT"}
{"created":"2024-03-06 14:37:30","title":"German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset","abstract":"The advent of Large Language Models (LLMs) has led to remarkable progress on a wide range of natural language processing tasks. Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document. Previous research addresses the challenging task of detecting hallucinations in the output (i.e. inconsistency detection) in order to evaluate the faithfulness of the generated summaries. However, these works primarily focus on English and recent multilingual approaches lack German data. This work presents absinth, a manually annotated dataset for hallucination detection in German news summarization and explores the capabilities of novel open-source LLMs on this task in both fine-tuning and in-context learning settings. We open-source and release the absinth dataset to foster further research on hallucination detection in German.","sentences":["The advent of Large Language Models (LLMs) has led to remarkable progress on a wide range of natural language processing tasks.","Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document.","Previous research addresses the challenging task of detecting hallucinations in the output (i.e. inconsistency detection) in order to evaluate the faithfulness of the generated summaries.","However, these works primarily focus on English and recent multilingual approaches lack German data.","This work presents absinth, a manually annotated dataset for hallucination detection in German news summarization and explores the capabilities of novel open-source LLMs on this task in both fine-tuning and in-context learning settings.","We open-source and release the absinth dataset to foster further research on hallucination detection in German."],"url":"http://arxiv.org/abs/2403.03750v1","category":"cs.CL"}
{"created":"2024-03-06 14:37:27","title":"Coulomb Green's function and an addition formula for the Whittaker functions","abstract":"A series of the form $\\sum_{\\ell=0}^{\\infty}c(\\kappa,\\ell)\\,M_{\\kappa,\\ell+1/2}(r_{0})W_{\\kappa,\\ell+1/2}(r)P_{\\ell}(\\cos(\\gamma))$ is evaluated explicitly where $c(\\kappa,\\ell)$ are suitable complex coefficients, $M_{\\kappa,\\mu}$ and $W_{\\kappa,\\mu}$ are the Whittaker functions, $P_{\\ell}$ are the Legendre polynomials, $r_{0}<r$ are radial variables, $\\gamma$ is an angle and $\\kappa$ is a complex parameter. The sum depends, as far as the radial variables and the angle are concerned, on their combinations $r+r_{0}$ and $(r^{2}+r_{0}^{\\,2}-rr_{0}\\cos(\\gamma))^{1/2}$. This addition formula generalizes in some respect Gegenbauer's Addition Theorem and follows rather straightforwardly from some already known results, particularly from Hostler's formula for Coulomb Green's function. In addition, several complementary summation formulas are derived. They suggest that a further extension of this addition formula may be possible.","sentences":["A series of the form $\\sum_{\\ell=0}^{\\infty}c(\\kappa,\\ell)\\,M_{\\kappa,\\ell+1/2}(r_{0})W_{\\kappa,\\ell+1/2}(r)P_{\\ell}(\\cos(\\gamma))$ is evaluated explicitly where $c(\\kappa,\\ell)$ are suitable complex coefficients, $M_{\\kappa,\\mu}$ and $W_{\\kappa,\\mu}$ are the Whittaker functions, $P_{\\ell}$ are the Legendre polynomials, $r_{0}<r$ are radial variables, $\\gamma$ is an angle and $\\kappa$ is a complex parameter.","The sum depends, as far as the radial variables and the angle are concerned, on their combinations $r+r_{0}$ and $(r^{2}+r_{0}^{\\,2}-rr_{0}\\cos(\\gamma))^{1/2}$. This addition formula generalizes in some respect Gegenbauer's Addition Theorem and follows rather straightforwardly from some already known results, particularly from Hostler's formula for Coulomb Green's function.","In addition, several complementary summation formulas are derived.","They suggest that a further extension of this addition formula may be possible."],"url":"http://arxiv.org/abs/2403.03749v1","category":"math-ph"}
{"created":"2024-03-06 14:36:27","title":"Accurate reference spectra of HD in H$_2$/He bath for planetary applications","abstract":"The hydrogen deuteride (HD) molecule is an important deuterium tracer in astrophysical studies. The atmospheres of gas giants are dominated by molecular hydrogen, and simultaneous observation of H$_2$ and HD lines provides reliable information on the D/H ratios on these planets. The reference spectroscopic parameters play a crucial role in such studies. Under thermodynamic conditions encountered in these atmospheres, the spectroscopic studies of HD require not only the knowledge of line intensities and positions but also accurate reference data on pressure-induced line shapes and shifts. Our aim is to provide accurate collision-induced line-shape parameters for HD lines that cover any thermodynamic conditions relevant to the atmospheres of giant planets, i.e., any relevant temperature, pressure, and perturbing gas (the H$_2$/He mixture) composition. We perform quantum-scattering calculations on a new highly accurate ab initio potential energy surface, and we use scattering S-matrices obtained this way to determine the collision-induced line-shape parameters. We use the cavity ring-down spectroscopy for validation of our theoretical methodology. We report accurate collision-induced line-shape parameters for the pure rotational R(0), R(1), and R(2) lines, the most relevant HD lines for the investigations of atmospheres of the giant planets. Besides the basic Voigt-profile collisional parameters (i.e. the broadening and shift parameters), we also report their speed dependences and the complex Dicke parameter, which can influence the effective width and height of the HD lines up to almost a factor of 2 for giant planet conditions. The sub-percent-level accuracy, reached in this work, considerably improves the previously available data. All the reported parameters are consistent with the HITRAN database format, hence allowing for the use of HAPI for generating the beyond-Voigt spectra of HD.","sentences":["The hydrogen deuteride (HD) molecule is an important deuterium tracer in astrophysical studies.","The atmospheres of gas giants are dominated by molecular hydrogen, and simultaneous observation of H$_2$ and HD lines provides reliable information on the D/H ratios on these planets.","The reference spectroscopic parameters play a crucial role in such studies.","Under thermodynamic conditions encountered in these atmospheres, the spectroscopic studies of HD require not only the knowledge of line intensities and positions but also accurate reference data on pressure-induced line shapes and shifts.","Our aim is to provide accurate collision-induced line-shape parameters for HD lines that cover any thermodynamic conditions relevant to the atmospheres of giant planets, i.e., any relevant temperature, pressure, and perturbing gas (the H$_2$/He mixture) composition.","We perform quantum-scattering calculations on a new highly accurate ab initio potential energy surface, and we use scattering S-matrices obtained this way to determine the collision-induced line-shape parameters.","We use the cavity ring-down spectroscopy for validation of our theoretical methodology.","We report accurate collision-induced line-shape parameters for the pure rotational R(0), R(1), and R(2) lines, the most relevant HD lines for the investigations of atmospheres of the giant planets.","Besides the basic Voigt-profile collisional parameters (i.e. the broadening and shift parameters), we also report their speed dependences and the complex Dicke parameter, which can influence the effective width and height of the HD lines up to almost a factor of 2 for giant planet conditions.","The sub-percent-level accuracy, reached in this work, considerably improves the previously available data.","All the reported parameters are consistent with the HITRAN database format, hence allowing for the use of HAPI for generating the beyond-Voigt spectra of HD."],"url":"http://arxiv.org/abs/2403.03747v1","category":"astro-ph.EP"}
{"created":"2024-03-06 14:34:25","title":"Thermally Stable Peltier Controlled Vacuum Chamber for Electrical Transport Measurements","abstract":"The design, manufacture and characterisation of an inexpensive, temperature controlled vacuum chamber with millikelvin stability for electrical transport measurements at and near room temperature is reported. A commercially available Peltier device and high-precision temperature controller are used to actively heat and cool the sample space. The system was designed to minimise thermal fluctuations in spintronic and semiconductor transport measurements but the general principle is relevant to a wide range of electrical measurement applications. The main issues overcome are the mounting of a sample with a path of high thermal conductivity through to the Peltier device and the heat-sinking of said Peltier device inside of a vacuum. A copper slug is used as the mount for a sample and a large copper block is used as a thermal feedthrough before a passive heatsink is used to cool this block. The Peltier device provides 20 W of heating and cooling power achieving a maximum range of 30 K below and 40 K above the ambient temperature. The temperature stability is within 5 mK at all set points with even better performance above ambient temperature. A vacuum pressure of 1e-8 hPa is achievable. As a demonstration, we present experimental results from current-induced electrical switching of a CuMnAs thin film. Transport measurements with and without the Peltier control emphasise the importance of a constant temperature in these applications. The thermal lag between the sample space measurement and the sample itself is observed through magnetoresistance values measured during a temperature sweep.","sentences":["The design, manufacture and characterisation of an inexpensive, temperature controlled vacuum chamber with millikelvin stability for electrical transport measurements at and near room temperature is reported.","A commercially available Peltier device and high-precision temperature controller are used to actively heat and cool the sample space.","The system was designed to minimise thermal fluctuations in spintronic and semiconductor transport measurements but the general principle is relevant to a wide range of electrical measurement applications.","The main issues overcome are the mounting of a sample with a path of high thermal conductivity through to the Peltier device and the heat-sinking of said Peltier device inside of a vacuum.","A copper slug is used as the mount for a sample and a large copper block is used as a thermal feedthrough before a passive heatsink is used to cool this block.","The Peltier device provides 20 W of heating and cooling power achieving a maximum range of 30 K below and 40 K above the ambient temperature.","The temperature stability is within 5 mK at all set points with even better performance above ambient temperature.","A vacuum pressure of 1e-8 hPa is achievable.","As a demonstration, we present experimental results from current-induced electrical switching of a CuMnAs thin film.","Transport measurements with and without the Peltier control emphasise the importance of a constant temperature in these applications.","The thermal lag between the sample space measurement and the sample itself is observed through magnetoresistance values measured during a temperature sweep."],"url":"http://arxiv.org/abs/2403.03745v1","category":"physics.ins-det"}
{"created":"2024-03-06 14:34:07","title":"Towards Safe and Aligned Large Language Models for Medicine","abstract":"The capabilities of large language models (LLMs) have been progressing at a breathtaking speed, leaving even their own developers grappling with the depth of their potential and risks. While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses, to our knowledge, the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights. To this end, we carry out the first safety evaluation for medical LLMs. Specifically, we set forth a definition of medical safety and alignment for medical artificial intelligence systems, develop a dataset of harmful medical questions to evaluate the medical safety and alignment of an LLM, evaluate both general and medical safety and alignment of medical LLMs, demonstrate fine-tuning as an effective mitigation strategy, and discuss broader, large-scale approaches used by the machine learning community to develop safe and aligned LLMs. We hope that this work casts light on the safety and alignment of medical LLMs and motivates future work to study it and develop additional mitigation strategies, minimizing the risks of harm of LLMs in medicine.","sentences":["The capabilities of large language models (LLMs) have been progressing at a breathtaking speed, leaving even their own developers grappling with the depth of their potential and risks.","While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses, to our knowledge, the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights.","To this end, we carry out the first safety evaluation for medical LLMs.","Specifically, we set forth a definition of medical safety and alignment for medical artificial intelligence systems, develop a dataset of harmful medical questions to evaluate the medical safety and alignment of an LLM, evaluate both general and medical safety and alignment of medical LLMs, demonstrate fine-tuning as an effective mitigation strategy, and discuss broader, large-scale approaches used by the machine learning community to develop safe and aligned LLMs.","We hope that this work casts light on the safety and alignment of medical LLMs and motivates future work to study it and develop additional mitigation strategies, minimizing the risks of harm of LLMs in medicine."],"url":"http://arxiv.org/abs/2403.03744v1","category":"cs.AI"}
{"created":"2024-03-06 14:33:51","title":"Don't forget the electrons: extending moderately-sized nuclear networks for multidimensional hydrodynamic codes","abstract":"We present here an extended nuclear network, with 90 species, designed for being coupled with hydrodynamic simulations, which includes neutrons, protons, electrons, positrons, and the corresponding neutrino and anti-neutrino emission. This network is also coupled with temperature, making it extremely robust and, together with its size, unique of its kind. The inclusion of electron captures on free protons makes the network very appropriate for multidimensional studies of Type Ia supernova explosions, especially when the exploding object is a massive white dwarf.   The results obtained with the proposed medium-sized network compare fairly well, to a few percent, with those computed with the extended network WinNet (> 2000 isotopes) in scenarios reproducing the gross physical conditions of current Type Ia supernova explosion models. In those cases where the carbon and oxygen fuel ignites at high density, the high-temperature plateau typical of the nuclear statistical equilibrium regime is well defined and stable, allowing large integration time steps. We show that the inclusion of electron captures on free protons substantially improves the estimation of the electron fraction of the mixture. Therefore, the pressure is better determined than in networks where electron captures are excluded, which will ultimately lead to more reliable hydrodynamic models. Explosive combustion of helium at low density, occurring near the surface layer of a white dwarf, is also better described with the proposed network, which gives nuclear energy generation rates much closer to WinNet than typical reduced alpha networks.","sentences":["We present here an extended nuclear network, with 90 species, designed for being coupled with hydrodynamic simulations, which includes neutrons, protons, electrons, positrons, and the corresponding neutrino and anti-neutrino emission.","This network is also coupled with temperature, making it extremely robust and, together with its size, unique of its kind.","The inclusion of electron captures on free protons makes the network very appropriate for multidimensional studies of Type Ia supernova explosions, especially when the exploding object is a massive white dwarf.   ","The results obtained with the proposed medium-sized network compare fairly well, to a few percent, with those computed with the extended network WinNet (> 2000 isotopes) in scenarios reproducing the gross physical conditions of current Type Ia supernova explosion models.","In those cases where the carbon and oxygen fuel ignites at high density, the high-temperature plateau typical of the nuclear statistical equilibrium regime is well defined and stable, allowing large integration time steps.","We show that the inclusion of electron captures on free protons substantially improves the estimation of the electron fraction of the mixture.","Therefore, the pressure is better determined than in networks where electron captures are excluded, which will ultimately lead to more reliable hydrodynamic models.","Explosive combustion of helium at low density, occurring near the surface layer of a white dwarf, is also better described with the proposed network, which gives nuclear energy generation rates much closer to WinNet than typical reduced alpha networks."],"url":"http://arxiv.org/abs/2403.03743v1","category":"astro-ph.SR"}
{"created":"2024-03-06 14:30:09","title":"SUPClust: Active Learning at the Boundaries","abstract":"Active learning is a machine learning paradigm designed to optimize model performance in a setting where labeled data is expensive to acquire. In this work, we propose a novel active learning method called SUPClust that seeks to identify points at the decision boundary between classes. By targeting these points, SUPClust aims to gather information that is most informative for refining the model's prediction of complex decision regions. We demonstrate experimentally that labeling these points leads to strong model performance. This improvement is observed even in scenarios characterized by strong class imbalance.","sentences":["Active learning is a machine learning paradigm designed to optimize model performance in a setting where labeled data is expensive to acquire.","In this work, we propose a novel active learning method called SUPClust that seeks to identify points at the decision boundary between classes.","By targeting these points, SUPClust aims to gather information that is most informative for refining the model's prediction of complex decision regions.","We demonstrate experimentally that labeling these points leads to strong model performance.","This improvement is observed even in scenarios characterized by strong class imbalance."],"url":"http://arxiv.org/abs/2403.03741v1","category":"cs.LG"}
{"created":"2024-03-06 14:28:53","title":"Self-supervised Photographic Image Layout Representation Learning","abstract":"In the domain of image layout representation learning, the critical process of translating image layouts into succinct vector forms is increasingly significant across diverse applications, such as image retrieval, manipulation, and generation. Most approaches in this area heavily rely on costly labeled datasets and notably lack in adapting their modeling and learning methods to the specific nuances of photographic image layouts. This shortfall makes the learning process for photographic image layouts suboptimal. In our research, we directly address these challenges. We innovate by defining basic layout primitives that encapsulate various levels of layout information and by mapping these, along with their interconnections, onto a heterogeneous graph structure. This graph is meticulously engineered to capture the intricate layout information within the pixel domain explicitly. Advancing further, we introduce novel pretext tasks coupled with customized loss functions, strategically designed for effective self-supervised learning of these layout graphs. Building on this foundation, we develop an autoencoder-based network architecture skilled in compressing these heterogeneous layout graphs into precise, dimensionally-reduced layout representations. Additionally, we introduce the LODB dataset, which features a broader range of layout categories and richer semantics, serving as a comprehensive benchmark for evaluating the effectiveness of layout representation learning methods. Our extensive experimentation on this dataset demonstrates the superior performance of our approach in the realm of photographic image layout representation learning.","sentences":["In the domain of image layout representation learning, the critical process of translating image layouts into succinct vector forms is increasingly significant across diverse applications, such as image retrieval, manipulation, and generation.","Most approaches in this area heavily rely on costly labeled datasets and notably lack in adapting their modeling and learning methods to the specific nuances of photographic image layouts.","This shortfall makes the learning process for photographic image layouts suboptimal.","In our research, we directly address these challenges.","We innovate by defining basic layout primitives that encapsulate various levels of layout information and by mapping these, along with their interconnections, onto a heterogeneous graph structure.","This graph is meticulously engineered to capture the intricate layout information within the pixel domain explicitly.","Advancing further, we introduce novel pretext tasks coupled with customized loss functions, strategically designed for effective self-supervised learning of these layout graphs.","Building on this foundation, we develop an autoencoder-based network architecture skilled in compressing these heterogeneous layout graphs into precise, dimensionally-reduced layout representations.","Additionally, we introduce the LODB dataset, which features a broader range of layout categories and richer semantics, serving as a comprehensive benchmark for evaluating the effectiveness of layout representation learning methods.","Our extensive experimentation on this dataset demonstrates the superior performance of our approach in the realm of photographic image layout representation learning."],"url":"http://arxiv.org/abs/2403.03740v1","category":"cs.CV"}
{"created":"2024-03-06 14:28:49","title":"A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural Network","abstract":"Binary neural networks utilize 1-bit quantized weights and activations to reduce both the model's storage demands and computational burden. However, advanced binary architectures still incorporate millions of inefficient and nonhardware-friendly full-precision multiplication operations. A&B BNN is proposed to directly remove part of the multiplication operations in a traditional BNN and replace the rest with an equal number of bit operations, introducing the mask layer and the quantized RPReLU structure based on the normalizer-free network architecture. The mask layer can be removed during inference by leveraging the intrinsic characteristics of BNN with straightforward mathematical transformations to avoid the associated multiplication operations. The quantized RPReLU structure enables more efficient bit operations by constraining its slope to be integer powers of 2. Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFAR-10, CIFAR-100, and ImageNet datasets, respectively, which are competitive with the state-of-the-art. Ablation studies have verified the efficacy of the quantized RPReLU structure, leading to a 1.14% enhancement on the ImageNet compared to using a fixed slope RLeakyReLU. The proposed add&bit-operation-only BNN offers an innovative approach for hardware-friendly network architecture.","sentences":["Binary neural networks utilize 1-bit quantized weights and activations to reduce both the model's storage demands and computational burden.","However, advanced binary architectures still incorporate millions of inefficient and nonhardware-friendly full-precision multiplication operations.","A&B BNN is proposed to directly remove part of the multiplication operations in a traditional BNN and replace the rest with an equal number of bit operations, introducing the mask layer and the quantized RPReLU structure based on the normalizer-free network architecture.","The mask layer can be removed during inference by leveraging the intrinsic characteristics of BNN with straightforward mathematical transformations to avoid the associated multiplication operations.","The quantized RPReLU structure enables more efficient bit operations by constraining its slope to be integer powers of 2.","Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFAR-10, CIFAR-100, and ImageNet datasets, respectively, which are competitive with the state-of-the-art.","Ablation studies have verified the efficacy of the quantized RPReLU structure, leading to a 1.14% enhancement on the ImageNet compared to using a fixed slope RLeakyReLU.","The proposed add&bit-operation-only BNN offers an innovative approach for hardware-friendly network architecture."],"url":"http://arxiv.org/abs/2403.03739v1","category":"cs.LG"}
{"created":"2024-03-06 14:27:02","title":"Unifying Generation and Compression: Ultra-low bitrate Image Coding Via Multi-stage Transformer","abstract":"Recent progress in generative compression technology has significantly improved the perceptual quality of compressed data. However, these advancements primarily focus on producing high-frequency details, often overlooking the ability of generative models to capture the prior distribution of image content, thus impeding further bitrate reduction in extreme compression scenarios (<0.05 bpp). Motivated by the capabilities of predictive language models for lossless compression, this paper introduces a novel Unified Image Generation-Compression (UIGC) paradigm, merging the processes of generation and compression. A key feature of the UIGC framework is the adoption of vector-quantized (VQ) image models for tokenization, alongside a multi-stage transformer designed to exploit spatial contextual information for modeling the prior distribution. As such, the dual-purpose framework effectively utilizes the learned prior for entropy estimation and assists in the regeneration of lost tokens. Extensive experiments demonstrate the superiority of the proposed UIGC framework over existing codecs in perceptual quality and human perception, particularly in ultra-low bitrate scenarios (<=0.03 bpp), pioneering a new direction in generative compression.","sentences":["Recent progress in generative compression technology has significantly improved the perceptual quality of compressed data.","However, these advancements primarily focus on producing high-frequency details, often overlooking the ability of generative models to capture the prior distribution of image content, thus impeding further bitrate reduction in extreme compression scenarios (<0.05 bpp).","Motivated by the capabilities of predictive language models for lossless compression, this paper introduces a novel Unified Image Generation-Compression (UIGC) paradigm, merging the processes of generation and compression.","A key feature of the UIGC framework is the adoption of vector-quantized (VQ) image models for tokenization, alongside a multi-stage transformer designed to exploit spatial contextual information for modeling the prior distribution.","As such, the dual-purpose framework effectively utilizes the learned prior for entropy estimation and assists in the regeneration of lost tokens.","Extensive experiments demonstrate the superiority of the proposed UIGC framework over existing codecs in perceptual quality and human perception, particularly in ultra-low bitrate scenarios (<=0.03 bpp), pioneering a new direction in generative compression."],"url":"http://arxiv.org/abs/2403.03736v1","category":"cs.CV"}
{"created":"2024-03-06 14:20:47","title":"Expansion properties of polynomials over finite fields","abstract":"We establish expansion properties for suitably generic polynomials of degree $d$ in $d+1$ variables over finite fields. In particular, we show that if $P\\in\\mathbb{F}_q[x_1,\\ldots,x_{d+1}]$ is a polynomial of degree $d$ coming from an explicit, Zariski dense set, and $X_1,\\ldots,X_{d+1}\\subseteq\\mathbb{F}_q$ are suitably large, then $|P(X_1,\\ldots,X_{d+1})|=q-O(1)$. Our methods rely on a higher-degree extension of a result of Vinh on point--line incidences over a finite field.","sentences":["We establish expansion properties for suitably generic polynomials of degree $d$ in $d+1$ variables over finite fields.","In particular, we show that if $P\\in\\mathbb{F}_q[x_1,\\ldots,x_{d+1}]$ is a polynomial of degree $d$ coming from an explicit, Zariski dense set, and $X_1,\\ldots,X_{d+1}\\subseteq\\mathbb{F}_q$ are suitably large, then $|P(X_1,\\ldots,X_{d+1})|=q-O(1)$. Our methods rely on a higher-degree extension of a result of Vinh on point--line incidences over a finite field."],"url":"http://arxiv.org/abs/2403.03732v1","category":"math.CO"}
{"created":"2024-03-06 14:20:39","title":"Detection prospects of Very and Ultra High-Energy gamma rays from extended sources with ASTRI, CTA, and LHAASO","abstract":"Context. The recent discovery of several ultra high-energy gamma-ray emitters in our Galaxy constitutes a significant advancement towards unveiling its most powerful accelerators and their properties. Nonetheless, in order to unambiguously locate the regions where the highest energy particles are produced and understand the responsible physical mechanisms, detailed spectral and morphological studies are required, especially given that most of the observed sources were found to be significantly extended. Aims. In these regards, pointing observations with the next-generation Imaging Atmospheric Cherenkov Telescopes, like the Cherenkov Telescope Array (CTA) Observatory and the ASTRI Mini-Array (ASTRI), are expected to provide significant improvements. Here we aim at identifying the most promising sources to target in future observations. Methods. To this purpose, we performed a comparative analysis of the expected performance of ASTRI and CTA, computing their differential sensitivities towards extended sources, and further explored their capabilities with respect to specific case studies, including follow-ups of existing gamma-ray source catalogs. Results. We find that almost all of the sources so far detected by LHAASO-WCDA and HGPS will be in the reach of ASTRI and CTA with 300 and 50 hours of exposure, respectively. For the highest energy emitters detected by LHAASO-KM2A, in turn, we provide the list of the most promising objects to be investigated. We further examined specific classes of sources in order to identify potentially detectable gamma-ray emitters, such as passive molecular clouds (i.e. illuminated by the cosmic-ray sea) and pulsars surrounded by a halo of runaway particles.","sentences":["Context.","The recent discovery of several ultra high-energy gamma-ray emitters in our Galaxy constitutes a significant advancement towards unveiling its most powerful accelerators and their properties.","Nonetheless, in order to unambiguously locate the regions where the highest energy particles are produced and understand the responsible physical mechanisms, detailed spectral and morphological studies are required, especially given that most of the observed sources were found to be significantly extended.","Aims.","In these regards, pointing observations with the next-generation Imaging Atmospheric Cherenkov Telescopes, like the Cherenkov Telescope Array (CTA) Observatory and the ASTRI Mini-Array (ASTRI), are expected to provide significant improvements.","Here we aim at identifying the most promising sources to target in future observations.","Methods.","To this purpose, we performed a comparative analysis of the expected performance of ASTRI and CTA, computing their differential sensitivities towards extended sources, and further explored their capabilities with respect to specific case studies, including follow-ups of existing gamma-ray source catalogs.","Results.","We find that almost all of the sources so far detected by LHAASO-WCDA and HGPS will be in the reach of ASTRI and CTA with 300 and 50 hours of exposure, respectively.","For the highest energy emitters detected by LHAASO-KM2A, in turn, we provide the list of the most promising objects to be investigated.","We further examined specific classes of sources in order to identify potentially detectable gamma-ray emitters, such as passive molecular clouds (i.e. illuminated by the cosmic-ray sea) and pulsars surrounded by a halo of runaway particles."],"url":"http://arxiv.org/abs/2403.03731v1","category":"astro-ph.HE"}
{"created":"2024-03-06 14:19:11","title":"Learning 3D object-centric representation through prediction","abstract":"As part of human core knowledge, the representation of objects is the building block of mental representation that supports high-level concepts and symbolic reasoning. While humans develop the ability of perceiving objects situated in 3D environments without supervision, models that learn the same set of abilities with similar constraints faced by human infants are lacking. Towards this end, we developed a novel network architecture that simultaneously learns to 1) segment objects from discrete images, 2) infer their 3D locations, and 3) perceive depth, all while using only information directly available to the brain as training data, namely: sequences of images and self-motion. The core idea is treating objects as latent causes of visual input which the brain uses to make efficient predictions of future scenes. This results in object representations being learned as an essential byproduct of learning to predict.","sentences":["As part of human core knowledge, the representation of objects is the building block of mental representation that supports high-level concepts and symbolic reasoning.","While humans develop the ability of perceiving objects situated in 3D environments without supervision, models that learn the same set of abilities with similar constraints faced by human infants are lacking.","Towards this end, we developed a novel network architecture that simultaneously learns to 1) segment objects from discrete images, 2) infer their 3D locations, and 3) perceive depth, all while using only information directly available to the brain as training data, namely: sequences of images and self-motion.","The core idea is treating objects as latent causes of visual input which the brain uses to make efficient predictions of future scenes.","This results in object representations being learned as an essential byproduct of learning to predict."],"url":"http://arxiv.org/abs/2403.03730v1","category":"cs.CV"}
{"created":"2024-03-06 14:18:24","title":"Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training","abstract":"This study addresses the integration of diversity-based and uncertainty-based sampling strategies in active learning, particularly within the context of self-supervised pre-trained models. We introduce a straightforward heuristic called TCM that mitigates the cold start problem while maintaining strong performance across various data levels. By initially applying TypiClust for diversity sampling and subsequently transitioning to uncertainty sampling with Margin, our approach effectively combines the strengths of both strategies. Our experiments demonstrate that TCM consistently outperforms existing methods across various datasets in both low and high data regimes.","sentences":["This study addresses the integration of diversity-based and uncertainty-based sampling strategies in active learning, particularly within the context of self-supervised pre-trained models.","We introduce a straightforward heuristic called TCM that mitigates the cold start problem while maintaining strong performance across various data levels.","By initially applying TypiClust for diversity sampling and subsequently transitioning to uncertainty sampling with Margin, our approach effectively combines the strengths of both strategies.","Our experiments demonstrate that TCM consistently outperforms existing methods across various datasets in both low and high data regimes."],"url":"http://arxiv.org/abs/2403.03728v1","category":"cs.LG"}
{"created":"2024-03-06 14:15:20","title":"Diffusion on language model embeddings for protein sequence generation","abstract":"Protein design requires a deep understanding of the inherent complexities of the protein universe. While many efforts lean towards conditional generation or focus on specific families of proteins, the foundational task of unconditional generation remains underexplored and undervalued. Here, we explore this pivotal domain, introducing DiMA, a model that leverages continuous diffusion on embeddings derived from the protein language model, ESM-2, to generate amino acid sequences. DiMA surpasses leading solutions, including autoregressive transformer-based and discrete diffusion models, and we quantitatively illustrate the impact of the design choices that lead to its superior performance. We extensively evaluate the quality, diversity, distribution similarity, and biological relevance of the generated sequences using multiple metrics across various modalities. Our approach consistently produces novel, diverse protein sequences that accurately reflect the inherent structural and functional diversity of the protein space. This work advances the field of protein design and sets the stage for conditional models by providing a robust framework for scalable and high-quality protein sequence generation.","sentences":["Protein design requires a deep understanding of the inherent complexities of the protein universe.","While many efforts lean towards conditional generation or focus on specific families of proteins, the foundational task of unconditional generation remains underexplored and undervalued.","Here, we explore this pivotal domain, introducing DiMA, a model that leverages continuous diffusion on embeddings derived from the protein language model, ESM-2, to generate amino acid sequences.","DiMA surpasses leading solutions, including autoregressive transformer-based and discrete diffusion models, and we quantitatively illustrate the impact of the design choices that lead to its superior performance.","We extensively evaluate the quality, diversity, distribution similarity, and biological relevance of the generated sequences using multiple metrics across various modalities.","Our approach consistently produces novel, diverse protein sequences that accurately reflect the inherent structural and functional diversity of the protein space.","This work advances the field of protein design and sets the stage for conditional models by providing a robust framework for scalable and high-quality protein sequence generation."],"url":"http://arxiv.org/abs/2403.03726v1","category":"cs.LG"}
{"created":"2024-03-06 14:15:18","title":"To Trust or Not to Trust: Assignment Mechanisms with Predictions in the Private Graph Model","abstract":"The realm of algorithms with predictions has led to the development of several new algorithms that leverage (potentially erroneous) predictions to enhance their performance guarantees. The challenge is to devise algorithms that achieve optimal approximation guarantees as the prediction quality varies from perfect (consistency) to imperfect (robustness). This framework is particularly appealing in mechanism design contexts, where predictions might convey private information about the agents. In this paper, we design strategyproof mechanisms that leverage predictions to achieve improved approximation guarantees for several variants of the Generalized Assignment Problem (GAP) in the private graph model. In this model, first introduced by Dughmi & Ghosh (2010), the set of resources that an agent is compatible with is private information. For the Bipartite Matching Problem (BMP), we give a deterministic group-strategyproof (GSP) mechanism that is $(1 +1/\\gamma)$-consistent and $(1 + \\gamma)$-robust, where $\\gamma \\ge 1$ is some confidence parameter. We also prove that this is best possible. Remarkably, our mechanism draws inspiration from the renowned Gale-Shapley algorithm, incorporating predictions as a crucial element. Additionally, we give a randomized mechanism that is universally GSP and improves on the guarantees in expectation. The other GAP variants that we consider all make use of a unified greedy mechanism that adds edges to the assignment according to a specific order. Our universally GSP mechanism randomizes over the greedy mechanism, our mechanism for BMP and the predicted assignment, leading to $(1+3/\\gamma)$-consistency and $(3+\\gamma)$-robustness in expectation. All our mechanisms also provide more fine-grained approximation guarantees that interpolate between the consistency and the robustness, depending on some natural error measure of the prediction.","sentences":["The realm of algorithms with predictions has led to the development of several new algorithms that leverage (potentially erroneous) predictions to enhance their performance guarantees.","The challenge is to devise algorithms that achieve optimal approximation guarantees as the prediction quality varies from perfect (consistency) to imperfect (robustness).","This framework is particularly appealing in mechanism design contexts, where predictions might convey private information about the agents.","In this paper, we design strategyproof mechanisms that leverage predictions to achieve improved approximation guarantees for several variants of the Generalized Assignment Problem (GAP) in the private graph model.","In this model, first introduced by Dughmi & Ghosh (2010), the set of resources that an agent is compatible with is private information.","For the Bipartite Matching Problem (BMP), we give a deterministic group-strategyproof (GSP) mechanism that is $(1 +1/\\gamma)$-consistent and $(1 + \\gamma)$-robust, where $\\gamma \\ge 1$ is some confidence parameter.","We also prove that this is best possible.","Remarkably, our mechanism draws inspiration from the renowned Gale-Shapley algorithm, incorporating predictions as a crucial element.","Additionally, we give a randomized mechanism that is universally GSP and improves on the guarantees in expectation.","The other GAP variants that we consider all make use of a unified greedy mechanism that adds edges to the assignment according to a specific order.","Our universally GSP mechanism randomizes over the greedy mechanism, our mechanism for BMP and the predicted assignment, leading to $(1+3/\\gamma)$-consistency and $(3+\\gamma)$-robustness in expectation.","All our mechanisms also provide more fine-grained approximation guarantees that interpolate between the consistency and the robustness, depending on some natural error measure of the prediction."],"url":"http://arxiv.org/abs/2403.03725v1","category":"cs.GT"}
{"created":"2024-03-06 14:15:01","title":"In the Search of Optimal Tree Networks: Hardness and Heuristics","abstract":"Demand-aware communication networks are networks whose topology is optimized toward the traffic they need to serve. These networks have recently been enabled by novel optical communication technologies and are investigated intensively in the context of datacenters. In this work, we consider networks with one of the most common topologies~ -- a binary tree.   We show that finding an optimal demand-aware binary tree network is NP-hard. Then, we propose optimization algorithms that generate efficient binary tree networks on real-life and synthetic workloads.","sentences":["Demand-aware communication networks are networks whose topology is optimized toward the traffic they need to serve.","These networks have recently been enabled by novel optical communication technologies and are investigated intensively in the context of datacenters.","In this work, we consider networks with one of the most common topologies~ -- a binary tree.   ","We show that finding an optimal demand-aware binary tree network is NP-hard.","Then, we propose optimization algorithms that generate efficient binary tree networks on real-life and synthetic workloads."],"url":"http://arxiv.org/abs/2403.03724v1","category":"cs.NI"}
{"created":"2024-03-06 14:12:38","title":"CMDA: Cross-Modal and Domain Adversarial Adaptation for LiDAR-Based 3D Object Detection","abstract":"Recent LiDAR-based 3D Object Detection (3DOD) methods show promising results, but they often do not generalize well to target domains outside the source (or training) data distribution. To reduce such domain gaps and thus to make 3DOD models more generalizable, we introduce a novel unsupervised domain adaptation (UDA) method, called CMDA, which (i) leverages visual semantic cues from an image modality (i.e., camera images) as an effective semantic bridge to close the domain gap in the cross-modal Bird's Eye View (BEV) representations. Further, (ii) we also introduce a self-training-based learning strategy, wherein a model is adversarially trained to generate domain-invariant features, which disrupt the discrimination of whether a feature instance comes from a source or an unseen target domain. Overall, our CMDA framework guides the 3DOD model to generate highly informative and domain-adaptive features for novel data distributions. In our extensive experiments with large-scale benchmarks, such as nuScenes, Waymo, and KITTI, those mentioned above provide significant performance gains for UDA tasks, achieving state-of-the-art performance.","sentences":["Recent LiDAR-based 3D Object Detection (3DOD) methods show promising results, but they often do not generalize well to target domains outside the source (or training) data distribution.","To reduce such domain gaps and thus to make 3DOD models more generalizable, we introduce a novel unsupervised domain adaptation (UDA) method, called CMDA, which (i) leverages visual semantic cues from an image modality (i.e., camera images) as an effective semantic bridge to close the domain gap in the cross-modal Bird's Eye View (BEV) representations.","Further, (ii) we also introduce a self-training-based learning strategy, wherein a model is adversarially trained to generate domain-invariant features, which disrupt the discrimination of whether a feature instance comes from a source or an unseen target domain.","Overall, our CMDA framework guides the 3DOD model to generate highly informative and domain-adaptive features for novel data distributions.","In our extensive experiments with large-scale benchmarks, such as nuScenes, Waymo, and KITTI, those mentioned above provide significant performance gains for UDA tasks, achieving state-of-the-art performance."],"url":"http://arxiv.org/abs/2403.03721v1","category":"cs.CV"}
{"created":"2024-03-06 14:11:45","title":"Multimodal Transformer for Comics Text-Cloze","abstract":"This work explores a closure task in comics, a medium where visual and textual elements are intricately intertwined. Specifically, Text-cloze refers to the task of selecting the correct text to use in a comic panel, given its neighboring panels. Traditional methods based on recurrent neural networks have struggled with this task due to limited OCR accuracy and inherent model limitations. We introduce a novel Multimodal Large Language Model (Multimodal-LLM) architecture, specifically designed for Text-cloze, achieving a 10% improvement over existing state-of-the-art models in both its easy and hard variants. Central to our approach is a Domain-Adapted ResNet-50 based visual encoder, fine-tuned to the comics domain in a self-supervised manner using SimCLR. This encoder delivers comparable results to more complex models with just one-fifth of the parameters. Additionally, we release new OCR annotations for this dataset, enhancing model input quality and resulting in another 1% improvement. Finally, we extend the task to a generative format, establishing new baselines and expanding the research possibilities in the field of comics analysis.","sentences":["This work explores a closure task in comics, a medium where visual and textual elements are intricately intertwined.","Specifically, Text-cloze refers to the task of selecting the correct text to use in a comic panel, given its neighboring panels.","Traditional methods based on recurrent neural networks have struggled with this task due to limited OCR accuracy and inherent model limitations.","We introduce a novel Multimodal Large Language Model (Multimodal-LLM) architecture, specifically designed for Text-cloze, achieving a 10% improvement over existing state-of-the-art models in both its easy and hard variants.","Central to our approach is a Domain-Adapted ResNet-50 based visual encoder, fine-tuned to the comics domain in a self-supervised manner using SimCLR.","This encoder delivers comparable results to more complex models with just one-fifth of the parameters.","Additionally, we release new OCR annotations for this dataset, enhancing model input quality and resulting in another 1% improvement.","Finally, we extend the task to a generative format, establishing new baselines and expanding the research possibilities in the field of comics analysis."],"url":"http://arxiv.org/abs/2403.03719v1","category":"cs.CV"}
{"created":"2024-03-06 14:11:30","title":"Density in the half-line Schwartz space of functions whose Fourier-Laplace transform has natural boundary the real line","abstract":"In this note, we prove that the Fourier-Laplace transform of the typical function (i.e., generic in the sense of Baire category theorem) in the Schwartz class of the half-line, being analytic in the lower half of the complex plane, has natural boundary the axis of the real numbers. We also provide variations and generalizations.","sentences":["In this note, we prove that the Fourier-Laplace transform of the typical function (i.e., generic in the sense of Baire category theorem) in the Schwartz class of the half-line, being analytic in the lower half of the complex plane, has natural boundary the axis of the real numbers.","We also provide variations and generalizations."],"url":"http://arxiv.org/abs/2403.03718v1","category":"math.CV"}
{"created":"2024-03-06 14:11:06","title":"A multiplexed control architecture for superconducting qubits with row-column addressing","abstract":"In state-of-the-art superconducting quantum processors, each qubit is controlled by at least one control line that delivers control pulses generated at room temperature to qubits at millikelvin temperatures. This strategy has been successfully applied to control hundreds of qubits but is unlikely to be scalable to control thousands of qubits, let alone millions or even billions of qubits needed in fault-tolerance quantum computing. The reason for this is due to the wiring challenge, the number of accommodated control lines is limited by factors, such as the cooling power and physical space of the cryogenic system, the control footprint area at the qubit chip level, and so on. Here, we introduce a multiplexed control architecture for superconducting qubits with two types of shared control lines, row and column lines, providing an efficient approach for parallel controlling $N$ qubits with $O(\\sqrt{N})$ control lines. With the combination of the two-type shared lines, unique pairs of control pulses are delivered to qubits on each row-column intersection, enabling parallel qubit addressing. Of particular concern here is that, unlike traditional gate schemes, both single- and two-qubit gates are implemented with pairs of control pulses. Considering the inherent parallelism and the control limitations, the integration of the architecture into quantum computing systems should be tailored as much as possible to the specific properties of the quantum circuits to be executed. As such, the architecture could be scalable for executing structured quantum circuits, such as quantum error correction circuits.","sentences":["In state-of-the-art superconducting quantum processors, each qubit is controlled by at least one control line that delivers control pulses generated at room temperature to qubits at millikelvin temperatures.","This strategy has been successfully applied to control hundreds of qubits but is unlikely to be scalable to control thousands of qubits, let alone millions or even billions of qubits needed in fault-tolerance quantum computing.","The reason for this is due to the wiring challenge, the number of accommodated control lines is limited by factors, such as the cooling power and physical space of the cryogenic system, the control footprint area at the qubit chip level, and so on.","Here, we introduce a multiplexed control architecture for superconducting qubits with two types of shared control lines, row and column lines, providing an efficient approach for parallel controlling $N$ qubits with $O(\\sqrt{N})$ control lines.","With the combination of the two-type shared lines, unique pairs of control pulses are delivered to qubits on each row-column intersection, enabling parallel qubit addressing.","Of particular concern here is that, unlike traditional gate schemes, both single- and two-qubit gates are implemented with pairs of control pulses.","Considering the inherent parallelism and the control limitations, the integration of the architecture into quantum computing systems should be tailored as much as possible to the specific properties of the quantum circuits to be executed.","As such, the architecture could be scalable for executing structured quantum circuits, such as quantum error correction circuits."],"url":"http://arxiv.org/abs/2403.03717v1","category":"quant-ph"}
{"created":"2024-03-06 14:00:31","title":"MeaCap: Memory-Augmented Zero-shot Image Captioning","abstract":"Zero-shot image captioning (IC) without well-paired image-text data can be divided into two categories, training-free and text-only-training. Generally, these two types of methods realize zero-shot IC by integrating pretrained vision-language models like CLIP for image-text similarity evaluation and a pre-trained language model (LM) for caption generation. The main difference between them is whether using a textual corpus to train the LM. Though achieving attractive performance w.r.t. some metrics, existing methods often exhibit some common drawbacks. Training-free methods tend to produce hallucinations, while text-only-training often lose generalization capability. To move forward, in this paper, we propose a novel Memory-Augmented zero-shot image Captioning framework (MeaCap). Specifically, equipped with a textual memory, we introduce a retrieve-then-filter module to get key concepts that are highly related to the image. By deploying our proposed memory-augmented visual-related fusion score in a keywords-to-sentence LM, MeaCap can generate concept-centered captions that keep high consistency with the image with fewer hallucinations and more world-knowledge. The framework of MeaCap achieves the state-of-the-art performance on a series of zero-shot IC settings. Our code is available at https://github.com/joeyz0z/MeaCap.","sentences":["Zero-shot image captioning (IC) without well-paired image-text data can be divided into two categories, training-free and text-only-training.","Generally, these two types of methods realize zero-shot IC by integrating pretrained vision-language models like CLIP for image-text similarity evaluation and a pre-trained language model (LM) for caption generation.","The main difference between them is whether using a textual corpus to train the LM.","Though achieving attractive performance w.r.t.","some metrics, existing methods often exhibit some common drawbacks.","Training-free methods tend to produce hallucinations, while text-only-training often lose generalization capability.","To move forward, in this paper, we propose a novel Memory-Augmented zero-shot image Captioning framework (MeaCap).","Specifically, equipped with a textual memory, we introduce a retrieve-then-filter module to get key concepts that are highly related to the image.","By deploying our proposed memory-augmented visual-related fusion score in a keywords-to-sentence LM, MeaCap can generate concept-centered captions that keep high consistency with the image with fewer hallucinations and more world-knowledge.","The framework of MeaCap achieves the state-of-the-art performance on a series of zero-shot IC settings.","Our code is available at https://github.com/joeyz0z/MeaCap."],"url":"http://arxiv.org/abs/2403.03715v1","category":"cs.CV"}
{"created":"2024-03-06 13:59:53","title":"Intent-aware Recommendation via Disentangled Graph Contrastive Learning","abstract":"Graph neural network (GNN) based recommender systems have become one of the mainstream trends due to the powerful learning ability from user behavior data. Understanding the user intents from behavior data is the key to recommender systems, which poses two basic requirements for GNN-based recommender systems. One is how to learn complex and diverse intents especially when the user behavior is usually inadequate in reality. The other is different behaviors have different intent distributions, so how to establish their relations for a more explainable recommender system. In this paper, we present the Intent-aware Recommendation via Disentangled Graph Contrastive Learning (IDCL), which simultaneously learns interpretable intents and behavior distributions over those intents. Specifically, we first model the user behavior data as a user-item-concept graph, and design a GNN based behavior disentangling module to learn the different intents. Then we propose the intent-wise contrastive learning to enhance the intent disentangling and meanwhile infer the behavior distributions. Finally, the coding rate reduction regularization is introduced to make the behaviors of different intents orthogonal. Extensive experiments demonstrate the effectiveness of IDCL in terms of substantial improvement and the interpretability.","sentences":["Graph neural network (GNN) based recommender systems have become one of the mainstream trends due to the powerful learning ability from user behavior data.","Understanding the user intents from behavior data is the key to recommender systems, which poses two basic requirements for GNN-based recommender systems.","One is how to learn complex and diverse intents especially when the user behavior is usually inadequate in reality.","The other is different behaviors have different intent distributions, so how to establish their relations for a more explainable recommender system.","In this paper, we present the Intent-aware Recommendation via Disentangled Graph Contrastive Learning (IDCL), which simultaneously learns interpretable intents and behavior distributions over those intents.","Specifically, we first model the user behavior data as a user-item-concept graph, and design a GNN based behavior disentangling module to learn the different intents.","Then we propose the intent-wise contrastive learning to enhance the intent disentangling and meanwhile infer the behavior distributions.","Finally, the coding rate reduction regularization is introduced to make the behaviors of different intents orthogonal.","Extensive experiments demonstrate the effectiveness of IDCL in terms of substantial improvement and the interpretability."],"url":"http://arxiv.org/abs/2403.03714v1","category":"cs.IR"}
{"created":"2024-03-06 13:48:44","title":"Portable, heterogeneous ensemble workflows at scale using libEnsemble","abstract":"libEnsemble is a Python-based toolkit for running dynamic ensembles, developed as part of the DOE Exascale Computing Project. The toolkit utilizes a unique generator--simulator--allocator paradigm, where generators produce input for simulators, simulators evaluate those inputs, and allocators decide whether and when a simulator or generator should be called. The generator steers the ensemble based on simulation results.   libEnsemble communicates between a manager and workers. Flexibility is provided through multiple manager--worker communication substrates each of which has different benefits. These include Python's multiprocessing, mpi4py, and TCP. Multisite ensembles are supported using Balsam or Globus Compute.   We overview the unique characteristics of libEnsemble as well as current and potential interoperability with other packages in the workflow ecosystem. We highlight libEnsemble's dynamic resource features: libEnsemble can detect system resources (nodes, cores, and GPUs) and assign these in a portable way. These features allow users to specify resources required for each simulation automatically on a range of systems, including Frontier, Aurora, and Perlmutter. Such ensembles can include multiple simulation types, some using GPUs and others using only CPUs, sharing nodes for maximum efficiency.   We demonstrate libEnsemble's capabilities, scalability, and scientific impact via a Gaussian process surrogate training problem for the longitudinal density profile at the exit of a plasma accelerator stage using Wake-T and WarpX simulations. We also describe the benefits of libEnsemble's generator--simulator coupling, which easily exposes to the user the ability to cancel, and portably kill, running simulations. Such control can be directed from the generator or allocator based on models that are updated with intermediate simulation output.","sentences":["libEnsemble is a Python-based toolkit for running dynamic ensembles, developed as part of the DOE Exascale Computing Project.","The toolkit utilizes a unique generator--simulator--allocator paradigm, where generators produce input for simulators, simulators evaluate those inputs, and allocators decide whether and when a simulator or generator should be called.","The generator steers the ensemble based on simulation results.   ","libEnsemble communicates between a manager and workers.","Flexibility is provided through multiple manager--worker communication substrates each of which has different benefits.","These include Python's multiprocessing, mpi4py, and TCP.","Multisite ensembles are supported using Balsam or Globus Compute.   ","We overview the unique characteristics of libEnsemble as well as current and potential interoperability with other packages in the workflow ecosystem.","We highlight libEnsemble's dynamic resource features: libEnsemble can detect system resources (nodes, cores, and GPUs) and assign these in a portable way.","These features allow users to specify resources required for each simulation automatically on a range of systems, including Frontier, Aurora, and Perlmutter.","Such ensembles can include multiple simulation types, some using GPUs and others using only CPUs, sharing nodes for maximum efficiency.   ","We demonstrate libEnsemble's capabilities, scalability, and scientific impact via a Gaussian process surrogate training problem for the longitudinal density profile at the exit of a plasma accelerator stage using Wake-T and WarpX simulations.","We also describe the benefits of libEnsemble's generator--simulator coupling, which easily exposes to the user the ability to cancel, and portably kill, running simulations.","Such control can be directed from the generator or allocator based on models that are updated with intermediate simulation output."],"url":"http://arxiv.org/abs/2403.03709v1","category":"cs.DC"}
{"created":"2024-03-06 13:39:18","title":"Causal Prototype-inspired Contrast Adaptation for Unsupervised Domain Adaptive Semantic Segmentation of High-resolution Remote Sensing Imagery","abstract":"Semantic segmentation of high-resolution remote sensing imagery (HRSI) suffers from the domain shift, resulting in poor performance of the model in another unseen domain. Unsupervised domain adaptive (UDA) semantic segmentation aims to adapt the semantic segmentation model trained on the labeled source domain to an unlabeled target domain. However, the existing UDA semantic segmentation models tend to align pixels or features based on statistical information related to labels in source and target domain data, and make predictions accordingly, which leads to uncertainty and fragility of prediction results. In this paper, we propose a causal prototype-inspired contrast adaptation (CPCA) method to explore the invariant causal mechanisms between different HRSIs domains and their semantic labels. It firstly disentangles causal features and bias features from the source and target domain images through a causal feature disentanglement module. Then, a causal prototypical contrast module is used to learn domain invariant causal features. To further de-correlate causal and bias features, a causal intervention module is introduced to intervene on the bias features to generate counterfactual unbiased samples. By forcing the causal features to meet the principles of separability, invariance and intervention, CPCA can simulate the causal factors of source and target domains, and make decisions on the target domain based on the causal features, which can observe improved generalization ability. Extensive experiments under three cross-domain tasks indicate that CPCA is remarkably superior to the state-of-the-art methods.","sentences":["Semantic segmentation of high-resolution remote sensing imagery (HRSI) suffers from the domain shift, resulting in poor performance of the model in another unseen domain.","Unsupervised domain adaptive (UDA) semantic segmentation aims to adapt the semantic segmentation model trained on the labeled source domain to an unlabeled target domain.","However, the existing UDA semantic segmentation models tend to align pixels or features based on statistical information related to labels in source and target domain data, and make predictions accordingly, which leads to uncertainty and fragility of prediction results.","In this paper, we propose a causal prototype-inspired contrast adaptation (CPCA) method to explore the invariant causal mechanisms between different HRSIs domains and their semantic labels.","It firstly disentangles causal features and bias features from the source and target domain images through a causal feature disentanglement module.","Then, a causal prototypical contrast module is used to learn domain invariant causal features.","To further de-correlate causal and bias features, a causal intervention module is introduced to intervene on the bias features to generate counterfactual unbiased samples.","By forcing the causal features to meet the principles of separability, invariance and intervention, CPCA can simulate the causal factors of source and target domains, and make decisions on the target domain based on the causal features, which can observe improved generalization ability.","Extensive experiments under three cross-domain tasks indicate that CPCA is remarkably superior to the state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.03704v1","category":"cs.CV"}
{"created":"2024-03-06 13:31:58","title":"Security Testing of RESTful APIs With Test Case Mutation","abstract":"The focus of this paper is on automating the security testing of RESTful APIs. The testing stage of this specific kind of components is often performed manually, and this is yet considered as a long and difficult activity. This paper proposes an automated approach to help developers generate test cases for experimenting with each service in isolation. This approach is based upon the notion of test case mutation, which automatically generates new test cases from an original test case set. Test case mutation operators perform slight test case modifications to mimic possible failures or to test the component under test with new interactions. In this paper, we examine test case mutation operators for RESTful APIs and define 17 operators specialised in security testing. Then, we present our test case mutation algorithm. We evaluate its effectiveness and performance on four web service compositions.","sentences":["The focus of this paper is on automating the security testing of RESTful APIs.","The testing stage of this specific kind of components is often performed manually, and this is yet considered as a long and difficult activity.","This paper proposes an automated approach to help developers generate test cases for experimenting with each service in isolation.","This approach is based upon the notion of test case mutation, which automatically generates new test cases from an original test case set.","Test case mutation operators perform slight test case modifications to mimic possible failures or to test the component under test with new interactions.","In this paper, we examine test case mutation operators for RESTful APIs and define 17 operators specialised in security testing.","Then, we present our test case mutation algorithm.","We evaluate its effectiveness and performance on four web service compositions."],"url":"http://arxiv.org/abs/2403.03701v1","category":"cs.CR"}
{"created":"2024-03-06 13:30:43","title":"K-stability of Fano threefolds of rank 3 and degree 14","abstract":"We prove that all general smooth Fano threefolds of Picard rank $3$ and degree $14$ are K-stable, where the generality condition is stated explicitly.","sentences":["We prove that all general smooth Fano threefolds of Picard rank $3$ and degree $14$ are K-stable, where the generality condition is stated explicitly."],"url":"http://arxiv.org/abs/2403.03700v1","category":"math.AG"}
{"created":"2024-03-06 13:27:34","title":"Towards Controllable Time Series Generation","abstract":"Time Series Generation (TSG) has emerged as a pivotal technique in synthesizing data that accurately mirrors real-world time series, becoming indispensable in numerous applications. Despite significant advancements in TSG, its efficacy frequently hinges on having large training datasets. This dependency presents a substantial challenge in data-scarce scenarios, especially when dealing with rare or unique conditions. To confront these challenges, we explore a new problem of Controllable Time Series Generation (CTSG), aiming to produce synthetic time series that can adapt to various external conditions, thereby tackling the data scarcity issue.   In this paper, we propose \\textbf{C}ontrollable \\textbf{T}ime \\textbf{S}eries (\\textsf{CTS}), an innovative VAE-agnostic framework tailored for CTSG. A key feature of \\textsf{CTS} is that it decouples the mapping process from standard VAE training, enabling precise learning of a complex interplay between latent features and external conditions. Moreover, we develop a comprehensive evaluation scheme for CTSG. Extensive experiments across three real-world time series datasets showcase \\textsf{CTS}'s exceptional capabilities in generating high-quality, controllable outputs. This underscores its adeptness in seamlessly integrating latent features with external conditions. Extending \\textsf{CTS} to the image domain highlights its remarkable potential for explainability and further reinforces its versatility across different modalities.","sentences":["Time Series Generation (TSG) has emerged as a pivotal technique in synthesizing data that accurately mirrors real-world time series, becoming indispensable in numerous applications.","Despite significant advancements in TSG, its efficacy frequently hinges on having large training datasets.","This dependency presents a substantial challenge in data-scarce scenarios, especially when dealing with rare or unique conditions.","To confront these challenges, we explore a new problem of Controllable Time Series Generation (CTSG), aiming to produce synthetic time series that can adapt to various external conditions, thereby tackling the data scarcity issue.   ","In this paper, we propose \\textbf{C}ontrollable \\textbf{T}ime \\textbf{S}eries (\\textsf{CTS}), an innovative VAE-agnostic framework tailored for CTSG.","A key feature of \\textsf{CTS} is that it decouples the mapping process from standard VAE training, enabling precise learning of a complex interplay between latent features and external conditions.","Moreover, we develop a comprehensive evaluation scheme for CTSG.","Extensive experiments across three real-world time series datasets showcase \\textsf{CTS}'s exceptional capabilities in generating high-quality, controllable outputs.","This underscores its adeptness in seamlessly integrating latent features with external conditions.","Extending \\textsf{CTS} to the image domain highlights its remarkable potential for explainability and further reinforces its versatility across different modalities."],"url":"http://arxiv.org/abs/2403.03698v1","category":"cs.LG"}
{"created":"2024-03-06 13:25:41","title":"Largest common subgraph of two forests","abstract":"A common subgraph of two graphs $G_1$ and $G_2$ is a graph that is isomorphic to subgraphs of $G_1$ and $G_2$. In the largest common subgraph problem the task is to determine a common subgraph for two given graphs $G_1$ and $G_2$ that is of maximum possible size ${\\rm lcs}(G_1,G_2)$. This natural problem generalizes the well-studied graph isomorphism problem, has many applications, and remains NP-hard even restricted to unions of paths. We present a simple $4$-approximation algorithm for forests, and, for every fixed $\\epsilon\\in (0,1)$, we show that, for two given forests $F_1$ and $F_2$ of order at most $n$, one can determine in polynomial time a common subgraph $F$ of $F_1$ and $F_2$ with at least ${\\rm lcs}(F_1,F_2)-\\epsilon n$ edges. Restricted to instances with ${\\rm lcs}(F_1,F_2)\\geq cn$ for some fixed positive $c$, this yields a polynomial time approximation scheme. Our approach relies on the approximation of the given forests by structurally simpler forests that are composed of copies of only $O(\\log (n))$ different starlike rooted trees and iterative quantizations of the options for the solutions.","sentences":["A common subgraph of two graphs $G_1$ and $G_2$ is a graph that is isomorphic to subgraphs of $G_1$ and $G_2$. In the largest common subgraph problem the task is to determine a common subgraph for two given graphs $G_1$ and $G_2$ that is of maximum possible size ${\\rm lcs}(G_1,G_2)$.","This natural problem generalizes the well-studied graph isomorphism problem, has many applications, and remains NP-hard even restricted to unions of paths.","We present a simple $4$-approximation algorithm for forests, and, for every fixed $\\epsilon\\in (0,1)$, we show that, for two given forests $F_1$ and $F_2$ of order at most $n$, one can determine in polynomial time a common subgraph $F$ of $F_1$ and $F_2$ with at least ${\\rm lcs}(F_1,F_2)-\\epsilon n$ edges.","Restricted to instances with ${\\rm lcs}(F_1,F_2)\\geq cn$ for some fixed positive $c$, this yields a polynomial time approximation scheme.","Our approach relies on the approximation of the given forests by structurally simpler forests that are composed of copies of only $O(\\log (n))$ different starlike rooted trees and iterative quantizations of the options for the solutions."],"url":"http://arxiv.org/abs/2403.03696v1","category":"cs.DS"}
{"created":"2024-03-06 13:17:41","title":"MolNexTR: A Generalized Deep Learning Model for Molecular Image Recognition","abstract":"In the field of chemical structure recognition, the task of converting molecular images into graph structures and SMILES string stands as a significant challenge, primarily due to the varied drawing styles and conventions prevalent in chemical literature. To bridge this gap, we proposed MolNexTR, a novel image-to-graph deep learning model that collaborates to fuse the strengths of ConvNext, a powerful Convolutional Neural Network variant, and Vision-TRansformer. This integration facilitates a more nuanced extraction of both local and global features from molecular images. MolNexTR can predict atoms and bonds simultaneously and understand their layout rules. It also excels at flexibly integrating symbolic chemistry principles to discern chirality and decipher abbreviated structures. We further incorporate a series of advanced algorithms, including improved data augmentation module, image contamination module, and a post-processing module to get the final SMILES output. These modules synergistically enhance the model's robustness against the diverse styles of molecular imagery found in real literature. In our test sets, MolNexTR has demonstrated superior performance, achieving an accuracy rate of 81-97%, marking a significant advancement in the domain of molecular structure recognition. Scientific contribution: MolNexTR is a novel image-to-graph model that incorporates a unique dual-stream encoder to extract complex molecular image features, and combines chemical rules to predict atoms and bonds while understanding atom and bond layout rules. In addition, it employs a series of novel augmentation algorithms to significantly enhance the robustness and performance of the model.","sentences":["In the field of chemical structure recognition, the task of converting molecular images into graph structures and SMILES string stands as a significant challenge, primarily due to the varied drawing styles and conventions prevalent in chemical literature.","To bridge this gap, we proposed MolNexTR, a novel image-to-graph deep learning model that collaborates to fuse the strengths of ConvNext, a powerful Convolutional Neural Network variant, and Vision-TRansformer.","This integration facilitates a more nuanced extraction of both local and global features from molecular images.","MolNexTR can predict atoms and bonds simultaneously and understand their layout rules.","It also excels at flexibly integrating symbolic chemistry principles to discern chirality and decipher abbreviated structures.","We further incorporate a series of advanced algorithms, including improved data augmentation module, image contamination module, and a post-processing module to get the final SMILES output.","These modules synergistically enhance the model's robustness against the diverse styles of molecular imagery found in real literature.","In our test sets, MolNexTR has demonstrated superior performance, achieving an accuracy rate of 81-97%, marking a significant advancement in the domain of molecular structure recognition.","Scientific contribution: MolNexTR is a novel image-to-graph model that incorporates a unique dual-stream encoder to extract complex molecular image features, and combines chemical rules to predict atoms and bonds while understanding atom and bond layout rules.","In addition, it employs a series of novel augmentation algorithms to significantly enhance the robustness and performance of the model."],"url":"http://arxiv.org/abs/2403.03691v1","category":"cs.CV"}
{"created":"2024-03-06 13:17:07","title":"Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese","abstract":"The creation of instruction data and evaluation benchmarks for serving Large language models often involves enormous human annotation. This issue becomes particularly pronounced when rapidly developing such resources for a non-English language like Japanese. Instead of following the popular practice of directly translating existing English resources into Japanese (e.g., Japanese-Alpaca), we propose an efficient self-instruct method based on GPT-4. We first translate a small amount of English instructions into Japanese and post-edit them to obtain native-level quality. GPT-4 then utilizes them as demonstrations to automatically generate Japanese instruction data. We also construct an evaluation benchmark containing 80 questions across 8 categories, using GPT-4 to automatically assess the response quality of LLMs without human references. The empirical results suggest that the models fine-tuned on our GPT-4 self-instruct data significantly outperformed the Japanese-Alpaca across all three base pre-trained models. Our GPT-4 self-instruct data allowed the LLaMA 13B model to defeat GPT-3.5 (Davinci-003) with a 54.37\\% win-rate. The human evaluation exhibits the consistency between GPT-4's assessments and human preference. Our high-quality instruction data and evaluation benchmark have been released here.","sentences":["The creation of instruction data and evaluation benchmarks for serving Large language models often involves enormous human annotation.","This issue becomes particularly pronounced when rapidly developing such resources for a non-English language like Japanese.","Instead of following the popular practice of directly translating existing English resources into Japanese (e.g., Japanese-Alpaca), we propose an efficient self-instruct method based on GPT-4.","We first translate a small amount of English instructions into Japanese and post-edit them to obtain native-level quality.","GPT-4 then utilizes them as demonstrations to automatically generate Japanese instruction data.","We also construct an evaluation benchmark containing 80 questions across 8 categories, using GPT-4 to automatically assess the response quality of LLMs without human references.","The empirical results suggest that the models fine-tuned on our GPT-4 self-instruct data significantly outperformed the Japanese-Alpaca across all three base pre-trained models.","Our GPT-4 self-instruct data allowed the LLaMA 13B model to defeat GPT-3.5 (Davinci-003) with a 54.37\\% win-rate.","The human evaluation exhibits the consistency between GPT-4's assessments and human preference.","Our high-quality instruction data and evaluation benchmark have been released here."],"url":"http://arxiv.org/abs/2403.03690v1","category":"cs.CL"}
{"created":"2024-03-06 13:15:21","title":"General2Specialized LLMs Translation for E-commerce","abstract":"Existing Neural Machine Translation (NMT) models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents. Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current NMT methods. To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain. Furthermore, we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce. The paradigm can be used for the NMT models based on Large language models (LLMs). Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and robustness of our G2ST approach, as compared with state-of-the-art NMT models such as LLaMA, Qwen, GPT-3.5, and even GPT-4.","sentences":["Existing Neural Machine Translation (NMT) models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents.","Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current NMT methods.","To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain.","Furthermore, we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce.","The paradigm can be used for the NMT models based on Large language models (LLMs).","Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and robustness of our G2ST approach, as compared with state-of-the-art NMT models such as LLaMA, Qwen, GPT-3.5, and even GPT-4."],"url":"http://arxiv.org/abs/2403.03689v1","category":"cs.CL"}
{"created":"2024-03-06 13:14:25","title":"The use of next-generation sequencing in personalized medicine","abstract":"The revolutionary progress in development of next-generation sequencing (NGS) technologies has made it possible to deliver accurate genomic information in a timely manner. Over the past several years, NGS has transformed biomedical and clinical research and found its application in the field of personalized medicine. Here we discuss the rise of personalized medicine and the history of NGS. We discuss current applications and uses of NGS in medicine, including infectious diseases, oncology, genomic medicine, and dermatology. We provide a brief discussion of selected studies where NGS was used to respond to wide variety of questions in biomedical research and clinical medicine. Finally, we discuss the challenges of implementing NGS into routine clinical use.","sentences":["The revolutionary progress in development of next-generation sequencing (NGS) technologies has made it possible to deliver accurate genomic information in a timely manner.","Over the past several years, NGS has transformed biomedical and clinical research and found its application in the field of personalized medicine.","Here we discuss the rise of personalized medicine and the history of NGS.","We discuss current applications and uses of NGS in medicine, including infectious diseases, oncology, genomic medicine, and dermatology.","We provide a brief discussion of selected studies where NGS was used to respond to wide variety of questions in biomedical research and clinical medicine.","Finally, we discuss the challenges of implementing NGS into routine clinical use."],"url":"http://arxiv.org/abs/2403.03688v1","category":"q-bio.GN"}
{"created":"2024-03-06 13:12:21","title":"A methodology for the cross-dock door platforms design under uncertainty","abstract":"The cross dock door design problem consists of deciding on the number and capacity of inbound and outbound doors for receiving product pallets from origin nodes and exiting them to destination nodes. The uncertainty, realized in scenarios, lies in the occurrence of these nodes, the number and cost of the pallets, and the disruption of the capacity of the doors. It is represented using a stochastic two stage binary quadratic model. The first stage decisions are related to the cross dock infrastructure design, and the second stage decisions are related to the node to assignments of the doors. This is the first time, as far as we know, that a stochastic two stage binary quadratic model has been presented for minimizing the construction cost of the infrastructure and its exploitation expected cost in the scenarios. Given the difficulty of solving this combinatorial problem, a mathematically equivalent mixed integer linear formulation is introduced. However, searching an optimal solution is still impractical for commercial solvers. Thus, a scenario cluster decomposition based matheuristic algorithm is introduced to obtain feasible solutions with small optimality gap and reasonable computational effort. A broad study to validate the proposal gives solutions with a much smaller gap than the ones provided by a state of the art general solver. In fact, the proposal provides solutions with a 1 to 5% optimality gap, while the solver does it with up to a 12% gap, if any, and requires a wall time two orders of magnitude higher.","sentences":["The cross dock door design problem consists of deciding on the number and capacity of inbound and outbound doors for receiving product pallets from origin nodes and exiting them to destination nodes.","The uncertainty, realized in scenarios, lies in the occurrence of these nodes, the number and cost of the pallets, and the disruption of the capacity of the doors.","It is represented using a stochastic two stage binary quadratic model.","The first stage decisions are related to the cross dock infrastructure design, and the second stage decisions are related to the node to assignments of the doors.","This is the first time, as far as we know, that a stochastic two stage binary quadratic model has been presented for minimizing the construction cost of the infrastructure and its exploitation expected cost in the scenarios.","Given the difficulty of solving this combinatorial problem, a mathematically equivalent mixed integer linear formulation is introduced.","However, searching an optimal solution is still impractical for commercial solvers.","Thus, a scenario cluster decomposition based matheuristic algorithm is introduced to obtain feasible solutions with small optimality gap and reasonable computational effort.","A broad study to validate the proposal gives solutions with a much smaller gap than the ones provided by a state of the art general solver.","In fact, the proposal provides solutions with a 1 to 5% optimality gap, while the solver does it with up to a 12% gap, if any, and requires a wall time two orders of magnitude higher."],"url":"http://arxiv.org/abs/2403.03686v1","category":"math.OC"}
{"created":"2024-03-06 13:09:40","title":"Quantifying Media Influence on Covid-19 Mask-Wearing Beliefs","abstract":"How political beliefs change in accordance with media exposure is a complicated matter. Some studies have been able to demonstrate that groups with different media diets in the aggregate (e.g., U.S. media consumers ingesting partisan news) arrive at different beliefs about policy issues, but proving this from data at a granular level -- at the level of attitudes expressed in news stories -- remains difficult. In contrast to existing opinion formation models that describe granular detail but are not data-driven, or data-driven studies that rely on simple keyword detection and miss linguistic nuances, being able to identify complicated attitudes in news text and use this data to drive models would enable more nuanced empirical study of opinion formation from media messaging. This study contributes a dataset as well as an analysis that allows the mapping of attitudes from individual news stories to aggregate changes of opinion over time for an important public health topic where opinion differed in the U.S. by partisan media diet: Covid mask-wearing beliefs. By gathering a dataset of U.S. news media stories, from April 6 to June 8, 2020, annotated according to Howard 2020's Face Mask Perception Scale for their statements regarding Covid-19 mask-wearing, we demonstrate fine-grained correlations between media messaging and empirical opinion polling data from a Gallup survey conducted during the same period. We also demonstrate that the data can be used for quantitative analysis of pro- and anti-mask sentiment throughout the period, identifying major events that drove opinion changes. This dataset is made publicly available and can be used by other researchers seeking to evaluate how mask-wearing attitudes were driven by news media content. Additionally, we hope that its general method can be used to enable other media researchers to conduct more detailed analyses of media effects on opinion.","sentences":["How political beliefs change in accordance with media exposure is a complicated matter.","Some studies have been able to demonstrate that groups with different media diets in the aggregate (e.g., U.S. media consumers ingesting partisan news) arrive at different beliefs about policy issues, but proving this from data at a granular level -- at the level of attitudes expressed in news stories -- remains difficult.","In contrast to existing opinion formation models that describe granular detail but are not data-driven, or data-driven studies that rely on simple keyword detection and miss linguistic nuances, being able to identify complicated attitudes in news text and use this data to drive models would enable more nuanced empirical study of opinion formation from media messaging.","This study contributes a dataset as well as an analysis that allows the mapping of attitudes from individual news stories to aggregate changes of opinion over time for an important public health topic where opinion differed in the U.S. by partisan media diet:","Covid mask-wearing beliefs.","By gathering a dataset of U.S. news media stories, from April 6 to June 8, 2020, annotated according to Howard 2020's Face Mask Perception Scale for their statements regarding Covid-19 mask-wearing, we demonstrate fine-grained correlations between media messaging and empirical opinion polling data from a Gallup survey conducted during the same period.","We also demonstrate that the data can be used for quantitative analysis of pro- and anti-mask sentiment throughout the period, identifying major events that drove opinion changes.","This dataset is made publicly available and can be used by other researchers seeking to evaluate how mask-wearing attitudes were driven by news media content.","Additionally, we hope that its general method can be used to enable other media researchers to conduct more detailed analyses of media effects on opinion."],"url":"http://arxiv.org/abs/2403.03684v1","category":"cs.SI"}
{"created":"2024-03-06 12:58:25","title":"Automatic Bi-modal Question Title Generation for Stack Overflow with Prompt Learning","abstract":"When drafting question posts for Stack Overflow, developers may not accurately summarize the core problems in the question titles, which can cause these questions to not get timely help. Therefore, improving the quality of question titles has attracted the wide attention of researchers. An initial study aimed to automatically generate the titles by only analyzing the code snippets in the question body. However, this study ignored the helpful information in their corresponding problem descriptions. Therefore, we propose an approach SOTitle+ by considering bi-modal information (i.e., the code snippets and the problem descriptions) in the question body. Then we formalize the title generation for different programming languages as separate but related tasks and utilize multi-task learning to solve these tasks. Later we fine-tune the pre-trained language model CodeT5 to automatically generate the titles. Unfortunately, the inconsistent inputs and optimization objectives between the pre-training task and our investigated task may make fine-tuning hard to fully explore the knowledge of the pre-trained model. To solve this issue, SOTitle+ further prompt-tunes CodeT5 with hybrid prompts (i.e., mixture of hard and soft prompts). To verify the effectiveness of SOTitle+, we construct a large-scale high-quality corpus from recent data dumps shared by Stack Overflow. Our corpus includes 179,119 high-quality question posts for six popular programming languages. Experimental results show that SOTitle+ can significantly outperform four state-of-the-art baselines in both automatic evaluation and human evaluation. Our work indicates that considering bi-modal information and prompt learning in Stack Overflow title generation is a promising exploration direction.","sentences":["When drafting question posts for Stack Overflow, developers may not accurately summarize the core problems in the question titles, which can cause these questions to not get timely help.","Therefore, improving the quality of question titles has attracted the wide attention of researchers.","An initial study aimed to automatically generate the titles by only analyzing the code snippets in the question body.","However, this study ignored the helpful information in their corresponding problem descriptions.","Therefore, we propose an approach SOTitle+ by considering bi-modal information (i.e., the code snippets and the problem descriptions) in the question body.","Then we formalize the title generation for different programming languages as separate but related tasks and utilize multi-task learning to solve these tasks.","Later we fine-tune the pre-trained language model CodeT5 to automatically generate the titles.","Unfortunately, the inconsistent inputs and optimization objectives between the pre-training task and our investigated task may make fine-tuning hard to fully explore the knowledge of the pre-trained model.","To solve this issue, SOTitle+ further prompt-tunes CodeT5 with hybrid prompts (i.e., mixture of hard and soft prompts).","To verify the effectiveness of SOTitle+, we construct a large-scale high-quality corpus from recent data dumps shared by Stack Overflow.","Our corpus includes 179,119 high-quality question posts for six popular programming languages.","Experimental results show that SOTitle+ can significantly outperform four state-of-the-art baselines in both automatic evaluation and human evaluation.","Our work indicates that considering bi-modal information and prompt learning in Stack Overflow title generation is a promising exploration direction."],"url":"http://arxiv.org/abs/2403.03677v1","category":"cs.SE"}
{"created":"2024-03-06 12:57:48","title":"Simplified PCNet with Robustness","abstract":"Graph Neural Networks (GNNs) have garnered significant attention for their success in learning the representation of homophilic or heterophilic graphs. However, they cannot generalize well to real-world graphs with different levels of homophily. In response, the Possion-Charlier Network (PCNet) \\cite{li2024pc}, the previous work, allows graph representation to be learned from heterophily to homophily. Although PCNet alleviates the heterophily issue, there remain some challenges in further improving the efficacy and efficiency. In this paper, we simplify PCNet and enhance its robustness. We first extend the filter order to continuous values and reduce its parameters. Two variants with adaptive neighborhood sizes are implemented. Theoretical analysis shows our model's robustness to graph structure perturbations or adversarial attacks. We validate our approach through semi-supervised learning tasks on various datasets representing both homophilic and heterophilic graphs.","sentences":["Graph Neural Networks (GNNs) have garnered significant attention for their success in learning the representation of homophilic or heterophilic graphs.","However, they cannot generalize well to real-world graphs with different levels of homophily.","In response, the Possion-Charlier Network (PCNet) \\cite{li2024pc}, the previous work, allows graph representation to be learned from heterophily to homophily.","Although PCNet alleviates the heterophily issue, there remain some challenges in further improving the efficacy and efficiency.","In this paper, we simplify PCNet and enhance its robustness.","We first extend the filter order to continuous values and reduce its parameters.","Two variants with adaptive neighborhood sizes are implemented.","Theoretical analysis shows our model's robustness to graph structure perturbations or adversarial attacks.","We validate our approach through semi-supervised learning tasks on various datasets representing both homophilic and heterophilic graphs."],"url":"http://arxiv.org/abs/2403.03676v1","category":"cs.LG"}
{"created":"2024-03-06 12:49:08","title":"Learning Adversarial MDPs with Stochastic Hard Constraints","abstract":"We study online learning problems in constrained Markov decision processes (CMDPs) with adversarial losses and stochastic hard constraints. We consider two different scenarios. In the first one, we address general CMDPs, where we design an algorithm that attains sublinear regret and cumulative positive constraints violation. In the second scenario, under the mild assumption that a policy strictly satisfying the constraints exists and is known to the learner, we design an algorithm that achieves sublinear regret while ensuring that the constraints are satisfied at every episode with high probability. To the best of our knowledge, our work is the first to study CMDPs involving both adversarial losses and hard constraints. Indeed, previous works either focus on much weaker soft constraints--allowing for positive violation to cancel out negative ones--or are restricted to stochastic losses. Thus, our algorithms can deal with general non-stationary environments subject to requirements much stricter than those manageable with state-of-the-art algorithms. This enables their adoption in a much wider range of real-world applications, ranging from autonomous driving to online advertising and recommender systems.","sentences":["We study online learning problems in constrained Markov decision processes (CMDPs) with adversarial losses and stochastic hard constraints.","We consider two different scenarios.","In the first one, we address general CMDPs, where we design an algorithm that attains sublinear regret and cumulative positive constraints violation.","In the second scenario, under the mild assumption that a policy strictly satisfying the constraints exists and is known to the learner, we design an algorithm that achieves sublinear regret while ensuring that the constraints are satisfied at every episode with high probability.","To the best of our knowledge, our work is the first to study CMDPs involving both adversarial losses and hard constraints.","Indeed, previous works either focus on much weaker soft constraints--allowing for positive violation to cancel out negative ones--or are restricted to stochastic losses.","Thus, our algorithms can deal with general non-stationary environments subject to requirements much stricter than those manageable with state-of-the-art algorithms.","This enables their adoption in a much wider range of real-world applications, ranging from autonomous driving to online advertising and recommender systems."],"url":"http://arxiv.org/abs/2403.03672v1","category":"cs.LG"}
{"created":"2024-03-06 12:43:53","title":"Spectral Algorithms on Manifolds through Diffusion","abstract":"The existing research on spectral algorithms, applied within a Reproducing Kernel Hilbert Space (RKHS), has primarily focused on general kernel functions, often neglecting the inherent structure of the input feature space. Our paper introduces a new perspective, asserting that input data are situated within a low-dimensional manifold embedded in a higher-dimensional Euclidean space. We study the convergence performance of spectral algorithms in the RKHSs, specifically those generated by the heat kernels, known as diffusion spaces. Incorporating the manifold structure of the input, we employ integral operator techniques to derive tight convergence upper bounds concerning generalized norms, which indicates that the estimators converge to the target function in strong sense, entailing the simultaneous convergence of the function itself and its derivatives. These bounds offer two significant advantages: firstly, they are exclusively contingent on the intrinsic dimension of the input manifolds, thereby providing a more focused analysis. Secondly, they enable the efficient derivation of convergence rates for derivatives of any k-th order, all of which can be accomplished within the ambit of the same spectral algorithms. Furthermore, we establish minimax lower bounds to demonstrate the asymptotic optimality of these conclusions in specific contexts. Our study confirms that the spectral algorithms are practically significant in the broader context of high-dimensional approximation.","sentences":["The existing research on spectral algorithms, applied within a Reproducing Kernel Hilbert Space (RKHS), has primarily focused on general kernel functions, often neglecting the inherent structure of the input feature space.","Our paper introduces a new perspective, asserting that input data are situated within a low-dimensional manifold embedded in a higher-dimensional Euclidean space.","We study the convergence performance of spectral algorithms in the RKHSs, specifically those generated by the heat kernels, known as diffusion spaces.","Incorporating the manifold structure of the input, we employ integral operator techniques to derive tight convergence upper bounds concerning generalized norms, which indicates that the estimators converge to the target function in strong sense, entailing the simultaneous convergence of the function itself and its derivatives.","These bounds offer two significant advantages: firstly, they are exclusively contingent on the intrinsic dimension of the input manifolds, thereby providing a more focused analysis.","Secondly, they enable the efficient derivation of convergence rates for derivatives of any k-th order, all of which can be accomplished within the ambit of the same spectral algorithms.","Furthermore, we establish minimax lower bounds to demonstrate the asymptotic optimality of these conclusions in specific contexts.","Our study confirms that the spectral algorithms are practically significant in the broader context of high-dimensional approximation."],"url":"http://arxiv.org/abs/2403.03669v1","category":"stat.ML"}
{"created":"2024-03-06 12:39:30","title":"Random covariant quantum channels","abstract":"The group symmetries inherent in quantum channels often make them tractable and applicable to various problems in quantum information theory. In this paper, we introduce natural probability distributions for covariant quantum channels. Specifically, this is achieved through the application of \"twirling operations\" on random quantum channels derived from the Stinespring representation that use Haar-distributed random isometries. We explore various types of group symmetries, including unitary and orthogonal covariance, hyperoctahedral covariance, diagonal orthogonal covariance (DOC), and analyze their properties related to quantum entanglement based on the model parameters. In particular, we discuss the threshold phenomenon for positive partial transpose and entanglement breaking properties, comparing thresholds among different classes of random covariant channels. Finally, we contribute to the PPT$^2$ conjecture by showing that the composition between two random DOC channels is generically entanglement breaking.","sentences":["The group symmetries inherent in quantum channels often make them tractable and applicable to various problems in quantum information theory.","In this paper, we introduce natural probability distributions for covariant quantum channels.","Specifically, this is achieved through the application of \"twirling operations\" on random quantum channels derived from the Stinespring representation that use Haar-distributed random isometries.","We explore various types of group symmetries, including unitary and orthogonal covariance, hyperoctahedral covariance, diagonal orthogonal covariance (DOC), and analyze their properties related to quantum entanglement based on the model parameters.","In particular, we discuss the threshold phenomenon for positive partial transpose and entanglement breaking properties, comparing thresholds among different classes of random covariant channels.","Finally, we contribute to the PPT$^2$ conjecture by showing that the composition between two random DOC channels is generically entanglement breaking."],"url":"http://arxiv.org/abs/2403.03667v1","category":"quant-ph"}
{"created":"2024-03-06 12:31:02","title":"Harnessing Meta-Learning for Improving Full-Frame Video Stabilization","abstract":"Video stabilization is a longstanding computer vision problem, particularly pixel-level synthesis solutions for video stabilization which synthesize full frames add to the complexity of this task. These techniques aim to stabilize videos by synthesizing full frames while enhancing the stability of the considered video. This intensifies the complexity of the task due to the distinct mix of unique motion profiles and visual content present in each video sequence, making robust generalization with fixed parameters difficult. In our study, we introduce a novel approach to enhance the performance of pixel-level synthesis solutions for video stabilization by adapting these models to individual input video sequences. The proposed adaptation exploits low-level visual cues accessible during test-time to improve both the stability and quality of resulting videos. We highlight the efficacy of our methodology of \"test-time adaptation\" through simple fine-tuning of one of these models, followed by significant stability gain via the integration of meta-learning techniques. Notably, significant improvement is achieved with only a single adaptation step. The versatility of the proposed algorithm is demonstrated by consistently improving the performance of various pixel-level synthesis models for video stabilization in real-world scenarios.","sentences":["Video stabilization is a longstanding computer vision problem, particularly pixel-level synthesis solutions for video stabilization which synthesize full frames add to the complexity of this task.","These techniques aim to stabilize videos by synthesizing full frames while enhancing the stability of the considered video.","This intensifies the complexity of the task due to the distinct mix of unique motion profiles and visual content present in each video sequence, making robust generalization with fixed parameters difficult.","In our study, we introduce a novel approach to enhance the performance of pixel-level synthesis solutions for video stabilization by adapting these models to individual input video sequences.","The proposed adaptation exploits low-level visual cues accessible during test-time to improve both the stability and quality of resulting videos.","We highlight the efficacy of our methodology of \"test-time adaptation\" through simple fine-tuning of one of these models, followed by significant stability gain via the integration of meta-learning techniques.","Notably, significant improvement is achieved with only a single adaptation step.","The versatility of the proposed algorithm is demonstrated by consistently improving the performance of various pixel-level synthesis models for video stabilization in real-world scenarios."],"url":"http://arxiv.org/abs/2403.03662v1","category":"cs.CV"}
{"created":"2024-03-06 12:29:47","title":"Development and evaluation of Artificial Intelligence techniques for IoT data quality assessment and curation","abstract":"Nowadays, data is becoming the new fuel for economic wealth and creation of novel and profitable business models. Multitude of technologies are contributing to an abundance of information sources which are already the baseline for multi-millionaire services and applications. Internet of Things (IoT), is probably the most representative one. However, for an economy of data to actually flourish there are still several critical challenges that have to be overcome. Among them, data quality can become an issue when data come from heterogeneous sources or have different formats, standards and scale. Improving data quality is of utmost importance for any domain since data are the basis for any decision-making system and decisions will not be accurate if they are based on inadequate low-quality data. In this paper we are presenting a solution for assessing several quality dimensions of IoT data streams as they are generated. Additionally, the solution described in the paper actually improves the quality of data streams by curating them through the application of Artificial Intelligence techniques. The approach followed in our work has been to append data quality information as metadata linked to each individual piece of curated data. We have leveraged linked-data principles and integrated the developed AI-based IoT data curation mechanisms within a Data Enrichment Toolchain (DET) that employs the NGSI-LD standard to harmonize and enrich heterogeneous data sources. Furthermore, we have evaluated our design under experimental research conditions, achieving a robust compromise between functionality and overhead. Besides, it demonstrates a stable and scalable performance.","sentences":["Nowadays, data is becoming the new fuel for economic wealth and creation of novel and profitable business models.","Multitude of technologies are contributing to an abundance of information sources which are already the baseline for multi-millionaire services and applications.","Internet of Things (IoT), is probably the most representative one.","However, for an economy of data to actually flourish there are still several critical challenges that have to be overcome.","Among them, data quality can become an issue when data come from heterogeneous sources or have different formats, standards and scale.","Improving data quality is of utmost importance for any domain since data are the basis for any decision-making system and decisions will not be accurate if they are based on inadequate low-quality data.","In this paper we are presenting a solution for assessing several quality dimensions of IoT data streams as they are generated.","Additionally, the solution described in the paper actually improves the quality of data streams by curating them through the application of Artificial Intelligence techniques.","The approach followed in our work has been to append data quality information as metadata linked to each individual piece of curated data.","We have leveraged linked-data principles and integrated the developed AI-based IoT data curation mechanisms within a Data Enrichment Toolchain (DET) that employs the NGSI-LD standard to harmonize and enrich heterogeneous data sources.","Furthermore, we have evaluated our design under experimental research conditions, achieving a robust compromise between functionality and overhead.","Besides, it demonstrates a stable and scalable performance."],"url":"http://arxiv.org/abs/2403.03661v1","category":"cs.DB"}
{"created":"2024-03-06 12:29:36","title":"$C$-Width of Graph of Groups","abstract":"In this paper, we study the $C$-width of HNN extension of a group via its proper isomorphic subgroups and amalgamated free product of two groups via their proper isomorphic subgroups with respect to conjugation invariant generating set. We will also establish that infinite one relator group has infinite $C$-width.","sentences":["In this paper, we study the $C$-width of HNN extension of a group via its proper isomorphic subgroups and amalgamated free product of two groups via their proper isomorphic subgroups with respect to conjugation invariant generating set.","We will also establish that infinite one relator group has infinite $C$-width."],"url":"http://arxiv.org/abs/2403.03660v1","category":"math.GR"}
{"created":"2024-03-06 12:28:14","title":"Finite elements for Mat\u00e9rn-type random fields: Uncertainty in computational mechanics and design optimization","abstract":"This work highlights an approach for incorporating realistic uncertainties into scientific computing workflows based on finite elements, focusing on applications in computational mechanics and design optimization. We leverage Mat\\'ern-type Gaussian random fields (GRFs) generated using the SPDE method to model aleatoric uncertainties, including environmental influences, variating material properties, and geometric ambiguities. Our focus lies on delivering practical GRF realizations that accurately capture imperfections and variations and understanding how they impact the predictions of computational models and the topology of optimized designs. We describe a numerical algorithm based on solving a generalized SPDE to sample GRFs on arbitrary meshed domains. The algorithm leverages established techniques and integrates seamlessly with the open-source finite element library MFEM and associated scientific computing workflows, like those found in industrial and national laboratory settings. Our solver scales efficiently for large-scale problems and supports various domain types, including surfaces and embedded manifolds. We showcase its versatility through biomechanics and topology optimization applications. The flexibility and efficiency of SPDE-based GRF generation empower us to run large-scale optimization problems on 2D and 3D domains, including finding optimized designs on embedded surfaces, and to generate topologies beyond the reach of conventional techniques. Moreover, these capabilities allow us to model geometric uncertainties of reconstructed submanifolds, such as the surfaces of cerebral aneurysms. In addition to offering benefits in these specific domains, the proposed techniques transcend specific applications and generalize to arbitrary forward and backward problems in uncertainty quantification involving finite elements.","sentences":["This work highlights an approach for incorporating realistic uncertainties into scientific computing workflows based on finite elements, focusing on applications in computational mechanics and design optimization.","We leverage Mat\\'ern-type Gaussian random fields (GRFs) generated using the SPDE method to model aleatoric uncertainties, including environmental influences, variating material properties, and geometric ambiguities.","Our focus lies on delivering practical GRF realizations that accurately capture imperfections and variations and understanding how they impact the predictions of computational models and the topology of optimized designs.","We describe a numerical algorithm based on solving a generalized SPDE to sample GRFs on arbitrary meshed domains.","The algorithm leverages established techniques and integrates seamlessly with the open-source finite element library MFEM and associated scientific computing workflows, like those found in industrial and national laboratory settings.","Our solver scales efficiently for large-scale problems and supports various domain types, including surfaces and embedded manifolds.","We showcase its versatility through biomechanics and topology optimization applications.","The flexibility and efficiency of SPDE-based GRF generation empower us to run large-scale optimization problems on 2D and 3D domains, including finding optimized designs on embedded surfaces, and to generate topologies beyond the reach of conventional techniques.","Moreover, these capabilities allow us to model geometric uncertainties of reconstructed submanifolds, such as the surfaces of cerebral aneurysms.","In addition to offering benefits in these specific domains, the proposed techniques transcend specific applications and generalize to arbitrary forward and backward problems in uncertainty quantification involving finite elements."],"url":"http://arxiv.org/abs/2403.03658v1","category":"cs.CE"}
{"created":"2024-03-06 12:27:16","title":"A practical and efficient approach for Bayesian reservoir inversion: Insights from the Alvheim field data","abstract":"Stochastic reservoir characterization, a critical aspect of subsurface exploration for oil and gas reservoirs, relies on stochastic methods to model and understand subsurface properties using seismic data. This paper addresses the computational challenges associated with Bayesian reservoir inversion methods, focusing on two key obstacles: the demanding forward model and the high dimensionality of Gaussian random fields. Leveraging the generalized Bayesian approach, we replace the intricate forward function with a computationally efficient multivariate adaptive regression splines method, resulting in a 34 acceleration in computational efficiency. For handling high-dimensional Gaussian random fields, we employ a fast Fourier transform (FFT) technique. Additionally, we explore the preconditioned Crank-Nicolson method for sampling, providing a more efficient exploration of high-dimensional parameter spaces. The practicality and efficacy of our approach are tested extensively in simulations and its validity is demonstrated in application to the Alvheim field data.","sentences":["Stochastic reservoir characterization, a critical aspect of subsurface exploration for oil and gas reservoirs, relies on stochastic methods to model and understand subsurface properties using seismic data.","This paper addresses the computational challenges associated with Bayesian reservoir inversion methods, focusing on two key obstacles: the demanding forward model and the high dimensionality of Gaussian random fields.","Leveraging the generalized Bayesian approach, we replace the intricate forward function with a computationally efficient multivariate adaptive regression splines method, resulting in a 34 acceleration in computational efficiency.","For handling high-dimensional Gaussian random fields, we employ a fast Fourier transform (FFT) technique.","Additionally, we explore the preconditioned Crank-Nicolson method for sampling, providing a more efficient exploration of high-dimensional parameter spaces.","The practicality and efficacy of our approach are tested extensively in simulations and its validity is demonstrated in application to the Alvheim field data."],"url":"http://arxiv.org/abs/2403.03656v1","category":"stat.AP"}
{"created":"2024-03-06 12:26:04","title":"Kronos: A Robust Sharding Blockchain Consensus with Optimal Communication Overhead","abstract":"Sharding enhances blockchain scalability by dividing the network into shards, each managing specific unspent transaction outputs or accounts. Cross-shard transactions pose a critical challenge to the security and efficiency of sharding blockchains. Current solutions, however, either prioritize security with assumptions and substantial investments, or focus on reducing overhead and overlooking security considerations.   In this paper, we present Kronos, a generic and efficient sharding blockchain consensus ensuring robust security. We introduce a buffer mechanism for atomic cross-shard transaction processing. Shard members collectively maintain a buffer to manage cross-shard inputs, ensuring that a transaction is committed only if all inputs are available, and no fund is transferred for invalid requests. While ensuring security, Kronos processes transactions with optimal intra-shard communication overhead. Additionally, we propose a reduction for transaction invalidity proof generation to simple and fast multicasting, leading to atomic rejection without executing full-fledged Byzantine fault tolerance protocol in optimistic scenarios. Moreover, Kronos adopts a newly designed batch mechanism, reducing inter-shard message complexity to $O((m$log$m/b)\\lambda)$.   Kronos operates without dependence on any time or client honesty assumption, serving as a plug-in sharding blockchain consensus supporting applications in diverse network environments including asynchronous ones. We implement Kronos using two prominent BFT protocols: Speeding Dumbo and HotStuff. Extensive experiments demonstrate Kronos achieving a substantial throughput of 68.6ktx/sec with 1.7sec latency. Compared with state-of-the-art solutions, Kronos outperforms in all cases, achieving up to a 42x improvement in throughput and a 50% reduction in latency when cross-shard transactions dominate the workload.","sentences":["Sharding enhances blockchain scalability by dividing the network into shards, each managing specific unspent transaction outputs or accounts.","Cross-shard transactions pose a critical challenge to the security and efficiency of sharding blockchains.","Current solutions, however, either prioritize security with assumptions and substantial investments, or focus on reducing overhead and overlooking security considerations.   ","In this paper, we present Kronos, a generic and efficient sharding blockchain consensus ensuring robust security.","We introduce a buffer mechanism for atomic cross-shard transaction processing.","Shard members collectively maintain a buffer to manage cross-shard inputs, ensuring that a transaction is committed only if all inputs are available, and no fund is transferred for invalid requests.","While ensuring security, Kronos processes transactions with optimal intra-shard communication overhead.","Additionally, we propose a reduction for transaction invalidity proof generation to simple and fast multicasting, leading to atomic rejection without executing full-fledged Byzantine fault tolerance protocol in optimistic scenarios.","Moreover, Kronos adopts a newly designed batch mechanism, reducing inter-shard message complexity to $O((m$log$m/b)\\lambda)$.   Kronos operates without dependence on any time or client honesty assumption, serving as a plug-in sharding blockchain consensus supporting applications in diverse network environments including asynchronous ones.","We implement Kronos using two prominent BFT protocols: Speeding Dumbo and HotStuff.","Extensive experiments demonstrate Kronos achieving a substantial throughput of 68.6ktx/sec with 1.7sec latency.","Compared with state-of-the-art solutions, Kronos outperforms in all cases, achieving up to a 42x improvement in throughput and a 50% reduction in latency when cross-shard transactions dominate the workload."],"url":"http://arxiv.org/abs/2403.03655v1","category":"cs.CR"}
{"created":"2024-03-06 12:15:49","title":"The Cost of Coming Out","abstract":"The fear of social stigma and discrimination leads many individuals worldwide to hesitate in openly disclosing their sexual orientation. Due to the large costs of concealing identity, it is crucial to understand the extent of anti-LGB sentiments and reactions to coming out. However, disclosing one's sexual orientation is a personal choice, complicating data access and introducing endogeneity issues. This paper tackles these challenges by using an innovative data source from a popular online video game together with a natural experiment. We exploit exogenous variation in the identity of a playable character to identify the effects of disclosure on players' revealed preferences for that character. Leveraging detailed daily data, we monitor players' preferences for the character across diverse regions globally and employ synthetic control methods to isolate the effect of the disclosure on players' preferences. Our findings reveal a substantial and persistent negative impact of coming out. To strengthen the plausibility of social stigma as the primary explanation for the estimated effects, we systematically address and eliminate several alternative game-related channels.","sentences":["The fear of social stigma and discrimination leads many individuals worldwide to hesitate in openly disclosing their sexual orientation.","Due to the large costs of concealing identity, it is crucial to understand the extent of anti-LGB sentiments and reactions to coming out.","However, disclosing one's sexual orientation is a personal choice, complicating data access and introducing endogeneity issues.","This paper tackles these challenges by using an innovative data source from a popular online video game together with a natural experiment.","We exploit exogenous variation in the identity of a playable character to identify the effects of disclosure on players' revealed preferences for that character.","Leveraging detailed daily data, we monitor players' preferences for the character across diverse regions globally and employ synthetic control methods to isolate the effect of the disclosure on players' preferences.","Our findings reveal a substantial and persistent negative impact of coming out.","To strengthen the plausibility of social stigma as the primary explanation for the estimated effects, we systematically address and eliminate several alternative game-related channels."],"url":"http://arxiv.org/abs/2403.03649v1","category":"econ.GN"}
{"created":"2024-03-06 12:13:41","title":"A Connector for Integrating NGSI-LD Data into Open Data Portals","abstract":"Nowadays, there are plenty of data sources generating massive amounts of information that, combined with novel data analytics frameworks, are meant to support optimisation in many application domains. Nonetheless, there are still shortcomings in terms of data discoverability, accessibility and interoperability. Open Data portals have emerged as a shift towards openness and discoverability. However, they do not impose any condition to the data itself, just stipulate how datasets have to be described. Alternatively, the NGSI-LD standard pursues harmonisation in terms of data modelling and accessibility. This paper presents a solution that bridges these two domains (i.e., Open Data portals and NGSI-LD-based data) in order to keep benefiting from the structured description of datasets offered by Open Data portals, while ensuring the interoperability provided by the NGSI-LD standard. Our solution aggregates the data into coherent datasets and generate high-quality descriptions, ensuring comprehensiveness, interoperability and accessibility. The proposed solution has been validated through a real-world implementation that exposes IoT data in NGSI-LD format through the European Data Portal (EDP). Moreover, the results from the Metadata Quality Assessment that the EDP implements, show that the datasets' descriptions generated achieve excellent ranking in terms of the Findability, Accessibility, Interoperability and Reusability (FAIR) data principles.","sentences":["Nowadays, there are plenty of data sources generating massive amounts of information that, combined with novel data analytics frameworks, are meant to support optimisation in many application domains.","Nonetheless, there are still shortcomings in terms of data discoverability, accessibility and interoperability.","Open Data portals have emerged as a shift towards openness and discoverability.","However, they do not impose any condition to the data itself, just stipulate how datasets have to be described.","Alternatively, the NGSI-LD standard pursues harmonisation in terms of data modelling and accessibility.","This paper presents a solution that bridges these two domains (i.e., Open Data portals and NGSI-LD-based data) in order to keep benefiting from the structured description of datasets offered by Open Data portals, while ensuring the interoperability provided by the NGSI-LD standard.","Our solution aggregates the data into coherent datasets and generate high-quality descriptions, ensuring comprehensiveness, interoperability and accessibility.","The proposed solution has been validated through a real-world implementation that exposes IoT data in NGSI-LD format through the European Data Portal (EDP).","Moreover, the results from the Metadata Quality Assessment that the EDP implements, show that the datasets' descriptions generated achieve excellent ranking in terms of the Findability, Accessibility, Interoperability and Reusability (FAIR) data principles."],"url":"http://arxiv.org/abs/2403.03648v1","category":"cs.DB"}
{"created":"2024-03-06 12:13:01","title":"The elementary theory of the 2-category of small categories","abstract":"We give an elementary description of $2$-categories $\\mathbf{Cat}\\left(\\mathcal{E}\\right)$ of internal categories, functors and natural transformations, where $\\mathcal{E}$ is a category modelling Lawvere's elementary theory of the category of sets (ETCS). This extends Bourke's characterisation of $2$-categories $\\mathbf{Cat}\\left(\\mathcal{E}\\right)$ where $\\mathcal{E}$ has pullbacks to take account for the extra properties in ETCS, and Lawvere's characterisation of the (one dimensional) category of small categories to take account of the two-dimensional structure. Important two-dimensional concepts which we introduce include $2$-well-pointedness, full-subobject classifiers, and the categorified axiom of choice. Along the way, we show how generating families (resp. orthogonal factorisation systems) on $\\mathcal{E}$ give rise to generating families (resp. orthogonal factorisation systems) on $\\mathbf{Cat}\\left(\\mathcal{E}\\right)_{1}$, results which we believe are of independent interest.","sentences":["We give an elementary description of $2$-categories $\\mathbf{Cat}\\left(\\mathcal{E}\\right)$ of internal categories, functors and natural transformations, where $\\mathcal{E}$ is a category modelling Lawvere's elementary theory of the category of sets (ETCS).","This extends Bourke's characterisation of $2$-categories $\\mathbf{Cat}\\left(\\mathcal{E}\\right)$ where $\\mathcal{E}$ has pullbacks to take account for the extra properties in ETCS, and Lawvere's characterisation of the (one dimensional) category of small categories to take account of the two-dimensional structure.","Important two-dimensional concepts which we introduce include $2$-well-pointedness, full-subobject classifiers, and the categorified axiom of choice.","Along the way, we show how generating families (resp.","orthogonal factorisation systems) on $\\mathcal{E}$ give rise to generating families (resp.","orthogonal factorisation systems) on $\\mathbf{Cat}\\left(\\mathcal{E}\\right)_{1}$, results which we believe are of independent interest."],"url":"http://arxiv.org/abs/2403.03647v1","category":"math.CT"}
{"created":"2024-03-06 12:10:48","title":"Bayesian Generalized Distributed Lag Regression with Variable Selection","abstract":"Distributed Lag Models (DLMs) and similar regression approaches such as MIDAS have been used for many decades in econometrics, and more recently in the study of air quality and its impact on human health. They are useful not only for quantifying accumulating and delayed effects, but also for estimating the lags that are most susceptible to these effects. Among other things, they have been used to infer the period of exposure to poor air quality which might negatively impact child birth weight. The increased attention DLMs have received in recent years is reflective of their potential to help us understand a great many issues, particularly in the investigation of how the environment affects human health. In this paper we describe how to expand the utility of these models for Bayesian inference by leveraging latent-variables. In particular we explain how to perform binary regression to better handle imbalanced data, how to incorporate negative binomial regression, and how to estimate the probability of predictor inclusion. Extra parameters introduced through the DLM framework may require calibration for the MCMC algorithm, but this will not be the case in DLM-based analyses often seen in pollution exposure literature. In these cases, the parameters are inferred through a fully automatic Gibbs sampling procedure.","sentences":["Distributed Lag Models (DLMs) and similar regression approaches such as MIDAS have been used for many decades in econometrics, and more recently in the study of air quality and its impact on human health.","They are useful not only for quantifying accumulating and delayed effects, but also for estimating the lags that are most susceptible to these effects.","Among other things, they have been used to infer the period of exposure to poor air quality which might negatively impact child birth weight.","The increased attention DLMs have received in recent years is reflective of their potential to help us understand a great many issues, particularly in the investigation of how the environment affects human health.","In this paper we describe how to expand the utility of these models for Bayesian inference by leveraging latent-variables.","In particular we explain how to perform binary regression to better handle imbalanced data, how to incorporate negative binomial regression, and how to estimate the probability of predictor inclusion.","Extra parameters introduced through the DLM framework may require calibration for the MCMC algorithm, but this will not be the case in DLM-based analyses often seen in pollution exposure literature.","In these cases, the parameters are inferred through a fully automatic Gibbs sampling procedure."],"url":"http://arxiv.org/abs/2403.03646v1","category":"stat.ME"}
{"created":"2024-03-06 12:08:14","title":"K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation Learning in Multivariate Time-Series Data","abstract":"Sourced from various sensors and organized chronologically, Multivariate Time-Series (MTS) data involves crucial spatial-temporal dependencies, e.g., correlations among sensors. To capture these dependencies, Graph Neural Networks (GNNs) have emerged as powerful tools, yet their effectiveness is restricted by the quality of graph construction from MTS data. Typically, existing approaches construct graphs solely from MTS signals, which may introduce bias due to a small training dataset and may not accurately represent underlying dependencies. To address this challenge, we propose a novel framework named K-Link, leveraging Large Language Models (LLMs) to encode extensive general knowledge and thereby providing effective solutions to reduce the bias. Leveraging the knowledge embedded in LLMs, such as physical principles, we extract a \\textit{Knowledge-Link graph}, capturing vast semantic knowledge of sensors and the linkage of the sensor-level knowledge. To harness the potential of the knowledge-link graph in enhancing the graph derived from MTS data, we propose a graph alignment module, facilitating the transfer of semantic knowledge within the knowledge-link graph into the MTS-derived graph. By doing so, we can improve the graph quality, ensuring effective representation learning with GNNs for MTS data. Extensive experiments demonstrate the efficacy of our approach for superior performance across various MTS-related downstream tasks.","sentences":["Sourced from various sensors and organized chronologically, Multivariate Time-Series (MTS) data involves crucial spatial-temporal dependencies, e.g., correlations among sensors.","To capture these dependencies, Graph Neural Networks (GNNs) have emerged as powerful tools, yet their effectiveness is restricted by the quality of graph construction from MTS data.","Typically, existing approaches construct graphs solely from MTS signals, which may introduce bias due to a small training dataset and may not accurately represent underlying dependencies.","To address this challenge, we propose a novel framework named K-Link, leveraging Large Language Models (LLMs) to encode extensive general knowledge and thereby providing effective solutions to reduce the bias.","Leveraging the knowledge embedded in LLMs, such as physical principles, we extract a \\textit{Knowledge-Link graph}, capturing vast semantic knowledge of sensors and the linkage of the sensor-level knowledge.","To harness the potential of the knowledge-link graph in enhancing the graph derived from MTS data, we propose a graph alignment module, facilitating the transfer of semantic knowledge within the knowledge-link graph into the MTS-derived graph.","By doing so, we can improve the graph quality, ensuring effective representation learning with GNNs for MTS data.","Extensive experiments demonstrate the efficacy of our approach for superior performance across various MTS-related downstream tasks."],"url":"http://arxiv.org/abs/2403.03645v1","category":"cs.AI"}
{"created":"2024-03-06 12:05:56","title":"A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation","abstract":"The challenge of spatial resource allocation is pervasive across various domains such as transportation, industry, and daily life. As the scale of real-world issues continues to expand and demands for real-time solutions increase, traditional algorithms face significant computational pressures, struggling to achieve optimal efficiency and real-time capabilities. In recent years, with the escalating computational power of computers, the remarkable achievements of reinforcement learning in domains like Go and robotics have demonstrated its robust learning and sequential decision-making capabilities. Given these advancements, there has been a surge in novel methods employing reinforcement learning to tackle spatial resource allocation problems. These methods exhibit advantages such as rapid solution convergence and strong model generalization abilities, offering a new perspective on resolving spatial resource allocation problems. Therefore, this paper aims to summarize and review recent theoretical methods and applied research utilizing reinforcement learning to address spatial resource allocation problems. It provides a summary and comprehensive overview of its fundamental principles, related methodologies, and applied research. Additionally, it highlights several unresolved issues that urgently require attention in this direction for the future.","sentences":["The challenge of spatial resource allocation is pervasive across various domains such as transportation, industry, and daily life.","As the scale of real-world issues continues to expand and demands for real-time solutions increase, traditional algorithms face significant computational pressures, struggling to achieve optimal efficiency and real-time capabilities.","In recent years, with the escalating computational power of computers, the remarkable achievements of reinforcement learning in domains like Go and robotics have demonstrated its robust learning and sequential decision-making capabilities.","Given these advancements, there has been a surge in novel methods employing reinforcement learning to tackle spatial resource allocation problems.","These methods exhibit advantages such as rapid solution convergence and strong model generalization abilities, offering a new perspective on resolving spatial resource allocation problems.","Therefore, this paper aims to summarize and review recent theoretical methods and applied research utilizing reinforcement learning to address spatial resource allocation problems.","It provides a summary and comprehensive overview of its fundamental principles, related methodologies, and applied research.","Additionally, it highlights several unresolved issues that urgently require attention in this direction for the future."],"url":"http://arxiv.org/abs/2403.03643v1","category":"cs.LG"}
{"created":"2024-03-06 12:02:07","title":"Generative Active Learning with Variational Autoencoder for Radiology Data Generation in Veterinary Medicine","abstract":"Recently, with increasing interest in pet healthcare, the demand for computer-aided diagnosis (CAD) systems in veterinary medicine has increased. The development of veterinary CAD has stagnated due to a lack of sufficient radiology data. To overcome the challenge, we propose a generative active learning framework based on a variational autoencoder. This approach aims to alleviate the scarcity of reliable data for CAD systems in veterinary medicine. This study utilizes datasets comprising cardiomegaly radiograph data. After removing annotations and standardizing images, we employed a framework for data augmentation, which consists of a data generation phase and a query phase for filtering the generated data. The experimental results revealed that as the data generated through this framework was added to the training data of the generative model, the frechet inception distance consistently decreased from 84.14 to 50.75 on the radiograph. Subsequently, when the generated data were incorporated into the training of the classification model, the false positive of the confusion matrix also improved from 0.16 to 0.66 on the radiograph. The proposed framework has the potential to address the challenges of data scarcity in medical CAD, contributing to its advancement.","sentences":["Recently, with increasing interest in pet healthcare, the demand for computer-aided diagnosis (CAD) systems in veterinary medicine has increased.","The development of veterinary CAD has stagnated due to a lack of sufficient radiology data.","To overcome the challenge, we propose a generative active learning framework based on a variational autoencoder.","This approach aims to alleviate the scarcity of reliable data for CAD systems in veterinary medicine.","This study utilizes datasets comprising cardiomegaly radiograph data.","After removing annotations and standardizing images, we employed a framework for data augmentation, which consists of a data generation phase and a query phase for filtering the generated data.","The experimental results revealed that as the data generated through this framework was added to the training data of the generative model, the frechet inception distance consistently decreased from 84.14 to 50.75 on the radiograph.","Subsequently, when the generated data were incorporated into the training of the classification model, the false positive of the confusion matrix also improved from 0.16 to 0.66 on the radiograph.","The proposed framework has the potential to address the challenges of data scarcity in medical CAD, contributing to its advancement."],"url":"http://arxiv.org/abs/2403.03642v1","category":"eess.IV"}
{"created":"2024-03-06 11:56:02","title":"Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People","abstract":"Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a proxy-tuning fashion. We will open-source training corpora, code, model weights and evaluation benchmark.","sentences":["Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources.","To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion.","This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark.","In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size.","Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a proxy-tuning fashion.","We will open-source training corpora, code, model weights and evaluation benchmark."],"url":"http://arxiv.org/abs/2403.03640v1","category":"cs.CL"}
{"created":"2024-03-06 11:52:33","title":"Efficient Search and Learning for Agile Locomotion on Stepping Stones","abstract":"Legged robots have become capable of performing highly dynamic maneuvers in the past few years. However, agile locomotion in highly constrained environments such as stepping stones is still a challenge. In this paper, we propose a combination of model-based control, search, and learning to design efficient control policies for agile locomotion on stepping stones. In our framework, we use nonlinear model predictive control (NMPC) to generate whole-body motions for a given contact plan. To efficiently search for an optimal contact plan, we propose to use Monte Carlo tree search (MCTS). While the combination of MCTS and NMPC can quickly find a feasible plan for a given environment (a few seconds), it is not yet suitable to be used as a reactive policy. Hence, we generate a dataset for optimal goal-conditioned policy for a given scene and learn it through supervised learning. In particular, we leverage the power of diffusion models in handling multi-modality in the dataset. We test our proposed framework on a scenario where our quadruped robot Solo12 successfully jumps to different goals in a highly constrained environment.","sentences":["Legged robots have become capable of performing highly dynamic maneuvers in the past few years.","However, agile locomotion in highly constrained environments such as stepping stones is still a challenge.","In this paper, we propose a combination of model-based control, search, and learning to design efficient control policies for agile locomotion on stepping stones.","In our framework, we use nonlinear model predictive control (NMPC) to generate whole-body motions for a given contact plan.","To efficiently search for an optimal contact plan, we propose to use Monte Carlo tree search (MCTS).","While the combination of MCTS and NMPC can quickly find a feasible plan for a given environment (a few seconds), it is not yet suitable to be used as a reactive policy.","Hence, we generate a dataset for optimal goal-conditioned policy for a given scene and learn it through supervised learning.","In particular, we leverage the power of diffusion models in handling multi-modality in the dataset.","We test our proposed framework on a scenario where our quadruped robot Solo12 successfully jumps to different goals in a highly constrained environment."],"url":"http://arxiv.org/abs/2403.03639v1","category":"cs.RO"}
{"created":"2024-03-06 11:48:08","title":"SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models","abstract":"Spreadsheet manipulation is widely existing in most daily works and significantly improves working efficiency. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce $\\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose $\\textbf{SheetAgent}$, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: $\\textit{Planner}$, $\\textit{Informer}$, and $\\textit{Retriever}$, achieving both advanced reasoning and accurate manipulation over spreadsheets without human interaction through iterative task reasoning and reflection. Extensive experiments demonstrate that SheetAgent delivers 20-30% pass rate improvements on multiple benchmarks over baselines, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table reasoning abilities. More details and visualizations are available at https://sheetagent.github.io.","sentences":["Spreadsheet manipulation is widely existing in most daily works and significantly improves working efficiency.","Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements).","To bridge the gap with the real-world requirements, we introduce $\\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges.","To mitigate the above challenges, we further propose $\\textbf{SheetAgent}$, a novel autonomous agent that utilizes the power of LLMs.","SheetAgent consists of three collaborative modules: $\\textit{Planner}$, $\\textit{Informer}$, and $\\textit{Retriever}$, achieving both advanced reasoning and accurate manipulation over spreadsheets without human interaction through iterative task reasoning and reflection.","Extensive experiments demonstrate that SheetAgent delivers 20-30% pass rate improvements on multiple benchmarks over baselines, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table reasoning abilities.","More details and visualizations are available at https://sheetagent.github.io."],"url":"http://arxiv.org/abs/2403.03636v1","category":"cs.AI"}
{"created":"2024-03-06 11:39:20","title":"A reduced-order modeling of pattern formations","abstract":"Chemical and biochemical reactions can exhibit surprisingly different behaviours from multiple steady-state solutions to oscillatory solutions and chaotic behaviours. Such behaviour has been of great interest to researchers for many decades. The Briggs-Rauscher, Belousov-Zhabotinskii and Bray-Liebhafsky reactions, for which periodic variations in concentrations can be visualized by changes in colour, are experimental examples of oscillating behaviour in chemical systems. These type of systems are modelled by a system of partial differential equations coupled by a nonlinearity.   However, analysing the pattern, one may suspect that the dynamic is only generated by a finite number of spatial Fourier modes. In fluid dynamics, it is shown that for large times, the solution is determined by a finite number of spatial Fourier modes, called determining modes. In the article, we first introduce the concept of determining modes and show that, indeed, it is sufficient to characterise the dynamic by only a finite number of spatial Fourier modes.   In particular, we analyse the exact number of the determining modes of $u$ and $v$, where the couple $(u,v)$ solves the following stochastic system   \\begin{equation*}   \\partial_t{u}(t) = r_1\\Delta u(t) -\\alpha_1u(t)- \\gamma_1u(t)v^2(t) + f(1 - u(t)) + g(t),\\quad   \\partial_t{v}(t) = r_2\\Delta v(t) -\\alpha_2v(t) + \\gamma_2 u(t)v^2(t) + h(t),\\quad   u(0) = u_0,\\;v(0) = v_0,   \\end{equation*}   where $r_1,r_2,\\gamma_1,\\gamma_2>0$, $\\alpha_1,\\alpha_2 \\ge 0$ and $g,h$ are time depending mappings specified later.","sentences":["Chemical and biochemical reactions can exhibit surprisingly different behaviours from multiple steady-state solutions to oscillatory solutions and chaotic behaviours.","Such behaviour has been of great interest to researchers for many decades.","The Briggs-Rauscher, Belousov-Zhabotinskii and Bray-Liebhafsky reactions, for which periodic variations in concentrations can be visualized by changes in colour, are experimental examples of oscillating behaviour in chemical systems.","These type of systems are modelled by a system of partial differential equations coupled by a nonlinearity.   ","However, analysing the pattern, one may suspect that the dynamic is only generated by a finite number of spatial Fourier modes.","In fluid dynamics, it is shown that for large times, the solution is determined by a finite number of spatial Fourier modes, called determining modes.","In the article, we first introduce the concept of determining modes and show that, indeed, it is sufficient to characterise the dynamic by only a finite number of spatial Fourier modes.   ","In particular, we analyse the exact number of the determining modes of $u$ and $v$, where the couple $(u,v)$ solves the following stochastic system   \\begin{equation*}   \\partial_t{u}(t) = r_1\\Delta u(t) -\\alpha_1u(t)- \\gamma_1u(t)v^2(t) + f(1 - u(t))","+ g(t),\\quad   \\partial_t{v}(t) = r_2\\Delta v(t) -\\alpha_2v(t)","+ \\gamma_2 u(t)v^2(t)","+ h(t),\\quad   u(0) = u_0,\\;v(0) = v_0,   \\end{equation*}   where $r_1,r_2,\\gamma_1,\\gamma_2>0$, $\\alpha_1,\\alpha_2 \\ge 0$ and $g,h$ are time depending mappings specified later."],"url":"http://arxiv.org/abs/2403.03632v1","category":"math.AP"}
{"created":"2024-03-06 11:38:08","title":"Tackling Missing Values in Probabilistic Wind Power Forecasting: A Generative Approach","abstract":"Machine learning techniques have been successfully used in probabilistic wind power forecasting. However, the issue of missing values within datasets due to sensor failure, for instance, has been overlooked for a long time. Although it is natural to consider addressing this issue by imputing missing values before model estimation and forecasting, we suggest treating missing values and forecasting targets indifferently and predicting all unknown values simultaneously based on observations. In this paper, we offer an efficient probabilistic forecasting approach by estimating the joint distribution of features and targets based on a generative model. It is free of preprocessing, and thus avoids introducing potential errors. Compared with the traditional \"impute, then predict\" pipeline, the proposed approach achieves better performance in terms of continuous ranked probability score.","sentences":["Machine learning techniques have been successfully used in probabilistic wind power forecasting.","However, the issue of missing values within datasets due to sensor failure, for instance, has been overlooked for a long time.","Although it is natural to consider addressing this issue by imputing missing values before model estimation and forecasting, we suggest treating missing values and forecasting targets indifferently and predicting all unknown values simultaneously based on observations.","In this paper, we offer an efficient probabilistic forecasting approach by estimating the joint distribution of features and targets based on a generative model.","It is free of preprocessing, and thus avoids introducing potential errors.","Compared with the traditional \"impute, then predict\" pipeline, the proposed approach achieves better performance in terms of continuous ranked probability score."],"url":"http://arxiv.org/abs/2403.03631v1","category":"cs.LG"}
{"created":"2024-03-06 11:37:46","title":"The chiral critical locus and topological structures","abstract":"We study a differential graded VOA associated to the derived critical locus of a function $f$ on a smooth oriented $D$-dimensional variety $(X,\\mathbf{vol})$. Informally, this VOA, $\\mathbf{crit}^{ch}_{f}$, is just the algebra of chiral differential operators on the derived critical locus $\\mathbf{crit}_{f}$. We prove, using a generalization of a physical construction of Witten, the $\\mathbf{crit}^{ch}_{f}$ admits a \\emph{topological structure} if $f$ is homogeneous for a $\\mathbf{G}_{m}$ action on $(X,\\mathbf{vol})$. If $\\mathbf{vol}$ has weight $b$ and $f$ has weight $a$, we compute the rank of the topological structure in terms of the discrete invariants of the theory to be $$d=\\Big(D-\\frac{2b}{a}\\Big).$$ We conclude with some remarks about BV quantization and a simple computation of characters.","sentences":["We study a differential graded VOA associated to the derived critical locus of a function $f$ on a smooth oriented $D$-dimensional variety $(X,\\mathbf{vol})$. Informally, this VOA, $\\mathbf{crit}^{ch}_{f}$, is just the algebra of chiral differential operators on the derived critical locus $\\mathbf{crit}_{f}$. We prove, using a generalization of a physical construction of Witten, the $\\mathbf{crit}^{ch}_{f}$ admits a \\emph{topological structure} if $f$ is homogeneous for a $\\mathbf{G}_{m}$ action on $(X,\\mathbf{vol})$. If $\\mathbf{vol}$ has weight $b$ and $f$ has weight $a$, we compute the rank of the topological structure in terms of the discrete invariants of the theory to be $$d=\\Big(D-\\frac{2b}{a}\\Big).$$ We conclude with some remarks about BV quantization and a simple computation of characters."],"url":"http://arxiv.org/abs/2403.03630v1","category":"math.AG"}
{"created":"2024-03-06 11:36:44","title":"Spatially Selective Reconfigurable Intelligent Surfaces Through Element Permutation","abstract":"A standard reconfigurable intelligent surface (RIS) can be configured to reflect signals from an arbitrary impinging direction to an arbitrary outgoing direction. However, if a signal impinges from any other direction, said signal is reflected, with full beamforming gain, to a specific direction, which is easily determined. The goal of this paper is to propose a RIS which \\emph{only} reflects signals from the configured impinging direction. This can be accomplished by a RIS architecture that permutes the antenna elements in the sense that a signal is re-radiated from a different antenna than the one receiving the signal. We analytically prove this fact, and also discuss several variants and hardware implementations.","sentences":["A standard reconfigurable intelligent surface (RIS) can be configured to reflect signals from an arbitrary impinging direction to an arbitrary outgoing direction.","However, if a signal impinges from any other direction, said signal is reflected, with full beamforming gain, to a specific direction, which is easily determined.","The goal of this paper is to propose a RIS which \\emph{only} reflects signals from the configured impinging direction.","This can be accomplished by a RIS architecture that permutes the antenna elements in the sense that a signal is re-radiated from a different antenna than the one receiving the signal.","We analytically prove this fact, and also discuss several variants and hardware implementations."],"url":"http://arxiv.org/abs/2403.03629v1","category":"eess.SP"}
{"created":"2024-03-06 11:34:20","title":"GPTopic: Dynamic and Interactive Topic Representations","abstract":"Topic modeling seems to be almost synonymous with generating lists of top words to represent topics within large text corpora. However, deducing a topic from such list of individual terms can require substantial expertise and experience, making topic modelling less accessible to people unfamiliar with the particularities and pitfalls of top-word interpretation. A topic representation limited to top-words might further fall short of offering a comprehensive and easily accessible characterization of the various aspects, facets and nuances a topic might have. To address these challenges, we introduce GPTopic, a software package that leverages Large Language Models (LLMs) to create dynamic, interactive topic representations. GPTopic provides an intuitive chat interface for users to explore, analyze, and refine topics interactively, making topic modeling more accessible and comprehensive. The corresponding code is available here: https://github. com/05ec6602be/GPTopic.","sentences":["Topic modeling seems to be almost synonymous with generating lists of top words to represent topics within large text corpora.","However, deducing a topic from such list of individual terms can require substantial expertise and experience, making topic modelling less accessible to people unfamiliar with the particularities and pitfalls of top-word interpretation.","A topic representation limited to top-words might further fall short of offering a comprehensive and easily accessible characterization of the various aspects, facets and nuances a topic might have.","To address these challenges, we introduce GPTopic, a software package that leverages Large Language Models (LLMs) to create dynamic, interactive topic representations.","GPTopic provides an intuitive chat interface for users to explore, analyze, and refine topics interactively, making topic modeling more accessible and comprehensive.","The corresponding code is available here: https://github. com/05ec6602be/GPTopic."],"url":"http://arxiv.org/abs/2403.03628v1","category":"cs.CL"}
{"created":"2024-03-06 11:32:41","title":"Multimodal Large Language Models to Support Real-World Fact-Checking","abstract":"Multimodal large language models (MLLMs) carry the potential to support humans in processing vast amounts of information. While MLLMs are already being used as a fact-checking tool, their abilities and limitations in this regard are understudied. Here is aim to bridge this gap. In particular, we propose a framework for systematically assessing the capacity of current multimodal models to facilitate real-world fact-checking. Our methodology is evidence-free, leveraging only these models' intrinsic knowledge and reasoning capabilities. By designing prompts that extract models' predictions, explanations, and confidence levels, we delve into research questions concerning model accuracy, robustness, and reasons for failure. We empirically find that (1) GPT-4V exhibits superior performance in identifying malicious and misleading multimodal claims, with the ability to explain the unreasonable aspects and underlying motives, and (2) existing open-source models exhibit strong biases and are highly sensitive to the prompt. Our study offers insights into combating false multimodal information and building secure, trustworthy multimodal models. To the best of our knowledge, we are the first to evaluate MLLMs for real-world fact-checking.","sentences":["Multimodal large language models (MLLMs) carry the potential to support humans in processing vast amounts of information.","While MLLMs are already being used as a fact-checking tool, their abilities and limitations in this regard are understudied.","Here is aim to bridge this gap.","In particular, we propose a framework for systematically assessing the capacity of current multimodal models to facilitate real-world fact-checking.","Our methodology is evidence-free, leveraging only these models' intrinsic knowledge and reasoning capabilities.","By designing prompts that extract models' predictions, explanations, and confidence levels, we delve into research questions concerning model accuracy, robustness, and reasons for failure.","We empirically find that (1) GPT-4V exhibits superior performance in identifying malicious and misleading multimodal claims, with the ability to explain the unreasonable aspects and underlying motives, and (2) existing open-source models exhibit strong biases and are highly sensitive to the prompt.","Our study offers insights into combating false multimodal information and building secure, trustworthy multimodal models.","To the best of our knowledge, we are the first to evaluate MLLMs for real-world fact-checking."],"url":"http://arxiv.org/abs/2403.03627v1","category":"cs.CL"}
{"created":"2024-03-06 11:31:08","title":"Data-Driven Superstabilizing Control under Quadratically-Bounded Errors-in-Variables Noise","abstract":"The Error-in-Variables model of system identification/control involves nontrivial input and measurement corruption of observed data, resulting in generically nonconvex optimization problems. This paper performs full-state-feedback stabilizing control of all discrete-time linear systems that are consistent with observed data for which the input and measurement noise obey quadratic bounds. Instances of such quadratic bounds include elementwise norm bounds (at each time sample), energy bounds (across the entire signal), and chance constraints arising from (sub)gaussian noise. Superstabilizing controllers are generated through the solution of a sum-of-squares hierarchy of semidefinite programs. A theorem of alternatives is employed to eliminate the input and measurement noise process, thus improving tractability. Effectiveness of the scheme is generated on an example system in the chance-constrained set-membership setting where the input and state-measurement noise are i.i.d. normally distributed.","sentences":["The Error-in-Variables model of system identification/control involves nontrivial input and measurement corruption of observed data, resulting in generically nonconvex optimization problems.","This paper performs full-state-feedback stabilizing control of all discrete-time linear systems that are consistent with observed data for which the input and measurement noise obey quadratic bounds.","Instances of such quadratic bounds include elementwise norm bounds (at each time sample), energy bounds (across the entire signal), and chance constraints arising from (sub)gaussian noise.","Superstabilizing controllers are generated through the solution of a sum-of-squares hierarchy of semidefinite programs.","A theorem of alternatives is employed to eliminate the input and measurement noise process, thus improving tractability.","Effectiveness of the scheme is generated on an example system in the chance-constrained set-membership setting where the input and state-measurement noise are i.i.d. normally distributed."],"url":"http://arxiv.org/abs/2403.03624v1","category":"math.OC"}
{"created":"2024-03-06 11:30:02","title":"An expansion formula for elliptic hypergeometric series","abstract":"We prove an expansion formula for elliptic hypergeometric series. In the $q$-case, when the nome $p=0$, it generalizes $q$-hypergeometric transformation formulas due to Liu, and Wang and Ma, results that are used to derive identities for Hecke-type series and transformations for false and mock-theta functions. Our formula contains an arbitrary sequence as an argument, and is thus flexible in the number of parameters it contains. As a result, we are able to derive several transformation formulas for elliptic hypergeometric series, as well as double summation formulas.","sentences":["We prove an expansion formula for elliptic hypergeometric series.","In the $q$-case, when the nome $p=0$, it generalizes $q$-hypergeometric transformation formulas due to Liu, and Wang and Ma, results that are used to derive identities for Hecke-type series and transformations for false and mock-theta functions.","Our formula contains an arbitrary sequence as an argument, and is thus flexible in the number of parameters it contains.","As a result, we are able to derive several transformation formulas for elliptic hypergeometric series, as well as double summation formulas."],"url":"http://arxiv.org/abs/2403.03623v1","category":"math.NT"}
{"created":"2024-03-06 11:28:47","title":"Medial Parametrization of Arbitrary Planar Compact Domains with Dipoles","abstract":"We present medial parametrization, a new approach to parameterizing any compact planar domain bounded by simple closed curves. The basic premise behind our proposed approach is to use two close Voronoi sites, which we call dipoles, to construct and reconstruct an approximate piecewise-linear version of the original boundary and medial axis through Voronoi tessellation. The boundaries and medial axes of such planar compact domains offer a natural way to describe the domain's interior. Any compact planar domain is homeomorphic to a compact unit circular disk admits a natural parameterization isomorphic to the polar parametrization of the disk. Specifically, the medial axis and the boundary generalize the radial and angular parameters, respectively. In this paper, we present a simple algorithm that puts these principles into practice. The algorithm is based on the simultaneous re-creation of the boundaries of the domain and its medial axis using Voronoi tessellation. This simultaneous re-creation provides partitions of the domain into a set of \"skinny\" convex polygons wherein each polygon is essentially a subset of the medial edges (which we call the spine) connected to the boundary through exactly two straight edges (which we call limbs). This unique structure enables us to convert the original Voronoi tessellation into quadrilaterals and triangles (at the poles of the medial axis) neatly ordered along the domain boundary, thereby allowing proper parametrization of the domain. Our approach is agnostic to the number of holes and disconnected components bounding the domain. We investigate the efficacy of our concept and algorithm through several examples.","sentences":["We present medial parametrization, a new approach to parameterizing any compact planar domain bounded by simple closed curves.","The basic premise behind our proposed approach is to use two close Voronoi sites, which we call dipoles, to construct and reconstruct an approximate piecewise-linear version of the original boundary and medial axis through Voronoi tessellation.","The boundaries and medial axes of such planar compact domains offer a natural way to describe the domain's interior.","Any compact planar domain is homeomorphic to a compact unit circular disk admits a natural parameterization isomorphic to the polar parametrization of the disk.","Specifically, the medial axis and the boundary generalize the radial and angular parameters, respectively.","In this paper, we present a simple algorithm that puts these principles into practice.","The algorithm is based on the simultaneous re-creation of the boundaries of the domain and its medial axis using Voronoi tessellation.","This simultaneous re-creation provides partitions of the domain into a set of \"skinny\" convex polygons wherein each polygon is essentially a subset of the medial edges (which we call the spine) connected to the boundary through exactly two straight edges (which we call limbs).","This unique structure enables us to convert the original Voronoi tessellation into quadrilaterals and triangles (at the poles of the medial axis) neatly ordered along the domain boundary, thereby allowing proper parametrization of the domain.","Our approach is agnostic to the number of holes and disconnected components bounding the domain.","We investigate the efficacy of our concept and algorithm through several examples."],"url":"http://arxiv.org/abs/2403.03622v1","category":"cs.GR"}
{"created":"2024-03-06 11:25:29","title":"Fast spectroscopic imaging using extreme ultraviolet interferometry","abstract":"Extreme ultraviolet pulses as generated by high harmonic generation (HHG) are a powerful tool for both time-resolved spectroscopy and coherent diffractive imaging. However, the integration of spectroscopy and microscopy to harness the unique broadband spectra provided by HHG is hardly explored due to the challenge to decouple spectroscopic and microscopic information. Here, we present an interferometric approach to this problem that combines Fourier transform spectroscopy (FTS) with Fourier transform holography (FTH). This is made possible by the generation of phase-locked pulses using a pair of HHG sources. Crucially, in our geometry the number of interferometric measurements required is at most equal to the number of high harmonics in the illumination, and can be further reduced by incorporating prior knowledge about the structure of the FTH sample. Compared to conventional FTS, this approach achieves over an order of magnitude increase in acquisition speed for full spectro-microscopic data, and furthermore allows diffraction-limited computational imaging.","sentences":["Extreme ultraviolet pulses as generated by high harmonic generation (HHG) are a powerful tool for both time-resolved spectroscopy and coherent diffractive imaging.","However, the integration of spectroscopy and microscopy to harness the unique broadband spectra provided by HHG is hardly explored due to the challenge to decouple spectroscopic and microscopic information.","Here, we present an interferometric approach to this problem that combines Fourier transform spectroscopy (FTS) with Fourier transform holography (FTH).","This is made possible by the generation of phase-locked pulses using a pair of HHG sources.","Crucially, in our geometry the number of interferometric measurements required is at most equal to the number of high harmonics in the illumination, and can be further reduced by incorporating prior knowledge about the structure of the FTH sample.","Compared to conventional FTS, this approach achieves over an order of magnitude increase in acquisition speed for full spectro-microscopic data, and furthermore allows diffraction-limited computational imaging."],"url":"http://arxiv.org/abs/2403.03620v1","category":"physics.optics"}
{"created":"2024-03-06 11:06:25","title":"Using the Dual-Privacy Framework to Understand Consumers' Perceived Privacy Violations Under Different Firm Practices in Online Advertising","abstract":"In response to privacy concerns about collecting and using personal data, the online advertising industry has been developing privacy-enhancing technologies (PETs), e.g., under Google's Privacy Sandbox initiative. In this research, we use the dual-privacy framework, which postulates that consumers have intrinsic and instrumental preferences for privacy, to understand consumers' perceived privacy violations (PPVs) for current and proposed online advertising practices. The key idea is that different practices differ in whether individual data leaves the consumer's machine or not and in how they track and target consumers; these affect, respectively, the intrinsic and instrumental components of privacy preferences differently, leading to different PPVs for different practices. We conducted online studies focused on consumers in the United States to elicit PPVs for various advertising practices. Our findings confirm the intuition that tracking and targeting consumers under the industry status quo of behavioral targeting leads to high PPV. New technologies or proposals that ensure that data are kept on the consumer's machine lower PPV relative to behavioral targeting but, importantly, this decrease is small. Furthermore, group-level targeting does not differ significantly from individual-level targeting in reducing PPV. Under contextual targeting, where there is no tracking, PPV is significantly reduced. Interestingly, with respect to PPV, consumers are indifferent between seeing untargeted ads and no ads when they are not being tracked. We find that consumer perceptions of privacy violations under different tracking and targeting practices may differ from what technical definitions suggest. Therefore, rather than relying solely on technical perspectives, a consumer-centric approach to privacy is needed, based on, for instance, the dual-privacy framework.","sentences":["In response to privacy concerns about collecting and using personal data, the online advertising industry has been developing privacy-enhancing technologies (PETs), e.g., under Google's Privacy Sandbox initiative.","In this research, we use the dual-privacy framework, which postulates that consumers have intrinsic and instrumental preferences for privacy, to understand consumers' perceived privacy violations (PPVs) for current and proposed online advertising practices.","The key idea is that different practices differ in whether individual data leaves the consumer's machine or not and in how they track and target consumers; these affect, respectively, the intrinsic and instrumental components of privacy preferences differently, leading to different PPVs for different practices.","We conducted online studies focused on consumers in the United States to elicit PPVs for various advertising practices.","Our findings confirm the intuition that tracking and targeting consumers under the industry status quo of behavioral targeting leads to high PPV.","New technologies or proposals that ensure that data are kept on the consumer's machine lower PPV relative to behavioral targeting but, importantly, this decrease is small.","Furthermore, group-level targeting does not differ significantly from individual-level targeting in reducing PPV.","Under contextual targeting, where there is no tracking, PPV is significantly reduced.","Interestingly, with respect to PPV, consumers are indifferent between seeing untargeted ads and no ads when they are not being tracked.","We find that consumer perceptions of privacy violations under different tracking and targeting practices may differ from what technical definitions suggest.","Therefore, rather than relying solely on technical perspectives, a consumer-centric approach to privacy is needed, based on, for instance, the dual-privacy framework."],"url":"http://arxiv.org/abs/2403.03612v1","category":"econ.GN"}
{"created":"2024-03-06 10:59:49","title":"Paying for Privacy: Pay-or-Tracking Walls","abstract":"Prestigious news publishers, and more recently, Meta, have begun to request that users pay for privacy. Specifically, users receive a notification banner, referred to as a pay-or-tracking wall, that requires them to (i) pay money to avoid being tracked or (ii) consent to being tracked. These walls have invited concerns that privacy might become a luxury. However, little is known about pay-or-tracking walls, which prevents a meaningful discussion about their appropriateness. This paper conducts several empirical studies and finds that top EU publishers use pay-or-tracking walls. Their implementations involve various approaches, including bundling the pay option with advertising-free access or additional content. The price for not being tracked exceeds the advertising revenue that publishers generate from a user who consents to being tracked. Notably, publishers' traffic does not decline when implementing a pay-or-tracking wall and most users consent to being tracked; only a few users pay. In short, pay-or-tracking walls seem to provide the means for expanding the practice of tracking. Publishers profit from pay-or-tracking walls and may observe a revenue increase of 16.4% due to tracking more users than under a cookie consent banner.","sentences":["Prestigious news publishers, and more recently, Meta, have begun to request that users pay for privacy.","Specifically, users receive a notification banner, referred to as a pay-or-tracking wall, that requires them to (i) pay money to avoid being tracked or (ii) consent to being tracked.","These walls have invited concerns that privacy might become a luxury.","However, little is known about pay-or-tracking walls, which prevents a meaningful discussion about their appropriateness.","This paper conducts several empirical studies and finds that top EU publishers use pay-or-tracking walls.","Their implementations involve various approaches, including bundling the pay option with advertising-free access or additional content.","The price for not being tracked exceeds the advertising revenue that publishers generate from a user who consents to being tracked.","Notably, publishers' traffic does not decline when implementing a pay-or-tracking wall and most users consent to being tracked; only a few users pay.","In short, pay-or-tracking walls seem to provide the means for expanding the practice of tracking.","Publishers profit from pay-or-tracking walls and may observe a revenue increase of 16.4% due to tracking more users than under a cookie consent banner."],"url":"http://arxiv.org/abs/2403.03610v1","category":"econ.GN"}
{"created":"2024-03-06 10:55:50","title":"GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding","abstract":"Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision. In this work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view images and the associated semantic maps can be produced for unseen scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and Depth-Guided Visual rendering. The former is able to observe multi-view image inputs to extract semantic and geometry features from a scene. Guided by the resulting image geometry information, the latter performs both image and semantic rendering with improved performances. Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effectiveness of our sampling strategy for visual rendering is further verified.","sentences":["Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision.","In this work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view images and the associated semantic maps can be produced for unseen scenes.","Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and Depth-Guided Visual rendering.","The former is able to observe multi-view image inputs to extract semantic and geometry features from a scene.","Guided by the resulting image geometry information, the latter performs both image and semantic rendering with improved performances.","Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effectiveness of our sampling strategy for visual rendering is further verified."],"url":"http://arxiv.org/abs/2403.03608v1","category":"cs.CV"}
{"created":"2024-03-06 10:53:51","title":"The Geometric Structure of Topic Models","abstract":"Topic models are a popular tool for clustering and analyzing textual data. They allow texts to be classified on the basis of their affiliation to the previously calculated topics. Despite their widespread use in research and application, an in-depth analysis of topic models is still an open research topic. State-of-the-art methods for interpreting topic models are based on simple visualizations, such as similarity matrices, top-term lists or embeddings, which are limited to a maximum of three dimensions. In this paper, we propose an incidence-geometric method for deriving an ordinal structure from flat topic models, such as non-negative matrix factorization. These enable the analysis of the topic model in a higher (order) dimension and the possibility of extracting conceptual relationships between several topics at once. Due to the use of conceptual scaling, our approach does not introduce any artificial topical relationships, such as artifacts of feature compression. Based on our findings, we present a new visualization paradigm for concept hierarchies based on ordinal motifs. These allow for a top-down view on topic spaces. We introduce and demonstrate the applicability of our approach based on a topic model derived from a corpus of scientific papers taken from 32 top machine learning venues.","sentences":["Topic models are a popular tool for clustering and analyzing textual data.","They allow texts to be classified on the basis of their affiliation to the previously calculated topics.","Despite their widespread use in research and application, an in-depth analysis of topic models is still an open research topic.","State-of-the-art methods for interpreting topic models are based on simple visualizations, such as similarity matrices, top-term lists or embeddings, which are limited to a maximum of three dimensions.","In this paper, we propose an incidence-geometric method for deriving an ordinal structure from flat topic models, such as non-negative matrix factorization.","These enable the analysis of the topic model in a higher (order) dimension and the possibility of extracting conceptual relationships between several topics at once.","Due to the use of conceptual scaling, our approach does not introduce any artificial topical relationships, such as artifacts of feature compression.","Based on our findings, we present a new visualization paradigm for concept hierarchies based on ordinal motifs.","These allow for a top-down view on topic spaces.","We introduce and demonstrate the applicability of our approach based on a topic model derived from a corpus of scientific papers taken from 32 top machine learning venues."],"url":"http://arxiv.org/abs/2403.03607v1","category":"cs.AI"}
{"created":"2024-03-06 10:53:12","title":"Enhancing Price Prediction in Cryptocurrency Using Transformer Neural Network and Technical Indicators","abstract":"This study presents an innovative approach for predicting cryptocurrency time series, specifically focusing on Bitcoin, Ethereum, and Litecoin. The methodology integrates the use of technical indicators, a Performer neural network, and BiLSTM (Bidirectional Long Short-Term Memory) to capture temporal dynamics and extract significant features from raw cryptocurrency data. The application of technical indicators, such facilitates the extraction of intricate patterns, momentum, volatility, and trends. The Performer neural network, employing Fast Attention Via positive Orthogonal Random features (FAVOR+), has demonstrated superior computational efficiency and scalability compared to the traditional Multi-head attention mechanism in Transformer models. Additionally, the integration of BiLSTM in the feedforward network enhances the model's capacity to capture temporal dynamics in the data, processing it in both forward and backward directions. This is particularly advantageous for time series data where past and future data points can influence the current state. The proposed method has been applied to the hourly and daily timeframes of the major cryptocurrencies and its performance has been benchmarked against other methods documented in the literature. The results underscore the potential of the proposed method to outperform existing models, marking a significant progression in the field of cryptocurrency price prediction.","sentences":["This study presents an innovative approach for predicting cryptocurrency time series, specifically focusing on Bitcoin, Ethereum, and Litecoin.","The methodology integrates the use of technical indicators, a Performer neural network, and BiLSTM (Bidirectional Long Short-Term Memory) to capture temporal dynamics and extract significant features from raw cryptocurrency data.","The application of technical indicators, such facilitates the extraction of intricate patterns, momentum, volatility, and trends.","The Performer neural network, employing Fast Attention Via positive Orthogonal Random features (FAVOR+), has demonstrated superior computational efficiency and scalability compared to the traditional Multi-head attention mechanism in Transformer models.","Additionally, the integration of BiLSTM in the feedforward network enhances the model's capacity to capture temporal dynamics in the data, processing it in both forward and backward directions.","This is particularly advantageous for time series data where past and future data points can influence the current state.","The proposed method has been applied to the hourly and daily timeframes of the major cryptocurrencies and its performance has been benchmarked against other methods documented in the literature.","The results underscore the potential of the proposed method to outperform existing models, marking a significant progression in the field of cryptocurrency price prediction."],"url":"http://arxiv.org/abs/2403.03606v1","category":"q-fin.CP"}
{"created":"2024-03-06 10:40:31","title":"Time-dependent scalings and Fock quantization of a massless scalar field in Kantowski-Sachs","abstract":"We address the issue of inequivalent Fock representations in Quantum Field Theory in a curved homogenous and anisotropic background, namely Kantowski-Sachs spacetime. A family of unitarily equivalent Fock representations that are invariant under the spatial isometries and implement a unitary dynamics can be achieved by means of a field redefinition that consists of a specific anisotropic scaling of the field configuration and a linear transformation of its momentum. Remarkably, we show that this kind of field redefinition is in fact unique under our symmetry and unitary requirements. However, the physical properties of the Hamiltonian dynamics that one obtains in this way are not satisfactory, inasmuch as the action of the Hamiltonian on the corresponding particle states is ill defined. To construct a quantum theory without this problem, we need a further canonical transformation that is time- and mode-dependent and is not interpretable as an anisotropic scaling. The old and new Fock representations, nevertheless, are unitarily equivalent. The freedom that is introduced when allowing for this further canonical transformation can be fixed by demanding an asymptotic diagonalization of the Hamiltonian and a minimal absorption of dynamical phases. In this way, the choice of vacuum and the associated Fock representation are asymptotically determined.","sentences":["We address the issue of inequivalent Fock representations in Quantum Field Theory in a curved homogenous and anisotropic background, namely Kantowski-Sachs spacetime.","A family of unitarily equivalent Fock representations that are invariant under the spatial isometries and implement a unitary dynamics can be achieved by means of a field redefinition that consists of a specific anisotropic scaling of the field configuration and a linear transformation of its momentum.","Remarkably, we show that this kind of field redefinition is in fact unique under our symmetry and unitary requirements.","However, the physical properties of the Hamiltonian dynamics that one obtains in this way are not satisfactory, inasmuch as the action of the Hamiltonian on the corresponding particle states is ill defined.","To construct a quantum theory without this problem, we need a further canonical transformation that is time- and mode-dependent and is not interpretable as an anisotropic scaling.","The old and new Fock representations, nevertheless, are unitarily equivalent.","The freedom that is introduced when allowing for this further canonical transformation can be fixed by demanding an asymptotic diagonalization of the Hamiltonian and a minimal absorption of dynamical phases.","In this way, the choice of vacuum and the associated Fock representation are asymptotically determined."],"url":"http://arxiv.org/abs/2403.03601v1","category":"gr-qc"}
{"created":"2024-03-06 10:40:08","title":"A Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain Recommendation","abstract":"Cross-domain recommendation (CDR) aims to enhance recommendation accuracy in a target domain with sparse data by leveraging rich information in a source domain, thereby addressing the data-sparsity problem. Some existing CDR methods highlight the advantages of extracting domain-common and domain-specific features to learn comprehensive user and item representations. However, these methods can't effectively disentangle these components as they often rely on simple user-item historical interaction information (such as ratings, clicks, and browsing), neglecting the rich multi-modal features. Additionally, they don't protect user-sensitive data from potential leakage during knowledge transfer between domains. To address these challenges, we propose a Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain Recommendation, called P2M2-CDR. Specifically, we first design a multi-modal disentangled encoder that utilizes multi-modal information to disentangle more informative domain-common and domain-specific embeddings. Furthermore, we introduce a privacy-preserving decoder to mitigate user privacy leakage during knowledge transfer. Local differential privacy (LDP) is utilized to obfuscate the disentangled embeddings before inter-domain exchange, thereby enhancing privacy protection. To ensure both consistency and differentiation among these obfuscated disentangled embeddings, we incorporate contrastive learning-based domain-inter and domain-intra losses. Extensive Experiments conducted on four real-world datasets demonstrate that P2M2-CDR outperforms other state-of-the-art single-domain and cross-domain baselines.","sentences":["Cross-domain recommendation (CDR) aims to enhance recommendation accuracy in a target domain with sparse data by leveraging rich information in a source domain, thereby addressing the data-sparsity problem.","Some existing CDR methods highlight the advantages of extracting domain-common and domain-specific features to learn comprehensive user and item representations.","However, these methods can't effectively disentangle these components as they often rely on simple user-item historical interaction information (such as ratings, clicks, and browsing), neglecting the rich multi-modal features.","Additionally, they don't protect user-sensitive data from potential leakage during knowledge transfer between domains.","To address these challenges, we propose a Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain Recommendation, called P2M2-CDR.","Specifically, we first design a multi-modal disentangled encoder that utilizes multi-modal information to disentangle more informative domain-common and domain-specific embeddings.","Furthermore, we introduce a privacy-preserving decoder to mitigate user privacy leakage during knowledge transfer.","Local differential privacy (LDP) is utilized to obfuscate the disentangled embeddings before inter-domain exchange, thereby enhancing privacy protection.","To ensure both consistency and differentiation among these obfuscated disentangled embeddings, we incorporate contrastive learning-based domain-inter and domain-intra losses.","Extensive Experiments conducted on four real-world datasets demonstrate that P2M2-CDR outperforms other state-of-the-art single-domain and cross-domain baselines."],"url":"http://arxiv.org/abs/2403.03600v1","category":"cs.AI"}
{"created":"2024-03-06 10:36:56","title":"Learning Invariant Representations of Graph Neural Networks via Cluster Generalization","abstract":"Graph neural networks (GNNs) have become increasingly popular in modeling graph-structured data due to their ability to learn node representations by aggregating local structure information. However, it is widely acknowledged that the test graph structure may differ from the training graph structure, resulting in a structure shift. In this paper, we experimentally find that the performance of GNNs drops significantly when the structure shift happens, suggesting that the learned models may be biased towards specific structure patterns. To address this challenge, we propose the Cluster Information Transfer (CIT) mechanism (Code available at https://github.com/BUPT-GAMMA/CITGNN), which can learn invariant representations for GNNs, thereby improving their generalization ability to various and unknown test graphs with structure shift. The CIT mechanism achieves this by combining different cluster information with the nodes while preserving their cluster-independent information. By generating nodes across different clusters, the mechanism significantly enhances the diversity of the nodes and helps GNNs learn the invariant representations. We provide a theoretical analysis of the CIT mechanism, showing that the impact of changing clusters during structure shift can be mitigated after transfer. Additionally, the proposed mechanism is a plug-in that can be easily used to improve existing GNNs. We comprehensively evaluate our proposed method on three typical structure shift scenarios, demonstrating its effectiveness in enhancing GNNs' performance.","sentences":["Graph neural networks (GNNs) have become increasingly popular in modeling graph-structured data due to their ability to learn node representations by aggregating local structure information.","However, it is widely acknowledged that the test graph structure may differ from the training graph structure, resulting in a structure shift.","In this paper, we experimentally find that the performance of GNNs drops significantly when the structure shift happens, suggesting that the learned models may be biased towards specific structure patterns.","To address this challenge, we propose the Cluster Information Transfer (CIT) mechanism (Code available at https://github.com/BUPT-GAMMA/CITGNN), which can learn invariant representations for GNNs, thereby improving their generalization ability to various and unknown test graphs with structure shift.","The CIT mechanism achieves this by combining different cluster information with the nodes while preserving their cluster-independent information.","By generating nodes across different clusters, the mechanism significantly enhances the diversity of the nodes and helps GNNs learn the invariant representations.","We provide a theoretical analysis of the CIT mechanism, showing that the impact of changing clusters during structure shift can be mitigated after transfer.","Additionally, the proposed mechanism is a plug-in that can be easily used to improve existing GNNs.","We comprehensively evaluate our proposed method on three typical structure shift scenarios, demonstrating its effectiveness in enhancing GNNs' performance."],"url":"http://arxiv.org/abs/2403.03599v1","category":"cs.LG"}
{"created":"2024-03-06 10:35:21","title":"Controlling volume fluctuations for studies of critical phenomena in nuclear collisions","abstract":"We generalize and extend the recently proposed method to account for contributions of system size (or volume/participant) fluctuations to the experimentally measured moments of particle multiplicity distributions. We find that in the general case there are additional biases which are not directly accessible to experiment. These biases are, however, parametrically suppressed if the multiplicity of the particles of interest is small compared to the total charged-particle multiplicity, e.g., in the case of proton number fluctuations at top RHIC and LHC energies. They are also small if the multiplicity distribution of charged particles per wounded nucleon is close to the Poissonian limit, which is the case at low energy nuclear collisions, e.g., at GSI/SIS18. We further find that mixed events are not necessarily needed to extract the correction for volume fluctuations, albeit it can help if event statistics is small, which is typically the case for reconstructing the higher-order cumulants. We provide the formulas to correct pure and mixed cumulants of particle multiplicity distributions up to any order together with their associated biases.","sentences":["We generalize and extend the recently proposed method to account for contributions of system size (or volume/participant) fluctuations to the experimentally measured moments of particle multiplicity distributions.","We find that in the general case there are additional biases which are not directly accessible to experiment.","These biases are, however, parametrically suppressed if the multiplicity of the particles of interest is small compared to the total charged-particle multiplicity, e.g., in the case of proton number fluctuations at top RHIC and LHC energies.","They are also small if the multiplicity distribution of charged particles per wounded nucleon is close to the Poissonian limit, which is the case at low energy nuclear collisions, e.g., at GSI/SIS18.","We further find that mixed events are not necessarily needed to extract the correction for volume fluctuations, albeit it can help if event statistics is small, which is typically the case for reconstructing the higher-order cumulants.","We provide the formulas to correct pure and mixed cumulants of particle multiplicity distributions up to any order together with their associated biases."],"url":"http://arxiv.org/abs/2403.03598v1","category":"nucl-th"}
{"created":"2024-03-06 10:32:51","title":"The 'Must Stock' Challenge in Academic Publishing: Pricing Implications of Transformative Agreements","abstract":"The high relevance of top-notch academic journals turns them into 'must stock' products that assign its often commercial owners with extraordinary market power. Intended to tackle this, university consortia around the globe negotiate so-called 'transformative agreements' with many publishing houses. It shall pave the way towards standard open-access publishing. While several contract designs exist, the 'publish-and-read' (PAR) scheme is the one that comes closest to the ideal of an entirely open access environment: Publishers are paid a fixed case-by-case rate for each publication, which includes a fee for their extensive libraries. In turn, all subscription payments are waived. I theoretically derive that this contract design benefits the included publishers regardless of whether the number of publications in these publishers' journals grows or declines. Consequently, widespread PAR contracts are likely to raise entry barriers for new (open-access) competitors even further. Intending to lower costs for the universities, their libraries, and, ultimately, the taxpayers, this PAR fee contract design of transformative agreements might cause the opposite.","sentences":["The high relevance of top-notch academic journals turns them into 'must stock' products that assign its often commercial owners with extraordinary market power.","Intended to tackle this, university consortia around the globe negotiate so-called 'transformative agreements' with many publishing houses.","It shall pave the way towards standard open-access publishing.","While several contract designs exist, the 'publish-and-read' (PAR) scheme is the one that comes closest to the ideal of an entirely open access environment: Publishers are paid a fixed case-by-case rate for each publication, which includes a fee for their extensive libraries.","In turn, all subscription payments are waived.","I theoretically derive that this contract design benefits the included publishers regardless of whether the number of publications in these publishers' journals grows or declines.","Consequently, widespread PAR contracts are likely to raise entry barriers for new (open-access) competitors even further.","Intending to lower costs for the universities, their libraries, and, ultimately, the taxpayers, this PAR fee contract design of transformative agreements might cause the opposite."],"url":"http://arxiv.org/abs/2403.03597v1","category":"econ.GN"}
{"created":"2024-03-06 10:27:09","title":"Assessing the Aesthetic Evaluation Capabilities of GPT-4 with Vision: Insights from Group and Individual Assessments","abstract":"Recently, it has been recognized that large language models demonstrate high performance on various intellectual tasks. However, few studies have investigated alignment with humans in behaviors that involve sensibility, such as aesthetic evaluation. This study investigates the performance of GPT-4 with Vision, a state-of-the-art language model that can handle image input, on the task of aesthetic evaluation of images. We employ two tasks, prediction of the average evaluation values of a group and an individual's evaluation values. We investigate the performance of GPT-4 with Vision by exploring prompts and analyzing prediction behaviors. Experimental results reveal GPT-4 with Vision's superior performance in predicting aesthetic evaluations and the nature of different responses to beauty and ugliness. Finally, we discuss developing an AI system for aesthetic evaluation based on scientific knowledge of the human perception of beauty, employing agent technologies that integrate traditional deep learning models with large language models.","sentences":["Recently, it has been recognized that large language models demonstrate high performance on various intellectual tasks.","However, few studies have investigated alignment with humans in behaviors that involve sensibility, such as aesthetic evaluation.","This study investigates the performance of GPT-4 with Vision, a state-of-the-art language model that can handle image input, on the task of aesthetic evaluation of images.","We employ two tasks, prediction of the average evaluation values of a group and an individual's evaluation values.","We investigate the performance of GPT-4 with Vision by exploring prompts and analyzing prediction behaviors.","Experimental results reveal GPT-4 with Vision's superior performance in predicting aesthetic evaluations and the nature of different responses to beauty and ugliness.","Finally, we discuss developing an AI system for aesthetic evaluation based on scientific knowledge of the human perception of beauty, employing agent technologies that integrate traditional deep learning models with large language models."],"url":"http://arxiv.org/abs/2403.03594v1","category":"cs.AI"}
{"created":"2024-03-06 10:27:08","title":"Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem","abstract":"Training high-quality deep learning models is a challenging task due to computational and technical requirements. A growing number of individuals, institutions, and companies increasingly rely on pre-trained, third-party models made available in public repositories. These models are often used directly or integrated in product pipelines with no particular precautions, since they are effectively just data in tensor form and considered safe. In this paper, we raise awareness of a new machine learning supply chain threat targeting neural networks. We introduce MaleficNet 2.0, a novel technique to embed self-extracting, self-executing malware in neural networks. MaleficNet 2.0 uses spread-spectrum channel coding combined with error correction techniques to inject malicious payloads in the parameters of deep neural networks. MaleficNet 2.0 injection technique is stealthy, does not degrade the performance of the model, and is robust against removal techniques. We design our approach to work both in traditional and distributed learning settings such as Federated Learning, and demonstrate that it is effective even when a reduced number of bits is used for the model parameters. Finally, we implement a proof-of-concept self-extracting neural network malware using MaleficNet 2.0, demonstrating the practicality of the attack against a widely adopted machine learning framework. Our aim with this work is to raise awareness against these new, dangerous attacks both in the research community and industry, and we hope to encourage further research in mitigation techniques against such threats.","sentences":["Training high-quality deep learning models is a challenging task due to computational and technical requirements.","A growing number of individuals, institutions, and companies increasingly rely on pre-trained, third-party models made available in public repositories.","These models are often used directly or integrated in product pipelines with no particular precautions, since they are effectively just data in tensor form and considered safe.","In this paper, we raise awareness of a new machine learning supply chain threat targeting neural networks.","We introduce MaleficNet 2.0, a novel technique to embed self-extracting, self-executing malware in neural networks.","MaleficNet 2.0 uses spread-spectrum channel coding combined with error correction techniques to inject malicious payloads in the parameters of deep neural networks.","MaleficNet 2.0 injection technique is stealthy, does not degrade the performance of the model, and is robust against removal techniques.","We design our approach to work both in traditional and distributed learning settings such as Federated Learning, and demonstrate that it is effective even when a reduced number of bits is used for the model parameters.","Finally, we implement a proof-of-concept self-extracting neural network malware using MaleficNet 2.0, demonstrating the practicality of the attack against a widely adopted machine learning framework.","Our aim with this work is to raise awareness against these new, dangerous attacks both in the research community and industry, and we hope to encourage further research in mitigation techniques against such threats."],"url":"http://arxiv.org/abs/2403.03593v1","category":"cs.CR"}
{"created":"2024-03-06 10:25:36","title":"Wildest Dreams: Reproducible Research in Privacy-preserving Neural Network Training","abstract":"Machine Learning (ML), addresses a multitude of complex issues in multiple disciplines, including social sciences, finance, and medical research. ML models require substantial computing power and are only as powerful as the data utilized. Due to high computational cost of ML methods, data scientists frequently use Machine Learning-as-a-Service (MLaaS) to outsource computation to external servers. However, when working with private information, like financial data or health records, outsourcing the computation might result in privacy issues. Recent advances in Privacy-Preserving Techniques (PPTs) have enabled ML training and inference over protected data through the use of Privacy-Preserving Machine Learning (PPML). However, these techniques are still at a preliminary stage and their application in real-world situations is demanding. In order to comprehend discrepancy between theoretical research suggestions and actual applications, this work examines the past and present of PPML, focusing on Homomorphic Encryption (HE) and Secure Multi-party Computation (SMPC) applied to ML. This work primarily focuses on the ML model's training phase, where maintaining user data privacy is of utmost importance. We provide a solid theoretical background that eases the understanding of current approaches and their limitations. In addition, we present a SoK of the most recent PPML frameworks for model training and provide a comprehensive comparison in terms of the unique properties and performances on standard benchmarks. Also, we reproduce the results for some of the papers and examine at what level existing works in the field provide support for open science. We believe our work serves as a valuable contribution by raising awareness about the current gap between theoretical advancements and real-world applications in PPML, specifically regarding open-source availability, reproducibility, and usability.","sentences":["Machine Learning (ML), addresses a multitude of complex issues in multiple disciplines, including social sciences, finance, and medical research.","ML models require substantial computing power and are only as powerful as the data utilized.","Due to high computational cost of ML methods, data scientists frequently use Machine Learning-as-a-Service (MLaaS) to outsource computation to external servers.","However, when working with private information, like financial data or health records, outsourcing the computation might result in privacy issues.","Recent advances in Privacy-Preserving Techniques (PPTs) have enabled ML training and inference over protected data through the use of Privacy-Preserving Machine Learning (PPML).","However, these techniques are still at a preliminary stage and their application in real-world situations is demanding.","In order to comprehend discrepancy between theoretical research suggestions and actual applications, this work examines the past and present of PPML, focusing on Homomorphic Encryption (HE) and Secure Multi-party Computation (SMPC) applied to ML.","This work primarily focuses on the ML model's training phase, where maintaining user data privacy is of utmost importance.","We provide a solid theoretical background that eases the understanding of current approaches and their limitations.","In addition, we present a SoK of the most recent PPML frameworks for model training and provide a comprehensive comparison in terms of the unique properties and performances on standard benchmarks.","Also, we reproduce the results for some of the papers and examine at what level existing works in the field provide support for open science.","We believe our work serves as a valuable contribution by raising awareness about the current gap between theoretical advancements and real-world applications in PPML, specifically regarding open-source availability, reproducibility, and usability."],"url":"http://arxiv.org/abs/2403.03592v1","category":"cs.CR"}
{"created":"2024-03-06 10:25:17","title":"Correlations of vorticity inside a coherent vortex","abstract":"We examine fluctuations of vorticity inside the coherent vortex, appearing as a consequence of the inverse energy cascade in two-dimensional turbulence. Temporal and spacial correlations can be characterized by the pair correlation function. The interaction between the fluctuations leads to non-zero value of the third moment of vorticity. We examine the pair correlation function and the third moment for the model where the pumping is short correlated in time. We find explicit expressions for the Gaussian spacial correlation function of the pumping force. They confirm the general predictions obtained earlier.","sentences":["We examine fluctuations of vorticity inside the coherent vortex, appearing as a consequence of the inverse energy cascade in two-dimensional turbulence.","Temporal and spacial correlations can be characterized by the pair correlation function.","The interaction between the fluctuations leads to non-zero value of the third moment of vorticity.","We examine the pair correlation function and the third moment for the model where the pumping is short correlated in time.","We find explicit expressions for the Gaussian spacial correlation function of the pumping force.","They confirm the general predictions obtained earlier."],"url":"http://arxiv.org/abs/2403.03591v1","category":"physics.flu-dyn"}
{"created":"2024-03-06 10:24:44","title":"Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choices","abstract":"This study designs an adaptive experiment for efficiently estimating average treatment effect (ATEs). We consider an adaptive experiment where an experimenter sequentially samples an experimental unit from a covariate density decided by the experimenter and assigns a treatment. After assigning a treatment, the experimenter observes the corresponding outcome immediately. At the end of the experiment, the experimenter estimates an ATE using gathered samples. The objective of the experimenter is to estimate the ATE with a smaller asymptotic variance. Existing studies have designed experiments that adaptively optimize the propensity score (treatment-assignment probability). As a generalization of such an approach, we propose a framework under which an experimenter optimizes the covariate density, as well as the propensity score, and find that optimizing both covariate density and propensity score reduces the asymptotic variance more than optimizing only the propensity score. Based on this idea, in each round of our experiment, the experimenter optimizes the covariate density and propensity score based on past observations. To design an adaptive experiment, we first derive the efficient covariate density and propensity score that minimizes the semiparametric efficiency bound, a lower bound for the asymptotic variance given a fixed covariate density and a fixed propensity score. Next, we design an adaptive experiment using the efficient covariate density and propensity score sequentially estimated during the experiment. Lastly, we propose an ATE estimator whose asymptotic variance aligns with the minimized semiparametric efficiency bound.","sentences":["This study designs an adaptive experiment for efficiently estimating average treatment effect (ATEs).","We consider an adaptive experiment where an experimenter sequentially samples an experimental unit from a covariate density decided by the experimenter and assigns a treatment.","After assigning a treatment, the experimenter observes the corresponding outcome immediately.","At the end of the experiment, the experimenter estimates an ATE using gathered samples.","The objective of the experimenter is to estimate the ATE with a smaller asymptotic variance.","Existing studies have designed experiments that adaptively optimize the propensity score (treatment-assignment probability).","As a generalization of such an approach, we propose a framework under which an experimenter optimizes the covariate density, as well as the propensity score, and find that optimizing both covariate density and propensity score reduces the asymptotic variance more than optimizing only the propensity score.","Based on this idea, in each round of our experiment, the experimenter optimizes the covariate density and propensity score based on past observations.","To design an adaptive experiment, we first derive the efficient covariate density and propensity score that minimizes the semiparametric efficiency bound, a lower bound for the asymptotic variance given a fixed covariate density and a fixed propensity score.","Next, we design an adaptive experiment using the efficient covariate density and propensity score sequentially estimated during the experiment.","Lastly, we propose an ATE estimator whose asymptotic variance aligns with the minimized semiparametric efficiency bound."],"url":"http://arxiv.org/abs/2403.03589v1","category":"stat.ME"}
{"created":"2024-03-06 10:11:39","title":"Studying ECG signals using nonlinear oscillators and Genetic Algorithm","abstract":"Cardiovascular diseases are the leading cause of death and disability in the world and thus their detection is extremely important as early as possible so that it can be prognosed and managed appropriately. Hence, electrophysiological models dealing with cardiac conduction are critically important in the field of interdisciplinary sciences. The primary aim of this paper is to reproduce a normal sinus rhythm ECG waveform which will act as the baseline for fitting and then fit any clinical ECG waveform that does not deviate much from normal sinus rhythm. To reproduce the ECG, we modeled the pacemaker complex using three coupled van der Pol (VDP) oscillators with appropriate delays to generate the action potentials. These action potentials are responsible for the excitation of the non-pacemaker cells of the atria and ventricles whose electrical activity gets recorded as the ECG signal. The ECG signal is composed of a periodic set of individual waves corresponding to atrial and ventricular contraction and relaxation. These waves are modeled with the help of four FitzHugh-Nagumo (FHN) equations with impulses corresponding to the action potentials generated by the pacemaker cells. After the successful reproduction of a normal sinus rhythm ECG, we have developed a framework where we have used genetic algorithm (GA) to fit a given clinical ECG data with parameters belonging to the above mentioned system of delay differential equations (DDEs). The GA framework has enabled us to fit ECG data representing different cardiac conditions reasonably well. We aim to use this work to get a better understanding of the cardiac conduction system and cardiovascular diseases which will help humanity in the future.","sentences":["Cardiovascular diseases are the leading cause of death and disability in the world and thus their detection is extremely important as early as possible so that it can be prognosed and managed appropriately.","Hence, electrophysiological models dealing with cardiac conduction are critically important in the field of interdisciplinary sciences.","The primary aim of this paper is to reproduce a normal sinus rhythm ECG waveform which will act as the baseline for fitting and then fit any clinical ECG waveform that does not deviate much from normal sinus rhythm.","To reproduce the ECG, we modeled the pacemaker complex using three coupled van der Pol (VDP) oscillators with appropriate delays to generate the action potentials.","These action potentials are responsible for the excitation of the non-pacemaker cells of the atria and ventricles whose electrical activity gets recorded as the ECG signal.","The ECG signal is composed of a periodic set of individual waves corresponding to atrial and ventricular contraction and relaxation.","These waves are modeled with the help of four FitzHugh-Nagumo (FHN) equations with impulses corresponding to the action potentials generated by the pacemaker cells.","After the successful reproduction of a normal sinus rhythm ECG, we have developed a framework where we have used genetic algorithm (GA) to fit a given clinical ECG data with parameters belonging to the above mentioned system of delay differential equations (DDEs).","The GA framework has enabled us to fit ECG data representing different cardiac conditions reasonably well.","We aim to use this work to get a better understanding of the cardiac conduction system and cardiovascular diseases which will help humanity in the future."],"url":"http://arxiv.org/abs/2403.03587v1","category":"physics.med-ph"}
{"created":"2024-03-06 10:01:35","title":"RouteExplainer: An Explanation Framework for Vehicle Routing Problem","abstract":"The Vehicle Routing Problem (VRP) is a widely studied combinatorial optimization problem and has been applied to various practical problems. While the explainability for VRP is significant for improving the reliability and interactivity in practical VRP applications, it remains unexplored. In this paper, we propose RouteExplainer, a post-hoc explanation framework that explains the influence of each edge in a generated route. Our framework realizes this by rethinking a route as the sequence of actions and extending counterfactual explanations based on the action influence model to VRP. To enhance the explanation, we additionally propose an edge classifier that infers the intentions of each edge, a loss function to train the edge classifier, and explanation-text generation by Large Language Models (LLMs). We quantitatively evaluate our edge classifier on four different VRPs. The results demonstrate its rapid computation while maintaining reasonable accuracy, thereby highlighting its potential for deployment in practical applications. Moreover, on the subject of a tourist route, we qualitatively evaluate explanations generated by our framework. This evaluation not only validates our framework but also shows the synergy between explanation frameworks and LLMs. See https://ntt-dkiku.github.io/xai-vrp for our code, datasets, models, and demo.","sentences":["The Vehicle Routing Problem (VRP) is a widely studied combinatorial optimization problem and has been applied to various practical problems.","While the explainability for VRP is significant for improving the reliability and interactivity in practical VRP applications, it remains unexplored.","In this paper, we propose RouteExplainer, a post-hoc explanation framework that explains the influence of each edge in a generated route.","Our framework realizes this by rethinking a route as the sequence of actions and extending counterfactual explanations based on the action influence model to VRP.","To enhance the explanation, we additionally propose an edge classifier that infers the intentions of each edge, a loss function to train the edge classifier, and explanation-text generation by Large Language Models (LLMs).","We quantitatively evaluate our edge classifier on four different VRPs.","The results demonstrate its rapid computation while maintaining reasonable accuracy, thereby highlighting its potential for deployment in practical applications.","Moreover, on the subject of a tourist route, we qualitatively evaluate explanations generated by our framework.","This evaluation not only validates our framework but also shows the synergy between explanation frameworks and LLMs.","See https://ntt-dkiku.github.io/xai-vrp for our code, datasets, models, and demo."],"url":"http://arxiv.org/abs/2403.03585v1","category":"cs.LG"}
{"created":"2024-03-06 09:58:57","title":"Interactive Bayesian Generative Models for Abnormality Detection in Vehicular Networks","abstract":"The following paper proposes a novel Vehicle-to-Everything (V2X) network abnormality detection scheme based on Bayesian generative models for enhanced network self-awareness functionality at the Base station (BS). In the learning phase, multi-modal data signals contrived by the vehicles' integrated and sensing module are imbued into data-driven Generalized Dynamic Bayesian network (GDBN) models. Following that, during the testing phase, an Interactive Modified Markov Jump Particle filter (IM-MJPF) is utilized to forecast forthcoming network states and vehicle trajectories by leveraging the assimilated semantics embedded in the coupled multi-GDBNs. This approach involves learning statistically correlated association between evolving trajectories and network communication links. Security and surveillance of Internet of Vehicles (IOVs) links are performed online with high detection probabilities by matching predicted with observed network connectivity maps (graphs).","sentences":["The following paper proposes a novel Vehicle-to-Everything (V2X) network abnormality detection scheme based on Bayesian generative models for enhanced network self-awareness functionality at the Base station (BS).","In the learning phase, multi-modal data signals contrived by the vehicles' integrated and sensing module are imbued into data-driven Generalized Dynamic Bayesian network (GDBN) models.","Following that, during the testing phase, an Interactive Modified Markov Jump Particle filter (IM-MJPF) is utilized to forecast forthcoming network states and vehicle trajectories by leveraging the assimilated semantics embedded in the coupled multi-GDBNs.","This approach involves learning statistically correlated association between evolving trajectories and network communication links.","Security and surveillance of Internet of Vehicles (IOVs) links are performed online with high detection probabilities by matching predicted with observed network connectivity maps (graphs)."],"url":"http://arxiv.org/abs/2403.03583v1","category":"eess.SP"}
{"created":"2024-03-06 09:57:52","title":"Design of an Open-Source Architecture for Neural Machine Translation","abstract":"adaptNMT is an open-source application that offers a streamlined approach to the development and deployment of Recurrent Neural Networks and Transformer models. This application is built upon the widely-adopted OpenNMT ecosystem, and is particularly useful for new entrants to the field, as it simplifies the setup of the development environment and creation of train, validation, and test splits. The application offers a graphing feature that illustrates the progress of model training, and employs SentencePiece for creating subword segmentation models. Furthermore, the application provides an intuitive user interface that facilitates hyperparameter customization. Notably, a single-click model development approach has been implemented, and models developed by adaptNMT can be evaluated using a range of metrics. To encourage eco-friendly research, adaptNMT incorporates a green report that flags the power consumption and kgCO${_2}$ emissions generated during model development. The application is freely available.","sentences":["adaptNMT is an open-source application that offers a streamlined approach to the development and deployment of Recurrent Neural Networks and Transformer models.","This application is built upon the widely-adopted OpenNMT ecosystem, and is particularly useful for new entrants to the field, as it simplifies the setup of the development environment and creation of train, validation, and test splits.","The application offers a graphing feature that illustrates the progress of model training, and employs SentencePiece for creating subword segmentation models.","Furthermore, the application provides an intuitive user interface that facilitates hyperparameter customization.","Notably, a single-click model development approach has been implemented, and models developed by adaptNMT can be evaluated using a range of metrics.","To encourage eco-friendly research, adaptNMT incorporates a green report that flags the power consumption and kgCO${_2}$ emissions generated during model development.","The application is freely available."],"url":"http://arxiv.org/abs/2403.03582v1","category":"cs.CL"}
{"created":"2024-03-06 09:57:42","title":"Enhancing ASD detection accuracy: a combined approach of machine learning and deep learning models with natural language processing","abstract":"Purpose: Our study explored the use of artificial intelligence (AI) to diagnose autism spectrum disorder (ASD). It focused on machine learning (ML) and deep learning (DL) to detect ASD from text inputs on social media, addressing challenges in traditional ASD diagnosis.   Methods: We used natural language processing (NLP), ML, and DL models (including decision trees, XGB, KNN, RNN, LSTM, Bi-LSTM, BERT, and BERTweet) to analyze 404,627 tweets, classifying them based on ASD or non-ASD authors. A subset of 90,000 tweets was used for model training and testing.   Results: Our AI models showed high accuracy, with an 88% success rate in identifying texts from individuals with ASD.   Conclusion: The study demonstrates AI's potential in improving ASD diagnosis, especially in children, highlighting the importance of early detection.","sentences":["Purpose: Our study explored the use of artificial intelligence (AI) to diagnose autism spectrum disorder (ASD).","It focused on machine learning (ML) and deep learning (DL) to detect ASD from text inputs on social media, addressing challenges in traditional ASD diagnosis.   ","Methods: We used natural language processing (NLP), ML, and DL models (including decision trees, XGB, KNN, RNN, LSTM, Bi-LSTM, BERT, and BERTweet) to analyze 404,627 tweets, classifying them based on ASD or non-ASD authors.","A subset of 90,000 tweets was used for model training and testing.   ","Results:","Our AI models showed high accuracy, with an 88% success rate in identifying texts from individuals with ASD.   ","Conclusion: The study demonstrates AI's potential in improving ASD diagnosis, especially in children, highlighting the importance of early detection."],"url":"http://arxiv.org/abs/2403.03581v1","category":"cs.CL"}
{"created":"2024-03-06 09:55:29","title":"Testing the unified bounds of quantum speed limit","abstract":"Quantum speed limits (QSLs) impose fundamental constraints on the evolution speed of quantum systems. Traditionally, the Mandelstam-Tamm (MT) and Margolus-Levitin (ML) bounds have been widely employed, relying on the standard deviation and mean of energy distribution to define the QSLs. However, these universal bounds only offer loose restrictions on the quantum evolution. Here we introduce the generalized ML bounds, which prove to be more stringent in constraining dynamic evolution, by utilizing moments of energy spectra of arbitrary orders, even noninteger orders. To validate our findings, we conduct experiments in a superconducting circuit, where we have the capability to prepare a wide range of quantum photonic states and rigorously test these bounds by measuring the evolution of the system and its photon statistics using quantum state tomography. While, in general, the MT bound is effective for short-time evolution, we identify specific parameter regimes where either the MT or the generalized ML bounds suffice to constrain the entire evolution. Our findings not only establish new criteria for estimating QSLs but also substantially enhance our comprehension of the dynamic evolution of quantum systems.","sentences":["Quantum speed limits (QSLs) impose fundamental constraints on the evolution speed of quantum systems.","Traditionally, the Mandelstam-Tamm (MT) and Margolus-Levitin (ML) bounds have been widely employed, relying on the standard deviation and mean of energy distribution to define the QSLs.","However, these universal bounds only offer loose restrictions on the quantum evolution.","Here we introduce the generalized ML bounds, which prove to be more stringent in constraining dynamic evolution, by utilizing moments of energy spectra of arbitrary orders, even noninteger orders.","To validate our findings, we conduct experiments in a superconducting circuit, where we have the capability to prepare a wide range of quantum photonic states and rigorously test these bounds by measuring the evolution of the system and its photon statistics using quantum state tomography.","While, in general, the MT bound is effective for short-time evolution, we identify specific parameter regimes where either the MT or the generalized ML bounds suffice to constrain the entire evolution.","Our findings not only establish new criteria for estimating QSLs but also substantially enhance our comprehension of the dynamic evolution of quantum systems."],"url":"http://arxiv.org/abs/2403.03579v1","category":"quant-ph"}
{"created":"2024-03-06 09:48:48","title":"Causal Disentanglement for Regulating Social Influence Bias in Social Recommendation","abstract":"Social recommendation systems face the problem of social influence bias, which can lead to an overemphasis on recommending items that friends have interacted with. Addressing this problem is crucial, and existing methods often rely on techniques such as weight adjustment or leveraging unbiased data to eliminate this bias. However, we argue that not all biases are detrimental, i.e., some items recommended by friends may align with the user's interests. Blindly eliminating such biases could undermine these positive effects, potentially diminishing recommendation accuracy. In this paper, we propose a Causal Disentanglement-based framework for Regulating Social influence Bias in social recommendation, named CDRSB, to improve recommendation performance. From the perspective of causal inference, we find that the user social network could be regarded as a confounder between the user and item embeddings (treatment) and ratings (outcome). Due to the presence of this social network confounder, two paths exist from user and item embeddings to ratings: a non-causal social influence path and a causal interest path. Building upon this insight, we propose a disentangled encoder that focuses on disentangling user and item embeddings into interest and social influence embeddings. Mutual information-based objectives are designed to enhance the distinctiveness of these disentangled embeddings, eliminating redundant information. Additionally, a regulatory decoder that employs a weight calculation module to dynamically learn the weights of social influence embeddings for effectively regulating social influence bias has been designed. Experimental results on four large-scale real-world datasets Ciao, Epinions, Dianping, and Douban book demonstrate the effectiveness of CDRSB compared to state-of-the-art baselines.","sentences":["Social recommendation systems face the problem of social influence bias, which can lead to an overemphasis on recommending items that friends have interacted with.","Addressing this problem is crucial, and existing methods often rely on techniques such as weight adjustment or leveraging unbiased data to eliminate this bias.","However, we argue that not all biases are detrimental, i.e., some items recommended by friends may align with the user's interests.","Blindly eliminating such biases could undermine these positive effects, potentially diminishing recommendation accuracy.","In this paper, we propose a Causal Disentanglement-based framework for Regulating Social influence Bias in social recommendation, named CDRSB, to improve recommendation performance.","From the perspective of causal inference, we find that the user social network could be regarded as a confounder between the user and item embeddings (treatment) and ratings (outcome).","Due to the presence of this social network confounder, two paths exist from user and item embeddings to ratings: a non-causal social influence path and a causal interest path.","Building upon this insight, we propose a disentangled encoder that focuses on disentangling user and item embeddings into interest and social influence embeddings.","Mutual information-based objectives are designed to enhance the distinctiveness of these disentangled embeddings, eliminating redundant information.","Additionally, a regulatory decoder that employs a weight calculation module to dynamically learn the weights of social influence embeddings for effectively regulating social influence bias has been designed.","Experimental results on four large-scale real-world datasets Ciao, Epinions, Dianping, and Douban book demonstrate the effectiveness of CDRSB compared to state-of-the-art baselines."],"url":"http://arxiv.org/abs/2403.03578v1","category":"cs.SI"}
{"created":"2024-03-06 09:39:14","title":"Unsupervised Incremental Learning with Dual Concept Drift Detection for Identifying Anomalous Sequences","abstract":"In the contemporary digital landscape, the continuous generation of extensive streaming data across diverse domains has become pervasive. Yet, a significant portion of this data remains unlabeled, posing a challenge in identifying infrequent events such as anomalies. This challenge is further amplified in non-stationary environments, where the performance of models can degrade over time due to concept drift. To address these challenges, this paper introduces a new method referred to as VAE4AS (Variational Autoencoder for Anomalous Sequences). VAE4AS integrates incremental learning with dual drift detection mechanisms, employing both a statistical test and a distance-based test. The anomaly detection is facilitated by a Variational Autoencoder. To gauge the effectiveness of VAE4AS, a comprehensive experimental study is conducted using real-world and synthetic datasets characterized by anomalous rates below 10\\% and recurrent drift. The results show that the proposed method surpasses both robust baselines and state-of-the-art techniques, providing compelling evidence for their efficacy in effectively addressing some of the challenges associated with anomalous sequence detection in non-stationary streaming data.","sentences":["In the contemporary digital landscape, the continuous generation of extensive streaming data across diverse domains has become pervasive.","Yet, a significant portion of this data remains unlabeled, posing a challenge in identifying infrequent events such as anomalies.","This challenge is further amplified in non-stationary environments, where the performance of models can degrade over time due to concept drift.","To address these challenges, this paper introduces a new method referred to as VAE4AS (Variational Autoencoder for Anomalous Sequences).","VAE4AS integrates incremental learning with dual drift detection mechanisms, employing both a statistical test and a distance-based test.","The anomaly detection is facilitated by a Variational Autoencoder.","To gauge the effectiveness of VAE4AS, a comprehensive experimental study is conducted using real-world and synthetic datasets characterized by anomalous rates below 10\\% and recurrent drift.","The results show that the proposed method surpasses both robust baselines and state-of-the-art techniques, providing compelling evidence for their efficacy in effectively addressing some of the challenges associated with anomalous sequence detection in non-stationary streaming data."],"url":"http://arxiv.org/abs/2403.03576v1","category":"cs.CE"}
{"created":"2024-03-06 09:36:36","title":"gaHealth: An English-Irish Bilingual Corpus of Health Data","abstract":"Machine Translation is a mature technology for many high-resource language pairs. However in the context of low-resource languages, there is a paucity of parallel data datasets available for developing translation models. Furthermore, the development of datasets for low-resource languages often focuses on simply creating the largest possible dataset for generic translation. The benefits and development of smaller in-domain datasets can easily be overlooked. To assess the merits of using in-domain data, a dataset for the specific domain of health was developed for the low-resource English to Irish language pair. Our study outlines the process used in developing the corpus and empirically demonstrates the benefits of using an in-domain dataset for the health domain. In the context of translating health-related data, models developed using the gaHealth corpus demonstrated a maximum BLEU score improvement of 22.2 points (40%) when compared with top performing models from the LoResMT2021 Shared Task. Furthermore, we define linguistic guidelines for developing gaHealth, the first bilingual corpus of health data for the Irish language, which we hope will be of use to other creators of low-resource data sets. gaHealth is now freely available online and is ready to be explored for further research.","sentences":["Machine Translation is a mature technology for many high-resource language pairs.","However in the context of low-resource languages, there is a paucity of parallel data datasets available for developing translation models.","Furthermore, the development of datasets for low-resource languages often focuses on simply creating the largest possible dataset for generic translation.","The benefits and development of smaller in-domain datasets can easily be overlooked.","To assess the merits of using in-domain data, a dataset for the specific domain of health was developed for the low-resource English to Irish language pair.","Our study outlines the process used in developing the corpus and empirically demonstrates the benefits of using an in-domain dataset for the health domain.","In the context of translating health-related data, models developed using the gaHealth corpus demonstrated a maximum BLEU score improvement of 22.2 points (40%) when compared with top performing models from the LoResMT2021 Shared Task.","Furthermore, we define linguistic guidelines for developing gaHealth, the first bilingual corpus of health data for the Irish language, which we hope will be of use to other creators of low-resource data sets.","gaHealth is now freely available online and is ready to be explored for further research."],"url":"http://arxiv.org/abs/2403.03575v1","category":"cs.CL"}
{"created":"2024-03-06 09:34:29","title":"Formation of limb-brightened radio jets by angle-dependent energy extraction from rapidly rotating black holes","abstract":"By general relativistic magnetohydrodynamic simulations, it is suggested that the rotational energy of a rapidly rotating black hole (BH) is preferentially extracted along the magnetic field lines threading the event horizon in the middle and lower latitudes. Applying this angle-dependent Poynting flux to the jet downstream, we demonstrate that the jets exhibit limb-brightened structures at various viewing angles, as observed from Mrk 501, M87, and Cyg A between 5 and 75 degrees, and that the limb-brightening is enhanced when the jet is collimated strongly. It is also found that the jet width perpendicular to the propagation direction shrinks at the projected distance of the altitude where the jet collimates from a conical shape (near the BH) to a parabolic one (in the jet). Comparing with the VLBI observations, we show this collimation takes place within the de-projected altitude of 100 Schwarzschild radii from the BH in the case of the M87 jet.","sentences":["By general relativistic magnetohydrodynamic simulations, it is suggested that the rotational energy of a rapidly rotating black hole (BH) is preferentially extracted along the magnetic field lines threading the event horizon in the middle and lower latitudes.","Applying this angle-dependent Poynting flux to the jet downstream, we demonstrate that the jets exhibit limb-brightened structures at various viewing angles, as observed from Mrk 501, M87, and Cyg A between 5 and 75 degrees, and that the limb-brightening is enhanced when the jet is collimated strongly.","It is also found that the jet width perpendicular to the propagation direction shrinks at the projected distance of the altitude where the jet collimates from a conical shape (near the BH) to a parabolic one (in the jet).","Comparing with the VLBI observations, we show this collimation takes place within the de-projected altitude of 100 Schwarzschild radii from the BH in the case of the M87 jet."],"url":"http://arxiv.org/abs/2403.03574v1","category":"astro-ph.HE"}
{"created":"2024-03-06 09:30:31","title":"Anisotropic power diagrams for polycrystal modelling: efficient generation of curved grains via optimal transport","abstract":"The microstructure of metals and foams can be effectively modelled with anisotropic power diagrams (APDs), which provide control over the shape of individual grains. One major obstacle to the wider adoption of APDs is the computational cost that is associated with their generation. We propose a novel approach to generate APDs with prescribed statistical properties, including fine control over the size of individual grains. To this end, we rely on fast optimal transport algorithms that stream well on Graphics Processing Units (GPU) and handle non-uniform, anisotropic distance functions. This allows us to find APDs that best fit experimental data in (tens of) seconds, which unlocks their use for computational homogenisation. This is especially relevant to machine learning methods that require the generation of large collections of representative microstructures as training data. The paper is accompanied by a Python library, PyAPD, which is freely available at: www.github.com/mbuze/PyAPD.","sentences":["The microstructure of metals and foams can be effectively modelled with anisotropic power diagrams (APDs), which provide control over the shape of individual grains.","One major obstacle to the wider adoption of APDs is the computational cost that is associated with their generation.","We propose a novel approach to generate APDs with prescribed statistical properties, including fine control over the size of individual grains.","To this end, we rely on fast optimal transport algorithms that stream well on Graphics Processing Units (GPU) and handle non-uniform, anisotropic distance functions.","This allows us to find APDs that best fit experimental data in (tens of) seconds, which unlocks their use for computational homogenisation.","This is especially relevant to machine learning methods that require the generation of large collections of representative microstructures as training data.","The paper is accompanied by a Python library, PyAPD, which is freely available at: www.github.com/mbuze/PyAPD."],"url":"http://arxiv.org/abs/2403.03571v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-06 09:29:56","title":"Optical and spin properties of nitrogen vacancy centers formed along the tracks of high energy heavy ions","abstract":"Exposure of nitrogen doped diamond to high energy, heavy ions induces formation of vacancy related color centers aligned along the trajectories of the ions. Quasi 1D chains of coupled NV centers with lengths of a few tens of microns can be building blocks for quantum information processing and they provide insights into harsh radiation-matter interactions. Here, we report on color center formation in diamond (1 ppm nitrogen) with 1 GeV gold and uranium ions. Using depth-resolved photoluminescence, we observe direct formation of single vacancy related color centers (GR1 centers) along the ion tracks. Mobile vacancies can form NV-centers with native nitrogen atoms during thermal annealing. Molecular dynamics simulations indicate that both isolated vacancies and defect clusters form along ion trajectory through electronic stopping processes, leading to broad color center profiles that range from the sample surface to a depth of about 25 microns. We quantify the spin properties of NV-centers formed by swift heavy ions through optical detection of magnetic resonance (ODMR) and validate the feasibility of using swift-heavy-ion-generated NV$^{-}$ along quasi 1D chains (for isolated tracks from low fluence irradiations) or in thin sheets of coupled 1D spin chains (formed with higher ion fluences) for NV-based magnetometry and for the exploration of quasi 1D and 2D spin textures in diamond.","sentences":["Exposure of nitrogen doped diamond to high energy, heavy ions induces formation of vacancy related color centers aligned along the trajectories of the ions.","Quasi 1D chains of coupled NV centers with lengths of a few tens of microns can be building blocks for quantum information processing and they provide insights into harsh radiation-matter interactions.","Here, we report on color center formation in diamond (1 ppm nitrogen) with 1 GeV gold and uranium ions.","Using depth-resolved photoluminescence, we observe direct formation of single vacancy related color centers (GR1 centers) along the ion tracks.","Mobile vacancies can form NV-centers with native nitrogen atoms during thermal annealing.","Molecular dynamics simulations indicate that both isolated vacancies and defect clusters form along ion trajectory through electronic stopping processes, leading to broad color center profiles that range from the sample surface to a depth of about 25 microns.","We quantify the spin properties of NV-centers formed by swift heavy ions through optical detection of magnetic resonance (ODMR) and validate the feasibility of using swift-heavy-ion-generated NV$^{-}$ along quasi 1D chains (for isolated tracks from low fluence irradiations) or in thin sheets of coupled 1D spin chains (formed with higher ion fluences) for NV-based magnetometry and for the exploration of quasi 1D and 2D spin textures in diamond."],"url":"http://arxiv.org/abs/2403.03570v1","category":"quant-ph"}
{"created":"2024-03-06 09:25:22","title":"On Transfer in Classification: How Well do Subsets of Classes Generalize?","abstract":"In classification, it is usual to observe that models trained on a given set of classes can generalize to previously unseen ones, suggesting the ability to learn beyond the initial task. This ability is often leveraged in the context of transfer learning where a pretrained model can be used to process new classes, with or without fine tuning. Surprisingly, there are a few papers looking at the theoretical roots beyond this phenomenon. In this work, we are interested in laying the foundations of such a theoretical framework for transferability between sets of classes. Namely, we establish a partially ordered set of subsets of classes. This tool allows to represent which subset of classes can generalize to others. In a more practical setting, we explore the ability of our framework to predict which subset of classes can lead to the best performance when testing on all of them. We also explore few-shot learning, where transfer is the golden standard. Our work contributes to better understanding of transfer mechanics and model generalization.","sentences":["In classification, it is usual to observe that models trained on a given set of classes can generalize to previously unseen ones, suggesting the ability to learn beyond the initial task.","This ability is often leveraged in the context of transfer learning where a pretrained model can be used to process new classes, with or without fine tuning.","Surprisingly, there are a few papers looking at the theoretical roots beyond this phenomenon.","In this work, we are interested in laying the foundations of such a theoretical framework for transferability between sets of classes.","Namely, we establish a partially ordered set of subsets of classes.","This tool allows to represent which subset of classes can generalize to others.","In a more practical setting, we explore the ability of our framework to predict which subset of classes can lead to the best performance when testing on all of them.","We also explore few-shot learning, where transfer is the golden standard.","Our work contributes to better understanding of transfer mechanics and model generalization."],"url":"http://arxiv.org/abs/2403.03569v1","category":"cs.LG"}
{"created":"2024-03-06 09:24:49","title":"On feasibility cuts for chance-constrained multicommodity network design problems","abstract":"Problem definition: We study efficient exact solution approaches to solve chance-constrained multicommodity network design problems under demand uncertainty, an important class of network design problems. The chance constraint requires us to construct a network that meets future commodity demand sufficiently often, which makes the problem challenging to solve. Methodology/results: We develop a solution approach based on Benders' decomposition, and accelerate the approach with valid inequalities and cut strengthening. We particularly investigate the effects of different subproblem formulations on the strength of the resulting feasibility cuts. We propose a new formulation that we term FlowMIS, and investigate its properties. Additionally, we numerically show that FlowMIS outperforms standard formulations: in our complete solution approach with all enhancements enabled, FlowMIS solves 67 out of 120 solved instances the fastest, with an average speed-up of 2.0x over a basic formulation. Implications: FlowMIS generates strong feasibility cuts tailored to subproblems with a network flow structure. This results in reduced solution times for existing decomposition-based algorithms in the context of network design, and the ability to solve larger problems.","sentences":["Problem definition: We study efficient exact solution approaches to solve chance-constrained multicommodity network design problems under demand uncertainty, an important class of network design problems.","The chance constraint requires us to construct a network that meets future commodity demand sufficiently often, which makes the problem challenging to solve.","Methodology/results: We develop a solution approach based on Benders' decomposition, and accelerate the approach with valid inequalities and cut strengthening.","We particularly investigate the effects of different subproblem formulations on the strength of the resulting feasibility cuts.","We propose a new formulation that we term FlowMIS, and investigate its properties.","Additionally, we numerically show that FlowMIS outperforms standard formulations: in our complete solution approach with all enhancements enabled, FlowMIS solves 67 out of 120 solved instances the fastest, with an average speed-up of 2.0x over a basic formulation.","Implications: FlowMIS generates strong feasibility cuts tailored to subproblems with a network flow structure.","This results in reduced solution times for existing decomposition-based algorithms in the context of network design, and the ability to solve larger problems."],"url":"http://arxiv.org/abs/2403.03567v1","category":"math.OC"}
{"created":"2024-03-06 09:09:40","title":"Sparse convex relaxations in polynomial optimization","abstract":"We present a novel, general, and unifying point of view on sparse approaches to polynomial optimization. Solving polynomial optimization problems to global optimality is a ubiquitous challenge in many areas of science and engineering. Historically, different approaches on how to solve nonconvex polynomial optimization problems based on convex relaxations have been developed in different scientific communities. Here, we introduce the concept of monomial patterns. A pattern determines what monomials are to be linked by convex constraints in a convex relaxation of a polynomial optimization problem. This concept helps to understand existing approaches from different schools of thought, to develop novel relaxation schemes, and to derive a flexible duality theory, which can be specialized to many concrete situations that have been considered in the literature. We unify different approaches to polynomial optimization including polyhedral approximations, dense semidefinite relaxations, SONC, SAGE, and TSSOS in a self-contained exposition. We also carry out computational experiments to demonstrate the practical advantages of a flexible usage of pattern-based sparse relaxations of polynomial optimization problems.","sentences":["We present a novel, general, and unifying point of view on sparse approaches to polynomial optimization.","Solving polynomial optimization problems to global optimality is a ubiquitous challenge in many areas of science and engineering.","Historically, different approaches on how to solve nonconvex polynomial optimization problems based on convex relaxations have been developed in different scientific communities.","Here, we introduce the concept of monomial patterns.","A pattern determines what monomials are to be linked by convex constraints in a convex relaxation of a polynomial optimization problem.","This concept helps to understand existing approaches from different schools of thought, to develop novel relaxation schemes, and to derive a flexible duality theory, which can be specialized to many concrete situations that have been considered in the literature.","We unify different approaches to polynomial optimization including polyhedral approximations, dense semidefinite relaxations, SONC, SAGE, and TSSOS in a self-contained exposition.","We also carry out computational experiments to demonstrate the practical advantages of a flexible usage of pattern-based sparse relaxations of polynomial optimization problems."],"url":"http://arxiv.org/abs/2403.03560v1","category":"math.OC"}
{"created":"2024-03-06 08:50:25","title":"Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models","abstract":"This study investigates the generation of synthetic disinformation by OpenAI's Large Language Models (LLMs) through prompt engineering and explores their responsiveness to emotional prompting. Leveraging various LLM iterations using davinci-002, davinci-003, gpt-3.5-turbo and gpt-4, we designed experiments to assess their success in producing disinformation. Our findings, based on a corpus of 19,800 synthetic disinformation social media posts, reveal that all LLMs by OpenAI can successfully produce disinformation, and that they effectively respond to emotional prompting, indicating their nuanced understanding of emotional cues in text generation. When prompted politely, all examined LLMs consistently generate disinformation at a high frequency. Conversely, when prompted impolitely, the frequency of disinformation production diminishes, as the models often refuse to generate disinformation and instead caution users that the tool is not intended for such purposes. This research contributes to the ongoing discourse surrounding responsible development and application of AI technologies, particularly in mitigating the spread of disinformation and promoting transparency in AI-generated content.","sentences":["This study investigates the generation of synthetic disinformation by OpenAI's Large Language Models (LLMs) through prompt engineering and explores their responsiveness to emotional prompting.","Leveraging various LLM iterations using davinci-002, davinci-003, gpt-3.5-turbo and gpt-4, we designed experiments to assess their success in producing disinformation.","Our findings, based on a corpus of 19,800 synthetic disinformation social media posts, reveal that all LLMs by OpenAI can successfully produce disinformation, and that they effectively respond to emotional prompting, indicating their nuanced understanding of emotional cues in text generation.","When prompted politely, all examined LLMs consistently generate disinformation at a high frequency.","Conversely, when prompted impolitely, the frequency of disinformation production diminishes, as the models often refuse to generate disinformation and instead caution users that the tool is not intended for such purposes.","This research contributes to the ongoing discourse surrounding responsible development and application of AI technologies, particularly in mitigating the spread of disinformation and promoting transparency in AI-generated content."],"url":"http://arxiv.org/abs/2403.03550v1","category":"cs.AI"}
{"created":"2024-03-06 08:48:56","title":"Extracting quadratic propagators by refined graphic rule","abstract":"One-loop integrands in Cachazo-He-Yuan (CHY) formula, which is based on the forward limit of tree-level amplitudes, involves linear propagators that are different from quadratic ones in traditional Feynman diagrams. In this paper, we provide a general approach to converting linear propagators in one-loop CHY formula into quadratic propagators, by refined graphic rule stemming from the recursive expansion of tree-level Einstein-Yang-Mills amplitudes. We establish the correspondence between refined graphs with cut lines and the bi-adjoint scalar (BS) Feynman diagrams with linear propagators. Using this correspondence and helpful properties of Berends-Giele currents in BS theory, we convert the linear propagators in single-trace Yang-Mills-scalar (YMS) integrands (with a pure-scalar loop) into quadratic ones. This result is further demonstrated to match the traditional one-loop Feynman diagrams. The discussions on single-trace YMS integrands are generalized to multi-trace YMS and Yang-Mills integrands.","sentences":["One-loop integrands in Cachazo-He-Yuan (CHY) formula, which is based on the forward limit of tree-level amplitudes, involves linear propagators that are different from quadratic ones in traditional Feynman diagrams.","In this paper, we provide a general approach to converting linear propagators in one-loop CHY formula into quadratic propagators, by refined graphic rule stemming from the recursive expansion of tree-level Einstein-Yang-Mills amplitudes.","We establish the correspondence between refined graphs with cut lines and the bi-adjoint scalar (BS) Feynman diagrams with linear propagators.","Using this correspondence and helpful properties of Berends-Giele currents in BS theory, we convert the linear propagators in single-trace Yang-Mills-scalar (YMS) integrands (with a pure-scalar loop) into quadratic ones.","This result is further demonstrated to match the traditional one-loop Feynman diagrams.","The discussions on single-trace YMS integrands are generalized to multi-trace YMS and Yang-Mills integrands."],"url":"http://arxiv.org/abs/2403.03547v1","category":"hep-th"}
{"created":"2024-03-06 08:47:31","title":"Diffusion-based Generative Prior for Low-Complexity MIMO Channel Estimation","abstract":"This work proposes a novel channel estimator based on diffusion models (DMs), one of the currently top-rated generative models. Contrary to related works utilizing generative priors, a lightweight convolutional neural network (CNN) with positional embedding of the signal-to-noise ratio (SNR) information is designed by learning the channel distribution in the sparse angular domain. Combined with an estimation strategy that avoids stochastic resampling and truncates reverse diffusion steps that account for lower SNR than the given pilot observation, the resulting DM estimator has both low complexity and memory overhead. Numerical results exhibit better performance than state-of-the-art channel estimators utilizing generative priors.","sentences":["This work proposes a novel channel estimator based on diffusion models (DMs), one of the currently top-rated generative models.","Contrary to related works utilizing generative priors, a lightweight convolutional neural network (CNN) with positional embedding of the signal-to-noise ratio (SNR) information is designed by learning the channel distribution in the sparse angular domain.","Combined with an estimation strategy that avoids stochastic resampling and truncates reverse diffusion steps that account for lower SNR than the given pilot observation, the resulting DM estimator has both low complexity and memory overhead.","Numerical results exhibit better performance than state-of-the-art channel estimators utilizing generative priors."],"url":"http://arxiv.org/abs/2403.03545v1","category":"eess.SP"}
{"created":"2024-03-06 08:43:30","title":"Prompt Mining for Language-based Human Mobility Forecasting","abstract":"With the advancement of large language models, language-based forecasting has recently emerged as an innovative approach for predicting human mobility patterns. The core idea is to use prompts to transform the raw mobility data given as numerical values into natural language sentences so that the language models can be leveraged to generate the description for future observations. However, previous studies have only employed fixed and manually designed templates to transform numerical values into sentences. Since the forecasting performance of language models heavily relies on prompts, using fixed templates for prompting may limit the forecasting capability of language models. In this paper, we propose a novel framework for prompt mining in language-based mobility forecasting, aiming to explore diverse prompt design strategies. Specifically, the framework includes a prompt generation stage based on the information entropy of prompts and a prompt refinement stage to integrate mechanisms such as the chain of thought. Experimental results on real-world large-scale data demonstrate the superiority of generated prompts from our prompt mining pipeline. Additionally, the comparison of different prompt variants shows that the proposed prompt refinement process is effective. Our study presents a promising direction for further advancing language-based mobility forecasting.","sentences":["With the advancement of large language models, language-based forecasting has recently emerged as an innovative approach for predicting human mobility patterns.","The core idea is to use prompts to transform the raw mobility data given as numerical values into natural language sentences so that the language models can be leveraged to generate the description for future observations.","However, previous studies have only employed fixed and manually designed templates to transform numerical values into sentences.","Since the forecasting performance of language models heavily relies on prompts, using fixed templates for prompting may limit the forecasting capability of language models.","In this paper, we propose a novel framework for prompt mining in language-based mobility forecasting, aiming to explore diverse prompt design strategies.","Specifically, the framework includes a prompt generation stage based on the information entropy of prompts and a prompt refinement stage to integrate mechanisms such as the chain of thought.","Experimental results on real-world large-scale data demonstrate the superiority of generated prompts from our prompt mining pipeline.","Additionally, the comparison of different prompt variants shows that the proposed prompt refinement process is effective.","Our study presents a promising direction for further advancing language-based mobility forecasting."],"url":"http://arxiv.org/abs/2403.03544v1","category":"cs.AI"}
{"created":"2024-03-06 08:38:34","title":"DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training","abstract":"Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performance on diverse downstream PDE tasks like 3D data. Code is available at \\url{https://github.com/thu-ml/DPOT}.","sentences":["Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings.","However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data.","In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks.","Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training.","We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories.","Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performance on diverse downstream PDE tasks like 3D data.","Code is available at \\url{https://github.com/thu-ml/DPOT}."],"url":"http://arxiv.org/abs/2403.03542v1","category":"cs.LG"}
{"created":"2024-03-06 08:37:36","title":"Seamless Virtual Reality with Integrated Synchronizer and Synthesizer for Autonomous Driving","abstract":"Virtual reality (VR) is a promising data engine for autonomous driving (AD). However, data fidelity in this paradigm is often degraded by VR inconsistency, for which the existing VR approaches become ineffective, as they ignore the inter-dependency between low-level VR synchronizer designs (i.e., data collector) and high-level VR synthesizer designs (i.e., data processor). This paper presents a seamless virtual reality SVR platform for AD, which mitigates such inconsistency, enabling VR agents to interact with each other in a shared symbiotic world. The crux to SVR is an integrated synchronizer and synthesizer IS2 design, which consists of a drift-aware lidar-inertial synchronizer for VR colocation and a motion-aware deep visual synthesis network for augmented reality image generation. We implement SVR on car-like robots in two sandbox platforms, achieving a cm-level VR colocalization accuracy and 3.2% VR image deviation, thereby avoiding missed collisions or model clippings. Experiments show that the proposed SVR reduces the intervention times, missed turns, and failure rates compared to other benchmarks. The SVR-trained neural network can handle unseen situations in real-world environments, by leveraging its knowledge learnt from the VR space.","sentences":["Virtual reality (VR) is a promising data engine for autonomous driving (AD).","However, data fidelity in this paradigm is often degraded by VR inconsistency, for which the existing VR approaches become ineffective, as they ignore the inter-dependency between low-level VR synchronizer designs (i.e., data collector) and high-level VR synthesizer designs (i.e., data processor).","This paper presents a seamless virtual reality SVR platform for AD, which mitigates such inconsistency, enabling VR agents to interact with each other in a shared symbiotic world.","The crux to SVR is an integrated synchronizer and synthesizer IS2 design, which consists of a drift-aware lidar-inertial synchronizer for VR colocation and a motion-aware deep visual synthesis network for augmented reality image generation.","We implement SVR on car-like robots in two sandbox platforms, achieving a cm-level VR colocalization accuracy and 3.2% VR image deviation, thereby avoiding missed collisions or model clippings.","Experiments show that the proposed SVR reduces the intervention times, missed turns, and failure rates compared to other benchmarks.","The SVR-trained neural network can handle unseen situations in real-world environments, by leveraging its knowledge learnt from the VR space."],"url":"http://arxiv.org/abs/2403.03541v1","category":"cs.RO"}
{"created":"2024-03-06 08:34:28","title":"RADIA -- Radio Advertisement Detection with Intelligent Analytics","abstract":"Radio advertising remains an integral part of modern marketing strategies, with its appeal and potential for targeted reach undeniably effective. However, the dynamic nature of radio airtime and the rising trend of multiple radio spots necessitates an efficient system for monitoring advertisement broadcasts. This study investigates a novel automated radio advertisement detection technique incorporating advanced speech recognition and text classification algorithms. RadIA's approach surpasses traditional methods by eliminating the need for prior knowledge of the broadcast content. This contribution allows for detecting impromptu and newly introduced advertisements, providing a comprehensive solution for advertisement detection in radio broadcasting. Experimental results show that the resulting model, trained on carefully segmented and tagged text data, achieves an F1-macro score of 87.76 against a theoretical maximum of 89.33. This paper provides insights into the choice of hyperparameters and their impact on the model's performance. This study demonstrates its potential to ensure compliance with advertising broadcast contracts and offer competitive surveillance. This groundbreaking research could fundamentally change how radio advertising is monitored and open new doors for marketing optimization.","sentences":["Radio advertising remains an integral part of modern marketing strategies, with its appeal and potential for targeted reach undeniably effective.","However, the dynamic nature of radio airtime and the rising trend of multiple radio spots necessitates an efficient system for monitoring advertisement broadcasts.","This study investigates a novel automated radio advertisement detection technique incorporating advanced speech recognition and text classification algorithms.","RadIA's approach surpasses traditional methods by eliminating the need for prior knowledge of the broadcast content.","This contribution allows for detecting impromptu and newly introduced advertisements, providing a comprehensive solution for advertisement detection in radio broadcasting.","Experimental results show that the resulting model, trained on carefully segmented and tagged text data, achieves an F1-macro score of 87.76 against a theoretical maximum of 89.33.","This paper provides insights into the choice of hyperparameters and their impact on the model's performance.","This study demonstrates its potential to ensure compliance with advertising broadcast contracts and offer competitive surveillance.","This groundbreaking research could fundamentally change how radio advertising is monitored and open new doors for marketing optimization."],"url":"http://arxiv.org/abs/2403.03538v1","category":"cs.SD"}
{"created":"2024-03-06 08:32:29","title":"On the Second-Order Asymptotics of the Hoeffding Test and Other Divergence Tests","abstract":"Consider a binary statistical hypothesis testing problem, where $n$ independent and identically distributed random variables $Z^n$ are either distributed according to the null hypothesis $P$ or the alternative hypothesis $Q$, and only $P$ is known. A well-known test that is suitable for this case is the so-called Hoeffding test, which accepts $P$ if the Kullback-Leibler (KL) divergence between the empirical distribution of $Z^n$ and $P$ is below some threshold. This work characterizes the first and second-order terms of the type-II error probability for a fixed type-I error probability for the Hoeffding test as well as for divergence tests, where the KL divergence is replaced by a general divergence. It is demonstrated that, irrespective of the divergence, divergence tests achieve the first-order term of the Neyman-Pearson test, which is the optimal test when both $P$ and $Q$ are known. In contrast, the second-order term of divergence tests is strictly worse than that of the Neyman-Pearson test. It is further demonstrated that divergence tests with an invariant divergence achieve the same second-order term as the Hoeffding test, but divergence tests with a non-invariant divergence may outperform the Hoeffding test for some alternative hypotheses $Q$. Potentially, this behavior could be exploited by a composite hypothesis test with partial knowledge of the alternative hypothesis $Q$ by tailoring the divergence of the divergence test to the set of possible alternative hypotheses.","sentences":["Consider a binary statistical hypothesis testing problem, where $n$ independent and identically distributed random variables $Z^n$ are either distributed according to the null hypothesis $P$ or the alternative hypothesis $Q$, and only $P$ is known.","A well-known test that is suitable for this case is the so-called Hoeffding test, which accepts $P$ if the Kullback-Leibler (KL) divergence between the empirical distribution of $Z^n$ and $P$ is below some threshold.","This work characterizes the first and second-order terms of the type-II error probability for a fixed type-I error probability for the Hoeffding test as well as for divergence tests, where the KL divergence is replaced by a general divergence.","It is demonstrated that, irrespective of the divergence, divergence tests achieve the first-order term of the Neyman-Pearson test, which is the optimal test when both $P$ and $Q$ are known.","In contrast, the second-order term of divergence tests is strictly worse than that of the Neyman-Pearson test.","It is further demonstrated that divergence tests with an invariant divergence achieve the same second-order term as the Hoeffding test, but divergence tests with a non-invariant divergence may outperform the Hoeffding test for some alternative hypotheses $Q$. Potentially, this behavior could be exploited by a composite hypothesis test with partial knowledge of the alternative hypothesis $Q$ by tailoring the divergence of the divergence test to the set of possible alternative hypotheses."],"url":"http://arxiv.org/abs/2403.03537v1","category":"cs.IT"}
{"created":"2024-03-06 08:31:35","title":"Towards Efficient and Effective Unlearning of Large Language Models for Recommendation","abstract":"The significant advancements in large language models (LLMs) give rise to a promising research direction, i.e., leveraging LLMs as recommenders (LLMRec). The efficacy of LLMRec arises from the open-world knowledge and reasoning capabilities inherent in LLMs. LLMRec acquires the recommendation capabilities through instruction tuning based on user interaction data. However, in order to protect user privacy and optimize utility, it is also crucial for LLMRec to intentionally forget specific user data, which is generally referred to as recommendation unlearning. In the era of LLMs, recommendation unlearning poses new challenges for LLMRec in terms of \\textit{inefficiency} and \\textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process. To this end, we propose \\textbf{E2URec}, the first \\underline{E}fficient and \\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Our proposed E2URec enhances the unlearning efficiency by updating only a few additional LoRA parameters, and improves the unlearning effectiveness by employing a teacher-student framework, where we maintain multiple teacher networks to guide the unlearning process. Extensive experiments show that E2URec outperforms state-of-the-art baselines on two real-world datasets. Specifically, E2URec can efficiently forget specific data without affecting recommendation performance. The source code is at \\url{https://github.com/justarter/E2URec}.","sentences":["The significant advancements in large language models (LLMs) give rise to a promising research direction, i.e., leveraging LLMs as recommenders (LLMRec).","The efficacy of LLMRec arises from the open-world knowledge and reasoning capabilities inherent in LLMs.","LLMRec acquires the recommendation capabilities through instruction tuning based on user interaction data.","However, in order to protect user privacy and optimize utility, it is also crucial for LLMRec to intentionally forget specific user data, which is generally referred to as recommendation unlearning.","In the era of LLMs, recommendation unlearning poses new challenges for LLMRec in terms of \\textit{inefficiency} and \\textit{ineffectiveness}.","Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming.","Besides, they always impact the model utility during the unlearning process.","To this end, we propose \\textbf{E2URec}, the first \\underline{E}fficient and \\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}.","Our proposed E2URec enhances the unlearning efficiency by updating only a few additional LoRA parameters, and improves the unlearning effectiveness by employing a teacher-student framework, where we maintain multiple teacher networks to guide the unlearning process.","Extensive experiments show that E2URec outperforms state-of-the-art baselines on two real-world datasets.","Specifically, E2URec can efficiently forget specific data without affecting recommendation performance.","The source code is at \\url{https://github.com/justarter/E2URec}."],"url":"http://arxiv.org/abs/2403.03536v1","category":"cs.IR"}
{"created":"2024-03-06 08:29:45","title":"Task Attribute Distance for Few-Shot Learning: Theoretical Analysis and Applications","abstract":"Few-shot learning (FSL) aims to learn novel tasks with very few labeled samples by leveraging experience from \\emph{related} training tasks. In this paper, we try to understand FSL by delving into two key questions: (1) How to quantify the relationship between \\emph{training} and \\emph{novel} tasks? (2) How does the relationship affect the \\emph{adaptation difficulty} on novel tasks for different models? To answer the two questions, we introduce Task Attribute Distance (TAD) built upon attributes as a metric to quantify the task relatedness. Unlike many existing metrics, TAD is model-agnostic, making it applicable to different FSL models. Then, we utilize TAD metric to establish a theoretical connection between task relatedness and task adaptation difficulty. By deriving the generalization error bound on a novel task, we discover how TAD measures the adaptation difficulty on novel tasks for FSL models. To validate our TAD metric and theoretical findings, we conduct experiments on three benchmarks. Our experimental results confirm that TAD metric effectively quantifies the task relatedness and reflects the adaptation difficulty on novel tasks for various FSL methods, even if some of them do not learn attributes explicitly or human-annotated attributes are not available. Finally, we present two applications of the proposed TAD metric: data augmentation and test-time intervention, which further verify its effectiveness and general applicability. The source code is available at https://github.com/hu-my/TaskAttributeDistance.","sentences":["Few-shot learning (FSL) aims to learn novel tasks with very few labeled samples by leveraging experience from \\emph{related} training tasks.","In this paper, we try to understand FSL by delving into two key questions: (1) How to quantify the relationship between \\emph{training} and \\emph{novel} tasks?","(2) How does the relationship affect the \\emph{adaptation difficulty} on novel tasks for different models?","To answer the two questions, we introduce Task Attribute Distance (TAD) built upon attributes as a metric to quantify the task relatedness.","Unlike many existing metrics, TAD is model-agnostic, making it applicable to different FSL models.","Then, we utilize TAD metric to establish a theoretical connection between task relatedness and task adaptation difficulty.","By deriving the generalization error bound on a novel task, we discover how TAD measures the adaptation difficulty on novel tasks for FSL models.","To validate our TAD metric and theoretical findings, we conduct experiments on three benchmarks.","Our experimental results confirm that TAD metric effectively quantifies the task relatedness and reflects the adaptation difficulty on novel tasks for various FSL methods, even if some of them do not learn attributes explicitly or human-annotated attributes are not available.","Finally, we present two applications of the proposed TAD metric: data augmentation and test-time intervention, which further verify its effectiveness and general applicability.","The source code is available at https://github.com/hu-my/TaskAttributeDistance."],"url":"http://arxiv.org/abs/2403.03535v1","category":"cs.CV"}
{"created":"2024-03-06 08:18:02","title":"Extend Your Own Correspondences: Unsupervised Distant Point Cloud Registration by Progressive Distance Extension","abstract":"Registration of point clouds collected from a pair of distant vehicles provides a comprehensive and accurate 3D view of the driving scenario, which is vital for driving safety related applications, yet existing literature suffers from the expensive pose label acquisition and the deficiency to generalize to new data distributions. In this paper, we propose EYOC, an unsupervised distant point cloud registration method that adapts to new point cloud distributions on the fly, requiring no global pose labels. The core idea of EYOC is to train a feature extractor in a progressive fashion, where in each round, the feature extractor, trained with near point cloud pairs, can label slightly farther point cloud pairs, enabling self-supervision on such far point cloud pairs. This process continues until the derived extractor can be used to register distant point clouds. Particularly, to enable high-fidelity correspondence label generation, we devise an effective spatial filtering scheme to select the most representative correspondences to register a point cloud pair, and then utilize the aligned point clouds to discover more correct correspondences. Experiments show that EYOC can achieve comparable performance with state-of-the-art supervised methods at a lower training cost. Moreover, it outwits supervised methods regarding generalization performance on new data distributions.","sentences":["Registration of point clouds collected from a pair of distant vehicles provides a comprehensive and accurate 3D view of the driving scenario, which is vital for driving safety related applications, yet existing literature suffers from the expensive pose label acquisition and the deficiency to generalize to new data distributions.","In this paper, we propose EYOC, an unsupervised distant point cloud registration method that adapts to new point cloud distributions on the fly, requiring no global pose labels.","The core idea of EYOC is to train a feature extractor in a progressive fashion, where in each round, the feature extractor, trained with near point cloud pairs, can label slightly farther point cloud pairs, enabling self-supervision on such far point cloud pairs.","This process continues until the derived extractor can be used to register distant point clouds.","Particularly, to enable high-fidelity correspondence label generation, we devise an effective spatial filtering scheme to select the most representative correspondences to register a point cloud pair, and then utilize the aligned point clouds to discover more correct correspondences.","Experiments show that EYOC can achieve comparable performance with state-of-the-art supervised methods at a lower training cost.","Moreover, it outwits supervised methods regarding generalization performance on new data distributions."],"url":"http://arxiv.org/abs/2403.03532v1","category":"cs.CV"}
{"created":"2024-03-06 08:07:11","title":"Radiation-mediated shocks in GRB prompt emission","abstract":"The debate regarding the emission mechanism in gamma-ray bursts has been long-standing. Here, we study the spectral signatures of photospheric emission, accounting for subphotospheric dissipation by a radiation-mediated shock. The shocks are modeled using the Kompaneets RMS approximation (KRA). We find that the resulting observed spectra are soft, broad, and exhibit an additional break at lower energies. When fitting a collection of 150 mock data samples generated by the model, we obtain a distribution of the low-energy index $\\alpha$ that is similar to the observed one. These results are promising and show that dissipative photospheric models can account for many of the observed properties of prompt gamma-ray burst emission.","sentences":["The debate regarding the emission mechanism in gamma-ray bursts has been long-standing.","Here, we study the spectral signatures of photospheric emission, accounting for subphotospheric dissipation by a radiation-mediated shock.","The shocks are modeled using the Kompaneets RMS approximation (KRA).","We find that the resulting observed spectra are soft, broad, and exhibit an additional break at lower energies.","When fitting a collection of 150 mock data samples generated by the model, we obtain a distribution of the low-energy index $\\alpha$ that is similar to the observed one.","These results are promising and show that dissipative photospheric models can account for many of the observed properties of prompt gamma-ray burst emission."],"url":"http://arxiv.org/abs/2403.03529v1","category":"astro-ph.HE"}
{"created":"2024-03-06 08:04:29","title":"High-harmonic generation in graphene under the application of a DC electric current: From perturbative to non-perturbative regimes","abstract":"We theoretically investigate high-harmonic generation (HHG) in honeycomb-lattice graphene models when subjected to a DC electric field. By integrating the quantum master equation with the Boltzmann equation, we develop a numerical method to compute laser-driven dynamics in many-electron lattice systems under DC electric current. The method enables us to treat both the weak-laser (perturbative) and intense-laser (non-perturbative) regimes in a unified way, accounting for the experimentally inevitable dissipation effects. From it, we obtain the HHG spectra and analyze their dependence on laser frequency, laser intensity, laser-field direction, and DC current strength. We show that the dynamical and static symmetries are partially broken by a DC current or staggered potential term, and such symmetry breakings drastically change the shape of the HHG spectra, especially in terms of the presence or absence of $(2n+1)$-th, $2n$-th, or $3n$-th order harmonics ($n\\in \\mathbb Z$). The laser intensity, frequency, and polarization are also shown to affect the shape of the HHG spectra. Our findings indicate that HHG spectra in conducting electron systems can be quantitatively or qualitatively controlled by tuning various external parameters, and DC electric current is used as such an efficient parameter.","sentences":["We theoretically investigate high-harmonic generation (HHG) in honeycomb-lattice graphene models when subjected to a DC electric field.","By integrating the quantum master equation with the Boltzmann equation, we develop a numerical method to compute laser-driven dynamics in many-electron lattice systems under DC electric current.","The method enables us to treat both the weak-laser (perturbative) and intense-laser (non-perturbative) regimes in a unified way, accounting for the experimentally inevitable dissipation effects.","From it, we obtain the HHG spectra and analyze their dependence on laser frequency, laser intensity, laser-field direction, and DC current strength.","We show that the dynamical and static symmetries are partially broken by a DC current or staggered potential term, and such symmetry breakings drastically change the shape of the HHG spectra, especially in terms of the presence or absence of $(2n+1)$-th, $2n$-th, or $3n$-th order harmonics ($n\\in \\mathbb Z$).","The laser intensity, frequency, and polarization are also shown to affect the shape of the HHG spectra.","Our findings indicate that HHG spectra in conducting electron systems can be quantitatively or qualitatively controlled by tuning various external parameters, and DC electric current is used as such an efficient parameter."],"url":"http://arxiv.org/abs/2403.03523v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-06 08:03:05","title":"Non-verbal information in spontaneous speech - towards a new framework of analysis","abstract":"Non-verbal signals in speech are encoded by prosody and carry information that ranges from conversation action to attitude and emotion. Despite its importance, the principles that govern prosodic structure are not yet adequately understood. This paper offers an analytical schema and a technological proof-of-concept for the categorization of prosodic signals and their association with meaning. The schema interprets surface-representations of multi-layered prosodic events. As a first step towards implementation, we present a classification process that disentangles prosodic phenomena of three orders. It relies on fine-tuning a pre-trained speech recognition model, enabling the simultaneous multi-class/multi-label detection. It generalizes over a large variety of spontaneous data, performing on a par with, or superior to, human annotation. In addition to a standardized formalization of prosody, disentangling prosodic patterns can direct a theory of communication and speech organization. A welcome by-product is an interpretation of prosody that will enhance speech- and language-related technologies.","sentences":["Non-verbal signals in speech are encoded by prosody and carry information that ranges from conversation action to attitude and emotion.","Despite its importance, the principles that govern prosodic structure are not yet adequately understood.","This paper offers an analytical schema and a technological proof-of-concept for the categorization of prosodic signals and their association with meaning.","The schema interprets surface-representations of multi-layered prosodic events.","As a first step towards implementation, we present a classification process that disentangles prosodic phenomena of three orders.","It relies on fine-tuning a pre-trained speech recognition model, enabling the simultaneous multi-class/multi-label detection.","It generalizes over a large variety of spontaneous data, performing on a par with, or superior to, human annotation.","In addition to a standardized formalization of prosody, disentangling prosodic patterns can direct a theory of communication and speech organization.","A welcome by-product is an interpretation of prosody that will enhance speech- and language-related technologies."],"url":"http://arxiv.org/abs/2403.03522v1","category":"cs.SD"}
{"created":"2024-03-06 08:02:21","title":"BiVert: Bidirectional Vocabulary Evaluation using Relations for Machine Translation","abstract":"Neural machine translation (NMT) has progressed rapidly in the past few years, promising improvements and quality translations for different languages. Evaluation of this task is crucial to determine the quality of the translation. Overall, insufficient emphasis is placed on the actual sense of the translation in traditional methods. We propose a bidirectional semantic-based evaluation method designed to assess the sense distance of the translation from the source text. This approach employs the comprehensive multilingual encyclopedic dictionary BabelNet. Through the calculation of the semantic distance between the source and its back translation of the output, our method introduces a quantifiable approach that empowers sentence comparison on the same linguistic level. Factual analysis shows a strong correlation between the average evaluation scores generated by our method and the human assessments across various machine translation systems for English-German language pair. Finally, our method proposes a new multilingual approach to rank MT systems without the need for parallel corpora.","sentences":["Neural machine translation (NMT) has progressed rapidly in the past few years, promising improvements and quality translations for different languages.","Evaluation of this task is crucial to determine the quality of the translation.","Overall, insufficient emphasis is placed on the actual sense of the translation in traditional methods.","We propose a bidirectional semantic-based evaluation method designed to assess the sense distance of the translation from the source text.","This approach employs the comprehensive multilingual encyclopedic dictionary BabelNet.","Through the calculation of the semantic distance between the source and its back translation of the output, our method introduces a quantifiable approach that empowers sentence comparison on the same linguistic level.","Factual analysis shows a strong correlation between the average evaluation scores generated by our method and the human assessments across various machine translation systems for English-German language pair.","Finally, our method proposes a new multilingual approach to rank MT systems without the need for parallel corpora."],"url":"http://arxiv.org/abs/2403.03521v1","category":"cs.CL"}
{"created":"2024-03-06 07:58:35","title":"Tracing Dirac points of topological surface states by ferromagnetic resonance","abstract":"Ferromagnetic resonance is used to reveal features of the buried electronic band structure at interfaces between ferromagnetic metals and topological insulators. By monitoring the evolution of magnetic damping, the application of this method to a hybrid structure consisting of a ferromagnetic layer and a 3D topological insulator reveals a clear fingerprint of the Dirac point and exhibits additional features of the interfacial band structure not otherwise observable. The underlying spin-pumping mechanism is discussed in the framework of dissipation of angular momentum by topological surface states (TSSs). Tuning of the Fermi level within the TSS was verified both by varying the stoichiometry of the topological insulator layer and by electrostatic backgating and the damping values obtained in both cases show a remarkable agreement. The high energy resolution of this method additionally allows us to resolve the energetic shift of the local Dirac points generated by local variations of the electrostatic potential. Calculations based on the chiral tunneling process naturally occurring in TSS agree well with the experimental results.","sentences":["Ferromagnetic resonance is used to reveal features of the buried electronic band structure at interfaces between ferromagnetic metals and topological insulators.","By monitoring the evolution of magnetic damping, the application of this method to a hybrid structure consisting of a ferromagnetic layer and a 3D topological insulator reveals a clear fingerprint of the Dirac point and exhibits additional features of the interfacial band structure not otherwise observable.","The underlying spin-pumping mechanism is discussed in the framework of dissipation of angular momentum by topological surface states (TSSs).","Tuning of the Fermi level within the TSS was verified both by varying the stoichiometry of the topological insulator layer and by electrostatic backgating and the damping values obtained in both cases show a remarkable agreement.","The high energy resolution of this method additionally allows us to resolve the energetic shift of the local Dirac points generated by local variations of the electrostatic potential.","Calculations based on the chiral tunneling process naturally occurring in TSS agree well with the experimental results."],"url":"http://arxiv.org/abs/2403.03518v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-06 07:54:40","title":"IB-Net: Initial Branch Network for Variable Decision in Boolean Satisfiability","abstract":"Boolean Satisfiability problems are vital components in Electronic Design Automation, particularly within the Logic Equivalence Checking process. Currently, SAT solvers are employed for these problems and neural network is tried as assistance to solvers. However, as SAT problems in the LEC context are distinctive due to their predominantly unsatisfiability nature and a substantial proportion of UNSAT-core variables, existing neural network assistance has proven unsuccessful in this specialized domain. To tackle this challenge, we propose IB-Net, an innovative framework utilizing graph neural networks and novel graph encoding techniques to model unsatisfiable problems and interact with state-of-the-art solvers. Extensive evaluations across solvers and datasets demonstrate IB-Net's acceleration, achieving an average runtime speedup of 5.0% on industrial data and 8.3% on SAT competition data empirically. This breakthrough advances efficient solving in LEC workflows.","sentences":["Boolean Satisfiability problems are vital components in Electronic Design Automation, particularly within the Logic Equivalence Checking process.","Currently, SAT solvers are employed for these problems and neural network is tried as assistance to solvers.","However, as SAT problems in the LEC context are distinctive due to their predominantly unsatisfiability nature and a substantial proportion of UNSAT-core variables, existing neural network assistance has proven unsuccessful in this specialized domain.","To tackle this challenge, we propose IB-Net, an innovative framework utilizing graph neural networks and novel graph encoding techniques to model unsatisfiable problems and interact with state-of-the-art solvers.","Extensive evaluations across solvers and datasets demonstrate IB-Net's acceleration, achieving an average runtime speedup of 5.0% on industrial data and 8.3% on SAT competition data empirically.","This breakthrough advances efficient solving in LEC workflows."],"url":"http://arxiv.org/abs/2403.03517v1","category":"cs.AI"}
{"created":"2024-03-06 07:49:06","title":"Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling","abstract":"Dense retrieval methods have demonstrated promising performance in multilingual information retrieval, where queries and documents can be in different languages. However, dense retrievers typically require a substantial amount of paired data, which poses even greater challenges in multilingual scenarios. This paper introduces UMR, an Unsupervised Multilingual dense Retriever trained without any paired data. Our approach leverages the sequence likelihood estimation capabilities of multilingual language models to acquire pseudo labels for training dense retrievers. We propose a two-stage framework which iteratively improves the performance of multilingual dense retrievers. Experimental results on two benchmark datasets show that UMR outperforms supervised baselines, showcasing the potential of training multilingual retrievers without paired data, thereby enhancing their practicality. Our source code, data, and models are publicly available at https://github.com/MiuLab/UMR","sentences":["Dense retrieval methods have demonstrated promising performance in multilingual information retrieval, where queries and documents can be in different languages.","However, dense retrievers typically require a substantial amount of paired data, which poses even greater challenges in multilingual scenarios.","This paper introduces UMR, an Unsupervised Multilingual dense Retriever trained without any paired data.","Our approach leverages the sequence likelihood estimation capabilities of multilingual language models to acquire pseudo labels for training dense retrievers.","We propose a two-stage framework which iteratively improves the performance of multilingual dense retrievers.","Experimental results on two benchmark datasets show that UMR outperforms supervised baselines, showcasing the potential of training multilingual retrievers without paired data, thereby enhancing their practicality.","Our source code, data, and models are publicly available at https://github.com/MiuLab/UMR"],"url":"http://arxiv.org/abs/2403.03516v1","category":"cs.CL"}
{"created":"2024-03-06 18:56:33","title":"Investigating the Collective Nature of Cavity Modified Chemical Kinetics under Vibrational Strong Coupling","abstract":"In this paper we develop quantum dynamical methods capable of treating the dynamics of chemically reacting systems in an optical cavity in the vibrationally strong-coupling (VSC) limit at finite temperatures and in the presence of a dissipative solvent in both the few and many molecule limits. In the context of two simple models we demonstrate how reactivity in the {\\em collective} VSC regime does not exhibit altered rate behavior in equilibrium, but may exhibit resonant cavity modification of reactivity when the system is explicitly out of equilibrium. Our results suggest experimental protocols that may be used to modify reactivity in the collective regime and point to features not included in the models studied which demand further scrutiny.","sentences":["In this paper we develop quantum dynamical methods capable of treating the dynamics of chemically reacting systems in an optical cavity in the vibrationally strong-coupling (VSC) limit at finite temperatures and in the presence of a dissipative solvent in both the few and many molecule limits.","In the context of two simple models we demonstrate how reactivity in the {\\em collective} VSC regime does not exhibit altered rate behavior in equilibrium, but may exhibit resonant cavity modification of reactivity when the system is explicitly out of equilibrium.","Our results suggest experimental protocols that may be used to modify reactivity in the collective regime and point to features not included in the models studied which demand further scrutiny."],"url":"http://arxiv.org/abs/2403.03951v1","category":"quant-ph"}
{"created":"2024-03-06 18:37:11","title":"Permutation Symmetry Restoration in Disordered Materials","abstract":"A disordered solid, such as an athermal jammed packing of soft spheres exists in a rugged potential-energy landscape in which there are a myriad of stable configurations that defy easy enumeration and characterization. Nevertheless, in three-dimensional monodisperse particle packings, we find an astonishing regularity in the distribution of basin volumes, $V_{Nd}$, as measured by their frequency of occurrence in a random sampling algorithm. Ordering the basins according to their size, from the largest at $n=1$, to the smallest, we find approximately that $V_{Nd} \\propto n^{-1}$. This statistical regularity persists up to the largest systems for which we can collect sufficient data. In monodisperse packings there is \"permutation symmetry\" since identical particles can always be interchanged without affecting the system or its properties. Introducing any polydispersity breaks this symmetry and leads to a proliferation of distinct configurations. We present an algorithm that partially restores permutation symmetry to such polydisperse packings.","sentences":["A disordered solid, such as an athermal jammed packing of soft spheres exists in a rugged potential-energy landscape in which there are a myriad of stable configurations that defy easy enumeration and characterization.","Nevertheless, in three-dimensional monodisperse particle packings, we find an astonishing regularity in the distribution of basin volumes, $V_{Nd}$, as measured by their frequency of occurrence in a random sampling algorithm.","Ordering the basins according to their size, from the largest at $n=1$, to the smallest, we find approximately that $V_{Nd} \\propto n^{-1}$.","This statistical regularity persists up to the largest systems for which we can collect sufficient data.","In monodisperse packings there is \"permutation symmetry\" since identical particles can always be interchanged without affecting the system or its properties.","Introducing any polydispersity breaks this symmetry and leads to a proliferation of distinct configurations.","We present an algorithm that partially restores permutation symmetry to such polydisperse packings."],"url":"http://arxiv.org/abs/2403.03926v1","category":"cond-mat.soft"}
{"created":"2024-03-06 18:14:22","title":"A Measure for Transparent Comparison of Linguistic Diversity in Multilingual NLP Data Sets","abstract":"Typologically diverse benchmarks are increasingly created to track the progress achieved in multilingual NLP. Linguistic diversity of these data sets is typically measured as the number of languages or language families included in the sample, but such measures do not consider structural properties of the included languages. In this paper, we propose assessing linguistic diversity of a data set against a reference language sample as a means of maximising linguistic diversity in the long run. We represent languages as sets of features and apply a version of the Jaccard index suitable for comparing sets of measures. In addition to the features extracted from typological data bases, we propose an automatic text-based measure, which can be used as a means of overcoming the well-known problem of data sparsity in manually collected features. Our diversity score is interpretable in terms of linguistic features and can identify the types of languages that are not represented in a data set. Using our method, we analyse a range of popular multilingual data sets (UD, Bible100, mBERT, XTREME, XGLUE, XNLI, XCOPA, TyDiQA, XQuAD). In addition to ranking these data sets, we find, for example, that (poly)synthetic languages are missing in almost all of them.","sentences":["Typologically diverse benchmarks are increasingly created to track the progress achieved in multilingual NLP.","Linguistic diversity of these data sets is typically measured as the number of languages or language families included in the sample, but such measures do not consider structural properties of the included languages.","In this paper, we propose assessing linguistic diversity of a data set against a reference language sample as a means of maximising linguistic diversity in the long run.","We represent languages as sets of features and apply a version of the Jaccard index suitable for comparing sets of measures.","In addition to the features extracted from typological data bases, we propose an automatic text-based measure, which can be used as a means of overcoming the well-known problem of data sparsity in manually collected features.","Our diversity score is interpretable in terms of linguistic features and can identify the types of languages that are not represented in a data set.","Using our method, we analyse a range of popular multilingual data sets (UD, Bible100, mBERT, XTREME, XGLUE, XNLI, XCOPA, TyDiQA, XQuAD).","In addition to ranking these data sets, we find, for example, that (poly)synthetic languages are missing in almost all of them."],"url":"http://arxiv.org/abs/2403.03909v1","category":"cs.CL"}
{"created":"2024-03-06 17:37:43","title":"On the stack of 0-dimensional coherent sheaves: structural aspects","abstract":"Let $X$ be a quasiprojective scheme. In this expository note we collect a series of useful structural results on the stack $\\mathscr{C}oh^n(X)$ parametrising $0$-dimensional coherent sheaves of length $n$ over $X$. For instance, we discuss its functoriality (in particular its behaviour along \\'etale maps), the support morphism to $\\mathrm{Sym}^n(X)$, and its relationship with the Quot scheme of points $\\mathrm{Quot}_X(\\mathcal E,n)$ for fixed $\\mathcal E\\in \\mathrm{Coh}(X)$.","sentences":["Let $X$ be a quasiprojective scheme.","In this expository note we collect a series of useful structural results on the stack $\\mathscr{C}oh^n(X)$ parametrising $0$-dimensional coherent sheaves of length $n$ over $X$. For instance, we discuss its functoriality (in particular its behaviour along \\'etale maps), the support morphism to $\\mathrm{Sym}^n(X)$, and its relationship with the Quot scheme of points $\\mathrm{Quot}_X(\\mathcal E,n)$ for fixed $\\mathcal E\\in \\mathrm{Coh}(X)$."],"url":"http://arxiv.org/abs/2403.03878v1","category":"math.AG"}
{"created":"2024-03-06 16:36:11","title":"Political polarisation in turbulent times: Tracking polarisation trends and partisan news link sharing on Finnish Twitter, 2015-2023","abstract":"The study analyses polarisation on Finnish social media with data from the platform X, which was known as Twitter during the time of data collection (during the Sipil\\\"a and Marin governments, 2015-2023). The users were clustered into three different ideological groups - the Conservative Right, the Moderate Right, and the Liberal Left - based on their retweeting of tweets referring to the different political parties in Finland. Trends in polarisation of several topics encompassing the most recent political crises - immigration, climate change, COVID-19, and security policy - between these ideological groups is analysed using network methods. To what extent the polarisation of each topic aligns with the polarisation of the other topics is also studied. In addition, the sharing of news links is examined in relation to the ideological groups of the users as well as to the sentiment and the virality of the tweets in which news links are shared.","sentences":["The study analyses polarisation on Finnish social media with data from the platform X, which was known as Twitter during the time of data collection (during the Sipil\\\"a and Marin governments, 2015-2023).","The users were clustered into three different ideological groups - the Conservative Right, the Moderate Right, and the Liberal Left - based on their retweeting of tweets referring to the different political parties in Finland.","Trends in polarisation of several topics encompassing the most recent political crises - immigration, climate change, COVID-19, and security policy - between these ideological groups is analysed using network methods.","To what extent the polarisation of each topic aligns with the polarisation of the other topics is also studied.","In addition, the sharing of news links is examined in relation to the ideological groups of the users as well as to the sentiment and the virality of the tweets in which news links are shared."],"url":"http://arxiv.org/abs/2403.03842v1","category":"cs.SI"}
{"created":"2024-03-06 16:15:13","title":"Temporal Enhanced Floating Car Observers","abstract":"Floating Car Observers (FCOs) are an innovative method to collect traffic data by deploying sensor-equipped vehicles to detect and locate other vehicles. We demonstrate that even a small penetration rate of FCOs can identify a significant amount of vehicles at a given intersection. This is achieved through the emulation of detection within a microscopic traffic simulation. Additionally, leveraging data from previous moments can enhance the detection of vehicles in the current frame. Our findings indicate that, with a 20-second observation window, it is possible to recover up to 20\\% of vehicles that are not visible by FCOs in the current timestep. To exploit this, we developed a data-driven strategy, utilizing sequences of Bird's Eye View (BEV) representations of detected vehicles and deep learning models. This approach aims to bring currently undetected vehicles into view in the present moment, enhancing the currently detected vehicles. Results of different spatiotemporal architectures show that up to 41\\% of the vehicles can be recovered into the current timestep at their current position. This enhancement enriches the information initially available by the FCO, allowing an improved estimation of traffic states and metrics (e.g. density and queue length) for improved implementation of traffic management strategies.","sentences":["Floating Car Observers (FCOs) are an innovative method to collect traffic data by deploying sensor-equipped vehicles to detect and locate other vehicles.","We demonstrate that even a small penetration rate of FCOs can identify a significant amount of vehicles at a given intersection.","This is achieved through the emulation of detection within a microscopic traffic simulation.","Additionally, leveraging data from previous moments can enhance the detection of vehicles in the current frame.","Our findings indicate that, with a 20-second observation window, it is possible to recover up to 20\\% of vehicles that are not visible by FCOs in the current timestep.","To exploit this, we developed a data-driven strategy, utilizing sequences of Bird's Eye View (BEV) representations of detected vehicles and deep learning models.","This approach aims to bring currently undetected vehicles into view in the present moment, enhancing the currently detected vehicles.","Results of different spatiotemporal architectures show that up to 41\\% of the vehicles can be recovered into the current timestep at their current position.","This enhancement enriches the information initially available by the FCO, allowing an improved estimation of traffic states and metrics (e.g. density and queue length) for improved implementation of traffic management strategies."],"url":"http://arxiv.org/abs/2403.03825v1","category":"cs.CV"}
{"created":"2024-03-06 16:06:08","title":"Does Documentation Matter? An Empirical Study of Practitioners' Perspective on Open-Source Software Adoption","abstract":"In recent years, open-source software (OSS) has become increasingly prevalent in developing software products. While OSS documentation is the primary source of information provided by the developers' community about a product, its role in the industry's adoption process has yet to be examined. We conducted semi-structured interviews and an online survey to provide insight into this area. Based on interviews and survey insights, we developed a topic model to collect relevant information from OSS documentation automatically. Additionally, according to our survey responses regarding challenges associated with OSS documentation, we propose a novel information augmentation approach, DocMentor, by combining OSS documentation corpus TF-IDF scores and ChatGPT. Through explaining technical terms and providing examples and references, our approach enhances the documentation context and improves practitioners' understanding. Our tool's effectiveness is assessed by surveying practitioners.","sentences":["In recent years, open-source software (OSS) has become increasingly prevalent in developing software products.","While OSS documentation is the primary source of information provided by the developers' community about a product, its role in the industry's adoption process has yet to be examined.","We conducted semi-structured interviews and an online survey to provide insight into this area.","Based on interviews and survey insights, we developed a topic model to collect relevant information from OSS documentation automatically.","Additionally, according to our survey responses regarding challenges associated with OSS documentation, we propose a novel information augmentation approach, DocMentor, by combining OSS documentation corpus TF-IDF scores and ChatGPT.","Through explaining technical terms and providing examples and references, our approach enhances the documentation context and improves practitioners' understanding.","Our tool's effectiveness is assessed by surveying practitioners."],"url":"http://arxiv.org/abs/2403.03819v1","category":"cs.SE"}
{"created":"2024-03-06 13:49:15","title":"Amplitude analysis of the $\u039b_b^0\\to pK^-\u03b3$ decay","abstract":"The resonant structure of the radiative decay $\\Lambda_b^0\\to pK^-\\gamma$ in the region of proton-kaon invariant-mass up to 2.5 GeV$/c^2$ is studied using proton-proton collision data recorded at centre-of-mass energies of 7, 8, and 13 TeV collected with the LHCb detector, corresponding to a total integrated luminosity of 9 fb$^{-1}$. Results are given in terms of fit and interference fractions between the different components contributing to this final state. Only $\\Lambda$ resonances decaying to $pK^-$ are found to be relevant, where the largest contributions stem from the $\\Lambda(1520)$, $\\Lambda(1600)$, $\\Lambda(1800)$, and $\\Lambda(1890)$ states.","sentences":["The resonant structure of the radiative decay $\\Lambda_b^0\\to pK^-\\gamma$ in the region of proton-kaon invariant-mass up to 2.5 GeV$/c^2$ is studied using proton-proton collision data recorded at centre-of-mass energies of 7, 8, and 13 TeV collected with the LHCb detector, corresponding to a total integrated luminosity of 9 fb$^{-1}$. Results are given in terms of fit and interference fractions between the different components contributing to this final state.","Only $\\Lambda$ resonances decaying to $pK^-$ are found to be relevant, where the largest contributions stem from the $\\Lambda(1520)$, $\\Lambda(1600)$, $\\Lambda(1800)$, and $\\Lambda(1890)$ states."],"url":"http://arxiv.org/abs/2403.03710v1","category":"hep-ex"}
{"created":"2024-03-06 13:29:00","title":"Model Parallelism on Distributed Infrastructure: A Literature Review from Theory to LLM Case-Studies","abstract":"Neural networks have become a cornerstone of machine learning. As the trend for these to get more and more complex continues, so does the underlying hardware and software infrastructure for training and deployment. In this survey we answer three research questions: \"What types of model parallelism exist?\", \"What are the challenges of model parallelism?\", and \"What is a modern use-case of model parallelism?\" We answer the first question by looking at how neural networks can be parallelised and expressing these as operator graphs while exploring the available dimensions. The dimensions along which neural networks can be parallelised are intra-operator and inter-operator. We answer the second question by collecting and listing both implementation challenges for the types of parallelism, as well as the problem of optimally partitioning the operator graph. We answer the last question by collecting and listing how parallelism is applied in modern multi-billion parameter transformer networks, to the extend that this is possible with the limited information shared about these networks.","sentences":["Neural networks have become a cornerstone of machine learning.","As the trend for these to get more and more complex continues, so does the underlying hardware and software infrastructure for training and deployment.","In this survey we answer three research questions: \"What types of model parallelism exist?\", \"What are the challenges of model parallelism?","\", and \"What is a modern use-case of model parallelism?\"","We answer the first question by looking at how neural networks can be parallelised and expressing these as operator graphs while exploring the available dimensions.","The dimensions along which neural networks can be parallelised are intra-operator and inter-operator.","We answer the second question by collecting and listing both implementation challenges for the types of parallelism, as well as the problem of optimally partitioning the operator graph.","We answer the last question by collecting and listing how parallelism is applied in modern multi-billion parameter transformer networks, to the extend that this is possible with the limited information shared about these networks."],"url":"http://arxiv.org/abs/2403.03699v1","category":"cs.DC"}
{"created":"2024-03-06 13:06:02","title":"Fate of the Mollow triplet in strongly-coupled atomic arrays","abstract":"Subwavelength arrays of quantum emitters have emerged as an interesting platform displaying prominent collective effects. Here we study such arrays under coherent driving, realizing an open quantum many-body problem in a strongly non-linear regime. We show that the combination of dipolar interactions and regular geometry have a dramatic effect on the spectrum of emitted light: the famous Mollow triplet characterizing the emission of a single atom develops a structured broadening with flat sidebands, with a bandwidth determined by the dipolar interactions. This emission spectrum characterizes atomic arrays and distinguishes them from disordered ensembles as well as non-interacting emitters. Our predictions are based on a novel dynamical mean-field theory (DMFT) approach to the problem, paving the way for further studies of these systems.","sentences":["Subwavelength arrays of quantum emitters have emerged as an interesting platform displaying prominent collective effects.","Here we study such arrays under coherent driving, realizing an open quantum many-body problem in a strongly non-linear regime.","We show that the combination of dipolar interactions and regular geometry have a dramatic effect on the spectrum of emitted light: the famous Mollow triplet characterizing the emission of a single atom develops a structured broadening with flat sidebands, with a bandwidth determined by the dipolar interactions.","This emission spectrum characterizes atomic arrays and distinguishes them from disordered ensembles as well as non-interacting emitters.","Our predictions are based on a novel dynamical mean-field theory (DMFT) approach to the problem, paving the way for further studies of these systems."],"url":"http://arxiv.org/abs/2403.03679v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-06 12:47:14","title":"CDC: A Simple Framework for Complex Data Clustering","abstract":"In today's data-driven digital era, the amount as well as complexity, such as multi-view, non-Euclidean, and multi-relational, of the collected data are growing exponentially or even faster. Clustering, which unsupervisely extracts valid knowledge from data, is extremely useful in practice. However, existing methods are independently developed to handle one particular challenge at the expense of the others. In this work, we propose a simple but effective framework for complex data clustering (CDC) that can efficiently process different types of data with linear complexity. We first utilize graph filtering to fuse geometry structure and attribute information. We then reduce the complexity with high-quality anchors that are adaptively learned via a novel similarity-preserving regularizer. We illustrate the cluster-ability of our proposed method theoretically and experimentally. In particular, we deploy CDC to graph data of size 111M.","sentences":["In today's data-driven digital era, the amount as well as complexity, such as multi-view, non-Euclidean, and multi-relational, of the collected data are growing exponentially or even faster.","Clustering, which unsupervisely extracts valid knowledge from data, is extremely useful in practice.","However, existing methods are independently developed to handle one particular challenge at the expense of the others.","In this work, we propose a simple but effective framework for complex data clustering (CDC) that can efficiently process different types of data with linear complexity.","We first utilize graph filtering to fuse geometry structure and attribute information.","We then reduce the complexity with high-quality anchors that are adaptively learned via a novel similarity-preserving regularizer.","We illustrate the cluster-ability of our proposed method theoretically and experimentally.","In particular, we deploy CDC to graph data of size 111M."],"url":"http://arxiv.org/abs/2403.03670v1","category":"cs.LG"}
{"created":"2024-03-06 12:20:49","title":"Maximally Extendable Sheaf Codes","abstract":"We study sheaf codes, a type of linear codes with a fixed hierarchical collection of local codes, viewed as a sheaf of vector spaces on a finite topological space we call coded space. Many existing codes, such as tensor product codes, Sipser-Spielman codes, and their more recent high-dimensional analogs, can be naturally represented as sheaf codes on simplicial and cubical complexes, considered as coded spaces. We introduce a new property of a sheaf code, called maximal extendibility, which ensures that within a class of codes on the same coded space, we encounter as few obstructions as possible when extending local sections globally. We show that in every class of sheaf codes defined on the same space and parameterized by parity-check matrices with polynomial entries, there always exists a maximally extendable sheaf code. Such codes are very interesting since it is possible to show that maximally extendable tensor product codes are good coboundary expanders, which potentially could be used to attack the qLTC conjecture.","sentences":["We study sheaf codes, a type of linear codes with a fixed hierarchical collection of local codes, viewed as a sheaf of vector spaces on a finite topological space we call coded space.","Many existing codes, such as tensor product codes, Sipser-Spielman codes, and their more recent high-dimensional analogs, can be naturally represented as sheaf codes on simplicial and cubical complexes, considered as coded spaces.","We introduce a new property of a sheaf code, called maximal extendibility, which ensures that within a class of codes on the same coded space, we encounter as few obstructions as possible when extending local sections globally.","We show that in every class of sheaf codes defined on the same space and parameterized by parity-check matrices with polynomial entries, there always exists a maximally extendable sheaf code.","Such codes are very interesting since it is possible to show that maximally extendable tensor product codes are good coboundary expanders, which potentially could be used to attack the qLTC conjecture."],"url":"http://arxiv.org/abs/2403.03651v1","category":"cs.IT"}
{"created":"2024-03-06 11:23:24","title":"Spectrum Occupancy Detection Supported by Federated Learning","abstract":"Dynamic spectrum access is essential for radiocommunication and its limited spectrum resources. The key element of dynamic spectrum access systems is effective spectrum occupancy detection. In many cases, machine learning algorithms improve detection effectiveness. Because of the recent trend of using federated learning, a federated learning algorithm is presented in the context of distributed spectrum occupancy detection. The results of the work presented in the paper are based on actual signal samples collected in the laboratory. The proposed algorithm is effective, especially in the context of a set of sensors with faulty sensors.","sentences":["Dynamic spectrum access is essential for radiocommunication and its limited spectrum resources.","The key element of dynamic spectrum access systems is effective spectrum occupancy detection.","In many cases, machine learning algorithms improve detection effectiveness.","Because of the recent trend of using federated learning, a federated learning algorithm is presented in the context of distributed spectrum occupancy detection.","The results of the work presented in the paper are based on actual signal samples collected in the laboratory.","The proposed algorithm is effective, especially in the context of a set of sensors with faulty sensors."],"url":"http://arxiv.org/abs/2403.03617v1","category":"cs.NI"}
{"created":"2024-03-06 10:01:55","title":"First observation of the $\u039b^0_b \\to D^+ D^- \u039b$ decay","abstract":"The $\\Lambda^0_b \\to D^+ D^- \\Lambda$ decay is observed for the first time using proton-proton collision data collected by the LHCb experiment at a center-of-mass energy of $13 \\mathrm{TeV}$, corresponding to an integrated luminosity of $5.3 \\mathrm{fb}^{-1}$. Using the $B^0 \\to D^+ D^- K_{\\mathrm{S}}^0$ decay as a reference channel, the product of the relative production cross-section and decay branching fractions is measured to be $$ {\\cal R}=\\frac{\\sigma_{\\Lambda^0_b}}{\\sigma_{B^0}} \\times \\frac{{\\cal B}(\\Lambda^0_b \\to D^+ D^- \\Lambda)}{{\\cal B}(B^0 \\to D^+ D^- K_{\\mathrm{S}}^0)}=0.179 \\pm 0.022 \\pm 0.014 $$ where the first uncertainty is statistical and the second is systematic. The known branching fraction of the reference channel, ${\\cal B}(B^0 \\to D^+ D^- K_{\\mathrm{S}}^0)$, and the cross-section ratio, $\\sigma_{\\Lambda^0_b} / \\sigma_{B^0}$, previously measured by $\\mathrm{LHCb}$ are used to derive the branching fraction of the $\\Lambda^0_b \\to D^+ D^- \\Lambda$ decay $$ {\\cal B}(\\Lambda^0_b \\to D^+ D^- \\Lambda)=(1.24 \\pm 0.15 \\pm 0.10 \\pm 0.28 \\pm 0.11) \\times 10^{-4}, $$ where the third and fourth contributions are due to uncertainties of ${\\cal B}(B^0 \\to D^+ D^- K_{\\mathrm{S}}^0)$ and $\\sigma_{\\Lambda^0_b} / \\sigma_{B^0}$, respectively. Inspection of the $D^+ \\Lambda$ and $D^+ D^-$ invariant-mass distributions suggests a rich presence of intermediate resonances in the decay. The $\\Lambda^0_b \\to D^{*+} D^- \\Lambda$ decay is also observed for the first time as a partially reconstructed component in the $D^+ D^- \\Lambda$ invariant mass spectrum.","sentences":["The $\\Lambda^0_b \\to D^+ D^- \\Lambda$ decay is observed for the first time using proton-proton collision data collected by the LHCb experiment at a center-of-mass energy of $13 \\mathrm{TeV}$, corresponding to an integrated luminosity of $5.3 \\mathrm{fb}^{-1}$. Using the $B^0 \\to D^+ D^- K_{\\mathrm{S}}^0$ decay as a reference channel, the product of the relative production cross-section and decay branching fractions is measured to be $$ {\\cal R}=\\frac{\\sigma_{\\Lambda^0_b}}{\\sigma_{B^0}} \\times \\frac{{\\cal B}(\\Lambda^0_b \\to D^+ D^- \\Lambda)}{{\\cal B}(B^0 \\to D^+ D^- K_{\\mathrm{S}}^0)}=0.179 \\pm 0.022 \\pm 0.014 $$ where the first uncertainty is statistical and the second is systematic.","The known branching fraction of the reference channel, ${\\cal B}(B^0 \\to D^+ D^- K_{\\mathrm{S}}^0)$, and the cross-section ratio, $\\sigma_{\\Lambda^0_b} / \\sigma_{B^0}$, previously measured by $\\mathrm{LHCb}$ are used to derive the branching fraction of the $\\Lambda^0_b \\to D^+ D^- \\Lambda$ decay $$ {\\cal B}(\\Lambda^0_b \\to D^+ D^- \\Lambda)=(1.24 \\pm 0.15 \\pm 0.10 \\pm 0.28 \\pm 0.11)","\\times 10^{-4}, $$ where the third and fourth contributions are due to uncertainties of ${\\cal B}(B^0 \\to D^+ D^- K_{\\mathrm{S}}^0)$ and $\\sigma_{\\Lambda^0_b} / \\sigma_{B^0}$, respectively.","Inspection of the $D^+ \\Lambda$ and $D^+ D^-$ invariant-mass distributions suggests a rich presence of intermediate resonances in the decay.","The $\\Lambda^0_b \\to D^{*+} D^- \\Lambda$ decay is also observed for the first time as a partially reconstructed component in the $D^+ D^- \\Lambda$ invariant mass spectrum."],"url":"http://arxiv.org/abs/2403.03586v1","category":"hep-ex"}
{"created":"2024-03-06 09:15:53","title":"Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip Perception of Mobile Manipulation Robots","abstract":"Object slip perception is essential for mobile manipulation robots to perform manipulation tasks reliably in the dynamic real-world. Traditional approaches to robot arms' slip perception use tactile or vision sensors. However, mobile robots still have to deal with noise in their sensor signals caused by the robot's movement in a changing environment. To solve this problem, we present an anomaly detection method that utilizes multisensory data based on a deep autoencoder model. The proposed framework integrates heterogeneous data streams collected from various robot sensors, including RGB and depth cameras, a microphone, and a force-torque sensor. The integrated data is used to train a deep autoencoder to construct latent representations of the multisensory data that indicate the normal status. Anomalies can then be identified by error scores measured by the difference between the trained encoder's latent values and the latent values of reconstructed input data. In order to evaluate the proposed framework, we conducted an experiment that mimics an object slip by a mobile service robot operating in a real-world environment with diverse household objects and different moving patterns. The experimental results verified that the proposed framework reliably detects anomalies in object slip situations despite various object types and robot behaviors, and visual and auditory noise in the environment.","sentences":["Object slip perception is essential for mobile manipulation robots to perform manipulation tasks reliably in the dynamic real-world.","Traditional approaches to robot arms' slip perception use tactile or vision sensors.","However, mobile robots still have to deal with noise in their sensor signals caused by the robot's movement in a changing environment.","To solve this problem, we present an anomaly detection method that utilizes multisensory data based on a deep autoencoder model.","The proposed framework integrates heterogeneous data streams collected from various robot sensors, including RGB and depth cameras, a microphone, and a force-torque sensor.","The integrated data is used to train a deep autoencoder to construct latent representations of the multisensory data that indicate the normal status.","Anomalies can then be identified by error scores measured by the difference between the trained encoder's latent values and the latent values of reconstructed input data.","In order to evaluate the proposed framework, we conducted an experiment that mimics an object slip by a mobile service robot operating in a real-world environment with diverse household objects and different moving patterns.","The experimental results verified that the proposed framework reliably detects anomalies in object slip situations despite various object types and robot behaviors, and visual and auditory noise in the environment."],"url":"http://arxiv.org/abs/2403.03563v1","category":"cs.RO"}
{"created":"2024-03-06 07:25:46","title":"Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts","abstract":"This study explores the challenge of sentence-level AI-generated text detection within human-AI collaborative hybrid texts. Existing studies of AI-generated text detection for hybrid texts often rely on synthetic datasets. These typically involve hybrid texts with a limited number of boundaries. We contend that studies of detecting AI-generated content within hybrid texts should cover different types of hybrid texts generated in realistic settings to better inform real-world applications. Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid texts generated through the collaboration between human writers and an intelligent writing system in multi-turn interactions. We adopt a two-step, segmentation-based pipeline: (i) detect segments within a given hybrid text where each segment contains sentences of consistent authorship, and (ii) classify the authorship of each identified segment. Our empirical findings highlight (1) detecting AI-generated sentences in hybrid texts is overall a challenging task because (1.1) human writers' selecting and even editing AI-generated sentences based on personal preferences adds difficulty in identifying the authorship of segments; (1.2) the frequent change of authorship between neighboring sentences within the hybrid text creates difficulties for segment detectors in identifying authorship-consistent segments; (1.3) the short length of text segments within hybrid texts provides limited stylistic cues for reliable authorship determination; (2) before embarking on the detection process, it is beneficial to assess the average length of segments within the hybrid text. This assessment aids in deciding whether (2.1) to employ a text segmentation-based strategy for hybrid texts with longer segments, or (2.2) to adopt a direct sentence-by-sentence classification strategy for those with shorter segments.","sentences":["This study explores the challenge of sentence-level AI-generated text detection within human-AI collaborative hybrid texts.","Existing studies of AI-generated text detection for hybrid texts often rely on synthetic datasets.","These typically involve hybrid texts with a limited number of boundaries.","We contend that studies of detecting AI-generated content within hybrid texts should cover different types of hybrid texts generated in realistic settings to better inform real-world applications.","Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid texts generated through the collaboration between human writers and an intelligent writing system in multi-turn interactions.","We adopt a two-step, segmentation-based pipeline: (i) detect segments within a given hybrid text where each segment contains sentences of consistent authorship, and (ii) classify the authorship of each identified segment.","Our empirical findings highlight (1) detecting AI-generated sentences in hybrid texts is overall a challenging task because (1.1) human writers' selecting and even editing AI-generated sentences based on personal preferences adds difficulty in identifying the authorship of segments; (1.2) the frequent change of authorship between neighboring sentences within the hybrid text creates difficulties for segment detectors in identifying authorship-consistent segments; (1.3) the short length of text segments within hybrid texts provides limited stylistic cues for reliable authorship determination; (2) before embarking on the detection process, it is beneficial to assess the average length of segments within the hybrid text.","This assessment aids in deciding whether (2.1) to employ a text segmentation-based strategy for hybrid texts with longer segments, or (2.2) to adopt a direct sentence-by-sentence classification strategy for those with shorter segments."],"url":"http://arxiv.org/abs/2403.03506v1","category":"cs.CL"}
{"created":"2024-03-06 07:04:26","title":"Observation of the decay $h_{c}\\to3(\u03c0^{+}\u03c0^{-})\u03c0^{0}$","abstract":"Based on $(2712.4\\pm14.1)\\times10^{6}$ $\\psi(3686)$ events collected with the BESIII detector, we study the decays $h_{c}\\to3(\\pi^{+}\\pi^{-})\\pi^{0}$, $h_{c}\\to2(\\pi^{+}\\pi^{-})\\omega$, $h_{c}\\to2(\\pi^{+}\\pi^{-})\\pi^{0}\\eta$, $h_{c}\\to2(\\pi^{+}\\pi^{-})\\eta$, and $h_{c}\\to p\\bar{p}$ via $\\psi(3686)\\to\\pi^{0}h_{c}$. The decay channel $h_{c}\\to3(\\pi^{+}\\pi^{-})\\pi^{0}$ is observed for the first time, and its branching fraction is determined to be $\\left( {9.28\\pm 1.14 \\pm 0.77} \\right) \\times {10^{ - 3}}$, where the first uncertainty is statistical and the second is systematic. In addition, first evidence is found for the modes $h_{c} \\to 2(\\pi^{+}\\pi^{-})\\pi^{0}\\eta$ and $h_{c}\\to2(\\pi^{+}\\pi^{-})\\omega$ with significances of 4.8$\\sigma$ and 4.7$\\sigma$, and their branching fractions are determined to be $(7.55\\pm1.51\\pm0.77)\\times10^{-3}$ and $\\left( {4.00 \\pm 0.86 \\pm 0.35}\\right) \\times {10^{ - 3}}$, respectively. No significant signals of $h_c\\to 2(\\pi^+\\pi^-)\\eta$ and $h_{c}\\to p\\bar{p}$ are observed, and the upper limits of the branching fractions of these decays are determined to be $<6.19\\times10^{-4}$ and $<4.40\\times10^{-5}$ at the 90% confidence level, respectively.","sentences":["Based on $(2712.4\\pm14.1)\\times10^{6}$ $\\psi(3686)$ events collected with the BESIII detector, we study the decays $h_{c}\\to3(\\pi^{+}\\pi^{-})\\pi^{0}$, $h_{c}\\to2(\\pi^{+}\\pi^{-})\\omega$, $h_{c}\\to2(\\pi^{+}\\pi^{-})\\pi^{0}\\eta$, $h_{c}\\to2(\\pi^{+}\\pi^{-})\\eta$, and $h_{c}\\to p\\bar{p}$ via $\\psi(3686)\\to\\pi^{0}h_{c}$. The decay channel $h_{c}\\to3(\\pi^{+}\\pi^{-})\\pi^{0}$ is observed for the first time, and its branching fraction is determined to be $\\left( {9.28\\pm 1.14 \\pm 0.77} \\right) \\times {10^{ - 3}}$, where the first uncertainty is statistical and the second is systematic.","In addition, first evidence is found for the modes $h_{c} \\to 2(\\pi^{+}\\pi^{-})\\pi^{0}\\eta$ and $h_{c}\\to2(\\pi^{+}\\pi^{-})\\omega$ with significances of 4.8$\\sigma$ and 4.7$\\sigma$, and their branching fractions are determined to be $(7.55\\pm1.51\\pm0.77)\\times10^{-3}$ and $\\left( {4.00 \\pm 0.86 \\pm 0.35}\\right)","\\times {10^{ - 3}}$, respectively.","No significant signals of $h_c\\to 2(\\pi^+\\pi^-)\\eta$ and $h_{c}\\to p\\bar{p}$ are observed, and the upper limits of the branching fractions of these decays are determined to be $<6.19\\times10^{-4}$ and $<4.40\\times10^{-5}$ at the 90% confidence level, respectively."],"url":"http://arxiv.org/abs/2403.03500v1","category":"hep-ex"}
{"created":"2024-03-06 06:58:14","title":"Adaptive coordination promotes collective cooperation in repeated social dilemmas","abstract":"Direct reciprocity based on the repeated prisoner's dilemma has been intensively studied. Most theoretical investigations have concentrated on memory-$1$ strategies, a class of elementary strategies just reacting to the previous-round outcomes. Though the properties of \"All-or-None\" strategies ($AoN_K$) have been discovered, simulations just confirmed the good performance of $AoN_K$ of very short memory lengths. It remains unclear how $AoN_K$ strategies would fare when players have access to longer rounds of history information. We construct a theoretical model to investigate the performance of the class of $AoN_K$ strategies of varying memory length $K$. We rigorously derive the payoffs and show that $AoN_K$ strategies of intermediate memory length $K$ are most prevalent, while strategies of larger memory lengths are less competent. Larger memory lengths make it hard for $AoN_K$ strategies to coordinate, and thus inhibiting their mutual reciprocity. We then propose the adaptive coordination strategy combining tolerance and $AoN_K$' coordination rule. This strategy behaves like $AoN_K$ strategy when coordination is not sufficient, and tolerates opponents' occasional deviations by still cooperating when coordination is sufficient. We found that the adaptive coordination strategy wins over other classic memory-$1$ strategies in various typical competition environments, and stabilizes the population at high levels of cooperation, suggesting the effectiveness of high level adaptability in resolving social dilemmas. Our work may offer a theoretical framework for exploring complex strategies using history information, which are different from traditional memory-$n$ strategies.","sentences":["Direct reciprocity based on the repeated prisoner's dilemma has been intensively studied.","Most theoretical investigations have concentrated on memory-$1$ strategies, a class of elementary strategies just reacting to the previous-round outcomes.","Though the properties of \"All-or-None\" strategies ($AoN_K$) have been discovered, simulations just confirmed the good performance of $AoN_K$ of very short memory lengths.","It remains unclear how $AoN_K$ strategies would fare when players have access to longer rounds of history information.","We construct a theoretical model to investigate the performance of the class of $AoN_K$ strategies of varying memory length $K$. We rigorously derive the payoffs and show that $AoN_K$ strategies of intermediate memory length $K$ are most prevalent, while strategies of larger memory lengths are less competent.","Larger memory lengths make it hard for $AoN_K$ strategies to coordinate, and thus inhibiting their mutual reciprocity.","We then propose the adaptive coordination strategy combining tolerance and $AoN_K$' coordination rule.","This strategy behaves like $AoN_K$ strategy when coordination is not sufficient, and tolerates opponents' occasional deviations by still cooperating when coordination is sufficient.","We found that the adaptive coordination strategy wins over other classic memory-$1$ strategies in various typical competition environments, and stabilizes the population at high levels of cooperation, suggesting the effectiveness of high level adaptability in resolving social dilemmas.","Our work may offer a theoretical framework for exploring complex strategies using history information, which are different from traditional memory-$n$ strategies."],"url":"http://arxiv.org/abs/2403.03497v1","category":"cs.GT"}
{"created":"2024-03-06 05:40:31","title":"Magic Markup: Maintaining Document-External Markup with an LLM","abstract":"Text documents, including programs, typically have human-readable semantic structure. Historically, programmatic access to these semantics has required explicit in-document tagging. Especially in systems where the text has an execution semantics, this means it is an opt-in feature that is hard to support properly. Today, language models offer a new method: metadata can be bound to entities in changing text using a model's human-like understanding of semantics, with no requirements on the document structure. This method expands the applications of document annotation, a fundamental operation in program writing, debugging, maintenance, and presentation. We contribute a system that employs an intelligent agent to re-tag modified programs, enabling rich annotations to automatically follow code as it evolves. We also contribute a formal problem definition, an empirical synthetic benchmark suite, and our benchmark generator. Our system achieves an accuracy of 90% on our benchmarks and can replace a document's tags in parallel at a rate of 5 seconds per tag. While there remains significant room for improvement, we find performance reliable enough to justify further exploration of applications.","sentences":["Text documents, including programs, typically have human-readable semantic structure.","Historically, programmatic access to these semantics has required explicit in-document tagging.","Especially in systems where the text has an execution semantics, this means it is an opt-in feature that is hard to support properly.","Today, language models offer a new method: metadata can be bound to entities in changing text using a model's human-like understanding of semantics, with no requirements on the document structure.","This method expands the applications of document annotation, a fundamental operation in program writing, debugging, maintenance, and presentation.","We contribute a system that employs an intelligent agent to re-tag modified programs, enabling rich annotations to automatically follow code as it evolves.","We also contribute a formal problem definition, an empirical synthetic benchmark suite, and our benchmark generator.","Our system achieves an accuracy of 90% on our benchmarks and can replace a document's tags in parallel at a rate of 5 seconds per tag.","While there remains significant room for improvement, we find performance reliable enough to justify further exploration of applications."],"url":"http://arxiv.org/abs/2403.03481v1","category":"cs.CL"}
{"created":"2024-03-06 04:54:00","title":"A Density-Guided Temporal Attention Transformer for Indiscernible Object Counting in Underwater Video","abstract":"Dense object counting or crowd counting has come a long way thanks to the recent development in the vision community. However, indiscernible object counting, which aims to count the number of targets that are blended with respect to their surroundings, has been a challenge. Image-based object counting datasets have been the mainstream of the current publicly available datasets. Therefore, we propose a large-scale dataset called YoutubeFish-35, which contains a total of 35 sequences of high-definition videos with high frame-per-second and more than 150,000 annotated center points across a selected variety of scenes. For benchmarking purposes, we select three mainstream methods for dense object counting and carefully evaluate them on the newly collected dataset. We propose TransVidCount, a new strong baseline that combines density and regression branches along the temporal domain in a unified framework and can effectively tackle indiscernible object counting with state-of-the-art performance on YoutubeFish-35 dataset.","sentences":["Dense object counting or crowd counting has come a long way thanks to the recent development in the vision community.","However, indiscernible object counting, which aims to count the number of targets that are blended with respect to their surroundings, has been a challenge.","Image-based object counting datasets have been the mainstream of the current publicly available datasets.","Therefore, we propose a large-scale dataset called YoutubeFish-35, which contains a total of 35 sequences of high-definition videos with high frame-per-second and more than 150,000 annotated center points across a selected variety of scenes.","For benchmarking purposes, we select three mainstream methods for dense object counting and carefully evaluate them on the newly collected dataset.","We propose TransVidCount, a new strong baseline that combines density and regression branches along the temporal domain in a unified framework and can effectively tackle indiscernible object counting with state-of-the-art performance on YoutubeFish-35 dataset."],"url":"http://arxiv.org/abs/2403.03461v1","category":"cs.CV"}
{"created":"2024-03-06 04:46:03","title":"DLP-GAN: Learning to Draw Modern Chinese Landscape Photos with Generative Adversarial Network","abstract":"Chinese landscape painting has a unique and artistic style, and its drawing technique is highly abstract in both the use of color and the realistic representation of objects. Previous methods focus on transferring from modern photos to ancient ink paintings. However, little attention has been paid to translating landscape paintings into modern photos. To solve such problems, in this paper, we (1) propose DLP-GAN (\\textbf{D}raw Modern Chinese \\textbf{L}andscape \\textbf{P}hotos with \\textbf{G}enerative \\textbf{A}dversarial \\textbf{N}etwork), an unsupervised cross-domain image translation framework with a novel asymmetric cycle mapping, and (2) introduce a generator based on a dense-fusion module to match different translation directions. Moreover, a dual-consistency loss is proposed to balance the realism and abstraction of model painting. In this way, our model can draw landscape photos and sketches in the modern sense. Finally, based on our collection of modern landscape and sketch datasets, we compare the images generated by our model with other benchmarks. Extensive experiments including user studies show that our model outperforms state-of-the-art methods.","sentences":["Chinese landscape painting has a unique and artistic style, and its drawing technique is highly abstract in both the use of color and the realistic representation of objects.","Previous methods focus on transferring from modern photos to ancient ink paintings.","However, little attention has been paid to translating landscape paintings into modern photos.","To solve such problems, in this paper, we (1) propose DLP-GAN (\\textbf{D}raw Modern Chinese \\textbf{L}andscape \\textbf{P}hotos with \\textbf{G}enerative \\textbf{A}dversarial \\textbf{N}etwork), an unsupervised cross-domain image translation framework with a novel asymmetric cycle mapping, and (2) introduce a generator based on a dense-fusion module to match different translation directions.","Moreover, a dual-consistency loss is proposed to balance the realism and abstraction of model painting.","In this way, our model can draw landscape photos and sketches in the modern sense.","Finally, based on our collection of modern landscape and sketch datasets, we compare the images generated by our model with other benchmarks.","Extensive experiments including user studies show that our model outperforms state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.03456v1","category":"cs.CV"}
{"created":"2024-03-06 04:02:30","title":"Uncertainty quantification for deeponets with ensemble kalman inversion","abstract":"In recent years, operator learning, particularly the DeepONet, has received much attention for efficiently learning complex mappings between input and output functions across diverse fields. However, in practical scenarios with limited and noisy data, accessing the uncertainty in DeepONet predictions becomes essential, especially in mission-critical or safety-critical applications. Existing methods, either computationally intensive or yielding unsatisfactory uncertainty quantification, leave room for developing efficient and informative uncertainty quantification (UQ) techniques tailored for DeepONets. In this work, we proposed a novel inference approach for efficient UQ for operator learning by harnessing the power of the Ensemble Kalman Inversion (EKI) approach. EKI, known for its derivative-free, noise-robust, and highly parallelizable feature, has demonstrated its advantages for UQ for physics-informed neural networks [28]. Our innovative application of EKI enables us to efficiently train ensembles of DeepONets while obtaining informative uncertainty estimates for the output of interest. We deploy a mini-batch variant of EKI to accommodate larger datasets, mitigating the computational demand due to large datasets during the training stage. Furthermore, we introduce a heuristic method to estimate the artificial dynamics covariance, thereby improving our uncertainty estimates. Finally, we demonstrate the effectiveness and versatility of our proposed methodology across various benchmark problems, showcasing its potential to address the pressing challenges of uncertainty quantification in DeepONets, especially for practical applications with limited and noisy data.","sentences":["In recent years, operator learning, particularly the DeepONet, has received much attention for efficiently learning complex mappings between input and output functions across diverse fields.","However, in practical scenarios with limited and noisy data, accessing the uncertainty in DeepONet predictions becomes essential, especially in mission-critical or safety-critical applications.","Existing methods, either computationally intensive or yielding unsatisfactory uncertainty quantification, leave room for developing efficient and informative uncertainty quantification (UQ) techniques tailored for DeepONets.","In this work, we proposed a novel inference approach for efficient UQ for operator learning by harnessing the power of the Ensemble Kalman Inversion (EKI) approach.","EKI, known for its derivative-free, noise-robust, and highly parallelizable feature, has demonstrated its advantages for UQ for physics-informed neural networks [28].","Our innovative application of EKI enables us to efficiently train ensembles of DeepONets while obtaining informative uncertainty estimates for the output of interest.","We deploy a mini-batch variant of EKI to accommodate larger datasets, mitigating the computational demand due to large datasets during the training stage.","Furthermore, we introduce a heuristic method to estimate the artificial dynamics covariance, thereby improving our uncertainty estimates.","Finally, we demonstrate the effectiveness and versatility of our proposed methodology across various benchmark problems, showcasing its potential to address the pressing challenges of uncertainty quantification in DeepONets, especially for practical applications with limited and noisy data."],"url":"http://arxiv.org/abs/2403.03444v1","category":"cs.LG"}
{"created":"2024-03-06 03:36:07","title":"An AI-enabled Agent-Based Model and Its Application in Measles Outbreak Simulation for New Zealand","abstract":"Agent Based Models (ABMs) have emerged as a powerful tool for investigating complex social interactions, particularly in the context of public health and infectious disease investigation. In an effort to enhance the conventional ABM, enabling automated model calibration and reducing the computational resources needed for scaling up the model, we have developed a tensorized and differentiable agent-based model by coupling Graph Neural Network (GNN) and Long Short-Term Memory (LSTM) network. The model was employed to investigate the 2019 measles outbreak occurred in New Zealand, demonstrating a promising ability to accurately simulate the outbreak dynamics, particularly during the peak period of repeated cases. This paper shows that by leveraging the latest Artificial Intelligence (AI) technology and the capabilities of traditional ABMs, we gain deeper insights into the dynamics of infectious disease outbreaks. This, in turn, helps us make more informed decision when developing effective strategies that strike a balance between managing outbreaks and minimizing disruptions to everyday life.","sentences":["Agent Based Models (ABMs) have emerged as a powerful tool for investigating complex social interactions, particularly in the context of public health and infectious disease investigation.","In an effort to enhance the conventional ABM, enabling automated model calibration and reducing the computational resources needed for scaling up the model, we have developed a tensorized and differentiable agent-based model by coupling Graph Neural Network (GNN) and Long Short-Term Memory (LSTM) network.","The model was employed to investigate the 2019 measles outbreak occurred in New Zealand, demonstrating a promising ability to accurately simulate the outbreak dynamics, particularly during the peak period of repeated cases.","This paper shows that by leveraging the latest Artificial Intelligence (AI) technology and the capabilities of traditional ABMs, we gain deeper insights into the dynamics of infectious disease outbreaks.","This, in turn, helps us make more informed decision when developing effective strategies that strike a balance between managing outbreaks and minimizing disruptions to everyday life."],"url":"http://arxiv.org/abs/2403.03434v1","category":"cs.MA"}
{"created":"2024-03-06 03:33:48","title":"Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models","abstract":"Instruction Tuning has the potential to stimulate or enhance specific capabilities of large language models (LLMs). However, achieving the right balance of data is crucial to prevent catastrophic forgetting and interference between tasks. To address these limitations and enhance training flexibility, we propose the Mixture-of-LoRAs (MoA) architecture which is a novel and parameter-efficient tuning method designed for multi-task learning with LLMs. In this paper, we start by individually training multiple domain-specific LoRA modules using corresponding supervised corpus data. These LoRA modules can be aligned with the expert design principles observed in Mixture-of-Experts (MoE). Subsequently, we combine the multiple LoRAs using an explicit routing strategy and introduce domain labels to facilitate multi-task learning, which help prevent interference between tasks and ultimately enhances the performance of each individual task. Furthermore, each LoRA model can be iteratively adapted to a new domain, allowing for quick domain-specific adaptation. Experiments on diverse tasks demonstrate superior and robust performance, which can further promote the wide application of domain-specific LLMs.","sentences":["Instruction Tuning has the potential to stimulate or enhance specific capabilities of large language models (LLMs).","However, achieving the right balance of data is crucial to prevent catastrophic forgetting and interference between tasks.","To address these limitations and enhance training flexibility, we propose the Mixture-of-LoRAs (MoA) architecture which is a novel and parameter-efficient tuning method designed for multi-task learning with LLMs.","In this paper, we start by individually training multiple domain-specific LoRA modules using corresponding supervised corpus data.","These LoRA modules can be aligned with the expert design principles observed in Mixture-of-Experts (MoE).","Subsequently, we combine the multiple LoRAs using an explicit routing strategy and introduce domain labels to facilitate multi-task learning, which help prevent interference between tasks and ultimately enhances the performance of each individual task.","Furthermore, each LoRA model can be iteratively adapted to a new domain, allowing for quick domain-specific adaptation.","Experiments on diverse tasks demonstrate superior and robust performance, which can further promote the wide application of domain-specific LLMs."],"url":"http://arxiv.org/abs/2403.03432v1","category":"cs.CL"}
{"created":"2024-03-06 03:19:48","title":"Detecting transitions between collective motion regimes using functional hypothesis test of the time-varying persistence homology","abstract":"In a system of many similar self-propelled entities such as flocks of birds, fish school, cells and molecules, the interactions with neighbors can lead to a \"coherent state\", meaning the formation of visually compelling aggregation patterns due to the local adjustment of speed and direction. In this study, we explore one of the open questions that arise in studying collective patterns. When such entities, considered here as particles, tend to assume a coherent state beginning from an incoherent (random) state, what is the time interval for the transition? Also, how do model parameters affect this transition time interval? Given the observations of particle migration over a given time period as a point cloud data sampled at discrete time points, we use Topological Data Analysis, specifically persistent homology, to infer the transition time interval in which the particles undergo regime change. The topology of the particle configuration at any given time instance is captured by the persistent homology specifically Persistence Landscapes. We localize (in time) when such a transition happens by conducting the statistical significance tests namely functional hypothesis tests on persistent homology outputs corresponding to subsets of the time evolution. This process is validated on a known collective behavior model of the self-propelled particles with the regime transitions triggered by changing the model parameters in time. As an application, the developed technique was ultimately used to describe the transition in cellular movement from a disordered state to collective motion when the environment was altered.","sentences":["In a system of many similar self-propelled entities such as flocks of birds, fish school, cells and molecules, the interactions with neighbors can lead to a \"coherent state\", meaning the formation of visually compelling aggregation patterns due to the local adjustment of speed and direction.","In this study, we explore one of the open questions that arise in studying collective patterns.","When such entities, considered here as particles, tend to assume a coherent state beginning from an incoherent (random) state, what is the time interval for the transition?","Also, how do model parameters affect this transition time interval?","Given the observations of particle migration over a given time period as a point cloud data sampled at discrete time points, we use Topological Data Analysis, specifically persistent homology, to infer the transition time interval in which the particles undergo regime change.","The topology of the particle configuration at any given time instance is captured by the persistent homology specifically Persistence Landscapes.","We localize (in time) when such a transition happens by conducting the statistical significance tests namely functional hypothesis tests on persistent homology outputs corresponding to subsets of the time evolution.","This process is validated on a known collective behavior model of the self-propelled particles with the regime transitions triggered by changing the model parameters in time.","As an application, the developed technique was ultimately used to describe the transition in cellular movement from a disordered state to collective motion when the environment was altered."],"url":"http://arxiv.org/abs/2403.03428v1","category":"stat.AP"}
{"created":"2024-03-06 03:08:20","title":"LEAD: Learning Decomposition for Source-free Universal Domain Adaptation","abstract":"Universal Domain Adaptation (UniDA) targets knowledge transfer in the presence of both covariate and label shifts. Recently, Source-free Universal Domain Adaptation (SF-UniDA) has emerged to achieve UniDA without access to source data, which tends to be more practical due to data protection policies. The main challenge lies in determining whether covariate-shifted samples belong to target-private unknown categories. Existing methods tackle this either through hand-crafted thresholding or by developing time-consuming iterative clustering strategies. In this paper, we propose a new idea of LEArning Decomposition (LEAD), which decouples features into source-known and -unknown components to identify target-private data. Technically, LEAD initially leverages the orthogonal decomposition analysis for feature decomposition. Then, LEAD builds instance-level decision boundaries to adaptively identify target-private data. Extensive experiments across various UniDA scenarios have demonstrated the effectiveness and superiority of LEAD. Notably, in the OPDA scenario on VisDA dataset, LEAD outperforms GLC by 3.5% overall H-score and reduces 75% time to derive pseudo-labeling decision boundaries. Besides, LEAD is also appealing in that it is complementary to most existing methods. The code is available at https://github.com/ispc-lab/LEAD.","sentences":["Universal Domain Adaptation (UniDA) targets knowledge transfer in the presence of both covariate and label shifts.","Recently, Source-free Universal Domain Adaptation (SF-UniDA) has emerged to achieve UniDA without access to source data, which tends to be more practical due to data protection policies.","The main challenge lies in determining whether covariate-shifted samples belong to target-private unknown categories.","Existing methods tackle this either through hand-crafted thresholding or by developing time-consuming iterative clustering strategies.","In this paper, we propose a new idea of LEArning Decomposition (LEAD), which decouples features into source-known and -unknown components to identify target-private data.","Technically, LEAD initially leverages the orthogonal decomposition analysis for feature decomposition.","Then, LEAD builds instance-level decision boundaries to adaptively identify target-private data.","Extensive experiments across various UniDA scenarios have demonstrated the effectiveness and superiority of LEAD.","Notably, in the OPDA scenario on VisDA dataset, LEAD outperforms GLC by 3.5% overall H-score and reduces 75% time to derive pseudo-labeling decision boundaries.","Besides, LEAD is also appealing in that it is complementary to most existing methods.","The code is available at https://github.com/ispc-lab/LEAD."],"url":"http://arxiv.org/abs/2403.03421v1","category":"cs.CV"}
{"created":"2024-03-06 03:02:38","title":"Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization","abstract":"Large language models (LLMs) have revolutionized the role of AI, yet also pose potential risks of propagating unethical content. Alignment technologies have been introduced to steer LLMs towards human preference, gaining increasing attention. Despite notable breakthroughs in this direction, existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy labels and the marginal distinction between preferred and dispreferred response data. Given recent LLMs' proficiency in generating helpful responses, this work pivots towards a new research focus: achieving alignment using solely human-annotated negative samples, preserving helpfulness while reducing harmfulness. For this purpose, we propose Distributional Dispreference Optimization (D$^2$O), which maximizes the discrepancy between the generated responses and the dispreferred ones to effectively eschew harmful information. We theoretically demonstrate that D$^2$O is equivalent to learning a distributional instead of instance-level preference model reflecting human dispreference against the distribution of negative responses. Besides, D$^2$O integrates an implicit Jeffrey Divergence regularization to balance the exploitation and exploration of reference policies and converges to a non-negative one during training. Extensive experiments demonstrate that our method achieves comparable generation quality and surpasses the latest baselines in producing less harmful and more informative responses with better training stability and faster convergence.","sentences":["Large language models (LLMs) have revolutionized the role of AI, yet also pose potential risks of propagating unethical content.","Alignment technologies have been introduced to steer LLMs towards human preference, gaining increasing attention.","Despite notable breakthroughs in this direction, existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy labels and the marginal distinction between preferred and dispreferred response data.","Given recent LLMs' proficiency in generating helpful responses, this work pivots towards a new research focus: achieving alignment using solely human-annotated negative samples, preserving helpfulness while reducing harmfulness.","For this purpose, we propose Distributional Dispreference Optimization (D$^2$O), which maximizes the discrepancy between the generated responses and the dispreferred ones to effectively eschew harmful information.","We theoretically demonstrate that D$^2$O is equivalent to learning a distributional instead of instance-level preference model reflecting human dispreference against the distribution of negative responses.","Besides, D$^2$O integrates an implicit Jeffrey Divergence regularization to balance the exploitation and exploration of reference policies and converges to a non-negative one during training.","Extensive experiments demonstrate that our method achieves comparable generation quality and surpasses the latest baselines in producing less harmful and more informative responses with better training stability and faster convergence."],"url":"http://arxiv.org/abs/2403.03419v1","category":"cs.CL"}
{"created":"2024-03-06 02:39:22","title":"Advancing Out-of-Distribution Detection through Data Purification and Dynamic Activation Function Design","abstract":"In the dynamic realms of machine learning and deep learning, the robustness and reliability of models are paramount, especially in critical real-world applications. A fundamental challenge in this sphere is managing Out-of-Distribution (OOD) samples, significantly increasing the risks of model misclassification and uncertainty. Our work addresses this challenge by enhancing the detection and management of OOD samples in neural networks. We introduce OOD-R (Out-of-Distribution-Rectified), a meticulously curated collection of open-source datasets with enhanced noise reduction properties. In-Distribution (ID) noise in existing OOD datasets can lead to inaccurate evaluation of detection algorithms. Recognizing this, OOD-R incorporates noise filtering technologies to refine the datasets, ensuring a more accurate and reliable evaluation of OOD detection algorithms. This approach not only improves the overall quality of data but also aids in better distinguishing between OOD and ID samples, resulting in up to a 2.5\\% improvement in model accuracy and a minimum 3.2\\% reduction in false positives. Furthermore, we present ActFun, an innovative method that fine-tunes the model's response to diverse inputs, thereby improving the stability of feature extraction and minimizing specificity issues. ActFun addresses the common problem of model overconfidence in OOD detection by strategically reducing the influence of hidden units, which enhances the model's capability to estimate OOD uncertainty more accurately. Implementing ActFun in the OOD-R dataset has led to significant performance enhancements, including an 18.42\\% increase in AUROC of the GradNorm method and a 16.93\\% decrease in FPR95 of the Energy method. Overall, our research not only advances the methodologies in OOD detection but also emphasizes the importance of dataset integrity for accurate algorithm evaluation.","sentences":["In the dynamic realms of machine learning and deep learning, the robustness and reliability of models are paramount, especially in critical real-world applications.","A fundamental challenge in this sphere is managing Out-of-Distribution (OOD) samples, significantly increasing the risks of model misclassification and uncertainty.","Our work addresses this challenge by enhancing the detection and management of OOD samples in neural networks.","We introduce OOD-R (Out-of-Distribution-Rectified), a meticulously curated collection of open-source datasets with enhanced noise reduction properties.","In-Distribution (ID) noise in existing OOD datasets can lead to inaccurate evaluation of detection algorithms.","Recognizing this, OOD-R incorporates noise filtering technologies to refine the datasets, ensuring a more accurate and reliable evaluation of OOD detection algorithms.","This approach not only improves the overall quality of data but also aids in better distinguishing between OOD and ID samples, resulting in up to a 2.5\\% improvement in model accuracy and a minimum 3.2\\% reduction in false positives.","Furthermore, we present ActFun, an innovative method that fine-tunes the model's response to diverse inputs, thereby improving the stability of feature extraction and minimizing specificity issues.","ActFun addresses the common problem of model overconfidence in OOD detection by strategically reducing the influence of hidden units, which enhances the model's capability to estimate OOD uncertainty more accurately.","Implementing ActFun in the OOD-R dataset has led to significant performance enhancements, including an 18.42\\% increase in AUROC of the GradNorm method and a 16.93\\% decrease in FPR95 of the Energy method.","Overall, our research not only advances the methodologies in OOD detection but also emphasizes the importance of dataset integrity for accurate algorithm evaluation."],"url":"http://arxiv.org/abs/2403.03412v1","category":"cs.LG"}
{"created":"2024-03-06 02:36:15","title":"Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN","abstract":"Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally efficient and brain-inspired learning model. The design of sparse RSNNs with fewer neurons and synapses helps reduce the computational complexity of RSNNs. Traditionally, sparse SNNs are obtained by first training a dense and complex SNN for a target task, and, then, pruning neurons with low activity (activity-based pruning) while maintaining task performance. In contrast, this paper presents a task-agnostic methodology for designing sparse RSNNs by pruning a large randomly initialized model. We introduce a novel Lyapunov Noise Pruning (LNP) algorithm that uses graph sparsification methods and utilizes Lyapunov exponents to design a stable sparse RSNN from a randomly initialized RSNN. We show that the LNP can leverage diversity in neuronal timescales to design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same sparse HRSNN model can be trained for different tasks, such as image classification and temporal prediction. We experimentally show that, in spite of being task-agnostic, LNP increases computational efficiency (fewer neurons and synapses) and prediction performance of RSNNs compared to traditional activity-based pruning of trained dense models.","sentences":["Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally efficient and brain-inspired learning model.","The design of sparse RSNNs with fewer neurons and synapses helps reduce the computational complexity of RSNNs.","Traditionally, sparse SNNs are obtained by first training a dense and complex SNN for a target task, and, then, pruning neurons with low activity (activity-based pruning) while maintaining task performance.","In contrast, this paper presents a task-agnostic methodology for designing sparse RSNNs by pruning a large randomly initialized model.","We introduce a novel Lyapunov Noise Pruning (LNP) algorithm that uses graph sparsification methods and utilizes Lyapunov exponents to design a stable sparse RSNN from a randomly initialized RSNN.","We show that the LNP can leverage diversity in neuronal timescales to design a sparse Heterogeneous RSNN (HRSNN).","Further, we show that the same sparse HRSNN model can be trained for different tasks, such as image classification and temporal prediction.","We experimentally show that, in spite of being task-agnostic, LNP increases computational efficiency (fewer neurons and synapses) and prediction performance of RSNNs compared to traditional activity-based pruning of trained dense models."],"url":"http://arxiv.org/abs/2403.03409v1","category":"cs.NE"}
{"created":"2024-03-06 02:23:32","title":"Human vs. Machine: Language Models and Wargames","abstract":"Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans. To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses. We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.","sentences":["Wargames have a long history in the development of military strategy and the response of nations to threats or attacks.","The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness.","However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans.","To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses.","We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations."],"url":"http://arxiv.org/abs/2403.03407v1","category":"cs.CY"}
{"created":"2024-03-06 02:09:50","title":"An EnKF-LSTM Assimilation Algorithm for Crop Growth Model","abstract":"Accurate and timely prediction of crop growth is of great significance to ensure crop yields and researchers have developed several crop models for the prediction of crop growth. However, there are large difference between the simulation results obtained by the crop models and the actual results, thus in this paper, we proposed to combine the simulation results with the collected crop data for data assimilation so that the accuracy of prediction will be improved. In this paper, an EnKF-LSTM data assimilation method for various crops is proposed by combining ensemble Kalman filter and LSTM neural network, which effectively avoids the overfitting problem of existing data assimilation methods and eliminates the uncertainty of the measured data. The verification of the proposed EnKF-LSTM method and the comparison of the proposed method with other data assimilation methods were performed using datasets collected by sensor equipment deployed on a farm.","sentences":["Accurate and timely prediction of crop growth is of great significance to ensure crop yields and researchers have developed several crop models for the prediction of crop growth.","However, there are large difference between the simulation results obtained by the crop models and the actual results, thus in this paper, we proposed to combine the simulation results with the collected crop data for data assimilation so that the accuracy of prediction will be improved.","In this paper, an EnKF-LSTM data assimilation method for various crops is proposed by combining ensemble Kalman filter and LSTM neural network, which effectively avoids the overfitting problem of existing data assimilation methods and eliminates the uncertainty of the measured data.","The verification of the proposed EnKF-LSTM method and the comparison of the proposed method with other data assimilation methods were performed using datasets collected by sensor equipment deployed on a farm."],"url":"http://arxiv.org/abs/2403.03406v1","category":"cs.AI"}
{"created":"2024-03-06 01:56:17","title":"BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving","abstract":"Artificial Intelligence for Theorem Proving has given rise to a plethora of benchmarks and methodologies, particularly in Interactive Theorem Proving (ITP). Research in the area is fragmented, with a diverse set of approaches being spread across several ITP systems. This presents a significant challenge to the comparison of methods, which are often complex and difficult to replicate. Addressing this, we present BAIT, a framework for fair and streamlined comparison of learning approaches in ITP. We demonstrate BAIT's capabilities with an in-depth comparison, across several ITP benchmarks, of state-of-the-art architectures applied to the problem of formula embedding. We find that Structure Aware Transformers perform particularly well, improving on techniques associated with the original problem sets. BAIT also allows us to assess the end-to-end proving performance of systems built on interactive environments. This unified perspective reveals a novel end-to-end system that improves on prior work. We also provide a qualitative analysis, illustrating that improved performance is associated with more semantically-aware embeddings. By streamlining the implementation and comparison of Machine Learning algorithms in the ITP context, we anticipate BAIT will be a springboard for future research.","sentences":["Artificial Intelligence for Theorem Proving has given rise to a plethora of benchmarks and methodologies, particularly in Interactive Theorem Proving (ITP).","Research in the area is fragmented, with a diverse set of approaches being spread across several ITP systems.","This presents a significant challenge to the comparison of methods, which are often complex and difficult to replicate.","Addressing this, we present BAIT, a framework for fair and streamlined comparison of learning approaches in ITP.","We demonstrate BAIT's capabilities with an in-depth comparison, across several ITP benchmarks, of state-of-the-art architectures applied to the problem of formula embedding.","We find that Structure Aware Transformers perform particularly well, improving on techniques associated with the original problem sets.","BAIT also allows us to assess the end-to-end proving performance of systems built on interactive environments.","This unified perspective reveals a novel end-to-end system that improves on prior work.","We also provide a qualitative analysis, illustrating that improved performance is associated with more semantically-aware embeddings.","By streamlining the implementation and comparison of Machine Learning algorithms in the ITP context, we anticipate BAIT will be a springboard for future research."],"url":"http://arxiv.org/abs/2403.03401v1","category":"cs.AI"}
{"created":"2024-03-06 01:37:03","title":"Japanese-English Sentence Translation Exercises Dataset for Automatic Grading","abstract":"This paper proposes the task of automatic assessment of Sentence Translation Exercises (STEs), that have been used in the early stage of L2 language learning. We formalize the task as grading student responses for each rubric criterion pre-specified by the educators. We then create a dataset for STE between Japanese and English including 21 questions, along with a total of 3, 498 student responses (167 on average). The answer responses were collected from students and crowd workers. Using this dataset, we demonstrate the performance of baselines including finetuned BERT and GPT models with few-shot in-context learning. Experimental results show that the baseline model with finetuned BERT was able to classify correct responses with approximately 90% in F1, but only less than 80% for incorrect responses. Furthermore, the GPT models with few-shot learning show poorer results than finetuned BERT, indicating that our newly proposed task presents a challenging issue, even for the stateof-the-art large language models.","sentences":["This paper proposes the task of automatic assessment of Sentence Translation Exercises (STEs), that have been used in the early stage of L2 language learning.","We formalize the task as grading student responses for each rubric criterion pre-specified by the educators.","We then create a dataset for STE between Japanese and English including 21 questions, along with a total of 3, 498 student responses (167 on average).","The answer responses were collected from students and crowd workers.","Using this dataset, we demonstrate the performance of baselines including finetuned BERT and GPT models with few-shot in-context learning.","Experimental results show that the baseline model with finetuned BERT was able to classify correct responses with approximately 90% in F1, but only less than 80% for incorrect responses.","Furthermore, the GPT models with few-shot learning show poorer results than finetuned BERT, indicating that our newly proposed task presents a challenging issue, even for the stateof-the-art large language models."],"url":"http://arxiv.org/abs/2403.03396v1","category":"cs.CL"}
{"created":"2024-03-06 01:33:48","title":"Interactive Melody Generation System for Enhancing the Creativity of Musicians","abstract":"This study proposes a system designed to enumerate the process of collaborative composition among humans, using automatic music composition technology. By integrating multiple Recurrent Neural Network (RNN) models, the system provides an experience akin to collaborating with several composers, thereby fostering diverse creativity. Through dynamic adaptation to the user's creative intentions, based on feedback, the system enhances its capability to generate melodies that align with user preferences and creative needs. The system's effectiveness was evaluated through experiments with composers of varying backgrounds, revealing its potential to facilitate musical creativity and suggesting avenues for further refinement. The study underscores the importance of interaction between the composer and AI, aiming to make music composition more accessible and personalized. This system represents a step towards integrating AI into the creative process, offering a new tool for composition support and collaborative artistic exploration.","sentences":["This study proposes a system designed to enumerate the process of collaborative composition among humans, using automatic music composition technology.","By integrating multiple Recurrent Neural Network (RNN) models, the system provides an experience akin to collaborating with several composers, thereby fostering diverse creativity.","Through dynamic adaptation to the user's creative intentions, based on feedback, the system enhances its capability to generate melodies that align with user preferences and creative needs.","The system's effectiveness was evaluated through experiments with composers of varying backgrounds, revealing its potential to facilitate musical creativity and suggesting avenues for further refinement.","The study underscores the importance of interaction between the composer and AI, aiming to make music composition more accessible and personalized.","This system represents a step towards integrating AI into the creative process, offering a new tool for composition support and collaborative artistic exploration."],"url":"http://arxiv.org/abs/2403.03395v1","category":"cs.SD"}
{"created":"2024-03-06 01:04:03","title":"Microscopic description of hexadecapole collectivity in even-even rare-earth nuclei around $N=90$","abstract":"We present an extensive study of hexadecapole correlations in the rare-earth region around $N=90$ and the effects these correlations have on various nuclear properties, such as the low-energy spectra, as well as quadrupole, hexadecapole and monopole transition strengths. In order to examine hexadecapole correlations, we employ a mapped $sdg$ interacting boson model, with parameters derived from a self-consistent mean-field calculations with a relativistic energy density functional. We apply this model to even-even isotopes of Nd, Sm, Gd, Dy and Er ($Z=60 - 68$) with the neutron number $N=84-96$. The obtained results show a good agreement with the experiment. By comparing the results with the ones obtained from a simpler mapped $sd$ interacting boson model, we show that the inclusion of hexadecapole, $g$ boson is necessary to improve the results of the $J^{\\pi} \\geq 6^{+}$ yrast energies in the nuclei with $N=84$ and 86, being near the neutron shell closure. The $sdg$ interacting boson model increases the quadrupole transition strengths between yrast states in the $N=90$ and 92 well deformed nuclei, which is in good agreement with the experiment for most of those isotopes. The presence of $g$ bosons does have an important effect on hexadecapole transition strengths, although experimental data for such transitions are limited. The obtained monopole transition strengths do not differ significantly from the ones obtained from the simpler $sd$ model.","sentences":["We present an extensive study of hexadecapole correlations in the rare-earth region around $N=90$ and the effects these correlations have on various nuclear properties, such as the low-energy spectra, as well as quadrupole, hexadecapole and monopole transition strengths.","In order to examine hexadecapole correlations, we employ a mapped $sdg$ interacting boson model, with parameters derived from a self-consistent mean-field calculations with a relativistic energy density functional.","We apply this model to even-even isotopes of Nd, Sm, Gd, Dy and Er ($Z=60 - 68$) with the neutron number $N=84-96$.","The obtained results show a good agreement with the experiment.","By comparing the results with the ones obtained from a simpler mapped $sd$ interacting boson model, we show that the inclusion of hexadecapole, $g$ boson is necessary to improve the results of the $J^{\\pi} \\geq 6^{+}$ yrast energies in the nuclei with $N=84$ and 86, being near the neutron shell closure.","The $sdg$ interacting boson model increases the quadrupole transition strengths between yrast states in the $N=90$ and 92 well deformed nuclei, which is in good agreement with the experiment for most of those isotopes.","The presence of $g$ bosons does have an important effect on hexadecapole transition strengths, although experimental data for such transitions are limited.","The obtained monopole transition strengths do not differ significantly from the ones obtained from the simpler $sd$ model."],"url":"http://arxiv.org/abs/2403.03393v1","category":"nucl-th"}
{"created":"2024-03-06 00:36:05","title":"Multi-modal Deep Learning","abstract":"This article investigates deep learning methodologies for single-modality clinical data analysis, as a crucial precursor to multi-modal medical research. Building on Guo JingYuan's work, the study refines clinical data processing through Compact Convolutional Transformer (CCT), Patch Up, and the innovative CamCenterLoss technique, establishing a foundation for future multimodal investigations. The proposed methodology demonstrates improved prediction accuracy and at tentiveness to critically ill patients compared to Guo JingYuan's ResNet and StageNet approaches. Novelty that using image-pretrained vision transformer backbone to perform transfer learning time-series clinical data.The study highlights the potential of CCT, Patch Up, and novel CamCenterLoss in processing single modality clinical data within deep learning frameworks, paving the way for future multimodal medical research and promoting precision and personalized healthcare","sentences":["This article investigates deep learning methodologies for single-modality clinical data analysis, as a crucial precursor to multi-modal medical research.","Building on Guo JingYuan's work, the study refines clinical data processing through Compact Convolutional Transformer (CCT), Patch Up, and the innovative CamCenterLoss technique, establishing a foundation for future multimodal investigations.","The proposed methodology demonstrates improved prediction accuracy and at tentiveness to critically ill patients compared to Guo JingYuan's ResNet and StageNet approaches.","Novelty that using image-pretrained vision transformer backbone to perform transfer learning time-series clinical data.","The study highlights the potential of CCT, Patch Up, and novel CamCenterLoss in processing single modality clinical data within deep learning frameworks, paving the way for future multimodal medical research and promoting precision and personalized healthcare"],"url":"http://arxiv.org/abs/2403.03385v1","category":"eess.IV"}
{"created":"2024-03-06 00:17:03","title":"Adaptive Discovering and Merging for Incremental Novel Class Discovery","abstract":"One important desideratum of lifelong learning aims to discover novel classes from unlabelled data in a continuous manner. The central challenge is twofold: discovering and learning novel classes while mitigating the issue of catastrophic forgetting of established knowledge. To this end, we introduce a new paradigm called Adaptive Discovering and Merging (ADM) to discover novel categories adaptively in the incremental stage and integrate novel knowledge into the model without affecting the original knowledge. To discover novel classes adaptively, we decouple representation learning and novel class discovery, and use Triple Comparison (TC) and Probability Regularization (PR) to constrain the probability discrepancy and diversity for adaptive category assignment. To merge the learned novel knowledge adaptively, we propose a hybrid structure with base and novel branches named Adaptive Model Merging (AMM), which reduces the interference of the novel branch on the old classes to preserve the previous knowledge, and merges the novel branch to the base model without performance loss and parameter growth. Extensive experiments on several datasets show that ADM significantly outperforms existing class-incremental Novel Class Discovery (class-iNCD) approaches. Moreover, our AMM also benefits the class-incremental Learning (class-IL) task by alleviating the catastrophic forgetting problem.","sentences":["One important desideratum of lifelong learning aims to discover novel classes from unlabelled data in a continuous manner.","The central challenge is twofold: discovering and learning novel classes while mitigating the issue of catastrophic forgetting of established knowledge.","To this end, we introduce a new paradigm called Adaptive Discovering and Merging (ADM) to discover novel categories adaptively in the incremental stage and integrate novel knowledge into the model without affecting the original knowledge.","To discover novel classes adaptively, we decouple representation learning and novel class discovery, and use Triple Comparison (TC) and Probability Regularization (PR) to constrain the probability discrepancy and diversity for adaptive category assignment.","To merge the learned novel knowledge adaptively, we propose a hybrid structure with base and novel branches named Adaptive Model Merging (AMM), which reduces the interference of the novel branch on the old classes to preserve the previous knowledge, and merges the novel branch to the base model without performance loss and parameter growth.","Extensive experiments on several datasets show that ADM significantly outperforms existing class-incremental Novel Class Discovery (class-iNCD) approaches.","Moreover, our AMM also benefits the class-incremental Learning (class-IL) task by alleviating the catastrophic forgetting problem."],"url":"http://arxiv.org/abs/2403.03382v1","category":"cs.AI"}
{"created":"2024-03-06 00:03:00","title":"Scalable Network Tomography for Dynamic Spectrum Access","abstract":"Mobile networks have increased spectral efficiency through advanced multiplexing strategies that are coordinated by base stations (BS) in licensed spectrum. However, external interference on clients leads to significant performance degradation during dynamic (unlicensed) spectrum access (DSA). We introduce the notion of network tomography for DSA, whereby clients are transformed into spectrum sensors, whose joint access statistics are measured and used to account for interfering sources. Albeit promising, performing such tomography naively incurs an impractical overhead that scales exponentially with the multiplexing order of the strategies deployed -- which will only continue to grow with 5G/6G technologies.   To this end, we propose a novel, scalable network tomography framework called NeTo-X that estimates joint client access statistics with just linear overhead, and forms a blue-print of the interference, thus enabling efficient DSA for future networks. NeTo-X's design incorporates intelligent algorithms that leverage multi-channel diversity and the spatial locality of interference impact on clients to accurately estimate the desired interference statistics from just pair-wise measurements of its clients. The merits of its framework are showcased in the context of resource management and jammer localization applications, where its performance significantly outperforms baseline approaches and closely approximates optimal performance at a scalable overhead.","sentences":["Mobile networks have increased spectral efficiency through advanced multiplexing strategies that are coordinated by base stations (BS) in licensed spectrum.","However, external interference on clients leads to significant performance degradation during dynamic (unlicensed) spectrum access (DSA).","We introduce the notion of network tomography for DSA, whereby clients are transformed into spectrum sensors, whose joint access statistics are measured and used to account for interfering sources.","Albeit promising, performing such tomography naively incurs an impractical overhead that scales exponentially with the multiplexing order of the strategies deployed -- which will only continue to grow with 5G/6G technologies.   ","To this end, we propose a novel, scalable network tomography framework called NeTo-X that estimates joint client access statistics with just linear overhead, and forms a blue-print of the interference, thus enabling efficient DSA for future networks.","NeTo-X's design incorporates intelligent algorithms that leverage multi-channel diversity and the spatial locality of interference impact on clients to accurately estimate the desired interference statistics from just pair-wise measurements of its clients.","The merits of its framework are showcased in the context of resource management and jammer localization applications, where its performance significantly outperforms baseline approaches and closely approximates optimal performance at a scalable overhead."],"url":"http://arxiv.org/abs/2403.03376v1","category":"cs.NI"}
{"created":"2024-03-05 23:37:43","title":"TartanAviation: Image, Speech, and ADS-B Trajectory Datasets for Terminal Airspace Operations","abstract":"We introduce TartanAviation, an open-source multi-modal dataset focused on terminal-area airspace operations. TartanAviation provides a holistic view of the airport environment by concurrently collecting image, speech, and ADS-B trajectory data using setups installed inside airport boundaries. The datasets were collected at both towered and non-towered airfields across multiple months to capture diversity in aircraft operations, seasons, aircraft types, and weather conditions. In total, TartanAviation provides 3.1M images, 3374 hours of Air Traffic Control speech data, and 661 days of ADS-B trajectory data. The data was filtered, processed, and validated to create a curated dataset. In addition to the dataset, we also open-source the code-base used to collect and pre-process the dataset, further enhancing accessibility and usability. We believe this dataset has many potential use cases and would be particularly vital in allowing AI and machine learning technologies to be integrated into air traffic control systems and advance the adoption of autonomous aircraft in the airspace.","sentences":["We introduce TartanAviation, an open-source multi-modal dataset focused on terminal-area airspace operations.","TartanAviation provides a holistic view of the airport environment by concurrently collecting image, speech, and ADS-B trajectory data using setups installed inside airport boundaries.","The datasets were collected at both towered and non-towered airfields across multiple months to capture diversity in aircraft operations, seasons, aircraft types, and weather conditions.","In total, TartanAviation provides 3.1M images, 3374 hours of Air Traffic Control speech data, and 661 days of ADS-B trajectory data.","The data was filtered, processed, and validated to create a curated dataset.","In addition to the dataset, we also open-source the code-base used to collect and pre-process the dataset, further enhancing accessibility and usability.","We believe this dataset has many potential use cases and would be particularly vital in allowing AI and machine learning technologies to be integrated into air traffic control systems and advance the adoption of autonomous aircraft in the airspace."],"url":"http://arxiv.org/abs/2403.03372v1","category":"cs.LG"}
{"created":"2024-03-05 23:03:56","title":"RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging","abstract":"Autonomous parallel-style on-ramp merging in human controlled traffic continues to be an existing issue for autonomous vehicle control. Existing non-learning based solutions for vehicle control rely on rules and optimization primarily. These methods have been seen to present significant challenges. Recent advancements in Deep Reinforcement Learning have shown promise and have received significant academic interest however the available learning based approaches show inadequate attention to other highway vehicles and often rely on inaccurate road traffic assumptions. In addition, the parallel-style case is rarely considered. A novel learning based model for acceleration and lane change decision making that explicitly considers the utility to both the ego vehicle and its surrounding vehicles which may be cooperative or uncooperative to produce behaviour that is socially acceptable is proposed. The novel reward function makes use of Social Value Orientation to weight the vehicle's level of social cooperation and is divided into ego vehicle and surrounding vehicle utility which are weighted according to the model's designated Social Value Orientation. A two-lane highway with an on-ramp divided into a taper-style and parallel-style section is considered. Simulation results indicated the importance of considering surrounding vehicles in reward function design and show that the proposed model matches or surpasses those in literature in terms of collisions while also introducing socially courteous behaviour avoiding near misses and anti-social behaviour through direct consideration of the effect of merging on surrounding vehicles.","sentences":["Autonomous parallel-style on-ramp merging in human controlled traffic continues to be an existing issue for autonomous vehicle control.","Existing non-learning based solutions for vehicle control rely on rules and optimization primarily.","These methods have been seen to present significant challenges.","Recent advancements in Deep Reinforcement Learning have shown promise and have received significant academic interest however the available learning based approaches show inadequate attention to other highway vehicles and often rely on inaccurate road traffic assumptions.","In addition, the parallel-style case is rarely considered.","A novel learning based model for acceleration and lane change decision making that explicitly considers the utility to both the ego vehicle and its surrounding vehicles which may be cooperative or uncooperative to produce behaviour that is socially acceptable is proposed.","The novel reward function makes use of Social Value Orientation to weight the vehicle's level of social cooperation and is divided into ego vehicle and surrounding vehicle utility which are weighted according to the model's designated Social Value Orientation.","A two-lane highway with an on-ramp divided into a taper-style and parallel-style section is considered.","Simulation results indicated the importance of considering surrounding vehicles in reward function design and show that the proposed model matches or surpasses those in literature in terms of collisions while also introducing socially courteous behaviour avoiding near misses and anti-social behaviour through direct consideration of the effect of merging on surrounding vehicles."],"url":"http://arxiv.org/abs/2403.03359v1","category":"cs.AI"}
{"created":"2024-03-05 22:54:15","title":"The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism, AI, and Health in Africa","abstract":"With growing application of machine learning (ML) technologies in healthcare, there have been calls for developing techniques to understand and mitigate biases these systems may exhibit. Fair-ness considerations in the development of ML-based solutions for health have particular implications for Africa, which already faces inequitable power imbalances between the Global North and South.This paper seeks to explore fairness for global health, with Africa as a case study. We conduct a scoping review to propose axes of disparities for fairness consideration in the African context and delineate where they may come into play in different ML-enabled medical modalities. We then conduct qualitative research studies with 672 general population study participants and 28 experts inML, health, and policy focused on Africa to obtain corroborative evidence on the proposed axes of disparities. Our analysis focuses on colonialism as the attribute of interest and examines the interplay between artificial intelligence (AI), health, and colonialism. Among the pre-identified attributes, we found that colonial history, country of origin, and national income level were specific axes of disparities that participants believed would cause an AI system to be biased.However, there was also divergence of opinion between experts and general population participants. Whereas experts generally expressed a shared view about the relevance of colonial history for the development and implementation of AI technologies in Africa, the majority of the general population participants surveyed did not think there was a direct link between AI and colonialism. Based on these findings, we provide practical recommendations for developing fairness-aware ML solutions for health in Africa.","sentences":["With growing application of machine learning (ML) technologies in healthcare, there have been calls for developing techniques to understand and mitigate biases these systems may exhibit.","Fair-ness considerations in the development of ML-based solutions for health have particular implications for Africa, which already faces inequitable power imbalances between the Global North and South.","This paper seeks to explore fairness for global health, with Africa as a case study.","We conduct a scoping review to propose axes of disparities for fairness consideration in the African context and delineate where they may come into play in different ML-enabled medical modalities.","We then conduct qualitative research studies with 672 general population study participants and 28 experts inML, health, and policy focused on Africa to obtain corroborative evidence on the proposed axes of disparities.","Our analysis focuses on colonialism as the attribute of interest and examines the interplay between artificial intelligence (AI), health, and colonialism.","Among the pre-identified attributes, we found that colonial history, country of origin, and national income level were specific axes of disparities that participants believed would cause an AI system to be biased.","However, there was also divergence of opinion between experts and general population participants.","Whereas experts generally expressed a shared view about the relevance of colonial history for the development and implementation of AI technologies in Africa, the majority of the general population participants surveyed did not think there was a direct link between AI and colonialism.","Based on these findings, we provide practical recommendations for developing fairness-aware ML solutions for health in Africa."],"url":"http://arxiv.org/abs/2403.03357v1","category":"cs.AI"}
{"created":"2024-03-05 22:21:45","title":"Learning to Maximize Mutual Information for Chain-of-Thought Distillation","abstract":"Knowledge distillation, the technique of transferring knowledge from large, complex models to smaller ones, marks a pivotal step towards efficient AI deployment. Distilling Step-by-Step (DSS), a novel method utilizing chain-of-thought (CoT) distillation, has demonstrated promise by imbuing smaller models with the superior reasoning capabilities of their larger counterparts. In DSS, the distilled model acquires the ability to generate rationales and predict labels concurrently through a multi-task learning framework. However, DSS overlooks the intrinsic relationship between the two training tasks, leading to ineffective integration of CoT knowledge with the task of label prediction. To this end, we investigate the mutual relationship of the two tasks from Information Bottleneck perspective and formulate it as maximizing the mutual information of the representation features of the two tasks. We propose a variational approach to solve this optimization problem using a learning-based method. Our experimental results across four datasets demonstrate that our method outperforms the state-of-the-art DSS. Our findings offer insightful guidance for future research on language model distillation as well as applications involving CoT. Code and models will be released soon.","sentences":["Knowledge distillation, the technique of transferring knowledge from large, complex models to smaller ones, marks a pivotal step towards efficient AI deployment.","Distilling Step-by-Step (DSS), a novel method utilizing chain-of-thought (CoT) distillation, has demonstrated promise by imbuing smaller models with the superior reasoning capabilities of their larger counterparts.","In DSS, the distilled model acquires the ability to generate rationales and predict labels concurrently through a multi-task learning framework.","However, DSS overlooks the intrinsic relationship between the two training tasks, leading to ineffective integration of CoT knowledge with the task of label prediction.","To this end, we investigate the mutual relationship of the two tasks from Information Bottleneck perspective and formulate it as maximizing the mutual information of the representation features of the two tasks.","We propose a variational approach to solve this optimization problem using a learning-based method.","Our experimental results across four datasets demonstrate that our method outperforms the state-of-the-art DSS.","Our findings offer insightful guidance for future research on language model distillation as well as applications involving CoT. Code and models will be released soon."],"url":"http://arxiv.org/abs/2403.03348v1","category":"cs.CL"}
{"created":"2024-03-05 22:12:01","title":"Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation","abstract":"The increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers. These contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (AI) models. The need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of AI models can lead to energy efficiency gains. Here, we provide an empirical study on green code and an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of AI models. In this framework, we evaluate the sustainability of auto-generated code. The auto-generate codes considered in this study are produced by generative commercial AI language models, GitHub Copilot, OpenAI ChatGPT-3, and Amazon CodeWhisperer. Within our methodology, in order to quantify the sustainability awareness of these AI models, we propose a definition of the code's \"green capacity\", based on certain sustainability metrics. We compare the performance and green capacity of human-generated code and code generated by the three AI language models in response to easy-to-hard problem statements. Our findings shed light on the current capacity of AI models to contribute to sustainable software development.","sentences":["The increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers.","These contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (AI) models.","The need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of AI models can lead to energy efficiency gains.","Here, we provide an empirical study on green code and an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of AI models.","In this framework, we evaluate the sustainability of auto-generated code.","The auto-generate codes considered in this study are produced by generative commercial AI language models, GitHub Copilot, OpenAI ChatGPT-3, and Amazon CodeWhisperer.","Within our methodology, in order to quantify the sustainability awareness of these AI models, we propose a definition of the code's \"green capacity\", based on certain sustainability metrics.","We compare the performance and green capacity of human-generated code and code generated by the three AI language models in response to easy-to-hard problem statements.","Our findings shed light on the current capacity of AI models to contribute to sustainable software development."],"url":"http://arxiv.org/abs/2403.03344v1","category":"cs.SE"}
{"created":"2024-03-05 21:36:23","title":"DIVERSE: Deciphering Internet Views on the U.S. Military Through Video Comment Stance Analysis, A Novel Benchmark Dataset for Stance Classification","abstract":"Stance detection of social media text is a key component of downstream tasks involving the identification of groups of users with opposing opinions on contested topics such as vaccination and within arguments. In particular, stance provides an indication of an opinion towards an entity. This paper introduces DIVERSE, a dataset of over 173,000 YouTube video comments annotated for their stance towards videos of the U.S. military. The stance is annotated through a human-guided, machine-assisted labeling methodology that makes use of weak signals of tone within the sentence as supporting indicators, as opposed to using manual annotations by humans. These weak signals consist of the presence of hate speech and sarcasm, the presence of specific keywords, the sentiment of the text, and the stance inference from two Large Language Models. The weak signals are then consolidated using a data programming model before each comment is annotated with a final stance label. On average, the videos have 200 comments each, and the stance of the comments skews slightly towards the \"against\" characterization for both the U.S. Army and the videos posted on the channel.","sentences":["Stance detection of social media text is a key component of downstream tasks involving the identification of groups of users with opposing opinions on contested topics such as vaccination and within arguments.","In particular, stance provides an indication of an opinion towards an entity.","This paper introduces DIVERSE, a dataset of over 173,000 YouTube video comments annotated for their stance towards videos of the U.S. military.","The stance is annotated through a human-guided, machine-assisted labeling methodology that makes use of weak signals of tone within the sentence as supporting indicators, as opposed to using manual annotations by humans.","These weak signals consist of the presence of hate speech and sarcasm, the presence of specific keywords, the sentiment of the text, and the stance inference from two Large Language Models.","The weak signals are then consolidated using a data programming model before each comment is annotated with a final stance label.","On average, the videos have 200 comments each, and the stance of the comments skews slightly towards the \"against\" characterization for both the U.S. Army and the videos posted on the channel."],"url":"http://arxiv.org/abs/2403.03334v1","category":"cs.CL"}
{"created":"2024-03-05 21:12:10","title":"An Ensemble Framework for Explainable Geospatial Machine Learning Models","abstract":"Analyzing spatial varying effect is pivotal in geographic analysis. Yet, accurately capturing and interpreting this variability is challenging due to the complexity and non-linearity of geospatial data. Herein, we introduce an integrated framework that merges local spatial weighting scheme, Explainable Artificial Intelligence (XAI), and cutting-edge machine learning technologies to bridge the gap between traditional geographic analysis models and general machine learning approaches. Through tests on synthetic datasets, this framework is verified to enhance the interpretability and accuracy of predictions in both geographic regression and classification by elucidating spatial variability. It significantly boosts prediction precision, offering a novel approach to understanding spatial phenomena.","sentences":["Analyzing spatial varying effect is pivotal in geographic analysis.","Yet, accurately capturing and interpreting this variability is challenging due to the complexity and non-linearity of geospatial data.","Herein, we introduce an integrated framework that merges local spatial weighting scheme, Explainable Artificial Intelligence (XAI), and cutting-edge machine learning technologies to bridge the gap between traditional geographic analysis models and general machine learning approaches.","Through tests on synthetic datasets, this framework is verified to enhance the interpretability and accuracy of predictions in both geographic regression and classification by elucidating spatial variability.","It significantly boosts prediction precision, offering a novel approach to understanding spatial phenomena."],"url":"http://arxiv.org/abs/2403.03328v1","category":"cs.LG"}
{"created":"2024-03-05 21:05:16","title":"Deep Configuration Performance Learning: A Systematic Survey and Taxonomy","abstract":"Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning.   In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evaluated and how they are exploited in different tasks related to software configuration. We also identify the good practice and the potentially problematic phenomena from the studies surveyed, together with insights on future opportunities for the field. To promote open science, all the raw results of this survey can be accessed at our repository: https://github.com/ideas-labo/DCPL-SLR.","sentences":["Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system.","However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance.","As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning.   ","In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed.","Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evaluated and how they are exploited in different tasks related to software configuration.","We also identify the good practice and the potentially problematic phenomena from the studies surveyed, together with insights on future opportunities for the field.","To promote open science, all the raw results of this survey can be accessed at our repository: https://github.com/ideas-labo/DCPL-SLR."],"url":"http://arxiv.org/abs/2403.03322v1","category":"cs.SE"}
{"created":"2024-03-05 20:21:49","title":"Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data","abstract":"Visual understanding and segmentation of materials and their states is fundamental for understanding the physical world. The infinite textures, shapes and often blurry boundaries formed by material make this task particularly hard to generalize. Whether it's identifying wet regions of a surface, minerals in rocks, infected regions in plants, or pollution in water, each material state has its own unique form. For neural nets to learn class-agnostic materials segmentation it is necessary to first collect and annotate data that capture this complexity. Collecting real-world images and manually annotating is limited both by the cost and limited precision of manual labor. In contrast, synthetic data is highly accurate and almost cost-free but fails to replicate the vast diversity of the material world. In this work, we suggest a method to bridge this crucial gap, by implanting patterns extracted from real-world images, in synthetic data. Hence, patterns automatically collected from natural images are used to map materials into synthetic scenes. This unsupervised approach allows the generated data to capture the vast complexity of the real world while maintaining the precision and scale of synthetic data. We also present the first general benchmark for class-agnostic material state segmentation. The benchmark images contain a wide range of real-world images of material states, from cooking, food, rocks, construction, plants, and liquids each in various states (wet/dry/stained/cooked/burned/worned/rusted/sediment/foam...). The annotation includes both partial similarity between regions with similar but not identical materials, and hard segmentation of only points of the exact same material state. We show that net trains on MatSeg significantly outperform existing state-of-the-art methods on this task.","sentences":["Visual understanding and segmentation of materials and their states is fundamental for understanding the physical world.","The infinite textures, shapes and often blurry boundaries formed by material make this task particularly hard to generalize.","Whether it's identifying wet regions of a surface, minerals in rocks, infected regions in plants, or pollution in water, each material state has its own unique form.","For neural nets to learn class-agnostic materials segmentation it is necessary to first collect and annotate data that capture this complexity.","Collecting real-world images and manually annotating is limited both by the cost and limited precision of manual labor.","In contrast, synthetic data is highly accurate and almost cost-free but fails to replicate the vast diversity of the material world.","In this work, we suggest a method to bridge this crucial gap, by implanting patterns extracted from real-world images, in synthetic data.","Hence, patterns automatically collected from natural images are used to map materials into synthetic scenes.","This unsupervised approach allows the generated data to capture the vast complexity of the real world while maintaining the precision and scale of synthetic data.","We also present the first general benchmark for class-agnostic material state segmentation.","The benchmark images contain a wide range of real-world images of material states, from cooking, food, rocks, construction, plants, and liquids each in various states (wet/dry/stained/cooked/burned/worned/rusted/sediment/foam...).","The annotation includes both partial similarity between regions with similar but not identical materials, and hard segmentation of only points of the exact same material state.","We show that net trains on MatSeg significantly outperform existing state-of-the-art methods on this task."],"url":"http://arxiv.org/abs/2403.03309v1","category":"cs.CV"}
{"created":"2024-03-05 20:08:32","title":"Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach for Relation Classification","abstract":"This paper introduces a novel neuro-symbolic architecture for relation classification (RC) that combines rule-based methods with contemporary deep learning techniques. This approach capitalizes on the strengths of both paradigms: the adaptability of rule-based systems and the generalization power of neural networks. Our architecture consists of two components: a declarative rule-based model for transparent classification and a neural component to enhance rule generalizability through semantic text matching. Notably, our semantic matcher is trained in an unsupervised domain-agnostic way, solely with synthetic data. Further, these components are loosely coupled, allowing for rule modifications without retraining the semantic matcher. In our evaluation, we focused on two few-shot relation classification datasets: Few-Shot TACRED and a Few-Shot version of NYT29. We show that our proposed method outperforms previous state-of-the-art models in three out of four settings, despite not seeing any human-annotated training data. Further, we show that our approach remains modular and pliable, i.e., the corresponding rules can be locally modified to improve the overall model. Human interventions to the rules for the TACRED relation \\texttt{org:parents} boost the performance on that relation by as much as 26\\% relative improvement, without negatively impacting the other relations, and without retraining the semantic matching component.","sentences":["This paper introduces a novel neuro-symbolic architecture for relation classification (RC) that combines rule-based methods with contemporary deep learning techniques.","This approach capitalizes on the strengths of both paradigms: the adaptability of rule-based systems and the generalization power of neural networks.","Our architecture consists of two components: a declarative rule-based model for transparent classification and a neural component to enhance rule generalizability through semantic text matching.","Notably, our semantic matcher is trained in an unsupervised domain-agnostic way, solely with synthetic data.","Further, these components are loosely coupled, allowing for rule modifications without retraining the semantic matcher.","In our evaluation, we focused on two few-shot relation classification datasets: Few-Shot TACRED and a Few-Shot version of NYT29.","We show that our proposed method outperforms previous state-of-the-art models in three out of four settings, despite not seeing any human-annotated training data.","Further, we show that our approach remains modular and pliable, i.e., the corresponding rules can be locally modified to improve the overall model.","Human interventions to the rules for the TACRED relation \\texttt{org:parents} boost the performance on that relation by as much as 26\\% relative improvement, without negatively impacting the other relations, and without retraining the semantic matching component."],"url":"http://arxiv.org/abs/2403.03305v1","category":"cs.CL"}
{"created":"2024-03-05 19:47:57","title":"AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis","abstract":"This paper discusses the effectiveness of leveraging Chatbot: Generative Pre-trained Transformer (ChatGPT) versions 3.5 and 4 for analyzing research papers for effective writing of scientific literature surveys. The study selected the \\textit{Application of Artificial Intelligence in Breast Cancer Treatment} as the research topic. Research papers related to this topic were collected from three major publication databases Google Scholar, Pubmed, and Scopus. ChatGPT models were used to identify the category, scope, and relevant information from the research papers for automatic identification of relevant papers related to Breast Cancer Treatment (BCT), organization of papers according to scope, and identification of key information for survey paper writing. Evaluations performed using ground truth data annotated using subject experts reveal, that GPT-4 achieves 77.3\\% accuracy in identifying the research paper categories and 50\\% of the papers were correctly identified by GPT-4 for their scopes. Further, the results demonstrate that GPT-4 can generate reasons for its decisions with an average of 27\\% new words, and 67\\% of the reasons given by the model were completely agreeable to the subject experts.","sentences":["This paper discusses the effectiveness of leveraging Chatbot: Generative Pre-trained Transformer (ChatGPT) versions 3.5 and 4 for analyzing research papers for effective writing of scientific literature surveys.","The study selected the \\textit{Application of Artificial Intelligence in Breast Cancer Treatment} as the research topic.","Research papers related to this topic were collected from three major publication databases Google Scholar, Pubmed, and Scopus.","ChatGPT models were used to identify the category, scope, and relevant information from the research papers for automatic identification of relevant papers related to Breast Cancer Treatment (BCT), organization of papers according to scope, and identification of key information for survey paper writing.","Evaluations performed using ground truth data annotated using subject experts reveal, that GPT-4 achieves 77.3\\% accuracy in identifying the research paper categories and 50\\% of the papers were correctly identified by GPT-4 for their scopes.","Further, the results demonstrate that GPT-4 can generate reasons for its decisions with an average of 27\\% new words, and 67\\% of the reasons given by the model were completely agreeable to the subject experts."],"url":"http://arxiv.org/abs/2403.03293v1","category":"cs.AI"}
{"created":"2024-03-05 19:40:53","title":"Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger's Philosophy","abstract":"In the rapidly evolving field of Large Language Models (LLMs), there is a critical need to thoroughly analyze their capabilities and risks. Central to our investigation are two novel elements. Firstly, it is the innovative parallels between the statistical patterns of word relationships within LLMs and Martin Heidegger's concepts of \"ready-to-hand\" and \"present-at-hand,\" which encapsulate the utilitarian and scientific altitudes humans employ in interacting with the world. This comparison lays the groundwork for positioning LLMs as the digital counterpart to the Faculty of Verbal Knowledge, shedding light on their capacity to emulate certain facets of human reasoning. Secondly, a structural analysis of human reasoning, viewed through Heidegger's notion of truth as \"unconcealment\" is conducted This foundational principle enables us to map out the inputs and outputs of the reasoning system and divide reasoning into four distinct categories. Respective cognitive faculties are delineated, allowing us to place LLMs within the broader schema of human reasoning, thus clarifying their strengths and inherent limitations. Our findings reveal that while LLMs possess the capability for Direct Explicative Reasoning and Pseudo Rational Reasoning, they fall short in authentic rational reasoning and have no creative reasoning capabilities, due to the current lack of many analogous AI models such as the Faculty of Judgement. The potential and risks of LLMs when they are augmented with other AI technologies are also evaluated. The results indicate that although LLMs have achieved proficiency in some reasoning abilities, the aspiration to match or exceed human intellectual capabilities is yet unattained. This research not only enriches our comprehension of LLMs but also propels forward the discourse on AI's potential and its bounds, paving the way for future explorations into AI's evolving landscape.","sentences":["In the rapidly evolving field of Large Language Models (LLMs), there is a critical need to thoroughly analyze their capabilities and risks.","Central to our investigation are two novel elements.","Firstly, it is the innovative parallels between the statistical patterns of word relationships within LLMs and Martin Heidegger's concepts of \"ready-to-hand\" and \"present-at-hand,\" which encapsulate the utilitarian and scientific altitudes humans employ in interacting with the world.","This comparison lays the groundwork for positioning LLMs as the digital counterpart to the Faculty of Verbal Knowledge, shedding light on their capacity to emulate certain facets of human reasoning.","Secondly, a structural analysis of human reasoning, viewed through Heidegger's notion of truth as \"unconcealment\" is conducted This foundational principle enables us to map out the inputs and outputs of the reasoning system and divide reasoning into four distinct categories.","Respective cognitive faculties are delineated, allowing us to place LLMs within the broader schema of human reasoning, thus clarifying their strengths and inherent limitations.","Our findings reveal that while LLMs possess the capability for Direct Explicative Reasoning and Pseudo Rational Reasoning, they fall short in authentic rational reasoning and have no creative reasoning capabilities, due to the current lack of many analogous AI models such as the Faculty of Judgement.","The potential and risks of LLMs when they are augmented with other AI technologies are also evaluated.","The results indicate that although LLMs have achieved proficiency in some reasoning abilities, the aspiration to match or exceed human intellectual capabilities is yet unattained.","This research not only enriches our comprehension of LLMs but also propels forward the discourse on AI's potential and its bounds, paving the way for future explorations into AI's evolving landscape."],"url":"http://arxiv.org/abs/2403.03288v1","category":"cs.AI"}
{"created":"2024-03-05 19:25:55","title":"Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits","abstract":"We consider the problem of late multi-modal fusion for discriminative learning. Motivated by noisy, multi-source domains that require understanding the reliability of each data source, we explore the notion of credibility in the context of multi-modal fusion. We propose a combination function that uses probabilistic circuits (PCs) to combine predictive distributions over individual modalities. We also define a probabilistic measure to evaluate the credibility of each modality via inference queries over the PC. Our experimental evaluation demonstrates that our fusion method can reliably infer credibility while maintaining competitive performance with the state-of-the-art.","sentences":["We consider the problem of late multi-modal fusion for discriminative learning.","Motivated by noisy, multi-source domains that require understanding the reliability of each data source, we explore the notion of credibility in the context of multi-modal fusion.","We propose a combination function that uses probabilistic circuits (PCs) to combine predictive distributions over individual modalities.","We also define a probabilistic measure to evaluate the credibility of each modality via inference queries over the PC.","Our experimental evaluation demonstrates that our fusion method can reliably infer credibility while maintaining competitive performance with the state-of-the-art."],"url":"http://arxiv.org/abs/2403.03281v1","category":"cs.LG"}
{"created":"2024-03-05 19:15:17","title":"ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures","abstract":"We proposed an Attentive Recurrent Neural Network (ARNN), which recurrently applies attention layers along a sequence and has linear complexity with respect to the sequence length. The proposed model operates on multi-channel EEG signals rather than single channel signals and leverages parallel computation. In this cell, the attention layer is a computational unit that efficiently applies self-attention and cross-attention mechanisms to compute a recurrent function over a wide number of state vectors and input signals. Our architecture is inspired in part by the attention layer and long short-term memory (LSTM) cells, and it uses long-short style gates, but it scales this typical cell up by several orders to parallelize for multi-channel EEG signals. It inherits the advantages of attention layers and LSTM gate while avoiding their respective drawbacks. We evaluated the model effectiveness through extensive experiments with heterogeneous datasets, including the CHB-MIT and UPenn and Mayos Clinic, CHB-MIT datasets. The empirical findings suggest that the ARNN model outperforms baseline methods such as LSTM, Vision Transformer (ViT), Compact Convolution Transformer (CCT), and R-Transformer (RT), showcasing superior performance and faster processing capabilities across a wide range of tasks. The code has been made publicly accessible at \\url{https://github.com/Salim-Lysiun/ARNN}.","sentences":["We proposed an Attentive Recurrent Neural Network (ARNN), which recurrently applies attention layers along a sequence and has linear complexity with respect to the sequence length.","The proposed model operates on multi-channel EEG signals rather than single channel signals and leverages parallel computation.","In this cell, the attention layer is a computational unit that efficiently applies self-attention and cross-attention mechanisms to compute a recurrent function over a wide number of state vectors and input signals.","Our architecture is inspired in part by the attention layer and long short-term memory (LSTM) cells, and it uses long-short style gates, but it scales this typical cell up by several orders to parallelize for multi-channel EEG signals.","It inherits the advantages of attention layers and LSTM gate while avoiding their respective drawbacks.","We evaluated the model effectiveness through extensive experiments with heterogeneous datasets, including the CHB-MIT and UPenn and Mayos Clinic, CHB-MIT datasets.","The empirical findings suggest that the ARNN model outperforms baseline methods such as LSTM, Vision Transformer (ViT), Compact Convolution Transformer (CCT), and R-Transformer (RT), showcasing superior performance and faster processing capabilities across a wide range of tasks.","The code has been made publicly accessible at \\url{https://github.com/Salim-Lysiun/ARNN}."],"url":"http://arxiv.org/abs/2403.03276v1","category":"eess.SP"}
{"created":"2024-03-05 19:13:57","title":"From Noise to Signal: Unveiling Treatment Effects from Digital Health Data through Pharmacology-Informed Neural-SDE","abstract":"Digital health technologies (DHT), such as wearable devices, provide personalized, continuous, and real-time monitoring of patient. These technologies are contributing to the development of novel therapies and personalized medicine. Gaining insight from these technologies requires appropriate modeling techniques to capture clinically-relevant changes in disease state. The data generated from these devices is characterized by being stochastic in nature, may have missing elements, and exhibits considerable inter-individual variability - thereby making it difficult to analyze using traditional longitudinal modeling techniques. We present a novel pharmacology-informed neural stochastic differential equation (SDE) model capable of addressing these challenges. Using synthetic data, we demonstrate that our approach is effective in identifying treatment effects and learning causal relationships from stochastic data, thereby enabling counterfactual simulation.","sentences":["Digital health technologies (DHT), such as wearable devices, provide personalized, continuous, and real-time monitoring of patient.","These technologies are contributing to the development of novel therapies and personalized medicine.","Gaining insight from these technologies requires appropriate modeling techniques to capture clinically-relevant changes in disease state.","The data generated from these devices is characterized by being stochastic in nature, may have missing elements, and exhibits considerable inter-individual variability - thereby making it difficult to analyze using traditional longitudinal modeling techniques.","We present a novel pharmacology-informed neural stochastic differential equation (SDE) model capable of addressing these challenges.","Using synthetic data, we demonstrate that our approach is effective in identifying treatment effects and learning causal relationships from stochastic data, thereby enabling counterfactual simulation."],"url":"http://arxiv.org/abs/2403.03274v1","category":"q-bio.QM"}
{"created":"2024-03-05 19:04:09","title":"TTPXHunter: Actionable Threat Intelligence Extraction as TTPs form Finished Cyber Threat Reports","abstract":"Understanding the modus operandi of adversaries aids organizations in employing efficient defensive strategies and sharing intelligence in the community. This knowledge is often present in unstructured natural language text within threat analysis reports. A translation tool is needed to interpret the modus operandi explained in the sentences of the threat report and translate it into a structured format. This research introduces a methodology named TTPXHunter for the automated extraction of threat intelligence in terms of Tactics, Techniques, and Procedures (TTPs) from finished cyber threat reports. It leverages cyber domain-specific state-of-the-art natural language processing (NLP) to augment sentences for minority class TTPs and refine pinpointing the TTPs in threat analysis reports significantly. The knowledge of threat intelligence in terms of TTPs is essential for comprehensively understanding cyber threats and enhancing detection and mitigation strategies. We create two datasets: an augmented sentence-TTP dataset of 39,296 samples and a 149 real-world cyber threat intelligence report-to-TTP dataset. Further, we evaluate TTPXHunter on the augmented sentence dataset and the cyber threat reports. The TTPXHunter achieves the highest performance of 92.42% f1-score on the augmented dataset, and it also outperforms existing state-of-the-art solutions in TTP extraction by achieving an f1-score of 97.09% when evaluated over the report dataset. TTPXHunter significantly improves cybersecurity threat intelligence by offering quick, actionable insights into attacker behaviors. This advancement automates threat intelligence analysis, providing a crucial tool for cybersecurity professionals fighting cyber threats.","sentences":["Understanding the modus operandi of adversaries aids organizations in employing efficient defensive strategies and sharing intelligence in the community.","This knowledge is often present in unstructured natural language text within threat analysis reports.","A translation tool is needed to interpret the modus operandi explained in the sentences of the threat report and translate it into a structured format.","This research introduces a methodology named TTPXHunter for the automated extraction of threat intelligence in terms of Tactics, Techniques, and Procedures (TTPs) from finished cyber threat reports.","It leverages cyber domain-specific state-of-the-art natural language processing (NLP) to augment sentences for minority class TTPs and refine pinpointing the TTPs in threat analysis reports significantly.","The knowledge of threat intelligence in terms of TTPs is essential for comprehensively understanding cyber threats and enhancing detection and mitigation strategies.","We create two datasets: an augmented sentence-TTP dataset of 39,296 samples and a 149 real-world cyber threat intelligence report-to-TTP dataset.","Further, we evaluate TTPXHunter on the augmented sentence dataset and the cyber threat reports.","The TTPXHunter achieves the highest performance of 92.42% f1-score on the augmented dataset, and it also outperforms existing state-of-the-art solutions in TTP extraction by achieving an f1-score of 97.09% when evaluated over the report dataset.","TTPXHunter significantly improves cybersecurity threat intelligence by offering quick, actionable insights into attacker behaviors.","This advancement automates threat intelligence analysis, providing a crucial tool for cybersecurity professionals fighting cyber threats."],"url":"http://arxiv.org/abs/2403.03267v1","category":"cs.CR"}
{"created":"2024-03-05 19:03:56","title":"Towards an AI-Enhanced Cyber Threat Intelligence Processing Pipeline","abstract":"Cyber threats continue to evolve in complexity, thereby traditional Cyber Threat Intelligence (CTI) methods struggle to keep pace. AI offers a potential solution, automating and enhancing various tasks, from data ingestion to resilience verification. This paper explores the potential of integrating Artificial Intelligence (AI) into CTI. We provide a blueprint of an AI-enhanced CTI processing pipeline, and detail its components and functionalities. The pipeline highlights the collaboration of AI and human expertise, which is necessary to produce timely and high-fidelity cyber threat intelligence. We also explore the automated generation of mitigation recommendations, harnessing AI's capabilities to provide real-time, contextual, and predictive insights. However, the integration of AI into CTI is not without challenges. Thereby, we discuss ethical dilemmas, potential biases, and the imperative for transparency in AI-driven decisions. We address the need for data privacy, consent mechanisms, and the potential misuse of technology. Moreover, we highlights the importance of addressing biases both during CTI analysis and AI models warranting their transparency and interpretability. Lastly, our work points out future research directions such as the exploration of advanced AI models to augment cyber defences, and the human-AI collaboration optimization. Ultimately, the fusion of AI with CTI appears to hold significant potential in cybersecurity domain.","sentences":["Cyber threats continue to evolve in complexity, thereby traditional Cyber Threat Intelligence (CTI) methods struggle to keep pace.","AI offers a potential solution, automating and enhancing various tasks, from data ingestion to resilience verification.","This paper explores the potential of integrating Artificial Intelligence (AI) into CTI.","We provide a blueprint of an AI-enhanced CTI processing pipeline, and detail its components and functionalities.","The pipeline highlights the collaboration of AI and human expertise, which is necessary to produce timely and high-fidelity cyber threat intelligence.","We also explore the automated generation of mitigation recommendations, harnessing AI's capabilities to provide real-time, contextual, and predictive insights.","However, the integration of AI into CTI is not without challenges.","Thereby, we discuss ethical dilemmas, potential biases, and the imperative for transparency in AI-driven decisions.","We address the need for data privacy, consent mechanisms, and the potential misuse of technology.","Moreover, we highlights the importance of addressing biases both during CTI analysis and AI models warranting their transparency and interpretability.","Lastly, our work points out future research directions such as the exploration of advanced AI models to augment cyber defences, and the human-AI collaboration optimization.","Ultimately, the fusion of AI with CTI appears to hold significant potential in cybersecurity domain."],"url":"http://arxiv.org/abs/2403.03265v1","category":"cs.CR"}
{"created":"2024-03-05 16:07:57","title":"Note: Harnessing Tellurium Nanoparticles in the Digital Realm Plasmon Resonance, in the Context of Brewster's Angle and the Drude Model for Fake News Adsorption in Incomplete Information Games","abstract":"This note explores the innovative application of soliton theory and plasmonic phenomena in modeling user behavior and engagement within digital health platforms. By introducing the concept of soliton solutions, we present a novel approach to understanding stable patterns of health improvement behaviors over time. Additionally, we delve into the role of tellurium nanoparticles and their plasmonic properties in adsorbing fake news, thereby influencing user interactions and engagement levels. Through a theoretical framework that combines nonlinear dynamics with the unique characteristics of tellurium nanoparticles, we aim to provide new insights into the dynamics of user engagement in digital health environments. Our analysis highlights the potential of soliton theory in capturing the complex, nonlinear dynamics of user behavior, while the application of plasmonic phenomena offers a promising avenue for enhancing the sensitivity and effectiveness of digital health platforms. This research ventures into an uncharted territory where optical phenomena such as Brewster's Angle and Snell's Law, along with the concept of spin solitons, are metaphorically applied to address the challenge of fake news dissemination. By exploring the analogy between light refraction, reflection, and the propagation of information in digital platforms, we unveil a novel perspective on how the 'angle' at which information is presented can significantly affect its acceptance and spread. Additionally, we propose the use of tellurium nanoparticles to manage 'information waves' through mechanisms akin to plasmonic resonance and soliton dynamics. This theoretical exploration aims to bridge the gap between physical sciences and digital communication, offering insights into the development of strategies for mitigating misinformation.","sentences":["This note explores the innovative application of soliton theory and plasmonic phenomena in modeling user behavior and engagement within digital health platforms.","By introducing the concept of soliton solutions, we present a novel approach to understanding stable patterns of health improvement behaviors over time.","Additionally, we delve into the role of tellurium nanoparticles and their plasmonic properties in adsorbing fake news, thereby influencing user interactions and engagement levels.","Through a theoretical framework that combines nonlinear dynamics with the unique characteristics of tellurium nanoparticles, we aim to provide new insights into the dynamics of user engagement in digital health environments.","Our analysis highlights the potential of soliton theory in capturing the complex, nonlinear dynamics of user behavior, while the application of plasmonic phenomena offers a promising avenue for enhancing the sensitivity and effectiveness of digital health platforms.","This research ventures into an uncharted territory where optical phenomena such as Brewster's Angle and Snell's Law, along with the concept of spin solitons, are metaphorically applied to address the challenge of fake news dissemination.","By exploring the analogy between light refraction, reflection, and the propagation of information in digital platforms, we unveil a novel perspective on how the 'angle' at which information is presented can significantly affect its acceptance and spread.","Additionally, we propose the use of tellurium nanoparticles to manage 'information waves' through mechanisms akin to plasmonic resonance and soliton dynamics.","This theoretical exploration aims to bridge the gap between physical sciences and digital communication, offering insights into the development of strategies for mitigating misinformation."],"url":"http://arxiv.org/abs/2403.03239v1","category":"physics.soc-ph"}
{"created":"2024-03-06 18:59:51","title":"Decoupling the electronic gap from the spin Chern number in disordered higher-order topological insulators","abstract":"In two-dimensional topological insulators, a disorder induced topological phase transition is typically identified with an Anderson localization transition at the Fermi energy. However, in higher-order, spin-resolved topological insulators it is the spectral gap of the spin-spectrum, in addition to the bulk mobility gap, which protects the non-trivial topology of the ground state. In this work, we show that these two gaps, the bulk electronic and spin gap, evolve distinctly upon introduction of disorder. This decoupling leads to a unique situation in which an Anderson localization transition occurs below the Fermi energy at the topological transition. Furthermore, in the clean limit the bulk-boundary correspondence of such higher-order insulators is dictated by crystalline protected topology, coexisting with the spin-resolved topology. By removing the crystalline symmetry, disorder allows for isolated study of the bulk-boundary correspondence of spin-resolved topology for which we demonstrate the absence of protected edge and corner modes in the Hamiltonian and yet the edge modes in the eigenstates of the projected spin operator survive. Our work shows that a non-zero spin-Chern number, in the absence of a non-trivial $\\mathbb{Z}_{2}$ index, does not dictate the existence of protected edge modes, resolving a fundamental question posed in 2009.","sentences":["In two-dimensional topological insulators, a disorder induced topological phase transition is typically identified with an Anderson localization transition at the Fermi energy.","However, in higher-order, spin-resolved topological insulators it is the spectral gap of the spin-spectrum, in addition to the bulk mobility gap, which protects the non-trivial topology of the ground state.","In this work, we show that these two gaps, the bulk electronic and spin gap, evolve distinctly upon introduction of disorder.","This decoupling leads to a unique situation in which an Anderson localization transition occurs below the Fermi energy at the topological transition.","Furthermore, in the clean limit the bulk-boundary correspondence of such higher-order insulators is dictated by crystalline protected topology, coexisting with the spin-resolved topology.","By removing the crystalline symmetry, disorder allows for isolated study of the bulk-boundary correspondence of spin-resolved topology for which we demonstrate the absence of protected edge and corner modes in the Hamiltonian and yet the edge modes in the eigenstates of the projected spin operator survive.","Our work shows that a non-zero spin-Chern number, in the absence of a non-trivial $\\mathbb{Z}_{2}$ index, does not dictate the existence of protected edge modes, resolving a fundamental question posed in 2009."],"url":"http://arxiv.org/abs/2403.03957v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-06 18:51:34","title":"MR.RGM: An R Package for Fitting Bayesian Multivariate Bidirectional Mendelian Randomization Networks","abstract":"Motivation: Mendelian randomization (MR) infers causal relationships between exposures and outcomes using genetic variants as instrumental variables. Typically, MR considers only a pair of exposure and outcome at a time, limiting its capability of capturing the entire causal network. We overcome this limitation by developing 'MR.RGM' (Mendelian randomization via reciprocal graphical model), a fast R-package that implements the Bayesian reciprocal graphical model and enables practitioners to construct holistic causal networks with possibly cyclic/reciprocal causation and proper uncertainty quantifications, offering a comprehensive understanding of complex biological systems and their interconnections. Results: We developed 'MR.RGM', an open-source R package that applies bidirectional MR using a network-based strategy, enabling the exploration of causal relationships among multiple variables in complex biological systems. 'MR.RGM' holds the promise of unveiling intricate interactions and advancing our understanding of genetic networks, disease risks, and phenotypic complexities.","sentences":["Motivation: Mendelian randomization (MR) infers causal relationships between exposures and outcomes using genetic variants as instrumental variables.","Typically, MR considers only a pair of exposure and outcome at a time, limiting its capability of capturing the entire causal network.","We overcome this limitation by developing 'MR.RGM' (Mendelian randomization via reciprocal graphical model), a fast R-package that implements the Bayesian reciprocal graphical model and enables practitioners to construct holistic causal networks with possibly cyclic/reciprocal causation and proper uncertainty quantifications, offering a comprehensive understanding of complex biological systems and their interconnections.","Results:","We developed 'MR.RGM', an open-source R package that applies bidirectional MR using a network-based strategy, enabling the exploration of causal relationships among multiple variables in complex biological systems.","'MR.RGM' holds the promise of unveiling intricate interactions and advancing our understanding of genetic networks, disease risks, and phenotypic complexities."],"url":"http://arxiv.org/abs/2403.03944v1","category":"stat.AP"}
{"created":"2024-03-06 18:47:11","title":"Settling the Competition Complexity of Additive Buyers over Independent Items","abstract":"The competition complexity of an auction setting is the number of additional bidders needed such that the simple mechanism of selling items separately (with additional bidders) achieves greater revenue than the optimal but complex (randomized, prior-dependent, Bayesian-truthful) optimal mechanism without the additional bidders. Our main result settles the competition complexity of $n$ bidders with additive values over $m < n$ independent items at $\\Theta(\\sqrt{nm})$. The $O(\\sqrt{nm})$ upper bound is due to [BW19], and our main result improves the prior lower bound of $\\Omega(\\ln n)$ to $\\Omega(\\sqrt{nm})$.   Our main result follows from an explicit construction of a Bayesian IC auction for $n$ bidders with additive values over $m<n$ independent items drawn from the Equal Revenue curve truncated at $\\sqrt{nm}$ ($\\mathcal{ER}_{\\le \\sqrt{nm}}$), which achieves revenue that exceeds $\\text{SRev}_{n+\\sqrt{nm}}(\\mathcal{ER}_{\\le \\sqrt{nm}}^m)$.   Along the way, we show that the competition complexity of $n$ bidders with additive values over $m$ independent items is exactly equal to the minimum $c$ such that $\\text{SRev}_{n+c}(\\mathcal{ER}_{\\le p}^m) \\geq \\text{Rev}_n(\\mathcal{ER}_{\\le p}^m)$ for all $p$ (that is, some truncated Equal Revenue witnesses the worst-case competition complexity). Interestingly, we also show that the untruncated Equal Revenue curve does not witness the worst-case competition complexity when $n > m$: $\\text{SRev}_n(\\mathcal{ER}^m) = nm+O_m(\\ln (n)) \\leq \\text{SRev}_{n+O_m(\\ln (n))}(\\mathcal{ER}^m)$, and therefore our result can only follow by considering all possible truncations.","sentences":["The competition complexity of an auction setting is the number of additional bidders needed such that the simple mechanism of selling items separately (with additional bidders) achieves greater revenue than the optimal but complex (randomized, prior-dependent, Bayesian-truthful) optimal mechanism without the additional bidders.","Our main result settles the competition complexity of $n$ bidders with additive values over $m < n$ independent items at $\\Theta(\\sqrt{nm})$. The $O(\\sqrt{nm})$ upper bound is due to [BW19], and our main result improves the prior lower bound of $\\Omega(\\ln n)$ to $\\Omega(\\sqrt{nm})$.   Our main result follows from an explicit construction of a Bayesian IC auction for $n$ bidders with additive values over $m<n$ independent items drawn from the Equal Revenue curve truncated at $\\sqrt{nm}$ ($\\mathcal{ER}_{\\le \\sqrt{nm}}$), which achieves revenue that exceeds $\\text{SRev}_{n+\\sqrt{nm}}(\\mathcal{ER}_{\\le \\sqrt{nm}}^m)$.   Along the way, we show that the competition complexity of $n$ bidders with additive values over $m$ independent items is exactly equal to the minimum $c$ such that $\\text{SRev}_{n+c}(\\mathcal{ER}_{\\le p}^m) \\geq","\\text{Rev}_n(\\mathcal{ER}_{\\le p}^m)$ for all $p$ (that is, some truncated Equal Revenue witnesses the worst-case competition complexity).","Interestingly, we also show that the untruncated Equal Revenue curve does not witness the worst-case competition complexity when $n > m$: $\\text{SRev}_n(\\mathcal{ER}^m) = nm+O_m(\\ln (n))","\\leq \\text{SRev}_{n+O_m(\\ln (n))}(\\mathcal{ER}^m)$, and therefore our result can only follow by considering all possible truncations."],"url":"http://arxiv.org/abs/2403.03937v1","category":"cs.GT"}
{"created":"2024-03-06 18:45:20","title":"Non-resonant conditions for the Klein-Gordon equation on the circle","abstract":"We consider the infinite dimensional vector of frequencies $\\omega(m)=( \\sqrt{j^2+m})_{j\\in \\mathbb{Z}}$, $m\\in [1,2]$ arising form a linear Klein-Gordon equation on the one dimensional torus and prove that there exists a positive measure set of masses $m'$s for which $\\omega(m)$ satisfies a diophantine condition similar to the one introduced by Bourgain in (JFA, 2005), in the context of Schr\\\"odinger equation with convolution potential. The main difficulties we have to deal with are the asymptotically linear nature of the (infinitely many) $\\omega_{j}'$s and the degeneracy coming from having only one parameter at disposal for their modulation. As an application we provide estimates on the inverse of the adjoint action of the associated quadratic Hamiltonian on homogenenous polynomials of any degree in Gevrey category.","sentences":["We consider the infinite dimensional vector of frequencies $\\omega(m)=( \\sqrt{j^2+m})_{j\\in \\mathbb{Z}}$, $m\\in [1,2]$ arising form a linear Klein-Gordon equation on the one dimensional torus and prove that there exists a positive measure set of masses $m'$s for which $\\omega(m)$ satisfies a diophantine condition similar to the one introduced by Bourgain in (JFA, 2005), in the context of Schr\\\"odinger equation with convolution potential.","The main difficulties we have to deal with are the asymptotically linear nature of the (infinitely many) $\\omega_{j}'$s and the degeneracy coming from having only one parameter at disposal for their modulation.","As an application we provide estimates on the inverse of the adjoint action of the associated quadratic Hamiltonian on homogenenous polynomials of any degree in Gevrey category."],"url":"http://arxiv.org/abs/2403.03936v1","category":"math.AP"}
{"created":"2024-03-06 18:42:01","title":"Polynomial Calculus sizes over the Boolean and Fourier bases are incomparable","abstract":"For every $n >0$, we show the existence of a CNF tautology over $O(n^2)$ variables of width $O(\\log n)$ such that it has a Polynomial Calculus Resolution refutation over $\\{0,1\\}$ variables of size $O(n^3polylog(n))$ but any Polynomial Calculus refutation over $\\{+1,-1\\}$ variables requires size $2^{\\Omega(n)}$. This shows that Polynomial Calculus sizes over the $\\{0,1\\}$ and $\\{+1,-1\\}$ bases are incomparable (since Tseitin tautologies show a separation in the other direction) and answers an open problem posed by Sokolov [Sok20] and Razborov.","sentences":["For every $n >0$, we show the existence of a CNF tautology over $O(n^2)$ variables of width $O(\\log n)$ such that it has a Polynomial Calculus Resolution refutation over $\\{0,1\\}$ variables of size $O(n^3polylog(n))$ but any Polynomial Calculus refutation over $\\{+1,-1\\}$ variables requires size $2^{\\Omega(n)}$.","This shows that Polynomial Calculus sizes over the $\\{0,1\\}$ and $\\{+1,-1\\}$ bases are incomparable (since Tseitin tautologies show a separation in the other direction) and answers an open problem posed by Sokolov [Sok20] and Razborov."],"url":"http://arxiv.org/abs/2403.03933v1","category":"cs.CC"}
{"created":"2024-03-06 18:33:51","title":"Did Translation Models Get More Robust Without Anyone Even Noticing?","abstract":"Neural machine translation (MT) models achieve strong results across a variety of settings, but it is widely believed that they are highly sensitive to \"noisy\" inputs, such as spelling errors, abbreviations, and other formatting issues. In this paper, we revisit this insight in light of recent multilingual MT models and large language models (LLMs) applied to machine translation. Somewhat surprisingly, we show through controlled experiments that these models are far more robust to many kinds of noise than previous models, even when they perform similarly on clean data. This is notable because, even though LLMs have more parameters and more complex training processes than past models, none of the open ones we consider use any techniques specifically designed to encourage robustness. Next, we show that similar trends hold for social media translation experiments -- LLMs are more robust to social media text. We include an analysis of the circumstances in which source correction techniques can be used to mitigate the effects of noise. Altogether, we show that robustness to many types of noise has increased.","sentences":["Neural machine translation (MT) models achieve strong results across a variety of settings, but it is widely believed that they are highly sensitive to \"noisy\" inputs, such as spelling errors, abbreviations, and other formatting issues.","In this paper, we revisit this insight in light of recent multilingual MT models and large language models (LLMs) applied to machine translation.","Somewhat surprisingly, we show through controlled experiments that these models are far more robust to many kinds of noise than previous models, even when they perform similarly on clean data.","This is notable because, even though LLMs have more parameters and more complex training processes than past models, none of the open ones we consider use any techniques specifically designed to encourage robustness.","Next, we show that similar trends hold for social media translation experiments -- LLMs are more robust to social media text.","We include an analysis of the circumstances in which source correction techniques can be used to mitigate the effects of noise.","Altogether, we show that robustness to many types of noise has increased."],"url":"http://arxiv.org/abs/2403.03923v1","category":"cs.CL"}
{"created":"2024-03-06 18:27:18","title":"Risk-Sensitive Mean Field Games with Common Noise: A Theoretical Study with Applications to Interbank Markets","abstract":"In this paper, we address linear-quadratic-Gaussian (LQG) risk-sensitive mean field games (MFGs) with common noise. In this framework agents are exposed to a common noise and aim to minimize an exponential cost functional that reflects their risk sensitivity. We leverage the convex analysis method to derive the optimal strategies of agents in the limit as the number of agents goes to infinity. These strategies yield a Nash equilibrium for the limiting model. The model is then applied to interbank markets, focusing on optimizing lending and borrowing activities to assess systemic and individual bank risks when reserves drop below a critical threshold. We employ Fokker-Planck equations and the first hitting time method to formulate the overall probability of a bank or market default. We observe that the risk-averse behavior of agents reduces the probability of individual defaults and systemic risk, enhancing the resilience of the financial system. Adopting a similar approach based on stochastic Fokker-Planck equations, we further expand our analysis to investigate the conditional probabilities of individual default under specific trajectories of the common market shock.","sentences":["In this paper, we address linear-quadratic-Gaussian (LQG) risk-sensitive mean field games (MFGs) with common noise.","In this framework agents are exposed to a common noise and aim to minimize an exponential cost functional that reflects their risk sensitivity.","We leverage the convex analysis method to derive the optimal strategies of agents in the limit as the number of agents goes to infinity.","These strategies yield a Nash equilibrium for the limiting model.","The model is then applied to interbank markets, focusing on optimizing lending and borrowing activities to assess systemic and individual bank risks when reserves drop below a critical threshold.","We employ Fokker-Planck equations and the first hitting time method to formulate the overall probability of a bank or market default.","We observe that the risk-averse behavior of agents reduces the probability of individual defaults and systemic risk, enhancing the resilience of the financial system.","Adopting a similar approach based on stochastic Fokker-Planck equations, we further expand our analysis to investigate the conditional probabilities of individual default under specific trajectories of the common market shock."],"url":"http://arxiv.org/abs/2403.03915v1","category":"math.OC"}
{"created":"2024-03-06 18:22:30","title":"Effect of Uncorrelated On-site Scalar Potential, and Mass Disorder on Transport of Two-Dimensional Dirac Fermions","abstract":"We investigate the transport properties of massive Dirac fermions subjected to uncorrelated scalar potential disorder, and mass disorder. Using a finite difference method, the conductance is calculated for a wide variety of combinations of these two disorder strengths. By calculating the scaling of conductivity with system size we find that, depending on the combination, the system can have an insulating, scale invariant, and metallic behavior. We identify the critical values of these disorder strengths where the phase transitions occur. We study both the zero and nonzero average mass cases to examine the effect of scalar potential disorder on band gap. Our results suggest a suppression of the band gap by the scalar potential disorder.","sentences":["We investigate the transport properties of massive Dirac fermions subjected to uncorrelated scalar potential disorder, and mass disorder.","Using a finite difference method, the conductance is calculated for a wide variety of combinations of these two disorder strengths.","By calculating the scaling of conductivity with system size we find that, depending on the combination, the system can have an insulating, scale invariant, and metallic behavior.","We identify the critical values of these disorder strengths where the phase transitions occur.","We study both the zero and nonzero average mass cases to examine the effect of scalar potential disorder on band gap.","Our results suggest a suppression of the band gap by the scalar potential disorder."],"url":"http://arxiv.org/abs/2403.03914v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-06 18:15:10","title":"Electron correlations in the kagome flat band metal $\\rm CsCr_3Sb_5$","abstract":"Kagome metals offer a unique platform for investigating robust electron-correlation effects because of their lattice geometry, flat bands and multi-orbital nature. In the cases with active flat bands, recent theoretical studies have pointed to a rich phase diagram that contains not only electronic orders but also quantum criticality. $\\rm CsCr_3Sb_5$ has emerged as a strong candidate for exploring such new physics. Here, using effective tight-binding models obtained from ab initio calculations, we study the effects of electronic correlations and symmetries on the electronic structure of $\\rm CsCr_3 Sb_5$. The effective tight-binding model and Fermi surface comprise multiple Cr-$d$ orbitals and Sb-$p$ orbitals. The introduction of Hubbard-Kanamori interactions leads to orbital-selective band renormalization dominated by the $d_{xz}$ band, concurrently producing emergent flat bands very close to the Fermi level. Our analysis sets the stage for further investigations into the electronic properties of $\\rm CsCr_3Sb_5$, including electronic orders, quantum criticality and unconventional superconductivity, which promise to shed much new light into the electronic materials with frustrated lattices and bring about new connections with the correlation physics of a variety of strongly correlated systems.","sentences":["Kagome metals offer a unique platform for investigating robust electron-correlation effects because of their lattice geometry, flat bands and multi-orbital nature.","In the cases with active flat bands, recent theoretical studies have pointed to a rich phase diagram that contains not only electronic orders but also quantum criticality.","$\\rm CsCr_3Sb_5$ has emerged as a strong candidate for exploring such new physics.","Here, using effective tight-binding models obtained from ab initio calculations, we study the effects of electronic correlations and symmetries on the electronic structure of $\\rm CsCr_3 Sb_5$.","The effective tight-binding model and Fermi surface comprise multiple Cr-$d$ orbitals and Sb-$p$ orbitals.","The introduction of Hubbard-Kanamori interactions leads to orbital-selective band renormalization dominated by the $d_{xz}$ band, concurrently producing emergent flat bands very close to the Fermi level.","Our analysis sets the stage for further investigations into the electronic properties of $\\rm CsCr_3Sb_5$, including electronic orders, quantum criticality and unconventional superconductivity, which promise to shed much new light into the electronic materials with frustrated lattices and bring about new connections with the correlation physics of a variety of strongly correlated systems."],"url":"http://arxiv.org/abs/2403.03911v1","category":"cond-mat.str-el"}
{"created":"2024-03-06 18:14:36","title":"A Unified Model for Active Battery Equalization Systems","abstract":"Lithium-ion battery packs demand effective active equalization systems to enhance their usable capacity and lifetime. Despite numerous topologies and control schemes proposed in the literature, conducting quantitative analyses, comprehensive comparisons, and systematic optimization of their performance remains challenging due to the absence of a unified mathematical model at the pack level. To address this gap, we introduce a novel, hypergraph-based approach to establish the first unified model for various active battery equalization systems. This model reveals the intrinsic relationship between battery cells and equalizers by representing them as the vertices and hyperedges of hypergraphs, respectively. With the developed model, we identify the necessary condition for all equalization systems to achieve balance through controllability analysis, offering valuable insights for selecting the number of equalizers. Moreover, we prove that the battery equalization time is inversely correlated with the second smallest eigenvalue of the hypergraph's Laplacian matrix of each equalization system. This significantly simplifies the selection and optimized design of equalization systems, obviating the need for extensive experiments or simulations to derive the equalization time. Illustrative results demonstrate the efficiency of the proposed model and validate our findings.","sentences":["Lithium-ion battery packs demand effective active equalization systems to enhance their usable capacity and lifetime.","Despite numerous topologies and control schemes proposed in the literature, conducting quantitative analyses, comprehensive comparisons, and systematic optimization of their performance remains challenging due to the absence of a unified mathematical model at the pack level.","To address this gap, we introduce a novel, hypergraph-based approach to establish the first unified model for various active battery equalization systems.","This model reveals the intrinsic relationship between battery cells and equalizers by representing them as the vertices and hyperedges of hypergraphs, respectively.","With the developed model, we identify the necessary condition for all equalization systems to achieve balance through controllability analysis, offering valuable insights for selecting the number of equalizers.","Moreover, we prove that the battery equalization time is inversely correlated with the second smallest eigenvalue of the hypergraph's Laplacian matrix of each equalization system.","This significantly simplifies the selection and optimized design of equalization systems, obviating the need for extensive experiments or simulations to derive the equalization time.","Illustrative results demonstrate the efficiency of the proposed model and validate our findings."],"url":"http://arxiv.org/abs/2403.03910v1","category":"eess.SY"}
{"created":"2024-03-06 18:07:20","title":"Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications","abstract":"The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. Motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood.   Our main contribution is significantly sharper bounds on the approximation parameter degradation of deflation methods for $k$-PCA. For a quadratic form notion of approximation we term ePCA (energy PCA), we show deflation methods suffer no parameter loss. For an alternative well-studied approximation notion we term cPCA (correlation PCA), we tightly characterize the parameter regimes where deflation methods are feasible. Moreover, we show that in all feasible regimes, $k$-cPCA deflation algorithms suffer no asymptotic parameter loss for any constant $k$. We apply our framework to obtain state-of-the-art $k$-PCA algorithms robust to dataset contamination, improving prior work both in sample complexity and approximation quality.","sentences":["The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications.","In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples.","Motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation.","Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood.   ","Our main contribution is significantly sharper bounds on the approximation parameter degradation of deflation methods for $k$-PCA.","For a quadratic form notion of approximation we term ePCA (energy PCA), we show deflation methods suffer no parameter loss.","For an alternative well-studied approximation notion we term cPCA (correlation PCA), we tightly characterize the parameter regimes where deflation methods are feasible.","Moreover, we show that in all feasible regimes, $k$-cPCA deflation algorithms suffer no asymptotic parameter loss for any constant $k$. We apply our framework to obtain state-of-the-art $k$-PCA algorithms robust to dataset contamination, improving prior work both in sample complexity and approximation quality."],"url":"http://arxiv.org/abs/2403.03905v1","category":"math.NA"}
{"created":"2024-03-06 18:00:15","title":"Mamba4Rec: Towards Efficient Sequential Recommendation with Selective State Space Models","abstract":"Sequential recommendation aims to estimate the dynamic user preferences and sequential dependencies among historical user behaviors. Although Transformer-based models have proven to be effective for sequential recommendation, they suffer from the inference inefficiency problem stemming from the quadratic computational complexity of attention operators, especially for long-range behavior sequences. Inspired by the recent success of state space models (SSMs), we propose Mamba4Rec, which is the first work to explore the potential of selective SSMs for efficient sequential recommendation. Built upon the basic Mamba block which is a selective SSM with an efficient hardware-aware parallel algorithm, we incorporate a series of sequential modeling techniques to further promote the model performance and meanwhile maintain the inference efficiency. Experiments on two public datasets demonstrate that Mamba4Rec is able to well address the effectiveness-efficiency dilemma, and defeat both RNN- and attention-based baselines in terms of both effectiveness and efficiency.","sentences":["Sequential recommendation aims to estimate the dynamic user preferences and sequential dependencies among historical user behaviors.","Although Transformer-based models have proven to be effective for sequential recommendation, they suffer from the inference inefficiency problem stemming from the quadratic computational complexity of attention operators, especially for long-range behavior sequences.","Inspired by the recent success of state space models (SSMs), we propose Mamba4Rec, which is the first work to explore the potential of selective SSMs for efficient sequential recommendation.","Built upon the basic Mamba block which is a selective SSM with an efficient hardware-aware parallel algorithm, we incorporate a series of sequential modeling techniques to further promote the model performance and meanwhile maintain the inference efficiency.","Experiments on two public datasets demonstrate that Mamba4Rec is able to well address the effectiveness-efficiency dilemma, and defeat both RNN- and attention-based baselines in terms of both effectiveness and efficiency."],"url":"http://arxiv.org/abs/2403.03900v1","category":"cs.IR"}
{"created":"2024-03-06 17:58:53","title":"Electrical Load Forecasting Model Using Hybrid LSTM Neural Networks with Online Correction","abstract":"Accurate electrical load forecasting is of great importance for the efficient operation and control of modern power systems. In this work, a hybrid long short-term memory (LSTM)-based model with online correction is developed for day-ahead electrical load forecasting. Firstly, four types of features are extracted from the original electrical load dataset, including the historical time series, time index features, historical statistical features, and similarity features. Then, a hybrid LSTM-based electrical load forecasting model is designed, where an LSTM neural network block and a fully-connected neural network block are integrated that can model both temporal features (historical time series) and non-temporal features (the rest features). A gradient regularization-based offline training algorithm and an output layer parameter fine-tuning-based online model correction method are developed to enhance the model's capabilities to defend against disturbance and adapt to the latest load data distribution, thus improving the forecasting accuracy. At last, extensive experiments are carried out to validate the effectiveness of the proposed electrical load forecasting strategy with superior accuracy compared with commonly used forecasting models.","sentences":["Accurate electrical load forecasting is of great importance for the efficient operation and control of modern power systems.","In this work, a hybrid long short-term memory (LSTM)-based model with online correction is developed for day-ahead electrical load forecasting.","Firstly, four types of features are extracted from the original electrical load dataset, including the historical time series, time index features, historical statistical features, and similarity features.","Then, a hybrid LSTM-based electrical load forecasting model is designed, where an LSTM neural network block and a fully-connected neural network block are integrated that can model both temporal features (historical time series) and non-temporal features (the rest features).","A gradient regularization-based offline training algorithm and an output layer parameter fine-tuning-based online model correction method are developed to enhance the model's capabilities to defend against disturbance and adapt to the latest load data distribution, thus improving the forecasting accuracy.","At last, extensive experiments are carried out to validate the effectiveness of the proposed electrical load forecasting strategy with superior accuracy compared with commonly used forecasting models."],"url":"http://arxiv.org/abs/2403.03898v1","category":"eess.SY"}
{"created":"2024-03-06 17:54:44","title":"The Lanczos Tau Framework for Time-Delay Systems: Pad\u00e9 Approximation and Collocation Revisited","abstract":"We reformulate the Lanczos tau method for the discretization of time-delay systems in terms of a pencil of operators, allowing for new insights into this approach. As a first main result, we show that, for the choice of a shifted Legendre basis, this method is equivalent to Pad\\'e approximation in the frequency domain. We illustrate that Lanczos tau methods straightforwardly give rise to sparse, self nesting discretizations. Equivalence is also demonstrated with pseudospectral collocation, where the non-zero collocation points are chosen as the zeroes of orthogonal polynomials. The importance of such a choice manifests itself in the approximation of the $H^2$-norm, where, under mild conditions, super-geometric convergence is observed and, for a special case, super convergence is proved; both significantly faster than the algebraic convergence reported in previous work.","sentences":["We reformulate the Lanczos tau method for the discretization of time-delay systems in terms of a pencil of operators, allowing for new insights into this approach.","As a first main result, we show that, for the choice of a shifted Legendre basis, this method is equivalent to Pad\\'e approximation in the frequency domain.","We illustrate that Lanczos tau methods straightforwardly give rise to sparse, self nesting discretizations.","Equivalence is also demonstrated with pseudospectral collocation, where the non-zero collocation points are chosen as the zeroes of orthogonal polynomials.","The importance of such a choice manifests itself in the approximation of the $H^2$-norm, where, under mild conditions, super-geometric convergence is observed and, for a special case, super convergence is proved; both significantly faster than the algebraic convergence reported in previous work."],"url":"http://arxiv.org/abs/2403.03895v1","category":"math.NA"}
{"created":"2024-03-06 17:49:13","title":"Non-reflective traveling waves in finite thin beams: A parametric study","abstract":"In the present study, the authors introduce a geometrically improved model inspired by the mammalian basilar membrane's properties and special vibratory behavior while conducting a parametric investigation. The goal of this model is to mimic the broadband non-reflective traveling wave response to excitation frequencies, as observed in the basilar membrane. Simple structural elements, such as beams, springs, and dashpots are utilized for this purpose. Therefore, the beam's equation of motion is developed using Hamilton's principle, and the Galerkin method is implemented as the discretization scheme. Then, the model is verified by comparing its outcome to results presented in the literature. Finally, a parametric study is conducted to reveal the effect of different parameters, i.e., the absorber's location and coefficients' values, excitation frequency, geometric tapering, and material grading on the non-reflective traveling wave response of the beam. The results reveal that traveling waves and their quality are strongly dependent on these parameters. In addition, this study suggests that adopting a single spring-damper system within the span of the beam as the wave reflection absorber may not address the entire bandwidth.","sentences":["In the present study, the authors introduce a geometrically improved model inspired by the mammalian basilar membrane's properties and special vibratory behavior while conducting a parametric investigation.","The goal of this model is to mimic the broadband non-reflective traveling wave response to excitation frequencies, as observed in the basilar membrane.","Simple structural elements, such as beams, springs, and dashpots are utilized for this purpose.","Therefore, the beam's equation of motion is developed using Hamilton's principle, and the Galerkin method is implemented as the discretization scheme.","Then, the model is verified by comparing its outcome to results presented in the literature.","Finally, a parametric study is conducted to reveal the effect of different parameters, i.e., the absorber's location and coefficients' values, excitation frequency, geometric tapering, and material grading on the non-reflective traveling wave response of the beam.","The results reveal that traveling waves and their quality are strongly dependent on these parameters.","In addition, this study suggests that adopting a single spring-damper system within the span of the beam as the wave reflection absorber may not address the entire bandwidth."],"url":"http://arxiv.org/abs/2403.03889v1","category":"math.DS"}
{"created":"2024-03-06 17:36:33","title":"A Survey on Adversarial Contention Resolution","abstract":"Contention resolution addresses the challenge of coordinating access by multiple processes to a shared resource such as memory, disk storage, or a communication channel. Originally spurred by challenges in database systems and bus networks, contention resolution has endured as an important abstraction for resource sharing, despite decades of technological change. Here, we survey the literature on resolving worst-case contention, where the number of processes and the time at which each process may start seeking access to the resource is dictated by an adversary. We highlight the evolution of contention resolution, where new concerns -- such as security, quality of service, and energy efficiency -- are motivated by modern systems. These efforts have yielded insights into the limits of randomized and deterministic approaches, as well as the impact of different model assumptions such as global clock synchronization, knowledge of the number of processors, feedback from access attempts, and attacks on the availability of the shared resource.","sentences":["Contention resolution addresses the challenge of coordinating access by multiple processes to a shared resource such as memory, disk storage, or a communication channel.","Originally spurred by challenges in database systems and bus networks, contention resolution has endured as an important abstraction for resource sharing, despite decades of technological change.","Here, we survey the literature on resolving worst-case contention, where the number of processes and the time at which each process may start seeking access to the resource is dictated by an adversary.","We highlight the evolution of contention resolution, where new concerns -- such as security, quality of service, and energy efficiency -- are motivated by modern systems.","These efforts have yielded insights into the limits of randomized and deterministic approaches, as well as the impact of different model assumptions such as global clock synchronization, knowledge of the number of processors, feedback from access attempts, and attacks on the availability of the shared resource."],"url":"http://arxiv.org/abs/2403.03876v1","category":"cs.DC"}
{"created":"2024-03-06 17:25:26","title":"A dormant, overmassive black hole in the early Universe","abstract":"Recent observations have found a large number of supermassive black holes already in place in the first few hundred million years after Big Bang. The channels of formation and growth of these early, massive black holes are not clear, with scenarios ranging from heavy seeds to light seeds experiencing bursts of high accretion rate. Here we present the detection, from the JADES survey, of broad Halpha emission in a galaxy at z=6.68, which traces a black hole with mass of ~ 4 * 10^8 Msun and accreting at a rate of only 0.02 times the Eddington limit. The host galaxy has low star formation rate (~ 1 Msun/yr, a factor of 3 below the star forming main sequence). The black hole to stellar mass ratio is ~ 0.4, i.e. about 1,000 times above the local relation, while the system is closer to the local relations in terms of dynamical mass and velocity dispersion of the host galaxy. This object is most likely the tip of the iceberg of a much larger population of dormant black holes around the epoch of reionisation. Its properties are consistent with scenarios in which short bursts of super-Eddington accretion have resulted in black hole overgrowth and massive gas expulsion from the accretion disk; in between bursts, black holes spend most of their life in a dormant state.","sentences":["Recent observations have found a large number of supermassive black holes already in place in the first few hundred million years after Big Bang.","The channels of formation and growth of these early, massive black holes are not clear, with scenarios ranging from heavy seeds to light seeds experiencing bursts of high accretion rate.","Here we present the detection, from the JADES survey, of broad Halpha emission in a galaxy at z=6.68, which traces a black hole with mass of ~ 4 * 10^8 Msun and accreting at a rate of only 0.02 times the Eddington limit.","The host galaxy has low star formation rate (~ 1 Msun/yr, a factor of 3 below the star forming main sequence).","The black hole to stellar mass ratio is ~ 0.4, i.e. about 1,000 times above the local relation, while the system is closer to the local relations in terms of dynamical mass and velocity dispersion of the host galaxy.","This object is most likely the tip of the iceberg of a much larger population of dormant black holes around the epoch of reionisation.","Its properties are consistent with scenarios in which short bursts of super-Eddington accretion have resulted in black hole overgrowth and massive gas expulsion from the accretion disk; in between bursts, black holes spend most of their life in a dormant state."],"url":"http://arxiv.org/abs/2403.03872v1","category":"astro-ph.GA"}
{"created":"2024-03-06 17:18:37","title":"Digitality as a \"longue dur\u00e8e\" historical phenomenon","abstract":"The digital age introduced the Digital Ecological Niche (DEN), revolutionizing human interactions. The advent of Digital History (DHy) has marked a methodological shift in historical studies, tracing its roots to Babbage and Lovelace's 19th-century work on \"coding\" as a foundational communication process, fostering a new interaction paradigm between humans and machines, termed \"person2persons2machines.\" This evolution, through digitization and informatization, builds upon ancient coding practices but was significantly advanced by Babbage and Lovelace's contributions to mathematical linguistic systems, laying the groundwork for Computer Science. This field, central to 20th-century mainframe interaction through programming languages and formalization, situates Digital History within a broader historical context. Here, coding and mathematical methodologies empower historians with advanced technologies for historical data preservation and analysis. Nonetheless, the extent to which computation and Turing machines can fully understand and interpret history remains a subject of debate.","sentences":["The digital age introduced the Digital Ecological Niche (DEN), revolutionizing human interactions.","The advent of Digital History (DHy) has marked a methodological shift in historical studies, tracing its roots to Babbage and Lovelace's 19th-century work on \"coding\" as a foundational communication process, fostering a new interaction paradigm between humans and machines, termed \"person2persons2machines.\"","This evolution, through digitization and informatization, builds upon ancient coding practices but was significantly advanced by Babbage and Lovelace's contributions to mathematical linguistic systems, laying the groundwork for Computer Science.","This field, central to 20th-century mainframe interaction through programming languages and formalization, situates Digital History within a broader historical context.","Here, coding and mathematical methodologies empower historians with advanced technologies for historical data preservation and analysis.","Nonetheless, the extent to which computation and Turing machines can fully understand and interpret history remains a subject of debate."],"url":"http://arxiv.org/abs/2403.03869v1","category":"cs.CY"}
{"created":"2024-03-06 17:11:38","title":"Designing Informative Metrics for Few-Shot Example Selection","abstract":"Pretrained language models (PLMs) have shown remarkable few-shot learning capabilities when provided with properly formatted examples. However, selecting the \"best\" examples remains an open challenge. We propose a complexity-based prompt selection approach for sequence tagging tasks. This approach avoids the training of a dedicated model for selection of examples, and instead uses certain metrics to align the syntactico-semantic complexity of test sentences and examples. We use both sentence- and word-level metrics to match the complexity of examples to the (test) sentence being considered. Our results demonstrate that our approach extracts greater performance from PLMs: it achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute improvement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.","sentences":["Pretrained language models (PLMs) have shown remarkable few-shot learning capabilities when provided with properly formatted examples.","However, selecting the \"best\" examples remains an open challenge.","We propose a complexity-based prompt selection approach for sequence tagging tasks.","This approach avoids the training of a dedicated model for selection of examples, and instead uses certain metrics to align the syntactico-semantic complexity of test sentences and examples.","We use both sentence- and word-level metrics to match the complexity of examples to the (test) sentence being considered.","Our results demonstrate that our approach extracts greater performance from PLMs: it achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute improvement in F1 score on the CoNLL2003 dataset for GPT-4.","We also see large gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B."],"url":"http://arxiv.org/abs/2403.03861v1","category":"cs.CL"}
{"created":"2024-03-06 17:06:17","title":"Emojinize : Enriching Any Text with Emoji Translations","abstract":"Emoji have become ubiquitous in written communication, on the Web and beyond. They can emphasize or clarify emotions, add details to conversations, or simply serve decorative purposes. This casual use, however, barely scratches the surface of the expressive power of emoji. To further unleash this power, we present Emojinize, a method for translating arbitrary text phrases into sequences of one or more emoji without requiring human input. By leveraging the power of large language models, Emojinize can choose appropriate emoji by disambiguating based on context (eg, cricket-bat vs bat) and can express complex concepts compositionally by combining multiple emoji (eq, ''Emojinize'' is translated to input-latin-letters right-arrow grinning-face). In a cloze test--based user study, we show that Emojinize's emoji translations increase the human guessability of masked words by 55%, whereas human-picked emoji translations do so by only 29%. These results suggest that emoji provide a sufficiently rich vocabulary to accurately translate a wide variety of words. Moreover, annotating words and phrases with Emojinize's emoji translations opens the door to numerous downstream applications, including children learning how to read, adults learning foreign languages, and text understanding for people with learning disabilities.","sentences":["Emoji have become ubiquitous in written communication, on the Web and beyond.","They can emphasize or clarify emotions, add details to conversations, or simply serve decorative purposes.","This casual use, however, barely scratches the surface of the expressive power of emoji.","To further unleash this power, we present Emojinize, a method for translating arbitrary text phrases into sequences of one or more emoji without requiring human input.","By leveraging the power of large language models, Emojinize can choose appropriate emoji by disambiguating based on context (eg, cricket-bat vs bat) and can express complex concepts compositionally by combining multiple emoji (eq, ''Emojinize'' is translated to input-latin-letters right-arrow grinning-face).","In a cloze test--based user study, we show that Emojinize's emoji translations increase the human guessability of masked words by 55%, whereas human-picked emoji translations do so by only 29%.","These results suggest that emoji provide a sufficiently rich vocabulary to accurately translate a wide variety of words.","Moreover, annotating words and phrases with Emojinize's emoji translations opens the door to numerous downstream applications, including children learning how to read, adults learning foreign languages, and text understanding for people with learning disabilities."],"url":"http://arxiv.org/abs/2403.03857v1","category":"cs.CL"}
{"created":"2024-03-06 17:04:18","title":"ShortGPT: Layers in Large Language Models are More Redundant Than You Expect","abstract":"As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as opposed to more complex pruning techniques, suggests a high degree of redundancy in the model architecture.","sentences":["As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters.","However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality.","Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs.","We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores.","Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning.","Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation.","The ability to achieve better results through simple layer removal, as opposed to more complex pruning techniques, suggests a high degree of redundancy in the model architecture."],"url":"http://arxiv.org/abs/2403.03853v1","category":"cs.CL"}
{"created":"2024-03-06 16:48:33","title":"Flexible Optimization for Cyber-Physical and Human Systems","abstract":"Can we allow humans to pick among different, yet reasonably similar, decisions? Are we able to construct optimization problems whose outcome are sets of feasible, close-to-optimal decisions for human users to pick from, instead of a single, hardly explainable, do-as-I-say ``optimal'' directive?   In this paper, we explore two complementary ways to render optimization problems stemming from cyber-physical applications flexible. In doing so, the optimization outcome is a trade off between engineering best and flexibility for the users to decide to do something slightly different. The first method is based on robust optimization and convex reformulations. The second method is stochastic and inspired from stochastic optimization with decision-dependent distributions.","sentences":["Can we allow humans to pick among different, yet reasonably similar, decisions?","Are we able to construct optimization problems whose outcome are sets of feasible, close-to-optimal decisions for human users to pick from, instead of a single, hardly explainable, do-as-I-say ``optimal'' directive?   ","In this paper, we explore two complementary ways to render optimization problems stemming from cyber-physical applications flexible.","In doing so, the optimization outcome is a trade off between engineering best and flexibility for the users to decide to do something slightly different.","The first method is based on robust optimization and convex reformulations.","The second method is stochastic and inspired from stochastic optimization with decision-dependent distributions."],"url":"http://arxiv.org/abs/2403.03847v1","category":"math.OC"}
{"created":"2024-03-06 16:38:37","title":"The Emerging Stellar Complex in Mon R2: Membership and Optical Variability Classification","abstract":"Monoceros R2 (Mon R2) is one of the closest large active star-forming regions. This extremely young and partially embedded region provides an excellent laboratory for studying star formation and the early evolution of young stellar objects (YSOs). In this paper, we conduct an optical study of the greater Mon R2 region. Beginning with 1690 previously identified candidate YSOs, we used 496 sources with good proper motions and parallaxes from Gaia DR3 to determine the astrometric properties for likely members of Mon R2. We then used both astrometric and photometric (isochronal and variability) criteria to determine that 308 of these stars are highly probable members. Using the same criteria, we considered a broad area search around Mon R2 in Gaia DR3, and separated candidate members from field stars. In total, we selected 651 likely new cluster members that had been missed in the previous x-ray and infrared excess selection techniques used in the past to establish cluster membership. Revised astrometric properties of the cluster were found using the combined sample of ~959 highly probable member stars. For the literature plus new candidate member list, optical light curves were compiled from the Zwicky Transient Facility. For 470 identified variable sources, we attempted classification based on the Flux Asymmetry (M) and Quasi-Periodicity (Q) metrics. We find that Mon R2 is dominated by quasi-periodic symmetric variables, with aperiodic sources also a significant population. A few tens of large-amplitude variables are identified that may be of interest for further study.","sentences":["Monoceros R2 (Mon R2) is one of the closest large active star-forming regions.","This extremely young and partially embedded region provides an excellent laboratory for studying star formation and the early evolution of young stellar objects (YSOs).","In this paper, we conduct an optical study of the greater Mon R2 region.","Beginning with 1690 previously identified candidate YSOs, we used 496 sources with good proper motions and parallaxes from Gaia DR3 to determine the astrometric properties for likely members of Mon R2.","We then used both astrometric and photometric (isochronal and variability) criteria to determine that 308 of these stars are highly probable members.","Using the same criteria, we considered a broad area search around Mon R2 in Gaia DR3, and separated candidate members from field stars.","In total, we selected 651 likely new cluster members that had been missed in the previous x-ray and infrared excess selection techniques used in the past to establish cluster membership.","Revised astrometric properties of the cluster were found using the combined sample of ~959 highly probable member stars.","For the literature plus new candidate member list, optical light curves were compiled from the Zwicky Transient Facility.","For 470 identified variable sources, we attempted classification based on the Flux Asymmetry (M) and Quasi-Periodicity (Q) metrics.","We find that Mon R2 is dominated by quasi-periodic symmetric variables, with aperiodic sources also a significant population.","A few tens of large-amplitude variables are identified that may be of interest for further study."],"url":"http://arxiv.org/abs/2403.03843v1","category":"astro-ph.SR"}
{"created":"2024-03-06 16:28:15","title":"Disorder-to-order transition of long fibers contained in evaporating sessile drops","abstract":"A liquid drop containing a long fiber is a complex system whose configuration is determined by an interplay of elastic stresses in the fiber and capillary forces due to the liquid. We study the morphological evolution of fibers that are much longer than the drop diameter in evaporating sessile drops. After insertion, the fibers are either found in an ordered or disordered state, with increasing disorder for increasing fiber length. Upon evaporation, the order increases, in such a way that the final configuration deposited on the solid surface is either a circle, an ellipse, or 8-shaped. The morphology of the deposit depends on the fiber length and the elastocapillary length, both non-dimensionalized with the characteristic drop size, which we classify in a morphology regime map. The disorder-to-order transition allows depositing ordered fiber structures on solid surfaces even in cases of a strongly disordered state after fiber insertion. Combined with technologies such as inkjet printing, this process could open new avenues to decorate surfaces with filamental structures whose morphology can be controlled by varying the fiber length.","sentences":["A liquid drop containing a long fiber is a complex system whose configuration is determined by an interplay of elastic stresses in the fiber and capillary forces due to the liquid.","We study the morphological evolution of fibers that are much longer than the drop diameter in evaporating sessile drops.","After insertion, the fibers are either found in an ordered or disordered state, with increasing disorder for increasing fiber length.","Upon evaporation, the order increases, in such a way that the final configuration deposited on the solid surface is either a circle, an ellipse, or 8-shaped.","The morphology of the deposit depends on the fiber length and the elastocapillary length, both non-dimensionalized with the characteristic drop size, which we classify in a morphology regime map.","The disorder-to-order transition allows depositing ordered fiber structures on solid surfaces even in cases of a strongly disordered state after fiber insertion.","Combined with technologies such as inkjet printing, this process could open new avenues to decorate surfaces with filamental structures whose morphology can be controlled by varying the fiber length."],"url":"http://arxiv.org/abs/2403.03836v1","category":"cond-mat.soft"}
{"created":"2024-03-06 16:22:07","title":"Formation of super-Mercuries via giant impacts","abstract":"During the final stage of planetary formation, different formation pathways of planetary embryos could significantly influence the observed variations in planetary densities. Of the approximately 5,000 exoplanets identified to date, a notable subset exhibit core fractions reminiscent of Mercury, potentially a consequence of high-velocity giant impacts. In order to better understand the influence of such collisions on planetary formation and compositional evolution, we conducted an extensive set of smoothed particle hydrodynamics giant impact simulations between two-layered rocky bodies. These simulations spanned a broad range of impact velocities from one to eleven times the mutual escape velocity. We derived novel scaling laws that estimate the mass and core mass fraction of the largest post-collision remnants. Our findings indicate that the extent of core vaporization markedly influences mantle stripping efficiency at low impact angles. We delineate the distinct roles played by two mechanisms -- kinetic momentum transfer and vaporization-induced ejection -- in mantle stripping. Our research suggests that collisional outcomes for multi-layered planets are more complex than those for undifferentiated planetesimal impacts. Thus, a single universal law may not encompass all collision processes. We found a significant decrease in the mantle stripping efficiency as the impact angle increases. To form a 5 M$_{\\oplus}$ super-Mercury at $45^{\\circ}$, an impact velocity over 200 km s$^{-1}$ is required. This poses a challenge to the formation of super-Mercuries through a single giant impact, implying that their formation would either favor relatively low-angle single impacts or multiple","sentences":["During the final stage of planetary formation, different formation pathways of planetary embryos could significantly influence the observed variations in planetary densities.","Of the approximately 5,000 exoplanets identified to date, a notable subset exhibit core fractions reminiscent of Mercury, potentially a consequence of high-velocity giant impacts.","In order to better understand the influence of such collisions on planetary formation and compositional evolution, we conducted an extensive set of smoothed particle hydrodynamics giant impact simulations between two-layered rocky bodies.","These simulations spanned a broad range of impact velocities from one to eleven times the mutual escape velocity.","We derived novel scaling laws that estimate the mass and core mass fraction of the largest post-collision remnants.","Our findings indicate that the extent of core vaporization markedly influences mantle stripping efficiency at low impact angles.","We delineate the distinct roles played by two mechanisms -- kinetic momentum transfer and vaporization-induced ejection -- in mantle stripping.","Our research suggests that collisional outcomes for multi-layered planets are more complex than those for undifferentiated planetesimal impacts.","Thus, a single universal law may not encompass all collision processes.","We found a significant decrease in the mantle stripping efficiency as the impact angle increases.","To form a 5 M$_{\\oplus}$ super-Mercury at $45^{\\circ}$, an impact velocity over 200 km s$^{-1}$ is required.","This poses a challenge to the formation of super-Mercuries through a single giant impact, implying that their formation would either favor relatively low-angle single impacts or multiple"],"url":"http://arxiv.org/abs/2403.03831v1","category":"astro-ph.EP"}
{"created":"2024-03-06 16:19:35","title":"Parameterized Algorithms for Balanced Cluster Edge Modification Problems","abstract":"We introduce Cluster Edge Modification problems with constraints on the size of the clusters and study their complexity. A graph $G$ is a cluster graph if every connected component of $G$ is a clique. In a typical Cluster Edge Modification problem such as the widely studied Cluster Editing, we are given a graph $G$ and a non-negative integer $k$ as input, and we have to decide if we can turn $G$ into a cluster graph by way of at most $k$ edge modifications -- that is, by adding or deleting edges. In this paper, we study the parameterized complexity of such problems, but with an additional constraint: The size difference between any two connected components of the resulting cluster graph should not exceed a given threshold. Depending on which modifications are permissible -- only adding edges, only deleting edges, both adding and deleting edges -- we have three different computational problems. We show that all three problems, when parameterized by $k$, admit single-exponential time FPT algorithms and polynomial kernels. Our problems may be thought of as the size-constrained or balanced counterparts of the typical Cluster Edge Modification problems, similar to the well-studied size-constrained or balanced counterparts of other clustering problems such as $k$-Means Clustering.","sentences":["We introduce Cluster Edge Modification problems with constraints on the size of the clusters and study their complexity.","A graph $G$ is a cluster graph if every connected component of $G$ is a clique.","In a typical Cluster Edge Modification problem such as the widely studied Cluster Editing, we are given a graph $G$ and a non-negative integer $k$ as input, and we have to decide if we can turn $G$ into a cluster graph by way of at most $k$ edge modifications -- that is, by adding or deleting edges.","In this paper, we study the parameterized complexity of such problems, but with an additional constraint: The size difference between any two connected components of the resulting cluster graph should not exceed a given threshold.","Depending on which modifications are permissible -- only adding edges, only deleting edges, both adding and deleting edges -- we have three different computational problems.","We show that all three problems, when parameterized by $k$, admit single-exponential time FPT algorithms and polynomial kernels.","Our problems may be thought of as the size-constrained or balanced counterparts of the typical Cluster Edge Modification problems, similar to the well-studied size-constrained or balanced counterparts of other clustering problems such as $k$-Means Clustering."],"url":"http://arxiv.org/abs/2403.03830v1","category":"cs.DS"}
{"created":"2024-03-06 16:19:11","title":"CIBRA identifies genomic alterations with a system-wide impact on tumor biology","abstract":"Background: Genomic instability is a hallmark of cancer, leading to many somatic alterations. Identifying which alterations have a system-wide impact is a challenging task. Nevertheless, this is an essential first step for prioritizing potential biomarkers. We developed CIBRA (Computational Identification of Biologically Relevant Alterations), a method that determines the system-wide impact of genomic alterations on tumor biology by integrating two distinct omics data types: one indicating genomic alterations (e.g., genomics), and another defining a system-wide expression response (e.g., transcriptomics). CIBRA was evaluated with genome-wide screens in 33 cancer types using primary and metastatic cancer data from the Cancer Genome Atlas and Hartwig Medical Foundation. Results: We demonstrate the capability of CIBRA by successfully confirming the impact of point mutations in experimentally validated oncogenes and tumor suppressor genes. Surprisingly, many genes affected by structural variants were identified to have a strong system-wide impact (30.3%), suggesting that their role in cancer development has thus far been largely underreported. Additionally, CIBRA can identify impact with only ten cases and controls, providing a novel way to prioritize genomic alterations with a prominent role in cancer biology. Conclusions: Our findings demonstrate that CIBRA can identify cancer drivers by combining genomics and transcriptomics data. Moreover, our work shows an unexpected substantial system-wide impact of structural variants in cancer. Hence, CIBRA has the potential to preselect and refine current definitions of genomic alterations to derive more nuanced biomarkers for diagnostics, disease progression, and treatment response. CIBRA is available at https://github.com/AIT4LIFE-UU/CIBRA","sentences":["Background: Genomic instability is a hallmark of cancer, leading to many somatic alterations.","Identifying which alterations have a system-wide impact is a challenging task.","Nevertheless, this is an essential first step for prioritizing potential biomarkers.","We developed CIBRA (Computational Identification of Biologically Relevant Alterations), a method that determines the system-wide impact of genomic alterations on tumor biology by integrating two distinct omics data types: one indicating genomic alterations (e.g., genomics), and another defining a system-wide expression response (e.g., transcriptomics).","CIBRA was evaluated with genome-wide screens in 33 cancer types using primary and metastatic cancer data from the Cancer Genome Atlas and Hartwig Medical Foundation.","Results:","We demonstrate the capability of CIBRA by successfully confirming the impact of point mutations in experimentally validated oncogenes and tumor suppressor genes.","Surprisingly, many genes affected by structural variants were identified to have a strong system-wide impact (30.3%), suggesting that their role in cancer development has thus far been largely underreported.","Additionally, CIBRA can identify impact with only ten cases and controls, providing a novel way to prioritize genomic alterations with a prominent role in cancer biology.","Conclusions: Our findings demonstrate that CIBRA can identify cancer drivers by combining genomics and transcriptomics data.","Moreover, our work shows an unexpected substantial system-wide impact of structural variants in cancer.","Hence, CIBRA has the potential to preselect and refine current definitions of genomic alterations to derive more nuanced biomarkers for diagnostics, disease progression, and treatment response.","CIBRA is available at https://github.com/AIT4LIFE-UU/CIBRA"],"url":"http://arxiv.org/abs/2403.03829v1","category":"q-bio.GN"}
{"created":"2024-03-06 15:57:56","title":"A Precision Drone Landing System using Visual and IR Fiducial Markers and a Multi-Payload Camera","abstract":"We propose a method for autonomous precision drone landing with fiducial markers and a gimbal-mounted, multi-payload camera with wide-angle, zoom, and IR sensors. The method has minimal data requirements; it depends primarily on the direction from the drone to the landing pad, enabling it to switch dynamically between the camera's different sensors and zoom factors, and minimizing auxiliary sensor requirements. It eliminates the need for data such as altitude above ground level, straight-line distance to the landing pad, fiducial marker size, and 6 DoF marker pose (of which the orientation is problematic). We leverage the zoom and wide-angle cameras, as well as visual April Tag fiducial markers to conduct successful precision landings from much longer distances than in previous work (168m horizontal distance, 102m altitude). We use two types of April Tags in the IR spectrum - active and passive - for precision landing both at daytime and nighttime, instead of simple IR beacons used in most previous work. The active IR landing pad is heated; the novel, passive one is unpowered, at ambient temperature, and depends on its high reflectivity and an IR differential between the ground and the sky. Finally, we propose a high-level control policy to manage initial search for the landing pad and subsequent searches if it is lost - not addressed in previous work. The method demonstrates successful landings with the landing skids at least touching the landing pad, achieving an average error of 0.19m. It also demonstrates successful recovery and landing when the landing pad is temporarily obscured.","sentences":["We propose a method for autonomous precision drone landing with fiducial markers and a gimbal-mounted, multi-payload camera with wide-angle, zoom, and IR sensors.","The method has minimal data requirements; it depends primarily on the direction from the drone to the landing pad, enabling it to switch dynamically between the camera's different sensors and zoom factors, and minimizing auxiliary sensor requirements.","It eliminates the need for data such as altitude above ground level, straight-line distance to the landing pad, fiducial marker size, and 6 DoF marker pose (of which the orientation is problematic).","We leverage the zoom and wide-angle cameras, as well as visual April Tag fiducial markers to conduct successful precision landings from much longer distances than in previous work (168m horizontal distance, 102m altitude).","We use two types of April Tags in the IR spectrum - active and passive - for precision landing both at daytime and nighttime, instead of simple IR beacons used in most previous work.","The active IR landing pad is heated; the novel, passive one is unpowered, at ambient temperature, and depends on its high reflectivity and an IR differential between the ground and the sky.","Finally, we propose a high-level control policy to manage initial search for the landing pad and subsequent searches if it is lost - not addressed in previous work.","The method demonstrates successful landings with the landing skids at least touching the landing pad, achieving an average error of 0.19m. It also demonstrates successful recovery and landing when the landing pad is temporarily obscured."],"url":"http://arxiv.org/abs/2403.03806v1","category":"cs.RO"}
{"created":"2024-03-06 15:56:48","title":"Ultrafast Band Structure Dynamics in Bulk 1$T$-VSe$_2$","abstract":"Complex materials encompassing different phases of matter can display new photoinduced metastable states differing from those attainable under equilibrium conditions. These states can be realized when energy is injected in the material following a non-equilibrium pathway, unbalancing the unperturbed energy landscape of the material. Guided by the fact that photoemission experiments allow for detailed insights in the electronic band structure of ordered systems, here we study bulk 1$T$-VSe$_2$ in its metallic and charge-density-wave phase by time- and angle-resolved photoelectron spectroscopy. After near-infrared optical excitation, the system shows a net increase of the density of states in the energy range of the valence bands, in the vicinity of the Fermi level, lasting for several picoseconds. We discuss possible origins as band shifts or correlation effects on the basis of a band structure analysis. Our results uncover the possibility of altering the electronic band structure of bulk 1$T$-VSe$_2$ for low excitation fluences, contributing to the understanding of light-induced electronic states.","sentences":["Complex materials encompassing different phases of matter can display new photoinduced metastable states differing from those attainable under equilibrium conditions.","These states can be realized when energy is injected in the material following a non-equilibrium pathway, unbalancing the unperturbed energy landscape of the material.","Guided by the fact that photoemission experiments allow for detailed insights in the electronic band structure of ordered systems, here we study bulk 1$T$-VSe$_2$ in its metallic and charge-density-wave phase by time- and angle-resolved photoelectron spectroscopy.","After near-infrared optical excitation, the system shows a net increase of the density of states in the energy range of the valence bands, in the vicinity of the Fermi level, lasting for several picoseconds.","We discuss possible origins as band shifts or correlation effects on the basis of a band structure analysis.","Our results uncover the possibility of altering the electronic band structure of bulk 1$T$-VSe$_2$ for low excitation fluences, contributing to the understanding of light-induced electronic states."],"url":"http://arxiv.org/abs/2403.03805v1","category":"cond-mat.str-el"}
{"created":"2024-03-06 15:42:33","title":"Pseudospin Polarization of Composite Fermions under Uniaxial Strain","abstract":"A two dimensional system with extra degrees of freedom, such as spin and valley, is of great interest in the study of quantum phase transitions. The critical condition when a transition between different multicomponent fractional quantum Hall states appears is one of the very few junctions for many body problems between theoretical calculations and experiments. In this work, we present that uniaxial strain induces pseudospin transitions of composite fermions in a two-dimensional hole gas. Determined from transport behavior, strain along <111> effectively changes pseudospin energy levels. We deduce that diagonal strain dominates these variations. Our experiment provides a wedge for manipulating two dimensional interacting systems mechanically.","sentences":["A two dimensional system with extra degrees of freedom, such as spin and valley, is of great interest in the study of quantum phase transitions.","The critical condition when a transition between different multicomponent fractional quantum Hall states appears is one of the very few junctions for many body problems between theoretical calculations and experiments.","In this work, we present that uniaxial strain induces pseudospin transitions of composite fermions in a two-dimensional hole gas.","Determined from transport behavior, strain along <111> effectively changes pseudospin energy levels.","We deduce that diagonal strain dominates these variations.","Our experiment provides a wedge for manipulating two dimensional interacting systems mechanically."],"url":"http://arxiv.org/abs/2403.03793v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-06 15:33:32","title":"PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion","abstract":"The growing dependence on Large Language Models (LLMs) for finishing user instructions necessitates a comprehensive understanding of their robustness to complex task completion in real-world situations. To address this critical need, we propose the PowerPoint Task Completion Robustness benchmark (PPTC-R) to measure LLMs' robustness to the user PPT task instruction and software version. Specifically, we construct adversarial user instructions by attacking user instructions at sentence, semantic, and multi-language levels. To assess the robustness of Language Models to software versions, we vary the number of provided APIs to simulate both the newest version and earlier version settings. Subsequently, we test 3 closed-source and 4 open-source LLMs using a benchmark that incorporates these robustness settings, aiming to evaluate how deviations impact LLMs' API calls for task completion. We find that GPT-4 exhibits the highest performance and strong robustness in our benchmark, particularly in the version update and the multilingual settings. However, we find that all LLMs lose their robustness when confronted with multiple challenges (e.g., multi-turn) simultaneously, leading to significant performance drops. We further analyze the robustness behavior and error reasons of LLMs in our benchmark, which provide valuable insights for researchers to understand the LLM's robustness in task completion and develop more robust LLMs and agents. We release the code and data at \\url{https://github.com/ZekaiGalaxy/PPTCR}.","sentences":["The growing dependence on Large Language Models (LLMs) for finishing user instructions necessitates a comprehensive understanding of their robustness to complex task completion in real-world situations.","To address this critical need, we propose the PowerPoint Task Completion Robustness benchmark (PPTC-R) to measure LLMs' robustness to the user PPT task instruction and software version.","Specifically, we construct adversarial user instructions by attacking user instructions at sentence, semantic, and multi-language levels.","To assess the robustness of Language Models to software versions, we vary the number of provided APIs to simulate both the newest version and earlier version settings.","Subsequently, we test 3 closed-source and 4 open-source LLMs using a benchmark that incorporates these robustness settings, aiming to evaluate how deviations impact LLMs' API calls for task completion.","We find that GPT-4 exhibits the highest performance and strong robustness in our benchmark, particularly in the version update and the multilingual settings.","However, we find that all LLMs lose their robustness when confronted with multiple challenges (e.g., multi-turn) simultaneously, leading to significant performance drops.","We further analyze the robustness behavior and error reasons of LLMs in our benchmark, which provide valuable insights for researchers to understand the LLM's robustness in task completion and develop more robust LLMs and agents.","We release the code and data at \\url{https://github.com/ZekaiGalaxy/PPTCR}."],"url":"http://arxiv.org/abs/2403.03788v1","category":"cs.CL"}
{"created":"2024-03-06 15:26:26","title":"Noise-induced oscillations for the mean-field Dissipative Contact Process","abstract":"We study a dissipative version of the contact process, with mean-field interaction, which admits a simple epidemiological interpretation. The propagation of chaos and the corresponding normal fluctuations reveal that the noise present in the finite-size system induces oscillations with a nearly deterministic period and a randomly varying amplitude. This is reminiscent of the emergence of pandemic waves in real epidemics.","sentences":["We study a dissipative version of the contact process, with mean-field interaction, which admits a simple epidemiological interpretation.","The propagation of chaos and the corresponding normal fluctuations reveal that the noise present in the finite-size system induces oscillations with a nearly deterministic period and a randomly varying amplitude.","This is reminiscent of the emergence of pandemic waves in real epidemics."],"url":"http://arxiv.org/abs/2403.03783v1","category":"math.PR"}
{"created":"2024-03-06 15:18:35","title":"High-Impedance Microwave Resonators with Two-Photon Nonlinear Effects","abstract":"In this article, we present an experimental study of a Josephson junction -based high-impedance resonator. By taking the resonator to the limit of consisting effectively only of one junction, results in strong non-linear effects already for the second photon while maintaining a high impedance of the resonance mode. Our experiment yields thus resonators with the strong interactions both between individual resonator photons and from the resonator photons to other electric quantum systems. We also present an energy diagram technique which enables to measure, identify and analyse different multi-photon optics processes along their energy conservation lines.","sentences":["In this article, we present an experimental study of a Josephson junction -based high-impedance resonator.","By taking the resonator to the limit of consisting effectively only of one junction, results in strong non-linear effects already for the second photon while maintaining a high impedance of the resonance mode.","Our experiment yields thus resonators with the strong interactions both between individual resonator photons and from the resonator photons to other electric quantum systems.","We also present an energy diagram technique which enables to measure, identify and analyse different multi-photon optics processes along their energy conservation lines."],"url":"http://arxiv.org/abs/2403.03779v1","category":"quant-ph"}
{"created":"2024-03-06 15:05:39","title":"Joint Sparsity Pattern Learning Based Channel Estimation for Massive MIMO-OTFS Systems","abstract":"We propose a channel estimation scheme based on joint sparsity pattern learning (JSPL) for massive multi-input multi-output (MIMO) orthogonal time-frequency-space (OTFS) modulation aided systems. By exploiting the potential joint sparsity of the delay-Doppler-angle (DDA) domain channel, the channel estimation problem is transformed into a sparse recovery problem. To solve it, we first apply the spike and slab prior model to iteratively estimate the support set of the channel matrix, and a higher-accuracy parameter update rule relying on the identified support set is introduced into the iteration. Then the specific values of the channel elements corresponding to the support set are estimated by the orthogonal matching pursuit (OMP) method. Both our simulation results and analysis demonstrate that the proposed JSPL channel estimation scheme achieves an improved performance over the representative state-of-the-art baseline schemes, despite its reduced pilot overhead.","sentences":["We propose a channel estimation scheme based on joint sparsity pattern learning (JSPL) for massive multi-input multi-output (MIMO) orthogonal time-frequency-space (OTFS) modulation aided systems.","By exploiting the potential joint sparsity of the delay-Doppler-angle (DDA) domain channel, the channel estimation problem is transformed into a sparse recovery problem.","To solve it, we first apply the spike and slab prior model to iteratively estimate the support set of the channel matrix, and a higher-accuracy parameter update rule relying on the identified support set is introduced into the iteration.","Then the specific values of the channel elements corresponding to the support set are estimated by the orthogonal matching pursuit (OMP) method.","Both our simulation results and analysis demonstrate that the proposed JSPL channel estimation scheme achieves an improved performance over the representative state-of-the-art baseline schemes, despite its reduced pilot overhead."],"url":"http://arxiv.org/abs/2403.03771v1","category":"eess.SP"}
{"created":"2024-03-06 15:01:29","title":"How to find optimal quantum states for optical micromanipulation and metrology in complex scattering problems: tutorial","abstract":"The interaction of quantum light with matter is of great importance to a wide range of scientific disciplines, ranging from optomechanics to high precision measurements. A central issue we discuss here, is how to make optimal use of both the spatial and the quantum degrees of freedom of light for characterizing and manipulating arbitrary observable parameters in a linear scattering system into which suitably engineered light fields are injected. Here, we discuss a comprehensive framework based on a quantum operator that can be assembled solely from the scattering matrix of a system and its dependence on the corresponding local parameter, making this operator experimentally measurable from the far-field using only classical light. From this, the effect of quantum light in the near-field, i.e., in the vicinity of the target object, can be inferred. Based on this framework, it is straightforward to formulate optimal protocols on how to jointly design both the spatial shape and the quantum characteristics of light for micromanipulation as well as for parameter estimation in arbitrarily complex media. Also the forces of the quantum vacuum naturally emerge from this formalism. The aim of our tutorial is to bring different perspectives into alignment and thereby build a bridge between the different communities of wave control, quantum optics, micromanipulation, quantum metrology and vacuum physics.","sentences":["The interaction of quantum light with matter is of great importance to a wide range of scientific disciplines, ranging from optomechanics to high precision measurements.","A central issue we discuss here, is how to make optimal use of both the spatial and the quantum degrees of freedom of light for characterizing and manipulating arbitrary observable parameters in a linear scattering system into which suitably engineered light fields are injected.","Here, we discuss a comprehensive framework based on a quantum operator that can be assembled solely from the scattering matrix of a system and its dependence on the corresponding local parameter, making this operator experimentally measurable from the far-field using only classical light.","From this, the effect of quantum light in the near-field, i.e., in the vicinity of the target object, can be inferred.","Based on this framework, it is straightforward to formulate optimal protocols on how to jointly design both the spatial shape and the quantum characteristics of light for micromanipulation as well as for parameter estimation in arbitrarily complex media.","Also the forces of the quantum vacuum naturally emerge from this formalism.","The aim of our tutorial is to bring different perspectives into alignment and thereby build a bridge between the different communities of wave control, quantum optics, micromanipulation, quantum metrology and vacuum physics."],"url":"http://arxiv.org/abs/2403.03766v1","category":"quant-ph"}
{"created":"2024-03-06 14:58:26","title":"The role of interfacial interactions and oxygen vacancies in tuning magnetic anisotropy in LaCrO$_{3}$/LaMnO$_{3}$ heterostructures","abstract":"The interplay of lattice, electronic, and spin degrees of freedom at epitaxial complex oxide interfaces provides a route to tune their magnetic ground states. Unraveling the competing contributions is critical for tuning their functional properties. We investigate the relationship between magnetic ordering and magnetic anisotropy and the lattice symmetry, oxygen content, and film thickness in compressively strained LaMnO$_3$/LaCrO$_3$ superlattices. Mn-O-Cr antiferromagnetic superexchange interactions across the heterointerface resulting in a net ferrimagnetic magnetic structure. Bulk magnetometry measurements reveal isotropic in-plane magnetism for as-grown oxygen-deficient thinner thin samples due to equal fractions of orthorhombic a+a-c-, and a-a+c- twin domains. As the superlattice thickness is increased, in-plane magnetic anisotropy emerges as the fraction of the a+a-c- domain increases. On annealing in oxygen, the suppression of oxygen vacancies results in a contraction of the lattice volume, and an orthorhombic to rhombohedral transition leads to isotropic magnetism independent of the film thickness. The complex interactions are investigated using high-resolution synchrotron diffraction and X-ray absorption spectroscopy. These results highlight the role of the evolution of structural domains with film thickness, interfacial spin interactions, and oxygen-vacancy-induced structural phase transitions in tuning the magnetic properties of complex oxide heterostructures.","sentences":["The interplay of lattice, electronic, and spin degrees of freedom at epitaxial complex oxide interfaces provides a route to tune their magnetic ground states.","Unraveling the competing contributions is critical for tuning their functional properties.","We investigate the relationship between magnetic ordering and magnetic anisotropy and the lattice symmetry, oxygen content, and film thickness in compressively strained LaMnO$_3$/LaCrO$_3$ superlattices.","Mn-O-Cr antiferromagnetic superexchange interactions across the heterointerface resulting in a net ferrimagnetic magnetic structure.","Bulk magnetometry measurements reveal isotropic in-plane magnetism for as-grown oxygen-deficient thinner thin samples due to equal fractions of orthorhombic a+a-c-, and a-a+c- twin domains.","As the superlattice thickness is increased, in-plane magnetic anisotropy emerges as the fraction of the a+a-c- domain increases.","On annealing in oxygen, the suppression of oxygen vacancies results in a contraction of the lattice volume, and an orthorhombic to rhombohedral transition leads to isotropic magnetism independent of the film thickness.","The complex interactions are investigated using high-resolution synchrotron diffraction and X-ray absorption spectroscopy.","These results highlight the role of the evolution of structural domains with film thickness, interfacial spin interactions, and oxygen-vacancy-induced structural phase transitions in tuning the magnetic properties of complex oxide heterostructures."],"url":"http://arxiv.org/abs/2403.03764v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-06 14:57:10","title":"Flipped non-associative polynomial rings and the Cayley-Dickson construction","abstract":"We introduce and study flipped non-associative polynomial rings which all Cayley-Dickson algebras naturally appear as quotients of. In particular, this extends the classical construction of the complex numbers (and quaternions) as a quotient of a (skew) polynomial ring to the octonions, and beyond. We also extend some classical results on algebraic properties of Cayley-Dickson algebras by McCrimmon to a class of flipped non-associative polynomial rings.","sentences":["We introduce and study flipped non-associative polynomial rings which all Cayley-Dickson algebras naturally appear as quotients of.","In particular, this extends the classical construction of the complex numbers (and quaternions) as a quotient of a (skew) polynomial ring to the octonions, and beyond.","We also extend some classical results on algebraic properties of Cayley-Dickson algebras by McCrimmon to a class of flipped non-associative polynomial rings."],"url":"http://arxiv.org/abs/2403.03763v1","category":"math.RA"}
{"created":"2024-03-06 14:52:27","title":"Homoclinic classes of geodesic flows on rank 1 manifolds","abstract":"We prove that the homoclinic class of every hyperbolic periodic orbit of a geodesic flow over a $C^\\infty$ closed rank 1 Riemannian manifold equals the unit tangent bundle. As an application, we give a proof using symbolic dynamics of the theorem of Knieper on the uniqueness of the measure of maximal entropy and theorems of Burns et al on the uniqueness of equilibrium states.","sentences":["We prove that the homoclinic class of every hyperbolic periodic orbit of a geodesic flow over a $C^\\infty$ closed rank 1 Riemannian manifold equals the unit tangent bundle.","As an application, we give a proof using symbolic dynamics of the theorem of Knieper on the uniqueness of the measure of maximal entropy and theorems of Burns et al on the uniqueness of equilibrium states."],"url":"http://arxiv.org/abs/2403.03759v1","category":"math.DS"}
{"created":"2024-03-06 14:42:40","title":"Maximizing Energy Charging for UAV-assisted MEC Systems with SWIPT","abstract":"A Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) scheme with simultaneous wireless information and power transfer (SWIPT) is proposed in this paper. Unlike existing MEC-WPT schemes that disregard the downlink period for returning computing results to the ground equipment (GEs), our proposed scheme actively considers and capitalizes on this period. By leveraging the SWIPT technique, the UAV can simultaneously transmit energy and the computing results during the downlink period. In this scheme, our objective is to maximize the remaining energy among all GEs by jointly optimizing computing task scheduling, UAV transmit and receive beamforming, BS receive beamforming, GEs' transmit power and power splitting ratio for information decoding, time scheduling, and UAV trajectory. We propose an alternating optimization algorithm that utilizes the semidefinite relaxation (SDR), singular value decomposition (SVD), and fractional programming (FP) methods to effectively solve the nonconvex problem. Numerous experiments validate the effectiveness of the proposed scheme.","sentences":["A Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) scheme with simultaneous wireless information and power transfer (SWIPT) is proposed in this paper.","Unlike existing MEC-WPT schemes that disregard the downlink period for returning computing results to the ground equipment (GEs), our proposed scheme actively considers and capitalizes on this period.","By leveraging the SWIPT technique, the UAV can simultaneously transmit energy and the computing results during the downlink period.","In this scheme, our objective is to maximize the remaining energy among all GEs by jointly optimizing computing task scheduling, UAV transmit and receive beamforming, BS receive beamforming, GEs' transmit power and power splitting ratio for information decoding, time scheduling, and UAV trajectory.","We propose an alternating optimization algorithm that utilizes the semidefinite relaxation (SDR), singular value decomposition (SVD), and fractional programming (FP) methods to effectively solve the nonconvex problem.","Numerous experiments validate the effectiveness of the proposed scheme."],"url":"http://arxiv.org/abs/2403.03756v1","category":"eess.SP"}
{"created":"2024-03-06 14:42:22","title":"Relativization in naturally functorial","abstract":"In this note, we provide some categorical perspectives on the relativization construction arising from quantum measurement theory in the presence of symmetries and occupying a central place in the operational approach to quantum reference frames. This construction provides, for any quantum system, a quantum channel from the system's algebra to the invariant algebra on the composite system also encompassing the chosen reference, contingent upon a choice of the pointer observable. These maps are understood as relativizing observables on systems upon the specification of a quantum reference frame. We begin by extending the construction to systems modelled on subspaces of algebras of operators to then define a functor taking a pair consisting of a reference frame and a system and assigning to them a subspace of relative operators defined in terms of an image of the corresponding relativization map. When a single frame and equivariant channels are considered, the relativization maps can be understood as a natural transformation. Upon fixing a system, the functor provides a novel kind of frame transformation that we call external. Results achieved provide a deeper structural understanding of the framework of interest and point towards its categorification and potential application to local systems of algebraic quantum field theories.","sentences":["In this note, we provide some categorical perspectives on the relativization construction arising from quantum measurement theory in the presence of symmetries and occupying a central place in the operational approach to quantum reference frames.","This construction provides, for any quantum system, a quantum channel from the system's algebra to the invariant algebra on the composite system also encompassing the chosen reference, contingent upon a choice of the pointer observable.","These maps are understood as relativizing observables on systems upon the specification of a quantum reference frame.","We begin by extending the construction to systems modelled on subspaces of algebras of operators to then define a functor taking a pair consisting of a reference frame and a system and assigning to them a subspace of relative operators defined in terms of an image of the corresponding relativization map.","When a single frame and equivariant channels are considered, the relativization maps can be understood as a natural transformation.","Upon fixing a system, the functor provides a novel kind of frame transformation that we call external.","Results achieved provide a deeper structural understanding of the framework of interest and point towards its categorification and potential application to local systems of algebraic quantum field theories."],"url":"http://arxiv.org/abs/2403.03755v1","category":"quant-ph"}
{"created":"2024-03-06 14:36:00","title":"Emotional Tandem Robots: How Different Robot Behaviors Affect Human Perception While Controlling a Mobile Robot","abstract":"In human-robot interaction (HRI), we study how humans interact with robots, but also the effects of robot behavior on human perception and well-being. Especially, the influence on humans by tandem robots with one human controlled and one autonomous robot or even semi-autonomous multi-robot systems is not yet fully understood. Here, we focus on a leader-follower scenario and study how emotionally expressive motion patterns of a small, mobile follower robot affect the perception of a human operator controlling the leading robot. We examined three distinct emotional behaviors for the follower compared to a neutral condition: angry, happy and sad. We analyzed how participants maneuvered the leader robot along a set path while experiencing each follower behavior in a randomized order. We identified a significant shift in attention toward the follower with emotionally expressive behaviors compared to the neutral condition. For example, the angry behavior significantly heightened participant stress levels and was considered the least preferred behavior. The happy behavior was the most preferred and associated with increased excitement by the participants. Integrating the proposed behaviors in robots can profoundly influence the human operator's attention, emotional state, and overall experience. These insights are valuable for future HRI tandem robot designs.","sentences":["In human-robot interaction (HRI), we study how humans interact with robots, but also the effects of robot behavior on human perception and well-being.","Especially, the influence on humans by tandem robots with one human controlled and one autonomous robot or even semi-autonomous multi-robot systems is not yet fully understood.","Here, we focus on a leader-follower scenario and study how emotionally expressive motion patterns of a small, mobile follower robot affect the perception of a human operator controlling the leading robot.","We examined three distinct emotional behaviors for the follower compared to a neutral condition: angry, happy and sad.","We analyzed how participants maneuvered the leader robot along a set path while experiencing each follower behavior in a randomized order.","We identified a significant shift in attention toward the follower with emotionally expressive behaviors compared to the neutral condition.","For example, the angry behavior significantly heightened participant stress levels and was considered the least preferred behavior.","The happy behavior was the most preferred and associated with increased excitement by the participants.","Integrating the proposed behaviors in robots can profoundly influence the human operator's attention, emotional state, and overall experience.","These insights are valuable for future HRI tandem robot designs."],"url":"http://arxiv.org/abs/2403.03746v1","category":"cs.RO"}
{"created":"2024-03-06 14:24:36","title":"Energy and Time Histograms: Fluxes determination in calorimeters","abstract":"In high-granularity calorimetry, as proposed for detectors at future Higgs factories, the requirements on electronics can have a strong impact on the design of the detector, especially via the cooling and acquisition systems. This project aims to establish the typical fluxes in the calorimeters: deposited energy, number of cells above the electronics threshold, additional heat, and associated data, etc. Here, a software package is presented which outputs histograms for different selections of the calorimeter components from fully simulated physics and background events. The ILD calorimeter system is taken as a specific example, upon which different histograms are obtained for representative parts of the calorimeter and for various machine configurations. Examples of histograms are shown, along with all details of the data used and the simulation.","sentences":["In high-granularity calorimetry, as proposed for detectors at future Higgs factories, the requirements on electronics can have a strong impact on the design of the detector, especially via the cooling and acquisition systems.","This project aims to establish the typical fluxes in the calorimeters: deposited energy, number of cells above the electronics threshold, additional heat, and associated data, etc.","Here, a software package is presented which outputs histograms for different selections of the calorimeter components from fully simulated physics and background events.","The ILD calorimeter system is taken as a specific example, upon which different histograms are obtained for representative parts of the calorimeter and for various machine configurations.","Examples of histograms are shown, along with all details of the data used and the simulation."],"url":"http://arxiv.org/abs/2403.03733v1","category":"hep-ex"}
{"created":"2024-03-06 14:16:46","title":"Robust MITL planning under uncertain navigation times","abstract":"In environments like offices, the duration of a robot's navigation between two locations may vary over time. For instance, reaching a kitchen may take more time during lunchtime since the corridors are crowded with people heading the same way. In this work, we address the problem of routing in such environments with tasks expressed in Metric Interval Temporal Logic (MITL) - a rich robot task specification language that allows us to capture explicit time requirements. Our objective is to find a strategy that maximizes the temporal robustness of the robot's MITL task. As the first step towards a solution, we define a Mixed-integer linear programming approach to solving the task planning problem over a Varying Weighted Transition System, where navigation durations are deterministic but vary depending on the time of day. Then, we apply this planner to optimize for MITL temporal robustness in Markov Decision Processes, where the navigation durations between physical locations are uncertain, but the time-dependent distribution over possible delays is known. Finally, we develop a receding horizon planner for Markov Decision Processes that preserves guarantees over MITL temporal robustness. We show the scalability of our planning algorithms in simulations of robotic tasks.","sentences":["In environments like offices, the duration of a robot's navigation between two locations may vary over time.","For instance, reaching a kitchen may take more time during lunchtime since the corridors are crowded with people heading the same way.","In this work, we address the problem of routing in such environments with tasks expressed in Metric Interval Temporal Logic (MITL) - a rich robot task specification language that allows us to capture explicit time requirements.","Our objective is to find a strategy that maximizes the temporal robustness of the robot's MITL task.","As the first step towards a solution, we define a Mixed-integer linear programming approach to solving the task planning problem over a Varying Weighted Transition System, where navigation durations are deterministic but vary depending on the time of day.","Then, we apply this planner to optimize for MITL temporal robustness in Markov Decision Processes, where the navigation durations between physical locations are uncertain, but the time-dependent distribution over possible delays is known.","Finally, we develop a receding horizon planner for Markov Decision Processes that preserves guarantees over MITL temporal robustness.","We show the scalability of our planning algorithms in simulations of robotic tasks."],"url":"http://arxiv.org/abs/2403.03727v1","category":"cs.RO"}
{"created":"2024-03-06 13:43:12","title":"Long period modulation of the classical T Tauri star CI Tau: evidence for an eccentric close-in massive planet at 0.17 au","abstract":"Detecting planets within protoplanetary disks around young stars is essential for understanding planet formation and evolution. However, planet detection using the radial velocity method faces challenges due to strong stellar activity in these early stages. We aim to detect long-term periodicities in photometric and spectroscopic time series of the classical T Tauri star (CTTS) CI Tau, and retrieve evidence for inner embedded planets in its disk. The study conducted photometric and spectroscopic analyses using K2 and Las Cumbres Observatory Global Network light curves, and high-resolution spectra from ESPaDOnS and SPIRou. We focus our radial velocity analysis on a wavelength domain less affected by spot activity. To account for spot effects, a quasi-periodic Gaussian process model was applied to K2 light curve, ESPaDOnS, and SPIRou radial velocity data. Additionally, a detailed bisector analysis on cross-correlation functions was carried out to understand the cause of long-term periodicity. We detect coherent periods at $\\sim$ 6.6 d, 9 d, $\\sim$ 11.5 d, $\\sim$ 14.2 d and $\\sim$ 25.2 d, the latter is seen consistently across all datasets. Bisector analysis of the cross-correlation functions provides strong hints for combined activity-induced and Doppler reflex signal in the radial velocities at a period of 25.2 d. Our analysis suggests that this periodicity is best explained by the presence of a 3.6$\\pm$0.3 M$_{Jup}$, eccentric (e$\\sim$0.58) planet at a semi-major axis of 0.17 au. Our study outlines the difficulty of searching for disk-embedded planets in the inner 0.1 au's of young and active systems. We demonstrate that, when searching for planets in actively accreting stars such as CI Tau, the primary limitation is stellar activity rather than the precision of RV measurements provided by the instrument.","sentences":["Detecting planets within protoplanetary disks around young stars is essential for understanding planet formation and evolution.","However, planet detection using the radial velocity method faces challenges due to strong stellar activity in these early stages.","We aim to detect long-term periodicities in photometric and spectroscopic time series of the classical T Tauri star (CTTS) CI Tau, and retrieve evidence for inner embedded planets in its disk.","The study conducted photometric and spectroscopic analyses using K2 and Las Cumbres Observatory Global Network light curves, and high-resolution spectra from ESPaDOnS and SPIRou.","We focus our radial velocity analysis on a wavelength domain less affected by spot activity.","To account for spot effects, a quasi-periodic Gaussian process model was applied to K2 light curve, ESPaDOnS, and SPIRou radial velocity data.","Additionally, a detailed bisector analysis on cross-correlation functions was carried out to understand the cause of long-term periodicity.","We detect coherent periods at $\\sim$ 6.6 d, 9 d, $\\sim$ 11.5 d, $\\sim$ 14.2 d and $\\sim$ 25.2 d, the latter is seen consistently across all datasets.","Bisector analysis of the cross-correlation functions provides strong hints for combined activity-induced and Doppler reflex signal in the radial velocities at a period of 25.2 d. Our analysis suggests that this periodicity is best explained by the presence of a 3.6$\\pm$0.3 M$_{Jup}$, eccentric (e$\\sim$0.58) planet at a semi-major axis of 0.17 au.","Our study outlines the difficulty of searching for disk-embedded planets in the inner 0.1 au's of young and active systems.","We demonstrate that, when searching for planets in actively accreting stars such as CI Tau, the primary limitation is stellar activity rather than the precision of RV measurements provided by the instrument."],"url":"http://arxiv.org/abs/2403.03706v1","category":"astro-ph.SR"}
{"created":"2024-03-06 13:36:31","title":"Online model error correction with neural networks: application to the Integrated Forecasting System","abstract":"In recent years, there has been significant progress in the development of fully data-driven global numerical weather prediction models. These machine learning weather prediction models have their strength, notably accuracy and low computational requirements, but also their weakness: they struggle to represent fundamental dynamical balances, and they are far from being suitable for data assimilation experiments. Hybrid modelling emerges as a promising approach to address these limitations. Hybrid models integrate a physics-based core component with a statistical component, typically a neural network, to enhance prediction capabilities. In this article, we propose to develop a model error correction for the operational Integrated Forecasting System (IFS) of the European Centre for Medium-Range Weather Forecasts using a neural network. The neural network is initially pre-trained offline using a large dataset of operational analyses and analysis increments. Subsequently, the trained network is integrated into the IFS within the Object-Oriented Prediction System (OOPS) so as to be used in data assimilation and forecast experiments. It is then further trained online using a recently developed variant of weak-constraint 4D-Var. The results show that the pre-trained neural network already provides a reliable model error correction, which translates into reduced forecast errors in many conditions and that the online training further improves the accuracy of the hybrid model in many conditions.","sentences":["In recent years, there has been significant progress in the development of fully data-driven global numerical weather prediction models.","These machine learning weather prediction models have their strength, notably accuracy and low computational requirements, but also their weakness: they struggle to represent fundamental dynamical balances, and they are far from being suitable for data assimilation experiments.","Hybrid modelling emerges as a promising approach to address these limitations.","Hybrid models integrate a physics-based core component with a statistical component, typically a neural network, to enhance prediction capabilities.","In this article, we propose to develop a model error correction for the operational Integrated Forecasting System (IFS) of the European Centre for Medium-Range Weather Forecasts using a neural network.","The neural network is initially pre-trained offline using a large dataset of operational analyses and analysis increments.","Subsequently, the trained network is integrated into the IFS within the Object-Oriented Prediction System (OOPS) so as to be used in data assimilation and forecast experiments.","It is then further trained online using a recently developed variant of weak-constraint 4D-Var.","The results show that the pre-trained neural network already provides a reliable model error correction, which translates into reduced forecast errors in many conditions and that the online training further improves the accuracy of the hybrid model in many conditions."],"url":"http://arxiv.org/abs/2403.03702v1","category":"stat.ML"}
{"created":"2024-03-06 13:23:55","title":"Spectral Phase Transition and Optimal PCA in Block-Structured Spiked models","abstract":"We discuss the inhomogeneous spiked Wigner model, a theoretical framework recently introduced to study structured noise in various learning scenarios, through the prism of random matrix theory, with a specific focus on its spectral properties. Our primary objective is to find an optimal spectral method and to extend the celebrated \\cite{BBP} (BBP) phase transition criterion -- well-known in the homogeneous case -- to our inhomogeneous, block-structured, Wigner model. We provide a thorough rigorous analysis of a transformed matrix and show that the transition for the appearance of 1) an outlier outside the bulk of the limiting spectral distribution and 2) a positive overlap between the associated eigenvector and the signal, occurs precisely at the optimal threshold, making the proposed spectral method optimal within the class of iterative methods for the inhomogeneous Wigner problem.","sentences":["We discuss the inhomogeneous spiked Wigner model, a theoretical framework recently introduced to study structured noise in various learning scenarios, through the prism of random matrix theory, with a specific focus on its spectral properties.","Our primary objective is to find an optimal spectral method and to extend the celebrated \\cite{BBP} (BBP) phase transition criterion -- well-known in the homogeneous case -- to our inhomogeneous, block-structured, Wigner model.","We provide a thorough rigorous analysis of a transformed matrix and show that the transition for the appearance of 1) an outlier outside the bulk of the limiting spectral distribution and 2) a positive overlap between the associated eigenvector and the signal, occurs precisely at the optimal threshold, making the proposed spectral method optimal within the class of iterative methods for the inhomogeneous Wigner problem."],"url":"http://arxiv.org/abs/2403.03695v1","category":"stat.ML"}
{"created":"2024-03-06 13:08:51","title":"Astronomical Lunisolar Cycles and Late Antique Chronology","abstract":"This article advances the hypothesis that the heightened eschatological sensitivity evident among the historians writing in the 5th century and its weaker echos in the time of Charlemagne were caused by the irregularities of the the lunisolar calendar and its particular realization, the Easter calendar. The lunisolar calendar that Christians used for the calculation of the date of the Easter had a number of key periods when the cycles of the Sun and the Moon came in sync in relationship to the beginning of the count and thus produced an effect of the times repeating themselves or ending with the nearly precise astronomical repetition. It is shown that Late Antique scholars who were actively involved in the construction of the Christian history's chronology were limited in their choices by the astronomical peculiarities of the Earth-Moon system. The total conjunctions of the astronomical Solar and Lunar calendars took place, some within the 1st century CE, and the next one, in 483 CE. This was also a special year because the lunar calendar lost one day. Thus the 5th century was the time of heightened expectations of whether the calendar and the Moon's showings will repeat those that accompanied the birth of Jesus. The Full Supermoon (or whatever phase it was on December 25th, 1 BCE) may have repeated in 410 CE (the entry of Goths into Rome), in 467 CE and in 476 CE (the Fall of the Roman Empire), marking the coming of the time very similar to Jesus' birth. The Full Moon was supposed to repeat December 25th, 800 CE and in the year 1000 CE. This may have determined the setting of the biblical calendar in a way that put the birth of Christ on 5199 CE (making the year 800 CE, the year of the Full Supermoon or of its phase on December 25th, 1 BCE) a critical turning point.","sentences":["This article advances the hypothesis that the heightened eschatological sensitivity evident among the historians writing in the 5th century and its weaker echos in the time of Charlemagne were caused by the irregularities of the the lunisolar calendar and its particular realization, the Easter calendar.","The lunisolar calendar that Christians used for the calculation of the date of the Easter had a number of key periods when the cycles of the Sun and the Moon came in sync in relationship to the beginning of the count and thus produced an effect of the times repeating themselves or ending with the nearly precise astronomical repetition.","It is shown that Late Antique scholars who were actively involved in the construction of the Christian history's chronology were limited in their choices by the astronomical peculiarities of the Earth-Moon system.","The total conjunctions of the astronomical Solar and Lunar calendars took place, some within the 1st century CE, and the next one, in 483 CE.","This was also a special year because the lunar calendar lost one day.","Thus the 5th century was the time of heightened expectations of whether the calendar and the Moon's showings will repeat those that accompanied the birth of Jesus.","The Full Supermoon (or whatever phase it was on December 25th, 1 BCE) may have repeated in 410 CE (the entry of Goths into Rome), in 467 CE and in 476 CE (the Fall of the Roman Empire), marking the coming of the time very similar to Jesus' birth.","The Full Moon was supposed to repeat December 25th, 800 CE and in the year 1000 CE.","This may have determined the setting of the biblical calendar in a way that put the birth of Christ on 5199 CE (making the year 800 CE, the year of the Full Supermoon or of its phase on December 25th, 1 BCE) a critical turning point."],"url":"http://arxiv.org/abs/2403.03682v1","category":"physics.hist-ph"}
{"created":"2024-03-06 13:01:55","title":"Application of Deep Learning Reduced-Order Modeling for Single-Phase Flow in Faulted Porous Media","abstract":"We apply reduced-order modeling (ROM) techniques to single-phase flow in faulted porous media, accounting for changing rock properties and fault geometry variations using a radial basis function mesh deformation method. This approach benefits from a mixed-dimensional framework that effectively manages the resulting non-conforming mesh. To streamline complex and repetitive calculations such as sensitivity analysis and solution of inverse problems, we utilize the Deep Learning Reduced Order Model (DL-ROM). This non-intrusive neural network-based technique is evaluated against the traditional Proper Orthogonal Decomposition (POD) method across various scenarios, demonstrating DL-ROM's capacity to expedite complex analyses with promising accuracy and efficiency.","sentences":["We apply reduced-order modeling (ROM) techniques to single-phase flow in faulted porous media, accounting for changing rock properties and fault geometry variations using a radial basis function mesh deformation method.","This approach benefits from a mixed-dimensional framework that effectively manages the resulting non-conforming mesh.","To streamline complex and repetitive calculations such as sensitivity analysis and solution of inverse problems, we utilize the Deep Learning Reduced Order Model (DL-ROM).","This non-intrusive neural network-based technique is evaluated against the traditional Proper Orthogonal Decomposition (POD) method across various scenarios, demonstrating DL-ROM's capacity to expedite complex analyses with promising accuracy and efficiency."],"url":"http://arxiv.org/abs/2403.03678v1","category":"math.NA"}
{"created":"2024-03-06 12:57:37","title":"ZF Beamforming Tensor Compression for Massive MIMO Fronthaul","abstract":"In the rapidly evolving landscape of 5G and beyond 5G (B5G) mobile cellular communications, efficient data compression and reconstruction strategies become paramount, especially in massive multiple-input multiple-output (MIMO) systems. A critical challenge in these systems is the capacity-limited fronthaul, particularly in the context of the Ethernet-based common public radio interface (eCPRI) connecting baseband units (BBUs) and remote radio units (RRUs). This capacity limitation hinders the effective handling of increased traffic and data flows. We propose a novel two-stage compression approach to address this bottleneck. The first stage employs sparse Tucker decomposition, targeting the weight tensor's low-rank components for compression. The second stage further compresses these components using complex givens decomposition and run-length encoding, substantially improving the compression ratio. Our approach specifically targets the Zero-Forcing (ZF) beamforming weights in BBUs. By reconstructing these weights in RRUs, we significantly alleviate the burden on eCPRI traffic, enabling a higher number of concurrent streams in the radio access network (RAN). Through comprehensive evaluations, we demonstrate the superior effectiveness of our method in Channel State Information (CSI) compression, paving the way for more efficient 5G/B5G fronthaul links.","sentences":["In the rapidly evolving landscape of 5G and beyond 5G (B5G) mobile cellular communications, efficient data compression and reconstruction strategies become paramount, especially in massive multiple-input multiple-output (MIMO) systems.","A critical challenge in these systems is the capacity-limited fronthaul, particularly in the context of the Ethernet-based common public radio interface (eCPRI) connecting baseband units (BBUs) and remote radio units (RRUs).","This capacity limitation hinders the effective handling of increased traffic and data flows.","We propose a novel two-stage compression approach to address this bottleneck.","The first stage employs sparse Tucker decomposition, targeting the weight tensor's low-rank components for compression.","The second stage further compresses these components using complex givens decomposition and run-length encoding, substantially improving the compression ratio.","Our approach specifically targets the Zero-Forcing (ZF) beamforming weights in BBUs.","By reconstructing these weights in RRUs, we significantly alleviate the burden on eCPRI traffic, enabling a higher number of concurrent streams in the radio access network (RAN).","Through comprehensive evaluations, we demonstrate the superior effectiveness of our method in Channel State Information (CSI) compression, paving the way for more efficient 5G/B5G fronthaul links."],"url":"http://arxiv.org/abs/2403.03675v1","category":"cs.IT"}
{"created":"2024-03-06 12:55:21","title":"Adversarial Infrared Geometry: Using Geometry to Perform Adversarial Attack against Infrared Pedestrian Detectors","abstract":"Currently, infrared imaging technology enjoys widespread usage, with infrared object detection technology experiencing a surge in prominence. While previous studies have delved into physical attacks on infrared object detectors, the implementation of these techniques remains complex. For instance, some approaches entail the use of bulb boards or infrared QR suits as perturbations to execute attacks, which entail costly optimization and cumbersome deployment processes. Other methodologies involve the utilization of irregular aerogel as physical perturbations for infrared attacks, albeit at the expense of optimization expenses and perceptibility issues. In this study, we propose a novel infrared physical attack termed Adversarial Infrared Geometry (\\textbf{AdvIG}), which facilitates efficient black-box query attacks by modeling diverse geometric shapes (lines, triangles, ellipses) and optimizing their physical parameters using Particle Swarm Optimization (PSO). Extensive experiments are conducted to evaluate the effectiveness, stealthiness, and robustness of AdvIG. In digital attack experiments, line, triangle, and ellipse patterns achieve attack success rates of 93.1\\%, 86.8\\%, and 100.0\\%, respectively, with average query times of 71.7, 113.1, and 2.57, respectively, thereby confirming the efficiency of AdvIG. Physical attack experiments are conducted to assess the attack success rate of AdvIG at different distances. On average, the line, triangle, and ellipse achieve attack success rates of 61.1\\%, 61.2\\%, and 96.2\\%, respectively. Further experiments are conducted to comprehensively analyze AdvIG, including ablation experiments, transfer attack experiments, and adversarial defense mechanisms. Given the superior performance of our method as a simple and efficient black-box adversarial attack in both digital and physical environments, we advocate for widespread attention to AdvIG.","sentences":["Currently, infrared imaging technology enjoys widespread usage, with infrared object detection technology experiencing a surge in prominence.","While previous studies have delved into physical attacks on infrared object detectors, the implementation of these techniques remains complex.","For instance, some approaches entail the use of bulb boards or infrared QR suits as perturbations to execute attacks, which entail costly optimization and cumbersome deployment processes.","Other methodologies involve the utilization of irregular aerogel as physical perturbations for infrared attacks, albeit at the expense of optimization expenses and perceptibility issues.","In this study, we propose a novel infrared physical attack termed Adversarial Infrared Geometry (\\textbf{AdvIG}), which facilitates efficient black-box query attacks by modeling diverse geometric shapes (lines, triangles, ellipses) and optimizing their physical parameters using Particle Swarm Optimization (PSO).","Extensive experiments are conducted to evaluate the effectiveness, stealthiness, and robustness of AdvIG.","In digital attack experiments, line, triangle, and ellipse patterns achieve attack success rates of 93.1\\%, 86.8\\%, and 100.0\\%, respectively, with average query times of 71.7, 113.1, and 2.57, respectively, thereby confirming the efficiency of AdvIG.","Physical attack experiments are conducted to assess the attack success rate of AdvIG at different distances.","On average, the line, triangle, and ellipse achieve attack success rates of 61.1\\%, 61.2\\%, and 96.2\\%, respectively.","Further experiments are conducted to comprehensively analyze AdvIG, including ablation experiments, transfer attack experiments, and adversarial defense mechanisms.","Given the superior performance of our method as a simple and efficient black-box adversarial attack in both digital and physical environments, we advocate for widespread attention to AdvIG."],"url":"http://arxiv.org/abs/2403.03674v1","category":"cs.CV"}
{"created":"2024-03-06 12:39:44","title":"On the Structure of Hamiltonian Graphs with Small Independence Number","abstract":"A Hamiltonian path (cycle) in a graph is a path (cycle, respectively) which passes through all of its vertices. The problems of deciding the existence of a Hamiltonian cycle (path) in an input graph are well known to be NP-complete, and restricted classes of graphs which allow for their polynomial-time solutions are intensively investigated. Until very recently the complexity was open even for graphs of independence number at most 3. So far unpublished result of Jedli\\v{c}kov\\'{a} and Kratochv\\'{\\i}l [arXiv:2309.09228] shows that for every integer $k$, Hamiltonian path and cycle are polynomial-time solvable in graphs of independence number bounded by $k$. As a companion structural result, we determine explicit obstacles for the existence of a Hamiltonian path for small values of $k$, namely for graphs of independence number 2, 3, and 4. Identifying these obstacles in an input graph yields alternative polynomial-time algorithms for Hamiltonian path and cycle with no large hidden multiplicative constants.","sentences":["A Hamiltonian path (cycle) in a graph is a path (cycle, respectively) which passes through all of its vertices.","The problems of deciding the existence of a Hamiltonian cycle (path) in an input graph are well known to be NP-complete, and restricted classes of graphs which allow for their polynomial-time solutions are intensively investigated.","Until very recently the complexity was open even for graphs of independence number at most 3.","So far unpublished result of Jedli\\v{c}kov\\'{a} and Kratochv\\'{\\i}l [arXiv:2309.09228] shows that for every integer $k$, Hamiltonian path and cycle are polynomial-time solvable in graphs of independence number bounded by $k$. As a companion structural result, we determine explicit obstacles for the existence of a Hamiltonian path for small values of $k$, namely for graphs of independence number 2, 3, and 4.","Identifying these obstacles in an input graph yields alternative polynomial-time algorithms for Hamiltonian path and cycle with no large hidden multiplicative constants."],"url":"http://arxiv.org/abs/2403.03668v1","category":"math.CO"}
{"created":"2024-03-06 12:37:02","title":"Robust radial basis function interpolation based on geodesic distance for the numerical coupling of multiphysics problems","abstract":"Multiphysics simulations frequently require transferring solution fields between subproblems with non-matching spatial discretizations, typically using interpolation techniques. Standard methods are usually based on measuring the closeness between points by means of the Euclidean distance, which does not account for curvature, cuts, cavities or other non-trivial geometrical or topological features of the domain. This may lead to spurious oscillations in the interpolant in proximity to these features. To overcome this issue, we propose a modification to rescaled localized radial basis function (RL-RBF) interpolation to account for the geometry of the interpolation domain, by yielding conformity and fidelity to geometrical and topological features. The proposed method, referred to as RL-RBF-G, relies on measuring the geodesic distance between data points. RL-RBF-G removes spurious oscillations appearing in the RL-RBF interpolant, resulting in increased accuracy in domains with complex geometries. We demonstrate the effectiveness of RL-RBF-G interpolation through a convergence study in an idealized setting. Furthermore, we discuss the algorithmic aspects and the implementation of RL-RBF-G interpolation in a distributed-memory parallel framework, and present the results of a strong scalability test yielding nearly ideal results. Finally, we show the effectiveness of RL-RBF-G interpolation in multiphysics simulations by considering an application to a whole-heart cardiac electromecanics model.","sentences":["Multiphysics simulations frequently require transferring solution fields between subproblems with non-matching spatial discretizations, typically using interpolation techniques.","Standard methods are usually based on measuring the closeness between points by means of the Euclidean distance, which does not account for curvature, cuts, cavities or other non-trivial geometrical or topological features of the domain.","This may lead to spurious oscillations in the interpolant in proximity to these features.","To overcome this issue, we propose a modification to rescaled localized radial basis function (RL-RBF) interpolation to account for the geometry of the interpolation domain, by yielding conformity and fidelity to geometrical and topological features.","The proposed method, referred to as RL-RBF-G, relies on measuring the geodesic distance between data points.","RL-RBF-G removes spurious oscillations appearing in the RL-RBF interpolant, resulting in increased accuracy in domains with complex geometries.","We demonstrate the effectiveness of RL-RBF-G interpolation through a convergence study in an idealized setting.","Furthermore, we discuss the algorithmic aspects and the implementation of RL-RBF-G interpolation in a distributed-memory parallel framework, and present the results of a strong scalability test yielding nearly ideal results.","Finally, we show the effectiveness of RL-RBF-G interpolation in multiphysics simulations by considering an application to a whole-heart cardiac electromecanics model."],"url":"http://arxiv.org/abs/2403.03665v1","category":"math.NA"}
{"created":"2024-03-06 12:33:15","title":"Robust Safety-Critical Control for Systems with Sporadic Measurements and Dwell Time Constraints","abstract":"This paper presents extensions of control barrier function (CBF) theory to systems with disturbances wherein a controller only receives measurements infrequently and operates open-loop between measurements, while still satisfying state constraints. The paper considers both impulsive and continuous actuators, and models the actuators, measurements, disturbances, and timing constraints as a hybrid dynamical system. We then design an open-loop observer that bounds the worst-case uncertainty between measurements. We develop definitions of CBFs for both actuation cases, and corresponding conditions on the control input to guarantee satisfaction of the state constraints. We apply these conditions to simulations of a satellite rendezvous in an elliptical orbit and autonomous orbit stationkeeping.","sentences":["This paper presents extensions of control barrier function (CBF) theory to systems with disturbances wherein a controller only receives measurements infrequently and operates open-loop between measurements, while still satisfying state constraints.","The paper considers both impulsive and continuous actuators, and models the actuators, measurements, disturbances, and timing constraints as a hybrid dynamical system.","We then design an open-loop observer that bounds the worst-case uncertainty between measurements.","We develop definitions of CBFs for both actuation cases, and corresponding conditions on the control input to guarantee satisfaction of the state constraints.","We apply these conditions to simulations of a satellite rendezvous in an elliptical orbit and autonomous orbit stationkeeping."],"url":"http://arxiv.org/abs/2403.03663v1","category":"eess.SY"}
{"created":"2024-03-06 12:27:44","title":"3D-Printed Dielectric Image Lines towards Chip-to-Chip Interconnects for subTHz-Applications","abstract":"This paper reports on 3D-printed dielectric image lines for low-loss subTHz applications between 140 and 220 GHz. In contrast to conventional dielectric waveguides, a conductive copper substrate is used to achieve robust routing and increased mechanical stability. For easy integration and characterization of the dielectric image line within a waveguide measurement setup, a low-loss mode-converter for flexible mounting is further designed. The characterized overall system exhibits a broadband match of at least 20 dB over the entire frequency band, with minimal losses of below 0.35 dB/cm. Furthermore, multi-line characterization is performed for de-embedding the propagation parameters {\\alpha} and \\b{eta} of both the dielectric transmission line and the mode-converter, and finally, the influence of discontinuities such as bending radii on the transmission behavior is evaluated. Due to the simplicity of the underlying 3D-printing technology, the proposed concept features extremely low cost and complexity, yet offers high flexibility and outperforms the losses of conventional transmission lines.","sentences":["This paper reports on 3D-printed dielectric image lines for low-loss subTHz applications between 140 and 220 GHz.","In contrast to conventional dielectric waveguides, a conductive copper substrate is used to achieve robust routing and increased mechanical stability.","For easy integration and characterization of the dielectric image line within a waveguide measurement setup, a low-loss mode-converter for flexible mounting is further designed.","The characterized overall system exhibits a broadband match of at least 20 dB over the entire frequency band, with minimal losses of below 0.35 dB/cm.","Furthermore, multi-line characterization is performed for de-embedding the propagation parameters {\\alpha} and \\b{eta} of both the dielectric transmission line and the mode-converter, and finally, the influence of discontinuities such as bending radii on the transmission behavior is evaluated.","Due to the simplicity of the underlying 3D-printing technology, the proposed concept features extremely low cost and complexity, yet offers high flexibility and outperforms the losses of conventional transmission lines."],"url":"http://arxiv.org/abs/2403.03657v1","category":"eess.SY"}
{"created":"2024-03-06 12:24:05","title":"Actuation manifold from snapshot data","abstract":"We propose a data-driven methodology to learn a low-dimensional actuation manifold of controlled flows. The starting point is resolving snapshot flow data for a representative ensemble of actuations. Key enablers for the actuation manifold are isometric mapping as encoder and k-nearest neighbour regression as a decoder. This methodology is tested for the fluidic pinball, a cluster of three parallel cylinders perpendicular to the oncoming uniform flow. The centers of these cylinders are the vertices of an equilateral triangle pointing upstream. The flow is manipulated by constant rotation of the cylinders, i.e. described by three actuation parameters. The Reynolds number based on a cylinder diameter is chosen to be 30. The unforced flow yields statistically symmetric unforced periodic shedding represented by a one-dimensional limit cycle. The proposed methodology yields a five-dimensional manifold describing a wide range of dynamics with small representation error. Interestingly, the manifold coordinates automatically unveil physically meaningful parameters. Two of them describe the downstream periodic vortex shedding. The other three ones describe the near-field actuation, i.e. the strength of boat-tailing, the Magnus effect and forward stagnation point. The manifold is shown to be a key enabler for control-oriented flow estimation.","sentences":["We propose a data-driven methodology to learn a low-dimensional actuation manifold of controlled flows.","The starting point is resolving snapshot flow data for a representative ensemble of actuations.","Key enablers for the actuation manifold are isometric mapping as encoder and k-nearest neighbour regression as a decoder.","This methodology is tested for the fluidic pinball, a cluster of three parallel cylinders perpendicular to the oncoming uniform flow.","The centers of these cylinders are the vertices of an equilateral triangle pointing upstream.","The flow is manipulated by constant rotation of the cylinders, i.e. described by three actuation parameters.","The Reynolds number based on a cylinder diameter is chosen to be 30.","The unforced flow yields statistically symmetric unforced periodic shedding represented by a one-dimensional limit cycle.","The proposed methodology yields a five-dimensional manifold describing a wide range of dynamics with small representation error.","Interestingly, the manifold coordinates automatically unveil physically meaningful parameters.","Two of them describe the downstream periodic vortex shedding.","The other three ones describe the near-field actuation, i.e. the strength of boat-tailing, the Magnus effect and forward stagnation point.","The manifold is shown to be a key enabler for control-oriented flow estimation."],"url":"http://arxiv.org/abs/2403.03653v1","category":"physics.flu-dyn"}
{"created":"2024-03-06 11:56:30","title":"Online Photon Guiding with 3D Gaussians for Caustics Rendering","abstract":"In production rendering systems, caustics are typically rendered via photon mapping and gathering, a process often hindered by insufficient photon density. In this paper, we propose a novel photon guiding method to improve the photon density and overall quality for caustic rendering. The key insight of our approach is the application of a global 3D Gaussian mixture model, used in conjunction with an adaptive light sampler. This combination effectively guides photon emission in expansive 3D scenes with multiple light sources. By employing a global 3D Gaussian mixture, our method precisely models the distribution of the points of interest. To sample emission directions from the distribution at any observation point, we introduce a novel directional transform of the 3D Gaussian, which ensures accurate photon emission guiding. Furthermore, our method integrates a global light cluster tree, which models the contribution distribution of light sources to the image, facilitating effective light source selection. We conduct experiments demonstrating that our approach robustly outperforms existing photon guiding techniques across a variety of scenarios, significantly advancing the quality of caustic rendering.","sentences":["In production rendering systems, caustics are typically rendered via photon mapping and gathering, a process often hindered by insufficient photon density.","In this paper, we propose a novel photon guiding method to improve the photon density and overall quality for caustic rendering.","The key insight of our approach is the application of a global 3D Gaussian mixture model, used in conjunction with an adaptive light sampler.","This combination effectively guides photon emission in expansive 3D scenes with multiple light sources.","By employing a global 3D Gaussian mixture, our method precisely models the distribution of the points of interest.","To sample emission directions from the distribution at any observation point, we introduce a novel directional transform of the 3D Gaussian, which ensures accurate photon emission guiding.","Furthermore, our method integrates a global light cluster tree, which models the contribution distribution of light sources to the image, facilitating effective light source selection.","We conduct experiments demonstrating that our approach robustly outperforms existing photon guiding techniques across a variety of scenarios, significantly advancing the quality of caustic rendering."],"url":"http://arxiv.org/abs/2403.03641v1","category":"cs.GR"}
{"created":"2024-03-06 11:47:05","title":"Processing Load Allocation of On-Board Multi-User Detection for Payload-Constrained Satellite Networks","abstract":"The rapid advance of mega-constellation facilitates the booming of direct-to-satellite massive access, where multi-user detection is critical to alleviate the induced inter-user interference. While centralized implementation of on-board detection induces unaffordable complexity for a single satellite, this paper proposes to allocate the processing load among cooperative satellites for finest exploitation of distributed processing power. Observing the inherent disparities among users, we first excavate the closed-form trade-offs between achievable sum-rate and the processing load corresponding to the satellite-user matchings, which leads to a system sum-rate maximization problem under stringent payload constraints. To address the non-trivial integer matching, we develop a quadratic transformation to the original problem, and prove it an equivalent conversion. The problem is further simplified into a series of subproblems employing successive lower bound approximation which obtains polynomial-time complexity and converges within a few iterations. Numerical results show remarkably complexity reduction compared with centralized processing, as well as around 20\\% sum-rate gain compared with other allocation methods.","sentences":["The rapid advance of mega-constellation facilitates the booming of direct-to-satellite massive access, where multi-user detection is critical to alleviate the induced inter-user interference.","While centralized implementation of on-board detection induces unaffordable complexity for a single satellite, this paper proposes to allocate the processing load among cooperative satellites for finest exploitation of distributed processing power.","Observing the inherent disparities among users, we first excavate the closed-form trade-offs between achievable sum-rate and the processing load corresponding to the satellite-user matchings, which leads to a system sum-rate maximization problem under stringent payload constraints.","To address the non-trivial integer matching, we develop a quadratic transformation to the original problem, and prove it an equivalent conversion.","The problem is further simplified into a series of subproblems employing successive lower bound approximation which obtains polynomial-time complexity and converges within a few iterations.","Numerical results show remarkably complexity reduction compared with centralized processing, as well as around 20\\% sum-rate gain compared with other allocation methods."],"url":"http://arxiv.org/abs/2403.03635v1","category":"cs.IT"}
{"created":"2024-03-06 11:42:35","title":"A hybrid dynamical system approach to the impulsive control of spacecraft rendezvous (extended version)","abstract":"This paper introduces a hybrid dynamical system methodology for managing impulsive control in spacecraft rendezvous and proximity operations under the Hill-Clohessy-Wiltshire model. We address the control design problem by isolating the out-of-plane from the in-plane dynamics and present a feedback control law for each of them. This law is based on a Lyapunov function tailored to each of the dynamics, capable of addressing thruster saturation and also a minimum impulse bit. These Lyapunov functions were found by reformulating the system's dynamics into coordinates that more intuitively represent their physical behavior. The effectiveness of our control laws is then shown through numerical simulation. This is an extended version of an ECC24 article of the same name, which includes the proofs omitted for lack of space.","sentences":["This paper introduces a hybrid dynamical system methodology for managing impulsive control in spacecraft rendezvous and proximity operations under the Hill-Clohessy-Wiltshire model.","We address the control design problem by isolating the out-of-plane from the in-plane dynamics and present a feedback control law for each of them.","This law is based on a Lyapunov function tailored to each of the dynamics, capable of addressing thruster saturation and also a minimum impulse bit.","These Lyapunov functions were found by reformulating the system's dynamics into coordinates that more intuitively represent their physical behavior.","The effectiveness of our control laws is then shown through numerical simulation.","This is an extended version of an ECC24 article of the same name, which includes the proofs omitted for lack of space."],"url":"http://arxiv.org/abs/2403.03633v1","category":"eess.SY"}
{"created":"2024-03-06 11:32:04","title":"Weyl points and anomalous transport effects tuned by the Fe doping in Mn$_3$Ge Weyl semimetal","abstract":"The discovery of a significantly large anomalous Hall effect in the chiral antiferromagnetic system - Mn$_3$Ge - indicates that the Weyl points are widely separated in phase space and positioned near the Fermi surface. In order to examine the effects of Fe substitution in Mn$_3$Ge on the presence and location of the Weyl points, we synthesized (Mn$_{1-\\alpha}$Fe$_{\\alpha})$$_3$Ge ($\\alpha=0-0.30$) compounds. The anomalous Hall effect was observed in compounds up to $\\alpha=0.22$, but only within the temperature range where the magnetic structure remains the same as the Mn$_3$Ge. Additionally, positive longitudinal magnetoconductance and planar Hall effect were detected within the same temperature and doping range. These findings strongly suggest the existence of Weyl points in (Mn$_{1-\\alpha}$Fe$_{\\alpha})$$_3$Ge ($\\alpha=0-0.22$) compounds. Further, we observed that with an increase in Fe doping fraction, there is a significant reduction in the magnitude of anomalous Hall conductivity, planar Hall effect, and positive longitudinal magnetoconductance, indicating that the Weyl points move further away from the Fermi surface. Consequently, it can be concluded that suitable dopants in the parent Weyl semimetals have the potential to tune the properties of Weyl points and the resulting anomalous electrical transport effects.","sentences":["The discovery of a significantly large anomalous Hall effect in the chiral antiferromagnetic system - Mn$_3$Ge - indicates that the Weyl points are widely separated in phase space and positioned near the Fermi surface.","In order to examine the effects of Fe substitution in Mn$_3$Ge on the presence and location of the Weyl points, we synthesized (Mn$_{1-\\alpha}$Fe$_{\\alpha})$$_3$Ge ($\\alpha=0-0.30$) compounds.","The anomalous Hall effect was observed in compounds up to $\\alpha=0.22$, but only within the temperature range where the magnetic structure remains the same as the Mn$_3$Ge.","Additionally, positive longitudinal magnetoconductance and planar Hall effect were detected within the same temperature and doping range.","These findings strongly suggest the existence of Weyl points in (Mn$_{1-\\alpha}$Fe$_{\\alpha})$$_3$Ge ($\\alpha=0-0.22$) compounds.","Further, we observed that with an increase in Fe doping fraction, there is a significant reduction in the magnitude of anomalous Hall conductivity, planar Hall effect, and positive longitudinal magnetoconductance, indicating that the Weyl points move further away from the Fermi surface.","Consequently, it can be concluded that suitable dopants in the parent Weyl semimetals have the potential to tune the properties of Weyl points and the resulting anomalous electrical transport effects."],"url":"http://arxiv.org/abs/2403.03626v1","category":"cond-mat.str-el"}
{"created":"2024-03-06 10:46:56","title":"Probing molecules in gas cells of subwavelength thickness with high frequency resolution","abstract":"Miniaturizing and integrating atomic vapor cells is widely investigated for the purposes of fundamental measurements and technological applications such as quantum sensing. Extending such platforms to the realm of molecular physics is a fascinating prospect that paves the way for compact frequency metrology as well as for exploring light-matter interactions with complex quantum objects. Here, we perform molecular rovibrational spectroscopy in a thin-cell of micrometric thickness, comparable to excitation wavelengths. We operate the cell in two distinct regions of the electromagnetic spectrum, probing $\\nu_1$+$\\nu_3$ resonances of acetylene at 1.530$\\mu$m, within the telecommunications wavelength range, as well as the $\\nu_3$ and $\\nu_2$ resonances of $SF_6$ and $NH_3$ respectively, in the mid-infrared fingerprint region around 10.55$\\mu$m. Thin-cell confinement allows linear sub-Doppler transmission spectroscopy due to the coherent Dicke narrowing effect, here demonstrated for molecular rovibrations. Our experiment can find applications extending to the fields of compact molecular frequency references, atmospheric physics or fundamental precision measurements.","sentences":["Miniaturizing and integrating atomic vapor cells is widely investigated for the purposes of fundamental measurements and technological applications such as quantum sensing.","Extending such platforms to the realm of molecular physics is a fascinating prospect that paves the way for compact frequency metrology as well as for exploring light-matter interactions with complex quantum objects.","Here, we perform molecular rovibrational spectroscopy in a thin-cell of micrometric thickness, comparable to excitation wavelengths.","We operate the cell in two distinct regions of the electromagnetic spectrum, probing $\\nu_1$+$\\nu_3$ resonances of acetylene at 1.530$\\mu$m, within the telecommunications wavelength range, as well as the $\\nu_3$ and $\\nu_2$ resonances of $SF_6$ and $NH_3$ respectively, in the mid-infrared fingerprint region around 10.55$\\mu$m.","Thin-cell confinement allows linear sub-Doppler transmission spectroscopy due to the coherent Dicke narrowing effect, here demonstrated for molecular rovibrations.","Our experiment can find applications extending to the fields of compact molecular frequency references, atmospheric physics or fundamental precision measurements."],"url":"http://arxiv.org/abs/2403.03604v1","category":"physics.atom-ph"}
{"created":"2024-03-06 10:44:55","title":"Data-Based In-Cylinder Pressure Model with Cyclic Variations for Combustion Control: A RCCI Engine Application","abstract":"Cylinder pressure-based control is a key enabler for advanced pre-mixed combustion concepts. Besides guaranteeing robust and safe operation, it allows for cylinder pressure and heat release shaping. This requires fast control-oriented combustion models. Over the years, mean-value models have been proposed that can predict combustion measures (e.g., Gross Indicated Mean Effective Pressure, or the crank angle where 50% of the total heat is released) or models that predict the full in-cylinder pressure. However, these models are not able to capture cyclic variations. This is important in the control design for combustion concepts, like Reactivity Controlled Compression Ignition, that can suffer from large cyclic variations. In this study, the in-cylinder pressure and cyclic variation are modelled using a data-based approach. The model combines Principle Component Decomposition and Gaussian Process Regression. A detailed study is performed on the effects of the different hyperparameters and kernel choices. The approach is applicable to any combustion concept, but most valuable for advance combustion concepts with large cyclic variation. The potential of the proposed approach is demonstrated for an Reactivity Controlled Compression Ignition engine running on Diesel and E85. The prediction quality of the evaluated combustion measures has an overall accuracy of 13.5% and 65.5% in mean behaviour and standard deviation, respectively. The peak-pressure rise-rate is traditionally hard to predict, in the proposed model it has an accuracy of 22.7% and 96.4% in mean behaviour and standard deviation, respectively. This Principle Component Decomposition-based approach is an important step towards in-cylinder pressure shaping. The use of Gaussian Process Regression provides important information on cyclic variation and provides next-cycle controls information on safety and performance criteria.","sentences":["Cylinder pressure-based control is a key enabler for advanced pre-mixed combustion concepts.","Besides guaranteeing robust and safe operation, it allows for cylinder pressure and heat release shaping.","This requires fast control-oriented combustion models.","Over the years, mean-value models have been proposed that can predict combustion measures (e.g., Gross Indicated Mean Effective Pressure, or the crank angle where 50% of the total heat is released) or models that predict the full in-cylinder pressure.","However, these models are not able to capture cyclic variations.","This is important in the control design for combustion concepts, like Reactivity Controlled Compression Ignition, that can suffer from large cyclic variations.","In this study, the in-cylinder pressure and cyclic variation are modelled using a data-based approach.","The model combines Principle Component Decomposition and Gaussian Process Regression.","A detailed study is performed on the effects of the different hyperparameters and kernel choices.","The approach is applicable to any combustion concept, but most valuable for advance combustion concepts with large cyclic variation.","The potential of the proposed approach is demonstrated for an Reactivity Controlled Compression Ignition engine running on Diesel and E85.","The prediction quality of the evaluated combustion measures has an overall accuracy of 13.5% and 65.5% in mean behaviour and standard deviation, respectively.","The peak-pressure rise-rate is traditionally hard to predict, in the proposed model it has an accuracy of 22.7% and 96.4% in mean behaviour and standard deviation, respectively.","This Principle Component Decomposition-based approach is an important step towards in-cylinder pressure shaping.","The use of Gaussian Process Regression provides important information on cyclic variation and provides next-cycle controls information on safety and performance criteria."],"url":"http://arxiv.org/abs/2403.03602v1","category":"eess.SY"}
{"created":"2024-03-06 10:29:21","title":"A reduced kinetic method for investigating non-local ion heat transport in ideal multi-species plasmas","abstract":"A reduced kinetic method (RKM) with a first-principle collision operator is introduced in a 1D2V planar geometry and implemented in a computationally inexpensive code to investigate non-local ion heat transport in multi-species plasmas. The RKM successfully reproduces local results for multi-species ion systems and the important features expected to arise due to non-local effects on the heat flux are captured. In addition to this, novel features associated with multi-species, as opposed to single species, case are found. Effects of non-locality on the heat flux are investigated in mass and charge symmetric and asymmetric ion mixtures with temperature, pressure, and concentration gradients. In particular, the enthalpy flux associated with diffusion is found to be insensitive to sharp pressure and concentration gradients, increasing its significance in comparison to the conductive heat flux driven by temperature gradients in non-local scenarios. The RKM code can be used for investigating other kinetic and non-local effects in a broader plasma physics context. Due to its relatively low computational cost it can also serve as a practical non-local ion heat flux closure in hydrodynamic simulations or as a training tool for machine learning surrogates.","sentences":["A reduced kinetic method (RKM) with a first-principle collision operator is introduced in a 1D2V planar geometry and implemented in a computationally inexpensive code to investigate non-local ion heat transport in multi-species plasmas.","The RKM successfully reproduces local results for multi-species ion systems and the important features expected to arise due to non-local effects on the heat flux are captured.","In addition to this, novel features associated with multi-species, as opposed to single species, case are found.","Effects of non-locality on the heat flux are investigated in mass and charge symmetric and asymmetric ion mixtures with temperature, pressure, and concentration gradients.","In particular, the enthalpy flux associated with diffusion is found to be insensitive to sharp pressure and concentration gradients, increasing its significance in comparison to the conductive heat flux driven by temperature gradients in non-local scenarios.","The RKM code can be used for investigating other kinetic and non-local effects in a broader plasma physics context.","Due to its relatively low computational cost it can also serve as a practical non-local ion heat flux closure in hydrodynamic simulations or as a training tool for machine learning surrogates."],"url":"http://arxiv.org/abs/2403.03595v1","category":"physics.plasm-ph"}
{"created":"2024-03-06 10:00:10","title":"Speed limits to the growth of Krylov complexity in open quantum systems","abstract":"Recently, the propagation of information through quantum many-body systems, developed to study quantum chaos, have found many application from black holes to disordered spin systems. Among other quantitative tools, Krylov complexity has been explored as a diagnostic tool for information scrambling in quantum many-body systems. We introduce a universal limit to the growth of the Krylov complexity in dissipative open quantum systems by utilizing the uncertainty relation for non-hermitian operators. We also present the analytical results of Krylov complexity for characteristic behavior of Lanczos coefficients in dissipative systems. The validity of these results are demonstrated by explicit study of transverse-field Ising model under dissipative effects.","sentences":["Recently, the propagation of information through quantum many-body systems, developed to study quantum chaos, have found many application from black holes to disordered spin systems.","Among other quantitative tools, Krylov complexity has been explored as a diagnostic tool for information scrambling in quantum many-body systems.","We introduce a universal limit to the growth of the Krylov complexity in dissipative open quantum systems by utilizing the uncertainty relation for non-hermitian operators.","We also present the analytical results of Krylov complexity for characteristic behavior of Lanczos coefficients in dissipative systems.","The validity of these results are demonstrated by explicit study of transverse-field Ising model under dissipative effects."],"url":"http://arxiv.org/abs/2403.03584v1","category":"quant-ph"}
{"created":"2024-03-06 09:56:44","title":"Stabilization via localized controls in nonlocal models of crowd dynamics","abstract":"We consider a control system driven by a nonlocal continuity equation. Admissible controls are Lipschitz vector fields acting inside a fixed open set. We demonstrate that small perturbations of the initial measure, traced along Wasserstein geodesics, may be neutralized by admissible controls. More specifically, initial perturbations of order $\\varepsilon$ can be reduced to order $\\varepsilon^{1+\\kappa}$, where $\\kappa$ is a positive constant.","sentences":["We consider a control system driven by a nonlocal continuity equation.","Admissible controls are Lipschitz vector fields acting inside a fixed open set.","We demonstrate that small perturbations of the initial measure, traced along Wasserstein geodesics, may be neutralized by admissible controls.","More specifically, initial perturbations of order $\\varepsilon$ can be reduced to order $\\varepsilon^{1+\\kappa}$, where $\\kappa$ is a positive constant."],"url":"http://arxiv.org/abs/2403.03580v1","category":"math.OC"}
{"created":"2024-03-06 09:34:27","title":"Time-optimal Point-to-point Motion Planning: A Two-stage Approach","abstract":"This paper proposes a two-stage approach to formulate the time-optimal point-to-point motion planning problem, involving a first stage with a fixed time grid and a second stage with a variable time grid. The proposed approach brings benefits through its straightforward optimal control problem formulation with a fixed and low number of control steps for manageable computational complexity and the avoidance of interpolation errors associated with time scaling, especially when aiming to reach a distant goal. Additionally, an asynchronous nonlinear model predictive control (NMPC) update scheme is integrated with this two-stage approach to address delayed and fluctuating computation times, facilitating online replanning. The effectiveness of the proposed two-stage approach and NMPC implementation is demonstrated through numerical examples centered on autonomous navigation with collision avoidance.","sentences":["This paper proposes a two-stage approach to formulate the time-optimal point-to-point motion planning problem, involving a first stage with a fixed time grid and a second stage with a variable time grid.","The proposed approach brings benefits through its straightforward optimal control problem formulation with a fixed and low number of control steps for manageable computational complexity and the avoidance of interpolation errors associated with time scaling, especially when aiming to reach a distant goal.","Additionally, an asynchronous nonlinear model predictive control (NMPC) update scheme is integrated with this two-stage approach to address delayed and fluctuating computation times, facilitating online replanning.","The effectiveness of the proposed two-stage approach and NMPC implementation is demonstrated through numerical examples centered on autonomous navigation with collision avoidance."],"url":"http://arxiv.org/abs/2403.03573v1","category":"cs.RO"}
{"created":"2024-03-06 09:25:11","title":"Equivalence between VMO functions and Zero Lelong numbers functions","abstract":"We prove that a plurisbharmonic function on a bounded domain $\\Omega$ in $\\mathbb{C}^n$ is a VMO (Vanishing Mean Oscillation) function if and only if its Lelong number at each point of $\\Omega$ vanishes. This result contributes to a better understanding of the behavior of singular plurisubharmonic functions.","sentences":["We prove that a plurisbharmonic function on a bounded domain $\\Omega$ in $\\mathbb{C}^n$ is a VMO (Vanishing Mean Oscillation) function if and only if its Lelong number at each point of $\\Omega$ vanishes.","This result contributes to a better understanding of the behavior of singular plurisubharmonic functions."],"url":"http://arxiv.org/abs/2403.03568v1","category":"math.CV"}
{"created":"2024-03-06 09:14:24","title":"Efficient Algorithms for Empirical Group Distributional Robust Optimization and Beyond","abstract":"We investigate the empirical counterpart of group distributionally robust optimization (GDRO), which aims to minimize the maximal empirical risk across $m$ distinct groups. We formulate empirical GDRO as a $\\textit{two-level}$ finite-sum convex-concave minimax optimization problem and develop a stochastic variance reduced mirror prox algorithm. Unlike existing methods, we construct the stochastic gradient by per-group sampling technique and perform variance reduction for all groups, which fully exploits the $\\textit{two-level}$ finite-sum structure of empirical GDRO. Furthermore, we compute the snapshot and mirror snapshot point by a one-index-shifted weighted average, which distinguishes us from the naive ergodic average. Our algorithm also supports non-constant learning rates, which is different from existing literature. We establish convergence guarantees both in expectation and with high probability, demonstrating a complexity of $\\mathcal{O}\\left(\\frac{m\\sqrt{\\bar{n}\\ln{m}}}{\\varepsilon}\\right)$, where $\\bar n$ is the average number of samples among $m$ groups. Remarkably, our approach outperforms the state-of-the-art method by a factor of $\\sqrt{m}$. Furthermore, we extend our methodology to deal with the empirical minimax excess risk optimization (MERO) problem and manage to give the expectation bound and the high probability bound, accordingly. The complexity of our empirical MERO algorithm matches that of empirical GDRO at $\\mathcal{O}\\left(\\frac{m\\sqrt{\\bar{n}\\ln{m}}}{\\varepsilon}\\right)$, significantly surpassing the bounds of existing methods.","sentences":["We investigate the empirical counterpart of group distributionally robust optimization (GDRO), which aims to minimize the maximal empirical risk across $m$ distinct groups.","We formulate empirical GDRO as a $\\textit{two-level}$ finite-sum convex-concave minimax optimization problem and develop a stochastic variance reduced mirror prox algorithm.","Unlike existing methods, we construct the stochastic gradient by per-group sampling technique and perform variance reduction for all groups, which fully exploits the $\\textit{two-level}$ finite-sum structure of empirical GDRO.","Furthermore, we compute the snapshot and mirror snapshot point by a one-index-shifted weighted average, which distinguishes us from the naive ergodic average.","Our algorithm also supports non-constant learning rates, which is different from existing literature.","We establish convergence guarantees both in expectation and with high probability, demonstrating a complexity of $\\mathcal{O}\\left(\\frac{m\\sqrt{\\bar{n}\\ln{m}}}{\\varepsilon}\\right)$, where $\\bar n$ is the average number of samples among $m$ groups.","Remarkably, our approach outperforms the state-of-the-art method by a factor of $\\sqrt{m}$. Furthermore, we extend our methodology to deal with the empirical minimax excess risk optimization (MERO) problem and manage to give the expectation bound and the high probability bound, accordingly.","The complexity of our empirical MERO algorithm matches that of empirical GDRO at $\\mathcal{O}\\left(\\frac{m\\sqrt{\\bar{n}\\ln{m}}}{\\varepsilon}\\right)$, significantly surpassing the bounds of existing methods."],"url":"http://arxiv.org/abs/2403.03562v1","category":"cs.LG"}
{"created":"2024-03-06 08:56:40","title":"Assessing the Global Natural Orbital Functional Approximation on Model Systems with Strong Correlation","abstract":"In the past decade, natural orbital functional (NOF) approximations have emerged as prominent tools for characterizing electron correlation. Despite their effectiveness, these approaches, which rely on natural orbitals and their associated occupation numbers, often require hybridization with other methods to fully account for all correlation effects. Recently, a global NOF (GNOF) has been proposed [Phys. Rev. Lett. 127, 233001 (2021)] to comprehensively address both dynamic and static correlations. This study evaluates the performance of GNOF on strongly correlated model systems, including comparisons with highly accurate Full Configuration Interaction (FCI) calculations for hydrogen atom clusters in one, two, and three dimensions. Additionally, the investigation extends to a BeH2 reaction, involving the insertion of a beryllium atom into a hydrogen molecule along a C2v pathway. According to the results obtained using GNOF, consistent behavior is observed across various correlation regions, encompassing a range of occupation and orbital schemes. Furthermore, distinctive features are identified when varying the dimensionality of the system.","sentences":["In the past decade, natural orbital functional (NOF) approximations have emerged as prominent tools for characterizing electron correlation.","Despite their effectiveness, these approaches, which rely on natural orbitals and their associated occupation numbers, often require hybridization with other methods to fully account for all correlation effects.","Recently, a global NOF (GNOF) has been proposed [Phys. Rev. Lett.","127, 233001 (2021)] to comprehensively address both dynamic and static correlations.","This study evaluates the performance of GNOF on strongly correlated model systems, including comparisons with highly accurate Full Configuration Interaction (FCI) calculations for hydrogen atom clusters in one, two, and three dimensions.","Additionally, the investigation extends to a BeH2 reaction, involving the insertion of a beryllium atom into a hydrogen molecule along a C2v pathway.","According to the results obtained using GNOF, consistent behavior is observed across various correlation regions, encompassing a range of occupation and orbital schemes.","Furthermore, distinctive features are identified when varying the dimensionality of the system."],"url":"http://arxiv.org/abs/2403.03554v1","category":"physics.chem-ph"}
{"created":"2024-03-06 08:55:34","title":"Population-aware Online Mirror Descent for Mean-Field Games by Deep Reinforcement Learning","abstract":"Mean Field Games (MFGs) have the ability to handle large-scale multi-agent systems, but learning Nash equilibria in MFGs remains a challenging task. In this paper, we propose a deep reinforcement learning (DRL) algorithm that achieves population-dependent Nash equilibrium without the need for averaging or sampling from history, inspired by Munchausen RL and Online Mirror Descent. Through the design of an additional inner-loop replay buffer, the agents can effectively learn to achieve Nash equilibrium from any distribution, mitigating catastrophic forgetting. The resulting policy can be applied to various initial distributions. Numerical experiments on four canonical examples demonstrate our algorithm has better convergence properties than SOTA algorithms, in particular a DRL version of Fictitious Play for population-dependent policies.","sentences":["Mean Field Games (MFGs) have the ability to handle large-scale multi-agent systems, but learning Nash equilibria in MFGs remains a challenging task.","In this paper, we propose a deep reinforcement learning (DRL) algorithm that achieves population-dependent Nash equilibrium without the need for averaging or sampling from history, inspired by Munchausen RL and Online Mirror Descent.","Through the design of an additional inner-loop replay buffer, the agents can effectively learn to achieve Nash equilibrium from any distribution, mitigating catastrophic forgetting.","The resulting policy can be applied to various initial distributions.","Numerical experiments on four canonical examples demonstrate our algorithm has better convergence properties than SOTA algorithms, in particular a DRL version of Fictitious Play for population-dependent policies."],"url":"http://arxiv.org/abs/2403.03552v1","category":"cs.GT"}
{"created":"2024-03-06 08:51:09","title":"Low-Dose CT Image Reconstruction by Fine-Tuning a UNet Pretrained for Gaussian Denoising for the Downstream Task of Image Enhancement","abstract":"Computed Tomography (CT) is a widely used medical imaging modality, and as it is based on ionizing radiation, it is desirable to minimize the radiation dose. However, a reduced radiation dose comes with reduced image quality, and reconstruction from low-dose CT (LDCT) data is still a challenging task which is subject to research. According to the LoDoPaB-CT benchmark, a benchmark for LDCT reconstruction, many state-of-the-art methods use pipelines involving UNet-type architectures. Specifically the top ranking method, ItNet, employs a three-stage process involving filtered backprojection (FBP), a UNet trained on CT data, and an iterative refinement step. In this paper, we propose a less complex two-stage method. The first stage also employs FBP, while the novelty lies in the training strategy for the second stage, characterized as the CT image enhancement stage. The crucial point of our approach is that the neural network is pretrained on a distinctly different pretraining task with non-CT data, namely Gaussian noise removal on a variety of natural grayscale images (photographs). We then fine-tune this network for the downstream task of CT image enhancement using pairs of LDCT images and corresponding normal-dose CT images (NDCT). Despite being notably simpler than the state-of-the-art, as the pretraining did not depend on domain-specific CT data and no further iterative refinement step was necessary, the proposed two-stage method achieves competitive results. The proposed method achieves a shared top ranking in the LoDoPaB-CT challenge and a first position with respect to the SSIM metric.","sentences":["Computed Tomography (CT) is a widely used medical imaging modality, and as it is based on ionizing radiation, it is desirable to minimize the radiation dose.","However, a reduced radiation dose comes with reduced image quality, and reconstruction from low-dose CT (LDCT) data is still a challenging task which is subject to research.","According to the LoDoPaB-CT benchmark, a benchmark for LDCT reconstruction, many state-of-the-art methods use pipelines involving UNet-type architectures.","Specifically the top ranking method, ItNet, employs a three-stage process involving filtered backprojection (FBP), a UNet trained on CT data, and an iterative refinement step.","In this paper, we propose a less complex two-stage method.","The first stage also employs FBP, while the novelty lies in the training strategy for the second stage, characterized as the CT image enhancement stage.","The crucial point of our approach is that the neural network is pretrained on a distinctly different pretraining task with non-CT data, namely Gaussian noise removal on a variety of natural grayscale images (photographs).","We then fine-tune this network for the downstream task of CT image enhancement using pairs of LDCT images and corresponding normal-dose CT images (NDCT).","Despite being notably simpler than the state-of-the-art, as the pretraining did not depend on domain-specific CT data and no further iterative refinement step was necessary, the proposed two-stage method achieves competitive results.","The proposed method achieves a shared top ranking in the LoDoPaB-CT challenge and a first position with respect to the SSIM metric."],"url":"http://arxiv.org/abs/2403.03551v1","category":"eess.IV"}
{"created":"2024-03-06 08:27:53","title":"Characterization of the low electric field and zero-temperature two-level-system loss in hydrogenated amorphous silicon","abstract":"Two-level systems (TLS) are an important, if not dominant, source of loss and noise for superconducting resonators such as those used in kinetic inductance detectors and some quantum information science platforms. They are similarly important for loss in photolithographically fabricated superconducting mm-wave/THz transmission lines. For both lumped-element and transmission-line structures, native amorphous surface oxide films are typically the sites of such TLS in non-microstripline geometries, while loss in the (usually amorphous) dielectric film itself usually dominates in microstriplines. We report here on the demonstration of low TLS loss at GHz frequencies in hydrogenated amorphous silicon (a-Si:H) films deposited by plasma-enhanced chemical vapor deposition in superconducting lumped-element resonators using parallel-plate capacitors (PPCs). The values we obtain from two recipes in different deposition machines, 7$\\,\\times\\,10^{-6}$ and 12$\\,\\times\\,10^{-6}$, improve on the best achieved in the literature by a factor of 2--4 for a-Si:H and are comparable to recent measurements of amorphous germanium. Moreover, we have taken care to extract the true zero-temperature, low-field loss tangent of these films, accounting for temperature and field saturation effects that can yield misleading results. Such robustly fabricated and characterized films render the use of PPCs with deposited amorphous films a viable architecture for superconducting resonators, and they also promise extremely low loss and high quality factor for photolithographically fabricated superconducting mm-wave/THz transmission lines used in planar antennas and resonant filters.","sentences":["Two-level systems (TLS) are an important, if not dominant, source of loss and noise for superconducting resonators such as those used in kinetic inductance detectors and some quantum information science platforms.","They are similarly important for loss in photolithographically fabricated superconducting mm-wave/THz transmission lines.","For both lumped-element and transmission-line structures, native amorphous surface oxide films are typically the sites of such TLS in non-microstripline geometries, while loss in the (usually amorphous) dielectric film itself usually dominates in microstriplines.","We report here on the demonstration of low TLS loss at GHz frequencies in hydrogenated amorphous silicon (a-Si:H) films deposited by plasma-enhanced chemical vapor deposition in superconducting lumped-element resonators using parallel-plate capacitors (PPCs).","The values we obtain from two recipes in different deposition machines, 7$\\,\\times\\,10^{-6}$ and 12$\\,\\times\\,10^{-6}$, improve on the best achieved in the literature by a factor of 2--4 for a-Si:H and are comparable to recent measurements of amorphous germanium.","Moreover, we have taken care to extract the true zero-temperature, low-field loss tangent of these films, accounting for temperature and field saturation effects that can yield misleading results.","Such robustly fabricated and characterized films render the use of PPCs with deposited amorphous films a viable architecture for superconducting resonators, and they also promise extremely low loss and high quality factor for photolithographically fabricated superconducting mm-wave/THz transmission lines used in planar antennas and resonant filters."],"url":"http://arxiv.org/abs/2403.03534v1","category":"physics.ins-det"}
{"created":"2024-03-06 08:10:02","title":"Data based constitutive modelling of rate independent inelastic effects in composite cables using Preisach hysteresis operators","abstract":"This contribution aims at introducing first steps to develop hysteresis operator type inelastic constitutive laws for Cosserat rods for the simulation of cables composed of complex interior components. Motivated by the basic elements of Cosserat rod theory, we develop a specific approach to constitutive modelling adapted for this application. Afterwards, we describe the hysteretical behaviour arising from cyclic bending experiments on cables by means of the Preisach operator. As shown in pure bending experiments, slender structures such as electric cables behave inelastically, and open hysteresis loops arise with noticeable difference between the first load cycle and the following ones. The Preisach operator plays an important role in describing the input-output relation in hysteresis behaviours, and it can be expressed as a superposition of relay operators. Hence, a mathematical formulation of the problem is introduced, and a first attempt is made to determine the hysteresis behaviour that describes the relation between curvature and bending moment. Therefore, a suitable kernel function is identified in a way that its integration over the Preisach plane results in the bending moment of the specimen, and a comparison between different kernel functions is performed.","sentences":["This contribution aims at introducing first steps to develop hysteresis operator type inelastic constitutive laws for Cosserat rods for the simulation of cables composed of complex interior components.","Motivated by the basic elements of Cosserat rod theory, we develop a specific approach to constitutive modelling adapted for this application.","Afterwards, we describe the hysteretical behaviour arising from cyclic bending experiments on cables by means of the Preisach operator.","As shown in pure bending experiments, slender structures such as electric cables behave inelastically, and open hysteresis loops arise with noticeable difference between the first load cycle and the following ones.","The Preisach operator plays an important role in describing the input-output relation in hysteresis behaviours, and it can be expressed as a superposition of relay operators.","Hence, a mathematical formulation of the problem is introduced, and a first attempt is made to determine the hysteresis behaviour that describes the relation between curvature and bending moment.","Therefore, a suitable kernel function is identified in a way that its integration over the Preisach plane results in the bending moment of the specimen, and a comparison between different kernel functions is performed."],"url":"http://arxiv.org/abs/2403.03531v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-06 08:08:02","title":"Average-case deterministic query complexity of boolean functions with fixed weight","abstract":"We explore the $\\textit{average-case deterministic query complexity}$ of boolean functions under the $\\textit{uniform distribution}$, denoted by $\\mathrm{D}_\\mathrm{ave}(f)$, the minimum average depth of zero-error decision tree computing a boolean function $f$. This measure found several applications across diverse fields. We study $\\mathrm{D}_\\mathrm{ave}(f)$ of several common functions, including penalty shoot-out functions, symmetric functions, linear threshold functions and tribes functions. Let $\\mathrm{wt}(f)$ denote the number of the inputs on which $f$ outputs $1$. We prove that $\\mathrm{D}_\\mathrm{ave}(f) \\le \\log \\frac{\\mathrm{wt}(f)}{\\log n} + O\\left(\\log \\log \\frac{\\mathrm{wt}(f)}{\\log n}\\right)$ when $\\mathrm{wt}(f) \\ge 4 \\log n$ (otherwise, $\\mathrm{D}_\\mathrm{ave}(f) = O(1)$), and that for almost all fixed-weight functions, $\\mathrm{D}_\\mathrm{ave}(f) \\geq \\log \\frac{\\mathrm{wt}(f)}{\\log n} - O\\left( \\log \\log \\frac{\\mathrm{wt}(f)}{\\log n}\\right)$, which implies the tightness of the upper bound up to an additive logarithmic term. We also study $\\mathrm{D}_\\mathrm{ave}(f)$ of circuits. Using H\\r{a}stad's switching lemma or Rossman's switching lemma [Comput. Complexity Conf. 137, 2019], one can derive upper bounds $\\mathrm{D}_\\mathrm{ave}(f) \\leq n\\left(1 - \\frac{1}{O(k)}\\right)$ for width-$k$ CNFs/DNFs and $\\mathrm{D}_\\mathrm{ave}(f) \\leq n\\left(1 - \\frac{1}{O(\\log s)}\\right)$ for size-$s$ CNFs/DNFs, respectively. For any $w \\ge 1.1 \\log n$, we prove the existence of some width-$w$ size-$(2^w/w)$ DNF formula with $\\mathrm{D}_\\mathrm{ave} (f) = n \\left(1 - \\frac{\\log n}{\\Theta(w)}\\right)$, providing evidence on the tightness of the switching lemmas.","sentences":["We explore the $\\textit{average-case deterministic query complexity}$ of boolean functions under the $\\textit{uniform distribution}$, denoted by $\\mathrm{D}_\\mathrm{ave}(f)$, the minimum average depth of zero-error decision tree computing a boolean function $f$. This measure found several applications across diverse fields.","We study $\\mathrm{D}_\\mathrm{ave}(f)$ of several common functions, including penalty shoot-out functions, symmetric functions, linear threshold functions and tribes functions.","Let $\\mathrm{wt}(f)$ denote the number of the inputs on which $f$ outputs $1$. We prove that $\\mathrm{D}_\\mathrm{ave}(f) \\le \\log \\frac{\\mathrm{wt}(f)}{\\log n} + O\\left(\\log \\log \\frac{\\mathrm{wt}(f)}{\\log n}\\right)$ when $\\mathrm{wt}(f) \\ge 4 \\log n$ (otherwise, $\\mathrm{D}_\\mathrm{ave}(f) = O(1)$), and that for almost all fixed-weight functions, $\\mathrm{D}_\\mathrm{ave}(f) \\geq \\log \\frac{\\mathrm{wt}(f)}{\\log n} - O\\left( \\log \\log \\frac{\\mathrm{wt}(f)}{\\log n}\\right)$, which implies the tightness of the upper bound up to an additive logarithmic term.","We also study $\\mathrm{D}_\\mathrm{ave}(f)$ of circuits.","Using H\\r{a}stad's switching lemma or Rossman's switching lemma","[Comput.","Complexity Conf. 137, 2019], one can derive upper bounds $\\mathrm{D}_\\mathrm{ave}(f) \\leq n\\left(1 - \\frac{1}{O(k)}\\right)$ for width-$k$ CNFs/DNFs and $\\mathrm{D}_\\mathrm{ave}(f) \\leq n\\left(1 - \\frac{1}{O(\\log s)}\\right)$ for size-$s$ CNFs/DNFs, respectively.","For any $w \\ge 1.1 \\log n$, we prove the existence of some width-$w$ size-$(2^w/w)$ DNF formula with $\\mathrm{D}_\\mathrm{ave} (f) = n \\left(1 - \\frac{\\log n}{\\Theta(w)}\\right)$, providing evidence on the tightness of the switching lemmas."],"url":"http://arxiv.org/abs/2403.03530v1","category":"cs.CC"}
{"created":"2024-03-06 08:05:58","title":"LDSF: Lightweight Dual-Stream Framework for SAR Target Recognition by Coupling Local Electromagnetic Scattering Features and Global Visual Features","abstract":"Mainstream DNN-based SAR-ATR methods still face issues such as easy overfitting of a few training data, high computational overhead, and poor interpretability of the black-box model. Integrating physical knowledge into DNNs to improve performance and achieve a higher level of physical interpretability becomes the key to solving the above problems. This paper begins by focusing on the electromagnetic (EM) backscattering mechanism. We extract the EM scattering (EMS) information from the complex SAR data and integrate the physical properties of the target into the network through a dual-stream framework to guide the network to learn physically meaningful and discriminative features. Specifically, one stream is the local EMS feature (LEMSF) extraction net. It is a heterogeneous graph neural network (GNN) guided by a multi-level multi-head attention mechanism. LEMSF uses the EMS information to obtain topological structure features and high-level physical semantic features. The other stream is a CNN-based global visual features (GVF) extraction net that captures the visual features of SAR pictures from the image domain. After obtaining the two-stream features, a feature fusion subnetwork is proposed to adaptively learn the fusion strategy. Thus, the two-stream features can maximize the performance. Furthermore, the loss function is designed based on the graph distance measure to promote intra-class aggregation. We discard overly complex design ideas and effectively control the model size while maintaining algorithm performance. Finally, to better validate the performance and generalizability of the algorithms, two more rigorous evaluation protocols, namely once-for-all (OFA) and less-for-more (LFM), are used to verify the superiority of the proposed algorithm on the MSTAR.","sentences":["Mainstream DNN-based SAR-ATR methods still face issues such as easy overfitting of a few training data, high computational overhead, and poor interpretability of the black-box model.","Integrating physical knowledge into DNNs to improve performance and achieve a higher level of physical interpretability becomes the key to solving the above problems.","This paper begins by focusing on the electromagnetic (EM) backscattering mechanism.","We extract the EM scattering (EMS) information from the complex SAR data and integrate the physical properties of the target into the network through a dual-stream framework to guide the network to learn physically meaningful and discriminative features.","Specifically, one stream is the local EMS feature (LEMSF) extraction net.","It is a heterogeneous graph neural network (GNN) guided by a multi-level multi-head attention mechanism.","LEMSF uses the EMS information to obtain topological structure features and high-level physical semantic features.","The other stream is a CNN-based global visual features (GVF) extraction net that captures the visual features of SAR pictures from the image domain.","After obtaining the two-stream features, a feature fusion subnetwork is proposed to adaptively learn the fusion strategy.","Thus, the two-stream features can maximize the performance.","Furthermore, the loss function is designed based on the graph distance measure to promote intra-class aggregation.","We discard overly complex design ideas and effectively control the model size while maintaining algorithm performance.","Finally, to better validate the performance and generalizability of the algorithms, two more rigorous evaluation protocols, namely once-for-all (OFA) and less-for-more (LFM), are used to verify the superiority of the proposed algorithm on the MSTAR."],"url":"http://arxiv.org/abs/2403.03527v1","category":"eess.IV"}
{"created":"2024-03-06 08:05:53","title":"FingerNet: EEG Decoding of A Fine Motor Imagery with Finger-tapping Task Based on A Deep Neural Network","abstract":"Brain-computer interface (BCI) technology facilitates communication between the human brain and computers, primarily utilizing electroencephalography (EEG) signals to discern human intentions. Although EEG-based BCI systems have been developed for paralysis individuals, ongoing studies explore systems for speech imagery and motor imagery (MI). This study introduces FingerNet, a specialized network for fine MI classification, departing from conventional gross MI studies. The proposed FingerNet could extract spatial and temporal features from EEG signals, improving classification accuracy within the same hand. The experimental results demonstrated that performance showed significantly higher accuracy in classifying five finger-tapping tasks, encompassing thumb, index, middle, ring, and little finger movements. FingerNet demonstrated dominant performance compared to the conventional baseline models, EEGNet and DeepConvNet. The average accuracy for FingerNet was 0.3049, whereas EEGNet and DeepConvNet exhibited lower accuracies of 0.2196 and 0.2533, respectively. Statistical validation also demonstrates the predominance of FingerNet over baseline networks. For biased predictions, particularly for thumb and index classes, we led to the implementation of weighted cross-entropy and also adapted the weighted cross-entropy, a method conventionally employed to mitigate class imbalance. The proposed FingerNet involves optimizing network structure, improving performance, and exploring applications beyond fine MI. Moreover, the weighted Cross Entropy approach employed to address such biased predictions appears to have broader applicability and relevance across various domains involving multi-class classification tasks. We believe that effective execution of motor imagery can be achieved not only for fine MI, but also for local muscle MI","sentences":["Brain-computer interface (BCI) technology facilitates communication between the human brain and computers, primarily utilizing electroencephalography (EEG) signals to discern human intentions.","Although EEG-based BCI systems have been developed for paralysis individuals, ongoing studies explore systems for speech imagery and motor imagery (MI).","This study introduces FingerNet, a specialized network for fine MI classification, departing from conventional gross MI studies.","The proposed FingerNet could extract spatial and temporal features from EEG signals, improving classification accuracy within the same hand.","The experimental results demonstrated that performance showed significantly higher accuracy in classifying five finger-tapping tasks, encompassing thumb, index, middle, ring, and little finger movements.","FingerNet demonstrated dominant performance compared to the conventional baseline models, EEGNet and DeepConvNet.","The average accuracy for FingerNet was 0.3049, whereas EEGNet and DeepConvNet exhibited lower accuracies of 0.2196 and 0.2533, respectively.","Statistical validation also demonstrates the predominance of FingerNet over baseline networks.","For biased predictions, particularly for thumb and index classes, we led to the implementation of weighted cross-entropy and also adapted the weighted cross-entropy, a method conventionally employed to mitigate class imbalance.","The proposed FingerNet involves optimizing network structure, improving performance, and exploring applications beyond fine MI.","Moreover, the weighted Cross Entropy approach employed to address such biased predictions appears to have broader applicability and relevance across various domains involving multi-class classification tasks.","We believe that effective execution of motor imagery can be achieved not only for fine MI, but also for local muscle MI"],"url":"http://arxiv.org/abs/2403.03526v1","category":"eess.SP"}
{"created":"2024-03-06 08:04:50","title":"Exploratory Factory Analysis of the Centrality Metrics for Complex Real-World Networks","abstract":"Exploratory factor analysis (EFA) is useful to identify the number and mapping of the hidden factors that could dominantly represent the features in the dataset. Principal component analysis (PCA) is the first step as part of the two-step procedure to conduct EFA, with the number of dominant principal components being the number of hidden factors and the entries for the features in the corresponding Eigenvectors serve as the initial values of the factor loadings. In this paper, we conduct EFA on a suite of 80 complex network datasets to identify the number and mapping of the hidden factors (expected to be less than four) that could dominantly represent the values incurred by the vertices with respect to the four major centrality metrics (degree: DEG, eigenvector: EVC, betweenness: BWC and closeness: CLC).","sentences":["Exploratory factor analysis (EFA) is useful to identify the number and mapping of the hidden factors that could dominantly represent the features in the dataset.","Principal component analysis (PCA) is the first step as part of the two-step procedure to conduct EFA, with the number of dominant principal components being the number of hidden factors and the entries for the features in the corresponding Eigenvectors serve as the initial values of the factor loadings.","In this paper, we conduct EFA on a suite of 80 complex network datasets to identify the number and mapping of the hidden factors (expected to be less than four) that could dominantly represent the values incurred by the vertices with respect to the four major centrality metrics (degree: DEG, eigenvector: EVC, betweenness: BWC and closeness: CLC)."],"url":"http://arxiv.org/abs/2403.03525v1","category":"cs.SI"}
{"created":"2024-03-06 08:01:58","title":"Modelling the inelastic constitutive behaviour of multi-layer spiral strands. Comparison of hysteresis operator approach to multi-scale model","abstract":"The simulation of inelastic effects in flexible slender technical devices has become of increasing interest in the past years. Different approaches have been considered depending on the effects relevant for the specific application. Recently, a mixed stress strain driven computational homogenisation has been proposed to model the dissipative nonlinear bending response of spiral strands subjected to axial force. In this study, we propose two different approaches, namely a rheological model and a databased greybox model, to predict the cyclic response of these strands using only their monotonic response. In the first approach, a system of so-called bending springs and sliders is used to model different contributions to the bending stiffness of the strands. The data-based approach makes use of mathematical tools called hysteresis operators. The Prandtl-Ishlinskii operator plays a relevant role in modelling the input-output relation in phenomena showing hysteretic behaviour and can be expressed as a weighted superposition of elementary stop operators. Comparing the two approaches leads to a better understanding and an explicit physical interpretation of the parameters of a specific class of hysteresis operator models.","sentences":["The simulation of inelastic effects in flexible slender technical devices has become of increasing interest in the past years.","Different approaches have been considered depending on the effects relevant for the specific application.","Recently, a mixed stress strain driven computational homogenisation has been proposed to model the dissipative nonlinear bending response of spiral strands subjected to axial force.","In this study, we propose two different approaches, namely a rheological model and a databased greybox model, to predict the cyclic response of these strands using only their monotonic response.","In the first approach, a system of so-called bending springs and sliders is used to model different contributions to the bending stiffness of the strands.","The data-based approach makes use of mathematical tools called hysteresis operators.","The Prandtl-Ishlinskii operator plays a relevant role in modelling the input-output relation in phenomena showing hysteretic behaviour and can be expressed as a weighted superposition of elementary stop operators.","Comparing the two approaches leads to a better understanding and an explicit physical interpretation of the parameters of a specific class of hysteresis operator models."],"url":"http://arxiv.org/abs/2403.03520v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-06 08:00:49","title":"KIAS Lectures on Symplectic Aspects of Degenerations","abstract":"This is a series of three lectures I gave at the Korea Institute of Advanced Study in June 2019 at a workshop about \"Algebraic and Symplectic Aspects of Degenerations of Complex Surfaces\". I focus on the symplectic aspects, in particular on the case of cyclic quotient surface singularities. These notes have been available on a public Git repository since 2019, and I noticed that people occasionally cited them in the years since. For that reason, I decided to post them on arXiv for a more permanent record; I have made some small corrections and annotations but otherwise they are unchanged. These notes are a purely expository account of stuff I was thinking about 2016-2019, and are largely self-aggrandising.","sentences":["This is a series of three lectures I gave at the Korea Institute of Advanced Study in June 2019 at a workshop about \"Algebraic and Symplectic Aspects of Degenerations of Complex Surfaces\".","I focus on the symplectic aspects, in particular on the case of cyclic quotient surface singularities.","These notes have been available on a public Git repository since 2019, and I noticed that people occasionally cited them in the years since.","For that reason, I decided to post them on arXiv for a more permanent record; I have made some small corrections and annotations but otherwise they are unchanged.","These notes are a purely expository account of stuff I was thinking about 2016-2019, and are largely self-aggrandising."],"url":"http://arxiv.org/abs/2403.03519v1","category":"math.SG"}
{"created":"2024-03-06 07:40:53","title":"Spectrum of random centrosymmetric matrices; CLT and Circular law","abstract":"We analyze the asymptotic fluctuations of linear eigenvalue statistics of random centrosymmetric matrices with i.i.d. entries. We prove that for a complex analytic test function, the centered and normalized linear eigenvalue statistics of random centrosymmetric matrices converge to a normal distribution. We find the exact expression of the variance of the limiting normal distribution via combinatorial arguments. Moreover, we also argue that the limiting spectral distribution of properly scaled centrosymmetric matrices follows the circular law.","sentences":["We analyze the asymptotic fluctuations of linear eigenvalue statistics of random centrosymmetric matrices with i.i.d. entries.","We prove that for a complex analytic test function, the centered and normalized linear eigenvalue statistics of random centrosymmetric matrices converge to a normal distribution.","We find the exact expression of the variance of the limiting normal distribution via combinatorial arguments.","Moreover, we also argue that the limiting spectral distribution of properly scaled centrosymmetric matrices follows the circular law."],"url":"http://arxiv.org/abs/2403.03513v1","category":"math.PR"}
{"created":"2024-03-06 07:38:31","title":"Illuminating the property space in crystal structure prediction using Quality-Diversity algorithms","abstract":"The identification of materials with exceptional properties is an essential objective to enable technological progress. We propose the application of \\textit{Quality-Diversity} algorithms to the field of crystal structure prediction. The objective of these algorithms is to identify a diverse set of high-performing solutions, which has been successful in a range of fields such as robotics, architecture and aeronautical engineering. As these methods rely on a high number of evaluations, we employ machine-learning surrogate models to compute the interatomic potential and material properties that are used to guide optimisation. Consequently, we also show the value of using neural networks to model crystal properties and enable the identification of novel composition--structure combinations. In this work, we specifically study the application of the MAP-Elites algorithm to predict polymorphs of TiO$_2$. We rediscover the known ground state, in addition to a set of other polymorphs with distinct properties. We validate our method for C, SiO$_2$ and SiC systems, where we show that the algorithm can uncover multiple local minima with distinct electronic and mechanical properties.","sentences":["The identification of materials with exceptional properties is an essential objective to enable technological progress.","We propose the application of \\textit{Quality-Diversity} algorithms to the field of crystal structure prediction.","The objective of these algorithms is to identify a diverse set of high-performing solutions, which has been successful in a range of fields such as robotics, architecture and aeronautical engineering.","As these methods rely on a high number of evaluations, we employ machine-learning surrogate models to compute the interatomic potential and material properties that are used to guide optimisation.","Consequently, we also show the value of using neural networks to model crystal properties and enable the identification of novel composition--structure combinations.","In this work, we specifically study the application of the MAP-Elites algorithm to predict polymorphs of TiO$_2$.","We rediscover the known ground state, in addition to a set of other polymorphs with distinct properties.","We validate our method for C, SiO$_2$ and SiC systems, where we show that the algorithm can uncover multiple local minima with distinct electronic and mechanical properties."],"url":"http://arxiv.org/abs/2403.03511v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-06 07:11:02","title":"Reducing the runtime of fault-tolerant quantum simulations in chemistry through symmetry-compressed double factorization","abstract":"Quantum phase estimation based on qubitization is the state-of-the-art fault-tolerant quantum algorithm for computing ground-state energies in chemical applications. In this context, the 1-norm of the Hamiltonian plays a fundamental role in determining the total number of required iterations and also the overall computational cost. In this work, we introduce the symmetry-compressed double factorization (SCDF) approach, which combines a compressed double factorization of the Hamiltonian with the symmetry shift technique, significantly reducing the 1-norm value. The effectiveness of this approach is demonstrated numerically by considering various benchmark systems, including the FeMoco molecule, cytochrome P450, and hydrogen chains of different sizes. To compare the efficiency of SCDF to other methods in absolute terms, we estimate Toffoli gate requirements, which dominate the execution time on fault-tolerant quantum computers. For the systems considered here, SCDF leads to a sizeable reduction of the Toffoli gate count in comparison to other variants of double factorization or even tensor hypercontraction, which is usually regarded as the most efficient approach for qubitization.","sentences":["Quantum phase estimation based on qubitization is the state-of-the-art fault-tolerant quantum algorithm for computing ground-state energies in chemical applications.","In this context, the 1-norm of the Hamiltonian plays a fundamental role in determining the total number of required iterations and also the overall computational cost.","In this work, we introduce the symmetry-compressed double factorization (SCDF) approach, which combines a compressed double factorization of the Hamiltonian with the symmetry shift technique, significantly reducing the 1-norm value.","The effectiveness of this approach is demonstrated numerically by considering various benchmark systems, including the FeMoco molecule, cytochrome P450, and hydrogen chains of different sizes.","To compare the efficiency of SCDF to other methods in absolute terms, we estimate Toffoli gate requirements, which dominate the execution time on fault-tolerant quantum computers.","For the systems considered here, SCDF leads to a sizeable reduction of the Toffoli gate count in comparison to other variants of double factorization or even tensor hypercontraction, which is usually regarded as the most efficient approach for qubitization."],"url":"http://arxiv.org/abs/2403.03502v1","category":"quant-ph"}
{"created":"2024-03-06 07:03:35","title":"CNN-based End-to-End Adaptive Controller with Stability Guarantees","abstract":"This letter proposes a convolutional neural network (CNN)-based adaptive controller wtih three notable features: 1) it determines control input directly from historical sensor data (in an end-to-end process); 2) it learns the desired control policy during real-time implementation without using a pretrained network (in an online adaptive manner); and 3) the asymptotic tracking error convergence is proven during the learning process (to deliver a stability guarantee). An adaptive law for learning the desired control policy is derived using the gradient descent optimization method, and its stability is analyzed based on the Lyapunov approach. A simulation study using a control-affine nonlinear system demonstrated that the proposed controller exhibits these features, and its performance can be tuned by manipulating the design parameters. In addition, it is shown that the proposed controller has a superior tracking performance to that of a deep neural network (DNN)-based adaptive controller.","sentences":["This letter proposes a convolutional neural network (CNN)-based adaptive controller wtih three notable features:","1) it determines control input directly from historical sensor data (in an end-to-end process); 2) it learns the desired control policy during real-time implementation without using a pretrained network (in an online adaptive manner); and 3) the asymptotic tracking error convergence is proven during the learning process (to deliver a stability guarantee).","An adaptive law for learning the desired control policy is derived using the gradient descent optimization method, and its stability is analyzed based on the Lyapunov approach.","A simulation study using a control-affine nonlinear system demonstrated that the proposed controller exhibits these features, and its performance can be tuned by manipulating the design parameters.","In addition, it is shown that the proposed controller has a superior tracking performance to that of a deep neural network (DNN)-based adaptive controller."],"url":"http://arxiv.org/abs/2403.03499v1","category":"eess.SY"}
{"created":"2024-03-06 06:54:02","title":"A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation","abstract":"Knowledge-based, open-domain dialogue generation aims to build chit-chat systems that talk to humans using mined support knowledge. Many types and sources of knowledge have previously been shown to be useful as support knowledge. Even in the era of large language models, response generation grounded in knowledge retrieved from additional up-to-date sources remains a practically important approach. While prior work using single-source knowledge has shown a clear positive correlation between the performances of knowledge selection and response generation, there are no existing multi-source datasets for evaluating support knowledge retrieval. Further, prior work has assumed that the knowledge sources available at test time are the same as during training. This unrealistic assumption unnecessarily handicaps models, as new knowledge sources can become available after a model is trained. In this paper, we present a high-quality benchmark named multi-source Wizard of Wikipedia (Ms.WoW) for evaluating multi-source dialogue knowledge selection and response generation. Unlike existing datasets, it contains clean support knowledge, grounded at the utterance level and partitioned into multiple knowledge sources. We further propose a new challenge, dialogue knowledge plug-and-play, which aims to test an already trained dialogue model on using new support knowledge from previously unseen sources in a zero-shot fashion.","sentences":["Knowledge-based, open-domain dialogue generation aims to build chit-chat systems that talk to humans using mined support knowledge.","Many types and sources of knowledge have previously been shown to be useful as support knowledge.","Even in the era of large language models, response generation grounded in knowledge retrieved from additional up-to-date sources remains a practically important approach.","While prior work using single-source knowledge has shown a clear positive correlation between the performances of knowledge selection and response generation, there are no existing multi-source datasets for evaluating support knowledge retrieval.","Further, prior work has assumed that the knowledge sources available at test time are the same as during training.","This unrealistic assumption unnecessarily handicaps models, as new knowledge sources can become available after a model is trained.","In this paper, we present a high-quality benchmark named multi-source Wizard of Wikipedia (Ms.WoW) for evaluating multi-source dialogue knowledge selection and response generation.","Unlike existing datasets, it contains clean support knowledge, grounded at the utterance level and partitioned into multiple knowledge sources.","We further propose a new challenge, dialogue knowledge plug-and-play, which aims to test an already trained dialogue model on using new support knowledge from previously unseen sources in a zero-shot fashion."],"url":"http://arxiv.org/abs/2403.03496v1","category":"cs.CL"}
{"created":"2024-03-06 06:35:33","title":"Joint User Association and Resource Allocation for Tailored QoS Provisioning in 6G HetNets","abstract":"The proliferation of wireless-enabled applications with divergent quality of service (QoS) requirements necessitates tailored QoS provisioning. With the growing complexity of wireless infrastructures, application-specific QoS perceived by a user equipment (UE) is jointly determined by its association with the supporting base station in heterogeneous networks (HetNets) and the amount of resource allocated to it. However, conventional application-agnostic objective-based user association and resource allocation often ignore the differences among applications' specific requirements for resources, inevitably preventing tailored QoS provisioning. Hence, in this paper, the problem of joint user association and resource allocation with application-specific objectives is investigated for achieving tailored QoS provisioning in 6G HetNets. This problem is intrinsically difficult to solve directly due to the extremely large solution space and the combination of discrete and continuous variables. Therefore, we decompose the original problem into two subproblems, i.e. user association and resource allocation, and propose an interactive optimization algorithm (IOA) to solve them iteratively in an interactive way until convergence is achieved. Specifically, matching theory is utilized to solve resource allocation and user association is solved heuristically. Extensive experimental results confirm that IOA algorithm outperforms several baseline algorithms in terms of both average utility and UE satisfaction ratio.","sentences":["The proliferation of wireless-enabled applications with divergent quality of service (QoS) requirements necessitates tailored QoS provisioning.","With the growing complexity of wireless infrastructures, application-specific QoS perceived by a user equipment (UE) is jointly determined by its association with the supporting base station in heterogeneous networks (HetNets) and the amount of resource allocated to it.","However, conventional application-agnostic objective-based user association and resource allocation often ignore the differences among applications' specific requirements for resources, inevitably preventing tailored QoS provisioning.","Hence, in this paper, the problem of joint user association and resource allocation with application-specific objectives is investigated for achieving tailored QoS provisioning in 6G HetNets.","This problem is intrinsically difficult to solve directly due to the extremely large solution space and the combination of discrete and continuous variables.","Therefore, we decompose the original problem into two subproblems, i.e. user association and resource allocation, and propose an interactive optimization algorithm (IOA) to solve them iteratively in an interactive way until convergence is achieved.","Specifically, matching theory is utilized to solve resource allocation and user association is solved heuristically.","Extensive experimental results confirm that IOA algorithm outperforms several baseline algorithms in terms of both average utility and UE satisfaction ratio."],"url":"http://arxiv.org/abs/2403.03492v1","category":"cs.NI"}
{"created":"2024-03-06 06:20:19","title":"Global Geolocated Realtime Data of Interfleet Urban Transit Bus Idling","abstract":"Urban transit bus idling is a contributor to ecological stress, economic inefficiency, and medically hazardous health outcomes due to emissions. The global accumulation of this frequent pattern of undesirable driving behavior is enormous. In order to measure its scale, we propose GRD-TRT- BUF-4I (Ground Truth Buffer for Idling) an extensible, realtime detection system that records the geolocation and idling duration of urban transit bus fleets internationally. Using live vehicle locations from General Transit Feed Specification (GTFS) Realtime, the system detects approximately 200,000 idling events per day from over 50 cities across North America, Europe, Oceania, and Asia. This realtime data was created to dynamically serve operational decision-making and fleet management to reduce the frequency and duration of idling events as they occur, as well as to capture its accumulative effects. Civil and Transportation Engineers, Urban Planners, Epidemiologists, Policymakers, and other stakeholders might find this useful for emissions modeling, traffic management, route planning, and other urban sustainability efforts at a variety of geographic and temporal scales.","sentences":["Urban transit bus idling is a contributor to ecological stress, economic inefficiency, and medically hazardous health outcomes due to emissions.","The global accumulation of this frequent pattern of undesirable driving behavior is enormous.","In order to measure its scale, we propose GRD-TRT- BUF-4I (Ground Truth Buffer for Idling) an extensible, realtime detection system that records the geolocation and idling duration of urban transit bus fleets internationally.","Using live vehicle locations from General Transit Feed Specification (GTFS)","Realtime, the system detects approximately 200,000 idling events per day from over 50 cities across North America, Europe, Oceania, and Asia.","This realtime data was created to dynamically serve operational decision-making and fleet management to reduce the frequency and duration of idling events as they occur, as well as to capture its accumulative effects.","Civil and Transportation Engineers, Urban Planners, Epidemiologists, Policymakers, and other stakeholders might find this useful for emissions modeling, traffic management, route planning, and other urban sustainability efforts at a variety of geographic and temporal scales."],"url":"http://arxiv.org/abs/2403.03489v1","category":"eess.SY"}
{"created":"2024-03-06 06:12:56","title":"Fast, nonlocal and neural: a lightweight high quality solution to image denoising","abstract":"With the widespread application of convolutional neural networks (CNNs), the traditional model based denoising algorithms are now outperformed. However, CNNs face two problems. First, they are computationally demanding, which makes their deployment especially difficult for mobile terminals. Second, experimental evidence shows that CNNs often over-smooth regular textures present in images, in contrast to traditional non-local models. In this letter, we propose a solution to both issues by combining a nonlocal algorithm with a lightweight residual CNN. This solution gives full latitude to the advantages of both models. We apply this framework to two GPU implementations of classic nonlocal algorithms (NLM and BM3D) and observe a substantial gain in both cases, performing better than the state-of-the-art with low computational requirements. Our solution is between 10 and 20 times faster than CNNs with equivalent performance and attains higher PSNR. In addition the final method shows a notable gain on images containing complex textures like the ones of the MIT Moire dataset.","sentences":["With the widespread application of convolutional neural networks (CNNs), the traditional model based denoising algorithms are now outperformed.","However, CNNs face two problems.","First, they are computationally demanding, which makes their deployment especially difficult for mobile terminals.","Second, experimental evidence shows that CNNs often over-smooth regular textures present in images, in contrast to traditional non-local models.","In this letter, we propose a solution to both issues by combining a nonlocal algorithm with a lightweight residual CNN.","This solution gives full latitude to the advantages of both models.","We apply this framework to two GPU implementations of classic nonlocal algorithms (NLM and BM3D) and observe a substantial gain in both cases, performing better than the state-of-the-art with low computational requirements.","Our solution is between 10 and 20 times faster than CNNs with equivalent performance and attains higher PSNR.","In addition the final method shows a notable gain on images containing complex textures like the ones of the MIT Moire dataset."],"url":"http://arxiv.org/abs/2403.03488v1","category":"eess.IV"}
{"created":"2024-03-06 06:04:32","title":"PhenoAuth: A Novel PUF-Phenotype-based Authentication Protocol for IoT Devices","abstract":"Physical Unclonable Functions (PUFs) have been shown to be a highly promising solution for enabling high security systems tailored for low-power devices. Commonly, PUFs are utilised to generate cryptographic keys on-the-fly, replacing the need to store keys in vulnerable, non-volatile memories. Due to the physical nature of PUFs, environmental variations cause noise, manifesting themselves as errors which are apparent in the initial PUF measurements. This necessitates expensive active error correction techniques which can run counter to the goal of lightweight security. ML-based techniques for authenticating noisy PUF measurements were explored as an alternative to error correction techniques, bringing about the concept of a PUF Phenotype, where PUF identity is considered as a structure agnostic representation of the PUF, with relevant noise encoding. This work proposes a full noise-tolerant authentication protocol based on the PUF Phenotype concept and methodology for an Internet-of-Things (IoT) network, demonstrating mutual authentication and forward secrecy in a setting suitable for device-to-device communication. Upon conducting security and performance analyses, it is evident that our proposed scheme demonstrates resilience against various attacks compared to the currently existing PUF protocols.","sentences":["Physical Unclonable Functions (PUFs) have been shown to be a highly promising solution for enabling high security systems tailored for low-power devices.","Commonly, PUFs are utilised to generate cryptographic keys on-the-fly, replacing the need to store keys in vulnerable, non-volatile memories.","Due to the physical nature of PUFs, environmental variations cause noise, manifesting themselves as errors which are apparent in the initial PUF measurements.","This necessitates expensive active error correction techniques which can run counter to the goal of lightweight security.","ML-based techniques for authenticating noisy PUF measurements were explored as an alternative to error correction techniques, bringing about the concept of a PUF Phenotype, where PUF identity is considered as a structure agnostic representation of the PUF, with relevant noise encoding.","This work proposes a full noise-tolerant authentication protocol based on the PUF Phenotype concept and methodology for an Internet-of-Things (IoT) network, demonstrating mutual authentication and forward secrecy in a setting suitable for device-to-device communication.","Upon conducting security and performance analyses, it is evident that our proposed scheme demonstrates resilience against various attacks compared to the currently existing PUF protocols."],"url":"http://arxiv.org/abs/2403.03486v1","category":"cs.CR"}
{"created":"2024-03-06 05:37:18","title":"Observation of counterflow superfluidity in a two-component Mott insulator","abstract":"The counterflow superfluidity (CSF) was predicted two decades ago. Counterintuitively, while both components in the CSF have fluidity, their correlated counterflow currents cancel out leading the overall system to an incompressible Mott insulator. However, realizing and identifying the CSF remain challenging due to the request on extreme experimental capabilities in a single setup. Here, we observe the CSF in a binary Bose mixture in optical lattices. We prepare a low-entropy spin-Mott state by conveying and merging two spin-1/2 bosonic atoms at every site and drive it adiabatically to the CSF at $\\sim$ 1 nK. Antipair correlations of the CSF are probed though a site- and spin-resolved quantum gas microscope in both real and momentum spaces. These techniques and observations provide accessibility to the symmetry-protected topological quantum matters.","sentences":["The counterflow superfluidity (CSF) was predicted two decades ago.","Counterintuitively, while both components in the CSF have fluidity, their correlated counterflow currents cancel out leading the overall system to an incompressible Mott insulator.","However, realizing and identifying the CSF remain challenging due to the request on extreme experimental capabilities in a single setup.","Here, we observe the CSF in a binary Bose mixture in optical lattices.","We prepare a low-entropy spin-Mott state by conveying and merging two spin-1/2 bosonic atoms at every site and drive it adiabatically to the CSF at $\\sim$ 1 nK.","Antipair correlations of the CSF are probed though a site- and spin-resolved quantum gas microscope in both real and momentum spaces.","These techniques and observations provide accessibility to the symmetry-protected topological quantum matters."],"url":"http://arxiv.org/abs/2403.03479v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-06 05:34:13","title":"Hermitian-preserving ansatz and variational open quantum eigensolver","abstract":"We propose a new variational quantum algorithm named Variational Open Quantum Eigensolver (VOQE) for solving steady states of open quantum systems described by either Lindblad master equations or non-Hermitian Hamiltonians. In VOQE, density matrices of mixed states are represented by pure states in doubled Hilbert space. We give a framework for building circuit ansatz which we call the Hermitian-preserving ansatz (HPA) to restrict the searching space. We also give a method to efficiently measure the operators' expectation values by post-selection measurements. We show the workflow of VOQE on solving steady states of the LMEs of the driven XXZ model and implement VOQE to solve the spectrum of the non-Hermitian Hamiltonians of the Ising spin chain in an imaginary field.","sentences":["We propose a new variational quantum algorithm named Variational Open Quantum Eigensolver (VOQE) for solving steady states of open quantum systems described by either Lindblad master equations or non-Hermitian Hamiltonians.","In VOQE, density matrices of mixed states are represented by pure states in doubled Hilbert space.","We give a framework for building circuit ansatz which we call the Hermitian-preserving ansatz (HPA) to restrict the searching space.","We also give a method to efficiently measure the operators' expectation values by post-selection measurements.","We show the workflow of VOQE on solving steady states of the LMEs of the driven XXZ model and implement VOQE to solve the spectrum of the non-Hermitian Hamiltonians of the Ising spin chain in an imaginary field."],"url":"http://arxiv.org/abs/2403.03478v1","category":"quant-ph"}
{"created":"2024-03-06 05:13:28","title":"Inverse-Free Fast Natural Gradient Descent Method for Deep Learning","abstract":"Second-order methods can converge much faster than first-order methods by incorporating second-order derivates or statistics, but they are far less prevalent in deep learning due to their computational inefficiency. To handle this, many of the existing solutions focus on reducing the size of the matrix to be inverted. However, it is still needed to perform the inverse operator in each iteration. In this paper, we present a fast natural gradient descent (FNGD) method, which only requires computing the inverse during the first epoch. Firstly, we reformulate the gradient preconditioning formula in the natural gradient descent (NGD) as a weighted sum of per-sample gradients using the Sherman-Morrison-Woodbury formula. Building upon this, to avoid the iterative inverse operation involved in computing coefficients, the weighted coefficients are shared across epochs without affecting the empirical performance.   FNGD approximates the NGD as a fixed-coefficient weighted sum, akin to the average sum in first-order methods. Consequently, the computational complexity of FNGD can approach that of first-order methods. To demonstrate the efficiency of the proposed FNGD, we perform empirical evaluations on image classification and machine translation tasks. For training ResNet-18 on the CIFAR-100 dataset, FNGD can achieve a speedup of 2.05$\\times$ compared with KFAC. For training Transformer on Multi30K, FNGD outperforms AdamW by 24 BLEU score while requiring almost the same training time.","sentences":["Second-order methods can converge much faster than first-order methods by incorporating second-order derivates or statistics, but they are far less prevalent in deep learning due to their computational inefficiency.","To handle this, many of the existing solutions focus on reducing the size of the matrix to be inverted.","However, it is still needed to perform the inverse operator in each iteration.","In this paper, we present a fast natural gradient descent (FNGD) method, which only requires computing the inverse during the first epoch.","Firstly, we reformulate the gradient preconditioning formula in the natural gradient descent (NGD) as a weighted sum of per-sample gradients using the Sherman-Morrison-Woodbury formula.","Building upon this, to avoid the iterative inverse operation involved in computing coefficients, the weighted coefficients are shared across epochs without affecting the empirical performance.   ","FNGD approximates the NGD as a fixed-coefficient weighted sum, akin to the average sum in first-order methods.","Consequently, the computational complexity of FNGD can approach that of first-order methods.","To demonstrate the efficiency of the proposed FNGD, we perform empirical evaluations on image classification and machine translation tasks.","For training ResNet-18 on the CIFAR-100 dataset, FNGD can achieve a speedup of 2.05$\\times$ compared with KFAC.","For training Transformer on Multi30K, FNGD outperforms AdamW by 24 BLEU score while requiring almost the same training time."],"url":"http://arxiv.org/abs/2403.03473v1","category":"cs.LG"}
{"created":"2024-03-06 05:13:23","title":"Boosting Meta-Training with Base Class Information for Few-Shot Learning","abstract":"Few-shot learning, a challenging task in machine learning, aims to learn a classifier adaptable to recognize new, unseen classes with limited labeled examples. Meta-learning has emerged as a prominent framework for few-shot learning. Its training framework is originally a task-level learning method, such as Model-Agnostic Meta-Learning (MAML) and Prototypical Networks. And a recently proposed training paradigm called Meta-Baseline, which consists of sequential pre-training and meta-training stages, gains state-of-the-art performance. However, as a non-end-to-end training method, indicating the meta-training stage can only begin after the completion of pre-training, Meta-Baseline suffers from higher training cost and suboptimal performance due to the inherent conflicts of the two training stages. To address these limitations, we propose an end-to-end training paradigm consisting of two alternative loops. In the outer loop, we calculate cross entropy loss on the entire training set while updating only the final linear layer. In the inner loop, we employ the original meta-learning training mode to calculate the loss and incorporate gradients from the outer loss to guide the parameter updates. This training paradigm not only converges quickly but also outperforms existing baselines, indicating that information from the overall training set and the meta-learning training paradigm could mutually reinforce one another. Moreover, being model-agnostic, our framework achieves significant performance gains, surpassing the baseline systems by approximate 1%.","sentences":["Few-shot learning, a challenging task in machine learning, aims to learn a classifier adaptable to recognize new, unseen classes with limited labeled examples.","Meta-learning has emerged as a prominent framework for few-shot learning.","Its training framework is originally a task-level learning method, such as Model-Agnostic Meta-Learning (MAML) and Prototypical Networks.","And a recently proposed training paradigm called Meta-Baseline, which consists of sequential pre-training and meta-training stages, gains state-of-the-art performance.","However, as a non-end-to-end training method, indicating the meta-training stage can only begin after the completion of pre-training, Meta-Baseline suffers from higher training cost and suboptimal performance due to the inherent conflicts of the two training stages.","To address these limitations, we propose an end-to-end training paradigm consisting of two alternative loops.","In the outer loop, we calculate cross entropy loss on the entire training set while updating only the final linear layer.","In the inner loop, we employ the original meta-learning training mode to calculate the loss and incorporate gradients from the outer loss to guide the parameter updates.","This training paradigm not only converges quickly but also outperforms existing baselines, indicating that information from the overall training set and the meta-learning training paradigm could mutually reinforce one another.","Moreover, being model-agnostic, our framework achieves significant performance gains, surpassing the baseline systems by approximate 1%."],"url":"http://arxiv.org/abs/2403.03472v1","category":"cs.LG"}
{"created":"2024-03-06 05:06:24","title":"Burnett's conjecture in generalized wave coordinates","abstract":"We prove Burnett's conjecture in general relativity when the metrics satisfy a generalized wave coordinate condition, i.e., suppose $\\{g_n\\}_{n=1}^\\infty$ is a sequence of Lorentzian metrics (in arbitrary dimensions $d \\geq 3$) satisfying a generalized wave coordinate condition and such that $g_n\\to g$ in a suitably weak and \"high-frequency\" manner, then the limit metric $g$ satisfies the Einstein--massless Vlasov system. Moreover, we show that the Vlasov field for the limiting metric can be taken to be a suitable microlocal defect measure corresponding to the convergence. The proof uses a compensation phenomenon based on the linear and nonlinear structure of the Einstein equations.","sentences":["We prove Burnett's conjecture in general relativity when the metrics satisfy a generalized wave coordinate condition, i.e., suppose $\\{g_n\\}_{n=1}^\\infty$ is a sequence of Lorentzian metrics (in arbitrary dimensions $d \\geq 3$) satisfying a generalized wave coordinate condition and such that $g_n\\to g$ in a suitably weak and \"high-frequency\" manner, then the limit metric $g$ satisfies the Einstein--massless Vlasov system.","Moreover, we show that the Vlasov field for the limiting metric can be taken to be a suitable microlocal defect measure corresponding to the convergence.","The proof uses a compensation phenomenon based on the linear and nonlinear structure of the Einstein equations."],"url":"http://arxiv.org/abs/2403.03470v1","category":"gr-qc"}
{"created":"2024-03-06 05:04:45","title":"Exponential learning advantages with conjugate states and minimal quantum memory","abstract":"The ability of quantum computers to directly manipulate and analyze quantum states stored in quantum memory allows them to learn about aspects of our physical world that would otherwise be invisible given a modest number of measurements. Here we investigate a new learning resource which could be available to quantum computers in the future -- measurements on the unknown state accompanied by its complex conjugate $\\rho \\otimes \\rho^\\ast$. For a certain shadow tomography task, we surprisingly find that measurements on only copies of $\\rho \\otimes \\rho^\\ast$ can be exponentially more powerful than measurements on $\\rho^{\\otimes K}$, even for large $K$. This expands the class of provable exponential advantages using only a constant overhead quantum memory, or minimal quantum memory, and we provide a number of examples where the state $\\rho^\\ast$ is naturally available in both computational and physical applications. In addition, we precisely quantify the power of classical shadows on single copies under a generalized Clifford ensemble and give a class of quantities that can be efficiently learned. The learning task we study in both the single copy and quantum memory settings is physically natural and corresponds to real-space observables with a limit of bosonic modes, where it achieves an exponential improvement in detecting certain signals under a noisy background. We quantify a new and powerful resource in quantum learning, and we believe the advantage may find applications in improving quantum simulation, learning from quantum sensors, and uncovering new physical phenomena.","sentences":["The ability of quantum computers to directly manipulate and analyze quantum states stored in quantum memory allows them to learn about aspects of our physical world that would otherwise be invisible given a modest number of measurements.","Here we investigate a new learning resource which could be available to quantum computers in the future -- measurements on the unknown state accompanied by its complex conjugate $\\rho \\otimes \\rho^\\ast$. For a certain shadow tomography task, we surprisingly find that measurements on only copies of $\\rho \\otimes \\rho^\\ast$ can be exponentially more powerful than measurements on $\\rho^{\\otimes K}$, even for large $K$. This expands the class of provable exponential advantages using only a constant overhead quantum memory, or minimal quantum memory, and we provide a number of examples where the state $\\rho^\\ast$ is naturally available in both computational and physical applications.","In addition, we precisely quantify the power of classical shadows on single copies under a generalized Clifford ensemble and give a class of quantities that can be efficiently learned.","The learning task we study in both the single copy and quantum memory settings is physically natural and corresponds to real-space observables with a limit of bosonic modes, where it achieves an exponential improvement in detecting certain signals under a noisy background.","We quantify a new and powerful resource in quantum learning, and we believe the advantage may find applications in improving quantum simulation, learning from quantum sensors, and uncovering new physical phenomena."],"url":"http://arxiv.org/abs/2403.03469v1","category":"quant-ph"}
{"created":"2024-03-06 05:04:40","title":"Multi-task Learning for Real-time Autonomous Driving Leveraging Task-adaptive Attention Generator","abstract":"Real-time processing is crucial in autonomous driving systems due to the imperative of instantaneous decision-making and rapid response. In real-world scenarios, autonomous vehicles are continuously tasked with interpreting their surroundings, analyzing intricate sensor data, and making decisions within split seconds to ensure safety through numerous computer vision tasks. In this paper, we present a new real-time multi-task network adept at three vital autonomous driving tasks: monocular 3D object detection, semantic segmentation, and dense depth estimation. To counter the challenge of negative transfer, which is the prevalent issue in multi-task learning, we introduce a task-adaptive attention generator. This generator is designed to automatically discern interrelations across the three tasks and arrange the task-sharing pattern, all while leveraging the efficiency of the hard-parameter sharing approach. To the best of our knowledge, the proposed model is pioneering in its capability to concurrently handle multiple tasks, notably 3D object detection, while maintaining real-time processing speeds. Our rigorously optimized network, when tested on the Cityscapes-3D datasets, consistently outperforms various baseline models. Moreover, an in-depth ablation study substantiates the efficacy of the methodologies integrated into our framework.","sentences":["Real-time processing is crucial in autonomous driving systems due to the imperative of instantaneous decision-making and rapid response.","In real-world scenarios, autonomous vehicles are continuously tasked with interpreting their surroundings, analyzing intricate sensor data, and making decisions within split seconds to ensure safety through numerous computer vision tasks.","In this paper, we present a new real-time multi-task network adept at three vital autonomous driving tasks: monocular 3D object detection, semantic segmentation, and dense depth estimation.","To counter the challenge of negative transfer, which is the prevalent issue in multi-task learning, we introduce a task-adaptive attention generator.","This generator is designed to automatically discern interrelations across the three tasks and arrange the task-sharing pattern, all while leveraging the efficiency of the hard-parameter sharing approach.","To the best of our knowledge, the proposed model is pioneering in its capability to concurrently handle multiple tasks, notably 3D object detection, while maintaining real-time processing speeds.","Our rigorously optimized network, when tested on the Cityscapes-3D datasets, consistently outperforms various baseline models.","Moreover, an in-depth ablation study substantiates the efficacy of the methodologies integrated into our framework."],"url":"http://arxiv.org/abs/2403.03468v1","category":"cs.CV"}
{"created":"2024-03-06 05:04:09","title":"Multimode Quantum Correlations in Supercontinuum Pulses","abstract":"Suprecontinuum (SC) light contains complex spectral noise structure and its accurate characterization is important for fundamental understanding of its physics as well as for its applications. Several experimental and theoretical noise characterizations have been performed so far. However, none of them takes into account the quantum mechanical properties. Here, we demonstrate experimental characterisation of quantum noise and its spectral correlations formed in the SC light generated from a photonic crystal fiber. Moreover, by applying an appropriate basis transformation to these correlations, we demonstrate that the SC noise amplitude can be squeezed below the shot-noise limit in some bases, even in the presence of excessively large nonlinearities.","sentences":["Suprecontinuum (SC) light contains complex spectral noise structure and its accurate characterization is important for fundamental understanding of its physics as well as for its applications.","Several experimental and theoretical noise characterizations have been performed so far.","However, none of them takes into account the quantum mechanical properties.","Here, we demonstrate experimental characterisation of quantum noise and its spectral correlations formed in the SC light generated from a photonic crystal fiber.","Moreover, by applying an appropriate basis transformation to these correlations, we demonstrate that the SC noise amplitude can be squeezed below the shot-noise limit in some bases, even in the presence of excessively large nonlinearities."],"url":"http://arxiv.org/abs/2403.03467v1","category":"quant-ph"}
{"created":"2024-03-06 05:02:57","title":"Systematic Improvement of DMC Calculations in Transition Metal Oxides: sCI-Driven Wavefunction Optimization for Reliable Band Gaps predictions","abstract":"Accurate determination of electronic properties of correlated oxides remains a significant challenge for computational theory. Traditional Hubbard-corrected density functional theory (DFT+U) frequently encounters limitations in precisely capturing electron correlation, particularly when predicting band gaps. We introduce a systematic methodology to enhance the accuracy of diffusion Monte Carlo (DMC) simulations for both ground and excited states, focusing on LiCoO$_2$ as a case study. By employing a selected CI (sCI) approach, we demonstrate the capability to optimize wavefunctions beyond the constraints of single-reference DFT+U trial wavefunctions. We show that the sCI framework enables accurate prediction of band gaps in LiCoO$_2$, closely aligning with experimental values and substantially improving upon traditional computational methods. The study uncovers a nuanced mixed state of $t_{2g}$ a $e_g$ orbitals at the band edges that is not captured by conventional single-reference methods, further elucidating the limitations of PBE+U in describing $d$-$d$ excitations. Our findings advocate for the adoption of beyond-DFT methodologies, such as sCI, to capture the essential physics of excited state wavefunctions in strongly correlated materials. The improved accuracy in band gap predictions and the ability to generate more reliable trial wavefunctions for DMC calculations underscore the potential of this approach for broader applications in the study of correlated oxides. This work not only provides a pathway for more accurate simulations of electronic structures in complex materials but also suggests a framework for future investigations into the excited states of other challenging systems.","sentences":["Accurate determination of electronic properties of correlated oxides remains a significant challenge for computational theory.","Traditional Hubbard-corrected density functional theory (DFT+U) frequently encounters limitations in precisely capturing electron correlation, particularly when predicting band gaps.","We introduce a systematic methodology to enhance the accuracy of diffusion Monte Carlo (DMC) simulations for both ground and excited states, focusing on LiCoO$_2$ as a case study.","By employing a selected CI (sCI) approach, we demonstrate the capability to optimize wavefunctions beyond the constraints of single-reference DFT+U trial wavefunctions.","We show that the sCI framework enables accurate prediction of band gaps in LiCoO$_2$, closely aligning with experimental values and substantially improving upon traditional computational methods.","The study uncovers a nuanced mixed state of $t_{2g}$ a $e_g$ orbitals at the band edges that is not captured by conventional single-reference methods, further elucidating the limitations of PBE+U in describing $d$-$d$ excitations.","Our findings advocate for the adoption of beyond-DFT methodologies, such as sCI, to capture the essential physics of excited state wavefunctions in strongly correlated materials.","The improved accuracy in band gap predictions and the ability to generate more reliable trial wavefunctions for DMC calculations underscore the potential of this approach for broader applications in the study of correlated oxides.","This work not only provides a pathway for more accurate simulations of electronic structures in complex materials but also suggests a framework for future investigations into the excited states of other challenging systems."],"url":"http://arxiv.org/abs/2403.03466v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-06 04:55:39","title":"Interactive Continual Learning Architecture for Long-Term Personalization of Home Service Robots","abstract":"For robots to perform assistive tasks in unstructured home environments, they must learn and reason on the semantic knowledge of the environments. Despite a resurgence in the development of semantic reasoning architectures, these methods assume that all the training data is available a priori. However, each user's environment is unique and can continue to change over time, which makes these methods unsuitable for personalized home service robots. Although research in continual learning develops methods that can learn and adapt over time, most of these methods are tested in the narrow context of object classification on static image datasets. In this paper, we combine ideas from continual learning, semantic reasoning, and interactive machine learning literature and develop a novel interactive continual learning architecture for continual learning of semantic knowledge in a home environment through human-robot interaction. The architecture builds on core cognitive principles of learning and memory for efficient and real-time learning of new knowledge from humans. We integrate our architecture with a physical mobile manipulator robot and perform extensive system evaluations in a laboratory environment over two months. Our results demonstrate the effectiveness of our architecture to allow a physical robot to continually adapt to the changes in the environment from limited data provided by the users (experimenters), and use the learned knowledge to perform object fetching tasks.","sentences":["For robots to perform assistive tasks in unstructured home environments, they must learn and reason on the semantic knowledge of the environments.","Despite a resurgence in the development of semantic reasoning architectures, these methods assume that all the training data is available a priori.","However, each user's environment is unique and can continue to change over time, which makes these methods unsuitable for personalized home service robots.","Although research in continual learning develops methods that can learn and adapt over time, most of these methods are tested in the narrow context of object classification on static image datasets.","In this paper, we combine ideas from continual learning, semantic reasoning, and interactive machine learning literature and develop a novel interactive continual learning architecture for continual learning of semantic knowledge in a home environment through human-robot interaction.","The architecture builds on core cognitive principles of learning and memory for efficient and real-time learning of new knowledge from humans.","We integrate our architecture with a physical mobile manipulator robot and perform extensive system evaluations in a laboratory environment over two months.","Our results demonstrate the effectiveness of our architecture to allow a physical robot to continually adapt to the changes in the environment from limited data provided by the users (experimenters), and use the learned knowledge to perform object fetching tasks."],"url":"http://arxiv.org/abs/2403.03462v1","category":"cs.RO"}
{"created":"2024-03-06 04:46:17","title":"Scrambling Transition in Free Fermion Systems Induced by a Single Impurity","abstract":"In quantum many-body systems, interactions play a crucial role in the emergence of information scrambling. When particles interact throughout the system, the entanglement between them can lead to a rapid and chaotic spreading of quantum information, typically probed by the growth in operator size in the Heisenberg picture. In this study, we explore whether the operator undergoes scrambling when particles interact solely through a single impurity in generic spatial dimensions, focusing on fermion systems with spatial and temporal random hoppings. By connecting the dynamics of the operator to the symmetric exclusion process with a source term, we demonstrate the presence of an escape-to-scrambling transition when tuning the interaction strength for fermions in three dimensions. As a comparison, systems in lower dimensions are proven to scramble at arbitrarily weak interactions unless the hopping becomes sufficiently long-ranged. Our predictions are validated using both a Brownian circuit with a single Majorana fermion per site and a solvable Brownian SYK model with a large local Hilbert space dimension. This suggests the universality of the theoretical picture for free fermion systems with spatial and temporal randomness.","sentences":["In quantum many-body systems, interactions play a crucial role in the emergence of information scrambling.","When particles interact throughout the system, the entanglement between them can lead to a rapid and chaotic spreading of quantum information, typically probed by the growth in operator size in the Heisenberg picture.","In this study, we explore whether the operator undergoes scrambling when particles interact solely through a single impurity in generic spatial dimensions, focusing on fermion systems with spatial and temporal random hoppings.","By connecting the dynamics of the operator to the symmetric exclusion process with a source term, we demonstrate the presence of an escape-to-scrambling transition when tuning the interaction strength for fermions in three dimensions.","As a comparison, systems in lower dimensions are proven to scramble at arbitrarily weak interactions unless the hopping becomes sufficiently long-ranged.","Our predictions are validated using both a Brownian circuit with a single Majorana fermion per site and a solvable Brownian SYK model with a large local Hilbert space dimension.","This suggests the universality of the theoretical picture for free fermion systems with spatial and temporal randomness."],"url":"http://arxiv.org/abs/2403.03457v1","category":"quant-ph"}
{"created":"2024-03-06 04:45:06","title":"Robust Control Lyapunov-Value Functions for Nonlinear Disturbed Systems","abstract":"Control Lyapunov Functions (CLFs) have been extensively used in the control community. A well-known drawback is the absence of a systematic way to construct CLFs for general nonlinear systems, and the problem can become more complex with input or state constraints. Our preliminary work on constructing Control Lyapunov Value Functions (CLVFs) using Hamilton-Jacobi (HJ) reachability analysis provides a method for finding a non-smooth CLF. In this paper, we extend our work on CLVFs to systems with bounded disturbance and define the Robust CLVF (R-CLVF). The R-CLVF naturally inherits all properties of the CLVF; i.e., it first identifies the \"smallest robust control invariant set (SRCIS)\" and stabilizes the system to it with a user-specified exponential rate. The region from which the exponential rate can be met is called the \"region of exponential stabilizability (ROES).\" We provide clearer definitions of the SRCIS and more rigorous proofs of several important theorems. Since the computation of the R-CLVF suffers from the \"curse of dimensionality,\" we also provide two techniques (warmstart and system decomposition) that solve it, along with necessary proofs. Three numerical examples are provided, validating our definition of SRCIS, illustrating the trade-off between a faster decay rate and a smaller ROES, and demonstrating the efficiency of computation using warmstart and decomposition.","sentences":["Control Lyapunov Functions (CLFs) have been extensively used in the control community.","A well-known drawback is the absence of a systematic way to construct CLFs for general nonlinear systems, and the problem can become more complex with input or state constraints.","Our preliminary work on constructing Control Lyapunov Value Functions (CLVFs) using Hamilton-Jacobi (HJ) reachability analysis provides a method for finding a non-smooth CLF.","In this paper, we extend our work on CLVFs to systems with bounded disturbance and define the Robust CLVF (R-CLVF).","The R-CLVF naturally inherits all properties of the CLVF; i.e., it first identifies the \"smallest robust control invariant set (SRCIS)\" and stabilizes the system to it with a user-specified exponential rate.","The region from which the exponential rate can be met is called the \"region of exponential stabilizability (ROES).\"","We provide clearer definitions of the SRCIS and more rigorous proofs of several important theorems.","Since the computation of the R-CLVF suffers from the \"curse of dimensionality,\" we also provide two techniques (warmstart and system decomposition) that solve it, along with necessary proofs.","Three numerical examples are provided, validating our definition of SRCIS, illustrating the trade-off between a faster decay rate and a smaller ROES, and demonstrating the efficiency of computation using warmstart and decomposition."],"url":"http://arxiv.org/abs/2403.03455v1","category":"math.OC"}
{"created":"2024-03-06 04:43:22","title":"Learning Constrained Optimization with Deep Augmented Lagrangian Methods","abstract":"Learning to Optimize (LtO) is a problem setting in which a machine learning (ML) model is trained to emulate a constrained optimization solver. Learning to produce optimal and feasible solutions subject to complex constraints is a difficult task, but is often made possible by restricting the input space to a limited distribution of related problems. Most LtO methods focus on directly learning solutions to the primal problem, and applying correction schemes or loss function penalties to encourage feasibility. This paper proposes an alternative approach, in which the ML model is trained instead to predict dual solution estimates directly, from which primal estimates are constructed to form dual-feasible solution pairs. This enables an end-to-end training scheme is which the dual objective is maximized as a loss function, and solution estimates iterate toward primal feasibility, emulating a Dual Ascent method. First it is shown that the poor convergence properties of classical Dual Ascent are reflected in poor convergence of the proposed training scheme. Then, by incorporating techniques from practical Augmented Lagrangian methods, we show how the training scheme can be improved to learn highly accurate constrained optimization solvers, for both convex and nonconvex problems.","sentences":["Learning to Optimize (LtO) is a problem setting in which a machine learning (ML) model is trained to emulate a constrained optimization solver.","Learning to produce optimal and feasible solutions subject to complex constraints is a difficult task, but is often made possible by restricting the input space to a limited distribution of related problems.","Most LtO methods focus on directly learning solutions to the primal problem, and applying correction schemes or loss function penalties to encourage feasibility.","This paper proposes an alternative approach, in which the ML model is trained instead to predict dual solution estimates directly, from which primal estimates are constructed to form dual-feasible solution pairs.","This enables an end-to-end training scheme is which the dual objective is maximized as a loss function, and solution estimates iterate toward primal feasibility, emulating a Dual Ascent method.","First it is shown that the poor convergence properties of classical Dual Ascent are reflected in poor convergence of the proposed training scheme.","Then, by incorporating techniques from practical Augmented Lagrangian methods, we show how the training scheme can be improved to learn highly accurate constrained optimization solvers, for both convex and nonconvex problems."],"url":"http://arxiv.org/abs/2403.03454v1","category":"cs.LG"}
{"created":"2024-03-06 04:32:26","title":"Mechanically Designing Protected Superconducting Qubits","abstract":"Significant progress is required in the engineering of large, interacting quantum systems in order to realize the promises of gate-model quantum computing. Designing such systems is challenging, as the dynamics of continuous variable quantum systems are generally unintuitive, and brute-force numerical solutions are difficult to impossible in more than a few dimensions. In this work, I draw analogies between modern superconducting qubits and mechanical mass-spring systems in attempt to gain a simple intuition for what makes each design special. In particular, I analyze superconducting qubits that are inherently protected from noise, and connect this protection to features of the corresponding mechanical system. The hope is that intuition gained from analyzing these systems mechanically will allow for intuitive design of useful superconducting circuits in the future.","sentences":["Significant progress is required in the engineering of large, interacting quantum systems in order to realize the promises of gate-model quantum computing.","Designing such systems is challenging, as the dynamics of continuous variable quantum systems are generally unintuitive, and brute-force numerical solutions are difficult to impossible in more than a few dimensions.","In this work, I draw analogies between modern superconducting qubits and mechanical mass-spring systems in attempt to gain a simple intuition for what makes each design special.","In particular, I analyze superconducting qubits that are inherently protected from noise, and connect this protection to features of the corresponding mechanical system.","The hope is that intuition gained from analyzing these systems mechanically will allow for intuitive design of useful superconducting circuits in the future."],"url":"http://arxiv.org/abs/2403.03451v1","category":"quant-ph"}
{"created":"2024-03-06 04:30:39","title":"Time-dependent invasion laws for a liquid-liquid displacement system","abstract":"Capillary-driven flow of fluids occurs frequently in nature and has a wide range of technological applications in the fields of industry, agriculture, medicine, biotechnology, and microfluidics. By using the Onsager variational principle, we propose a model to systematically study the capillary imbibition in titled tubes, and find different laws of time-dependent capillary invasion length for liquid-liquid displacement system other than Lucas-Washburn type under different circumstances. The good agreement between our model and experimental results shows that the imbibition dynamics in a capillary tube with a prefilled liquid slug can be well captured by the dynamic equation derived in this paper. Our results bear important implications for macroscopic descriptions of multiphase flows in microfluidic systems and porous media.","sentences":["Capillary-driven flow of fluids occurs frequently in nature and has a wide range of technological applications in the fields of industry, agriculture, medicine, biotechnology, and microfluidics.","By using the Onsager variational principle, we propose a model to systematically study the capillary imbibition in titled tubes, and find different laws of time-dependent capillary invasion length for liquid-liquid displacement system other than Lucas-Washburn type under different circumstances.","The good agreement between our model and experimental results shows that the imbibition dynamics in a capillary tube with a prefilled liquid slug can be well captured by the dynamic equation derived in this paper.","Our results bear important implications for macroscopic descriptions of multiphase flows in microfluidic systems and porous media."],"url":"http://arxiv.org/abs/2403.03450v1","category":"cond-mat.soft"}
{"created":"2024-03-06 04:27:10","title":"SalienTime: User-driven Selection of Salient Time Steps for Large-Scale Geospatial Data Visualization","abstract":"The voluminous nature of geospatial temporal data from physical monitors and simulation models poses challenges to efficient data access, often resulting in cumbersome temporal selection experiences in web-based data portals. Thus, selecting a subset of time steps for prioritized visualization and pre-loading is highly desirable. Addressing this issue, this paper establishes a multifaceted definition of salient time steps via extensive need-finding studies with domain experts to understand their workflows. Building on this, we propose a novel approach that leverages autoencoders and dynamic programming to facilitate user-driven temporal selections. Structural features, statistical variations, and distance penalties are incorporated to make more flexible selections. User-specified priorities, spatial regions, and aggregations are used to combine different perspectives. We design and implement a web-based interface to enable efficient and context-aware selection of time steps and evaluate its efficacy and usability through case studies, quantitative evaluations, and expert interviews.","sentences":["The voluminous nature of geospatial temporal data from physical monitors and simulation models poses challenges to efficient data access, often resulting in cumbersome temporal selection experiences in web-based data portals.","Thus, selecting a subset of time steps for prioritized visualization and pre-loading is highly desirable.","Addressing this issue, this paper establishes a multifaceted definition of salient time steps via extensive need-finding studies with domain experts to understand their workflows.","Building on this, we propose a novel approach that leverages autoencoders and dynamic programming to facilitate user-driven temporal selections.","Structural features, statistical variations, and distance penalties are incorporated to make more flexible selections.","User-specified priorities, spatial regions, and aggregations are used to combine different perspectives.","We design and implement a web-based interface to enable efficient and context-aware selection of time steps and evaluate its efficacy and usability through case studies, quantitative evaluations, and expert interviews."],"url":"http://arxiv.org/abs/2403.03449v1","category":"cs.HC"}
{"created":"2024-03-06 04:13:29","title":"HDRFlow: Real-Time HDR Video Reconstruction with Large Motions","abstract":"Reconstructing High Dynamic Range (HDR) video from image sequences captured with alternating exposures is challenging, especially in the presence of large camera or object motion. Existing methods typically align low dynamic range sequences using optical flow or attention mechanism for deghosting. However, they often struggle to handle large complex motions and are computationally expensive. To address these challenges, we propose a robust and efficient flow estimator tailored for real-time HDR video reconstruction, named HDRFlow. HDRFlow has three novel designs: an HDR-domain alignment loss (HALoss), an efficient flow network with a multi-size large kernel (MLK), and a new HDR flow training scheme. The HALoss supervises our flow network to learn an HDR-oriented flow for accurate alignment in saturated and dark regions. The MLK can effectively model large motions at a negligible cost. In addition, we incorporate synthetic data, Sintel, into our training dataset, utilizing both its provided forward flow and backward flow generated by us to supervise our flow network, enhancing our performance in large motion regions. Extensive experiments demonstrate that our HDRFlow outperforms previous methods on standard benchmarks. To the best of our knowledge, HDRFlow is the first real-time HDR video reconstruction method for video sequences captured with alternating exposures, capable of processing 720p resolution inputs at 25ms.","sentences":["Reconstructing High Dynamic Range (HDR) video from image sequences captured with alternating exposures is challenging, especially in the presence of large camera or object motion.","Existing methods typically align low dynamic range sequences using optical flow or attention mechanism for deghosting.","However, they often struggle to handle large complex motions and are computationally expensive.","To address these challenges, we propose a robust and efficient flow estimator tailored for real-time HDR video reconstruction, named HDRFlow.","HDRFlow has three novel designs: an HDR-domain alignment loss (HALoss), an efficient flow network with a multi-size large kernel (MLK), and a new HDR flow training scheme.","The HALoss supervises our flow network to learn an HDR-oriented flow for accurate alignment in saturated and dark regions.","The MLK can effectively model large motions at a negligible cost.","In addition, we incorporate synthetic data, Sintel, into our training dataset, utilizing both its provided forward flow and backward flow generated by us to supervise our flow network, enhancing our performance in large motion regions.","Extensive experiments demonstrate that our HDRFlow outperforms previous methods on standard benchmarks.","To the best of our knowledge, HDRFlow is the first real-time HDR video reconstruction method for video sequences captured with alternating exposures, capable of processing 720p resolution inputs at 25ms."],"url":"http://arxiv.org/abs/2403.03447v1","category":"cs.CV"}
{"created":"2024-03-06 04:04:50","title":"Evaluations and relations for finite trigonometric sums","abstract":"Several methods are used to evaluate finite trigonometric sums. In each case, either the sum had not previously been evaluated, or it had been evaluated, but only by analytic means, e.g., by complex analysis or modular transformation formulas. We establish both reciprocity and three sum relations for trigonometric sums. Motivated by certain sums that we have evaluated, we add coprime conditions to the summands and thereby define analogues of Ramanujan sums, which we in turn evaluate. One of these analogues leads to a criterion for the Riemann Hypothesis, analogous to the Franel-Landau criterion.","sentences":["Several methods are used to evaluate finite trigonometric sums.","In each case, either the sum had not previously been evaluated, or it had been evaluated, but only by analytic means, e.g., by complex analysis or modular transformation formulas.","We establish both reciprocity and three sum relations for trigonometric sums.","Motivated by certain sums that we have evaluated, we add coprime conditions to the summands and thereby define analogues of Ramanujan sums, which we in turn evaluate.","One of these analogues leads to a criterion for the Riemann Hypothesis, analogous to the Franel-Landau criterion."],"url":"http://arxiv.org/abs/2403.03445v1","category":"math.NT"}
{"created":"2024-03-06 04:01:33","title":"CAMASim: A Comprehensive Simulation Framework for Content-Addressable Memory based Accelerators","abstract":"Content addressable memory (CAM) stands out as an efficient hardware solution for memory-intensive search operations by supporting parallel computation in memory. However, developing a CAM-based accelerator architecture that achieves acceptable accuracy, while minimizing hardware cost and catering to both exact and approximate search, still presents a significant challenge especially when considering a broader spectrum of applications. This complexity stems from CAM's rapid evolution across multiple levels--algorithms, architectures, circuits, and underlying devices. This paper introduces CAMASim, a first comprehensive CAM accelerator simulation framework, emphasizing modularity, flexibility, and generality. CAMASim establishes the detailed design space for CAM-based accelerators, incorporates automated functional simulation for accuracy, and enables hardware performance prediction, by leveraging a circuit-level CAM modeling tool. This work streamlines the design space exploration for CAM-based accelerator, aiding researchers in developing effective CAM-based accelerators for various search-intensive applications.","sentences":["Content addressable memory (CAM) stands out as an efficient hardware solution for memory-intensive search operations by supporting parallel computation in memory.","However, developing a CAM-based accelerator architecture that achieves acceptable accuracy, while minimizing hardware cost and catering to both exact and approximate search, still presents a significant challenge especially when considering a broader spectrum of applications.","This complexity stems from CAM's rapid evolution across multiple levels--algorithms, architectures, circuits, and underlying devices.","This paper introduces CAMASim, a first comprehensive CAM accelerator simulation framework, emphasizing modularity, flexibility, and generality.","CAMASim establishes the detailed design space for CAM-based accelerators, incorporates automated functional simulation for accuracy, and enables hardware performance prediction, by leveraging a circuit-level CAM modeling tool.","This work streamlines the design space exploration for CAM-based accelerator, aiding researchers in developing effective CAM-based accelerators for various search-intensive applications."],"url":"http://arxiv.org/abs/2403.03442v1","category":"cs.AR"}
{"created":"2024-03-06 04:00:30","title":"Third-order nonlinear optical response of 2D materials in the telecom band","abstract":"All-optical signal processing based on nonlinear optical devices is promising for ultrafast information processing in optical communication systems. Recent advances in two-dimensional (2D) layered materials with unique structures and distinctive properties have opened up new ave-nues for nonlinear optics and the fabrication of related devices with high performance. This paper reviews the recent advances in research on third-order optical nonlinearities of 2D materials, focus-ing on all-optical processing applications in the optical telecommunications band near 1550 nm. First, we provide an overview of the material properties of different 2D materials. Next, we review different methods for characterizing the third-order optical nonlinearities of 2D materials, including the Z-scan technique, third-harmonic generation (THG) measurement, and hybrid device character-ization, together with a summary of the measured n2 values in the telecommunications band. Fi-nally, the current challenges and future perspectives are discussed.","sentences":["All-optical signal processing based on nonlinear optical devices is promising for ultrafast information processing in optical communication systems.","Recent advances in two-dimensional (2D) layered materials with unique structures and distinctive properties have opened up new ave-nues for nonlinear optics and the fabrication of related devices with high performance.","This paper reviews the recent advances in research on third-order optical nonlinearities of 2D materials, focus-ing on all-optical processing applications in the optical telecommunications band near 1550 nm.","First, we provide an overview of the material properties of different 2D materials.","Next, we review different methods for characterizing the third-order optical nonlinearities of 2D materials, including the Z-scan technique, third-harmonic generation (THG) measurement, and hybrid device character-ization, together with a summary of the measured n2 values in the telecommunications band.","Fi-nally, the current challenges and future perspectives are discussed."],"url":"http://arxiv.org/abs/2403.03441v1","category":"physics.optics"}
{"created":"2024-03-06 03:42:39","title":"ALMA Spectral Survey of An eruptive Young star, V883 Ori (ASSAY): I. What triggered the current episode of eruption?","abstract":"An unbiased spectral survey of V883 Ori, an eruptive young star, was carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) in Band 6. The detected line emission from various molecules reveals morphological/kinematical features in both the Keplerian disk and the infalling envelope. A direct infall signature, red-shifted absorption against continuum, has been detected in CO, HCO$^+$, HCN, HNC, and H$_2$CO. HCO$^+$ and SO show large arm-like structures that probably connect from an infalling envelope to the disk. HCN and H$_2$CO reveal a distinct boundary between the inner and outer disk and reveal tentative spiral structures connecting the outer disk to the inner disk. HNC shows a large central emission hole (r $\\sim$0.3\\arcsec) due to its chemical conversion to HCN at high temperatures. The HDO emission, a direct tracer of the water sublimation region, has been detected in the disk. Molecular emission from complex organic molecules (COMs) is confined within the HDO emission boundary, and HCO$^+$ has an emission hole in its distribution due to its destruction by water. Together, these features suggest that the current episode of eruption in V883 Ori may be triggered by the infall from the envelope to the outer disk, generating a spiral wave, which propagates inward and greatly enhances the accretion onto the central star.","sentences":["An unbiased spectral survey of V883 Ori, an eruptive young star, was carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) in Band 6.","The detected line emission from various molecules reveals morphological/kinematical features in both the Keplerian disk and the infalling envelope.","A direct infall signature, red-shifted absorption against continuum, has been detected in CO, HCO$^+$, HCN, HNC, and H$_2$CO.","HCO$^+$ and SO show large arm-like structures that probably connect from an infalling envelope to the disk.","HCN and H$_2$CO reveal a distinct boundary between the inner and outer disk and reveal tentative spiral structures connecting the outer disk to the inner disk.","HNC shows a large central emission hole (r $\\sim$0.3\\arcsec) due to its chemical conversion to HCN at high temperatures.","The HDO emission, a direct tracer of the water sublimation region, has been detected in the disk.","Molecular emission from complex organic molecules (COMs) is confined within the HDO emission boundary, and HCO$^+$ has an emission hole in its distribution due to its destruction by water.","Together, these features suggest that the current episode of eruption in V883 Ori may be triggered by the infall from the envelope to the outer disk, generating a spiral wave, which propagates inward and greatly enhances the accretion onto the central star."],"url":"http://arxiv.org/abs/2403.03436v1","category":"astro-ph.SR"}
{"created":"2024-03-06 03:32:01","title":"Discrete Consensus-Based Optimization","abstract":"We propose Discrete Consensus-Based Optimization (DCBO), a fully discrete version of the Consensus-Based Optimization (CBO) framework. DCBO is a multi-agent method for the global optimization of possibly non-convex and non-differentiable functions. It aligns with the CBO paradigm, which promotes a consensus among agents towards a global optimum through simple stochastic dynamics amenable to rigorous mathematical analysis. Despite the promises, there has been a gap between the analysis of CBO and the actual behavior of the agents from its time-discrete implementation, as the former has focused on the system of continuous stochastic differential equations defining the model or its mean-field approximation. In particular, direct analysis of CBO-type algorithms with heterogeneous stochasticity is very challenging. DCBO distinguishes itself from these approaches in the sense that it has no continuous counterpart, thanks to the replacement of the \"softmin\" operator with the \"hardmin\" one, which is inherently discrete. Yet, it maintains the operational principles of CBO and allows for rich mathematical analysis. We present conditions, independent of the number of agents, for achieving a consensus or convergence and study the circumstances under which global optimization occurs. We test DCBO on a large number of benchmark functions to show its merits. We also demonstrate that DCBO is applicable to a diverse range of real-world problems, including neural network training, compressed sensing, and portfolio optimization, with competitive performance.","sentences":["We propose Discrete Consensus-Based Optimization (DCBO), a fully discrete version of the Consensus-Based Optimization (CBO) framework.","DCBO is a multi-agent method for the global optimization of possibly non-convex and non-differentiable functions.","It aligns with the CBO paradigm, which promotes a consensus among agents towards a global optimum through simple stochastic dynamics amenable to rigorous mathematical analysis.","Despite the promises, there has been a gap between the analysis of CBO and the actual behavior of the agents from its time-discrete implementation, as the former has focused on the system of continuous stochastic differential equations defining the model or its mean-field approximation.","In particular, direct analysis of CBO-type algorithms with heterogeneous stochasticity is very challenging.","DCBO distinguishes itself from these approaches in the sense that it has no continuous counterpart, thanks to the replacement of the \"softmin\" operator with the \"hardmin\" one, which is inherently discrete.","Yet, it maintains the operational principles of CBO and allows for rich mathematical analysis.","We present conditions, independent of the number of agents, for achieving a consensus or convergence and study the circumstances under which global optimization occurs.","We test DCBO on a large number of benchmark functions to show its merits.","We also demonstrate that DCBO is applicable to a diverse range of real-world problems, including neural network training, compressed sensing, and portfolio optimization, with competitive performance."],"url":"http://arxiv.org/abs/2403.03430v1","category":"math.OC"}
{"created":"2024-03-06 03:16:47","title":"Single Transit Detection In Kepler With Machine Learning And Onboard Spacecraft Diagnostics","abstract":"Exoplanet discovery at long orbital periods requires reliably detecting individual transits without additional information about the system. Techniques like phase-folding of light curves and periodogram analysis of radial velocity data are more sensitive to planets with shorter orbital periods, leaving a dearth of planet discoveries at long periods. We present a novel technique using an ensemble of Convolutional Neural Networks incorporating the onboard spacecraft diagnostics of \\emph{Kepler} to classify transits within a light curve. We create a pipeline to recover the location of individual transits, and the period of the orbiting planet, which maintains $>80\\%$ transit recovery sensitivity out to an 800-day orbital period. Our neural network pipeline has the potential to discover additional planets in the \\emph{Kepler} dataset, and crucially, within the $\\eta$-Earth regime. We report our first candidate from this pipeline, KOI 1271.02. KOI 1271.01 is known to exhibit strong Transit Timing Variations (TTVs), and so we jointly model the TTVs and transits of both transiting planets to constrain the orbital configuration and planetary parameters and conclude with a series of potential parameters for KOI 1271.02, as there is not enough data currently to uniquely constrain the system. We conclude that KOI 1271.02 has a radius of 5.32 $\\pm$ 0.20 $R_{\\oplus}$ and a mass of $28.94^{0.23}_{-0.47}$ $M_{\\oplus}$. Future constraints on the nature of KOI 1271.02 require measuring additional TTVs of KOI 1271.01 or observing a second transit of KOI 1271.02.","sentences":["Exoplanet discovery at long orbital periods requires reliably detecting individual transits without additional information about the system.","Techniques like phase-folding of light curves and periodogram analysis of radial velocity data are more sensitive to planets with shorter orbital periods, leaving a dearth of planet discoveries at long periods.","We present a novel technique using an ensemble of Convolutional Neural Networks incorporating the onboard spacecraft diagnostics of \\emph{Kepler} to classify transits within a light curve.","We create a pipeline to recover the location of individual transits, and the period of the orbiting planet, which maintains $>80\\%$ transit recovery sensitivity out to an 800-day orbital period.","Our neural network pipeline has the potential to discover additional planets in the \\emph{Kepler} dataset, and crucially, within the $\\eta$-Earth regime.","We report our first candidate from this pipeline, KOI 1271.02.","KOI 1271.01 is known to exhibit strong Transit Timing Variations (TTVs), and so we jointly model the TTVs and transits of both transiting planets to constrain the orbital configuration and planetary parameters and conclude with a series of potential parameters for KOI 1271.02, as there is not enough data currently to uniquely constrain the system.","We conclude that KOI 1271.02 has a radius of 5.32 $\\pm$ 0.20 $R_{\\oplus}$ and a mass of $28.94^{0.23}_{-0.47}$ $M_{\\oplus}$. Future constraints on the nature of KOI 1271.02 require measuring additional TTVs of KOI 1271.01 or observing a second transit of KOI 1271.02."],"url":"http://arxiv.org/abs/2403.03427v1","category":"astro-ph.EP"}
{"created":"2024-03-06 03:15:25","title":"Sculpting Molecules in 3D: A Flexible Substructure Aware Framework for Text-Oriented Molecular Optimization","abstract":"The integration of deep learning, particularly AI-Generated Content, with high-quality data derived from ab initio calculations has emerged as a promising avenue for transforming the landscape of scientific research. However, the challenge of designing molecular drugs or materials that incorporate multi-modality prior knowledge remains a critical and complex undertaking. Specifically, achieving a practical molecular design necessitates not only meeting the diversity requirements but also addressing structural and textural constraints with various symmetries outlined by domain experts. In this article, we present an innovative approach to tackle this inverse design problem by formulating it as a multi-modality guidance generation/optimization task. Our proposed solution involves a textural-structure alignment symmetric diffusion framework for the implementation of molecular generation/optimization tasks, namely 3DToMolo. 3DToMolo aims to harmonize diverse modalities, aligning them seamlessly to produce molecular structures adhere to specified symmetric structural and textural constraints by experts in the field. Experimental trials across three guidance generation settings have shown a superior hit generation performance compared to state-of-the-art methodologies. Moreover, 3DToMolo demonstrates the capability to generate novel molecules, incorporating specified target substructures, without the need for prior knowledge. This work not only holds general significance for the advancement of deep learning methodologies but also paves the way for a transformative shift in molecular design strategies. 3DToMolo creates opportunities for a more nuanced and effective exploration of the vast chemical space, opening new frontiers in the development of molecular entities with tailored properties and functionalities.","sentences":["The integration of deep learning, particularly AI-Generated Content, with high-quality data derived from ab initio calculations has emerged as a promising avenue for transforming the landscape of scientific research.","However, the challenge of designing molecular drugs or materials that incorporate multi-modality prior knowledge remains a critical and complex undertaking.","Specifically, achieving a practical molecular design necessitates not only meeting the diversity requirements but also addressing structural and textural constraints with various symmetries outlined by domain experts.","In this article, we present an innovative approach to tackle this inverse design problem by formulating it as a multi-modality guidance generation/optimization task.","Our proposed solution involves a textural-structure alignment symmetric diffusion framework for the implementation of molecular generation/optimization tasks, namely 3DToMolo.","3DToMolo aims to harmonize diverse modalities, aligning them seamlessly to produce molecular structures adhere to specified symmetric structural and textural constraints by experts in the field.","Experimental trials across three guidance generation settings have shown a superior hit generation performance compared to state-of-the-art methodologies.","Moreover, 3DToMolo demonstrates the capability to generate novel molecules, incorporating specified target substructures, without the need for prior knowledge.","This work not only holds general significance for the advancement of deep learning methodologies but also paves the way for a transformative shift in molecular design strategies.","3DToMolo creates opportunities for a more nuanced and effective exploration of the vast chemical space, opening new frontiers in the development of molecular entities with tailored properties and functionalities."],"url":"http://arxiv.org/abs/2403.03425v1","category":"cs.LG"}
{"created":"2024-03-06 03:12:22","title":"Johnson-noise-limited cancellation-free microwave impedance microscopy with monolithic silicon cantilever probes","abstract":"Microwave impedance microscopy (MIM) is an emerging scanning probe technique for nanoscale complex permittivity mapping and has made significant impacts in diverse fields from semiconductors to quantum materials. To date, the most significant hurdles that limit its widespread use are the requirements of specialized microwave probes and high-precision cancellation circuits. Here we show that forgoing both elements not only is feasible but actually enhances MIM performance. Using monolithic silicon cantilever probes and a cancellation-free architecture, we demonstrate thermal Johnson-noise-limited, drift-free MIM operation with 15 nm spatial resolution, minimal topography crosstalk, and an unprecedented sensitivity of 0.26 zF/$\\sqrt{\\text{Hz}}$. We accomplish this by taking advantage of the high mechanical resonant frequency and spatial resolution of silicon probes, the inherent common-mode phase noise rejection of self-referenced homodyne detection, and the exceptional stability of the streamlined architecture. Our approach makes MIM drastically more accessible and paves the way for more advanced operation modes and integration with complementary techniques.","sentences":["Microwave impedance microscopy (MIM) is an emerging scanning probe technique for nanoscale complex permittivity mapping and has made significant impacts in diverse fields from semiconductors to quantum materials.","To date, the most significant hurdles that limit its widespread use are the requirements of specialized microwave probes and high-precision cancellation circuits.","Here we show that forgoing both elements not only is feasible but actually enhances MIM performance.","Using monolithic silicon cantilever probes and a cancellation-free architecture, we demonstrate thermal Johnson-noise-limited, drift-free MIM operation with 15 nm spatial resolution, minimal topography crosstalk, and an unprecedented sensitivity of 0.26 zF/$\\sqrt{\\text{Hz}}$. We accomplish this by taking advantage of the high mechanical resonant frequency and spatial resolution of silicon probes, the inherent common-mode phase noise rejection of self-referenced homodyne detection, and the exceptional stability of the streamlined architecture.","Our approach makes MIM drastically more accessible and paves the way for more advanced operation modes and integration with complementary techniques."],"url":"http://arxiv.org/abs/2403.03423v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-06 02:58:56","title":"On discrete-time polynomial dynamical systems on hypergraphs","abstract":"This paper studies the stability of discrete-time polynomial dynamical systems on hypergraphs by utilizing the Perron-Frobenius theorem for nonnegative tensors with respect to the tensors' Z-eigenvalues and Z-eigenvectors. First of all, for a multilinear polynomial system on a uniform hypergraph, we study the stability of the origin of the corresponding systems. Afterward, we extend our results to non-homogeneous polynomial systems on non-uniform hypergraphs. We confirm that the local stability of any discrete-time polynomial system is in general dominated by pairwise terms. In particular, given the origin is locally stable, we construct a conservative (but explicit) region of attraction from the system parameters. Finally, we validate our results via some numerical examples.","sentences":["This paper studies the stability of discrete-time polynomial dynamical systems on hypergraphs by utilizing the Perron-Frobenius theorem for nonnegative tensors with respect to the tensors' Z-eigenvalues and Z-eigenvectors.","First of all, for a multilinear polynomial system on a uniform hypergraph, we study the stability of the origin of the corresponding systems.","Afterward, we extend our results to non-homogeneous polynomial systems on non-uniform hypergraphs.","We confirm that the local stability of any discrete-time polynomial system is in general dominated by pairwise terms.","In particular, given the origin is locally stable, we construct a conservative (but explicit) region of attraction from the system parameters.","Finally, we validate our results via some numerical examples."],"url":"http://arxiv.org/abs/2403.03416v1","category":"eess.SY"}
{"created":"2024-03-06 02:46:17","title":"Leveraging The Finite States of Emotion Processing to Study Late-Life Mental Health","abstract":"Traditional approaches in mental health research apply General Linear Models (GLM) to describe the longitudinal dynamics of observed psycho-behavioral measurements (questionnaire summary scores). Similarly, GLMs are also applied to characterize relationships between neurobiological measurements (regional fMRI signals) and perceptual stimuli or other regional signals. While these methods are useful for exploring linear correlations among the isolated signals of those constructs (i.e., summary scores or fMRI signals), these classical frameworks fall short in providing insights into the comprehensive system-level dynamics underlying observable changes. Hidden Markov Models (HMM) are a statistical model that enable us to describe the sequential relations among multiple observable constructs, and when applied through the lens of Finite State Automata (FSA), can provide a more integrated and intuitive framework for modeling and understanding the underlying controller (the prescription for how to respond to inputs) that fundamentally defines any system, as opposed to linearly correlating output signals produced by the controller. We present a simple and intuitive HMM processing pipeline vcHMM (See Preliminary Data) that highlights FSA theory and is applicable for both behavioral analysis of questionnaire data and fMRI data. HMMs offer theoretic promise as they are computationally equivalent to the FSA, the control processor of a Turing Machine (TM) The dynamic programming Viterbi algorithm is used to leverage the HMM model. It efficiently identifies the most likely sequence of hidden states. The vcHMM pipeline leverages this grammar to understand how behavior and neural activity relate to depression.","sentences":["Traditional approaches in mental health research apply General Linear Models (GLM) to describe the longitudinal dynamics of observed psycho-behavioral measurements (questionnaire summary scores).","Similarly, GLMs are also applied to characterize relationships between neurobiological measurements (regional fMRI signals) and perceptual stimuli or other regional signals.","While these methods are useful for exploring linear correlations among the isolated signals of those constructs (i.e., summary scores or fMRI signals), these classical frameworks fall short in providing insights into the comprehensive system-level dynamics underlying observable changes.","Hidden Markov Models (HMM) are a statistical model that enable us to describe the sequential relations among multiple observable constructs, and when applied through the lens of Finite State Automata (FSA), can provide a more integrated and intuitive framework for modeling and understanding the underlying controller (the prescription for how to respond to inputs) that fundamentally defines any system, as opposed to linearly correlating output signals produced by the controller.","We present a simple and intuitive HMM processing pipeline vcHMM (See Preliminary Data) that highlights FSA theory and is applicable for both behavioral analysis of questionnaire data and fMRI data.","HMMs offer theoretic promise as they are computationally equivalent to the FSA, the control processor of a Turing Machine (TM) The dynamic programming Viterbi algorithm is used to leverage the HMM model.","It efficiently identifies the most likely sequence of hidden states.","The vcHMM pipeline leverages this grammar to understand how behavior and neural activity relate to depression."],"url":"http://arxiv.org/abs/2403.03414v1","category":"cs.LG"}
{"created":"2024-03-06 02:40:53","title":"Online Learning and Control Synthesis for Reachable Paths of Unknown Nonlinear Systems","abstract":"In this paper, we present a novel method to drive a nonlinear system to a desired state, with limited a priori knowledge of its dynamic model: local dynamics at a single point and the bounds on the rate of change of these dynamics. This method synthesizes control actions by utilizing locally learned dynamics along a trajectory, based on data available up to that moment, and known proxy dynamics, which can generate an underapproximation of the unknown system's true reachable set. An important benefit to the contributions of this paper is the lack of knowledge needed to execute the presented control method. We establish sufficient conditions to ensure that a controlled trajectory reaches a small neighborhood of any provably reachable state within a short time horizon, with precision dependent on the tunable parameters of these conditions.","sentences":["In this paper, we present a novel method to drive a nonlinear system to a desired state, with limited a priori knowledge of its dynamic model: local dynamics at a single point and the bounds on the rate of change of these dynamics.","This method synthesizes control actions by utilizing locally learned dynamics along a trajectory, based on data available up to that moment, and known proxy dynamics, which can generate an underapproximation of the unknown system's true reachable set.","An important benefit to the contributions of this paper is the lack of knowledge needed to execute the presented control method.","We establish sufficient conditions to ensure that a controlled trajectory reaches a small neighborhood of any provably reachable state within a short time horizon, with precision dependent on the tunable parameters of these conditions."],"url":"http://arxiv.org/abs/2403.03413v1","category":"math.OC"}
{"created":"2024-03-06 02:39:21","title":"CrossNet: Leveraging Global, Cross-Band, Narrow-Band, and Positional Encoding for Single- and Multi-Channel Speaker Separation","abstract":"We introduce CrossNet, a complex spectral mapping approach to speaker separation and enhancement in reverberant and noisy conditions. The proposed architecture comprises an encoder layer, a global multi-head self-attention module, a cross-band module, a narrow-band module, and an output layer. CrossNet captures global, cross-band, and narrow-band correlations in the time-frequency domain. To address performance degradation in long utterances, we introduce a random chunk positional encoding. Experimental results on multiple datasets demonstrate the effectiveness and robustness of CrossNet, achieving state-of-the-art performance in tasks including reverberant and noisy-reverberant speaker separation. Furthermore, CrossNet exhibits faster and more stable training in comparison to recent baselines. Additionally, CrossNet's high performance extends to multi-microphone conditions, demonstrating its versatility in various acoustic scenarios.","sentences":["We introduce CrossNet, a complex spectral mapping approach to speaker separation and enhancement in reverberant and noisy conditions.","The proposed architecture comprises an encoder layer, a global multi-head self-attention module, a cross-band module, a narrow-band module, and an output layer.","CrossNet captures global, cross-band, and narrow-band correlations in the time-frequency domain.","To address performance degradation in long utterances, we introduce a random chunk positional encoding.","Experimental results on multiple datasets demonstrate the effectiveness and robustness of CrossNet, achieving state-of-the-art performance in tasks including reverberant and noisy-reverberant speaker separation.","Furthermore, CrossNet exhibits faster and more stable training in comparison to recent baselines.","Additionally, CrossNet's high performance extends to multi-microphone conditions, demonstrating its versatility in various acoustic scenarios."],"url":"http://arxiv.org/abs/2403.03411v1","category":"cs.SD"}
{"created":"2024-03-06 01:58:12","title":"How habitable are M-dwarf Exoplanets? Modeling surface conditions and exploring the role of melanins in the survival of Aspergillus niger spores under exoplanet-like radiation","abstract":"Exoplanet habitability remains a challenging field due to the large distances separating Earth from other stars. Using insights from biology and astrophysics, we studied the habitability of M-dwarf exoplanets by modeling their surface temperature and flare UV and X-ray doses using the Martian atmosphere as a shielding model. Analyzing the Proxima Centauri and TRAPPIST-1 systems, our models suggest that Proxima b and TRAPPIST-1 e are likeliest to have temperatures compatible with surface liquid water, as well as tolerable radiation environments. Results of the modeling were used as a basis for microbiology experiments to assess spore survival of the melanin-rich fungus Aspergillus niger to exoplanet-like radiation (UV-C and X-rays). Results showed that A. niger spores can endure superflare events on M-dwarf planets when shielded by a Mars-like atmosphere or by a thin layer of soil or water. Melanin-deficient spores suspended in a melanin-rich solution showed higher survival rates and germination efficiency when compared to melanin-free solutions. Overall, the models developed in this work establish a framework for microbiological research in habitability studies. Finally, we showed that A. niger spores can survive harsh radiation conditions of simulated exoplanets, also emphasizing the importance of multifunctional molecules like melanins in radiation shielding beyond Earth.","sentences":["Exoplanet habitability remains a challenging field due to the large distances separating Earth from other stars.","Using insights from biology and astrophysics, we studied the habitability of M-dwarf exoplanets by modeling their surface temperature and flare UV and X-ray doses using the Martian atmosphere as a shielding model.","Analyzing the Proxima Centauri and TRAPPIST-1 systems, our models suggest that Proxima b and TRAPPIST-1 e are likeliest to have temperatures compatible with surface liquid water, as well as tolerable radiation environments.","Results of the modeling were used as a basis for microbiology experiments to assess spore survival of the melanin-rich fungus Aspergillus niger to exoplanet-like radiation (UV-C and X-rays).","Results showed that A. niger spores can endure superflare events on M-dwarf planets when shielded by a Mars-like atmosphere or by a thin layer of soil or water.","Melanin-deficient spores suspended in a melanin-rich solution showed higher survival rates and germination efficiency when compared to melanin-free solutions.","Overall, the models developed in this work establish a framework for microbiological research in habitability studies.","Finally, we showed that A. niger spores can survive harsh radiation conditions of simulated exoplanets, also emphasizing the importance of multifunctional molecules like melanins in radiation shielding beyond Earth."],"url":"http://arxiv.org/abs/2403.03403v1","category":"astro-ph.EP"}
{"created":"2024-03-06 01:57:30","title":"Two-dimensional Kagome-in-Honeycomb materials (MN$_4$)$_3$C$_{32}$ (M=Pt or Mn)","abstract":"We propose two novel two-dimensional (2D) topological materials, (PtN$_4$)$_3$C$_{32}$ and (MnN$_4$)$_3$C$_{32}$, with a special geometry that we named as kagome-in-honeycomb (KIH) lattice structure, to illustrate the coexistence of the paradigmatic states of kagome physics, Dirac fermions and flat bands, that are difficult to be simultaneously observed in three-dimensional realistic systems. In such system, MN$_4$(M=Pt or Mn) moieties are embedded in honeycomb graphene sheet according to kagome lattice structure, thereby resulting in a KIH lattice. Using the first-principles calculations, we have systemically studied the structural, electronic, and topological properties of these two materials. In the absence of spin-orbit coupling (SOC), they both exhibit the coexistence of Dirac/quadratic-crossing cone and flat band near the Fermi level. When SOC is included, a sizable topological gap is opened at the Dirac/quadratic-crossing nodal point. For nonmagnetic (PtN$_4$)$_3$C$_{32}$, the system is converted into a $\\mathbb{Z}_2$ topological quantum spin Hall insulator defined on a curved Fermi level, while for ferromagnetic (MnN$_4$)$_3$C$_{32}$, the material is changed from a half-semi-metal to a quantum anomalous Hall insulator with nonzero Chern number and nontrivial chiral edge states. Our findings not only predict a new family of 2D quantum materials, but also provide an experimentally feasible platform to explore the emergent kagome physics, topological quantum Hall physics, strongly correlated phenomena, and theirs fascinating applications.","sentences":["We propose two novel two-dimensional (2D) topological materials, (PtN$_4$)$_3$C$_{32}$ and (MnN$_4$)$_3$C$_{32}$, with a special geometry that we named as kagome-in-honeycomb (KIH) lattice structure, to illustrate the coexistence of the paradigmatic states of kagome physics, Dirac fermions and flat bands, that are difficult to be simultaneously observed in three-dimensional realistic systems.","In such system, MN$_4$(M=Pt or Mn) moieties are embedded in honeycomb graphene sheet according to kagome lattice structure, thereby resulting in a KIH lattice.","Using the first-principles calculations, we have systemically studied the structural, electronic, and topological properties of these two materials.","In the absence of spin-orbit coupling (SOC), they both exhibit the coexistence of Dirac/quadratic-crossing cone and flat band near the Fermi level.","When SOC is included, a sizable topological gap is opened at the Dirac/quadratic-crossing nodal point.","For nonmagnetic (PtN$_4$)$_3$C$_{32}$, the system is converted into a $\\mathbb{Z}_2$ topological quantum spin Hall insulator defined on a curved Fermi level, while for ferromagnetic (MnN$_4$)$_3$C$_{32}$, the material is changed from a half-semi-metal to a quantum anomalous Hall insulator with nonzero Chern number and nontrivial chiral edge states.","Our findings not only predict a new family of 2D quantum materials, but also provide an experimentally feasible platform to explore the emergent kagome physics, topological quantum Hall physics, strongly correlated phenomena, and theirs fascinating applications."],"url":"http://arxiv.org/abs/2403.03402v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-06 01:39:56","title":"Energetic and Control Trade-offs in Spring-Wing Systems","abstract":"Flying insects are thought to achieve energy-efficient flapping flight by storing and releasing elastic energy in their muscles, tendons, and thorax. However, flight systems consisting elastic elements coupled to nonlinear, unsteady aerodynamic forces also present possible challenges to generating steady and responsive wing motions. In previous work, we examined the resonance properties of a dynamically-scaled robophysical system consisting of a rigid wing actuated by a motor in series with a spring, which we call a spring-wing system \\cite{Lynch2021-ri}. In this paper, we seek to better understand the effects of perturbations on resonant systems via a non-dimensional parameter, the Weis-Fogh number. We drive a spring-wing system at a fixed resonant frequency and study the response to an internal control perturbation and an external aerodynamic perturbation with varying Weis-Fogh number. In our first experiments, we provide a step change in the input forcing amplitude and study the wing motion response. In our second experiments we provide an external fluid flow directed at the flapping wing and study the perturbed steady-state wing motion. We evaluate results across the Weis-Fogh number, which describes the ratio of inertial and aerodynamic forces and the potential energetic benefits of elastic resonance. The results suggest that spring-wing systems designed for maximum energetic efficiency also experience trade-offs in agility and stability as the Weis-Fogh number increases. Our results demonstrate that energetic efficiency and wing maneuverability are in conflict in resonant spring-wing systems suggesting that mechanical resonance presents tradeoffs in insect flight.","sentences":["Flying insects are thought to achieve energy-efficient flapping flight by storing and releasing elastic energy in their muscles, tendons, and thorax.","However, flight systems consisting elastic elements coupled to nonlinear, unsteady aerodynamic forces also present possible challenges to generating steady and responsive wing motions.","In previous work, we examined the resonance properties of a dynamically-scaled robophysical system consisting of a rigid wing actuated by a motor in series with a spring, which we call a spring-wing system \\cite{Lynch2021-ri}.","In this paper, we seek to better understand the effects of perturbations on resonant systems via a non-dimensional parameter, the Weis-Fogh number.","We drive a spring-wing system at a fixed resonant frequency and study the response to an internal control perturbation and an external aerodynamic perturbation with varying Weis-Fogh number.","In our first experiments, we provide a step change in the input forcing amplitude and study the wing motion response.","In our second experiments we provide an external fluid flow directed at the flapping wing and study the perturbed steady-state wing motion.","We evaluate results across the Weis-Fogh number, which describes the ratio of inertial and aerodynamic forces and the potential energetic benefits of elastic resonance.","The results suggest that spring-wing systems designed for maximum energetic efficiency also experience trade-offs in agility and stability as the Weis-Fogh number increases.","Our results demonstrate that energetic efficiency and wing maneuverability are in conflict in resonant spring-wing systems suggesting that mechanical resonance presents tradeoffs in insect flight."],"url":"http://arxiv.org/abs/2403.03398v1","category":"physics.bio-ph"}
{"created":"2024-03-06 18:52:48","title":"Pressure-enhanced $f$-electron orbital weighting in UTe2 mapped by quantum interferometry","abstract":"The phase landscape of UTe$_2$ features a remarkable diversity of superconducting phases under applied pressure and magnetic field. Recent quantum oscillation studies at ambient pressure have revealed the quasi-2D Fermi surface of this material. However, the pressure-dependence of the Fermi surface remains an open question. Here we track the evolution of the UTe$_2$ Fermi surface as a function of pressure up to 19.5 kbar by measuring quantum interference oscillations. We find that in sufficient magnetic field to suppress both superconductivity at low pressures and incommensurate antiferromagnetism at higher pressures, the quasi-2D Fermi surface found at ambient pressure smoothly connects to that at 19.5 kbar, with no signs of a reconstruction over this pressure interval. The warping of the cylindrical Fermi sheets continuously increases with pressure, which is consistent with increased $f$-orbital contribution at the Fermi level, up to and beyond the critical pressure at which superconductivity is truncated. These findings highlight the value of high pressure quantum interference measurements as a new probe of the electronic structure in heavy fermion materials.","sentences":["The phase landscape of UTe$_2$ features a remarkable diversity of superconducting phases under applied pressure and magnetic field.","Recent quantum oscillation studies at ambient pressure have revealed the quasi-2D Fermi surface of this material.","However, the pressure-dependence of the Fermi surface remains an open question.","Here we track the evolution of the UTe$_2$ Fermi surface as a function of pressure up to 19.5 kbar by measuring quantum interference oscillations.","We find that in sufficient magnetic field to suppress both superconductivity at low pressures and incommensurate antiferromagnetism at higher pressures, the quasi-2D Fermi surface found at ambient pressure smoothly connects to that at 19.5 kbar, with no signs of a reconstruction over this pressure interval.","The warping of the cylindrical Fermi sheets continuously increases with pressure, which is consistent with increased $f$-orbital contribution at the Fermi level, up to and beyond the critical pressure at which superconductivity is truncated.","These findings highlight the value of high pressure quantum interference measurements as a new probe of the electronic structure in heavy fermion materials."],"url":"http://arxiv.org/abs/2403.03946v1","category":"cond-mat.supr-con"}
{"created":"2024-03-06 18:52:39","title":"SPEAR:Exact Gradient Inversion of Batches in Federated Learning","abstract":"Federated learning is a popular framework for collaborative machine learning where multiple clients only share gradient updates on their local data with the server and not the actual data. Unfortunately, it was recently shown that gradient inversion attacks can reconstruct this data from these shared gradients. Existing attacks enable exact reconstruction only for a batch size of $b=1$ in the important honest-but-curious setting, with larger batches permitting only approximate reconstruction. In this work, we propose \\emph{the first algorithm reconstructing whole batches with $b >1$ exactly}. This approach combines mathematical insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected networks and show that it recovers batches of $b \\lesssim 25$ elements exactly while being tractable for large network widths and depths.","sentences":["Federated learning is a popular framework for collaborative machine learning where multiple clients only share gradient updates on their local data with the server and not the actual data.","Unfortunately, it was recently shown that gradient inversion attacks can reconstruct this data from these shared gradients.","Existing attacks enable exact reconstruction only for a batch size of $b=1$ in the important honest-but-curious setting, with larger batches permitting only approximate reconstruction.","In this work, we propose \\emph{the first algorithm reconstructing whole batches with $b >1$ exactly}.","This approach combines mathematical insights into the explicit low-rank structure of gradients with a sampling-based algorithm.","Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable.","We provide an efficient GPU implementation for fully connected networks and show that it recovers batches of $b \\lesssim 25$ elements exactly while being tractable for large network widths and depths."],"url":"http://arxiv.org/abs/2403.03945v1","category":"cs.LG"}
{"created":"2024-03-06 18:49:25","title":"The large and moderate deviations approach in geometric functional analysis","abstract":"The work of Gantert, Kim, and Ramanan [Large deviations for random projections of $\\ell^p$ balls, Ann. Probab. 45 (6B), 2017] has initiated and inspired a new direction of research in the asymptotic theory of geometric functional analysis. The moderate deviations perspective, describing the asymptotic behavior between the scale of a central limit theorem and a large deviations principle, was later added by Kabluchko, Prochno, and Th\\\"ale in [High-dimensional limit theorems for random vectors in $\\ell_p^n$ balls.~II, Commun.~Contemp.~Math.~23(3), 2021]. These two approaches nicely complement the classical study of central limit phenomena or non-asymptotic concentration bounds for high-dimensional random geometric quantities. Beyond studying large and moderate deviations principles for random geometric quantities that appear in geometric functional analysis, other ideas emerged from the theory of large deviations and the closely related field of statistical mechanics, and have provided new insight and become the origin for new developments. Within less than a decade, a variety of results have appeared and formed this direction of research. Recently, a connection to the famous Kannan--Lov\\'asz--Simonovits conjecture and the study of moderate and large deviations for isotropic log-concave random vectors was discovered. In this manuscript, we introduce the basic principles, survey the work that has been done, and aim to manifest this direction of research, at the same time making it more accessible to a wider community of researchers.","sentences":["The work of Gantert, Kim, and Ramanan","[Large deviations for random projections of $\\ell^p$ balls, Ann. Probab.","45 (6B), 2017] has initiated and inspired a new direction of research in the asymptotic theory of geometric functional analysis.","The moderate deviations perspective, describing the asymptotic behavior between the scale of a central limit theorem and a large deviations principle, was later added by Kabluchko, Prochno, and Th\\\"ale in [High-dimensional limit theorems for random vectors in $\\ell_p^n$ balls.~II, Commun.~Contemp.~Math.~23(3), 2021].","These two approaches nicely complement the classical study of central limit phenomena or non-asymptotic concentration bounds for high-dimensional random geometric quantities.","Beyond studying large and moderate deviations principles for random geometric quantities that appear in geometric functional analysis, other ideas emerged from the theory of large deviations and the closely related field of statistical mechanics, and have provided new insight and become the origin for new developments.","Within less than a decade, a variety of results have appeared and formed this direction of research.","Recently, a connection to the famous Kannan--Lov\\'asz--Simonovits conjecture and the study of moderate and large deviations for isotropic log-concave random vectors was discovered.","In this manuscript, we introduce the basic principles, survey the work that has been done, and aim to manifest this direction of research, at the same time making it more accessible to a wider community of researchers."],"url":"http://arxiv.org/abs/2403.03940v1","category":"math.FA"}
{"created":"2024-03-06 18:27:43","title":"The Geochemical Potential for Metabolic Processes on the Sub-Neptune Exoplanet K2-18b","abstract":"Quantifying disequilibria is important to understand whether an environment could be habitable. It has been proposed that the exoplanet K2-18b has a hydrogen-rich atmosphere and a water ocean, making it a \"hycean world\". The James Webb Space Telescope recently made measurements of methane, CO$_2$, and possibly dimethyl sulfide (DMS) in the atmosphere of this planet. The initial interpretation of these data is that they may support the occurrence of hycean conditions. Here, I attempt to take a next step in exploring the prospects for habitability. I use constraints on the abundances of atmospheric gases to calculate how much chemical disequilibrium there could be, assuming K2-18b is a hycean world. I find that the presence of oxidized carbon species coexisting with abundant H$_2$ (1-1000 bar) at cool to warm (25-120{\\deg}C) conditions creates a strong thermodynamic drive for methanogenesis. More than ~75 kJ (mol C)$^{-1}$ of free energy can be released from CO$_2$ hydrogenation. Partially oxidized carbon compounds such as DMS (if present) also have potential to provide metabolic energy, albeit in smaller quantities. Because of the thermodynamic instability of CO$_2$ under hycean conditions, other reductive reactions of CO$_2$ are likely to be favored, including the synthesis of amino acids. Glycine and alanine synthesis can be energy-releasing or at least much less costly on K2-18b than in Earth's ocean, even when NH$_3$ is scarce but not totally absent. These first bioenergetic calculations for a proposed ocean-bearing exoplanet lay new groundwork for assessing exoplanetary habitability.","sentences":["Quantifying disequilibria is important to understand whether an environment could be habitable.","It has been proposed that the exoplanet K2-18b has a hydrogen-rich atmosphere and a water ocean, making it a \"hycean world\".","The James Webb Space Telescope recently made measurements of methane, CO$_2$, and possibly dimethyl sulfide (DMS) in the atmosphere of this planet.","The initial interpretation of these data is that they may support the occurrence of hycean conditions.","Here, I attempt to take a next step in exploring the prospects for habitability.","I use constraints on the abundances of atmospheric gases to calculate how much chemical disequilibrium there could be, assuming K2-18b is a hycean world.","I find that the presence of oxidized carbon species coexisting with abundant H$_2$ (1-1000 bar) at cool to warm (25-120{\\deg}C) conditions creates a strong thermodynamic drive for methanogenesis.","More than ~75 kJ (mol C)$^{-1}$ of free energy can be released from CO$_2$ hydrogenation.","Partially oxidized carbon compounds such as DMS (if present) also have potential to provide metabolic energy, albeit in smaller quantities.","Because of the thermodynamic instability of CO$_2$ under hycean conditions, other reductive reactions of CO$_2$ are likely to be favored, including the synthesis of amino acids.","Glycine and alanine synthesis can be energy-releasing or at least much less costly on K2-18b than in Earth's ocean, even when NH$_3$ is scarce but not totally absent.","These first bioenergetic calculations for a proposed ocean-bearing exoplanet lay new groundwork for assessing exoplanetary habitability."],"url":"http://arxiv.org/abs/2403.03918v1","category":"astro-ph.EP"}
{"created":"2024-03-06 18:13:53","title":"The gonality of chess graphs","abstract":"Chess graphs encode the moves that a particular chess piece can make on an $m\\times n$ chessboard. We study through these graphs through the lens of chip-firing games and graph gonality. We provide upper and lower bounds for the gonality of king's, bishop's, and knight's graphs, as well as for the toroidal versions of these graphs. We also prove that among all chess graphs, there exists an upper bound on gonality solely in terms of $\\min\\{m,n\\}$, except for queen's, toroidal queen's, rook's, and toroidal bishop's graphs.","sentences":["Chess graphs encode the moves that a particular chess piece can make on an $m\\times n$ chessboard.","We study through these graphs through the lens of chip-firing games and graph gonality.","We provide upper and lower bounds for the gonality of king's, bishop's, and knight's graphs, as well as for the toroidal versions of these graphs.","We also prove that among all chess graphs, there exists an upper bound on gonality solely in terms of $\\min\\{m,n\\}$, except for queen's, toroidal queen's, rook's, and toroidal bishop's graphs."],"url":"http://arxiv.org/abs/2403.03907v1","category":"math.CO"}
{"created":"2024-03-06 17:51:04","title":"Joint multi-task learning improves weakly-supervised biomarker prediction in computational pathology","abstract":"Deep Learning (DL) can predict biomarkers directly from digitized cancer histology in a weakly-supervised setting. Recently, the prediction of continuous biomarkers through regression-based DL has seen an increasing interest. Nonetheless, clinical decision making often requires a categorical outcome. Consequently, we developed a weakly-supervised joint multi-task Transformer architecture which has been trained and evaluated on four public patient cohorts for the prediction of two key predictive biomarkers, microsatellite instability (MSI) and homologous recombination deficiency (HRD), trained with auxiliary regression tasks related to the tumor microenvironment. Moreover, we perform a comprehensive benchmark of 16 approaches of task balancing for weakly-supervised joint multi-task learning in computational pathology. Using our novel approach, we improve over the state-of-the-art area under the receiver operating characteristic by +7.7% and +4.1%, as well as yielding better clustering of latent embeddings by +8% and +5% for the prediction of MSI and HRD in external cohorts, respectively.","sentences":["Deep Learning (DL) can predict biomarkers directly from digitized cancer histology in a weakly-supervised setting.","Recently, the prediction of continuous biomarkers through regression-based DL has seen an increasing interest.","Nonetheless, clinical decision making often requires a categorical outcome.","Consequently, we developed a weakly-supervised joint multi-task Transformer architecture which has been trained and evaluated on four public patient cohorts for the prediction of two key predictive biomarkers, microsatellite instability (MSI) and homologous recombination deficiency (HRD), trained with auxiliary regression tasks related to the tumor microenvironment.","Moreover, we perform a comprehensive benchmark of 16 approaches of task balancing for weakly-supervised joint multi-task learning in computational pathology.","Using our novel approach, we improve over the state-of-the-art area under the receiver operating characteristic by +7.7% and +4.1%, as well as yielding better clustering of latent embeddings by +8% and +5% for the prediction of MSI and HRD in external cohorts, respectively."],"url":"http://arxiv.org/abs/2403.03891v1","category":"eess.IV"}
{"created":"2024-03-06 17:29:34","title":"The algebra $\\mathcal{D}(W)$ via strong Darboux transformations","abstract":"The Matrix Bochner Problem aims to classify weight matrices $W$ such that its algebra $\\mathcal D(W)$, of all differential operators that have a sequence of these matrix orthogonal polynomials as eigenfunctions, contains a second-order differential operator. In [5] it is proven that, under certain assumptions, the solutions to the Matrix Bochner Problem can be obtained through a noncommutative bispectral Darboux transformation of some classical scalar weights.   The main aim of this paper is to introduce the concept of strong Darboux transformation among weight matrices and explore the relationship between the algebras $\\mathcal{D}(W)$ and $\\mathcal{D}(\\widetilde{W})$ when $\\widetilde{W}$ is a strong Darboux transformation of $W$. Starting from a direct sum of classical scalar weights $\\widetilde W$, and leveraging our complete knowledge of the algebra of $\\mathcal D(\\widetilde W)$, we can easily determine the algebra $\\mathcal D(W)$ of a weight $W$ that is a strong Darboux transformation of $\\widetilde W$.","sentences":["The Matrix Bochner Problem aims to classify weight matrices $W$ such that its algebra $\\mathcal D(W)$, of all differential operators that have a sequence of these matrix orthogonal polynomials as eigenfunctions, contains a second-order differential operator.","In [5] it is proven that, under certain assumptions, the solutions to the Matrix Bochner Problem can be obtained through a noncommutative bispectral Darboux transformation of some classical scalar weights.   ","The main aim of this paper is to introduce the concept of strong Darboux transformation among weight matrices and explore the relationship between the algebras $\\mathcal{D}(W)$ and $\\mathcal{D}(\\widetilde{W})$ when $\\widetilde{W}$ is a strong Darboux transformation of $W$. Starting from a direct sum of classical scalar weights $\\widetilde W$, and leveraging our complete knowledge of the algebra of $\\mathcal D(\\widetilde W)$, we can easily determine the algebra $\\mathcal D(W)$ of a weight $W$ that is a strong Darboux transformation of $\\widetilde W$."],"url":"http://arxiv.org/abs/2403.03873v1","category":"math.CA"}
{"created":"2024-03-06 17:12:55","title":"An analysis of the NCAA college football playoff team selections using an Elo ratings model","abstract":"In December 2023 the Florida State Seminoles became the first Power 5 school to have an undefeated season and miss selection for the College Football Playoff. In order to assess this decision, we employed an Elo ratings model to rank the teams and found that the selection committee's decision was justified and that Florida State were not one of the four best teams in college football in that season (ranking only 11th!). We extended this analysis to all other years of the CFP and found that the top four teams by Elo ratings differ greatly from the four teams selected in almost every year of the CFP's existence. Furthermore, we found that there have been more egregious non-selections including when Alabama was ranked first by Elo ratings in 2022 and were not selected. The analysis suggests that the current criteria are too subjective and a ratings model should be implemented to provide transparency for the sport, its teams, and its fans.","sentences":["In December 2023 the Florida State Seminoles became the first Power 5 school to have an undefeated season and miss selection for the College Football Playoff.","In order to assess this decision, we employed an Elo ratings model to rank the teams and found that the selection committee's decision was justified and that Florida State were not one of the four best teams in college football in that season (ranking only 11th!).","We extended this analysis to all other years of the CFP and found that the top four teams by Elo ratings differ greatly from the four teams selected in almost every year of the CFP's existence.","Furthermore, we found that there have been more egregious non-selections including when Alabama was ranked first by Elo ratings in 2022 and were not selected.","The analysis suggests that the current criteria are too subjective and a ratings model should be implemented to provide transparency for the sport, its teams, and its fans."],"url":"http://arxiv.org/abs/2403.03862v1","category":"stat.OT"}
{"created":"2024-03-06 16:55:40","title":"Conformal prediction for multi-dimensional time series by ellipsoidal sets","abstract":"Conformal prediction (CP) has been a popular method for uncertainty quantification because it is distribution-free, model-agnostic, and theoretically sound. For forecasting problems in supervised learning, most CP methods focus on building prediction intervals for univariate responses. In this work, we develop a sequential CP method called $\\texttt{MultiDimSPCI}$ that builds prediction regions for a multivariate response, especially in the context of multivariate time series, which are not exchangeable. Theoretically, we estimate finite-sample high-probability bounds on the conditional coverage gap. Empirically, we demonstrate that $\\texttt{MultiDimSPCI}$ maintains valid coverage on a wide range of multivariate time series while producing smaller prediction regions than CP and non-CP baselines.","sentences":["Conformal prediction (CP) has been a popular method for uncertainty quantification because it is distribution-free, model-agnostic, and theoretically sound.","For forecasting problems in supervised learning, most CP methods focus on building prediction intervals for univariate responses.","In this work, we develop a sequential CP method called $\\texttt{MultiDimSPCI}$ that builds prediction regions for a multivariate response, especially in the context of multivariate time series, which are not exchangeable.","Theoretically, we estimate finite-sample high-probability bounds on the conditional coverage gap.","Empirically, we demonstrate that $\\texttt{MultiDimSPCI}$ maintains valid coverage on a wide range of multivariate time series while producing smaller prediction regions than CP and non-CP baselines."],"url":"http://arxiv.org/abs/2403.03850v1","category":"stat.ML"}
{"created":"2024-03-06 16:41:57","title":"Surveys of clumps, cores, and condensations in Cygnus X: Temperature and nonthermal velocity dispersion revealed by VLA NH3 observations","abstract":"The physical properties, evolution, and fragmentation of massive dense cores (MDCs, $\\sim$ 0.1 pc) are fundamental pieces in our understanding of high-mass star formation. We aim to characterize the temperature, velocity dispersion, and fragmentation of the MDCs in the Cygnus X giant molecular cloud and to investigate the stability and dynamics of these cores. We present the Karl G. Jansky Very Large Array (VLA) observations of the NH$_3$ (J,K) = (1,1) and (2,2) inversion lines towards 35 MDCs in Cygnus X, from which we calculated the temperature and velocity dispersion. We extracted 202 fragments ($\\sim$ 0.02 pc) from the NH$_3$ (1,1) moment-0 maps with the GAUSSCLUMPS algorithm. We analyzed the stability of the MDCs and their NH$_3$ fragments through evaluating the corresponding kinetic, gravitational potential, and magnetic energies and the virial parameters. The MDCs in Cygnus X have a typical mean kinetic temperature T$_K$ of $\\sim$ 20 K. Our virial analysis shows that many MDCs are in subvirialized states, indicating that the kinetic energy is insufficient to support these MDCs against their gravity. The calculated nonthermal velocity dispersions of most MDCs are at transonic to mildly supersonic levels, and the bulk motions make only a minor contribution to the velocity dispersion. Regarding the NH$_3$ fragments, with T$_K$ $\\sim$ 19 K, their nonthermal velocity dispersions are mostly trans-sonic to subsonic. Unless there is a strong magnetic field, most NH$_3$ fragments are probably not in virialized states. We also find that most of the NH$_3$ fragments are dynamically quiescent, while only a few are active due to star formation activity.","sentences":["The physical properties, evolution, and fragmentation of massive dense cores (MDCs, $\\sim$ 0.1 pc) are fundamental pieces in our understanding of high-mass star formation.","We aim to characterize the temperature, velocity dispersion, and fragmentation of the MDCs in the Cygnus X giant molecular cloud and to investigate the stability and dynamics of these cores.","We present the Karl G. Jansky Very Large Array (VLA) observations of the NH$_3$ (J,K) = (1,1) and (2,2) inversion lines towards 35 MDCs in Cygnus X, from which we calculated the temperature and velocity dispersion.","We extracted 202 fragments ($\\sim$ 0.02 pc) from the NH$_3$ (1,1) moment-0 maps with the GAUSSCLUMPS algorithm.","We analyzed the stability of the MDCs and their NH$_3$ fragments through evaluating the corresponding kinetic, gravitational potential, and magnetic energies and the virial parameters.","The MDCs in Cygnus X have a typical mean kinetic temperature T$_K$ of $\\sim$ 20 K. Our virial analysis shows that many MDCs are in subvirialized states, indicating that the kinetic energy is insufficient to support these MDCs against their gravity.","The calculated nonthermal velocity dispersions of most MDCs are at transonic to mildly supersonic levels, and the bulk motions make only a minor contribution to the velocity dispersion.","Regarding the NH$_3$ fragments, with T$_K$ $\\sim$ 19 K, their nonthermal velocity dispersions are mostly trans-sonic to subsonic.","Unless there is a strong magnetic field, most NH$_3$ fragments are probably not in virialized states.","We also find that most of the NH$_3$ fragments are dynamically quiescent, while only a few are active due to star formation activity."],"url":"http://arxiv.org/abs/2403.03845v1","category":"astro-ph.GA"}
{"created":"2024-03-06 16:33:22","title":"How open is the asteroid-mass primordial black hole window?","abstract":"Primordial black holes (PBHs) can make up all of the dark matter (DM) if their mass, $m$, is in the so-called 'asteroid-mass window', $10^{17} \\, {\\rm g} \\lesssim m \\lesssim 10^{22} \\, {\\rm g}$. Observational constraints on the abundance of PBHs are usually calculated assuming they all have the same mass, however this is unlikely to be a good approximation. PBHs formed from the collapse of large density perturbations during radiation domination are expected to have an extended mass function (MF), due to the effects of critical collapse. The PBH MF is often assumed to be lognormal, however it has recently been shown that other functions are a better fit to numerically calculated MFs. We recalculate both current and potential future constraints for these improved fitting functions. We find that for current constraints the asteroid-mass window narrows, but remains open (i.e. all of the DM can be in the form of PBHs) unless the PBH MF is wider than expected. Future evaporation and microlensing constraints may together exclude all of the DM being in PBHs, depending on the width of the PBH MF and also the shape of its low and high mass tails.","sentences":["Primordial black holes (PBHs) can make up all of the dark matter (DM) if their mass, $m$, is in the so-called 'asteroid-mass window', $10^{17} \\, {\\rm g} \\lesssim m \\lesssim 10^{22} \\, {\\rm g}$. Observational constraints on the abundance of PBHs are usually calculated assuming they all have the same mass, however this is unlikely to be a good approximation.","PBHs formed from the collapse of large density perturbations during radiation domination are expected to have an extended mass function (MF), due to the effects of critical collapse.","The PBH MF is often assumed to be lognormal, however it has recently been shown that other functions are a better fit to numerically calculated MFs.","We recalculate both current and potential future constraints for these improved fitting functions.","We find that for current constraints the asteroid-mass window narrows, but remains open (i.e. all of the DM can be in the form of PBHs) unless the PBH MF is wider than expected.","Future evaporation and microlensing constraints may together exclude all of the DM being in PBHs, depending on the width of the PBH MF and also the shape of its low and high mass tails."],"url":"http://arxiv.org/abs/2403.03839v1","category":"astro-ph.CO"}
{"created":"2024-03-06 16:08:51","title":"HoLens: A Visual Analytics Design for Higher-order Movement Modeling and Visualization","abstract":"Higher-order patterns reveal sequential multistep state transitions, which are usually superior to origin-destination analysis, which depicts only first-order geospatial movement patterns. Conventional methods for higher-order movement modeling first construct a directed acyclic graph (DAG) of movements, then extract higher-order patterns from the DAG. However, DAG-based methods heavily rely on the identification of movement keypoints that are challenging for sparse movements and fail to consider the temporal variants that are critical for movements in urban environments. To overcome the limitations, we propose HoLens, a novel approach for modeling and visualizing higher-order movement patterns in the context of an urban environment. HoLens mainly makes twofold contributions: first, we design an auto-adaptive movement aggregation algorithm that self-organizes movements hierarchically by considering spatial proximity, contextual information, and temporal variability; second, we develop an interactive visual analytics interface consisting of well-established visualization techniques, including the H-Flow for visualizing the higher-order patterns on the map and the higher-order state sequence chart for representing the higher-order state transitions. Two real-world case studies manifest that the method can adaptively aggregate the data and exhibit the process of how to explore the higher-order patterns by HoLens. We also demonstrate our approach's feasibility, usability, and effectiveness through an expert interview with three domain experts.","sentences":["Higher-order patterns reveal sequential multistep state transitions, which are usually superior to origin-destination analysis, which depicts only first-order geospatial movement patterns.","Conventional methods for higher-order movement modeling first construct a directed acyclic graph (DAG) of movements, then extract higher-order patterns from the DAG.","However, DAG-based methods heavily rely on the identification of movement keypoints that are challenging for sparse movements and fail to consider the temporal variants that are critical for movements in urban environments.","To overcome the limitations, we propose HoLens, a novel approach for modeling and visualizing higher-order movement patterns in the context of an urban environment.","HoLens mainly makes twofold contributions: first, we design an auto-adaptive movement aggregation algorithm that self-organizes movements hierarchically by considering spatial proximity, contextual information, and temporal variability; second, we develop an interactive visual analytics interface consisting of well-established visualization techniques, including the H-Flow for visualizing the higher-order patterns on the map and the higher-order state sequence chart for representing the higher-order state transitions.","Two real-world case studies manifest that the method can adaptively aggregate the data and exhibit the process of how to explore the higher-order patterns by HoLens.","We also demonstrate our approach's feasibility, usability, and effectiveness through an expert interview with three domain experts."],"url":"http://arxiv.org/abs/2403.03822v1","category":"cs.HC"}
{"created":"2024-03-06 16:03:37","title":"Targeted Variance Reduction: Robust Bayesian Optimization of Black-Box Simulators with Noise Parameters","abstract":"The optimization of a black-box simulator over control parameters $\\mathbf{x}$ arises in a myriad of scientific applications. In such applications, the simulator often takes the form $f(\\mathbf{x},\\boldsymbol{\\theta})$, where $\\boldsymbol{\\theta}$ are parameters that are uncertain in practice. Robust optimization aims to optimize the objective $\\mathbb{E}[f(\\mathbf{x},\\boldsymbol{\\Theta})]$, where $\\boldsymbol{\\Theta} \\sim \\mathcal{P}$ is a random variable that models uncertainty on $\\boldsymbol{\\theta}$. For this, existing black-box methods typically employ a two-stage approach for selecting the next point $(\\mathbf{x},\\boldsymbol{\\theta})$, where $\\mathbf{x}$ and $\\boldsymbol{\\theta}$ are optimized separately via different acquisition functions. As such, these approaches do not employ a joint acquisition over $(\\mathbf{x},\\boldsymbol{\\theta})$, and thus may fail to fully exploit control-to-noise interactions for effective robust optimization. To address this, we propose a new Bayesian optimization method called Targeted Variance Reduction (TVR). The TVR leverages a novel joint acquisition function over $(\\mathbf{x},\\boldsymbol{\\theta})$, which targets variance reduction on the objective within the desired region of improvement. Under a Gaussian process surrogate on $f$, the TVR acquisition can be evaluated in closed form, and reveals an insightful exploration-exploitation-precision trade-off for robust black-box optimization. The TVR can further accommodate a broad class of non-Gaussian distributions on $\\mathcal{P}$ via a careful integration of normalizing flows. We demonstrate the improved performance of TVR over the state-of-the-art in a suite of numerical experiments and an application to the robust design of automobile brake discs under operational uncertainty.","sentences":["The optimization of a black-box simulator over control parameters $\\mathbf{x}$ arises in a myriad of scientific applications.","In such applications, the simulator often takes the form $f(\\mathbf{x},\\boldsymbol{\\theta})$, where $\\boldsymbol{\\theta}$ are parameters that are uncertain in practice.","Robust optimization aims to optimize the objective $\\mathbb{E}[f(\\mathbf{x},\\boldsymbol{\\Theta})]$, where $\\boldsymbol{\\Theta} \\sim \\mathcal{P}$ is a random variable that models uncertainty on $\\boldsymbol{\\theta}$. For this, existing black-box methods typically employ a two-stage approach for selecting the next point $(\\mathbf{x},\\boldsymbol{\\theta})$, where $\\mathbf{x}$ and $\\boldsymbol{\\theta}$ are optimized separately via different acquisition functions.","As such, these approaches do not employ a joint acquisition over $(\\mathbf{x},\\boldsymbol{\\theta})$, and thus may fail to fully exploit control-to-noise interactions for effective robust optimization.","To address this, we propose a new Bayesian optimization method called Targeted Variance Reduction (TVR).","The TVR leverages a novel joint acquisition function over $(\\mathbf{x},\\boldsymbol{\\theta})$, which targets variance reduction on the objective within the desired region of improvement.","Under a Gaussian process surrogate on $f$, the TVR acquisition can be evaluated in closed form, and reveals an insightful exploration-exploitation-precision trade-off for robust black-box optimization.","The TVR can further accommodate a broad class of non-Gaussian distributions on $\\mathcal{P}$ via a careful integration of normalizing flows.","We demonstrate the improved performance of TVR over the state-of-the-art in a suite of numerical experiments and an application to the robust design of automobile brake discs under operational uncertainty."],"url":"http://arxiv.org/abs/2403.03816v1","category":"stat.ML"}
{"created":"2024-03-06 16:00:46","title":"Incentivized Learning in Principal-Agent Bandit Games","abstract":"This work considers a repeated principal-agent bandit game, where the principal can only interact with her environment through the agent. The principal and the agent have misaligned objectives and the choice of action is only left to the agent. However, the principal can influence the agent's decisions by offering incentives which add up to his rewards. The principal aims to iteratively learn an incentive policy to maximize her own total utility. This framework extends usual bandit problems and is motivated by several practical applications, such as healthcare or ecological taxation, where traditionally used mechanism design theories often overlook the learning aspect of the problem. We present nearly optimal (with respect to a horizon $T$) learning algorithms for the principal's regret in both multi-armed and linear contextual settings. Finally, we support our theoretical guarantees through numerical experiments.","sentences":["This work considers a repeated principal-agent bandit game, where the principal can only interact with her environment through the agent.","The principal and the agent have misaligned objectives and the choice of action is only left to the agent.","However, the principal can influence the agent's decisions by offering incentives which add up to his rewards.","The principal aims to iteratively learn an incentive policy to maximize her own total utility.","This framework extends usual bandit problems and is motivated by several practical applications, such as healthcare or ecological taxation, where traditionally used mechanism design theories often overlook the learning aspect of the problem.","We present nearly optimal (with respect to a horizon $T$) learning algorithms for the principal's regret in both multi-armed and linear contextual settings.","Finally, we support our theoretical guarantees through numerical experiments."],"url":"http://arxiv.org/abs/2403.03811v1","category":"stat.ML"}
{"created":"2024-03-06 15:50:35","title":"Strengthening nuclear symmetry energy constraints using multiple resonant shattering flares of neutron stars with realistic mass uncertainties","abstract":"With current and planned gravitational-wave (GW) observing runs, coincident multimessenger timing of Resonant Shattering Flares (RSFs) and GWs may soon allow for neutron star (NS) asteroseismology to be used to constrain the nuclear symmetry energy, an important property of fundamental nuclear physics that influences the composition and equation of state of NSs. In this work we examine the effects of combining multiple RSF detections on these symmetry energy constraints, and consider how realistic uncertainties in the masses of the progenitor NSs may weaken them. We show that the detection of subsequent multimessenger events has the potential to substantially improve constraints beyond those obtained from the first, and that this improvement is insensitive to the mass of the NSs which produce the RSFs and its uncertainty. This sets these asteroseismic constraints apart from bulk NS properties such as radius, for which the NS mass is highly important, meaning that any multimessenger RSF and GW events can equally improve our knowledge of fundamental physics.","sentences":["With current and planned gravitational-wave (GW) observing runs, coincident multimessenger timing of Resonant Shattering Flares (RSFs) and GWs may soon allow for neutron star (NS) asteroseismology to be used to constrain the nuclear symmetry energy, an important property of fundamental nuclear physics that influences the composition and equation of state of NSs.","In this work we examine the effects of combining multiple RSF detections on these symmetry energy constraints, and consider how realistic uncertainties in the masses of the progenitor NSs may weaken them.","We show that the detection of subsequent multimessenger events has the potential to substantially improve constraints beyond those obtained from the first, and that this improvement is insensitive to the mass of the NSs which produce the RSFs and its uncertainty.","This sets these asteroseismic constraints apart from bulk NS properties such as radius, for which the NS mass is highly important, meaning that any multimessenger RSF and GW events can equally improve our knowledge of fundamental physics."],"url":"http://arxiv.org/abs/2403.03798v1","category":"astro-ph.HE"}
{"created":"2024-03-06 15:42:37","title":"Convergence rate for a regularized scalar conservation law","abstract":"This work revisits a recent finding by the first author concerning the local convergence of a regularized scalar conservation law. We significantly improve the original statement by establishing a global convergence result within the Lebesgue spaces $L^\\infty_{\\mathrm{loc}}(\\mathbb{R}^+;L^p(\\mathbb{R}))$, for any $p \\in [1,\\infty)$, as the regularization parameter $\\ell$ approaches zero. Notably, we demonstrate that this stability result is accompanied by a quantifiable rate of convergence. A key insight in our proof lies in the observation that the fluctuations of the solutions remain under control in low regularity spaces, allowing for a potential quantification of their behavior in the limit as $\\ell\\to 0$. This is achieved through a careful asymptotic analysis of the perturbative terms in the regularized equation, which, in our view, constitutes a pivotal contribution to the core findings of this paper.","sentences":["This work revisits a recent finding by the first author concerning the local convergence of a regularized scalar conservation law.","We significantly improve the original statement by establishing a global convergence result within the Lebesgue spaces $L^\\infty_{\\mathrm{loc}}(\\mathbb{R}^+;L^p(\\mathbb{R}))$, for any $p \\in [1,\\infty)$, as the regularization parameter $\\ell$ approaches zero.","Notably, we demonstrate that this stability result is accompanied by a quantifiable rate of convergence.","A key insight in our proof lies in the observation that the fluctuations of the solutions remain under control in low regularity spaces, allowing for a potential quantification of their behavior in the limit as $\\ell\\to 0$.","This is achieved through a careful asymptotic analysis of the perturbative terms in the regularized equation, which, in our view, constitutes a pivotal contribution to the core findings of this paper."],"url":"http://arxiv.org/abs/2403.03794v1","category":"math.AP"}
{"created":"2024-03-06 15:30:41","title":"A machine learning workflow to address credit default prediction","abstract":"Due to the recent increase in interest in Financial Technology (FinTech), applications like credit default prediction (CDP) are gaining significant industrial and academic attention. In this regard, CDP plays a crucial role in assessing the creditworthiness of individuals and businesses, enabling lenders to make informed decisions regarding loan approvals and risk management. In this paper, we propose a workflow-based approach to improve CDP, which refers to the task of assessing the probability that a borrower will default on his or her credit obligations. The workflow consists of multiple steps, each designed to leverage the strengths of different techniques featured in machine learning pipelines and, thus best solve the CDP task. We employ a comprehensive and systematic approach starting with data preprocessing using Weight of Evidence encoding, a technique that ensures in a single-shot data scaling by removing outliers, handling missing values, and making data uniform for models working with different data types. Next, we train several families of learning models, introducing ensemble techniques to build more robust models and hyperparameter optimization via multi-objective genetic algorithms to consider both predictive accuracy and financial aspects. Our research aims at contributing to the FinTech industry in providing a tool to move toward more accurate and reliable credit risk assessment, benefiting both lenders and borrowers.","sentences":["Due to the recent increase in interest in Financial Technology (FinTech), applications like credit default prediction (CDP) are gaining significant industrial and academic attention.","In this regard, CDP plays a crucial role in assessing the creditworthiness of individuals and businesses, enabling lenders to make informed decisions regarding loan approvals and risk management.","In this paper, we propose a workflow-based approach to improve CDP, which refers to the task of assessing the probability that a borrower will default on his or her credit obligations.","The workflow consists of multiple steps, each designed to leverage the strengths of different techniques featured in machine learning pipelines and, thus best solve the CDP task.","We employ a comprehensive and systematic approach starting with data preprocessing using Weight of Evidence encoding, a technique that ensures in a single-shot data scaling by removing outliers, handling missing values, and making data uniform for models working with different data types.","Next, we train several families of learning models, introducing ensemble techniques to build more robust models and hyperparameter optimization via multi-objective genetic algorithms to consider both predictive accuracy and financial aspects.","Our research aims at contributing to the FinTech industry in providing a tool to move toward more accurate and reliable credit risk assessment, benefiting both lenders and borrowers."],"url":"http://arxiv.org/abs/2403.03785v1","category":"cs.CE"}
{"created":"2024-03-06 15:26:05","title":"On the Injectivity Radius of the Stiefel Manifold: Numerical investigations and an explicit construction of a cut point at short distance","abstract":"Arguably, geodesics are the most important geometric objects on a differentiable manifold. They describe candidates for shortest paths and are guaranteed to be unique shortest paths when the starting velocity stays within the so-called injectivity radius of the manifold. In this work, we investigate the injectivity radius of the Stiefel manifold under the canonical metric. The Stiefel manifold $St(n,p)$ is the set of rectangular matrices of dimension $n$-by-$p$ with orthogonal columns, sometimes also called the space of orthogonal $p$-frames in $\\mathbb{R}^n$. Using a standard curvature argument, Rentmeesters has shown in 2013 that the injectivity radius of the Stiefel manifold is bounded by $\\sqrt{\\frac{4}{5}}\\pi$. It is an open question, whether this bound is sharp. With the definition of the injectivity radius via cut points of geodesics, we gain access to the information of the injectivity radius by investigating geodesics. More precisely, we consider the behavior of special variations of geodesics, called Jacobi fields. By doing so, we are able to present an explicit example of a cut point. In addition, since the theoretical analysis of geodesics for cut points and especially conjugate points as a type of cut points is difficult, we investigate the question of the sharpness of the bound by means of numerical experiments.","sentences":["Arguably, geodesics are the most important geometric objects on a differentiable manifold.","They describe candidates for shortest paths and are guaranteed to be unique shortest paths when the starting velocity stays within the so-called injectivity radius of the manifold.","In this work, we investigate the injectivity radius of the Stiefel manifold under the canonical metric.","The Stiefel manifold $St(n,p)$ is the set of rectangular matrices of dimension $n$-by-$p$ with orthogonal columns, sometimes also called the space of orthogonal $p$-frames in $\\mathbb{R}^n$. Using a standard curvature argument, Rentmeesters has shown in 2013 that the injectivity radius of the Stiefel manifold is bounded by $\\sqrt{\\frac{4}{5}}\\pi$. It is an open question, whether this bound is sharp.","With the definition of the injectivity radius via cut points of geodesics, we gain access to the information of the injectivity radius by investigating geodesics.","More precisely, we consider the behavior of special variations of geodesics, called Jacobi fields.","By doing so, we are able to present an explicit example of a cut point.","In addition, since the theoretical analysis of geodesics for cut points and especially conjugate points as a type of cut points is difficult, we investigate the question of the sharpness of the bound by means of numerical experiments."],"url":"http://arxiv.org/abs/2403.03782v1","category":"math.NA"}
{"created":"2024-03-06 15:06:11","title":"AcceleratedLiNGAM: Learning Causal DAGs at the speed of GPUs","abstract":"Existing causal discovery methods based on combinatorial optimization or search are slow, prohibiting their application on large-scale datasets. In response, more recent methods attempt to address this limitation by formulating causal discovery as structure learning with continuous optimization but such approaches thus far provide no statistical guarantees. In this paper, we show that by efficiently parallelizing existing causal discovery methods, we can in fact scale them to thousands of dimensions, making them practical for substantially larger-scale problems. In particular, we parallelize the LiNGAM method, which is quadratic in the number of variables, obtaining up to a 32-fold speed-up on benchmark datasets when compared with existing sequential implementations. Specifically, we focus on the causal ordering subprocedure in DirectLiNGAM and implement GPU kernels to accelerate it. This allows us to apply DirectLiNGAM to causal inference on large-scale gene expression data with genetic interventions yielding competitive results compared with specialized continuous optimization methods, and Var-LiNGAM for causal discovery on U.S. stock data.","sentences":["Existing causal discovery methods based on combinatorial optimization or search are slow, prohibiting their application on large-scale datasets.","In response, more recent methods attempt to address this limitation by formulating causal discovery as structure learning with continuous optimization but such approaches thus far provide no statistical guarantees.","In this paper, we show that by efficiently parallelizing existing causal discovery methods, we can in fact scale them to thousands of dimensions, making them practical for substantially larger-scale problems.","In particular, we parallelize the LiNGAM method, which is quadratic in the number of variables, obtaining up to a 32-fold speed-up on benchmark datasets when compared with existing sequential implementations.","Specifically, we focus on the causal ordering subprocedure in DirectLiNGAM and implement GPU kernels to accelerate it.","This allows us to apply DirectLiNGAM to causal inference on large-scale gene expression data with genetic interventions yielding competitive results compared with specialized continuous optimization methods, and Var-LiNGAM for causal discovery on U.S. stock data."],"url":"http://arxiv.org/abs/2403.03772v1","category":"cs.LG"}
{"created":"2024-03-06 15:04:31","title":"Impact of theoretical uncertainties on model parameter reconstruction from GW signals sourced by cosmological phase transitions","abstract":"Different computational techniques for cosmological phase transition parameters can impact the Gravitational Wave (GW) spectra predicted in a given particle physics model. To scrutinize the importance of this effect, we perform large-scale parameter scans of the dynamical real-singlet extended Standard Model using three perturbative approximations for the effective potential: the $\\overline{\\rm MS}$ and on-shell schemes at leading order, and three-dimensional thermal effective theory (3D EFT) at next-to-leading order. While predictions of GW amplitudes are typically unreliable in the absence of higher-order corrections, we show that the reconstructed model parameter spaces are robust up to a few percent in uncertainty. While 3D EFT is accurate from one loop order, theoretical uncertainties of reconstructed model parameters, using four-dimensional standard techniques, remain dominant over the experimental ones even for signals merely strong enough to claim a detection by LISA.","sentences":["Different computational techniques for cosmological phase transition parameters can impact the Gravitational Wave (GW) spectra predicted in a given particle physics model.","To scrutinize the importance of this effect, we perform large-scale parameter scans of the dynamical real-singlet extended Standard Model using three perturbative approximations for the effective potential: the $\\overline{\\rm MS}$ and on-shell schemes at leading order, and three-dimensional thermal effective theory (3D EFT) at next-to-leading order.","While predictions of GW amplitudes are typically unreliable in the absence of higher-order corrections, we show that the reconstructed model parameter spaces are robust up to a few percent in uncertainty.","While 3D EFT is accurate from one loop order, theoretical uncertainties of reconstructed model parameters, using four-dimensional standard techniques, remain dominant over the experimental ones even for signals merely strong enough to claim a detection by LISA."],"url":"http://arxiv.org/abs/2403.03769v1","category":"hep-ph"}
{"created":"2024-03-06 14:41:37","title":"Twisted Knots and the Perturbed Alexander Invariant","abstract":"The perturbed Alexander invariant $\\rho_1$, defined by Bar-Natan and van der Veen, is a powerful, easily computable polynomial knot invariant with deep connections to the Alexander and colored Jones polynomials. We study the behavior of $\\rho_1$ for families of knots $\\{K_t\\}$ given by performing $t$ full twists on a set of coherently oriented strands in a knot $K_0 \\subset S^3$. We prove that as $t \\to \\infty$ the coefficients of $\\rho_1$ grow asymptotically linearly, and we show how to compute this growth rate for any such family. As an application we give the first theorem on the ability of $\\rho_1$ to distinguish knots in infinite families. Our methods also yield a new proof that the Alexander polynomial stabilizes under such twisting and we show how to compute the limit of Alexander polynomials, strengthening a recent result of Chen. Finally, we conjecture that $\\rho_1$ obstructs knot positivity via a ``perturbed Conway invariant.''","sentences":["The perturbed Alexander invariant $\\rho_1$, defined by Bar-Natan and van der Veen, is a powerful, easily computable polynomial knot invariant with deep connections to the Alexander and colored Jones polynomials.","We study the behavior of $\\rho_1$ for families of knots $\\{K_t\\}$ given by performing $t$ full twists on a set of coherently oriented strands in a knot $K_0 \\subset S^3$.","We prove that as $t \\to \\infty$ the coefficients of $\\rho_1$ grow asymptotically linearly, and we show how to compute this growth rate for any such family.","As an application we give the first theorem on the ability of $\\rho_1$ to distinguish knots in infinite families.","Our methods also yield a new proof that the Alexander polynomial stabilizes under such twisting and we show how to compute the limit of Alexander polynomials, strengthening a recent result of Chen.","Finally, we conjecture that $\\rho_1$ obstructs knot positivity via a ``perturbed Conway invariant.''"],"url":"http://arxiv.org/abs/2403.03754v1","category":"math.GT"}
{"created":"2024-03-06 14:39:15","title":"Two 100 TeV neutrinos coincident with the Seyfert galaxy NGC 7469","abstract":"In 2013, the IceCube collaboration announced the detection of a diffuse high-energy astrophysical neutrino flux. The origin of this flux is still largely unknown. The most significant individual source is the close-by Seyfert galaxy NGC 1068 at 4.2-sigma level with a soft spectral index. To identify sources based on their counterpart, IceCube releases realtime alerts corresponding to neutrinos with a high probability of astrophysical origin. We report here the spatial coincidence of two neutrino alerts, IC220424A and IC230416A, with the Seyfert galaxy NGC 7469 at a distance of 70 Mpc. We evaluate, a-posteriori, the chance probability of such a coincidence and discuss this source as a neutrino emitter based on its multi-wavelength properties and in comparison to NGC 1068. To calculate the chance coincidence considering neutrino emission from a specific source population, we perform a Goodness-of-Fit test with a test statistic derived from a likelihood ratio that includes the neutrino angular uncertainty and the source distance. We apply this test first to a catalog of AGN sources and second to a catalog of Seyfert galaxies only. Our a-posteriori evaluation excludes the chance coincidence of the two neutrinos with the Seyfert galaxy NGC 7469 at 3.3-sigma level. To be compatible with non-detections of TeV neutrinos, the source would need to have a hard spectral index.","sentences":["In 2013, the IceCube collaboration announced the detection of a diffuse high-energy astrophysical neutrino flux.","The origin of this flux is still largely unknown.","The most significant individual source is the close-by Seyfert galaxy NGC 1068 at 4.2-sigma level with a soft spectral index.","To identify sources based on their counterpart, IceCube releases realtime alerts corresponding to neutrinos with a high probability of astrophysical origin.","We report here the spatial coincidence of two neutrino alerts, IC220424A and IC230416A, with the Seyfert galaxy NGC 7469 at a distance of 70 Mpc.","We evaluate, a-posteriori, the chance probability of such a coincidence and discuss this source as a neutrino emitter based on its multi-wavelength properties and in comparison to NGC 1068.","To calculate the chance coincidence considering neutrino emission from a specific source population, we perform a Goodness-of-Fit test with a test statistic derived from a likelihood ratio that includes the neutrino angular uncertainty and the source distance.","We apply this test first to a catalog of AGN sources and second to a catalog of Seyfert galaxies only.","Our a-posteriori evaluation excludes the chance coincidence of the two neutrinos with the Seyfert galaxy NGC 7469 at 3.3-sigma level.","To be compatible with non-detections of TeV neutrinos, the source would need to have a hard spectral index."],"url":"http://arxiv.org/abs/2403.03752v1","category":"astro-ph.HE"}
{"created":"2024-03-06 14:13:48","title":"Gravitational waves from first-order phase transitions in LISA: reconstruction pipeline and physics interpretation","abstract":"We develop a tool for the analysis of stochastic gravitational wave backgrounds from cosmological first-order phase transitions with LISA: we initiate a template databank for these signals, prototype their searches, and forecast their reconstruction. The templates encompass the gravitational wave signals sourced by bubble collisions, sound waves and turbulence. Accounting for Galactic and extra-Galactic foregrounds, we forecast the region of the parameter space that LISA will reconstruct with better than $\\sim 10\\,\\%$ accuracy, if certain experimental and theoretical uncertainties are solved by the time LISA flies. We illustrate the accuracy with which LISA can reconstruct the parameters on a few benchmark signals, both in terms of the template parameters and the phase transition ones. To show the impact of the forecasts on physics beyond the Standard Model, we map the reconstructed benchmark measurements into the parameter spaces of the singlet extension of the Standard Model and of the classically conformal invariant $U(1)_{B-L}$ model.","sentences":["We develop a tool for the analysis of stochastic gravitational wave backgrounds from cosmological first-order phase transitions with LISA: we initiate a template databank for these signals, prototype their searches, and forecast their reconstruction.","The templates encompass the gravitational wave signals sourced by bubble collisions, sound waves and turbulence.","Accounting for Galactic and extra-Galactic foregrounds, we forecast the region of the parameter space that LISA will reconstruct with better than $\\sim 10\\,\\%$ accuracy, if certain experimental and theoretical uncertainties are solved by the time LISA flies.","We illustrate the accuracy with which LISA can reconstruct the parameters on a few benchmark signals, both in terms of the template parameters and the phase transition ones.","To show the impact of the forecasts on physics beyond the Standard Model, we map the reconstructed benchmark measurements into the parameter spaces of the singlet extension of the Standard Model and of the classically conformal invariant $U(1)_{B-L}$ model."],"url":"http://arxiv.org/abs/2403.03723v1","category":"astro-ph.CO"}
{"created":"2024-03-06 14:12:18","title":"Criminal organizations exhibit hysteresis, resilience, and robustness by balancing security and efficiency","abstract":"The interplay between criminal organizations and law enforcement disruption strategies is crucial in criminology. Criminal enterprises, like legitimate businesses, balance visibility and security to thrive. This study uses evolutionary game theory to analyze criminal networks' dynamics, resilience to interventions, and responses to external conditions. We find strong hysteresis effects, challenging traditional deterrence-focused strategies. Optimal thresholds for organization formation or dissolution are defined by these effects. Stricter punishment doesn't always deter organized crime linearly. Network structure, particularly link density and skill assortativity, significantly influences organization formation and stability. These insights advocate for adaptive policy-making and strategic law enforcement to effectively disrupt criminal networks.","sentences":["The interplay between criminal organizations and law enforcement disruption strategies is crucial in criminology.","Criminal enterprises, like legitimate businesses, balance visibility and security to thrive.","This study uses evolutionary game theory to analyze criminal networks' dynamics, resilience to interventions, and responses to external conditions.","We find strong hysteresis effects, challenging traditional deterrence-focused strategies.","Optimal thresholds for organization formation or dissolution are defined by these effects.","Stricter punishment doesn't always deter organized crime linearly.","Network structure, particularly link density and skill assortativity, significantly influences organization formation and stability.","These insights advocate for adaptive policy-making and strategic law enforcement to effectively disrupt criminal networks."],"url":"http://arxiv.org/abs/2403.03720v1","category":"physics.soc-ph"}
{"created":"2024-03-06 13:50:12","title":"Dust Survival in Galactic Winds","abstract":"We present a suite of high-resolution numerical simulations to study the evolution and survival of dust in hot galactic winds. We implement a novel dust framework in the Cholla hydrodynamics code and use wind tunnel simulations of cool, dusty clouds to understand how thermal sputtering affects the dust content of galactic winds. Our simulations illustrate how various regimes of cloud evolution impact dust survival, dependent on cloud size, wind properties, and dust grain size. We find that significant amounts of dust can survive in winds in all scenarios, even without shielding from the cool phase of outflows. We present an analytic framework that explains this result, along with an analysis of the impact of cloud evolution on the total fraction of dust survival. Using these results, we estimate that 60 percent of dust that enters a starburst-driven wind could survive to populate the halo, based on a simulated distribution of cloud properties. We also investigate how these conclusions depend on grain size, exploring grains from 0.1 micron to 10 Angstrom. Under most circumstances, grains smaller than 0.01 micron cannot withstand hot-phase exposure, suggesting that the small grains observed in the CGM are either formed in situ due to shattering of larger grains, or must be carried there in cool phase outflows. Finally, we show that the dust-to-gas ratio of clouds declines as a function of distance from the galaxy due to cloud-wind mixing and condensation. These results provide an explanation for the vast amounts of dust observed in the CGMs of galaxies and beyond.","sentences":["We present a suite of high-resolution numerical simulations to study the evolution and survival of dust in hot galactic winds.","We implement a novel dust framework in the Cholla hydrodynamics code and use wind tunnel simulations of cool, dusty clouds to understand how thermal sputtering affects the dust content of galactic winds.","Our simulations illustrate how various regimes of cloud evolution impact dust survival, dependent on cloud size, wind properties, and dust grain size.","We find that significant amounts of dust can survive in winds in all scenarios, even without shielding from the cool phase of outflows.","We present an analytic framework that explains this result, along with an analysis of the impact of cloud evolution on the total fraction of dust survival.","Using these results, we estimate that 60 percent of dust that enters a starburst-driven wind could survive to populate the halo, based on a simulated distribution of cloud properties.","We also investigate how these conclusions depend on grain size, exploring grains from 0.1 micron to 10 Angstrom.","Under most circumstances, grains smaller than 0.01 micron cannot withstand hot-phase exposure, suggesting that the small grains observed in the CGM are either formed in situ due to shattering of larger grains, or must be carried there in cool phase outflows.","Finally, we show that the dust-to-gas ratio of clouds declines as a function of distance from the galaxy due to cloud-wind mixing and condensation.","These results provide an explanation for the vast amounts of dust observed in the CGMs of galaxies and beyond."],"url":"http://arxiv.org/abs/2403.03711v1","category":"astro-ph.GA"}
{"created":"2024-03-06 13:18:53","title":"Operational Space and Plasma Performance with an RMP-ELM Suppressed Edge","abstract":"The operational space and global performance of plasmas with edge-localized modes (ELMs) suppressed by resonant magnetic perturbations (RMPs) are surveyed by comparing AUG, DIII-D, EAST, and KSTAR stationary operating points. RMP-ELM suppression is achieved over a range of plasma currents, toroidal fields, and RMP toroidal mode numbers. Consistent operational windows in edge safety factor are found across devices, while windows in plasma shaping parameters are distinct. Accessed pedestal parameters reveal a quantitatively similar pedestal-top density limit for RMP-ELM suppression in all devices of just over 3x1019 m-3. This is surprising given the wide variance of many engineering parameters and edge collisionalities, and poses a challenge to extrapolation of the regime. Wide ranges in input power, confinement time, and stored energy are observed, with the achieved triple product found to scale like the product of current, field, and radius. Observed energy confinement scaling with engineering parameters for RMP-ELM suppressed plasmas are presented and compared with expectations from established H and L-mode scalings, including treatment of uncertainty analysis. Different scaling exponents for individual engineering parameters are found as compared to the established scalings. However, extrapolation to next-step tokamaks ITER and SPARC find overall consistency within uncertainties with the established scalings, finding no obvious performance penalty when extrapolating from the assembled multi-device RMP-ELM suppressed database. Overall this work identifies common physics for RMP-ELM suppression and highlights the need to pursue this no-ELM regime at higher magnetic field and different plasma physical size.","sentences":["The operational space and global performance of plasmas with edge-localized modes (ELMs) suppressed by resonant magnetic perturbations (RMPs) are surveyed by comparing AUG, DIII-D, EAST, and KSTAR stationary operating points.","RMP-ELM suppression is achieved over a range of plasma currents, toroidal fields, and RMP toroidal mode numbers.","Consistent operational windows in edge safety factor are found across devices, while windows in plasma shaping parameters are distinct.","Accessed pedestal parameters reveal a quantitatively similar pedestal-top density limit for RMP-ELM suppression in all devices of just over 3x1019 m-3.","This is surprising given the wide variance of many engineering parameters and edge collisionalities, and poses a challenge to extrapolation of the regime.","Wide ranges in input power, confinement time, and stored energy are observed, with the achieved triple product found to scale like the product of current, field, and radius.","Observed energy confinement scaling with engineering parameters for RMP-ELM suppressed plasmas are presented and compared with expectations from established H and L-mode scalings, including treatment of uncertainty analysis.","Different scaling exponents for individual engineering parameters are found as compared to the established scalings.","However, extrapolation to next-step tokamaks ITER and SPARC find overall consistency within uncertainties with the established scalings, finding no obvious performance penalty when extrapolating from the assembled multi-device RMP-ELM suppressed database.","Overall this work identifies common physics for RMP-ELM suppression and highlights the need to pursue this no-ELM regime at higher magnetic field and different plasma physical size."],"url":"http://arxiv.org/abs/2403.03693v1","category":"physics.plasm-ph"}
{"created":"2024-03-06 13:07:42","title":"3D Object Visibility Prediction in Autonomous Driving","abstract":"With the rapid advancement of hardware and software technologies, research in autonomous driving has seen significant growth. The prevailing framework for multi-sensor autonomous driving encompasses sensor installation, perception, path planning, decision-making, and motion control. At the perception phase, a common approach involves utilizing neural networks to infer 3D bounding box (Bbox) attributes from raw sensor data, including classification, size, and orientation. In this paper, we present a novel attribute and its corresponding algorithm: 3D object visibility. By incorporating multi-task learning, the introduction of this attribute, visibility, negligibly affects the model's effectiveness and efficiency. Our proposal of this attribute and its computational strategy aims to expand the capabilities for downstream tasks, thereby enhancing the safety and reliability of real-time autonomous driving in real-world scenarios.","sentences":["With the rapid advancement of hardware and software technologies, research in autonomous driving has seen significant growth.","The prevailing framework for multi-sensor autonomous driving encompasses sensor installation, perception, path planning, decision-making, and motion control.","At the perception phase, a common approach involves utilizing neural networks to infer 3D bounding box (Bbox) attributes from raw sensor data, including classification, size, and orientation.","In this paper, we present a novel attribute and its corresponding algorithm: 3D object visibility.","By incorporating multi-task learning, the introduction of this attribute, visibility, negligibly affects the model's effectiveness and efficiency.","Our proposal of this attribute and its computational strategy aims to expand the capabilities for downstream tasks, thereby enhancing the safety and reliability of real-time autonomous driving in real-world scenarios."],"url":"http://arxiv.org/abs/2403.03681v1","category":"cs.RO"}
{"created":"2024-03-06 12:29:13","title":"Robust Graph Structure Learning under Heterophily","abstract":"Graph is a fundamental mathematical structure in characterizing relations between different objects and has been widely used on various learning tasks. Most methods implicitly assume a given graph to be accurate and complete. However, real data is inevitably noisy and sparse, which will lead to inferior results. Despite the remarkable success of recent graph representation learning methods, they inherently presume that the graph is homophilic, and largely overlook heterophily, where most connected nodes are from different classes. In this regard, we propose a novel robust graph structure learning method to achieve a high-quality graph from heterophilic data for downstream tasks. We first apply a high-pass filter to make each node more distinctive from its neighbors by encoding structure information into the node features. Then, we learn a robust graph with an adaptive norm characterizing different levels of noise. Afterwards, we propose a novel regularizer to further refine the graph structure. Clustering and semi-supervised classification experiments on heterophilic graphs verify the effectiveness of our method.","sentences":["Graph is a fundamental mathematical structure in characterizing relations between different objects and has been widely used on various learning tasks.","Most methods implicitly assume a given graph to be accurate and complete.","However, real data is inevitably noisy and sparse, which will lead to inferior results.","Despite the remarkable success of recent graph representation learning methods, they inherently presume that the graph is homophilic, and largely overlook heterophily, where most connected nodes are from different classes.","In this regard, we propose a novel robust graph structure learning method to achieve a high-quality graph from heterophilic data for downstream tasks.","We first apply a high-pass filter to make each node more distinctive from its neighbors by encoding structure information into the node features.","Then, we learn a robust graph with an adaptive norm characterizing different levels of noise.","Afterwards, we propose a novel regularizer to further refine the graph structure.","Clustering and semi-supervised classification experiments on heterophilic graphs verify the effectiveness of our method."],"url":"http://arxiv.org/abs/2403.03659v1","category":"cs.LG"}
{"created":"2024-03-06 11:31:29","title":"Some direct and inverse problems for the Restricted Signed sumset in set of integers","abstract":"Given a positive integer $h$ and a nonempty finite set of integers $A=\\{a_{1},a_{2},\\ldots,a_{k}\\}$, the restricted $h$-fold signed sumset of $A$, denoted by $h^{\\wedge}_{\\pm}A$, is defined as $$h^{\\wedge}_{\\pm}A=\\left\\lbrace \\sum_{i=1}^{k} \\lambda_{i} a_{i}: \\lambda_{i} \\in \\left\\lbrace -1, 0, 1\\right\\rbrace \\ \\text{for} \\ i= 1, 2, \\ldots, k \\ \\text{and} \\ \\sum_{i=1}^{k} \\left| \\lambda_{i} \\right| =h\\right\\rbrace.$$ The direct problem associated with this sumset is to find the optimal lower bound of $|h^{\\wedge}_{\\pm}A|$, and the inverse problem associated with this sumset is to determine the structure of the underlying set $A$, when $|h^{\\wedge}_{\\pm}A|$ attains the optimal lower bound. Bhanja, Komatsu and Pandey studied the direct and inverse problem for the restricted $h$-fold signed sumset for $h=2, 3$, and $k$ and conjectured some direct and inverse results for $h \\geq 4$. In this paper, we prove these conjectures for $h=4$. We also prove the direct and inverse theorems for arbitrary $h$ under certain restrictions on the set $A$ which are particular cases of the conjectures. Moreover, we prove these conjectures for arithmetic progressions.","sentences":["Given a positive integer $h$ and a nonempty finite set of integers $A=\\{a_{1},a_{2},\\ldots,a_{k}\\}$, the restricted $h$-fold signed sumset of $A$, denoted by $h^{\\wedge}_{\\pm}A$, is defined as $$h^{\\wedge}_{\\pm}A=\\left\\lbrace \\sum_{i=1}^{k} \\lambda_{i} a_{i}: \\lambda_{i} \\in \\left\\lbrace -1, 0, 1\\right\\rbrace \\ \\text{for} \\ i= 1, 2, \\ldots, k \\ \\text{and} \\ \\sum_{i=1}^{k} \\left| \\lambda_{i} \\right| =h\\right\\rbrace.$$ The direct problem associated with this sumset is to find the optimal lower bound of $|h^{\\wedge}_{\\pm}A|$, and the inverse problem associated with this sumset is to determine the structure of the underlying set $A$, when $|h^{\\wedge}_{\\pm}A|$ attains the optimal lower bound.","Bhanja, Komatsu and Pandey studied the direct and inverse problem for the restricted $h$-fold signed sumset for $h=2, 3$, and $k$ and conjectured some direct and inverse results for $h \\geq","4$.","In this paper, we prove these conjectures for $h=4$. We also prove the direct and inverse theorems for arbitrary $h$ under certain restrictions on the set $A$ which are particular cases of the conjectures.","Moreover, we prove these conjectures for arithmetic progressions."],"url":"http://arxiv.org/abs/2403.03625v1","category":"math.NT"}
{"created":"2024-03-06 11:25:09","title":"\"My lollipop dropped...\"-Probing Design Opportunities for SEL Agents through Children's Peer Co-Creation of Social-Emotional Stories","abstract":"This Late-Breaking Work explores the significance of socio-emotional learning (SEL) and the challenges inherent in designing child-appropriate technologies, namely storytelling agents, to support SEL. We aim to probe their needs and preferences regarding agents for SEL by conducting co-design which involves children co-creating characters and social-emotional stories. We conducted collaborative story-making activities with children aged four to six years old. Our findings could inform the design of both verbal and nonverbal interactions of agents, which are to be aligned with children's understanding and interest. Based on the child-led peer co-design, our work enhances the understanding of SEL agent designs and behaviors tailored to children's socio-emotional needs, thereby offering practical implications for more effective SEL tools in future HCI research and practice.","sentences":["This Late-Breaking Work explores the significance of socio-emotional learning (SEL) and the challenges inherent in designing child-appropriate technologies, namely storytelling agents, to support SEL.","We aim to probe their needs and preferences regarding agents for SEL by conducting co-design which involves children co-creating characters and social-emotional stories.","We conducted collaborative story-making activities with children aged four to six years old.","Our findings could inform the design of both verbal and nonverbal interactions of agents, which are to be aligned with children's understanding and interest.","Based on the child-led peer co-design, our work enhances the understanding of SEL agent designs and behaviors tailored to children's socio-emotional needs, thereby offering practical implications for more effective SEL tools in future HCI research and practice."],"url":"http://arxiv.org/abs/2403.03618v1","category":"cs.HC"}
{"created":"2024-03-06 10:48:09","title":"Multi-time-step coupling of peridynamics and classical continuum mechanics for dynamic brittle fracture","abstract":"Peridynamics (PD), as a nonlocal theory, is well-suited for solving problems with discontinuities, such as cracks. However, the nonlocal effect of peridynamics makes it computationally expensive for dynamic fracture problems in large-scale engineering applications. As an alternative, this study proposes a multi-time-step (MTS) coupling model of PD and classical continuum mechanics (CCM) based on the Arlequin framework. Peridynamics is applied to the fracture domain of the structure, while continuum mechanics is applied to the rest of the structure. The MTS method enables the peridynamic model to be solved at a small time step and the continuum mechanical model is solved at a larger time step. Consequently, higher computational efficiency is achieved for the fracture domain of the structure while ensuring computational accuracy, and this coupling method can be easily applied to large-scale engineering fracture problems.","sentences":["Peridynamics (PD), as a nonlocal theory, is well-suited for solving problems with discontinuities, such as cracks.","However, the nonlocal effect of peridynamics makes it computationally expensive for dynamic fracture problems in large-scale engineering applications.","As an alternative, this study proposes a multi-time-step (MTS) coupling model of PD and classical continuum mechanics (CCM) based on the Arlequin framework.","Peridynamics is applied to the fracture domain of the structure, while continuum mechanics is applied to the rest of the structure.","The MTS method enables the peridynamic model to be solved at a small time step and the continuum mechanical model is solved at a larger time step.","Consequently, higher computational efficiency is achieved for the fracture domain of the structure while ensuring computational accuracy, and this coupling method can be easily applied to large-scale engineering fracture problems."],"url":"http://arxiv.org/abs/2403.03605v1","category":"cs.CE"}
{"created":"2024-03-06 09:08:11","title":"Resonant perturbation of a family of young asteroids associated with (5026) Martes","abstract":"The orbital dynamics of a very young asteroid pair (5026) Martes and 2005 WW113 is studied. We detect strong resonant perturbations of the larger member of the pair (5026) Martes by the 3:11 mean motion resonance with the Earth. The second asteroid of the pair (2005 WW113) has orbited far from the resonance and is not perturbed. We provide a new estimation of the resonance structure and found that, under the planetary perturbations, a single resonance splits into a multiplet. The nominal position of the resonance is 2.37783 AU. However, the center of the corresponding chaotic zone is detected at 2.37754 AU. As a result, we noted a small but not negligible difference between the calculated and observed position of resonance. The multiplet structure of the 3:11 Earth resonance cannot explain this offset (the position of the main term of the multiplet is 2.377698 AU).","sentences":["The orbital dynamics of a very young asteroid pair (5026) Martes and 2005 WW113 is studied.","We detect strong resonant perturbations of the larger member of the pair (5026)","Martes by the 3:11 mean motion resonance with the Earth.","The second asteroid of the pair (2005 WW113) has orbited far from the resonance and is not perturbed.","We provide a new estimation of the resonance structure and found that, under the planetary perturbations, a single resonance splits into a multiplet.","The nominal position of the resonance is 2.37783 AU.","However, the center of the corresponding chaotic zone is detected at 2.37754 AU.","As a result, we noted a small but not negligible difference between the calculated and observed position of resonance.","The multiplet structure of the 3:11 Earth resonance cannot explain this offset (the position of the main term of the multiplet is 2.377698 AU)."],"url":"http://arxiv.org/abs/2403.03559v1","category":"astro-ph.EP"}
{"created":"2024-03-06 08:59:54","title":"Application of Nash equilibrium for developing an optimal forest harvesting strategy in Toru\u0144 Forest District","abstract":"This study investigates the application of Nash equilibrium strategies in optimizing forest harvesting decisions, focusing on multiple management objectives in forestry. Through simulation-based analysis, the research explores the evolution of various indicators during the game: 1) the mass of CO2 sequestration, 2) forest stands biodiversity, 3) the harvested wood volume, 4) native species fraction, and 5) protective functions. The results underscore the importance of considering diverse objectives and balancing competing interests in forestry decision processes. The forest stands designated for harvesting in the Toru\\'n Forest District were defined as the initial strategy, and indicators for all objectives were calculated accordingly. A Nash equilibrium was identified through a game involving five players representing individual objectives with partially conflicting aims. The final strategy was obtained by modifying specific forest stands designated for harvesting, thereby maintaining the planned wood volume extraction while simultaneously reducing biodiversity loss by nearly 40%, preserving protective functions across over 600 hectares of forested areas, enhancing decadal carbon sequestration in the forest district by 100,000 tons, and additionally improving species suitability by nearly 10%. The findings suggest the potential for further research and refinement of Nash equilibrium-based optimization approaches to enhance the effectiveness and sustainability of forest management practices.","sentences":["This study investigates the application of Nash equilibrium strategies in optimizing forest harvesting decisions, focusing on multiple management objectives in forestry.","Through simulation-based analysis, the research explores the evolution of various indicators during the game: 1) the mass of CO2 sequestration, 2) forest stands biodiversity, 3) the harvested wood volume, 4) native species fraction, and 5) protective functions.","The results underscore the importance of considering diverse objectives and balancing competing interests in forestry decision processes.","The forest stands designated for harvesting in the Toru\\'n Forest District were defined as the initial strategy, and indicators for all objectives were calculated accordingly.","A Nash equilibrium was identified through a game involving five players representing individual objectives with partially conflicting aims.","The final strategy was obtained by modifying specific forest stands designated for harvesting, thereby maintaining the planned wood volume extraction while simultaneously reducing biodiversity loss by nearly 40%, preserving protective functions across over 600 hectares of forested areas, enhancing decadal carbon sequestration in the forest district by 100,000 tons, and additionally improving species suitability by nearly 10%.","The findings suggest the potential for further research and refinement of Nash equilibrium-based optimization approaches to enhance the effectiveness and sustainability of forest management practices."],"url":"http://arxiv.org/abs/2403.03555v1","category":"cs.GT"}
{"created":"2024-03-06 08:56:07","title":"Tailoring the Nucleation and Growth of Silver Nanoparticles by Sputtering Deposition under Acoustic Wave Activation. Assessment of Plasma Conditions and 2D Patterning Phenomena","abstract":"Early results on the plasma deposition of dielectric thin films on acoustic wave (AW) activated substrates revealed a densification pattern arisen from the focusing of plasma ions and their impact on specific areas of the piezoelectric substrate. Herein, we extend this methodology to tailor the plasma deposition of metals onto AW-activated LiNbO3 piezoelectric substrates. Our investigation reveals the tracking of the initial stages of nanoparticle (NP) formation and growth during the submonolayer deposition of silver. We elucidate the specific role of AW activation in reducing particle size, enhancing particle circularity, and retarding NP agglomeration and account for the physical phenomena making these processes differ from those occurring on non-activated substrates. We provide a comparative analysis of the results obtained under two representative plasma conditions: diode DC sputtering and magnetron sputtering. In the latter case, the AW activation gives rise to a 2D pattern of domains with different amounts of silver and a distinct size and circularity for the silver NPs. This difference was attributed to the specific characteristics of the plasma sheath formed onto the substrate in each case. The possibilities of tuning the plasmon resonance absorption of silver NPs by AW activation of the sputtering deposition process are discussed.","sentences":["Early results on the plasma deposition of dielectric thin films on acoustic wave (AW) activated substrates revealed a densification pattern arisen from the focusing of plasma ions and their impact on specific areas of the piezoelectric substrate.","Herein, we extend this methodology to tailor the plasma deposition of metals onto AW-activated LiNbO3 piezoelectric substrates.","Our investigation reveals the tracking of the initial stages of nanoparticle (NP) formation and growth during the submonolayer deposition of silver.","We elucidate the specific role of AW activation in reducing particle size, enhancing particle circularity, and retarding NP agglomeration and account for the physical phenomena making these processes differ from those occurring on non-activated substrates.","We provide a comparative analysis of the results obtained under two representative plasma conditions: diode DC sputtering and magnetron sputtering.","In the latter case, the AW activation gives rise to a 2D pattern of domains with different amounts of silver and a distinct size and circularity for the silver NPs.","This difference was attributed to the specific characteristics of the plasma sheath formed onto the substrate in each case.","The possibilities of tuning the plasmon resonance absorption of silver NPs by AW activation of the sputtering deposition process are discussed."],"url":"http://arxiv.org/abs/2403.03553v1","category":"physics.app-ph"}
{"created":"2024-03-06 08:37:35","title":"Contraction rates and projection subspace estimation with Gaussian process priors in high dimension","abstract":"This work explores the dimension reduction problem for Bayesian nonparametric regression and density estimation. More precisely, we are interested in estimating a functional parameter $f$ over the unit ball in $\\mathbb{R}^d$, which depends only on a $d_0$-dimensional subspace of $\\mathbb{R}^d$, with $d_0 < d$.It is well-known that rescaled Gaussian process priors over the function space achieve smoothness adaptation and posterior contraction with near minimax-optimal rates. Moreover, hierarchical extensions of this approach, equipped with subspace projection, can also adapt to the intrinsic dimension $d_0$ (\\cite{Tokdar2011DimensionAdapt}).When the ambient dimension $d$ does not vary with $n$, the minimax rate remains of the order $n^{-\\beta/(2\\beta +d_0)}$.%When $d$ does not vary with $n$, the order of the minimax rate remains the same regardless of the ambient dimension $d$. However, this is up to multiplicative constants that can become prohibitively large when $d$ grows. The dependences between the contraction rate and the ambient dimension have not been fully explored yet and this work provides a first insight: we let the dimension $d$ grow with $n$ and, by combining the arguments of \\cite{Tokdar2011DimensionAdapt} and \\cite{Jiang2021VariableSelection}, we derive a growth rate for $d$ that still leads to posterior consistency with minimax rate.The optimality of this growth rate is then discussed.Additionally, we provide a set of assumptions under which consistent estimation of $f$ leads to a correct estimation of the subspace projection, assuming that $d_0$ is known.","sentences":["This work explores the dimension reduction problem for Bayesian nonparametric regression and density estimation.","More precisely, we are interested in estimating a functional parameter $f$ over the unit ball in $\\mathbb{R}^d$, which depends only on a $d_0$-dimensional subspace of $\\mathbb{R}^d$, with $d_0 <","d$.It is well-known that rescaled Gaussian process priors over the function space achieve smoothness adaptation and posterior contraction with near minimax-optimal rates.","Moreover, hierarchical extensions of this approach, equipped with subspace projection, can also adapt to the intrinsic dimension $d_0$ (\\cite{Tokdar2011DimensionAdapt}).When the ambient dimension $d$ does not vary with $n$, the minimax rate remains of the order $n^{-\\beta/(2\\beta","+d_0)}$.%When $d$ does not vary with $n$, the order of the minimax rate remains the same regardless of the ambient dimension $d$.","However, this is up to multiplicative constants that can become prohibitively large when $d$ grows.","The dependences between the contraction rate and the ambient dimension have not been fully explored yet and this work provides a first insight: we let the dimension $d$ grow with $n$ and, by combining the arguments of \\cite{Tokdar2011DimensionAdapt} and \\cite{Jiang2021VariableSelection}, we derive a growth rate for $d$ that still leads to posterior consistency with minimax rate.","The optimality of this growth rate is then discussed.","Additionally, we provide a set of assumptions under which consistent estimation of $f$ leads to a correct estimation of the subspace projection, assuming that $d_0$ is known."],"url":"http://arxiv.org/abs/2403.03540v1","category":"math.ST"}
{"created":"2024-03-06 07:46:40","title":"Lattice thermal conductivity and mechanical properties of the single-layer penta-NiN2 explored by a deep-learning interatomic potential","abstract":"Penta-NiN2, a novel pentagonal 2D sheet with potential nanoelectronic applications, is investigated in terms of its lattice thermal conductivity, stability, and mechanical behavior. A deep learning interatomic potential (DLP) is firstly generated from ab-initio molecular dynamics (AIMD) data and then utilized for classical molecular dynamics simulations. The DLP's accuracy is verified, showing strong agreement with AIMD results. The dependence of thermal conductivity on size, temperature, and tensile strain, reveals important insights into the material's thermal properties. Additionally, the mechanical response of penta-NiN2 under uniaxial loading is examined, yielding a Young's modulus of approximately 368 GPa. The influence of vacancy defects on mechanical properties is analyzed, demonstrating significant reduction in modulus, fracture stress, and ultimate strength. This study also investigates the influence of strain on phonon dispersion relations and phonon group velocity in penta-NiN2, shedding light on how alterations in the atomic lattice affect the phonon dynamics and, consequently, impact the thermal conductivity. This investigation showcases the ability of deep learning based interatomic potentials in studying the properties of 2D Penta-NiN2.","sentences":["Penta-NiN2, a novel pentagonal 2D sheet with potential nanoelectronic applications, is investigated in terms of its lattice thermal conductivity, stability, and mechanical behavior.","A deep learning interatomic potential (DLP) is firstly generated from ab-initio molecular dynamics (AIMD) data and then utilized for classical molecular dynamics simulations.","The DLP's accuracy is verified, showing strong agreement with AIMD results.","The dependence of thermal conductivity on size, temperature, and tensile strain, reveals important insights into the material's thermal properties.","Additionally, the mechanical response of penta-NiN2 under uniaxial loading is examined, yielding a Young's modulus of approximately 368 GPa.","The influence of vacancy defects on mechanical properties is analyzed, demonstrating significant reduction in modulus, fracture stress, and ultimate strength.","This study also investigates the influence of strain on phonon dispersion relations and phonon group velocity in penta-NiN2, shedding light on how alterations in the atomic lattice affect the phonon dynamics and, consequently, impact the thermal conductivity.","This investigation showcases the ability of deep learning based interatomic potentials in studying the properties of 2D Penta-NiN2."],"url":"http://arxiv.org/abs/2403.03515v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-06 07:21:34","title":"Unveiling the Complete Variant of Spherical Robots","abstract":"This study presents a systematic enumeration of spherical ($SO(3)$) type parallel robots' variants using an analytical velocity-level approach. These robots are known for their ability to perform arbitrary rotations around a fixed point, making them suitable for numerous applications. Despite their architectural diversity, existing research has predominantly approached them on a case-by-case basis. This approach hinders the exploration of all possible variants, thereby limiting the benefits derived from architectural diversity. By employing a generalized analytical approach through the reciprocal screw method, we systematically explore all the kinematic conditions for limbs yielding $SO(3)$ motion.Consequently, all 73 possible types of non-redundant limbs suitable for generating the target $SO(3)$ motion are identified. The approach involves performing an in-depth algebraic motion-constraint analysis and identifying common characteristics among different variants. This leads us to systematically explore all 73 symmetric and 5256 asymmetric variants, which in turn become a total of 5329, each potentially having different workspace capability, stiffness performance, and dynamics. Hence, having all these variants can facilitate the innovation of novel spherical robots and help us easily find the best and optimal ones for our specific applications.","sentences":["This study presents a systematic enumeration of spherical ($SO(3)$) type parallel robots' variants using an analytical velocity-level approach.","These robots are known for their ability to perform arbitrary rotations around a fixed point, making them suitable for numerous applications.","Despite their architectural diversity, existing research has predominantly approached them on a case-by-case basis.","This approach hinders the exploration of all possible variants, thereby limiting the benefits derived from architectural diversity.","By employing a generalized analytical approach through the reciprocal screw method, we systematically explore all the kinematic conditions for limbs yielding $SO(3)$ motion.","Consequently, all 73 possible types of non-redundant limbs suitable for generating the target $SO(3)$ motion are identified.","The approach involves performing an in-depth algebraic motion-constraint analysis and identifying common characteristics among different variants.","This leads us to systematically explore all 73 symmetric and 5256 asymmetric variants, which in turn become a total of 5329, each potentially having different workspace capability, stiffness performance, and dynamics.","Hence, having all these variants can facilitate the innovation of novel spherical robots and help us easily find the best and optimal ones for our specific applications."],"url":"http://arxiv.org/abs/2403.03505v1","category":"cs.RO"}
{"created":"2024-03-06 06:39:43","title":"VastTrack: Vast Category Visual Object Tracking","abstract":"In this paper, we introduce a novel benchmark, dubbed VastTrack, towards facilitating the development of more general visual tracking via encompassing abundant classes and videos. VastTrack possesses several attractive properties: (1) Vast Object Category. In particular, it covers target objects from 2,115 classes, largely surpassing object categories of existing popular benchmarks (e.g., GOT-10k with 563 classes and LaSOT with 70 categories). With such vast object classes, we expect to learn more general object tracking. (2) Larger scale. Compared with current benchmarks, VastTrack offers 50,610 sequences with 4.2 million frames, which makes it to date the largest benchmark regarding the number of videos, and thus could benefit training even more powerful visual trackers in the deep learning era. (3) Rich Annotation. Besides conventional bounding box annotations, VastTrack also provides linguistic descriptions for the videos. The rich annotations of VastTrack enables development of both the vision-only and the vision-language tracking. To ensure precise annotation, all videos are manually labeled with multiple rounds of careful inspection and refinement. To understand performance of existing trackers and to provide baselines for future comparison, we extensively assess 25 representative trackers. The results, not surprisingly, show significant drops compared to those on current datasets due to lack of abundant categories and videos from diverse scenarios for training, and more efforts are required to improve general tracking. Our VastTrack and all the evaluation results will be made publicly available https://github.com/HengLan/VastTrack.","sentences":["In this paper, we introduce a novel benchmark, dubbed VastTrack, towards facilitating the development of more general visual tracking via encompassing abundant classes and videos.","VastTrack possesses several attractive properties: (1) Vast Object Category.","In particular, it covers target objects from 2,115 classes, largely surpassing object categories of existing popular benchmarks (e.g., GOT-10k with 563 classes and LaSOT with 70 categories).","With such vast object classes, we expect to learn more general object tracking.","(2) Larger scale.","Compared with current benchmarks, VastTrack offers 50,610 sequences with 4.2 million frames, which makes it to date the largest benchmark regarding the number of videos, and thus could benefit training even more powerful visual trackers in the deep learning era.","(3) Rich Annotation.","Besides conventional bounding box annotations, VastTrack also provides linguistic descriptions for the videos.","The rich annotations of VastTrack enables development of both the vision-only and the vision-language tracking.","To ensure precise annotation, all videos are manually labeled with multiple rounds of careful inspection and refinement.","To understand performance of existing trackers and to provide baselines for future comparison, we extensively assess 25 representative trackers.","The results, not surprisingly, show significant drops compared to those on current datasets due to lack of abundant categories and videos from diverse scenarios for training, and more efforts are required to improve general tracking.","Our VastTrack and all the evaluation results will be made publicly available https://github.com/HengLan/VastTrack."],"url":"http://arxiv.org/abs/2403.03493v1","category":"cs.CV"}
{"created":"2024-03-06 06:27:21","title":"A comparative study of cosmological constraints from weak lensing using Convolutional Neural Networks","abstract":"Weak Lensing (WL) surveys are reaching unprecedented depths, enabling the investigation of very small angular scales. At these scales, nonlinear gravitational effects lead to higher-order correlations making the matter distribution highly non-Gaussian. Extracting this information using traditional statistics has proven difficult, and Machine Learning based summary statistics have emerged as a powerful alternative. We explore the capabilities of a discriminative, Convolutional Neural Networks (CNN) based approach, focusing on parameter constraints in the ($\\Omega_m$, $\\sigma_8$) cosmological parameter space. Leveraging novel training loss functions and network representations on WL mock datasets without baryons, we show that our models achieve $\\sim 5$ times stronger constraints than the power spectrum, $\\sim 3$ stronger constraints than peak counts, and $\\sim 2$ stronger constraints than previous CNN-learned summary statistics and scattering transforms, for noise levels relevant to Rubin or Euclid. For WL convergence maps with baryonic physics, our models achieve $\\sim 2.3$ times stronger constraining power than the power spectrum at these noise levels, also outperforming previous summary statistics. To further explore the possibilities of CNNs for this task, we also discuss transfer learning where we adapt pre-trained models, trained on different tasks or datasets, for cosmological inference, finding that these do not improve the performance.","sentences":["Weak Lensing (WL) surveys are reaching unprecedented depths, enabling the investigation of very small angular scales.","At these scales, nonlinear gravitational effects lead to higher-order correlations making the matter distribution highly non-Gaussian.","Extracting this information using traditional statistics has proven difficult, and Machine Learning based summary statistics have emerged as a powerful alternative.","We explore the capabilities of a discriminative, Convolutional Neural Networks (CNN) based approach, focusing on parameter constraints in the ($\\Omega_m$, $\\sigma_8$) cosmological parameter space.","Leveraging novel training loss functions and network representations on WL mock datasets without baryons, we show that our models achieve $\\sim 5$ times stronger constraints than the power spectrum, $\\sim 3$ stronger constraints than peak counts, and $\\sim 2$ stronger constraints than previous CNN-learned summary statistics and scattering transforms, for noise levels relevant to Rubin or Euclid.","For WL convergence maps with baryonic physics, our models achieve $\\sim 2.3$ times stronger constraining power than the power spectrum at these noise levels, also outperforming previous summary statistics.","To further explore the possibilities of CNNs for this task, we also discuss transfer learning where we adapt pre-trained models, trained on different tasks or datasets, for cosmological inference, finding that these do not improve the performance."],"url":"http://arxiv.org/abs/2403.03490v1","category":"astro-ph.CO"}
{"created":"2024-03-06 05:32:48","title":"Korovkin-type approximation for non positive operators","abstract":"This article investigates the convergence properties of Grunwald-type operators defined using the Lagrange interpolation operator. In $C[0, \\pi]$, we establish sufficient conditions under which alternate grid point selection yields a convergence result. Employing a quantitative approach grounded in the modulus of continuity, we derive a theorem that provides insightful estimates for the convergence of Grunwald operator, accompanied by illustrative numerical examples. Addressing challenges related to continuity, we extend the operator to $L^1(\\mathbb{R})$ and prove similar convergence results. Notably, the new operator demonstrates a uniform order of convergence on a dense subspace of $L^1(\\mathbb{R})$. Moreover, it has a better convergence rate than Grunwald operator. We establish the non positivity of the Lagrange interpolation operator and, consequently, the constructed operators. Significant emphasis is placed on D.E. Wulbert theorem ($1968$), which is a Korovkin-type theorem that can be applied to non-positive operators. We extend this theorem to a broader class of operators, enabling its effective application to the constructed operators. This article, overall, highlights the broad applicability of Lagrange interpolation across diverse disciplines.","sentences":["This article investigates the convergence properties of Grunwald-type operators defined using the Lagrange interpolation operator.","In $C[0, \\pi]$, we establish sufficient conditions under which alternate grid point selection yields a convergence result.","Employing a quantitative approach grounded in the modulus of continuity, we derive a theorem that provides insightful estimates for the convergence of Grunwald operator, accompanied by illustrative numerical examples.","Addressing challenges related to continuity, we extend the operator to $L^1(\\mathbb{R})$ and prove similar convergence results.","Notably, the new operator demonstrates a uniform order of convergence on a dense subspace of $L^1(\\mathbb{R})$. Moreover, it has a better convergence rate than Grunwald operator.","We establish the non positivity of the Lagrange interpolation operator and, consequently, the constructed operators.","Significant emphasis is placed on D.E. Wulbert theorem ($1968$), which is a Korovkin-type theorem that can be applied to non-positive operators.","We extend this theorem to a broader class of operators, enabling its effective application to the constructed operators.","This article, overall, highlights the broad applicability of Lagrange interpolation across diverse disciplines."],"url":"http://arxiv.org/abs/2403.03476v1","category":"math.FA"}
{"created":"2024-03-06 05:30:26","title":"Non-homogeneous anisotropic bulk viscosity for acoustic wave attenuation in weakly compressible methods","abstract":"A major limitation of the weakly compressible approaches to simulate incompressible flows is the appearance of artificial acoustic waves that introduce a large mass conservation error and lead to spurious oscillations in the force coefficients. In this work, we propose a non-homogeneous anisotropic bulk viscosity term to effectively damp the acoustic waves. By implementing this term in a computational framework based on the recently proposed general pressure equation, we demonstrate that the non-homogeneous and anisotropic nature of the term makes it significantly more effective than the isotropic homogeneous version widely used in the literature. Moreover, it is computationally more efficient than the pressure (or mass) diffusion term, which is an alternative mechanism used to suppress acoustic waves. We simulate a range of benchmark problems to comprehensively investigate the performance of the bulk viscosity on the effective suppression of acoustic waves, mass conservation error, order of convergence of the solver, and computational efficiency. The proposed form of the bulk viscosity enables fairly accurate modelling of the initial transients of unsteady simulations, which is highly challenging for weakly compressible approaches, and to the best of our knowledge, existing approaches can't provide an accurate prediction of such transients.","sentences":["A major limitation of the weakly compressible approaches to simulate incompressible flows is the appearance of artificial acoustic waves that introduce a large mass conservation error and lead to spurious oscillations in the force coefficients.","In this work, we propose a non-homogeneous anisotropic bulk viscosity term to effectively damp the acoustic waves.","By implementing this term in a computational framework based on the recently proposed general pressure equation, we demonstrate that the non-homogeneous and anisotropic nature of the term makes it significantly more effective than the isotropic homogeneous version widely used in the literature.","Moreover, it is computationally more efficient than the pressure (or mass) diffusion term, which is an alternative mechanism used to suppress acoustic waves.","We simulate a range of benchmark problems to comprehensively investigate the performance of the bulk viscosity on the effective suppression of acoustic waves, mass conservation error, order of convergence of the solver, and computational efficiency.","The proposed form of the bulk viscosity enables fairly accurate modelling of the initial transients of unsteady simulations, which is highly challenging for weakly compressible approaches, and to the best of our knowledge, existing approaches can't provide an accurate prediction of such transients."],"url":"http://arxiv.org/abs/2403.03475v1","category":"physics.flu-dyn"}
{"created":"2024-03-06 05:00:31","title":"Self-Attention Empowered Graph Convolutional Network for Structure Learning and Node Embedding","abstract":"In representation learning on graph-structured data, many popular graph neural networks (GNNs) fail to capture long-range dependencies, leading to performance degradation. Furthermore, this weakness is magnified when the concerned graph is characterized by heterophily (low homophily). To solve this issue, this paper proposes a novel graph learning framework called the graph convolutional network with self-attention (GCN-SA). The proposed scheme exhibits an exceptional generalization capability in node-level representation learning. The proposed GCN-SA contains two enhancements corresponding to edges and node features. For edges, we utilize a self-attention mechanism to design a stable and effective graph-structure-learning module that can capture the internal correlation between any pair of nodes. This graph-structure-learning module can identify reliable neighbors for each node from the entire graph. Regarding the node features, we modify the transformer block to make it more applicable to enable GCN to fuse valuable information from the entire graph. These two enhancements work in distinct ways to help our GCN-SA capture long-range dependencies, enabling it to perform representation learning on graphs with varying levels of homophily. The experimental results on benchmark datasets demonstrate the effectiveness of the proposed GCN-SA. Compared to other outstanding GNN counterparts, the proposed GCN-SA is competitive.","sentences":["In representation learning on graph-structured data, many popular graph neural networks (GNNs) fail to capture long-range dependencies, leading to performance degradation.","Furthermore, this weakness is magnified when the concerned graph is characterized by heterophily (low homophily).","To solve this issue, this paper proposes a novel graph learning framework called the graph convolutional network with self-attention (GCN-SA).","The proposed scheme exhibits an exceptional generalization capability in node-level representation learning.","The proposed GCN-SA contains two enhancements corresponding to edges and node features.","For edges, we utilize a self-attention mechanism to design a stable and effective graph-structure-learning module that can capture the internal correlation between any pair of nodes.","This graph-structure-learning module can identify reliable neighbors for each node from the entire graph.","Regarding the node features, we modify the transformer block to make it more applicable to enable GCN to fuse valuable information from the entire graph.","These two enhancements work in distinct ways to help our GCN-SA capture long-range dependencies, enabling it to perform representation learning on graphs with varying levels of homophily.","The experimental results on benchmark datasets demonstrate the effectiveness of the proposed GCN-SA.","Compared to other outstanding GNN counterparts, the proposed GCN-SA is competitive."],"url":"http://arxiv.org/abs/2403.03465v1","category":"cs.LG"}
{"created":"2024-03-06 04:05:32","title":"Weak approximation of Schr\u00f6dinger-F\u00f6llmer diffusion","abstract":"We consider the weak convergence of the Euler-Maruyama approximation for Schr\\\"odinger-F\\\"ollmer diffusions, which are solutions of Schr\\\"odinger bridge problems and can be used for sampling from given distributions. We show that the distribution of the terminal random variable of the time-discretized process weakly converges to the target one under mild regularity conditions.","sentences":["We consider the weak convergence of the Euler-Maruyama approximation for Schr\\\"odinger-F\\\"ollmer diffusions, which are solutions of Schr\\\"odinger bridge problems and can be used for sampling from given distributions.","We show that the distribution of the terminal random variable of the time-discretized process weakly converges to the target one under mild regularity conditions."],"url":"http://arxiv.org/abs/2403.03446v1","category":"math.PR"}
{"created":"2024-03-06 03:56:14","title":"Line defect half-indices of $SU(N)$ Chern-Simons theories","abstract":"We study the Wilson line defect half-indices of 3d $\\mathcal{N}=2$ supersymmetric $SU(N)$ Chern-Simons theories of level $k\\le -N$ with Neumann boundary conditions for the gauge fields, together with 2d Fermi multiplets and fundamental 3d chirals multiplets to cancel the gauge anomaly. We derive some exact results and also make some conjectures based on expansions of the $q$-series. We find several interesting connections with special functions known in the literature, including Rogers-Rumanujan functions for which we conjecture integral representations, and the appearance of Appell-Lerch sums for certain Wilson line half-index grand canonical ensembles which reveal an unexpected appearance of mock modular functions. We also find intriguing $q$-difference equations relating half-indices to Wilson line half-indices. Some of these results also have a description in terms of a dual theory with Dirichlet boundary conditions for the vector multiplet in the dual theory.","sentences":["We study the Wilson line defect half-indices of 3d $\\mathcal{N}=2$ supersymmetric $SU(N)$ Chern-Simons theories of level $k\\le -N$ with Neumann boundary conditions for the gauge fields, together with 2d Fermi multiplets and fundamental 3d chirals multiplets to cancel the gauge anomaly.","We derive some exact results and also make some conjectures based on expansions of the $q$-series.","We find several interesting connections with special functions known in the literature, including Rogers-Rumanujan functions for which we conjecture integral representations, and the appearance of Appell-Lerch sums for certain Wilson line half-index grand canonical ensembles which reveal an unexpected appearance of mock modular functions.","We also find intriguing $q$-difference equations relating half-indices to Wilson line half-indices.","Some of these results also have a description in terms of a dual theory with Dirichlet boundary conditions for the vector multiplet in the dual theory."],"url":"http://arxiv.org/abs/2403.03439v1","category":"hep-th"}
{"created":"2024-03-06 02:37:26","title":"Prediction Of Cryptocurrency Prices Using LSTM, SVM And Polynomial Regression","abstract":"The rapid development of information technology, especially the Internet, has facilitated users with a quick and easy way to seek information. With these convenience offered by internet services, many individuals who initially invested in gold and precious metals are now shifting into digital investments in form of cryptocurrencies. However, investments in crypto coins are filled with uncertainties and fluctuation in daily basis. This risk posed as significant challenges for coin investors that could result in substantial investment losses. The uncertainty of the value of these crypto coins is a critical issue in the field of coin investment. Forecasting, is one of the methods used to predict the future value of these crypto coins. By utilizing the models of Long Short Term Memory, Support Vector Machine, and Polynomial Regression algorithm for forecasting, a performance comparison is conducted to determine which algorithm model is most suitable for predicting crypto currency prices. The mean square error is employed as a benchmark for the comparison. By applying those three constructed algorithm models, the Support Vector Machine uses a linear kernel to produce the smallest mean square error compared to the Long Short Term Memory and Polynomial Regression algorithm models, with a mean square error value of 0.02. Keywords: Cryptocurrency, Forecasting, Long Short Term Memory, Mean Square Error, Polynomial Regression, Support Vector Machine","sentences":["The rapid development of information technology, especially the Internet, has facilitated users with a quick and easy way to seek information.","With these convenience offered by internet services, many individuals who initially invested in gold and precious metals are now shifting into digital investments in form of cryptocurrencies.","However, investments in crypto coins are filled with uncertainties and fluctuation in daily basis.","This risk posed as significant challenges for coin investors that could result in substantial investment losses.","The uncertainty of the value of these crypto coins is a critical issue in the field of coin investment.","Forecasting, is one of the methods used to predict the future value of these crypto coins.","By utilizing the models of Long Short Term Memory, Support Vector Machine, and Polynomial Regression algorithm for forecasting, a performance comparison is conducted to determine which algorithm model is most suitable for predicting crypto currency prices.","The mean square error is employed as a benchmark for the comparison.","By applying those three constructed algorithm models, the Support Vector Machine uses a linear kernel to produce the smallest mean square error compared to the Long Short Term Memory and Polynomial Regression algorithm models, with a mean square error value of 0.02.","Keywords: Cryptocurrency, Forecasting, Long Short Term Memory, Mean Square Error, Polynomial Regression, Support Vector Machine"],"url":"http://arxiv.org/abs/2403.03410v1","category":"cs.LG"}
{"created":"2024-03-06 01:46:25","title":"Parameterized Post-Tolman-Oppenheimer-Volkoff Framework for Screened Modified Gravity with an Application to the Secondary Component of GW190814","abstract":"The secondary component of GW190814 has mass in the range $2.5$--$2.67{\\rm M}_\\odot$, placing it within the lower mass gap separating neutron stars from black holes. According to the predictions of general relativity and state-of-the-art nuclear equations of state, this object is too heavy to be a neutron star.~In this work, we explore the possibility that this object is a neutron star under the hypothesis that general relativity is modified to include screening mechanisms, and that the neutron star formed in an unscreened environment. We introduce a set of parameterized-post-Tolman-Oppenheimer-Volkoff (post-TOV) equations appropriate for screened modified gravity whose free parameters are environment-dependent. We find that it is possible that the GW190814 secondary could be a neutron star that formed in an unscreened environment for a range of reasonable post-TOV parameters.","sentences":["The secondary component of GW190814 has mass in the range $2.5$--$2.67{\\rm M}_\\odot$, placing it within the lower mass gap separating neutron stars from black holes.","According to the predictions of general relativity and state-of-the-art nuclear equations of state, this object is too heavy to be a neutron star.~In this work, we explore the possibility that this object is a neutron star under the hypothesis that general relativity is modified to include screening mechanisms, and that the neutron star formed in an unscreened environment.","We introduce a set of parameterized-post-Tolman-Oppenheimer-Volkoff (post-TOV) equations appropriate for screened modified gravity whose free parameters are environment-dependent.","We find that it is possible that the GW190814 secondary could be a neutron star that formed in an unscreened environment for a range of reasonable post-TOV parameters."],"url":"http://arxiv.org/abs/2403.03399v1","category":"gr-qc"}
{"created":"2024-03-06 01:03:13","title":"Pulse shape discrimination in an organic scintillation phoswich detector using machine learning techniques","abstract":"We developed machine learning algorithms for distinguishing scintillation signals from a plastic-liquid coupled detector known as a phoswich. The challenge lies in discriminating signals from organic scintillators with similar shapes and short decay times. Using a single-readout phoswich detector, we successfully identified $\\gamma$ radiation signals from two scintillating components. Our Boosted Decision Tree algorithm demonstrated a maximum discrimination power of 3.02 $\\pm$ 0.85 standard deviation in the 950 keV region, providing an efficient solution for self-shielding and enhancing radiation detection capabilities.","sentences":["We developed machine learning algorithms for distinguishing scintillation signals from a plastic-liquid coupled detector known as a phoswich.","The challenge lies in discriminating signals from organic scintillators with similar shapes and short decay times.","Using a single-readout phoswich detector, we successfully identified $\\gamma$ radiation signals from two scintillating components.","Our Boosted Decision Tree algorithm demonstrated a maximum discrimination power of 3.02 $\\pm$ 0.85 standard deviation in the 950 keV region, providing an efficient solution for self-shielding and enhancing radiation detection capabilities."],"url":"http://arxiv.org/abs/2403.03392v1","category":"physics.ins-det"}
{"created":"2024-03-06 00:28:24","title":"A Probabilistic Focalization Approach for Single Receiver Underwater Localization","abstract":"We introduce a Bayesian estimation approach for the passive localization of an acoustic source in shallow water using a single mobile receiver. The proposed probabilistic focalization method estimates the time-varying source location in the presence of measurement-origin uncertainty. In particular, probabilistic data association is performed to match time-differences-of-arrival (TDOA) observations extracted from the acoustic signal to TDOAs predictions provided by the statistical model. The performance of our approach is evaluated using real acoustic data recorded by a single mobile receiver.","sentences":["We introduce a Bayesian estimation approach for the passive localization of an acoustic source in shallow water using a single mobile receiver.","The proposed probabilistic focalization method estimates the time-varying source location in the presence of measurement-origin uncertainty.","In particular, probabilistic data association is performed to match time-differences-of-arrival (TDOA) observations extracted from the acoustic signal to TDOAs predictions provided by the statistical model.","The performance of our approach is evaluated using real acoustic data recorded by a single mobile receiver."],"url":"http://arxiv.org/abs/2403.03384v1","category":"eess.SP"}
{"created":"2024-03-05 23:54:00","title":"Complexity Matters: Dynamics of Feature Learning in the Presence of Spurious Correlations","abstract":"Existing research often posits spurious features as \"easier\" to learn than core features in neural network optimization, but the impact of their relative simplicity remains under-explored. Moreover they mainly focus on the end performance intead of the learning dynamics of feature learning. In this paper, we propose a theoretical framework and associated synthetic dataset grounded in boolean function analysis which allows for fine-grained control on the relative complexity (compared to core features) and correlation strength (with respect to the label) of spurious features to study the dynamics of feature learning under spurious correlation. Our setup uncovers several interesting phenomenon: (1) stronger spurious correlations or simpler spurious features slow down the rate of learning for the core features, (2) learning phases of spurious features and core features are not always separable, (3) spurious features are not forgotten even after core features are fully learned. We show that our findings justify the success of retraining the last layer to remove spurious correlation and also identifies limitations of popular debiasing algorithms that exploit early learning of spurious features. We support our empirical findings with theoretical analyses for the case of learning XOR features with a one-hidden-layer ReLU network.","sentences":["Existing research often posits spurious features as \"easier\" to learn than core features in neural network optimization, but the impact of their relative simplicity remains under-explored.","Moreover they mainly focus on the end performance intead of the learning dynamics of feature learning.","In this paper, we propose a theoretical framework and associated synthetic dataset grounded in boolean function analysis which allows for fine-grained control on the relative complexity (compared to core features) and correlation strength (with respect to the label) of spurious features to study the dynamics of feature learning under spurious correlation.","Our setup uncovers several interesting phenomenon: (1) stronger spurious correlations or simpler spurious features slow down the rate of learning for the core features, (2) learning phases of spurious features and core features are not always separable, (3) spurious features are not forgotten even after core features are fully learned.","We show that our findings justify the success of retraining the last layer to remove spurious correlation and also identifies limitations of popular debiasing algorithms that exploit early learning of spurious features.","We support our empirical findings with theoretical analyses for the case of learning XOR features with a one-hidden-layer ReLU network."],"url":"http://arxiv.org/abs/2403.03375v1","category":"cs.LG"}
{"created":"2024-03-05 23:31:07","title":"Leveraging Federated Learning for Automatic Detection of Clopidogrel Treatment Failures","abstract":"The effectiveness of clopidogrel, a widely used antiplatelet medication, varies significantly among individuals, necessitating the development of precise predictive models to optimize patient care. In this study, we leverage federated learning strategies to address clopidogrel treatment failure detection. Our research harnesses the collaborative power of multiple healthcare institutions, allowing them to jointly train machine learning models while safeguarding sensitive patient data. Utilizing the UK Biobank dataset, which encompasses a vast and diverse population, we partitioned the data based on geographic centers and evaluated the performance of federated learning. Our results show that while centralized training achieves higher Area Under the Curve (AUC) values and faster convergence, federated learning approaches can substantially narrow this performance gap. Our findings underscore the potential of federated learning in addressing clopidogrel treatment failure detection, offering a promising avenue for enhancing patient care through personalized treatment strategies while respecting data privacy. This study contributes to the growing body of research on federated learning in healthcare and lays the groundwork for secure and privacy-preserving predictive models for various medical conditions.","sentences":["The effectiveness of clopidogrel, a widely used antiplatelet medication, varies significantly among individuals, necessitating the development of precise predictive models to optimize patient care.","In this study, we leverage federated learning strategies to address clopidogrel treatment failure detection.","Our research harnesses the collaborative power of multiple healthcare institutions, allowing them to jointly train machine learning models while safeguarding sensitive patient data.","Utilizing the UK Biobank dataset, which encompasses a vast and diverse population, we partitioned the data based on geographic centers and evaluated the performance of federated learning.","Our results show that while centralized training achieves higher Area Under the Curve (AUC) values and faster convergence, federated learning approaches can substantially narrow this performance gap.","Our findings underscore the potential of federated learning in addressing clopidogrel treatment failure detection, offering a promising avenue for enhancing patient care through personalized treatment strategies while respecting data privacy.","This study contributes to the growing body of research on federated learning in healthcare and lays the groundwork for secure and privacy-preserving predictive models for various medical conditions."],"url":"http://arxiv.org/abs/2403.03368v1","category":"cs.LG"}
{"created":"2024-03-05 23:28:50","title":"am-AMM: An Auction-Managed Automated Market Maker","abstract":"Automated market makers (AMMs) have emerged as the dominant market mechanism for trading on decentralized exchanges implemented on blockchains. This paper presents a single mechanism that targets two important unsolved problems for AMMs: reducing losses to informed orderflow, and maximizing revenue from uninformed orderflow. The \"auction-managed AMM\" works by running a censorship-resistant onchain auction for the right to temporarily act as \"pool manager\" for a constant-product AMM. The pool manager sets the swap fee rate on the pool, and also receives the accrued fees from swaps. The pool manager can exclusively capture some arbitrage by trading against the pool in response to small price movements, and also can set swap fees incorporating price sensitivity of retail orderflow and adapting to changing market conditions, with the benefits from both ultimately accruing to liquidity providers. Liquidity providers can enter and exit the pool freely in response to changing rent, though they must pay a small fee on withdrawal. We prove that under certain assumptions, this AMM should have higher liquidity in equilibrium than any standard, fixed-fee AMM.","sentences":["Automated market makers (AMMs) have emerged as the dominant market mechanism for trading on decentralized exchanges implemented on blockchains.","This paper presents a single mechanism that targets two important unsolved problems for AMMs: reducing losses to informed orderflow, and maximizing revenue from uninformed orderflow.","The \"auction-managed AMM\" works by running a censorship-resistant onchain auction for the right to temporarily act as \"pool manager\" for a constant-product AMM.","The pool manager sets the swap fee rate on the pool, and also receives the accrued fees from swaps.","The pool manager can exclusively capture some arbitrage by trading against the pool in response to small price movements, and also can set swap fees incorporating price sensitivity of retail orderflow and adapting to changing market conditions, with the benefits from both ultimately accruing to liquidity providers.","Liquidity providers can enter and exit the pool freely in response to changing rent, though they must pay a small fee on withdrawal.","We prove that under certain assumptions, this AMM should have higher liquidity in equilibrium than any standard, fixed-fee AMM."],"url":"http://arxiv.org/abs/2403.03367v1","category":"q-fin.TR"}
{"created":"2024-03-05 23:08:18","title":"Chained Information-Theoretic bounds and Tight Regret Rate for Linear Bandit Problems","abstract":"This paper studies the Bayesian regret of a variant of the Thompson-Sampling algorithm for bandit problems. It builds upon the information-theoretic framework of [Russo and Van Roy, 2015] and, more specifically, on the rate-distortion analysis from [Dong and Van Roy, 2020], where they proved a bound with regret rate of $O(d\\sqrt{T \\log(T)})$ for the $d$-dimensional linear bandit setting. We focus on bandit problems with a metric action space and, using a chaining argument, we establish new bounds that depend on the metric entropy of the action space for a variant of Thompson-Sampling.   Under suitable continuity assumption of the rewards, our bound offers a tight rate of $O(d\\sqrt{T})$ for $d$-dimensional linear bandit problems.","sentences":["This paper studies the Bayesian regret of a variant of the Thompson-Sampling algorithm for bandit problems.","It builds upon the information-theoretic framework of [Russo and Van Roy, 2015] and, more specifically, on the rate-distortion analysis from [Dong and Van Roy, 2020], where they proved a bound with regret rate of $O(d\\sqrt{T \\log(T)})$ for the $d$-dimensional linear bandit setting.","We focus on bandit problems with a metric action space and, using a chaining argument, we establish new bounds that depend on the metric entropy of the action space for a variant of Thompson-Sampling.   ","Under suitable continuity assumption of the rewards, our bound offers a tight rate of $O(d\\sqrt{T})$ for $d$-dimensional linear bandit problems."],"url":"http://arxiv.org/abs/2403.03361v1","category":"stat.ML"}
{"created":"2024-03-05 22:46:35","title":"The vehicle routing problem with synchronization constraints and support vehicle-dependent service times","abstract":"Many production processes require the cooperation of various resources. Especially when using expensive machines, their utilization plays a decisive role in efficient production. In agricultural production or civil construction processes, e.g., harvesting or road building, the machines are typically mobile, and synchronization of different machine types is required to perform operations. In addition, the productivity of one type often depends on the availability of another type. In this paper, we consider two types of vehicles, called primary and support vehicles. Primary vehicles perform operations and are assisted by at least one support vehicle, with more support vehicles resulting in faster service times for primary vehicles. We call this practical problem the vehicle routing and scheduling problem with support vehicle-dependent service times and introduce two mixed-integer linear programming models. The first represents each support vehicle individually with binary decision variables, while the second considers the cumulative flow of support vehicles with integer decision variables. Furthermore, the models are defined on a graph that allows easy transformation into multiple variants. These variants are based on allowing or prohibiting switching support vehicles between primary vehicles and splitting services among primary vehicles. We show in our extensive computational experiments that: i) the integer representation of support vehicles is superior to the binary representation, ii) the benefit of additional vehicles is subject to saturation effects and depends on the ratio of support and primary vehicles, and iii) switching and splitting lead to problems that are more difficult to solve, but also result in better solutions with higher primary vehicle utilization.","sentences":["Many production processes require the cooperation of various resources.","Especially when using expensive machines, their utilization plays a decisive role in efficient production.","In agricultural production or civil construction processes, e.g., harvesting or road building, the machines are typically mobile, and synchronization of different machine types is required to perform operations.","In addition, the productivity of one type often depends on the availability of another type.","In this paper, we consider two types of vehicles, called primary and support vehicles.","Primary vehicles perform operations and are assisted by at least one support vehicle, with more support vehicles resulting in faster service times for primary vehicles.","We call this practical problem the vehicle routing and scheduling problem with support vehicle-dependent service times and introduce two mixed-integer linear programming models.","The first represents each support vehicle individually with binary decision variables, while the second considers the cumulative flow of support vehicles with integer decision variables.","Furthermore, the models are defined on a graph that allows easy transformation into multiple variants.","These variants are based on allowing or prohibiting switching support vehicles between primary vehicles and splitting services among primary vehicles.","We show in our extensive computational experiments that: i) the integer representation of support vehicles is superior to the binary representation, ii) the benefit of additional vehicles is subject to saturation effects and depends on the ratio of support and primary vehicles, and iii) switching and splitting lead to problems that are more difficult to solve, but also result in better solutions with higher primary vehicle utilization."],"url":"http://arxiv.org/abs/2403.03355v1","category":"math.OC"}
{"created":"2024-03-05 21:48:29","title":"An Online Approach to Solving Public Transit Stationing and Dispatch Problem","abstract":"Public bus transit systems provide critical transportation services for large sections of modern communities. On-time performance and maintaining the reliable quality of service is therefore very important. Unfortunately, disruptions caused by overcrowding, vehicular failures, and road accidents often lead to service performance degradation. Though transit agencies keep a limited number of vehicles in reserve and dispatch them to relieve the affected routes during disruptions, the procedure is often ad-hoc and has to rely on human experience and intuition to allocate resources (vehicles) to affected trips under uncertainty. In this paper, we describe a principled approach using non-myopic sequential decision procedures to solve the problem and decide (a) if it is advantageous to anticipate problems and proactively station transit buses near areas with high-likelihood of disruptions and (b) decide if and which vehicle to dispatch to a particular problem. Our approach was developed in partnership with the Metropolitan Transportation Authority for a mid-sized city in the USA and models the system as a semi-Markov decision problem (solved as a Monte-Carlo tree search procedure) and shows that it is possible to obtain an answer to these two coupled decision problems in a way that maximizes the overall reward (number of people served). We sample many possible futures from generative models, each is assigned to a tree and processed using root parallelization. We validate our approach using 3 years of data from our partner agency. Our experiments show that the proposed framework serves 2% more passengers while reducing deadhead miles by 40%.","sentences":["Public bus transit systems provide critical transportation services for large sections of modern communities.","On-time performance and maintaining the reliable quality of service is therefore very important.","Unfortunately, disruptions caused by overcrowding, vehicular failures, and road accidents often lead to service performance degradation.","Though transit agencies keep a limited number of vehicles in reserve and dispatch them to relieve the affected routes during disruptions, the procedure is often ad-hoc and has to rely on human experience and intuition to allocate resources (vehicles) to affected trips under uncertainty.","In this paper, we describe a principled approach using non-myopic sequential decision procedures to solve the problem and decide (a) if it is advantageous to anticipate problems and proactively station transit buses near areas with high-likelihood of disruptions and (b) decide if and which vehicle to dispatch to a particular problem.","Our approach was developed in partnership with the Metropolitan Transportation Authority for a mid-sized city in the USA and models the system as a semi-Markov decision problem (solved as a Monte-Carlo tree search procedure) and shows that it is possible to obtain an answer to these two coupled decision problems in a way that maximizes the overall reward (number of people served).","We sample many possible futures from generative models, each is assigned to a tree and processed using root parallelization.","We validate our approach using 3 years of data from our partner agency.","Our experiments show that the proposed framework serves 2% more passengers while reducing deadhead miles by 40%."],"url":"http://arxiv.org/abs/2403.03339v1","category":"cs.CY"}
{"created":"2024-03-05 21:40:10","title":"Fine-Grained Privacy Guarantees for Coverage Problems","abstract":"We introduce a new notion of neighboring databases for coverage problems such as Max Cover and Set Cover under differential privacy. In contrast to the standard privacy notion for these problems, which is analogous to node-privacy in graphs, our new definition gives a more fine-grained privacy guarantee, which is analogous to edge-privacy. We illustrate several scenarios of Set Cover and Max Cover where our privacy notion is desired one for the application.   Our main result is an $\\epsilon$-edge differentially private algorithm for Max Cover which obtains an $(1-1/e-\\eta,\\tilde{O}(k/\\epsilon))$-approximation with high probability. Furthermore, we show that this result is nearly tight: we give a lower bound show that an additive error of $\\Omega(k/\\epsilon)$ is necessary under edge-differential privacy. Via group privacy properties, this implies a new algorithm for $\\epsilon$-node differentially private Max Cover which obtains an $(1-1/e-\\eta,\\tilde{O}(fk/\\epsilon))$-approximation, where $f$ is the maximum degree of an element in the set system. When $f\\ll k$, this improves over the best known algorithm for Max Cover under pure (node) differential privacy, which obtains an $(1-1/e,\\tilde{O}(k^2/\\epsilon))$-approximation.","sentences":["We introduce a new notion of neighboring databases for coverage problems such as Max Cover and Set Cover under differential privacy.","In contrast to the standard privacy notion for these problems, which is analogous to node-privacy in graphs, our new definition gives a more fine-grained privacy guarantee, which is analogous to edge-privacy.","We illustrate several scenarios of Set Cover and Max Cover where our privacy notion is desired one for the application.   ","Our main result is an $\\epsilon$-edge differentially private algorithm for Max Cover which obtains an $(1-1/e-\\eta,\\tilde{O}(k/\\epsilon))$-approximation with high probability.","Furthermore, we show that this result is nearly tight: we give a lower bound show that an additive error of $\\Omega(k/\\epsilon)$ is necessary under edge-differential privacy.","Via group privacy properties, this implies a new algorithm for $\\epsilon$-node differentially private Max Cover which obtains an $(1-1/e-\\eta,\\tilde{O}(fk/\\epsilon))$-approximation, where $f$ is the maximum degree of an element in the set system.","When $f\\ll k$, this improves over the best known algorithm for Max Cover under pure (node) differential privacy, which obtains an $(1-1/e,\\tilde{O}(k^2/\\epsilon))$-approximation."],"url":"http://arxiv.org/abs/2403.03337v1","category":"cs.DS"}
{"created":"2024-03-05 21:38:19","title":"Scope of Large Language Models for Mining Emerging Opinions in Online Health Discourse","abstract":"In this paper, we develop an LLM-powered framework for the curation and evaluation of emerging opinion mining in online health communities. We formulate emerging opinion mining as a pairwise stance detection problem between (title, comment) pairs sourced from Reddit, where post titles contain emerging health-related claims on a topic that is not predefined. The claims are either explicitly or implicitly expressed by the user. We detail (i) a method of claim identification -- the task of identifying if a post title contains a claim and (ii) an opinion mining-driven evaluation framework for stance detection using LLMs.   We facilitate our exploration by releasing a novel test dataset, Long COVID-Stance, or LC-stance, which can be used to evaluate LLMs on the tasks of claim identification and stance detection in online health communities. Long Covid is an emerging post-COVID disorder with uncertain and complex treatment guidelines, thus making it a suitable use case for our task. LC-Stance contains long COVID treatment related discourse sourced from a Reddit community. Our evaluation shows that GPT-4 significantly outperforms prior works on zero-shot stance detection. We then perform thorough LLM model diagnostics, identifying the role of claim type (i.e. implicit vs explicit claims) and comment length as sources of model error.","sentences":["In this paper, we develop an LLM-powered framework for the curation and evaluation of emerging opinion mining in online health communities.","We formulate emerging opinion mining as a pairwise stance detection problem between (title, comment) pairs sourced from Reddit, where post titles contain emerging health-related claims on a topic that is not predefined.","The claims are either explicitly or implicitly expressed by the user.","We detail (i) a method of claim identification -- the task of identifying if a post title contains a claim and (ii) an opinion mining-driven evaluation framework for stance detection using LLMs.   ","We facilitate our exploration by releasing a novel test dataset, Long COVID-Stance, or LC-stance, which can be used to evaluate LLMs on the tasks of claim identification and stance detection in online health communities.","Long Covid is an emerging post-COVID disorder with uncertain and complex treatment guidelines, thus making it a suitable use case for our task.","LC-Stance contains long COVID treatment related discourse sourced from a Reddit community.","Our evaluation shows that GPT-4 significantly outperforms prior works on zero-shot stance detection.","We then perform thorough LLM model diagnostics, identifying the role of claim type (i.e. implicit vs explicit claims) and comment length as sources of model error."],"url":"http://arxiv.org/abs/2403.03336v1","category":"cs.CL"}
{"created":"2024-03-05 21:34:23","title":"Solution Simplex Clustering for Heterogeneous Federated Learning","abstract":"We tackle a major challenge in federated learning (FL) -- achieving good performance under highly heterogeneous client distributions. The difficulty partially arises from two seemingly contradictory goals: learning a common model by aggregating the information from clients, and learning local personalized models that should be adapted to each local distribution. In this work, we propose Solution Simplex Clustered Federated Learning (SosicFL) for dissolving such contradiction. Based on the recent ideas of learning solution simplices, SosicFL assigns a subregion in a simplex to each client, and performs FL to learn a common solution simplex. This allows the client models to possess their characteristics within the degrees of freedom in the solution simplex, and at the same time achieves the goal of learning a global common model. Our experiments show that SosicFL improves the performance and accelerates the training process for global and personalized FL with minimal computational overhead.","sentences":["We tackle a major challenge in federated learning (FL) -- achieving good performance under highly heterogeneous client distributions.","The difficulty partially arises from two seemingly contradictory goals: learning a common model by aggregating the information from clients, and learning local personalized models that should be adapted to each local distribution.","In this work, we propose Solution Simplex Clustered Federated Learning (SosicFL) for dissolving such contradiction.","Based on the recent ideas of learning solution simplices, SosicFL assigns a subregion in a simplex to each client, and performs FL to learn a common solution simplex.","This allows the client models to possess their characteristics within the degrees of freedom in the solution simplex, and at the same time achieves the goal of learning a global common model.","Our experiments show that SosicFL improves the performance and accelerates the training process for global and personalized FL with minimal computational overhead."],"url":"http://arxiv.org/abs/2403.03333v1","category":"cs.LG"}
{"created":"2024-03-05 21:22:39","title":"Assortment Optimization For Conference Goodies With Indifferent Attendees","abstract":"Conferences such as FUN with Algorithms routinely buy goodies (e.g., t-shirts, coffee mugs, etc) for their attendees. Often, said goodies come in different types, varying by color or design, and organizers need to decide how many goodies of each type to buy. We study the problem of buying optimal amounts of each type under a simple model of preferences by the attendees: they are indifferent to the types but want to be able to choose between more than one type of goodies at the time of their arrival. The indifference of attendees suggests that the optimal policy is to buy roughly equal amounts for every goodie type. Despite how intuitive this conjecture sounds, we show that this simple model of assortment optimization is quite rich, and even though we make progress towards proving the conjecture (e.g., we succeed when the number of goodie types is 2 or 3), the general case with K types remains open. We also present asymptotic results and computer simulations, and finally, to motivate further progress, we offer a reward of $100usd for a full proof.","sentences":["Conferences such as FUN with Algorithms routinely buy goodies (e.g., t-shirts, coffee mugs, etc) for their attendees.","Often, said goodies come in different types, varying by color or design, and organizers need to decide how many goodies of each type to buy.","We study the problem of buying optimal amounts of each type under a simple model of preferences by the attendees: they are indifferent to the types but want to be able to choose between more than one type of goodies at the time of their arrival.","The indifference of attendees suggests that the optimal policy is to buy roughly equal amounts for every goodie type.","Despite how intuitive this conjecture sounds, we show that this simple model of assortment optimization is quite rich, and even though we make progress towards proving the conjecture (e.g., we succeed when the number of goodie types is 2 or 3), the general case with K types remains open.","We also present asymptotic results and computer simulations, and finally, to motivate further progress, we offer a reward of $100usd for a full proof."],"url":"http://arxiv.org/abs/2403.03330v1","category":"math.OC"}
{"created":"2024-03-05 21:08:01","title":"Imaging the event horizon of M87* from space on different timescales","abstract":"The concept of a new space very long baseline interferometry system named the Event Horizon Imager (EHI) has been proposed to dramatically improve black hole imaging and provide precise tests of the theory of general relativity. We investigate the ability to make high-resolution movies of the black hole shadow and jet launching region around the supermassive black hole M87* and other black hole jets with a three-satellite EHI configuration. We aim to identify orbital configurations to optimize the uv-coverage to image variable sources. Observations of general relativistic magnetohydrodynamics models were simulated for the configuration, consisting of three satellites in circular medium earth orbits with an orbital plane perpendicular to the line of sight. The expected noise was based on preliminary system parameters. Movie frames, for which a part of the uv-coverage may be excessively sparse, were reconstructed with algorithms that recover missing information from other frames. Averaging visibilities accumulated over multiple epochs of observations with an appropriate orbital configuration then improves the image quality. With an enhanced signal-to-noise ratio (S/N), timescales of observed variability were decreased. Our simulations show that the EHI with standard system parameters is capable of imaging the variability in the M87* environment on event horizon scales with approximately a month-long temporal resolution. The EHI with more optimistic noise parameters (enhancing S/N about 100-fold) would allow for imaging of the variability on gravitational timescales. Observations with an EHI setup at lower frequencies are capable of imaging the variability in extended jets. The EHI concept can be used to image the variability in a black hole environment and extended jets, allowing for stronger tests of gravity theories and models of black hole accretion, plasma dynamics, and jet launching.","sentences":["The concept of a new space very long baseline interferometry system named the Event Horizon Imager (EHI) has been proposed to dramatically improve black hole imaging and provide precise tests of the theory of general relativity.","We investigate the ability to make high-resolution movies of the black hole shadow and jet launching region around the supermassive black hole M87* and other black hole jets with a three-satellite EHI configuration.","We aim to identify orbital configurations to optimize the uv-coverage to image variable sources.","Observations of general relativistic magnetohydrodynamics models were simulated for the configuration, consisting of three satellites in circular medium earth orbits with an orbital plane perpendicular to the line of sight.","The expected noise was based on preliminary system parameters.","Movie frames, for which a part of the uv-coverage may be excessively sparse, were reconstructed with algorithms that recover missing information from other frames.","Averaging visibilities accumulated over multiple epochs of observations with an appropriate orbital configuration then improves the image quality.","With an enhanced signal-to-noise ratio (S/N), timescales of observed variability were decreased.","Our simulations show that the EHI with standard system parameters is capable of imaging the variability in the M87* environment on event horizon scales with approximately a month-long temporal resolution.","The EHI with more optimistic noise parameters (enhancing S/N about 100-fold) would allow for imaging of the variability on gravitational timescales.","Observations with an EHI setup at lower frequencies are capable of imaging the variability in extended jets.","The EHI concept can be used to image the variability in a black hole environment and extended jets, allowing for stronger tests of gravity theories and models of black hole accretion, plasma dynamics, and jet launching."],"url":"http://arxiv.org/abs/2403.03327v1","category":"astro-ph.HE"}
{"created":"2024-03-05 20:47:53","title":"Competing Mechanisms in Games Played Through Agents: Theory and Experiment","abstract":"This paper proposes Competing Mechanism Games Played Through Agent (CMGPTA), an extension of the GPTA (Prat and Rustichini (2003)), where a Principal can offer any arbitrary mechanism that specifies a transfer schedule for each agent conditional on all Agents' messages. We identify the set of equilibrium allocations using deviator-reporting mechanisms (DRMs) on the path and single transfer schedules off the path. We design a lab experiment implementing DRMs. We observe that implemented outcomes are efficient more often than random. A majority of the time, Agents do tell the truth on the identity of a deviating Principal, despite potential gains from (tacit) collusion on false reports. As play progresses, Agents learn to play with their counterparty Agent with the average predicted probability of collusion on false reports across groups increasing from about 9% at the beginning of the experiment to just under 20% by the end. However, group heterogeneity is significant.","sentences":["This paper proposes Competing Mechanism Games Played Through Agent (CMGPTA), an extension of the GPTA (Prat and Rustichini (2003)), where a Principal can offer any arbitrary mechanism that specifies a transfer schedule for each agent conditional on all Agents' messages.","We identify the set of equilibrium allocations using deviator-reporting mechanisms (DRMs) on the path and single transfer schedules off the path.","We design a lab experiment implementing DRMs.","We observe that implemented outcomes are efficient more often than random.","A majority of the time, Agents do tell the truth on the identity of a deviating Principal, despite potential gains from (tacit) collusion on false reports.","As play progresses, Agents learn to play with their counterparty Agent with the average predicted probability of collusion on false reports across groups increasing from about 9% at the beginning of the experiment to just under 20% by the end.","However, group heterogeneity is significant."],"url":"http://arxiv.org/abs/2403.03317v1","category":"econ.TH"}
{"created":"2024-03-05 20:38:16","title":"Temporal relaxation of disordered many-body quantum systems under driving and dissipation","abstract":"Strong disorder inhibits thermalization in isolated quantum systems and may lead to many-body localization (MBL). In realistic situations, however, the observation of MBL is hindered by residual couplings of the system to an environment, which acts as a bath and pushes the system to thermal equilibrium. This paper is concerned with the transient dynamics prior to thermalization and studies how the relaxation of a disordered system is altered under the influence of external driving and dissipation. We consider a scenario where a disordered quantum spin chain is placed into a strong magnetic field that polarizes the system. By suddenly removing the external field, a nonequilibrium situation is induced and the decay of magnetization probes the degree of localization. We show that by driving the system with light, one can distinguish between different dynamical regimes as the spins are more or less susceptible to the drive depending on the strength of the disorder. We provide evidence that some of these signatures remain observable at intermediate time scales even when the spin chain is subject to noise due to coupling to an environment. From a numerical point of view, we demonstrate that the open-system dynamics starting from a class of experimentally relevant mixed initial states can be efficiently simulated by combining dynamical quantum typicality with stochastic unraveling of Lindblad master equations.","sentences":["Strong disorder inhibits thermalization in isolated quantum systems and may lead to many-body localization (MBL).","In realistic situations, however, the observation of MBL is hindered by residual couplings of the system to an environment, which acts as a bath and pushes the system to thermal equilibrium.","This paper is concerned with the transient dynamics prior to thermalization and studies how the relaxation of a disordered system is altered under the influence of external driving and dissipation.","We consider a scenario where a disordered quantum spin chain is placed into a strong magnetic field that polarizes the system.","By suddenly removing the external field, a nonequilibrium situation is induced and the decay of magnetization probes the degree of localization.","We show that by driving the system with light, one can distinguish between different dynamical regimes as the spins are more or less susceptible to the drive depending on the strength of the disorder.","We provide evidence that some of these signatures remain observable at intermediate time scales even when the spin chain is subject to noise due to coupling to an environment.","From a numerical point of view, we demonstrate that the open-system dynamics starting from a class of experimentally relevant mixed initial states can be efficiently simulated by combining dynamical quantum typicality with stochastic unraveling of Lindblad master equations."],"url":"http://arxiv.org/abs/2403.03315v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-05 20:36:26","title":"Collision Avoidance Verification of Multiagent Systems with Learned Policies","abstract":"For many multiagent control problems, neural networks (NNs) have enabled promising new capabilities. However, many of these systems lack formal guarantees (e.g., collision avoidance, robustness), which prevents leveraging these advances in safety-critical settings. While there is recent work on formal verification of NN-controlled systems, most existing techniques cannot handle scenarios with more than one agent. To address this research gap, this paper presents a backward reachability-based approach for verifying the collision avoidance properties of Multi-Agent Neural Feedback Loops (MA-NFLs). Given the dynamics models and trained control policies of each agent, the proposed algorithm computes relative backprojection sets by solving a series of Mixed Integer Linear Programs (MILPs) offline for each pair of agents. Our pair-wise approach is parallelizable and thus scales well with increasing number of agents, and we account for state measurement uncertainties, making it well aligned with real-world scenarios. Using those results, the agents can quickly check for collision avoidance online by solving low-dimensional Linear Programs (LPs). We demonstrate the proposed algorithm can verify collision-free properties of a MA-NFL with agents trained to imitate a collision avoidance algorithm (Reciprocal Velocity Obstacles). We further demonstrate the computational scalability of the approach on systems with up to 10 agents.","sentences":["For many multiagent control problems, neural networks (NNs) have enabled promising new capabilities.","However, many of these systems lack formal guarantees (e.g., collision avoidance, robustness), which prevents leveraging these advances in safety-critical settings.","While there is recent work on formal verification of NN-controlled systems, most existing techniques cannot handle scenarios with more than one agent.","To address this research gap, this paper presents a backward reachability-based approach for verifying the collision avoidance properties of Multi-Agent Neural Feedback Loops (MA-NFLs).","Given the dynamics models and trained control policies of each agent, the proposed algorithm computes relative backprojection sets by solving a series of Mixed Integer Linear Programs (MILPs) offline for each pair of agents.","Our pair-wise approach is parallelizable and thus scales well with increasing number of agents, and we account for state measurement uncertainties, making it well aligned with real-world scenarios.","Using those results, the agents can quickly check for collision avoidance online by solving low-dimensional Linear Programs (LPs).","We demonstrate the proposed algorithm can verify collision-free properties of a MA-NFL with agents trained to imitate a collision avoidance algorithm (Reciprocal Velocity Obstacles).","We further demonstrate the computational scalability of the approach on systems with up to 10 agents."],"url":"http://arxiv.org/abs/2403.03314v1","category":"eess.SY"}
{"created":"2024-03-05 20:00:48","title":"Understanding and avoiding the \"weights of regression\": Heterogeneous effects, misspecification, and longstanding solutions","abstract":"Researchers in many fields endeavor to estimate treatment effects by regressing outcome data (Y) on a treatment (D) and observed confounders (X). Even absent unobserved confounding, the regression coefficient on the treatment reports a weighted average of strata-specific treatment effects (Angrist, 1998). Where heterogeneous treatment effects cannot be ruled out, the resulting coefficient is thus not generally equal to the average treatment effect (ATE), and is unlikely to be the quantity of direct scientific or policy interest. The difference between the coefficient and the ATE has led researchers to propose various interpretational, bounding, and diagnostic aids (Humphreys, 2009; Aronow and Samii, 2016; Sloczynski, 2022; Chattopadhyay and Zubizarreta, 2023). We note that the linear regression of Y on D and X can be misspecified when the treatment effect is heterogeneous in X. The \"weights of regression\", for which we provide a new (more general) expression, simply characterize how the OLS coefficient will depart from the ATE under the misspecification resulting from unmodeled treatment effect heterogeneity. Consequently, a natural alternative to suffering these weights is to address the misspecification that gives rise to them. For investigators committed to linear approaches, we propose relying on the slightly weaker assumption that the potential outcomes are linear in X. Numerous well-known estimators are unbiased for the ATE under this assumption, namely regression-imputation/g-computation/T-learner, regression with an interaction of the treatment and covariates (Lin, 2013), and balancing weights. Any of these approaches avoid the apparent weighting problem of the misspecified linear regression, at an efficiency cost that will be small when there are few covariates relative to sample size. We demonstrate these lessons using simulations in observational and experimental settings.","sentences":["Researchers in many fields endeavor to estimate treatment effects by regressing outcome data (Y) on a treatment (D) and observed confounders (X).","Even absent unobserved confounding, the regression coefficient on the treatment reports a weighted average of strata-specific treatment effects (Angrist, 1998).","Where heterogeneous treatment effects cannot be ruled out, the resulting coefficient is thus not generally equal to the average treatment effect (ATE), and is unlikely to be the quantity of direct scientific or policy interest.","The difference between the coefficient and the ATE has led researchers to propose various interpretational, bounding, and diagnostic aids (Humphreys, 2009; Aronow and Samii, 2016; Sloczynski, 2022; Chattopadhyay and Zubizarreta, 2023).","We note that the linear regression of Y on D and X can be misspecified when the treatment effect is heterogeneous in X.","The \"weights of regression\", for which we provide a new (more general) expression, simply characterize how the OLS coefficient will depart from the ATE under the misspecification resulting from unmodeled treatment effect heterogeneity.","Consequently, a natural alternative to suffering these weights is to address the misspecification that gives rise to them.","For investigators committed to linear approaches, we propose relying on the slightly weaker assumption that the potential outcomes are linear in X. Numerous well-known estimators are unbiased for the ATE under this assumption, namely regression-imputation/g-computation/T-learner, regression with an interaction of the treatment and covariates (Lin, 2013), and balancing weights.","Any of these approaches avoid the apparent weighting problem of the misspecified linear regression, at an efficiency cost that will be small when there are few covariates relative to sample size.","We demonstrate these lessons using simulations in observational and experimental settings."],"url":"http://arxiv.org/abs/2403.03299v1","category":"stat.ME"}
{"created":"2024-03-05 19:43:33","title":"Efficient Representation of Gaussian Fermionic Pure States in Non-Computational Bases","abstract":"This paper introduces an innovative approach for representing Gaussian fermionic states, pivotal in quantum spin systems and fermionic models, within a range of alternative quantum bases. We focus on transitioning these states from the conventional computational (\\sigma^z) basis to more complex bases, such as ((\\phi, \\frac{\\pi}{2}, \\alpha)), which are essential for accurately calculating critical quantities like formation probabilities and Shannon entropy. We present a novel algorithm that not only simplifies the basis transformation but also reduces computational complexity, making it feasible to calculate amplitudes of large systems efficiently. Our key contribution is a technique that translates amplitude calculations into the Pfaffian computation of submatrices from an antisymmetric matrix, a process facilitated by understanding domain wall relationships across different bases. As an application, we will determine the formation probabilities for various bases and configurations within the critical transverse field Ising chain, considering both periodic and open boundary conditions. We aim to categorize the configurations and bases by examining the universal constant term that characterizes the scaling of the logarithm of the formation probability in the periodic system, as well as the coefficient of the logarithmic term in the case of open systems. In the open system scenario, this coefficient is influenced by the central charge and the conformal weight of the boundary condition-changing operator. This work is set to expand the toolkit available for researchers in quantum information theory and many-body physics, providing a more efficient and elegant solution for exploring Gaussian fermionic states in non-standard quantum bases.","sentences":["This paper introduces an innovative approach for representing Gaussian fermionic states, pivotal in quantum spin systems and fermionic models, within a range of alternative quantum bases.","We focus on transitioning these states from the conventional computational (\\sigma^z) basis to more complex bases, such as ((\\phi, \\frac{\\pi}{2}, \\alpha)), which are essential for accurately calculating critical quantities like formation probabilities and Shannon entropy.","We present a novel algorithm that not only simplifies the basis transformation but also reduces computational complexity, making it feasible to calculate amplitudes of large systems efficiently.","Our key contribution is a technique that translates amplitude calculations into the Pfaffian computation of submatrices from an antisymmetric matrix, a process facilitated by understanding domain wall relationships across different bases.","As an application, we will determine the formation probabilities for various bases and configurations within the critical transverse field Ising chain, considering both periodic and open boundary conditions.","We aim to categorize the configurations and bases by examining the universal constant term that characterizes the scaling of the logarithm of the formation probability in the periodic system, as well as the coefficient of the logarithmic term in the case of open systems.","In the open system scenario, this coefficient is influenced by the central charge and the conformal weight of the boundary condition-changing operator.","This work is set to expand the toolkit available for researchers in quantum information theory and many-body physics, providing a more efficient and elegant solution for exploring Gaussian fermionic states in non-standard quantum bases."],"url":"http://arxiv.org/abs/2403.03289v1","category":"quant-ph"}
{"created":"2024-03-05 19:33:45","title":"Quantum communication networks with defects in silicon carbide","abstract":"Quantum communication promises unprecedented communication capabilities enabled by the transmission of quantum states of light. However, current implementations face severe limitations in communication distance due to photon loss. Silicon carbide (SiC) defects have emerged as a promising quantum device platform, offering strong optical transitions, long spin coherence lifetimes and the opportunity for integration with semiconductor devices. Some defects with optical transitions in the telecom range have been identified, allowing to interface with fiber networks without the need for wavelength conversion. These unique properties make SiC an attractive platform for the implementation of quantum nodes for quantum communication networks. We provide an overview of the most prominent defects in SiC and their implementation in spin-photon interfaces. Furthermore, we model a memory-enhanced quantum communication protocol in order to extract the parameters required to surpass a direct point-to-point link performance. Based on these insights, we summarize the key steps required towards the deployment of SiC devices in large-scale quantum communication networks.","sentences":["Quantum communication promises unprecedented communication capabilities enabled by the transmission of quantum states of light.","However, current implementations face severe limitations in communication distance due to photon loss.","Silicon carbide (SiC) defects have emerged as a promising quantum device platform, offering strong optical transitions, long spin coherence lifetimes and the opportunity for integration with semiconductor devices.","Some defects with optical transitions in the telecom range have been identified, allowing to interface with fiber networks without the need for wavelength conversion.","These unique properties make SiC an attractive platform for the implementation of quantum nodes for quantum communication networks.","We provide an overview of the most prominent defects in SiC and their implementation in spin-photon interfaces.","Furthermore, we model a memory-enhanced quantum communication protocol in order to extract the parameters required to surpass a direct point-to-point link performance.","Based on these insights, we summarize the key steps required towards the deployment of SiC devices in large-scale quantum communication networks."],"url":"http://arxiv.org/abs/2403.03284v1","category":"quant-ph"}
{"created":"2024-03-05 19:18:44","title":"Linear Codes for Hyperdimensional Computing","abstract":"Hyperdimensional Computing (HDC) is an emerging computational paradigm for representing compositional information as high-dimensional vectors, and has a promising potential in applications ranging from machine learning to neuromorphic computing. One of the long-standing challenges in HDC is factoring a compositional representation to its constituent factors, also known as the recovery problem. In this paper we take a novel approach to solve the recovery problem, and propose the use of random linear codes. These codes are subspaces over the Boolean field, and are a well-studied topic in information theory with various applications in digital communication. We begin by showing that hyperdimensional encoding using random linear codes retains favorable properties of the prevalent (ordinary) random codes, and hence HD representations using the two methods have comparable information storage capabilities. We proceed to show that random linear codes offer a rich subcode structure that can be used to form key-value stores, which encapsulate most use cases of HDC. Most importantly, we show that under the framework we develop, random linear codes admit simple recovery algorithms to factor (either bundled or bound) compositional representations. The former relies on constructing certain linear equation systems over the Boolean field, the solution to which reduces the search space dramatically and strictly outperforms exhaustive search in many cases. The latter employs the subspace structure of these codes to achieve provably correct factorization. Both methods are strictly faster than the state-of-the-art resonator networks, often by an order of magnitude. We implemented our techniques in Python using a benchmark software library, and demonstrated promising experimental results.","sentences":["Hyperdimensional Computing (HDC) is an emerging computational paradigm for representing compositional information as high-dimensional vectors, and has a promising potential in applications ranging from machine learning to neuromorphic computing.","One of the long-standing challenges in HDC is factoring a compositional representation to its constituent factors, also known as the recovery problem.","In this paper we take a novel approach to solve the recovery problem, and propose the use of random linear codes.","These codes are subspaces over the Boolean field, and are a well-studied topic in information theory with various applications in digital communication.","We begin by showing that hyperdimensional encoding using random linear codes retains favorable properties of the prevalent (ordinary) random codes, and hence HD representations using the two methods have comparable information storage capabilities.","We proceed to show that random linear codes offer a rich subcode structure that can be used to form key-value stores, which encapsulate most use cases of HDC.","Most importantly, we show that under the framework we develop, random linear codes admit simple recovery algorithms to factor (either bundled or bound) compositional representations.","The former relies on constructing certain linear equation systems over the Boolean field, the solution to which reduces the search space dramatically and strictly outperforms exhaustive search in many cases.","The latter employs the subspace structure of these codes to achieve provably correct factorization.","Both methods are strictly faster than the state-of-the-art resonator networks, often by an order of magnitude.","We implemented our techniques in Python using a benchmark software library, and demonstrated promising experimental results."],"url":"http://arxiv.org/abs/2403.03278v1","category":"cs.IT"}
{"created":"2024-03-05 19:10:18","title":"Active Information Gathering for Long-Horizon Navigation Under Uncertainty by Learning the Value of Information","abstract":"We address the task of long-horizon navigation in partially mapped environments for which active gathering of information about faraway unseen space is essential for good behavior. We present a novel planning strategy that, at training time, affords tractable computation of the value of information associated with revealing potentially informative regions of unseen space, data used to train a graph neural network to predict the goodness of temporally-extended exploratory actions. Our learning-augmented model-based planning approach predicts the expected value of information of revealing unseen space and is capable of using these predictions to actively seek information and so improve long-horizon navigation. Across two simulated office-like environments, our planner outperforms competitive learned and non-learned baseline navigation strategies, achieving improvements of up to 63.76% and 36.68%, demonstrating its capacity to actively seek performance-critical information.","sentences":["We address the task of long-horizon navigation in partially mapped environments for which active gathering of information about faraway unseen space is essential for good behavior.","We present a novel planning strategy that, at training time, affords tractable computation of the value of information associated with revealing potentially informative regions of unseen space, data used to train a graph neural network to predict the goodness of temporally-extended exploratory actions.","Our learning-augmented model-based planning approach predicts the expected value of information of revealing unseen space and is capable of using these predictions to actively seek information and so improve long-horizon navigation.","Across two simulated office-like environments, our planner outperforms competitive learned and non-learned baseline navigation strategies, achieving improvements of up to 63.76% and 36.68%, demonstrating its capacity to actively seek performance-critical information."],"url":"http://arxiv.org/abs/2403.03269v1","category":"cs.RO"}
{"created":"2024-03-05 19:02:06","title":"How to Break the Mass Sheet Degeneracy with the Lightcurves of Microlensed Type Ia Supernovae","abstract":"The standardizable nature of gravitationally lensed Type Ia supernovae (glSNe Ia) makes them an attractive target for time delay cosmography, since a source with known luminosity breaks the mass sheet degeneracy. It is known that microlensing by stars in the lensing galaxy can add significant stochastic uncertainty to the unlensed luminosity which is often much larger than the intrinsic scatter of the Ia population. In this work, we show how the temporal microlensing variations as the supernova disc expands can be used to improve the standardization of glSNe Ia. We find that SNe are standardizable if they do not cross caustics as they expand. We estimate that this will be the case for $\\approx$6 doubly imaged systems and $\\approx$0.3 quadruply imaged systems per year in LSST. At the end of the ten year LSST survey, these systems should enable us to test for systematics in $H_0$ due to the mass sheet degeneracy at the $1.00^{+0.07}_{-0.06}$\\% level, or $1.8\\pm0.2$\\% if we can only extract time delays from the third of systems with counter images brighter than $i=24$ mag.","sentences":["The standardizable nature of gravitationally lensed Type Ia supernovae (glSNe Ia) makes them an attractive target for time delay cosmography, since a source with known luminosity breaks the mass sheet degeneracy.","It is known that microlensing by stars in the lensing galaxy can add significant stochastic uncertainty to the unlensed luminosity which is often much larger than the intrinsic scatter of the Ia population.","In this work, we show how the temporal microlensing variations as the supernova disc expands can be used to improve the standardization of glSNe Ia.","We find that SNe are standardizable if they do not cross caustics as they expand.","We estimate that this will be the case for $\\approx$6 doubly imaged systems and $\\approx$0.3 quadruply imaged systems per year in LSST.","At the end of the ten year LSST survey, these systems should enable us to test for systematics in $H_0$ due to the mass sheet degeneracy at the $1.00^{+0.07}_{-0.06}$\\% level, or $1.8\\pm0.2$\\% if we can only extract time delays from the third of systems with counter images brighter than $i=24$ mag."],"url":"http://arxiv.org/abs/2403.03264v1","category":"astro-ph.GA"}
{"created":"2024-03-05 19:00:14","title":"Kinematical small-scale fluctuations do not affect the measurement of the dynamical mass of galaxies","abstract":"The stellar kinematics of low-mass galaxies are usually observed to be very unsmooth with significant kinematical fluctuations in small scales, which cannot be consistent with the projected centrosymmetric stellar kinematics obtained from commonly used dynamical models. In this work, we aim to test whether the high degree of kinematical fluctuations affects the dynamical mass estimate of galaxies. We use the asymmetry parameter $\\eta$ obtained from the $180^{\\circ}$ rotation self-subtraction of stellar kinematics of galaxies to quantify the degree of kinematical small-scale fluctuations. We use TNG50 numerical simulation to construct a large sample of mock galaxies with known total masses, and then obtained the virial dynamical mass estimator of these mock galaxies. We find that the dynamical masses within three-dimensional $R_{\\rm e}$ to the mock galaxy centres are overall averagely accurate within around 0.1 dex under the symmetric assumption, while $R_{\\rm e}$ means the projected circularized half-stellar mass radius in this work. We study the local virial mass estimation bias for mock galaxies of different $\\eta$. The maximum bias difference of two $\\eta$ bins is around 0.16 dex, which with other local biases may help apply the observational virial mass estimators obtained from massive galaxies to other types of galaxies. We find that the Spearman's $\\rho$ of $\\eta$ with the intrinsic mass estimation deviations is near zero if the local bias is eliminated properly. The results indicate that even for low-mass galaxies, the existence of high degree of kinematical small-scale fluctuations does not affect the measurement of the dynamical mass of galaxies.","sentences":["The stellar kinematics of low-mass galaxies are usually observed to be very unsmooth with significant kinematical fluctuations in small scales, which cannot be consistent with the projected centrosymmetric stellar kinematics obtained from commonly used dynamical models.","In this work, we aim to test whether the high degree of kinematical fluctuations affects the dynamical mass estimate of galaxies.","We use the asymmetry parameter $\\eta$ obtained from the $180^{\\circ}$ rotation self-subtraction of stellar kinematics of galaxies to quantify the degree of kinematical small-scale fluctuations.","We use TNG50 numerical simulation to construct a large sample of mock galaxies with known total masses, and then obtained the virial dynamical mass estimator of these mock galaxies.","We find that the dynamical masses within three-dimensional $R_{\\rm e}$ to the mock galaxy centres are overall averagely accurate within around 0.1 dex under the symmetric assumption, while $R_{\\rm e}$ means the projected circularized half-stellar mass radius in this work.","We study the local virial mass estimation bias for mock galaxies of different $\\eta$.","The maximum bias difference of two $\\eta$ bins is around 0.16 dex, which with other local biases may help apply the observational virial mass estimators obtained from massive galaxies to other types of galaxies.","We find that the Spearman's $\\rho$ of $\\eta$ with the intrinsic mass estimation deviations is near zero if the local bias is eliminated properly.","The results indicate that even for low-mass galaxies, the existence of high degree of kinematical small-scale fluctuations does not affect the measurement of the dynamical mass of galaxies."],"url":"http://arxiv.org/abs/2403.03260v1","category":"astro-ph.GA"}
{"created":"2024-03-05 19:00:08","title":"Kinematical Fluctuations Vary with Galaxy Surface Mass Density","abstract":"The Galaxy inner parts are generally considered to be optically symmetric, as well as kinematically symmetric for most massive early-type galaxies. At the lower-mass end, many galaxies contain lots of small patches in their velocity maps, causing their kinematics to be nonsmooth in small scales and far from symmetry. These small patches can easily be mistaken for measurement uncertainties and have not been well discussed. We used the comparison of observations and numerical simulations to demonstrate the small patches existence beyond uncertainties. For the first time we have found that the fluctuation degrees have an approximate inverse loglinear relation with the galaxy stellar surface mass densities. This tight relation among galaxies that do not show obvious optical asymmetry that traces environmental perturbations indicates that stellar motion in galaxies has inherent asymmetry besides external environment influences. The degree of the kinetic asymmetry is closely related to and constrained by the intrinsic properties of the host galaxy.","sentences":["The Galaxy inner parts are generally considered to be optically symmetric, as well as kinematically symmetric for most massive early-type galaxies.","At the lower-mass end, many galaxies contain lots of small patches in their velocity maps, causing their kinematics to be nonsmooth in small scales and far from symmetry.","These small patches can easily be mistaken for measurement uncertainties and have not been well discussed.","We used the comparison of observations and numerical simulations to demonstrate the small patches existence beyond uncertainties.","For the first time we have found that the fluctuation degrees have an approximate inverse loglinear relation with the galaxy stellar surface mass densities.","This tight relation among galaxies that do not show obvious optical asymmetry that traces environmental perturbations indicates that stellar motion in galaxies has inherent asymmetry besides external environment influences.","The degree of the kinetic asymmetry is closely related to and constrained by the intrinsic properties of the host galaxy."],"url":"http://arxiv.org/abs/2403.03257v1","category":"astro-ph.GA"}
{"created":"2024-03-05 19:00:02","title":"Turbocharging constraints on dark matter substructure through a synthesis of strong lensing flux ratios and extended lensed arcs","abstract":"Strong gravitational lensing provides a purely gravitational means to infer properties of dark matter halos and thereby constrain the particle nature of dark matter. Strong lenses sometimes appear as four lensed images of a background quasar accompanied by spatially-resolved emission from the quasar host galaxy encircling the main deflector (lensed arcs). We present methodology to simultaneously reconstruct lensed arcs and relative image magnifications (flux ratios) in the presence of full populations of dark matter subhalos and line-of-sight halos. To this end, we develop a new approach for multi-plane ray tracing that accelerates lens mass and source light reconstruction by factors of $\\sim 100-1000$. Using simulated data with a cold dark matter (CDM) ground truth, we show that simultaneous reconstruction of lensed arcs and flux ratios isolates small-scale perturbations to flux ratios by dark matter substructure from uncertainties associated with the main deflector mass profile on larger angular scales. Relative to analyses that use only image positions and flux ratios to constrain the lens model, incorporating arcs strengthens likelihood ratios penalizing warm dark matter (WDM) with a characteristic suppression scale $m_{\\rm{hm}} / M_{\\odot}$ in the range $\\left[10^7 - 10^{7.5}\\right]$, $\\left[10^{7.5} - 10^{8}\\right]$, $\\left[10^8 - 10^{8.5}\\right]$, $\\left[10^{8.5} - 10^{9}\\right]$ by factors of $1.3$, $2.5$, $5.6$, and $13.1$, respectively, and the $95\\%$ exclusion limit improves by 0.5 dex in $\\log_{10} m_{\\rm{hm}}$. The enhanced sensitivity to low-mass halos enabled by these methods pushes the observational frontier of substructure lensing to the threshold of galaxy formation, enabling stringent tests of any theory that alters the properties of dark matter halos.","sentences":["Strong gravitational lensing provides a purely gravitational means to infer properties of dark matter halos and thereby constrain the particle nature of dark matter.","Strong lenses sometimes appear as four lensed images of a background quasar accompanied by spatially-resolved emission from the quasar host galaxy encircling the main deflector (lensed arcs).","We present methodology to simultaneously reconstruct lensed arcs and relative image magnifications (flux ratios) in the presence of full populations of dark matter subhalos and line-of-sight halos.","To this end, we develop a new approach for multi-plane ray tracing that accelerates lens mass and source light reconstruction by factors of $\\sim 100-1000$. Using simulated data with a cold dark matter (CDM) ground truth, we show that simultaneous reconstruction of lensed arcs and flux ratios isolates small-scale perturbations to flux ratios by dark matter substructure from uncertainties associated with the main deflector mass profile on larger angular scales.","Relative to analyses that use only image positions and flux ratios to constrain the lens model, incorporating arcs strengthens likelihood ratios penalizing warm dark matter (WDM) with a characteristic suppression scale $m_{\\rm{hm}} / M_{\\odot}$ in the range $\\left[10^7 - 10^{7.5}\\right]$, $\\left[10^{7.5} - 10^{8}\\right]$, $\\left[10^8 - 10^{8.5}\\right]$, $\\left[10^{8.5} - 10^{9}\\right]$ by factors of $1.3$, $2.5$, $5.6$, and $13.1$, respectively, and the $95\\%$ exclusion limit improves by 0.5 dex in $\\log_{10} m_{\\rm{hm}}$. The enhanced sensitivity to low-mass halos enabled by these methods pushes the observational frontier of substructure lensing to the threshold of galaxy formation, enabling stringent tests of any theory that alters the properties of dark matter halos."],"url":"http://arxiv.org/abs/2403.03253v1","category":"astro-ph.CO"}
{"created":"2024-03-05 19:00:01","title":"Listening to the long ringdown: a novel way to pinpoint the equation of state in neutron-star cores","abstract":"Multimessenger signals from binary neutron star (BNS) mergers are promising tools to infer the largely unknown properties of nuclear matter at densities that are presently inaccessible to laboratory experiments. The gravitational waves (GWs) emitted by BNS merger remnants, in particular, have the potential of setting tight constraints on the neutron-star equation of state (EOS) that would complement those coming from the late inspiral, direct mass-radius measurements, or ab-initio dense-matter calculations. To explore this possibility, we perform a representative series of general-relativistic simulations of BNS systems with EOSs carefully constructed so as to cover comprehensively the high-density regime of the EOS space. From these simulations, we identify a novel and tight correlation between the ratio of the energy and angular-momentum losses in the late-time portion of the post-merger signal, i.e., the \"long ringdown\", and the properties of the EOS at the highest pressures and densities in neutron-star cores. When applying this correlation to post-merger GW signals, we find a significant reduction of the EOS uncertainty at densities several times the nuclear saturation density, where no direct constraints are currently available. Hence, the long ringdown has the potential of providing new and stringent constraints on the state of matter in neutron stars in general and, in particular, in their cores.","sentences":["Multimessenger signals from binary neutron star (BNS) mergers are promising tools to infer the largely unknown properties of nuclear matter at densities that are presently inaccessible to laboratory experiments.","The gravitational waves (GWs) emitted by BNS merger remnants, in particular, have the potential of setting tight constraints on the neutron-star equation of state (EOS) that would complement those coming from the late inspiral, direct mass-radius measurements, or ab-initio dense-matter calculations.","To explore this possibility, we perform a representative series of general-relativistic simulations of BNS systems with EOSs carefully constructed so as to cover comprehensively the high-density regime of the EOS space.","From these simulations, we identify a novel and tight correlation between the ratio of the energy and angular-momentum losses in the late-time portion of the post-merger signal, i.e., the \"long ringdown\", and the properties of the EOS at the highest pressures and densities in neutron-star cores.","When applying this correlation to post-merger GW signals, we find a significant reduction of the EOS uncertainty at densities several times the nuclear saturation density, where no direct constraints are currently available.","Hence, the long ringdown has the potential of providing new and stringent constraints on the state of matter in neutron stars in general and, in particular, in their cores."],"url":"http://arxiv.org/abs/2403.03246v1","category":"astro-ph.HE"}
{"created":"2024-03-05 19:00:01","title":"Asteroid collisions: expected visibility and rate","abstract":"Asteroid collisions are one of the main processes responsible for the evolution of bodies in the main belt. Using observations of the Dimorphos impact by the DART spacecraft, we estimate how asteroid collisions in the main belt may look in the first hours after the impact. If the DART event is representative of asteroid collisions with a ~1m size impactor, then the light curves of these collisions will rise on time scales of about >100s and will remain bright for about one hour. Next, the light curve will decay on a few hours time scale to an intermediate luminosity level in which it will remain for several weeks, before slowly returning to its baseline magnitude. This estimate suffers from several uncertainties due to, e.g., the diversity of asteroid composition, their material strength, and spread in collision velocities. We estimate that the rate of collisions in the main belt with energy similar or larger than the DART impact is of the order of 7000 per year (+/-1dex). The large range is due to the uncertainty in the abundance of ~1-m size asteroids. We estimate the magnitude distribution of such events in the main belt, and we show that ~6% of these events may peak at magnitudes brighter than 21. The detection of these events requires a survey with <1hr cadence and may contribute to our understanding of the asteroids' size distribution, collisional physics, and dust production. With an adequate survey strategy, new survey telescopes may regularly detect asteroid collisions.","sentences":["Asteroid collisions are one of the main processes responsible for the evolution of bodies in the main belt.","Using observations of the Dimorphos impact by the DART spacecraft, we estimate how asteroid collisions in the main belt may look in the first hours after the impact.","If the DART event is representative of asteroid collisions with a ~1m size impactor, then the light curves of these collisions will rise on time scales of about >100s and will remain bright for about one hour.","Next, the light curve will decay on a few hours time scale to an intermediate luminosity level in which it will remain for several weeks, before slowly returning to its baseline magnitude.","This estimate suffers from several uncertainties due to, e.g., the diversity of asteroid composition, their material strength, and spread in collision velocities.","We estimate that the rate of collisions in the main belt with energy similar or larger than the DART impact is of the order of 7000 per year (+/-1dex).","The large range is due to the uncertainty in the abundance of ~1-m size asteroids.","We estimate the magnitude distribution of such events in the main belt, and we show that ~6% of these events may peak at magnitudes brighter than 21.","The detection of these events requires a survey with <1hr cadence and may contribute to our understanding of the asteroids' size distribution, collisional physics, and dust production.","With an adequate survey strategy, new survey telescopes may regularly detect asteroid collisions."],"url":"http://arxiv.org/abs/2403.03248v1","category":"astro-ph.EP"}
{"created":"2024-03-05 19:00:00","title":"Possible Hycean conditions in the sub-Neptune TOI-270 d","abstract":"The JWST has ushered in a new era in atmospheric characterisations of temperate low-mass exoplanets with recent detections of carbon-bearing molecules in the candidate Hycean world K2-18 b. We investigated JWST observations of the TOI-270 system, with two sub-Neptunes simultaneously transiting the nearby M dwarf during the visit. We report our atmospheric characterisation of the outer planet TOI-270 d, a candidate Hycean world, with JWST transmission spectroscopy using the NIRSpec G395H instrument in the 2.7-5.2 $\\mu$m range, combined with previous observations obtained with the HST WFC3 spectrograph (1.1-1.6 $\\mu$m). The spectrum reveals strong signatures of CH4 and CO2 at 3.8-4.9$\\sigma$ and 2.9-3.9$\\sigma$ confidence, respectively, and no evidence of NH3. The abundant CH4 and CO2, at ~0.1-1% mixing ratios, and the non-detection of NH3 are similar to the findings reported for K2-18 b and consistent with predictions for a Hycean world with a planet-wide ocean under a H2-rich atmosphere. We also report evidence of CS2 at a 2.3-3.0$\\sigma$ confidence and a potential inference of H2O at 1.6-4.4$\\sigma$, depending on the data analysis approach, and discuss possible interpretations of these results. The spectrum does not provide strong constraints on the presence of clouds or hazes in the observable atmosphere, nor any evidence for the effects of stellar heterogeneities, which is consistent with previous studies. For the smaller inner planet TOI-270 b, we find that the spectrum is inconsistent with a featureless spectrum at ~3$\\sigma$, showing some preference for an H2-rich atmosphere in a super-Earth. We discuss the implications of our findings and future prospects.","sentences":["The JWST has ushered in a new era in atmospheric characterisations of temperate low-mass exoplanets with recent detections of carbon-bearing molecules in the candidate Hycean world K2-18 b.","We investigated JWST observations of the TOI-270 system, with two sub-Neptunes simultaneously transiting the nearby M dwarf during the visit.","We report our atmospheric characterisation of the outer planet TOI-270 d, a candidate Hycean world, with JWST transmission spectroscopy using the NIRSpec G395H instrument in the 2.7-5.2 $\\mu$m range, combined with previous observations obtained with the HST WFC3 spectrograph (1.1-1.6 $\\mu$m).","The spectrum reveals strong signatures of CH4 and CO2 at 3.8-4.9$\\sigma$ and 2.9-3.9$\\sigma$ confidence, respectively, and no evidence of NH3.","The abundant CH4 and CO2, at ~0.1-1% mixing ratios, and the non-detection of NH3 are similar to the findings reported for K2-18 b and consistent with predictions for a Hycean world with a planet-wide ocean under a H2-rich atmosphere.","We also report evidence of CS2 at a 2.3-3.0$\\sigma$ confidence and a potential inference of H2O at 1.6-4.4$\\sigma$, depending on the data analysis approach, and discuss possible interpretations of these results.","The spectrum does not provide strong constraints on the presence of clouds or hazes in the observable atmosphere, nor any evidence for the effects of stellar heterogeneities, which is consistent with previous studies.","For the smaller inner planet TOI-270 b, we find that the spectrum is inconsistent with a featureless spectrum at ~3$\\sigma$, showing some preference for an H2-rich atmosphere in a super-Earth.","We discuss the implications of our findings and future prospects."],"url":"http://arxiv.org/abs/2403.03244v1","category":"astro-ph.EP"}
{"created":"2024-03-06 17:06:07","title":"ECAP: Extensive Cut-and-Paste Augmentation for Unsupervised Domain Adaptive Semantic Segmentation","abstract":"We consider unsupervised domain adaptation (UDA) for semantic segmentation in which the model is trained on a labeled source dataset and adapted to an unlabeled target dataset. Unfortunately, current self-training methods are susceptible to misclassified pseudo-labels resulting from erroneous predictions. Since certain classes are typically associated with less reliable predictions in UDA, reducing the impact of such pseudo-labels without skewing the training towards some classes is notoriously difficult. To this end, we propose an extensive cut-and-paste strategy (ECAP) to leverage reliable pseudo-labels through data augmentation. Specifically, ECAP maintains a memory bank of pseudo-labeled target samples throughout training and cut-and-pastes the most confident ones onto the current training batch. We implement ECAP on top of the recent method MIC and boost its performance on two synthetic-to-real domain adaptation benchmarks. Notably, MIC+ECAP reaches an unprecedented performance of 69.1 mIoU on the Synthia->Cityscapes benchmark. Our code is available at https://github.com/ErikBrorsson/ECAP.","sentences":["We consider unsupervised domain adaptation (UDA) for semantic segmentation in which the model is trained on a labeled source dataset and adapted to an unlabeled target dataset.","Unfortunately, current self-training methods are susceptible to misclassified pseudo-labels resulting from erroneous predictions.","Since certain classes are typically associated with less reliable predictions in UDA, reducing the impact of such pseudo-labels without skewing the training towards some classes is notoriously difficult.","To this end, we propose an extensive cut-and-paste strategy (ECAP) to leverage reliable pseudo-labels through data augmentation.","Specifically, ECAP maintains a memory bank of pseudo-labeled target samples throughout training and cut-and-pastes the most confident ones onto the current training batch.","We implement ECAP on top of the recent method MIC and boost its performance on two synthetic-to-real domain adaptation benchmarks.","Notably, MIC+ECAP reaches an unprecedented performance of 69.1 mIoU on the Synthia->Cityscapes benchmark.","Our code is available at https://github.com/ErikBrorsson/ECAP."],"url":"http://arxiv.org/abs/2403.03854v1","category":"cs.CV"}
{"created":"2024-03-06 16:57:36","title":"Numerical study of a viscous breaking water wave and the limit of vanishing viscosity","abstract":"We introduce a numerical strategy to study the evolution of 2D water waves in the presence of a plunging jet. The free-surface Navier-Stokes solution is obtained with a finite but small viscosity. We observe the formation of a surface boundary layer where the vorticity is localised. We highlight convergence to the inviscid solution. The effects of dissipation on the development of a singularity at the tip of the wave is also investigated by characterising the vorticity boundary layer appearing near the interface.","sentences":["We introduce a numerical strategy to study the evolution of 2D water waves in the presence of a plunging jet.","The free-surface Navier-Stokes solution is obtained with a finite but small viscosity.","We observe the formation of a surface boundary layer where the vorticity is localised.","We highlight convergence to the inviscid solution.","The effects of dissipation on the development of a singularity at the tip of the wave is also investigated by characterising the vorticity boundary layer appearing near the interface."],"url":"http://arxiv.org/abs/2403.03851v1","category":"physics.flu-dyn"}
{"created":"2024-03-06 16:28:31","title":"An Adaptive Multivariate Functional EWMA Control Chart","abstract":"In many modern industrial scenarios, the measurements of the quality characteristics of interest are often required to be represented as functional data or profiles. This motivates the growing interest in extending traditional univariate statistical process monitoring (SPM) schemes to the functional data setting. This article proposes a new SPM scheme, which is referred to as adaptive multivariate functional EWMA (AMFEWMA), to extend the well-known exponentially weighted moving average (EWMA) control chart from the univariate scalar to the multivariate functional setting. The favorable performance of the AMFEWMA control chart over existing methods is assessed via an extensive Monte Carlo simulation. Its practical applicability is demonstrated through a case study in the monitoring of the quality of a resistance spot welding process in the automotive industry through the online observations of dynamic resistance curves, which are associated with multiple spot welds on the same car body and recognized as the full technological signature of the process.","sentences":["In many modern industrial scenarios, the measurements of the quality characteristics of interest are often required to be represented as functional data or profiles.","This motivates the growing interest in extending traditional univariate statistical process monitoring (SPM) schemes to the functional data setting.","This article proposes a new SPM scheme, which is referred to as adaptive multivariate functional EWMA (AMFEWMA), to extend the well-known exponentially weighted moving average (EWMA) control chart from the univariate scalar to the multivariate functional setting.","The favorable performance of the AMFEWMA control chart over existing methods is assessed via an extensive Monte Carlo simulation.","Its practical applicability is demonstrated through a case study in the monitoring of the quality of a resistance spot welding process in the automotive industry through the online observations of dynamic resistance curves, which are associated with multiple spot welds on the same car body and recognized as the full technological signature of the process."],"url":"http://arxiv.org/abs/2403.03837v1","category":"stat.ME"}
{"created":"2024-03-06 15:16:31","title":"Ancestor regression in structural vector autoregressive models","abstract":"We present a new method for causal discovery in linear structural vector autoregressive models. We adapt an idea designed for independent observations to the case of time series while retaining its favorable properties, i.e., explicit error control for false causal discovery, at least asymptotically. We apply our method to several real-world bivariate time series datasets and discuss its findings which mostly agree with common understanding. The arrow of time in a model can be interpreted as background knowledge on possible causal mechanisms. Hence, our ideas could be extended to incorporating different background knowledge, even for independent observations.","sentences":["We present a new method for causal discovery in linear structural vector autoregressive models.","We adapt an idea designed for independent observations to the case of time series while retaining its favorable properties, i.e., explicit error control for false causal discovery, at least asymptotically.","We apply our method to several real-world bivariate time series datasets and discuss its findings which mostly agree with common understanding.","The arrow of time in a model can be interpreted as background knowledge on possible causal mechanisms.","Hence, our ideas could be extended to incorporating different background knowledge, even for independent observations."],"url":"http://arxiv.org/abs/2403.03778v1","category":"stat.ME"}
{"created":"2024-03-06 13:43:36","title":"Multi-Grained Cross-modal Alignment for Learning Open-vocabulary Semantic Segmentation from Text Supervision","abstract":"Recently, learning open-vocabulary semantic segmentation from text supervision has achieved promising downstream performance. Nevertheless, current approaches encounter an alignment granularity gap owing to the absence of dense annotations, wherein they learn coarse image/region-text alignment during training yet perform group/pixel-level predictions at inference. Such discrepancy leads to suboptimal learning efficiency and inferior zero-shot segmentation results. In this paper, we introduce a Multi-Grained Cross-modal Alignment (MGCA) framework, which explicitly learns pixel-level alignment along with object- and region-level alignment to bridge the granularity gap without any dense annotations. Specifically, MGCA ingeniously constructs pseudo multi-granular semantic correspondences upon image-text pairs and collaborates with hard sampling strategies to facilitate fine-grained cross-modal contrastive learning. Further, we point out the defects of existing group and pixel prediction units in downstream segmentation and develop an adaptive semantic unit which effectively mitigates their dilemmas including under- and over-segmentation. Training solely on CC3M, our method achieves significant advancements over state-of-the-art methods, demonstrating its effectiveness and efficiency.","sentences":["Recently, learning open-vocabulary semantic segmentation from text supervision has achieved promising downstream performance.","Nevertheless, current approaches encounter an alignment granularity gap owing to the absence of dense annotations, wherein they learn coarse image/region-text alignment during training yet perform group/pixel-level predictions at inference.","Such discrepancy leads to suboptimal learning efficiency and inferior zero-shot segmentation results.","In this paper, we introduce a Multi-Grained Cross-modal Alignment (MGCA) framework, which explicitly learns pixel-level alignment along with object- and region-level alignment to bridge the granularity gap without any dense annotations.","Specifically, MGCA ingeniously constructs pseudo multi-granular semantic correspondences upon image-text pairs and collaborates with hard sampling strategies to facilitate fine-grained cross-modal contrastive learning.","Further, we point out the defects of existing group and pixel prediction units in downstream segmentation and develop an adaptive semantic unit which effectively mitigates their dilemmas including under- and over-segmentation.","Training solely on CC3M, our method achieves significant advancements over state-of-the-art methods, demonstrating its effectiveness and efficiency."],"url":"http://arxiv.org/abs/2403.03707v1","category":"cs.CV"}
{"created":"2024-03-06 12:53:27","title":"Revisiting phonon thermal transport in two-dimensional gallium nitride: higher-order phonon-phonon and phonon-electron scattering","abstract":"Two-dimensional gallium nitride (2D-GaN) has great potential in power electronics and optoelectronics. Heat dissipation is a critical issue for these applications of 2D-GaN. Previous studies showed that higher-order phonon-phonon scattering has extremely strong effects on the lattice thermal conductivity of 2D-GaN, which exhibits noticeable discrepancies with lattice thermal conductivity calculated from molecular dynamics. In this work, it is found that the fourth-order interatomic force constants (4th-IFCs) of 2D-GaN are quite sensitive to atomic displacement in the finite different method. The effects of the four-phonon scattering can be severely overestimated with non-convergent 4th-IFCs. The lattice thermal conductivity from three-phonon scattering is reduced by 65.6% due to four-phonon scattering. The reflection symmetry allows significantly more four-phonon processes than three-phonon processes. It was previously thought the electron-phonon interactions have significant effects on the lattice thermal conductivity of two-dimensional materials. However, the effects of phonon-electron interactions on the lattice thermal conductivity of both n-type and p-type 2D-GaN at high charge carrier concentrations can be neglected due to the few phonon-electron scattering channels and the relatively strong four-phonon scattering.","sentences":["Two-dimensional gallium nitride (2D-GaN) has great potential in power electronics and optoelectronics.","Heat dissipation is a critical issue for these applications of 2D-GaN. Previous studies showed that higher-order phonon-phonon scattering has extremely strong effects on the lattice thermal conductivity of 2D-GaN, which exhibits noticeable discrepancies with lattice thermal conductivity calculated from molecular dynamics.","In this work, it is found that the fourth-order interatomic force constants (4th-IFCs) of 2D-GaN are quite sensitive to atomic displacement in the finite different method.","The effects of the four-phonon scattering can be severely overestimated with non-convergent 4th-IFCs.","The lattice thermal conductivity from three-phonon scattering is reduced by 65.6% due to four-phonon scattering.","The reflection symmetry allows significantly more four-phonon processes than three-phonon processes.","It was previously thought the electron-phonon interactions have significant effects on the lattice thermal conductivity of two-dimensional materials.","However, the effects of phonon-electron interactions on the lattice thermal conductivity of both n-type and p-type 2D-GaN at high charge carrier concentrations can be neglected due to the few phonon-electron scattering channels and the relatively strong four-phonon scattering."],"url":"http://arxiv.org/abs/2403.03673v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-06 12:22:56","title":"3D Printed Waveguide for Augmented Reality","abstract":"Mass production of augmented reality (AR) waveguides has been challenging due to the intricate nature of the fabrication technique and the high precision required for its optical characteristics. In this paper, we have presented a novel and low-cost approach for fabricating geometric optical waveguides designed for AR applications utilizing 3D printing techniques. To strike a balance between optical performance and fabrication feasibility, we have optimized the conventional geometric waveguide design to facilitate easier fabrication. It is worth noting that our proposed method does not require molding, dicing, and post-surface polishing after printing. A prototype based on this method has been successfully fabricated, showing the immersion between the virtual image and the real-world scene. The proposed method has great potential for adaptation to mass production in various AR applications.","sentences":["Mass production of augmented reality (AR) waveguides has been challenging due to the intricate nature of the fabrication technique and the high precision required for its optical characteristics.","In this paper, we have presented a novel and low-cost approach for fabricating geometric optical waveguides designed for AR applications utilizing 3D printing techniques.","To strike a balance between optical performance and fabrication feasibility, we have optimized the conventional geometric waveguide design to facilitate easier fabrication.","It is worth noting that our proposed method does not require molding, dicing, and post-surface polishing after printing.","A prototype based on this method has been successfully fabricated, showing the immersion between the virtual image and the real-world scene.","The proposed method has great potential for adaptation to mass production in various AR applications."],"url":"http://arxiv.org/abs/2403.03652v1","category":"physics.optics"}
{"created":"2024-03-06 08:35:29","title":"Gadolinium dose reduction for brain MRI using conditional deep learning","abstract":"Recently, deep learning (DL)-based methods have been proposed for the computational reduction of gadolinium-based contrast agents (GBCAs) to mitigate adverse side effects while preserving diagnostic value. Currently, the two main challenges for these approaches are the accurate prediction of contrast enhancement and the synthesis of realistic images. In this work, we address both challenges by utilizing the contrast signal encoded in the subtraction images of pre-contrast and post-contrast image pairs. To avoid the synthesis of any noise or artifacts and solely focus on contrast signal extraction and enhancement from low-dose subtraction images, we train our DL model using noise-free standard-dose subtraction images as targets. As a result, our model predicts the contrast enhancement signal only; thereby enabling synthesization of images beyond the standard dose. Furthermore, we adapt the embedding idea of recent diffusion-based models to condition our model on physical parameters affecting the contrast enhancement behavior. We demonstrate the effectiveness of our approach on synthetic and real datasets using various scanners, field strengths, and contrast agents.","sentences":["Recently, deep learning (DL)-based methods have been proposed for the computational reduction of gadolinium-based contrast agents (GBCAs) to mitigate adverse side effects while preserving diagnostic value.","Currently, the two main challenges for these approaches are the accurate prediction of contrast enhancement and the synthesis of realistic images.","In this work, we address both challenges by utilizing the contrast signal encoded in the subtraction images of pre-contrast and post-contrast image pairs.","To avoid the synthesis of any noise or artifacts and solely focus on contrast signal extraction and enhancement from low-dose subtraction images, we train our DL model using noise-free standard-dose subtraction images as targets.","As a result, our model predicts the contrast enhancement signal only; thereby enabling synthesization of images beyond the standard dose.","Furthermore, we adapt the embedding idea of recent diffusion-based models to condition our model on physical parameters affecting the contrast enhancement behavior.","We demonstrate the effectiveness of our approach on synthetic and real datasets using various scanners, field strengths, and contrast agents."],"url":"http://arxiv.org/abs/2403.03539v1","category":"eess.IV"}
{"created":"2024-03-06 07:29:57","title":"GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection","abstract":"Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.","sentences":["Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states.","Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states.","However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start.","In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA.","Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks.","Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline.","Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies."],"url":"http://arxiv.org/abs/2403.03507v1","category":"cs.LG"}
{"created":"2024-03-06 07:19:36","title":"Universality of pseudo-Goldstone damping near critical points","abstract":"Recently, in studies of holographic models and hydrodynamics with spontaneous breaking of approximate symmetries, it has been proposed that the damping of pseudo-Goldstone modes at finite temperatures is universally constrained in the way that $\\Omega_{\\varphi}/m_{\\varphi}^2\\simeq D_{\\varphi}$ in the broken phase, where $\\Omega_{\\varphi}$ and $m_{\\varphi} $ are the relaxation rate at zero wavenumber and the mass of pseudo-Goldstones, $D_{\\varphi}$ is the Goldstone diffusivity in the limit of purely spontaneous breaking. In this paper, we investigate the pseudo-Goldstone damping in a purely relaxational O($N$) model by performing the functional renormalization group calculations at the full quantum and stochastic level within the Schwinger-Keldysh formalism. We find that, away from the critical temperature, the proposed relation is always valid. When the temperature is very close to the critical value such that the mass of the Higgs mode is comparable to the mass of the pseudo-Goldstone modes, the pseudo-Goldstone damping displays a novel scaling behavior that follows $\\Omega_\\varphi/m_\\varphi^2\\propto m_{\\varphi}^{\\Delta_\\eta}$ with a correction $\\Delta_\\eta$ controlled by the critical universalities. Moreover, we study how the correction depends on the value of $N$ and show that $\\Delta_\\eta \\rightarrow 0$ when fluctuations are infinitely suppressed in the large $N$ limit. In this case, the proposed relation works even in the critical region. Finally, we match our results to the dissipative sector of the pion dynamics near the chiral phase transition.","sentences":["Recently, in studies of holographic models and hydrodynamics with spontaneous breaking of approximate symmetries, it has been proposed that the damping of pseudo-Goldstone modes at finite temperatures is universally constrained in the way that $\\Omega_{\\varphi}/m_{\\varphi}^2\\simeq D_{\\varphi}$ in the broken phase, where $\\Omega_{\\varphi}$ and $m_{\\varphi} $ are the relaxation rate at zero wavenumber and the mass of pseudo-Goldstones, $D_{\\varphi}$ is the Goldstone diffusivity in the limit of purely spontaneous breaking.","In this paper, we investigate the pseudo-Goldstone damping in a purely relaxational O($N$) model by performing the functional renormalization group calculations at the full quantum and stochastic level within the Schwinger-Keldysh formalism.","We find that, away from the critical temperature, the proposed relation is always valid.","When the temperature is very close to the critical value such that the mass of the Higgs mode is comparable to the mass of the pseudo-Goldstone modes, the pseudo-Goldstone damping displays a novel scaling behavior that follows $\\Omega_\\varphi/m_\\varphi^2\\propto m_{\\varphi}^{\\Delta_\\eta}$ with a correction $\\Delta_\\eta$ controlled by the critical universalities.","Moreover, we study how the correction depends on the value of $N$ and show that $\\Delta_\\eta \\rightarrow 0$ when fluctuations are infinitely suppressed in the large $N$ limit.","In this case, the proposed relation works even in the critical region.","Finally, we match our results to the dissipative sector of the pion dynamics near the chiral phase transition."],"url":"http://arxiv.org/abs/2403.03503v1","category":"hep-th"}
{"created":"2024-03-06 03:02:09","title":"An arbitrarily high order unfitted finite element method for elliptic interface problems with automatic mesh generation, Part II. Piecewise-smooth interfaces","abstract":"We consider the reliable implementation of an adaptive high-order unfitted finite element method on Cartesian meshes for solving elliptic interface problems with geometrically curved singularities. We extend our previous work on the reliable cell merging algorithm for smooth interfaces to automatically generate the induced mesh for piecewise smooth interfaces. An $hp$ a posteriori error estimate is derived for a new unfitted finite element method whose finite element functions are conforming in each subdomain. Numerical examples illustrate the competitive performance of the method.","sentences":["We consider the reliable implementation of an adaptive high-order unfitted finite element method on Cartesian meshes for solving elliptic interface problems with geometrically curved singularities.","We extend our previous work on the reliable cell merging algorithm for smooth interfaces to automatically generate the induced mesh for piecewise smooth interfaces.","An $hp$ a posteriori error estimate is derived for a new unfitted finite element method whose finite element functions are conforming in each subdomain.","Numerical examples illustrate the competitive performance of the method."],"url":"http://arxiv.org/abs/2403.03418v1","category":"math.NA"}
{"created":"2024-03-06 02:01:38","title":"Causality-based Cross-Modal Representation Learning for Vision-and-Language Navigation","abstract":"Vision-and-Language Navigation (VLN) has gained significant research interest in recent years due to its potential applications in real-world scenarios. However, existing VLN methods struggle with the issue of spurious associations, resulting in poor generalization with a significant performance gap between seen and unseen environments. In this paper, we tackle this challenge by proposing a unified framework CausalVLN based on the causal learning paradigm to train a robust navigator capable of learning unbiased feature representations. Specifically, we establish reasonable assumptions about confounders for vision and language in VLN using the structured causal model (SCM). Building upon this, we propose an iterative backdoor-based representation learning (IBRL) method that allows for the adaptive and effective intervention on confounders. Furthermore, we introduce the visual and linguistic backdoor causal encoders to enable unbiased feature expression for multi-modalities during training and validation, enhancing the agent's capability to generalize across different environments. Experiments on three VLN datasets (R2R, RxR, and REVERIE) showcase the superiority of our proposed method over previous state-of-the-art approaches. Moreover, detailed visualization analysis demonstrates the effectiveness of CausalVLN in significantly narrowing down the performance gap between seen and unseen environments, underscoring its strong generalization capability.","sentences":["Vision-and-Language Navigation (VLN) has gained significant research interest in recent years due to its potential applications in real-world scenarios.","However, existing VLN methods struggle with the issue of spurious associations, resulting in poor generalization with a significant performance gap between seen and unseen environments.","In this paper, we tackle this challenge by proposing a unified framework CausalVLN based on the causal learning paradigm to train a robust navigator capable of learning unbiased feature representations.","Specifically, we establish reasonable assumptions about confounders for vision and language in VLN using the structured causal model (SCM).","Building upon this, we propose an iterative backdoor-based representation learning (IBRL) method that allows for the adaptive and effective intervention on confounders.","Furthermore, we introduce the visual and linguistic backdoor causal encoders to enable unbiased feature expression for multi-modalities during training and validation, enhancing the agent's capability to generalize across different environments.","Experiments on three VLN datasets (R2R, RxR, and REVERIE) showcase the superiority of our proposed method over previous state-of-the-art approaches.","Moreover, detailed visualization analysis demonstrates the effectiveness of CausalVLN in significantly narrowing down the performance gap between seen and unseen environments, underscoring its strong generalization capability."],"url":"http://arxiv.org/abs/2403.03405v1","category":"cs.CV"}
{"created":"2024-03-06 00:47:38","title":"Bath-induced interactions and transient dynamics in open quantum systems at strong coupling: Effective Hamiltonian approach","abstract":"Understanding the dynamics of dissipative quantum systems, particularly beyond the weak coupling approximation, is central to various quantum applications. While numerically exact methods provide accurate solutions, they often lack the analytical insight provided by theoretical approaches. In this study, we employ the recently-developed method dubbed the \\emph{effective Hamiltonian theory} to understand the dynamics of system-bath configurations without resorting to a perturbative description of the system-bath coupling energy. Through a combination of mapping steps and truncation, the effective Hamiltonian theory offers both analytical insights into signatures of strong couplings in open quantum systems and a straightforward path for numerical simulations. To validate the accuracy of the method, we apply it to two canonical models: a single spin immersed in a bosonic bath and two noninteracting spins in a common bath. In both cases, we study the transient regime and the steady state limit at nonzero temperature, and spanning system-bath interactions from the weak to the strong regime. By comparing the results of the effective Hamiltonian theory with numerically exact simulations, we show that although the former overlooks non-Markovian features in the transient equilibration dynamics, it correctly captures non-perturbative bath-generated couplings between otherwise non-interacting spins as observed in their synchronization dynamics and correlations. Altogether, the effective Hamiltonian theory offers a powerful approach to understanding strong coupling dynamics and thermodynamics, capturing the signatures of such interactions in both relaxation dynamics and in the steady state limit.","sentences":["Understanding the dynamics of dissipative quantum systems, particularly beyond the weak coupling approximation, is central to various quantum applications.","While numerically exact methods provide accurate solutions, they often lack the analytical insight provided by theoretical approaches.","In this study, we employ the recently-developed method dubbed the \\emph{effective Hamiltonian theory} to understand the dynamics of system-bath configurations without resorting to a perturbative description of the system-bath coupling energy.","Through a combination of mapping steps and truncation, the effective Hamiltonian theory offers both analytical insights into signatures of strong couplings in open quantum systems and a straightforward path for numerical simulations.","To validate the accuracy of the method, we apply it to two canonical models: a single spin immersed in a bosonic bath and two noninteracting spins in a common bath.","In both cases, we study the transient regime and the steady state limit at nonzero temperature, and spanning system-bath interactions from the weak to the strong regime.","By comparing the results of the effective Hamiltonian theory with numerically exact simulations, we show that although the former overlooks non-Markovian features in the transient equilibration dynamics, it correctly captures non-perturbative bath-generated couplings between otherwise non-interacting spins as observed in their synchronization dynamics and correlations.","Altogether, the effective Hamiltonian theory offers a powerful approach to understanding strong coupling dynamics and thermodynamics, capturing the signatures of such interactions in both relaxation dynamics and in the steady state limit."],"url":"http://arxiv.org/abs/2403.03386v1","category":"quant-ph"}
{"created":"2024-03-05 23:31:39","title":"Action potentials in vitro: theory and experiment","abstract":"Action potential generation underlies some of the most consequential dynamical systems on Earth, from brains to hearts. It is therefore interesting to develop synthetic cell-free systems, based on the same molecular mechanisms, which may allow for the exploration of parameter regions and phenomena not attainable, or not apparent, in the live cell. We previously constructed such a synthetic system, based on biological components, which fires action potentials. We call it \"Artificial Axon\". The system is minimal in that it relies on a single ion channel species for its dynamics. Here we characterize the Artificial Axon as a dynamical system in time, using a simplified Hodgkin-Huxley model adapted to our experimental context. We construct a phase diagram in parameter space identifying regions corresponding to different temporal behavior, such as Action Potential (AP) trains, single shot APs, or damped oscillations. The main new result is the finding that our system with a single ion channel species, with inactivation, is dynamically equivalent to the system of two channel species without inactivation (the Morris-Lecar system), which exists in nature. We discuss the transitions and bifurcations occurring crossing phase boundaries in the phase diagram, and obtain criteria for the channels' properties necessary to obtain the desired dynamical behavior. In the second part of the paper we present new experimental results obtained with a system of two AAs connected by excitatory and/or inhibitory electronic \"synapses\". We discuss the feasibility of constructing an autonomous oscillator with this system.","sentences":["Action potential generation underlies some of the most consequential dynamical systems on Earth, from brains to hearts.","It is therefore interesting to develop synthetic cell-free systems, based on the same molecular mechanisms, which may allow for the exploration of parameter regions and phenomena not attainable, or not apparent, in the live cell.","We previously constructed such a synthetic system, based on biological components, which fires action potentials.","We call it \"Artificial Axon\".","The system is minimal in that it relies on a single ion channel species for its dynamics.","Here we characterize the Artificial Axon as a dynamical system in time, using a simplified Hodgkin-Huxley model adapted to our experimental context.","We construct a phase diagram in parameter space identifying regions corresponding to different temporal behavior, such as Action Potential (AP) trains, single shot APs, or damped oscillations.","The main new result is the finding that our system with a single ion channel species, with inactivation, is dynamically equivalent to the system of two channel species without inactivation (the Morris-Lecar system), which exists in nature.","We discuss the transitions and bifurcations occurring crossing phase boundaries in the phase diagram, and obtain criteria for the channels' properties necessary to obtain the desired dynamical behavior.","In the second part of the paper we present new experimental results obtained with a system of two AAs connected by excitatory and/or inhibitory electronic \"synapses\".","We discuss the feasibility of constructing an autonomous oscillator with this system."],"url":"http://arxiv.org/abs/2403.03369v1","category":"physics.bio-ph"}
{"created":"2024-03-05 23:18:38","title":"Exchange-correlation energy from Green's functions","abstract":"DFT calculations yield useful ground-state energies and densities, while Green's function techniques (such as $GW$) are mostly used to produce spectral functions. From the Galitskii-Migdal formula, we extract the exchange-correlation of DFT directly from a Green's function. This spectral representation provides an alternative to the fluctuation-dissipation theorem of DFT, identifying distinct single-particle and many-particle contributions. Results are illustrated on the uniform electron gas and the two-site Hubbard model.","sentences":["DFT calculations yield useful ground-state energies and densities, while Green's function techniques (such as $GW$) are mostly used to produce spectral functions.","From the Galitskii-Migdal formula, we extract the exchange-correlation of DFT directly from a Green's function.","This spectral representation provides an alternative to the fluctuation-dissipation theorem of DFT, identifying distinct single-particle and many-particle contributions.","Results are illustrated on the uniform electron gas and the two-site Hubbard model."],"url":"http://arxiv.org/abs/2403.03364v1","category":"physics.chem-ph"}
{"created":"2024-03-05 20:23:25","title":"Graph Learning for Parameter Prediction of Quantum Approximate Optimization Algorithm","abstract":"In recent years, quantum computing has emerged as a transformative force in the field of combinatorial optimization, offering novel approaches to tackling complex problems that have long challenged classical computational methods. Among these, the Quantum Approximate Optimization Algorithm (QAOA) stands out for its potential to efficiently solve the Max-Cut problem, a quintessential example of combinatorial optimization. However, practical application faces challenges due to current limitations on quantum computational resource. Our work optimizes QAOA initialization, using Graph Neural Networks (GNN) as a warm-start technique. This sacrifices affordable computational resource on classical computer to reduce quantum computational resource overhead, enhancing QAOA's effectiveness. Experiments with various GNN architectures demonstrate the adaptability and stability of our framework, highlighting the synergy between quantum algorithms and machine learning. Our findings show GNN's potential in improving QAOA performance, opening new avenues for hybrid quantum-classical approaches in quantum computing and contributing to practical applications.","sentences":["In recent years, quantum computing has emerged as a transformative force in the field of combinatorial optimization, offering novel approaches to tackling complex problems that have long challenged classical computational methods.","Among these, the Quantum Approximate Optimization Algorithm (QAOA) stands out for its potential to efficiently solve the Max-Cut problem, a quintessential example of combinatorial optimization.","However, practical application faces challenges due to current limitations on quantum computational resource.","Our work optimizes QAOA initialization, using Graph Neural Networks (GNN) as a warm-start technique.","This sacrifices affordable computational resource on classical computer to reduce quantum computational resource overhead, enhancing QAOA's effectiveness.","Experiments with various GNN architectures demonstrate the adaptability and stability of our framework, highlighting the synergy between quantum algorithms and machine learning.","Our findings show GNN's potential in improving QAOA performance, opening new avenues for hybrid quantum-classical approaches in quantum computing and contributing to practical applications."],"url":"http://arxiv.org/abs/2403.03310v1","category":"quant-ph"}
{"created":"2024-03-05 19:13:45","title":"DINOv2 based Self Supervised Learning For Few Shot Medical Image Segmentation","abstract":"Deep learning models have emerged as the cornerstone of medical image segmentation, but their efficacy hinges on the availability of extensive manually labeled datasets and their adaptability to unforeseen categories remains a challenge. Few-shot segmentation (FSS) offers a promising solution by endowing models with the capacity to learn novel classes from limited labeled examples. A leading method for FSS is ALPNet, which compares features between the query image and the few available support segmented images. A key question about using ALPNet is how to design its features. In this work, we delve into the potential of using features from DINOv2, which is a foundational self-supervised learning model in computer vision. Leveraging the strengths of ALPNet and harnessing the feature extraction capabilities of DINOv2, we present a novel approach to few-shot segmentation that not only enhances performance but also paves the way for more robust and adaptable medical image analysis.","sentences":["Deep learning models have emerged as the cornerstone of medical image segmentation, but their efficacy hinges on the availability of extensive manually labeled datasets and their adaptability to unforeseen categories remains a challenge.","Few-shot segmentation (FSS) offers a promising solution by endowing models with the capacity to learn novel classes from limited labeled examples.","A leading method for FSS is ALPNet, which compares features between the query image and the few available support segmented images.","A key question about using ALPNet is how to design its features.","In this work, we delve into the potential of using features from DINOv2, which is a foundational self-supervised learning model in computer vision.","Leveraging the strengths of ALPNet and harnessing the feature extraction capabilities of DINOv2, we present a novel approach to few-shot segmentation that not only enhances performance but also paves the way for more robust and adaptable medical image analysis."],"url":"http://arxiv.org/abs/2403.03273v1","category":"cs.CV"}
{"created":"2024-03-05 19:04:09","title":"High-z Gamma-Ray Bursts detection by SVOM/ECLAIRs: Impact of instrumental biases on the burst measured properties","abstract":"Context. Gamma Ray Bursts (GRBs) can be detected at cosmological distances and therefore can be used to study the contents and phases of the early Universe. The 4 -- 150 keV wide-field trigger camera ECLAIRs to fly on-board the Space-based multi-band Variable Object Monitor (SVOM) mission dedicated to study high-energy transient sky in synergy with multi-messenger follow-up instruments, is adapted to detect high-z GRBs. Aims. Investigating the detection capabilities of ECLAIRs for high redshift GRBs and estimating the impacts of instrumental biases in reconstructing some of the source measured properties, focusing on GRB duration biases as a function of redshift. Methods. We simulated realistic detection scenarios for a sample of 162 already observed GRBs with known redshift values as they would have been seen by ECLAIRs. We simulated them at redshift values equal and higher than their measured value. Then, we assessed whether they would be detected with a trigger algorithm resembling that on-board of ECLAIRs, and derived some quantities such as T90 for those that would have been detected. Results. We find that ECLAIRs would be capable of detecting GRBs up to very high redshift values (e.g. 20 GRBs of our sample are detectable within more than 0.4 of the ECLAIRs Field of View for z > 12). The ECLAIRs low-energy threshold of 4 keV, contributes to this great detection capability, as it may enhance it at high redshift (z > 10) by over 10% compared to a 15 keV low-energy threshold. We have also shown that the detection of GRBs at high-z values may imprint tip-of-the-iceberg biases on the GRB duration measurements, which can affect the reconstruction of other source properties.","sentences":["Context.","Gamma Ray Bursts (GRBs) can be detected at cosmological distances and therefore can be used to study the contents and phases of the early Universe.","The 4 -- 150 keV wide-field trigger camera ECLAIRs to fly on-board the Space-based multi-band Variable Object Monitor (SVOM) mission dedicated to study high-energy transient sky in synergy with multi-messenger follow-up instruments, is adapted to detect high-z GRBs.","Aims.","Investigating the detection capabilities of ECLAIRs for high redshift GRBs and estimating the impacts of instrumental biases in reconstructing some of the source measured properties, focusing on GRB duration biases as a function of redshift.","Methods.","We simulated realistic detection scenarios for a sample of 162 already observed GRBs with known redshift values as they would have been seen by ECLAIRs.","We simulated them at redshift values equal and higher than their measured value.","Then, we assessed whether they would be detected with a trigger algorithm resembling that on-board of ECLAIRs, and derived some quantities such as T90 for those that would have been detected.","Results.","We find that ECLAIRs would be capable of detecting GRBs up to very high redshift values (e.g. 20 GRBs of our sample are detectable within more than 0.4 of the ECLAIRs Field of View for z > 12).","The ECLAIRs low-energy threshold of 4 keV, contributes to this great detection capability, as it may enhance it at high redshift (z > 10) by over 10% compared to a 15 keV low-energy threshold.","We have also shown that the detection of GRBs at high-z values may imprint tip-of-the-iceberg biases on the GRB duration measurements, which can affect the reconstruction of other source properties."],"url":"http://arxiv.org/abs/2403.03266v1","category":"astro-ph.HE"}
{"created":"2024-03-05 19:00:05","title":"Dynamical tidal response of non-rotating relativistic stars","abstract":"Accurately modeling the tidal response of neutron stars is crucial to connecting gravitational wave observations of binaries to ultra-dense nuclear physics. Most current models of the tidal response of relativistic stars either assume a static response model, or use phenomenological models inspired by Newtonian gravity. In this work, we present a general formalism for computing the linear dynamical tidal response function of relativistic, spherically symmetric stars. Our formalism incorporates stratification due to thermal and chemical imbalances, allowing one to study the effects of g modes on the tidal response function. We also describe how to incorporate sources of dissipation due to shear and bulk viscosity. To showcase the utility of our approach, we present several applications for polytropic stars in general relativity. We show how our formalism can capture the dynamical tidal resonance due to the f and g modes of inviscid stars and explore the sensitivity of the dynamical tidal response to the compactness of the star. We also compute the dissipative tidal deformability due to bulk and shear viscous dissipation. We find that, for the same viscous profile, the shear viscous tidal lag parameter is 100-1000 times larger than the dimensionless bulk viscous tidal lag parameter.","sentences":["Accurately modeling the tidal response of neutron stars is crucial to connecting gravitational wave observations of binaries to ultra-dense nuclear physics.","Most current models of the tidal response of relativistic stars either assume a static response model, or use phenomenological models inspired by Newtonian gravity.","In this work, we present a general formalism for computing the linear dynamical tidal response function of relativistic, spherically symmetric stars.","Our formalism incorporates stratification due to thermal and chemical imbalances, allowing one to study the effects of g modes on the tidal response function.","We also describe how to incorporate sources of dissipation due to shear and bulk viscosity.","To showcase the utility of our approach, we present several applications for polytropic stars in general relativity.","We show how our formalism can capture the dynamical tidal resonance due to the f and g modes of inviscid stars and explore the sensitivity of the dynamical tidal response to the compactness of the star.","We also compute the dissipative tidal deformability due to bulk and shear viscous dissipation.","We find that, for the same viscous profile, the shear viscous tidal lag parameter is 100-1000 times larger than the dimensionless bulk viscous tidal lag parameter."],"url":"http://arxiv.org/abs/2403.03254v1","category":"gr-qc"}
{"created":"2024-03-06 18:34:24","title":"Relaxation of maximally entangled quantum states of two nonequivalent nuclear spins in a liquid","abstract":"We investigate both experimentally and theoretically the relaxation of pseudo-pure maximally entangled states (Bell states) of two nuclear spins 1H-13C belonging to a molecule in a liquid. The Bell states are obtained by a method based on a detuned Hartmann-Hahn cross-polarization condition. Their entangled character is verified by quantum-state tomography. Our relaxation measurements reveal different relaxation rates for different Bell states. We interpret this difference as originating from cross-correlations between different relaxation mechanisms, thereby demonstrating that the measurements of the differential relaxation of Bell states are potentially useful for advanced NMR characterization of liquids.","sentences":["We investigate both experimentally and theoretically the relaxation of pseudo-pure maximally entangled states (Bell states) of two nuclear spins 1H-13C belonging to a molecule in a liquid.","The Bell states are obtained by a method based on a detuned Hartmann-Hahn cross-polarization condition.","Their entangled character is verified by quantum-state tomography.","Our relaxation measurements reveal different relaxation rates for different Bell states.","We interpret this difference as originating from cross-correlations between different relaxation mechanisms, thereby demonstrating that the measurements of the differential relaxation of Bell states are potentially useful for advanced NMR characterization of liquids."],"url":"http://arxiv.org/abs/2403.03924v1","category":"quant-ph"}
{"created":"2024-03-06 17:40:26","title":"Graph neural network outputs are almost surely asymptotically constant","abstract":"Graph neural networks (GNNs) are the predominant architectures for a variety of learning tasks on graphs. We present a new angle on the expressive power of GNNs by studying how the predictions of a GNN probabilistic classifier evolve as we apply it on larger graphs drawn from some random graph model. We show that the output converges to a constant function, which upper-bounds what these classifiers can express uniformly. This convergence phenomenon applies to a very wide class of GNNs, including state of the art models, with aggregates including mean and the attention-based mechanism of graph transformers. Our results apply to a broad class of random graph models, including the (sparse) Erd\\H{o}s-R\\'enyi model and the stochastic block model. We empirically validate these findings, observing that the convergence phenomenon already manifests itself on graphs of relatively modest size.","sentences":["Graph neural networks (GNNs) are the predominant architectures for a variety of learning tasks on graphs.","We present a new angle on the expressive power of GNNs by studying how the predictions of a GNN probabilistic classifier evolve as we apply it on larger graphs drawn from some random graph model.","We show that the output converges to a constant function, which upper-bounds what these classifiers can express uniformly.","This convergence phenomenon applies to a very wide class of GNNs, including state of the art models, with aggregates including mean and the attention-based mechanism of graph transformers.","Our results apply to a broad class of random graph models, including the (sparse) Erd\\H{o}s-R\\'enyi model and the stochastic block model.","We empirically validate these findings, observing that the convergence phenomenon already manifests itself on graphs of relatively modest size."],"url":"http://arxiv.org/abs/2403.03880v1","category":"cs.LG"}
{"created":"2024-03-06 17:10:15","title":"ProxNF: Neural Field Proximal Training for High-Resolution 4D Dynamic Image Reconstruction","abstract":"Accurate spatiotemporal image reconstruction methods are needed for a wide range of biomedical research areas but face challenges due to data incompleteness and computational burden. Data incompleteness arises from the undersampling often required to increase frame rates and reduce acquisition times, while computational burden emerges due to the memory footprint of high-resolution images with three spatial dimensions and extended time horizons. Neural fields, an emerging class of neural networks that act as continuous representations of spatiotemporal objects, have previously been introduced to solve these dynamic imaging problems by reframing image reconstruction to a problem of estimating network parameters. Neural fields can address the twin challenges of data incompleteness and computational burden by exploiting underlying redundancies in these spatiotemporal objects. This work proposes ProxNF, a novel neural field training approach for spatiotemporal image reconstruction leveraging proximal splitting methods to separate computations involving the imaging operator from updates of the network parameter. Specifically, ProxNF evaluates the (subsampled) gradient of the data-fidelity term in the image domain and uses a fully supervised learning approach to update the neural field parameters. By reducing the memory footprint and the computational cost of evaluating the imaging operator, the proposed ProxNF approach allows for reconstructing large, high-resolution spatiotemporal images. This method is demonstrated in two numerical studies involving virtual dynamic contrast-enhanced photoacoustic computed tomography imaging of an anatomically realistic dynamic numerical mouse phantom and a two-compartment model of tumor perfusion.","sentences":["Accurate spatiotemporal image reconstruction methods are needed for a wide range of biomedical research areas but face challenges due to data incompleteness and computational burden.","Data incompleteness arises from the undersampling often required to increase frame rates and reduce acquisition times, while computational burden emerges due to the memory footprint of high-resolution images with three spatial dimensions and extended time horizons.","Neural fields, an emerging class of neural networks that act as continuous representations of spatiotemporal objects, have previously been introduced to solve these dynamic imaging problems by reframing image reconstruction to a problem of estimating network parameters.","Neural fields can address the twin challenges of data incompleteness and computational burden by exploiting underlying redundancies in these spatiotemporal objects.","This work proposes ProxNF, a novel neural field training approach for spatiotemporal image reconstruction leveraging proximal splitting methods to separate computations involving the imaging operator from updates of the network parameter.","Specifically, ProxNF evaluates the (subsampled) gradient of the data-fidelity term in the image domain and uses a fully supervised learning approach to update the neural field parameters.","By reducing the memory footprint and the computational cost of evaluating the imaging operator, the proposed ProxNF approach allows for reconstructing large, high-resolution spatiotemporal images.","This method is demonstrated in two numerical studies involving virtual dynamic contrast-enhanced photoacoustic computed tomography imaging of an anatomically realistic dynamic numerical mouse phantom and a two-compartment model of tumor perfusion."],"url":"http://arxiv.org/abs/2403.03860v1","category":"eess.IV"}
{"created":"2024-03-06 16:33:37","title":"Caloric functions and boundary regularity for the fractional Laplacian in Lipschitz open sets","abstract":"We give Martin representation of nonnegative functions caloric with respect to the fractional Laplacian in Lipschitz open sets. The caloric functions are defined in terms of the mean value property for the space-time isotropic $\\alpha$-stable L\\'evy process. To derive the representation, we first establish the existence of the parabolic Martin kernel. This involves proving new boundary regularity results for both the fractional heat equation and the fractional Poisson equation with Dirichlet exterior conditions. Specifically, we demonstrate that the ratio of the solution and the Green function is H\\\"older continuous up to the boundary.","sentences":["We give Martin representation of nonnegative functions caloric with respect to the fractional Laplacian in Lipschitz open sets.","The caloric functions are defined in terms of the mean value property for the space-time isotropic $\\alpha$-stable L\\'evy process.","To derive the representation, we first establish the existence of the parabolic Martin kernel.","This involves proving new boundary regularity results for both the fractional heat equation and the fractional Poisson equation with Dirichlet exterior conditions.","Specifically, we demonstrate that the ratio of the solution and the Green function is H\\\"older continuous up to the boundary."],"url":"http://arxiv.org/abs/2403.03840v1","category":"math.AP"}
{"created":"2024-03-06 16:23:27","title":"Second order Sobolev regularity for normalized parabolic $p(x)$-Laplace equations via the algebraic structure","abstract":"Denote by $\\Delta$ the Laplacian and by $\\Delta_\\infty$ the $\\infty$-Laplacian. A fundamental inequality is proved for the algebraic structure of $\\Delta v\\Delta_\\infty v$: for every $v\\in C^{\\infty}$, $$\\bigg| |D^2vDv|^2-\\Delta v\\Delta_\\infty v-\\frac{1}{2}[|D^2v|^2-(\\Delta v)^2]|Dv|^2\\bigg| \\le\\frac{n-2}{2}[|D^2v|^2|Dv|^2-|D^2vDv|^2]$$ Based on this, we prove the result: When $n\\ge2$ and $p(x)\\in(1,2)\\cup(2,3+\\frac{2}{n-2})$, the viscosity solutions to parabolic normalized $p(x)$-Laplace equation have the $W^{2,2}_{loc}$-regularity in the spatial variable and the $W^{1,2}_{loc}$-regularity in the time variable.","sentences":["Denote by $\\Delta$ the Laplacian and by $\\Delta_\\infty$ the $\\infty$-Laplacian.","A fundamental inequality is proved for the algebraic structure of $\\Delta v\\Delta_\\infty v$: for every $v\\in C^{\\infty}$, $$\\bigg| |D^2vDv|^2-\\Delta v\\Delta_\\infty v-\\frac{1}{2}[|D^2v|^2-(\\Delta v)^2]|Dv|^2\\bigg| \\le\\frac{n-2}{2}[|D^2v|^2|Dv|^2-|D^2vDv|^2]$$ Based on this, we prove the result: When $n\\ge2$ and $p(x)\\in(1,2)\\cup(2,3+\\frac{2}{n-2})$, the viscosity solutions to parabolic normalized $p(x)$-Laplace equation have the $W^{2,2}_{loc}$-regularity in the spatial variable and the $W^{1,2}_{loc}$-regularity in the time variable."],"url":"http://arxiv.org/abs/2403.03834v1","category":"math.AP"}
{"created":"2024-03-06 15:55:48","title":"Dynamic Scaling of Two-Dimensional Polar Flocks","abstract":"We propose a hydrodynamic description of the homogeneous ordered phase of polar flocks. Starting from symmetry principles, we construct the appropriate equation for the dynamics of the Goldstone mode associated with the broken rotational symmetry. We then focus on the two-dimensional case considering both \"Malthusian flocks\" for which the density field is a fast variable that does not enter the hydrodynamic description and \"Vicsek flocks\" for which it does. In both cases, we argue in favor of scaling relations that allow to compute exactly the scaling exponents, which are found in excellent agreement with previous simulations of the Vicsek model and with the numerical integration of our hydrodynamic equations.","sentences":["We propose a hydrodynamic description of the homogeneous ordered phase of polar flocks.","Starting from symmetry principles, we construct the appropriate equation for the dynamics of the Goldstone mode associated with the broken rotational symmetry.","We then focus on the two-dimensional case considering both \"Malthusian flocks\" for which the density field is a fast variable that does not enter the hydrodynamic description and \"Vicsek flocks\" for which it does.","In both cases, we argue in favor of scaling relations that allow to compute exactly the scaling exponents, which are found in excellent agreement with previous simulations of the Vicsek model and with the numerical integration of our hydrodynamic equations."],"url":"http://arxiv.org/abs/2403.03804v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-06 15:50:29","title":"Modeling thermocapillary microgear rotation and transfer to translational particle propulsion","abstract":"In this study, we investigate the thermocapillary rotation of microgears at fluid interfaces and extend the concept of geometric asymmetry to the translational propulsion of micron-sized particles. We introduce a transient numerical model that couples the Navier-Stokes equations with heat transfer, displaying particle motion through a moving mesh interface. The model incorporates absorbed light illumination as a heat source and predicts both rotational and translational speeds of particles. Our simulations explore the influence of microgear design geometry and determine the scale at which thermocapillary Marangoni motion could serve as a viable propulsion method. A clear correlation between Reynolds number and propulsion efficiency can be recognized. To transfer the asymmetry-based propulsion principle from rotational to directed translational motion, various particle geometries are considered. The exploration of breaking geometric symmetry for translational propulsion is mostly ignored in the existing literature, thus warranting further discussion. Therefore, we analyse expected translational speeds in comparison to corresponding microgears to provide insights into this promising propulsion method.","sentences":["In this study, we investigate the thermocapillary rotation of microgears at fluid interfaces and extend the concept of geometric asymmetry to the translational propulsion of micron-sized particles.","We introduce a transient numerical model that couples the Navier-Stokes equations with heat transfer, displaying particle motion through a moving mesh interface.","The model incorporates absorbed light illumination as a heat source and predicts both rotational and translational speeds of particles.","Our simulations explore the influence of microgear design geometry and determine the scale at which thermocapillary Marangoni motion could serve as a viable propulsion method.","A clear correlation between Reynolds number and propulsion efficiency can be recognized.","To transfer the asymmetry-based propulsion principle from rotational to directed translational motion, various particle geometries are considered.","The exploration of breaking geometric symmetry for translational propulsion is mostly ignored in the existing literature, thus warranting further discussion.","Therefore, we analyse expected translational speeds in comparison to corresponding microgears to provide insights into this promising propulsion method."],"url":"http://arxiv.org/abs/2403.03797v1","category":"physics.flu-dyn"}
{"created":"2024-03-06 15:47:22","title":"Complete NLO corrections to top-quark pair production with isolated photons","abstract":"We compute for the first time the so-called complete NLO corrections to top-quark pair production with one and two isolated photons in the di-lepton top-quark decay channel. The Narrow Width Approximation is used for the modeling of unstable top quarks and $W$ bosons. Higher-order QCD and EW effects as well as photon bremsstrahlung are consistently included at all stages: in production and top-quark decays. We present results at the integrated and differential fiducial cross-section level for both processes for the LHC Run II center-of-mass energy of $\\sqrt{s}=13$ TeV. In addition, we investigate the scale choice in photonic observables. Finally, the individual size of each subleading contribution is discussed in detail and the origin of the main subleading corrections is scrutinised. For the latter case, alternative calculations are performed in which the subleading NLO corrections are included only in the production of $t\\bar{t}\\gamma$ and $t\\bar{t}\\gamma\\gamma$.","sentences":["We compute for the first time the so-called complete NLO corrections to top-quark pair production with one and two isolated photons in the di-lepton top-quark decay channel.","The Narrow Width Approximation is used for the modeling of unstable top quarks and $W$ bosons.","Higher-order QCD and EW effects as well as photon bremsstrahlung are consistently included at all stages: in production and top-quark decays.","We present results at the integrated and differential fiducial cross-section level for both processes for the LHC Run II center-of-mass energy of $\\sqrt{s}=13$ TeV. In addition, we investigate the scale choice in photonic observables.","Finally, the individual size of each subleading contribution is discussed in detail and the origin of the main subleading corrections is scrutinised.","For the latter case, alternative calculations are performed in which the subleading NLO corrections are included only in the production of $t\\bar{t}\\gamma$ and $t\\bar{t}\\gamma\\gamma$."],"url":"http://arxiv.org/abs/2403.03796v1","category":"hep-ph"}
{"created":"2024-03-06 15:30:00","title":"A quantitative second order Sobolev regularity for (inhmogeneous) normalized $p(\\cdot)$-Laplace equations","abstract":"Let $\\Omega$ be a domain of $\\mathbb R^n$ with $n\\ge 2$ and $p(\\cdot)$ be a local Lipschitz funcion in $\\Omega$ with $1<p(x)<\\infty$ in $\\Omega$. We build up an interior quantitative second order Sobolev regularity for the normalized $p(\\cdot)$-Laplace equation $-\\Delta^N_{p(\\cdot)}u=0$ in $\\Omega$ as well as the corresponding inhomogeneous equation $-\\Delta^N_{p(\\cdot)}u=f$ in $\\Omega$ with $f\\in C^0(\\Omega)$.   In particular, given any viscosity solution $u$ to $-\\Delta^N_{p(\\cdot)}u=0$ in $\\Omega$, we prove the following:   (i) in dimension $n=2$, for any subdomain $U\\Subset\\Omega$ and any $\\beta\\ge 0$, one has $|Du|^\\beta Du\\in L^{2+\\delta}(U)$ locally with a quantitative upper bound, and moreover, the map $(x_1,x_2)\\to |Du|^\\beta(u_{x_1},-u_{x_2})$ is quasiregular in $U$ in the sense that   $$|D[|Du|^\\beta Du]|^2\\leq -C\\det D[|Du|^\\beta Du] \\quad \\mbox{a.e. in $U$}.$$   (ii) in dimension $n\\geq3$, for any subdomain $U\\Subset\\Omega$ with   $ \\inf_U p(x)>1$ and $\\sup_Up(x)<3+\\frac2{n-2}$, one has $D^2u\\in L^{2+\\delta}(U)$ locally with a quantitative upper bound, and also with a pointwise upper bound $$|D^2u|^2\\le -C\\sum_{1\\leq i<j\\le n}[u_{x_ix_j}u_{x_jx_i}-u_{x_ix_i}u_{x_jx_j}] \\quad \\mbox{a.e. in $U$}.$$   Here constants $\\delta>0$ and $C\\geq 1$ are independent of $u$. These extend the related results obtaind by Adamowicz-H\\\"ast\\\"o \\cite{AH2010} when $n=2$ and $\\beta=0$.","sentences":["Let $\\Omega$ be a domain of $\\mathbb R^n$ with $n\\ge 2$ and $p(\\cdot)$ be a local Lipschitz funcion in $\\Omega$ with $1<p(x)<\\infty$ in $\\Omega$. We build up an interior quantitative second order Sobolev regularity for the normalized $p(\\cdot)$-Laplace equation $-\\Delta^N_{p(\\cdot)}u=0$ in $\\Omega$ as well as the corresponding inhomogeneous equation $-\\Delta^N_{p(\\cdot)}u=f$ in $\\Omega$ with $f\\in C^0(\\Omega)$.   In particular, given any viscosity solution $u$ to $-\\Delta^N_{p(\\cdot)}u=0$ in $\\Omega$, we prove the following:   (i) in dimension $n=2$, for any subdomain $U\\Subset\\Omega$ and any $\\beta\\ge 0$, one has $|Du|^\\beta Du\\in L^{2+\\delta}(U)$ locally with a quantitative upper bound, and moreover, the map $(x_1,x_2)\\to |Du|^\\beta(u_{x_1},-u_{x_2})$ is quasiregular in $U$ in the sense that   $$|D[|Du|^\\beta Du]|^2\\leq -C\\det D[|Du|^\\beta","Du] \\quad \\mbox{a.e.","in $U$}.$$   (ii) in dimension $n\\geq3$, for any subdomain $U\\Subset\\Omega$ with   $ \\inf_U p(x)>1$ and $\\sup_Up(x)<3+\\frac2{n-2}$, one has $D^2u\\in L^{2+\\delta}(U)$ locally with a quantitative upper bound, and also with a pointwise upper bound $$|D^2u|^2\\le","-C\\sum_{1\\leq","i<j\\le n}[u_{x_ix_j}u_{x_jx_i}-u_{x_ix_i}u_{x_jx_j}]","\\quad \\mbox{a.e.","in $U$}.$$   Here constants $\\delta>0$ and $C\\geq 1$ are independent of $u$. These extend the related results obtaind by Adamowicz-H\\\"ast\\\"o \\cite{AH2010} when $n=2$ and $\\beta=0$."],"url":"http://arxiv.org/abs/2403.03784v1","category":"math.AP"}
{"created":"2024-03-06 14:27:29","title":"Probabilistic Topic Modelling with Transformer Representations","abstract":"Topic modelling was mostly dominated by Bayesian graphical models during the last decade. With the rise of transformers in Natural Language Processing, however, several successful models that rely on straightforward clustering approaches in transformer-based embedding spaces have emerged and consolidated the notion of topics as clusters of embedding vectors. We propose the Transformer-Representation Neural Topic Model (TNTM), which combines the benefits of topic representations in transformer-based embedding spaces and probabilistic modelling. Therefore, this approach unifies the powerful and versatile notion of topics based on transformer embeddings with fully probabilistic modelling, as in models such as Latent Dirichlet Allocation (LDA). We utilize the variational autoencoder (VAE) framework for improved inference speed and modelling flexibility. Experimental results show that our proposed model achieves results on par with various state-of-the-art approaches in terms of embedding coherence while maintaining almost perfect topic diversity. The corresponding source code is available at https://github.com/ArikReuter/TNTM.","sentences":["Topic modelling was mostly dominated by Bayesian graphical models during the last decade.","With the rise of transformers in Natural Language Processing, however, several successful models that rely on straightforward clustering approaches in transformer-based embedding spaces have emerged and consolidated the notion of topics as clusters of embedding vectors.","We propose the Transformer-Representation Neural Topic Model (TNTM), which combines the benefits of topic representations in transformer-based embedding spaces and probabilistic modelling.","Therefore, this approach unifies the powerful and versatile notion of topics based on transformer embeddings with fully probabilistic modelling, as in models such as Latent Dirichlet Allocation (LDA).","We utilize the variational autoencoder (VAE) framework for improved inference speed and modelling flexibility.","Experimental results show that our proposed model achieves results on par with various state-of-the-art approaches in terms of embedding coherence while maintaining almost perfect topic diversity.","The corresponding source code is available at https://github.com/ArikReuter/TNTM."],"url":"http://arxiv.org/abs/2403.03737v1","category":"cs.LG"}
{"created":"2024-03-06 14:18:52","title":"Case studies on time-dependent Ginzburg-Landau simulations for superconducting applications","abstract":"The macroscopic electromagnetic properties of type II superconductors are primarily influenced by the behavior of microscopic superconducting flux quantum units. Time-dependent Ginzburg-Landau (TDGL) equations provide an elegant and powerful tool for describing and examining both the statics and dynamics of these superconducting entities. They have been instrumental in replicating and elucidating numerous experimental results over the past decades.This paper provides a comprehensive overview of the progress in TDGL simulations, focusing on three key aspects of superconductor applications. The initial section delves into vortex rectification in superconductors described within the TDGL framework. We specifically highlight the superconducting diode effect achieved through asymmetric pinning landscapes and the reversible manipulation of vortex ratchets with dynamic pinning landscapes. The subsequent section reviews the achievements of TDGL simulations concerning the critical current density of superconductors, emphasizing the optimization of pinning sites, particularly vortex pinning and dynamics in polycrystalline Nb$_3$Sn with grain boundaries. The third part concentrates on numerical modeling of vortex penetration and dynamics in superconducting radio frequency (SRF) cavities, including a discussion of superconductor insulator superconductor multilayer structures. In the last section, we present key findings, insights, and perspectives derived from the discussed simulations.","sentences":["The macroscopic electromagnetic properties of type II superconductors are primarily influenced by the behavior of microscopic superconducting flux quantum units.","Time-dependent Ginzburg-Landau (TDGL) equations provide an elegant and powerful tool for describing and examining both the statics and dynamics of these superconducting entities.","They have been instrumental in replicating and elucidating numerous experimental results over the past decades.","This paper provides a comprehensive overview of the progress in TDGL simulations, focusing on three key aspects of superconductor applications.","The initial section delves into vortex rectification in superconductors described within the TDGL framework.","We specifically highlight the superconducting diode effect achieved through asymmetric pinning landscapes and the reversible manipulation of vortex ratchets with dynamic pinning landscapes.","The subsequent section reviews the achievements of TDGL simulations concerning the critical current density of superconductors, emphasizing the optimization of pinning sites, particularly vortex pinning and dynamics in polycrystalline Nb$_3$Sn with grain boundaries.","The third part concentrates on numerical modeling of vortex penetration and dynamics in superconducting radio frequency (SRF) cavities, including a discussion of superconductor insulator superconductor multilayer structures.","In the last section, we present key findings, insights, and perspectives derived from the discussed simulations."],"url":"http://arxiv.org/abs/2403.03729v1","category":"cond-mat.supr-con"}
{"created":"2024-03-06 12:37:49","title":"Provable Filter for Real-world Graph Clustering","abstract":"Graph clustering, an important unsupervised problem, has been shown to be more resistant to advances in Graph Neural Networks (GNNs). In addition, almost all clustering methods focus on homophilic graphs and ignore heterophily. This significantly limits their applicability in practice, since real-world graphs exhibit a structural disparity and cannot simply be classified as homophily and heterophily. Thus, a principled way to handle practical graphs is urgently needed. To fill this gap, we provide a novel solution with theoretical support. Interestingly, we find that most homophilic and heterophilic edges can be correctly identified on the basis of neighbor information. Motivated by this finding, we construct two graphs that are highly homophilic and heterophilic, respectively. They are used to build low-pass and high-pass filters to capture holistic information. Important features are further enhanced by the squeeze-and-excitation block. We validate our approach through extensive experiments on both homophilic and heterophilic graphs. Empirical results demonstrate the superiority of our method compared to state-of-the-art clustering methods.","sentences":["Graph clustering, an important unsupervised problem, has been shown to be more resistant to advances in Graph Neural Networks (GNNs).","In addition, almost all clustering methods focus on homophilic graphs and ignore heterophily.","This significantly limits their applicability in practice, since real-world graphs exhibit a structural disparity and cannot simply be classified as homophily and heterophily.","Thus, a principled way to handle practical graphs is urgently needed.","To fill this gap, we provide a novel solution with theoretical support.","Interestingly, we find that most homophilic and heterophilic edges can be correctly identified on the basis of neighbor information.","Motivated by this finding, we construct two graphs that are highly homophilic and heterophilic, respectively.","They are used to build low-pass and high-pass filters to capture holistic information.","Important features are further enhanced by the squeeze-and-excitation block.","We validate our approach through extensive experiments on both homophilic and heterophilic graphs.","Empirical results demonstrate the superiority of our method compared to state-of-the-art clustering methods."],"url":"http://arxiv.org/abs/2403.03666v1","category":"cs.LG"}
{"created":"2024-03-06 11:51:57","title":"Stochastic partial differential equations for superprocesses in random environments","abstract":"Let $X=(X_t, t\\geq 0)$ be a superprocess in a random environment described by a Gaussian noise $W^g=\\{W^g(t,x), t\\geq 0, x\\in \\mathbb{R}^d\\}$ white in time and colored in space with correlation kernel $g(x,y)$. We show that when $d=1$, $X_t$ admits a jointly continuous density function $X_t(x)$ that is a unique in law solution to a stochastic partial differential equation   \\begin{align*} \\frac{\\partial }{\\partial t}X_t(x)=\\frac{\\Delta}{2} X_t(x)+\\sqrt{X_t(x)} \\dot{V}(t,x)+X_t(x)\\dot{W}^g(t, x) , \\quad X_t(x)\\geq 0, \\end{align*} where $V=\\{V(t,x), t\\geq 0, x\\in \\mathbb{R}\\}$ is a space-time white noise and is orthogonal with $W^g$. When $d\\geq 2$, we prove that $X_t$ is singular and hence density does not exist.","sentences":["Let $X=(X_t, t\\geq 0)$ be a superprocess in a random environment described by a Gaussian noise $W^g=\\{W^g(t,x), t\\geq 0, x\\in \\mathbb{R}^d\\}$ white in time and colored in space with correlation kernel $g(x,y)$. We show that when $d=1$, $X_t$ admits a jointly continuous density function $X_t(x)$ that is a unique in law solution to a stochastic partial differential equation   \\begin{align*} \\frac{\\partial }{\\partial t}X_t(x)=\\frac{\\Delta}{2} X_t(x)+\\sqrt{X_t(x)} \\dot{V}(t,x)+X_t(x)\\dot{W}^g(t, x) , \\quad X_t(x)\\geq 0, \\end{align*} where $V=\\{V(t,x), t\\geq 0, x\\in \\mathbb{R}\\}$ is a space-time white noise and is orthogonal with $W^g$.","When $d\\geq 2$, we prove that $X_t$ is singular and hence density does not exist."],"url":"http://arxiv.org/abs/2403.03638v1","category":"math.PR"}
{"created":"2024-03-06 11:02:07","title":"Comparison Performance of Spectrogram and Scalogram as Input of Acoustic Recognition Task","abstract":"Acoustic recognition is a common task for deep learning in recent researches, with the employment of spectral feature extraction such as Short-time Fourier transform and Wavelet transform. However, not many researches have found that discuss the advantages and drawbacks, as well as performance comparison amongst spectral feature extractors. In this consideration, this paper aims to comparing the attributes of these two transform types, called spectrogram and scalogram. A Convolutional Neural Networks for acoustic faults recognition is implemented, then the performance of these two types of spectral extractor is recorded for comparison. A latest research on the same audio database is considered for benchmarking to see how good the designed spectrogram and scalogram is. The advantages and limitations of them are also analyzed. By doing so, the results of this paper provide indications for application scenarios of spectrogram and scalogram, as well as potential further research directions in acoustic recognition.","sentences":["Acoustic recognition is a common task for deep learning in recent researches, with the employment of spectral feature extraction such as Short-time Fourier transform and Wavelet transform.","However, not many researches have found that discuss the advantages and drawbacks, as well as performance comparison amongst spectral feature extractors.","In this consideration, this paper aims to comparing the attributes of these two transform types, called spectrogram and scalogram.","A Convolutional Neural Networks for acoustic faults recognition is implemented, then the performance of these two types of spectral extractor is recorded for comparison.","A latest research on the same audio database is considered for benchmarking to see how good the designed spectrogram and scalogram is.","The advantages and limitations of them are also analyzed.","By doing so, the results of this paper provide indications for application scenarios of spectrogram and scalogram, as well as potential further research directions in acoustic recognition."],"url":"http://arxiv.org/abs/2403.03611v1","category":"eess.AS"}
{"created":"2024-03-06 10:24:47","title":"DeepEclipse: How to Break White-Box DNN-Watermarking Schemes","abstract":"Deep Learning (DL) models have become crucial in digital transformation, thus raising concerns about their intellectual property rights. Different watermarking techniques have been developed to protect Deep Neural Networks (DNNs) from IP infringement, creating a competitive field for DNN watermarking and removal methods. The predominant watermarking schemes use white-box techniques, which involve modifying weights by adding a unique signature to specific DNN layers. On the other hand, existing attacks on white-box watermarking usually require knowledge of the specific deployed watermarking scheme or access to the underlying data for further training and fine-tuning. We propose DeepEclipse, a novel and unified framework designed to remove white-box watermarks. We present obfuscation techniques that significantly differ from the existing white-box watermarking removal schemes. DeepEclipse can evade watermark detection without prior knowledge of the underlying watermarking scheme, additional data, or training and fine-tuning. Our evaluation reveals that DeepEclipse excels in breaking multiple white-box watermarking schemes, reducing watermark detection to random guessing while maintaining a similar model accuracy as the original one. Our framework showcases a promising solution to address the ongoing DNN watermark protection and removal challenges.","sentences":["Deep Learning (DL) models have become crucial in digital transformation, thus raising concerns about their intellectual property rights.","Different watermarking techniques have been developed to protect Deep Neural Networks (DNNs) from IP infringement, creating a competitive field for DNN watermarking and removal methods.","The predominant watermarking schemes use white-box techniques, which involve modifying weights by adding a unique signature to specific DNN layers.","On the other hand, existing attacks on white-box watermarking usually require knowledge of the specific deployed watermarking scheme or access to the underlying data for further training and fine-tuning.","We propose DeepEclipse, a novel and unified framework designed to remove white-box watermarks.","We present obfuscation techniques that significantly differ from the existing white-box watermarking removal schemes.","DeepEclipse can evade watermark detection without prior knowledge of the underlying watermarking scheme, additional data, or training and fine-tuning.","Our evaluation reveals that DeepEclipse excels in breaking multiple white-box watermarking schemes, reducing watermark detection to random guessing while maintaining a similar model accuracy as the original one.","Our framework showcases a promising solution to address the ongoing DNN watermark protection and removal challenges."],"url":"http://arxiv.org/abs/2403.03590v1","category":"cs.CR"}
{"created":"2024-03-06 09:23:08","title":"Neutron radius determination of 133Cs and constraint on the weak mixing angle","abstract":"Proton-$^{133}$Cs elastic scattering at low momentum transfer is performed using an in-ring reaction technique at the Cooler Storage Ring at the Heavy Ion Research Facility in Lanzhou. Recoil protons from the elastic collisions between the internal H$_2$-gas target and the circulating $^{133}$Cs ions at 199.4 MeV/u are detected by a silicon-strip detector. The matter radius of $^{133}$Cs is deduced by describing the measured differential cross sections using the Glauber model. Employing the adopted proton distribution radius, a point-neutron radius of 4.86(21) fm for $^{133}$Cs is obtained. With the newly determined neutron radius, the weak mixing angle sin$^2 \\theta_W$ is independently extracted to be 0.227(28) by fitting the coherent elastic neutrino-nucleus scattering data. This work limits the sin$^2 \\theta_W$ value in a range smaller than the ones proposed by the previous independent approaches.","sentences":["Proton-$^{133}$Cs elastic scattering at low momentum transfer is performed using an in-ring reaction technique at the Cooler Storage Ring at the Heavy Ion Research Facility in Lanzhou.","Recoil protons from the elastic collisions between the internal H$_2$-gas target and the circulating $^{133}$Cs ions at 199.4 MeV/u are detected by a silicon-strip detector.","The matter radius of $^{133}$Cs is deduced by describing the measured differential cross sections using the Glauber model.","Employing the adopted proton distribution radius, a point-neutron radius of 4.86(21) fm for $^{133}$Cs is obtained.","With the newly determined neutron radius, the weak mixing angle sin$^2 \\theta_W$ is independently extracted to be 0.227(28) by fitting the coherent elastic neutrino-nucleus scattering data.","This work limits the sin$^2 \\theta_W$ value in a range smaller than the ones proposed by the previous independent approaches."],"url":"http://arxiv.org/abs/2403.03566v1","category":"nucl-ex"}
{"created":"2024-03-06 08:06:56","title":"Improved theoretical prediction of nanoparticle sizes with the resistive-pulse technique","abstract":"With the resistive-pulse technique (RPT), nanopores serve as the nanofluidic sensors of various analytes for their many physical and chemical properties. Here, we focus on the size measurement and its theoretical prediction for sub-200 nm nanoparticles with RPT. Through systematical investigation of the current blockade of nanoparticles across cylindrical nanopores with simulations, Maxwell method considering the shape coefficient and access resistances agrees well with simulation results. However, the widely used integration method of the resistance has distinct deviations in various cases. With the introduction of a correction factor \\b{eta} to the integration method, our revised equations can provide good predictions for simulation results. \\b{eta} shows a strong dependence on the diameter ratio (d over D) of the nanoparticle and nanopore. Following the same strategy, modified equations are provided for the accurate size prediction for nanoparticles across conical nanopores, where the integration method is the default convenient way. The correction factor \\b{eta}' relates to \\b{eta} in cylindrical nanopores. \\b{eta}' exhibits independence on the pore geometry parameters and diameters of nanoparticles, but dependence on the surface charge density of conical nanopores. Our improved equations can provide theoretical predictions for the accurate size detection of 100-200 nm diameter nanoparticles across cylindrical and conical nanopores.","sentences":["With the resistive-pulse technique (RPT), nanopores serve as the nanofluidic sensors of various analytes for their many physical and chemical properties.","Here, we focus on the size measurement and its theoretical prediction for sub-200 nm nanoparticles with RPT.","Through systematical investigation of the current blockade of nanoparticles across cylindrical nanopores with simulations, Maxwell method considering the shape coefficient and access resistances agrees well with simulation results.","However, the widely used integration method of the resistance has distinct deviations in various cases.","With the introduction of a correction factor \\b{eta} to the integration method, our revised equations can provide good predictions for simulation results.","\\b{eta} shows a strong dependence on the diameter ratio (d over D) of the nanoparticle and nanopore.","Following the same strategy, modified equations are provided for the accurate size prediction for nanoparticles across conical nanopores, where the integration method is the default convenient way.","The correction factor \\b{eta}' relates to \\b{eta} in cylindrical nanopores.","\\b{eta}' exhibits independence on the pore geometry parameters and diameters of nanoparticles, but dependence on the surface charge density of conical nanopores.","Our improved equations can provide theoretical predictions for the accurate size detection of 100-200 nm diameter nanoparticles across cylindrical and conical nanopores."],"url":"http://arxiv.org/abs/2403.03528v1","category":"physics.chem-ph"}
{"created":"2024-03-06 07:34:47","title":"Probing the Robustness of Time-series Forecasting Models with CounterfacTS","abstract":"A common issue for machine learning models applied to time-series forecasting is the temporal evolution of the data distributions (i.e., concept drift). Because most of the training data does not reflect such changes, the models present poor performance on the new out-of-distribution scenarios and, therefore, the impact of such events cannot be reliably anticipated ahead of time. We present and publicly release CounterfacTS, a tool to probe the robustness of deep learning models in time-series forecasting tasks via counterfactuals. CounterfacTS has a user-friendly interface that allows the user to visualize, compare and quantify time series data and their forecasts, for a number of datasets and deep learning models. Furthermore, the user can apply various transformations to the time series and explore the resulting changes in the forecasts in an interpretable manner. Through example cases, we illustrate how CounterfacTS can be used to i) identify the main features characterizing and differentiating sets of time series, ii) assess how the model performance depends on these characateristics, and iii) guide transformations of the original time series to create counterfactuals with desired properties for training and increasing the forecasting performance in new regions of the data distribution. We discuss the importance of visualizing and considering the location of the data in a projected feature space to transform time-series and create effective counterfactuals for training the models. Overall, CounterfacTS aids at creating counterfactuals to efficiently explore the impact of hypothetical scenarios not covered by the original data in time-series forecasting tasks.","sentences":["A common issue for machine learning models applied to time-series forecasting is the temporal evolution of the data distributions (i.e., concept drift).","Because most of the training data does not reflect such changes, the models present poor performance on the new out-of-distribution scenarios and, therefore, the impact of such events cannot be reliably anticipated ahead of time.","We present and publicly release CounterfacTS, a tool to probe the robustness of deep learning models in time-series forecasting tasks via counterfactuals.","CounterfacTS has a user-friendly interface that allows the user to visualize, compare and quantify time series data and their forecasts, for a number of datasets and deep learning models.","Furthermore, the user can apply various transformations to the time series and explore the resulting changes in the forecasts in an interpretable manner.","Through example cases, we illustrate how CounterfacTS can be used to i) identify the main features characterizing and differentiating sets of time series, ii) assess how the model performance depends on these characateristics, and iii) guide transformations of the original time series to create counterfactuals with desired properties for training and increasing the forecasting performance in new regions of the data distribution.","We discuss the importance of visualizing and considering the location of the data in a projected feature space to transform time-series and create effective counterfactuals for training the models.","Overall, CounterfacTS aids at creating counterfactuals to efficiently explore the impact of hypothetical scenarios not covered by the original data in time-series forecasting tasks."],"url":"http://arxiv.org/abs/2403.03508v1","category":"cs.LG"}
{"created":"2024-03-06 06:40:16","title":"Aharonov-Bohm effect mediated by massive photons","abstract":"Virtual photons play an essential role in the locally realistic description of the Aharonov-Bohm interference. We show that the effect of virtual photons in the interferometer is manifested by a change in their spectrum. In particular, when a vacuum is confined between two ideal conducting plates, the photons obey the two-dimensional Proca equation, the wave equation with finite effective mass. This results in a short-range interaction between a test charge and a magnetic flux, and hence the Aharonov-Bohm effect is reduced exponentially at a large distance between the two bodies. On the other hand, a semiclassical description is also possible, and this raises the interesting question of how to prove the physical reality of virtual photons.","sentences":["Virtual photons play an essential role in the locally realistic description of the Aharonov-Bohm interference.","We show that the effect of virtual photons in the interferometer is manifested by a change in their spectrum.","In particular, when a vacuum is confined between two ideal conducting plates, the photons obey the two-dimensional Proca equation, the wave equation with finite effective mass.","This results in a short-range interaction between a test charge and a magnetic flux, and hence the Aharonov-Bohm effect is reduced exponentially at a large distance between the two bodies.","On the other hand, a semiclassical description is also possible, and this raises the interesting question of how to prove the physical reality of virtual photons."],"url":"http://arxiv.org/abs/2403.03495v1","category":"quant-ph"}
{"created":"2024-03-06 05:52:13","title":"A Teacher-Free Graph Knowledge Distillation Framework with Dual Self-Distillation","abstract":"Recent years have witnessed great success in handling graph-related tasks with Graph Neural Networks (GNNs). Despite their great academic success, Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical industrial applications. One reason for such an academic-industry gap is the neighborhood-fetching latency incurred by data dependency in GNNs. To reduce their gaps, Graph Knowledge Distillation (GKD) is proposed, usually based on a standard teacher-student architecture, to distill knowledge from a large teacher GNN into a lightweight student GNN or MLP. However, we found in this paper that neither teachers nor GNNs are necessary for graph knowledge distillation. We propose a Teacher-Free Graph Self-Distillation (TGS) framework that does not require any teacher model or GNNs during both training and inference. More importantly, the proposed TGS framework is purely based on MLPs, where structural information is only implicitly used to guide dual knowledge self-distillation between the target node and its neighborhood. As a result, TGS enjoys the benefits of graph topology awareness in training but is free from data dependency in inference. Extensive experiments have shown that the performance of vanilla MLPs can be greatly improved with dual self-distillation, e.g., TGS improves over vanilla MLPs by 15.54% on average and outperforms state-of-the-art GKD algorithms on six real-world datasets. In terms of inference speed, TGS infers 75X-89X faster than existing GNNs and 16X-25X faster than classical inference acceleration methods.","sentences":["Recent years have witnessed great success in handling graph-related tasks with Graph Neural Networks (GNNs).","Despite their great academic success, Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical industrial applications.","One reason for such an academic-industry gap is the neighborhood-fetching latency incurred by data dependency in GNNs.","To reduce their gaps, Graph Knowledge Distillation (GKD) is proposed, usually based on a standard teacher-student architecture, to distill knowledge from a large teacher GNN into a lightweight student GNN or MLP.","However, we found in this paper that neither teachers nor GNNs are necessary for graph knowledge distillation.","We propose a Teacher-Free Graph Self-Distillation (TGS) framework that does not require any teacher model or GNNs during both training and inference.","More importantly, the proposed TGS framework is purely based on MLPs, where structural information is only implicitly used to guide dual knowledge self-distillation between the target node and its neighborhood.","As a result, TGS enjoys the benefits of graph topology awareness in training but is free from data dependency in inference.","Extensive experiments have shown that the performance of vanilla MLPs can be greatly improved with dual self-distillation, e.g., TGS improves over vanilla MLPs by 15.54% on average and outperforms state-of-the-art GKD algorithms on six real-world datasets.","In terms of inference speed, TGS infers 75X-89X faster than existing GNNs and 16X-25X faster than classical inference acceleration methods."],"url":"http://arxiv.org/abs/2403.03483v1","category":"cs.LG"}
{"created":"2024-03-06 04:49:18","title":"TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs","abstract":"We introduce the Transformed Generative Pre-Trained Physics-Informed Neural Networks (TGPT-PINN) for accomplishing nonlinear model order reduction (MOR) of transport-dominated partial differential equations in an MOR-integrating PINNs framework. Building on the recent development of the GPT-PINN that is a network-of-networks design achieving snapshot-based model reduction, we design and test a novel paradigm for nonlinear model reduction that can effectively tackle problems with parameter-dependent discontinuities. Through incorporation of a shock-capturing loss function component as well as a parameter-dependent transform layer, the TGPT-PINN overcomes the limitations of linear model reduction in the transport-dominated regime. We demonstrate this new capability for nonlinear model reduction in the PINNs framework by several nontrivial parametric partial differential equations.","sentences":["We introduce the Transformed Generative Pre-Trained Physics-Informed Neural Networks (TGPT-PINN) for accomplishing nonlinear model order reduction (MOR) of transport-dominated partial differential equations in an MOR-integrating PINNs framework.","Building on the recent development of the GPT-PINN that is a network-of-networks design achieving snapshot-based model reduction, we design and test a novel paradigm for nonlinear model reduction that can effectively tackle problems with parameter-dependent discontinuities.","Through incorporation of a shock-capturing loss function component as well as a parameter-dependent transform layer, the TGPT-PINN overcomes the limitations of linear model reduction in the transport-dominated regime.","We demonstrate this new capability for nonlinear model reduction in the PINNs framework by several nontrivial parametric partial differential equations."],"url":"http://arxiv.org/abs/2403.03459v1","category":"math.NA"}
{"created":"2024-03-06 03:57:18","title":"A component-splitting implicit time integration for multicomponent reacting flows simulations","abstract":"A component-splitting method is proposed to improve convergence characteristics for implicit time integration of compressible multicomponent reactive flows. The characteristic decomposition of flux jacobian of multicomponent Navier-Stokes equations yields a large sparse eigensystem, presenting challenges of slow convergence and high computational costs for implicit methods. To addresses this issue, the component-splitting method segregates the implicit operator into two parts: one for the flow equations (density/momentum/energy) and the other for the component equations. Each part's implicit operator employs flux-vector splitting based on their respective spectral radii to achieve accelerated convergence. This approach improves the computational efficiency of implicit iteration, mitigating the quadratic increase in time cost with the number of species. Two consistence corrections are developed to reduce the introduced component-splitting error and ensure the numerical consistency of mass fraction. Importantly, the impact of component-splitting method on accuracy is minimal as the residual approaches convergence. The accuracy, efficiency, and robustness of component-splitting method are thoroughly investigated and compared with the coupled implicit scheme through several numerical cases involving thermo-chemical nonequilibrium hypersonic flows. The results demonstrate that the component-splitting method decreases the required number of iteration steps for convergence of residual and wall heat flux, decreases the computation time per iteration step, and diminishes the residual to lower magnitude. The acceleration efficiency is enhanced with increases in CFL number and number of species.","sentences":["A component-splitting method is proposed to improve convergence characteristics for implicit time integration of compressible multicomponent reactive flows.","The characteristic decomposition of flux jacobian of multicomponent Navier-Stokes equations yields a large sparse eigensystem, presenting challenges of slow convergence and high computational costs for implicit methods.","To addresses this issue, the component-splitting method segregates the implicit operator into two parts: one for the flow equations (density/momentum/energy) and the other for the component equations.","Each part's implicit operator employs flux-vector splitting based on their respective spectral radii to achieve accelerated convergence.","This approach improves the computational efficiency of implicit iteration, mitigating the quadratic increase in time cost with the number of species.","Two consistence corrections are developed to reduce the introduced component-splitting error and ensure the numerical consistency of mass fraction.","Importantly, the impact of component-splitting method on accuracy is minimal as the residual approaches convergence.","The accuracy, efficiency, and robustness of component-splitting method are thoroughly investigated and compared with the coupled implicit scheme through several numerical cases involving thermo-chemical nonequilibrium hypersonic flows.","The results demonstrate that the component-splitting method decreases the required number of iteration steps for convergence of residual and wall heat flux, decreases the computation time per iteration step, and diminishes the residual to lower magnitude.","The acceleration efficiency is enhanced with increases in CFL number and number of species."],"url":"http://arxiv.org/abs/2403.03440v1","category":"math.NA"}
{"created":"2024-03-06 03:11:24","title":"A class of polynomial recurrences resulting in $(n/\\log n, n/\\log^2n)$-asymptotic normality","abstract":"We consider sequences of polynomials that satisfy differential-difference recurrences. Polynomials satisfying such recurrences frequently appear as generating polynomials of integer valued random variables that are of interest in discrete mathematics. It is, therefore, of interest to understand the properties of such polynomials and their probabilistic consequences. We identify a class of polynomial recurrences that lead to a normal law with the expected value and the variance proportional to $n/\\log n$ and $n/\\log^2n$, respectively. Examples include Stirling number of the second kind and other polynomials concerning set partitions as well as polynomials related to Whitney numbers of Dowling lattices.","sentences":["We consider sequences of polynomials that satisfy differential-difference recurrences.","Polynomials satisfying such recurrences frequently appear as generating polynomials of integer valued random variables that are of interest in discrete mathematics.","It is, therefore, of interest to understand the properties of such polynomials and their probabilistic consequences.","We identify a class of polynomial recurrences that lead to a normal law with the expected value and the variance proportional to $n/\\log n$ and $n/\\log^2n$, respectively.","Examples include Stirling number of the second kind and other polynomials concerning set partitions as well as polynomials related to Whitney numbers of Dowling lattices."],"url":"http://arxiv.org/abs/2403.03422v1","category":"math.CO"}
{"created":"2024-03-06 01:38:42","title":"Explaining Genetic Programming Trees using Large Language Models","abstract":"Genetic programming (GP) has the potential to generate explainable results, especially when used for dimensionality reduction. In this research, we investigate the potential of leveraging eXplainable AI (XAI) and large language models (LLMs) like ChatGPT to improve the interpretability of GP-based non-linear dimensionality reduction. Our study introduces a novel XAI dashboard named GP4NLDR, the first approach to combine state-of-the-art GP with an LLM-powered chatbot to provide comprehensive, user-centred explanations. We showcase the system's ability to provide intuitive and insightful narratives on high-dimensional data reduction processes through case studies. Our study highlights the importance of prompt engineering in eliciting accurate and pertinent responses from LLMs. We also address important considerations around data privacy, hallucinatory outputs, and the rapid advancements in generative AI. Our findings demonstrate its potential in advancing the explainability of GP algorithms. This opens the door for future research into explaining GP models with LLMs.","sentences":["Genetic programming (GP) has the potential to generate explainable results, especially when used for dimensionality reduction.","In this research, we investigate the potential of leveraging eXplainable AI (XAI) and large language models (LLMs) like ChatGPT to improve the interpretability of GP-based non-linear dimensionality reduction.","Our study introduces a novel XAI dashboard named GP4NLDR, the first approach to combine state-of-the-art GP with an LLM-powered chatbot to provide comprehensive, user-centred explanations.","We showcase the system's ability to provide intuitive and insightful narratives on high-dimensional data reduction processes through case studies.","Our study highlights the importance of prompt engineering in eliciting accurate and pertinent responses from LLMs.","We also address important considerations around data privacy, hallucinatory outputs, and the rapid advancements in generative AI.","Our findings demonstrate its potential in advancing the explainability of GP algorithms.","This opens the door for future research into explaining GP models with LLMs."],"url":"http://arxiv.org/abs/2403.03397v1","category":"cs.NE"}
{"created":"2024-03-05 22:43:53","title":"Bergman spaces for the bicomplex Vekua equation with bounded coefficients","abstract":"We develop the theory for the Bergman spaces of generalized $L_p$-solutions of the bicomplex-Vekua equation $\\overline{\\boldsymbol{\\partial}}W=aW+b\\overline{W}$ on bounded domains, where the coefficients $a$ and $b$ are bounded bicomplex-valued functions. We study the completeness of the Bergman space, the regularity of the solutions, and the boundedness of the evaluation functional. For the case $p=2$, the existence of a reproducing kernel is established, along with a representation of the orthogonal projection onto the Bergman space in terms of the obtained reproducing kernel, and an explicit expression for the orthogonal complement. Additionally, we analyze the main Vekua equation ($a=0$, $b = \\frac{\\overline{\\boldsymbol{\\partial}}f}{f}$ with $f$ being a non-vanishing complex-valued function). Results concerning its relationship with a pair of conductivity equations, the construction of metaharmonic conjugates, and the Runge property are presented.","sentences":["We develop the theory for the Bergman spaces of generalized $L_p$-solutions of the bicomplex-Vekua equation $\\overline{\\boldsymbol{\\partial}}W=aW+b\\overline{W}$ on bounded domains, where the coefficients $a$ and $b$ are bounded bicomplex-valued functions.","We study the completeness of the Bergman space, the regularity of the solutions, and the boundedness of the evaluation functional.","For the case $p=2$, the existence of a reproducing kernel is established, along with a representation of the orthogonal projection onto the Bergman space in terms of the obtained reproducing kernel, and an explicit expression for the orthogonal complement.","Additionally, we analyze the main Vekua equation ($a=0$, $b = \\frac{\\overline{\\boldsymbol{\\partial}}f}{f}$ with $f$ being a non-vanishing complex-valued function).","Results concerning its relationship with a pair of conductivity equations, the construction of metaharmonic conjugates, and the Runge property are presented."],"url":"http://arxiv.org/abs/2403.03354v1","category":"math.AP"}
{"created":"2024-03-05 22:42:29","title":"Hypothesis Spaces for Deep Learning","abstract":"This paper introduces a hypothesis space for deep learning that employs deep neural networks (DNNs). By treating a DNN as a function of two variables, the physical variable and parameter variable, we consider the primitive set of the DNNs for the parameter variable located in a set of the weight matrices and biases determined by a prescribed depth and widths of the DNNs. We then complete the linear span of the primitive DNN set in a weak* topology to construct a Banach space of functions of the physical variable. We prove that the Banach space so constructed is a reproducing kernel Banach space (RKBS) and construct its reproducing kernel. We investigate two learning models, regularized learning and minimum interpolation problem in the resulting RKBS, by establishing representer theorems for solutions of the learning models. The representer theorems unfold that solutions of these learning models can be expressed as linear combination of a finite number of kernel sessions determined by given data and the reproducing kernel.","sentences":["This paper introduces a hypothesis space for deep learning that employs deep neural networks (DNNs).","By treating a DNN as a function of two variables, the physical variable and parameter variable, we consider the primitive set of the DNNs for the parameter variable located in a set of the weight matrices and biases determined by a prescribed depth and widths of the DNNs.","We then complete the linear span of the primitive DNN set in a weak* topology to construct a Banach space of functions of the physical variable.","We prove that the Banach space so constructed is a reproducing kernel Banach space (RKBS) and construct its reproducing kernel.","We investigate two learning models, regularized learning and minimum interpolation problem in the resulting RKBS, by establishing representer theorems for solutions of the learning models.","The representer theorems unfold that solutions of these learning models can be expressed as linear combination of a finite number of kernel sessions determined by given data and the reproducing kernel."],"url":"http://arxiv.org/abs/2403.03353v1","category":"stat.ML"}
{"created":"2024-03-05 22:41:09","title":"Characterization of admissible quasisymmetries","abstract":"We solve \"half\" the problem of finding three-dimensional quasisymmetric magnetic fields that do not necessarily satisfy force balance. This involves determining which hidden symmetries are admissible as quasisymmetries, and then showing explicitly how to construct quasisymmetric magnetic fields given an admissible symmetry. The admissibility conditions take the form of a system of overdetermined nonlinear partial differential equations involving second derivatives of the symmetry's infinitesimal generator.","sentences":["We solve \"half\" the problem of finding three-dimensional quasisymmetric magnetic fields that do not necessarily satisfy force balance.","This involves determining which hidden symmetries are admissible as quasisymmetries, and then showing explicitly how to construct quasisymmetric magnetic fields given an admissible symmetry.","The admissibility conditions take the form of a system of overdetermined nonlinear partial differential equations involving second derivatives of the symmetry's infinitesimal generator."],"url":"http://arxiv.org/abs/2403.03352v1","category":"physics.plasm-ph"}
{"created":"2024-03-05 22:20:32","title":"A noncommutative Bianchi I model with radiation","abstract":"In the present work, we study the dynamical evolution of an homogeneous and anisotropic, noncommutative (NC) Bianchi I (BI) model coupled to a radiation perfect fluid. Our first motivation is determining if the present model tends to an homogeneous and isotropic NC Friedmann-Robertson-Walker (FRW) model, during its evolution. In order to simplify our task, we use the Misner parametrization of the BI metric. In terms of that parametrization the BI metric has three metric functions: the scale factor $a(t)$ and the two parameters $\\beta_\\pm (t)$, which measure the spatial anisotropy of the model. Our second motivation is trying to describe the present accelerated expansion of the universe using noncommutativity (NCTY). The NCTY is introduced by two nontrivial Poisson brackets between some geometrical as well as matter variables of the model. We recover the description in terms of commutative variables by introducing some variables transformations that depend on the NC parameter. Using those variables transformations, we rewrite the total NC Hamiltonian of the model in terms of commutative variables. From the resulting Hamiltonian, we obtain the dynamical equations for a generic perfect fluid. In order to solve these equations, we restrict our attention to a model where the perfect fluid is radiation. We solve, numerically, these equations and compare the NC solutions to the corresponding commutative ones. The comparison shows that the NC model may be considered as a possible candidate for describing the accelerated expansion of the universe. Finally, we obtain estimates for the NC parameter and compare the main results of the NC BI model coupled to radiation with the same NC BI model coupled to other perfect fluids. As our main result, we show that the solutions, after some time, produce an isotropic universe.","sentences":["In the present work, we study the dynamical evolution of an homogeneous and anisotropic, noncommutative (NC) Bianchi I (BI) model coupled to a radiation perfect fluid.","Our first motivation is determining if the present model tends to an homogeneous and isotropic NC Friedmann-Robertson-Walker (FRW) model, during its evolution.","In order to simplify our task, we use the Misner parametrization of the BI metric.","In terms of that parametrization the BI metric has three metric functions: the scale factor $a(t)$ and the two parameters $\\beta_\\pm (t)$, which measure the spatial anisotropy of the model.","Our second motivation is trying to describe the present accelerated expansion of the universe using noncommutativity (NCTY).","The NCTY is introduced by two nontrivial Poisson brackets between some geometrical as well as matter variables of the model.","We recover the description in terms of commutative variables by introducing some variables transformations that depend on the NC parameter.","Using those variables transformations, we rewrite the total NC Hamiltonian of the model in terms of commutative variables.","From the resulting Hamiltonian, we obtain the dynamical equations for a generic perfect fluid.","In order to solve these equations, we restrict our attention to a model where the perfect fluid is radiation.","We solve, numerically, these equations and compare the NC solutions to the corresponding commutative ones.","The comparison shows that the NC model may be considered as a possible candidate for describing the accelerated expansion of the universe.","Finally, we obtain estimates for the NC parameter and compare the main results of the NC BI model coupled to radiation with the same NC BI model coupled to other perfect fluids.","As our main result, we show that the solutions, after some time, produce an isotropic universe."],"url":"http://arxiv.org/abs/2403.03347v1","category":"gr-qc"}
{"created":"2024-03-05 19:37:21","title":"Neural network backflow for ab-initio quantum chemistry","abstract":"The ground state of second-quantized quantum chemistry Hamiltonians provides access to an important set of chemical properties. Wavefunctions based on ML architectures have shown promise in approximating these ground states in a variety of physical systems. In this work, we show how to achieve state-of-the-art energies for molecular Hamiltonians using the the neural network backflow wave-function. To accomplish this, we optimize this ansatz with a variant of the deterministic optimization scheme based on SCI introduced by [Li, et. al JCTC (2023)] which we find works better than standard MCMC sampling. For the molecules we studied, NNBF gives lower energy states than both CCSD and other neural network quantum states. We systematically explore the role of network size as well as optimization parameters in improving the energy. We find that while the number of hidden layers and determinants play a minor role in improving the energy, there is significant improvements in the energy from increasing the number of hidden units as well as the batch size used in optimization with the batch size playing a more important role.","sentences":["The ground state of second-quantized quantum chemistry Hamiltonians provides access to an important set of chemical properties.","Wavefunctions based on ML architectures have shown promise in approximating these ground states in a variety of physical systems.","In this work, we show how to achieve state-of-the-art energies for molecular Hamiltonians using the the neural network backflow wave-function.","To accomplish this, we optimize this ansatz with a variant of the deterministic optimization scheme based on SCI introduced by [Li, et.","al JCTC (2023)] which we find works better than standard MCMC sampling.","For the molecules we studied, NNBF gives lower energy states than both CCSD and other neural network quantum states.","We systematically explore the role of network size as well as optimization parameters in improving the energy.","We find that while the number of hidden layers and determinants play a minor role in improving the energy, there is significant improvements in the energy from increasing the number of hidden units as well as the batch size used in optimization with the batch size playing a more important role."],"url":"http://arxiv.org/abs/2403.03286v1","category":"physics.chem-ph"}
{"created":"2024-03-05 19:13:32","title":"Correlated decoding of logical algorithms with transversal gates","abstract":"Quantum error correction is believed to be essential for scalable quantum computation, but its implementation is challenging due to its considerable space-time overhead. Motivated by recent experiments demonstrating efficient manipulation of logical qubits using transversal gates (Bluvstein et al., Nature 626, 58-65 (2024)), we show that the performance of logical algorithms can be substantially improved by decoding the qubits jointly to account for physical error propagation during transversal entangling gates. We find that such correlated decoding improves the performance of both Clifford and non-Clifford transversal entangling gates, and explore two decoders offering different computational runtimes and accuracies. By considering deep logical Clifford circuits, we find that correlated decoding can significantly improve the space-time cost by reducing the number of rounds of noisy syndrome extraction per gate. These results demonstrate that correlated decoding provides a major advantage in early fault-tolerant computation, and indicate it has considerable potential to reduce the space-time cost in large-scale logical algorithms.","sentences":["Quantum error correction is believed to be essential for scalable quantum computation, but its implementation is challenging due to its considerable space-time overhead.","Motivated by recent experiments demonstrating efficient manipulation of logical qubits using transversal gates (Bluvstein et al., Nature 626, 58-65 (2024)), we show that the performance of logical algorithms can be substantially improved by decoding the qubits jointly to account for physical error propagation during transversal entangling gates.","We find that such correlated decoding improves the performance of both Clifford and non-Clifford transversal entangling gates, and explore two decoders offering different computational runtimes and accuracies.","By considering deep logical Clifford circuits, we find that correlated decoding can significantly improve the space-time cost by reducing the number of rounds of noisy syndrome extraction per gate.","These results demonstrate that correlated decoding provides a major advantage in early fault-tolerant computation, and indicate it has considerable potential to reduce the space-time cost in large-scale logical algorithms."],"url":"http://arxiv.org/abs/2403.03272v1","category":"quant-ph"}
{"created":"2024-03-05 19:05:27","title":"A Transient Thermal Model for Power Electronics Systems","abstract":"An equation based reduced order model applicable to generalized heat equation and thermal simulations of power electronics systems developed in commercial CFD tools, is presented in this work. The model considers the physics of heat transfer between multiple objects in different mediums and presents a set of equations that can be applied to a wide range of heat transfer scenarios including conduction, natural and forced convection problems. A few case studies including heat transfer in a power electronic system are simulated in Ansys Icepak and the temperatures from the simulations are compared with the temperatures predicted by the models. The models are observed to be highly accurate when compared with the simulations. The predictive model described in this work reduces large complex simulations down to a few parameters which tremendously improves the computation speed, uses very low physical disk space and enables fast evaluation of thermal performance of the system for any changes in the input parameters.","sentences":["An equation based reduced order model applicable to generalized heat equation and thermal simulations of power electronics systems developed in commercial CFD tools, is presented in this work.","The model considers the physics of heat transfer between multiple objects in different mediums and presents a set of equations that can be applied to a wide range of heat transfer scenarios including conduction, natural and forced convection problems.","A few case studies including heat transfer in a power electronic system are simulated in Ansys Icepak and the temperatures from the simulations are compared with the temperatures predicted by the models.","The models are observed to be highly accurate when compared with the simulations.","The predictive model described in this work reduces large complex simulations down to a few parameters which tremendously improves the computation speed, uses very low physical disk space and enables fast evaluation of thermal performance of the system for any changes in the input parameters."],"url":"http://arxiv.org/abs/2403.03268v1","category":"math.NA"}
{"created":"2024-03-05 19:01:05","title":"Nonassociative cyclic algebras and the semiassociative Brauer monoid","abstract":"We look at classes of semiassociative algebras, with an emphasis on those that canonically generalize associative (generalized) cyclic algebras, and at their behaviour in the semiassociative Brauer monoid defined by Blachar, Haile, Matri, Rein, and Vishne. A possible way to generalize this monoid in characteristic $p$ that includes nonassociative differential algebras is briefly considered.","sentences":["We look at classes of semiassociative algebras, with an emphasis on those that canonically generalize associative (generalized) cyclic algebras, and at their behaviour in the semiassociative Brauer monoid defined by Blachar, Haile, Matri, Rein, and Vishne.","A possible way to generalize this monoid in characteristic $p$ that includes nonassociative differential algebras is briefly considered."],"url":"http://arxiv.org/abs/2403.03263v1","category":"math.RA"}
{"created":"2024-03-05 19:00:01","title":"Neural Network Learning and Quantum Gravity","abstract":"The landscape of low-energy effective field theories stemming from string theory is too vast for a systematic exploration. However, the meadows of the string landscape may be fertile ground for the application of machine learning techniques. Employing neural network learning may allow for inferring novel, undiscovered properties that consistent theories in the landscape should possess, or checking conjectural statements about alleged characteristics thereof. The aim of this work is to describe to what extent the string landscape can be explored with neural network-based learning. Our analysis is motivated by recent studies that show that the string landscape is characterized by finiteness properties, emerging from its underlying tame, o-minimal structures. Indeed, employing these results, we illustrate that any low-energy effective theory of string theory is endowed with certain statistical learnability properties. Consequently, several learning problems therein formulated, including interpolations and multi-class classification problems, can be concretely addressed with machine learning, delivering results with sufficiently high accuracy.","sentences":["The landscape of low-energy effective field theories stemming from string theory is too vast for a systematic exploration.","However, the meadows of the string landscape may be fertile ground for the application of machine learning techniques.","Employing neural network learning may allow for inferring novel, undiscovered properties that consistent theories in the landscape should possess, or checking conjectural statements about alleged characteristics thereof.","The aim of this work is to describe to what extent the string landscape can be explored with neural network-based learning.","Our analysis is motivated by recent studies that show that the string landscape is characterized by finiteness properties, emerging from its underlying tame, o-minimal structures.","Indeed, employing these results, we illustrate that any low-energy effective theory of string theory is endowed with certain statistical learnability properties.","Consequently, several learning problems therein formulated, including interpolations and multi-class classification problems, can be concretely addressed with machine learning, delivering results with sufficiently high accuracy."],"url":"http://arxiv.org/abs/2403.03245v1","category":"hep-th"}
{"created":"2024-03-05 19:00:00","title":"The nature of diffuse ionised gas in star-forming galaxies","abstract":"We present an analysis of the diffuse ionised gas (DIG) in a high-resolution simulation of an isolated Milky Way-like galaxy, incorporating on-the-fly radiative transfer and non-equilibrium thermochemistry. We utilise the Monte-Carlo radiative transfer code COLT to self-consistently obtain ionisation states and line emission in post-processing. We find a clear bimodal distribution in the electron densities of ionised gas ($n_{\\rm e}$), allowing us to define a threshold of $n_{\\rm e}=10\\,\\mathrm{cm}^{-3}$ to differentiate DIG from HII regions. The DIG is primarily ionised by stars aged 5-25 Myr, which become exposed directly to low-density gas after HII regions have been cleared. Leakage from recently formed stars ($<5$ Myr) is only moderately important for DIG ionisation. We forward model local observations and validate our simulated DIG against observed line ratios in [SII]/H$\\alpha$, [NII]/H$\\alpha$, [OI]/H$\\alpha$, and [OIII]/H$\\beta$ against $\\Sigma_{\\rm H\\alpha}$. The mock observations not only reproduce observed correlations, but also demonstrate that such trends are related to an increasing temperature and hardening ionising radiation field with decreasing $n_{\\rm e}$. The hardening of radiation within the DIG is caused by the gradual transition of the dominant ionising source with decreasing $n_{\\rm e}$ from 0 Myr to 25 Myr stars, which have progressively harder intrinsic ionising spectra primarily due to the extended Wolf-Rayet phase caused by binary interactions. Consequently, the DIG line ratio trends can be attributed to ongoing star formation, rather than secondary ionisation sources, and therefore present a potent test for stellar feedback and stellar population models.","sentences":["We present an analysis of the diffuse ionised gas (DIG) in a high-resolution simulation of an isolated Milky Way-like galaxy, incorporating on-the-fly radiative transfer and non-equilibrium thermochemistry.","We utilise the Monte-Carlo radiative transfer code COLT to self-consistently obtain ionisation states and line emission in post-processing.","We find a clear bimodal distribution in the electron densities of ionised gas ($n_{\\rm e}$), allowing us to define a threshold of $n_{\\rm e}=10\\,\\mathrm{cm}^{-3}$ to differentiate DIG from HII regions.","The DIG is primarily ionised by stars aged 5-25 Myr, which become exposed directly to low-density gas after HII regions have been cleared.","Leakage from recently formed stars ($<5$ Myr) is only moderately important for DIG ionisation.","We forward model local observations and validate our simulated DIG against observed line ratios in [SII]/H$\\alpha$, [NII]/H$\\alpha$, [OI]/H$\\alpha$, and [OIII]/H$\\beta$ against $\\Sigma_{\\rm H\\alpha}$.","The mock observations not only reproduce observed correlations, but also demonstrate that such trends are related to an increasing temperature and hardening ionising radiation field with decreasing $n_{\\rm e}$. The hardening of radiation within the DIG is caused by the gradual transition of the dominant ionising source with decreasing $n_{\\rm e}$ from 0 Myr to 25 Myr stars, which have progressively harder intrinsic ionising spectra primarily due to the extended Wolf-Rayet phase caused by binary interactions.","Consequently, the DIG line ratio trends can be attributed to ongoing star formation, rather than secondary ionisation sources, and therefore present a potent test for stellar feedback and stellar population models."],"url":"http://arxiv.org/abs/2403.03243v1","category":"astro-ph.GA"}
{"created":"2024-03-05 18:55:11","title":"A Deep Learning Framework for Wireless Radiation Field Reconstruction and Channel Prediction","abstract":"We present NeWRF, a deep learning framework for predicting wireless channels. Wireless channel prediction is a long-standing problem in the wireless community and is a key technology for improving the coverage of wireless network deployments. Today, a wireless deployment is evaluated by a site survey which is a cumbersome process requiring an experienced engineer to perform extensive channel measurements. To reduce the cost of site surveys, we develop NeWRF, which is based on recent advances in Neural Radiance Fields (NeRF). NeWRF trains a neural network model with a sparse set of channel measurements, and predicts the wireless channel accurately at any location in the site. We introduce a series of techniques that integrate wireless propagation properties into the NeRF framework to account for the fundamental differences between the behavior of light and wireless signals. We conduct extensive evaluations of our framework and show that our approach can accurately predict channels at unvisited locations with significantly lower measurement density than prior state-of-the-art","sentences":["We present NeWRF, a deep learning framework for predicting wireless channels.","Wireless channel prediction is a long-standing problem in the wireless community and is a key technology for improving the coverage of wireless network deployments.","Today, a wireless deployment is evaluated by a site survey which is a cumbersome process requiring an experienced engineer to perform extensive channel measurements.","To reduce the cost of site surveys, we develop NeWRF, which is based on recent advances in Neural Radiance Fields (NeRF).","NeWRF trains a neural network model with a sparse set of channel measurements, and predicts the wireless channel accurately at any location in the site.","We introduce a series of techniques that integrate wireless propagation properties into the NeRF framework to account for the fundamental differences between the behavior of light and wireless signals.","We conduct extensive evaluations of our framework and show that our approach can accurately predict channels at unvisited locations with significantly lower measurement density than prior state-of-the-art"],"url":"http://arxiv.org/abs/2403.03241v1","category":"cs.NI"}
{"created":"2024-03-05 16:55:06","title":"CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver","abstract":"We propose an RNN-based efficient Ising model solver, the Criticality-ordered Recurrent Mean Field (CoRMF), for forward Ising problems. In its core, a criticality-ordered spin sequence of an $N$-spin Ising model is introduced by sorting mission-critical edges with greedy algorithm, such that an autoregressive mean-field factorization can be utilized and optimized with Recurrent Neural Networks (RNNs). Our method has two notable characteristics: (i) by leveraging the approximated tree structure of the underlying Ising graph, the newly-obtained criticality order enables the unification between variational mean-field and RNN, allowing the generally intractable Ising model to be efficiently probed with probabilistic inference; (ii) it is well-modulized, model-independent while at the same time expressive enough, and hence fully applicable to any forward Ising inference problems with minimal effort. Computationally, by using a variance-reduced Monte Carlo gradient estimator, CoRFM solves the Ising problems in a self-train fashion without data/evidence, and the inference tasks can be executed by directly sampling from RNN. Theoretically, we establish a provably tighter error bound than naive mean-field by using the matrix cut decomposition machineries. Numerically, we demonstrate the utility of this framework on a series of Ising datasets.","sentences":["We propose an RNN-based efficient Ising model solver, the Criticality-ordered Recurrent Mean Field (CoRMF), for forward Ising problems.","In its core, a criticality-ordered spin sequence of an $N$-spin Ising model is introduced by sorting mission-critical edges with greedy algorithm, such that an autoregressive mean-field factorization can be utilized and optimized with Recurrent Neural Networks (RNNs).","Our method has two notable characteristics: (i) by leveraging the approximated tree structure of the underlying Ising graph, the newly-obtained criticality order enables the unification between variational mean-field and RNN, allowing the generally intractable Ising model to be efficiently probed with probabilistic inference; (ii) it is well-modulized, model-independent while at the same time expressive enough, and hence fully applicable to any forward Ising inference problems with minimal effort.","Computationally, by using a variance-reduced Monte Carlo gradient estimator, CoRFM solves the Ising problems in a self-train fashion without data/evidence, and the inference tasks can be executed by directly sampling from RNN.","Theoretically, we establish a provably tighter error bound than naive mean-field by using the matrix cut decomposition machineries.","Numerically, we demonstrate the utility of this framework on a series of Ising datasets."],"url":"http://arxiv.org/abs/2403.03391v1","category":"stat.ML"}
{"created":"2024-03-06 17:42:02","title":"Self and Mixed Supervision to Improve Training Labels for Multi-Class Medical Image Segmentation","abstract":"Accurate training labels are a key component for multi-class medical image segmentation. Their annotation is costly and time-consuming because it requires domain expertise. This work aims to develop a dual-branch network and automatically improve training labels for multi-class image segmentation. Transfer learning is used to train the network and improve inaccurate weak labels sequentially. The dual-branch network is first trained by weak labels alone to initialize model parameters. After the network is stabilized, the shared encoder is frozen, and strong and weak decoders are fine-tuned by strong and weak labels together. The accuracy of weak labels is iteratively improved in the fine-tuning process. The proposed method was applied to a three-class segmentation of muscle, subcutaneous and visceral adipose tissue on abdominal CT scans. Validation results on 11 patients showed that the accuracy of training labels was statistically significantly improved, with the Dice similarity coefficient of muscle, subcutaneous and visceral adipose tissue increased from 74.2% to 91.5%, 91.2% to 95.6%, and 77.6% to 88.5%, respectively (p<0.05). In comparison with our earlier method, the label accuracy was also significantly improved (p<0.05). These experimental results suggested that the combination of the dual-branch network and transfer learning is an efficient means to improve training labels for multi-class segmentation.","sentences":["Accurate training labels are a key component for multi-class medical image segmentation.","Their annotation is costly and time-consuming because it requires domain expertise.","This work aims to develop a dual-branch network and automatically improve training labels for multi-class image segmentation.","Transfer learning is used to train the network and improve inaccurate weak labels sequentially.","The dual-branch network is first trained by weak labels alone to initialize model parameters.","After the network is stabilized, the shared encoder is frozen, and strong and weak decoders are fine-tuned by strong and weak labels together.","The accuracy of weak labels is iteratively improved in the fine-tuning process.","The proposed method was applied to a three-class segmentation of muscle, subcutaneous and visceral adipose tissue on abdominal CT scans.","Validation results on 11 patients showed that the accuracy of training labels was statistically significantly improved, with the Dice similarity coefficient of muscle, subcutaneous and visceral adipose tissue increased from 74.2% to 91.5%, 91.2% to 95.6%, and 77.6% to 88.5%, respectively (p<0.05).","In comparison with our earlier method, the label accuracy was also significantly improved (p<0.05).","These experimental results suggested that the combination of the dual-branch network and transfer learning is an efficient means to improve training labels for multi-class segmentation."],"url":"http://arxiv.org/abs/2403.03882v1","category":"cs.CV"}
{"created":"2024-03-06 17:17:36","title":"On the Origins of Linear Representations in Large Language Models","abstract":"Recent works have argued that high-level semantic concepts are encoded \"linearly\" in the representation space of large language models. In this work, we study the origins of such linear representations. To that end, we introduce a simple latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to show that the next token prediction objective (softmax with cross-entropy) and the implicit bias of gradient descent together promote the linear representation of concepts. Experiments show that linear representations emerge when learning from data matching the latent variable model, confirming that this simple structure already suffices to yield linear representations. We additionally confirm some predictions of the theory using the LLaMA-2 large language model, giving evidence that the simplified model yields generalizable insights.","sentences":["Recent works have argued that high-level semantic concepts are encoded \"linearly\" in the representation space of large language models.","In this work, we study the origins of such linear representations.","To that end, we introduce a simple latent variable model to abstract and formalize the concept dynamics of the next token prediction.","We use this formalism to show that the next token prediction objective (softmax with cross-entropy) and the implicit bias of gradient descent together promote the linear representation of concepts.","Experiments show that linear representations emerge when learning from data matching the latent variable model, confirming that this simple structure already suffices to yield linear representations.","We additionally confirm some predictions of the theory using the LLaMA-2 large language model, giving evidence that the simplified model yields generalizable insights."],"url":"http://arxiv.org/abs/2403.03867v1","category":"cs.CL"}
{"created":"2024-03-06 16:42:10","title":"On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder","abstract":"In this paper, we study a defense against poisoned encoders in SSL called distillation, which is a defense used in supervised learning originally. Distillation aims to distill knowledge from a given model (a.k.a the teacher net) and transfer it to another (a.k.a the student net). Now, we use it to distill benign knowledge from poisoned pre-trained encoders and transfer it to a new encoder, resulting in a clean pre-trained encoder. In particular, we conduct an empirical study on the effectiveness and performance of distillation against poisoned encoders. Using two state-of-the-art backdoor attacks against pre-trained image encoders and four commonly used image classification datasets, our experimental results show that distillation can reduce attack success rate from 80.87% to 27.51% while suffering a 6.35% loss in accuracy. Moreover, we investigate the impact of three core components of distillation on performance: teacher net, student net, and distillation loss. By comparing 4 different teacher nets, 3 student nets, and 6 distillation losses, we find that fine-tuned teacher nets, warm-up-training-based student nets, and attention-based distillation loss perform best, respectively.","sentences":["In this paper, we study a defense against poisoned encoders in SSL called distillation, which is a defense used in supervised learning originally.","Distillation aims to distill knowledge from a given model (a.k.a the teacher net) and transfer it to another (a.k.a the student net).","Now, we use it to distill benign knowledge from poisoned pre-trained encoders and transfer it to a new encoder, resulting in a clean pre-trained encoder.","In particular, we conduct an empirical study on the effectiveness and performance of distillation against poisoned encoders.","Using two state-of-the-art backdoor attacks against pre-trained image encoders and four commonly used image classification datasets, our experimental results show that distillation can reduce attack success rate from 80.87% to 27.51% while suffering a 6.35% loss in accuracy.","Moreover, we investigate the impact of three core components of distillation on performance: teacher net, student net, and distillation loss.","By comparing 4 different teacher nets, 3 student nets, and 6 distillation losses, we find that fine-tuned teacher nets, warm-up-training-based student nets, and attention-based distillation loss perform best, respectively."],"url":"http://arxiv.org/abs/2403.03846v1","category":"cs.LG"}
{"created":"2024-03-06 13:09:05","title":"The Visual Debugger: Past, Present, and Future","abstract":"The Visual Debugger is an IntelliJ IDEA plugin that presents debug information as an object diagram to enhance program understanding. Reflecting on our past development, we detail the lessons learned and roadblocks we have experienced while implementing and integrating the Visual Debugger into the IntelliJ IDEA. Furthermore, we describe recent improvements to the Visual Debugger, greatly enhancing the plugin in the present. Looking into the future, we propose solutions to overcome the roadblocks encountered while developing the plugin and further plans for the Visual Debugger.","sentences":["The Visual Debugger is an IntelliJ IDEA plugin that presents debug information as an object diagram to enhance program understanding.","Reflecting on our past development, we detail the lessons learned and roadblocks we have experienced while implementing and integrating the Visual Debugger into the IntelliJ IDEA.","Furthermore, we describe recent improvements to the Visual Debugger, greatly enhancing the plugin in the present.","Looking into the future, we propose solutions to overcome the roadblocks encountered while developing the plugin and further plans for the Visual Debugger."],"url":"http://arxiv.org/abs/2403.03683v1","category":"cs.SE"}
{"created":"2024-03-06 12:34:50","title":"Environmental Insights: Democratizing Access to Ambient Air Pollution Data and Predictive Analytics with an Open-Source Python Package","abstract":"Ambient air pollution is a pervasive issue with wide-ranging effects on human health, ecosystem vitality, and economic structures. Utilizing data on ambient air pollution concentrations, researchers can perform comprehensive analyses to uncover the multifaceted impacts of air pollution across society. To this end, we introduce Environmental Insights, an open-source Python package designed to democratize access to air pollution concentration data. This tool enables users to easily retrieve historical air pollution data and employ a Machine Learning model for forecasting potential future conditions. Moreover, Environmental Insights includes a suite of tools aimed at facilitating the dissemination of analytical findings and enhancing user engagement through dynamic visualizations. This comprehensive approach ensures that the package caters to the diverse needs of individuals looking to explore and understand air pollution trends and their implications.","sentences":["Ambient air pollution is a pervasive issue with wide-ranging effects on human health, ecosystem vitality, and economic structures.","Utilizing data on ambient air pollution concentrations, researchers can perform comprehensive analyses to uncover the multifaceted impacts of air pollution across society.","To this end, we introduce Environmental Insights, an open-source Python package designed to democratize access to air pollution concentration data.","This tool enables users to easily retrieve historical air pollution data and employ a Machine Learning model for forecasting potential future conditions.","Moreover, Environmental Insights includes a suite of tools aimed at facilitating the dissemination of analytical findings and enhancing user engagement through dynamic visualizations.","This comprehensive approach ensures that the package caters to the diverse needs of individuals looking to explore and understand air pollution trends and their implications."],"url":"http://arxiv.org/abs/2403.03664v1","category":"physics.soc-ph"}
{"created":"2024-03-06 11:51:52","title":"Exact objectives of random linear programs and mean widths of random polyhedrons","abstract":"We consider \\emph{random linear programs} (rlps) as a subclass of \\emph{random optimization problems} (rops) and study their typical behavior. Our particular focus is on appropriate linear objectives which connect the rlps to the mean widths of random polyhedrons/polytopes. Utilizing the powerful machinery of \\emph{random duality theory} (RDT) \\cite{StojnicRegRndDlt10}, we obtain, in a large dimensional context, the exact characterizations of the program's objectives. In particular, for any $\\alpha=\\lim_{n\\rightarrow\\infty}\\frac{m}{n}\\in(0,\\infty)$, any unit vector $\\mathbf{c}\\in{\\mathbb R}^n$, any fixed $\\mathbf{a}\\in{\\mathbb R}^n$, and $A\\in {\\mathbb R}^{m\\times n}$ with iid standard normal entries, we have   \\begin{eqnarray*}   \\lim_{n\\rightarrow\\infty}{\\mathbb P}_{A} \\left ( (1-\\epsilon) \\xi_{opt}(\\alpha;\\mathbf{a})   \\leq \\min_{A\\mathbf{x}\\leq \\mathbf{a}}\\mathbf{c}^T\\mathbf{x} \\leq (1+\\epsilon) \\xi_{opt}(\\alpha;\\mathbf{a}) \\right ) \\longrightarrow 1, \\end{eqnarray*}   where   \\begin{equation*} \\xi_{opt}(\\alpha;\\mathbf{a}) \\triangleq \\min_{x>0} \\sqrt{x^2- x^2 \\lim_{n\\rightarrow\\infty} \\frac{\\sum_{i=1}^{m} \\left ( \\frac{1}{2} \\left (\\left ( \\frac{\\mathbf{a}_i}{x}\\right )^2 + 1\\right ) \\mbox{erfc}\\left( \\frac{\\mathbf{a}_i}{x\\sqrt{2}}\\right ) - \\frac{\\mathbf{a}_i}{x} \\frac{e^{-\\frac{\\mathbf{a}_i^2}{2x^2}}}{\\sqrt{2\\pi}} \\right )   }{n} }. \\end{equation*}   For example, for $\\mathbf{a}=\\mathbf{1}$, one uncovers   \\begin{equation*}   \\xi_{opt}(\\alpha)   =   \\min_{x>0} \\sqrt{x^2- x^2 \\alpha \\left ( \\frac{1}{2} \\left ( \\frac{1}{x^2} + 1\\right ) \\mbox{erfc} \\left ( \\frac{1}{x\\sqrt{2}}\\right ) - \\frac{1}{x} \\frac{e^{-\\frac{1}{2x^2}}}{\\sqrt{2\\pi}} \\right ) }. \\end{equation*}   Moreover, $2 \\xi_{opt}(\\alpha)$ is precisely the concentrating point of the mean width of the polyhedron $\\{\\mathbf{x}|A\\mathbf{x} \\leq \\mathbf{1}\\}$.","sentences":["We consider \\emph{random linear programs} (rlps) as a subclass of \\emph{random optimization problems} (rops) and study their typical behavior.","Our particular focus is on appropriate linear objectives which connect the rlps to the mean widths of random polyhedrons/polytopes.","Utilizing the powerful machinery of \\emph{random duality theory} (RDT) \\cite{StojnicRegRndDlt10}, we obtain, in a large dimensional context, the exact characterizations of the program's objectives.","In particular, for any $\\alpha=\\lim_{n\\rightarrow\\infty}\\frac{m}{n}\\in(0,\\infty)$, any unit vector $\\mathbf{c}\\in{\\mathbb R}^n$, any fixed $\\mathbf{a}\\in{\\mathbb R}^n$, and $A\\in {\\mathbb R}^{m\\times n}$ with iid standard normal entries, we have   \\begin{eqnarray*}   \\lim_{n\\rightarrow\\infty}{\\mathbb P}_{A} \\left ( (1-\\epsilon) \\xi_{opt}(\\alpha;\\mathbf{a})   ","\\leq \\min_{A\\mathbf{x}\\leq \\mathbf{a}}\\mathbf{c}^T\\mathbf{x} \\leq (1+\\epsilon) \\xi_{opt}(\\alpha;\\mathbf{a}) \\right )","\\longrightarrow 1, \\end{eqnarray*}   where   \\begin{equation*} \\xi_{opt}(\\alpha;\\mathbf{a})","\\triangleq \\min_{x>0} \\sqrt{x^2- x^2 \\lim_{n\\rightarrow\\infty} \\frac{\\sum_{i=1}^{m} \\left ( \\frac{1}{2} \\left (\\left ( \\frac{\\mathbf{a}_i}{x}\\right )^2","+ 1\\right ) \\mbox{erfc}\\left( \\frac{\\mathbf{a}_i}{x\\sqrt{2}}\\right ) - \\frac{\\mathbf{a}_i}{x} \\frac{e^{-\\frac{\\mathbf{a}_i^2}{2x^2}}}{\\sqrt{2\\pi}} \\right )   }{n} }.","\\end{equation*}   For example, for $\\mathbf{a}=\\mathbf{1}$, one uncovers   \\begin{equation*}   \\xi_{opt}(\\alpha)   =   \\min_{x>0} \\sqrt{x^2- x^2 \\alpha \\left ( \\frac{1}{2} \\left ( \\frac{1}{x^2} + 1\\right ) \\mbox{erfc} \\left ( \\frac{1}{x\\sqrt{2}}\\right ) - \\frac{1}{x} \\frac{e^{-\\frac{1}{2x^2}}}{\\sqrt{2\\pi}} \\right ) }.","\\end{equation*}   Moreover, $2 \\xi_{opt}(\\alpha)$ is precisely the concentrating point of the mean width of the polyhedron $\\{\\mathbf{x}|A\\mathbf{x} \\leq \\mathbf{1}\\}$."],"url":"http://arxiv.org/abs/2403.03637v1","category":"math.OC"}
{"created":"2024-03-06 11:09:36","title":"Reducing the dimensionality and granularity in hierarchical categorical variables","abstract":"Hierarchical categorical variables often exhibit many levels (high granularity) and many classes within each level (high dimensionality). This may cause overfitting and estimation issues when including such covariates in a predictive model. In current literature, a hierarchical covariate is often incorporated via nested random effects. However, this does not facilitate the assumption of classes having the same effect on the response variable. In this paper, we propose a methodology to obtain a reduced representation of a hierarchical categorical variable. We show how entity embedding can be applied in a hierarchical setting. Subsequently, we propose a top-down clustering algorithm which leverages the information encoded in the embeddings to reduce both the within-level dimensionality as well as the overall granularity of the hierarchical categorical variable. In simulation experiments, we show that our methodology can effectively approximate the true underlying structure of a hierarchical covariate in terms of the effect on a response variable, and find that incorporating the reduced hierarchy improves model fit. We apply our methodology on a real dataset and find that the reduced hierarchy is an improvement over the original hierarchical structure and reduced structures proposed in the literature.","sentences":["Hierarchical categorical variables often exhibit many levels (high granularity) and many classes within each level (high dimensionality).","This may cause overfitting and estimation issues when including such covariates in a predictive model.","In current literature, a hierarchical covariate is often incorporated via nested random effects.","However, this does not facilitate the assumption of classes having the same effect on the response variable.","In this paper, we propose a methodology to obtain a reduced representation of a hierarchical categorical variable.","We show how entity embedding can be applied in a hierarchical setting.","Subsequently, we propose a top-down clustering algorithm which leverages the information encoded in the embeddings to reduce both the within-level dimensionality as well as the overall granularity of the hierarchical categorical variable.","In simulation experiments, we show that our methodology can effectively approximate the true underlying structure of a hierarchical covariate in terms of the effect on a response variable, and find that incorporating the reduced hierarchy improves model fit.","We apply our methodology on a real dataset and find that the reduced hierarchy is an improvement over the original hierarchical structure and reduced structures proposed in the literature."],"url":"http://arxiv.org/abs/2403.03613v1","category":"stat.ME"}
{"created":"2024-03-06 09:10:36","title":"HMD-Poser: On-Device Real-time Human Motion Tracking from Scalable Sparse Observations","abstract":"It is especially challenging to achieve real-time human motion tracking on a standalone VR Head-Mounted Display (HMD) such as Meta Quest and PICO. In this paper, we propose HMD-Poser, the first unified approach to recover full-body motions using scalable sparse observations from HMD and body-worn IMUs. In particular, it can support a variety of input scenarios, such as HMD, HMD+2IMUs, HMD+3IMUs, etc. The scalability of inputs may accommodate users' choices for both high tracking accuracy and easy-to-wear. A lightweight temporal-spatial feature learning network is proposed in HMD-Poser to guarantee that the model runs in real-time on HMDs. Furthermore, HMD-Poser presents online body shape estimation to improve the position accuracy of body joints. Extensive experimental results on the challenging AMASS dataset show that HMD-Poser achieves new state-of-the-art results in both accuracy and real-time performance. We also build a new free-dancing motion dataset to evaluate HMD-Poser's on-device performance and investigate the performance gap between synthetic data and real-captured sensor data. Finally, we demonstrate our HMD-Poser with a real-time Avatar-driving application on a commercial HMD. Our code and free-dancing motion dataset are available https://pico-ai-team.github.io/hmd-poser","sentences":["It is especially challenging to achieve real-time human motion tracking on a standalone VR Head-Mounted Display (HMD) such as Meta Quest and PICO.","In this paper, we propose HMD-Poser, the first unified approach to recover full-body motions using scalable sparse observations from HMD and body-worn IMUs.","In particular, it can support a variety of input scenarios, such as HMD, HMD+2IMUs, HMD+3IMUs, etc.","The scalability of inputs may accommodate users' choices for both high tracking accuracy and easy-to-wear.","A lightweight temporal-spatial feature learning network is proposed in HMD-Poser to guarantee that the model runs in real-time on HMDs.","Furthermore, HMD-Poser presents online body shape estimation to improve the position accuracy of body joints.","Extensive experimental results on the challenging AMASS dataset show that HMD-Poser achieves new state-of-the-art results in both accuracy and real-time performance.","We also build a new free-dancing motion dataset to evaluate HMD-Poser's on-device performance and investigate the performance gap between synthetic data and real-captured sensor data.","Finally, we demonstrate our HMD-Poser with a real-time Avatar-driving application on a commercial HMD.","Our code and free-dancing motion dataset are available https://pico-ai-team.github.io/hmd-poser"],"url":"http://arxiv.org/abs/2403.03561v1","category":"cs.CV"}
{"created":"2024-03-06 09:06:34","title":"Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem","abstract":"Large language models (LLMs) are highly effective in various natural language processing (NLP) tasks. However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination. This paper presents a new method for evaluating LLM hallucination in Question Answering (QA) based on the unanswerable math word problem (MWP). To support this approach, we innovatively develop a dataset called Unanswerable Math Word Problem (UMWP) which comprises 5200 questions across five categories. We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether LLM considers the question unanswerable. The results of extensive experiments conducted on 31 LLMs, including GPT-3, InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and reinforcement learning with human feedback (RLHF) training significantly enhance the model's ability to avoid hallucination. We show that utilizing MWP is a reliable and effective approach to assess hallucination. Our code and data are available at https://github.com/Yuki-Asuuna/UMWP.","sentences":["Large language models (LLMs) are highly effective in various natural language processing (NLP) tasks.","However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination.","This paper presents a new method for evaluating LLM hallucination in Question Answering (QA) based on the unanswerable math word problem (MWP).","To support this approach, we innovatively develop a dataset called Unanswerable Math Word Problem (UMWP) which comprises 5200 questions across five categories.","We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether LLM considers the question unanswerable.","The results of extensive experiments conducted on 31 LLMs, including GPT-3, InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and reinforcement learning with human feedback (RLHF) training significantly enhance the model's ability to avoid hallucination.","We show that utilizing MWP is a reliable and effective approach to assess hallucination.","Our code and data are available at https://github.com/Yuki-Asuuna/UMWP."],"url":"http://arxiv.org/abs/2403.03558v1","category":"cs.CL"}
{"created":"2024-03-06 08:22:52","title":"Quantum machine learning with indefinite causal order","abstract":"In a conventional circuit for quantum machine learning, the quantum gates used to encode the input parameters and the variational parameters are constructed with a fixed order. The resulting output function, which can be expressed in the form of a restricted Fourier series, has limited flexibility in the distributions of its Fourier coefficients. This indicates that a fixed order of quantum gates can limit the performance of quantum machine learning. Building on this key insight (also elaborated with examples), we introduce indefinite causal order to quantum machine learning. Because the indefinite causal order of quantum gates allows for the superposition of different orders, the performance of quantum machine learning can be significantly enhanced. Considering that the current accessible quantum platforms only allow to simulate a learning structure with a fixed order of quantum gates, we reform the existing simulation protocol to implement indefinite causal order and further demonstrate the positive impact of indefinite causal order on specific learning tasks. Our results offer useful insights into possible quantum effects in quantum machine learning.","sentences":["In a conventional circuit for quantum machine learning, the quantum gates used to encode the input parameters and the variational parameters are constructed with a fixed order.","The resulting output function, which can be expressed in the form of a restricted Fourier series, has limited flexibility in the distributions of its Fourier coefficients.","This indicates that a fixed order of quantum gates can limit the performance of quantum machine learning.","Building on this key insight (also elaborated with examples), we introduce indefinite causal order to quantum machine learning.","Because the indefinite causal order of quantum gates allows for the superposition of different orders, the performance of quantum machine learning can be significantly enhanced.","Considering that the current accessible quantum platforms only allow to simulate a learning structure with a fixed order of quantum gates, we reform the existing simulation protocol to implement indefinite causal order and further demonstrate the positive impact of indefinite causal order on specific learning tasks.","Our results offer useful insights into possible quantum effects in quantum machine learning."],"url":"http://arxiv.org/abs/2403.03533v1","category":"quant-ph"}
{"created":"2024-03-06 07:39:33","title":"Dcl-Net: Dual Contrastive Learning Network for Semi-Supervised Multi-Organ Segmentation","abstract":"Semi-supervised learning is a sound measure to relieve the strict demand of abundant annotated datasets, especially for challenging multi-organ segmentation . However, most existing SSL methods predict pixels in a single image independently, ignoring the relations among images and categories. In this paper, we propose a two-stage Dual Contrastive Learning Network for semi-supervised MoS, which utilizes global and local contrastive learning to strengthen the relations among images and classes. Concretely, in Stage 1, we develop a similarity-guided global contrastive learning to explore the implicit continuity and similarity among images and learn global context. Then, in Stage 2, we present an organ-aware local contrastive learning to further attract the class representations. To ease the computation burden, we introduce a mask center computation algorithm to compress the category representations for local contrastive learning. Experiments conducted on the public 2017 ACDC dataset and an in-house RC-OARs dataset has demonstrated the superior performance of our method.","sentences":["Semi-supervised learning is a sound measure to relieve the strict demand of abundant annotated datasets, especially for challenging multi-organ segmentation .","However, most existing SSL methods predict pixels in a single image independently, ignoring the relations among images and categories.","In this paper, we propose a two-stage Dual Contrastive Learning Network for semi-supervised MoS, which utilizes global and local contrastive learning to strengthen the relations among images and classes.","Concretely, in Stage 1, we develop a similarity-guided global contrastive learning to explore the implicit continuity and similarity among images and learn global context.","Then, in Stage 2, we present an organ-aware local contrastive learning to further attract the class representations.","To ease the computation burden, we introduce a mask center computation algorithm to compress the category representations for local contrastive learning.","Experiments conducted on the public 2017 ACDC dataset and an in-house RC-OARs dataset has demonstrated the superior performance of our method."],"url":"http://arxiv.org/abs/2403.03512v1","category":"cs.CV"}
{"created":"2024-03-06 05:33:50","title":"Continual Segmentation with Disentangled Objectness Learning and Class Recognition","abstract":"Most continual segmentation methods tackle the problem as a per-pixel classification task. However, such a paradigm is very challenging, and we find query-based segmenters with built-in objectness have inherent advantages compared with per-pixel ones, as objectness has strong transfer ability and forgetting resistance. Based on these findings, we propose CoMasTRe by disentangling continual segmentation into two stages: forgetting-resistant continual objectness learning and well-researched continual classification. CoMasTRe uses a two-stage segmenter learning class-agnostic mask proposals at the first stage and leaving recognition to the second stage. During continual learning, a simple but effective distillation is adopted to strengthen objectness. To further mitigate the forgetting of old classes, we design a multi-label class distillation strategy suited for segmentation. We assess the effectiveness of CoMasTRe on PASCAL VOC and ADE20K. Extensive experiments show that our method outperforms per-pixel and query-based methods on both datasets. Code will be available at https://github.com/jordangong/CoMasTRe.","sentences":["Most continual segmentation methods tackle the problem as a per-pixel classification task.","However, such a paradigm is very challenging, and we find query-based segmenters with built-in objectness have inherent advantages compared with per-pixel ones, as objectness has strong transfer ability and forgetting resistance.","Based on these findings, we propose CoMasTRe by disentangling continual segmentation into two stages: forgetting-resistant continual objectness learning and well-researched continual classification.","CoMasTRe uses a two-stage segmenter learning class-agnostic mask proposals at the first stage and leaving recognition to the second stage.","During continual learning, a simple but effective distillation is adopted to strengthen objectness.","To further mitigate the forgetting of old classes, we design a multi-label class distillation strategy suited for segmentation.","We assess the effectiveness of CoMasTRe on PASCAL VOC and ADE20K. Extensive experiments show that our method outperforms per-pixel and query-based methods on both datasets.","Code will be available at https://github.com/jordangong/CoMasTRe."],"url":"http://arxiv.org/abs/2403.03477v1","category":"cs.CV"}
{"created":"2024-03-06 04:59:38","title":"FLAME Diffuser: Grounded Wildfire Image Synthesis using Mask Guided Diffusion","abstract":"The rise of machine learning in recent years has brought benefits to various research fields such as wide fire detection. Nevertheless, small object detection and rare object detection remain a challenge. To address this problem, we present a dataset automata that can generate ground truth paired datasets using diffusion models. Specifically, we introduce a mask-guided diffusion framework that can fusion the wildfire into the existing images while the flame position and size can be precisely controlled. In advance, to fill the gap that the dataset of wildfire images in specific scenarios is missing, we vary the background of synthesized images by controlling both the text prompt and input image. Furthermore, to solve the color tint problem or the well-known domain shift issue, we apply the CLIP model to filter the generated massive dataset to preserve quality. Thus, our proposed framework can generate a massive dataset of that images are high-quality and ground truth-paired, which well addresses the needs of the annotated datasets in specific tasks.","sentences":["The rise of machine learning in recent years has brought benefits to various research fields such as wide fire detection.","Nevertheless, small object detection and rare object detection remain a challenge.","To address this problem, we present a dataset automata that can generate ground truth paired datasets using diffusion models.","Specifically, we introduce a mask-guided diffusion framework that can fusion the wildfire into the existing images while the flame position and size can be precisely controlled.","In advance, to fill the gap that the dataset of wildfire images in specific scenarios is missing, we vary the background of synthesized images by controlling both the text prompt and input image.","Furthermore, to solve the color tint problem or the well-known domain shift issue, we apply the CLIP model to filter the generated massive dataset to preserve quality.","Thus, our proposed framework can generate a massive dataset of that images are high-quality and ground truth-paired, which well addresses the needs of the annotated datasets in specific tasks."],"url":"http://arxiv.org/abs/2403.03463v1","category":"cs.CV"}
{"created":"2024-03-06 04:49:02","title":"Slot Abstractors: Toward Scalable Abstract Visual Reasoning","abstract":"Abstract visual reasoning is a characteristically human ability, allowing the identification of relational patterns that are abstracted away from object features, and the systematic generalization of those patterns to unseen problems. Recent work has demonstrated strong systematic generalization in visual reasoning tasks involving multi-object inputs, through the integration of slot-based methods used for extracting object-centric representations coupled with strong inductive biases for relational abstraction. However, this approach was limited to problems containing a single rule, and was not scalable to visual reasoning problems containing a large number of objects. Other recent work proposed Abstractors, an extension of Transformers that incorporates strong relational inductive biases, thereby inheriting the Transformer's scalability and multi-head architecture, but it has yet to be demonstrated how this approach might be applied to multi-object visual inputs. Here we combine the strengths of the above approaches and propose Slot Abstractors, an approach to abstract visual reasoning that can be scaled to problems involving a large number of objects and multiple relations among them. The approach displays state-of-the-art performance across four abstract visual reasoning tasks.","sentences":["Abstract visual reasoning is a characteristically human ability, allowing the identification of relational patterns that are abstracted away from object features, and the systematic generalization of those patterns to unseen problems.","Recent work has demonstrated strong systematic generalization in visual reasoning tasks involving multi-object inputs, through the integration of slot-based methods used for extracting object-centric representations coupled with strong inductive biases for relational abstraction.","However, this approach was limited to problems containing a single rule, and was not scalable to visual reasoning problems containing a large number of objects.","Other recent work proposed Abstractors, an extension of Transformers that incorporates strong relational inductive biases, thereby inheriting the Transformer's scalability and multi-head architecture, but it has yet to be demonstrated how this approach might be applied to multi-object visual inputs.","Here we combine the strengths of the above approaches and propose Slot Abstractors, an approach to abstract visual reasoning that can be scaled to problems involving a large number of objects and multiple relations among them.","The approach displays state-of-the-art performance across four abstract visual reasoning tasks."],"url":"http://arxiv.org/abs/2403.03458v1","category":"cs.CV"}
{"created":"2024-03-06 04:24:43","title":"Kernel Correlation-Dissimilarity for Multiple Kernel k-Means Clustering","abstract":"The main objective of the Multiple Kernel k-Means (MKKM) algorithm is to extract non-linear information and achieve optimal clustering by optimizing base kernel matrices. Current methods enhance information diversity and reduce redundancy by exploiting interdependencies among multiple kernels based on correlations or dissimilarities. Nevertheless, relying solely on a single metric, such as correlation or dissimilarity, to define kernel relationships introduces bias and incomplete characterization. Consequently, this limitation hinders efficient information extraction, ultimately compromising clustering performance. To tackle this challenge, we introduce a novel method that systematically integrates both kernel correlation and dissimilarity. Our approach comprehensively captures kernel relationships, facilitating more efficient classification information extraction and improving clustering performance. By emphasizing the coherence between kernel correlation and dissimilarity, our method offers a more objective and transparent strategy for extracting non-linear information and significantly improving clustering precision, supported by theoretical rationale. We assess the performance of our algorithm on 13 challenging benchmark datasets, demonstrating its superiority over contemporary state-of-the-art MKKM techniques.","sentences":["The main objective of the Multiple Kernel k-Means (MKKM) algorithm is to extract non-linear information and achieve optimal clustering by optimizing base kernel matrices.","Current methods enhance information diversity and reduce redundancy by exploiting interdependencies among multiple kernels based on correlations or dissimilarities.","Nevertheless, relying solely on a single metric, such as correlation or dissimilarity, to define kernel relationships introduces bias and incomplete characterization.","Consequently, this limitation hinders efficient information extraction, ultimately compromising clustering performance.","To tackle this challenge, we introduce a novel method that systematically integrates both kernel correlation and dissimilarity.","Our approach comprehensively captures kernel relationships, facilitating more efficient classification information extraction and improving clustering performance.","By emphasizing the coherence between kernel correlation and dissimilarity, our method offers a more objective and transparent strategy for extracting non-linear information and significantly improving clustering precision, supported by theoretical rationale.","We assess the performance of our algorithm on 13 challenging benchmark datasets, demonstrating its superiority over contemporary state-of-the-art MKKM techniques."],"url":"http://arxiv.org/abs/2403.03448v1","category":"cs.LG"}
{"created":"2024-03-06 03:32:56","title":"Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing","abstract":"Deep Text-to-Image Synthesis (TIS) models such as Stable Diffusion have recently gained significant popularity for creative Text-to-image generation. Yet, for domain-specific scenarios, tuning-free Text-guided Image Editing (TIE) is of greater importance for application developers, which modify objects or object properties in images by manipulating feature components in attention layers during the generation process. However, little is known about what semantic meanings these attention layers have learned and which parts of the attention maps contribute to the success of image editing. In this paper, we conduct an in-depth probing analysis and demonstrate that cross-attention maps in Stable Diffusion often contain object attribution information that can result in editing failures. In contrast, self-attention maps play a crucial role in preserving the geometric and shape details of the source image during the transformation to the target image. Our analysis offers valuable insights into understanding cross and self-attention maps in diffusion models. Moreover, based on our findings, we simplify popular image editing methods and propose a more straightforward yet more stable and efficient tuning-free procedure that only modifies self-attention maps of the specified attention layers during the denoising process. Experimental results show that our simplified method consistently surpasses the performance of popular approaches on multiple datasets.","sentences":["Deep Text-to-Image Synthesis (TIS) models such as Stable Diffusion have recently gained significant popularity for creative Text-to-image generation.","Yet, for domain-specific scenarios, tuning-free Text-guided Image Editing (TIE) is of greater importance for application developers, which modify objects or object properties in images by manipulating feature components in attention layers during the generation process.","However, little is known about what semantic meanings these attention layers have learned and which parts of the attention maps contribute to the success of image editing.","In this paper, we conduct an in-depth probing analysis and demonstrate that cross-attention maps in Stable Diffusion often contain object attribution information that can result in editing failures.","In contrast, self-attention maps play a crucial role in preserving the geometric and shape details of the source image during the transformation to the target image.","Our analysis offers valuable insights into understanding cross and self-attention maps in diffusion models.","Moreover, based on our findings, we simplify popular image editing methods and propose a more straightforward yet more stable and efficient tuning-free procedure that only modifies self-attention maps of the specified attention layers during the denoising process.","Experimental results show that our simplified method consistently surpasses the performance of popular approaches on multiple datasets."],"url":"http://arxiv.org/abs/2403.03431v1","category":"cs.CV"}
{"created":"2024-03-06 01:49:28","title":"Contrastive Learning of Person-independent Representations for Facial Action Unit Detection","abstract":"Facial action unit (AU) detection, aiming to classify AU present in the facial image, has long suffered from insufficient AU annotations. In this paper, we aim to mitigate this data scarcity issue by learning AU representations from a large number of unlabelled facial videos in a contrastive learning paradigm. We formulate the self-supervised AU representation learning signals in two-fold: (1) AU representation should be frame-wisely discriminative within a short video clip; (2) Facial frames sampled from different identities but show analogous facial AUs should have consistent AU representations. As to achieve these goals, we propose to contrastively learn the AU representation within a video clip and devise a cross-identity reconstruction mechanism to learn the person-independent representations. Specially, we adopt a margin-based temporal contrastive learning paradigm to perceive the temporal AU coherence and evolution characteristics within a clip that consists of consecutive input facial frames. Moreover, the cross-identity reconstruction mechanism facilitates pushing the faces from different identities but show analogous AUs close in the latent embedding space. Experimental results on three public AU datasets demonstrate that the learned AU representation is discriminative for AU detection. Our method outperforms other contrastive learning methods and significantly closes the performance gap between the self-supervised and supervised AU detection approaches.","sentences":["Facial action unit (AU) detection, aiming to classify AU present in the facial image, has long suffered from insufficient AU annotations.","In this paper, we aim to mitigate this data scarcity issue by learning AU representations from a large number of unlabelled facial videos in a contrastive learning paradigm.","We formulate the self-supervised AU representation learning signals in two-fold: (1) AU representation should be frame-wisely discriminative within a short video clip; (2) Facial frames sampled from different identities but show analogous facial AUs should have consistent AU representations.","As to achieve these goals, we propose to contrastively learn the AU representation within a video clip and devise a cross-identity reconstruction mechanism to learn the person-independent representations.","Specially, we adopt a margin-based temporal contrastive learning paradigm to perceive the temporal AU coherence and evolution characteristics within a clip that consists of consecutive input facial frames.","Moreover, the cross-identity reconstruction mechanism facilitates pushing the faces from different identities but show analogous AUs close in the latent embedding space.","Experimental results on three public AU datasets demonstrate that the learned AU representation is discriminative for AU detection.","Our method outperforms other contrastive learning methods and significantly closes the performance gap between the self-supervised and supervised AU detection approaches."],"url":"http://arxiv.org/abs/2403.03400v1","category":"cs.CV"}
{"created":"2024-03-06 00:59:51","title":"Performance Evaluation of Semi-supervised Learning Frameworks for Multi-Class Weed Detection","abstract":"Effective weed control plays a crucial role in optimizing crop yield and enhancing agricultural product quality. However, the reliance on herbicide application not only poses a critical threat to the environment but also promotes the emergence of resistant weeds. Fortunately, recent advances in precision weed management enabled by ML and DL provide a sustainable alternative. Despite great progress, existing algorithms are mainly developed based on supervised learning approaches, which typically demand large-scale datasets with manual-labeled annotations, which is time-consuming and labor-intensive. As such, label-efficient learning methods, especially semi-supervised learning, have gained increased attention in the broader domain of computer vision and have demonstrated promising performance. These methods aim to utilize a small number of labeled data samples along with a great number of unlabeled samples to develop high-performing models comparable to the supervised learning counterpart trained on a large amount of labeled data samples. In this study, we assess the effectiveness of a semi-supervised learning framework for multi-class weed detection, employing two well-known object detection frameworks, namely FCOS and Faster-RCNN. Specifically, we evaluate a generalized student-teacher framework with an improved pseudo-label generation module to produce reliable pseudo-labels for the unlabeled data. To enhance generalization, an ensemble student network is employed to facilitate the training process. Experimental results show that the proposed approach is able to achieve approximately 76\\% and 96\\% detection accuracy as the supervised methods with only 10\\% of labeled data in CottenWeedDet3 and CottonWeedDet12, respectively. We offer access to the source code, contributing a valuable resource for ongoing semi-supervised learning research in weed detection and beyond.","sentences":["Effective weed control plays a crucial role in optimizing crop yield and enhancing agricultural product quality.","However, the reliance on herbicide application not only poses a critical threat to the environment but also promotes the emergence of resistant weeds.","Fortunately, recent advances in precision weed management enabled by ML and DL provide a sustainable alternative.","Despite great progress, existing algorithms are mainly developed based on supervised learning approaches, which typically demand large-scale datasets with manual-labeled annotations, which is time-consuming and labor-intensive.","As such, label-efficient learning methods, especially semi-supervised learning, have gained increased attention in the broader domain of computer vision and have demonstrated promising performance.","These methods aim to utilize a small number of labeled data samples along with a great number of unlabeled samples to develop high-performing models comparable to the supervised learning counterpart trained on a large amount of labeled data samples.","In this study, we assess the effectiveness of a semi-supervised learning framework for multi-class weed detection, employing two well-known object detection frameworks, namely FCOS and Faster-RCNN.","Specifically, we evaluate a generalized student-teacher framework with an improved pseudo-label generation module to produce reliable pseudo-labels for the unlabeled data.","To enhance generalization, an ensemble student network is employed to facilitate the training process.","Experimental results show that the proposed approach is able to achieve approximately 76\\% and 96\\% detection accuracy as the supervised methods with only 10\\% of labeled data in CottenWeedDet3 and CottonWeedDet12, respectively.","We offer access to the source code, contributing a valuable resource for ongoing semi-supervised learning research in weed detection and beyond."],"url":"http://arxiv.org/abs/2403.03390v1","category":"cs.CV"}
{"created":"2024-03-05 23:16:13","title":"Level Set Teleportation: An Optimization Perspective","abstract":"We study level set teleportation, an optimization sub-routine which seeks to accelerate gradient methods by maximizing the gradient norm on a level-set of the objective function. Since the descent lemma implies that gradient descent (GD) decreases the objective proportional to the squared norm of the gradient, level-set teleportation maximizes this one-step progress guarantee. For convex functions satisfying Hessian stability, we prove that GD with level-set teleportation obtains a combined sub-linear/linear convergence rate which is strictly faster than standard GD when the optimality gap is small. This is in sharp contrast to the standard (strongly) convex setting, where we show level-set teleportation neither improves nor worsens convergence rates. To evaluate teleportation in practice, we develop a projected-gradient-type method requiring only Hessian-vector products. We use this method to show that gradient methods with access to a teleportation oracle uniformly out-perform their standard versions on a variety of learning problems.","sentences":["We study level set teleportation, an optimization sub-routine which seeks to accelerate gradient methods by maximizing the gradient norm on a level-set of the objective function.","Since the descent lemma implies that gradient descent (GD) decreases the objective proportional to the squared norm of the gradient, level-set teleportation maximizes this one-step progress guarantee.","For convex functions satisfying Hessian stability, we prove that GD with level-set teleportation obtains a combined sub-linear/linear convergence rate which is strictly faster than standard GD when the optimality gap is small.","This is in sharp contrast to the standard (strongly) convex setting, where we show level-set teleportation neither improves nor worsens convergence rates.","To evaluate teleportation in practice, we develop a projected-gradient-type method requiring only Hessian-vector products.","We use this method to show that gradient methods with access to a teleportation oracle uniformly out-perform their standard versions on a variety of learning problems."],"url":"http://arxiv.org/abs/2403.03362v1","category":"cs.LG"}
{"created":"2024-03-05 21:38:05","title":"From virtual patients to digital twins in immuno-oncology: lessons learned from mechanistic quantitative systems pharmacology modeling","abstract":"Virtual patients and digital patients/twins are two similar concepts gaining increasing attention in health care with goals to accelerate drug development and improve patients' survival, but with their own limitations. Although methods have been proposed to generate virtual patient populations using mechanistic models, there are limited number of applications in immuno-oncology research. Furthermore, due to the stricter requirements of digital twins, they are often generated in a study-specific manner with models customized to particular clinical settings (e.g., treatment, cancer, and data types). Here, we discuss the challenges for virtual patient generation in immuno-oncology with our most recent experiences, initiatives to develop digital twins, and how research on these two concepts can inform each other.","sentences":["Virtual patients and digital patients/twins are two similar concepts gaining increasing attention in health care with goals to accelerate drug development and improve patients' survival, but with their own limitations.","Although methods have been proposed to generate virtual patient populations using mechanistic models, there are limited number of applications in immuno-oncology research.","Furthermore, due to the stricter requirements of digital twins, they are often generated in a study-specific manner with models customized to particular clinical settings (e.g., treatment, cancer, and data types).","Here, we discuss the challenges for virtual patient generation in immuno-oncology with our most recent experiences, initiatives to develop digital twins, and how research on these two concepts can inform each other."],"url":"http://arxiv.org/abs/2403.03335v1","category":"q-bio.OT"}
{"created":"2024-03-05 21:07:50","title":"AnatoMix: Anatomy-aware Data Augmentation for Multi-organ Segmentation","abstract":"Multi-organ segmentation in medical images is a widely researched task and can save much manual efforts of clinicians in daily routines. Automating the organ segmentation process using deep learning (DL) is a promising solution and state-of-the-art segmentation models are achieving promising accuracy. In this work, We proposed a novel data augmentation strategy for increasing the generalizibility of multi-organ segmentation datasets, namely AnatoMix. By object-level matching and manipulation, our method is able to generate new images with correct anatomy, i.e. organ segmentation mask, exponentially increasing the size of the segmentation dataset. Initial experiments have been done to investigate the segmentation performance influenced by our method on a public CT dataset. Our augmentation method can lead to mean dice of 76.1, compared with 74.8 of the baseline method.","sentences":["Multi-organ segmentation in medical images is a widely researched task and can save much manual efforts of clinicians in daily routines.","Automating the organ segmentation process using deep learning (DL) is a promising solution and state-of-the-art segmentation models are achieving promising accuracy.","In this work, We proposed a novel data augmentation strategy for increasing the generalizibility of multi-organ segmentation datasets, namely AnatoMix.","By object-level matching and manipulation, our method is able to generate new images with correct anatomy, i.e. organ segmentation mask, exponentially increasing the size of the segmentation dataset.","Initial experiments have been done to investigate the segmentation performance influenced by our method on a public CT dataset.","Our augmentation method can lead to mean dice of 76.1, compared with 74.8 of the baseline method."],"url":"http://arxiv.org/abs/2403.03326v1","category":"eess.IV"}
{"created":"2024-03-05 20:32:10","title":"Beyond the Dashboard: Investigating Distracted Driver Communication Preferences for ADAS","abstract":"Distracted driving is a major cause of road fatalities. With improvements in driver (in)attention detection, these distracted situations can be caught early to alert drivers and improve road safety and comfort. However, drivers may have differing preferences for the modes of such communication based on the driving scenario and their current distraction state. To this end, we present a user study (N=147) where videos of simulated driving scenarios were utilized to learn drivers preferences for modes of communication and their evolution with the drivers changing attention. The survey queried participants preferred modes of communication for scenarios such as collisions or stagnation at a green light. We validate our hypotheses and provide key results that inform the future of communication between drivers and their vehicles. We showcase the different driver preferences based on the nature of the driving scenario and also show that they evolve as the drivers distraction state changes.","sentences":["Distracted driving is a major cause of road fatalities.","With improvements in driver (in)attention detection, these distracted situations can be caught early to alert drivers and improve road safety and comfort.","However, drivers may have differing preferences for the modes of such communication based on the driving scenario and their current distraction state.","To this end, we present a user study (N=147) where videos of simulated driving scenarios were utilized to learn drivers preferences for modes of communication and their evolution with the drivers changing attention.","The survey queried participants preferred modes of communication for scenarios such as collisions or stagnation at a green light.","We validate our hypotheses and provide key results that inform the future of communication between drivers and their vehicles.","We showcase the different driver preferences based on the nature of the driving scenario and also show that they evolve as the drivers distraction state changes."],"url":"http://arxiv.org/abs/2403.03312v1","category":"cs.HC"}
{"created":"2024-03-05 20:12:05","title":"Book2Dial: Generating Teacher-Student Interactions from Textbooks for Cost-Effective Development of Educational Chatbots","abstract":"Educational chatbots are a promising tool for assisting student learning. However, the development of effective chatbots in education has been challenging, as high-quality data is seldom available in this domain. In this paper, we propose a framework for generating synthetic teacher-student interactions grounded in a set of textbooks. Our approaches capture one aspect of learning interactions where curious students with partial knowledge interactively ask a teacher questions about the material in the textbook. We highlight various quality criteria that such dialogues should fulfill and compare several approaches relying on either prompting or fine-tuning large language models. We use synthetic dialogues to train educational chatbots and show benefits of further fine-tuning in different educational domains. However, human evaluation shows that our best data synthesis method still suffers from hallucinations and tends to reiterate information from previous conversations. Our findings offer insights for future efforts in synthesizing conversational data that strikes a balance between size and quality. We will open-source our data and code.","sentences":["Educational chatbots are a promising tool for assisting student learning.","However, the development of effective chatbots in education has been challenging, as high-quality data is seldom available in this domain.","In this paper, we propose a framework for generating synthetic teacher-student interactions grounded in a set of textbooks.","Our approaches capture one aspect of learning interactions where curious students with partial knowledge interactively ask a teacher questions about the material in the textbook.","We highlight various quality criteria that such dialogues should fulfill and compare several approaches relying on either prompting or fine-tuning large language models.","We use synthetic dialogues to train educational chatbots and show benefits of further fine-tuning in different educational domains.","However, human evaluation shows that our best data synthesis method still suffers from hallucinations and tends to reiterate information from previous conversations.","Our findings offer insights for future efforts in synthesizing conversational data that strikes a balance between size and quality.","We will open-source our data and code."],"url":"http://arxiv.org/abs/2403.03307v1","category":"cs.CL"}
{"created":"2024-03-05 20:07:42","title":"Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data","abstract":"Document-Level Event Argument Extraction (DocEAE) is an extremely difficult information extraction problem -- with significant limitations in low-resource cross-domain settings. To address this problem, we introduce Mad Lib Aug (MLA), a novel generative DocEAE data augmentation framework. Our approach leverages the intuition that Mad Libs, which are categorically masked documents used as a part of a popular game, can be generated and solved by LLMs to produce data for DocEAE. Using MLA, we achieve a 2.6-point average improvement in overall F1 score. Moreover, this approach achieves a 3.9 and 5.2 point average increase in zero and few-shot event roles compared to augmentation-free baselines across all experiments.   To better facilitate analysis of cross-domain DocEAE, we additionally introduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to identify roles in the target domain which are semantic outliers with respect to roles observed in the source domain. Our experiments show that MLA augmentation can boost RDF1 performance by an average of 5.85 points compared to non-augmented datasets.","sentences":["Document-Level Event Argument Extraction (DocEAE) is an extremely difficult information extraction problem -- with significant limitations in low-resource cross-domain settings.","To address this problem, we introduce Mad Lib Aug (MLA), a novel generative DocEAE data augmentation framework.","Our approach leverages the intuition that Mad Libs, which are categorically masked documents used as a part of a popular game, can be generated and solved by LLMs to produce data for DocEAE.","Using MLA, we achieve a 2.6-point average improvement in overall F1 score.","Moreover, this approach achieves a 3.9 and 5.2 point average increase in zero and few-shot event roles compared to augmentation-free baselines across all experiments.   ","To better facilitate analysis of cross-domain DocEAE, we additionally introduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to identify roles in the target domain which are semantic outliers with respect to roles observed in the source domain.","Our experiments show that MLA augmentation can boost RDF1 performance by an average of 5.85 points compared to non-augmented datasets."],"url":"http://arxiv.org/abs/2403.03304v1","category":"cs.CL"}
{"created":"2024-03-06 15:54:54","title":"Resonant switching current detector based on underdamped Josephson junctions","abstract":"Current-biased Josephson junctions can act as detectors of electromagnetic radiation. At optimal conditions, their sensitivity is limited by fluctuations causing stochastic switching from the superconducting to the resistive state. This work provides a quantitative description of a stochastic switching current detector, based on an underdamped Josephson junction. It is shown that activation of a Josephson plasma resonance can greatly enhance the detector responsivity in proportion to the quality factor of the junction. The ways of tuning the detector for achieving optimal operation are discussed. For realistic parameters of Nb/AlOx/Nb tunnel junctions, the sensitivity and noise-equivalent power can reach values of $S\\simeq 2\\times10^{13}$ (V/W) and $NEP\\simeq 5\\times10^{-24}$ (WHz$^{-1/2}$), respectively. These outstanding characteristics facilitate both bolometric and single-photon detection in microwave and terahertz ranges.","sentences":["Current-biased Josephson junctions can act as detectors of electromagnetic radiation.","At optimal conditions, their sensitivity is limited by fluctuations causing stochastic switching from the superconducting to the resistive state.","This work provides a quantitative description of a stochastic switching current detector, based on an underdamped Josephson junction.","It is shown that activation of a Josephson plasma resonance can greatly enhance the detector responsivity in proportion to the quality factor of the junction.","The ways of tuning the detector for achieving optimal operation are discussed.","For realistic parameters of Nb/AlOx/Nb tunnel junctions, the sensitivity and noise-equivalent power can reach values of $S\\simeq 2\\times10^{13}$ (V/W) and $NEP\\simeq 5\\times10^{-24}$ (WHz$^{-1/2}$), respectively.","These outstanding characteristics facilitate both bolometric and single-photon detection in microwave and terahertz ranges."],"url":"http://arxiv.org/abs/2403.03803v1","category":"cond-mat.supr-con"}
{"created":"2024-03-06 08:41:41","title":"Split Covariance Intersection with Correlated Components for Distributed Estimation","abstract":"This paper introduces a new conservative fusion method to exploit the correlated components within the estimation errors. Fusion is the process of combining multiple estimates of a given state to produce a new estimate with a smaller MSE. To perform the optimal linear fusion, the (centralized) covariance associated with the errors of all estimates is required. If it is partially unknown, the optimal fusion cannot be computed. Instead, a solution is to perform a conservative fusion. A conservative fusion provides a gain and a bound on the resulting MSE matrix which guarantees that the error is not underestimated. A well-known conservative fusion is the Covariance Intersection fusion. It has been modified to exploit the uncorrelated components within the errors. In this paper, it is further extended to exploit the correlated components as well. The resulting fusion is integrated into standard distributed algorithms where it allows exploiting the process noise observed by all agents. The improvement is confirmed by simulations.","sentences":["This paper introduces a new conservative fusion method to exploit the correlated components within the estimation errors.","Fusion is the process of combining multiple estimates of a given state to produce a new estimate with a smaller MSE.","To perform the optimal linear fusion, the (centralized) covariance associated with the errors of all estimates is required.","If it is partially unknown, the optimal fusion cannot be computed.","Instead, a solution is to perform a conservative fusion.","A conservative fusion provides a gain and a bound on the resulting MSE matrix which guarantees that the error is not underestimated.","A well-known conservative fusion is the Covariance Intersection fusion.","It has been modified to exploit the uncorrelated components within the errors.","In this paper, it is further extended to exploit the correlated components as well.","The resulting fusion is integrated into standard distributed algorithms where it allows exploiting the process noise observed by all agents.","The improvement is confirmed by simulations."],"url":"http://arxiv.org/abs/2403.03543v1","category":"eess.SP"}
{"created":"2024-03-06 07:36:27","title":"Self-sustained optomechanical state destruction triggered by the Kerr nonlinearity","abstract":"Cavity optomechanics implements a unique platform where moving objects can be probed by quantum fields, either laser light or microwave signals. With a pump tone driving at a frequency above the cavity resonance, self-sustained oscillations can be triggered at large injected powers. These limit cycle dynamics are particularly rich, presenting hysteretic behaviours, broad comb signals and especially large motion amplitudes. All of these features can be exploited for both fundamental quantum research and engineering. Here we present low temperature microwave experiments performed on a high-Q cavity resonance capacitively coupled to the flexure of a beam resonator. We study the limit cycle dynamics phase space as a function of pump parameters (detuning, power). Unexpectedly, we find that in a region of this phase space the microwave resonance is irremediably destroyed: only a dramatic power-reset can restore the dynamics to its original state. The phenomenon can be understood as an optical instability linked to the Kerr nonlinearity of the cavity. A theory supporting this claim is presented, reproducing almost quantitatively the measurement. This remarkable feature might be further optimized and represents a new resource for quantum microwave circuits.","sentences":["Cavity optomechanics implements a unique platform where moving objects can be probed by quantum fields, either laser light or microwave signals.","With a pump tone driving at a frequency above the cavity resonance, self-sustained oscillations can be triggered at large injected powers.","These limit cycle dynamics are particularly rich, presenting hysteretic behaviours, broad comb signals and especially large motion amplitudes.","All of these features can be exploited for both fundamental quantum research and engineering.","Here we present low temperature microwave experiments performed on a high-Q cavity resonance capacitively coupled to the flexure of a beam resonator.","We study the limit cycle dynamics phase space as a function of pump parameters (detuning, power).","Unexpectedly, we find that in a region of this phase space the microwave resonance is irremediably destroyed: only a dramatic power-reset can restore the dynamics to its original state.","The phenomenon can be understood as an optical instability linked to the Kerr nonlinearity of the cavity.","A theory supporting this claim is presented, reproducing almost quantitatively the measurement.","This remarkable feature might be further optimized and represents a new resource for quantum microwave circuits."],"url":"http://arxiv.org/abs/2403.03509v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-06 05:42:38","title":"Chemically Tailored Growth of 2D Semiconductors via Hybrid Metal-Organic Chemical Vapor Deposition","abstract":"Two-dimensional (2D) semiconducting transition-metal dichalcogenides (TMDCs) are an exciting platform for new excitonic physics and next-generation electronics, creating a strong demand to understand their growth, doping, and heterostructures. Despite significant progress in solid-source (SS-) and metal-organic chemical vapor deposition (MOCVD), further optimization is necessary to grow highly crystalline 2D TMDCs with controlled doping. Here, we report a hybrid MOCVD growth method that combines liquid-phase metal precursor deposition and vapor-phase organo-chalcogen delivery to leverage the advantages of both MOCVD and SS-CVD. Using our hybrid approach, we demonstrate WS$_2$ growth with tunable morphologies - from separated single-crystal domains to continuous monolayer films - on a variety of substrates, including sapphire, SiO$_2$, and Au. These WS$_2$ films exhibit narrow neutral exciton photoluminescence linewidths down to 33 meV and room-temperature mobility up to 34 - 36 cm$^2$V$^-$$^1$s$^-$$^1$). Through simple modifications to the liquid precursor composition, we demonstrate the growth of V-doped WS$_2$, MoxW$_1$$_-$$_x$S$_2$ alloys, and in-plane WS$_2$-MoS$_2$ heterostructures. This work presents an efficient approach for addressing a variety of TMDC synthesis needs on a laboratory scale.","sentences":["Two-dimensional (2D) semiconducting transition-metal dichalcogenides (TMDCs) are an exciting platform for new excitonic physics and next-generation electronics, creating a strong demand to understand their growth, doping, and heterostructures.","Despite significant progress in solid-source (SS-) and metal-organic chemical vapor deposition (MOCVD), further optimization is necessary to grow highly crystalline 2D TMDCs with controlled doping.","Here, we report a hybrid MOCVD growth method that combines liquid-phase metal precursor deposition and vapor-phase organo-chalcogen delivery to leverage the advantages of both MOCVD and SS-CVD.","Using our hybrid approach, we demonstrate WS$_2$ growth with tunable morphologies - from separated single-crystal domains to continuous monolayer films - on a variety of substrates, including sapphire, SiO$_2$, and Au.","These WS$_2$ films exhibit narrow neutral exciton photoluminescence linewidths down to 33 meV and room-temperature mobility up to 34 - 36 cm$^2$V$^-$$^1$s$^-$$^1$).","Through simple modifications to the liquid precursor composition, we demonstrate the growth of V-doped WS$_2$, MoxW$_1$$_-$$_x$S$_2$ alloys, and in-plane WS$_2$-MoS$_2$ heterostructures.","This work presents an efficient approach for addressing a variety of TMDC synthesis needs on a laboratory scale."],"url":"http://arxiv.org/abs/2403.03482v1","category":"physics.app-ph"}
{"created":"2024-03-06 03:49:59","title":"Rabi resonance splitting phenomena in photonic integrated circuits","abstract":"Realizing optical analogues of quantum phenomena in atomic, molecular, or condensed matter physics has underpinned a range of photonic technologies. Rabi splitting is a quantum phenomenon induced by a strong interaction between two quantum states, and its optical analogues are of fundamental importance for the manipulation of light-matter interactions with wide applications in optoelectronics and nonlinear optics. Here, we propose and theoretically investigate purely optical analogues of Rabi splitting in integrated waveguide-coupled resonators formed by two Sagnac interferometers. By tailoring the coherent mode interference, the spectral response of the devices is engineered to achieve optical analogues of Rabi splitting with anti-crossing behavior in the resonances. Transitions between the Lorentzian, Fano, and Rabi splitting spectral lineshapes are achieved by simply changing the phase shift along the waveguide connecting the two Sagnac interferometers, revealing interesting physical insights about the evolution of different optical analogues of quantum phenomena. The impact of the device structural parameters is also analyzed to facilitate device design and optimization. These results suggest a new way for realizing optical analogues of Rabi splitting based on integrated waveguide-coupled resonators, paving the way for many potential applications that manipulate light-matter interactions in the strong coupling regime.","sentences":["Realizing optical analogues of quantum phenomena in atomic, molecular, or condensed matter physics has underpinned a range of photonic technologies.","Rabi splitting is a quantum phenomenon induced by a strong interaction between two quantum states, and its optical analogues are of fundamental importance for the manipulation of light-matter interactions with wide applications in optoelectronics and nonlinear optics.","Here, we propose and theoretically investigate purely optical analogues of Rabi splitting in integrated waveguide-coupled resonators formed by two Sagnac interferometers.","By tailoring the coherent mode interference, the spectral response of the devices is engineered to achieve optical analogues of Rabi splitting with anti-crossing behavior in the resonances.","Transitions between the Lorentzian, Fano, and Rabi splitting spectral lineshapes are achieved by simply changing the phase shift along the waveguide connecting the two Sagnac interferometers, revealing interesting physical insights about the evolution of different optical analogues of quantum phenomena.","The impact of the device structural parameters is also analyzed to facilitate device design and optimization.","These results suggest a new way for realizing optical analogues of Rabi splitting based on integrated waveguide-coupled resonators, paving the way for many potential applications that manipulate light-matter interactions in the strong coupling regime."],"url":"http://arxiv.org/abs/2403.03438v1","category":"physics.optics"}
{"created":"2024-03-06 03:16:18","title":"Combined optimization ghost imaging based on random speckle field","abstract":"Ghost imaging is a non local imaging technology, which can obtain target information by measuring the second-order intensity correlation between the reference light field and the target detection light field. However, the current imaging environment requires a large number of measurement data, and the imaging results also have the problems of low image resolution and long reconstruction time. Therefore, using orthogonal methods such as QR decomposition, a variety of optimization methods for speckle patterns are designed combined with Kronecker product,which can help to shorten the imaging time, improve the imaging quality and image noise resistance.","sentences":["Ghost imaging is a non local imaging technology, which can obtain target information by measuring the second-order intensity correlation between the reference light field and the target detection light field.","However, the current imaging environment requires a large number of measurement data, and the imaging results also have the problems of low image resolution and long reconstruction time.","Therefore, using orthogonal methods such as QR decomposition, a variety of optimization methods for speckle patterns are designed combined with Kronecker product,which can help to shorten the imaging time, improve the imaging quality and image noise resistance."],"url":"http://arxiv.org/abs/2403.03426v1","category":"physics.optics"}
{"created":"2024-03-05 23:06:34","title":"Bridge the Future: High-Performance Networks in Confidential VMs without Trusted I/O devices","abstract":"Trusted I/O (TIO) is an appealing solution to improve I/O performance for confidential VMs (CVMs), with the potential to eliminate broad sources of I/O overhead. However, this paper emphasizes that not all types of I/O can derive substantial benefits from TIO, particularly network I/O. Given the obligatory use of encryption protocols for network traffic in CVM's threat model, TIO's approach of I/O encryption over the PCIe bus becomes redundant. Furthermore, TIO solutions need to expand the Trusted Computing Base (TCB) to include TIO devices and are commercially unavailable.   Motivated by these insights, the goal of this paper is to propose a software solution that helps CVMs immediately benefit from high-performance networks, while confining trust only to the on-chip CVM. We present FOLIO, a software solution crafted from a secure and efficient Data Plane Development Kit (DPDK) extension compatible with the latest version of AMD Secure Encrypted Virtualization (SEV), a.k.a., Secure Nested Paging (SNP). Our design is informed by a thorough analysis of all possible factors that impact SNP VM's network performance. By extensively removing overhead sources, we arrive at a design that approaches the efficiency of an optimal TIO-based configuration. Evaluation shows that FOLIO has a performance dip less than 6% relative to the optimal TIO configuration, while only relying on off-the-shelf CPUs.","sentences":["Trusted I/O (TIO) is an appealing solution to improve I/O performance for confidential VMs (CVMs), with the potential to eliminate broad sources of I/O overhead.","However, this paper emphasizes that not all types of I/O can derive substantial benefits from TIO, particularly network I/O.","Given the obligatory use of encryption protocols for network traffic in CVM's threat model, TIO's approach of I/O encryption over the PCIe bus becomes redundant.","Furthermore, TIO solutions need to expand the Trusted Computing Base (TCB) to include TIO devices and are commercially unavailable.   ","Motivated by these insights, the goal of this paper is to propose a software solution that helps CVMs immediately benefit from high-performance networks, while confining trust only to the on-chip CVM.","We present FOLIO, a software solution crafted from a secure and efficient Data Plane Development Kit (DPDK) extension compatible with the latest version of AMD Secure Encrypted Virtualization (SEV), a.k.a., Secure Nested Paging (SNP).","Our design is informed by a thorough analysis of all possible factors that impact SNP VM's network performance.","By extensively removing overhead sources, we arrive at a design that approaches the efficiency of an optimal TIO-based configuration.","Evaluation shows that FOLIO has a performance dip less than 6% relative to the optimal TIO configuration, while only relying on off-the-shelf CPUs."],"url":"http://arxiv.org/abs/2403.03360v1","category":"cs.CR"}
{"created":"2024-03-05 22:24:30","title":"Quantum Dynamical Emulation","abstract":"We introduce the concept of \\textit{Quantum Dynamical Emulation}, a constructive method for mapping the solutions of non-unitary dynamics to a weighted set of unitary operations. This allows us to derive a new correspondence between real and imaginary time, which we term Imaginary Time Quantum Dynamical Emulation (ITQDE). This enables an imaginary time evolution to be constructed from the overlaps of states evolved in opposite directions in real time. We show that a single trajectory evolved using ITQDE can be used not only to infer ground and thermal states, but also to resolve information about the complete Hamiltonian spectrum. We further employ ITQDE to derive novel thermodynamic results, including a generalisation of the Hubbard-Stratonovich transform. We go on to develop a quantum algorithm for computing the spectra of quantum systems that is based on this premise. We demonstrate the utility of this method through numerical simulation, as well as quantum hardware implementations.","sentences":["We introduce the concept of \\textit{Quantum Dynamical Emulation}, a constructive method for mapping the solutions of non-unitary dynamics to a weighted set of unitary operations.","This allows us to derive a new correspondence between real and imaginary time, which we term Imaginary Time Quantum Dynamical Emulation (ITQDE).","This enables an imaginary time evolution to be constructed from the overlaps of states evolved in opposite directions in real time.","We show that a single trajectory evolved using ITQDE can be used not only to infer ground and thermal states, but also to resolve information about the complete Hamiltonian spectrum.","We further employ ITQDE to derive novel thermodynamic results, including a generalisation of the Hubbard-Stratonovich transform.","We go on to develop a quantum algorithm for computing the spectra of quantum systems that is based on this premise.","We demonstrate the utility of this method through numerical simulation, as well as quantum hardware implementations."],"url":"http://arxiv.org/abs/2403.03350v1","category":"quant-ph"}
{"created":"2024-03-05 22:01:14","title":"Stabilization of a Nonholonomic Car Model with Off-Hooked Trailers","abstract":"We consider a kinematic model of a controlled car with two trailers by assuming that each trailer is attached at some distance from the preceding axle (\"off-hooked trailers\"). For this model, we derive the transformation towards privileged coordinates and present the corresponding nilpotent quasihomogeneous approximate system. The components of this nilpotent approximation are written explicitly in terms of mechanical parameters of the original system. The constructed system does not satisfy the Brockett necessary stabilizability condition, and the design of time-varying feedback controllers with oscillating components is proposed. It is proved that these controllers ensure the exponential convergence of solutions to the trivial equilibrium, and simulation results are presented to illustrate the behavior of the closed-loop system.","sentences":["We consider a kinematic model of a controlled car with two trailers by assuming that each trailer is attached at some distance from the preceding axle (\"off-hooked trailers\").","For this model, we derive the transformation towards privileged coordinates and present the corresponding nilpotent quasihomogeneous approximate system.","The components of this nilpotent approximation are written explicitly in terms of mechanical parameters of the original system.","The constructed system does not satisfy the Brockett necessary stabilizability condition, and the design of time-varying feedback controllers with oscillating components is proposed.","It is proved that these controllers ensure the exponential convergence of solutions to the trivial equilibrium, and simulation results are presented to illustrate the behavior of the closed-loop system."],"url":"http://arxiv.org/abs/2403.03341v1","category":"math.OC"}
{"created":"2024-03-05 21:29:16","title":"Physical Limits on Raman Scattering: The Critical Role of Pump and Signal Co-design","abstract":"We present a rigorous method for deriving limits on Raman scattering in structured media. We exploit this framework to constrain the maximum Raman signal resulting from a planewave incident on two experimentally relevant systems, consisting of either a single Raman molecule in the vicinity of a structured medium or a designable Raman medium (a distribution of Raman molecules). Results pertaining to metallic and dielectric structures illustrate the efficacy of structural optimization and the importance of accounting and co-designing for the nonlinear interplay between pump and signal fields. In particular, we show that treating the pump-focusing and signal-extraction processes separately, as has been done in prior works, leads to highly unrealistic predictions of achievable enhancements. The formulation could readily find applications in guiding further improvements on surface-enhanced Raman scattering (SERS) sensitivity and Raman-assisted lasing.","sentences":["We present a rigorous method for deriving limits on Raman scattering in structured media.","We exploit this framework to constrain the maximum Raman signal resulting from a planewave incident on two experimentally relevant systems, consisting of either a single Raman molecule in the vicinity of a structured medium or a designable Raman medium (a distribution of Raman molecules).","Results pertaining to metallic and dielectric structures illustrate the efficacy of structural optimization and the importance of accounting and co-designing for the nonlinear interplay between pump and signal fields.","In particular, we show that treating the pump-focusing and signal-extraction processes separately, as has been done in prior works, leads to highly unrealistic predictions of achievable enhancements.","The formulation could readily find applications in guiding further improvements on surface-enhanced Raman scattering (SERS) sensitivity and Raman-assisted lasing."],"url":"http://arxiv.org/abs/2403.03332v1","category":"physics.optics"}
{"created":"2024-03-05 21:28:03","title":"Verification of First-Order Methods for Parametric Quadratic Optimization","abstract":"We introduce a numerical framework to verify the finite step convergence of first-order methods for parametric convex quadratic optimization. We formulate the verification problem as a mathematical optimization problem where we maximize a performance metric (e.g., fixed-point residual at the last iteration) subject to constraints representing proximal algorithm steps (e.g., linear system solutions, projections, or gradient steps). Our framework is highly modular because we encode a wide range of proximal algorithms as variations of two primitive steps: affine steps and element-wise maximum steps. Compared to standard convergence analysis and performance estimation techniques, we can explicitly quantify the effects of warm-starting by directly representing the sets where the initial iterates and parameters live. We show that the verification problem is NP-hard, and we construct strong semidefinite programming relaxations using various constraint tightening techniques. Numerical examples in nonnegative least squares, network utility maximization, Lasso, and optimal control show a significant reduction in pessimism of our framework compared to standard worst-case convergence analysis techniques.","sentences":["We introduce a numerical framework to verify the finite step convergence of first-order methods for parametric convex quadratic optimization.","We formulate the verification problem as a mathematical optimization problem where we maximize a performance metric (e.g., fixed-point residual at the last iteration) subject to constraints representing proximal algorithm steps (e.g., linear system solutions, projections, or gradient steps).","Our framework is highly modular because we encode a wide range of proximal algorithms as variations of two primitive steps: affine steps and element-wise maximum steps.","Compared to standard convergence analysis and performance estimation techniques, we can explicitly quantify the effects of warm-starting by directly representing the sets where the initial iterates and parameters live.","We show that the verification problem is NP-hard, and we construct strong semidefinite programming relaxations using various constraint tightening techniques.","Numerical examples in nonnegative least squares, network utility maximization, Lasso, and optimal control show a significant reduction in pessimism of our framework compared to standard worst-case convergence analysis techniques."],"url":"http://arxiv.org/abs/2403.03331v1","category":"math.OC"}
{"created":"2024-03-05 20:43:07","title":"Environmentally driven symmetry-breaking quenches dual fluorescence in proflavine","abstract":"Nonadiabatic couplings between several electronic excited states are ubiquitous in many organic chromophores and can significantly influence optical properties. A recent experimental study demonstrated that the proflavine molecule exhibits surprising dual fluorescence in the gas phase, that is suppressed in polar solvent environments. Here, we uncover the origin of this phenomenon by parameterizing a linear-vibronic coupling (LVC) Hamiltonian from spectral densities of system-bath coupling constructed along molecular dynamics trajectories, fully accounting for interactions with the condensed-phase environment. The finite-temperature absorption, steady-stat emission, and time-resolved emission spectra are then computed using powerful, numerically exact tensor network approaches. We find that the dual fluorescence in vacuum is driven by a single well-defined coupling mode, but is quenched in solution due to dynamic solvent-driven symmetry-breaking that mixes the two low-lying electronic states. We expect the computational framework developed here to be widely applicable to the study of non-Condon effects in complex condensed-phase environments.","sentences":["Nonadiabatic couplings between several electronic excited states are ubiquitous in many organic chromophores and can significantly influence optical properties.","A recent experimental study demonstrated that the proflavine molecule exhibits surprising dual fluorescence in the gas phase, that is suppressed in polar solvent environments.","Here, we uncover the origin of this phenomenon by parameterizing a linear-vibronic coupling (LVC) Hamiltonian from spectral densities of system-bath coupling constructed along molecular dynamics trajectories, fully accounting for interactions with the condensed-phase environment.","The finite-temperature absorption, steady-stat emission, and time-resolved emission spectra are then computed using powerful, numerically exact tensor network approaches.","We find that the dual fluorescence in vacuum is driven by a single well-defined coupling mode, but is quenched in solution due to dynamic solvent-driven symmetry-breaking that mixes the two low-lying electronic states.","We expect the computational framework developed here to be widely applicable to the study of non-Condon effects in complex condensed-phase environments."],"url":"http://arxiv.org/abs/2403.03316v1","category":"physics.chem-ph"}
{"created":"2024-03-05 20:06:35","title":"Understanding the Interlayer Coupling in 1T/1H-NbSe$_2$ Hetero-Bilayers","abstract":"The properties of 2D materials are strongly influenced by their substrate, leading to a variety of \"proximity effects\" like screening, charge transfer, and hybridization. Surprisingly, there is a dearth of theoretical studies on these effects. Particularly, previous theoretical research on the Star of David (SOD) structure in 1T-NbSe$_2$ has focused on single-layer configurations or stacking with the same 1T phase without any real substrate. Here, we depart from these approaches and explore how these proximity effects shape the electronic and magnetic properties of the 1T-NbSe$_2$ phase when it is grown on the metallic 1H-NbSe$_2$ substrate. Using Density Functional Calculations, we establish a common framework to define the key characteristics of both free-standning 1T-NbSe$_2$ and 1H-NbSe$_2$. We then identify the optimal stacking arrangement for these two layers, revealing a transfer from the 1T to the 1H phase and a reorganization of charge within each layer. Our findings indicate that the magnetic moment of the SOD structure is still robust; however, is diminished due to a reduction in the on-site Coulomb interaction of the Hubbard bands. Additionally, the interlayer coupling induces metallicity in the 1T phase and increases the decoupling of the lower Hubbard band from the valence band.","sentences":["The properties of 2D materials are strongly influenced by their substrate, leading to a variety of \"proximity effects\" like screening, charge transfer, and hybridization.","Surprisingly, there is a dearth of theoretical studies on these effects.","Particularly, previous theoretical research on the Star of David (SOD) structure in 1T-NbSe$_2$ has focused on single-layer configurations or stacking with the same 1T phase without any real substrate.","Here, we depart from these approaches and explore how these proximity effects shape the electronic and magnetic properties of the 1T-NbSe$_2$ phase when it is grown on the metallic 1H-NbSe$_2$ substrate.","Using Density Functional Calculations, we establish a common framework to define the key characteristics of both free-standning 1T-NbSe$_2$ and","1H-NbSe$_2$. We then identify the optimal stacking arrangement for these two layers, revealing a transfer from the 1T to the 1H phase and a reorganization of charge within each layer.","Our findings indicate that the magnetic moment of the SOD structure is still robust; however, is diminished due to a reduction in the on-site Coulomb interaction of the Hubbard bands.","Additionally, the interlayer coupling induces metallicity in the 1T phase and increases the decoupling of the lower Hubbard band from the valence band."],"url":"http://arxiv.org/abs/2403.03302v1","category":"cond-mat.str-el"}
{"created":"2024-03-05 19:29:40","title":"First Principles Validation of Energy Barriers in Ni$_{75}$Al$_{25}$","abstract":"Precipitates in Nickel-based superalloys form during heat treatment on a time scale inaccessible to direct molecular dynamics simulation, but could be studied using kinetic Monte Carlo (KMC). This requires reliable values for the barrier energies separating distinct configurations over the trajectory of the system. In this study, we validate vacancy migration barriers found with the Activation-Relaxation Technique nouveau (ARTn) method in partially ordered Ni$_{75}$Al$_{25}$ with a monovacancy using published potentials for the atomic interactions against first-principles methods. In a first step, we confirm that the ARTn barrier energies agree with those determined with the nudged elastic band (NEB) method. As the number of atoms used in those calculations is too great for direct ab initio calculations, we then cut the cell size to 255 atoms, thus controlling finite size effects. We then use the plane-wave density functional theory (DFT) code CASTEP and its inbuilt NEB method in the smaller cells. This provides us with a continuous validation chain from first principles to kinetic Monte Carlo simulations with interatomic potentials. We then evaluate the barrier energies of five further interatomic potentials with NEB, demonstrating that none yields these with sufficient reliability for KMC simulations, with some of them failing completely. This is a first step towards quantifying the errors incurred in KMC simulations of precipitate formation and evolution.","sentences":["Precipitates in Nickel-based superalloys form during heat treatment on a time scale inaccessible to direct molecular dynamics simulation, but could be studied using kinetic Monte Carlo (KMC).","This requires reliable values for the barrier energies separating distinct configurations over the trajectory of the system.","In this study, we validate vacancy migration barriers found with the Activation-Relaxation Technique nouveau (ARTn) method in partially ordered Ni$_{75}$Al$_{25}$ with a monovacancy using published potentials for the atomic interactions against first-principles methods.","In a first step, we confirm that the ARTn barrier energies agree with those determined with the nudged elastic band (NEB) method.","As the number of atoms used in those calculations is too great for direct ab initio calculations, we then cut the cell size to 255 atoms, thus controlling finite size effects.","We then use the plane-wave density functional theory (DFT) code CASTEP and its inbuilt NEB method in the smaller cells.","This provides us with a continuous validation chain from first principles to kinetic Monte Carlo simulations with interatomic potentials.","We then evaluate the barrier energies of five further interatomic potentials with NEB, demonstrating that none yields these with sufficient reliability for KMC simulations, with some of them failing completely.","This is a first step towards quantifying the errors incurred in KMC simulations of precipitate formation and evolution."],"url":"http://arxiv.org/abs/2403.03282v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-05 19:00:01","title":"Isles of regularity in a sea of chaos amid the gravitational three-body problem","abstract":"The three-body problem (3BP) poses a longstanding challenge in physics and celestial mechanics. Despite the impossibility of obtaining general analytical solutions, statistical theories have been developed based on the ergodic principle. This assumption is justified by chaos, which is expected to fully mix the accessible phase space of the 3BP. This study probes the presence of regular (i.e. non chaotic) trajectories within the 3BP and assesses their impact on statistical escape theories. Using numerical simulations, we establish criteria for identifying regular trajectories and analyse their impact on statistical outcomes. Our analysis reveals that regular trajectories occupy up to 32% of the phase space, and their outcomes defy the predictions of statistical escape theories. The coexistence of regular and chaotic regions at all scales is characterized by a multi-fractal behaviour. Integration errors manifest as numerical chaos, artificially enhancing the mixing of the phase space and affecting the reliability of individual simulations, yet preserving the statistical correctness of an ensemble of realizations. Our findings underscore the challenges in applying statistical escape theories to astrophysical problems, as they may bias results by excluding the outcome of regular trajectories. This is particularly important in the context of formation scenarios of gravitational wave mergers, where biased estimates of binary eccentricity can have significant consequences.","sentences":["The three-body problem (3BP) poses a longstanding challenge in physics and celestial mechanics.","Despite the impossibility of obtaining general analytical solutions, statistical theories have been developed based on the ergodic principle.","This assumption is justified by chaos, which is expected to fully mix the accessible phase space of the 3BP.","This study probes the presence of regular (i.e. non chaotic) trajectories within the 3BP and assesses their impact on statistical escape theories.","Using numerical simulations, we establish criteria for identifying regular trajectories and analyse their impact on statistical outcomes.","Our analysis reveals that regular trajectories occupy up to 32% of the phase space, and their outcomes defy the predictions of statistical escape theories.","The coexistence of regular and chaotic regions at all scales is characterized by a multi-fractal behaviour.","Integration errors manifest as numerical chaos, artificially enhancing the mixing of the phase space and affecting the reliability of individual simulations, yet preserving the statistical correctness of an ensemble of realizations.","Our findings underscore the challenges in applying statistical escape theories to astrophysical problems, as they may bias results by excluding the outcome of regular trajectories.","This is particularly important in the context of formation scenarios of gravitational wave mergers, where biased estimates of binary eccentricity can have significant consequences."],"url":"http://arxiv.org/abs/2403.03247v1","category":"astro-ph.EP"}
{"created":"2024-03-06 18:58:09","title":"Backfiring Bosonisation","abstract":"For a fermionic quantum field theory in $d=1+1$ dimensions, there is a subtle difference between summing over spin structures and gauging $(-1)^F$. If the gravitational anomaly vanishes mod 16, then both operations are equivalent and yield a bosonic theory. But if the gravitational anomaly only vanishes mod 8, then only gauging $(-1)^F$ is allowed, and the result is a fermionic theory. Our goal is to understand in detail how this happens, despite the fact $(-1)^F$ is defined in terms of shifting the spin structure, which would na\\\"ively suggest that both operations are equivalent. We do this from three perspectives: an abstract view in terms of anomalies, explicit CFT calculations, and a Symmetry TFT perspective. To conclude, we illustrate our results using the heterotic string and the famous self-triality of 8 Majorana-Weyl fermions.","sentences":["For a fermionic quantum field theory in $d=1+1$ dimensions, there is a subtle difference between summing over spin structures and gauging $(-1)^F$.","If the gravitational anomaly vanishes mod 16, then both operations are equivalent and yield a bosonic theory.","But if the gravitational anomaly only vanishes mod 8, then only gauging $(-1)^F$ is allowed, and the result is a fermionic theory.","Our goal is to understand in detail how this happens, despite the fact $(-1)^F$ is defined in terms of shifting the spin structure, which would na\\\"ively suggest that both operations are equivalent.","We do this from three perspectives: an abstract view in terms of anomalies, explicit CFT calculations, and a Symmetry TFT perspective.","To conclude, we illustrate our results using the heterotic string and the famous self-triality of 8 Majorana-Weyl fermions."],"url":"http://arxiv.org/abs/2403.03953v1","category":"hep-th"}
{"created":"2024-03-06 18:54:27","title":"Estimating the household secondary attack rate with the Incomplete Chain Binomial model","abstract":"The Secondary Attack Rate (SAR) is a measure of how infectious a communicable disease is, and is often estimated based on studies of disease transmission in households. The Chain Binomial model is a simple model for disease outbreaks, and the final size distribution derived from it can be used to estimate the SAR using simple summary statistics. The final size distribution of the Chain Binomial model assume that the outbreaks have concluded, which in some instances may require long follow-up time. We develop a way to compute the probability distribution of the number of infected before the outbreak has concluded, which we call the Incomplete Chain Binomial distribution. We study a few theoretical properties of the model. We develop Maximum Likelihood estimation routines for inference on the SAR and explore the model by analyzing two real world data sets.","sentences":["The Secondary Attack Rate (SAR) is a measure of how infectious a communicable disease is, and is often estimated based on studies of disease transmission in households.","The Chain Binomial model is a simple model for disease outbreaks, and the final size distribution derived from it can be used to estimate the SAR using simple summary statistics.","The final size distribution of the Chain Binomial model assume that the outbreaks have concluded, which in some instances may require long follow-up time.","We develop a way to compute the probability distribution of the number of infected before the outbreak has concluded, which we call the Incomplete Chain Binomial distribution.","We study a few theoretical properties of the model.","We develop Maximum Likelihood estimation routines for inference on the SAR and explore the model by analyzing two real world data sets."],"url":"http://arxiv.org/abs/2403.03948v1","category":"stat.ME"}
{"created":"2024-03-06 18:40:56","title":"Holographic thermal entropy from geodesic bit threads","abstract":"The holographic bit threads are an insightful tool to investigate the holographic entanglement entropy and other quantities related to the bipartite entanglement in AdS/CFT. We mainly explore the geodesic bit threads in various static backgrounds, for the bipartitions characterized by either a sphere or an infinite strip. In pure AdS and for the sphere, the geodesic bit threads provide a gravitational dual of the map implementing the geometric action of the modular conjugation in the dual CFT. In Schwarzschild AdS black brane and for the sphere, our numerical analysis shows that the flux of the geodesic bit threads through the horizon gives the holographic thermal entropy of the sphere. This feature is not observed when the subsystem is an infinite strip, whenever we can construct the corresponding bit threads. The bit threads are also determined by the global structure of the gravitational background; indeed, for instance, we show that the geodesic bit threads of an arc in the BTZ black hole cannot be constructed.","sentences":["The holographic bit threads are an insightful tool to investigate the holographic entanglement entropy and other quantities related to the bipartite entanglement in AdS/CFT.","We mainly explore the geodesic bit threads in various static backgrounds, for the bipartitions characterized by either a sphere or an infinite strip.","In pure AdS and for the sphere, the geodesic bit threads provide a gravitational dual of the map implementing the geometric action of the modular conjugation in the dual CFT.","In Schwarzschild AdS black brane and for the sphere, our numerical analysis shows that the flux of the geodesic bit threads through the horizon gives the holographic thermal entropy of the sphere.","This feature is not observed when the subsystem is an infinite strip, whenever we can construct the corresponding bit threads.","The bit threads are also determined by the global structure of the gravitational background; indeed, for instance, we show that the geodesic bit threads of an arc in the BTZ black hole cannot be constructed."],"url":"http://arxiv.org/abs/2403.03930v1","category":"hep-th"}
{"created":"2024-03-06 18:27:20","title":"Recent Developments within The Cosmic Ray Extremely Distributed Observatory (CREDO)","abstract":"This contribution presents the recent research developments within the Cosmic Ray Extremely Distributed Observatory (CREDO) in the search for resolution of various scientific puzzles, ranging from fundamental physical questions to applications like the determination of earthquake precursors. The state-of-the art theoretical, numerical and computational aspects of these phenomena are addressed, as well as recent experimental developments for detection.","sentences":["This contribution presents the recent research developments within the Cosmic Ray Extremely Distributed Observatory (CREDO) in the search for resolution of various scientific puzzles, ranging from fundamental physical questions to applications like the determination of earthquake precursors.","The state-of-the art theoretical, numerical and computational aspects of these phenomena are addressed, as well as recent experimental developments for detection."],"url":"http://arxiv.org/abs/2403.03916v1","category":"astro-ph.HE"}
{"created":"2024-03-06 17:10:05","title":"Cosmological forecasts from the combination of Stage-IV photometric galaxy surveys and the magnification from forthcoming GW observatories","abstract":"In this work we have investigated the synergy between Stage-IV galaxy surveys and future GW observatories for constraining the underlying cosmological model of the Universe, focussing on photometric galaxy clustering, cosmic shear and GW magnification as cosmological probes. We have implemented a Fisher matrix approach for the evaluation of the full $6\\times2$pt statistics composed by the angular power spectra of the single probes together with their combination. For our analysis, we have in particular considered dynamical dark energy and massive neutrino scenarios. We have found that the improvement to galaxy survey performance is below 1\\%, in the case $\\ell^{\\rm GW}_{\\rm max}=100$ and a luminosity distance error of $\\sigma_{d_L}/d_L=10\\%$. However, when extending the analysis to $\\ell^{\\rm GW}_{\\rm max}=1000$, we find that the GW magnification improves the galaxy survey performance on all the cosmological parameters, reducing their errors by $3\\%$-$5\\%$, when $\\sigma_{d_L}/d_L=10\\%$, and by $10\\%$-$18\\%$ when $\\sigma_{d_L}/d_L=1\\%$, especially for $M_\\nu$, $w_0$ and $w_a$. However, here our analysis is unavoidably optimistic: a much more detailed and realistic approach will be needed, especially by including systematic effects. But we can conclude that, in the case of future gravitational wave observatories the inclusion of the gravitational wave magnification can improve Stage-IV galaxy surveys performance on constraining the underlying cosmological model of the Universe.","sentences":["In this work we have investigated the synergy between Stage-IV galaxy surveys and future GW observatories for constraining the underlying cosmological model of the Universe, focussing on photometric galaxy clustering, cosmic shear and GW magnification as cosmological probes.","We have implemented a Fisher matrix approach for the evaluation of the full $6\\times2$pt statistics composed by the angular power spectra of the single probes together with their combination.","For our analysis, we have in particular considered dynamical dark energy and massive neutrino scenarios.","We have found that the improvement to galaxy survey performance is below 1\\%, in the case $\\ell^{\\rm GW}_{\\rm max}=100$ and a luminosity distance error of $\\sigma_{d_L}/d_L=10\\%$. However, when extending the analysis to $\\ell^{\\rm GW}_{\\rm","max}=1000$, we find that the GW magnification improves the galaxy survey performance on all the cosmological parameters, reducing their errors by $3\\%$-$5\\%$, when $\\sigma_{d_L}/d_L=10\\%$, and by $10\\%$-$18\\%$ when $\\sigma_{d_L}/d_L=1\\%$, especially for $M_\\nu$, $w_0$ and $w_a$.","However, here our analysis is unavoidably optimistic: a much more detailed and realistic approach will be needed, especially by including systematic effects.","But we can conclude that, in the case of future gravitational wave observatories the inclusion of the gravitational wave magnification can improve Stage-IV galaxy surveys performance on constraining the underlying cosmological model of the Universe."],"url":"http://arxiv.org/abs/2403.03859v1","category":"astro-ph.CO"}
{"created":"2024-03-06 16:04:28","title":"On the vacuum energy in the Einstein Universe and the conformal anomaly","abstract":"An oldish question is resurrected concerning the significance of the ambiguous `b-type' terms encountered in calculations of the vacuum, Casimir energy on the Einstein Universe for conformally coupled scalar fields. Some remarks in the literature are hopefully clarified and the relevance of much earlier evaluations is pointed out. A consistency principle is suggested.","sentences":["An oldish question is resurrected concerning the significance of the ambiguous `b-type' terms encountered in calculations of the vacuum, Casimir energy on the Einstein Universe for conformally coupled scalar fields.","Some remarks in the literature are hopefully clarified and the relevance of much earlier evaluations is pointed out.","A consistency principle is suggested."],"url":"http://arxiv.org/abs/2403.03817v1","category":"hep-th"}
{"created":"2024-03-06 15:45:34","title":"Investigating introductory and advanced students' difficulties with change in internal energy, work and heat transfer using a validated instrument","abstract":"We use the Survey of Thermodynamic Processes and First and Second Laws-Long (STPFaSL-Long), a research-based survey instrument with 78 items at the level of introductory physics, to investigate introductory and advanced students' difficulties with internal energy, work, and heat transfer. We present analysis of data from 12 different introductory and advanced physics classes at four different higher education public institutions in the US in which the survey was administered in person to more than 1000 students. We find that not only introductory but also advanced physics students have many common difficulties with these introductory thermodynamic concepts after traditional lecture-based instruction in relevant concepts. We utilize a wide variety of problem types and contexts and our sample includes large numbers of introductory algebra-based, calculus-based, and advanced students. Some of our findings are consistent with prior research in this area, but others expand upon them and reveal previously unreported aspects of students' thinking. Findings related to common difficulties of students before and after traditional lecture-based instruction in college physics courses can help instructors of these courses plan instruction and curricula to improve student understanding. These findings can also be valuable for developing effective research-based curricula and pedagogies to address student difficulties and help students develop a functional understanding of these fundamental thermodynamic concepts.","sentences":["We use the Survey of Thermodynamic Processes and First and Second Laws-Long (STPFaSL-Long), a research-based survey instrument with 78 items at the level of introductory physics, to investigate introductory and advanced students' difficulties with internal energy, work, and heat transfer.","We present analysis of data from 12 different introductory and advanced physics classes at four different higher education public institutions in the US in which the survey was administered in person to more than 1000 students.","We find that not only introductory but also advanced physics students have many common difficulties with these introductory thermodynamic concepts after traditional lecture-based instruction in relevant concepts.","We utilize a wide variety of problem types and contexts and our sample includes large numbers of introductory algebra-based, calculus-based, and advanced students.","Some of our findings are consistent with prior research in this area, but others expand upon them and reveal previously unreported aspects of students' thinking.","Findings related to common difficulties of students before and after traditional lecture-based instruction in college physics courses can help instructors of these courses plan instruction and curricula to improve student understanding.","These findings can also be valuable for developing effective research-based curricula and pedagogies to address student difficulties and help students develop a functional understanding of these fundamental thermodynamic concepts."],"url":"http://arxiv.org/abs/2403.03795v1","category":"physics.ed-ph"}
{"created":"2024-03-06 15:33:18","title":"Using Schroedinger cat quantum state for detection of a given phase shift","abstract":"We show that injecting a light pulse prepared in the Shroedinger cat quantum state into the dark port of a two-arm interferometer and the strong classical light into the bright one, it is possible, in principle, to detect a given phase shift unambiguously. The value of this phase shift is inversely proportional to the amplitudes of both the classical carrier and Shroedinger cat state. However, an exotic detection procedure is required for this purpose.   By measuring the number of photons at the output dark port, it is possible to detect the phase shift with the vanishing \"false positive\" probability. The \"false negative\" probability in this case decreases with the increase on the amplitude of the Schroedinger cat state and, for reasonable values of this amplitude, can be made as small as about 0.1.","sentences":["We show that injecting a light pulse prepared in the Shroedinger cat quantum state into the dark port of a two-arm interferometer and the strong classical light into the bright one, it is possible, in principle, to detect a given phase shift unambiguously.","The value of this phase shift is inversely proportional to the amplitudes of both the classical carrier and Shroedinger cat state.","However, an exotic detection procedure is required for this purpose.   ","By measuring the number of photons at the output dark port, it is possible to detect the phase shift with the vanishing \"false positive\" probability.","The \"false negative\" probability in this case decreases with the increase on the amplitude of the Schroedinger cat state and, for reasonable values of this amplitude, can be made as small as about 0.1."],"url":"http://arxiv.org/abs/2403.03787v1","category":"quant-ph"}
{"created":"2024-03-06 15:18:41","title":"Loophole-free Bell inequality violations cannot disprove local realism","abstract":"For almost three decades in the twentieth century, the physics community believed that John von Neumann had proved the impossibility of completing quantum mechanics by a local realist, hidden-variables theory. Although Grete Hermann had raised strong objections to von Neumann's proof, she was largely ignored. This situation lasted, until John Bell rediscovered that von Neumann's proof was flawed: a \\emph{sufficient} condition for local realism had been taken as a \\emph{necessary} one. Bell subsequently established various constraints on hidden-variables theories, in the form of inequalities that can be submitted to experimental test. All performed tests to date have opened some loopholes. The quest to close them motivated great technical achievements and ongoing efforts to improve what has already been reached. There is, however, a rather ironic twist concerning Bell inequalities. On deriving them, Bell also took a sufficient condition for local-realism, as if it were a necessary one. As a consequence, even completely loophole-free Bell inequality violations would not disprove local realism. We argue that Bell inequalities cannot follow from local-realism alone. The proof is given by constructing three local-realist models that entail Bell inequality violations.","sentences":["For almost three decades in the twentieth century, the physics community believed that John von Neumann had proved the impossibility of completing quantum mechanics by a local realist, hidden-variables theory.","Although Grete Hermann had raised strong objections to von Neumann's proof, she was largely ignored.","This situation lasted, until John Bell rediscovered that von Neumann's proof was flawed: a \\emph{sufficient} condition for local realism had been taken as a \\emph{necessary} one.","Bell subsequently established various constraints on hidden-variables theories, in the form of inequalities that can be submitted to experimental test.","All performed tests to date have opened some loopholes.","The quest to close them motivated great technical achievements and ongoing efforts to improve what has already been reached.","There is, however, a rather ironic twist concerning Bell inequalities.","On deriving them, Bell also took a sufficient condition for local-realism, as if it were a necessary one.","As a consequence, even completely loophole-free Bell inequality violations would not disprove local realism.","We argue that Bell inequalities cannot follow from local-realism alone.","The proof is given by constructing three local-realist models that entail Bell inequality violations."],"url":"http://arxiv.org/abs/2403.03780v1","category":"quant-ph"}
{"created":"2024-03-06 15:11:02","title":"Lars Brink: November 12, 1943 - October 29, 2022","abstract":"We give some personal reflections on the person and scientist Lars Brink and on some of his scientific achievements. Our relations to Lars are briefly described in [1] and [2], while the sources relevant for this text are summarised in [3].","sentences":["We give some personal reflections on the person and scientist Lars Brink and on some of his scientific achievements.","Our relations to Lars are briefly described in [1] and [2], while the sources relevant for this text are summarised in [3]."],"url":"http://arxiv.org/abs/2403.03776v1","category":"physics.hist-ph"}
{"created":"2024-03-06 13:57:52","title":"Nuclear magnetism in the deformed halo nucleus $^{31}$Ne","abstract":"Based on the point-coupling density functional, the time-odd deformed relativistic Hartree-Bogoliubov theory in continuum (TODRHBc) is developed. Then the effects of nuclear magnetism on halo phenomenon are explored by taking the experimentally suggested deformed halo nucleus $^{31}$Ne as an example. For $^{31}$Ne, nuclear magnetism contributes 0.09 MeV to total binding energy, and the breaking of Kramers degeneracy results in 0-0.2 MeV splitting in canonical single-particle spectra. The blocked neutron level has a dominant component of $p$ wave and it is marginally bound. However, if we ignore nuclear magnetism, the level becomes unbound. This shows a subtle mechanism that nuclear magnetism changes the single-particle energies, causing a nucleus to become bound. Based on the TODRHBc results, a prolate one-neutron halo is formed around the near-spherical core in $^{31}$Ne. The nucleon current is mostly contributed by the halo rather than the core, except near the center of the nucleus. A layered structure in the neutron current distribution is observed and studied in detail.","sentences":["Based on the point-coupling density functional, the time-odd deformed relativistic Hartree-Bogoliubov theory in continuum (TODRHBc) is developed.","Then the effects of nuclear magnetism on halo phenomenon are explored by taking the experimentally suggested deformed halo nucleus $^{31}$Ne as an example.","For $^{31}$Ne, nuclear magnetism contributes 0.09 MeV to total binding energy, and the breaking of Kramers degeneracy results in 0-0.2 MeV splitting in canonical single-particle spectra.","The blocked neutron level has a dominant component of $p$ wave and it is marginally bound.","However, if we ignore nuclear magnetism, the level becomes unbound.","This shows a subtle mechanism that nuclear magnetism changes the single-particle energies, causing a nucleus to become bound.","Based on the TODRHBc results, a prolate one-neutron halo is formed around the near-spherical core in $^{31}$Ne.","The nucleon current is mostly contributed by the halo rather than the core, except near the center of the nucleus.","A layered structure in the neutron current distribution is observed and studied in detail."],"url":"http://arxiv.org/abs/2403.03713v1","category":"nucl-th"}
{"created":"2024-03-06 13:42:35","title":"How these characterized higher charmonia shape the puzzling data of the $e^+e^-\\to \u03b7J/\u03c8$ cross section","abstract":"Recently, the BESIII Collaboration performed a precise measurement of the $e^+e^-\\to \\eta J/\\psi$ cross section. It is puzzling that the resonance parameters of the reported $Y(4230)$ show a substantial divergence from the previously measured results in both the open-charmed and hidden-charmed decay channels, and the line shape asymmetry of the data approaching 4.2 GeV also suggests that it might be difficult to characterize the details of the structure around 4.2 GeV by a single resonance. This has motivated our great curiosity about how the charmonium states are distributed in the measured energy range and how they shape the puzzling data of the $e^+e^-\\to \\eta J/\\psi$ cross section. In this work, we use five theoretically constructed charmonia in the range of $4.0\\sim4.5$ $\\text{GeV}$, i.e. $\\psi(4040)$, $\\psi(4160)$, $\\psi(4220)$, $\\psi(4380)$, and $\\psi(4415)$, to apply a combined fit to the data, in which their calculated decay ratios into $\\eta J/\\psi$ via hadronic loop mechanism are taken as input. The fit results can reproduce the measured cross section data well, especially for the subtle line shape around 4.2 GeV, showing that the structure around 4.2 GeV is possible from the contribution of both $\\psi(4160)$ and $\\psi(4220)$.","sentences":["Recently, the BESIII Collaboration performed a precise measurement of the $e^+e^-\\to \\eta J/\\psi$ cross section.","It is puzzling that the resonance parameters of the reported $Y(4230)$ show a substantial divergence from the previously measured results in both the open-charmed and hidden-charmed decay channels, and the line shape asymmetry of the data approaching 4.2 GeV also suggests that it might be difficult to characterize the details of the structure around 4.2 GeV by a single resonance.","This has motivated our great curiosity about how the charmonium states are distributed in the measured energy range and how they shape the puzzling data of the $e^+e^-\\to \\eta J/\\psi$ cross section.","In this work, we use five theoretically constructed charmonia in the range of $4.0\\sim4.5$ $\\text{GeV}$, i.e. $\\psi(4040)$, $\\psi(4160)$, $\\psi(4220)$, $\\psi(4380)$, and $\\psi(4415)$, to apply a combined fit to the data, in which their calculated decay ratios into $\\eta J/\\psi$ via hadronic loop mechanism are taken as input.","The fit results can reproduce the measured cross section data well, especially for the subtle line shape around 4.2 GeV, showing that the structure around 4.2 GeV is possible from the contribution of both $\\psi(4160)$ and $\\psi(4220)$."],"url":"http://arxiv.org/abs/2403.03705v1","category":"hep-ph"}
{"created":"2024-03-06 13:27:00","title":"The double-peaked type I X-ray bursts with different mass accretion rate and fuel composition","abstract":"Using the MESA code, we have carried out a detailed survey of the available parameter space for the double-peaked type I X-ray bursts. We find that the double-peaked structure appears at mass accretion rate $\\dot{M}$ in the range of $\\sim(4-8)\\times10^{-10}\\,M_{\\odot}/{\\rm yr}$ when metallicity $Z=0.01$, while in the range of $\\sim(4-8)\\times10^{-9}\\,M_{\\odot}/\\rm{yr}$ when $Z=0.05$. Calculations of the metallicity impact suggest that the double peaks will disappear when $Z\\lesssim0.005$ for $\\dot{M}=5\\times10^{-10}\\,M_{\\odot}/\\rm{yr}$ and $Z\\lesssim0.04$ for $\\dot{M}=5\\times10^{-9}\\,M_{\\odot}/\\rm{yr}$. Besides, the impacts of base heating $Q_{\\rm b}$, as well as nuclear reaction waiting points: $^{22}\\rm{Mg}$, $^{26}\\rm{Si}$, $^{30}\\rm{S}$, $^{34}\\rm{Ar}$, $^{56}{\\rm Ni}$, $^{60}\\rm Zn$, $^{64}\\rm{Ge}$, $^{68}\\rm{Se}$, $^{72}\\rm{Kr}$ have been explored. The luminosity of the two peaks decreases as $Q_{\\rm b}$ increases. $^{68}{\\rm Se}(p,\\gamma){^{69}{\\rm Br}}$ is the most sensitive reaction, the double peaks disappear assuming that $^{56}{\\rm Ni}(p,\\gamma)^{57}{\\rm Cu}$ and $^{64}{\\rm Ge}(p,\\gamma)^{65}{\\rm As}$ reaction rates have been underestimated by a factor of 100 and the $^{22}{\\rm Mg}(\\alpha,p)^{25}{\\rm Al}$ reaction rate has been overestimated by a factor of 100, which indicates that $^{22}{\\rm Mg}$, $^{56}{\\rm Ni}$, $^{64}{\\rm Ge}$, $^{68}{\\rm Se}$ are possibly the most important nuclear waiting points impedance. Comparisons to the double-peaked bursts from 4U 1636-53 and 4U 1730-22 suggest that the nuclear origins of double-peaked type I X-ray bursts are difficult to explain the observed larger peak times ($t_{\\rm p,1}\\gtrsim4\\,{\\rm s}$, $t_{\\rm p,2}\\gtrsim8\\,{\\rm s}$) and smaller peak ratio($r_{1,2}\\lesssim0.5$). The composition of ashes from double-peaked bursts is very different from the single-peaked bursts especially for the heavier p-nuclei.","sentences":["Using the MESA code, we have carried out a detailed survey of the available parameter space for the double-peaked type I X-ray bursts.","We find that the double-peaked structure appears at mass accretion rate $\\dot{M}$ in the range of $\\sim(4-8)\\times10^{-10}\\,M_{\\odot}/{\\rm yr}$ when metallicity $Z=0.01$, while in the range of $\\sim(4-8)\\times10^{-9}\\,M_{\\odot}/\\rm{yr}$ when $Z=0.05$. Calculations of the metallicity impact suggest that the double peaks will disappear when $Z\\lesssim0.005$ for $\\dot{M}=5\\times10^{-10}\\,M_{\\odot}/\\rm{yr}$ and $Z\\lesssim0.04$ for $\\dot{M}=5\\times10^{-9}\\,M_{\\odot}/\\rm{yr}$. Besides, the impacts of base heating $Q_{\\rm b}$, as well as nuclear reaction waiting points: $^{22}\\rm{Mg}$, $^{26}\\rm{Si}$, $^{30}\\rm{S}$, $^{34}\\rm{Ar}$, $^{56}{\\rm Ni}$, $^{60}\\rm Zn$, $^{64}\\rm{Ge}$, $^{68}\\rm{Se}$, $^{72}\\rm{Kr}$ have been explored.","The luminosity of the two peaks decreases as $Q_{\\rm b}$ increases.","$^{68}{\\rm Se}(p,\\gamma){^{69}{\\rm Br}}$ is the most sensitive reaction, the double peaks disappear assuming that $^{56}{\\rm Ni}(p,\\gamma)^{57}{\\rm Cu}$ and $^{64}{\\rm Ge}(p,\\gamma)^{65}{\\rm As}$ reaction rates have been underestimated by a factor of 100 and the $^{22}{\\rm Mg}(\\alpha,p)^{25}{\\rm Al}$ reaction rate has been overestimated by a factor of 100, which indicates that $^{22}{\\rm Mg}$, $^{56}{\\rm Ni}$, $^{64}{\\rm Ge}$, $^{68}{\\rm Se}$ are possibly the most important nuclear waiting points impedance.","Comparisons to the double-peaked bursts from 4U 1636-53 and 4U 1730-22 suggest that the nuclear origins of double-peaked type I X-ray bursts are difficult to explain the observed larger peak times ($t_{\\rm p,1}\\gtrsim4\\,{\\rm s}$, $t_{\\rm p,2}\\gtrsim8\\,{\\rm s}$) and smaller peak ratio($r_{1,2}\\lesssim0.5$).","The composition of ashes from double-peaked bursts is very different from the single-peaked bursts especially for the heavier p-nuclei."],"url":"http://arxiv.org/abs/2403.03697v1","category":"astro-ph.HE"}
{"created":"2024-03-06 13:10:57","title":"Correlations of fluctuations of two-dimensional flow forced by a random force on top of a shear flow","abstract":"We examine fluctuations of vorticity excited by an external random force in two-dimensional fluid in the presence of a strong external shear flow. The problem is motivated by the analysis of big coherent vortices appearing as a consequence of the inverse energy cascade in a finite box at large Reynolds numbers. We develop the perturbation theory for calculating nonlinear corrections to correlation functions of the flow fluctuations assuming that the external force is short correlated in time. We analyze corrections to the pair correlation function of vorticity and some moments. The analysis enables one to establish validity of the perturbation theory for laboratory experiments and numerical simulations.","sentences":["We examine fluctuations of vorticity excited by an external random force in two-dimensional fluid in the presence of a strong external shear flow.","The problem is motivated by the analysis of big coherent vortices appearing as a consequence of the inverse energy cascade in a finite box at large Reynolds numbers.","We develop the perturbation theory for calculating nonlinear corrections to correlation functions of the flow fluctuations assuming that the external force is short correlated in time.","We analyze corrections to the pair correlation function of vorticity and some moments.","The analysis enables one to establish validity of the perturbation theory for laboratory experiments and numerical simulations."],"url":"http://arxiv.org/abs/2403.03685v1","category":"physics.flu-dyn"}
{"created":"2024-03-06 11:45:52","title":"Correlated Rotational Alignment Spectroscopy: A New Tool for High-Resolution Spectroscopy and the Analysis of Heterogeneous Samples","abstract":"Correlated rotational alignment spectroscopy correlates observables of ultrafast gas-phase spectroscopy with high-resolution, broad-band rotational Raman spectra. This article reviews the measurement principle of CRASY, existing implementations for mass-correlated measurements, and the potential for future developments. New spectroscopic capabilities are discussed in detail: Signals for individual sample components can be separated even in highly heterogeneous samples. Isotopologue rotational spectra can be observed at natural isotope abundance. Fragmentation channels are readily assigned in molecular and cluster mass spectra. And finally, rotational Raman spectra can be measured with sub-MHz resolution, an improvement of several orders-of-magnitude as compared to preceding experiments.","sentences":["Correlated rotational alignment spectroscopy correlates observables of ultrafast gas-phase spectroscopy with high-resolution, broad-band rotational Raman spectra.","This article reviews the measurement principle of CRASY, existing implementations for mass-correlated measurements, and the potential for future developments.","New spectroscopic capabilities are discussed in detail: Signals for individual sample components can be separated even in highly heterogeneous samples.","Isotopologue rotational spectra can be observed at natural isotope abundance.","Fragmentation channels are readily assigned in molecular and cluster mass spectra.","And finally, rotational Raman spectra can be measured with sub-MHz resolution, an improvement of several orders-of-magnitude as compared to preceding experiments."],"url":"http://arxiv.org/abs/2403.03634v1","category":"physics.chem-ph"}
{"created":"2024-03-06 11:25:16","title":"The Tensor Track VIII: Stochastic Analysis","abstract":"Assuming some familiarity with quantum field theory and with the tensor track approach that we presented in the previous series Tensor Track I-VII, we provide, as usual, the developments in tensors models of the last two years. Then we expose the fundamental breakthrough of Martin Hairer on regularity structures and the work of L\\'eonard Ferdinand on stochastic analysis applied to super-renormalizable tensor field theories. We conclude with the hope that this work could be extended to just-renormalizable and asymptotically free models.","sentences":["Assuming some familiarity with quantum field theory and with the tensor track approach that we presented in the previous series Tensor Track I-VII, we provide, as usual, the developments in tensors models of the last two years.","Then we expose the fundamental breakthrough of Martin Hairer on regularity structures and the work of L\\'eonard Ferdinand on stochastic analysis applied to super-renormalizable tensor field theories.","We conclude with the hope that this work could be extended to just-renormalizable and asymptotically free models."],"url":"http://arxiv.org/abs/2403.03619v1","category":"math-ph"}
{"created":"2024-03-06 11:21:03","title":"The nature of the X-ray filaments around bow shock pulsar wind nebulae","abstract":"Context. We propose that the X-ray filaments emerging from selected bow shock pulsar wind nebulae are due to a charge-separated outflow of electrons and/or positrons escaping the nebula and propagating along the local Galactic magnetic field. Aims. The X-ray brightness, length, and thickness of filaments are all accounted for if a nonresonant streaming instability is excited. Methods. This is possible if particles are released in the interstellar medium as a collimated beam, as would be expected in a reconnection region between the nebular and interstellar magnetic fields. Results. We successfully test this idea on the Guitar Nebula filament and discuss other cases. Conclusions. These filaments provide the best diagnostics available for particle escape from evolved pulsar wind nebulae, a process essential to assessing the contribution of these sources to cosmic ray positrons. The same phenomenology might govern the occurrence of TeV halos and their importance for cosmic ray transport.","sentences":["Context.","We propose that the X-ray filaments emerging from selected bow shock pulsar wind nebulae are due to a charge-separated outflow of electrons and/or positrons escaping the nebula and propagating along the local Galactic magnetic field.","Aims.","The X-ray brightness, length, and thickness of filaments are all accounted for if a nonresonant streaming instability is excited.","Methods.","This is possible if particles are released in the interstellar medium as a collimated beam, as would be expected in a reconnection region between the nebular and interstellar magnetic fields.","Results.","We successfully test this idea on the Guitar Nebula filament and discuss other cases.","Conclusions.","These filaments provide the best diagnostics available for particle escape from evolved pulsar wind nebulae, a process essential to assessing the contribution of these sources to cosmic ray positrons.","The same phenomenology might govern the occurrence of TeV halos and their importance for cosmic ray transport."],"url":"http://arxiv.org/abs/2403.03616v1","category":"astro-ph.HE"}
{"created":"2024-03-06 10:12:05","title":"Projector-based efficient estimation of force constants","abstract":"Estimating force constants for crystal structures is a fundamental aspect of calculating various phonon-related properties. However, it can be a challenging task, particularly when dealing with a large number of atoms or when third and higher-order force constants are needed. In this study, we propose an efficient approach that involves constructing a complete orthonormal basis set of the force constants. We formulate this approach using projector matrices and their eigenvectors corresponding to the requirements for the force constants. This basis set allows us to infer the force constants from displacement-force datasets precisely. Our efficient algorithms for the basis-set construction and force constant estimation from a displacement-force dataset are implemented in the symfc code. Furthermore, several applications are demonstrated in this study, indicating that the current approach helps to obtain force constants efficiently and precisely.","sentences":["Estimating force constants for crystal structures is a fundamental aspect of calculating various phonon-related properties.","However, it can be a challenging task, particularly when dealing with a large number of atoms or when third and higher-order force constants are needed.","In this study, we propose an efficient approach that involves constructing a complete orthonormal basis set of the force constants.","We formulate this approach using projector matrices and their eigenvectors corresponding to the requirements for the force constants.","This basis set allows us to infer the force constants from displacement-force datasets precisely.","Our efficient algorithms for the basis-set construction and force constant estimation from a displacement-force dataset are implemented in the symfc code.","Furthermore, several applications are demonstrated in this study, indicating that the current approach helps to obtain force constants efficiently and precisely."],"url":"http://arxiv.org/abs/2403.03588v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-06 09:18:48","title":"Tellurization of Pd(111): absence of PdTe$_2$ but formation of a TePd$_2$ surface alloy","abstract":"In a recent publication [2D Materials, 8, 045033 (2021), arXiv:2103.11403], it was reported that the growth of a monolayer PdTe$_2$ in ultra-high vacuum could be achieved by deposition of tellurium on a palladium (111) crystal surface and subsequent thermal annealing. By means of low-energy electron diffraction intensity (LEED-IV) structural analysis, we show that the obtained $\\left(\\sqrt{3}\\times \\sqrt{3} \\right)\\textrm{R30}^\\circ$ superstructure is in fact a TePd$_2$ surface alloy. Attempts to produce a PdTe$_2$ layer in ultra-high vacuum by increasing the Te content on the surface were not successful.","sentences":["In a recent publication [2D Materials, 8, 045033 (2021), arXiv:2103.11403], it was reported that the growth of a monolayer PdTe$_2$ in ultra-high vacuum could be achieved by deposition of tellurium on a palladium (111) crystal surface and subsequent thermal annealing.","By means of low-energy electron diffraction intensity (LEED-IV) structural analysis, we show that the obtained $\\left(\\sqrt{3}\\times \\sqrt{3} \\right)\\textrm{R30}^\\circ$ superstructure is in fact a TePd$_2$ surface alloy.","Attempts to produce a PdTe$_2$ layer in ultra-high vacuum by increasing the Te content on the surface were not successful."],"url":"http://arxiv.org/abs/2403.03564v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-06 09:06:07","title":"An IDE Plugin for Gamified Continuous Integration","abstract":"Interruptions and context switches resulting from meetings, urgent tasks, emails, and queries from colleagues contribute to productivity losses in developers' daily routines. This is particularly challenging for tasks like software testing, which are already perceived as less enjoyable, prompting developers to seek distractions. To mitigate this, applying gamification to testing activities can enhance motivation for test writing. One such gamification tool is Gamekins, which integrates challenges, quests, achievements, and leaderboards into the Jenkins CI (continuous integration) platform. However, as Gamekins is typically accessed through a browser, it introduces a context switch. This paper presents an IntelliJ plugin designed to seamlessly integrate Gamekins' gamification elements into the IDE, aiming to minimize context switches and boost developer motivation for test writing.","sentences":["Interruptions and context switches resulting from meetings, urgent tasks, emails, and queries from colleagues contribute to productivity losses in developers' daily routines.","This is particularly challenging for tasks like software testing, which are already perceived as less enjoyable, prompting developers to seek distractions.","To mitigate this, applying gamification to testing activities can enhance motivation for test writing.","One such gamification tool is Gamekins, which integrates challenges, quests, achievements, and leaderboards into the Jenkins CI (continuous integration) platform.","However, as Gamekins is typically accessed through a browser, it introduces a context switch.","This paper presents an IntelliJ plugin designed to seamlessly integrate Gamekins' gamification elements into the IDE, aiming to minimize context switches and boost developer motivation for test writing."],"url":"http://arxiv.org/abs/2403.03557v1","category":"cs.SE"}
{"created":"2024-03-06 08:50:15","title":"Mass and decay width of $T_{ccs}$ from symmetries","abstract":"We analyze the mass and width of the doubly heavy tetraquark $T_{ccs}$ composed of a heavy diquark and a light quark cloud with strangeness with assuming that a color anti-triplet heavy diquark is a dominant component of the doubly charmed tetraquarks $T_{cc}$ and $T_{ccs}$. We construct an effective Lagrangian for masses of heavy hadrons based on the superflavor symmetry between the doubly heavy tetraquarks and the singly heavy baryons with including the terms which simultaneously break the heavy-quark and light flavor symmetries, and predict the mass of $T_{ccs}$ as $M(T_{ccs}) = 4057\\pm40$\\,MeV. The comparison of this prediction with future experimental observation will give a clue to understand the color structure of the heavy diquark. We also predict the spin-averaged mass of $\\Omega_{cc}$ ($J^P = 1/2^+, 3/2^+)$ as $M(\\Omega_{cc}) = 3760\\pm 18\\,$MeV. We next calculate the decay width of $T_{ccs}$, based on solely the light flavor symmetry, as $\\Gamma(T_{ccs}) = 1.2\\pm 0.3$\\,MeV.","sentences":["We analyze the mass and width of the doubly heavy tetraquark $T_{ccs}$ composed of a heavy diquark and a light quark cloud with strangeness with assuming that a color anti-triplet heavy diquark is a dominant component of the doubly charmed tetraquarks $T_{cc}$ and $T_{ccs}$. We construct an effective Lagrangian for masses of heavy hadrons based on the superflavor symmetry between the doubly heavy tetraquarks and the singly heavy baryons with including the terms which simultaneously break the heavy-quark and light flavor symmetries, and predict the mass of $T_{ccs}$ as $M(T_{ccs})","= 4057\\pm40$\\,MeV.","The comparison of this prediction with future experimental observation will give a clue to understand the color structure of the heavy diquark.","We also predict the spin-averaged mass of $\\Omega_{cc}$ ($J^P = 1/2^+, 3/2^+)$ as $M(\\Omega_{cc})","= 3760\\pm","18\\,$MeV.","We next calculate the decay width of $T_{ccs}$, based on solely the light flavor symmetry, as $\\Gamma(T_{ccs})","= 1.2\\pm 0.3$\\,MeV."],"url":"http://arxiv.org/abs/2403.03548v1","category":"hep-ph"}
{"created":"2024-03-06 06:40:06","title":"Scalable ATLAS pMSSM computational workflows using containerised REANA reusable analysis platform","abstract":"In this paper we describe the development of a streamlined framework for large-scale ATLAS pMSSM reinterpretations of LHC Run-2 analyses using containerised computational workflows. The project is looking to assess the global coverage of BSM physics and requires running O(5k) computational workflows representing pMSSM model points. Following ATLAS Analysis Preservation policies, many analyses have been preserved as containerised Yadage workflows, and after validation were added to a curated selection for the pMSSM study. To run the workflows at scale, we utilised the REANA reusable analysis platform. We describe how the REANA platform was enhanced to ensure the best concurrent throughput by internal service scheduling changes. We discuss the scalability of the approach on Kubernetes clusters from 500 to 5000 cores. Finally, we demonstrate a possibility of using additional ad-hoc public cloud infrastructure resources by running the same workflows on the Google Cloud Platform.","sentences":["In this paper we describe the development of a streamlined framework for large-scale ATLAS pMSSM reinterpretations of LHC Run-2 analyses using containerised computational workflows.","The project is looking to assess the global coverage of BSM physics and requires running O(5k) computational workflows representing pMSSM model points.","Following ATLAS Analysis Preservation policies, many analyses have been preserved as containerised Yadage workflows, and after validation were added to a curated selection for the pMSSM study.","To run the workflows at scale, we utilised the REANA reusable analysis platform.","We describe how the REANA platform was enhanced to ensure the best concurrent throughput by internal service scheduling changes.","We discuss the scalability of the approach on Kubernetes clusters from 500 to 5000 cores.","Finally, we demonstrate a possibility of using additional ad-hoc public cloud infrastructure resources by running the same workflows on the Google Cloud Platform."],"url":"http://arxiv.org/abs/2403.03494v1","category":"cs.DC"}
{"created":"2024-03-06 05:55:33","title":"Hubble tension may indicate time-dependent dark matter energy density","abstract":"The values of Hubble constant H0 by direct measurements with standard distance ladder are typically larger than those obtained from the observation of cosmic microwave background and the galaxy survey with inverse distance ladder. On the other hand, although the errors are still large, various determinations of the value of matter density parameter Omega_m are consistent with each other. Therefore, it is possible that the difference in Hubble constant is translated to the difference of physical matter energy density omega_m = Omega_m h^2, where h = H0 / (100 Km/s/Mpc). In this article it is examined the possibility of an increase of the physical dark matter energy density by a fast transition at a certain value of redshift as a possible resolution of the Hubble tension. A phenomenological fluid model of the dark sector, which is the modification of a so-called unified dark matter model, is introduced to concretely realize such a fast transition in the physical dark matter energy density.","sentences":["The values of Hubble constant H0 by direct measurements with standard distance ladder are typically larger than those obtained from the observation of cosmic microwave background and the galaxy survey with inverse distance ladder.","On the other hand, although the errors are still large, various determinations of the value of matter density parameter Omega_m are consistent with each other.","Therefore, it is possible that the difference in Hubble constant is translated to the difference of physical matter energy density omega_m = Omega_m h^2, where h = H0 / (100 Km/s/Mpc).","In this article it is examined the possibility of an increase of the physical dark matter energy density by a fast transition at a certain value of redshift as a possible resolution of the Hubble tension.","A phenomenological fluid model of the dark sector, which is the modification of a so-called unified dark matter model, is introduced to concretely realize such a fast transition in the physical dark matter energy density."],"url":"http://arxiv.org/abs/2403.03484v1","category":"astro-ph.CO"}
{"created":"2024-03-06 05:39:05","title":"Impurities, or dopants, that is the question","abstract":"The numerous stories around LK-99 as a possible room-temperature superconductor over the summer of 2023 epitomise that materials are more than a bulk crystallographic structure or an expected composition. Like all materials, those at the core of technologies for the energy generation transition, including batteries, catalysts or quantum materials draw their properties from a hierarchy of microstructural features where impurities can dramatically influence the outcomes. As we move towards a circular economy, the recycling of materials will inevitably create fluxes of increasingly impure materials, generating new challenges for fabricating materials with controlled properties. Here, we provide our perspective on how high-end microscopy and microanalysis have helped us to understand relationships between synthesis, processing and microstructure, avoiding imprecise or even erroneous interpretations on the origins of the properties from a range of materials. We highlight examples of how unexpected impurities and their spatial distribution on the nanoscale can be turned into an advantage to define pathways for synthesis of materials with new and novel sets of physical properties.","sentences":["The numerous stories around LK-99 as a possible room-temperature superconductor over the summer of 2023 epitomise that materials are more than a bulk crystallographic structure or an expected composition.","Like all materials, those at the core of technologies for the energy generation transition, including batteries, catalysts or quantum materials draw their properties from a hierarchy of microstructural features where impurities can dramatically influence the outcomes.","As we move towards a circular economy, the recycling of materials will inevitably create fluxes of increasingly impure materials, generating new challenges for fabricating materials with controlled properties.","Here, we provide our perspective on how high-end microscopy and microanalysis have helped us to understand relationships between synthesis, processing and microstructure, avoiding imprecise or even erroneous interpretations on the origins of the properties from a range of materials.","We highlight examples of how unexpected impurities and their spatial distribution on the nanoscale can be turned into an advantage to define pathways for synthesis of materials with new and novel sets of physical properties."],"url":"http://arxiv.org/abs/2403.03480v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-06 05:07:59","title":"Coupled free fermion conformal field theories","abstract":"We study a specific class of CFTs that involve coupled free fermions, arising from parafermion CFTs and lattice constructions. We analyse their representation spaces and the underlying exclusion statistics of coupled free fermions using specific bases. In one particular case, we reveal an unexpected connection between the coset construction of parafermions, the lattice construction, and and orbifold thereof. This connection is supported by proving a range of character identities within the context of coupled free fermions. Simultaneously, we obtain explicit expressions of certain string functions in terms of Dedekind eta functions.","sentences":["We study a specific class of CFTs that involve coupled free fermions, arising from parafermion CFTs and lattice constructions.","We analyse their representation spaces and the underlying exclusion statistics of coupled free fermions using specific bases.","In one particular case, we reveal an unexpected connection between the coset construction of parafermions, the lattice construction, and and orbifold thereof.","This connection is supported by proving a range of character identities within the context of coupled free fermions.","Simultaneously, we obtain explicit expressions of certain string functions in terms of Dedekind eta functions."],"url":"http://arxiv.org/abs/2403.03471v1","category":"hep-th"}
{"created":"2024-03-06 05:00:30","title":"q-Rious identities for parafermion theories","abstract":"In this paper we will prove a series of $q$-identities suggested by the realisation of certain conformal field theories by so-called `coupled free fermions'. We will consider $q$-series arising from coupled free fermions constructed by the parafermion coset construction as well as from scaled root lattices, and some interesting relations between the two.","sentences":["In this paper we will prove a series of $q$-identities suggested by the realisation of certain conformal field theories by so-called `coupled free fermions'.","We will consider $q$-series arising from coupled free fermions constructed by the parafermion coset construction as well as from scaled root lattices, and some interesting relations between the two."],"url":"http://arxiv.org/abs/2403.03464v1","category":"hep-th"}
{"created":"2024-03-06 04:52:51","title":"Foot Shape-Dependent Resistive Force Model for Bipedal Walkers on Granular Terrains","abstract":"Legged robots have demonstrated high efficiency and effectiveness in unstructured and dynamic environments. However, it is still challenging for legged robots to achieve rapid and efficient locomotion on deformable, yielding substrates, such as granular terrains. We present an enhanced resistive force model for bipedal walkers on soft granular terrains by introducing effective intrusion depth correction. The enhanced force model captures fundamental kinetic results considering the robot foot shape, walking gait speed variation, and energy expense. The model is validated by extensive foot intrusion experiments with a bipedal robot. The results confirm the model accuracy on the given type of granular terrains. The model can be further integrated with the motion control of bipedal robotic walkers.","sentences":["Legged robots have demonstrated high efficiency and effectiveness in unstructured and dynamic environments.","However, it is still challenging for legged robots to achieve rapid and efficient locomotion on deformable, yielding substrates, such as granular terrains.","We present an enhanced resistive force model for bipedal walkers on soft granular terrains by introducing effective intrusion depth correction.","The enhanced force model captures fundamental kinetic results considering the robot foot shape, walking gait speed variation, and energy expense.","The model is validated by extensive foot intrusion experiments with a bipedal robot.","The results confirm the model accuracy on the given type of granular terrains.","The model can be further integrated with the motion control of bipedal robotic walkers."],"url":"http://arxiv.org/abs/2403.03460v1","category":"cs.RO"}
{"created":"2024-03-06 03:48:47","title":"Dark Dragon Breaks Magnetic Chain: Dynamical Substructures of IRDC G28.34 Form in Supported Environments","abstract":"We have comprehensively studied the multi-scale physical properties of the infrared dark cloud (IRDC) G28.34 (the Dragon cloud) with dust polarization and molecular line data from Planck, FCRAO-14m, JCMT, and ALMA. We find that the averaged magnetic fields of clumps tend to be either parallel with or perpendicular to the cloud-scale magnetic fields, while the cores in clump MM4 tend to have magnetic fields aligned with the clump fields. Implementing the relative orientation analysis (for magnetic fields, column density gradients, and local gravity), Velocity Gradient Technique, and modified Davis-Chandrasekhar-Fermi analysis, we find that: G28.34 is located in a trans-to-sub-Alfv\\'{e}nic environment ($\\mathcal{M}_{A}=0.74$ within $r=15$ pc); the magnetic field is effectively resisting gravitational collapse in large-scale diffuse gas, but is distorted by gravity within the cloud and affected by star formation activities in high-density regions; and the normalized mass-to-flux ratio tends to increase with density and decrease with radius. Considering both the magnetic and turbulent support, we find that the environmental gas of G28.34 is in a super-virial (supported) state, the infrared dark clumps may be in a near-equilibrium state, and core MM4-core4 is in a sub-virial (gravity-dominant) state. In summary, we suggest that magnetic fields dominate gravity and turbulence in the cloud environment at large scales, resulting in relatively slow cloud formation and evolution processes. Within the cloud, gravity could overwhelm both magnetic fields and turbulence, allowing local dynamical star formation to happen.","sentences":["We have comprehensively studied the multi-scale physical properties of the infrared dark cloud (IRDC) G28.34 (the Dragon cloud) with dust polarization and molecular line data from Planck, FCRAO-14m, JCMT, and ALMA.","We find that the averaged magnetic fields of clumps tend to be either parallel with or perpendicular to the cloud-scale magnetic fields, while the cores in clump MM4 tend to have magnetic fields aligned with the clump fields.","Implementing the relative orientation analysis (for magnetic fields, column density gradients, and local gravity), Velocity Gradient Technique, and modified Davis-Chandrasekhar-Fermi analysis, we find that: G28.34 is located in a trans-to-sub-Alfv\\'{e}nic environment ($\\mathcal{M}_{A}=0.74$ within $r=15$ pc); the magnetic field is effectively resisting gravitational collapse in large-scale diffuse gas, but is distorted by gravity within the cloud and affected by star formation activities in high-density regions; and the normalized mass-to-flux ratio tends to increase with density and decrease with radius.","Considering both the magnetic and turbulent support, we find that the environmental gas of G28.34 is in a super-virial (supported) state, the infrared dark clumps may be in a near-equilibrium state, and core MM4-core4 is in a sub-virial (gravity-dominant) state.","In summary, we suggest that magnetic fields dominate gravity and turbulence in the cloud environment at large scales, resulting in relatively slow cloud formation and evolution processes.","Within the cloud, gravity could overwhelm both magnetic fields and turbulence, allowing local dynamical star formation to happen."],"url":"http://arxiv.org/abs/2403.03437v1","category":"astro-ph.GA"}
{"created":"2024-03-06 03:06:22","title":"Yet another lattice formulation of 2D $U(1)$ chiral gauge theory via bosonization","abstract":"Recently, lattice formulations of Abelian chiral gauge theory in two dimensions have been devised on the basis of the Abelian bosonization. A salient feature of these two-dimensional lattice formulations is that the gauge invariance is exactly preserved for anomaly-free theories and thus is completely free from the question of the gauge mode decoupling. In the present paper, we propose a yet another lattice formulation sharing this desired property. A particularly unique point in our formulation is that the vertex operator of the dual scalar field, which carries the vector charge of the fermion and the ``magnetic charge'' in the bosonization, is represented by a ``hole'' excised from the lattice; this is the excision method formulated recently by Abe et al. in a somewhat different context.","sentences":["Recently, lattice formulations of Abelian chiral gauge theory in two dimensions have been devised on the basis of the Abelian bosonization.","A salient feature of these two-dimensional lattice formulations is that the gauge invariance is exactly preserved for anomaly-free theories and thus is completely free from the question of the gauge mode decoupling.","In the present paper, we propose a yet another lattice formulation sharing this desired property.","A particularly unique point in our formulation is that the vertex operator of the dual scalar field, which carries the vector charge of the fermion and the ``magnetic charge'' in the bosonization, is represented by a ``hole'' excised from the lattice; this is the excision method formulated recently by Abe et al. in a somewhat different context."],"url":"http://arxiv.org/abs/2403.03420v1","category":"hep-lat"}
{"created":"2024-03-06 03:02:07","title":"Stimulated Raman phase shift spectroscopy: a pathway to hyperfine fingerprint spectra","abstract":"The principle and experimental realization of a novel Raman spectroscopic technique entitled stimulated Raman phase shift (SRPS) spectroscopy was demonstrated. This technique depends on the measurement of the stimulated Raman scattering (SRS) induced phase shift of Stokes light field ($\\Delta$ $\\phi$) that is related to the real part of the third order nonlinear susceptibility of SRS. In principle, the spectral lineshape of 1/|$\\Delta$ $\\phi$| is a delta function waveform, which is insensitive to the fluctuation of Stokes light intensity, the decoherence of phonon in materials, as well as the inhomogeneous fluorescence background. In order to measure 1/|$\\Delta$ $\\phi$|, a SRPS including a Mach-Zender interferometer and a signal processing device was developed. Using the developed spectrometer, the SRPS and stimulated Raman gain (SRG) spectra of neat dimethyl sulfoxide were detected simultaneously. Seven Raman peaks corresponding to specific molecule vibrational and rotational modes were observed in the SRPS spectra, while only two peaks could be identified in the SRG spectra without a priori knowledge. The linewidth of the Raman peak centered at 2913.283 cm$^{-1}$ indicating the v$_s$(CH$_3$)stretching mode of the methyl groups was less than 0.00036 cm$^{-1}$ in the measured SRPS spectra, which was almost four orders of magnitude narrower than that in the measured SRG spectra. Meanwhile, the detection signal-to-noise ratio of the Raman peak centered at 2913.283 cm$^{-1}$ was 25.3 dB, representing an increase of 14.3 dB compared to the SRG spectra. The reliability of SRPS technique was verified by 10 independent measurements, and the standard deviation of the Raman peak frequency was less than $\\pm$0.338 cm$^{-1}$ . The SRPS technique paves the way for characterizing the hyperfine fingerprint of materials.","sentences":["The principle and experimental realization of a novel Raman spectroscopic technique entitled stimulated Raman phase shift (SRPS) spectroscopy was demonstrated.","This technique depends on the measurement of the stimulated Raman scattering (SRS) induced phase shift of Stokes light field ($\\Delta$ $\\phi$) that is related to the real part of the third order nonlinear susceptibility of SRS.","In principle, the spectral lineshape of 1/|$\\Delta$ $\\phi$| is a delta function waveform, which is insensitive to the fluctuation of Stokes light intensity, the decoherence of phonon in materials, as well as the inhomogeneous fluorescence background.","In order to measure 1/|$\\Delta$ $\\phi$|, a SRPS including a Mach-Zender interferometer and a signal processing device was developed.","Using the developed spectrometer, the SRPS and stimulated Raman gain (SRG) spectra of neat dimethyl sulfoxide were detected simultaneously.","Seven Raman peaks corresponding to specific molecule vibrational and rotational modes were observed in the SRPS spectra, while only two peaks could be identified in the SRG spectra without a priori knowledge.","The linewidth of the Raman peak centered at 2913.283 cm$^{-1}$ indicating the v$_s$(CH$_3$)stretching mode of the methyl groups was less than 0.00036 cm$^{-1}$ in the measured SRPS spectra, which was almost four orders of magnitude narrower than that in the measured SRG spectra.","Meanwhile, the detection signal-to-noise ratio of the Raman peak centered at 2913.283 cm$^{-1}$ was 25.3 dB, representing an increase of 14.3 dB compared to the SRG spectra.","The reliability of SRPS technique was verified by 10 independent measurements, and the standard deviation of the Raman peak frequency was less than $\\pm$0.338 cm$^{-1}$ .","The SRPS technique paves the way for characterizing the hyperfine fingerprint of materials."],"url":"http://arxiv.org/abs/2403.03417v1","category":"physics.optics"}
{"created":"2024-03-06 00:14:07","title":"On the Monotonicity of Information Aging","abstract":"In this paper, we analyze the monotonicity of information aging in a remote estimation system, where historical observations of a Gaussian autoregressive AR(p) process are used to predict its future values. We consider two widely used loss functions in estimation: (i) logarithmic loss function for maximum likelihood estimation and (ii) quadratic loss function for MMSE estimation. The estimation error of the AR(p) process is written as a generalized conditional entropy which has closed-form expressions. By using a new information-theoretic tool called $\\epsilon$-Markov chain, we can evaluate the divergence of the AR(p) process from being a Markov chain. When the divergence $\\epsilon$ is large, the estimation error of the AR(p) process can be far from a non-decreasing function of the Age of Information (AoI). Conversely, for small divergence $\\epsilon$, the inference error is close to a non-decreasing AoI function. Each observation is a short sequence taken from the AR(p) process. As the observation sequence length increases, the parameter $\\epsilon$ progressively reduces to zero, and hence the estimation error becomes a non-decreasing AoI function. These results underscore a connection between the monotonicity of information aging and the divergence of from being a Markov chain.","sentences":["In this paper, we analyze the monotonicity of information aging in a remote estimation system, where historical observations of a Gaussian autoregressive AR(p) process are used to predict its future values.","We consider two widely used loss functions in estimation: (i) logarithmic loss function for maximum likelihood estimation and (ii) quadratic loss function for MMSE estimation.","The estimation error of the AR(p) process is written as a generalized conditional entropy which has closed-form expressions.","By using a new information-theoretic tool called $\\epsilon$-Markov chain, we can evaluate the divergence of the AR(p) process from being a Markov chain.","When the divergence $\\epsilon$ is large, the estimation error of the AR(p) process can be far from a non-decreasing function of the Age of Information (AoI).","Conversely, for small divergence $\\epsilon$, the inference error is close to a non-decreasing AoI function.","Each observation is a short sequence taken from the AR(p) process.","As the observation sequence length increases, the parameter $\\epsilon$ progressively reduces to zero, and hence the estimation error becomes a non-decreasing AoI function.","These results underscore a connection between the monotonicity of information aging and the divergence of from being a Markov chain."],"url":"http://arxiv.org/abs/2403.03380v1","category":"cs.IT"}
{"created":"2024-03-06 00:10:46","title":"Faddeev-Jackiw Hamiltonian formulation for general exotic bi-gravity","abstract":"General exotic bi-gravity, obtained in [Phys. Rev. Lett. 123, 031303 (2019)], is a unitary parity-preserving model which describes two interacting spin-two fields in three-dimensional spacetime. Adopting a symplectic viewpoint, we investigate the dynamical structure of general exotic bi-gravity theory. In particular, by exploiting the properties of the corresponding pre-symplectic matrix and its associated zero-modes, we explicitly derive all constraints of the theory, including the integrability conditions and scalar relationships between all the parameters and fields defining the model. Then, as an application, these scalar relationships are used for studying the anti-de Sitter background. After that, we derive the gauge transformations for the dynamical variables from the structure of the remaining zero-modes, meaning that such zero-modes are indeed the generators of the gauge symmetry of the theory. Finally, by switching off one of the four coupling constants $\\beta_{n}$ and assuming the invertibility of some of the dreibeins, we find that the general exotic bi-gravity theory has two physical degrees of freedom.","sentences":["General exotic bi-","gravity, obtained in [Phys. Rev. Lett.","123, 031303 (2019)], is a unitary parity-preserving model which describes two interacting spin-two fields in three-dimensional spacetime.","Adopting a symplectic viewpoint, we investigate the dynamical structure of general exotic bi-gravity theory.","In particular, by exploiting the properties of the corresponding pre-symplectic matrix and its associated zero-modes, we explicitly derive all constraints of the theory, including the integrability conditions and scalar relationships between all the parameters and fields defining the model.","Then, as an application, these scalar relationships are used for studying the anti-de Sitter background.","After that, we derive the gauge transformations for the dynamical variables from the structure of the remaining zero-modes, meaning that such zero-modes are indeed the generators of the gauge symmetry of the theory.","Finally, by switching off one of the four coupling constants $\\beta_{n}$ and assuming the invertibility of some of the dreibeins, we find that the general exotic bi-gravity theory has two physical degrees of freedom."],"url":"http://arxiv.org/abs/2403.03378v1","category":"hep-th"}
{"created":"2024-03-05 23:52:20","title":"Remarks on celestial amplitudes and Liouville theory","abstract":"The relation between celestial holography and Liouville field theory is investigated. It is shown that duality relations between different Selberg type integrals appearing in the Coulomb gas realization of Liouville correlation functions induce a series of relations between celestial amplitudes with shifted values of the operators dimensions $\\Delta $.   This is a transcript of the talk delivered by the author at the Workshop on Celestial Holography and Asymptotic Symmetries, Santiago de Chile, March 4-6, 2024.","sentences":["The relation between celestial holography and Liouville field theory is investigated.","It is shown that duality relations between different Selberg type integrals appearing in the Coulomb gas realization of Liouville correlation functions induce a series of relations between celestial amplitudes with shifted values of the operators dimensions $\\Delta $.   ","This is a transcript of the talk delivered by the author at the Workshop on Celestial Holography and Asymptotic Symmetries, Santiago de Chile, March 4-6, 2024."],"url":"http://arxiv.org/abs/2403.03374v1","category":"hep-th"}
{"created":"2024-03-05 22:59:32","title":"Magnon kinetic theory of the antiferromagnetic Hanle effect","abstract":"Motivated by the recently discovered magnonic Hanle effect in an insulating antiferromagnet [Wimmer et al., Phys. Rev. Lett. 125, 247204 (2020)], we develop a spin transport theory based on low-energy waves of antiferromagnetic N\\'eel order. These waves have two polarizations, which we describe in analogy to optics using the Stokes vector on the Poincar\\'e sphere. We find that the polarization, which encodes the magnon spin angular momentum, changes periodically with a frequency that is nonlinear in the magnetic field. This explains the observed asymmetry in the Hanle signal as a function of the magnetic field, along with other salient experimental features. By providing an energy-resolved description of the spin injection, our theory combines the kinetic transport of magnons with the coherent dynamics of their polarization in an intuitive way. This opens a general perspective on a coherent control of magnonic spin density in collinear antiferromagnets.","sentences":["Motivated by the recently discovered magnonic Hanle effect in an insulating antiferromagnet","[Wimmer et al., Phys. Rev. Lett.","125, 247204 (2020)], we develop a spin transport theory based on low-energy waves of antiferromagnetic N\\'eel order.","These waves have two polarizations, which we describe in analogy to optics using the Stokes vector on the Poincar\\'e sphere.","We find that the polarization, which encodes the magnon spin angular momentum, changes periodically with a frequency that is nonlinear in the magnetic field.","This explains the observed asymmetry in the Hanle signal as a function of the magnetic field, along with other salient experimental features.","By providing an energy-resolved description of the spin injection, our theory combines the kinetic transport of magnons with the coherent dynamics of their polarization in an intuitive way.","This opens a general perspective on a coherent control of magnonic spin density in collinear antiferromagnets."],"url":"http://arxiv.org/abs/2403.03358v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-05 22:50:07","title":"Quark Counting, Drell-Yan West, and the Pion Wave Function","abstract":"The relation between the pion's quark distribution function, $q(x)$, its light-front wave function, and the elastic charge form factor, $F(\\Delta^2)$ is explored. The square of the leading-twist pion wave function at a special probe scale, $\\zeta_H$, is determined using models and Poincare covariance from realistic results for $q(x)$. This wave function is then used to compute form factors with the result that the Drell-Yan-West and quark counting relationships are not satisfied. A new relationship between $q(x)$ and $F(\\Delta^2)$ is proposed.","sentences":["The relation between the pion's quark distribution function, $q(x)$, its light-front wave function, and the elastic charge form factor, $F(\\Delta^2)$ is explored.","The square of the leading-twist pion wave function at a special probe scale, $\\zeta_H$, is determined using models and Poincare covariance from realistic results for $q(x)$.","This wave function is then used to compute form factors with the result that the Drell-Yan-West and quark counting relationships are not satisfied.","A new relationship between $q(x)$ and $F(\\Delta^2)$ is proposed."],"url":"http://arxiv.org/abs/2403.03356v1","category":"hep-ph"}
{"created":"2024-03-05 22:37:21","title":"Quasiparticle effects in magnetic-field-resilient 3D transmons","abstract":"Recent research shows that quasiparticle-induced decoherence of superconducting qubits depends on the superconducting-gap asymmetry originating from the different thicknesses of the top and bottom films in Al/AlO$_x$/Al junctions. Magnetic field is a key tuning knob to investigate this dependence as it can change the superconducting gaps in situ. We present measurements of the parity-switching time of a field-resilient 3D transmon with in-plane field up to 0.41T. At low fields, small parity splitting requires qutrit pulse sequences for parity measurements. We measure a non-monotonic evolution of the parity lifetime with in-plane magnetic field, increasing up to 0.2T, followed by a decrease at higher fields. We demonstrate that the superconducting-gap asymmetry plays a crucial role in the observed behavior. At zero field, the qubit frequency is nearly resonant with the superconducting-gap difference, favoring the energy exchange with the quasiparticles and so enhancing the parity-switching rate. With a higher magnetic field, the qubit frequency decreases and gets detuned from the gap difference, causing the initial increase of the parity lifetime, while photon-assisted qubit transitions increase, producing the subsequent decrease at higher fields. Besides giving a deeper insight into the parity-switching mechanism in conventional transmon qubits, we establish that Al-AlO$_x$-Al JJs could be used in architectures for the parity-readout and manipulation of topological qubits based on Majorana zero modes.","sentences":["Recent research shows that quasiparticle-induced decoherence of superconducting qubits depends on the superconducting-gap asymmetry originating from the different thicknesses of the top and bottom films in Al/AlO$_x$/Al junctions.","Magnetic field is a key tuning knob to investigate this dependence as it can change the superconducting gaps in situ.","We present measurements of the parity-switching time of a field-resilient 3D transmon with in-plane field up to 0.41T.","At low fields, small parity splitting requires qutrit pulse sequences for parity measurements.","We measure a non-monotonic evolution of the parity lifetime with in-plane magnetic field, increasing up to 0.2T, followed by a decrease at higher fields.","We demonstrate that the superconducting-gap asymmetry plays a crucial role in the observed behavior.","At zero field, the qubit frequency is nearly resonant with the superconducting-gap difference, favoring the energy exchange with the quasiparticles and so enhancing the parity-switching rate.","With a higher magnetic field, the qubit frequency decreases and gets detuned from the gap difference, causing the initial increase of the parity lifetime, while photon-assisted qubit transitions increase, producing the subsequent decrease at higher fields.","Besides giving a deeper insight into the parity-switching mechanism in conventional transmon qubits, we establish that Al-AlO$_x$-Al JJs could be used in architectures for the parity-readout and manipulation of topological qubits based on Majorana zero modes."],"url":"http://arxiv.org/abs/2403.03351v1","category":"quant-ph"}
{"created":"2024-03-05 21:45:00","title":"Correlated scalar perturbations and gravitational waves from axion inflation","abstract":"The scalar and tensor fluctuations produced during inflation can be correlated, if arising from the same underlying mechanism. In this paper we investigate such correlation in the model of axion inflation, where the rolling inflaton produces quanta of a $U(1)$ gauge field which, in turn, source scalar and tensor fluctuations. We compute the primordial correlator of the curvature perturbation, $\\zeta$, with the amplitude of the gravitational waves squared, $h_{ij}h_{ij}$, at frequencies probed by gravitational wave detectors. This two-point function receives two contributions: one arising from the correlation of gravitational waves with the scalar perturbations generated by the standard mechanism of amplification of vacuum fluctuations, and the other coming from the correlation of gravitational waves with the scalar perturbations sourced by the gauge field. Our analysis shows that the latter effect is generally dominant. The correlator, normalized by the amplitude of $\\zeta$ and of $h_{ij}h_{ij}$, turns out to be of the order of $ 10^{-2}\\,\\times (f_{\\rm NL}^{\\rm equil})^{1/3}$, where $f_{\\rm NL}^{\\rm equil}$ measures the scalar bispectrum sourced by the gauge modes.","sentences":["The scalar and tensor fluctuations produced during inflation can be correlated, if arising from the same underlying mechanism.","In this paper we investigate such correlation in the model of axion inflation, where the rolling inflaton produces quanta of a $U(1)$ gauge field which, in turn, source scalar and tensor fluctuations.","We compute the primordial correlator of the curvature perturbation, $\\zeta$, with the amplitude of the gravitational waves squared, $h_{ij}h_{ij}$, at frequencies probed by gravitational wave detectors.","This two-point function receives two contributions: one arising from the correlation of gravitational waves with the scalar perturbations generated by the standard mechanism of amplification of vacuum fluctuations, and the other coming from the correlation of gravitational waves with the scalar perturbations sourced by the gauge field.","Our analysis shows that the latter effect is generally dominant.","The correlator, normalized by the amplitude of $\\zeta$ and of $h_{ij}h_{ij}$, turns out to be of the order of $ 10^{-2}\\,\\times (f_{\\rm NL}^{\\rm equil})^{1/3}$, where $f_{\\rm NL}^{\\rm equil}$ measures the scalar bispectrum sourced by the gauge modes."],"url":"http://arxiv.org/abs/2403.03338v1","category":"astro-ph.CO"}
{"created":"2024-03-05 21:07:43","title":"JWST Reveals CH$_4$, CO$_2$, and H$_2$O in a Metal-rich Miscible Atmosphere on a Two-Earth-Radius Exoplanet","abstract":"Even though sub-Neptunes likely represent the most common outcome of planet formation, their natures remain poorly understood. In particular, planets near 1.5-2.5$\\,R_\\oplus$ often have bulk densities that can be explained equally well with widely different compositions and interior structures, resulting in grossly divergent implications for their formation. Here, we present the full 0.6-5.2$\\,\\mu \\mathrm{m}$ JWST NIRISS/SOSS+NIRSpec/G395H transmission spectrum of the 2.2$\\,R_\\oplus$ TOI-270d ($4.78\\,M_\\oplus$, $T_\\mathrm{eq}$=350-380 K), delivering unprecedented sensitivity for atmospheric characterization in the sub-Neptune regime. We detect five vibrational bands of CH$_4$ at 1.15, 1.4, 1.7, 2.3, and 3.3$\\,\\mu$m (9.4$\\sigma$), the signature of CO$_2$ at 4.3$\\,\\mu$m (4.8$\\sigma$), water vapor (2.5$\\sigma$), and potential signatures of SO$_2$ at 4.0$\\,\\mu \\mathrm{m}$ and CS$_2$ at 4.6$\\,\\mu\\mathrm{m}$. Intriguingly, we find an overall highly metal-rich atmosphere, with a mean molecular weight of $5.47_{-1.14}^{+1.25}$. We infer an atmospheric metal mass fraction of $58_{-12}^{+8}\\%$ and a C/O of $0.47_{-0.19}^{+0.16}$, indicating that approximately half the mass of the outer envelope is in high-molecular-weight volatiles (H$_2$O, CH$_4$, CO, CO$_2$) rather than H$_2$/He. We introduce a sub-Neptune classification scheme and identify TOI-270d as a \"miscible-envelope sub-Neptune\" in which H$_2$/He is well-mixed with the high-molecular-weight volatiles in a miscible supercritical metal-rich envelope. For a fully miscible envelope, we conclude that TOI-270d's interior is $90_{-4}^{+3}\\,$wt$\\,\\%$ rock/iron, indicating that it formed as a rocky planet that accreted a few wt % of H$_2$/He, with the overall envelope metal content explained by magma-ocean/envelope reactions without the need for significant ice accretion. TOI-270d may well be an archetype of the overall population of sub-Neptunes.","sentences":["Even though sub-Neptunes likely represent the most common outcome of planet formation, their natures remain poorly understood.","In particular, planets near 1.5-2.5$\\,R_\\oplus$ often have bulk densities that can be explained equally well with widely different compositions and interior structures, resulting in grossly divergent implications for their formation.","Here, we present the full 0.6-5.2$\\,\\mu \\mathrm{m}$ JWST NIRISS/SOSS+NIRSpec/G395H transmission spectrum of the 2.2$\\,R_\\oplus$ TOI-270d ($4.78\\,M_\\oplus$, $T_\\mathrm{eq}$=350-380 K), delivering unprecedented sensitivity for atmospheric characterization in the sub-Neptune regime.","We detect five vibrational bands of CH$_4$ at 1.15, 1.4, 1.7, 2.3, and 3.3$\\,\\mu$m (9.4$\\sigma$), the signature of CO$_2$ at 4.3$\\,\\mu$m (4.8$\\sigma$), water vapor (2.5$\\sigma$), and potential signatures of SO$_2$ at 4.0$\\,\\mu \\mathrm{m}$ and CS$_2$ at 4.6$\\,\\mu\\mathrm{m}$. Intriguingly, we find an overall highly metal-rich atmosphere, with a mean molecular weight of $5.47_{-1.14}^{+1.25}$. We infer an atmospheric metal mass fraction of $58_{-12}^{+8}\\%$ and a C/O of $0.47_{-0.19}^{+0.16}$, indicating that approximately half the mass of the outer envelope is in high-molecular-weight volatiles (H$_2$O, CH$_4$, CO, CO$_2$) rather than H$_2$/He.","We introduce a sub-Neptune classification scheme and identify TOI-270d as a \"miscible-envelope sub-Neptune\" in which H$_2$/He is well-mixed with the high-molecular-weight volatiles in a miscible supercritical metal-rich envelope.","For a fully miscible envelope, we conclude that TOI-270d's interior is $90_{-4}^{+3}\\,$wt$\\,\\%$ rock/iron, indicating that it formed as a rocky planet that accreted a few wt % of H$_2$/He, with the overall envelope metal content explained by magma-ocean/envelope reactions without the need for significant ice accretion.","TOI-270d may well be an archetype of the overall population of sub-Neptunes."],"url":"http://arxiv.org/abs/2403.03325v1","category":"astro-ph.EP"}
{"created":"2024-03-05 20:51:59","title":"A Novel Method for Clustering Cellular Data to Improve Classification","abstract":"Many fields, such as neuroscience, are experiencing the vast proliferation of cellular data, underscoring the need for organizing and interpreting large datasets. A popular approach partitions data into manageable subsets via hierarchical clustering, but objective methods to determine the appropriate classification granularity are missing. We recently introduced a technique to systematically identify when to stop subdividing clusters based on the fundamental principle that cells must differ more between than within clusters. Here we present the corresponding protocol to classify cellular datasets by combining data-driven unsupervised hierarchical clustering with statistical testing. These general-purpose functions are applicable to any cellular dataset that can be organized as two-dimensional matrices of numerical values, including molecular, physiological, and anatomical datasets. We demonstrate the protocol using cellular data from the Janelia MouseLight project to characterize morphological aspects of neurons.","sentences":["Many fields, such as neuroscience, are experiencing the vast proliferation of cellular data, underscoring the need for organizing and interpreting large datasets.","A popular approach partitions data into manageable subsets via hierarchical clustering, but objective methods to determine the appropriate classification granularity are missing.","We recently introduced a technique to systematically identify when to stop subdividing clusters based on the fundamental principle that cells must differ more between than within clusters.","Here we present the corresponding protocol to classify cellular datasets by combining data-driven unsupervised hierarchical clustering with statistical testing.","These general-purpose functions are applicable to any cellular dataset that can be organized as two-dimensional matrices of numerical values, including molecular, physiological, and anatomical datasets.","We demonstrate the protocol using cellular data from the Janelia MouseLight project to characterize morphological aspects of neurons."],"url":"http://arxiv.org/abs/2403.03318v1","category":"q-bio.QM"}
{"created":"2024-03-05 20:19:34","title":"Know your footprint -- Evaluation of the professional carbon footprint for individual researchers in high energy physics and related fields","abstract":"Understanding the environmental impact of professional activities is becoming paramount in current times, especially within sectors that historically have had significant resource utilisation, such as High Energy Physics (HEP) and related fields. The young High Energy Physicists (yHEP) association launched the Know your footprint (Kyf) campaign to evaluate the CO$_\\text{2}$-equivalent emissions generated by HEP-related research. This study delves into the carbon footprints associated with four distinct categories: Experiments, tied to extensive collaborations with substantial infrastructure; Institutional, representing the resource consumption of research institutes and universities; Computing, focusing on simulations and data analysis; and Travel, covering professional trips such as to conferences, meetings, and workshops. The findings in this assessment are integrated into a tool for self-evaluation, the Know-your-footprint (Kyf) calculator, which allows colleagues to assess their personal and professional footprint, and optionally share their data with the yHEP association. The aim of the Kyf campaign is to heighten awareness, foster sustainability, and inspire the community to adopt more environmentally-responsible research methodologies.","sentences":["Understanding the environmental impact of professional activities is becoming paramount in current times, especially within sectors that historically have had significant resource utilisation, such as High Energy Physics (HEP) and related fields.","The young High Energy Physicists (yHEP) association launched the Know your footprint (Kyf) campaign to evaluate the CO$_\\text{2}$-equivalent emissions generated by HEP-related research.","This study delves into the carbon footprints associated with four distinct categories: Experiments, tied to extensive collaborations with substantial infrastructure; Institutional, representing the resource consumption of research institutes and universities; Computing, focusing on simulations and data analysis; and Travel, covering professional trips such as to conferences, meetings, and workshops.","The findings in this assessment are integrated into a tool for self-evaluation, the Know-your-footprint (Kyf) calculator, which allows colleagues to assess their personal and professional footprint, and optionally share their data with the yHEP association.","The aim of the Kyf campaign is to heighten awareness, foster sustainability, and inspire the community to adopt more environmentally-responsible research methodologies."],"url":"http://arxiv.org/abs/2403.03308v1","category":"physics.soc-ph"}
{"created":"2024-03-05 20:09:10","title":"Wrist-bound Guanxi, Jiazu, and Kuolie: Unpacking Chinese Adolescent Smartwatch-Mediated Socialization","abstract":"Adolescent peer relationships, essential for their development, are increasingly mediated by digital technologies. As this trend continues, wearable devices, especially smartwatches tailored for adolescents, are reshaping their socialization. In China, smartwatches like XTC have gained wide popularity, introducing unique features such as \"Bump-to-Connect\" and exclusive social platforms. Nonetheless, how these devices influence adolescents' peer experience remains unknown. Addressing this, we interviewed 18 Chinese adolescents (age: 11 -- 16), discovering a smartwatch-mediated social ecosystem. Our findings highlight the ice-breaking role of smartwatches in friendship initiation and their use for secret messaging with local peers. Within the online smartwatch community, peer status is determined by likes and visibility, leading to diverse pursuit activities (i.e., chu guanxi, jiazu, kuolie) and negative social dynamics. We discuss the core affordances of smartwatches and Chinese cultural factors that influence adolescent social behavior and offer implications for designing future wearables that responsibly and safely support adolescent socialization.","sentences":["Adolescent peer relationships, essential for their development, are increasingly mediated by digital technologies.","As this trend continues, wearable devices, especially smartwatches tailored for adolescents, are reshaping their socialization.","In China, smartwatches like XTC have gained wide popularity, introducing unique features such as \"Bump-to-Connect\" and exclusive social platforms.","Nonetheless, how these devices influence adolescents' peer experience remains unknown.","Addressing this, we interviewed 18 Chinese adolescents (age: 11 -- 16), discovering a smartwatch-mediated social ecosystem.","Our findings highlight the ice-breaking role of smartwatches in friendship initiation and their use for secret messaging with local peers.","Within the online smartwatch community, peer status is determined by likes and visibility, leading to diverse pursuit activities (i.e., chu guanxi, jiazu, kuolie) and negative social dynamics.","We discuss the core affordances of smartwatches and Chinese cultural factors that influence adolescent social behavior and offer implications for designing future wearables that responsibly and safely support adolescent socialization."],"url":"http://arxiv.org/abs/2403.03306v1","category":"cs.HC"}
{"created":"2024-03-05 20:06:40","title":"The cosmic-ray positron excess and its imprint in the Galactic gamma-ray sky","abstract":"We study the origin of the positron excess observed in the local cosmic-ray spectrum at high energies, and relate it to the cosmic rays and gamma-ray emission across the entire Galaxy. In particular, we explore the hypothesis of a single, dominant source accountable for primary electron-positron pairs. Since we are agnostic about the physical nature of the underlying source population, we consider four simple models that are representative of young pulsars, old stars (as a tracer of millisecond pulsars), and annihilating dark matter particles. In the dark matter hypothesis, we consider both a cored and a cuspy model for the halo in the Milky Way. Then, we compare the associated gamma-ray sky maps with Fermi-LAT data. The aim of this work is not to derive constraints or upper limits for the different models considered, but rather to explore the possibility, as a proof of concept, of building a self-consistent model able to explain simultaneously the origin of all cosmic-ray species, including positrons, as well as the Galactic center GeV gamma-ray emission. We find that the emission arising from pulsar wind nebulae is fairly concentrated near the mid plane, and therefore additional cosmic-ray sources must be invoked to explain the emission at the center of the Galaxy. If the local positron excess were mainly due to millisecond pulsars, inverse Compton scattering by the particles injected in the Milky Way bulge would naturally account for a non-negligible fraction of the central gamma-ray emission. The case of annihilating dark matter is very sensitive to the precise shape of the dark matter profile. The results for a standard NFW cuspy profile are above the gamma-ray measurements by as much as a factor of 2 in some regions of the Galaxy, while the results for an isothermal, cored profile are still compatible with the data. However, the cross-sections exceed the current constraints.","sentences":["We study the origin of the positron excess observed in the local cosmic-ray spectrum at high energies, and relate it to the cosmic rays and gamma-ray emission across the entire Galaxy.","In particular, we explore the hypothesis of a single, dominant source accountable for primary electron-positron pairs.","Since we are agnostic about the physical nature of the underlying source population, we consider four simple models that are representative of young pulsars, old stars (as a tracer of millisecond pulsars), and annihilating dark matter particles.","In the dark matter hypothesis, we consider both a cored and a cuspy model for the halo in the Milky Way.","Then, we compare the associated gamma-ray sky maps with Fermi-LAT data.","The aim of this work is not to derive constraints or upper limits for the different models considered, but rather to explore the possibility, as a proof of concept, of building a self-consistent model able to explain simultaneously the origin of all cosmic-ray species, including positrons, as well as the Galactic center GeV gamma-ray emission.","We find that the emission arising from pulsar wind nebulae is fairly concentrated near the mid plane, and therefore additional cosmic-ray sources must be invoked to explain the emission at the center of the Galaxy.","If the local positron excess were mainly due to millisecond pulsars, inverse Compton scattering by the particles injected in the Milky Way bulge would naturally account for a non-negligible fraction of the central gamma-ray emission.","The case of annihilating dark matter is very sensitive to the precise shape of the dark matter profile.","The results for a standard NFW cuspy profile are above the gamma-ray measurements by as much as a factor of 2 in some regions of the Galaxy, while the results for an isothermal, cored profile are still compatible with the data.","However, the cross-sections exceed the current constraints."],"url":"http://arxiv.org/abs/2403.03303v1","category":"astro-ph.HE"}
{"created":"2024-03-05 19:51:52","title":"CenterDisks: Real-time instance segmentation with disk covering","abstract":"Increasing the accuracy of instance segmentation methods is often done at the expense of speed. Using coarser representations, we can reduce the number of parameters and thus obtain real-time masks. In this paper, we take inspiration from the set cover problem to predict mask approximations. Given ground-truth binary masks of objects of interest as training input, our method learns to predict the approximate coverage of these objects by disks without supervision on their location or radius. Each object is represented by a fixed number of disks with different radii. In the learning phase, we consider the radius as proportional to a standard deviation in order to compute the error to propagate on a set of two-dimensional Gaussian functions rather than disks. We trained and tested our instance segmentation method on challenging datasets showing dense urban settings with various road users. Our method achieve state-of-the art results on the IDD and KITTI dataset with an inference time of 0.040 s on a single RTX 3090 GPU.","sentences":["Increasing the accuracy of instance segmentation methods is often done at the expense of speed.","Using coarser representations, we can reduce the number of parameters and thus obtain real-time masks.","In this paper, we take inspiration from the set cover problem to predict mask approximations.","Given ground-truth binary masks of objects of interest as training input, our method learns to predict the approximate coverage of these objects by disks without supervision on their location or radius.","Each object is represented by a fixed number of disks with different radii.","In the learning phase, we consider the radius as proportional to a standard deviation in order to compute the error to propagate on a set of two-dimensional Gaussian functions rather than disks.","We trained and tested our instance segmentation method on challenging datasets showing dense urban settings with various road users.","Our method achieve state-of-the art results on the IDD and KITTI dataset with an inference time of 0.040 s on a single RTX 3090 GPU."],"url":"http://arxiv.org/abs/2403.03296v1","category":"cs.CV"}
{"created":"2024-03-05 19:38:54","title":"Spatially resolving the AGB star V3 in the metal-poor globular cluster 47 Tuc with VLTI/GRAVITY","abstract":"Mass loss at the asymptotic giant branch (AGB) plays an important role not only in the final fates of stars, but also in the chemical evolution of galaxies. Nevertheless, the metallicity effects on AGB mass loss are not yet fully understood. We present spatially resolved observations of an AGB star, V3, in the metal-poor globular cluster 47 Tuc (NGC 104). The AGB star 47 Tuc V3 was observed using the GRAVITY instrument at ESO's Very Large Telescope Interferometer (VLTI) at 2-2.45 micron with a projected baseline length of up to 96 m. The object 47 Tuc V3 has been spatially resolved and stands as the first to attempt to spatially resolve an individual star in a globular cluster. The uniform-disk fit to the observed data results in an angular diameter of ~0.7 mas. Our modeling of the spectral energy distribution and near-infrared interferometric GRAVITY data suggests that the observed data can be explained by an optically thin dust shell with a 0.55 micron optical depth of 0.05-0.25, consisting of metallic iron grains, likely together with effects of the extended atmosphere of the central star. The dust temperature at the inner shell boundary is 500-800 K (corresponding to 23-90 stellar radii), significantly lower than observed in nearby oxygen-rich AGB stars. Radiation pressure on small (< 0.05 micron) iron grains is not sufficient to drive stellar winds. Therefore, iron grains may grow to larger sizes, even in the metal-poor environment. Alternatively, it is possible that the observed iron grain formation is a result of the mass outflow initiated by some other mechanism(s). The sensitivity and angular resolution of VLTI provides a new window onto spatially resolving individual stars in metal-poor globular clusters. This allows us to improve subsequent studies of the metallicity dependence of dust formation and mass loss.","sentences":["Mass loss at the asymptotic giant branch (AGB) plays an important role not only in the final fates of stars, but also in the chemical evolution of galaxies.","Nevertheless, the metallicity effects on AGB mass loss are not yet fully understood.","We present spatially resolved observations of an AGB star, V3, in the metal-poor globular cluster 47 Tuc (NGC 104).","The AGB star 47 Tuc V3 was observed using the GRAVITY instrument at ESO's Very Large Telescope Interferometer (VLTI) at 2-2.45 micron with a projected baseline length of up to 96 m. The object 47 Tuc V3 has been spatially resolved and stands as the first to attempt to spatially resolve an individual star in a globular cluster.","The uniform-disk fit to the observed data results in an angular diameter of ~0.7 mas.","Our modeling of the spectral energy distribution and near-infrared interferometric GRAVITY data suggests that the observed data can be explained by an optically thin dust shell with a 0.55 micron optical depth of 0.05-0.25, consisting of metallic iron grains, likely together with effects of the extended atmosphere of the central star.","The dust temperature at the inner shell boundary is 500-800 K (corresponding to 23-90 stellar radii), significantly lower than observed in nearby oxygen-rich AGB stars.","Radiation pressure on small (< 0.05 micron) iron grains is not sufficient to drive stellar winds.","Therefore, iron grains may grow to larger sizes, even in the metal-poor environment.","Alternatively, it is possible that the observed iron grain formation is a result of the mass outflow initiated by some other mechanism(s).","The sensitivity and angular resolution of VLTI provides a new window onto spatially resolving individual stars in metal-poor globular clusters.","This allows us to improve subsequent studies of the metallicity dependence of dust formation and mass loss."],"url":"http://arxiv.org/abs/2403.03287v1","category":"astro-ph.SR"}
{"created":"2024-03-05 19:36:24","title":"Bootstrapping AdS$_2 \\times$ S$^2$ hypermultiplets: hidden four-dimensional conformal symmetry","abstract":"We bootstrap the $4$-point amplitude of $\\mathcal{N}=2$ hypermultiplets in $\\text{AdS}_2 \\times \\text{S}^2$ at tree-level and for arbitrary external weights. We hereby explicitly demonstrate the existence of a hidden four-dimensional conformal symmetry that was used as an assumption in previous studies to derive this result.","sentences":["We bootstrap the $4$-point amplitude of $\\mathcal{N}=2$ hypermultiplets in $\\text{AdS}_2 \\times \\text{S}^2$ at tree-level and for arbitrary external weights.","We hereby explicitly demonstrate the existence of a hidden four-dimensional conformal symmetry that was used as an assumption in previous studies to derive this result."],"url":"http://arxiv.org/abs/2403.03285v1","category":"hep-th"}
{"created":"2024-03-05 19:18:00","title":"Bright coherent attosecond X-ray pulses from beam-driven relativistic mirrors","abstract":"Bright ultrashort X-ray pulses allow scientists to observe ultrafast motion of atoms and molecules. Coherent light sources, such as the X-ray free electron laser (XFEL), enable remarkable discoveries in cell biology, protein crystallography, chemistry or materials science. However, in contrast to optical lasers, lack of X-ray mirrors demands XFELs to amplify radiation over a single pass, requiring tens or hundreds of meters long undulators to produce bright femtosecond X-ray pulses. Here, we propose a new ultrafast coherent light source based on laser reflection from a relativistic mirror driven by a relativistic charged particle beam in micrometer-scale plasma. We show that reflection of millijoule-level laser pulses from such mirrors can produce bright, coherent and bandwidth-tunable attosecond X-ray pulses with peak intensity and spectral brightness comparable to XFELs. In addition, we find that beam-driven relativistic mirrors are highly robust, with laser-induced damage threshold exceeding solid-state components by at least two orders of magnitude. Our results promise a new way for bright coherent attosecond X-ray pulse generation, suitable for unique applications in fundamental physics, biology and chemistry.","sentences":["Bright ultrashort X-ray pulses allow scientists to observe ultrafast motion of atoms and molecules.","Coherent light sources, such as the X-ray free electron laser (XFEL), enable remarkable discoveries in cell biology, protein crystallography, chemistry or materials science.","However, in contrast to optical lasers, lack of X-ray mirrors demands XFELs to amplify radiation over a single pass, requiring tens or hundreds of meters long undulators to produce bright femtosecond X-ray pulses.","Here, we propose a new ultrafast coherent light source based on laser reflection from a relativistic mirror driven by a relativistic charged particle beam in micrometer-scale plasma.","We show that reflection of millijoule-level laser pulses from such mirrors can produce bright, coherent and bandwidth-tunable attosecond X-ray pulses with peak intensity and spectral brightness comparable to XFELs.","In addition, we find that beam-driven relativistic mirrors are highly robust, with laser-induced damage threshold exceeding solid-state components by at least two orders of magnitude.","Our results promise a new way for bright coherent attosecond X-ray pulse generation, suitable for unique applications in fundamental physics, biology and chemistry."],"url":"http://arxiv.org/abs/2403.03277v1","category":"physics.plasm-ph"}
{"created":"2024-03-05 19:14:28","title":"A two-line representation of stationary measure for open TASEP","abstract":"We show that the stationary measure for the totally asymmetric simple exclusion process on a segment with open boundaries is given by a marginal of a two-line measure with a simple and explicit description. We use this representation to analyze asymptotic fluctuations of the height function near the triple point for a larger set of parameters than was previously studied. As a second application, we determine a single expression for the rate function in the large deviation principle for the height function in the fan and in the shock region. We then discuss how this expression relates to the expressions available in the literature.","sentences":["We show that the stationary measure for the totally asymmetric simple exclusion process on a segment with open boundaries is given by a marginal of a two-line measure with a simple and explicit description.","We use this representation to analyze asymptotic fluctuations of the height function near the triple point for a larger set of parameters than was previously studied.","As a second application, we determine a single expression for the rate function in the large deviation principle for the height function in the fan and in the shock region.","We then discuss how this expression relates to the expressions available in the literature."],"url":"http://arxiv.org/abs/2403.03275v1","category":"math.PR"}
